{
    "abstractText": "Controlling fish feeding machine is challenging problem because experienced fishermen can adequately control based on assumption. To build robust method for reasonable application, we propose automatic controlling fish feeding machine based on computer vision using combination of counting nutriments and estimating ripple behavior using regression and textural feature, respectively. To count number of nutriments, we apply object detection and tracking methods to acknowledge the nutriments moving to sea surface. Recently, object tracking is active research and challenging problem in computer vision. Unfortunately, the robust tracking method for multiple small objects with dense and complex relationships is unsolved problem in aquaculture field with more appearance creatures. Based on the number of nutriments and ripple behavior, we can control fish feeding machine which consistently performs well in real environment. Proposed method presents the agreement for automatic controlling fish feeding by the activation graphs and textural feature of ripple behavior. Our tracking method can precisely track the nutriments in next frame comparing with other methods. Based on computational time, proposed method reaches 3.86 fps while other methods spend lower than 1.93 fps. Quantitative evaluation can promise that proposed method is valuable for aquaculture fish farm with widely applied to real environment.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hilmil Pradana"
        },
        {
            "affiliations": [],
            "name": "Keiichi Horio"
        },
        {
            "affiliations": [],
            "name": "KEIICHI HORIO"
        }
    ],
    "id": "SP:7c84041b72f471f207064eec9795d01e0c2dc54f",
    "references": [
        {
            "authors": [
                "R. Subasinghe",
                "D. Soto",
                "J. Jia"
            ],
            "title": "Global aquaculture and its role in sustainable development",
            "venue": "Rev.Aquaculture, vol.1, no.1, pp.2\u20139",
            "year": 2009
        },
        {
            "authors": [
                "I. Aliyu",
                "K.J. Gana",
                "A.A. Musa",
                "J. Agajo",
                "A.M. Orire",
                "F.T. Abiodun",
                "M.A. Adegboye"
            ],
            "title": "A proposed fish counting algorithm using digital image processing technique",
            "venue": "ATBU J. Sci., Technol. Educ., vol.5, no.1, pp.1\u201311",
            "year": 2017
        },
        {
            "authors": [
                "J. Guillen",
                "F. Natale"
            ],
            "title": "N",
            "venue": "Carvalho et al., Global seafood consumption footprint. Ambio, vol.48, pp.111\u2013122",
            "year": 2019
        },
        {
            "authors": [
                "Y. Atoum"
            ],
            "title": "Automatic Feeding Control for Dense Aquaculture Fish Tanks",
            "venue": "IEEE Signal Process. Lett., vol.22,",
            "year": 2015
        },
        {
            "authors": [
                "P.C. Oostlander"
            ],
            "title": "Microalgae production cost in aquaculture hatcheries",
            "venue": "Aquaculture, vol.525,",
            "year": 2020
        },
        {
            "authors": [
                "J.E. Andrew",
                "C. Noble",
                "S. Kadri"
            ],
            "title": "H",
            "venue": "Jewell, and F.A. Huntingford, The effect of demand feeding on swimming speed and feeding responses in Atlantic salmon Salmo salar L., gilthead sea bream Sparus aurata L. and European sea bass Dicentrarchus labrax L. in sea cages. Aquac. Res., vol.33, pp.501\u2013507",
            "year": 2002
        },
        {
            "authors": [
                "C. Rillahan",
                "M. Chambers",
                "W.H. Howell",
                "W.H. Watson"
            ],
            "title": "A self-contained system for observing and quantifying the behavior of Atlantic cod",
            "venue": "Gadus morhua, in an offshore aquaculture cage. Aquaculture, vol.293, pp.49\u201356",
            "year": 2009
        },
        {
            "authors": [
                "J.F. L\u00f3pez-Olmeda",
                "C. Noble",
                "F.J. S\u00e1nchez-V\u00e1zquez"
            ],
            "title": "Does feeding time affect fish welfare? Fish Physiol",
            "venue": "Biochem., vol.38, pp.143\u2013152",
            "year": 2012
        },
        {
            "authors": [
                "T. Asaeda",
                "T.K. Vu",
                "J. Manatunge"
            ],
            "title": "Effects of flow velocity on feeding behavior and microhabitat selection of the stone Moroko Pseudorasbora parva: a trade-off between feeding and swimming costs",
            "venue": "Trans. Am. Fish. Soc., vol.134, pp.537\u2013547",
            "year": 2005
        },
        {
            "authors": [
                "H. Tanoue",
                "T. Komatsu",
                "T. Tsujino",
                "I. Suzuki",
                "M. Watanabe",
                "H. Goto",
                "N. Miyazaki"
            ],
            "title": "Feeding events of Japanese lates Lates japonicus detected by a high-speed video camera and three-axis microacceleration data-logger",
            "venue": "Fish. Sci., vol.78, pp.533\u2013538",
            "year": 2012
        },
        {
            "authors": [
                "T. Noda",
                "Y. Kawabata",
                "N. Arai",
                "H. Mitamura",
                "S. Watanabe"
            ],
            "title": "Animal-mounted gyroscope/accelerometer/magnetometer: in situ measurement of the movement performance of fast-start behavior in fish",
            "venue": "J. Exp. Mar. Biol. Ecol., vol.451, pp.55\u201368",
            "year": 2014
        },
        {
            "authors": [
                "J. Horie",
                "H. Mitamura",
                "Y. Ina",
                "Y. Mashino",
                "T. Noda",
                "K. Moriya",
                "N. Arai",
                "T. Sasakura"
            ],
            "title": "Development of a method for classifying and transmitting high-resolution feeding behavior of fish using an acceleration pinger",
            "venue": "Anim. Biotelem, vol.5",
            "year": 2017
        },
        {
            "authors": [
                "A.W. Stoner",
                "M.L. Ottmar",
                "T.P Hurst"
            ],
            "title": "Temperature affects activity and feeding motivation in Pacific halibut: implications for bait-dependent fishing",
            "venue": "Fish. Res., vol.81, pp.202\u2013209",
            "year": 2006
        },
        {
            "authors": [
                "G.M. Soto-Zaraz\u00faa",
                "E. Rico-Gar\u0107\u0131a",
                "R. Ocampo",
                "R.G. Guevara-Gonz\u00e1lez",
                "G. Herrera-Ruiz"
            ],
            "title": "Fuzzy-logic-based feeder system for intensive tilapia production (Oreochromis niloticus)",
            "venue": "Aquac. Int., vol.18, pp.379\u2013391",
            "year": 2010
        },
        {
            "authors": [
                "T. Wu",
                "Y. Huang",
                "J. Chen"
            ],
            "title": "Development of an adaptive neural-based fuzzy inference system for feeding decision-making assessment in silver perch (Bidyanus bidyanus) culture",
            "venue": "Aquac. Eng., vol.66, pp.41\u201351",
            "year": 2015
        },
        {
            "authors": [
                "S. Zhao",
                "W. Ding",
                "S. Zhao",
                "J. Gu"
            ],
            "title": "Adaptive neural fuzzy inference system for feeding decisionmaking of grass carp (Ctenopharyngodon idellus) in outdoor intensive culturing ponds",
            "venue": "Aquaculture, vol.498, pp.28\u201336",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhou",
                "K. Lin",
                "D. Xu",
                "L. Chen",
                "Q. Guo",
                "C. Sun",
                "X. Yang"
            ],
            "title": "Near infrared computer vision and neuro-fuzzy model-based feeding decision system for fish in aquaculture",
            "venue": "Comput. Electron. Agric., vol.146, pp.114\u2013124",
            "year": 2018
        },
        {
            "authors": [
                "C. Zhou",
                "D. Xu",
                "K. Lin",
                "C. Sun",
                "X. Yang"
            ],
            "title": "Intelligent feeding control methods in aquaculture with an emphasis on fish: a review",
            "venue": "Rev. Aquac., vol.10, pp.975\u2013993",
            "year": 2018
        },
        {
            "authors": [
                "A. Pautsina",
                "P. \u0106\u0131sa\u0159",
                "D. \u0160tys",
                "B.F. Terjesen",
                "A.M.O. Espmark"
            ],
            "title": "Infrared reflection system for indoor 3D tracking of fish",
            "venue": "Aquac. Eng., vol.69, pp.7\u201317",
            "year": 2015
        },
        {
            "authors": [
                "C. Hung",
                "S. Tsao",
                "K. Huang",
                "J. Jang",
                "H. Chang",
                "F.C. Dobbs"
            ],
            "title": "A highly sensitive underwater video system for use in turbid aquaculture ponds",
            "venue": "Sci. Rep., vol.6",
            "year": 2016
        },
        {
            "authors": [
                "C. Zhou",
                "B. Zhang",
                "K. Lin",
                "D. Xu",
                "C. Chen",
                "X. Yang",
                "C. Sun"
            ],
            "title": "Near-infrared imaging to quantify the feeding behavior of fish in aquaculture",
            "venue": "Comput. Electron. Agric., vol.135, pp.233\u2013241",
            "year": 2017
        },
        {
            "authors": [
                "Z. Liu",
                "X. Li",
                "L. Fan",
                "H. Lu",
                "L. Liu",
                "Y. Liu"
            ],
            "title": "Measuring feeding activity of fish in RAS using computer vision",
            "venue": "Aquac. Eng., vol.60, pp.20\u201327",
            "year": 2014
        },
        {
            "authors": [
                "Q. Guo",
                "X. Yang",
                "C. Zhou",
                "K. Li",
                "C. Sun",
                "M. Chen"
            ],
            "title": "Fish feeding behavior detection method based on shape and texture features",
            "venue": "J. Shanghai Ocean Univ., vol.27, pp.181\u2013189",
            "year": 2018
        },
        {
            "authors": [
                "C. Zhou",
                "D. Xu",
                "L. Chen",
                "S. Zhang",
                "C. Sun",
                "X. Yang",
                "Y. Wang"
            ],
            "title": "Evaluation of fish feeding intensity in aquaculture using a convolutional neural network and machine vision",
            "venue": "Aquaculture, vol.507, pp.457\u2013465",
            "year": 2019
        },
        {
            "authors": [
                "B. Zion"
            ],
            "title": "The use of computer vision technologies in aquaculture - a review",
            "venue": "Comput. Electron. Agric., vol.88, pp.125\u2013132",
            "year": 2012
        },
        {
            "authors": [
                "S.G. Hassan",
                "M. Hasan",
                "D. Li"
            ],
            "title": "Information fusion in aquaculture: a state-of the art review",
            "venue": "Front. Agr. Sci. Eng. vol.3, pp.206",
            "year": 2016
        },
        {
            "authors": [
                "M. Saberioon",
                "A. Gholizadeh",
                "P. Cisar",
                "A. Pautsina",
                "J. Urban"
            ],
            "title": "Application of machine vision systems in aquaculture with emphasis on fish: state-of-the-art and key issues",
            "venue": "Rev. Aquac. vol.9, pp.369\u2013387",
            "year": 2017
        },
        {
            "authors": [
                "A. Soetedjo",
                "I.K. Somawirata"
            ],
            "title": "Improving Traffic Sign Detection by Combining MSER and Lucas Kanade Tracking",
            "venue": "International Journal of Innovative Computing, Information and Control (IJI- CIC), vol.15, no.2",
            "year": 2019
        },
        {
            "authors": [
                "Y. Hashimoto",
                "H. Hama",
                "T.T. Zin"
            ],
            "title": "Robust Tracking of Cattle using Super Pixels and Local Graph Cut for Monitoring Systems",
            "venue": "International Journal of Innovative Computing, Information and Control (IJICIC), vol.16, no.4",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wang"
            ],
            "title": "Towards Real-Time Multi-Object Tracking",
            "venue": "CoRR, vol.abs/1909.12605,",
            "year": 2019
        },
        {
            "authors": [
                "M. Babaee"
            ],
            "title": "A dual CNN-RNN for multiple people tracking",
            "venue": "Neurocomputing, vol.368,",
            "year": 2019
        },
        {
            "authors": [
                "F. Yu"
            ],
            "title": "POI: Multiple Object Tracking with High Performance Detection and Appearance Feature",
            "venue": "Computer Vision - ECCV 2016,",
            "year": 2016
        },
        {
            "authors": [
                "W. Lin"
            ],
            "title": "Real-time multi-object tracking with hyper-plane matching",
            "venue": "Technical report,",
            "year": 2017
        },
        {
            "authors": [
                "S. Tang"
            ],
            "title": "Multiple People Tracking by Lifted Multicut and Person Re-identification",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Y. Zhang"
            ],
            "title": "A Simple Baseline for Multi-Object Tracking",
            "venue": "CoRR, vol.abs/2004.01888,",
            "year": 2020
        },
        {
            "authors": [
                "B. Pang"
            ],
            "title": "TubeTK: Adopting Tubes to Track Multi-Object in a One-Step Training Model",
            "year": 2006
        },
        {
            "authors": [
                "H. Pradana"
            ],
            "title": "and K",
            "venue": "Horio., Tuna Nutriment Tracking using Trajectory Mapping in Application to Aquaculture Fish Tank. 2020 Digital Image Computing: Techniques and Applications, pp.1\u20138",
            "year": 2020
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "NIPS 2015, vol.1, pp.91\u201399",
            "year": 2015
        },
        {
            "authors": [
                "Z. Zhu",
                "Q. Wang",
                "B. Li",
                "W. Wu",
                "J. Yan",
                "W. Hu"
            ],
            "title": "Distractor-aware siamese networks for visual object tracking",
            "venue": "ECCV, pp.101\u2013117",
            "year": 2018
        },
        {
            "authors": [
                "B. Li",
                "W. Wu",
                "Q. Wang",
                "F. Zhang",
                "J. Xing",
                "J. Yan"
            ],
            "title": "Siamrpn++: Evolution of siamese visual tracking with very deep networks",
            "venue": "CVPR, pp.4282\u20134291",
            "year": 2019
        },
        {
            "authors": [
                "K. Simoyan",
                "A. Zisserman"
            ],
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "venue": "ArXiv",
            "year": 2015
        },
        {
            "authors": [
                "A. Bochkovskiy",
                "C.Y. Wang",
                "H.Y. Liao"
            ],
            "title": "YOLOv4: Optimal Speed and Accuracy of Object Detection",
            "venue": "CoRR, vol.507",
            "year": 2020
        },
        {
            "authors": [
                "J. Redmon",
                "A. Farhadi"
            ],
            "title": "YOLOv3: An Incremental Improvement",
            "venue": "CoRR, vol.abs/1804.02767",
            "year": 2018
        },
        {
            "authors": [
                "A. Ess"
            ],
            "title": "Depth and Appearance for Mobile Scene Analysis",
            "venue": "ICCV 2007,",
            "year": 2007
        },
        {
            "authors": [
                "L. Leal-Taixe"
            ],
            "title": "Towards a Benchmark for Multi-Target Tracking",
            "venue": "MOTChallenge",
            "year": 2015
        },
        {
            "authors": [
                "A. Milan"
            ],
            "title": "MOT16: A Benchmark for Multi-Object Tracking",
            "venue": "CoRR, vol.abs/1603.00831,",
            "year": 2016
        },
        {
            "authors": [
                "Bewley"
            ],
            "title": "Simple online and realtime tracking",
            "venue": "ICIP 2016,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "1. Introduction. Raising fish utilization to complete the demand of human and animal resourcing in worldwide drives researchers exploring and improving technology in aquaculture industry [1\u20133]. Based on global aquaculture production statistic database [4], the proportion of aquatic animals farmed is 55.1 million tonnes in 2009 and significantly increasing to 82.1 million tonnes in 2018. This trend indicates that aquaculture industry has to create new technique to enlarge economic scale with reducing production cost and increasing production efficiency.\nOptimizing fish feeding process is the most influential aspect because the process itself takes up to 40 percent of total production cost [5\u20137]. Increasing company profit with controlling adjustment of nutriments is current problem of active aquaculture fish farm without wasting nutriments and dropping quality of fishes. Wasting nutriments create a big effect in water pollution because quality of water is highly related to ensure survival rate of fishes and to enlarge fish fertility rate. Adjustment for giving nutriments is critical component to manage aquaculture development related to quality of water. From economic aspect, controlling fish feeding gives more benefits and determines quality of management expenses for aquaculture industry.\n1\nar X\niv :2\n20 8.\n07 01\n1v 1\n[ cs\n.C V\n] 1\n5 A\nug 2\nRecently, there are numbers of researchers creating several techniques to control amount of nutriments given to fishes. Those techniques can be defined as mechanical device controls [8\u201318], and computer vision approaches [5,19\u201329]. Mechanical device control uses external sensor which has different function for monitoring, identifying, and evaluating fish feeding behavior [8\u201311]. External sensors can be categorized by acceleration sensors attached inside fish body and sensors related to water quality parameters. Accelerative data loggers is used to identify and to classify different kind of prey or nutriment types [11\u2013 14]. In conclusion, this sensor has higher accuracy and lower false detection rate. However, it can create injury to fish body and installation sensor is costly for each fish body. Another type of safe sensor, without harming fish body [15\u201318], is measurable sensors for detecting alteration of temperature and oxygen concentration. These sensors attach in underwater fish tank and calculate changing flow rate and temperature of the water caused by the activity of fish. Although these sensors create higher accuracy and prove save cost of fish feeding up to 29%, they can be easily interrupted by other parameters such as weather temperature.\nOn the other hand, computer vision approach has been commonly used in aquaculture industry because it performs in real-time controlling and low cost for maintainable equipment [27\u201329]. This approach can be useful to classify gender and species, age and size measurement, quantity and quality inspection, counting and monitoring fish behaviors [19,20]. Based on wavelength signal imaging, it can be categorized into infrared [19\u201323] and visible light [5,24\u201326]. The input video from infrared light is suitable to identify fish behavior and achieves better imaging effects under low visible light condition. However, it is suitable to identity fish behavior, analyzing of infrared imaging is remaining unsolved because object appearance from this imaging is unlike real condition. Beside that, application of detecting fish behavior using visible light gives several problems which only focuses on laboratory culture model [27\u201329]. It shows that the methods applied on laboratory culture model perform worst in real environment because several conditions cannot be anticipated such as flying bird, waste, appearance of fish tank boundary, and other undesirable objects. Automatic controlling fish feeding in real environment is still challenging problem in aquaculture field because data appearance and weather condition can affect accuracy of detecting and tracking results. To analyze fish feeding behavior, two feature extractions such as tracking approach using regression and estimating ripple behavior using textural analysis are applied to control fish feeding machine which gives valuable for aquaculture fish farm with widely applied to real environment datasets.\nObject tracking is active research in applications to computer vision [30\u201339]. Not only single-object-tracking but also multiple-object-tracking is unsolved problem since multiple-object association requires correctly collision in sequence frames. Multipleobject-tracking is a more reliable solution rather than applying many of single-objecttracking. The tracking problem can closely be determined to detection problem and even can be interpreted as an extension of object detection problem. Over the past decades, there are number of researchers focused on developing tracking-by-detection to generate more accurate and faster determination of object tracking [32\u201339]. The tracking-bydetection method is the most popular tracking paradigm because object tracking is result of detection with same label in sequential frame. Good quality of the detection algorithms determines tracking result and these algorithms mostly use convolutional neural networks (CNNs) for the detection step. Proving that object tracking is an extension of object detection techniques is that the region proposal network (RPN), one of faster R-CNN detector [40], adopts SiamRPN tracker [41,42]. Multi-aspect-ratio anchors resolve bounding box estimation problem which has interference from previous trackers and significantly enhances the performance of siamese-network-based trackers. Unfortunately, the robust\ntracking method for multiple small objects with dense and complex relationships is still challenging problem in aquaculture field with more appearance creatures.\nThe idea of textural feature for understanding ripple behavior is applied from image quality assessment(IQA) index which can be represented as quantity of human perception referred to quality of image. Since texture is represented in photographic images, it is important to develop objective IQA index which has consistent value with perceptual similarity. In past decades, mean square error (MSE) and structural similarity (SSIM) had been the standard approach to evaluate the signal accuracy and quality although these approaches had the poor correlation result with human assumption and perception. Estimation of ripple behavior can also be defined by human assumption of which the size and number of ripple can be used to adjust the activity level of ripple.\nIn this paper, we present an automatic controlling fish feeding machine using combination of two feature extractions which estimate nutriment position and ripple behavior using regression and textural feature, respectively. Proposed method can ignore undesirable object by understanding behavior for each tracking object. Although proposed method cannot reach higher accuracy comparing use of sensors [11\u201318], the advantage of our method can reduce equipment costs with requiring single camera placed above vessel with a highly distraction of ocean wave and innumerable small and dense nutriments. Our method is constructed by object detection of nutriment and ripple. Regression model is used to predict each detection of nutriment in next frame. Estimating ripple behavior is determined by textural feature with ripple detection area as input image to compute global variance of VGG network [43]. To summarize our work, we present several contributions:\n\u2022 We propose a new novel automatic controlling fish feeding machine using feature extraction of nutriment and ripple behavior which can be useful to improve the production profit in fish farms by controlling the amount of nutriment in optimal rate and to optimize the use of fish feeding machine. \u2022 We propose tracking approach using regression model which outperforms result com-\npared with state-of-the-art multiple object tracking methods. \u2022 We present the advantage of proposed method which can perform well in real envi-\nronment datasets.\nThe remainder of this paper is structured as follows. In Section 2, we briefly present regression and textural feature of ripple behavior technique to give robust solution performance and learning accuracy by combining regression as tracking approach and textural feature for estimating of ripple behavior. Experimental section containing the information of datasets, evaluation approach, and quantitative evaluation for regression and textural feature of ripple behavior is depicted in Section 3. Evaluation result and performance of proposed method and comparison with state-of-the-art benchmark methods are presented in Sections 4. In Section 5, we then review advantages of proposed method, limitations and the conclusions.\n2. Proposed Method. Our formulation is based on tracking algorithm using regression where we estimate each nutriment detection in next frame and textural feature of ripple behavior as input image to compute global variance of VGG network. In order to provide some background and formally introduce our approach, we firstly provide diagram and algorithm of regression and textural feature of ripple behavior approaches. We then explain how the proposed method works to real environment. The proposed method contains three parts: detection, regression and textural feature of ripple behavior which are shown in Figure 1 where the detection result is represented as black color value and\npredicted result of regression is shown by red color value. Regression contains both transformation and inverse transformation, regression approach and counting nutriments while textural feature of ripple behavior uses VGG network as extracted features to compute global variance of VGG network. We also present the algorithm of the proposed method in Algorithm 1.\n2.1. Object Detection. The idea of object detection is to find bounding box in each nutriment and ripple area associated in regression and textural feature. In implementation of object detection, YOLOv4 [44] has different part of object detectors: backbone, neck, and head. Backbone uses CSPDarknet53 as feature-extractor model and is more suitable\nAlgorithm 1: Regression and textural feature of ripple behavior algorithm\nInput : D: a fish feeding video data which contains f frames. Auxiliary methods: Tuna nutriment detection, transformation, inverse\ntransformation, regression, counting nutriments, VGG features.\nOutput : A graph of regression and textural feature of behavior 1 input a fish feeding video data D. 2 for \u03d1\u2190 1 to f do 3 calculate bounding box of tuna nutriment detection \u03d5t(\u03d1) and ripple area detection \u03d5r(\u03d1). 4 create an area of top-left points tl\u03d11,2 = (x\u0302 tl \u03d1 , y\u0302 tl \u03d1 ), top-right points\ntr\u03d11,2 = (x\u0302 br \u03d1 , y\u0302 tl \u03d1 ), bottom-left points bl\u03d11,2 = (x\u0302 tl \u03d1 , y\u0302 br \u03d1 ) and bottom-right points br\u03d11,2 = (x\u0302 br \u03d1 , y\u0302 br \u03d1 ) of ripple detection.\n5 compute normalized nutriments \u03d5\u03ba(\u03d1) from geometry transformation \u03d5\u03be(\u03d1) and rotated nutriments \u03d5\u03c8(\u03d1). 6 predict position of normalized nutriments \u03d5\u03ba(\u03d1+ 1) in next frame using regression model \u00b5x and \u00b5y. 7 transform the result of regression \u03d5\u03ba(\u03d1+ 1) into original pixel position in real\nimages \u03d5t(\u03d1+ 1) using rotated nutriments \u03d5\u03c8(\u03d1+ 1) and geometry transformation \u03d5\u03be(\u03d1+ 1).\n8 compute passed line yl(\u03d1) = \u03b1\u03d1x l \u03d1 + \u03b2\u03d1 to count the nutriment. 9 calculate global variance of VGG network \u03c3\u03d1 using variance in each feature\nmaps in ith VGG convolution layers \u03c3 (i) \u03d1 and variance of each feature maps \u03c3 (i) \u03d1(j).\n10 end\nfor classification than for detection. Neck utilizes spatial pyramid pooling (SPP) which is pooling layer removing fixed size constraint of network and path aggregation network (PAN) which aims to improve the information flow of segmentation network. The latter of neck architecture has different forms to the original PAN and it is modified version with replacing the addition layer with a concat. On the other hand, the head of YOLOv4 [44] has similar form with YOLOv3 [45] which computes training model with bounding box of object detection B = (x\u0302b, y\u0302b, w\u0302b, h\u0302b) and bounding box G = (xg, yg, wg, hg) of ground truth data where xg, yg, wg, and hg are center x, center y, width, and height of bounding box in ground truth data, respectively. \u03bbx and \u03bby represent the absolute top-left corner location of the current grid cell. w and h are width and height referenced to size of image. Bounding box prediction B can be defined as:\nx\u0302b = \u03b6(xg) + \u03bbx,\ny\u0302b = \u03b6(yg) + \u03bby,\nw\u0302b = ew g \u2217 w,\nh\u0302b = eh g \u2217 h,\n(1)\nwhere \u03b6 is model reffed to [44].\n2.2. Regression Approach. Given \u03d5t(f) = {Tf1, Tf2, \u00b7 \u00b7 \u00b7 , Tfa} is tuna nutriment detection a in frame f . For each detection, it is represented by Tfa = (x\u0302 t fa, y\u0302 t fa, w\u0302 t fa, h\u0302 t fa).\nEach bounding box of tuna nutriment detection occurs center of bounding box x\u0302tfa, center of bounding box y\u0302tfa, width w\u0302 t fa, and height h\u0302 t fa. To give robust solution, center of tuna nutriment x\u0302tfa and y\u0302 t fa is transformed into different domains to reduce rotation and unknown position of camera effect. Regression is used to predict tuna nutriment x\u0302tfa and y\u0302 t fa in next frame f + 1. The results of regression apply the inverse transformation method to move back into original center point of predicted nutriments x\u0302(f+1)a and y\u0302(f+1)a.\n2.2.1. Transformation. We are given a pair of ripple area detection \u03d5r(f) = {Rf1, Rf2} in frame f . Each ripple detection is represented by Rf1,2 = (x\u0302 r f , y\u0302 r f , w\u0302 r f , h\u0302 r f ). We compute each component of ripple detection to be top-left points tlf1,2 = (x\u0302 tl f , y\u0302 tl f ), top-right points trf1,2 = (x\u0302 br f , y\u0302 tl f ), bottom-left points blf1,2 = (x\u0302 tl f , y\u0302 br f ) and bottom-right points brf1,2 = (x\u0302brf , y\u0302 br f ) of ripple detection by following:\nx\u0302tlf = 2x\u0302rf \u2212 w\u0302rf\n2 ,\ny\u0302tlf = 2y\u0302rf \u2212 h\u0302rf\n2 ,\nx\u0302brf = 2x\u0302rf + w\u0302 r f\n2 ,\ny\u0302brf = 2y\u0302rf + h\u0302 r f\n2 .\n(2)\nAfter calculating the corner of ripple detection, normalizing coordinate of nutriment is important aspect to give robust solution. The idea behind the coordinate transformation of nutriments is that the video camera always moves around and is also that there is no distance information between camera and ripple location. The goal of normalizing coordinate is to transform the function \u03d5t(f) of nutriments into \u03d5\u03ba(f) = {\u03baf1, \u03baf2, \u00b7 \u00b7 \u00b7 , \u03bafa} as normalization of nutriments. We assume that the ripple location can be defined as the parameter to acknowledge moving of camera and changing the distance between camera and ripple area. Bottom-left point lbf1 of ripple Rf1 is the new center point of normalizing coordinate with moving the position of (0, 0) pixel in image shown in Figure 2. To compute angle \u03b8f of both ripples, we use center of Rf1 and Rf2 which is defined as follow:\n\u03b8f = arctan2((y\u0302 r f2 \u2212 y\u0302rf1), (x\u0302rf2 \u2212 x\u0302rf1)), (3)\nAngle \u03b8f is used to rotate center of tuna nutriment detection (x\u0302 t fa, y\u0302 t fa) by bottom-left points (x\u0302tlf1, y\u0302 br f1) of ripple Rf1 which is presented by:\nx\u0302\u03c8fa =(x\u0302 t fa \u2212 x\u0302tlf ) cos(\u03b8f )\u2212 (y\u0302tfa \u2212 y\u0302brf ) sin(\u03b8f ) + x\u0302tlf , y\u0302\u03c8fa =(x\u0302 t fa \u2212 x\u0302tlf ) sin(\u03b8f ) + (y\u0302tfa \u2212 y\u0302brf ) cos(\u03b8f ) + y\u0302brf ,\n(4)\nwhere \u03c8fa = (x\u0302 \u03c8 fa, y\u0302 \u03c8 fa) is the rotating result from a rotated function of nutriments\u03d5 \u03c8(f) = {\u03c8f1, \u03c8f2, \u00b7 \u00b7 \u00b7 , \u03c8fa}. After normalizing using rotation, translation is applied to change the center point from the position of (0, 0) pixel in image to bottom-left points blf1 of ripple Rf1 which is shown in Figure 2 defined as follow:\nx\u0302\u03befa =x\u0302 \u03c8 fa \u2212 x\u0302 tl f1, y\u0302\u03befa =y\u0302 br f1 \u2212 y\u0302 \u03c8 fa,\n(5)\nwhere \u03befa = (x\u0302 \u03be fa, y\u0302 \u03be fa) is geometry transformation result after rotation from a geometry transformation function of nutriments \u03d5\u03be(f) = {\u03bef1, \u03bef2, \u00b7 \u00b7 \u00b7 , \u03befa}. Distance z is computed by top-left point tlf1 of ripple Rf1 and bottom-right point brf2 of ripple Rf2 shown by:\nz = \u221a\n(x\u0302tlf1 \u2212 x\u0302brf2)2 + (y\u0302tlf1 \u2212 y\u0302brf2)2. (6)\nNormalized of nutriment \u03bafa = (x\u0302 \u03ba fa, y\u0302 \u03ba fa) is defined as fraction of geometry transforma-\ntion \u03befa with distance z which is explained by:\nx\u0302\u03bafa = x\u0302\u03befa z , y\u0302\u03bafa = y\u0302\u03befa z .\n(7)\n2.2.2. Inverse Transformation. Inverse transformation is used to move back into original center points of nutriments x\u0302fa and y\u0302fa. This process is to calculate reverse transformation process from a function of normalized nutriment \u03d5\u03ba(f) to \u03d5t(f). The inverse transformation is started from normalized of nutriments \u03d5\u03ba(f) to geometry transformation \u03d5\u03be(f) presented by:\nx\u0302\u03befa =x\u0302 \u03ba fa \u2217 z, y\u0302\u03befa =y\u0302 \u03ba fa \u2217 z,\n(8)\nAfter getting geometry transformation \u03d5\u03be(f), translation is applied to convert from a function of geometry transformation \u03d5\u03be(f) to rotated function \u03d5\u03c8(f) which is represented by:\nx\u0302\u03c8fa =x\u0302 \u03be fa + x\u0302 tl f1, y\u0302\u03c8fa =y\u0302 br f1 \u2212 y\u0302 \u03be fa,\n(9)\nOur goal is to transform a function of rotated nutriment \u03d5\u03c8(f) into original pixel position in real images \u03d5t(f) using (4) with negative of angle \u03b8f presented by:\n\u03b8f = \u2212 arctan2((y\u0302rf2 \u2212 y\u0302rf1) \u00b7 (x\u0302rf2 \u2212 x\u0302rf1)), (10)\n2.2.3. Regression. Small and dense nutriment can easily fail to be detected for several frames because current position and trajectory for each individual nutriment have different angle and speed. To propose robust solution for undetected nutriments in several frames, regression can be superior algorithm rather than tracking algorithm because performance of tracking depends on nutriment detection results in sequential frames. To improve regression result, the complexity of network by using multiple neuron and hidden layer is needed to precisely predict the moving nutriment with lower error direction. Model\nof regression \u00b5x and \u00b5y predicts the nutriments from current frame to next position of nutriment (x\u0302\u03ba(f+1)a, y\u0302 \u03ba (f+1)a) in the next frame which is defined as:\nx\u0302\u03ba(f+1)a =\u00b5 x(x\u0302\u03bafa), y\u0302\u03ba(f+1)a =\u00b5 y(y\u0302\u03bafa).\n(11)\nThe primary architecture of regression network is made in fully connected layers of 100, 80, 60, 40, 40, and 20 neurons with ReLU as activation function for individual x\u0302\u03bafa and y\u0302 \u03ba fa presented at Figure 3. MSE loss is used to predict position of nutriments in next frame. To predict y\u0302\u03ba(f+1)a, it requires two inputs x\u0302 \u03ba fa and y\u0302 \u03ba fa because in the object trajectory, y component has positive and negative value depending on turning point. To detect turning point, the position of component x is necessary to classify the movement of nutriments in correct direction.\n2.2.4. Counting Nutriments. Assuming that bottom-left points lbf1 as the center, it can be used to define passed line to count the nutriment. Given linear function yl(f) = \u03b1fx l f +\u03b2f is created from two points (x\u0302lf1, y\u0302 l f1) and (x\u0302 l f2, y\u0302 l f2). These points are produced from the corner and center of ripple which defined as:\nx\u0302lf1 =x\u0302 tl f1,\ny\u0302lf1 =y\u0302 br f1 \u2212\nz \u03c1 ,\nx\u0302lf2 =x\u0302 l f1 + (x\u0302 r f2 \u2212 x\u0302rf1), y\u0302lf2 =y\u0302 l f1 \u2212 (y\u0302rf1 \u2212 y\u0302rf2),\n(12)\nwhere \u03c1 = 3.6 is constant value. \u03c1 is used to define the distance between passed line and ripple area. The gradient \u03b1f and coefficient \u03b2f of passed line y l(f) = \u03b1fx l f + \u03b2f is calculated as:\n\u03b1f = y\u0302lf2 \u2212 y\u0302lf1 x\u0302lf2 \u2212 x\u0302lf1 , \u03b2f =y\u0302 l f1 \u2212 \u03b1f x\u0302lf1.\n(13)\n2.3. Textural Feature of Ripple Behavior. Ripple activity level is determined as human perception for the size and number of ripple appearing from the fish feeding process. Human perception cannot continuously be used to adjust fish feeding machine because its perception has no standard for validation or subjective methods. By following that, we propose robust method to estimate ripple behavior using VGG as base model for feature extraction which is presented in Figure 4. This idea is adopted from full reference image quality assessment (FR-IQA). FR-IQA is assessment the quality of test images comparing with reference image for standard quality. The difference of FR-IQA and our approach is that FR-IQA requires reference and test image while our approach only needs single images to define ripple activity level.\n2.3.1. VGG Feature. Given cropped image df from point tlf1 to brf2 in original image as the input image of VGG network, VGG feature of convolutional layers is calculated by:\nf(df ) = {d\u0302(i)f(j); i = 1, \u00b7 \u00b7 \u00b7 , \u03c9; j = 1, \u00b7 \u00b7 \u00b7 , \u03c6i}, (14)\nwhere \u03c9 = 5 represents the number of VGG convolution layers. \u03c6i is the number of feature maps in ith VGG convolution layers. Variance \u03c3 (i) f of feature maps in i\nth VGG convolution layers is presented as:\n\u03c3 (i) f =\n\u221a\u221a\u221a\u221a\u221a\u221a\u2211(\u03c3(i)f(j) \u2212 \u2211 \u03c3 (i) f(j) \u03c6i )2\n\u03c6i ,\n(15)\nwhere \u03c3 (i) f(j) is variance of each feature map d\u0302 (i) f(j) in VGG convolution layers. Global variance \u03c3f is computed by:\n\u03c3f = \u221a\u221a\u221a\u221a\u221a\u2211(\u03c3(i)f \u2212 \u2211\u03c3(i)f\u03c9 )2 \u03c9 . (16)\n3. Experiment. We firstly explain the details of dataset used to this experiment. We then describe the evaluation approach for precisely presenting quantitative evaluation with various regression models.\n3.1. Datasets. Our dataset contains a video of fish feeding process which has small and dense nutriments with 418 frames. Each dimension of video frame is 1920 \u00d7 1080 pixels. This sequential video is saved in MOV format with frame rate 30 frames/second. Range size of nutriment is starting from 9 \u00d7 6 to 13 \u00d7 36 pixels.\n3.2. Evaluation Approach. Evaluation approach is computed by measuring minimum euclidean distance based on result for each regression model with ground truth \u00b5gt. Best regression model \u00b5\u2217 with minimum error distance to the ground truth \u00b5gt is defined as:\n\u00b5\u2217 = arg min \u03b9\n((\u00b5gt \u2212 \u221a (\u00b5x\u03b9 (x\u0302 \u03ba fa)\u2212 \u00b5 y \u03b9 (y\u0302\u03bafa)) 2)T (\u00b5gt \u2212 \u221a (\u00b5x\u03b9 (x\u0302 \u03ba fa)\u2212 \u00b5 y \u03b9 (y\u0302\u03bafa)) 2)), (17)\nwhere \u03b9 \u2208 [1, 6].\n3.3. Quantitative Evaluation. In this section, we aim to answer two main questions towards understanding our model. Firstly, we compare the performance of different regression models and observe performance of proposed method. In Table 1, it shows the description of various regression models namely R1, R2, R3, R4, R5, and R6. We train each regression model with same parameter with 106 iterations and 10\u22127 as learning rate. For conclusion, we present the result for each regression model in Table 2 and the visualization for each regression model is shown by Figure 5. We manually marking the correct position of nutriments in the next frame with N = 50. By following quantitative\nevaluation, regression model R1 has lowest minimum error distance and we apply this model to be a base of regression model.\nSecondly, we observe estimation of ripple behavior and use different images to represent performance of global variance of VGG network which is shown in Figure 6. For fair comparison, we use the image with same dimension in which the size is 843\u00d7 162 pixels. In conclusion, image without ripple in Figure 6a archives 0.029 while image with ripple in Figure 6b reaches 0.3037. The value can represent the activity level for ripple area which means that ripple image in Figure 6a has lower activity than ripple image in Figure 6b.\n4. Result. Before we start to explain result of proposed method, we firstly show evaluation result for better understanding proposed method. We then compare proposed method and state-of-the-art benchmark methods of tracking algorithm on our datasets and show the computational time to figure out the performance and advantage proposed method comparing state-of-the-art benchmark methods.\n4.1. Evaluation Result. To acknowledge the performance YOLOv4 [44], mAP and loss are used as the training parameter. To train object detection using our dataset, we present the detail of parameter using 10k iterations with 512\u00d7 288 pixels for image resizing from 1920\u00d7 1080 pixels. Figure 7 displays training result and reaches 75% of maximum mAP and 0.9 as loss value after two days training using GeForce RTX 2080 Ti. Figure 8 shows the result of proposed method in sequential frames of our video. By following these graphs, number of nutriments and estimation of ripple behaviors can clearly use to identify the behavior of fish feeding activity. The number of nutriment is calculated by cumulative summation of nutriments in 20 frames while estimation of ripple behavior is computed by cumulative average of global variance in 20 frames. We also tested performance regression model of proposed method with state-of-the-art tracking methods. Many of state-ofthe-art tracking methods use multiple object tracking (MOT) [32\u201338]. These methods perform well in six publicly databases on pedestrian detection [46\u201348]. For evaluations, JDE method [32] has been chosen to represent MOT as benchmark method because its method has an accurate prediction based on re-implementation of faster object detection compared with [33\u201338]. We also use trajectory mapping [39] as a benchmark method\nbecause its method has a good performance in application to aquaculture fish tank. The last method to be a benchmark method is SORT [49]. We add the detection model of trajectory mapping to completely understand performance of tracking method.\nIn Figure 9a, the proposed method is demonstrated to be able to track small nutriment well while trajectory mapping(TM), JDE and SORT with original YOLOv3 and trajectory mapping detection model perform poor (Figure 9b, 9c, 9d, 9e) without tracking results of nutriments even trajectory mapping and SORT are able to detect some nutriments and ripple area. In this cases, we assume that the benchmark methods fail to run our datasets because the size of nutriment is too small (maximum size is 13\u00d7 36 pixels) and the speed of nutriment is fast (average nutriment movement from fish feeding machine to ripple area is 23.8 frames).\n4.2. Computational Time. To describe the computational complexity and execution time of the proposed methodology, a computational time analysis is conducted using a\nvideo with 418 frames. The specification of hardware and software used to analyze proposed method and state-of-the-art benchmark methods is shown in Table 3. Table 4 presents comparison of computation time (in fps) between proposed method and benchmark methods (trajectory mapping, JDE and SORT with original YOLOv3 and trajectory mapping detection model). We reach 3.86 and 0.11 fps for average and standard deviation of computational time, respectively, while trajectory mapping needs 1.93 and 0.61 fps and\nJDE spends 1.87 and 0.07 fps in which these benchmark methods runs twice slower than proposed method. SORT provides average computational time without information of computational time for individual frame. Computational time for both detection model of YOLOv3 and trajectory mapping model with SORT performs worst comparing with proposed method and other benchmark methods in which it runs 0.45 and 0.47 fps, respectively. By analyzing computational complexity, proposed method is the fastest model with twice for the different speed comparing benchmark methods.\n5. Conclusion and Discussion. Automatic controlling fish feeding machine using combination of two feature extractions is important to optimize the amount of nutriment giving to the fish. Feature extraction of nutriment and ripple behavior has presented an agreement to complement each others. Recent studies have shown that it is possible to track all detected objects in sequence frames on video. However, there is no agreement to track multiple small and dense nutriments and also to detect ripple activity area in the video. In this paper, automatic controlling fish feeding machine using feature extraction of nutriment and ripple behavior has been presented and demonstrated to be promising for optimizing fish feeding process to give an optimal rate of both costs and profits. We have demonstrated an area with and without ripple activity which has a big difference gap and the proposed method consistently performs well on the video contains small and dense nutriments. We expect proposed method to open the door for future work and to go beyond for developing a large community-generated database and focus on integrating with the sensors to give more accurate and robust results."
        }
    ],
    "title": "AUTOMATIC CONTROLLING FISH FEEDING MACHINE USING FEATURE EXTRACTION OF NUTRIMENT AND RIPPLE BEHAVIOR",
    "year": 2022
}