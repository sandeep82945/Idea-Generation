{
    "abstractText": "12 Despite excellent performance in quiet, cochlear implants (CIs) only partially restore normal levels 13 of intelligibility in noisy settings. Recent developments in machine learning have resulted in deep neural 14 network (DNN) models that achieve noteworthy performance in speech enhancement and separation 15 tasks. However, there are no commercially available CI audio processors that utilize DNN models for 16 noise reduction. We implemented two DNN models intended for applications in CIs: (1) a recurrent 17 neural network (RNN), which is a lightweight template model, and (2) SepFormer, which is the current 18 top-performing speech separation model in the literature. The models were trained with a custom 19 training dataset (30 hours) that included four configurations: speech in non-speech noise and speech 20 in 1-talker, 2-talker, and 4-talker speech babble backgrounds. The enhancement of the target speech 21 (or the suppression of the noise) by the models was evaluated by commonly used acoustic evaluation 22 metrics of quality and intelligibility, including (1) signal-to-distortion ratio, (2) \u201cperceptual\u201d evaluation 23 of speech quality, and (3) short-time objective intelligibility. Both DNN models yielded significant 24 improvements in all acoustic metrics tested. The two DNN models were also evaluated with thirteen 25 CI users using two types of background noise: (1) CCITT noise (speech-shaped stationary noise) and 26 (2) 2-talker babble. Significant improvements in speech intelligibility were observed when the noisy 27 speech was processed by the models, compared to the unprocessed conditions. This work serves as a 28 proof of concept for the application of DNN technology in CIs for improved listening experience and 29 speech comprehension in noisy environments. 30",
    "authors": [
        {
            "affiliations": [],
            "name": "Kostas Kokkinakis"
        },
        {
            "affiliations": [],
            "name": "Hari M. Bharadwaj"
        },
        {
            "affiliations": [],
            "name": "Joshua S. Stohl"
        }
    ],
    "id": "SP:7ab578f8700ece054fa168b9ee4d3fb4637b7305",
    "references": [
        {
            "authors": [
                "AdvancedBionics."
            ],
            "title": "CLEARVOICE, clinical results",
            "venue": "Retrieved from https://",
            "year": 2012
        },
        {
            "authors": [
                "T. Bentsen",
                "T. May",
                "A.A. Kressner",
                "Dau",
                "May"
            ],
            "title": "The benefit of combining a deep neural",
            "year": 2018
        },
        {
            "authors": [
                "Boll",
                "April"
            ],
            "title": "Suppression of acoustic noise in speech using spectral subtraction",
            "year": 1979
        },
        {
            "authors": [
                "J. Chen",
                "Wang",
                "June"
            ],
            "title": "Long short-term memory for speaker generalization in supervised",
            "year": 2017
        },
        {
            "authors": [
                "J. Chen",
                "Y. Wang",
                "S.E. Yoho",
                "D. Wang",
                "Healy",
                "E. W",
                "May"
            ],
            "title": "Large-scale training to increase",
            "year": 2016
        },
        {
            "authors": [
                "P.W. Dawson",
                "S.J. Mauger",
                "Hersbach",
                "A. A",
                "June"
            ],
            "title": "Clinical Evaluation of Signal-to-Noise",
            "year": 2011
        },
        {
            "authors": [
                "428 Fu",
                "Q.-J",
                "R.V. Shannon",
                "X. Wang"
            ],
            "title": "December). Effects of noise and spectral resolution on",
            "year": 1998
        },
        {
            "authors": [
                "E.W. Healy",
                "M. Delfarah",
                "E.M. Johnson",
                "Wang",
                "March"
            ],
            "title": "A deep learning algorithm",
            "year": 2019
        },
        {
            "authors": [
                "E.W. Healy",
                "S.E. Yoho",
                "Y. Wang",
                "Wang",
                "October"
            ],
            "title": "An algorithm to improve speech",
            "year": 2013
        },
        {
            "authors": [
                "I. Hochmair",
                "E. Hochmair",
                "P. Nopp",
                "M. Waller",
                "Jolly",
                "April"
            ],
            "title": "Deep electrode insertion",
            "year": 2015
        },
        {
            "authors": [
                "Y. 478 Hu",
                "Loizou",
                "P. C",
                "June"
            ],
            "title": "Environment-specific noise suppression for improved speech",
            "year": 2010
        },
        {
            "authors": [
                "M. Keshavarzi",
                "T. Goehring",
                "R.E. Turner",
                "Moore",
                "B.C. J",
                "March"
            ],
            "title": "Comparison of effects",
            "year": 2019
        },
        {
            "authors": [
                "M. Keshavarzi",
                "T. Goehring",
                "J. Zakis",
                "R.E. Turner",
                "Moore",
                "B.C. J",
                "January"
            ],
            "title": "Use of a Deep",
            "year": 2018
        },
        {
            "authors": [
                "G. Kim",
                "Y. Lu",
                "Y. Hu",
                "P.C. Loizou"
            ],
            "title": "September). An algorithm that improves speech",
            "year": 2009
        },
        {
            "authors": [
                "J. Kim",
                "M. El-Khamy",
                "Lee",
                "May"
            ],
            "title": "T-GSA: Transformer with Gaussian-Weighted",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "Ba",
                "January"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "year": 2017
        },
        {
            "authors": [
                "K. Kokkinakis",
                "Stohl",
                "J. S",
                "August"
            ],
            "title": "Optimized gain functions in ideal time-frequency masks",
            "year": 2021
        },
        {
            "authors": [
                "N. Li",
                "S. Liu",
                "Y. Liu",
                "S. Zhao",
                "Liu",
                "July"
            ],
            "title": "Neural Speech Synthesis with Transformer",
            "year": 2019
        },
        {
            "authors": [
                "P.C. 529 Loizou",
                "A. Lobo",
                "Y. Hu"
            ],
            "title": "November). Subspace algorithms for noise reduction in cochlear",
            "year": 2005
        },
        {
            "authors": [
                "Y. Luo",
                "Mesgarani",
                "August"
            ],
            "title": "Conv-TasNet: Surpassing Ideal Time\u2013Frequency Magnitude",
            "year": 2019
        },
        {
            "authors": [
                "T. May",
                "T. Dau"
            ],
            "title": "December). Requirements for the evaluation of computational speech segre",
            "year": 2014
        },
        {
            "authors": [
                "M.A. Stone",
                "Moore",
                "B.C. J",
                "June"
            ],
            "title": "Tolerable Hearing Aid Delays",
            "year": 1999
        },
        {
            "authors": [
                "C. Subakan",
                "M. Ravanelli",
                "S. Cornell",
                "M. Bronzi",
                "Zhong",
                "March"
            ],
            "title": "Attention is All You Need",
            "year": 2021
        },
        {
            "authors": [
                "Y. Tang",
                "C. Arnold",
                "T. Cox"
            ],
            "title": "December). A Study on the Relationship between the Intel",
            "year": 2017
        },
        {
            "authors": [
                "E. Vincent",
                "R. Gribonval",
                "C. Fevotte"
            ],
            "title": "July). Performance measurement in blind audio",
            "year": 2006
        },
        {
            "authors": [
                "J. 596 Wouters",
                "J. Vanden Berghe"
            ],
            "title": "October). Speech Recognition in Noise for Cochlear Implantees",
            "year": 2001
        },
        {
            "authors": [
                "S. Zirn",
                "S. Arndt",
                "A. Aschendorff",
                "Wesarg",
                "October"
            ],
            "title": "Interaural stimulation timing",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Deep neural network algorithms for noise reduction and their1\napplication to cochlear implants2\nShort Title: Deep Neural Network Models for Cochlear Implants3\n\u2217Agudemu Borjigina,b, Kostas Kokkinakisc, Hari M. Bharadwajd, and Joshua S. Stohle4\naWeldon School of Biomedical Engineering, Purdue University, West Lafayette 47907, IN, USA5 bWaisman Center, University of Wisconsin Madison, Madison, WI 53705, USA6 cConcha Labs, San Francisco, CA 94114, USA7 dDepartment of Communication Science and Disorders, University of Pittsburgh, Pittsburgh, PA8 15213, USA9\neNorth American Research Laboratory, MED-EL Corporation, Durham, NC 27713, USA10\nAugust 25, 202211\n\u2217Correspondence: dagu@wisc.edu\nAbstract12\nDespite excellent performance in quiet, cochlear implants (CIs) only partially restore normal levels13 of intelligibility in noisy settings. Recent developments in machine learning have resulted in deep neural14 network (DNN) models that achieve noteworthy performance in speech enhancement and separation15 tasks. However, there are no commercially available CI audio processors that utilize DNN models for16 noise reduction. We implemented two DNN models intended for applications in CIs: (1) a recurrent17 neural network (RNN), which is a lightweight template model, and (2) SepFormer, which is the current18 top-performing speech separation model in the literature. The models were trained with a custom19 training dataset (30 hours) that included four configurations: speech in non-speech noise and speech20 in 1-talker, 2-talker, and 4-talker speech babble backgrounds. The enhancement of the target speech21 (or the suppression of the noise) by the models was evaluated by commonly used acoustic evaluation22 metrics of quality and intelligibility, including (1) signal-to-distortion ratio, (2) \u201cperceptual\u201d evaluation23 of speech quality, and (3) short-time objective intelligibility. Both DNN models yielded significant24 improvements in all acoustic metrics tested. The two DNN models were also evaluated with thirteen25 CI users using two types of background noise: (1) CCITT noise (speech-shaped stationary noise) and26 (2) 2-talker babble. Significant improvements in speech intelligibility were observed when the noisy27 speech was processed by the models, compared to the unprocessed conditions. This work serves as a28 proof of concept for the application of DNN technology in CIs for improved listening experience and29 speech comprehension in noisy environments.30 Key words: deep learning models, recurrent neural network, transformer, speech enhancement,31 speech intelligibility in noise32\nIntroduction33\nCochlear implant (CI) listeners struggle to understand speech in noisy environments, despite the fact34 that most listeners can achieve satisfactory speech intelligibility in quiet settings. This is especially true35 when the background noise is non-stationary and contains modulations (Cullington & Zeng, 2008; Fu et36 al., 1998). Some acoustic front-end processing algorithms have been shown to be capable of improving37 speech intelligibility in fluctuating noise. For example, some studies (Wouters & Vanden Berghe, 2001;38 Hersbach et al., 2012) as well as existing commercial solutions for CI (MED-EL, 2021; AdvancedBionics,39 2012) have demonstrated that two-microphone directionality can result in a masking release benefit of40 up to 10 dB. However, these approaches require a listener to face the target, to help facilitate spatial41 separation between the target and background. In this work, we implemented and evaluated two single-42 microphone noise reduction algorithms that do not rely on spatial separation. Traditionally, most single-43 channel noise reduction algorithms designed for hearing-assistive devices are driven by signal-processing44 strategies based on time-averaged signal statistics. Classic examples include spectral subtraction (Boll,45 1979), wiener filtering (Scalart & Filho, 1996), as well as commercial solutions such as ambient and46 transient noise reduction from MED-EL (2021), and ClearVoice from AdvancedBionics (2012). These47 models can improve speech intelligibility to a certain extent in statistically consistent backgrounds, such48 as a stationary noise (Loizou et al., 2005; Mauger et al., 2012; Dawson et al., 2011). For speech in49 a more complex, non-stationary background (e.g., a multi-talker babble), machine-learning techniques,50 such as deep neural networks (DNNs) and Gaussian mixture models (GMMs), have been shown to be51 successful in improving speech intelligibility for listeners with typical hearing (G. Kim et al., 2009), for52 listeners with hearing loss (Healy et al., 2013, 2015, 2019; Chen et al., 2016; Monaghan et al., 2017;53 Bramsl\u00f8w et al., 2018), and for CI listeners (Hu & Loizou, 2010; Goehring et al., 2017; Lai et al., 2018).54 Recent improvements over these models were introduced by DNN-based regression models, where the55 training target was a continuous (i.e., soft) mask, instead of a binary mask (Madhu et al., 2013; Bentsen56 et al., 2018). However, all models mentioned above operate based on a priori knowledge of the target57 and/or background by using the same target speaker (Lai et al., 2018; Chen et al., 2016), background58 interference (Goehring et al., 2017), or both (G. Kim et al., 2009; Hu & Loizou, 2010; Healy et al., 2013,59 2015, 2019; Goehring et al., 2017; Lai et al., 2018; Bramsl\u00f8w et al., 2018; Bentsen et al., 2018) for the60 training and testing process.61\nThe generalization of such models to unseen instances of noise beyond the training materials, is critical62 for ensuring the efficacy of the models in real-time and real-world applications that can be put to use in63 audio processors. It is, however, impractical to train the models with all speech-in-noise mixtures that64 a user would normally encounter in real-world listening situations. Indeed, in most studies, the model65 performance according to acoustic intelligibility metrics decreased significantly when unseen testing data66 were presented to the models (May & Dau, 2014; Chen & Wang, 2017; Goehring et al., 2017). Recent67 studies have demonstrated the potential of using recurrent neural network (RNN) models for better68 generalization by including recurrent connections, feedback, and gate elements (Weninger et al., 2015;69 Chen & Wang, 2017; Kolb\u00e6k et al., 2017; Graves et al., 2013). A classic architecture with these elements70 is the so-called long short-term memory (LSTM) structure that accumulates information from the past71 and hence enables the network to form a temporary memory (Hochmair et al., 2015; LeCun et al., 2015),72 which is essential for properly managing and learning speech context. RNN-LSTM based models have73 been shown to substantially improve speech-in-noise perception for listeners with hearing loss (Bramsl\u00f8w74 et al., 2018; Healy et al., 2019; Keshavarzi et al., 2019, 2018) and CI users (Goehring et al., 2019).75 Despite the wide adoption of RNN models in modern audio processing systems in many domains,76 the sequential nature of the RNN architecture often renders them difficult to train. This computational77 inefficiency is inherent in speech recognition and synthesis, as well as speech enhancement and speech78 segregation problems (Subakan et al., 2021). The impact from the limitation of RNN sequential process-79 ing becomes especially prominent with long speech sentences. Practically, this shortcoming of RNN can80 be avoided by using a mechanism known in the literature as a \u201ctransformer\u201d. The transformer is a fully81 attention-based mechanism that can effectively replace the recurrence structure present in common RNN82 models (Vaswani et al., 2017). This architecture allows the model to attend to the entire sequence all83 at once and establish connections between distinct elements, which ultimately leads to a more efficient84 learning of long-term dependencies. The transformer has gained competitive performance and consid-85 erable popularity in speech recognition (Karita et al., 2019), speech synthesis (Li et al., 2019), speech86 enhancement (J. Kim et al., 2020), and audio source separation (Subakan et al., 2021). The SepFormer87 model developed by Subakan et al. (2021) is currently the top-performing model in speech separation88 applications, according to Papers with Code website.89 In this work, we applied the state-of-the-art SepFormer model towards speech enhancement. Although90 being state-of-the-art in terms of performance, this model is not amenable to real-time operation due to91\nits complex architecture. Therefore, while using SepFormer as a reference for the flagship benchmark92 model, we also applied a low-complexity RNN model to account for the constraints on processing time93 and computational power in audio processors. The effectiveness of these two proposed DNN models94 was verified through commonly used acoustic intelligibility metrics. We also investigated the clinical95 effectiveness of these two models by behaviorally testing CI listeners with challenging noise types and96 signal-to-noise ratio (SNR) levels. The goal of this work is to serve as a proof-of-concept towards the97 adoption of DNN technology in CI devices for better speech perception in complex everyday listening98 environments.99\nNetwork Architectures100\nRNN101\nThe schematic of the single-channel, RNN-based speech enhancement algorithm is illustrated in Figure 1a.102 A clean target speech and either babble-speech or non-speech noise were mixed to create unprocessed103 noisy speech. The features used as input to the RNN model were the spectral magnitudes of the short-104 time Fourier transformation (STFT) of the mixtures. The spectral magnitudes were extracted using105 Hamming-windowed frames with a window size of 32 samples and a hop size of 16 samples applied to106 signals sampled at 16 kHz. \u201cAdd-one\u201d log (i.e., adding 1 to the value before taking log) was applied to107 the spectral magnitude to reduce the influence of very small values (values that are smaller than 1). The108 predicted mask (i.e., the model outcome) was a continuous \u201csoft\u201d mask instead of a binary mask (where109 the values of the mask are either one or zero). Previous work suggests that \u201csoft\u201d continuous masks result110 in better speech quality and intelligibility than binary masks under various noise conditions (Madhu et111 al., 2013). The original unprocessed noisy mixtures were multiplied with the predicted mask derived from112 the RNN model in order to generate the \u201cde-noised\u201d spectrum of the mixture. The \u201cde-noised\u201d estimate113 would ideally consist of the enhanced target speech only. This \u201cde-noised\u201d spectrum was compared114 with the spectrum of the clean target speech to compute the mean square error (MSE) loss for training115 optimization. The processed speech estimate was then recovered by resynthesizing (i.e., taking the inverse116 STFT of) the \u201cde-noised\u201d spectrum. The RNN network consists of two LSTM layers, with each followed117 by a projection layer. A PyTorch-powered speech toolkit\u2014SpeechBrain\u2014was used to implement, train,118\nand test the RNN model. The Adam optimizer was used for minimizing the MSE loss during the training119 process (Kingma & Ba, 2017), with learning rate set to 0.0001. The model performance was evaluated120 and monitored with a validation dataset at the end of each learning cycle with all training samples (i.e.,121 epoch). The training was terminated after 100 epochs to avoid overfitting, where the model performance122 with validation dataset stabilized with no further significant improvements.123\nSepFormer124\nWhile the simple, light-weight RNN model serves as a proof of concept for DNN algorithms that are125 suitable for small devices such as CIs, we also implemented the current state-of-the-art model for speech126 separation applications\u2014SepFormer, to explore the limits of current DNN technology in speech enhance-127 ment and source separation domains. The model architecture is shown in Figure 1b. A single-layer128 convolutional network was used as an encoder to learn the STFT-like representation of the input noisy129 signal. Similarly, at the end of the process, a transposed convolution layer with the same stride and130 kernel size as in the encoder was used to turn the STFT-like representations back into separate sources131 within the mixture. The extracted STFT-like features of the noisy mixture go into the masking network,132 which estimates the masks for the foreground (i.e., target speech) and background. These masks were133 also continuous or soft-decision masks that provided continuous gains from 0 to 1 as in the RNN model.134 In the masking network, the features were first normalized and processed by a linear layer. They were135 then buffered into chunks along the time axis with an overlap factor of 50%. Next, they were fed into136 the core of the masking net\u2014SepFormer block. This block consists of two transformer structures that137 learned both short and long-term dependencies. More details about this model can be found in Subakan138 et al. (2021). The output of the SepFormer block was then processed by a parametric rectified linear unit139 (PReLU) and linear layer. The overlap-add scheme, described in Luo & Mesgarani (2019), was used to140 sum up the chunks. This summed representation was passed through two feed-forward layers and a ReLU141 activation function to finally generate the masks for both the foreground and background sources. The142 training procedure and infrastructure were the same as for the RNN model.143\nAcoustic intelligibility models144\nThe DNN models were first evaluated quantitatively using three commonly used acoustic evaluation met-145 rics: source-to-distortion ratio (SDR) (Vincent et al., 2006), short-time objective intelligibility (STOI) (Taal146 et al., 2011, 2010), and \u201cperceptual\u201d evaluation of speech quality (PESQ) (Rix et al., 2001; Hu & Loizou,147 2008). These accurate and reliable acoustic evaluation methods provided helpful information regarding148 the overall expected benefit before conducting behavioral listening test with CI users. All three evaluation149 metrics compare the clean reference speech and the same speech recovered from the noisy mixture and150 quantify the agreement between the two, which allows for an estimate of the improvement in quality151 and intelligibility due to model processing according to each metric. The SDR metric decomposes the152 estimated source into four components representing respectively the true source, spatial distortions, in-153 terference, and artifacts. The final SDR score is computed by calculating the ratio of the source energy to154\nthe sum of all other projection energies (i.e., spatial distortions, interference, and artifacts) as described155 in Vincent et al. (2006). The STOI metric was initially designed to predict the intelligibility of speech156 processed by enhancement algorithms. Recently, Falk et al. (2015) demonstrated that STOI outperformed157 all other measures for predicting intelligibility demonstrated by CI listeners. The STOI first applies time-158 frequency analysis to both clean reference and processed speech. An intermediate intelligibility measure is159 obtained by estimating the linear correlation coefficient between clean and processed time-frequency units.160 The final STOI score is the average of all intermediate intelligibility estimates from all time-frequency161 units. The PESQ score ranges between \u20130.5 and 4.5. It was calculated by comparing the reference signal162 with the processed signal by deploying a perceptual model of the human auditory system. The PESQ is163 computed as a linear combination of average disturbance value and average asymmetric disturbance value.164 The parameters for the linear combination can be further modified towards predicting different aspects165 of speech quality. More details can be found in Rix et al. (2001), Hu & Loizou (2008) and Kokkinakis166 & Loizou (2011). In general, the PESQ has been shown to be capable of reliably predicting the quality167 of processed speech. Kokkinakis & Stohl (2021) showed that parameter optimization for reverberation168 suppression algorithms based on the PESQ metric resulted in better performance than the STOI metric.169 Therefore, in the present context, the PESQ was chosen to detect and quantify the overall effects of DNN170 processing on the signal quality.171\nBehavioral Listening Experiments172\nSubjects173\nA total of thirteen adults fitted with MED-EL CIs (MED-EL GmbH, Innsbruck, Austria) participated in174 the study. They were between the ages of 20 and 72 (7 male and 6 female). Their mean age at testing was175 58.6 years (SD = 14.7 years). The average duration of CI use was 6.5 years (SD = 5 years). Demographic176 information is provided in Table 1, including each subject\u2019s default clinical sound coding strategy. This177 study was approved by the Western Institutional Review Board (Protocol 20100066). All subjects gave178 informed written consent prior to testing. Only research subjects whose participation in the study would179 cause financial hardship received financial compensation for their participation.180\nTest setup181\nAll participants were tested using their everyday program. Before the testing session, their audio processor182 was programmed with the recipient\u2019s daily program. The stimulus was delivered to the audio processor183 through the direct audio input (DAI) cable, which attenuates the microphone inputs by approximately 30184 dB while passing the direct input signal without attenuation and also bypasses the front-end directionality185 and wind-noise reduction features. The test stimuli were presented at an input level corresponding to 65186 dB SPL (root mean square (RMS) level). None of the participating subjects used MED-EL\u2019s channel-187 specific, ambient and transient noise reduction algorithms in their daily maps at the time of testing. At188 the beginning of the testing session, the audiologist provided instructions regarding the study procedures,189 and then connected the recipient\u2019s processor to the audio port of a Windows-based touchscreen tablet190 (Microsoft Surface Pro) through the DAI cable. The proprietary psychophysical software suite, PsyWorks191 v.6.1 (MED-EL GmbH, Innsbruck, Austria), was used to present the speech stimuli from the tablet to the192 audio processor. The calibration was performed using a built-in feature within the PsyWorks software193 and was adjusted according to each recipient\u2019s audio processor.194\nProcedure195\nSentences from the IEEE corpus were mixed with CCITT noise (speech shaped stationary noise according196 to ITU-T Rec. G.227) or 2-talker babble (TTB) at SNRs of +5 and +10 dB. Each processing condition197 (unprocessed, RNN, SepFormer) was evaluated with a list of twenty sentences for each combination of198 masker type and SNR. Each subject performed a total of 13 tests (2 masker types x 3 conditions (2 models199 and 1 unprocessed) x 2 SNRs + 1 quiet). The testing was carried out in a self-administered manner. The200 subjects used the tablet and PsyWorks to present the speech materials to their own audio processors.201 Subjects were assigned a unique presentation order using a \u201cLatin square\u201d design and were blinded to202 the processing condition. The subjects either vocalized their responses through a microphone located in203 front of them or typed them, according to their preference. The responses were captured in real-time204 by an automatic speech-to-text module (Google API) in the case of spoken responses. Spoken responses205 could be edited by typing before submission, and the PsyWorks software automatically scored words as206 correctly or incorrectly identified. Words containing additions, substitutions, or omissions were scored207 as incorrect. The percent correct scores for each condition were calculated by dividing the number of208\ncorrect words by the total number of words. After each list, the percent correct was displayed and stored209 electronically. All participants were native English speakers, and none of the participants that elected to210 speak their responses had speech difficulties that prevented the automatic scoring of their responses. The211 total testing time for all experimental conditions tested, was approximately 2.5 hours including multiple212 breaks.213\nResults214\nAcoustic evaluations215\nThe models produced significant improvements across all acoustic evaluation metrics. The acoustic eval-216 uation scores for the RNN model obtained from the 340 test samples are shown in Figures 2a-2c. As217 shown in Figure (2a), processing with the RNN model improved SDR scores over the unprocessed con-218 dition (dashed lines) across all SNRs, as well as across both masker types (green: 2-talker babble, blue:219 CCITT, non-speech noise). These improvements in SDR introduced by the RNN model (i.e., the eleva-220 tion from the dashed lines to solid lines) were all statistically significant (p-values less than 0.0001). The221 improvements in speech quality and intelligibility introduced by the RNN model can also be evidenced222 by the other two metrics: PESQ (see Figure 2b) and STOI (see Figure 2c). These improvements were223 also statistically significant, with a p-value that is less than 0.0001. Although statistically significant, the224 improvement in the speech intelligibility metric (i.e., STOI) was not as prominent as in the two speech225 quality metrics (i.e., SDR and PESQ). This is probably because of ceiling effects; the SNR tested was226 high overall (starting from 1 dB SNR) and speech intelligibility was not a significant issue in these models227 of normal hearing (Tang et al., 2017). For the SepFormer model, the acoustic evaluation scores for the228 unprocessed noisy mixtures (dashed lines) remained the same since the test materials did not change, as229 shown in Figure 2d-2f. However, the scores for the processed audio signals by the SepFormer model (solid230 lines) had even more separation from the dashed lines, indicating better performance by SepFormer than231 RNN. The superior performance of SepFormer over RNN is especially evident in the 1 dB SNR condition232 across all three evaluation metrics.233\nBehavioral testing with CI listeners234\nThe speech intelligibility scores were measured behaviorally as percent correct for the three testing condi-235 tions, including \u201cunprocessed\u201d, \u201cprocessed by RNN\u201d, and \u201cprocessed by SepFormer\u201d. The DNN models236 introduced significant improvements in speech intelligibility scores for all CI listeners tested. As with the237 acoustic evaluation metrics, CI listeners were tested with both 2-talker babble and CCITT masker, as238 well as both 5 and 10-dB SNR conditions. The percent correct scores are normally distributed according239 to the Shapiro-Wilk test for normality. Therefore, we conducted the following statistical analyses. A240 three-way analysis of variance (ANOVA, with repeated measures), involving the processing condition,241 SNR, and the type of masker as within-subject factors, was carried out. The test results indicated that242 there were no statistically significant three-way, but significant two-way interactions (masker type and243 conditions: p = 0.03). Since there was a significant main effect of SNR (p < .0001), we evaluated the data244 under each SNR separately. Masker type and condition were preserved due to the interaction observed245 between them.246\nFor the 5 dB SNR conditions, a repeated measures two-way ANOVA indicated a statistically significant247 effect of the processing conditions (F [2, 24] = 38.0, p = 3.4e \u2212 08). There was also a main effect of248 masker type (F [1, 12] = 15.0, p = 2.0e \u2212 03), and a significant interaction between processing condition249 and masker type (F [2, 24] = 4.0, p = 3.2e \u2212 02). Given the significant interaction between processing250 condition and masker type, post-hoc analyses were performed separately for CCITT noise and two-talker251 babble. Post-hoc comparisons (with Bonferroni corrections) between scores in CCITT noise revealed that252 the SepFormer scores were significantly greater than the scores obtained in the unprocessed condition253 (p = 1.8e \u2212 2, 32.1% increase). For scores obtained with the two-talker babble masker, both the RNN254 (p = 1.3e\u2212 2, 37.1% increase) and SepFormer (p = 1.5e\u2212 3, 41% increase) models produced significantly255 better scores than the unprocessed condition. Scores obtained in the RNN and SepFormer conditions were256 not significantly different from one another for the CCITT noise (p = 4.9e\u2212 1) or the two-talker babble257 (p = 1.0). Very interestingly, both models introduced remarkably more performance increase in speech258 noise than non-speech noise (an increase of 37.1% vs. 14.5% by RNN; 41% vs. 32.1% by SepFormer).259 As expected, when the background noise level was reduced such that the SNR increased from 5 to 10260 dB, performance increased overall. Without any model processing, the median scores in the unprocessed261 conditions increased by 28.3% and 31.1% in non-speech and speech noise, respectively. Both models262 further improved the scores for both masker types: RNN introduced an increase of 3.4% and 18% in263 non-speech and speech noise, respectively. As shown previously, SepFormer still outperformed RNN and264 further increased the scores by 13.4% and 22.9% non-speech and speech noise, respectively. Even though265 the room for improvements from model processing were limited in these relatively easier conditions, the266 aforementioned trend of greater speech enhancement in speech noise over non-speech stationary noise267 persists in both models. In the 10 dB SNR conditions, a repeated measures two-way ANOVA indicated a268 statistically significant effect of the processing condition (F [2, 24] = 31.6, p = 1.9e\u221207). Based on post-hoc269 comparisons (with Bonferroni corrections),the scores from unprocessed conditions were significantly lower270 than those obtained in the RNN condition (p = 2.9e\u221202) and in the SepFormer condition (p = 9.5e\u221204).271 Neither the effect of masker type (F [1, 12] = 1.9, p = 1.9e\u2212 1), nor the interaction between masker type272 and condition were significant (F [2, 24] = 2.1, p = 1.4e\u2212 1).273 Instead of showing absolute scores, Figure 3 depicts scores for each masker type separately, with274 reference to listening scores obtained in quiet without interference (therefore mostly negative values below275 the zero line). Figure 3a shows the data collected in the 5 dB SNR condition, while Figure 3b shows276\nthe data obtained in the 10 dB SNR condition. At first glance, in both 5 and 10 dB SNR conditions,277 across both speech and non-speech masker types, both the RNN and SepFormer models restored speech278 intelligibility to performance levels approaching those of quiet settings. The aforementioned statistical279 significance still remains in these referenced scores. Note that We showed the 10-dB SNR data separated280 by masker type to make it match the 5-dB SNR figure. Note also that there was no significant effect of281 masker type, and no interaction with condition. Therefore, statistical differences represent the pattern of282 results when scores from the two masker types were averaged.283 In addition, as shown in Figure 4, participants rated the quality of the processed speech by both284 RNN and SepFormer models as approximately similar to that of the unprocessed signals before mixing285 with noise (i.e., in quiet). This suggests that model processing did not significantly distort the speech286 quality while suppressing the background interference. The benefits from model processing that have been287 reported so far are also consistently present across almost all individuals tested. Considering the large288 individual variability that is typical of CI population, these results demonstrate great promise towards289 clinical application in the near future for better noise reduction in more complex listening environments.290\nImprovement with models vs. demographic factors291\nA linear, mixed effects model with the change in score relative to the unprocessed noisy condition as292 the dependent variable, masker type, model (RNN/SepFormer), and demographic factors as independent293 variables, and ID as a random factor was performed. The demographic information is listed in Table 1294 (gender, age at testing, duration of CI use, age at onset of hearing loss, number of active electrodes, and295 coding strategy). A stepwise regression analysis was performed on the full linear model, and the output296 of the stepwise analysis was a linear model that only included masker type and DNN model, with ID as297 a random factor (demographic factors always increased the Akaiki criterion of the linear model.) This298 suggests that, for this sample, the potential benefit of a DNN-based speech enhancement algorithm was299 not related to any of the selected demographic factors.300\nDiscussion301\nBoth the RNN and SepFormer models introduced significant improvements in speech quality and intel-302 ligibility. The improvements were demonstrated across all acoustic evaluation metrics, including SDR,303\nPESQ, and STOI. The improvement in the evaluation scores from the unprocessed to processed noisy304 mixtures was statistically significant across all SNR test conditions, in both masker types, across all305 three metrics, and for both models. The observed statistical significance is visually apparent for the two306 quality metrics assessed herein: SDR and PESQ. The observed differences are not as apparent for the307 intelligibility metric\u2014STOI. This is perhaps because the room for improvement in normal-hearing model308 intelligibility is small in the relatively high-SNR conditions we tested, with the worst condition being309 1 dB SNR. In general, the SepFormer model, as the current state-of-the-art, outperformed the simpler310 RNN model. This advantage is especially prominent in lower-SNR conditions. For instance, in the 1311 dB SNR condition, the SepFormer introduced an improvement of 8.52 in SDR score as compared to the312 improvement of 3.35 from the RNN model. Even in STOI scores from the intelligibility metric, where313 the benefits from both models were not as great as in the other two evaluation metrics of quality, the314 SepFormer model still outperformed the simpler RNN structure.315 The improvements in speech quality and intelligibility shown across the acoustic evaluation metrics316 were largely mirrored in the behavioral testing conducted with CI listeners. The participants\u2019 intelli-317 gibility scores were significantly improved from unprocessed conditions after processing with the both318 DNN models, closing the distance between the scores with and without the presence of background noise.319 Given that SepFormer is the current state-of-the-art model for speech enhancement, while RNN is a sim-320 ple two-layer template model, it is not surprising that the SepFormer outperformed the RNN in every321 scenario. Both models still demonstrated improvements in speech intelligibility scores, even in the 10322 dB SNR conditions, where \u201cde-noising\u201d or enhancement of the target speech stimulus was a relatively323 straightforward task. Both models introduced more improvements in speech noise than non-speech noise,324 which is contrary to the benefit demonstrated by traditional signal processing strategies that are designed325 for removing statistically predictable and relatively stationary noises. These traditional signal processing326 algorithms fall short in more complex non-stationary speech backgrounds (Boll, 1979; Scalart & Filho,327 1996; Dawson et al., 2011; Loizou et al., 2005; Mauger et al., 2012). The better performance in speech-328 in-noise demonstrated by the RNN and SepFormer models is in agreement with the results from previous329 studies on machine-earning based noise-reduction models, such as DNNs and Gaussian mixture models330 (GMMs) (G. Kim et al., 2009; Bramsl\u00f8w et al., 2018; Chen et al., 2016; Healy et al., 2019, 2015, 2013;331 Monaghan et al., 2017; Goehring et al., 2017; Hu & Loizou, 2010; Lai et al., 2018). This result shows332 the promise of using machine-learning based models as a complementary algorithm to current existing333\nsignal processing strategies, especially for tackling more complex listening challenges, where existing solu-334 tions generally fail (Loizou et al., 2005; Mauger et al., 2012; Dawson et al., 2011). The model processing335 also introduced statistically negligible distortions in perceived speech quality as shown in the subjective336 evaluation scores. The fact that there were no significant contributions from any demographic disparity337 among participants suggests that the algorithms could broadly benefit the CI population regardless of338 their hearing etiology, duration of deafness, experience with CI listening, and processor settings.339 These results are consistent with previous studies on the application of machine learning algorithms340 to \u201cde-noising\u201d strategies for CIs (Hu & Loizou, 2010; Goehring et al., 2017, 2019; Lai et al., 2018). Our341 study further confirms the promise of using DNN-based algorithms to improve speech-in-noise hearing342 outcomes in CIs. While Hu & Loizou (2010) and Lai et al. (2018) used the same speaker for training343 and testing, our study further investigated the models\u2019 capability of generalization by using different344 speech and non-speech materials for testing vs. training. We also used a much larger and more diverse345 training dataset. The training dataset was approximately 30 hours in total duration with four distinct346 target-masker configurations. Within each configuration, ten different SNR conditions in a 1-dB step347 were implemented with equal representation. The models were also trained more extensively with 100348 learning cycles or epochs, whereas some other studies trained their models with a much less number of349 learning cycles. For instance, Goehring et al. (2019) trained their model for only one epoch. Although the350 successful training with such a small number of learning cycles helps demonstrate the model\u2019s promise of351 being continuously trained \u201con the go\u201d with smaller resources, we wanted to explore the full capacity of352 the models for noise reduction through a more thorough training process.353 In addition to the basic RNN model, we also implemented and tested the current best-performing354 model in the field of speech separation\u2014SepFormer, to explore the current limit to which a DNN-based355 strategy could suppress the noise for CI devices. While, not surprisingly, the SepFormer model outper-356 formed RNN in every test, it is a very complicated model, containing over 26 million parameters. The357 processing time of such a computationally heavy model turned out to be almost 5 times the duration of358 the incoming signal on average, which renders it not suitable for a real-time applications such as a CI359 audio processor. The time constraint for the processing delay in a real-time device such as CI should360 be below about 10-20 ms to avoid disturbance in speech production and audio-visual integration (Stone361 & Moore, 1999; Goehring et al., 2018, 2019; Bramsl\u00f8w et al., 2018). This timing delay should be even362 lower for individuals with single sided deafness who are fitted with CI on the deaf side (Zirn et al., 2015).363\nAnother limitation comes from the high demand for computational power and memory. Thus, at this364 time, it is unrealistic to run these highly complex state-of-the-art models even in high-end audio digital365 signal processors used in existing CIs. The RNN model, on the other hand, was a much computationally366 lighter model and the processing only took around 3% of the input signal duration on average. This367 implies that RNNs can in theory retain low-latency during inference, which makes them well-suited for368 real-time speech enhancement in assisted hearing devices.369\nConclusions370\nImproving speech comprehension in noisy environments is one of the most challenging problems in CI371 research and the development of future CI technologies. The results from this study demonstrate the372 promise of using DNN-based technologies for noise reduction in more complex and unpredictable, but373 common everyday listening environments, where current signal processing strategies fail to produce satis-374 factory performance. Both models evaluated in this study are single-channel models and therefore do not375 rely on a priori assumptions about the location of the sound sources. Future machine learning models376 could consider simulating more aspects of the \u201ccocktail-party listening\u201d, such as spatial-hearing sensitiv-377 ity, for even better de-noising performance. Machine learning technologies have already widely infiltrated378 many aspects of our daily life. Along with other recent research efforts, our study further showcases379 the potential benefit of introducing machine learning algorithms into existing CI technologies in order to380 enhance speech perception in complex noisy listening settings. Future plans include the deployment of381 the DNN models tested herein in a research processor to be tested against existing commercial solutions382 for noise reduction, as well as to be further evaluated in a wider variety of listening conditions.383\nFunding Acknowledgements: This research was supported by funding from the National Institutes384 of Health [Grant R01DC015989]; and MED-EL corporation.385 Acknowledgements: We thank our research subjects for their participation in this study. We also386 thank Jenna Felder for helping collect data from CI participants.387 Declaration of Conflicting Interests: The Authors declare that there is no conflict of interest.388 Data Accessibility: All data will be made available upon reasonable requests.389\n1Fine structure processing with parallel stimulation in the four apical channels. 2Fine structure processing with sequential stimulation in the four apical channels. 3Fine structure processing with sequential stimulation in a variable number of apical channels.\nReferences390\nAdvancedBionics. (2012). CLEARVOICE, clinical results. Retrieved from https://391 cochlearimplanthelp.files.wordpress.com/2012/06/ab clearvoice data etc.pdf392\nBentsen, T., May, T., Kressner, A. A., & Dau, T. (2018, May). The benefit of combining a deep neural393 network architecture with ideal ratio mask estimation in computational speech segregation to improve394 speech intelligibility. PLOS ONE , 13 (5), e0196924. Retrieved 2022-03-17, from https://journals395 .plos.org/plosone/article?id=10.1371/journal.pone.0196924 (Publisher: Public Library of396 Science) doi: 10.1371/journal.pone.0196924397\nBoll, S. (1979, April). Suppression of acoustic noise in speech using spectral subtraction. IEEE Transac-398 tions on Acoustics, Speech, and Signal Processing , 27 (2), 113\u2013120. (Conference Name: IEEE Transac-399 tions on Acoustics, Speech, and Signal Processing) doi: 10.1109/TASSP.1979.1163209400\nBramsl\u00f8w, L., Naithani, G., Hafez, A., Barker, T., Pontoppidan, N. H., & Virtanen, T. (2018, July).401 Improving competing voices segregation for hearing impaired listeners using a low-latency deep neural402 network algorithm. The Journal of the Acoustical Society of America, 144 (1), 172\u2013185. Retrieved403 2022-03-20, from https://asa.scitation.org/doi/full/10.1121/1.5045322 (Publisher: Acousti-404 cal Society of America) doi: 10.1121/1.5045322405\nChen, J., & Wang, D. (2017, June). Long short-term memory for speaker generalization in supervised406 speech separation. The Journal of the Acoustical Society of America, 141 (6), 4705\u20134714. Retrieved407 2022-03-21, from https://asa.scitation.org/doi/full/10.1121/1.4986931 (Publisher: Acousti-408 cal Society of America) doi: 10.1121/1.4986931409\nChen, J., Wang, Y., Yoho, S. E., Wang, D., & Healy, E. W. (2016, May). Large-scale training to increase410 speech intelligibility for hearing-impaired listeners in novel noises. The Journal of the Acoustical Society411 of America, 139 (5), 2604\u20132612. Retrieved 2022-03-20, from https://asa.scitation.org/doi/full/412 10.1121/1.4948445 (Publisher: Acoustical Society of America) doi: 10.1121/1.4948445413\nCullington, H. E., & Zeng, F.-G. (2008, January). Speech recognition with varying numbers and types414 of competing talkers by normal-hearing, cochlear-implant, and implant simulation subjects. The Jour-415 nal of the Acoustical Society of America, 123 (1), 450\u2013461. Retrieved 2022-03-17, from https://416\nasa.scitation.org/doi/full/10.1121/1.2805617 (Publisher: Acoustical Society of America) doi:417 10.1121/1.2805617418\nDawson, P. W., Mauger, S. J., & Hersbach, A. A. (2011, June). Clinical Evaluation of Signal-to-Noise419 Ratio\u2013Based Noise Reduction in Nucleus\u00ae Cochlear Implant Recipients. Ear and Hearing , 32 (3), 382\u2013420 390. Retrieved 2022-03-20, from https://journals.lww.com/ear-hearing/fulltext/2011/05000/421 Clinical Evaluation of Signal to Noise Ratio Based.11.aspx?casa token=-OkDlsdi-ScAAAAA:422 5Wm499dl3lCixxtBYmN5LHOcXL4qrKIFGYlYLBdctDIdf67m0oPoSq5Fh5Pj9ja068BbUzZWyLPYmkoAZJuom5c423 doi: 10.1097/AUD.0b013e318201c200424\nFalk, T. H., Parsa, V., Santos, J. F., Arehart, K., Hazrati, O., Huber, R., . . . Scollie, S. (2015, March).425 Objective Quality and Intelligibility Prediction for Users of Assistive Listening Devices: Advantages426 and limitations of existing tools. IEEE Signal Processing Magazine, 32 (2), 114\u2013124. (Conference Name:427 IEEE Signal Processing Magazine) doi: 10.1109/MSP.2014.2358871428\nFu, Q.-J., Shannon, R. V., & Wang, X. (1998, December). Effects of noise and spectral resolution on429 vowel and consonant recognition: Acoustic and electric hearing. The Journal of the Acoustical Society430 of America, 104 (6), 3586\u20133596. Retrieved 2022-03-17, from https://asa.scitation.org/doi/abs/431 10.1121/1.423941 (Publisher: Acoustical Society of America) doi: 10.1121/1.423941432\nGoehring, T., Bolner, F., Monaghan, J. J. M., van Dijk, B., Zarowski, A., & Bleeck, S. (2017, February).433 Speech enhancement based on neural networks improves speech intelligibility in noise for cochlear im-434 plant users. Hearing Research, 344 , 183\u2013194. Retrieved 2022-03-17, from https://www.sciencedirect435 .com/science/article/pii/S0378595516304348 doi: 10.1016/j.heares.2016.11.012436\nGoehring, T., Chapman, J. L., Bleeck, S., & Monaghan*, J. J. M. (2018, January). Tol-437 erable delay for speech production and perception: effects of hearing ability and experience438 with hearing aids. International Journal of Audiology , 57 (1), 61\u201368. Retrieved 2022-03-22,439 from https://doi.org/10.1080/14992027.2017.1367848 (Publisher: Taylor & Francis eprint:440 https://doi.org/10.1080/14992027.2017.1367848) doi: 10.1080/14992027.2017.1367848441\nGoehring, T., Keshavarzi, M., Carlyon, R. P., & Moore, B. C. J. (2019, July). Using recurrent neural442 networks to improve the perception of speech in non-stationary noise by people with cochlear im-443 plants. The Journal of the Acoustical Society of America, 146 (1), 705\u2013718. Retrieved 2022-03-15,444\nfrom https://asa.scitation.org/doi/full/10.1121/1.5119226 (Publisher: Acoustical Society of445 America) doi: 10.1121/1.5119226446\nGraves, A., Mohamed, A.-r., & Hinton, G. (2013, May). Speech recognition with deep recurrent neural447 networks. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp.448 6645\u20136649). (ISSN: 2379-190X) doi: 10.1109/ICASSP.2013.6638947449\nHealy, E. W., Delfarah, M., Johnson, E. M., & Wang, D. (2019, March). A deep learning algorithm to450 increase intelligibility for hearing-impaired listeners in the presence of a competing talker and rever-451 beration. The Journal of the Acoustical Society of America, 145 (3), 1378\u20131388. Retrieved 2022-03-20,452 from https://asa.scitation.org/doi/full/10.1121/1.5093547 (Publisher: Acoustical Society of453 America) doi: 10.1121/1.5093547454\nHealy, E. W., Yoho, S. E., Chen, J., Wang, Y., & Wang, D. (2015, September). An algorithm to increase455 speech intelligibility for hearing-impaired listeners in novel segments of the same noise type. The456 Journal of the Acoustical Society of America, 138 (3), 1660\u20131669. Retrieved 2022-03-20, from https://457 asa.scitation.org/doi/full/10.1121/1.4929493 (Publisher: Acoustical Society of America) doi:458 10.1121/1.4929493459\nHealy, E. W., Yoho, S. E., Wang, Y., & Wang, D. (2013, October). An algorithm to improve speech460 recognition in noise for hearing-impaired listeners. The Journal of the Acoustical Society of America,461 134 (4), 3029\u20133038. Retrieved 2022-03-20, from https://asa.scitation.org/doi/full/10.1121/462 1.4820893 (Publisher: Acoustical Society of America) doi: 10.1121/1.4820893463\nHersbach, A. A., Arora, K., Mauger, S. J., & Dawson, P. W. (2012, August). Combining Di-464 rectional Microphone and Single-Channel Noise Reduction Algorithms: A Clinical Evaluation465 in Difficult Listening Conditions With Cochlear Implant Users. Ear and Hearing , 33 (4), e13.466 Retrieved 2022-03-17, from https://journals.lww.com/ear-hearing/FullText/2012/07000/467 Combining Directional Microphone and.12.aspx?casa token=OGC6EKKsm6AAAAAA: cQ0afdE468 71RjPLpCBBH5YsVmBQ-lDgkhcWSgll7xfSburVOJ NhVCEZ8q8Gd6i-TaIBVcOKtro7BMFQ54jx0Ks doi:469 10.1097/AUD.0b013e31824b9e21470\nHochmair, I., Hochmair, E., Nopp, P., Waller, M., & Jolly, C. (2015, April). Deep electrode insertion and471 sound coding in cochlear implants. Hearing Research, 322 , 14\u201323. Retrieved 2021-08-23, from https://472\nwww.sciencedirect.com/science/article/pii/S0378595514001701 doi: 10.1016/j.heares.2014.10473 .006474\nHu, Y., & Loizou, P. C. (2008, January). Evaluation of Objective Quality Measures for Speech En-475 hancement. IEEE Transactions on Audio, Speech, and Language Processing , 16 (1), 229\u2013238. (Con-476 ference Name: IEEE Transactions on Audio, Speech, and Language Processing) doi: 10.1109/477 TASL.2007.911054478\nHu, Y., & Loizou, P. C. (2010, June). Environment-specific noise suppression for improved speech479 intelligibility by cochlear implant users. The Journal of the Acoustical Society of America, 127 (6),480 3689\u20133695. Retrieved 2022-03-20, from https://asa.scitation.org/doi/full/10.1121/1.3365256481 (Publisher: Acoustical Society of America) doi: 10.1121/1.3365256482\nKarita, S., Chen, N., Hayashi, T., Hori, T., Inaguma, H., Jiang, Z., . . . Zhang, W. (2019, December).483 A Comparative Study on Transformer vs RNN in Speech Applications. In 2019 IEEE Automatic484 Speech Recognition and Understanding Workshop (ASRU) (pp. 449\u2013456). doi: 10.1109/ASRU46091485 .2019.9003750486\nKeshavarzi, M., Goehring, T., Turner, R. E., & Moore, B. C. J. (2019, March). Comparison of effects487 on subjective intelligibility and quality of speech in babble for two algorithms: A deep recurrent neural488 network and spectral subtraction. The Journal of the Acoustical Society of America, 145 (3), 1493\u20131503.489 Retrieved 2022-03-21, from https://asa.scitation.org/doi/full/10.1121/1.5094765 (Publisher:490 Acoustical Society of America) doi: 10.1121/1.5094765491\nKeshavarzi, M., Goehring, T., Zakis, J., Turner, R. E., & Moore, B. C. J. (2018, January). Use of a Deep492 Recurrent Neural Network to Reduce Wind Noise: Effects on Judged Speech Intelligibility and Sound493 Quality. Trends in Hearing , 22 , 2331216518770964. Retrieved 2022-03-21, from https://doi.org/494 10.1177/2331216518770964 (Publisher: SAGE Publications Inc) doi: 10.1177/2331216518770964495\nKim, G., Lu, Y., Hu, Y., & Loizou, P. C. (2009, September). An algorithm that improves speech496 intelligibility in noise for normal-hearing listeners. The Journal of the Acoustical Society of America,497 126 (3), 1486\u20131494. Retrieved 2022-03-20, from https://asa.scitation.org/doi/full/10.1121/498 1.3184603 (Publisher: Acoustical Society of America) doi: 10.1121/1.3184603499\nKim, J., El-Khamy, M., & Lee, J. (2020, May). T-GSA: Transformer with Gaussian-Weighted500 Self-Attention for Speech Enhancement. In ICASSP 2020 - 2020 IEEE International Conference501 on Acoustics, Speech and Signal Processing (ICASSP) (pp. 6649\u20136653). (ISSN: 2379-190X) doi:502 10.1109/ICASSP40776.2020.9053591503\nKingma, D. P., & Ba, J. (2017, January). Adam: A Method for Stochastic Optimization. arXiv:1412.6980504 [cs] . Retrieved 2022-03-25, from http://arxiv.org/abs/1412.6980 (arXiv: 1412.6980)505\nKokkinakis, K., & Loizou, P. C. (2011, May). Evaluation of objective measures for quality assessment of506 reverberant speech. In 2011 IEEE International Conference on Acoustics, Speech and Signal Processing507 (ICASSP) (pp. 2420\u20132423). (ISSN: 2379-190X) doi: 10.1109/ICASSP.2011.5946972508\nKokkinakis, K., & Stohl, J. S. (2021, August). Optimized gain functions in ideal time-frequency masks509 and their application to dereverberation for cochlear implants. JASA Express Letters, 1 (8), 084401. Re-510 trieved 2022-07-25, from https://asa.scitation.org/doi/10.1121/10.0005740 (Publisher: Acous-511 tical Society of America) doi: 10.1121/10.0005740512\nKolb\u00e6k, M., Yu, D., Tan, Z.-H., & Jensen, J. (2017, October). Multitalker Speech Separation With513 Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks. , 25 (10), 1901\u2013514 1913. (Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language Processing) doi:515 10.1109/TASLP.2017.2726762516\nLai, Y.-H., Tsao, Y., Lu, X., Chen, F., Su, Y.-T., Chen, K.-C., . . . Lee, C.-H. (2018, August).517 Deep Learning\u2013Based Noise Reduction Approach to Improve Speech Intelligibility for Cochlear518 Implant Recipients. Ear and Hearing , 39 (4), 795\u2013809. Retrieved 2022-03-15, from https://519 journals.lww.com/ear-hearing/fulltext/2018/07000/deep learning based noise reduction520 approach to.18.aspx?casa token=mCO32iLZAyEAAAAA:h0F2Zb-fgVA1vujMPNehxLFHCXs4toevdOHq521 -XE1KGBzrn3V-AMp-Cdq 24zrRSD-KpGY8OuWN25k wR0J2na9Cf doi: 10.1097/AUD.0000000000000537522\nLeCun, Y., Bengio, Y., & Hinton, G. (2015, May). Deep learning. Nature, 521 (7553), 436\u2013444. Retrieved523 2022-03-21, from https://www.nature.com/articles/nature14539 (Number: 7553 Publisher: Na-524 ture Publishing Group) doi: 10.1038/nature14539525\nLi, N., Liu, S., Liu, Y., Zhao, S., & Liu, M. (2019, July). Neural Speech Synthesis with Transformer526 Network. Proceedings of the AAAI Conference on Artificial Intelligence, 33 (01), 6706\u20136713. Retrieved527 2022-03-22, from https://ojs.aaai.org/index.php/AAAI/article/view/4642 (Number: 01) doi:528 10.1609/aaai.v33i01.33016706529\nLoizou, P. C., Lobo, A., & Hu, Y. (2005, November). Subspace algorithms for noise reduction in cochlear530 implants. The Journal of the Acoustical Society of America, 118 (5), 2791\u20132793. Retrieved 2022-03-20,531 from https://asa.scitation.org/doi/full/10.1121/1.2065847 (Publisher: Acoustical Society of532 America) doi: 10.1121/1.2065847533\nLuo, Y., & Mesgarani, N. (2019, August). Conv-TasNet: Surpassing Ideal Time\u2013Frequency Magnitude534 Masking for Speech Separation. IEEE/ACM Transactions on Audio, Speech, and Language Processing ,535 27 (8), 1256\u20131266. (Conference Name: IEEE/ACM Transactions on Audio, Speech, and Language536 Processing) doi: 10.1109/TASLP.2019.2915167537\nMadhu, N., Spriet, A., Jansen, S., Koning, R., & Wouters, J. (2013, January). The Potential for Speech538 Intelligibility Improvement Using the Ideal Binary Mask and the Ideal Wiener Filter in Single Channel539 Noise Reduction Systems: Application to Auditory Prostheses. IEEE Transactions on Audio, Speech,540 and Language Processing , 21 (1), 63\u201372. (Conference Name: IEEE Transactions on Audio, Speech, and541 Language Processing) doi: 10.1109/TASL.2012.2213248542\nMauger, S. J., Dawson, P. W., & Hersbach, A. A. (2012, January). Perceptually optimized gain function543 for cochlear implant signal-to-noise ratio based noise reduction. The Journal of the Acoustical Society544 of America, 131 (1), 327\u2013336. Retrieved 2022-03-20, from https://asa.scitation.org/doi/full/545 10.1121/1.3665990 (Publisher: Acoustical Society of America) doi: 10.1121/1.3665990546\nMay, T., & Dau, T. (2014, December). Requirements for the evaluation of computational speech segre-547 gation systems. The Journal of the Acoustical Society of America, 136 (6), EL398\u2013EL404. Retrieved548 2022-03-21, from https://asa.scitation.org/doi/full/10.1121/1.4901133 (Publisher: Acousti-549 cal Society of America) doi: 10.1121/1.4901133550\nMED-EL. (2021). Automatic Sound Management 3.0 with the SONNET 2 and RONDO 3 Audio Proces-551 sors. Retrieved from https://www.medel.pro/online-resources/white-papers552\nMonaghan, J. J. M., Goehring, T., Yang, X., Bolner, F., Wang, S., Wright, M. C. M., & Bleeck, S. (2017,553 March). Auditory inspired machine learning techniques can improve speech intelligibility and quality554 for hearing-impaired listeners. The Journal of the Acoustical Society of America, 141 (3), 1985\u20131998.555 Retrieved 2022-03-20, from https://asa.scitation.org/doi/full/10.1121/1.4977197 (Publisher:556 Acoustical Society of America) doi: 10.1121/1.4977197557\nRix, A., Beerends, J., Hollier, M., & Hekstra, A. (2001, May). Perceptual evaluation of speech558 quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In559 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat.560 No.01CH37221) (Vol. 2, pp. 749\u2013752 vol.2). (ISSN: 1520-6149) doi: 10.1109/ICASSP.2001.941023561\nScalart, P., & Filho, J. (1996, May). Speech enhancement based on a priori signal to noise estima-562 tion. In 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference563 Proceedings (Vol. 2, pp. 629\u2013632 vol. 2). (ISSN: 1520-6149) doi: 10.1109/ICASSP.1996.543199564\nStone, M. A., & Moore, B. C. J. (1999, June). Tolerable Hearing Aid Delays. I. Estima-565 tion of Limits Imposed by the Auditory Path Alone Using Simulated Hearing Losses. Ear566 and Hearing , 20 (3), 182\u2013192. Retrieved 2022-03-22, from https://journals.lww.com/567 ear-hearing/Fulltext/1999/06000/Tolerable Hearing Aid Delays I Estimation of568 .2.aspx?casa token=yreyDi0 9qIAAAAA:Iihb31R3n Z NBPd9MPB7lodfbL4rQI4-7XEIdi5b569 R2hGOmL6QUTiXGOxy6ME9BXoI4nu66HB82hfM1blELt7k570\nSubakan, C., Ravanelli, M., Cornell, S., Bronzi, M., & Zhong, J. (2021, March). Attention is All You Need571 in Speech Separation. arXiv:2010.13154 [cs, eess] . Retrieved 2021-03-18, from http://arxiv.org/572 abs/2010.13154 (arXiv: 2010.13154)573\nTaal, C. H., Hendriks, R. C., Heusdens, R., & Jensen, J. (2010, March). A short-time objec-574 tive intelligibility measure for time-frequency weighted noisy speech. In 2010 IEEE International575 Conference on Acoustics, Speech and Signal Processing (pp. 4214\u20134217). (ISSN: 2379-190X) doi:576 10.1109/ICASSP.2010.5495701577\nTaal, C. H., Hendriks, R. C., Heusdens, R., & Jensen, J. (2011, September). An Algorithm for Intelli-578 gibility Prediction of Time\u2013Frequency Weighted Noisy Speech. IEEE Transactions on Audio, Speech,579\nand Language Processing , 19 (7), 2125\u20132136. (Conference Name: IEEE Transactions on Audio, Speech,580 and Language Processing) doi: 10.1109/TASL.2011.2114881581\nTang, Y., Arnold, C., & Cox, T. (2017, December). A Study on the Relationship between the Intel-582 ligibility and Quality of Algorithmically-Modified Speech for Normal Hearing Listeners. Journal of583 Otorhinolaryngology, Hearing and Balance Medicine, 1 , 5. doi: 10.3390/ohbm1010005584\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., . . . Polosukhin, I. (2017).585 Attention is All you Need. In Advances in Neural Information Processing Systems (Vol. 30). Curran586 Associates, Inc. Retrieved 2022-03-22, from https://proceedings.neurips.cc/paper/2017/hash/587 3f5ee243547dee91fbd053c1c4a845aa-Abstract.html588\nVincent, E., Gribonval, R., & Fevotte, C. (2006, July). Performance measurement in blind audio589 source separation. IEEE Transactions on Audio, Speech, and Language Processing , 14 (4), 1462\u2013590 1469. (Conference Name: IEEE Transactions on Audio, Speech, and Language Processing) doi:591 10.1109/TSA.2005.858005592\nWeninger, F., Erdogan, H., Watanabe, S., Vincent, E., Le Roux, J., Hershey, J. R., & Schuller, B. (2015).593 Speech Enhancement with LSTM Recurrent Neural Networks and its Application to Noise-Robust ASR.594 In E. Vincent, A. Yeredor, Z. Koldovsky\u0301, & P. Tichavsky\u0301 (Eds.), Latent Variable Analysis and Signal595 Separation (pp. 91\u201399). Cham: Springer International Publishing. doi: 10.1007/978-3-319-22482-4 11596\nWouters, J., & Vanden Berghe, J. (2001, October). Speech Recognition in Noise for Cochlear Implantees597 with a Two-Microphone Monaural Adaptive Noise Reduction System. Ear and Hearing , 22 (5), 420\u2013598 430.599\nZirn, S., Arndt, S., Aschendorff, A., & Wesarg, T. (2015, October). Interaural stimulation timing600 in single sided deaf cochlear implant users. Hearing Research, 328 , 148\u2013156. Retrieved 2022-07-27,601 from https://www.sciencedirect.com/science/article/pii/S037859551500177X doi: 10.1016/602 j.heares.2015.08.010603"
        }
    ],
    "title": "Deep neural network algorithms for noise reduction and their application to cochlear implants",
    "year": 2022
}