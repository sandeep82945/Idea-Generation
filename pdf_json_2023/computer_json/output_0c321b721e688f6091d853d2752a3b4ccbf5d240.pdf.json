{
    "abstractText": "Cancer is one of the most life-threatening diseases worldwide, and head and neck (H&N) cancer is a prevalent type with hundreds of thousands of new cases recorded each year. Clinicians use medical imaging modalities such as computed tomography and positron emission tomography to detect the presence of a tumor, and they combine that information with clinical data for patient prognosis. The process is mostly challenging and time-consuming. Machine learning and deep learning can automate these tasks to help clinicians with highly promising results. This work studies two approaches for H&N tumor segmentation: (i) exploration and comparison of vision transformer (ViT)-based and convolutional neural network-based models; and (ii) proposal of a novel 2D perspective to working with 3D data. Furthermore, this work proposes two new architectures for the prognosis task. An ensemble of several models predicts patient outcomes (which won the HECKTOR 2021 challenge prognosis task), and a ViT-based framework concurrently performs patient outcome prediction and tumor segmentation, which outperforms the ensemble model.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ikboljon Sobirov"
        },
        {
            "affiliations": [],
            "name": "Mohammad Yaqub"
        }
    ],
    "id": "SP:2b6776624065d373bcd00f55edaf63cd6fd35eb0",
    "references": [
        {
            "authors": [
                "Harris A Ahmad",
                "Hui Jing Yu",
                "Colin G Miller"
            ],
            "title": "Medical imaging modalities",
            "venue": "In Medical imaging in clinical trials,",
            "year": 2014
        },
        {
            "authors": [
                "Byung Bok Ahn"
            ],
            "title": "The compact 3d convolutional neural network for medical images",
            "venue": "Standford University,",
            "year": 2017
        },
        {
            "authors": [
                "Chengyang An",
                "Huai Chen",
                "Lisheng Wang"
            ],
            "title": "A coarse-to-fine framework for head and neck tumor segmentation in ct and pet images. In 3D Head and Neck Tumor Segmentation in PET/CT Challenge",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Andrearczyk",
                "Valentin Oreiller",
                "Sarah Boughdad",
                "Catherine Chez Le Rest",
                "Hesham Elhalawani",
                "Mario Jreige",
                "John O Prior",
                "Martin Valli\u00e8res",
                "Dimitris Visvikis",
                "Mathieu Hatt"
            ],
            "title": "Overview of the hecktor challenge at miccai 2021: Automatic head and neck tumor segmentation and outcome prediction in pet/ct images",
            "venue": "arXiv preprint arXiv:2201.04138,",
            "year": 2022
        },
        {
            "authors": [
                "Yoshiko Ariji",
                "Motoki Fukuda",
                "Yoshitaka Kise",
                "Michihito Nozawa",
                "Yudai Yanashita",
                "Hiroshi Fujita",
                "Akitoshi Katsumata",
                "Eiichiro Ariji"
            ],
            "title": "Contrast-enhanced computed tomography image assessment of cervical lymph node metastasis in patients with oral cancer by using a deep learning system of artificial intelligence",
            "venue": "Oral surgery, oral medicine, oral pathology and oral radiology,",
            "year": 2019
        },
        {
            "authors": [
                "Anne Aup\u00e9rin"
            ],
            "title": "Epidemiology of head and neck cancers: an update",
            "venue": "Current opinion in oncology,",
            "year": 2020
        },
        {
            "authors": [
                "Ed-Edily Mohd Azhari",
                "Muhd Mudzakkir Mohd Hatta",
                "Zaw Zaw Htike",
                "Shoon Lei Win"
            ],
            "title": "Tumor detection in medical imaging: a survey",
            "venue": "International Journal of Advanced Information Technology,",
            "year": 2014
        },
        {
            "authors": [
                "Christian F Baumgartner",
                "Lisa M Koch",
                "Marc Pollefeys",
                "Ender Konukoglu"
            ],
            "title": "An exploration of 2d and 3d deep learning techniques for cardiac mr image segmentation",
            "venue": "In International Workshop on Statistical Atlases and Computational Models of the Heart,",
            "year": 2017
        },
        {
            "authors": [
                "Ralf Bender",
                "Thomas Augustin",
                "Maria Blettner"
            ],
            "title": "Generating survival times to simulate cox proportional hazards models",
            "venue": "Statistics in medicine,",
            "year": 2005
        },
        {
            "authors": [
                "Karl Y Bilimoria",
                "Andrew K Stewart",
                "David P Winchester",
                "Clifford Y Ko"
            ],
            "title": "The national cancer data base: a powerful initiative to improve cancer care in the united states",
            "venue": "Annals of surgical oncology,",
            "year": 2008
        },
        {
            "authors": [
                "Freddie Bray",
                "Jacques Ferlay",
                "Isabelle Soerjomataram",
                "Rebecca L Siegel",
                "Lindsey A Torre",
                "Ahmedin Jemal"
            ],
            "title": "Global cancer statistics 2018: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries",
            "venue": "CA: a cancer journal for clinicians,",
            "year": 2018
        },
        {
            "authors": [
                "Siow-Wee Chang",
                "Sameem Abdul-Kareem",
                "Amir Feisal Merican",
                "Rosnah Binti Zain"
            ],
            "title": "Oral cancer prognosis based on clinicopathologic and genomic markers using a hybrid of feature selection and machine learning methods",
            "venue": "BMC bioinformatics,",
            "year": 2013
        },
        {
            "authors": [
                "Jieneng Chen",
                "Yongyi Lu",
                "Qihang Yu",
                "Xiangde Luo",
                "Ehsan Adeli",
                "Yan Wang",
                "Le Lu",
                "Alan L Yuille",
                "Yuyin Zhou"
            ],
            "title": "Transunet: Transformers make strong encoders for medical image segmentation",
            "venue": "arXiv preprint arXiv:2102.04306,",
            "year": 2021
        },
        {
            "authors": [
                "Laura QM Chow"
            ],
            "title": "Head and neck cancer",
            "venue": "New England Journal of Medicine,",
            "year": 2020
        },
        {
            "authors": [
                "Li-Yeh Chuang",
                "Kuo-Chuan Wu",
                "Hsueh-Wei Chang",
                "Cheng-Hong Yang"
            ],
            "title": "Support vector machine-based prediction for oral cancer using four snps in dna repair genes",
            "venue": "In World Congress on Engineering",
            "year": 2012
        },
        {
            "authors": [
                "\u00d6zg\u00fcn \u00c7i\u00e7ek",
                "Ahmed Abdulkadir",
                "Soeren S Lienkamp",
                "Thomas Brox",
                "Olaf Ronneberger"
            ],
            "title": "3d u-net: learning dense volumetric segmentation from sparse annotation",
            "venue": "In International conference on medical image computing and computer-assisted intervention,",
            "year": 2016
        },
        {
            "authors": [
                "David R Cox"
            ],
            "title": "Regression models and life-tables",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1972
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Lee R Dice"
            ],
            "title": "Measures of the amount of ecologic association between species",
            "venue": "Ecology, 26(3):297\u2013302,",
            "year": 1945
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Ahmed Elnakib",
                "Georgy Gimel\u2019farb",
                "Jasjit S Suri",
                "Ayman El-Baz"
            ],
            "title": "Medical image segmentation: a brief survey. Multi Modality State-of-the-Art Medical Image Segmentation and Registration Methodologies",
            "year": 2011
        },
        {
            "authors": [
                "Quanfu Fan",
                "Chun-Fu Chen",
                "Rameswar Panda"
            ],
            "title": "Can an image classifier suffice for action recognition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Xue Feng",
                "Nicholas J. Tustison",
                "Sohil H. Patel",
                "Craig H. Meyer"
            ],
            "title": "Brain tumor segmentation using an ensemble of 3d u-nets and overall survival prediction using radiomic features",
            "venue": "Frontiers in Computational Neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Stephane Fotso"
            ],
            "title": "Deep neural networks for survival analysis based on a multi-task framework",
            "venue": "arXiv preprint arXiv:1801.05512,",
            "year": 2018
        },
        {
            "authors": [
                "Stephane Fotso"
            ],
            "title": "PySurvival: Open source package for survival analysis modeling, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Mogana Darshini Ganggayah",
                "Nur Aishah Taib",
                "Yip Cheng Har",
                "Pietro Lio",
                "Sarinder Kaur Dhillon"
            ],
            "title": "Predicting factors for survival of breast cancer patients using machine learning techniques",
            "venue": "BMC medical informatics and decision making,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Giraud",
                "Philippe Giraud",
                "Anne Gasnier",
                "Radouane El Ayachy",
                "Sarah Kreps",
                "Jean-Philippe Foy",
                "Catherine Durdux",
                "Florence Huguet",
                "Anita Burgun",
                "Jean- Emmanuel Bibault"
            ],
            "title": "Radiomics and machine learning for radiotherapy in head and neck cancers",
            "venue": "Frontiers in oncology,",
            "year": 2019
        },
        {
            "authors": [
                "Cyril Goutte",
                "Eric Gaussier"
            ],
            "title": "A probabilistic interpretation of precision, recall and f-score, with implication for evaluation",
            "venue": "In European conference on information retrieval,",
            "year": 2005
        },
        {
            "authors": [
                "Patrick K Ha",
                "Alia Hdeib",
                "David Goldenberg",
                "Heather Jacene",
                "Pavni Patel",
                "Wayne Koch",
                "Joseph Califano",
                "Charles W Cummings",
                "Paul W Flint",
                "Richard Wahl"
            ],
            "title": "The role of positron emission tomography and computed tomography fusion in the management of early-stage and advanced-stage primary head and neck squamous cell carcinoma",
            "venue": "Archives of Otolaryngology\u2013Head & Neck Surgery,",
            "year": 2006
        },
        {
            "authors": [
                "D Hashim",
                "E Genden",
                "M Posner",
                "M Hashibe",
                "P Boffetta"
            ],
            "title": "Head and neck cancer prevention: from primary prevention to impact of clinicians on reducing burden",
            "venue": "Annals of Oncology,",
            "year": 2019
        },
        {
            "authors": [
                "Zaki Hasnain",
                "Jeremy Mason",
                "Karanvir Gill",
                "Gus Miranda",
                "Inderbir S Gill",
                "Peter Kuhn",
                "Paul K Newton"
            ],
            "title": "Machine learning models for predicting post-cystectomy recurrence and survival in bladder cancer patients",
            "venue": "PloS one,",
            "year": 2019
        },
        {
            "authors": [
                "Ali Hatamizadeh",
                "Yucheng Tang",
                "Vishwesh Nath",
                "Dong Yang",
                "Andriy Myronenko",
                "Bennett Landman",
                "Holger R Roth",
                "Daguang Xu"
            ],
            "title": "Unetr: Transformers for 3d medical image segmentation",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Havaei",
                "Axel Davy",
                "David Warde-Farley",
                "Antoine Biard",
                "Aaron Courville",
                "Yoshua Bengio",
                "Chris Pal",
                "Pierre-Marc Jodoin",
                "Hugo Larochelle"
            ],
            "title": "Brain tumor segmentation with deep neural networks",
            "venue": "Medical image analysis,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "QI Hou",
                "Zhi-Tong Bing",
                "Cheng Hu",
                "Mao-Yin Li",
                "Ke-Hu Yang",
                "Zu Mo",
                "Xiang-Wei Xie",
                "Ji-Lin Liao",
                "Yan Lu",
                "Shigeo Horie"
            ],
            "title": "Rankprod combined with genetic algorithm optimized artificial neural network establishes a diagnostic and prognostic prediction model that revealed c1qtnf3 as a biomarker for prostate cancer",
            "year": 2018
        },
        {
            "authors": [
                "Frederick Matthew Howard",
                "Sara Kochanny",
                "Matthew Koshy",
                "Michael Spiotto",
                "Alexander T Pearson"
            ],
            "title": "Machine learning\u2013guided adjuvant treatment of head and neck cancer",
            "venue": "JAMA network open,",
            "year": 2025
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Man Hung",
                "Jungweon Park",
                "Eric S Hon",
                "Jerry Bounsanga",
                "Sara Moazzami",
                "Bianca Ruiz-Negr\u00f3n",
                "Dawei Wang"
            ],
            "title": "Artificial intelligence in dentistry: harnessing big data to predict oral cancer survival",
            "venue": "World journal of clinical oncology,",
            "year": 2020
        },
        {
            "authors": [
                "Andrei Iantsen",
                "Dimitris Visvikis",
                "Mathieu Hatt"
            ],
            "title": "Squeeze-and-excitation normalization for automated delineation of head and neck primary tumors in combined pet and ct images. In 3D Head and Neck Tumor Segmentation in PET/CT Challenge",
            "year": 2020
        },
        {
            "authors": [
                "Fabian Isensee",
                "Paul F J\u00e4ger",
                "Peter M Full",
                "Philipp Vollmuth",
                "Klaus H Maier- Hein"
            ],
            "title": "nnu-net for brain tumor segmentation",
            "venue": "In International MICCAI Brainlesion Workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Hemant Ishwaran",
                "Udaya B Kogalur",
                "Eugene H Blackstone",
                "Michael S Lauer"
            ],
            "title": "Random survival forests",
            "venue": "The annals of applied statistics,",
            "year": 2008
        },
        {
            "authors": [
                "Md Shafiul Islam",
                "Naima Kaabouch",
                "Wen Chen Hu"
            ],
            "title": "A survey of medical imaging techniques used for breast cancer detection",
            "venue": "In IEEE International Conference on Electro-Information",
            "year": 2013
        },
        {
            "authors": [
                "Daniel E Johnson",
                "Barbara Burtness",
                "C Ren\u00e9 Leemans",
                "Vivian Wai Yan Lui",
                "Julie E Bauman",
                "Jennifer R Grandis"
            ],
            "title": "Head and neck squamous cell carcinoma",
            "venue": "Nature reviews Disease primers,",
            "year": 2020
        },
        {
            "authors": [
                "Jared L Katzman",
                "Uri Shaham",
                "Alexander Cloninger",
                "Jonathan Bates",
                "Tingting Jiang",
                "Yuval Kluger"
            ],
            "title": "Deepsurv: personalized treatment recommender system using a cox proportional hazards deep neural network",
            "venue": "BMC medical research methodology,",
            "year": 2018
        },
        {
            "authors": [
                "Michal Kazmierski",
                "Mattea Welch",
                "Sejin Kim",
                "Chris McIntosh",
                "Princess Margaret Head",
                "Neck Cancer Group",
                "Katrina Rey-McIntyre",
                "Shao Hui Huang",
                "Tirth Patel",
                "Tony Tadic"
            ],
            "title": "A machine learning challenge for prognostic modelling in head and neck cancer using multi-modal data",
            "venue": "arXiv preprint arXiv:2101.11935,",
            "year": 2021
        },
        {
            "authors": [
                "Salman Khan",
                "Muzammal Naseer",
                "Munawar Hayat",
                "Syed Waqas Zamir",
                "Fahad Shahbaz Khan",
                "Mubarak Shah"
            ],
            "title": "Transformers in vision: A survey",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "Naresh Khuriwal",
                "Nidhi Mishra"
            ],
            "title": "Breast cancer diagnosis using deep learning algorithm",
            "venue": "In 2018 International Conference on Advances in Computing, Communication Control and Networking (ICACCCN),",
            "year": 2018
        },
        {
            "authors": [
                "Dhananjay Kumar",
                "Bengt"
            ],
            "title": "Klefsj\u00f6. Proportional hazards model: a review",
            "venue": "Reliability Engineering & System Safety,",
            "year": 1994
        },
        {
            "authors": [
                "Christopher D Lansford",
                "Reidar Grenman",
                "Henning Bier",
                "Kenneth D Somers",
                "Sang Yoon Kim",
                "Theresa L Whiteside",
                "Gary L Clayman",
                "Hans-J Welkoborsky",
                "Thomas E Carey"
            ],
            "title": "Head and neck cancers",
            "venue": "In Human cell culture,",
            "year": 1999
        },
        {
            "authors": [
                "Jahanzaib Latif",
                "Chuangbai Xiao",
                "Azhar Imran",
                "Shanshan Tu"
            ],
            "title": "Medical imaging using machine learning and deep learning algorithms: a review",
            "venue": "In 2019 2nd International conference on computing, mathematics and engineering technologies (iCoMET),",
            "year": 2019
        },
        {
            "authors": [
                "Jiayun Li",
                "Karthik V Sarma",
                "King Chung Ho",
                "Arkadiusz Gertych",
                "Beatrice S Knudsen",
                "Corey W Arnold"
            ],
            "title": "A multi-scale u-net for semantic segmentation of histological images from radical prostatectomies",
            "venue": "In AMIA Annual Symposium Proceedings,",
            "year": 2017
        },
        {
            "authors": [
                "Siqi Liu",
                "Donghao Zhang",
                "Yang Song",
                "Hanchuan Peng",
                "Weidong Cai"
            ],
            "title": "Triplecrossing 2.5 d convolutional neural network for detecting neuronal arbours in 3d microscopic images",
            "venue": "In International Workshop on Machine Learning in Medical Imaging,",
            "year": 2017
        },
        {
            "authors": [
                "Chip M. Lynch",
                "Behnaz Abdollahi",
                "Joshua D. Fuqua",
                "Alexandra R. de Carlo",
                "James A. Bartholomai",
                "Rayeanne N. Balgemann",
                "Victor H. van Berkel",
                "Hermann B. 38 Frieboes"
            ],
            "title": "Prediction of lung cancer patient survival via supervised machine learning classification techniques",
            "venue": "International Journal of Medical Informatics,",
            "year": 2017
        },
        {
            "authors": [
                "Jun Ma",
                "Xiaoping Yang"
            ],
            "title": "Combining cnn and hybrid active contours for head and neck tumor segmentation in ct and pet images. In 3D Head and Neck Tumor Segmentation in PET/CT Challenge",
            "year": 2020
        },
        {
            "authors": [
                "Shanthi Marur",
                "Arlene A Forastiere"
            ],
            "title": "Head and neck cancer: changing epidemiology, diagnosis, and treatment",
            "venue": "In Mayo Clinic Proceedings,",
            "year": 2008
        },
        {
            "authors": [
                "Saleha Masood",
                "Muhammad Sharif",
                "Afifa Masood",
                "Mussarat Yasmin",
                "Mudassar Raza"
            ],
            "title": "A survey on medical image segmentation",
            "venue": "Current Medical Imaging,",
            "year": 2015
        },
        {
            "authors": [
                "Bjoern H Menze",
                "Andras Jakab",
                "Stefan Bauer",
                "Jayashree Kalpathy-Cramer",
                "Keyvan Farahani",
                "Justin Kirby",
                "Yuliya Burren",
                "Nicole Porz",
                "Johannes Slotboom",
                "Roland Wiest"
            ],
            "title": "The multimodal brain tumor image segmentation benchmark (brats)",
            "venue": "IEEE transactions on medical imaging,",
            "year": 1993
        },
        {
            "authors": [
                "Fausto Milletari",
                "Nassir Navab",
                "Seyed-Ahmad Ahmadi"
            ],
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "In 2016 fourth international conference on 3D vision (3DV),",
            "year": 2016
        },
        {
            "authors": [
                "Khushboo Munir",
                "Hassan Elahi",
                "Afsheen Ayub",
                "Fabrizio Frezza",
                "Antonello Rizzi"
            ],
            "title": "Cancer diagnosis using deep learning: a bibliographic review",
            "venue": "Cancers,",
            "year": 2019
        },
        {
            "authors": [
                "\u015a\u0131lvia Delgado Olabarriaga",
                "Arnold WM Smeulders"
            ],
            "title": "Interaction in the segmentation of medical images: A survey",
            "venue": "Medical image analysis,",
            "year": 2001
        },
        {
            "authors": [
                "Valentin Oreiller",
                "Vincent Andrearczyk",
                "Mario Jreige",
                "Sarah Boughdad",
                "Hesham Elhalawani",
                "Joel Castelli",
                "Martin Valli\u00e8res",
                "Simeng Zhu",
                "Juanying Xie",
                "Ying Peng"
            ],
            "title": "Head and neck tumor segmentation in pet/ct: the hecktor challenge",
            "venue": "Medical image analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Kanghee Park",
                "Amna Ali",
                "Dokyoon Kim",
                "Yeolwoo An",
                "Minkoo Kim",
                "Hyunjung Shin"
            ],
            "title": "Robust predictive model for evaluating breast cancer",
            "venue": "survivability. Engineering Applications of Artificial Intelligence,",
            "year": 2013
        },
        {
            "authors": [
                "Chintan Parmar",
                "Patrick Grossmann",
                "Derek Rietveld",
                "Michelle M Rietbergen",
                "Philippe Lambin",
                "Hugo JWL Aerts"
            ],
            "title": "Radiomic machine-learning classifiers for prognostic biomarkers of head and neck cancer",
            "venue": "Frontiers in oncology,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaofeng Qi",
                "Junjie Hu",
                "Lei Zhang",
                "Sen Bai",
                "Zhang Yi"
            ],
            "title": "Automated segmentation of the clinical target volume in the planning ct for breast cancer using deep neural networks",
            "venue": "IEEE Transactions on Cybernetics,",
            "year": 2020
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In International Conference on Medical image computing and computer-assisted intervention,",
            "year": 2015
        },
        {
            "authors": [
                "Holger R Roth",
                "Le Lu",
                "Ari Seff",
                "Kevin M Cherry",
                "Joanne Hoffman",
                "Shijun Wang",
                "Jiamin Liu",
                "Evrim Turkbey",
                "Ronald M Summers"
            ],
            "title": "A new 2.5 d representation for lymph node detection using random sets of deep convolutional neural network observations",
            "venue": "In International conference on medical image computing and computerassisted intervention,",
            "year": 2014
        },
        {
            "authors": [
                "Numan Saeed",
                "Roba Al Majzoub",
                "Ikboljon Sobirov",
                "Mohammad Yaqub"
            ],
            "title": "An ensemble approach for patient prognosis of head and neck tumor using multimodal data. In 3D Head and Neck Tumor Segmentation in PET/CT Challenge",
            "year": 2021
        },
        {
            "authors": [
                "Numan Saeed",
                "Ikboljon Sobirov",
                "Roba Al Majzoub",
                "Mohammad Yaqub"
            ],
            "title": "Tmss: An end-to-end transformer-based multimodal network for segmentation and survival prediction",
            "venue": "In International Conference on Medical Image Computing and Computer- Assisted Intervention,",
            "year": 2022
        },
        {
            "authors": [
                "Agustina La Greca Saint-Esteven",
                "Marta Bogowicz",
                "Ender Konukoglu",
                "Oliver Riesterer",
                "Panagiotis Balermpas",
                "Matthias Guckenberger",
                "Stephanie Tanadini-Lang",
                "Janita E van Timmeren"
            ],
            "title": "A 2.5 d convolutional neural network for hpv prediction in advanced oropharyngeal cancer",
            "venue": "Computers in biology and medicine,",
            "year": 2022
        },
        {
            "authors": [
                "Khailash Santhakumar",
                "B Ravi Kiran",
                "Thomas Gauthier",
                "Senthil Yogamani"
            ],
            "title": "Exploring 2d data augmentation for 3d monocular object detection",
            "venue": "arXiv preprint arXiv:2104.10786,",
            "year": 2021
        },
        {
            "authors": [
                "Jin-ah Sim",
                "Young Ho Yun"
            ],
            "title": "Predicting disease-free lung cancer survival using patient reported outcome (pro) measurements with comparisons of five machine learning techniques (mlt)",
            "venue": "MEDINFO",
            "year": 2019
        },
        {
            "authors": [
                "Leslie H Sobin",
                "Mary K Gospodarowicz",
                "Christian Wittekind"
            ],
            "title": "TNM classification of malignant tumours",
            "year": 2011
        },
        {
            "authors": [
                "Ikboljon Sobirov",
                "Otabek Nazarov",
                "Hussain Alasmawi",
                "Mohammad Yaqub"
            ],
            "title": "Automatic segmentation of head and neck tumor: How powerful transformers are",
            "venue": "In Medical Imaging with Deep Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ikboljon Sobirov",
                "Numan Saeed",
                "Mohammad Yaqub"
            ],
            "title": "Segmentation with super images: A new 2d perspective on 3d medical image analysis",
            "venue": "arXiv preprint arXiv:2205.02847,",
            "year": 2022
        },
        {
            "authors": [
                "Qingyuan Song",
                "John D Seigne",
                "Alan R Schned",
                "Karl T Kelsey",
                "Margaret R Karagas",
                "Saeed Hassanpour"
            ],
            "title": "A machine learning approach for long-term prognosis of bladder cancer based on clinical and molecular features",
            "venue": "AMIA Summits on Translational Science Proceedings,",
            "year": 2020
        },
        {
            "authors": [
                "Primo\u017e Strojan",
                "Katherine A Hutcheson",
                "Avraham Eisbruch",
                "Jonathan J Beitler",
                "Johannes A Langendijk",
                "Anne WM Lee",
                "June Corry",
                "William M Mendenhall",
                "Robert Smee",
                "Alessandra Rinaldo"
            ],
            "title": "Treatment of late sequelae after radiotherapy for head and neck cancer",
            "venue": "Cancer treatment reviews,",
            "year": 2017
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor Lempitsky"
            ],
            "title": "Instance normalization: The missing ingredient for fast stylization",
            "venue": "arXiv preprint arXiv:1607.08022,",
            "year": 2016
        },
        {
            "authors": [
                "Wenxuan Wang",
                "Chen Chen",
                "Meng Ding",
                "Jiangyun Li",
                "Hong Yu",
                "Sen Zha"
            ],
            "title": "Transbts: Multimodal brain tumor segmentation using transformer",
            "year": 2021
        },
        {
            "authors": [
                "Wenxuan Wang",
                "Chen Chen",
                "Meng Ding",
                "Hong Yu",
                "Sen Zha",
                "Jiangyun Li"
            ],
            "title": "Transbts: Multimodal brain tumor segmentation using transformer",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew J Wong",
                "Aasheesh Kanwar",
                "Abdallah S Mohamed",
                "Clifton D Fuller"
            ],
            "title": "Radiomics in head and neck cancer: from exploration to application",
            "venue": "Translational cancer research,",
            "year": 2016
        },
        {
            "authors": [
                "Juanying Xie",
                "Ying Peng"
            ],
            "title": "The head and neck tumor segmentation based on 3d unet. In 3D Head and Neck Tumor Segmentation in PET/CT Challenge",
            "year": 2021
        },
        {
            "authors": [
                "Yutong Xie",
                "Jianpeng Zhang",
                "Chunhua Shen",
                "Yong Xia"
            ],
            "title": "Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation",
            "venue": "In International conference on medical image computing and computer-assisted intervention,",
            "year": 2021
        },
        {
            "authors": [
                "Zhaohan Xiong",
                "Qing Xia",
                "Zhiqiang Hu",
                "Ning Huang",
                "Cheng Bian",
                "Yefeng Zheng",
                "Sulaiman Vesal",
                "Nishant Ravikumar",
                "Andreas Maier",
                "Xin Yang"
            ],
            "title": "A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging",
            "venue": "Medical Image Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Guoping Xu",
                "Xingrong Wu",
                "Xuan Zhang",
                "Xinwei He"
            ],
            "title": "Levit-unet: Make faster encoders with transformer for medical image segmentation",
            "venue": "arXiv preprint arXiv:2107.08623,",
            "year": 2021
        },
        {
            "authors": [
                "Jiancheng Yang",
                "Xiaoyang Huang",
                "Yi He",
                "Jingwei Xu",
                "Canqian Yang",
                "Guozheng Xu",
                "Bingbing Ni"
            ],
            "title": "Reinventing 2d convolutions for 3d images",
            "venue": "IEEE Journal of Biomedical and Health Informatics,",
            "year": 2021
        },
        {
            "authors": [
                "Lifeng Yang",
                "Jingbo Yang",
                "Xiaobo Zhou",
                "Liyu Huang",
                "Weiling Zhao",
                "Tao Wang",
                "Jian Zhuang",
                "Jie Tian"
            ],
            "title": "Development of a radiomics nomogram based on the 2d and 3d ct features to predict the survival of non-small cell lung cancer patients",
            "venue": "European Radiology,",
            "year": 2019
        },
        {
            "authors": [
                "Chun-Nam Yu",
                "Russell Greiner",
                "Hsiu-Chin Lin",
                "Vickie Baracos"
            ],
            "title": "Learning patientspecific cancer survival distributions as a sequence of dependent regressors",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Yading Yuan"
            ],
            "title": "Automatic head and neck tumor segmentation in pet/ct with scale attention network. In 3D Head and Neck Tumor Segmentation in PET/CT Challenge",
            "year": 2020
        },
        {
            "authors": [
                "Wenbin Yue",
                "Zidong Wang",
                "Hongwei Chen",
                "Annette Payne",
                "Xiaohui Liu"
            ],
            "title": "Machine learning with applications in breast cancer diagnosis and prognosis",
            "year": 2018
        },
        {
            "authors": [
                "Yundong Zhang",
                "Huiye Liu",
                "Qiang Hu"
            ],
            "title": "Transfuse: Fusing transformers and cnns for medical image segmentation",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengxin Zhang",
                "Qingjie Liu",
                "Yunhong Wang"
            ],
            "title": "Road extraction by deep residual u-net",
            "venue": "IEEE Geoscience and Remote Sensing Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Xiner Zhu",
                "Haoji Hu",
                "Hualiang Wang",
                "Jincao Yao",
                "Di Ou",
                "Dong Xu"
            ],
            "title": "Region aware transformer for automatic breast ultrasound tumor segmentation",
            "venue": "In Medical Imaging with Deep Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bla\u017e Zupan",
                "Janez Dem\u0161ar",
                "Michael W Kattan",
                "J Robert Beck",
                "Ivan Bratko"
            ],
            "title": "Machine learning for survival analysis: a case study on recurrence of prostate cancer",
            "venue": "Artificial intelligence in medicine,",
            "year": 2000
        }
    ],
    "sections": [
        {
            "heading": "Diagnosis and Prognosis of",
            "text": ""
        },
        {
            "heading": "Head and Neck Cancer Patients",
            "text": "using Artificial Intelligence\nby"
        },
        {
            "heading": "Ikboljon Sobirov",
            "text": "Thesis submitted to the Deanship of Graduate and Postdoctoral Studies\nIn partial fulfillment of the requirements For the Master of Science degree in\nComputer Vision\nDepartment of Computer Vision Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)\n\u00a9 Ikboljon Sobirov, Abu Dhabi, UAE, 2022\nar X\niv :2\n30 6.\n00 03\n4v 1\n[ ee\nss .I\nV ]\n3 1\nM ay\n2 02\n3"
        },
        {
            "heading": "Examining Committee Membership",
            "text": "The following served on the Examining Committee for this thesis. The decision of the Examining Committee is by majority vote.\nSupervisor(s): Mohammad Yaqub Professor, Department of Computer Vision, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)\nInternal Member: Fahad Khan Professor, Department of Computer Vision, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI)\nii"
        },
        {
            "heading": "Author\u2019s Declaration",
            "text": "I hereby declare that I am the sole author of this thesis. This is a true copy of the thesis, including any required final revisions, as accepted by my examiners.\nI understand that my thesis may be made electronically available to the public.\niii"
        },
        {
            "heading": "Abstract",
            "text": "Cancer is one of the most life-threatening diseases worldwide, and head and neck (H&N) cancer is a prevalent type with hundreds of thousands of new cases recorded each year. Clinicians use medical imaging modalities such as computed tomography and positron emission tomography to detect the presence of a tumor, and they combine that information with clinical data for patient prognosis. The process is mostly challenging and time-consuming. Machine learning and deep learning can automate these tasks to help clinicians with highly promising results. This work studies two approaches for H&N tumor segmentation: (i) exploration and comparison of vision transformer (ViT)-based and convolutional neural network-based models; and (ii) proposal of a novel 2D perspective to working with 3D data. Furthermore, this work proposes two new architectures for the prognosis task. An ensemble of several models predicts patient outcomes (which won the HECKTOR 2021 challenge prognosis task), and a ViT-based framework concurrently performs patient outcome prediction and tumor segmentation, which outperforms the ensemble model.\niv"
        },
        {
            "heading": "Acknowledgements",
            "text": "I would like to thank my supervisor, Dr Mohammad Yaqub for being an amazing guide, teacher and mentor, and for supporting me in pursuing the research topic of my interest. He was always extremely supportive and approachable both academically and personally, ensuring that my research work was indeed diligent and impactful. I was fully supported by the MBZUAI Scholarship program for my degree studies and research, and would like to express my gratitude for that opportunity. I was lucky enough to work with some of the most brilliant minds in AI, and my special thanks go to my esteemed friends and co-authors - Numan Saeed, Roba Al Majzoub, Otabek Nazarov, Hussain Alasmawi, and Hashmat Malik. This thesis work would not exist without the hard work, insightful ideas and continued support of these people. I also thank all the MBZUAI professors, especially Dr Fahad Khan and his team for the two excellent courses, CV701 and CV703 that immensely helped me in my research and helped me build solid foundations in the field.\nFinally, I would like to thank my parents, Sherzodbek and Ugilkhon, and the rest of my family, Islombek, Ilyosbek, Sadokatkhon, Samiyya, and Sa\u2019diya for always believing in me, loving me unconditionally and supporting me on this journey. None of this would be possible without them.\nv\n\u2018Actions are but by intentions, and each person will have but that which he intended.\u2019\nMuhammad S.A.W.\nvi"
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "List of Tables x",
            "text": ""
        },
        {
            "heading": "List of Figures xi",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": "1.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.3 Thesis Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2"
        },
        {
            "heading": "2 Literature Review 3",
            "text": "2.1 Clinical Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.1.1 Gravity of Cancer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.1.2 Head and Neck Cancer . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.1.3 Risk Factors of H&N Cancer . . . . . . . . . . . . . . . . . . . . . . 3\n2.1.4 Treatment of H&N Cancer . . . . . . . . . . . . . . . . . . . . . . . 4\n2.1.5 Diagnosis of H&N Cancer . . . . . . . . . . . . . . . . . . . . . . . 4\n2.1.6 Prognosis of H&N Cancer . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Automation of Prognosis and Diagnosis . . . . . . . . . . . . . . . . . . . . 5\n2.2.1 Literature Review on Prognosis . . . . . . . . . . . . . . . . . . . . 5\n2.2.2 Literature Review on Diagnosis . . . . . . . . . . . . . . . . . . . . 8\n2.3 Metrics of Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.4 Research Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
        },
        {
            "heading": "3 Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are? 12",
            "text": "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\nvii\n3.2 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.2.2 Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3.1 HECKTOR Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.3.2 Image Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.3.3 Data Augmentations . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.5 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.6 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.6.1 Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n3.6.2 Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"
        },
        {
            "heading": "4 Segmentation with Super Images: A New 2D Perspective on 3D Medical Image Analysis 22",
            "text": "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n4.2 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n4.2.1 Datasets and Preprocessing . . . . . . . . . . . . . . . . . . . . . . 23\n4.3 Super Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n4.4 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n4.5 Experiments and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n4.6 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.7 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n4.8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
        },
        {
            "heading": "5 Tumor Segmentation and Survival Prediction of Patients with Head and Neck Cancer 31",
            "text": ""
        },
        {
            "heading": "6 Thesis Conclusion 33",
            "text": "References 34\nAPPENDICES 34\nviii"
        },
        {
            "heading": "A Papers and Implementations 44",
            "text": "A.1 Chapter 3 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\nA.2 Chapter 4 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\nA.3 Chapter 5 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\nix"
        },
        {
            "heading": "List of Tables",
            "text": "2.1 Confusion matrix where l is the ground truth label and z is the model prediction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.1 The table shows results of using different data augmentations with our UNETR model. Model performance is presented when using different data augmentations. NA=No Augmentation, MR=Mirroring, RT=Rotation, ZM=Zooming, GC=Gamma Correction, ED=Elastic Deformation. . . . . . . . . . . . . . 17\n3.2 Dice, precision and recall of different models on the validation set are shown on the left. The dice, precision and recall of the UNETRmodel on the testing set are shown on the right. Mean and standard deviation are reported for the three models (left) which were trained and cross validated from scratch to provide a fair comparison. . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n4.1 The table shows the results of vanilla 3D U-Net (comparison target) to SIbased 2D U-Net on the HECKTOR training/validation dataset. The results are the mean and standard deviation of 5-fold cross validation. . . . . . . . 26\n4.2 The table shows the results of vanilla 3D U-Net (comparison target) to SIbased 2D U-Net on the atrial segmentation training/validation dataset. The results are the mean and standard deviation of 4-fold cross validation. PT stands for 2D U-Net pretrained on ImageNet1k, and A stands for augmentations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nx"
        },
        {
            "heading": "List of Figures",
            "text": "2.1 Anatomical location of head and neck cancers. They arise primarily from mucosal epithelia of the oral cavity, larynx and pharynx. (a) from [15], (b) from [82] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3.1 Overall architecture of UNETR. Features are extracted using a ViT encoder; CNN deconvolutional and convolutional blocks are used as a decoder; skip connections are used between the encoder and the decoder. . . . . . . . . . 14\n3.2 The figure shows segmentation examples where the models performed well. White represents the ground truth mask and red represents the model\u2019s prediction mask. All the models mostly produce these kinds of segmentation results. SE-based U-Net and UNETR segments the tumors very accurately, while nnU-Net over-segments by a small extent. . . . . . . . . . . . . . . . 19\n3.3 The figure shows segmentation examples where the models did not perform well. White represents the ground truth mask and red represents the model\u2019s prediction mask. The models fail to accurately segment the tumor on account of the unclarity in PET and CT scans and the small size of tumors. UNETR model can partially locate the tumor region in the samples, whereas the other two models fail to do that. SE-based U-Net occasionally shows better output than UNETR. . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n3.4 The figure depicts one sample with which the models struggled to segment; (a) shows a CT slice with artifacts, (b) shows an unclear PET slice, and (c) shows a small sized mask (in red) superimposed on the CT bone structure. Note that this is a single scan, containing all the three issues. . . . . . . . . 21\n4.1 The figure shows sample slices from the two datasets. (a) and (b) depict CT and PET scans with red region highlighting tumor from the HECKTOR dataset respectively; and (c) shows an MRI slice with the red region corresponding to atrium from the Atria segmentation dataset. . . . . . . . 24\n4.2 The figure shows the construction of super images from volumetric data. We rearrange the depth dimension by assembling the slices together to generate the super image. It is then fed to a 2D segmentation network. The model yields the prediction mask which is then rearranged back to the original shape. Note that the volumetric prediction mask shows a tumor region for visualization purposes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nxi\n4.3 The figure shows qualitative results on 3D U-Net (on volume) and 2D UNet (on SI) segmentation results on HECKTOR dataset sample respectively. White is the ground truth and red represents the prediction mask. Note that 3D U-Net results were cast to an SI form after its prediction for full-view comparison. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n5.1 Overall architecture of Deep Fusion V2. Features are extracted from fused CT and PET scans using the CNN network and concatenated with the EHR features. Then, the output is passed to the FC layers before MTLR. Lastly, risk scores from MTLR and CoxPH models are averaged to get the final risk predictions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n5.2 An illustration of the proposed TMSS architecture and the multimodal training strategy. TMSS linearly projects EHR and multimodal images into a feature vector and feeds it into a Transformer encoder. The CNN decoder is fed with the input images, skip connection outputs at different layers, and the final layer output to perform the segmentation, whereas the prognostic end utilizes the output of the last layer of the encoder to predict the risk score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nxii"
        },
        {
            "heading": "List of Abbreviations",
            "text": "AI Artificial Intelligence BN Batch Normalization CNN Convolutional Neural Network CT Computed Tomography\nCoxPH Cox Proportional Hazard DL Deep Learning DSC Dice Similarity Coefficient FC Fully Connected\nFDG-PET Fluorodeoxyglucose Positron Emission Tomography HECKTOR Head and Neck Tumor Segmentation and Outcome Prediction in\nPET/CT Images H&N Head and Neck\nINHANCE International Head and Neck Cancer Epidemiology MICCAI Medical Image Computing and Computer Assisted Intervention\nML Machine Learning MLP Multi Layer Perceptron MRI Magnetic Resonance Imaging MSA Multi-head Self Attention MTLR Multi-task Logistic Regression NLP Natural Language Processing\nN-MTLR Neural Multi-task Logistic Regression PET Positron Emission Tomography ReLU Rectified Linear Unit RT Radiation Therapy SA Self Attention SI Super Images SSL Self-Supervised Learning TNM Tumor-Node-Metastasis\nUNETR UNet TRansformer ViT Vision Transformer\nxiii\nChapter 1"
        },
        {
            "heading": "Introduction",
            "text": ""
        },
        {
            "heading": "1.1 Background",
            "text": "Cancer is one of the leading causes of mortality worldwide. Head and neck (H&N) cancer is the sixth most common type of cancer with 507,000 deaths in 2007 [6]. Clinically, radiologists perform diagnosis using medical imaging, such as computed tomography (CT) and positron emission tomography (PET) images, and if the patient has the cancer, they study electronic health records (EHR) of the patient on top of the imaging data to perform prognosis.\nThe automation of tumor segmentation and patient prognosis has been studied extensively using deep learning (DL). Many solutions using convolutional neural networks (CNNs), vision transformers (ViTs), and traditional machine learning (ML) models were proposed for these tasks in various approaches. U-Net [17] is the most commonly used architecture with numerous variations for the segmentation task. Recently, ViT networks have been more popular in segmentation of different organs and tissues. The prognosis task is generally addressed in two ways: (i) initially extracting radiomic features from imaging data and using machine learning models for prognosis, or (ii) using an ensemble of several models concurrently extracting and producing risk scores for prognosis."
        },
        {
            "heading": "1.2 Problem Statement",
            "text": "Although it is one of the commonly faced cancers, the study in the automation of H&N tumor segmentation and patient analysis using artificial intelligence (AI) models is rather limited. Head and neck tumor segmentation and outcome prediction in PET/CT images (HECKTOR) challenge in 2020 paved the way for a further analysis into the problem. The challenge has two tasks: segmentation of H&N tumor, and outcome prediction of patients with H&N cancer. Still, the problem is understudied considering that the challenge is new.\nTo the best of our knowledge, solutions for the segmentation of H&N tumor heavily rely on CNNs, and no work has been carried out to see the effects of using ViT-based models\n1\nfor this task. Additionally, CNN models that were proposed thus far primarily make use of 3D networks for segmentation, and no 2D approach has been investigated. This shows how similar the existing solutions are for the task. Prognosis of patients with H&N cancer in existing literature is generally performed (i) using only traditional ML models or (ii) using DL for feature extraction and then applying traditional ML models. No ensemble of models that can concurrently analyze imaging and EHR data and predict patient outcomes has been suggested. Also, ViTs, which have the ability to digest different modalities of data at the input, were not explored for prognosis."
        },
        {
            "heading": "1.3 Thesis Overview",
            "text": "This work studies H&N cancer by proposing diagnostic and prognostic solutions for the problem using traditional ML and DL. Chapter 2 provides more detailed background to the problem clinically and reviews the currently existing literature on the automation of the tasks. It also mentions the gap in the research community that this work is trying to fill in. Chapter 3 investigates the use of ViT-based model and its comparison to leading CNN networks for the segmentation task. Chapter 4 proposes a novel 2D perspective to working with volumetric medical data and argues that this approach can be a new field of study. Chapter 5 studies outcome prediction of patients with H&N cancer, proposing two different methods for the task. The first model is an ensemble of several DL and ML models together; and the second method is a ViT-based framework that takes the imaging and EHR data at input to perform outcome prediction and segmentation simultaneously. Finally, Chapter 6 concludes the thesis work and provides future directions of study for H&N cancer.\n2\nChapter 2"
        },
        {
            "heading": "Literature Review",
            "text": ""
        },
        {
            "heading": "2.1 Clinical Background",
            "text": ""
        },
        {
            "heading": "2.1.1 Gravity of Cancer",
            "text": "Cancer is projected to become the leading cause of death worldwide in the 21st century [12]. According to The Global Burden of Disease, 890,000 new cases of and 507,000 deaths from H&N cancer were globally reported in 2017 [6], and it accounts for the sixth most prevalent cancer today [51]. The number of patients diagnosed with early stage is estimated to account for 30-40%, and they are promised a five year survival rate of 70-90% when supplemented with treatment. Unfortunately, H&N cancer is predominantly diagnosed at later stages. Treatment is much harder with medicine and surgery at the late stages, and is highly likely to inflict serious damage to speech and swallowing functions [32]."
        },
        {
            "heading": "2.1.2 Head and Neck Cancer",
            "text": "Let us start by clarifying what H&N cancer refers to anatomically. Cancers developing in the H&N areas are collectively referred to as squamous cell carcinomas of the head and neck. They originate in the squamous cells that form mucosal epithelia of oral cavity, oropharynx, hypopharynx, nasopharynx, and larynx as can be seen in Figure 2.1[45]. Those that arise in sinuses, glands, nerves, or muscles in the same areas can also be framed within the same category but are much less ubiquitous."
        },
        {
            "heading": "2.1.3 Risk Factors of H&N Cancer",
            "text": "International Head and Neck Cancer Epidemiology (INHANCE) consortium was established in 2004 to conduct study on H&N cancer in particular, including its risk factors [6]. They listed tobacco and sub-products of tobacco, alcohol, human papillomavirus, occupational exposure, diet habits, oral hygiene, lifestyle, and socioeconomic level as the main contributors to the development of H&N cancer. Amongst this list, tobacco and alcohol\n3\nplay the most consequential role in leading to the cancer [6, 45]. In fact, heavy smokers are five to twenty-five times more prone to develop the cancer than non-smokers, and alcohol complements tobacco, increasing the risk [57]."
        },
        {
            "heading": "2.1.4 Treatment of H&N Cancer",
            "text": "Until the 1980s, the only effective methods for patient curative management were surgery and radiation therapy (RT). Chemotherapy\u2019s role in designing a sequential treatment plan for patients with H&N cancer in which induction chemotherapy is followed by RT was game-changing [57]. H&N cancer treatment is a demanding task, requiring the presence of a radiation oncologist, a medical oncologist and a H&N surgeon to collectively diagnose the patient with an accurate cancer stage and resectability of the tumor. The current approaches to treat patients highly depend on the tumor stage at T1, T2, T3, and T4 (for detailed differences between stages and nodes, refer to [15]). T1 and T2 stage patients, defined as early stage, do not yet develop nodes, and are thus easier to treat with surgery or RT. Concurrent chemoradiation (i.e. induction chemotherapy followed by RT) is applied as treatment to more advanced tumors [57]. As can be seen, the role of RT in treating patients with all H&N cancer types is substantial, with each patient receiving RT at least once during the course of treatment [78]."
        },
        {
            "heading": "2.1.5 Diagnosis of H&N Cancer",
            "text": "Once we understand how significant this disease is and how we can treat cancerous patients, let us step back and go over the diagnosis and prognosis procedures for H&N cancer patients. Once a patient is admitted to a hospital, a detailed history is collected and a physical examination is carried out by clinicians. Radiographic imaging is required to\n4\ndetect the presence and location of a tumor\u2019s region, and is proven to be much safer than biopsy especially when diagnostic analysis is incomplete [15].\nRadiographic imaging is used as one of the initial stages for cancer diagnosis since it is a non-invasive and inexpensive approach compared to biopsy sampling via surgery, for example. Primarily, MRI, CT and PET are the three prevalent types of modalities utilized for imaging head and neck areas [57]. Note here that all three modalities are designed to capture volumetric image of the targeted organ. The innate nature of tumor also requires volumetric imaging capacity. It is substantially beneficial for doctors to accurately diagnose and prognose a patient using the tumor volume computed by the 3D data [1].\nEndoscopy and biopsy are performed once an abnormality is detected via physical examination; these two can come before or after the radiographic imaging. It is preferred to capture radiography before endoscopy and biopsy since radiography is much faster, noninvasive, less dangerous and cheaper [57]. Imaging can be used after them to accurately localize the tumor and use the volumetric data for tumor staging. Imaging also helps with understanding the metastases that may be spreading over other neighboring organs [31].\nCT has the capability to provide structural formation of the organs of interest, whereas PET can easily highlight metabolically active tissues such as the tumor [31, 57]. This has led to the application of the two modalities in parallel to diagnose head and neck cancer. Clinicians can effortlessly pinpoint the locality and the extent of tumor using PET and understand its structural location using CT; and therefore, the conjunction of the two modalities are the most prevalent these days [31]. In this thesis work as well, PET and CT scans of patients diagnosed with H&N cancer are used in combination."
        },
        {
            "heading": "2.1.6 Prognosis of H&N Cancer",
            "text": "Once clinicians identify if a patient is suffering from H&N tumor, and if they are, how serious the disease is, the next most essential step would be to do prognosis. With accurate prognosis, doctors can plan the right course of treatment, may it be chemotherapy, RT, surgery, or the combination of them. The foremost prognostic assessment tool is the tumor-node-metastasis (TNM) staging method [74]. This scheme assesses the patient to be classified into rankings using the primary tumor (T), regional lymph nodes (N), and distant metastasis (M). Such schemes as TNM are stored in EHR for each patient to perform prognosis."
        },
        {
            "heading": "2.2 Automation of Prognosis and Diagnosis",
            "text": ""
        },
        {
            "heading": "2.2.1 Literature Review on Prognosis",
            "text": "Before the prevalence of DL, traditional ML and statistical models were mostly used for the automation of the prognosis task. Chapter 5 focuses on two distinct solutions for the prognosis of patients with H&N cancer, and as such this section summarizes the literature that performed the task using statistical, ML and more recent DL approaches.\n5"
        },
        {
            "heading": "Statistical Models",
            "text": "Statistical models were the foundation for automatic prognosis of patients with different types of diseases. One of the earliest, yet still commonly used, approaches is Cox proportional hazard (CoxPH) model proposed by Cox in 1972 [18]. This approach models a survival (or hazard) function that produces probability of a particular event occurring at a given time t. It is widely used in the medical research to predict the survival time of patients based on a set of given clinical features. The hazard function h(t, xi) assumes that time element \u03bb0(t) and feature element \u03b7( \u2212\u2192xi ) are proportional, and is defined as [18]:\nh(t,\u2212\u2192xi ) = h0(t)\u03b7(\u2212\u2192xi ) (2.1)\nwhere h0(t) is the baseline reliability function (generally not specified), and \u03b7( \u2212\u2192xi ) is the risk function. The risk function \u03b7(\u2212\u2192xi ) is often framed with a linear representation such that \u03b7(\u2212\u2192xi ) = exp( \u2211p j=1 x i j\u03c9j).\u03c9j are the coefficients to find [50, 10]. The equation is adapted from [26].\nAn alternative to CoxPH is the MTLR model that can essentially be described as a sequence of logistic regression models dedicated to a number of time intervals whose task is to calculate the probability of an event occurring in that specific time interval. Yu [90] developed this model in 2011 to tackle the issues introduced by CoxPH counterpart. Specifically, CoxPH brings three major issues: (i) the time component of the hazard function is unspecified, which makes the model unfitting to actual risk prediction, (ii) it assumes that the ratio of hazards for two given patients is constant over time, and (iii) the function is not computationally efficient when it concerns the formula responsible for ties, forcing the model to use approximations [90, 26]. The MTLR model can handle these issues. Its loss function is defined as:\nmin \u0398\nC\n2 m\u2211 j=1 \u2225\u2225\u2225\u03b8\u20d7j\u2225\u2225\u22252 \u2212 n\u2211 i=1 [ m\u2211 j=1 yj (si) ( \u03b8\u20d7j \u00b7 x\u20d7i + bj ) \u2212 log m\u2211 k=0 exp f\u0398 (x\u20d7i, k) ] (2.2)\nwhere the smoothness of the predicted survival curves depends on the change between consecutive time intervals and is controlled by the constant C (set as a hyperparameter) in the l2 regularization term.\nFotso [25] in 2018 integrated neural networks on top of MTRL (dubbing as N-MTLR) to introduce nonlinearity to the model. In other words, when there is nonlinear elements in data, the two previous models fail to produce satisfactory results since they both are based on a linear transformation. Neural nets introduced in the N-MTLR solves this issue, outperforming the MTLR model in numerous applications and CoxPH when nonlinearity is found in data [25]."
        },
        {
            "heading": "ML & DL Models",
            "text": "While statistical models make good use of data when making decisions, they do not directly craft useful features from them. ML, on the other hand, is able to automatically recognize\n6\npatterns in data, thus acquiring actionable and meaningful knowledge [29]. Computers\u2019 capability to learn from data to make well-informed decisions in intelligent heathcare systems started to gain more popularity with the machine learning tools and algorithms getting more diverse and prevalent. ML has advanced the automation of numerous tasks of human specialists, yielding real and tangible results. Therefore, an extensive amount of research work has been put into the automation of prognosis of patients using their clinical data with machine learning algorithms. The patient prognosis of breast [92, 27, 64], oral [13, 16, 40], bladder [77, 33], prostate [96, 37], and lung [55, 73] cancer are some of the examples for their acclaim in the research community.\nAlong with the clinical information of patients diagnosed with cancer, most studies attempt to integrate meaningful features extracted from the imaging data. Radiomics refers to the automatic extraction of quantitative mineable features from radiographic images, such as CT, MRI and PET, that can be helpful in survival prediction and other predictive tools [89, 28]. Such features can be characterised by tumor image intensity, shape, texture, multiscale wavelet and so forth, and a high number of them need to be extracted and filtered carefully to keep the meaningful and useful ones. Many papers on the prognosis of H&N cancer patients as well make use of radiomic features in conjunction with the clinical data [83].\nHoward et al. [38] compared three different ML survival algorithms to study whether intermediate-risk H&N cancer patients would benefit from adjuvant chemotherapy. They used 33,527 patient cases from National Cancer Database [11] (the largest clinical cancer registry) to construct DeepSurv [46], random survival forests [43] and N-MTLR [25] models for the patient survival prediction for treatment recommendations with C-index of 0.693, 0.695 and 0.691 respectively.\nParmar et al. [65] studied the prediction of overall survival of H&N cancer patients. They extracted 440 radiomic features from two cohorts of H&N cancer patient CT scans using 14 feature selection methods. They investigated 12 ML classifiers to compare the prognostic performance and stability of these models and the feature selection methods using the assessment with the area under receiver operator characteristic curve (AUC). Random forests (AUC=0.67), Bayesian (AUC=0.67) and nearest neighbors (AUC=0.62) were the best performing classifiers for the prognosis, and minimum redundancy maximum relevance (AUC=0.69), conditional infomax feature extraction (AUC=0.68) and mutual information feature selection (AUC=0.66) were found to yield the highest prognostic performance.\nKazmierski et al. [47] organized a challenge for the survival analysis of patients with H&N cancer. The primary goal of the challenge was to predict two-year overall survival, and predicting death risk of a patient and full survival curve was the secondary task. The data included EHR (demographic, clinical, and interventional) and pre-treatment contrastenhanced CT scans. Twelve submissions were received in the challenge that use EHR data only, imaging data only, or fuse them in various fashions. Participants who used the imaging data mostly relied on DL to extract features automatically. Four submissions used the hand-crafting approach to extract representations from the imaging data. Their observation over the submissions shows that the most models proposed do not learn valuable\n7\nrepresentations from radiomic features even if they are considered to be the common understanding, claiming that the EHR are the primary source of model predictions. The authors signify the importance of tumor volume as a simple yet powerful predictor for the prognosis. The most intriguing aspect of the challenge is that, even if there were many DL-based model submissions, the winning solution used Deep MTLR model using EHR data and tumor volume for the task."
        },
        {
            "heading": "2.2.2 Literature Review on Diagnosis",
            "text": ""
        },
        {
            "heading": "H&N cancer diagnosis",
            "text": "The automatic diagnosis of different cancer types is well-researched [61]. Breast [66, 49], oral [5], and brain [35] are some of the examples for such studies. However, research on H&N tumor segmentation using deep learning is limited. The study on the automatic diagnosis of H&N cancer has gained more popularity after the introduction of the HECKTOR challenge, with 18 teams participating in 2020 and 44 teams in 2021 [4]. Chapter 3 and 4 study the automatic segmentation of H&N tumor using different DL approaches, and as such this section looks over the recent solutions proposed for the diagnosis task using CNN approaches.\nIantsen et al. [41] used squeeze and excitation normalization (SE norm) layers on top of the traditional 3D U-Net [17] architecture with residual blocks. Their model achieved DSC of 0.759 in the 2020 HECKTOR challenge testing set, winning the competition. The SE norm is similar to instance normalization [79] but different in shift and scale values, which are treated as functions of input X during inference. SE norm is used in the encoder within the residual blocks and in the decoder after convolutional blocks. They used an ensemble of the same model for different splits to reach such a performance.\nHybrid active contour was integrated within the U-Net model in the work by Ma and Yang [56]. This traditional ML technique, combined with the powerful CNN model, saved them the second place in the challenge. Such an additional integration was performed by Yua [91] who implemented a dynamic scale attention network on top of U-Net for the segmentation task. They claim that this approach helps enhance the utilization of feature maps coming from the encoder to the decoder. The scale attention network combines different scale features using a scale attention block for each decoder layer that is connected to all the extracted features (except the last encoder layer). They prove their model performs better than the vanilla U-Net.\nIn [84], Xie and Peng used a similar architecture to [41], a U-Net model with SE norm, with a difference in the learning rate being controlled with polyLR. They split the data into five-folds for training and validation, and five test predictions were then ensembled for the final predictions. They won the 2021 challenge with DSC of 0.7785.\nAn et al. [3] implemented three subsequent U-Net models as a single framework, each model being responsible for a different task. The first U-Net is utilized to coarsely delineate the tumor region and select the bounding box. The second model is responsible for a finer segmentation output using the small bounding box. The final model then takes the\n8\nPET and CT concatenation and the preceding segmentation output for the final refined predictions. They achieved DSC of 0.7733, saving the second spot in the 2021 competition."
        },
        {
            "heading": "Transformers for diagnosis",
            "text": "All the solutions proposed in both the 2020 and 2021 challenges heavily relied on CNN architecture, namely U-Net with certain variations and ensembles. However, the recent years have witnessed a growing interest in transformer-based architectures [87, 81, 85, 93, 34]. Chapter 3 studies the use of transformers for the H&N tumor segmentation, and as such, this section briefly discusses the recent advances in ViT-driven architectures for volumetric segmentation.\nThe use of transformers in the segmentation task still requires a U-shaped architecture designs, and the ViT can be used in the encoder, decoder, or the both. For example, Wang et al. [81] effectively encoded local and global representations in depth as well as spatial dimensions to perform brain tumor segmentation. Spatial feature extraction was performed using a 3D CNN encoder. Following that, transformers were used for the global context, and finally a 3D CNN decoder to upsample back to the original dimensions. In [95], Zhu et al. integrated region awareness into transformers to segment breast tumor. They proved the model outperforms CNN-based counterparts using extensive experiments on ultrasound breast scans.\nUNETR model, proposed in [34], applies 12 layers of ViT in the encoder that generates features at different layers and links them back in the decoder as skip connections. The model is tested on brain tumor segmentation and abdominal multi-organ segmentation tasks, and the authors achieve comparable results to other leading models.\nTo the best of our knowledge, the proposed solutions for H&N cancer diagnosis all apply CNN-based U-Net variations, and no work has been carried out to see the performance of transformers for the task. The study on the use of transformers for the H&N cancer segmentation and its performance comparison to other leading CNN models is given in Chapter 3."
        },
        {
            "heading": "2.3 Metrics of Performance",
            "text": "In this section, the metrics of performance reported in this thesis work are provided and explained. For the prognosis task, C-index is used as the measure of risk scores, and for the segmentation task, dice similarity coefficient (DSC), precision and recall are calculated to measure the robustness of the proposed models."
        },
        {
            "heading": "C-index",
            "text": "The concordance index is a generalization of the area under the receiver operating characteristic curve that takes censored data into consideration and is defined as:\n9\nC \u2212 index = \u2211\ni,j 1Ti>Tj \u00b7 1\u03b7i>\u03b7j \u00b7 \u03b4j\u2211 i,j 1Ti>Tj \u00b7 \u03b4j\n(2.3)\nwhere:\n\u2022 \u03b7i is the risk score of a unit i,\n\u2022 1Ti>Tj =\n{ 1 if Ti > Tj\n0 otherwise , and 1\u03b7i>\u03b7j =\n{ 1 if \u03b7i > \u03b7j\n0 otherwise\nC-index of 1.0 represents that the model achieved its most optimal performance, and C-index of 0.5 means that the model is randomly predicting. The equation is adopted from [25]. C-index is used in all prognostic models\u2019 evaluations."
        },
        {
            "heading": "Dice Similarity Coefficient",
            "text": "Dice similarity coefficient (DSC) measures the spatial overlap of the two volumes or areas within a range of 0 to 1, where 0 represents no overlap between the two sets of binary segmentation results and 1 represents a full overlap. It was first proposed by Dice [20].\nDice(A,B) = 2|A \u00b7B| |A|+ |B|\n(2.4)\nwhere A and B are the two binary vectors; one represent the ground truth values and the other represent the prediction values. In lucid words, the score is calculated as two times the overlap divided by the sum of the two volumes/areas. DSC is used in all segmentation models\u2019 evaluations."
        },
        {
            "heading": "Recall and Precision",
            "text": "To understand recall and precision, a confusion table is generally used. To illustrate it, let us take the following scenario: there is a binary label l with which each object in the dataset is linked, and it represents the ground truth; there is a prediction z which represents the model output for the given input, as shown in Table 2.1. TP , FN , FP , and TN indicate true positive, false negative, false positive, and true negative values; + and - indicate the positive and negative values respectively. Equations are adopted from [30].\nUsing this confusion matrix, we now can calculate recall and precision measures as follows:\nRecall = TP\nTP + FN (2.5)\nPrecision = TP\nTP + FP (2.6)\n10\nIntuitively, recall can be understood as the measure of how many instances were correctly predicted of all the actual positives, whereas precision is the measure of how many instances were correctly predicted of the all predicted values. Precision and recall are used in segmentation models\u2019 evaluations."
        },
        {
            "heading": "2.4 Research Gap",
            "text": "Now that we have some background on the criticality of H&N cancer and how much work has been carried out for the automation of the tumor segmentation and patient prognosis, this section summarizes the research gap that the thesis work addresses.\nH&N tumor segmentation studies primarily utilize U-Net and variations of it based on CNNs. All the proposed solutions for the HECKTOR challenge in 2020 and 2021 heavily rely on CNNs for their work [4]. However, the introduction of ViT models were gamechanging with its many applications in medical imaging tasks regarding other problems. Chapter 3 studies the use of transformers for this problem and makes a comparison of its results to other leading CNNs.\nChapter 4 introduces a novel approach to working with volumetric data in a 2D fashion by casting the data to super images. This work contributes to the study of 3D versus 2D networks for 3D medical images, which is a heated discussion. It argues that generating super images and using 2D networks, which are much more lightweight, can perform similar to 3D networks.\nFinally, Chapter 5 fills in the gap of prognosis of patients diagnosed with H&N cancer by incorporating clinical data with imaging data in two different ways. The first part ensembles the three different models to make risk predictions. Being similar to previous works, this approach not only outperforms other models, but also introduces a new way to integrate EHR and volumetric imaging data. The second part introduces a novel method for blending EHR and imaging data at the input level with the use of transformers.\n11\nChapter 3"
        },
        {
            "heading": "Automatic Segmentation of Head",
            "text": "and Neck Tumor: How Powerful"
        },
        {
            "heading": "Transformers Are?",
            "text": ""
        },
        {
            "heading": "3.1 Introduction",
            "text": "Globally, cancer is considered to be one of the leading causes of death, and H&N cancer is one of the most commonly encountered types. Clinically, PET and CT are used in conjunction to detect, segment, quantify and stage the tumor region, but it is highly timeconsuming, extensively labor-demanding and prone to error. ML and DL models have been proposed to automatically segment the tumor region that yield comparable results to the result of a clinician. However, most of them rely on using CNN networks for the segmentation of H&N tumor. This chapter investigates a vision transformer-driven model to delineate H&N tumor and compare its results to dominant CNN-based counterparts. To do that, we use the imaging data of CT and PET from the HECKTOR challenge [63, 4]. The transformer-based network shows the potential to yield results comparable to CNN models. It achieves a mean DSC of 0.736, mean precision of 0.766 and mean recall of 0.766 cross validated in-house; compared to that, the 2020 challenge winning model in the same cross validation achieves merely 0.021 more. On the challenge testing set, the transformerbased model achieves 0.736 DSC, 0.773 precision and 0.760 recall, being only 0.023 lower in DSC than the 2020 challenge winning model. We show that the use of transformers in the cancer segmentation task is a promising research area.\nWith recent strides in DL, automatic segmentation of tumor found in various body parts is being explored with great interest. The main driver to using DL models as an alternative or auxiliary to radiologists in the task is that the former can perform as well as the latter in most cases, with an additional benefit of speeding up processing time.\nAlthough the prevalence of H&N cancer is high worldwide, its study on automation using DL is insufficient in literature. The HECKTOR challenge [63, 4] has boosted the study of the diagnosis and prognosis of H&N cancer using DL. At a glance, most of the\n12\nproposed solutions in the segmentation task of the competition relied on U-Net [67] and its variations with additional techniques and features such as 3D convolution blocks [17], squeeze and excitation blocks [39], residual blocks [36, 94], multi-scale patches [53], so forth. For example, Iantsen et al. [41] used squeeze and excitation normalization on residual 3D U-Net; Ma and Yang [56] integrated 3D U-Net with hybrid active contour to perform the task; and Yuan [91] used a dynamic scale attention network within 3D U-Net for the segmentation task.\nTransformers, in tandem with CNNs, are gaining more popularity in the community, with their use in the segmentation task being no exception. Numerous variations of ViT [21]-based models were proposed for the tumor segmentation [87, 81, 85, 14, 93, 34]. However, to the best of our knowledge, no work has been carried out in analyzing transformer-based models in H&N cancer diagnosis. Transformer architectures are claimed to be superior to CNNs in capturing long-range dependencies [48], and understanding the context information is assumed to benefit the network to accurately segment tumor regions. Furthermore, transformers are relatively new and understudied compared to CNNs, thus exploring their use in various applications would further benefit other tasks. Contributions of this work are as follows:\n\u2022 Investigating the comparison of a transformer-based model and leading CNN based counterparts;\n\u2022 Showing that the transformer model can perform as well as the CNN models;\n\u2022 Showing that data augmentations are essential to the transformer model\u2019s performance;\n\u2022 Studying a multimodal setting of CT and PET in the case of transformers;\n\u2022 Testing the validity of a newly proposed network in a new medical task."
        },
        {
            "heading": "3.2 Methodology",
            "text": ""
        },
        {
            "heading": "3.2.1 Architecture",
            "text": "Since their success in NLP, transformers have been welcomed into the computer vision tasks as well, with ViT [21] laying the groundwork. Inspired by Hatamizadeh et al. [34], we developed a transformer-based model for H&N tumor segmentation and compared its results to best performing CNN models. The network is depicted in Figure 3.1. The overall architecture composes of an encoder, a decoder and skip connections between the two, mimicking the standard U-Net design.\nA 3D volumetric input has the size of x \u2208 RH\u00d7W\u00d7D\u00d7C , where H, W, D denotes the height, width and depth respectively, and C is the number of channels. It gets embedded into a 1D sequence of flattened uniform non-overlapping patches with a new size of xv \u2208\n13\nRN\u00d7(P 3.C), where N = (H \u00d7 W \u00d7 D)/P 3 is the length of the sequence, and P 3 is the resolution of each patch.\nFollowing the patch embeddings, a linear layer is used to project them into K dimensional latent space (remains constant). An additional 1D learnable positional encoding Epos \u2208 RN\u00d7K is attached to the projected patch embeddings E \u2208 R(P\n3.C)\u00d7K to keep the spatial information as:\nz0 = [x 1 vE;x 2 vE; . . . ;x N v E] + Epos (3.1)\nTwelve layers of transformer blocks, with multi-head attention (MSA) and multi-layer perceptron (MLP), follow the embedding layer. MSA and MLP follows as such:\nz\u2032i = MSA(Norm(zi\u22121)) + zi\u22121, i = 1 . . . L, (3.2)\nzi = MLP (Norm(z \u2032 i)) + z \u2032 i, i = 1 . . . L, (3.3)\nwhere Norm() is a layer normalization as in [8], i is the identifier in the intermediate block, L denotes the number of transformer blocks, and MLP consists of two linear layers.\nAn n number of self-attention (SA) heads, existent in the MSA component, learn mapping between a q query and the respective k key and v value in a given sequence of z \u2208 RN\u00d7K . The SA for the given sequence z is calculated as the following:\nSA(z) = (\u03c3( qkT\u221a Kh ))v, (3.4)\n14\nwhere \u03c3() is the softmax function, Kh = K/n is the scaling factor, and v is the values in the input sequence. Respectively, MSA for the given sequence z is calculated as:\nMSA(z) = [SA1(z);SA2(z); . . . ;SAn(z)]Wmsa, (3.5)\nwhere Wmsa \u2208 Rn.Kh\u00d7K denotes the trainable parameters in the multi-head attention network.\nThe skip connection is applied with this network, outputting sequence representations at different layers (zi, i \u2208 3, 6, 9, 12) of the transformer. Their size of H\u00d7W\u00d7DP 3 \u00d7K is reshaped to H\nP \u00d7 W P \u00d7 D P tensors. Moreover, a set of 3\u00d7 3\u00d7 3 convolutional and normalization layers is applied onto the embedding space tensors to bring them back to the input space for each skip connection.\nA deconvolutional layer is used on the output of the last layer of the transformers to increase its shape by a factor of 2. The resized output is concatenated with the output of the preceding transformer layer (e.g. z9) before going into a set of 3\u00d7 3\u00d7 3 convolutional layers. This output is then upsampled once again using a deconvolutional layer. All the way up to the top of the network, the process repeats to bring the feature map to the original input size. A 1 \u00d7 1 \u00d7 1 convolutional layer with a softmax activation is then applied on the final representation to output a voxel-wise predictions for segmentation."
        },
        {
            "heading": "3.2.2 Loss Function",
            "text": "The network is supported by a sum of a dice loss as given in Equation 3.6 and a focal loss as given in Equation 3.7.\nLDice = 2 \u2211N\ni p\u0302iyi\u2211N i p\u0302i 2 + \u2211N i y 2 i , (3.6)\nLFocal = \u2212 N\u2211 i \u03b1yi(1\u2212 p\u0302i)\u03b3log(p\u0302i)\u2212 (1\u2212 yi)p\u0302i\u03b3log(1\u2212 p\u0302i), (3.7)\nLFinal = LDice + LFocal (3.8)\nwhere N is the sample size, p\u0302 is the model prediction, y is the ground truth, \u03b1 is the weightage for the trade-off between precision and recall in the focal loss (empirically set to 1), and \u03b3 is focusing parameter (empirically set to 2)."
        },
        {
            "heading": "3.3 Dataset",
            "text": ""
        },
        {
            "heading": "3.3.1 HECKTOR Dataset",
            "text": "The HECKTOR challenge [63, 4] provides the dataset of CT, FDG-PET, segmentation masks, tumor bounding box information in a csv file and EHR data. We disregard the\n15\nEHR data since our task is segmentation using only the imaging data. A total of 325 patient cases (224 training, 101 testing) are provided in the dataset, with the testing set ground truth being inaccessible for competition purposes. The data is multicentric (six centers) and multimodal (CT and PET)."
        },
        {
            "heading": "3.3.2 Image Preprocessing",
            "text": "A few preprocessing techniques were used to clean and manage the data. The first was to crop the randomly and massively sized images of CT and PET into 144\u00d7 144\u00d7 144mm3 using the provided bounding box information. Given that this information is provided by the challenge committee, the cropping is accurate, the mapping between modalities is consistent, and the full tumor is within this region. Furthermore, normalization was applied on both CT and PET scans. CT images were clipped within the range of (-1024, 1024) and then normalized to (-1, 1). PET images were normalized using Z-score normalization. Finally, resampling to an isotropic voxel spacing of 1.0mm was used for all scans."
        },
        {
            "heading": "3.3.3 Data Augmentations",
            "text": "Several data augmentations were applied on the imaging data to let the model get exposed to variations of the given data. Random rotation in the range of (-45, 45), zooming, elastic deformation, mirroring and gamma correction in the range of (0.5, 2) were experimented with in different combinations. Note that gamma correction is applied only on the PET scans as they are dark in non-tumor regions, and investigating different intensities is supposed to help improve the model. Zooming with a factor of 1.25, reducing the current size down to 115\u00d7115\u00d7155mm3, is assumed to accentuate the tumor regions where the tumor is small in size."
        },
        {
            "heading": "3.4 Experimental Setup",
            "text": "For our experiments, we used two NVIDIA RTX A6000 (48GB) GPUs, and the implementation was carried out using PyTorch library. The network has a VIT-B16 [21] as the encoder with L = 12 layers, a patch resolution of 16\u00d716\u00d716, andK = 768 embedding size. Two input channels corresponding to CT and PET, and one output channel corresponding to the segmentation mask were used in the network.\nAll the experiments with UNETR were trained with a batch size of 8 for 800 epochs. The reason why the model was trained for long was to explore how it would act over long epochs. An AdamW optimizer with a base learning rate of 1e-3 and weight decay of 1e-5, and a cosine annealing schedule, which starts with the base learning rate and reduces it to 1e-5 every 25 epochs, were using in the experiments.\n16"
        },
        {
            "heading": "3.5 Ablation Studies",
            "text": "First set of experiments were to explore different combinations of data augmentations that help the model achieve the highest DSC score. Table 3.1 lists all the combinations that were applied on the data using the model.\nAnother set of experiments that we did were to implement two leading CNN-based models as in their original work. Squeeze and excitation normalization based residual U-Net proposed by Iantsen et al. [41] that won the HECKTOR competition in 2020 was selected as the first CNN model, dubbed as SE-based U-Net. An automatically configured U-Net architecture design by Isensee et al. [42] that won the BRATS 2020 [59] was selected as the second model, dubbed as nnU-Net. Both the models were trained and validated as per the corresponding papers using the HECKTOR 2021 dataset."
        },
        {
            "heading": "3.6 Results",
            "text": ""
        },
        {
            "heading": "3.6.1 Quantitative Results",
            "text": "Augmentation Results. The model was trained with several combinations of data augmentations and with no augmentations to verify the essence of such techniques to the transformer model. For this set of experiments, we used the training data, splitting it into 169 cases for training and 55 cases for validation. Table 3.1 shows the list of combinations and their corresponding results. With no augmentation, the model achieves DSC of 0.741, precision of 0.726, and recall of 0.805. The results improved with all the augmentation combinations in all metrics, proving the essence of augmentations to the model. The set of mirroring, rotation, gamma correction on PET, and elastic deformation yields the highest performance with DSC of 0.794, precision of 0.761, and recall of 0.861.\nCross Validation Results. Once we identified the right set of augmentations (rotation, mirroring, gamma correction, and elastic deformation), we ran a cross validation using leave-one-center-out approach, in which one of the five centers in the training set was used as validation iteratively over all the five centers. The results are summarized in Table 3.2. It achieved DSC of 0.736 (\u00b10.043), precision of 0.766(\u00b10.022), and recall of 0.766(\u00b10.058).\n17\nComparison of Models. In comparison to UNETR, we used two CNN based models: SE-based U-Net and nnU-Net. All the three models were trained and validated from scratch using the competition training data in a leave-one-center-out fashion. The results are listed in Table 3.2. UNETR achieved DSC of 0.736 (\u00b10.043) that is only 0.021 and 0.012 lower that the leading CNN models respectively. That verifies that the transformer model is capable of learning and competing closely when trained from scratch.\nTesting Results. We are thankful to the organizers of the HECKTOR team for kindly checking the UNETR predictions on the withheld testing set for further verification on the model\u2019s capacity. The metrics achieved on the testing set are highly similar to the cross validation results, with DSC of 0.736, precision of 0.773, and recall of 0.760 (compared to 0.736, 0.766, and 0.766 in the validation set respectively)."
        },
        {
            "heading": "3.6.2 Qualitative Results",
            "text": "Further comparison and study of the models were conducted via qualitative analysis. The prediction masks from each model were compared against each other and the ground truth to understand how and where the models\u2019 outputs differ from each other. Figure 3.2 illustrates CT, PET, ground truth in white and prediction masks from the three models in red. Such a performance of high quality is true in most data samples for all the three models, where the models performed fairly equally well, with only minor differences. Figure 3.3 depicts cases where the models performed poorly, missegmenting the tumor regions. It is worthy of note that all the three models heavily depend on both the CT and PET scans. CT is responsible for the structural body information that the models need to distinguish different organs, whereas PET provides clarity in intensities of different structures, such as highlighting tumor regions as they are densely structured, as is visible in Figure 3.2, 3.3.\nFurther study on the poorly segmented cases was conducted to understand the foundational reasons. Figure 3.3 shows the cases where the models failed to segment accurately. Such issues occur with only a certain group of samples. To understand the struggle of the models, the cases with the lowest DSC scores were extracted and examined. Three major reasons were found as the cause of the failure, as shown in Figure 3.4. First, as depicted in Figure 3.4a, most of the poorly segmented scans suffer from streak artifacts in CT scans\n18\nintroducing irregularities in data, thereby causing the model confusion. Such artifacts are assumed to be caused by dental implants as they occur in the tooth area. The second reason is that the intensity values for different structures in PET scans in these cases are not well-outlines, especially in the tumor regions. This causes the models to produce faulty output, segmenting other regions with higher intensities. Such a sample can be seen in Figure 3.4b. Lastly, the tumor sizes in such scans are relatively small, especially when compared to well-performing cases, as depicted in Figure 3.3, 3.4c. The models struggle to figure out where to focus on when they encounter the detection of tiny regions with a largely abundant data in the background."
        },
        {
            "heading": "3.7 Discussion",
            "text": "To further discuss on the results of the data augmentations applied for the UNETR model, we can say that all of them were fruitful for the model\u2019s improvement. The combination with the zooming with a factor of 1.25 did not show much potential. It is hypothesized that zooming would help preserve information and highlight the tumor regions, especially the ones with small sizes, however, it did not show much effectiveness with the transformer backbone.\nThe comparison of the models shows that the CNN-based models outperform the ViTbased counterpart by a slight extent. This, however, should be an indicator that, even with the limited data, the transformer model can perform well, achieving promising results. Its consistency on the withheld testing set, while verifying the cross-validation, manifests that the model is indeed learning properly (i.e. no overfitting, no data leakage, etc.) even when\n19\ntrained from scratch.\nQualitative analysis demonstrates that the three models are side-by-side in terms of the performance, with slight differences in over- or under-segmentation. It also shows that there are certain reasons as to why the models struggled to perform the segmentation, including the artifacts in CT, obscureness in PET, and small tumor sizes."
        },
        {
            "heading": "3.8 Conclusion",
            "text": "H&N cancer is one of the most common types of cancer, and automatic segmentation of H&N tumor is an essential task that should be investigated to a greater extent. This work focused on the transformer-based model for the task for the first time, and compared its performance against two powerful CNN models. We showed that the model performs comparable to the CNNs, achieving a mean DSC of 0.736 (\u00b10.043), a precision of 0.766(\u00b10.022), and a recall of 0.766(\u00b10.058) when trained from scratch. On the HECKTOR testing set, the model yielded similar results, with DSC of 0.736, precision of 0.773, and recall of 0.760.\n20\nReported results of the transformer network are slightly lower than the well-mature CNN counterparts (0.021 and 0.012 for SE-based U-Net and nnU-Net respectively). Although this is the case, we believe that CNNs have undergone a number of improvements in structure, design, subcomponents, etc., whereas the transformers are yet to be studied in more detail. This chapter is one of the early works to compare the two designs, different in nature, for the segmentation task. Furthermore, pretraining via self-supervised learning has been noted numerous times as the key reason for the success of transformers in the NLP domain, and therefore, we believe that the self-supervised approach to the task should be studied, particularly in the case of transformer-based models with limited data for image segmentation.\n21\nChapter 4"
        },
        {
            "heading": "Segmentation with Super Images: A",
            "text": "New 2D Perspective on 3D Medical"
        },
        {
            "heading": "Image Analysis",
            "text": ""
        },
        {
            "heading": "4.1 Introduction",
            "text": "The latest advances in DL have shown their efficacy in numerous computer vision tasks such as classification [52], detection [44, 7] and segmentation [58, 22, 62]. Medical imaging analysis was no exception in availing itself with the benefits of the CNNs. The availability of medical datasets helped the field progress even faster. Automation of many medical tasks has been attracting researchers to explore the field to deeper extent.\nNumerous DL solutions have been proposed for different tasks within the medical sphere. In the segmentation task, U-Net [67] is considered the most widely utilized model, which was later augmented with many additional features [17, 24, 41, 94]. ViT [21] introduced a completely new way to look into such problems, with its many applications [14, 93, 80] for segmentation.\nThe nature of medical images is different from natural counterparts, being volumetric in many modalities such as CT, PET and MRI. This factor contributed to a still-ongoing discussion over the design of architectures for medical images. Should we enforce using 2D designs for three dimensional data? Or, should we leave this idea, and move to more complex 3D designs? Or, is there in-between consensus?\nThose who support designing 3D networks for volumetric data claim that the depth information is crucial for model learning and can be captured only by 3D networks [2, 17, 60]. 2D counterparts lack this attribute, and therefore, cannot perform as well as the 3D models. Another argument they put forth is that the nature of 3D data is similar to real life, which is why 3D models achieve better performance [2, 60]. However, the drawback to using such networks is that 3D convolutional, max pooling and up-convolutional operations are performed instead of 2D which are by design more complex, requiring more computational power and training/inference time [9, 23].\n22\nThose who support designing 2D networks argue that such models are much more costand time-effective as well as offers more options to apply transfer learning [23, 88, 68]. Pretraining models on large-scale datasets like ImageNet [19] and then fine-tuning it with the dataset at hand can be considerably beneficial to model learning, especially when the medical dataset is small in quantity. Another benefit to supporting 2D networks is that volumetric data can be converted to 2D slices, generating a larger quantity of data. Finally, there are more 2D architectural options in choosing an encoder for U-Net-like models, which makes it easier to customize and adjust new models to the need of the problem at hand [9].\nCertainly, there is a group who claim that the hybrid of 2D and 3D in various fashions can bring out the best of the two approaches [54, 68, 71]. However, in reality it proves not to be the case since hybrid approaches are fundamentally failing to capture innate benefits of 2D and 3D architectures [88].\nThis chapter introduces a new perspective to dealing with three dimensional data in a 2D fashion. We generate two dimensional super images (SIs) from a 3D input by stacking the depth (i.e. slices) side by side, and we train a 2D network for the segmentation task. A similar work was proposed in [23] on natural videos; however unlike them, this work newly introduces the idea to the medical field and volumetric data in particular. This novel approach can produce comparable results to 3D networks and we believe it lays the foundation for a new look into using volumetric medical data. The main contributions of the chapter are:\n\u2022 We introduce a new perspective to using biomedical three dimensional data by casting them into super images and training 2D networks with them;\n\u2022 We empirically show that pretraining and preprocessing techniques on top of using super images can easily improve the model performance;\n\u2022 We validate the proposed method on different datasets of CT, PET, and MRI to show the viability of the approach."
        },
        {
            "heading": "4.2 Methodology",
            "text": ""
        },
        {
            "heading": "4.2.1 Datasets and Preprocessing",
            "text": "To validate the new method, we applied it on two different datasets: HECKTOR 2021 challenge [4] and atrial segmentation challenge [86] datasets. The datasets are different in modality, organ-at-risk, and dimensions; therefore, they are preprocessed accordingly."
        },
        {
            "heading": "Head and Neck Tumor",
            "text": "HECKTOR dataset1 comprises 224 training CT and PET scans of patients diagnosed with head and neck tumor. Bounding box information is also available for tumor localization,\n1aicrowd.com/challenges/miccai-2021-hecktor\n23\nwhich was used to crop the scans and masks down to the size of 144\u00d7 144\u00d7 144mm3 with consistency between the images. Sample CT and PET slices are depicted in Figure 4.1 (a) and (b) respectively. Here, red lining correspond to the tumor region. The images were further cropped down to the size of 80\u00d7 80\u00d7 48 for a faster performance as ablation studies. This clearly highlights the tumor region and helps the model learn more effeciently. Additionally, both the CT and PET scans were resampled to have an isotropic voxel spacing of 1mm3 and their intensity values were normalized. The CT scans were clipped within (\u22121024, 1024) and normalized to (\u22121, 1); and the PET scans were normalized using Z -score normalization."
        },
        {
            "heading": "Atrium",
            "text": "Atrial segmentation challenge dataset2 comprises 100 gadolinium contrast (GE) MRIs in the training set. Figure 4.1 (c) shows a sample slice of a scan with red lining corresponding to the atrial region. The dimensions of the scans are differing from patient to patient, and they were all resized to the same size of 512\u00d7 512\u00d7 82mm3. Intensity normalization was also applied to the images. No further data augmentation was applied on the dataset unless reported otherwise.\nTo show the applicability and generalizability of the proposed idea, we intentionally chose two datasets with different modalities, with CT, PET and MRI. On top of that, the tissues that are to be segmented are different; HECKTOR dataset is tumor segmentation and the second dataset is atrial segmentation. This shows the idea is not limited to a specific tissue type.\n2atriaseg2018.cardiacatlas.org/data\n24"
        },
        {
            "heading": "4.3 Super Image Generation",
            "text": "A 3D network is claimed to capture and learn from features extracted from the depth information since they use three dimensional kernels; however, we assume that these features are still detectable and learnable in two dimensional SIs by well-designed deep neural networks. To that end, we generate SIs from volumetric data by slicing and stacking these slices side by side in order, as depicted in Figure 4.2.\nGiven a 3D image x \u2208 RH\u00d7W\u00d7D\u00d7C , where H is the height, W is the width, D is the depth, and C is the number of channels, the depth dimension is rearranged. The resulting image s \u2208 RH\u0302\u00d7W\u0302\u00d7C is now 2D, where H\u0302 = H\u00d7sh, and W\u0302 = W \u00d7sw; sh and sw represent the degree by which the height and width should be rearranged respectively to generate a grid size of sh \u00d7 sw. As a demonstration, the size of 80 \u00d7 80 \u00d7 48 \u00d7 2 (2 for both CT and PET slices), having 48 as the depth, can be considered with sh of 6 and sw of 8, thus generating the SI in the dimensions of 480 \u00d7 640 \u00d7 2, as shown in Figure 4.2. 2D U-Net (or any other 2D segmentation model for that matter) can then be applied on these SIs to perform the segmentation. The model output in the SI form is finally cast back to the\n25\noriginal size of the input scan."
        },
        {
            "heading": "4.4 Implementation Details",
            "text": "Two NVIDIA RTX A6000 GPUs were used to perform the experiments, and PyTorch library was used for implementation. We ran all the experiments for 100 epochs. We used AdamW optimizer with a base learning rate of 1e-3 and weight decay of 1e-5, and cosine annealing scheduler that starts with the base learning rate, lowering it to 1e-5 in 25 epochs before setting it back to the base learning rate. The batch size was set to 4, 8, or 16 depending on the dataset and architecture."
        },
        {
            "heading": "4.5 Experiments and Results",
            "text": "We applied the idea on two datasets. The ground truth for the testing set of both datasets were unavailable for competition purposes; therefore, k -fold cross validation on the training set was used for all experiments.\nThe HECKTOR dataset was explored in two settings: (a) with the initial size of 144\u00d7 144\u00d7 144mm3, and (b) the cropped size of 80\u00d7 80\u00d7 48mm3. In both settings, 5-fold cross validation was used, and the mean values along with standard deviations are reported in Table 4.1.\nIn the first setting, we trained and compared two models from scratch: 3D U-Net with volumetric data and 2D U-Net with SIs. DSC of 0.718, precision of 0.749, and recall of 0.747 were achieved with 3D U-Net. The 2D model using SIs achieved marginally lower results than its 3D counterpart, with DSC of 0.700, precision of 0.731 and recall of 0.731.\nIn the second setting, the 3D model was trained as a target for the SI-based 2D model. The 3D network reached the mean DSC of 0.779, precision of 0.787, and recall of 0.822.\n26\nFor the 2D network, differently sized SIs were used; i.e. different values were used for sh and sw as listed in Table 4.1. We generated SIs with sh and sw set to 8\u00d7 6, 6\u00d7 8, 12\u00d7 4, 4 \u00d7 12, 24 \u00d7 2, and 2 \u00d7 24 grid sizes before feeding into 2D U-Net. Note that the grid sizes are the multiples of the depth dimension since this dimension is rearranged to create SIs. Table 4.1 shows that the SI-based network with the most square-like shape, i.e. grid sizes of 8\u00d7 6 and 6\u00d7 8, produced the highest metric values, with DSC of 0.778 and 0.777 respectively. This is highly comparable to the 3D model. The performance for SIs with more disproportionate ratios was slightly worse, with DSC range of 0.744 to 0.770. This set of experiments prove that the generation of SIs with similar sh and sw is more favorable for better performance.\nThe atrial segmentation dataset contains only 100 MR images; therefore, 4-fold cross validation is used, leaving more data to the validation set for more strict training. We understood with the HECKTOR dataset that using similar sh and sw is better for performance. With the atrial dataset, we first perform the vanilla U-Net comparison of 2D and 3D, and secondly, we apply simple preprocessing techniques to see how far the model can reach. The first setting simply shows the generalizability of the idea in a new tissue type, whereas the second setting shows how image preprocessing techniques can easily be employed and can boost the vanilla network performance.\nVanilla U-Net in 2D and 3D comparison was carried out using the MR scans with the dimensions of 512 \u00d7 512 \u00d7 88. Table 4.2 shows that 3D U-Net yielded DSC of 0.893, precision of 0.898, and recall of 0.894, whereas the 2D network performed much more poorly (DSC of 0.812) with the grid layout of 11\u00d7 8. This is, as hypothesized, due to the disproportionate layout of the SIs. As such, we dropped a few slices from either end of the depth (i.e. cropped off the same number of slices from both sides of a scan). Specifically, the images were clipped to have 64 slices. This helps with the volume reduction for faster training/inference time, and more importantly, with the square-like formation of the SIs. With that, the model\u2019s performance jumped to 0.851 in DSC.\nFurther techniques included applying pretraining weights and augmentations to see how far we can stretch the performance. The 2D model was initialized with ImageNet1k pretrained weights (shown as PT in Table 4.2) instead of randomly initializing it. Even in the medical domain, weights pretrained on natural images showed a significant improvement with DSC reaching 0.895, overshadowing the 3D model. Lastly, a set of augmentations\n27\nwere applied on top of the pretraining technique. Random flip, random affine, random elastic deformation, random anisotropy, and random gamma were all used as a heavy set of augmentations. Note that these augmentations are only specific for this experiment. These aggressive augmentations pushed all the three metrics, with DSC of 0.901 compared to the baseline DSC of 0.812. The small and easy sets of techniques such as these prove to be highly useful for the model\u2019s performance.\nThe two models were compared in terms of the number of learnable parameters and multiply\u2013accumulate operations (MACs). The vanilla 3D U-Net has 3.61M parameters, which is three times the number of parameters of a 2D model with only 1.21M. In terms of operations, the 3D model calculates to have 518.61 GMACs as compared to 319.98 GMACs for the 2D counterpart when the HECKTOR dataset with the initial size of 144\u00d7144\u00d7144 is considered. The values naturally decrease with smaller size setting as shown in Table 4.1."
        },
        {
            "heading": "4.6 Qualitative Analysis",
            "text": "We conducted qualitative analysis to gain further insight into how 3D model on volume data and 2D model on SIs are differently performing. Figure 4.3 illustrates the segmentation results for 3D (Figure a) and 2D (Figure b) models on the HECKTOR dataset sample respectively. The sample we chose here is the one both models perform objectively well. The white region on the figures represent the ground truth, and the red region represents model predictions. Note that the output from the 3D network was cast to SI form to compare it with its counterpart in a full-view.\nAs is shown, both models output very similar results for this sample. 2D U-Net oversegments the slices with tumor edges, whereas the 3D model undersegments or completely misses them. The central areas of the tumor is well segmented by the 3D network while 2D again oversegments, causing for a drop in metrics of performance. This phenomenon is common across the dataset and is believed to be caused by the non-volumetric characteristic of SIs."
        },
        {
            "heading": "4.7 Discussion",
            "text": "The proposed idea of converting a volumetric problem into a 2D form was validated through two different types of datasets. At a glance, we see that 2D U-Net using this approach can yield results comparable to 3D U-Net. In the head and neck tumor segmentation problem, the 2D model with SIs produced results slightly lower than the 3D model when the original size of 144\u00d7 144\u00d7 144 was used. It is because the task in and of itself is very challenging. With its high dimensions, the tumor regions appear very small as compared to the background. Additionally, when we visually analyzed the dataset, we found several scans with artefacts in the teeth area; and these are the scans the both models are struggling with commonly. Such an artefact is depicted in Figure 4.2.\n28\nOn the upside, the 2D model performs well with the edges of tumor even if they look tiny in the formation of SIs, as is seen in Figure 4.3. Although counter intuitive, the 3D model misses such tiny regions for its strict predicting capability, and as it receives the volumetric data in a cubic form, it is assumed that it is unable to accurately pinpoint the contouring edges. It is for its strict predicting feature that its overall performance is slightly higher than its 2D counterpart.\nThe atrial dataset, with MR images and atrium as organ-of-target, is an entirely different segmentation task from the HECKTOR one. Yet, it did not pose a challenge to both models. Employing basic preprocessing and pretraining techniques, the model\u2019s performance using SIs improves, with around 9 percent increase in DSC. Such techniques as applying pretrained weights or clipping the scan to have a square-like form for SIs can help the model drastically improve.\nMethodically walking through the problem, we present four arguments as to why it might be favorable to use SIs and 2D networks over using 3D counterparts. First, pretrained weights on large-scale natural images for 2D models are more widespread and easily implementable, both for the encoder and the encoder-decoder models. The performance boost using ImageNet1k weights is illustrated in our experiments too. Pretraining the model on large-scale natural datasets and finetuning on medical applications is much easier with SIs. Second, 2D approach in general offers more options for easily employable data augmentations. Although not explored in details in this work, 3D augmentations are either not yet available or too costly to implement [72]. It is widely known, and is partially shown in our work, that picking the right set of augmentations is imperative to model learning. Third, a higher number of 2D networks are generally available because of natural images and larger computer vision community. Such networks initially are introduced in the 2D world before being employed in medical imaging tasks. Implementation of these models becomes much easier with SIs when working with volumetric data. Fourth, it is\n29\nmuch simpler and quicker to implement SSL on 2D data and 2D networks compared to 3D. Additionally, there is a higher availability of SSL pretrained models in the 2D community; thus studying this aspect of SIs is believed to be a crucial next step."
        },
        {
            "heading": "4.8 Conclusion",
            "text": "When we work on the segmentation task in medical imaging, we often face volumetric data. 3D networks such as 3D U-Net and its variations have been explored intensively for this task; and their performances are indeed impressive. However, the downside to that is they are generally computationally expensive and time-consuming. In this chapter, we studied a new approach of working with volumetric data by casting them to super images and using 2D networks for similar performance. In the HECKTOR dataset, a vanilla 2D U-Net with SIs achieved results comparable to powerful 3D U-Net results. Similarly, in atrial segmentation dataset, the idea is validated with promising performance and potential, especially when the approach is enhanced with various preprocessing techniques. We believe that there is potential for the new perspective of SIs when working with 3D medical data. We will look into further preprocessing techniques, stronger 2D networks such as ViT or deeper CNNs, and self-supervised learning as next steps to improve overall performance of using super images.\n30\nChapter 5"
        },
        {
            "heading": "Tumor Segmentation and Survival",
            "text": ""
        },
        {
            "heading": "Prediction of Patients with Head and",
            "text": ""
        },
        {
            "heading": "Neck Cancer",
            "text": "The prognosis of H&N cancer patients using DL approaches is understudied. The task by its very nature is very challenging. The previous study results are not encouraging and impractical in clinical use. In this chapter, we propose two distinct solutions 3 for the prognosis task.\nThe first is an ensemble network of MTLR, CoxPH and CNN models, dubbed as Deep Fusion, to prognose patients with H&N cancer using their clinical and imaging (CT and PET) data. The network architecture for Deep Fusion is depicted in Figure 5.1. We use CNNs to extract features from the CT and PET, and fuse them with the clinical data to make the risk predictions. We trained and tested the proposed solution on 224 and 101 patient cases respectively. The model is tested on the HECKTOR 2021 testing set and achieved the C-index of 0.72, winning the competition.\nThe second solution is an end-to-end Transformer based Multimodal network for Segmentation and Survival (TMSS) prediction, presented in Figure 5.2. This architecture benefits from the use of transformers for the fact that they can handle different data modalities. The model is designed to perform both the segmentation and prognosis tasks. The same data is used in a five-fold cross-validation form, outperforming the previous prognosis models with C-index of 0.763 and achieving DSC of 0.772 that is comparable to a standalone segmentation model.\n3This work has a filed patent in the US (USPTO: 17849943)\n31\n32\nChapter 6"
        },
        {
            "heading": "Thesis Conclusion",
            "text": "Cancer is one of the most lethal diseases in the world, and H&N cancer is considered a severe type of cancer with a significant number of deaths. While doctors spend hours to manually segment the tumor region from medical imaging data and then perform prognosis using that outcome and other clinical data, traditional ML and DL approaches can automate both these tasks, yielding comparable results instantaneously. This thesis work focuses on providing different solutions to the problem. The segmentation task was studied in depth in two ways: exploring the use of leading CNNs and ViTs for the task, and proposing the concept of super images with 2D networks for segmentation with such volumetric data. Patient outcome prediction task was studied using an ensemble of traditional ML and CNN networks to set the state-of-the-art on the task. A ViT-based model proposed later renews this state-of-the-art because of its capability to handle multimodal data and attend to the right modality while learning.\nThe work is highly detailed from different aspects; however, there still exist certain limitations. The primary limitation is that the work is predominantly focused on head and neck cancer, using only the HECKTOR dataset. Further verification of the proposed ideas, especially the super images concept, on external datasets is essential. Generalizability of the concept on other various tasks, such as classification and detection should be tested. Another strong limitation of the whole work is the clinical verification for all the models. Although the prognosis and diagnosis models from Chapter 5 are under a patent, they were not yet tested on a clinical trial. This should be an imperative component of the work since we are trying to propose to solve the issue at a level of clinician\u2019s precision.\nFor future work, Chapter 3 can benefit from exploring more recent ViT models such as Swin UNETR for faster and more desirable results. More ablation studies could strengthen the argument of using ViTs for such problems. Chapter 4, as mentioned, should be enhanced with more experiments using deeper CNN and transformer models to improve its results. Although seemingly reaching comparable results to its 3D counterparts, the study of SIs in more depth is believed to be a promising field of research, especially when it is supported by SSL pretraining. Chapter 5 survival analysis can further be boosted with more exploration into the multimodal data fusion approaches so that it becomes favorable even in clinical practice. While intuitive, ViT data embedding and positional encoding can further be analyzed in various fashions to understand the efficacy of the modules within the network. Finally, more recent encoders can be explored to reach even higher performance.\n33"
        },
        {
            "heading": "APPENDICES",
            "text": "43"
        },
        {
            "heading": "Appendix A",
            "text": ""
        },
        {
            "heading": "Papers and Implementations",
            "text": ""
        },
        {
            "heading": "A.1 Chapter 3 Implementation",
            "text": "Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are? [75]\nAuthor contributions: Data Analysis and Preprocessing, Modeling and Implementation, Paper Writing.\nPyTorch implementation can be found at: https://github.com/ikboljon/hecktor_midl_unetr.\nThe publication can be found at: https://proceedings.mlr.press/v172/sobirov22a.html."
        },
        {
            "heading": "A.2 Chapter 4 Implementation",
            "text": ""
        },
        {
            "heading": "Segmentation with Super Images: A New 2D Perspective on 3D Medical Image Analysis [76]:",
            "text": "Author contributions: Data Analysis and Preprocessing, Modeling and Implementation, Paper Writing.\nThe publication can be found at: https://arxiv.org/pdf/2205.02847.pdf."
        },
        {
            "heading": "A.3 Chapter 5 Implementation",
            "text": ""
        },
        {
            "heading": "An Ensemble Approach for Patient Prognosis of Head and Neck Tumor Using",
            "text": "Multimodal Data [69]\n44\nAuthor contributions: Data Analysis and Preprocessing, Paper Writing.\nPyTorch implementation can be found at: https://github.com/numanai/BioMedIA-Hecktor2021.\nThe publication can be found at: https://link.springer.com/chapter/10.1007/978-3-030-98253-9_26."
        },
        {
            "heading": "TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction [70]:",
            "text": "Author contributions: Data Analysis and Preprocessing, Modeling and Implementation, Paper Writing.\nPyTorch implementation can be found at https://github.com/ikboljon/tmss_miccai.\nThe publication can be found at: https://link.springer.com/chapter/10.1007/978-3-031-16449-1_31.\n45"
        }
    ],
    "title": "Diagnosis and Prognosis of Head and Neck Cancer Patients using Artificial Intelligence",
    "year": 2023
}