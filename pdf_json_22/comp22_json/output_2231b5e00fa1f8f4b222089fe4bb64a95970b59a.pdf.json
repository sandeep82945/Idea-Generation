{
    "abstractText": "Multilingual models have been widely used for cross-lingual transfer to low-resource languages. However, the performance on these languages is hindered by their underrepresentation in the pretraining data. To alleviate this problem, we propose a novel multilingual training technique based on teacherstudent knowledge distillation. In this setting, we utilize monolingual teacher models optimized for their language. We use those teachers along with balanced (sub-sampled) data to distill the teachers\u2019 knowledge into a single multilingual student. Our method outperforms standard training methods in lowresource languages and retains performance on high-resource languages. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Tomasz Limisiewicz"
        },
        {
            "affiliations": [],
            "name": "Dan Malkin"
        },
        {
            "affiliations": [],
            "name": "Gabriel Stanovsky"
        }
    ],
    "id": "SP:09a0da0aa12633424208d485562ca05a6f25b45e",
    "references": [
        {
            "authors": [
                "Mikel Artetxe",
                "Sebastian Ruder",
                "Dani Yogatama."
            ],
            "title": "On the cross-lingual transferability of monolingual representations",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4623\u20134637, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Giusepppe Attardi."
            ],
            "title": "Wikiextractor",
            "venue": "https:// github.com/attardi/wikiextractor.",
            "year": 2015
        },
        {
            "authors": [
                "Yonatan Belinkov."
            ],
            "title": "Probing classifiers: Promises, shortcomings, and advances",
            "venue": "Computational Linguistics, 48(1):207\u2013219.",
            "year": 2022
        },
        {
            "authors": [
                "Ethan C. Chau",
                "Lucy H. Lin",
                "Noah A. Smith."
            ],
            "title": "Parsing with multilingual BERT, a small corpus, and a small treebank",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1324\u20131334, Online. Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Heffernan",
                "Onur \u00c7elebi",
                "Holger Schwenk."
            ],
            "title": "Bitext mining using distilled sentence representations for low-resource languages",
            "venue": "arXiv preprint arXiv:2205.12654.",
            "year": 2022
        },
        {
            "authors": [
                "Damoc",
                "Aurelia Guy",
                "Simon Osindero",
                "Karen Simonyan",
                "Erich Elsen",
                "Jack W. Rae",
                "Oriol Vinyals",
                "Laurent Sifre."
            ],
            "title": "Training compute-optimal large language models",
            "venue": "CoRR, abs/2203.15556.",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani."
            ],
            "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
            "venue": "To appear.",
            "year": 2017
        },
        {
            "authors": [
                "Junjie Hu",
                "Sebastian Ruder",
                "Aditya Siddhant",
                "Graham Neubig",
                "Orhan Firat",
                "Melvin Johnson."
            ],
            "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation",
            "venue": "Proceedings of the 37th International",
            "year": 2020
        },
        {
            "authors": [
                "Pratik Joshi",
                "Sebastin Santy",
                "Amar Budhiraja",
                "Kalika Bali",
                "Monojit Choudhury."
            ],
            "title": "The state and fate of linguistic diversity and inclusion in the NLP world",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Karthikeyan K",
                "Zihan Wang",
                "Stephen Mayhew",
                "Dan Roth."
            ],
            "title": "Cross-lingual ability of multilingual BERT: an empirical study",
            "venue": "8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-",
            "year": 2020
        },
        {
            "authors": [
                "Dan Malkin",
                "Tomasz Limisiewicz",
                "Gabriel Stanovsky."
            ],
            "title": "A balanced data approach for evaluating cross-lingual transfer: Mapping the linguistic blood bank",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Manning",
                "Mihai Surdeanu",
                "John Bauer",
                "Jenny Finkel",
                "Steven Bethard",
                "David McClosky."
            ],
            "title": "The Stanford CoreNLP natural language processing toolkit",
            "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Linguis-",
            "year": 2014
        },
        {
            "authors": [
                "Benjamin Muller",
                "Antonios Anastasopoulos",
                "Beno\u00eet Sagot",
                "Djam\u00e9 Seddah."
            ],
            "title": "When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Joakim Nivre",
                "Marie-Catherine de Marneffe",
                "Filip Ginter",
                "Jan Haji\u010d",
                "Christopher D. Manning",
                "Sampo Pyysalo",
                "Sebastian Schuster",
                "Francis Tyers",
                "Daniel Zeman."
            ],
            "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Iryna Gurevych",
                "Sebastian Ruder."
            ],
            "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Phillip Rust",
                "Jonas Pfeiffer",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Iryna Gurevych."
            ],
            "title": "How good is your tokenizer? on the monolingual performance of multilingual language models",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
            "venue": "NeurIPS EMC2 Workshop.",
            "year": 2019
        },
        {
            "authors": [
                "Haipeng Sun",
                "Rui Wang",
                "Kehai Chen",
                "Masao Utiyama",
                "Eiichiro Sumita",
                "Tiejun Zhao."
            ],
            "title": "Knowledge distillation for multilingual unsupervised neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Henry Tsai",
                "Jason Riesa",
                "Melvin Johnson",
                "Naveen Arivazhagan",
                "Xin Li",
                "Amelia Archer."
            ],
            "title": "Small and practical BERT models for sequence labeling",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
            "year": 2019
        },
        {
            "authors": [
                "Xinyi Wang",
                "Yulia Tsvetkov",
                "Graham Neubig."
            ],
            "title": "Balancing training for multilingual neural machine translation",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8526\u20138537, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Shijie Wu",
                "Mark Dredze."
            ],
            "title": "Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120\u2013130, Online",
            "venue": "Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Yonghui Wu",
                "Mike Schuster",
                "Zhifeng Chen",
                "Quoc V Le",
                "Mohammad Norouzi",
                "Wolfgang Macherey",
                "Maxim Krikun",
                "Yuan Cao",
                "Qin Gao",
                "Klaus Macherey"
            ],
            "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "While multilingual language models have been gaining popularity, largely thanks to their crosslingual transfer ability, their performance has been shown to be skewed toward languages with abundant data (Joshi et al., 2020; Wu and Dredze, 2020). Introducing language models that better incorporate diverse and low-resource languages can increase accessibility to NLP technologies in these languages and help improve cross-lingual transfer (Malkin et al., 2022).\nIn this work, we address two research questions. First, we ask if we can we improve performance on low-resource languages without hurting it on high-resource ones? Second, does a better trade-off between high- and low-resource languages improve cross-lingual transfer?\nTo answer these two questions, we distill multiple monolingual teacher models optimized for various languages into a single multilingual student\n\u2217 Equal contribution. The order was decided by a coin toss.\n\u2020 Work done while visiting the Hebrew University. 1We will make all of our code and resources publicly avail-\nable.\nmodel, using a small balanced multilingual dataset (Figure 1). Our experiments show that this allows taking advantage of data in high-resource languages while avoiding under-fitting low-resource languages."
        },
        {
            "heading": "2 Background: Soft Vs. Hard Labels",
            "text": "We compare two alternatives for the masked LM loss functions: the original loss used for masked language modeling, i.e., hard labeling and soft labeling as defined in Sanh et al. (2019): (1) hard labeling, which takes into account a single gold masked token in a sentence, ygold, and evaluates the model\u2019s prediction for this word, i.e., standard cross-entropy loss:\nLHARD = \u2212 log(P (ygold)) (1)\nar X\niv :2\n21 0.\n07 13\n5v 2\n[ cs\n.C L\n] 2\n6 M\nay 2\n02 3\n(2) soft labeling, which allows for multiple valid candidates using the output distribution of an oracle (or a strong LM) M\u0302l as a soft label:\nLSOFT = \u2212 \u2211 y\u2208V PM\u0302l(y) log P (y) PM\u0302l(y) (2)\nWhere y denotes tokens in the model\u2019s vocabulary V . Please note that LSOFT is also equivalent to a KL-divergence between oracle and predicted distributions.\nIn the following sections, we will explain how soft labeling allows us to distill multiple teachers into a single multilingual student while accounting for balanced performance in high- and lowresource languages."
        },
        {
            "heading": "3 Teacher-Student Distillation for Multilingual Language Models",
            "text": "We train a multilingual student using the maskedlanguage modeling objective and a collection of monolingual teachers optimized for each student\u2019s language. All models share one multilingual vocabulary. Sharing vocabulary was necessary to apply our soft labeling loss, which requires that the student\u2019s and teacher\u2019s probability space (in the case of language models: vocabularies) are the same.2\nTo avoid under-fitting low-resource languages, we naively balance the students\u2019 training data by truncating data in all target languages to the data size of the lowest resource language. To make the most out of high-resource languages, we rely on soft labeling. For a mask in a given language, we use the high-resource language-specific teacher\u2019s distribution over the mask and use it as the oracle M\u0302l in Equation 2 as a soft label. Our intuition is that this allows the student to gain the broader teachers\u2019 knowledge in its language and thus compensate for the sub-sampled data size. Figure 1 provides a visual scheme for this approach.\nFormally, given a set of languages L = {l1, l2, ..., lK}, their corresponding teachers Tl1 , Tl2 , ..., TlK , and their data D = {D1, D2, ..., DK} we teach the student model using the K teachers (which are trained for each of the languages). For student training, we truncate the data size of all languages in D to the smallest dataset size (min(|D1|, |D2|, ..., |DK |)).\n2Please refer to Section 8, \u201cTeacher model availability\u201d for discussion about vocabulary sharing across monolingual models.\nSize [characters] Shared Script Diverse Script\nData selection and processing. We collect pretraining data from Wikipedia,3 aiming to capture a diverse set of high and low-resource languages, as summarized in Table 1. We subsample the corpora by randomly choosing sentences from each language\u2019s full corpus. We designate high-resource languages as ones with 50 or 100 million characters in their corpus after sampling, while low-resource languages\u2019 corpora consist of 10, 20, and 30 million characters.\nThroughout our experiments, we compare 7 languages that share the Latin script versus 7 languages with varying scripts, as the script was found to be an essential factor for multilingual performance (K et al., 2020; Muller et al., 2021; Malkin et al., 2022). We include German in both sets (as one of 7 languages), to compare its performance in both settings.\nModels\u2019 Architecture and Hyper-parameters. Each of our models comprises of 6 hidden layers and 4 attention heads, an MLM task head. The embedding dimension is 512 and sentences were truncated to 128 tokens. In total, our models consist of 51193168 parameters. We train a single uncased wordpiece tokenizer (Wu et al., 2016) on the 100mb splits of 15 languages.4 Before tokenization, we strip accents for all languages except Korean.\nWe train all models for 10 epochs, with a batch size of 8. We used linear decay of the learning rate\n3Obtained and cleaned using wikiextractor (Attardi, 2015). We chose Wikipedia as it consists of roughly similar encyclopedic domains across languages and is widely used for training PLMs (Devlin et al., 2019).\n413 languages presented in Table 1 with Hebrew and Lithuanian that were added for future experiments.\nwith the initial value of 5e-5. Exact configurations and parameters are available in our code."
        },
        {
            "heading": "4 Experiments",
            "text": "We validate our method using two experiments. First, we ascertain that our method indeed improves performance for low-resource languages while maintaining performance for high-resource languages. This is done by comparing the performance of our approach in masked language modeling with two multilingual baselines. Second, we show that our method is competitive for downstream tasks and cross-lingual transfer by probing the pre-trained models for POS and NER tagging.\nMultilingual modeling. We evaluate masked language modeling performance on monolingual test sets by measuring mean reciprocal rank (MRR). Since the performance of multilingual models is often compared to the performances of monolingual baselines, we report the average performance difference between a multilingual model and the monolingual models trained on the same set of respective languages.\nDownstream probing. We use the models trained in the previous experiment and train a probe,5 keeping the base model parameters frozen, to predict part-of-speech tagging (POS) and name entity recognition (NER), as provided respectively by universal dependencies (Nivre et al., 2020) and the XTREME benchmark (Hu et al., 2020).6 We chose those two tasks because they commonly appear in NLP pipelines (Manning et al., 2014; Honnibal and Montani, 2017). We measure the models\u2019 performance in two cases: when the training and test datasets are in the same language (denoted INLANG) and when a probe trained for a language l1 is tested on another one l2 (denoted ZERO-SHOT). As noted by Hu et al. (2020), zero-shot evaluation is a good measure of a model\u2019s cross-lingual transfer. We use probing because it offers a good insight into the representation learned by the model (Belinkov, 2022).\nBaselines. We compare the students\u2019 performance to multilingual models trained with hard labels, on the same data and languages as the student and its teachers. One such model was trained on all the available data in each language to examine\n5 6See Section D.2 in the Appendix for more information.\nthe extent of under-fitting low-resource languages, denoted HL. Additionally, to measure how much our student gains from its teacher\u2019s knowledge, we train another model on the corpora constrained to the size of the least resourceful language using the standard hard labels, denoted HL balanced.\nExperimental Setup Each teacher is a monolingual model trained with hard labels. The teachers are trained on the entire training corpus available in their language. In a student model, we distill the knowledge of multiple monolingual teachers into a multilingual student using soft labels, as described above. The distillation into the student is performed on groups of shared and diverse script languages. The data is constrained to 10 million characters for each language. All our models are trained using default BERT hyper-parameters detailed in Section 3."
        },
        {
            "heading": "5 Results",
            "text": "We report the experimental results on our test sets, in three language sets grouped by the amount of data available in pre-training, i.e., low-resource, high-resource, and all data. We address our research questions in light of the results:\nOur method offers a good trade-off between performance on high- and low-resource languages. Figure 2 shows the trend of language modeling scores (MRR) when changing from low- to highresource set. Table 2 summarizes performance differences from monolingual models for our method and the two control baseline models.\nIn low-resource setting, our model outperforms\nHL and achieves similar results to HL balanced. For high-resource languages, our approach closely trails HL and is better than HL balanced, which was trained on the same data as our student model. It indicates that the student model effectively acquires knowledge from the teachers\u2019 distributions. Our model achieves the best results overall when calculated over all languages.\nBetter trade-off between high- and low-resource languages improves results on downstream. Table 3 shows that IN-LANG and ZERO-SHOT results of probing for POS and NER labels. Our method achieves better or on-par average results in both tasks and language sets. The only exception is HL balanced baselines, which scores better in NER for low-resource languages.\nSharing script is not necessary for good multilingual performance. As seen in Figure 2 and Table 2 for low-resource languages, shared script results are consistently closer to monolingual results compared to the diverse script setting. Whereas, for high-resource set, the average difference between the results of monolingual models and our model or HL is smaller in the diverse script scenario. For the language included in both sets (German), MRR is higher when coupled with distinct script languages. The performance difference is 0.4 and 0.9 percent in favor of diverse scripts, for HL and our model. HL balanced scores 2.8% better in shared script scenario. This implies that diverse scripts can benefit multilingual modeling when we reveal enough monolingual data (as in high-resource setting).\nIn Table 3, we observe that the results for German in the shared-script scenario are better for POS tagging and worse for NER in comparison to diverse-script. Those findings align with previous results suggesting that shared vocabulary is not necessary for cross-lingual transfer and has a varying effect depending on the task (K et al., 2020; Malkin et al., 2022)."
        },
        {
            "heading": "6 Related Work",
            "text": "Recent work utilized knowledge distillation in training NLP models. However, to the best of our knowledge, we are the first to do this in low-resource, balanced data settings. Contrary to the approaches of Tsai et al. (2019); Sanh et al. (2019), we do not scale down student models but constraint training datasets.\nSun et al. (2020) use one teacher model and train for machine translation, and Heffernan et al. (2022) use a single multilingual teacher to train a sentence embedding model for low-resource languages. Both rely on parallel corpora for target low-resource languages. Other works on multilingual language modeling addressed how to improve low-resource performance, largely using post-hoc or language-specific solutions. Chau et al. (2020) change the vocabulary to account for low-resource languages, while Muller et al. (2021) transliterate tokens of low-resource languages to the most similar available high-resource language.\nFinally, Pfeiffer et al. (2020) introduce crosslingual adapters, compact components that allow adapting a given model pre-trained for a task in a different desired language."
        },
        {
            "heading": "7 Conclusions",
            "text": "We train multilingual language models aimed at balancing the models\u2019 performance for languages with uneven data sizes. We outperform standard models for low-resource languages while maintaining performance on high-resource languages. Noticeably, our method gives better results overall than the naive data sub-sampling. Lastly, our model is a good representation learner for downstream tasks, outperforming baselines for two probing tasks.\nTaken together, our results suggest a new direction for multilingual modeling that accounts for a more even performance across low- and highresource languages and improves cross-lingual transfer."
        },
        {
            "heading": "8 Limitations",
            "text": "Restricted model size and training. Due to limited computational resources, we performed experiments for models significantly smaller than the ones developed by the industry. We based our down-scaling choices on previous ablation studies on cross-lingual models (K et al., 2020). In line with their findings, we prioritized model depth (6 hidden layers) over width (4 attention heads). Also, we examine only BERT based models. This work serves as a proof of concept for a new multilingual language modeling, and future work can extend the study to bigger models with different architectures.\nRestricted data. We decided to train our models on sub-sampled Wikipedia to achieve reasonable training times. As shown in appendix B.2\nthe chosen sample follows the resource-richness trend across languages but does not fully reflect the imbalance between high- and low-resource languages. Nevertheless, we think that this issue does not weaken our point, as even our \u201cunbalanced\u201d baseline model is trained on less skewed data than currently deployed multilingual models. Furthermore, we train our models on 7 languages. Our method needs to be verified on larger data sizes and broader language sets.\nWorking with limited training data might still be valuable in several aspects. First, there\u2019s a growing interest in efficient, and green AI. Smaller and more efficient models will reduce training and inference costs while allowing them to run on less capable hardware and make them accessible to a wider community. Second, from a linguistic perspective, many of the world\u2019s languages lack large corpora, and hence will benefit from models that leverage a limited amount of available resources (Joshi et al., 2020).\nNaive balancing method. We truncate our training to the size of the smallest low-resource languages, which might be a naive and aggressive approach leading to a sub-optimal performance on our available data. However, our simple approach achieves good results even with naive balancing. Future work can extend it with complex data balancing approaches, such as weighing training data using a learned data scorer (as done in Wang et al. (2020)).\nTeacher model availability. Our teacher-student training method assumes the existence of pretrained monolingual teachers for each considered language, which is considerably less sustainable than training only one multilingual model. Nevertheless, we believe that it is possible to re-use publicly available models as teachers for high-resource languages, while for low-resource languages, competitive results can be obtained with smaller models requiring less computation (Hoffmann et al., 2022). Because our distillation method works on predicted distribution and not latent representations, to combine knowledge of teachers from multiple source languages, we will need to align their vocabularies, which was shown to be feasible by Artetxe et al. (2020); Rust et al. (2021). We leave this engineering task for future work.\nMetrics for probing tasks. To evaluate probing for NER we used macro-F1 measured per token\nand not per entity as in usual NER evaluation. We observed that the probes underperformed in correctly classifying all tokens in a single entity. It led to overall low results in regular F1 that would not allow meaningful comparison between analyzed models. Importantly, macro-F1 equally weights the performance in predicting each class. Thus, it is appropriate to evaluate NER task, where most tokens are annotated as not belonging to any entity."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers for their valuable comments on the previous versions of this article. This work was supported in part by a research gift from the Allen Institute for AI, and a research grant 2336 from the Israeli Ministry of Science and Technology. Tomasz Limisiewicz\u2019s visit to the Hebrew University has been supported by grant 338521 of the Charles University Grant Agency and the Mobility Fund of Charles University."
        },
        {
            "heading": "A Appendix",
            "text": "In the appendix: we provide details on datasets used in this work Section B; show how proposed teacher-student distillation behaves in the monolingual scenario with just one teacher Section C; present detailed results of our two experimental for each language Section D; provide details of our training procedure and hardware usage Section E."
        },
        {
            "heading": "B Datasets Details",
            "text": "B.1 Data Splits\nFor pre-training (monolingual) teacher and HL models, we use Wikipedia splits of sizes indicated in Table 1, for training student and HL balanced models, we subsample training corpus to 10 million characters. We use validation and test sets containing 10000 Wikipedia sentences each.\nFor downstream probing, we use train and test splits from XTREME. The numbers of sentences in these splits per language are shown in table 4.\nB.2 Correspondence of the Sizes of Our Corpora and Wikipedias\nFigure 3 shows the per language correspondence between our corpora size and the whole Wikipedia. The latter was used to pre-train MBERT (Devlin et al., 2019). We observe a good linear fit between character numbers in our corpora and the logarithm\nof Wikipedia byte size. It suggests that the multilingual imbalance is even more severe in the original dataset than in our sample."
        },
        {
            "heading": "C Teacher-Student Method in the Monolingual World",
            "text": "The purpose of this experiment is to visualize how the model\u2019s performance scales with the size of the pre-training dataset. Also, we check the behavior of the teacher-student knowledge distillation with the change of data size used to train a teacher and a student in a monolingual setting.\nWe train a monolingual model on German Wikipedia data with five sizes (in millions of characters): 10, 20, 30, 50, and 100. Subsequently, we designate 10, 50, and 100 million character models as teachers and distill their knowledge into students on the same size or smaller corpus.7\nAs presented in figure 4, the teacher performance 7In monolingual knowledge distillation, we used a learning rate 5 times higher than in the default BERT training script. This choice led to better results.\ncan be nearly matched by a student trained on a considerably smaller corpus. For the teacher trained on the largest split, the student performance rises steadily with the increase of distillation detest from 10 to 30 million characters and drops after that point. The performance of the student trained on 100 million characters is noticeably low. It is a sign of over-fitting, as in our setting, distillation set is always a subset of the teacher\u2019s training set. Also, in the case of teachers trained on smaller corpora, distillation on the dataset of the same size (as the teacher training set) leads to a drop in performance. Therefore, we claim that the distillation is beneficial when the teacher\u2019s training set is larger than the student\u2019s one."
        },
        {
            "heading": "D Per Language Results",
            "text": "D.1 German: Comparing Shared and Diverse Scripts\nTable 5 and Figure 5 present masked language modeling performance for German for three analyzed multilingual model types. German is the language included both in the shared and diverse script language sets. Therefore the results allow comparing which setting is more effective in multilingual language modeling.\nD.2 Results for Every Language We present per language results in masked language modeling performance in Figure 6 and for probing tasks (POS and NER) in Tables 6 and 7."
        },
        {
            "heading": "E GPUs and training procedures",
            "text": "All of our models (monolingual teachers, students, and multilingual models trained using hard labels) are trained on a single GPU core.\nWe used varying GPUs architectures allocated for each model upon availability (nvidia gtx 980, tesla M60, and RTX 2080Ti). Training time varied between 1 to 3 hours for monolingual models (depending on the data size, language, and GPU core). Multilingual models\u2019 training took around 18 hours to complete. Early stopping was used for all models based on results on a balanced dev set.\nMLM evaluation was run on the same machines as training or on CPU. the run time ranged from 2 to 4 hours. Training a probe on top of a frozen model took from 1 to 20 minutes, depending on the number of training examples available for a language. The evaluation time on a downstream task was less than 2 minutes."
        }
    ],
    "title": "You Can Have Your Data and Balance It Too: Towards Balanced and Efficient Multilingual Models",
    "year": 2023
}