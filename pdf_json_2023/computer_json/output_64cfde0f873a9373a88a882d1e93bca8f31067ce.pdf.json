{
    "abstractText": "Graphs are a natural representation of brain activity derived from functional magnetic imaging (fMRI) data. It is well known that clusters of anatomical brain regions, known as functional connectivity networks (FCNs), encode temporal relationships which can serve as useful biomarkers for understanding brain function and dysfunction. Previous works, however, ignore the temporal dynamics of the brain and focus on static graphs. In this paper, we propose a dynamic brain graph deep generative model (DBGDGM) which simultaneously clusters brain regions into temporally evolving communities and learns dynamic unsupervised node embeddings. Specifically, DBGDGM represents brain graph nodes as embeddings sampled from a distribution over communities that evolve over time. We parameterise this community distribution using neural networks that learn from subject and node embeddings as well as past community assignments. Experiments demonstrate DBGDGM outperforms baselines in graph generation, dynamic link prediction, and is comparable for graph classification. Finally, an analysis of the learnt community distributions reveals overlap with known FCNs reported in neuroscience literature.",
    "authors": [
        {
            "affiliations": [],
            "name": "Alexander Campbell"
        },
        {
            "affiliations": [],
            "name": "Simeon Spasov"
        },
        {
            "affiliations": [],
            "name": "Nicola Toschi"
        },
        {
            "affiliations": [],
            "name": "Pietro Li\u00f2"
        },
        {
            "affiliations": [],
            "name": "Campbell Spasov"
        },
        {
            "affiliations": [],
            "name": "Toschi Li\u00f2"
        }
    ],
    "id": "SP:775fe4324a0018050a2e2f05fbb00a0f851cd247",
    "references": [
        {
            "authors": [
                "Alexandre Abraham",
                "Michael P Milham",
                "Adriana Di Martino",
                "R Cameron Craddock",
                "Dimitris Samaras",
                "Bertrand Thirion",
                "Gael Varoquaux"
            ],
            "title": "Deriving reproducible biomarkers from multi-site resting-state data: An autism-based",
            "venue": "example. NeuroImage,",
            "year": 2017
        },
        {
            "authors": [
                "Bijaya Adhikari",
                "Yao Zhang",
                "Naren Ramakrishnan",
                "B Aditya Prakash"
            ],
            "title": "Distributed representations of subgraphs",
            "venue": "IEEE International Conference on Data Mining Workshops (ICDMW),",
            "year": 2017
        },
        {
            "authors": [
                "Fidel Alfaro-Almagro",
                "Mark Jenkinson",
                "Neal K Bangerter",
                "Jesper LR Andersson",
                "Ludovica Griffanti",
                "Gwena\u00eblle Douaud",
                "Stamatios N Sotiropoulos",
                "Saad Jbabdi",
                "Moises HernandezFernandez",
                "Emmanuel Vallee"
            ],
            "title": "Image processing and quality control for the first 10,000 brain imaging datasets from uk biobank",
            "year": 2018
        },
        {
            "authors": [
                "Elena A Allen",
                "Eswar Damaraju",
                "Sergey M Plis",
                "Erik B Erhardt",
                "Tom Eichele",
                "Vince D Calhoun"
            ],
            "title": "Tracking whole-brain connectivity dynamics in the resting state",
            "venue": "Cerebral cortex,",
            "year": 2014
        },
        {
            "authors": [
                "Sonsoles Alonso Mart\u0301\u0131nez",
                "Gustavo Deco",
                "Gert J Ter Horst",
                "Joana Cabral"
            ],
            "title": "The dynamics of functional brain networks associated with depressive symptoms in a nonclinical sample",
            "venue": "Frontiers in neural circuits,",
            "year": 2020
        },
        {
            "authors": [
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si",
                "R\u00e9ka Albert"
            ],
            "title": "Emergence of scaling in random networks. science",
            "year": 1999
        },
        {
            "authors": [
                "Carlo Bonferroni"
            ],
            "title": "Teoria statistica delle classi e calcolo delle probabilita",
            "venue": "Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commericiali di Firenze,",
            "year": 1936
        },
        {
            "authors": [
                "Vince D Calhoun",
                "Robyn Miller",
                "Godfrey Pearlson",
                "Tulay Adal\u0131"
            ],
            "title": "The chronnectome: time-varying connectivity networks as the next frontier in fmri data",
            "venue": "discovery. Neuron,",
            "year": 2014
        },
        {
            "authors": [
                "Sandro Cavallari",
                "Vincent W Zheng",
                "Hongyun Cai",
                "Kevin Chen-Chuan Chang",
                "Erik Cambria"
            ],
            "title": "Learning community embedding with community detection and node embedding on graphs",
            "venue": "In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management,",
            "year": 2017
        },
        {
            "authors": [
                "Simon Dahan",
                "Logan ZJ Williams",
                "Daniel Rueckert",
                "Emma C Robinson"
            ],
            "title": "Improving phenotype prediction using long-range spatio-temporal dynamics of functional connectivity",
            "venue": "In International Workshop on Machine Learning in Clinical Neuroimaging,",
            "year": 2021
        },
        {
            "authors": [
                "Sophie Dautricourt",
                "Julie Gonneaud",
                "Brigitte Landeau",
                "Vince D Calhoun",
                "Robin de Flores",
                "G\u00e9raldine Poisnel",
                "Salma Bougacha",
                "Valentin Ourry",
                "Edelweiss Touron",
                "Elizabeth Kuhn"
            ],
            "title": "Dynamic functional connectivity patterns associated with dementia risk",
            "venue": "Alzheimer\u2019s Research & Therapy,",
            "year": 2022
        },
        {
            "authors": [
                "Rotem Dror",
                "Segev Shlomov",
                "Roi Reichart"
            ],
            "title": "Deep dominance - how to properly compare deep neural models",
            "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Erdos",
                "Alfr\u00e9d R\u00e9nyi"
            ],
            "title": "On the evolution of random graphs",
            "venue": "Publ. Math. Inst. Hung. Acad. Sci,",
            "year": 1960
        },
        {
            "authors": [
                "Farnaz Zamani Esfahlani",
                "Youngheun Jo",
                "Maria Grazia Puxeddu",
                "Haily Merritt",
                "Jacob C Tanner",
                "Sarah Greenwell",
                "Riya Patel",
                "Joshua Faskowitz",
                "Richard F Betzel"
            ],
            "title": "Modularity maximization as a flexible and generic framework for brain network exploratory analysis",
            "year": 2021
        },
        {
            "authors": [
                "Amirreza Farnoosh",
                "Sarah Ostadabbas"
            ],
            "title": "Deep markov factor analysis: Towards concurrent temporal and spatial analysis of fmri data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Gao",
                "Gang Fu",
                "Chunping Ouyang",
                "Satoshi Tsutsui",
                "Xiaozhong Liu",
                "Jeremy Yang",
                "Christopher Gessner",
                "Brian Foote",
                "David Wild",
                "Ying Ding"
            ],
            "title": "edge2vec: Representation learning using edge semantics for biomedical knowledge discovery",
            "venue": "BMC bioinformatics,",
            "year": 2019
        },
        {
            "authors": [
                "Matthew F Glasser",
                "Stamatios N Sotiropoulos",
                "J Anthony Wilson",
                "Timothy S Coalson",
                "Bruce Fischl",
                "Jesper L Andersson",
                "Junqian Xu",
                "Saad Jbabdi",
                "Matthew Webster",
                "Jonathan R Polimeni"
            ],
            "title": "The minimal preprocessing pipelines for the human connectome",
            "venue": "project. Neuroimage,",
            "year": 2013
        },
        {
            "authors": [
                "Palash Goyal",
                "Emilio Ferrara"
            ],
            "title": "Graph embedding techniques, applications, and performance: A survey",
            "venue": "Knowledge-Based Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Palash Goyal",
                "Nitin Kamra",
                "Xinran He",
                "Yan Liu"
            ],
            "title": "Dyngem: Deep embedding method for dynamic graphs",
            "venue": "arXiv preprint arXiv:1805.11273,",
            "year": 2018
        },
        {
            "authors": [
                "Palash Goyal",
                "Sujit Rokka Chhetri",
                "Arquimedes Canedo"
            ],
            "title": "dyngraph2vec: Capturing network dynamics using dynamic graph representation learning",
            "venue": "Knowledge-Based Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tony Gracious",
                "Shubham Gupta",
                "Arun Kanthali",
                "Rui M Castro",
                "Ambedkar Dukkipati"
            ],
            "title": "Neural latent space model for dynamic networks and temporal knowledge graphs",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Grover",
                "Jure Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Shubham Gupta",
                "Gaurav Sharma",
                "Ambedkar Dukkipati"
            ],
            "title": "A generative model for dynamic networks with applications",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Ehsan Hajiramezanali",
                "Arman Hasanzadeh",
                "Krishna Narayanan",
                "Nick Duffield",
                "Mingyuan Zhou",
                "Xiaoning Qian"
            ],
            "title": "Variational graph recurrent neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "William L Hamilton"
            ],
            "title": "Graph representation learning",
            "venue": "Synthesis Lectures on Artifical Intelligence and Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Christoph Gohlke",
                "Travis E Oliphant"
            ],
            "title": "Array programming with NumPy",
            "venue": "Nature, 585(7825):357\u2013362,",
            "year": 2020
        },
        {
            "authors": [
                "S Heitmann",
                "M Breakspear"
            ],
            "title": "Putting the \u2018dynamic\u2019back into dynamic functional connectivity",
            "year": 2017
        },
        {
            "authors": [
                "Fabian Hirsch",
                "Afra Wohlschlaeger"
            ],
            "title": "Graph analysis of nonlinear fmri connectivity dynamics reveals distinct brain network configurations for integrative and segregated information processing",
            "venue": "Nonlinear Dynamics,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew D Hoffman",
                "David M Blei"
            ],
            "title": "Structured stochastic variational inference",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Scott A Huettel",
                "Allen W Song",
                "Gregory McCarthy"
            ],
            "title": "Functional magnetic resonance imaging, volume 1",
            "venue": "Sinauer Associates Sunderland,",
            "year": 2004
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbelsoftmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "Jie Lisa Ji",
                "Marjolein Spronk",
                "Kaustubh Kulkarni",
                "Grega Repov\u0161",
                "Alan Anticevic",
                "Michael W Cole"
            ],
            "title": "Mapping the human brain\u2019s cortical-subcortical functional network",
            "venue": "organization. Neuroimage,",
            "year": 2019
        },
        {
            "authors": [
                "Michael I Jordan",
                "Zoubin Ghahramani",
                "Tommi S Jaakkola",
                "Lawrence K Saul"
            ],
            "title": "An introduction to variational methods for graphical models",
            "venue": "Machine learning,",
            "year": 1999
        },
        {
            "authors": [
                "Rayyan Ahmad Khan",
                "Muhammad Umer Anwaar",
                "Omran Kaddah",
                "Zhiwei Han",
                "Martin Kleinsteuber"
            ],
            "title": "Unsupervised learning of joint embeddings for node representation and community detection",
            "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
            "year": 2021
        },
        {
            "authors": [
                "Byung-Hoon Kim",
                "Jong Chul Ye",
                "Jae-Jin Kim"
            ],
            "title": "Learning dynamic graph representation of brain connectome with spatio-temporal attention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Variational graph auto-encoders",
            "venue": "arXiv preprint arXiv:1611.07308,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Chris J Maddison",
                "Andriy Mnih",
                "Yee Whye Teh"
            ],
            "title": "The concrete distribution: A continuous relaxation of discrete random variables",
            "venue": "arXiv preprint arXiv:1611.00712,",
            "year": 2016
        },
        {
            "authors": [
                "Sedigheh Mahdavi",
                "Shima Khoshraftar",
                "Aijun An"
            ],
            "title": "dynnode2vec: Scalable dynamic network embedding",
            "venue": "IEEE international conference on big data (Big Data),",
            "year": 2018
        },
        {
            "authors": [
                "L-E Martinet",
                "MA Kramer",
                "W Viles",
                "LN Perkins",
                "E Spencer",
                "CJ Chu",
                "SS Cash",
                "ED Kolaczyk"
            ],
            "title": "Robust dynamic community detection with applications to human brain functional networks",
            "venue": "Nature communications,",
            "year": 2020
        },
        {
            "authors": [
                "Nikhil Mehta",
                "Lawrence Carin Duke",
                "Piyush Rai"
            ],
            "title": "Stochastic blockmodels meet graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Kurt Miller",
                "Michael Jordan",
                "Thomas Griffiths"
            ],
            "title": "Nonparametric latent feature models for link prediction",
            "venue": "Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "Annamalai Narayanan",
                "Mahinthan Chandramohan",
                "Rajasekar Venkatesan",
                "Lihui Chen",
                "Yang Liu",
                "Shantanu Jaiswal"
            ],
            "title": "graph2vec: Learning distributed representations of graphs",
            "venue": "arXiv preprint arXiv:1707.05005,",
            "year": 2017
        },
        {
            "authors": [
                "Krzysztof Nowicki",
                "Tom A B Snijders"
            ],
            "title": "Estimation and prediction for stochastic blockstructures",
            "venue": "Journal of the American statistical association,",
            "year": 2001
        },
        {
            "authors": [
                "Aldo Pareja",
                "Giacomo Domeniconi",
                "Jie Chen",
                "Tengfei Ma",
                "Toyotaro Suzumura",
                "Hiroki Kanezashi",
                "Tim Kaler",
                "Tao Schardl",
                "Charles Leiserson"
            ],
            "title": "Evolvegcn: Evolving graph convolutional networks for dynamic graphs",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "arXiv preprint arXiv:1912.01703,",
            "year": 2019
        },
        {
            "authors": [
                "Dragana M Pavlovi\u0107",
                "Bryan RL Guillaume",
                "Soroosh Afyouni",
                "Thomas E Nichols"
            ],
            "title": "Multisubject stochastic blockmodels with mixed effects for adaptive analysis of individual differences in human brain network cluster structure",
            "venue": "Statistica Neerlandica,",
            "year": 2020
        },
        {
            "authors": [
                "Fabian Pedregosa",
                "Ga\u00ebl Varoquaux",
                "Alexandre Gramfort",
                "Vincent Michel",
                "Bertrand Thirion",
                "Olivier Grisel",
                "Mathieu Blondel",
                "Peter Prettenhofer",
                "Ron Weiss",
                "Vincent Dubourg"
            ],
            "title": "Scikit-learn: Machine learning in python",
            "venue": "Journal of machine Learning research,",
            "year": 2011
        },
        {
            "authors": [
                "Farimah Poursafaei",
                "Shenyang Huang",
                "Kellin Pelrine",
                "Reihaneh Rabbany"
            ],
            "title": "Towards better evaluation for dynamic link prediction",
            "venue": "arXiv preprint arXiv:2207.10128,",
            "year": 2022
        },
        {
            "authors": [
                "Python Core Team"
            ],
            "title": "Python: A dynamic, open source programming language",
            "venue": "Python Software Foundation,",
            "year": 2019
        },
        {
            "authors": [
                "Gal Raz",
                "Alexandra Touroutoglou",
                "Christine Wilson-Mendenhall",
                "Gadi Gilam",
                "Tamar Lin",
                "Tal Gonen",
                "Yael Jacob",
                "Shir Atzil",
                "Roee Admon",
                "Maya Bleich-Cohen"
            ],
            "title": "Functional connectivity dynamics during film viewing reveal common networks for different emotional experiences. Cognitive, Affective",
            "venue": "Behavioral Neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Herbert Robbins",
                "Sutton Monro"
            ],
            "title": "A stochastic approximation method",
            "venue": "The annals of mathematical statistics,",
            "year": 1951
        },
        {
            "authors": [
                "David E Rumelhart",
                "Geoffrey E Hinton",
                "Ronald J Williams"
            ],
            "title": "Learning representations by back-propagating",
            "venue": "errors. nature,",
            "year": 1986
        },
        {
            "authors": [
                "Aravind Sankar",
                "Yanhong Wu",
                "Liang Gou",
                "Wei Zhang",
                "Hao Yang"
            ],
            "title": "Dysat: Deep neural representation learning on dynamic graphs via self-attention networks",
            "venue": "In Proceedings of the 13th international conference on web search and data mining,",
            "year": 2020
        },
        {
            "authors": [
                "Arindam Sarkar",
                "Nikhil Mehta",
                "Piyush Rai"
            ],
            "title": "Graph representation learning via ladder gamma variational autoencoders",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Lawrence Saul",
                "Michael Jordan"
            ],
            "title": "Exploiting tractable substructures in intractable networks",
            "venue": "Advances in neural information processing systems,",
            "year": 1995
        },
        {
            "authors": [
                "Joakim Skarding",
                "Matthew Hellmich",
                "Bogdan Gabrys",
                "Katarzyna Musial"
            ],
            "title": "A robust comparative analysis of graph neural networks on dynamic link prediction",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur PC Spencer",
                "Marc Goodfellow"
            ],
            "title": "Using deep clustering to improve fmri dynamic functional connectivity analysis",
            "year": 2022
        },
        {
            "authors": [
                "Cathie Sudlow",
                "John Gallacher",
                "Naomi Allen",
                "Valerie Beral",
                "Paul Burton",
                "John Danesh",
                "Paul Downey"
            ],
            "title": "Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age",
            "venue": "Plos med,",
            "year": 2015
        },
        {
            "authors": [
                "Fan-Yun Sun",
                "Meng Qu",
                "Jordan Hoffmann",
                "Chin-Wei Huang",
                "Jian Tang"
            ],
            "title": "vgraph: A generative model for joint community detection and node representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Chee-Ming Ting",
                "S Balqis Samdin",
                "Meini Tang",
                "Hernando Ombao"
            ],
            "title": "Detecting dynamic community structure in functional brain networks across individuals: a multilayer approach",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2020
        },
        {
            "authors": [
                "Dennis Ulmer",
                "Christian Hardmeier",
                "Jes Frellsen"
            ],
            "title": "deep-significance-easy and meaningful statistical significance testing in the age of neural networks",
            "venue": "arXiv preprint arXiv:2204.06815,",
            "year": 2022
        },
        {
            "authors": [
                "David C Van Essen",
                "Stephen M Smith",
                "Deanna M Barch",
                "Timothy EJ Behrens",
                "Essa Yacoub",
                "Kamil Ugurbil",
                "Wu-Minn HCP Consortium"
            ],
            "title": "The wu-minn human connectome project: an overview",
            "year": 2013
        },
        {
            "authors": [
                "Xiao Wang",
                "Peng Cui",
                "Jing Wang",
                "Jian Pei",
                "Wenwu Zhu",
                "Shiqiang Yang"
            ],
            "title": "Community preserving network embedding",
            "venue": "In Thirty-first AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yaara Yeshurun",
                "Mai Nguyen",
                "Uri Hasson"
            ],
            "title": "The default mode network: where the idiosyncratic self meets the shared social world",
            "venue": "Nature Reviews Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Lekui Zhou",
                "Yang Yang",
                "Xiang Ren",
                "Fei Wu",
                "Yueting Zhuang"
            ],
            "title": "Dynamic network",
            "venue": "Mining (SDM),",
            "year": 2021
        },
        {
            "authors": [
                "Nowicki",
                "Snijders"
            ],
            "title": "DGMs that exploit the learning capacity of NNs are able to learn more expressive graph distributions (Mehta et al., 2019",
            "venue": "Kipf and Welling, 2016b; Sarkar et al.,",
            "year": 2001
        },
        {
            "authors": [
                "Maddison"
            ],
            "title": "2016) for the categorical distributions. All gradients are now easily",
            "year": 2016
        },
        {
            "authors": [
                "Campbell Spasov Toschi Li\u00f2"
            ],
            "title": "For both datasets, we choose W = 30 and = 5 resulting in T = b490/30c = 16 graph snapshots each with E(s",
            "year": 2021
        },
        {
            "authors": [
                "D. Appendix"
            ],
            "title": "Baselines We compare DBGDGM against a range of static and dynamic unsupervised graph representation learning baseline models, all with publicly available code. In particular, we focus on baselines that are generative and can quantify uncertainty. We leave comparisons to popular deterministic baselines such as DynamicTriad",
            "venue": "DySAT (Sankar",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Keywords: Dynamic graph, generative model, functional magnetic resonance imaging"
        },
        {
            "heading": "1. Introduction",
            "text": "Functional magnetic resonance imaging (fMRI) is a non-invasive imaging technique primarily used to measure blood-oxygen level dependent (BOLD) signal in the brain (Huettel et al., 2004). A natural representation of fMRI data is as a discrete-time graph, henceforth referred to as a dynamic brain graph (DBG), consisting of a set of fixed nodes corresponding to anatomically separated brain regions and a set of time-varying edges determined by a measure of dynamic functional connectivity (dFC) (Calhoun et al., 2014). DBGs have been widely used in graph-based network analysis for understanding brain function (Hirsch and Wohlschlaeger, 2022; Raz et al., 2016) and dysfunction (Alonso Mart\u0301\u0131nez et al., 2020; Dautricourt et al., 2022; Yu et al., 2015).\n\u2217 Contributed equally\n\u00a9 2023 A. Campbell, S. Spasov, N. Toschi & P. Lio\u0300.\nar X\niv :2\n30 1.\n11 40\n8v 1\n[ cs\n.L G\n] 2\n6 Ja\nRecently, there is growing interest in using deep learning-based methods for learning representations of graph-structured data (Goyal and Ferrara, 2018; Hamilton, 2020). A graph representation typically consists of a low-dimensional vector embedding of either the entire graph (Narayanan et al., 2017) or a part of it\u2019s structure such as nodes (Grover and Leskovec, 2016), edges (Gao et al., 2019), or sub-graphs (Adhikari et al., 2017). Although originally formulated for static graphs (i.e. not time-varying), several existing methods have been extended (Mahdavi et al., 2018; Goyal et al., 2020), and new ones proposed (Zhou et al., 2018; Sankar et al., 2020), for dynamic graphs. The embeddings are usually learnt in either a supervised or unsupervised fashion and typically used in tasks such as node classification (Pareja et al., 2020) and dynamic link prediction (Goyal et al., 2018).\nTo date, very few deep learning-based methods have been designed for, or existing methods applied to, representation learning of DBGs. Those that do, tend to use graph neural networks (GNNs) that are designed for learning node- and graph-level embeddings for use in graph classification (Kim et al., 2021; Dahan et al., 2021). Although node/graphlevel embeddings are effective at representing local/global graph structure, they are less adept at representing topological structures in-between these two extremes such a clusters of nodes or communities (Wang et al., 2017). Recent methods that explicitly incorporate community embeddings alongside node embeddings have shown improved performance for static graph representation learning tasks (Sun et al., 2019; Cavallari et al., 2017). How to leverage the relatedness of graph, node, and community embeddings in a unified framework for DBG representation learning remains under-explored. We refer to Appendix A for a summary of related work.\nContributions To address these shortcomings, we propose DBGDGM, a hierarchical deep generative model (DGM) designed for unsupervised representing learning on DBGs derived from multi-subject fMRI data. Specifically, DBGDGM represents nodes as embeddings sampled from a distribution over communities that evolve over time. The community distribution is parameterized using neural networks (NNs) that learn from graph and node embeddings as well as past community assignments. We evaluate DBGDGM on multiple real-world fMRI datasets and show that it outperforms state-of-the-art baselines for graph reconstruction, dynamic link prediction, and achieves comparable results for graph classification."
        },
        {
            "heading": "2. Problem formulation",
            "text": "We consider a dataset of multi-subject DBGs derived from fMRI data D \u2261 G(1:S, 1:T ) = {G(s, t)}S, Ts, t=1 that share a common set of nodes V = {v1, . . . , vN} over T \u2208 N timepoints for S \u2208 N subjects. Each G(s, t) \u2208 G(1:S, 1:T ) denotes a non-attributed, unweighted, and undirected brain graph snapshot for the s-th subject at the t-th timepoint. We define a brain graph snapshot as a tuple G(s, t) = (V, E(s, t)) where E(s, t) \u2286 V \u00d7 V denotes an edge set. The i-th edge for the s-th subject at the t-th timepoint e\n(s, t) i \u2208 E(s, t) is defined e (s, t) i =\n(w (s,t) i , c (s,t) i ) where w (s,t) i is a source node and c (s,t) i is a target node. We assume each node corresponds to a brain region making the number of nodes |V| = V \u2208 N fixed over subjects and time. We also assume edges correspond to a measure of dFC allowing the number of edges |E(s, t)| = E(s, t) \u2208 N vary over subjects and time. We further assume there exists\nK \u2208 N clusters of nodes, or communities, the membership of which dynamically changes over time for each subject. Let z\n(s, t) i \u2208 [1 : K] denote the latent community assignment of\nthe i-th edge for the s-th subject at the t-th timepoint. For each subject\u2019s DBG our aim is to learn, in an unsupervised fashion, graph \u03b1(s) \u2208 RH\u03b1 , node \u03c6(s, t)1:N = [\u03c6 (s, t) n ] \u2208 RN\u00d7H\u03c6 , and community \u03c8 (s, t) 1:K = [\u03c8 (s, t) k ] \u2208 R\nK\u00d7H\u03c8 representations of dimensions H\u03b1, H\u03c6, H\u03c8 \u2208 N, respectively, for use in a variety of downstream tasks.\n3. Method\nDBGDGM defines a hierarchical deep generative model and inference network for the end-to-end learning of graph, node, and community embeddings from multisubject DBG data. Specifically, DBGDGM treats the embeddings and edge community assignments as latent random variables collectively denoted \u2126(s, t) = {\u03b1(s), \u03c6\n(s, t) 1:N , \u03c8 (s, t) 1:K , {z (s, t) i }E (s, t) i=1 }, which along with the observed DBGs, defines a probabilistic latent variable model with joint density p\u03b8(G1:S, 1:T ,\u21261:S, 1:T )."
        },
        {
            "heading": "3.1. Generative model",
            "text": "Graph embeddings We begin the generative process by sampling graph embeddings from a prior \u03b1(s) \u223c p\u03b8\u03b1(\u03b1(s)) implemented as a normal distribution following\np\u03b8\u03b1(\u03b1 (s)) = Normal(0H\u03b1 , IH\u03b1) (1)\nwhere 0H\u03b1 is a matrix of zeros and IH\u03b1 is a identity matrix. Each embedding is a vector \u03b1(s) \u2208 RH\u03b1 representing subject-specific information that remains fixed over time.\nNode and community embeddings Next, let \u03c6 (s, t) n \u2208 RH\u03c6 and \u03c8(s, t)k \u2208 R H\u03c8 denote the n-th node and the k-th community embedding, respectively. To incorporate temporal dynamics, we assume node and community embeddings are related through Markov chains with prior transition distributions \u03c6 (s, t) n \u223c p\u03b8\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n , \u03b1(s)) and \u03c8(s, t)k \u223c p\u03b8\u03c8(\u03c8 (s, t) k |\u03c8 (s, t\u22121) k , \u03b1 (s)). We specify each prior to be a normal distribution following\np\u03b8\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n , \u03b1(s)) = Normal(\u03c6(s, t\u22121)n , \u03c3\u03c6IH\u03c6) (2)\np\u03b8\u03c8(\u03c8 (s, t) k |\u03c8 (s, t\u22121) k , \u03b1 (s)) = Normal(\u03c8 (s, t\u22121) k , \u03c3\u03c8IH\u03c8) (3)\nwhere the graph embeddings are used for initializing the means, i.e., \u03c6 (s, 0) n = \u03b1(s), \u03c8 (s, 0) k = \u03b1(s) and the standard deviations \u03c3\u03c6, \u03c3\u03c8 \u2208 R>0 are hyperparameters controlling how smoothly each embedding changes between consecutive timepoints.\nEdge generation We next describe the edge generative process of a graph snapshot G(s, t) \u2208 G(1:S, 1:T ). Similar to Sun et al. (2019), for each edge e(s, t)i = (w (s, t) i , c (s, t) i ) \u2208 E(s, t) we first sample a latent community assignment z (s, t) i \u2208 [1 : K] from a conditional prior z (s, t) i \u223c p\u03b8z(z (s, t) i |w (s, t) i ) implemented as a categorical distribution\np\u03b8z(z (s, t) i |w (s, t) i ) = Categorical(\u03c0 (s, t) \u03b8z ), \u03c0 (s, t) \u03b8z = MLP\u03b8z(\u03c6 (s, t) wi ) (4)\nwhere MLP\u03b8z : RH\u03c6 \u2192 RK is a Lz-layered multilayered perception (MLP) that parameterizes community probabilities using node embeddings indexed by w\n(s, t) i . In other words,\neach source node w (s, t) i is represented as a mixture of communities. A linked target node c (s, t) i \u2208 [1 : N ] is then sampled from the conditional likelihood c (s, t) i \u223c p\u03b8c(c (s, t) i |z (s, t) i ) which is also implemented as a categorical distribution\np\u03b8c(c (s, t) i |z (s, t) i ) = Categorical(\u03c0 (s, t) \u03b8c ), \u03c0 (s, t) \u03b8c = MLP\u03b8c(\u03c8 (s, t) zi ) (5)\nwhere MLP\u03b8c : RH\u03c8 \u2192 RN is a Lc-layered MLP that parameterizes node probabilities using community embeddings indexed by z\n(s, t) i . That is, each community assignment z (s, t) i is\nrepresented as a mixture of nodes. By integrating out the latent community assignment variable\np(c (s, t) i |w (s, t) i ) = \u2211 z (s, t) i \u2208[1:K] p\u03b8c(c (s, t) i |z (s, t) i )p\u03b8z(z (s, t) i |w (s, t) i ) (6)\nwe define the likelihood of node c (s, t) i being a linked neighbor of node w (s, t) i , in a given graph snapshot.\nFactorized generative model Given this model specification, the joint probability of the observed data and the latent variables can be factorized following\np\u03b8(G1:S 1:T , \u21261:S,1:T ) = S\u220f s=1 ( p\u03b8\u03b1(\u03b1 (s)) T\u220f t=1 ( V\u220f n=1 p\u03b8\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n )\nK\u220f k=1 p\u03b8\u03c8(\u03c8 (s,t) k |\u03c8 (s,t\u22121) k ) E(s, t)\u220f i=1 p\u03b8z(z (s, t) i |\u03c6 (s, t) wi )p\u03b8c(c (s, t) i |\u03c8 (s, t) zi ) )) (7)\nwhere \u03b8 = {\u03b8c , \u03b8z} is the set of generative model parameters, i.e., NN weights. The generative model of DBGDGM summarized in Appendix B"
        },
        {
            "heading": "3.2. Inference network",
            "text": "To learn the embeddings, we must infer the posterior distribution over all latent variables conditioned on the observed data p\u03b8(\u2126\n(1:S, 1:T )|G(1:S, 1:T )). However, exact inference is intractable due the log marginal likelihood requiring integrals that are hard to evaluate, i.e.,\nlog p\u03b8(G(1:S, 1:T )) = \u222b \u2126 log p\u03b8(G (1:S, 1:T ),\u2126(1:S, 1:T ))d\u2126. As a result, we use variational inference (Jordan et al., 1999) to approximate the true posterior with a variational distribution q\u03bb(\u2126\n(1:S,1:T )) with parameters \u03bb. To do this, we maximize a lower bound on the log marginal likelihood of the DBGs, referred to as the ELBO (evidence lower bound), defined as\nLELBO(\u03b8, \u03bb) = Eq\u03bb\n[ log\np\u03b8(G1:S, 1:T , \u21261:S, 1:T ) q\u03bb(\u2126(1:S, 1:T ))\n] \u2264 log p\u03b8(G(1:S, 1:T )) (8)\nwhere Eq\u03bb [\u00b7] denotes the expectation taken with respect to the variational distribution q\u03bb(\u2126\n(1:S, 1:T )). By maximizing the ELBO with respect to the generative and variational parameters \u03b8 and \u03bb we train our generative model and perform Bayesian inference, respectively.\nStructured variational distribution To ensure a good approximation to true posterior, we retain the Markov properties of the node and community embeddings. This results in a structured variational distribution (Hoffman and Blei, 2015; Saul and Jordan, 1995) which factorizes following\nq\u03bb(\u2126 (1:S, 1:T )) = S\u220f s=1 ( q\u03bb\u03b1(\u03b1 (s)) T\u220f t=1 ( V\u220f n=1 q\u03bb\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n )\nK\u220f k=1 q\u03bb\u03c8(\u03c8 (s, t) k |\u03c8 (s, t\u22121) k ) E(s, t)\u220f i=1 q\u03bbz(z (s, t) i |\u03c6 (s, t) wi , \u03c6 (s, t) ci )\n)) (9)\nwhere each distribution is specified to mimic the structure of the generative model so that\nq\u03bb\u03b1(\u03b1 (s)) = Normal(\u00b5 (s) \u03bb\u03b1 , \u03c3 (s) \u03bb\u03b1 ) (10)\nq\u03bb\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n ) = Normal(\u00b5 (s, t) \u03bb\u03c6 , \u03c3 (s, t) \u03bb\u03c6 ) {\u00b5(s, t)\u03bb\u03c6 , \u03c3 (s, t) \u03bb\u03c6 } = GRU\u03bb\u03c6(\u03c6 (s, t\u22121) n ) (11)\nq\u03bb\u03c8(\u03c8 (s, t) k |\u03c8 (s, t\u22121) n ) = Normal(\u00b5 (s, t) \u03bb\u03c8 , \u03c3 (s, t) \u03bb\u03c8 ) {\u00b5(s, t)\u03bb\u03c8 , \u03c3 (s, t) \u03bb\u03c8 } = GRU\u03bb\u03c8(\u03c8 (s, t\u22121) k ) (12)\nq\u03bbz(z (s, t) i |\u03c6 (s, t) wi , \u03c6 (s, t) ci ) = Categorical(\u03c0 (s, t) \u03bbz ) \u03c0 (s, t) \u03bbz = MLP\u03bbz(\u03c6 (s, t) wi \u03c6 (s, t) ci )\n(13)\nwhere GRU\u03bbj : RHj \u2192 RHj is a Lj-layered GRU for each j \u2208 {\u03c6, \u03c8} and MLP\u03bbz : RH\u03c6 \u2192 RK is Lz-layered MLP. Furthermore, we use MLPs to initialize the GRUs with the graph embeddings such that \u03c6\n(s, 0) n = MLP\u03bb\u03c6(\u03b1 (s)) and \u03c8 (s, 0) k = MLP\u03bb\u03c8(\u03b1 (s)) where\nMLP\u03bbj : RN\u03b1 \u2192 RNj . This allows for subject-specific variation to be incorporated in the temporal dynamics of the node and community embeddings. Another difference with the generative model is now the variational distribution of the community assignment q\u03bbz(\u00b7) includes information from neighboring nodes via c\n(s, t) i . Finally, we use the same NN from the\ngenerative model to parameterize the variational distribution of the community assignment, i.e., \u03bbz = \u03b8z. This not only spares additional trainable parameters for the variational distribution but also further links the variational parameters of q\u03bb(\u00b7) to generative parameters of p\u03b8(\u00b7) resulting in more robust learning (Farnoosh and Ostadabbas, 2021). The set of parameters for the inference network is therefore \u03bb = {\u03bb\u03b1 = {\u00b5(s)\u03bb\u03b1 , \u03c3 (s) \u03bb\u03b1 }Ss=1, \u03bb\u03c6, \u03bb\u03c8, \u03bbz = \u03b8z}.\nTraining objective Substituting the variational distribution from (9) and the joint distribution from (7) into the ELBO (8) gives the full training objective which can be optimized using stochastic gradient descent. We estimate all gradients using the reparameterization trick (Kingma and Welling, 2013) and the Gumbel-softmax trick (Jang et al., 2016; Maddison et al., 2016). We refer to Appendix B further details on the ELBO and learning the parameters."
        },
        {
            "heading": "4. Experiments",
            "text": "We evaluate DBGDGM against baseline models on the tasks of graph reconstruction, dynamic link prediction, and graph classification. Each task is designed to evaluate the usefulness of the learnt embeddings.\nDatasets We construct two multi-subject DBG datasets using publicly available fMRI scans from the Human Connectome Project (HCP) (Van Essen et al., 2013) and UK Biobank (UKB) (Sudlow et al., 2015). We randomly sample S = 300 subjects ensuring an even male/female split. To create DBGs, we parcellate each scan into V = 360 region-wise BOLD signals using the Glasser atlas (Glasser et al., 2016), apply sliding-window Pearson correlation (Calhoun et al., 2014) with a non-overlapping window of size and stride of 30, and threshold the top 5% values of the lower triangle of each correlation matrix as connected following Kim et al. (2021). The described procedure gives T = 16 graph snapshots for each subject. Biological sex is taken as graph-level labels. We refer to Appendix C for further details on each dataset.\nBaselines We compare DBGDGM against a range of different unsupervised probabilistic baseline models. For static baselines, we include variational graph autoencoder (VGAE) (Kipf and Welling, 2016b), a deep generative version of the overlapping stochastic block model (OSBM) (Mehta et al., 2019), and vGraph (VGRAPH) (Sun et al., 2019). For dynamic baselines we include variational graph recurrent neural network (VGRNN) (Hajiramezanali et al., 2019) and evolving latent space model (ELSM) (Gupta et al., 2019). For the graph reconstruction and link prediction tasks, we also include a heuristic baseline based on common neighbors between nodes at previous snapshots (CMN). Finally, for graph classification we include a support vector machine which takes as import static FC matrices (FCM) (Abraham et al., 2017). Further details about baseline model can be found in Appendix D.\nImplementation We split both datasets into 80/10/10% training/validation/test data along the time dimension. We train all models using the Adam optimizer (Kingma and Ba, 2014) with decoupled weight decay (Loshchilov and Hutter, 2017). All baseline hyperparameters are set following their original implementations. For DBGDGM, choose the number of communities K based on validation NLL. Finally, we train all models 5 times using different random seeds. Implementation details can be found in Appendix E.\nEvaluation metrics For graph reconstruction, we evaluate the probability of the edges in the test dataset using negative log-likelihood (NLL). We also compare the mean-squared error (MSE) between actual and reconstructed node degree over all test snapshots. For dynamic link prediction, we sample an equal number of positive and negative edges in the test dataset and measure performance using area under the receiver operator curve (AUROC) and average precision (AP). Finally, for graph classification we predict the biological sex for each subjects\u2019 DBG and evaluate on accuracy. To predict graph labels, we average node embeddings per subject for the baselines and the community embeddings for DBGDGM before training a SVM using 10-fold cross-validation. For comparing models, we use the almost stochastic order (ASO) test (Dror et al., 2019) with significance level 0.05 and correct for multiple comparisons (Bonferroni, 1936)."
        },
        {
            "heading": "5. Results",
            "text": "Dynamic graph reconstruction and link prediction. We summarize the average test results of all models over 5 runs using optimally tuned hyperparameters. From Table 1, it is clear that DBGDGM outperforms baselines on both tasks. For graph reconstruction, DBGDGM shows an 18% and 30% relative improvement in NLL on HCP and UKB, respectively, compared to the second-best baselines. For dynamic link prediction, the relative improvement is > 11% in AUCROC and > 5% in AP compared to second-best baselines depending on dataset. We attribute these statistically significant gains to DBGDGM\u2019s ability to learn dynamic brain connectivity more effectively.\nGraph classification For graph classification, DBGDGM achieves \u223c 75% accuracy for HCP and \u223c 73% for UKB (see Fig. 2). We outperform 4 baselines and show indiscernible performance to VGAE and OSBM. To show the interpretative power of DBGDGM, we rerun the graph classification experiment for HCP with the embeddings of each community separately. We find a community which comprises brain regions in the Cingulo-opercular\n(CON) and the Somatomotor (SMN) networks, which achieves 68% accuracy. This finding is in agreement with studies that show SMN is predictive of gender (Zhang et al., 2018).\nInterpretability analysis We use the learnt distributions over the nodes to calculate overlap between each community and known functional connectivity networks (FCNs) from Ji et al. (2019) (see Appendix F). Figure 3 shows that DBGDGM finds communities that significantly overlap with existing FCNs. In particular, nodes in community 1 almost fully corresponds to the visual network (VIS1 + VIS2), which is in keeping with the nature of the experiment (the resting state data was acquired with eyes open and cross-hair fixation). Remarkably, the second and third most homogeneous communities correspond to a large degree to the DMN, which is well known to dominate resting state activity as a whole (Yeshurun et al., 2021). The inspection of additional communities and respective predictive power, along with their evolution in time at the region-of-interest granularity, has the potential to unveil the yet largely unexplored relationships between dynamic brain connectivity changes and, e.g. psychiatric or neurological disorders (Heitmann and Breakspear, 2017)."
        },
        {
            "heading": "6. Conclusion",
            "text": "We propose DBGDGM, a hierarchical DGM designed for unsupervised representing learning of DBGs. Specifically, DBGDGM jointly learns graph-, community-, and node-level embeddings that outperform baselines on classification, interpretability, and dynamic link prediction with statistical significance. Moreover, an analysis of the learnt dynamic communitynode distributions shows significant overlap with existing FCNs from neuroscience literature further validating our method."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work is supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. Data were provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University."
        },
        {
            "heading": "Appendix A. Related work",
            "text": "Dynamic graph generative models Classic generative models for graph-structured data are designed for capturing a small set of specific properties (e.g., degree distribution, eigenvalues, modularity) of static graphs (Erdos et al., 1960; Baraba\u0301si and Albert, 1999; Nowicki and Snijders, 2001). DGMs that exploit the learning capacity of NNs are able to learn more expressive graph distributions (Mehta et al., 2019; Kipf and Welling, 2016b; Sarkar et al., 2020). Recent DGMs for dynamic graphs are majority VAE-based (Kingma and Welling, 2013) and cannot learn community representations (Hajiramezanali et al., 2019; Gracious et al., 2021; Zhang et al., 2021). The few that do, are designed for static graphs (Sun et al., 2019; Khan et al., 2021; Cavallari et al., 2017).\nLearning representations of dynamic brain graphs Unsupervised representation learning methods for DBGs tend to focus on clustering DBGs into a finite number of connectivity patterns that recur over time (Allen et al., 2014; Spencer and Goodfellow, 2022). Community detection is another commonly used method but mainly applied to static brain graphs (Pavlovic\u0301 et al., 2020; Esfahlani et al., 2021). Extensions to DBGs are typically not end-to-end trainable and do not scale to multi-subject datasets (Ting et al., 2020; Martinet et al., 2020). Recent deep learning-based methods are predominately GNN-based (Kim et al., 2021; Dahan et al., 2021). Unlike DBGDGM, these methods are supervised and focus on learning deterministic node- and graph-level representations."
        },
        {
            "heading": "Appendix B. Method",
            "text": "B.1. Generative model\nAlgorithm 1 summarizes the generative model for DBGDGM.\nAlgorithm 1: DBGDGM generative model\nInput: {E(s, t)}S, Ts, t=1 Hyperparameters: K, H\u03b1, H\u03c8, H\u03c6, L\u03c8, L\u03c6, Lz, \u03c3 2 \u03c8, \u03c3 2 \u03c6, Initialize: D \u2190 \u2205 for s\u2190 1 to S do\n\u03b1(s) \u223c p(\u03b1(s)) = Normal(0H\u03b1 , IH\u03b1) for t\u2190 1 to T do\nfor k \u2190 1 to K do \u03c8\n(s,t) k \u223c p(\u03c8 (s, t) k |\u03c8 (s, t\u22121) k ) = Normal(\u03c8 (s, t\u22121) k , \u03c3\u03c8IH\u03c8)\nend for n\u2190 1 to V do\n\u03c6 (s,t) n \u223c p(\u03c6(s, t)n |\u03c6(s, t\u22121)n ) = Normal(\u03c6(s, t\u22121)k , \u03c3\u03c6IH\u03c6)\nend E\u0303(s, t) \u2190 \u2205 for i\u2190 1 to |E(s, t)| do\nz (s, t) i \u223c p(z (s, t) i |w (s, t) i ) = Categorical(f\u03b8\u03c0(\u03c6 (s, t) wi )) c (s, t) i \u223c p(c (s, t) i |z (s, t) i ) = Categorical(f\u03b8\u03c0(\u03c8 (s, t) zi )) E\u0303(s, t) \u2190 E\u0303(s, t) \u222a {(w(s, t)i , c (s, t) i )}\nend G(s, t) \u2190 (V, E\u0303(s, t)) D \u2190 D \u222a {G(s, t)}\nend\nend\nB.2. Training objective and learning the parameters\nSubstituting the variational distribution from (9) and the joint distribution from (7) into the ELBO (8) gives the full training objective defined as\nLELBO(\u03b8, \u03bb) = S\u2211 s=1 T\u2211 t=1 E(s, t)\u2211 i=1 ( Eq\u03bbz q\u03bb\u03c8 [ log p\u03b8(c (s, t) i |w (s, t) i , \u03c8 (s, t) zi ) ] \u2212 Eq\u03bb\u03c6 [ DKL[q\u03bbz(z (s, t) i |\u03c6 (s, t) wi , \u03c6 (s, t) ci )||p\u03b8z(z (s, t) i |\u03c6 (s, t) wi )]\n]) \u2212\nS\u2211 s=1 ( DKL[q\u03bb\u03b1(\u03b1 (s))||p\u03b8\u03b1(\u03b1(s))] T\u2211 t=1 ( (14)\n\u2212 V\u2211 n=1 Eq\u03bb\u03c6 [ DKL[q\u03bb\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n )||p\u03b8\u03c6(\u03c6 (s, t) n |\u03c6(s, t\u22121)n )] ] \u2212\nK\u2211 k=1 Eq\u03bb\u03c8 [ DKL[q\u03bb\u03c8(\u03c8 (s, t) k |\u03c8 (s, t\u22121) k )||p\u03b8\u03c8(\u03c8 (s, t) k |\u03c8 (s, t\u22121) k )]\n]))\nwhere DKL[\u00b7||\u00b7] denotes the Kullback-Leibler (KL) divergence. By maximizing (14), the parameters (\u03b8, \u03bb) of the generative model and inference network can be jointly learnt.\nLearning the parameters In order to use efficient stochastic gradient-based optimization techniques (Robbins and Monro, 1951) for learning (\u03b8, \u03bb), the gradient of the ELBO has to be estimated. The main challenge of this is obtaining gradients of the variables under expectation, i.e., Eq\u2217 [\u00b7], since they are sampled. To allow gradients to flow through these sampling steps, we use the reparameterization trick (Kingma and Welling, 2013; Rezende et al., 2014) for the normal distributions and the Gumbel-softmax trick (Jang et al., 2016; Maddison et al., 2016) for the categorical distributions. All gradients are now easily computed via back-propagation (Rumelhart et al., 1986) making DBGDGM end-to-end trainable. In addition, we analytically calculate the KL terms for both normal and categorical distributions, which leads to lower variance gradient estimates and faster training as compared to noisy Monte Carlo estimates."
        },
        {
            "heading": "Appendix C. Datasets",
            "text": "To create multi-subject DBG datasets, we use real fMRI scans from the UK Biobank (Sudlow et al., 2015) and Human Connectome Project (Van Essen et al., 2013). Both data sources represent well-characterized population cohorts that have undergone standardized neuroimaging and clinical assessments to ensure high quality.\nUK Biobank1 (UKB) The UKB dataset consists of S = 300 resting-rate fMRI scans (i.e. 3D image of the brain taken over consecutive timepoints) randomly sampled from the v1.3 January 2017 release ensuring an equal male/female split (i.e. sex balanced) with an age range of 44\u2212 57 years. The total number of images for each scan is 490 timepoints (6 minutes duration with a repetition time of 0.74s). The dataset is minimally preprocessed following the pipeline described in Alfaro-Almagro et al. (2018).\nHuman Connectome Project2 (HCP) The HCP dataset similarly consists of S = 300 sex balanced resting-state fMRI scans randomly sampled from the S1200 release with an age range of 22 \u2212 35 years. Only images from the first scanning-session using left-right phase encoding are used. The total number of images for each scan is 1, 200 timepoints (15 minutes duration with a repetition time of 0.72s). The dataset is minimally preprocessed following the pipeline described in Glasser et al. (2013)\nFurther preprocessing The fMRI scans from each dataset are further preprocessed to create DBGs. Firstly, each scan is transformed into a multivariate timeseries of BOLD signals using the Glasser atlas (Glasser et al., 2016) to average voxels within V = 360 brain regions. Next, to ensure comparability with UKB, we truncate the length of HCP timeseries to 490 timepoints. Following the commonly used sliding-window method (Calhoun et al., 2014), we use Pearson correlation to calculate FC matrices within non-overlapping windows of length 1 < W \u2264 490 along the temporal dimension. At every window, we create an edge set of a unweighted and undirected graph with no self-edges by thresholding the top 1 \u2264 < 100 percentile values of the lower triangle of the FC matrix (excluding the principal\n1. https://www.ukbiobank.ac.uk 2. https://www.humanconnectome.org\ndiagonal) as connected following Kim et al. (2021). For both datasets, we choose W = 30 and = 5 resulting in T = b490/30c = 16 graph snapshots each with E(s, t) = b(360(360\u2212 1)/2)(5/100)c = 3, 231 edges."
        },
        {
            "heading": "Appendix D. Baselines",
            "text": "We compare DBGDGM against a range of static and dynamic unsupervised graph representation learning baseline models, all with publicly available code. In particular, we focus on baselines that are generative and can quantify uncertainty. We leave comparisons to popular deterministic baselines such as DynamicTriad (Zhou et al., 2018), DySAT (Sankar et al., 2020), and DynNode2Vec (Mahdavi et al., 2018) for future work. Furthermore, since all of the baselines were originally designed to model large single-graph datasets, we had to adapt each implementation to work with smaller multi-graph datasets.\nVariational graph auto encoder3 (VGAE) (Kipf and Welling, 2016b) An extension of the variational autoencoder (Kingma and Welling, 2013) (VAE) for graph structured data. Specifically, VGAE uses a graph convolutional network (GCN) (Kipf and Welling, 2016a) to learn a distribution over node embeddings. Originally designed for static graphs, we train VGAE on each dynamic graph snapshot independently.\nOverlapping stochastic block model4 (OSBM) (Mehta et al., 2019) A deep generative version of the overlapping stochastic block model (Miller et al., 2009). In particular, OSBM places a stick-breaking prior over the number of communities which allows the model to automatically infer the optimal number of communities from the data during training. Similar to VGAE, OSBM uses a GCN to parameterize the distribution over node embeddings and is designed for static graphs.\nVariational graph RNN5 (VGRNN) (Hajiramezanali et al., 2019) An extension of VGAE for dynamic graphs. Using a modified graph RNN architecture, VGRNN is able to learn dependencies between and within changing graph topology over time. Similar to DBGDGM, the prior distribution over node embeddings is parameterized using hidden states from previous timepoints.\nEvolving latent space model6 (ELSM) (Gupta et al., 2019) A generative model for dynamic graphs that learns node embeddings and performs community detection. In particular, node embeddings are initially sampled from a Gaussian mixture model over communities and then evolved over time using an LSTM. Unlike the previous baselines, ELSM does not use a GNNs to parameterize model distributions.\nvGraph7 (VGRAPH) (Sun et al., 2019) Similar to DBGDGM, VGRAPH simultaneously learns node embeddings and community assignments by modeling nodes as being\n3. https://github.com/tkipf/gae 4. https://github.com/nikhil-dce/SBM-meet-GNN 5. https://github.com/VGraphRNN/VGRNN 6. https://github.com/sh-gupta/ELSM 7. https://github.com/fanyun-sun/vGraph\ngenerated from a mixture of communities. The generative process of VGRAPH also relies on edge information. Since VGRAPH only models static graphs, we train it on each dynamic graph snapshot independently.\nCommon neighbors (CMN) In light of recent work demonstrating that heuristic methods are able to outperform deep-learning based models on dynamic link prediction tasks (Skarding et al., 2022; Poursafaei et al., 2022), we include our own heuristic-based generative model baseline. More formally, let \u03c0 (t) vi \u2208 [0, 1]V denote a vector of Jaccard index scores for node v (t) i \u2208 V with all other nodes v (t) j \u2208 V for i 6= j. The Jaccard index between two nodes v (t) i , v (t) j \u2208 V is defined |\u0393(v (t) i ) \u2229 \u0393(v (t) j )|/|\u0393(v (t) i ) \u222a \u0393(v (t) j )| where \u0393(v (t) i ) denotes the set of neighbors of node v (t) i . We define the probability of node v (t) i having a linked neighbor v (t) j at snapshot t as\np(v (t) j |v (t) i ) = Categorical(\u03c0 (t\u22121) vi ). (15)\nThis simple generative model captures the intuition that nodes are more likely to form links if they had common neighbors in a previous snapshot."
        },
        {
            "heading": "Appendix E. Implementation details",
            "text": "Software and hardware All models are developed in Python 3.7 (Python Core Team, 2019) using scikit-learn 1.1.1 (Pedregosa et al., 2011), PyTorch(Paszke et al., 2019), and numpy 1.1.1 (Harris et al., 2020). Statistical significance tests are carried out using deepsignificance 1.1.1 (Ulmer et al., 2022). Experiments are performed on a Linux server (Debian 5.10.113-1) with a NVIDIA RTX A6000 GPU with 48 GB memory and 16 CPUs.\nTraining and testing All baselines are implemented as per the original paper and/or code repository given in Appendix D. For the static graph baselines VGAE, OSBM, VGRAPH we train on each snapshot independently and use the node and/or community embeddings at the last training snapshot to make predictions.\nHyperparameter optimization We use model and training hyperparameter values described in the original implementation of each baseline as a starting point for tuning on the validation dataset. Since searching for optional values for each hyperparameter configuration was outside the scope of this paper, we focus mainly on tuning the dimensions of hidden layers. For DBGDGM, we use a learning rate of 1e-4 with a weight decay of 0. We choose the number of communities K \u2208 {3, 6, 8, 12, 16, 24} based on lowest average validation NLL (see Figure 4). In the generative model, we fix the temporal smoothness hyperparameters \u03c3\u03c6 = \u03c3\u03c8 = 0.01. In the inference network, we fix the number of layers for all NNs to L\u03c6 = L\u03c8 = Lz = 1. For the Gumbel-softmax reparameterization trick we anneal the softmax temperature parameter starting from a maximum of 1 to a minimum of 0.05 at a rate of 3e-4. Finally, we train all models for 1, 000 epochs using early-stopping with a patience of 15 based on the lowest validation NLL."
        },
        {
            "heading": "Appendix F. Interpretability analysis",
            "text": "Using DBGDGM, for each community we average the node distributions across subjects and timepoints and take the top 10% most probable nodes. We use these high probability\nnodes to calculate overlap between each community and the brain regions that comprise each functional network from Ji et al. (2019). More specifically, the coloured proportions in Figure 3 represent the proportion of top nodes in each community, which belong to a given functional network."
        }
    ],
    "title": "DBGDGM: Dynamic Brain Graph Deep Generative Model",
    "year": 2023
}