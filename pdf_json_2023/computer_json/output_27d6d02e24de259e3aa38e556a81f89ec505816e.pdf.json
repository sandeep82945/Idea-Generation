{
    "abstractText": "In the real world, knowledge often exists in a multimodal and heterogeneous form. Addressing the task of question answering with hybrid data types, including text, tables, and images, is a challenging task (MMHQA). Recently, with the rise of large language models (LLM), incontext learning (ICL) has become the most popular way to solve QA problems. We propose MMHQA-ICL framework for addressing this problems, which includes stronger heterogeneous data retriever and an image caption module. Most importantly, we propose a Type-specific In-context Learning Strategy for MMHQA, enabling LLMs to leverage their powerful performance in this task. We are the first to use end-to-end LLM prompting method for this task. Experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Weihao Liu"
        },
        {
            "affiliations": [],
            "name": "Fangyu Lei"
        },
        {
            "affiliations": [],
            "name": "Tongxu Luo"
        },
        {
            "affiliations": [],
            "name": "Jiahe Lei"
        },
        {
            "affiliations": [],
            "name": "Shizhu He"
        },
        {
            "affiliations": [],
            "name": "Jun Zhao"
        },
        {
            "affiliations": [],
            "name": "Kang Liu"
        }
    ],
    "id": "SP:c125658b61ef91cf3a029e96e9b739d850d58c6d",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Vqa: Visual question answering",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433.",
            "year": 2015
        },
        {
            "authors": [
                "Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen."
            ],
            "title": "Large language models are few (1)-shot table reasoners",
            "venue": "arXiv preprint arXiv:2210.06710.",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hexiang Hu",
                "Xi Chen",
                "Pat Verga",
                "William W Cohen."
            ],
            "title": "Murag: Multimodal retrieval-augmented generator for open question answering over images and text",
            "venue": "arXiv preprint arXiv:2210.02928.",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hanwen Zha",
                "Zhiyu Chen",
                "Wenhan Xiong",
                "Hong Wang",
                "William Yang Wang."
            ],
            "title": "Hybridqa: A dataset of multi-hop question answering over tabular and textual data",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Zhoujun Cheng",
                "Tianbao Xie",
                "Peng Shi",
                "Chengzu Li",
                "Rahul Nadkarni",
                "Yushi Hu",
                "Caiming Xiong",
                "Dragomir Radev",
                "Mari Ostendorf",
                "Luke Zettlemoyer"
            ],
            "title": "Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui."
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234.",
            "year": 2022
        },
        {
            "authors": [
                "Darryl Hannan",
                "Akshay Jain",
                "Mohit Bansal."
            ],
            "title": "Manymodalqa: Modality disambiguation and qa over diverse inputs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7879\u20137886.",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen."
            ],
            "title": "DeBERTav3: Improving deBERTa using ELECTRAstyle pre-training with gradient-disentangled embedding sharing",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Fangyu Lei",
                "Shizhu He",
                "Xiang Li",
                "Jun Zhao",
                "Kang Liu."
            ],
            "title": "Answering numerical reasoning questions in table-text hybrid contents with graph-based encoder and tree-based decoder",
            "venue": "arXiv preprint arXiv:2209.07692.",
            "year": 2022
        },
        {
            "authors": [
                "Fangyu Lei",
                "Xiang Li",
                "Yifan Wei",
                "Shizhu He",
                "Yiming Huang",
                "Jun Zhao",
                "Kang Liu"
            ],
            "title": "S3hqa: A three-stage approach for multi-hop text-table hybrid question answering",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "ArXiv, abs/2304.08485.",
            "year": 2023
        },
        {
            "authors": [
                "Pengfei Liu",
                "Weizhe Yuan",
                "Jinlan Fu",
                "Zhengbao Jiang",
                "Hiroaki Hayashi",
                "Graham Neubig."
            ],
            "title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "venue": "ACM Computing Surveys, 55(9):1\u201335.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language",
            "year": 2015
        },
        {
            "authors": [
                "Jack W Rae",
                "Sebastian Borgeaud",
                "Trevor Cai",
                "Katie Millican",
                "Jordan Hoffmann",
                "Francis Song",
                "John Aslanides",
                "Sarah Henderson",
                "Roman Ring",
                "Susannah Young"
            ],
            "title": "Scaling language models: Methods, analysis & insights from training gopher",
            "year": 2021
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100,000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392.",
            "year": 2016
        },
        {
            "authors": [
                "Revanth Gangi Reddy",
                "Xilin Rui",
                "Manling Li",
                "Xudong Lin",
                "Haoyang Wen",
                "Jaemin Cho",
                "Lifu Huang",
                "Mohit Bansal",
                "Avirup Sil",
                "Shih-Fu Chang"
            ],
            "title": "Mumuqa: Multimedia multi-hop news question answering via cross-media knowledge extraction",
            "year": 2021
        },
        {
            "authors": [
                "Hrituraj Singh",
                "Anshul Nasery",
                "Denil Mehta",
                "Aishwarya Agarwal",
                "Jatin Lamba",
                "Balaji Vasan Srinivasan."
            ],
            "title": "Mimoqa: Multimodal input multimodal output question answering",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the",
            "year": 2021
        },
        {
            "authors": [
                "Alon Talmor",
                "Ori Yoran",
                "Amnon Catav",
                "Dan Lahav",
                "Yizhong Wang",
                "Akari Asai",
                "Gabriel Ilharco",
                "Hannaneh Hajishirzi",
                "Jonathan Berant."
            ],
            "title": "Multimodalqa: complex question answering over text, tables and images",
            "venue": "International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Romal Thoppilan",
                "Daniel De Freitas",
                "Jamie Hall",
                "Noam Shazeer",
                "Apoorv Kulshreshtha",
                "Heng-Tze Cheng",
                "Alicia Jin",
                "Taylor Bos",
                "Leslie Baker",
                "Yu Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "An Yang",
                "Rui Men",
                "Junyang Lin",
                "Shuai Bai",
                "Zhikang Li",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang."
            ],
            "title": "Unifying architectures, tasks, and modalities through a simple sequenceto-sequence learning framework",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Yingyao Wang",
                "Yongwei Zhou",
                "Chaoqun Duan",
                "Junwei Bao",
                "Tiejun Zhao."
            ],
            "title": "Hoppg: Self-iterative program generation for multi-hop question answering over heterogeneous knowledge",
            "venue": "arXiv preprint arXiv:2308.11257.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Huai hsin Chi",
                "F. Xia",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "ArXiv, abs/2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Wei",
                "Fangyu Lei",
                "Yuanzhe Zhang",
                "Jun Zhao",
                "Kang Liu."
            ],
            "title": "Multi-view graph representation learning for answering hybrid numerical reasoning question",
            "venue": "arXiv preprint arXiv:2305.03458.",
            "year": 2023
        },
        {
            "authors": [
                "Qian Yang",
                "Qian Chen",
                "Wen Wang",
                "Baotian Hu",
                "Min Zhang."
            ],
            "title": "Enhancing multi-modal and multi-hop question answering via structured knowledge and unified retrieval-generation",
            "venue": "arXiv preprint arXiv:2212.08632.",
            "year": 2022
        },
        {
            "authors": [
                "Yunhu Ye",
                "Binyuan Hui",
                "Min Yang",
                "Binhua Li",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
            "venue": "arXiv preprint arXiv:2301.13808.",
            "year": 2023
        },
        {
            "authors": [
                "Ori Yoran",
                "Alon Talmor",
                "Jonathan Berant."
            ],
            "title": "Turning tables: Generating examples from semistructured tables for endowing language models with reasoning skills",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Tao Yu",
                "Rui Zhang",
                "Kai Yang",
                "Michihiro Yasunaga",
                "Dongxu Wang",
                "Zifan Li",
                "James Ma",
                "Irene Li",
                "Qingning Yao",
                "Shanelle Roman"
            ],
            "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql",
            "year": 2018
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Hai Zhao",
                "George Karypis",
                "Alex Smola."
            ],
            "title": "Multimodal chain-of-thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2302.00923.",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Zhao",
                "Yunxiang Li",
                "Chenying Li",
                "Rui Zhang."
            ],
            "title": "MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Fengbin Zhu",
                "Wenqiang Lei",
                "Youcheng Huang",
                "Chao Wang",
                "Shuo Zhang",
                "Jiancheng Lv",
                "Fuli Feng",
                "Tat-Seng Chua."
            ],
            "title": "Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance",
            "venue": "Proceedings of the 59th Annual Meet-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Question answering systems are designed to answer various questions using evidence located in structured knowledge bases (e.g., tables) (Pasupat and Liang, 2015; Yu et al., 2018) or unstructured texts (Rajpurkar et al., 2016), as well as images (Antol et al., 2015). Considering that many questions need to utilize multiple sources of knowledge jointly in real-world applications, the hybrid form of multimodal hybrid question answering over text, tables and images (MMHQA) has been proposed and attracted more and more attention (Chen et al., 2020; Zhu et al., 2021; Talmor et al., 2020; Lei et al., 2022).\nMMHQA is a challenging task that involves integrating information from various sources with different modals, such as text, tables, and images, to answer questions. This task requires not only the ability to retrieve evidence from multiple modalities but also the capability to use multi-hop reason-\ning to arrive at the correct answer. As shown in the Figure 1, there is a highly complex question. To answer the final question \"which of the states has hosted the event fewer times?\", we first need to use the image information to answer \"Whose flag features the moon and a palm tree?\" and the text information to answer \"Where was the movie \u2019Bite the Bullet\u2019 filmed?\". After we obtain the corresponding intermediate answers, including \u201cSouth Carolina\u201d and \u201cNevada\u201d, we then use the information in the table to compare the \"hosted times\" of these two states and arrive at the final answer, i.e., \"South Carolina\". This example demonstrates the challenge and complexity of the MMHQA task, which requires models to have the ability to fuse multimodal information and perform multiple rounds of reasoning to complete the task.\nBefore the advent of Large Language Models (LLMs) (Chowdhery et al., 2022; Rae et al., 2021; Thoppilan et al., 2022), prior works struggled to decompose questions into some sub-questions and solve them step by step, which often leads to the information loss among the steps. While some works (Chen et al., 2022; Yang et al., 2022) attempt to use visual language models to obtain answers directly, these adhoc models require a welltrained multimodal model and extensive fine-tuning on downstream datasets. More importantly, these models lack vital ability to multi-hop reasoning and cannot be easily adopted to other datasets.\nWith the evolution of large language models (LLMs) taken place, question answering systems have gotten phenomenal improvements. However, LLMs lack the ability to deal with crossmodal information like images or tables. Motivated by this observation, it has been more and more attractive to study how to excavate the powerful semantic understanding and logical reasoning capabilities of LLMs on cross-modal task. Binder (Cheng et al., 2022) develops a new text-toSQL pipeline that mainly utilizes Codex (Brown\nar X\niv :2\n30 9.\n04 79\n0v 1\n[ cs\n.C L\n] 9\nS ep\n2 02\n3\net al., 2020) as an SQL generator to transform questions into formal language. However, this approach often fails to answer questions due to SQL syntax errors. Moreover, their method is primarily designed for tables and performs poorly on examples that contain images.\nTo this end, this paper proposes a MMHQAICL framework to solve such MMHQA task. Compared to the fully supervised training approach, our method is more concise. Considering that LLMgenerated SQL queries often contain syntax errors (Cheng et al., 2022), our framework adopts an end-to-end approach that generates the answer directly without generating intermediate SQL queries. To process images, we propose a LLaVA-based Premium Captioning Module that endows a framework with powerful retrieval and reasoning abilities for image data by generating more semantically abundant image captions. In addition, to relieve the limitation of the number of input tokens and boost the capacity of ICL, we propose a Type-specific In-context Learning Strategy which allows for different prompting strategies to be selected based on the type of question. Experiments demonstrate that our method significantly outperforms previous methods.\nIn summary, our contributions are as follows: (1) We are the first to utilize an end-to-end QA method in three-modality hybrid question answering system with SOTA performance.\n(2) We propose a LLaVA based Premium Captioning Module, which enhances the quality of\nimage captioning, making it more capable for retrieval and reasoning question.\n(3) We introduce a Type-specific In-context Learning Strategy to make LLM more adaptive to different questions, despite the limitation of the amount of input tokens."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 Task Definition",
            "text": "Given a MultimodalQA task with question and data source in format T = {Q,D}, where D is constructed of I,P and T , representing the threemodality data source (image, passage and table), respectively, MMQA aims to synthesize the information from different data sources and make inferences to give the correct answer A to the question ultimately."
        },
        {
            "heading": "2.2 Framework Overview",
            "text": "As shown in Figure 2, we first generate a caption for each image and format the table rows by using tabs so that every data with different modalities can be regarded as texts. Then we put all these questions and texts into a Prompt Generator Module. Subsequently, we gain the input and thus generating final answer of the question through LLM."
        },
        {
            "heading": "2.3 LLaVA-based Premium Captioning",
            "text": "This module is designed to generate textual descriptions of images that can be used as evidence for answering questions in a novel way.\nTraditional image captioning models take an image as input to a transformer-based encoder to get the embedding, then send it into a decoder to generate a captioning text. For instance, prior works over MMQA dataset like binder use a vision-text pretrained model OFA (Wang et al., 2022) to generate image captions. However, its evaluation metrics are limited to match-based methods such as BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), leading into a poor and tedious text which may contain little useful evidence (e.g., \"Plymouth Argyle F.C.\" \u2192 \"the logo of the fundraiser\").\nAnyway, considering that text-based multimodal question answering systems need a high-quality summary of the exact image, we wish for image captions including more features that are easy to draw people\u2019s attention, like colors, characters, costumes, movements, main objects, etc. Consequently, we introduce a LLaVA-based Premium Captioning Module which can generate rich caption texts through LLaVA (Liu et al., 2023a). LLaVA is a large Language and Vision Assistant fine-tuned from LLaMA using Visual Instruction Tuning. For example, the picture containing a palm tree in Figure 1 is captioned into text \"The image features a blue flag with a white palmetto tree on it, which represents the state of South Carolina.\""
        },
        {
            "heading": "2.4 Classifier and Retriever",
            "text": "In the Classifier and Retriever (CR) Module, we use DeBERTa-large (He et al., 2023) as the encoder.\nFor classifier, unlike the baseline which classifies all questions into 16 possible types, we merely need four types (image, text, table and compose). We classify all questions through their question text, and finally the accuracy is 96.07% over the validation dataset.\nFor retriever, we train two separate models for image and passage. Since images are captioned into texts, we can refer to both images and passages that may be related to a question as documents. Therefore, we can describe the question Q and related documents doc as several (Q, doc) pairs. Assuming there are Ki related documents for the ith question Qi, our goal is to give each (Qi, docij) pair a score Sj to evaluate their correlation. Such process for Qi can be formulated as:\n(S1,S2, . . . ,SKi) = DeBERTa(Vi1, Vi2, . . . , ViKi)\nwhere Vij represents (Qi, docij) and DeBERTa means the DeBERTa-large model. In detail, the Vij is the concatenation of the question, the title and content of the document (e.g.,[CLS]Is it clear or rainy in durban?[SEP]Durban[SEP]The image depicts a lively beach scene with a group of people enjoying their time near the ocean.[SEP]).\nDuring training, if there are n gold documents, then the label yi should be a vector of length Ki:\nyij =\n{ 1 n , if docij is gold document\n0, else\nOur loss function Li for the i-th question based on CrossEntropy loss can be calculated as follows:\nLi = CrossEntropy(yi, y\u0302i)\nwhere y\u0302i indicates the output (S1,S2, . . . ,SKi) of DeBERTa model .\nDuring evaluation, we take the top-3 of S documents and calculate the recall to get a better retriever model."
        },
        {
            "heading": "2.5 Prompt Generator Module",
            "text": "Type-specific ICL Strategy. Our work leveraged Chain-of-Thought (CoT) (Wei et al., 2022) for all questions at the beginning. However, in subsequent experiments, we found that this technology is not universally applicable. Specifically, CoT has a good effect in single-modal tabular questions (TableQ) and multi-modal compose question answering (Compose). But in the task of the singlemodal image (ImageQ) or text (TextQ) question answering, this technology does not show a strong effect, and even worse.\nAfter analyzing, we speculated that this is due to the greater emphasis on information recognition and extraction capabilities in single-modal text and image questioning, without the need to generate an explicit chain of reasoning. Meanwhile, due to the limitation of the number of tokens, a single example will consume more tokens when using CoT, resulting in a sharp decrease in the number of examples available for in-context learning, which cannot stimulate the ability of LLM to deal with such problems. Furthermore, the use of CoT would increase inference time exponentially, leading to a low system efficiency.\nThus, we concluded that not all types of questions are suitable for using the chain of thought technology and we introduced a type-specific prompt generator, which selects different prompt examples depending on the question type for incontext learning. As an example, in the TableQ and cross-modal questions, we use CoT to construct examples. With such an approach, we achieve state-of-the-art performance on the MMQA dataset, showing that type-specific prompt generation can enable LLM to better solve different types of questions. The prompt generation algorithm can be summarized as follows:\nAlgorithm 1 Prompt Generator Algorithm. Input: questionQ, question type type, formatted table T , re-\ntrieved passages P , retrieved image captions I, examples file Demos Output: Type-specific prompt TP 1: Generate specific prompt for different question types 2: TPdemo \u2190 get_demo_by_type(Demos, type) 3: TPcot \u2190 \u201dPlease answer the question step by step.\u201d 4: TPnocot \u2190 \u201dAnswer : \u201d 5: if type = image then 6: TPs \u2190 Q+ I + TPnocot 7: else 8: if type = text then 9: TPs \u2190 Q+ P + TPnocot 10: else 11: if type = table then 12: TPs \u2190 Q+ T + TPcot 13: else 14: TPs \u2190 Q+ I + P + T + TPcot 15: end if 16: end if 17: end if 18: TP \u2190 TPdemo + TPs"
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Experiment Setup",
            "text": "Dataset We conduct our experiments over MultimodalQA (Talmor et al., 2020) dataset. This dataset contains human-annotated multimodal and multi-hop questions over different modalities including tables, text, and images. Wikipedia tables are used as anchors to connect different modalities. The authors first use the template to generate questions and then ask crowd-workers to filter and paraphrase the generated questions. The key statistics of MMQA dataset is listed on Table 1. Meanwhile, we show the statistics of three singlemodal question types and one compose question type in Table 2. We can see that text-based questions and composite questions are the main part with relatively less ImageQ and TableQ, exhibiting\na uniform type distribution. It should be note that the author of the MMQA dataset does not disclose the answers to the test dataset or give evaluation channels, so we can only use the development set for our experiments.\nImplementation Details Before the experiment, we leverage LLaVA-13B to generate all captions for each image. Then we regard the image caption as the image itself.\nDuring the classifying and retrieval phase, we used DeBERTa-large as pre-trained language models. For classifier, we confirmed the type of an exact question depending on the highest score among four types. The accuracy of the classifier on the MMQA development set is 96.1%. For retriever, with k is set to 3, we picked the top k linked passages and images. It identifies all gold documents in 99% of the paragraphs and 80% of the images, which has achieved much higher performance on recall concerning passage (17.3% \u2191) and image (40% \u2191) compared with prior works.\nStepping into prompt generation stage, we first extracted some demos by question type from a demos file which has been manually built before. Then concatenate them with the information with regard to current question as the input for LLMs.\nDuring the reasoning phase, we used textdavinci-003 API with the setting temperature=0.4 to get the final answer to the question. The detailed settings for each question type about n-shot and n can be found in Table 3."
        },
        {
            "heading": "3.2 Baselines",
            "text": "AutoRouting AutoRouting (Talmor et al., 2020) is a simple approach for answering questions without using cross-modal reasoning by determining the modality where the answer is anticipated to occur and then running the associated single-modality module.\nImplicitDecomp ImplicitDecomp (Talmor et al., 2020) introduces a 2-hop implicit decomposition baseline, which is able to automatically determines which portion of the question is pertinent at the present hop. Answers from the first hop are also provided as input in the second hop so that the model can make use of this data and engage in cross-modal reasoning to get the outcome. The model only uses the first hop to determine the response for all single-modality question types (such as TextQ and TableQ).\nMuRAG MuRAG (Chen et al., 2022) is built on top of a backbone model, which is pre-trained to encode image-text pairs such that they are suitable for both answer generation and retrieval. It takes a query q of any modality as input and retrieves TopK nearest neighbours from an extracted non-parametric multimodal memory M of imagetext pairs. Then the retrievals are combined with the query q as an augmented input [m1, ...,mk, q], and then fed to the backbone encoder-decoder for answer generation.\nSKURG SKURG (Yang et al., 2022) differs from previous methods in that it tightly relates the retrieval and reasoning stages and adapts to arbitrary retrieval hops by unifying the evidence retrieval and answer generation. It uses shared entities to align the sources from various modalities and structured knowledge to map them into a common semantic space.\nPReasM PReasM-Large (Yoran et al., 2022) is a pre-trained and fine-tuned language model based on T5-Large for getting strong reasoning ability using different reasoning skills. PReasM also trained two classification models. One for determining whether a question need an image and the other for identifying all gold paragraphs related to a question. They use ImplicitDecomp for question that requires images and PReasM model for the remain\nquestions.\nBinder Binder (Cheng et al., 2022) generates SQL programs and extends the capability of the programming language to solve commonsense problems. The main paradigm of Binder framework is to convert a question into an SQL query through LLM and then execute the SQL to obtain the final answer. Our approach, on the other hand, is different in that we use an end-to-end prompt method, which obtains the answer directly through prompt LLM."
        },
        {
            "heading": "3.3 Main Results",
            "text": "We show our main results in Table 4. We can see that MMHQA-ICL achieves the best performance on F1 scores among compared baselines, guaranteeing the feasibility of out method. Considering these frameworks without fine-tuning, we can see that we substantially surpasses Binder by 8.7% on F1 and 3.8% on EM. Under oracle settings, we furthermore outperforms Binder by 11.4% on F1 and 6.9% on EM. It indicates that our method has strong robustness and can be easily adopted to a new dataset.\nIn Table 5, we show performance of our method and prior works for the questions with different types. The main advantage of our method is that we are more capable to solve single-modal questions (2.7% \u2191 on F1), especially text queries(5.7% \u2191 on F1) compared with SKURG. Predictably, we have much potential towards multi-modal questions since we get a increase of 15.5% on EM and 15.2% on F1 with oracle settings. Here oracle settings mean that we use gold types and gold documents as the output of the CR Module.\nAdditionally, we test the effectiveness of our LPC Module and report the result under oracle settings in Table 6. We can see a sharp drop if we use traditional captioning method, showing that the diversity of semantic information will largely affect the performance on image-related questions. Our LPC Module makes the useful textual description of an image more abundant and achieves huge boost in comparison with traditional works.\nIn summary, our MMHQA-ICL model consistently demonstrates high performance in answering questions across various modalities. It outperforms previous methods and showcases the ability to effectively utilize text, image, and multi-modal information for question answering tasks. Furthermore, the model\u2019s performance is significantly en-\nhanced when perfect information is available. Overall, these results validate the effectiveness of the MMHQA-ICL model and LPC Module in handling multi-modal question answering tasks and its potential for achieving state-of-the-art performance in this domain."
        },
        {
            "heading": "3.4 Ablation Study",
            "text": "We conduct ablation experiments to verify the effectiveness of our unique Type-specific ICL Strategy, which are presented in Table 7 and Table 8.\nDiverse-Prompt or Coherent-Prompt. In Table 7, we compare Diverse-Prompt with CoherentPrompt which using the same complex prompt for all types. Considering the situation of noCoT, we can observe that using Diverse-Prompt boosts 13.2% on EM and 12.3% on F1 for Image as well as roughly 4% increase over all questions. Such\nenhancement indicates that utilizing different demos for different question types does work effectively. Under CoT settings, the phenomenon is similar. Moreover, it should be clarified that due to wrong classification result that classifies crossmodal question into single-modal, Diverse-Prompt gets a lower scores than Coherent-Prompt in crossmodal questions, but it is not the main issue.\nUsing CoT or noCoT. We furthermore investigate the influence of using CoT or not. Table 8 demonstrates the results under all six cases. When using CoT in all types, the inference time increases heavily but we can see that performance drops by 7.3% under normal settings and 19.1% under oracle settings in Image, 2.4% and 1.7% in Text and eventually causing 1.8% and 3.4% decline over the whole dataset. Without using any CoT under normal settings, it causes 4.9% down on EM and 5% down on F1 in Cross-Modal questions, finally leading to 2.4% decline over all questions. Such phenomenon is also more obvious under oracle settings."
        },
        {
            "heading": "3.5 Case Study",
            "text": "We list three examples from MMQA in Figure 3. The left example is a question only related to visual modality. In this case, we only select similar visual questions as demos without CoT and only use the image part of the related documents to build prompt. As we can see, LLM can infer that the sky is in orange while the image caption only provide\na sunset background. The middle example is a table-based question so we using demos that contains each reasoning steps. Motivated by these demos, LLM derives correct answer following similar steps:\n1) Examine the table structure. 2) Locate the information needed in the table. 3) Identify the required data. 4) Answer the question. The right example is a compose question containing both passage and image. To solve this question, LLM is supposed to synthesize the information from two modalities. Such process is suitable for using CoT. Thus, LLM analyses the question and related data step by step and responds with the right answer at last."
        },
        {
            "heading": "4 Related Work",
            "text": "Multimodal Hybrid Question Answering Multimodal hybrid question answering has been extensively studied in recent years. For image-text hybrid question answering, MuMuQA (Reddy et al., 2021), ManyModelQA (Hannan et al., 2020), and MIMOQA (Singh et al., 2021) have been introduced, which provide questions that require reasoning over images and explicitly provided text snippets (Wang et al., 2023). Another branch of the task is TextTableQA (Zhu et al., 2021; Chen et al., 2020; Wei et al., 2023; Lei et al., 2023), which aims to obtain answers by utilizing hybrid data types of text and tables, including structured database tables (Yu\nImageQ\nGive you some images with format like \"tittle\" (description), answer questions: Images: \"alice\" (a 18 years old girl) Question: Is Alice a 18 years old boy? Answer: no Images: \"High Jump\" ( person competes in the high jump.) \"Long Jump\" ( person, competes in the long jump.) Question: Which event shows a competitor in the 2003 Girls U-17 Leeward Islands Junior Athletics Championships jumping? Answer: Long Jump | High Jump \u2026\u2026 Images: \"Bristol\" (The image captures a beautiful sunset over a city skyline, with the warm hues of the sky setting the scene. In the foreground, a large clock tower stands prominently against the fading sky, serving as a striking focal point\u2026) Question: What color is the sky in bristol? Answer:\nQuestion: What color is the sky in bristol? Images: \"Bristol\" (The image captures a beautiful sunset over a city skyline, with the warm hues of the sky setting the scene. In the foreground, a large clock tower stands prominently against the fading sky, serving as a striking focal point\u2026)\norange\nTableQ\nGive you a table with multirows, answer questiones step by step: Table student: name gender age alice girl 18 bob boy 18 Question: Is Bob a 18 years old girl? Please answer the question step by step. We need to analyze the given table and compare the information provided with the question. Step 1: Examine the table structure: (...) Step 2: Identify the relevant row: (\u2026) Step 3: Compare the information with the question: (\u2026) Step 4: Analyze the information: (\u2026) Step 5: Compare the age: (\u2026) Step 6: Answer the question: (\u2026) Answer: no (\u2026)\nTable 2018 Allsvenskan: Team Head coach 1 Captain Kit manufacturer Main shirt sponsor AIK Norling Goitom Nike Notar BK H\u00e4cken Alm Lindgren Nike BRA Bygg Malm\u00f6 FF R\u00f6sler Rosenberg Puma Volkswagen Trelleborgs FF Winqvist Camara J\u00f6nsson Nike Mellby G\u00e5rd \u00d6stersunds FK Ian Burchnall Pettersson Adidas Various Question: Who was the head coach when the captain was Rosenberg in personnel and sponsoring for 2018 Allsvenskan? Please answer the question step by step.\nQuestion: Who was the head coach when the captain was Rosenberg in personnel and sponsoring for 2018 Allsvenskan?\nStep 1: Examine the table structure: The table \"2018 Allsvenskan\" lists different teams, head coaches, captains, kit manufacturers, and main shirt sponsors. Step 2: Locate the team with Rosenberg as the captain: From the table, we can see that Malm\u00f6 FF had Rosenberg as the captain. Step 3: Identify the head coach: From the table, we can see that the head coach of Malm\u00f6 FF was R\u00f6sler. Step 4: Answer the question: The head coach when the captain was Rosenberg in the personnel and sponsoring for the 2018 Allsvenskan was R\u00f6sler. Answer: R\u00f6sler\nTeam Head coach Captain Kit manufacturer Main shirt sponsor AIK Norling Goitom Nike Notar\nBK H\u00e4cken Alm Lindgren Nike BRA Bygg Malm\u00f6 FF R\u00f6sler R\u00f6sler Puma Puma Trelleborgs FF Winqvist Camara J\u00f6nsson Nike Mellby G\u00e5rd \u00d6stersunds FK Ian Burchnall Pettersson Pettersson Various\n2018 Allsvenskan\nCompose\nQuestion: The place where the he first international earth summit held has how many photos in its collage? Passages: \"Earth Summit\" (The United Nations Conference on Environment and Development (UNCED), also known as the Rio de Janeiro Earth Summit, the Rio Summit, the Rio Conference, and the Earth Summit (Portuguese: ECO92), was a major United Nations conference held in Rio de Janeiro from 3 to 14 June 1992.)\nGive you some images, passages and at most one table, answer questions step by step: Images: \"alice\" (a girl) Passages: \"alice\" (alice is 18 years old) Table student: name gender age alice girl 18 Question: Is Alice a 18 years old girl? Please answer the question step by step. Step 1: Analyzing the Images and Passages: (\u2026) Step 2: Checking the Table: (\u2026) Step 3: Answering the Question: (\u2026) Answer: yes Images: \"Rio de Janeiro\" (six views of city rio de janeiro, brazil) Passages: \"Earth Summit\" (The United Nations Conference on Environment and Development (UNCED), also known as the Rio de Janeiro Earth Summit, the Rio Summit, the Rio Conference, and the Earth Summit (Portuguese: ECO92), was a major United Nations conference held in Rio de Janeiro from 3 to 14 June 1992.) Table: Question: The place where the he first international earth summit held has how many photos in its collage? Please answer the question step by step.\nStep 1: Analyzing the Images and Passages You provided images labeled \"Rio de Janeiro\" and a passage mentioning the \"Earth Summit.\" (\u2026) Step 2: Understanding the Question (\u2026) Step 3: Examining the Images (\u2026) Step 4: Answering the Question Given that the images provided represent the location where the Earth Summit took place, and there are six images of Rio de Janeiro, we can conclude that the collage of the place where the first international Earth Summit was held consists of six photos. Answer: 6\nImages: \"Rio de Janeiro\" (six views of city rio de janeiro, brazil)\nFigure 3: Case study when using MMHQA-ICL\net al., 2018) and unstructured spreadsheets (Zhao et al., 2022). Our work is distinct from prior studies as we address three types of data, namely tables, text, and images, and achieve state-of-the-art results on the MultimodalQA dataset. In contrast, previous research mainly focused on two types.\nIn-Context Learning Large language models such as GPT-3 exhibit impressive few-shot learning ability (Liu et al., 2023b; Dong et al., 2022), requiring only a few questions and answers as prompts in the context without the need for finetuning on a dataset of training examples. In addition to textbased QA (Wei et al., 2022), in-context learning has also shown promising results in tableQA (Cheng et al., 2022; Chen, 2022; Ye et al., 2023) and visual (Zhang et al., 2023) QA tasks. However, our proposed MMHQA-ICL framework distinguishes itself from these works by incorporating a typespecific in-context learning strategy, which can effectively handle mixed QA data of three different types: images, text, and tables."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we proposed a MMHQA-ICL framework for the challenging task of hybrid question answering over text, tables, and images. Our framework includes a stronger heterogeneous data retriever and an image captioning module, as well as a type-specific in-context learning strategy that en-\nables LLMs to leverage their powerful performance in this task. We are the first to use an end-to-end prompting method for this task, and our experimental results demonstrate that our framework outperforms all baselines and methods trained on the full dataset, achieving state-of-the-art results under the few-shot setting on the MultimodalQA dataset.\nLimitations\nSince the multimodal hybrid question answering problem has only one dataset MultimodalQA, our model has experimented on only one dataset. This may lead to a lack of generalizability of our model. Transparency and interpretability are important in multi-hop question answering. While our model achieves the best results, the model does not fully predict the reasoning path explicitly. In future work, we will design more interpretable MMHQA models."
        }
    ],
    "title": "MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images",
    "year": 2023
}