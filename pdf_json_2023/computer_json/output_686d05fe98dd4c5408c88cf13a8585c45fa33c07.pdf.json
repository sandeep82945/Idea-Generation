{
    "abstractText": "This paper investigates the problem of scene graph generation in videos with the aim of capturing semantic relations between subjects and objects in the form of \u27e8subject, predicate, object\u27e9 triplets. Recognizing the predicate between subject and object pairs is imbalanced and multi-label in nature, ranging from ubiquitous interactions such as spatial relationships (e.g. in front of ) to rare interactions such as twisting. In widely-used benchmarks such as Action Genome and VidOR, the imbalance ratio between the most and least frequent predicates reaches 3,218 and 3,408, respectively, surpassing even benchmarks specifically designed for long-tailed recognition. Due to the long-tailed distributions and label co-occurrences, recent state-of-the-art methods predominantly focus on the most frequently occurring predicate classes, ignoring those in the long tail. In this paper, we analyze the limitations of current approaches for scene graph generation in videos and identify a one-to-one correspondence between predicate frequency and recall performance. To make the step towards unbiased scene graph generation in videos, we introduce a multi-label meta-learning framework to deal with the biased predicate distribution. Our meta-learning framework learns a meta-weight network for each training sample over all possible label losses. We evaluate our approach on the Action Genome and VidOR benchmarks by building upon two current state-of-theart methods for each benchmark. The experiments demonstrate that the multi-label meta-weight network improves the performance for predicates in the long tail without compromising performance for head classes, resulting in better overall performance and favorable generalizability. Code: https://github.com/shanshuo/ML-MWN. CCS CONCEPTS \u2022Computingmethodologies\u2192 Scene understanding;Activity recognition and understanding.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuo Chen"
        },
        {
            "affiliations": [],
            "name": "Yingjun Du"
        },
        {
            "affiliations": [],
            "name": "Pascal Mettes"
        },
        {
            "affiliations": [],
            "name": "Cees G. M. Snoek"
        }
    ],
    "id": "SP:620d6d1294bf5bfd9fe452ee7eda633d5c2d7c1e",
    "references": [
        {
            "authors": [
                "Stanislaw Antol",
                "Aishwarya Agrawal",
                "Jiasen Lu",
                "Margaret Mitchell",
                "Dhruv Batra",
                "C. Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "In ICCV. IEEE Computer Society, Santiago,",
            "year": 2015
        },
        {
            "authors": [
                "Ondrej Bohdal",
                "Yongxin Yang",
                "Timothy M. Hospedales"
            ],
            "title": "EvoGrad: Efficient Gradient-Based Meta-Learning and Hyperparameter Optimization",
            "venue": "In NeurIPS. Neural Information Processing Systems Foundation,",
            "year": 2021
        },
        {
            "authors": [
                "Qianwen Cao",
                "Heyan Huang",
                "Xindi Shang",
                "BoranWang",
                "Tat-Seng Chua"
            ],
            "title": "3-D Relation Network for visual relation recognition",
            "venue": "in videos. Neurocomputing",
            "year": 2021
        },
        {
            "authors": [
                "Shuo Chen",
                "Pascal Mettes",
                "Tao Hu",
                "Cees GM Snoek"
            ],
            "title": "Interactivity Proposals for Surveillance Videos",
            "venue": "In ICMR. ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Shuo Chen",
                "Pascal Mettes",
                "Cees GM Snoek"
            ],
            "title": "Diagnosing Errors in Video Relation Detectors",
            "venue": "In BMVC. BMVA Press,",
            "year": 2021
        },
        {
            "authors": [
                "Shuo Chen",
                "Zenglin Shi",
                "Pascal Mettes",
                "Cees GM Snoek"
            ],
            "title": "Social Fabric: Tubelet Compositions for Video Relation Detection",
            "venue": "In ICCV. IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Yuren Cong",
                "Wentong Liao",
                "Hanno Ackermann",
                "Bodo Rosenhahn",
                "Michael Ying Yang"
            ],
            "title": "Spatial-temporal transformer for dynamic scene graph generation",
            "venue": "In ICCV. IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Alakh Desai",
                "Tz-Ying Wu",
                "Subarna Tripathi",
                "Nuno Vasconcelos"
            ],
            "title": "Learning of visual relations: The devil is in the tails",
            "venue": "In ICCV. IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Xingning Dong",
                "Tian Gan",
                "Xuemeng Song",
                "JianlongWu",
                "Yuan Cheng",
                "Liqiang Nie"
            ],
            "title": "Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation",
            "venue": "In CVPR. IEEE,",
            "year": 2022
        },
        {
            "authors": [
                "Kaifeng Gao",
                "Long Chen",
                "Yulei Niu",
                "Jian Shao",
                "Jun Xiao"
            ],
            "title": "Classificationthen-grounding: Reformulating video scene graphs as temporal bipartite graphs",
            "venue": "In CVPR. IEEE,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR. IEEE Computer Society,",
            "year": 2016
        },
        {
            "authors": [
                "Jingwei Ji",
                "Ranjay Krishna",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Action genome: Actions as compositions of spatio-temporal scene graphs",
            "venue": "In CVPR",
            "year": 2020
        },
        {
            "authors": [
                "Justin Johnson",
                "Ranjay Krishna",
                "Michael Stark",
                "Li-Jia Li",
                "David Shamma",
                "Michael Bernstein",
                "Li Fei-Fei"
            ],
            "title": "Image retrieval using scene graphs",
            "venue": "In CVPR. IEEE Computer Society,",
            "year": 2015
        },
        {
            "authors": [
                "Anna Kukleva",
                "Makarand Tapaswi",
                "Ivan Laptev"
            ],
            "title": "Learning Interactions and Relationships Between Movie Characters",
            "venue": "In CVPR",
            "year": 2020
        },
        {
            "authors": [
                "Rongjie Li",
                "Songyang Zhang",
                "Bo Wan",
                "Xuming He"
            ],
            "title": "Bipartite graph network with adaptive message passing for unbiased scene graph generation",
            "venue": "In CVPR. Computer Vision Foundation / IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Wei Li",
                "Haiwei Zhang",
                "Qijie Bai",
                "Guoqing Zhao",
                "Ning Jiang",
                "Xiaojie Yuan"
            ],
            "title": "PPDL: Predicate Probability Distribution Based Loss for Unbiased Scene Graph Generation",
            "venue": "In CVPR. IEEE,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Xiaoshan Yang",
                "Changsheng Xu"
            ],
            "title": "Dynamic Scene Graph Generation via Anticipatory Pre-Training",
            "venue": "In CVPR. IEEE,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross B. Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal Loss for Dense Object Detection",
            "venue": "In ICCV. IEEE Computer Society, Venice,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft COCO: Common objects in context",
            "venue": "In ECCV. Springer, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Chenchen Liu",
                "Yang Jin",
                "Kehan Xu",
                "Guoqiang Gong",
                "Yadong Mu"
            ],
            "title": "Beyond Short-Term Snippet: Video Relation Detection with Spatio-Temporal Global Context",
            "venue": "In CVPR",
            "year": 2020
        },
        {
            "authors": [
                "Hengyue Liu",
                "Ning Yan",
                "Masood Mortazavi",
                "Bir Bhanu"
            ],
            "title": "Fully convolutional scene graph generation",
            "venue": "In CVPR. Computer Vision Foundation / IEEE,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2019
        },
        {
            "authors": [
                "Xufeng Qian",
                "Yueting Zhuang",
                "Yimeng Li",
                "Shaoning Xiao",
                "Shiliang Pu",
                "Jun Xiao"
            ],
            "title": "Video relation detection with spatio-temporal graph",
            "venue": "In ACM MM. ACM,",
            "year": 2019
        },
        {
            "authors": [
                "Mengye Ren",
                "Wenyuan Zeng",
                "Bin Yang",
                "Raquel Urtasun"
            ],
            "title": "Learning to reweight examples for robust deep learning",
            "venue": "In ICML. PMLR, Stockholmsma\u0308ssan, Stockholm,",
            "year": 2018
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
            "venue": "In NeurIPS. Neural Information Processing Systems Foundation, Montreal,",
            "year": 2015
        },
        {
            "authors": [
                "Xindi Shang",
                "Donglin Di",
                "Junbin Xiao",
                "Yu Cao",
                "Xun Yang",
                "Tat-Seng Chua"
            ],
            "title": "Annotating objects and relations in user-generated videos",
            "venue": "In ICMR. ACM, Ottawa, ON,",
            "year": 2019
        },
        {
            "authors": [
                "Xindi Shang",
                "Tongwei Ren",
                "Jingfan Guo",
                "Hanwang Zhang",
                "Tat-Seng Chua"
            ],
            "title": "Video visual relation detection",
            "venue": "In ACM MM. ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Jun Shu",
                "Qi Xie",
                "Lixuan Yi",
                "Qian Zhao",
                "Sanping Zhou",
                "Zongben Xu",
                "Deyu Meng"
            ],
            "title": "Meta-weight-net: Learning an explicit mapping for sample weighting",
            "venue": "In NeurIPS. Neural Information Processing Systems Foundation,",
            "year": 2019
        },
        {
            "authors": [
                "Cees G.M. Snoek",
                "Marcel Worring"
            ],
            "title": "Concept-Based Video Retrieval",
            "venue": "Found. Trends Inf. Retr. 2,",
            "year": 2009
        },
        {
            "authors": [
                "Zixuan Su",
                "Xindi Shang",
                "Jingjing Chen",
                "Yu-Gang Jiang",
                "Zhiyong Qiu",
                "Tat- Seng Chua"
            ],
            "title": "Video Relation Detection via Multiple Hypothesis Association",
            "venue": "In ACM MM. ACM, Virtual Event / Seattle, WA,",
            "year": 2020
        },
        {
            "authors": [
                "Xu Sun",
                "Tongwei Ren",
                "Yuan Zi",
                "Gangshan Wu"
            ],
            "title": "Video visual relation detection via multi-modal feature fusion",
            "venue": "In ACM MM. ACM,",
            "year": 2019
        },
        {
            "authors": [
                "Sai Praneeth Reddy Sunkesula",
                "Rishabh Dabral",
                "Ganesh Ramakrishnan"
            ],
            "title": "LIGHTEN: Learning Interactions with Graph and Hierarchical TEmporal Networks for HOI in Videos",
            "venue": "In ACM MM. ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Leitian Tao",
                "Li Mi",
                "Nannan Li",
                "Xianhang Cheng",
                "Yaosi Hu",
                "Zhenzhong Chen"
            ],
            "title": "Predicate Correlation Learning for Scene Graph Generation",
            "venue": "TIP",
            "year": 2022
        },
        {
            "authors": [
                "Bart Thomee",
                "David A. Shamma",
                "Gerald Friedland",
                "Benjamin Elizalde",
                "Karl Ni",
                "Douglas Poland",
                "Damian Borth",
                "Li-Jia Li"
            ],
            "title": "YFCC100M: the new data in multimedia research",
            "venue": "Commun. ACM 59,",
            "year": 2016
        },
        {
            "authors": [
                "Junjiao Tian",
                "Niluthpol Chowdhury Mithun",
                "Zachary Seymour",
                "Han-Pang Chiu",
                "Zsolt Kira"
            ],
            "title": "Striking the Right Balance: Recall Loss for Semantic Segmentation",
            "venue": "In ICRA. IEEE,",
            "year": 2022
        },
        {
            "authors": [
                "NicolaiWojke",
                "Alex Bewley",
                "Dietrich Paulus"
            ],
            "title": "Simple online and realtime tracking with a deep association metric",
            "venue": "In ICIP. IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "TongWu",
                "Qingqiu Huang",
                "Ziwei Liu",
                "YuWang",
                "Dahua Lin"
            ],
            "title": "Distributionbalanced loss for multi-label classification in long-tailed datasets. In ECCV",
            "year": 2020
        },
        {
            "authors": [
                "Wentao Xie",
                "Guanghui Ren",
                "Si Liu"
            ],
            "title": "Video Relation Detection with Trajectory-aware Multi-modal Features",
            "venue": "In ACM MM. ACM, Virtual Event / Seattle, WA,",
            "year": 2020
        },
        {
            "authors": [
                "Danfei Xu",
                "Yuke Zhu",
                "Christopher B Choy",
                "Li Fei-Fei"
            ],
            "title": "Scene graph generation by iterative message passing",
            "venue": "In CVPR. IEEE Computer Society,",
            "year": 2017
        },
        {
            "authors": [
                "Kelvin Xu",
                "Jimmy Ba",
                "Ryan Kiros",
                "Kyunghyun Cho",
                "Aaron Courville",
                "Ruslan Salakhutdinov",
                "Richard Zemel",
                "Yoshua Bengio"
            ],
            "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",
            "venue": "In ICML. JMLR.org,",
            "year": 2015
        },
        {
            "authors": [
                "Shaotian Yan",
                "Chen Shen",
                "Zhongming Jin",
                "Jianqiang Huang",
                "Rongxin Jiang",
                "Yaowu Chen",
                "Xian-Sheng Hua"
            ],
            "title": "PCPL: Predicate-Correlation Perception Learning for Unbiased Scene Graph Generation",
            "venue": "In ACMMM. ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Jianwei Yang",
                "Jiasen Lu",
                "Stefan Lee",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Graph r-cnn for scene graph generation. In ECCV",
            "year": 2018
        },
        {
            "authors": [
                "Rowan Zellers",
                "Mark Yatskar",
                "Sam Thomson",
                "Yejin Choi"
            ],
            "title": "Neural motifs: Scene graph parsing with global context",
            "venue": "In CVPR. Computer Vision Foundation / IEEE Computer Society, Salt Lake City, UT,",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Zhang",
                "Bingyi Kang",
                "Bryan Hooi",
                "Shuicheng Yan",
                "Jiashi Feng"
            ],
            "title": "Deep long-tailed learning: A survey",
            "year": 2021
        },
        {
            "authors": [
                "Sipeng Zheng",
                "Xiangyu Chen",
                "Shizhe Chen",
                "Qin Jin"
            ],
            "title": "Relation understanding in videos",
            "venue": "In ACM MM. ACM,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022Computingmethodologies\u2192 Scene understanding;Activity recognition and understanding.\nKEYWORDS Scene Graph Generation, Long-Tailed Distribution, Multi-Label Meta-Learning, Imbalanced Data, Video Understanding, Semantic Relations, Action Genome, VidOR\nACM Reference Format: Shuo Chen, Yingjun Du, Pascal Mettes, and Cees G. M. Snoek. 2023. MultiLabel Meta Weighting for Long-Tailed Dynamic Scene Graph Generation.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0178-8/23/06. . . $15.00 https://doi.org/10.1145/3591106.3592267\nIn International Conference on Multimedia Retrieval (ICMR \u201923), June 12\u2013 15, 2023, Thessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/3591106.3592267"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Scene graph generation in videos focuses on detecting and recognizing relationships between pairs of subjects and objects. The resulting dynamic scene graph is a directed graph whose nodes are objects with their relationships as edges in a video. Extracting such graphs from videos constitutes a highly challenging research problem [13], with broad applicability in multimedia and computer vision. Effectively capturing such structural-semantic information boosts downstream tasks such as captioning [40], video retrieval [29], visual question answering [1], and numerous other visual-language tasks.\nCurrent methods place a heavy emphasis on recognizing subjectto-object relationship categories. A leading approach to date involves extracting multi-modal features for relation instances, followed by either pooling the multi-modal features [23, 30, 38] or learning a feature representation [6] to feed into the predicate classifier network. Despite the strong focus on relation recognition, existing methods overlook the extremely long-tailed distribution of predicate classes. Figure 1 displays the recall per predicate class from STTran [7] and its corresponding occurrences on the Action Genome dataset. This trend is even more pronounced on the VidOR dataset. Figure 2 illustrates the occurrence distribution vs. Recall@50 from Social Fabric [6] for the video relation detection task on the VidOR dataset, where a few head predicates dominate all other classes. This phenomenon has not been actively investigated, as the evaluation metrics do not penalize lower scores for predicates in the long tail. In light of these observations, this paper advocates for the development for scene graph generation methods in videos that effectively handle both common and rare predicates.\nWe introduce a meta-learning framework to address the longtailed dynamic scene graph generation problem. Drawing inspiration from the concept of meta weighting [28], we propose a MultiLabel Meta Weight Network (ML-MWN) to learn meta weights across both examples and classes explicitly. These meta weights are, in turn, used to steer the downstream loss to optimize the parameters of the predicate classifier. We adopt a meta-learning framework to optimize the ML-MWN parameters, where we compute each instance\u2019s per-class loss in a training batch and obtain a loss matrix. The loss matrix is fed into our ML-MWN, which outputs a weight matrix, with each row representing the weight vector for an instance\u2019s loss vector. We sample a meta-validation batch and use an unbiased meta-loss to guide the training of ML-MWN. We adopt the inverse frequency binary cross-entropy loss as the meta-loss. Finally, we integrate our framework with existing methods to guide the predicate classification. ar X\niv :2\n30 6.\n10 12\n2v 1\n[ cs\n.C V\n] 1\n6 Ju\nn 20\n23\nTo evaluate our meta-learning framework, we employ two recent state-of-the-art methods [6, 7], one for the scene graph generation task on the Action Genome dataset and one for video relation detection on the VidOR dataset. We empirically demonstrate that our approach enhances predicate predictions for these recent methods across various evaluation metrics. Furthermore, we show that our framework improves the performance of long-tailed predicates without hampering the performance of more common classes. Our approach is generic and works on top of any scene graph generation method, ensuring broad applicability. We make the code available on https://github.com/shanshuo/ML-MWN.\nIn summary, our contributions are three-fold: 1. We investigate the long-tail issue in dynamic scene graph generation and analyze the limitations of existing methods. 2. We introduce a multi-label meta-learning framework to address the biased predicate class distribution. 3. We propose a Multi-Label Meta Weight Network (ML-MWN) to explicitly learn a weighting function, which demonstrates generalization ability performance on two benchmarks when plugged into two existing approaches,"
        },
        {
            "heading": "2 RELATEDWORKS",
            "text": "Dynamic scene graph generation. Scene graph generation was first pioneered in [13] for image retrieval, and the task quickly gained further traction, as seen in e.g. [21, 33, 39, 42, 43]. Recently, a number of papers have identified the long-tailed distribution in image scene graphs and focused on generating unbiased scene graphs [8, 9, 15\u201317, 41]. We seek to bring the same problem to light in the video domain. Ji et al. [12] firstly extended scene graph generation to videos and introduced the Action Genome dataset. A wide range of works have since proposed solutions to the problem [3, 6, 10, 14, 20, 30\u201332, 38, 45]. Recently, Li et al. [17] proposed an anticipatory pre-training paradigm based on Transformer to\nmodel the temporal correlation of visual relationships. Similarly, the VidOR dataset collected by Shang et al. [26] is another popular benchmark. Leading approaches generate proposals [4] for individual objects on short video snippets, encode the proposals, predict a relation, and associate the relations over the entire video, e.g. [23, 30, 38]. Liu et al. [20] generate the proposals using the sliding window way. More recently, Gao et al. [10] proposed a classification-then-grounding framework, which can avoid the high influence of proposal quality on performance. Chen et al. [5] performed a series of analyses on video relation detection. In this paper, we use STTran [7] and Social Fabric [6] to capture the relation feature and insert our multi-label meta-weight network on top. Cong et al. [7] proposed a spatial-temporal Transformer to capture the spatial context and temporal dependencies for a dynamic scene graph. Moreover, Chen et al. [6] proposed an encoding that represents a pair of object tubelets as a composition of interaction primitives. Both approaches provide competitive results and form a fruitful testbed for our meta-learning framework.\nMulti-label long-tailed classification. Multi-label long-tailed recognition is a challenging problem that deals with sampling differences and biased label co-occurrences [44]. A few works have studied this topic, with most solutions based on new loss formulations. Specifically, Wu et al. [37] proposed a distribution-based loss for multi-label long-tailed image recognition. More recently, Tian et al. [35] proposed a hard-class mining loss for the semantic segmentation task by dynamically weighting the loss for each class based on instantaneous recall performance. Inspired by these loss-based works, we utilize inverse frequency cross-entropy loss during our meta-learning process.\nMeta learning for sample weighting. Ren et al. [24] pioneered the adoption of a meta learning framework to re-weight samples for imbalanced datasets. Based on [24], Shu et al. [28] utilize an MLP to explicitly learn the weighting function. Recently, Bohdal et al. [2] presented EvoGrad to compute gradients more efficiently by preventing the computation of second-order derivatives in [28]. However, these methods are targeted for multi-class single-label classification. Therefore, we present the multi-label meta weight net for predicate classification, with an MLP that output a weight for each class loss."
        },
        {
            "heading": "3 MULTI-LABEL METAWEIGHT NETWORK",
            "text": "Dynamic scene graph generation [7] takes a video as the input and generates directed graphs whose objects of interest are represented as nodes, and their relationships are represented as edges. Each relationship edge, along with its connected two object nodes, form a \u27e8subject, predicate, object\u27e9 semantic triplet. These directed graphs are structural representations of the video\u2019s semantic information. Highly related to dynamic scene graph generation, video relation detection [27] also outputs \u27e8subject, predicate\u27e9 object triplets, aiming to classify and detect the relationship between object tubelets occurring within a video. Due to the high similarity between the two tasks, we consider them both in the experiments. For brevity, in this paper, we use the term dynamic scene graph generation to denote both tasks throughout this paper.\nAction Genome [12] and VidOR [26] are two popular benchmark datasets for dynamic scene graph generation. However, both datasets suffer from a long-tailed distribution in predicate occurrences, as shown in Figure 1. The evaluation metrics forgo the class-wise differences and count all classes during inference, resulting in a trained predicate classifier with a strong bias toward head classes such as in_front_of and next_to. Although these predicate classes are often spatial-oriented and object-agnostic, tail classes like carrying, twisting, and driving are of more interest to us. In addition to the long-tailed distribution, predicate classification faces another challenge. Since multiple relationships can occur between a subject-object pair simultaneously, predicate classification is a multi-label classification problem. The co-occurrence of labels leads to head-class predicate labels frequently appearing alongside tailclass predicate labels, further exacerbating the imbalance problem.\nIn this paper, we propose a meta-learning framework that addresses on the long-tailed multi-label predicate classification task. We introduce a Multi-Label Meta Weight Net (ML-MWN) to learn a weight vector for each training instance\u2019s multi-label loss. The gradient of the sum of weighted loss is then calculated to optimize the classifier network\u2019s parameters during backward propagation. Our model-agnostic approach can be incorporated into existing dynamic scene graph generation methods. In particular, the framework includes two stages: (1) Relation feature extraction, where we use existing dynamic scene graph generation methods to obtain the feature representation of the relation instances, and (2) multi-label meta-weighting learning. We adopt a meta-learning framework to re-weight each instance\u2019s multi-label loss and propose learning an explicit weighting function that maps from training loss to weight vector. We learn a weight vector for each training instance to reweight its multi-label loss, i.e., multi-label binary cross-entropy loss. We achieve this by using an MLP, which takes the multi-label training loss as input and outputs the weight vector. We sample a\nmeta-validation set to guide the training of MLP. Ideally, the metavalidation set should be clean and free from the long-tailed issue, as in [28]. However, we cannot sample such a clean meta-validation set due to the label-occurrence issue. To deal with the issue, we adopt the inverse frequency binary cross-entropy loss on metavalidation set. In the following sections, we describe the ML-MWN and the meta-learning framework in detail."
        },
        {
            "heading": "3.1 Learning weights for multi-label losses",
            "text": "Let \ud835\udc65\ud835\udc56 denote the feature representation of \ud835\udc56-th relation instance from the training set D and \ud835\udc66\ud835\udc56 \u2208 R\ud835\udc36 represent the corresponding multi-label one-hot vector, where D = {\ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56 }\ud835\udc41\ud835\udc56=1. The multi-label predicate classifier network is represented by \ud835\udc53\ud835\udf03 with \ud835\udf03 as its parameters. To enhance the robustness of training in the presence of long-tailed multi-label training instances, we impose weights\ud835\udc64\ud835\udc56,\ud835\udc50 on the \ud835\udc56-th instance\u2019s \ud835\udc50-th class loss \ud835\udc59\ud835\udc56,\ud835\udc50 . Instead of pre-specifying the weights based on class size [18, 35], we learn an explicit weighting function directly from the data. Specifically, we propose the ML-MWN (Multi-Label Meta Weight Net) denoted by \ud835\udc54\ud835\udf19 , with \ud835\udf19 as its parameters, to obtain the weighting vector for each relation instance\u2019s multi-label loss. We use the loss from \ud835\udc53\ud835\udf03 as the input.\nA small meta-validation set D\u0302 = {\ud835\udc65 \ud835\udc57 , \ud835\udc66 \ud835\udc57 }\ud835\udc40\ud835\udc57=1, where \ud835\udc40 is the number of meta-validation instances and \ud835\udc40 \u226a \ud835\udc41 , is sampled to guide the training of ML-MWN. The meta-validation set does not overlap with the training set. The weighted losses are then calculated to guarantee that the learned multi-label predicate classifier is unbiased toward dominant classes.\nDuring training, the optimal classifier parameter \ud835\udf03\u2217 can be extracted by minimizing the training loss:\n\ud835\udc3f\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (\ud835\udf03 ) = 1 \ud835\udc5b 1 \ud835\udc36 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc36\u2211\ufe01 \ud835\udc50=1 \ud835\udc64\ud835\udc56,\ud835\udc50 \u00b7 \ud835\udc59\ud835\udc56,\ud835\udc50 , (1)\nwhere \ud835\udc5b is the number of training instances in a batch, and \ud835\udc36 is the number of classes. During inference, we only use the optimal classifier network \ud835\udc53\ud835\udf03 \u2217 for evaluation."
        },
        {
            "heading": "3.2 The meta-learning process",
            "text": "We adopt a meta-learning framework to update the classifier and ML-MWN. The meta-validation set represents the unbiased relation instances following a balanced predicate class distribution. Due to the multi-label classification label-occurrence issue [44], we employ an inverse frequency BCE loss on themeta-validation set to simulate a balanced label distribution. As illustrated in Figure 3, the process comprises three main steps to optimize \ud835\udf03 and \ud835\udf19 within a batch.\nSuppose we are at \ud835\udc61-th iteration during training. First, for a batch of \ud835\udc5b training instances with corresponding feature representations and multi-labels {\ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56 }, 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b, we feed \ud835\udc65\ud835\udc56 into the classifier and obtain \ud835\udc66\ud835\udc56 = \ud835\udc53\ud835\udf03\ud835\udc61 (\ud835\udc65\ud835\udc56 ) \u2208 R\ud835\udc36 . The unweighted BCE training loss is calculated as\n\ud835\udc59\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 ) = \u2212\ud835\udc66\ud835\udc56,\ud835\udc50 \u00b7 log ( \ud835\udc66\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 ) ) + (1 \u2212 \ud835\udc66\ud835\udc56,\ud835\udc50 ) \u00b7 log ( 1 \u2212 \ud835\udc66\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 ) ) . (2)\nThen \ud835\udc59\ud835\udc56,\ud835\udc50 is fed into the ML-MWN to obtain the weight ?\u0302?\ud835\udc56,\ud835\udc50 = \ud835\udc54\ud835\udf19\ud835\udc61 ( \ud835\udc59\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 ) ) . After calculating the weighted loss as ?\u0302?\ud835\udc56,\ud835\udc50 \u00b7 \ud835\udc59\ud835\udc56,\ud835\udc50 , we update \ud835\udf03\ud835\udc61 :\n\ud835\udf03\ud835\udc61 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udefc 1 \ud835\udc5b 1 \ud835\udc36 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc36\u2211\ufe01 \ud835\udc50=1 \ud835\udc54\u2032 \ud835\udf19\ud835\udc61 (\ud835\udc59\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 ))\u2207\ud835\udf03\ud835\udc61 \ud835\udc59\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 )\n\ud835\udf03\ud835\udc61\n, (3)\nwhere \ud835\udefc is the step size. We call the updated \ud835\udf03\ud835\udc61 the pseudo classifier parameters since they are not used for the next batch.\nIn the second step, we update the ML-MWN parameters based on the meta-validation loss. We feed the meta-validation relation instance into the pseudo classifier and obtain \ud835\udc66 \ud835\udc57 = \ud835\udc53\ud835\udf03\ud835\udc61 (\ud835\udc65 \ud835\udc57 ) \u2208 R\n\ud835\udc36 . Let\ud835\udc40\ud835\udc50 denote the total number of relation instances belonging to predicate class \ud835\udc50 \u2208 {1, . . . ,\ud835\udc36}. The frequency of a predicate class is calculated as \ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc5e(\ud835\udc50) = \ud835\udc40\ud835\udc50/\ud835\udc40 . By using inverse frequency weighting, the meta-validation loss is re-balanced to mimic a balanced predicate label distribution. We then update the ML-MWN parameters \ud835\udf19 on the meta-validation data:\n\ud835\udf19\ud835\udc61+1 = \ud835\udf19\ud835\udc61 \u2212 \ud835\udefd 1 \ud835\udc40 \ud835\udc40\u2211\ufe01 \ud835\udc57=1 \ud835\udc36\u2211\ufe01 \ud835\udc50=1 1 \ud835\udc53 \ud835\udc5f\ud835\udc52\ud835\udc5e(\ud835\udc50) \u2207\ud835\udf19\ud835\udc61 \ud835\udc59 \ud835\udc57,\ud835\udc50 (\ud835\udf03 \ud835\udc61 )\n\ud835\udf19\ud835\udc61 = \ud835\udf19\ud835\udc61 \u2212 \ud835\udefd \ud835\udc36\u2211\ufe01 \ud835\udc50=1 \ud835\udc40 \ud835\udc40\ud835\udc50 \u2207\ud835\udf19\ud835\udc61 \ud835\udc59 \ud835\udc57,\ud835\udc50 (\ud835\udf03\ud835\udc61 )\n\ud835\udf19\ud835\udc61\n,\n(4)\nwhere \ud835\udefd is the step size. Lastly, the updated \ud835\udf19\ud835\udc61+1 is employed to output the new weights \ud835\udc64\ud835\udc56,\ud835\udc50 . The new weighted losses are used to improve the parameters \ud835\udf03 of the classifier network:\n\ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udefc 1 \ud835\udc5b 1 \ud835\udc36 \ud835\udc5b\u2211\ufe01 \ud835\udc56=1 \ud835\udc36\u2211\ufe01 \ud835\udc50=1 \ud835\udc54\u2032 \ud835\udf19\ud835\udc61+1 (\ud835\udc59\ud835\udc56,\ud835\udc50 (\ud835\udf03 \ud835\udc61 ))\u2207\ud835\udf03\ud835\udc61 \ud835\udc59\ud835\udc56,\ud835\udc50 (\ud835\udf03\ud835\udc61 ) \ud835\udf03\ud835\udc61 . (5)\nThe ultimate goal is to guide the classifier network to achieve a balanced performance on the unbiased meta-validation set. The sequences of steps are shown in Algorithm 1. By alternating between standard and meta-learning, we can learn unbiased dynamic scene graphs by specifically increasing the focus on those examples and predicate classes that do not often occur in a dataset.\nAlgorithm 1 The ML-MWN learning algorithm\nRequire: Training data set D, meta-validation set D\u0302, max epochs \ud835\udc41\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e Ensure: Predicate multi-label classifier network parameter \ud835\udf03\u2217 1: for t = 1 to \ud835\udc41\ud835\udc38\ud835\udc5d\ud835\udc5c\ud835\udc50\u210e do 2: for each mini batch {\ud835\udc65\ud835\udc56 , \ud835\udc66\ud835\udc56 }, 1 \u2264 \ud835\udc56 \u2264 \ud835\udc5b do 3: Calculate the prediction \ud835\udc66\ud835\udc56 . 4: Calculate the unweighted loss using Eq. 2. 5: Formulate the pseudo predicate classifier \ud835\udf03\ud835\udc61 by Eq. 3. 6: Get meta-validation instances {\ud835\udc65 \ud835\udc57 , \ud835\udc66 \ud835\udc57 } \u2208 D\u0302. 7: Update \ud835\udf19\ud835\udc61+1 by Eq. 4. 8: Update \ud835\udf03\ud835\udc61+1 by Eq. 5. 9: end for 10: end for"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "4.1.1 Action Genome. [12] is a dataset which provides frame-level scene graph labels. It contains 234,253 annotated frameswith 476,229 bounding boxes of 35 object classes (without person) and 1,715,568 instances of 25 relationship classes. For the 25 relationships, there are three different types: (1) attention relationships indicating if a person is looking at an object or not, (2) spatial relationships describing where objects are relative to one another, and (3) contact relationships denoting the different ways the person is contacting an object. In AG, there are 135,484 subject-object pairs. Each pair is labeled with multiple spatial relationships (e.g. \u27e8phone-in front of-person\u27e9 and \u27e8phone-on the side of-person\u27e9) or contact relationships (e.g. \u27e8person-eating-food\u27e9 and \u27e8person-holding-food\u27e9). There are three strategies to generate a scene graph with the inferred relation distribution [7]: (a) with constraint allows each subjectobject pair to have one predicate at most. (b) semi constraint allows a subject-object pair has multiple predicates. The predicate is regarded as positive only if the corresponding confidence is higher than the threshold (0.9 in the experiments). (c) no constraint allows a subject-object pair to have multiple relationships guesses without constraint.\nEvaluation metrics. We have three tasks for evaluation following [7]: (1) predicate classification (PREDCLS): with the subject and object\u2019s ground truth labels and bounding boxes, only predict predicate labels of the subject-object pair. (2) scene graph classification (SGCLS): with the subject and object\u2019s ground truth bounding boxes given, predict the subject, object\u2019s label and their corresponding predicate. (3) scene graph detection (SGDET): detect the subject and object\u2019s bounding boxes and predict the subject, object, and predicate\u2019s labels. The object detection is regarded as positive if the IoU between the predicted and ground-truth box is at least 0.5. Since traditional metrics Recall@K (R@K) are not able to reflect the impact of long-tailed data, we use the mean Recall@K (mR@K), which evaluates the R@K (K = [10, 20, 50] of each relationship class and averages them. We follow the same selection of K as [7].\nImplementation details. We randomly sample 10% samples from the training set as the meta-validation set. In line with [7], we adopt\nthe Faster-RCNN [25] based on the ResNet101 [11] as the object detection backbone. The Faster-RCNN model is trained on AG and provided by Cong et al. [7]. We use an AdamW [22] optimizer with an initial learning rate 1\ud835\udc52\u22124 and batch size 1 to train our relation feature model STTran part. We train ML-MWN using SGD with a momentum of 0.9, weight decay of 0.01, and an initial learning rate of 0.01. We train for 10 epochs. Other hyperparameter settings are identical to Cong et al. [7]. If not specified, the ML-MWN is an MLP of 1-100-1.\n4.1.2 VidOR. [26] is a dataset that includes 10,000 user-generated videos selected from YFCC-100M [34], totaling approximately 84 hours of footage. It contains 80 object categories and 50 predicate categories. Besides providing annotated relation triplets, the dataset also provides bounding boxes of objects. VidOR is split into a training set with 7,000 videos, a validation set with 835 videos, and a testing set with 2,165 videos. Since the ground truth of the test set is unavailable, we follow [20, 23, 30, 38] and use the training set for training and the validation set for testing. We report the analysis of method performance on the VidOR validation set.\nEvaluation metrics. We use the relation detection task for evaluation. The output requires a \u27e8subject, predicate, object\u27e9 triplet prediction, along with the subject and object boxes. We adopt mR@K (K = [50, 100]) as the evaluation metric. We disregard the mAP used in Chen et al. [6] because we are more concerned with covering ground truth relationships belonging to tail classes during predictions. Calculating mR@K. For annotated video \ud835\udc3c\ud835\udc63 , its \ud835\udc3a\ud835\udc63 ground truth relationship triplets contain \ud835\udc3a\ud835\udc63,\ud835\udc50 ground truth triplets with\nrelationship class \ud835\udc50 . With \ud835\udc36 relationship classes, the model successfully predicts\ud835\udc47\ud835\udc3e\ud835\udc63,\ud835\udc50 triplets. In the\ud835\udc49 videos of validation/test dataset, for relationship \ud835\udc50 , there are\ud835\udc49\ud835\udc50 videos containing at least one ground truth triplet with this relationship. The R@K of relationship \ud835\udc50 can be calculated:\n\ud835\udc45@\ud835\udc3e\ud835\udc50 = 1 \ud835\udc49\ud835\udc50 \ud835\udc49\ud835\udc50\u2211\ufe01 \ud835\udc63=1,\ud835\udc3a\ud835\udc63,\ud835\udc50\u22600 \ud835\udc47\ud835\udc3e\ud835\udc63,\ud835\udc50 \ud835\udc3a\ud835\udc63,\ud835\udc50 (6)\nThen we can calculate\n\ud835\udc5a\ud835\udc45@\ud835\udc3e = 1 \ud835\udc36 \ud835\udc36\u2211\ufe01 \ud835\udc50=1 \ud835\udc45@\ud835\udc3e\ud835\udc50 . (7)\nImplementation details. We randomly sample 10% samples from the training set as the meta-validation set. Our experiments are conducted using 1 NVIDIA V100 GPU.We adopt the same training strategy of Chen et al. [6] for the relation feature extraction model. First, we detect all objects in each video frame using Faster R-CNN [25] with a ResNet-101 [11] backbone trained onMS-COCO [19]. The detected bounding boxes are linked with the Deep SORT tracker [36] to obtain individual object tubelets. Then, each tubelet is paired with any other tubelet to generate the tubelet pairs. We extract spatial location features [31], language features, I3D features, and location mask features for each pair. Then the multi-modal features are used as the representation of the relation instance. For the classifier and ML-MWN, we use an SGD optimizer with an initial learning rate of 0.01 and train 10 epochs."
        },
        {
            "heading": "4.2 Multi-label meta weighting on top of the state-of-the-art",
            "text": "Video scene graph generation. First, we investigate the effect of incorporating our meta-learning approach on top of existing stateof-the-art methods for scene graph generation in videos and video relation detection. We build upon the recent STTran approach of Cong et al. [7] for video scene graph generation. We compare STTran as is and as a baseline that uses conventional meta-learning without considering the multi-label nature of scene graphs, namely MW-Net [28]. Table 1 shows the results for the with constraints setting. Across the PredCLS, SGCLS, and SGDET tasks, incorporating our meta-learning approach improves the results. For PredCLS, our proposed STTran + ML-MWN enhances mR@10 by 5.27, compared to the STTran baseline. On mean recall @ 50, we improve the scores by 4.98, from 39.66 to 44.64. On SGDET, the mean recall @ 50 increases from 22.89 to 28.52. The MW-Net baseline already improves the STTran results, emphasizing the overall potential of meta-learning to address the long-tailed nature of scene graphs. However, our proposed multi-label meta-learning framework performs best across all tasks and recall thresholds. This improvement is a direct result of increasing the weight of classes in the long tail when optimizing the classifier network.\nThe results are consistent for the semi constraint and no constraint settings, as shown in Table 2 and Table 3. In Table 2, the mean recall is higher than in the with constraint setting since more predicted results are involved. For the SGCLS task, our framework achieves 50.60% on mR@20, which is 6.33% better than STTran and 3.49% better than STTran + MW-Net. Our framework outperforms all metrics in the no constraint setting. In particular, for SGDET, our method reaches 27.59% at mR@10, 5.95% better than STTran, and 3.35% higher than STTran + MV-Net. We conclude that our meta learning framework is effective for video scene graph generation and can be adopted by any existing work. In Table 3, the mean recall is the highest among the three settings. Unlimited predictions contribute to enhanced recall performance. Under this setting, STTran + ML-MWN still achieves the best on all metrics across all tasks. The results prove our method\u2019s generality on various tasks with different settings.\nVideo relation detection. For video relation detection, we begin with the recent Social Fabric approach by Chen et al. [6]. Table 4 demonstrates the effect of incorporating our proposed meta learning framework for relation detection. The Social Fabric baseline, which is the current state-of-the-art in this setting, struggles to achieve good results for relation detection using mean recall as metrics. This underlines the problem\u2019s difficulty. This holds similarly\nfor the baseline by Sun etal [31]. When incorporating MW-Net [28], the results noticeably improve and further enhance with multi-label meta weighting. For mR@50, adding our meta-learning on top of Social Fabric boosts the results from 2.37 to 6.35. We conclude that multi-label meta-learning is crucial in video relation detection to achieve meaningful relation detection recalls across all classes."
        },
        {
            "heading": "4.3 Analyses, ablations, and qualitative examples",
            "text": "Predicate-level analysis. We present the class-wise R@10 of the predicate classification task on Action Genome in Figure 4. Observing Figure 4, we see that our method surpasses STTran [7] in all predicate categories. The improvement is much more significant for tail classes with limited training samples compared to head classes.\nThe superior performance demonstrates that the meta-validation set effectively guides the classifier to balance the tail classes without compromising the performance of head predicate classes.\nAblating the MLP architecture. We conduct an ablation study on the MLP architecture for the PredCLS task on Action Genome. Table 5 shows the results for six structures with varying depths and widths. We find that maximum width and depth are not necessary, with the best results achieved by the 1-100-1 variant, which we use as default in all experiments.\nQualitative examples. We provide the qualitative results in Figure 5 and Figure 6. In Figure 5, we compare our method with STTran [7] on the Action Genome dataset. Our method demonstrates better recognition of tail predicates in Action Genome. In the top row, STTran incorrectly classifies the tail class beneath as the head class in front of, and sit on as touch. In the bottom row, STTran misses drink from amongst others, while our method classifies them all correctly. In Figure 6, we compare our method with Social Fabric [6] on the VidOR dataset. Social Fabric fails to detect the tail class lean_on in all frames, while our method successfully predicts it."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "Predicate recognition plays a crucial role in contemporary dynamic scene graph generation methods, but the long-tailed and multi-label nature of the predicate distribution is commonly ignored. We observe that rare predicates on popular benchmarks are inadequately recovered or even disregarded by recent methods. To move toward unbiased scene graph generation in videos, we propose amulti-label meta-learning framework that learns to weight samples and classes to optimize any predicate classifier effectively. Our approach is versatile and can be incorporated into any existing methods. Experiments demonstrate the potential of our multi-label meta-learning framework, with superior overall performance and an improved focus on rare predicates. We believe our method could be extended to other multi-label long-tailed recognition tasks and may offer inspiration for future research."
        }
    ],
    "title": "Multi-Label Meta Weightingfor Long-Tailed Dynamic Scene Graph Generation",
    "year": 2023
}