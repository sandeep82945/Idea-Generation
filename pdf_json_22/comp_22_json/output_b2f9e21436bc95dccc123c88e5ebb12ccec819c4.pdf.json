{
    "abstractText": "Data augmentation via voice conversion (VC) has been successfully applied to low-resource expressive text-to-speech (TTS) when only neutral data for the target speaker are available. Although the quality of VC is crucial for this approach, it is challenging to learn a stable VC model because the amount of data is limited in lowresource scenarios, and highly expressive speech has large acoustic variety. To address this issue, we propose a novel data augmentation method that combines pitch-shifting and VC techniques. Because pitch-shift data augmentation enables the coverage of a variety of pitch dynamics, it greatly stabilizes training for both VC and TTS models, even when only 1,000 utterances of the target speaker\u2019s neutral data are available. Subjective test results showed that a FastSpeech 2-based emotional TTS system with the proposed method improved naturalness and emotional similarity compared with conventional methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ryo Terashima"
        },
        {
            "affiliations": [],
            "name": "Ryuichi Yamamoto"
        },
        {
            "affiliations": [],
            "name": "Eunwoo Song"
        },
        {
            "affiliations": [],
            "name": "Yuma Shirahata"
        },
        {
            "affiliations": [],
            "name": "Hyun-Wook Yoon"
        },
        {
            "affiliations": [],
            "name": "Jae-Min Kim"
        },
        {
            "affiliations": [],
            "name": "Kentaro Tachibana"
        }
    ],
    "id": "SP:b33bde1bb9128634c9b7fd31897b63428380bfdd",
    "references": [
        {
            "authors": [
                "X. Zhu",
                "S. Yang",
                "G. Yang"
            ],
            "title": "Controlling emotion strength with relative attribute for end-to-end speech synthesis",
            "venue": "Proc. ASRU, 2019, pp. 192\u2013199.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Lei",
                "S. Yang",
                "L. Xie"
            ],
            "title": "Fine-grained emotion strength transfer, control and prediction for emotional speech synthesis",
            "venue": "Proc. SLT, 2021, pp. 423\u2013430.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "D. Stanton",
                "Y. Zhang"
            ],
            "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
            "venue": "Proc. ICML, PMLR, 2018, pp. 5180\u20135189.",
            "year": 2018
        },
        {
            "authors": [
                "R. Skerry-Ryan",
                "E. Battenberg",
                "Y. Xiao"
            ],
            "title": "Towards endto-end prosody transfer for expressive speech synthesis with tacotron",
            "venue": "Proc. ICML, PMLR, 2018, pp. 4693\u20134702.",
            "year": 2018
        },
        {
            "authors": [
                "W.-N. Hsu",
                "Y. Zhang",
                "R.J. Weiss"
            ],
            "title": "Hierarchical generative modeling for controllable speech synthesis",
            "venue": "Proc. ICLR, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y.-J. Zhang",
                "S. Pan",
                "L. He"
            ],
            "title": "Learning latent representations for style control and transfer in end-to-end speech synthesis",
            "venue": "Proc. ICASSP, 2019, pp. 6945\u20136949.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Jia",
                "Y. Zhang",
                "R. Weiss"
            ],
            "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
            "venue": "Proc. NeurIPS, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Tits",
                "K. El Haddad",
                "T. Dutoit"
            ],
            "title": "Exploring transfer learning for low resource emotional TTS",
            "venue": "Proc. IntelliSys, Springer, 2019, pp. 52\u201360.",
            "year": 2019
        },
        {
            "authors": [
                "Y.-A. Chung",
                "Y. Wang",
                "W.-N. Hsu"
            ],
            "title": "Semi-supervised training for improving data efficiency in end-to-end speech synthesis",
            "venue": "Proc. ICASSP, 2019, pp. 6940\u20136944.",
            "year": 2019
        },
        {
            "authors": [
                "R. Valle",
                "J. Li",
                "R. Prenger"
            ],
            "title": "Mellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens",
            "venue": "Proc. ICASSP, 2020, pp. 6189\u20136193.",
            "year": 2020
        },
        {
            "authors": [
                "A. Gibiansky",
                "S. Arik",
                "G. Diamos"
            ],
            "title": "Deep voice 2: Multi-speaker neural text-to-speech",
            "venue": "Proc. NeurIPS, 2017, pp. 2962\u20132970.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Byambadorj",
                "R. Nishimura",
                "A. Ayush"
            ],
            "title": "Multi-speaker TTS system for low-resource language using cross-lingual transfer learning and data augmentation",
            "venue": "Proc. APSIPA, 2021, pp. 849\u2013853.",
            "year": 2021
        },
        {
            "authors": [
                "M.-J. Hwang",
                "R. Yamamoto",
                "E. Song"
            ],
            "title": "TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis",
            "venue": "Proc. ICASSP, 2021, pp. 6598\u20136602.",
            "year": 2021
        },
        {
            "authors": [
                "G. Huybrechts",
                "T. Merritt",
                "G. Comini"
            ],
            "title": "Low-resource expressive text-to-speech using data augmentation",
            "venue": "Proc. ICASSP, 2021, pp. 6593\u20136597.",
            "year": 2021
        },
        {
            "authors": [
                "R. Shah",
                "K. Pokora",
                "A. Ezzerg"
            ],
            "title": "Non-autoregressive TTS with explicit duration modelling for low-resource highly expressive speech",
            "venue": "Proc. SSW, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.S. Ribeiro",
                "J. Roth",
                "G. Comini"
            ],
            "title": "Cross-speaker style transfer for text-to-speech using data augmentation",
            "venue": "Proc. ICASSP (in press), 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Kaneko",
                "H. Kameoka",
                "K. Tanaka"
            ],
            "title": "CycleGAN-VC2: Improved CycleGAN-based non-parallel voice conversion",
            "venue": "Proc. ICASSP, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "K. Liu",
                "J. Zhang",
                "Y. Yan"
            ],
            "title": "High quality voice conversion through phoneme-based linear mapping functions with straight for Mandarin",
            "venue": "Proc. FSKD, 2007, pp. 410\u2013414.",
            "year": 2007
        },
        {
            "authors": [
                "A. Kanagaki",
                "M. Tanaka",
                "T. Nose"
            ],
            "title": "CycleGAN-based high-quality non-parallel voice conversion with spectrogram and WaveRNN",
            "venue": "Proc. GCCE, 2020, pp. 356\u2013357.",
            "year": 2020
        },
        {
            "authors": [
                "J.-Y. Zhu",
                "T. Park",
                "P. Isola"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "Proc. ICCV, 2017, pp. 2223\u20132232.",
            "year": 2017
        },
        {
            "authors": [
                "F. Charpentier",
                "M. Stella"
            ],
            "title": "Diphone synthesis using an overlap-add technique for speech waveforms concatenation",
            "venue": "Proc. ICASSP, 1986, pp. 2015\u20132018.",
            "year": 1986
        },
        {
            "authors": [
                "M. Morise",
                "F. Yokomori",
                "K. Ozawa"
            ],
            "title": "WORLD: A vocoderbased high-quality speech synthesis system for real-time applications",
            "venue": "IEICE Trans. on Information and Systems, vol. 99, no. 7, pp. 1877\u20131884, 2016.",
            "year": 1877
        },
        {
            "authors": [
                "Y. Tohkura",
                "F. Itakura",
                "S. Hashimoto"
            ],
            "title": "Spectral smoothing technique in PARCOR speech analysis-synthesis",
            "venue": "IEEE Trans. on Acoustics, Speech, and Signal Process., vol. 26, no. 6, pp. 587\u2013596, 1978.",
            "year": 1978
        },
        {
            "authors": [
                "M. Morise"
            ],
            "title": "Speech analysis and synthesis (in Japanese)",
            "venue": "CORONA PUBLISHING CO.,LTD.,",
            "year": 2018
        },
        {
            "authors": [
                "T. Okamoto",
                "T. Toda",
                "Y. Shiga"
            ],
            "title": "Real-time neural textto-speech with sequence-to-sequence acoustic model and Wave- Glow or single gaussian WaveRNN vocoders",
            "venue": "Proc. Interspeech, 2019, pp. 1308\u20131312.",
            "year": 2019
        },
        {
            "authors": [
                "K. Yu",
                "S. Young"
            ],
            "title": "Continuous f0 modeling for hmm based statistical parametric speech synthesis",
            "venue": "IEEE Trans. on Audio, Speech, and Lang. Process., vol. 19, no. 5, pp. 1071\u20131079, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "M.-J. Hwang",
                "R. Yamamoto",
                "E. Song"
            ],
            "title": "High-fidelity Parallel WaveGAN with multi-band harmonic-plus-noise model",
            "venue": "Proc. Interspeech, 2021, pp. 2227\u20132231.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ren",
                "C. Hu",
                "T. Qin"
            ],
            "title": "FastSpeech 2: Fast and highquality end-to-end text-to-speech",
            "venue": "Proc. ICLR, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Ming",
                "D. Huang",
                "M. Dong"
            ],
            "title": "Fundamental frequency modeling using wavelets for emotional voice conversion",
            "venue": "Proc. ACII, 2015, pp. 804\u2013809.",
            "year": 2015
        },
        {
            "authors": [
                "R. Yamamoto",
                "E. Song",
                "J.-M. Kim"
            ],
            "title": "Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram",
            "venue": "Proc. ICASSP, 2020, pp. 6199\u20136203.",
            "year": 2020
        },
        {
            "authors": [
                "A. Gulati",
                "J. Qin",
                "C.-C. Chiu"
            ],
            "title": "Conformer: Convolutionaugmented transformer for speech recognition",
            "venue": "Proc. Interspeech, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Guo",
                "F. Boyer",
                "X. Chang"
            ],
            "title": "Recent developments on ESPnet toolkit boosted by conformer",
            "venue": "Proc. ICASSP, 2021, pp. 5874\u20135878.",
            "year": 2021
        },
        {
            "authors": [
                "H. Choi",
                "S. Park",
                "J. Park"
            ],
            "title": "Multi-speaker emotional acoustic modeling for cnn-based speech synthesis",
            "venue": "Proc. ICASSP, 2019, pp. 6950\u20136954.",
            "year": 2019
        },
        {
            "authors": [
                "K. Matsubara",
                "T. Okamoto",
                "R. Takashima"
            ],
            "title": "Investigation of training data size for real-time neural vocoders on CPUs",
            "venue": "Acoust. Sci. Tech., vol. 42, no. 1, pp. 65\u201368, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Song",
                "F.K. Soong",
                "H.-G. Kang"
            ],
            "title": "Effective spectral and excitation modeling techniques for LSTM-RNN-based speech synthesis systems",
            "venue": "IEEE/ACM Trans. Audio, Speech, and Lang. Process., vol. 25, no. 11, pp. 2152\u20132161, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proc. ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Yasuda",
                "X. Wang",
                "S. Takaki"
            ],
            "title": "Investigation of enhanced Tacotron text-to-speech synthesis systems with self-attention for pitch accent language",
            "venue": "Proc. ICASSP, 2019, pp. 6905\u20136909.",
            "year": 2019
        },
        {
            "authors": [
                "T. Hayashi",
                "R. Yamamoto",
                "K. Inoue"
            ],
            "title": "ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end textto-speech toolkit",
            "venue": "Proc. ICASSP, 2020, pp. 7654\u20137658.",
            "year": 2020
        },
        {
            "authors": [
                "L. Liu",
                "H. Jiang",
                "P. He"
            ],
            "title": "On the variance of the adaptive learning rate and beyond",
            "venue": "Proc. ICLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhao",
                "W.-C. Huang",
                "X. Tian"
            ],
            "title": "Voice conversion challenge 2020 \u2013 intra-lingual semi-parallel and cross-lingual voice conversion ",
            "venue": "Proc. Joint workshop for the Blizzard Challenge and Voice Conversion Challenge 2020, 2020, pp. 80\u201398.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Deep learning approaches have been successfully applied to expressive text-to-speech (TTS). Expressive styles of speech can be modeled using explicit labels for style attributes [1], [2] or by extracting high-level latent features from input speech [3]\u2013[6]. However, achieving competitive performance in low-resource scenarios remains a challenge.\nIn previous studies on low-resource TTS, researchers used transfer learning [7]\u2013[9] or multi-speaker modeling [10]\u2013[12]. Most recently, data augmentation techniques have been successfully applied in low-resource scenarios [13]\u2013[15]. In particular, a cross-speaker style transfer method via voice conversion (VC) enables expressive TTS systems to be built where expressive data is only available for some existing speakers (i.e., source speaker) [16]. In this method, a pair of neutral speech databases of source and target speakers is used to learn a VC model. Then, the learned VC model is used to transfer the source model\u2019s expressive style (e.g., conversation) to the target speaker. Finally, a TTS acoustic model is trained using the VC-augmented speech together with the recorded neutral speech.\nHowever, although a high-quality VC model is crucial for data augmentation approaches, it is challenging to learn a stable VC model when (1) the amount of data is limited under low-resource conditions or (2) highly expressive speech has large acoustic variety. Under such conditions, a lack of accurate prosody conversion is often observed because VC models tend to focus on spectral (e.g., Mel-spectrogram) conversion [17]. Although some VC models use a mean-variance normalization method for fundamental frequency (Fo) conversion [18], this is not sufficient to stably generate the highly emotional voice of the target speaker.\nTo address the aforementioned problem, we propose a novel data augmentation method that combines pitch-shift (PS) augmentation and non-parallel VC-based augmentation. Our method differs from existing methods [16] in that the proposed system focuses on improving VC performance to make it suitable for con-\nverting emotional attributes, even though the target speaker\u2019s data only consist of neutral recordings.\nIn detail, in the proposed method, we first apply PS-based augmentation to both the source and target speaker\u2019s neutral recordings. As this enables the VC model to cover a variety of pitch dynamics, it substantially improves the stability of the training process. Additionally, we also propose incorporating a short-time Fourier transform (STFT)-based Fo regularization loss into the optimization criteria of the VC training process. This also stabilizes the target speaker\u2019s Fo trajectory, which is crucial for converting highly emotional speech segments. As a result, the VC model learned by the proposed method stably transfers the source speaker\u2019s speaking style to the target speaker, and even makes it possible to build the target speaker\u2019s emotional TTS system.\nWe investigated the effectiveness of the proposed data augmentation approach by performing subjective evaluation tasks. Note that PS-based augmentation and STFT Fo regularization loss can be extended to any neural VC model; however, our focus is the Scyclone model [19] based on a cycle-consistent adversarial network (CycleGAN) [20]. The experimental results demonstrated that our VC-augmented TTS system achieved better naturalness and emotional similarity than conventional methods when only 1,000 utterances of the target speaker\u2019s neutral data were available."
        },
        {
            "heading": "2. Method",
            "text": "Figure 1 shows an overview of our proposed method. In this study, we investigate three speaking styles: neutral, happiness, and sadness. The proposed method consists of PS-based data augmentation, VC-based data augmentation and emotional TTS system. In the following, we describe the details of each component."
        },
        {
            "heading": "2.1. PS-based data augmentation",
            "text": "Figure 2 shows an overview of our PS-based data augmentation. Unlike traditional PS methods such as pitch-synchronous overlapadd [21] and vocoders [22], our method does not require Fo estimation. Furthermore, since it does not involve waveform synthesis, there is no need to reconstruct the phase information. Specifically, the proposed method applies a stretching technique to the spectral fine structure to convert the pitch of the input signal. In the separation step as shown in Figure 2a, we first compute a speech spectrogram using STFT and then separate it into spectral envelopes and fine structures based on the lag-window method [23]. Next, by applying a linear interpolation method, we stretch the spectral fine structure along the frequency axis. Let St,k denote the spectral fine structure for the t-th time index and k-th frequency bin. Then, we obtain the stretched spectrum as follows [24]:\nS\u0302t,\u03b1k = St,k, (1)\n\u03b1 = 2p/12, (2)\nwhere \u03b1 denotes the stretching ratio determined by the semitone unit p. In the generation step as shown in Figure 2b, we obtain the pitch-shifted spectrogram by multiplying the original spectral envelope and corresponding stretched spectral fine structure.\nar X\niv :2\n20 4.\n10 02\n0v 2\n[ ee\nss .A\nS] 5\nJ ul\n2 02\n2\nPitch-shift on spectrogram\n(a)\n+2 semitones\n+1 semitones\n+2 semitones\n+1 semitones\nNeutral Neutral\nSource DB Target DB\n(a)\nit - ift gtr r\n( )\ns it s\ns it s\ns it s\ns it s\ntr l tr l\nr r t\nitch-shift on spegtrogra\n( )\n+2 se itones\n+1 se itones\n+2 se itones\n+1 se itones\neutral eutral\nrce ar et\nAs shown in Figure 1a, we apply the proposed PS method to augment both the source and target speaker\u2019s neutral data. In detail, we vary the semitone unit p in the range [-3, 12], which results in generating data 15 times larger amount of data than the original recordings. All the augmented datasets are used to train the VC model, which we explain further in the following section."
        },
        {
            "heading": "2.2. Non-parallel voice conversion",
            "text": ""
        },
        {
            "heading": "2.2.1. Model",
            "text": "From the many state-of-the-art VC models, we adopt a non-parallel Scyclone model [19] because of its stable generation and competitive quality. This method uses two separate modules: a CycleGANbased spectrogram conversion model [20] and a single-Gaussian WaveRNN-based vocoder [25]. However, we only use the spectrogram conversion model because VC aims to augment acoustic features when training TTS models. Note that we use the log-Mel spectrogram as the target acoustic features together with continuous log Fo [26], and voiced/unvoiced flags (V/UV). Predicting these additional features using the VC model is essential to create emotional TTS models that include Fo-dependent high-fidelity neural vocoders [27]."
        },
        {
            "heading": "2.2.2. STFT Fo regularization loss function",
            "text": "To avoid unnatural conversion of the prosody features, we propose an STFT-based Fo regularization loss function. Following a previous study on a spectrogram domain Fo loss function [28], we also define the regularization loss function on the spectrogram domain.\nLetXn,k and X\u0302n,k be STFT magnitudes for extracted and predicted Fo sequences for the n-th frame index and k-th frequency bin, respectively. The regularization loss is defined as follows:\nLFo = 1\nM N\u2211 n=1 K\u2211 k=\u03b2 \u2223\u2223logXn,k \u2212 log X\u0302n,k\u2223\u2223 , (3) where N,K, and M represent the number of frames, number of frequency bins, and number of elements in the magnitude, respectively; \u03b2 denotes a hyperparameter that controls the regularization strength. To regularize only the fine structure component of Fo (i.e., high-frequency components of the STFT magnitude) that contains little information about speaking styles for reading speech, we set \u03b2 = 3 based on our preliminary experiments. Furthermore, we extend the loss function to multiple resolutions inspired by previous studies on multi-level Fo modeling [29] and multi-resolution STFT loss [30]. Consequently, we optimize the VC model using the proposed regularization loss along with the adversarial loss, cycle consistency loss, and identity mapping loss functions, as described in Scyclone [19].\nAs shown in Figure 3, the Fo trajectory produced without the regularization method oscillates unstably. By contrast, with the regularization, the stability of the Fo trajectory improves as the VC model can focus on converting essential aspects of prosody variations."
        },
        {
            "heading": "2.2.3. VC-based data augmentation",
            "text": "For the criteria described above, we train the Scyclone model using a pair of source and target speaker\u2019s speech databases. Note that the\ntraining data consists of neutral recordings and PS-augmented data from each speaker. As illustrated in Figure 1b, we use the resulting VC model to convert all the source speaker\u2019s emotional voice into the target speaker\u2019s voice. Simultaneously, to stabilize the training process of the TTS model, we also convert the source speaker\u2019s neutral voice to the target speaker\u2019s voice.\nWe use all the converted data, together with the target speaker\u2019s neutral recordings, to train the target speaker\u2019s emotional TTS system."
        },
        {
            "heading": "2.3. Text-to-speech",
            "text": "Our TTS model consists of two components: (1) an acoustic model that converts an input phoneme sequence into acoustic features and (2) a vocoder that converts the acoustic feature into the waveform. For the acoustic model, we use FastSpeech 2 [28] with a Conformer encoder [31] because of its fast but high-quality TTS capability [32]. To adapt FastSpeech 2 for emotional TTS, we condition the model using external emotion code [33]. For the vocoder, we use the high-fidelity harmonic-plus-noise Parallel WaveGAN (HNPWG) [27].\nFigure 1 (c) shows the training process of TTS with the proposed data augmentation. We mix synthetic and recorded data for the target speaker and use them to train the acoustic model. At the inference stage, the TTS model generates emotional speech by inputting text and an emotion code. Note that we do not use data augmentation for training the vocoder because (1) it has been reported that using a large amount of training data is not crucial for the vocoder [34], and (2) our preliminary experiments also confirmed subtle improvements when the amount of the source speaker\u2019s data was sufficiently large."
        },
        {
            "heading": "3. Experiments",
            "text": ""
        },
        {
            "heading": "3.1. Experimental setup",
            "text": ""
        },
        {
            "heading": "3.1.1. Database and feature extraction settings",
            "text": "For the experiments, we used two phonetically and prosodically rich speech corpora recorded by two female Japanese professional speakers, which represent data for the source and target speakers. We sampled speech signals at 24 kHz with 16 bit quantization. The source speaker data contained three speaking styles: neutral, happiness, and sadness, whereas the target speaker data contained only neutral style1.\nWe concatenated the 80-dimensional log-Mel spectrogram, continuous log Fo, and V/UV with 5 ms analysis intervals as 82- dimensional features. We used them as the target acoustic features for both the VC and acoustic models. We calculated the logMel spectrogram with a 40 ms window length. We extracted Fo and V/UV using the improved time-frequency trajectory excitation vocoder [35]. We obtained Fo for the acoustic features generated by PS data augmentation by shifting the Fo extracted from the original speech. We used V/UV extracted from the original speech as the\n1The average sentence duration for each data set was 5.0, 4.7, 4.2, and 5.1 seconds, respectively.\nV/UV for the generated data. We normalized the acoustic features so that they had a zero mean and unit variance for each dimension using the statistics of the training data."
        },
        {
            "heading": "3.1.2. Model details",
            "text": "For the CycleGAN-based VC model, the generator and discriminator were composed of four and three residual blocks, respectively, and each block consisted of two convolutional layers with leaky ReLU activation. We set the kernel size to three for all the convolutional layers. We trained the VC model for 400 K steps using an Adam optimizer [36]. We set the learning rate to 0.0002, and reduced this by a factor of ten every 100 K steps. We set the minibatch size to 64. We set the weight of the proposed regularization loss described in Section 2.2 to 0.1, and used the FFT sizes (32, 64, 128), window sizes (32, 64, 128), and hop sizes (8, 16, 32) for the multi-resolution STFT loss. We used the identity mapping loss only for the first 10 K steps [17].\nFor the TTS acoustic model, we used four Conformer and Transformer blocks for the encoder and decoder, respectively. For each block, we set the hidden sizes of the self-attention and feedforward layers to 384 and 1024, respectively. To achieve natural prosody for Japanese, for the model, we used accent information as external input [37]. For emotional TTS, we added emotion embedding followed by a projection layer with 256-dimensional phoneme and accent embedding. To improve the duration stability, we used manually annotated phoneme durations. At the training stage, we used a dynamic batch size with an average of 23 samples to create a minibatch [38], and trained the models for 200 K steps using the RAdam optimizer [39].\nTable 1 summarizes the systems used in our experiments. We trained the following TTS systems:\nSRC-TTS: Baseline TTS model trained with the source speaker\u2019s recordings.\nTGT-NEU-TTS: Baseline TTS model trained with the target speaker\u2019s recordings (neutral style alone).\nMS-TTS: Baseline multi-speaker TTS model trained with source and target speaker\u2019s recordings.\nVC-TTS: Baseline TTS model trained with target speaker\u2019s recordings and VC-augmented data.\nVC-TTS-PS: Proposed TTS model trained with target speaker\u2019s recordings and PS-VC-augmented data\nVC-TTS-PS-1K: Proposed TTS model similarly configured with VC-TTS-PS system, but trained with a limited amount of recordings.\nAs the vocoder, we trained HN-PWG [27] for 400 K steps with the RAdam optimizer [39]. For training, we used 5,000 utterances of neutral style, 2,500 utterances of happiness style, and 2,500 utterances of sadness style from the source speaker, and 1,000 utterances of neutral style from the target speaker. We used the same vocoder for all the aforementioned TTS systems.\nTable 2: Naturalness, speaker similarity, and emotional similarity MOS test results with 95% confidence intervals. Results for the highest score in the VC-based TTS systems are shown in bold.\nModel Naturalness Speaker similarity Emotional similarity\nNeutral Happiness Sadness Neutral Happiness Sadness Happiness Sadness Source 4.88 \u00b1 0.05 4.84 \u00b1 0.05 4.69 \u00b1 0.07 - - - - - SRC-TTS 4.56 \u00b1 0.06 4.46 \u00b1 0.08 4.40 \u00b1 0.08 1.15 \u00b1 0.05 1.28 \u00b1 0.08 1.29 \u00b1 0.08 3.48 \u00b1 0.08 3.71 \u00b1 0.07 TGT-NEU-TTS - - - - - - 1.69 \u00b1 0.09 1.29 \u00b1 0.07 MS-TTS 3.91 \u00b1 0.09 2.85 \u00b1 0.10 2.74 \u00b1 0.11 2.85 \u00b1 0.13 2.76 \u00b1 0.11 2.51 \u00b1 0.12 2.48 \u00b1 0.10 2.72 \u00b1 0.11 VC-TTS 4.06 \u00b1 0.08 3.88 \u00b1 0.10 4.00 \u00b1 0.10 2.98 \u00b1 0.12 2.89 \u00b1 0.11 3.26 \u00b1 0.11 3.33 \u00b1 0.08 3.63 \u00b1 0.07 VC-TTS-PS 4.03 \u00b1 0.08 4.20 \u00b1 0.09 4.00 \u00b1 0.09 2.98 \u00b1 0.12 2.87 \u00b1 0.12 3.35 \u00b1 0.10 3.82 \u00b1 0.05 3.67 \u00b1 0.07 VC-TTS-PS-1K 4.20 \u00b1 0.08 4.08 \u00b1 0.09 3.96 \u00b1 0.11 3.19 \u00b1 0.11 2.90 \u00b1 0.12 3.31 \u00b1 0.09 3.87 \u00b1 0.04 3.63 \u00b1 0.07"
        },
        {
            "heading": "3.2. Evaluation",
            "text": "To evaluate the effectiveness of our proposed method, we conducted subjective listening tests: 5-point naturalness mean opinion score (MOS), 4-point speaker similarity MOS, and 4-point emotional similarity MOS2.\nWe asked native Japanese raters to make a quality judgment. The number of subjects for each evaluation was 14, 12, and 12, respectively. For all the tests, we randomly selected 20 utterances from the evaluation set for each system. In the naturalness evaluation, we evaluated the recorded speech of the source speaker and synthetic speech of five TTS systems, for a total of 360 utterances. In the speaker similarity evaluation, we set the recorded speech of the target speaker as a reference, and evaluated five TTS systems, for a total of 300 utterances. Note that the reference samples contained neutral, happiness, and sadness emotions, which constituted of 25 seconds in total. In the emotional similarity test, we evaluated 240 pairs of utterances that consisted of the recorded emotional speech of the source speaker and the synthetic speech from six TTS systems. Note that we used the neutral TTS system of the target speaker (i.e., TGT-NEU-TTS) as an anchor system only in the emotional similarity test."
        },
        {
            "heading": "3.3. Results",
            "text": "The results of the MOS evaluations are shown in Table 2. The findings can be summarized as follows: (1) VC data augmentation was effective for improving naturalness and speaker/emotional similarities over the multi-speaker TTS baseline (VC-TTS vs. MS-TTS), particularly for emotional styles; and (2) the proposed PS data augmentation further improved performance. In particular, naturalness and emotional similarity significantly improved for happiness (VCTTS vs. VC-TTS-PS) while achieving high emotion reproducibility of the source speaker nearly the same or even better than SRCTTS.; and (3) our proposed method achieved competitive performance, even with a limited number of training data (VC-TTS-PS vs. VC-TTS-PS-1K). We observed that VC-TTS-PS-1K achieved better naturalness and speaker similarity than VC-TTS-PS for the neutral style. For naturalness, this could be explained by the source speaker\u2019s database having a more natural speaking style than the target speaker, and the style of the source speaker was transferred to the target speaker\u2019s TTS when the relative amount of VC augmented data is high (i.e., VC-TTS-PS-1K). For speaker similarity, we hypothesized that it was caused by the difference of Fo statistics of the training data. To verify this, we examined the Fo statistics of pseudo neutral data used for training VC-TTS-PS and VC-TTSPS-1K, and found that the latter contained higher Fo on average 4.04 Hz. Because Fo of the target speaker was higher than that of the source speaker in our experiments, higher-pitched samples of VC-TTS-PS-1K tended to have higher speaker similarity for the neutral style. We encourage readers to listen to the samples provided on our demo page3.\n2Following the method used in the VC challenge [40], we used the 5-point responses 1 = Bad; 2 = Poor; 3 = Fair; 4 = Good; and 5 = Excellent; and the 4-point responses 1 = Different, absolutely sure; 2 = Different, not sure; 3 = Same, not sure; and 4 = Same, absolutely sure.\n3https://ryojerky.github.io/demo_vc-tts-ps/\nTo further verify the effectiveness of the proposed method, we analyzed the Fo distributions of the original data and the pseudo data generated by the VC model. As illustrated in Figure 4, the distribution of Fo with PS data augmentation was closer in shape to that of the original data for happiness. The results confirmed that the VC model trained on the proposed PS-augmented data generated richer pitch variations that were close to natural recordings compared with the VC model trained without PS data augmentation. By contrast, we observed similar distributions with and without the proposed PS augmentation for sadness. This can be explained as follows: sadness is less dynamic and has fewer pitch variations than happiness. The results suggest that our proposed method was particularly suited for emotionally expressive and dynamic styles, such as happiness."
        },
        {
            "heading": "4. Conclusion",
            "text": "We proposed a cross-speaker emotion style transfer method for a low-resource expressive TTS system, where expressive data is not available for the target speaker. Our proposed method combines PS-based and VC-based augmentation methods to stabilize training for both VC and TTS acoustic models. Subjective test results showed that the FastSpeech 2-based emotional TTS system learned by the proposed method improved naturalness and emotional similarity compared with conventional methods. In the future, we aim to apply the proposed method to more distinctive, expressive, and dynamic styles of speech."
        },
        {
            "heading": "5. Acknowledgments",
            "text": "This work was supported by Clova Voice, NAVER Corp., Seongnam, Korea."
        },
        {
            "heading": "6. References",
            "text": "[1] X. Zhu, S. Yang, G. Yang, et al., \u201cControlling emotion strength\nwith relative attribute for end-to-end speech synthesis,\u201d in Proc. ASRU, 2019, pp. 192\u2013199.\n[2] Y. Lei, S. Yang, and L. Xie, \u201cFine-grained emotion strength transfer, control and prediction for emotional speech synthesis,\u201d in Proc. SLT, 2021, pp. 423\u2013430.\n[3] Y. Wang, D. Stanton, Y. Zhang, et al., \u201cStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\u201d in Proc. ICML, PMLR, 2018, pp. 5180\u20135189.\n[4] R. Skerry-Ryan, E. Battenberg, Y. Xiao, et al., \u201cTowards endto-end prosody transfer for expressive speech synthesis with tacotron,\u201d in Proc. ICML, PMLR, 2018, pp. 4693\u20134702.\n[5] W.-N. Hsu, Y. Zhang, R. J. Weiss, et al., \u201cHierarchical generative modeling for controllable speech synthesis,\u201d in Proc. ICLR, 2019.\n[6] Y.-J. Zhang, S. Pan, L. He, et al., \u201cLearning latent representations for style control and transfer in end-to-end speech synthesis,\u201d in Proc. ICASSP, 2019, pp. 6945\u20136949.\n[7] Y. Jia, Y. Zhang, R. Weiss, et al., \u201cTransfer learning from speaker verification to multispeaker text-to-speech synthesis,\u201d in Proc. NeurIPS, 2018.\n[8] N. Tits, K. El Haddad, and T. Dutoit, \u201cExploring transfer learning for low resource emotional TTS,\u201d in Proc. IntelliSys, Springer, 2019, pp. 52\u201360.\n[9] Y.-A. Chung, Y. Wang, W.-N. Hsu, et al., \u201cSemi-supervised training for improving data efficiency in end-to-end speech synthesis,\u201d in Proc. ICASSP, 2019, pp. 6940\u20136944.\n[10] R. Valle, J. Li, R. Prenger, et al., \u201cMellotron: Multispeaker expressive voice synthesis by conditioning on rhythm, pitch and global style tokens,\u201d in Proc. ICASSP, 2020, pp. 6189\u20136193.\n[11] A. Gibiansky, S. Arik, G. Diamos, et al., \u201cDeep voice 2: Multi-speaker neural text-to-speech,\u201d in Proc. NeurIPS, 2017, pp. 2962\u20132970.\n[12] Z. Byambadorj, R. Nishimura, A. Ayush, et al., \u201cMulti-speaker TTS system for low-resource language using cross-lingual transfer learning and data augmentation,\u201d in Proc. APSIPA, 2021, pp. 849\u2013853.\n[13] M.-J. Hwang, R. Yamamoto, E. Song, et al., \u201cTTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis,\u201d in Proc. ICASSP, 2021, pp. 6598\u20136602.\n[14] G. Huybrechts, T. Merritt, G. Comini, et al., \u201cLow-resource expressive text-to-speech using data augmentation,\u201d in Proc. ICASSP, 2021, pp. 6593\u20136597.\n[15] R. Shah, K. Pokora, A. Ezzerg, et al., \u201cNon-autoregressive TTS with explicit duration modelling for low-resource highly expressive speech,\u201d in Proc. SSW, 2021.\n[16] M. S. Ribeiro, J. Roth, G. Comini, et al., \u201cCross-speaker style transfer for text-to-speech using data augmentation,\u201d in Proc. ICASSP (in press), 2022.\n[17] T. Kaneko, H. Kameoka, K. Tanaka, et al., \u201cCycleGAN-VC2: Improved CycleGAN-based non-parallel voice conversion,\u201d in Proc. ICASSP, 2019.\n[18] K. Liu, J. Zhang, and Y. Yan, \u201cHigh quality voice conversion through phoneme-based linear mapping functions with straight for Mandarin,\u201d in Proc. FSKD, 2007, pp. 410\u2013414.\n[19] A. Kanagaki, M. Tanaka, T. Nose, et al., \u201cCycleGAN-based high-quality non-parallel voice conversion with spectrogram and WaveRNN,\u201d in Proc. GCCE, 2020, pp. 356\u2013357.\n[20] J.-Y. Zhu, T. Park, P. Isola, et al., \u201cUnpaired image-to-image translation using cycle-consistent adversarial networks,\u201d in Proc. ICCV, 2017, pp. 2223\u20132232.\n[21] F. Charpentier and M. Stella, \u201cDiphone synthesis using an overlap-add technique for speech waveforms concatenation,\u201d in Proc. ICASSP, 1986, pp. 2015\u20132018.\n[22] M. Morise, F. Yokomori, and K. Ozawa, \u201cWORLD: A vocoderbased high-quality speech synthesis system for real-time applications,\u201d IEICE Trans. on Information and Systems, vol. 99, no. 7, pp. 1877\u20131884, 2016.\n[23] Y. Tohkura, F. Itakura, and S. Hashimoto, \u201cSpectral smoothing technique in PARCOR speech analysis-synthesis,\u201d IEEE Trans. on Acoustics, Speech, and Signal Process., vol. 26, no. 6, pp. 587\u2013596, 1978.\n[24] M. Morise, Speech analysis and synthesis (in Japanese). CORONA PUBLISHING CO.,LTD., 2018.\n[25] T. Okamoto, T. Toda, Y. Shiga, et al., \u201cReal-time neural textto-speech with sequence-to-sequence acoustic model and WaveGlow or single gaussian WaveRNN vocoders.,\u201d in Proc. Interspeech, 2019, pp. 1308\u20131312.\n[26] K. Yu and S. Young, \u201cContinuous f0 modeling for hmm based statistical parametric speech synthesis,\u201d IEEE Trans. on Audio, Speech, and Lang. Process., vol. 19, no. 5, pp. 1071\u20131079, 2010.\n[27] M.-J. Hwang, R. Yamamoto, E. Song, et al., \u201cHigh-fidelity Parallel WaveGAN with multi-band harmonic-plus-noise model,\u201d in Proc. Interspeech, 2021, pp. 2227\u20132231.\n[28] Y. Ren, C. Hu, T. Qin, et al., \u201cFastSpeech 2: Fast and highquality end-to-end text-to-speech,\u201d in Proc. ICLR, 2021.\n[29] H. Ming, D. Huang, M. Dong, et al., \u201cFundamental frequency modeling using wavelets for emotional voice conversion,\u201d in Proc. ACII, 2015, pp. 804\u2013809.\n[30] R. Yamamoto, E. Song, and J.-M. Kim, \u201cParallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,\u201d in Proc. ICASSP, 2020, pp. 6199\u20136203.\n[31] A. Gulati, J. Qin, C.-C. Chiu, et al., \u201cConformer: Convolutionaugmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020.\n[32] P. Guo, F. Boyer, X. Chang, et al., \u201cRecent developments on ESPnet toolkit boosted by conformer,\u201d in Proc. ICASSP, 2021, pp. 5874\u20135878.\n[33] H. Choi, S. Park, J. Park, et al., \u201cMulti-speaker emotional acoustic modeling for cnn-based speech synthesis,\u201d in Proc. ICASSP, 2019, pp. 6950\u20136954.\n[34] K. Matsubara, T. Okamoto, R. Takashima, et al., \u201cInvestigation of training data size for real-time neural vocoders on CPUs,\u201d Acoust. Sci. Tech., vol. 42, no. 1, pp. 65\u201368, 2021.\n[35] E. Song, F. K. Soong, and H.-G. Kang, \u201cEffective spectral and excitation modeling techniques for LSTM-RNN-based speech synthesis systems,\u201d IEEE/ACM Trans. Audio, Speech, and Lang. Process., vol. 25, no. 11, pp. 2152\u20132161, 2017.\n[36] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR, 2015.\n[37] Y. Yasuda, X. Wang, S. Takaki, et al., \u201cInvestigation of enhanced Tacotron text-to-speech synthesis systems with self-attention for pitch accent language,\u201d in Proc. ICASSP, 2019, pp. 6905\u20136909.\n[38] T. Hayashi, R. Yamamoto, K. Inoue, et al., \u201cESPnet-TTS: Unified, reproducible, and integratable open source end-to-end textto-speech toolkit,\u201d in Proc. ICASSP, 2020, pp. 7654\u20137658.\n[39] L. Liu, H. Jiang, P. He, et al., \u201cOn the variance of the adaptive learning rate and beyond,\u201d in Proc. ICLR, 2020.\n[40] Y. Zhao, W.-C. Huang, X. Tian, et al., \u201cVoice conversion challenge 2020 \u2013 intra-lingual semi-parallel and cross-lingual voice conversion \u2013,\u201d in Proc. Joint workshop for the Blizzard Challenge and Voice Conversion Challenge 2020, 2020, pp. 80\u201398."
        }
    ],
    "title": "Cross-Speaker Emotion Transfer for Low-Resource Text-to-Speech Using Non-Parallel Voice Conversion with Pitch-Shift Data Augmentation",
    "year": 2022
}