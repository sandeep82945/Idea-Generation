{
    "abstractText": "Association rule mining is one of the most studied research fields of data mining, with applications ranging from grocery basket problems to explainable classification systems. Classical association rule mining algorithms have several limitations, especially with regards to their high execution times and number of rules produced. Over the past decade, neural network solutions have been used to solve various optimization problems, such as classification, regression or clustering. However there are still no efficient way association rules using neural networks. In this paper, we present an auto-encoder solution to mine association rule called ARM-AE. We compare our algorithm to FP-Growth and NSGAII on three categorical datasets, and show that our algorithm discovers high support and confidence rule set and has a better execution time than classical methods while preserving the quality of the rule set produced.",
    "authors": [
        {
            "affiliations": [],
            "name": "Th\u00e9ophile Berteloot"
        },
        {
            "affiliations": [],
            "name": "Richard Khoury"
        },
        {
            "affiliations": [],
            "name": "Audrey Durand"
        }
    ],
    "id": "SP:89abf3425ddef06e7d50380aa1bfe385ad2c759c",
    "references": [
        {
            "authors": [
                "Rakesh Agrawal",
                "Ramakrishnan Srikant"
            ],
            "title": "Fast algorithms for mining association rules",
            "venue": "In Proc. 20th int. conf. very large data bases, VLDB,",
            "year": 1994
        },
        {
            "authors": [
                "Binoy B Nair",
                "VP Mohandas",
                "Nikhil Nayanar",
                "ESR Teja",
                "S Vigneshwari",
                "KVNS Teja"
            ],
            "title": "A stock trading recommender system based on temporal association rule mining",
            "venue": "SAGE Open,",
            "year": 2015
        },
        {
            "authors": [
                "Azhar Hussein Alkeshuosh",
                "Mariam Zomorodi Moghadam",
                "Inas Al Mansoori",
                "Moloud Abdar"
            ],
            "title": "Using pso algorithm for producing best rules in diagnosis of heart disease",
            "venue": "In 2017 international conference on computer and applications (ICCA),",
            "year": 2017
        },
        {
            "authors": [
                "Liqiang Geng",
                "Howard J Hamilton"
            ],
            "title": "Choosing the right lens: Finding what is interesting in data mining",
            "venue": "In Quality measures in data mining,",
            "year": 2007
        },
        {
            "authors": [
                "Christian Borgelt"
            ],
            "title": "Keeping things simple: finding frequent item sets by recursive elimination. In Proceedings of the 1st international workshop on open source data mining: frequent pattern mining",
            "year": 2005
        },
        {
            "authors": [
                "Borgelt Christian"
            ],
            "title": "An implementation of the fp-growth algorithm. In Proceedings of the 1st international workshop on open source data mining: frequent pattern mining",
            "year": 2005
        },
        {
            "authors": [
                "Jian Pei",
                "Jiawei Han",
                "Hongjun Lu",
                "Shojiro Nishio",
                "Shiwei Tang",
                "Dongqing Yang"
            ],
            "title": "H-mine: Fast and space-preserving frequent pattern mining in large databases",
            "venue": "IIE transactions,",
            "year": 2007
        },
        {
            "authors": [
                "Ian Fund"
            ],
            "title": "Comparing Association Rules and Deep Neural Networks on Medical Data",
            "venue": "PhD thesis, University of Houston,",
            "year": 2019
        },
        {
            "authors": [
                "Philippe Fournier-Viger",
                "Cheng-Wei Wu",
                "Vincent S Tseng"
            ],
            "title": "Mining top-k association rules",
            "venue": "In Canadian Conference on Artificial Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Akbar Telikani",
                "Amir H Gandomi",
                "Asadollah Shahbahrami"
            ],
            "title": "A survey of evolutionary computation for association rule mining",
            "venue": "Information Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "D Mart\u00edn",
                "Alejandro Rosete",
                "Jes\u00fas Alcal\u00e1-Fdez",
                "Francisco Herrera"
            ],
            "title": "Qar-cip-nsga-ii: A new multi-objective evolutionary algorithm to mine quantitative association rules",
            "venue": "Information Sciences,",
            "year": 2014
        },
        {
            "authors": [
                "Kamel Eddine Heraguemi",
                "Nadjet Kamel",
                "Habiba Drias"
            ],
            "title": "Multi-swarm bat algorithm for association rule mining using multiple cooperative strategies",
            "venue": "Applied Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Youcef Djenouri",
                "Ahcene Bendjoudi",
                "Djamel Djenouri",
                "Marco Comuzzi"
            ],
            "title": "Gpu-based bio-inspired model for solving association rules mining problem",
            "venue": "25th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP),",
            "year": 2017
        },
        {
            "authors": [
                "Kin Gwn Lore",
                "Adedotun Akintayo",
                "Soumik Sarkar"
            ],
            "title": "Llnet: A deep autoencoder approach to natural low-light image enhancement",
            "venue": "Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Alan Ramponi",
                "Barbara Plank"
            ],
            "title": "Neural unsupervised domain adaptation in nlp\u2014a survey",
            "venue": "arXiv preprint arXiv:2006.00632,",
            "year": 2020
        },
        {
            "authors": [
                "Erxue Min",
                "Xifeng Guo",
                "Qiang Liu",
                "Gen Zhang",
                "Jianjing Cui",
                "Jun Long"
            ],
            "title": "A survey of clustering with deep learning: From the perspective of network architecture",
            "venue": "IEEE Access,",
            "year": 2018
        },
        {
            "authors": [
                "Fuzhen Zhuang",
                "Xiaohu Cheng",
                "Ping Luo",
                "Sinno Jialin Pan",
                "Qing He"
            ],
            "title": "Supervised representation learning: Transfer learning with deep autoencoders",
            "venue": "In Twenty-Fourth International Joint Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Zhenhua Zhang",
                "Qing He",
                "Jing Gao",
                "Ming Ni"
            ],
            "title": "A deep learning approach for detecting traffic accidents from social media data",
            "venue": "Transportation research part C: emerging technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Ana Valdivia",
                "Eugenio Mart\u00ednez-C\u00e1mara",
                "Iti Chaturvedi",
                "M Luzon",
                "Erik Cambria",
                "Yew-Soon Ong",
                "Francisco Herrera"
            ],
            "title": "What do people think about this monument? understanding negative reviews via deep learning, clustering and descriptive rules",
            "venue": "Journal of Ambient Intelligence and Humanized Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Konstantinos Vougas",
                "Magdalena Krochmal",
                "Thomas Jackson",
                "Alexander Polyzos",
                "Archimides Aggelopoulos",
                "Ioannis S Pateras",
                "Michael Liontos",
                "Anastasia Varvarigou",
                "Elizabeth O Johnson",
                "Vassilis Georgoulias"
            ],
            "title": "Deep learning and association rule mining for predicting drug response in cancer. a personalised medicine",
            "venue": "approach. BioRxiv,",
            "year": 2017
        },
        {
            "authors": [
                "Peddi Kishor",
                "Porika Sammulal"
            ],
            "title": "Association rule mining using an unsupervised neural network with an optimized genetic algorithm",
            "venue": "In International Conference on Communications and Cyber Physical Engineering",
            "year": 2018
        },
        {
            "authors": [
                "Xiaofeng Li",
                "Dong Li",
                "Yuanbei Deng",
                "Jinming Xing"
            ],
            "title": "Intelligent mining algorithm for complex medical data based on deep learning",
            "venue": "Journal of Ambient Intelligence and Humanized Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Raschka"
            ],
            "title": "Mlxtend: Providing machine learning and data science utilities and extensions to python\u2019s scientific computing stack",
            "venue": "The Journal of Open Source Software,",
            "year": 2018
        },
        {
            "authors": [
                "Kalyanmoy Deb",
                "Amrit Pratap",
                "Sameer Agarwal",
                "TAMT Meyarivan"
            ],
            "title": "A fast and elitist multiobjective genetic algorithm: Nsga-ii",
            "venue": "IEEE transactions on evolutionary computation,",
            "year": 2002
        },
        {
            "authors": [
                "F\u00e9lix-Antoine Fortin",
                "Fran\u00e7ois-Michel De Rainville",
                "Marc-Andr\u00e9 Gardner Gardner",
                "Marc Parizeau",
                "Christian Gagn\u00e9"
            ],
            "title": "Deap: Evolutionary algorithms made easy",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Keywords Association rules mining \u00b7 Deep learning \u00b7 Auto-encoder"
        },
        {
            "heading": "1 Introduction",
            "text": "Association rule mining (ARM) was first introduced by Agrawal [1] to solve the grocery basket problem, and since then it has found numerous applications in Knowledge Discovery in Database (KDD) problems ranging from financial analysis [2] to medical diagnostics [3]. An association rule (AR) is an implication of the form A\u21d2 C, which can be read as \u201cif antecedent A is true then consequent C must be true\u201d, where A and C are sets of different items (itemsets) in a database. An AR is defined by its antecedent, its consequent and two measures [4].The first one is the support, which is the proportion of rows in the dataset where both the antecedent and the consequent appear. The second measure is the confidence, the conditional probability to observe the consequent given an observation of the antecedent.\nThe most widely-used mining strategies Apriori [1] and other exhaustive strategies [5, 6, 7] typically work by first mining frequent itemsets, then combining those itemsets to produce association rules. However, all these algorithms face the same problems: the number of rules they produce increases exponentially with the number of items in the database, and thus it becomes impossible for a human to sort through the rules returned to pick out the best ones [8]. Their execution time also become an issue with massive datasets [8]. Finally, these algorithms need support and confidence thresholds in order to efficiently search through the solution space, and those thresholds need to be carefully chosen: low values can lead to long execution times and an overabundance of rules, while high values cause the algorithm to miss interesting rules.\nResearchers have tried to address these issues using top-k algorithms [9], metaheuristics [10] such as NSGAII [11], parallelism [12], and GPU [13]. Meanwhile, deep learning has been use in research from image analysis [14] to natural language processing [15] and have demonstrated a formidable capacity to find underlying relationships between input data [16]. Auto-encoders (AE) are a kind of neural network often used to find clusters [17] or to learn representations [18]. AE are trained in a supervised way using a dataset as an input and the same dataset as a target .\nThus, AE seem applicable to ARM, because while trying to rebuild the original dataset, the AE will learn link between columns in the dataset, we then can use those links to mine AR.\nThe contributions of this paper are therefore twofold:\nar X\niv :2\n30 4.\n13 71\n7v 1\n[ cs\n.L G\n] 2\n6 A\npr 2\n02 3\n\u2022 We propose a new approach to mine AR using AE, which we call ARM-AE.\n\u2022 We compare our approach to FP-Growth, an exhaustive state-of-the-art algorithm, and NSGAII, a commonly used metaheuristic, based on execution time, number of rules produced, average support and confidence of the rules produced, and coverage (the percentage of FP-Growth rules that our approach finds). We use three categorical datasets taken from the popular UCI repository [19], ranging from a small dataset to a massive one. In all cases, ARM-AE is able to efficiently mine a limited number of high-quality rules, without the need to specify any mandatory thresholds.\nThe rest of this paper is organized as follows. In section 2 we present the literature related to the usage of neural networks to solve ARM problems. In section 3 we present our ARM-AE model. We conduct experiments with it and with FP-Growth as a benchmark in 4, and discuss the results in section 5. Finally, we provide a synthesis of our results and concluding remarks in section 6."
        },
        {
            "heading": "2 Related Work",
            "text": "Several previous work have used deep learning within the context of ARM. For example, AR and a deep neural network have been used previously to predict car accidents based on tweets [20]. More specifically, AR were mined using Apriori on the tweets\u2019 tokens and used as input for a deep belief network to classify whether a tweet is about a car accident or not. AR and convolutional neural networks (CNNs) have also been used together for classifying reviews of three Spanish monuments and extracting the aspects that visitors didn\u2019t like to help the monuments to improve the visitors\u2019 experience [21]. CNNs were first used to extract the aspects of monuments people felt strongly about. Then they were clustered into groups with the same meaning but represented by different words. Finally AR were mined to summarize sentiment paired with aspects. As a last example, Apriori has been used together with deep learning to predict which anti-cancer drugs should be given to an individual patient [22]. In this work, AR were mined from a pharmacogenomics dataset, and the extracted features were used to train a deep neural network.\nWhile examples of previous works cited above perform ARM and deep learning, they do not perform ARM using deep learning. Few works have tackled that challenge. One of them is [23], in which a neural network and a genetic algorithm were used to mine AR. They used a self-organizing map (SOM) to mine clustered pattern from the data, and then used a genetic algorithm to create the AR based on these patterns. In [24], a convolutional neural network was used to mine AR on medical datasets with complex attributes, then a probability estimation method validate the AR found.\nNone of the systems above use deep-learning to mine ARs directly, they either use deep learning to mine frequent itemsets or they use it alongside of ARM algorithms. Testing itemset combinations to find good ARs is a very time consuming task. The proposed ARM-AE approach mines ARs directly and thus doesn\u2019t spend time trying different combinations."
        },
        {
            "heading": "3 ARM-AE: Association Rule Mining with Auto-Encoders",
            "text": "In the ARM-AE context an item is the name of a column of a dataset and an itemset is a group of item with no duplicates. In order to compare the different rules we use the support and the confidence. The support is define as the proportion of rows in the dataset D where both A and C appear: Supp(A,C) = |A \u2229 C|/|D|, with A the antecedent and C the consequent, two itemsets without item in common. The confidence is the conditional probability to observe the consequent given an observation of the antecedent: Conf(A,C) = |A \u2229 C|/|A|. We propose to leverage the representation power of auto-encoders (AEs) to directly extract AR. An AE consists of two parts, first the encoder which consists of several layers of decreasing size, and second the decoder which consists of several layer of increasing size. The output of the encoder is the input of the decoder. The training goal of an AE is for the encoder to generate a compressed representation of the input, and for the decoder to generate a transformed version of the original input. Training for ARM-AE doesn\u2019t differ from a classical AE training. The input and the requested output of the AE are each row of the dataset. The AE will thus learn to encode an input itemset in relation to similar itemsets, and the decoded output will be the similar itemsets, with item having a value between 0 and 1 representing how likely they are to belong in that itemset. Given that our goal is to learn a representation of the entire dataset, there is no need for evaluation or test datasets, and we train our model on the entire dataset. During training the AE will learn to recreate the input binary row, thus after training if an itemset is used as input, the AE will recreate it, but not perfectly. Items which appear frequently with the input itemset in the dataset will have higher output value than items which never appear with the input itemset in the dataset.\nThe core of our ARM-AE approach is presented in Algorithm 1. Our algorithm tries to mine a set of N \u00d7M rules, or N rules of maximum antecedent length M along with the most interesting subsets of the antecedent itemset, for each consequent. In ARM-AE, each item in the dataset becomes a consequent (line 5), though it would be trivial to change that line to focus the algorithm on a specific subset of items of interest. The consequent is composed of only one item, which is the most common use-case for ARM. Initially, the consequent is the only input for the AE (lines 8-11). The AE\nAlgorithm 1 Association Rule Mining Auto-Encoder. 1: function ARM-AE(N,M,L) . N number of maximum-length rules per consequent, M maximum number of\nantecedents per rule, L similarity threshold 2: Consequents\u2190 items 3: AE\u2190trained Auto-encoder 4: Rules\u2190 \u2205 5: for \u2200Consequent \u2208 Consequents do 6: ConsequentRules\u2190 \u2205 7: for from I = 1 to N do 8: Antecedents\u2190 \u2205 9: for from J = 1 to M do\n10: InputArray\u2190 Antecedents \u222a Consequent 11: AEOutput\u2190 AE.forward(InputArray) 12: AEOutput\u2190 Sort(AEOutput) 13: for Antecedent \u2208 AEOutput do 14: if Antecedent 6= Consequent and Antecedent /\u2208 Antecedents and\nComputeSimilarity(ConsequentRules,Antecedents \u222a Antecedent) \u2264 L then 15: Antecedents\u2190 Antecedents \u222a Antecedent 16: ConsequentRules\u2190 ConsequentRules \u222a Antecedents 17: Exit loop of line 13 (go to line 9) 18: Rules\u2190 Rules \u222a ConsequentRules 19: return Rules\ngives as output a score between 0 and 1 for each item in the dataset; the higher the score, the higher the probability that this item appears often with the AE input. This output is sorted in decreasing order, and the item with the highest value that isn\u2019t the consequent, already part of the antecedent, or would make the antecedent more similar than a threshold L to a previously-discovered one as describe in paragraph 3.2, is added to the antecedent of the rule and this new rule is added to the set of AR for that consequent (lines 12-16). The antecedent is appended to the consequent as input to the AE (line 10) and the loop repeats, adding one item at a time to the antecedent until it reaches the maximum length of M items (line 9). Then another rule is generated from scratch for that consequent until N rules are found (line 7) before moving on to the next consequent (line 5).The complete code of ARM-AE is available on GitHub.1."
        },
        {
            "heading": "3.1 AE Training and Hyperparameters",
            "text": "As was shown previously, there are three main hyperparameters to our algorithm, namely the number of maximumlength rules produced for each consequent N , the maximum number of items in the antecedent M , and the maximum similarity threshold L. There are also the hyperparameters for the AE itself, such as the number of layers and of neurons per layer, the number of training epochs, the learning rate, the activation function, the loss function, and the optimizer. These are typical for all AE architectures."
        },
        {
            "heading": "3.2 Similarity",
            "text": "An important step is computing the similarity between a new antecedent and those already found for a consequent, at line 14 of Algorithm 1. Without this step, starting from the same consequent, the AE would always discover the same antecedent. Imposing a maximum level of similarity between a new antecedent and those already discovered forces the AE to explore the solution space and to generate a variety of different AR for each consequent.\nFor our implementation of ARM-AE, the similarity metric we use is the maximum overlap between the items in the antecedent of the candidate rule and of existing rules for the same consequent, expressed as a percentage of the length of the existing rule. We only consider the similarity to rules of the same length as the candidate or longer, since otherwise an existing short and general rule would prevent the discovery of longer and more specific candidate rules. We present\n1https://github.com/TheophileBERTELOOT/ARM-AE\nour similarity function in Algorithm 2. We can note however that this similarity function is not necessary for ARM-AE to function; another problem-specific similarity function could be used instead.\nAlgorithm 2 Compute similarity between a set of antecedents and a candidate antecedent 1: function COMPUTE SIMILARITY(PreviousAntecedents,Candidate) 2: . PreviousAntecedents the list of antecedents find for a specific consequent, Candidate the antecedent whose\nsimilarity is being calculated 3: MaximumSimilarity\u2190 0 4: for \u2200Antecedent \u2208 PreviousAntecedents do 5: if |Antecedent| < |Candidate| then 6: Skip Antecedent 7: Similarity\u2190 0 8: for \u2200Item \u2208 Antecedent do 9: if Item \u2208 Candidate then Similarity\u2190 Similarity + 1\n10: Similarity\u2190 Similarity|Antecedent| 11: MaximumSimilarity\u2190 max(MaximumSimilarity,similarity) 12: return MaximumSimilarity"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We tested ARM-AE using three categorical datasets taken from the popular UCI repository [19]. Those datasets were selected because they have a wide range of number of rows and items. Each dataset is preprocessed by replacing each categorical attribute with a one-hot set of binary attributes, with one column for each category value. Table 1 presents the list of datasets along with their number of rows and attributes after binarization."
        },
        {
            "heading": "4.2 Hyperpameters",
            "text": "For the architecture of our AE, both the encoder and decoder are made up of three fully connected layers. The number of neurons for each layer is the number of different items in the dataset. This architecture has shown the best results in our experiments. The neurons use the tanh function shown in Equation 1 as their activation function because ARM-AE is build to approximate the support of the rules and support is define between 0 and 1 therefore we choose a common activation function define between 0 and 1. The learning rate is 10\u22123. The MSELoss shown in equation 2 is our loss function (where x is the input and y the target), and we use the Adam optimizer.\nTanh(x) = expx\u2212 exp\u2212x expx+ exp\u2212x\n(1)\nMSELoss(x, y) = (x\u2212 y)2 (2)\nWe set the algorithm to look for rules with a maximum of two items in the antecedent (M = 2) and for two rules per consequent (N = 2). The similarity threshold L = 0.5, so there will be a maximum of one item overlap between the antecedents of the two pairs of rules. In order to set the number of training epochs, we use the difference of loss between two epochs; when this difference becomes lower than a threshold then we take the previous epoch as our final model. We tested several values of for this threshold. Figure 1 shows the average support of the rules found by ARM-AE for different values of the threshold on the three datasets; we did 10 experiments for each threshold value and trained for 100 epochs in each case, and show the average in the figure. A threshold of 1 mean that there is no training\nat all, and the result is the a random initialisation of the network. The figure shows that the best threshold is 0.1, which in practice mean 1 to 5 epoch. Performance decreases significantly after this threshold. Thus we choose to use 0.1 as our threshold for the rest of our experiments."
        },
        {
            "heading": "4.3 Benchmarks",
            "text": "We use FPGrowth [5] as one of our two benchmarks, as it is one of the fastest and most popular state-of-the-art exhaustive ARM algorithms. We used the implementation found in the Mlxtend library [25]. The support and confidence thresholds for each dataset are picked so the number of rules mined with one or two items in the antecedents and one item in the consequent is nearly the same as the number of rules produced by our algorithm. The thresholds are included in Table 1. Our second benchmark is the NSGAII [26] algorithm, chosen because genetic algorithms and domination concepts are a dominant trend in ARM [10] and NSGAII is a commonly-used baseline for meta-heuristic. We used the DEAP library [27] to implement our NSGAII. For our benchmark algorithm NSGAII, we used a population of 100 individuals, a crossover ratio of 0.9 and a mutation probability of 0.01. We set a maximum of 2 antecedents and 1 consequent in the rules. We keep the 300 best rules found across all generation for the plants and chess datasets and the 150 best for the nursery dataset, so results are comparable with ARM-AE and FP-Growth outputs. To set the number of generation we check if the average support and average confidence of the best rules found so far increase above than a threshold. If both decrease or increase less than the threshold then the NSGAII algorithm stops. We use a threshold of 0.01."
        },
        {
            "heading": "4.4 Metrics",
            "text": "We compute several metrics to compare ARM-AE and our benchmarks. The first one is the number of rules found by ARM-AE that have a support greater than 0. A rule that has a support greater than 0 is one that actually exists in the dataset. But since ARM-AE does not compute the support of the rules during its search, unlike FP-Growth and other exhaustive algorithms, it offers no guarantee in that respect, thus the importance of this metric. The second metric is the percentage of rules found by FP-Growth that are also found by our algorithm and NSGAII. Since FP-Growth performs an exhaustive search for all the best rules, this will indicate how close to complete the ARM-AE and NSGAII rule sets are. Recall however that we set ARM-AE\u2019s hyperparameters to generate four AR per consequent with only one or two items in the antecedent. To have a fair comparison, we have therefore set FP-Growth\u2019s thresholds to extract the best four AR per consequent with that length, as mentioned previously. Next, we compare the execution times for all three algorithms. Finally we compare the average support and confidence of the rules produced by the three algorithms."
        },
        {
            "heading": "5 Results and Analysis",
            "text": "We trained and tested our AE and NSGAII independently 10 times with each dataset, in order to account for differences in the network\u2019s random initialization and the stochastic nature of meta-heuristics. We report the average results over all 10 runs. The FPGrowth algorithm, having no random initialization, was only run once.\nThe first metric to consider is the percentage of rules produced by ARM-AE that have a support greater than 0. The results shown in Table 2 indicate that up to 2% of the rules produced actually don\u2019t exist in the dataset. Since ARM-AE is required to generate a number of rules for each consequent, some very-low-support or zero-support rules will be included when no high-support rules exist for a consequent. Without a computation of support, ARM-AE cannot detect or filter out these cases. This is a limitation of our approach. However, since such rules account for less than 2% of the extracted rule set, the amount of noise included is not a major issue.\nThe next metrics compare the AR set mined by ARM-AE, NSGAII and FP-Growth. First we consider the proportion of FP-Growth\u2019s rule set that is mined by ARM-AE and NSGAII. FP-Growth exhaustively mines all rules with one consequent and one or two antecedents that are above the set support and confidence threshold. For its part, ARM-AE discovers between 20% and 44% of that set, as is shown in Table 3. Regarding NSGAII, it mines up 8% of the rules set mined by FP-Growth. That can be explain by the highly stochastic aspect of genetic algorithm and because NSGAII focuses on non-dominated rules where FP-Growth focuses on high support and high confidence rules. In non-dominated mining, if a rule has a very low support but a very high confidence and no other rules have a higher support with at least the same confidence, then it may not be kept. Table 3 also shows that the average support and confidence of the rules mined by our three algorithms. The results show that the rules found by ARM-AE have a support and confidence almost as high as those mined by FP-Growth, and in fact the confidence is higher in the case of the Plants dataset. That is despite ARM-AE including up to 2% of zero-support rules in its results, as mentioned previously. This shows that, although ARM-AE mines less than half the rules found by FP-Growth, it mines a high-quality subset of the rules that exist in the database. NSGAII mines a rule set with very high confidence and lower support than the two other algorithms. ARM-AE mines a rule set that is more similar to the one found by FP-Growth than NSGAII.\nFinally, we compare the performance of the three algorithms in terms of execution time and total number of rules generated, in Table 4. We consider first the execution time for the entire algorithm, including training the AE in the case of ARM-AE. The most time-consuming step in every ARM algorithm is to compute the support and confidence of rules. FP-Growth use a Fp-tree to speed up the mining of frequent itemsets, but then must compute the support and confidence of all the pairs of itemsets. NSGAII compute the support and confidence for a large part of the population at each generation explaining why it is slower than FP-Growth. By contrast, ARM-AE compute the support and confidence only for the subset of rules mined. For the three datasets, ARM-AE is much more efficient than FP-Growth and NSGAII in terms of execution time. In terms of number of rules, ARM-AE and NSGAII have the advantage of generating a small rule set whose size is controlled by the algorithm\u2019s hyperparameters. By contrast, the exhaustive search of FP-Growth generates tens to hundreds of thousands of rules. As mentioned previously, this is one of the main weaknesses of algorithms such as this one [8]: it is impossible for a human to read and understand such a massive set of rules, and additional filtering or ranking algorithms must then be applied in post-processing before the rules are suitable for human use. ARM-AE and NSGAII do not suffer from that issue."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we introduced a new deep learning architecture to mine association rules, the Association Rules Mining Auto-Encoder(ARM-AE) algorithm. We compared this new algorithm to the popular state-of-the-art FP-Growth algorithm and to the state-of-the-art genetic algorithm NSGAII on three categorical datasets. Our results show that ARM-AE produces a set of rules with an average support and confidence close to that of FP-Growth and with higher support than the one of NSGAII, but with a much smaller execution time and by generating a much smaller number of rules than FP-Growth, which makes the results much more usable for a human being. Moreover, ARM-AE has hyperparameters to control the number of rule produced for each consequent, the maximum number of items in the antecedent of the rules, and the similarity between the rules, which again makes the algorithm much more user-friendly. The main drawbacks of ARM-AE stem from the fact it does not compute support and confidence for its rules, which mean it cannot filter out zero-support rules and cannot give an importance ranking of the rules it mined, but these will be addressed in future works."
        },
        {
            "heading": "Acknowledgement",
            "text": "This research was made possible by the support of the INSPQ, as well as the financial support of the Canadian research funding agencies CIHR and NSERC."
        }
    ],
    "title": "ASSOCIATION RULES MINING WITH AUTO-ENCODERS",
    "year": 2023
}