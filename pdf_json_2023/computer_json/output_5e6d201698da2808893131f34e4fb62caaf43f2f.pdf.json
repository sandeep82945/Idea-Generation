{
    "abstractText": "In some machine learning applications the availability of labeled instances for supervised classification is limited while unlabeled instances are abundant. Semisupervised learning algorithms deal with these scenarios and attempt to exploit the information contained in the unlabeled examples. In this paper, we address the question of how to evolve neural networks for semi-supervised problems. We introduce neuroevolutionary approaches that exploit unlabeled instances by using neuron coverage metrics computed on the neural network architecture encoded by each candidate solution. Neuron coverage metrics resemble code coverage metrics used to test software, but are oriented to quantify how the different neural network components are covered by test instances. In our neuroevolutionary approach, we define fitness functions that combine classification accuracy computed on labeled examples and neuron coverage metrics evaluated using unlabeled examples. We assess the impact of these functions on semi-supervised problems with a varying amount of labeled instances. Our results show that the use of neuron coverage metrics helps neuroevolution to become less sensitive to the scarcity of labeled data, and can lead in some cases to a more robust generalization of the learned classifiers. keywords: neuroevolution, neuron coverage, semi-supervised learning, neural networks, deep learning, NAS",
    "authors": [
        {
            "affiliations": [],
            "name": "Roberto Santana"
        },
        {
            "affiliations": [],
            "name": "Ivan Hidalgo-Cenalmor"
        },
        {
            "affiliations": [],
            "name": "Unai Garciarena"
        },
        {
            "affiliations": [],
            "name": "Alexander Mendiburu"
        },
        {
            "affiliations": [],
            "name": "Jose Antonio Lozano"
        }
    ],
    "id": "SP:60c5cb2df9247e2d6055d0a138a23b2ef7517c57",
    "references": [
        {
            "authors": [
                "Edgar Galv\u00e1n",
                "Peter Mooney"
            ],
            "title": "Neuroevolution in deep neural networks: Current trends and future challenges",
            "venue": "IEEE Transactions on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Kenneth O Stanley",
                "Jeff Clune",
                "Joel Lehman",
                "Risto Miikkulainen"
            ],
            "title": "Designing neural networks through neuroevolution",
            "venue": "Nature Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Avrim Blum",
                "Tom Mitchell"
            ],
            "title": "Combining labeled and unlabeled data with co-training",
            "venue": "In Proceedings of the Eleventh Annual Conference on Computational Learning Theory,",
            "year": 1998
        },
        {
            "authors": [
                "Avrim Blum",
                "Shuchi Chawla"
            ],
            "title": "Learning from labeled and unlabeled data using graph mincuts",
            "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,",
            "year": 2001
        },
        {
            "authors": [
                "Kristin Bennett",
                "Ayhan Demiriz"
            ],
            "title": "Semi-supervised support vector machines",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1998
        },
        {
            "authors": [
                "Jeannie M Fitzgerald",
                "R Muhammad Atif Azad",
                "Conor Ryan"
            ],
            "title": "Geml: Evolutionary unsupervised and semi-supervised learning of multi-class classification with grammatical evolution",
            "venue": "In 2015 7th International Joint Conference on Computational Intelligence (IJCCI),",
            "year": 2015
        },
        {
            "authors": [
                "Mikhail Kamalov",
                "Aur\u00e9lie Boisbunon",
                "Carlo Fanara",
                "Ingrid Grenet",
                "Jonathan Daeden"
            ],
            "title": "Pazoe: classifying time series with few labels",
            "venue": "29th European Signal Processing Conference (EUSIPCO),",
            "year": 2021
        },
        {
            "authors": [
                "Sara Silva",
                "Leonardo Vanneschi",
                "Ana IR Cabral",
                "Maria J Vasconcelos"
            ],
            "title": "A semi-supervised Genetic Programming method for dealing with noisy labels and hidden overfitting",
            "venue": "Swarm and Evolutionary Computation,",
            "year": 2018
        },
        {
            "authors": [
                "Chenxi Liu",
                "Piotr Doll\u00e1r",
                "Kaiming He",
                "Ross Girshick",
                "Alan Yuille",
                "Saining Xie"
            ],
            "title": "Are labels necessary for neural architecture search",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European 14 Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Lo\u0131\u0308c Pauletto",
                "Massih-Reza Amini",
                "Nicolas Winckler"
            ],
            "title": "Se2nas: Self-semi-supervised architecture optimization for semantic segmentation",
            "venue": "In 2022 26th International Conference on Pattern Recognition (ICPR),",
            "year": 2022
        },
        {
            "authors": [
                "Lei Ma",
                "Felix Juefei-Xu",
                "Fuyuan Zhang",
                "Jiyuan Sun",
                "Minhui Xue",
                "Bo Li",
                "Chunyang Chen",
                "Ting Su",
                "Li Li",
                "Yang Liu"
            ],
            "title": "Deepgauge: Multi-granularity testing criteria for deep learning systems",
            "venue": "In Proceedings of the 33rd ACM/IEEE international conference on automated software engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Kexin Pei",
                "Yinzhi Cao",
                "Junfeng Yang",
                "Suman Jana"
            ],
            "title": "Deepxplore: Automated whitebox testing of deep learning systems",
            "venue": "In Proceedings of the 26th Symposium on Operating Systems Principles,",
            "year": 2017
        },
        {
            "authors": [
                "Zhou Yang",
                "Jieke Shi",
                "Muhammad Hilmi Asyrofi",
                "David Lo"
            ],
            "title": "Revisiting neuron coverage metrics and quality of deep neural networks",
            "venue": "IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER),",
            "year": 2022
        },
        {
            "authors": [
                "Seokhyun Lee",
                "Sooyoung Cha",
                "Dain Lee",
                "Hakjoo Oh"
            ],
            "title": "Effective white-box testing of deep neural networks with adaptive neuron-selection strategy",
            "venue": "In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Simos Gerasimou",
                "Hasan Ferit Eniser",
                "Alper Sen",
                "Alper Cakan"
            ],
            "title": "Importance-driven deep learning system testing",
            "venue": "In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Jer\u00f3nimo Hern\u00e1ndez-Gonz\u00e1lez",
                "Inaki Inza",
                "Jose A Lozano"
            ],
            "title": "Weak supervision and other non-standard classification problems: a taxonomy",
            "venue": "Pattern Recognition Letters,",
            "year": 2016
        },
        {
            "authors": [
                "Shenao Yan",
                "Guanhong Tao",
                "Xuwei Liu",
                "Juan Zhai",
                "Shiqing Ma",
                "Lei Xu",
                "Xiangyu Zhang"
            ],
            "title": "Correlations between deep neural network model coverage criteria and model quality",
            "venue": "In Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2020
        },
        {
            "authors": [
                "Jianmin Guo",
                "Yu Jiang",
                "Yue Zhao",
                "Quan Chen",
                "Jiaguang Sun"
            ],
            "title": "Dlfuzz: Differential fuzzing testing of deep learning systems",
            "venue": "In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Yang Feng",
                "Qingkai Shi",
                "Xinyu Gao",
                "Jun Wan",
                "Chunrong Fang",
                "Zhenyu Chen"
            ],
            "title": "Deepgini: prioritizing massive tests to enhance the robustness of deep neural networks",
            "venue": "In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Weiss",
                "Paolo Tonella"
            ],
            "title": "Simple techniques work surprisingly well for neural network test prioritization and active learning (replicability study)",
            "venue": "In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis,",
            "year": 2022
        },
        {
            "authors": [
                "Miller Trujillo",
                "Mario Linares-V\u00e1squez",
                "Camilo Escobar-Vel\u00e1squez",
                "Ivana Dusparic",
                "Nicol\u00e1s Cardozo"
            ],
            "title": "Does neuron coverage matter for deep reinforcement learning? a preliminary study",
            "venue": "In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Peter J Rousseeuw"
            ],
            "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
            "venue": "Journal of Computational and Applied Mathematics,",
            "year": 1987
        },
        {
            "authors": [
                "Filipe de Lima Arcanjo",
                "Gisele Lobo Pappa",
                "Paulo Viana Bicalho",
                "Wagner Meira Jr.",
                "Altigran Soares da Silva"
            ],
            "title": "Semi-supervised genetic programming for classification",
            "venue": "In Proceedings of the 13th annual Conference on Genetic and Evolutionary Computation,",
            "year": 2011
        },
        {
            "authors": [
                "Antti Rasmus",
                "Mathias Berglund",
                "Mikko Honkala",
                "Harri Valpola",
                "Tapani Raiko"
            ],
            "title": "Semisupervised learning with ladder networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2010
        },
        {
            "authors": [
                "Unai Garciarena",
                "Roberto Santana",
                "Alexander Mendiburu"
            ],
            "title": "EvoFlow: A Python library for evolving deep neural network architectures in tensorflow",
            "venue": "In Proceedings of the 2020 IEEE Symposium Series on Computational Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Mart\u0131\u0301n Abadi",
                "Paul Barham",
                "Jianmin Chen",
                "Zhifeng Chen",
                "Andy Davis",
                "Jeffrey Dean",
                "Matthieu Devin",
                "Sanjay Ghemawat",
                "Geoffrey Irving",
                "Michael Isard"
            ],
            "title": "Tensorflow: a system for large-scale machine learning",
            "venue": "In Osdi,",
            "year": 2016
        },
        {
            "authors": [
                "F\u00e9lix-Antoine Fortin",
                "De Rainville",
                "Marc-Andr\u00e9 Gardner Gardner",
                "Marc Parizeau",
                "Christian Gagn\u00e9"
            ],
            "title": "DEAP: Evolutionary algorithms made easy",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Kay Henning Brodersen",
                "Cheng Soon Ong",
                "Klaas Enno Stephan",
                "Joachim M Buhmann"
            ],
            "title": "The balanced accuracy and its posterior distribution",
            "venue": "In 20th International Conference on Pattern Recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Randal S. Olson",
                "William La Cava",
                "Patryk Orzechowski",
                "Ryan J. Urbanowicz",
                "Jason H. Moore"
            ],
            "title": "PMLB: a large benchmark suite for machine learning evaluation and comparison",
            "venue": "BioData Mining,",
            "year": 2017
        },
        {
            "authors": [
                "Joseph D Romano",
                "Trang T Le",
                "William La Cava",
                "John T Gregg",
                "Daniel J Goldberg",
                "Praneel Chakraborty",
                "Natasha L Ray",
                "Daniel Himmelstein",
                "Weixuan Fu",
                "Jason H Moore"
            ],
            "title": "PMLB v1.0: an open source dataset collection for benchmarking machine learning methods",
            "venue": "arXiv preprint arXiv:2012.00058v2,",
            "year": 2021
        },
        {
            "authors": [
                "Serhat Selcuk Bucak",
                "Rong Jin",
                "Anil K Jain"
            ],
            "title": "Multi-label learning with incomplete class assignments",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 3.\n02 80\n1v 1\n[ cs\n.N E"
        },
        {
            "heading": "1 Introduction",
            "text": "Neuroevolutionary approaches [1, 2] are extensively used to optimize the neural network architecture in supervised machine learning tasks. Usually, a training dataset is used to guide the search for the optimal model. However, in some domains, the availability of labeled examples is limited due to the cost of human labeling or the scarcity of data. In these scenarios, unlabeled data become more precious. Semi-supervised machine learning algorithms have been proposed to take advantage of the unlabeled examples during the learning stage of the model. Among the semi-supervised machine learning approaches, we can find disagreement-based methods such as co-training [3], graph-based methods like those based on min-cuts [4], and low-density separation methods such as semi-supervised support vector machines [5].\nEvolutionary algorithms have been also applied to semi-supervised problems. In [6], the application of grammatical evolution (GE) for semi-supervised classification is proposed. The quality of\nthe model is evaluated by combining the accuracy on the labeled data with one measure of cluster quality on the unlabeled examples. Kamalov et al. [7] apply a GP algorithm combined with a self-labeling algorithm to classify time-series. They show that the combination of both types of algorithms improves state-of-the-art semi-supervised methods for the addressed classification problems. Silva et al. [8] propose a way to use unlabeled data as part of GP learning to increase the accuracy of GP classifiers for problems with noisy labels.\nIn this paper, we focus on neuroevolutionary approaches to semi-supervised learning. More specifically, we investigate the behavior of evolutionary approaches that evolve neural networks for semisupervised classification. There are some works in the field of neural architecture search (NAS) that consider semi-supervised tasks [9, 10]. The approach in these papers consists of the identification of an unsupervised pretext task from which a neural architecture is learned that is then transferred to the target supervised task for which labeled data is available. For example, for an image segmentation task, an input image is first rotated in four preset directions and then a neural network architecture is searched for the pretext task of predicting the rotation. Subsequently, the selected architecture is retrained to solve the original image segmentation task. In this paper, we follow a completely different approach, in which unlabeled data is used to evaluate different metrics describing the neuron coverage of the neural network architecture encoded by the candidate solution.\nNeuron coverage metrics [11, 12, 13] are inspired by code coverage metrics which serve the functional and structural testing of code systems. Usually, a software coverage metric serves to quantify to what extent different components of a code (e.g., lines, program branches, modules, etc.) are \u201ccovered\u201d by a set of test examples. For neural networks, the components whose coverage is targeted by the corresponding metrics are the neurons, layers, or even more fine-grain functional characterization of the network, such as the range of possible values for the activation functions of the neurons. Neuron coverage metrics have mainly been applied as a way to verify neural networks and detect possible errors [14, 15]. The rationale behind their application is that a test set that covers all possible components of a neural network serves as a detailed characterization of the network behavior.\nIn this paper, we propose to use neuron coverage metrics as a supervisory signal to discriminate among evolved architectures. The assumption is that these metrics, when computed using unlabeled data, can serve to predict whether the neural network architecture is likely to, at least, \u201cevaluate properly\u201d similar data. Therefore, neuroevolution will be driven by the performance of the evolved network architectures on labeled data and their potential to be covered by unlabeled data.\nThe paper is organized as follows:\nIn the next section, we present the necessary background on semi-supervised classification and neuron coverage metrics. Related work is analyzed in Section 3. Section 4 introduces the neuroevolutionary approach, explaining the evolutionary operators and the characteristics of the tensorflowbased implementation. The characteristics of the training process and the fitness function specifically conceived for the semi-supervised scenario are discussed in Section 5. Section 6 explains the experimental framework and discusses the results of the experiments. Finally, Section 7 concludes the paper and discusses future work."
        },
        {
            "heading": "2 Semi-supervised classification and neuron coverage metrics",
            "text": ""
        },
        {
            "heading": "2.1 Semi-supervised classification",
            "text": "We address the semi-supervised classification task of learning a function f : X 7\u2192 Y from a training data set Dtrain = {(x1, y1), . . . , (xi, yi), . . . , (xl, yl), xl+1, . . . , xm}, where X is the feature space, Y \u2208 {0, 1}, l is the number of labeled training examples, and u = m\u2212 l is the number of unlabeled instances. This type of problem can be considered as an example of incomplete supervision since only a subset of training data is given with labels [16]."
        },
        {
            "heading": "2.2 Neuron coverage metrics",
            "text": "In the literature, there are slightly different definitions of the neuron coverage metrics. We have mainly adopted the conventions used in [11, 13] with some few changes.\nLet c represent a neuron of a multi-layer perceptron (MLP) of A layers. M1,M2, . . . ,MA represent the number of neurons in each layer and N = \u2211A\nj=1 Mj is the total number of neurons in the\nnetwork.\nWe use \u03c6(xi, c) to denote the function that returns the output of neuron c given xi as input. For a given neuron c, it is said to be activated for a given input xi if \u03c6(xi, c) > t, where t is a given threshold. Lc and Hc will respectively represent the lower and upper bounds of function \u03c6(x\ni, c) for xi \u2208 D. These values are determined by analyzing the values of \u03c6(xi, c) for the training dataset D. Usually, the set D corresponds to a set of instances in the training dataset, i.e., D = Dtrain."
        },
        {
            "heading": "2.2.1 Neuron coverage",
            "text": "Given a set of instances D and a given threshold t, the neural network coverage [12] measures the proportion of neurons in MLP that have been activated by at least one instance in D:\nNC =\n\u2223 \u2223{c| \u2203xi \u2208 D : \u03c6(xi, c) > t} \u2223 \u2223\nN (1)\n2.2.2 Top-K neuron coverage\nFor a given test input xi and neurons c and c\u2032 in the same layer, c is more active than c\u2032 if \u03c6(xi, c) > \u03c6(xi, c\u2032). For the j-th layer, topjK(x i) on layer j denotes the set of neurons that have the largest K outputs on that layer given xi.\nThe top-K neuron coverage (TKNC) measures how many neurons have once been among the most active K neurons on each layer.\nTKNC(Dtest,K) = | \u22c3 xi\u2208Dtest ( \u22c3 1\u2264j\u2264A top j K(x i)))|\nN (2)"
        },
        {
            "heading": "2.2.3 k-multi-section neuron coverage",
            "text": "Given a neuron c, the multi-section neuron coverage measures how thoroughly the given set of test instances covers the range [Lc, Hc]. The range is divided into k > 0 equal sections, called multisections. A multi-section Ssc , s \u2208 {1, . . . , k} is said to be covered if \u03c6(x i, c) \u2208 Ssc for x i \u2208 Dtest.\nThe k-multi-section neuron coverage for neuron c is defined [11] as the ratio between the number of sections covered by Dtest and k,\nKMN(c) =\n\u2223 \u2223{Ssc | \u2203x i \u2208 Dtest : \u03c6(xi, c) \u2208 Ssc} \u2223 \u2223\nk (3)\nThe k-multi-section neuron coverage of an MLP [11] is defined as:\nKMN(Dtest, k) =\n\u2211\ncKMN(c)\nk \u00b7N (4)"
        },
        {
            "heading": "2.2.4 Neuron boundary coverage and strong neuron activation coverage",
            "text": "A test input xi \u2208 Dtest is said to be located in the corner-case region of an MLP if thre is a neuron c such that \u03c6(xi, c) is lower than Lc or higher than Hc.\nTo cover corner-case regions of MLPs, the sets of covered corner-case regions are defined as:\nLCN = {c| \u2203xi \u2208 Dtest : \u03c6(x i, c) \u2208 (\u2212\u221e, Lc)} (5)\nUCN = {c| \u2203xi \u2208 Dtest : \u03c6(x i, c) \u2208 (Hc,+\u221e)} (6)\nThe neuron boundary coverage (NBC) measures how many corner-case regions have been covered by the given test input set Dtest.\nNBC(Dtest) = |LCN |+ |UCN |\n2 \u00b7N (7)\nThe strong neuron activation coverage (SNAC) measures how many corner cases, with respect to the upper boundary value, have been covered by the given test inputs Dtest.\nSNAC(Dtest) = |UCN |\nN (8)"
        },
        {
            "heading": "3 Related work",
            "text": ""
        },
        {
            "heading": "3.1 Neural network verification",
            "text": "In the literature, the use of neuron coverage metrics is mainly associated with the evaluation and creation of test instances for verification of neural networks [11, 12, 17]. Test prioritization consists of ranking the raw inputs to a model according to their potential to improve it. A typical form to improve the model is by uncovering unexpected behavior from the model that could lead to its enhancement. Usually, the neuron metric of choice is evaluated for each of the test examples that are then sorted in descending order of coverage amount. The top ranking test instances are given higher priority.\nPei et al. [12] introduced the concept of neuron coverage and used it for white-box testing of deep learning systems. They reported that neuron coverage is a better metric than code coverage for measuring the comprehensiveness of the DNN test inputs, and that inputs from different classes of a classification problem usually activate more unique neurons than inputs that belong to the same class. Ma et al. [11] extended the set of neuron and layer coverage metrics, and used them combined with the creation of adversarial examples, to quantify the defect detection ability of test data on DNNs. In [15], neuron importance analysis was introduced as a way to identify neurons that play a more important role for decision-making within the neural network. The authors show that the introduced metric can detect those neurons of convolutional networks that are more sensitive to changes in relevant pixels of a given input.\nLee et al. [14] proposed the application of neuron coverage metrics for a problem that is not directly related to test selection. They applied these metrics as the basis for neuron selection for gradient-based white-box testing of neural networks. These white-box testing methods require the computation of the gradient of neurons to quantify their behavior. Since such a computation can be expensive, some authors propose strategies for selecting or prioritizing neurons. Examples of such strategies include the random selection of un-activated neurons [12], or the identification of neurons near the activation threshold [18]. The use of neuron coverage metrics for neuron prioritization adds to the repertoire of existing methods and indicates that neuron coverage metrics can also be used for distinguishing or categorizing different behaviors or roles of the neural network components.\nThe effectiveness of coverage-based methods for test prioritization has also been questioned in a number of works where other statistical-based methods for evaluating neural networks were proposed [19, 20]. Other authors report [13] that coverage-driven methods are less effective than gradient-based methods for uncovering defects and improving neural network robustness.\nA number of works have recently investigated the suitability of neuron coverage metrics to evaluate other machine learning paradigms. For instance, Trujillo et al. [21] present a preliminary study on the use of these metrics for testing deep reinforcement learning (DeepRL) systems. They compute the correlation between coverage evolutionary patterns of the RL process and the rewards. They conclude that neuron coverage is not sufficient to reach substantial conclusions about the design or structure of DeepRL networks.\nNone of this previous research has employed the neuron-coverage metric as a way to search in the space of neural architectures or to find the solution of semi-supervised problems."
        },
        {
            "heading": "3.2 Neuroevolution for semi-supervised problems",
            "text": "There are several papers that address semi-supervised learning using evolutionary optimization techniques (e.g., see [8] for a discussion of some of these approaches). We briefly cover only some of the papers that describe research in this area, with a focus on methods that share some commonality with our contribution.\nIn [6], Fitzgerald et al. addressed semi-supervised problems using grammatical evolution (GE). They employ a grammar codifying if-then rules and evolve programs able to assign instances to different clusters based on their features. Unlabeled instances are used to measure the clustering performance by means of the silhouette co-efficient or silhouette score (SC) [22], and the labeled data is used to measure the performance of the model in terms of classification accuracy. The fitness function is computed as the sum of the aforementioned two scores. While, in this paper, we also propose the fitness evaluation of each evolved model as a combination of different scores respectively computed using labeled and unlabeled data, the neuron coverage metrics are fundamentally different from SC and other clustering scores. They are not associated to computing the performance on any auxiliary or target task. Furthermore, the evolutionary algorithm that we use to evolve neural networks is a genetic algorithm (GA) working on a list-based representation of neural network architectures.\nAnother approach to semi-supervised classification problems is self-labelling, or retraining, in which the modeled trained on labeled instances is then used to make predictions on unlabeled instances. These predictions are used as pseudo-labels of the unlabeled examples and used for retraining the classifier. A similar approach was presented in [7] where the authors combined the PageRank and PCA algorithms with a variant of genetic programming (GP) specifically tailored for non-linear symbolic regression. The algorithm was tested on three time series datasets and it was reported that the performance of the hybrid-algorithm overcomes the two algorithms individually. There are other papers that propose the application of GP to semisupervised problems, they mainly use a tree-based program representation [8] and also apply variants of self-labeling strategies [23].\nThere is an increasing number of works [24, 25] that propose semi-supervised learning methods for fixed neural networks, i.e., the architecture of the network is not changed as part of the semisupervised approach. For example, the consistency regularization method introduced in [24] evaluates each data point with and without added artificial noise, and then computes the consistency cost between the two predictions. Only recently, the question of semi-supervised classification in NAS has been addressed. In [10], two semi-supervised approaches are applied to semantic segmentation. The algorithm jointly optimizes an architecture of a neural network and its parameters. This approach works minimizing the weighted sum of a supervised loss, and two unsupervised losses. As in previous examples discussed in this section, this approach requires the definition of an auxiliary task (e.g., clustering) and the model is evaluated according to its performance on all the tasks."
        },
        {
            "heading": "4 Neuroevolutionary approach",
            "text": "The neuroevolutionary approach we use is based on the application of a GA with genetic operators designed to work on a list representation of tensorflow programs. In this section, we explain the main components of the algorithm, and in the next section we focus on the main contributions of this paper which are related to the way in which the fitness functions are implemented to deal with the semi-supervised learning scenario."
        },
        {
            "heading": "4.1 Neural network representation",
            "text": "In this work, the evolved DNNs are standard, feed-forward, sequential MLPs that are characterized by the following aspects:\n\u2022 Number of hidden layers: Since we consider standard feed-forward sequential architectures, a single integer is enough to encode this aspect.\n\u2022 Initialization functions: The weights in any given layer can be initialized in a different manner and this can condition the local optimum the network reaches. It consists of a list of indices of initialization functions.\n\u2022 Activation functions: Similarly, the activation functions applied in each layer is not fixed. Similarly to the initialization functions, it consists of a list of indices.\n\u2022 Dropout: Also a per-layer characteristic, this is implemented by means of a list of Boolean elements determining whether the application of dropout after the activation functions should be applied.\n\u2022 Batch normalization: Similarly to the previous aspect, this consists of a list of Boolean elements indicating whether each layer implements batch normalization before the activation functions.\nThe evolvable components have a number of options for variation:\n\u2022 The DNN weights can be initialized by drawing values from a normal or uniform distribution, or by applying the xavier [26] variation of the normal initialization.\n\u2022 The following activation functions can be applied to the layers of the DNN: Identity, ReLU, eLU, Softplus, Softsign, Sigmoid, Hyperbolic Tangent."
        },
        {
            "heading": "4.2 A GA with list-based encoding",
            "text": "Because the DNN\u2019s parameters are encoded using lists, we define a list-based DNN descriptor which specifies the network architecture as well as other parameters, such as the loss function, weight initialization functions, etc. This can be considered a declarative representation, as it exclusively contains the specification of the network, the weights being left outside of the evolutionary procedure. Algorithm 1 shows the pseudocode of the GA.\nAlgorithm 1: GA for evolving DNN.\n1 Set t \u21d0 0. Create a population D0 by generating N random DNN descriptions; 2 while halting condition is not met do 3 Evaluate Dt using the fitness function; 4 From Dt, select a population D S t of Q \u2264 N solutions according to a selection method; 5 Apply mutation with probability pm = 1\u2212 px to DSt and create the offspring set Ot. Choice of the mutation operator is made uniformly at random; 6 Create Dt+1 by using the selection method over {Dt, Ot}; 7 t \u21d0 t+ 1; 8 end"
        },
        {
            "heading": "4.3 Genetic operators",
            "text": "The operators used to mutate individuals are the following:\n\u2022 The layer change operator randomly reinitializes the description of a layer chosen at random, e.g., its weight initialization and activation functions; and the number of neurons.\n\u2022 The add layer operator introduces a new (randomly initialized) layer in a random position of the DNN.\n\u2022 The del layer operator deletes a randomly chosen layer in the DNN.\n\u2022 The activ change operator changes the activation function of a random layer to another function, chosen at random.\n\u2022 The weight change operator, similarly to activ change, changes the function used to obtain the initial weights of the DNN layer."
        },
        {
            "heading": "4.4 Implementation",
            "text": "We have used the implementation of the neuron coverage metrics1 developed as part of the work presented in [15]. To implement the neuron coverage based neuroevolutionary approach, we used\n1Available from https://github.com/DeepImportance/deepimportance_code_release\nthe deatf library 2 which is an extension to Tensorflow2 of the Evoflow library 3 [27], originally conceived to evolve neural networks implemented in tensorflow[28].\nIn these libraries, the representation of the DNN is split into two types of classes: The representation of the DNN architectures is contained in NetworkDescriptor, which encompasses all the lists mentioned in Section 4.1. The architecture is implemented in a tensorflow DNN, which is contained in the Network class. We use the MLPDescriptor and MLP classes conceived to deal with multi-layer perceptrons.\nDifferent selection operators are available in EvoFlow through the DEAP library [29]. We use the truncation selection strategy."
        },
        {
            "heading": "5 Network training and fitness evaluation",
            "text": "In the application of neuroevolutionary approaches to supervised classification problems, the original dataset D is usually split into three parts D = Dtrain \u222aDval \u222aDtest. Dtrain is used to train the network, Dval is used to estimate the performance of the trained network by computing the fitness function, and Dtest is only used at the end of the evolution to assess the quality of the best networks found by the algorithm. We consider neuroevolutionary scenarios that use this partition of the data. Notice that another validation set could be used for early stopping of the neural network training process. However, we do not consider this type of early stopping strategy.\nIn our approach to semi-supervised problems, we assume that the Dval and Dtest sets will keep all the labels. They will respectively be employed in the usual way to evaluate the accuracy of the model during the evolution, and at the end of the evolution. Dtrain will have a q \u2208 [0, 1) proportion of unlabeled instances and a 1\u2212 q proportion of labeled instances. The two sets will be respectively named as Dltrain and D u train."
        },
        {
            "heading": "5.1 Fitness evaluation for the fully-supervised case",
            "text": "For the evaluation process, one or more metrics (e.g., accuracy, learning time, etc.) describing the DNN performance could be computed. For the binary problems addressed in this paper, we use the balanced accuracy metric [30] that is appropriate to deal with unbalanced classification problems.\nb acc(z,Dval) = 1 2 ( TP P + TN N ) (9)\nwhere z is the neural network being evaluated, P and N are respectively the number of positive and negative instances in Dval, and TP and TN are respectively the number of correct positive and negative predictions made by the model on instances in Dval.\nWhen q = 0, we have the fully-supervised case in which all the training data are labeled. In this case, for a candidate neural network z, the fitness function f(z) is simply the balanced accuracy b acc(z,Dval)."
        },
        {
            "heading": "5.2 Fitness evaluation for the semi-supervised case",
            "text": "When q \u2208 (0, 1) andDutrain 6= \u2205, the fitness evaluation will also take into account the neural network coverage computed using the unlabeled cases NNCov(z,Dutrain), where NNCov can be one of the following neuron coverage metrics introduced in Section 2.2: NC, TKNC, KMN , NBC, and SNAC.\nFinally, the fitness function for the semi-supervised case is defined as:\nf(z) = q \u00b7NNCov(z,Dutrain) + (1\u2212 q) \u00b7 b acc(z,Dval) (10)\n2Available from https://github.com/IvanHCenalmor/deatf 3Available from https://github.com/unaigarciarena/EvoFlow"
        },
        {
            "heading": "5.3 Baselines for semi-supervised classification with neuroevolutionary approaches",
            "text": "In order to assess the performance of the introduced algorithms, we implemented two methods inspired by semi-supervised approaches used in the field.\nThe first baseline is based on the use of uncertainty quantifiers [20] which are intended to measure the uncertainty of the model at the time of predicting the class of a given instance. For instance prioritization, examples that are classified by the model with low confidence (i.e., for binary classification problems, prediction probability close to 0.5) are of particular interest since they can represent problematic cases. In [20], it is argued that simple metrics can be more effective than neuron coverage metrics for test instance prioritization. Therefore, it is a relevant question to determine how these metrics perform in the context of neuroevolutionary optimization.\nFor evaluating network architectures, we assume that a neural network is more promising when it predicts, with high certainty, the class of the unlabeled instances. We define the CERT metric as:\nCERT (z,Dtest) =\n\u2211\nxi \u2208 Dtestmax(p(x i), 1\u2212 p(xi))\n|Dtest| (11)\nwhere p(xi) is the probability of xi belonging to class 1 as assigned by the neural network z.\nThis metric is not expensive to compute since it only requires calculating the predictions of the model for all instances in Dutrain.\nThe second baseline uses retraining, an approach frequently described in the literature for semisupervised learning [23, 7]. Retraining consists of using the model learned on labeled instances to make predictions on the unlabeled instances. The pseudo-labels predicted by the model are then used to retrain it. In some cases, a confidence value is required in order to consider a pseudo-label to be valid for retraining.\nIn our baseline, we set a threshold of p(xi) \u2264 0.4 in order to consider a class-0 pseudo-label to be correct. Similarly, we set p(xi) \u2265 0.6 in order to consider a class-1 pseudo-label to be correct. The algorithm initially learns the model using Dltrain, then it makes predictions for instances in D u train. Subsequently, the unlabeled instances for which prediction thresholds are satisfied are selected and combined with instances in Dltrain to retrain the model. Finally, the fitness function is the balanced accuracy of the retrained model as computed on Dval. If none of the unlabeled instances satisfy the constraints on the predicted probabilities to be selected, then no retraining is carried out and the fitness value corresponding to the network architecture is the balanced accuracy for Dval produced by the network trained on Dltrain.\nThe retraining approach which we denote as RET can be computationally costly because it requires conducting the learning process twice."
        },
        {
            "heading": "6 Experiments",
            "text": "The general aim of the experiments is to determine whether, and to what extent, the use of neuron coverage metrics influences the evolutionary search of neural network architectures for semisupervised classification problems. In particular, we address the following questions:\n\u2022 How the quality of the evolved classifiers degrade with respect to the amount of unlabeled instances?\n\u2022 Which of the investigated coverage metrics produces a more beneficial effect in the performance of the evolved architectures?\n\u2022 How do the neuroevolutionary approaches based on neuron coverage compare to approaches based on uncertainty quantification and retraining?\nWe first present the experimental design, including the characteristics of the benchmark. Subsequently, we address the aforementioned questions, presenting the numerical results of the experiments."
        },
        {
            "heading": "6.1 Experimental design",
            "text": "To create the classification problem benchmark, we follow the approach described in [6], in which fully labeled datasets are used and partial labeling is simulated by only considering a random subset of training data to be labeled; the rest of the data set is treated as unlabeled. Starting from a binary classification problem for which all labels are known, we will simulate the semi-supervised scenario by removing the labels for a proportion q of the instances in the dataset. We will use different values of q to investigate the influence of the amount of missing labels.\nWe have selected 20 binary classification problems included as part of the PMLB library [31, 32]. Each problem has an associated dataset whose characteristics are described in Table 1. In the table, Imbalance refers to the amount of imbalance in the number of observations corresponding to the two classes. It is calculated by measuring the squared distance of the incidence proportion of each class from perfect balance in the dataset [31].\nEach experiment consists of running a neuroevolutionary algorithm for solving a particular binary classification problem. We use a population size of 20 individuals and 30 generations. The neural network architectures are constrained to have a maximum depth of 8, and the maximum number of neurons in each layer was also set to 8. The batch-size and the number of epochs were respectively set to 10 and 50.\nEach possible application of the neuroevolutionary algorithm is parameterized by the following parameters:\n1. Classification problem addressed.\n2. Proportion of unlabeled data in Dtrain, q \u2208 {0, 0.2, 0.4, 0.6, 0.8}.\n3. Type of neuron coverage metric used (only when q > 0), i.e., NC, TKNC, KMN , NBC, and SNAC.\nFor each possible configuration, we have executed 10 repetitions of the neuroevolutionary search for a total of 20 \u00d7 10 \u00d7 (4 \u00d7 5 + 1) = 4200 experiments. Notice that, when q = 0, none of the five coverage metrics is used.\nFor each algorithm, and once the evolution has finished, we retrain the architectures encoded by all individuals in the last generation using the dataset Dtrain \u222aDval. We then make the predictions for Dtest, and compute the balanced accuracy using these predictions."
        },
        {
            "heading": "6.2 Numerical results",
            "text": ""
        },
        {
            "heading": "6.2.1 Initial experiments",
            "text": "For an initial assessment of the influence of q in the performance of the different algorithms, we present in Figure 1 and Figure 2 the distribution of the balanced accuracy for two classification problems (analcatdata, and breast cancer).\nThe distribution is computed using the 20 architectures in the last generation for all the 10 runs. In the figures, and to ease the comparison between the algorithms, the results for the fully-supervised problem (q = 0) are displayed five times, once for each of the neuron coverage metrics.\nThe two problems illustrate a different scenario of the performance of the evolved classifiers. The analcatdata problem is easy to solve and, for most of the configurations, the accuracy results are high. This is also an example where the performance of the classifiers does not suffer much when the proportion of unlabeled data is increased. For this problem, among the neuron coverage metrics, NC shows a more stable behavior when q varies. This example shows that, at least for some problems, using the coverage metrics together with a significant amount of unlabeled data can contribute to obtain a high-performing classifier.\nFigure 2 shows a problem for which the quality of the evolved classifiers is rather poor and all accuracy values are below 0.7. It is noticeable that, in this problem, the results of the architectures evolved using the KMN , NBC, and SNAC coverage metrics significantly deteriorate when the amount of unlabeled data is increased. This is a common trend for other problems as will be shown in the following experiments.\nIt is also worth noting that in some cases, such as multiple instances related to the analcatdata database, and instances with q = 20 and q = 40 for the breast-cancer problem, the balanced accuracy achieved by the models evolved using neuron-coverage metrics was higher than that obtained using the whole labeling of the database."
        },
        {
            "heading": "6.2.2 Evaluation of the algorithms on all datasets",
            "text": "Figure 3 shows the accuracy of the best classifier, in terms of balanced accuracy on the test set, found for each dataset and configuration. The analysis of the figure reveals that, in terms of the best solution found, the amount of unlabeled data does not seem to have a critical impact in terms of the accuracy of the best classifier. However, differences are difficult to spot due to the variability of the problem difficulty among the datasets. Therefore, we summarize the information contained in Figure 3 by computing the average of the balanced accuracy considering the 20 problems. This information is shown in Figure 4.\nAs can be seen in Figure 4, for three of the metrics, there is a noticeable impact in the accuracy when less labeled data is used for training the network. For metrics KMN , NBC, and SNAC, as q increases, the mean accuracy decreases. Neuron coverage metrics NC and TKNC confirm to be more stable and show their capacity to guide the search towards classifiers that are at least as good as those learned using the full set of labeled data."
        },
        {
            "heading": "6.3 Comparison with the baseline algorithms",
            "text": "In this section, we compare the performance of the introduced algorithms with two methods that were introduced in Section 5.3. We focus our comparison on the algorithms that use NC and TKNC, since they produced the best results in the previous experiments. In the comparison, all the neural network and evolutionary algorithm settings are the same as in previous experiments. The only difference among the algorithms is the way in which the fitness function is computed.\nTable 2 summarizes the results of the comparison among the algorithms for 10 of the 20 problems (due to page limit constraints). For datasets agaricus lepiota and coil2000 the retraining variant had not finished after 15 hours of computation. On the other hand, datasets analcatdat l and breast cancer wand were not included in the analysis since, for all configurations and runs, classifiers with perfect accuracy on the test data were found.\nFor each data set, algorithm, and proportion of unlabeled data, we compute the average accuracy of the best solution in the last population for the 10 experiments. In Table 2, the algorithm that produces the best result for each dataset and q is underlined. In can be clearly seen that the best results are achieved by using the NC and TKNC neuron coverage metrics. Uncertainty quantification proves to\nq=0\n(NC,q=0.2)\n(NC,q=0.4)\n(NC,q=0.6)\n(NC,q=0.8)\n(TKNC,q=0.2)\n(TKNC,q=0.4)\n(TKNC,q=0.6)\n(TKNC,q=0.8)\n(KMN,q=0.2)\n(KMN,q=0.4)\n(KMN,q=0.6)\n(KMN,q=0.8)\n(NBC,q=0.2)\n(NBC,q=0.4)\n(NBC,q=0.6)\n(NBC,q=0.8)\n(SNAC,q=0.2)\n(SNAC,q=0.4)\n(SNAC,q=0.6)\n(SNAC,q=0.8)\nagaricus_lepiota analcatdata_l australian backache biom ed breast breast_cancer breast_cancer_w breast_w buggyC rx bupa chess churn cleve coil2000 colic credit_a credit_g crx diabetes\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nF ig u re 3 : B alan ced accu racy o f th e b est so lu tio n s co m p u ted fo r all d atasets an d m etrics. q=0 (NC,q=0.2) (NC,q=0.4) (NC,q=0.6) (NC,q=0.8) (TKNC,q=0.2) (TKNC,q=0.4) (TKNC,q=0.6) (TKNC,q=0.8) (KMN,q=0.2) (KMN,q=0.4) (KMN,q=0.6) (KMN,q=0.8) (NBC,q=0.2) (NBC,q=0.4) (NBC,q=0.6) (NBC,q=0.8) (SNAC,q=0.2) (SNAC,q=0.4) (SNAC,q=0.6) (SNAC,q=0.8) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Mean\u00a0balanced\u00a0accuracy F ig u re 4 : M ean accu racy o f th e b est classifi ers co m p u ted fo r all p ro b lem s. 1 2\nnot be a competitive approach for guiding neuroevolution. Retraining can achieve better results than CERT, but at a higher computational cost.\nWe also investigated the dynamics of the neuroevolutionary algorithms during the evolution. Figure 5 shows an example of the evolution of the accuracy of the best solution (computed on validation data Dval) as the number of evaluations increase from evaluation 1 to evaluation 620. The illustrative example shown in Figure 5 corresponds to the chess problem and when q = 0.8. The balanced accuracy values have been computed as the average of the 10 runs.\nThe curves in Figure 5 show that all the fitness functions are able to guide the search to areas with better performing neural architectures. The difference in the dynamics is given by the quality of the solutions that are found and the speed of convergence. For the chess problem the neuroevolutionary algorithms using NC and TKNC converge faster to architectures of better accuracy. Notice that maximizing the accuracy of the model forDval is not a guarantee of architectures that will generalize to other data (e.g.,Dtest) but the example illustrates that, in addition to producing better architectures as shown in Table 2, convergence can be faster."
        },
        {
            "heading": "7 Conclusions",
            "text": "Semi-supervised problems for which labeled instances are difficult or costly to obtain are common in many fields. When neuroevolutionary approaches are applied to these problems the question of how\nto use the unlabeled data to improve the search for classifiers arises. In this paper, we have proposed the use of neuron coverage metrics as a way to asses how promising each candidate architecture is. The implicit assumption is that architectures that are better covered by the unlabeled examples are more promising. We have evaluated five different neuron coverage metrics and identified the NC and TKNC metrics as the more stable in terms of the degradation of the results when the number of labeled instances is diminished. Our results also show that for some problems the use of these metrics can even improve the performance of neuroevolutionary search."
        },
        {
            "heading": "7.1 Future work",
            "text": "There are a number of ways in which the work presented in this paper could be extended to deal with other classes of semi-supervised problems. One needed step is to go beyond binary-classification problems to address multi-class problems. The fitness functions proposed in this paper could also be used for the multi-class problems. Another research direction is defining strategies to deal with multi-label problems learning with incomplete class assignments [33]. While the computation of the coverage metrics do not change for these problems, the fitness functions should be modified to account for the existence of multiple classification problems. An analysis of the evolved architectures should also be conducted to determine whether the use of the neuron coverage metrics introduces any bias in the type of neural network components (e.g., the type of activation functions) that are included in the best solutions.\nWhile we have focused on semi-supervised problems, neuron coverage metrics could be used in other scenarios where neuroevolution is applied. They could be applied as an additional regularization mechanism that prioritizes architectures that are fully covered by the inputs of the problem. They could be employed, in single or multi-objective scenarios, as a diversification mechanism for problems where a large number of candidate neural architectures have the same value of the objective function being optimized. Finally, neuroevolution based on neuron coverage metrics could be used combined with adaptive instance selection for early verification (and correction) of neural networks to be deployed in machine learning systems."
        }
    ],
    "title": "Neuroevolutionary algorithms driven by neuron coverage metrics for semi-supervised classification",
    "year": 2023
}