{
    "abstractText": "Survival prediction based on whole slide images (WSIs) is a challenging task for patient-level multiple instance learning (MIL). Due to the vast amount of data for a patient (one or multiple gigapixels WSIs) and the irregularly shaped property of WSI, it is difficult to fully explore spatial, contextual, and hierarchical interaction in the patient-level bag. Many studies adopt random sampling pre-processing strategy and WSI-level aggregation models, which inevitably lose critical prognostic information in the patient-level bag. In this work, we propose a hierarchical vision Transformer framework named HVTSurv, which can encode the local-level relative spatial information, strengthen WSI-level context-aware communication, and establish patient-level hierarchical interaction. Firstly, we design a feature pre-processing strategy, including feature rearrangement and random window masking. Then, we devise three layers to progressively obtain patient-level representation, including a local-level interaction layer adopting Manhattan distance, a WSI-level interaction layer employing spatial shuffle, and a patientlevel interaction layer using attention pooling. Moreover, the design of hierarchical network helps the model become more computationally efficient. Finally, we validate HVTSurv with 3,104 patients and 3,752 WSIs across 6 cancer types from The Cancer Genome Atlas (TCGA). The average C-Index is 2.50-11.30% higher than all the prior weakly supervised methods over 6 TCGA datasets. Ablation study and attention visualization further verify the superiority of the proposed HVTSurv. Implementation is available at: https://github.com/szc19990412/HVTSurv.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhuchen Shao"
        },
        {
            "affiliations": [],
            "name": "Yang Chen"
        },
        {
            "affiliations": [],
            "name": "Hao Bian"
        },
        {
            "affiliations": [],
            "name": "Jian Zhang"
        },
        {
            "affiliations": [],
            "name": "Guojun Liu"
        },
        {
            "affiliations": [],
            "name": "Yongbing Zhang"
        }
    ],
    "id": "SP:8114c78cd11cb7c90e87ee1f14695242cf8ddfa9",
    "references": [
        {
            "authors": [
                "C. Abbet",
                "I. Zlobec",
                "B. Bozorgtabar",
                "J.-P. Thiran"
            ],
            "title": "Divide-and-rule: self-supervised learning for survival analysis in colorectal cancer",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 480\u2013489. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "H. Bian",
                "Z. Shao",
                "Y. Chen",
                "Y. Wang",
                "H. Wang",
                "J. Zhang",
                "Y. Zhang"
            ],
            "title": "Multiple Instance Learning with Mixed Supervision in Gleason Grading",
            "venue": "arXiv preprint arXiv:2206.12798.",
            "year": 2022
        },
        {
            "authors": [
                "J.M. Bland",
                "D.G. Altman"
            ],
            "title": "The logrank test",
            "venue": "Bmj, 328(7447): 1073.",
            "year": 2004
        },
        {
            "authors": [
                "G. Campanella",
                "M.G. Hanna",
                "L. Geneslaw",
                "A. Miraflor",
                "V.W.K. Silva",
                "K.J. Busam",
                "E. Brogi",
                "V.E. Reuter",
                "D.S. Klimstra",
                "T.J. Fuchs"
            ],
            "title": "Clinical-grade computational pathology using weakly supervised deep learning on whole slide images",
            "venue": "Nature medicine, 1301\u20131309.",
            "year": 2019
        },
        {
            "authors": [
                "I. Carmichael",
                "A.H. Song",
                "R.J. Chen",
                "D.F. Williamson",
                "T.Y. Chen",
                "F. Mahmood"
            ],
            "title": "Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling",
            "venue": "arXiv preprint arXiv:2206.08885.",
            "year": 2022
        },
        {
            "authors": [
                "R.J. Chen",
                "M.Y. Lu",
                "M. Shaban",
                "C. Chen",
                "T.Y. Chen",
                "D.F. Williamson",
                "F. Mahmood"
            ],
            "title": "Whole Slide Images are 2D Point Clouds: Context-Aware Survival Prediction using Patch-based Graph Convolutional Networks",
            "venue": "International Conference on Medical Image Computing",
            "year": 2021
        },
        {
            "authors": [
                "R.J. Chen",
                "M.Y. Lu",
                "W.-H. Weng",
                "T.Y. Chen",
                "D.F. Williamson",
                "T. Manz",
                "M. Shady",
                "F. Mahmood"
            ],
            "title": "Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vi-",
            "year": 2021
        },
        {
            "authors": [
                "R.J. Chen",
                "M.Y. Lu",
                "D.F. Williamson",
                "T.Y. Chen",
                "J. Lipkova",
                "M. Shaban",
                "M. Shady",
                "M. Williams",
                "B. Joo",
                "Z Noor"
            ],
            "title": "Pan-cancer integrative histologygenomic analysis via multimodal deep learning",
            "venue": "Cancer Cell",
            "year": 2022
        },
        {
            "authors": [
                "P. Chikontwe",
                "M. Kim",
                "S. Nam",
                "H. Go",
                "S. Park"
            ],
            "title": "Multiple Instance Learning with Center Embeddings for Histopathology Classification",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 519\u2013528.",
            "year": 2020
        },
        {
            "authors": [
                "D. Di",
                "J. Zhang",
                "F. Lei",
                "Q. Tian",
                "Y. Gao"
            ],
            "title": "BigHypergraph Factorization Neural Network for Survival Prediction from Whole Slide Image",
            "venue": "IEEE Transactions on Image Processing.",
            "year": 2022
        },
        {
            "authors": [
                "L. Fan",
                "A. Sowmya",
                "E. Meijering",
                "Y. Song"
            ],
            "title": "Learning Visual Features by Colorization for SlideConsistent Survival Prediction from Whole Slide Images",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 592\u2013601. Springer.",
            "year": 2021
        },
        {
            "authors": [
                "N. Hashimoto",
                "D. Fukushima",
                "R. Koga",
                "Y. Takagi",
                "K. Ko",
                "K. Kohno",
                "M. Nakaguro",
                "S. Nakamura",
                "H. Hontani",
                "I. Takeuchi"
            ],
            "title": "Multi-scale domain-adversarial multipleinstance CNN for cancer subtype classification with unannotated histopathological images",
            "venue": "Proceedings of the IEEE",
            "year": 2020
        },
        {
            "authors": [
                "P.J. Heagerty",
                "Y. Zheng"
            ],
            "title": "Survival model predictive accuracy and ROC curves",
            "venue": "Biometrics, 61(1): 92\u2013105.",
            "year": 2005
        },
        {
            "authors": [
                "W. Hou",
                "L. Yu",
                "C. Lin",
                "H. Huang",
                "R. Yu",
                "J. Qin",
                "L. Wang"
            ],
            "title": "H2-MIL: Exploring Hierarchical Representation with Heterogeneous Multiple Instance Learning for Whole Slide Image Analysis",
            "year": 2022
        },
        {
            "authors": [
                "Z. Huang",
                "Y. Ben",
                "G. Luo",
                "P. Cheng",
                "G. Yu",
                "B. Fu"
            ],
            "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
            "venue": "arXiv preprint arXiv:2106.03650.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Huang",
                "H. Chai",
                "R. Wang",
                "H. Wang",
                "Y. Yang",
                "H. Wu"
            ],
            "title": "Integration of patch features through selfsupervised learning and transformer for survival analysis on whole slide images",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2021
        },
        {
            "authors": [
                "F. Kanavati",
                "G. Toyokawa",
                "S. Momosaki",
                "M. Rambeau",
                "Y. Kozuma",
                "F. Shoji",
                "K. Yamazaki",
                "S. Takeo",
                "O. Iizuka",
                "M. Tsuneki"
            ],
            "title": "Weakly-supervised learning for lung carcinoma classification using deep learning",
            "venue": "Scientific reports, 1\u201311.",
            "year": 2020
        },
        {
            "authors": [
                "E.L. Kaplan",
                "P. Meier"
            ],
            "title": "Nonparametric estimation from incomplete observations",
            "venue": "Journal of the American statistical association, 53(282): 457\u2013481.",
            "year": 1958
        },
        {
            "authors": [
                "J.N. Kather",
                "J. Krisam",
                "P. Charoentong",
                "T. Luedde",
                "E. Herpel",
                "C.-A. Weis",
                "T. Gaiser",
                "A. Marx",
                "N.A. Valous",
                "D Ferber"
            ],
            "title": "Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study",
            "venue": "PLoS medicine,",
            "year": 2019
        },
        {
            "authors": [
                "Y.-G. Kim",
                "I.H. Song",
                "H. Lee",
                "S. Kim",
                "D.H. Yang",
                "N. Kim",
                "D. Shin",
                "Y. Yoo",
                "K. Lee",
                "D Kim"
            ],
            "title": "Challenge for diagnostic assessment of deep learning algorithm for metastases classification in sentinel lymph nodes on frozen tissue section digital slides in women with breast",
            "year": 2020
        },
        {
            "authors": [
                "M. Lerousseau",
                "M. Vakalopoulou",
                "M. Classe",
                "J. Adam",
                "E. Battistella",
                "A. Carr\u00e9",
                "T. Estienne",
                "T. Henry",
                "E. Deutsch",
                "N. Paragios"
            ],
            "title": "Weakly supervised multiple instance learning histopathological tumor segmentation",
            "venue": "International Conference on Medical Image Computing and",
            "year": 2020
        },
        {
            "authors": [
                "B. Li",
                "Y. Li",
                "K.W. Eliceiri"
            ],
            "title": "Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "R. Li",
                "J. Yao",
                "X. Zhu",
                "Y. Li",
                "J. Huang"
            ],
            "title": "Graph CNN for survival analysis on whole slide pathological images",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 174\u2013182. Springer.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 10012\u201310022.",
            "year": 2021
        },
        {
            "authors": [
                "C. Loeffler",
                "J.N. Kather"
            ],
            "title": "Manual tumor annotations in TCGA",
            "year": 2021
        },
        {
            "authors": [
                "M.Y. Lu",
                "D.F. Williamson",
                "T.Y. Chen",
                "R.J. Chen",
                "M. Barbieri",
                "F. Mahmood"
            ],
            "title": "Data-efficient and weakly supervised computational pathology on whole-slide images",
            "venue": "Nature Biomedical Engineering, 5(6): 555\u2013570.",
            "year": 2021
        },
        {
            "authors": [
                "Y.A. Malkov",
                "D.A. Yashunin"
            ],
            "title": "Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, 42(4): 824\u2013836.",
            "year": 2018
        },
        {
            "authors": [
                "H. Muhammad",
                "C. Xie",
                "C.S. Sigel",
                "M. Doukas",
                "L. Alpert",
                "A.L. Simpson",
                "T.J. Fuchs"
            ],
            "title": "EPIC-Survival: End-to-end Part Inferred Clustering for Survival Analysis, with Prognostic Stratification Boosting",
            "venue": "Medical Imaging with Deep Learning, 520\u2013531. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "N. Naik",
                "A. Madani",
                "A. Esteva",
                "N.S. Keskar",
                "M.F. Press",
                "D. Ruderman",
                "D.B. Agus",
                "R. Socher"
            ],
            "title": "Deep learning-enabled breast cancer hormonal receptor status determination from base-level H&E stains",
            "venue": "Nature communications, 1\u20138.",
            "year": 2020
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikitlearn: Machine Learning in Python",
            "venue": "Journal of Machine",
            "year": 2011
        },
        {
            "authors": [
                "M. Shaban",
                "S.A. Khurram",
                "M.M. Fraz",
                "N. Alsubaie",
                "I. Masood",
                "S. Mushtaq",
                "M. Hassan",
                "A. Loya",
                "N.M. Rajpoot"
            ],
            "title": "A novel digital score for abundance of tumour infiltrating lymphocytes predicts disease free survival in oral squamous cell carcinoma",
            "venue": "Scientific reports, 9(1): 1\u201313.",
            "year": 2019
        },
        {
            "authors": [
                "W. Shao",
                "T. Wang",
                "Z. Huang",
                "Z. Han",
                "J. Zhang",
                "K. Huang"
            ],
            "title": "Weakly supervised deep ordinal cox model for survival prediction from whole-slide pathological images",
            "venue": "IEEE Transactions on Medical Imaging, 40(12): 3739\u20133747.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Shao",
                "H. Bian",
                "Y. Chen",
                "Y. Wang",
                "J. Zhang",
                "X Ji"
            ],
            "title": "2021b. Transmil: Transformer based correlated multiple instance learning for whole slide image classification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Shen",
                "L. Liu",
                "Z. Tang",
                "Z. Chen",
                "G. Ma",
                "J. Dong",
                "X. Zhang",
                "L. Yang",
                "Q. Zheng"
            ],
            "title": "Explainable Survival Analysis with Convolution-Involved Vision Transformer",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, 2207\u20132215.",
            "year": 2022
        },
        {
            "authors": [
                "C.L. Srinidhi",
                "O. Ciga",
                "A.L. Martel"
            ],
            "title": "Deep neural network models for computational histopathology: A survey",
            "venue": "Medical Image Analysis, 67: 101813.",
            "year": 2021
        },
        {
            "authors": [
                "N. Tomita",
                "B. Abdollahi",
                "J. Wei",
                "B. Ren",
                "A. Suriawinata",
                "S. Hassanpour"
            ],
            "title": "Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides",
            "venue": "JAMA Network Open.",
            "year": 2019
        },
        {
            "authors": [
                "L.A. Vale-Silva",
                "K. Rohr"
            ],
            "title": "Long-term cancer survival prediction using multimodal deep learning",
            "venue": "Scientific Reports, 11(1): 1\u201312.",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "I. Vitale",
                "E. Shema",
                "S. Loi",
                "L. Galluzzi"
            ],
            "title": "Intratumoral heterogeneity in cancer progression and response to immunotherapy",
            "venue": "Nature medicine, 27(2): 212\u2013224.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wang",
                "J. Li",
                "Z. Pan",
                "W. Li",
                "A. Sisk",
                "H. Ye",
                "W. Speier",
                "C.W. Arnold"
            ],
            "title": "Hierarchical Graph Pathomic Network for Progression Free Survival Prediction",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 227\u2013237. Springer.",
            "year": 2021
        },
        {
            "authors": [
                "L. Wright"
            ],
            "title": "Ranger - a synergistic optimizer",
            "venue": "https: //github.com/lessw2020/Ranger-Deep-Learning-Optimizer.",
            "year": 2019
        },
        {
            "authors": [
                "K. Wu",
                "H. Peng",
                "M. Chen",
                "J. Fu",
                "H. Chao"
            ],
            "title": "Rethinking and improving relative position encoding for vision transformer",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 10033\u201310041.",
            "year": 2021
        },
        {
            "authors": [
                "G. Xu",
                "Z. Song",
                "Z. Sun",
                "C. Ku",
                "Z. Yang",
                "C. Liu",
                "S. Wang",
                "J. Ma",
                "W. Xu"
            ],
            "title": "CAMEL: A Weakly Supervised Learning Framework for Histopathology Image Segmentation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 10681\u201310690.",
            "year": 2019
        },
        {
            "authors": [
                "J. Yao",
                "X. Zhu",
                "J. Jonnagaddala",
                "N. Hawkins",
                "J. Huang"
            ],
            "title": "Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks",
            "venue": "Medical Image Analysis, 65: 101789.",
            "year": 2020
        },
        {
            "authors": [
                "S.G. Zadeh",
                "M. Schmid"
            ],
            "title": "Bias in cross-entropybased training of deep survival networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9): 3126\u2013 3137.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhu",
                "J. Yao",
                "F. Zhu",
                "J. Huang"
            ],
            "title": "Wsisa: Making survival prediction from whole slide histopathological images",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 7234\u20137242.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "In computational pathology, survival prediction based on gigapixels whole slide images (WSIs) is a weakly supervised learning (WSL) task involving local-level tumor microenvironment interactions (Chen et al. 2021b), WSI-level tumorrelated tissue interactions (Abbet et al. 2020) and patientlevel heterogeneous tumor interactions (Carmichael et al. 2022). Multiple instance learning (MIL) is usually adopted to tackle such a WSL problem (Shao et al. 2021b,a). However, bag-based representation learning in MIL still remains an open and challenging problem.\n*Corresponding author: Yongbing Zhang. Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nDifferent from natural images, WSIs have the property of high-resolution and wide field of view (Srinidhi, Ciga, and Martel 2021), so the aggregation of bag-level representation will impose a great demand on computational complexity. In addition, different from the WSI-level MIL problem, as shown in Fig. 1, survival prediction based on WSIs is a patient-level MIL problem (Fan et al. 2021). Since the tumor may have a composite tissue structure, multiple WSIs are usually collected for patient diagnosis. Therefore, in survival prediction, we have to face two dilemmas: 1) multiple WSIs inevitably lead to linearly multiplied data volume; 2) the aggregation of multiple WSI-level bags for a patient.\nFor the aggregation of the WSI-level bag, the risk information in survival prediction is often reflected in a series of histological patterns corresponding to disease progression. For example, the local-level co-localization of tumors and lymphocytes (Shaban et al. 2019) and the WSI-level metastatic distribution of sentinel lymph nodes (Kim et al. 2020) have been shown to correlate with prognosis. Therefore, the spatial and contextual information must be fully considered in a WSI-level bag. Moreover, for the patientlevel bag, intratumoral heterogeneity will inevitably lead to diverse tumor microenvironments in different WSIs (Vitale et al. 2021). So the patient-level contextual information between instances in different WSI-level bags must be considered, and these three-level interactions further constitute hierarchical information in a patient-level bag.\nar X\niv :2\n30 6.\n17 37\n3v 1\n[ cs\n.C V\n] 3\n0 Ju\nn 20\n23\nTo address the challenges mentioned above, numerous works are proposed from two majority aspects: 1) computational cost; 2) spatial, contextual and hierarchical information encoding. For the computational cost problem, randomsample-based methods (Huang et al. 2021b; Di et al. 2022) and cluster-based methods (Yao et al. 2020; Muhammad et al. 2021; Shao et al. 2021a) are widely used. By randomly selecting from different clusters, many cluster-based methods try to include various richer tissue types. However, a large number of tissue patches are discarded and usually lack structural information. For the spatial information encoding, some GNN-based methods (Wang et al. 2021; Li et al. 2018; Chen et al. 2021a) adopt the topological structure to encode neighbor node information in WSI. In addition, Transformer-based methods (Huang et al. 2021b; Shao et al. 2021b) adopt trade-off strategies like using the sincos embedding or applying convolution to implicitly encode location information. However, it is still an unsolved problem to efficiently encode 2D spatial information over such high resolution and irregularly shaped WSIs. For the contextual and hierarchical information encoding, patch-based graph convolutional network (Chen et al. 2021a), Nystrom self-attention (Shao et al. 2021b) and non-local attention (Li, Li, and Eliceiri 2021) are used to encode WSI-level interactions. There are also other methods (Di et al. 2022; Fan et al. 2021) hierarchically processing WSI and patient-level bags to encode hierarchical information. However, limited by the vast amount of data for patient-level survival prediction, randomly sampled patches are always used in the methods above, which inevitably lose potential risk information.\nIn this work, we propose a hierarchical vision Transformer for patient-level survival prediction (HVTSurv) that progressively explores local-level spatial, WSI-level contextual and patient-level hierarchical interactions in the patientlevel bag. The main contributions are as follows:\n1) To alleviate high computational complexity, we devise a local-to-global hierarchically processing framework. Specifically, we leverage the window attention mechanism to design the local and shuffle window block, which can significantly reduce the cost of Transformer. Therefore, we can take advantage of all the patches in the patient-level bag.\n2) We propose a new feature generation method for spatial information embedding, which can fully reflect the local characteristics in both horizontal and vertical directions. In addition, we adopt Manhattan distance in the local window block to measure the local relative position.\n3) For the contextual and hierarchical information encoding, we design the local-level, WSI-level and patient-level interaction layers to hierarchically deal with survival prediction. Besides, we adopt a random window masking strategy in feature generation to further exploit the advantages of our hierarchical processing framework.\n4) Our HVTSurv significantly outperforms state-of-theart (SOTA) methods over 6 public cancer datasets from The Cancer Genome Atlas (TCGA) with less GPU Memory Costs. Besides, in the patient stratification experiment, there is a statistically significant difference (P-Value < 0.05) over all of the 6 cancer types. Attention visualization further verifies our conclusion from experiments."
        },
        {
            "heading": "Related Work",
            "text": ""
        },
        {
            "heading": "Application of MIL in WSIs",
            "text": "The application of MIL in WSIs can be divided into two categories. As shown in Fig. 1, MIL tasks in WSIs include WSI and patient-level MIL. The WSI-level MIL is suitable for some tasks such as tumor&non-tumor classification. Common solutions include instance-based methods (Campanella et al. 2019; Xu et al. 2019; Kanavati et al. 2020; Lerousseau et al. 2020; Chikontwe et al. 2020) and embedding-based methods (Tomita et al. 2019; Hashimoto et al. 2020; Naik et al. 2020; Lu et al. 2021; Hou et al. 2022).\nThe patient-level MIL is suitable for tasks like survival prediction that only has patient-level labels. Common solutions include simultaneous processing (Zhu et al. 2017; Yao et al. 2020; Muhammad et al. 2021; Shao et al. 2021a; Abbet et al. 2020; Wang et al. 2021; Li et al. 2018; Huang et al. 2021b) and hierarchical processing (Chen et al. 2021a; Di et al. 2022; Fan et al. 2021) of all the WSIs from a patient. Simultaneous processing methods generally consider all the WSI-level bags in a patient as one bag. In contrast, the hierarchical processing methods first aggregate features in the WSI-level bag and then further aggregate different WSIlevel bags in the patient-level bag. In general, hierarchical processing methods have the potential to more effectively model the spatial information in the WSI-level bag and contextual and hierarchical information in the patient-level bag."
        },
        {
            "heading": "Application of WSIs for Survival Prediction",
            "text": "For the application of WSIs in survival prediction, a twostage framework is widely used to predict patient hazard scores: 1) sampling and encoding patches; 2) patch features aggregation. In the first stage, constrained by limited computing resources, clustering and random sampling methods (Zhu et al. 2017; Yao et al. 2020; Muhammad et al. 2021; Shao et al. 2021a) are widely used to select representative tissue phenotypes in WSI. However, these randomly sampled patches are not context-aware and lost the interactions between cells and tissue types, which are prognostic for patient survival prediction.\nIn the second stage, many CNN based, GNN based and Transformer based methods are used. Both Yao et al. (Yao et al. 2020) and Shao et al. (Shao et al. 2021a) use a small CNN network to aggregate the sampled feature. However, CNN-based methods have inherent limitations in modeling global topological information. For the GNN-based method, Chen et al. (Chen et al. 2021a) formulate WSIs as a graphbased data structure to obtain hierarchical representations. Wang et al. (Wang et al. 2021) emphasize the tumor microenvironment graph construction. Di et al. (Di et al. 2022) propose a big-hypergraph factorization neural network to obtain the high-order representations. However, the network depth limitation brought by a large amount of data makes GNN more challenging to encode WSI-level information. For the Transformer based method, Huang et al. (Huang et al. 2021b) adopt 2D sin-cos position encoding and Transformer encoder blocks to obtain the bag-level feature. There is still room for improvement in position encoding and feature aggregation to this method."
        },
        {
            "heading": "Method",
            "text": "Considering a set of N patients Xi, for i = 1, . . . , N , each patient Xi has one or multiple WSIs, we have follow-up label (Ti, Ci), where Ti stands for observation time and Ci stands for survival status. The binary status Ci \u2208 {0, 1} indicates whether Ti is a survival time (Ci = 0) or a rightcensored time (Ci = 1). Our task is to predict the survival probability based on all the WSIs for each patient. The accuracy is measured by the consistency between the sorted survival probability and sorted follow-up label sets.\nTo better perform survival prediction, the discrete-time survival model (Vale-Silva and Rohr 2021; Chen et al. 2022) is used in this paper. Briefly, we subdivide the survival time scale into n intervals: [t0, t1) , . . . , [tn\u22121, tn), where t1, . . . , tn\u22121 define the evenly divided points of survival times for uncensored patients and t0 = 0, tn = \u221e. Each\npatient observation time will be attributed to an interval as:\nTi = k iff Ti \u2208 [tk, tk+1). (1)\nTherefore, for each patient, the conditional hazard probability h(k | Xi) can be defined as its failure probability in interval [tk, tk+1):\nh(k | Xi) = P (Ti = k | Ti \u2265 k,Xi). (2)\nSurvival probability S(k | Xi) can be defined as its observation probability at least to the end of interval [tk, tk+1):\nS(k | Xi) = P (Ti > k | Xi) = k\u220f\ns=1\n(1\u2212 h(s | Xi)). (3)\nSince each patient label is known, while neither WSIs label nor patches label is unknown, survival prediction is a\nWSL problem, which can be solved by the MIL methods. To better predict h(k | Xi) from the patient-level bag, as shown in Fig. 7, we propose a Transformer-based framework which is composed of feature generation and feature aggregation."
        },
        {
            "heading": "Feature Generation",
            "text": "We first convert patches to features in WSI processing. Then, we adopt feature rearrangement to maintain the local 2D relative position in rearranged features. Finally, we employ the random window masking to further strengthen the contextual and hierarchical interactions in feature aggregation.\nWSI processing WSIs have gigapixels and often contain many blank regions. We follow the CLAM (Lu et al. 2021) processing steps to remove the background regions, and then cut out 256\u00d7256 images at 20\u00d7 resolution (0.5 \u00b5m/pixel). A ResNet50 model pre-trained on ImageNet is employed to embed each patch in a 1024-dimensional feature vector.\nFeature rearrangement In patient-level survival prediction, a patient may correspond to multiple WSIs. To relieve computational cost, we employ the window attention mechanism (Liu et al. 2021). Limited by irregularly shaped property of WSI, previous raster scanning method will inevitably lose correct 2D spatial information. To better reflect the local characteristics in both horizontal and vertical directions within a window, a feature rearrangement method is proposed to ensure the closeness in both directions of the 2D space after the window partition. The specific implementation is shown in Algorithm 1, and the Euclidean distance is used in the HNSW (Malkov and Yashunin 2018). We also present qualitative and quantitative experimental results in Appendix Fig. 1 and Appendix Fig. 2, respectively.\nAlgorithm 1: Feature rearrangement\nInput A WSI-level bag Hi = {hi,1, . . . ,hi,b}, where hi,j \u2208 Rd is the embedding of the jth instance, Hi \u2208 Rd\u00d7b. Corresponding coordinates Zi = {zi,1, . . . ,zi,b}, where zi,j \u2208 R2 is the original coordinate of the jth instance in WSI, Zi \u2208 R2\u00d7b. Window size w. Output Rearranged features Hr \u2208 Rd\u00d7B . ba \u2190 \u2308 bw \u2309 \u00d7 w \u2212 b \u25b7 Padding width B \u2190 b+ ba \u25b7 Length after padding Hs \u2190 ReflectPadding ( Hi,width = ( \u230a ba2 \u230b, ba \u2212 \u230a ba 2 \u230b\n)) Zs \u2190 ReflectPadding ( Zi,width = ( \u230a ba2 \u230b, ba \u2212 \u230a ba 2 \u230b\n)) Zs \u2190 Zs/256 \u25b7 Scale the original coordinates Zs \u2190 [Zs \u2212 (xmin, ymin)] + 1 \u25b7 (x, y) is the coordinate Initialize Hr as \u2205 for idx \u2208 [0 : B : w] do\n\u25b7 Select the w features closest to zs,1 in Zs, including zs,1 itself\nselect idx\u2190 Hnsw. query (zs,1, topn =w) \u25b7 Add the w closest features to the new array Hr \u2190 Hr +Hs [select idx] Hs \u2190 Hs \u2212Hs[select idx] \u25b7 Delete selected h Zs \u2190 Zs \u2212 Zs[select idx] \u25b7 Delete selected z\nend for\nRandom window masking To increase the robustness of the model for tumor heterogeneity and further exploit the advantages of our hierarchical processing framework, we propose a random window masking strategy. A WSI bag will be further split into several sub-WSI bags. Inspired by the superpixel sampling strategy (Bian et al. 2022), we sample at the window level to maintain 2D spatial information in each local window. Specifically, we perform m random window sampling for the rearranged feature sequence. A WSI feature is divided into m sub-WSIs for subsequent feature aggregation. To avoid adding additional computational burden, the feature number of each sub-WSI is 1/m of the original WSI."
        },
        {
            "heading": "Feature Aggregation",
            "text": "To better encode the spatial, contextual and hierarchical information in the patient-level bag, we propose a hierarchical vision Transformer named HVTSurv to perform feature aggregation in the patient-level bag. The HVTSurv is mainly composed of three layers, including the local-level, WSIlevel and patient-level interaction layer. In our paper, locallevel means patch features within the same window, WSIlevel means patch features from different local windows within a sub-WSI, and patient-level means patch features from different sub-WSIs within a patient. The overview of proposed three interaction layers is shown in Fig. 7.\nLocal-level interaction layer To encode local spatial information in each WSI, we design a local-level interaction layer. Due to the irregularly shaped property of WSIs, the local windows usually appear irregularly shaped. In our intuitive experience, the distance information in the local space always contains more near range spatial structure information than the direction information in WSI. So in this paper, we use the Manhattan distance to encode the relative position information between different patches in each window. The 2D spatial information between different patches is consequently reduced to 1D distance information. Similar to the relative position encoding method used in SwinTransformer (Liu et al. 2021), a learnable matrix B\u0302 is used to learn the embedding of different distances, which is combined with the self-attention (SA). In the local window block, the self-attention (Liu et al. 2021) corresponding to each head in computing similarity can be defined as:\nSAlocal = softmax\n( QKT +B\u221a\nd\n) , (4)\nwhere Q \u2208 Rw\u00d7d, K \u2208 Rw\u00d7d, B \u2208 Rw\u00d7w is the relative position bias, and values in B are taken from B\u0302, with w being the number of patch features in a window.\nA segmented Manhattan distance is used to make the model more sensitive to short rather than long distances. Inspired by the method in (Wu et al. 2021), the expression of the piecewise function is defined as follows:\ng(x) = { [|x|], |x| \u2264 \u03b1 min ( \u03bb, [ \u03b1+ ln(|x|/\u03b1)ln(\u03b3/\u03b1) (\u03b2 \u2212 2\u03b1) ]) , |x| > \u03b1\n(5)\nwhere [\u00b7] is a round operation, \u03b1, \u03b2, \u03bb, \u03b3 are all hyperparameters and we parameterize a learnable matrix B\u0302 \u2208 R(2\u03bb+1)\u00d7head for all heads. WSI-level interaction layer To encode WSI-level longdistance contextual information, we design a WSI-level interaction layer. We adopt the spatial shuffle method so the patch features from different regions in a WSI-level bag can be used for similarity computation in the same window. Specifically, for each WSI-level bag after the local-level interaction layer, we spatially shuffle the feature sequence before dividing the window and calculating the window attention. It should be noted that in the self-attention calculation, we do not add spatial information. For the spatial shuffle algorithm, we use the shuffle method noted in (Huang et al. 2021a). In shuffle window block, the self-attention (Vaswani et al. 2017) corresponding to each head in computing similarity can be defined as:\nSAshuffle = softmax\n( QKT\u221a\nd\n) , (6)\nwhere Q \u2208 Rw\u00d7d, K \u2208 Rw\u00d7d, with w being the number of patch features in a window.\nPatient-level interaction layer To further explore the hierarchical information from the WSI to the patient level, we design a patient-level interaction layer focusing on global contextual interaction across the entire patient-level bag. Specifically, we first concatenate all sub-WSI features corresponding to a patient, and then an attention pooling layer (AttnPool) is used to obtain patient-level representation hpatient to estimate the patient\u2019s hazard risk h(k | Xi). Specifically, AttnPool can be defined as:\nag = exp {U (tanh (Vhg))}\u2211G j=1 exp {U (tanh (Vhj))} ,\nhpatient = G\u2211 g=1 aghg,\n(7)\nwhere U \u2208 R1\u00d7dh , V \u2208 Rdh\u00d7d, with dh being the dimension of hidden layer, G is the number of patches in a patient.\nTo optimize the model parameters, we adopt the log likelihood function (Zadeh and Schmid 2020; Chen et al. 2022) as loss function. For an uncensored patient (Ci = 0) with failure in interval [tk, tk+1), the likelihood can be calculated as the survival probability in [t0, tk) multiplied by the failure probability in [tk, tk+1):\nluncensored = h(k | Xi)S(k \u2212 1 | Xi). (8) For a censored patient (Ci = 1) with censored in interval [tk, tk+1), the likelihood can be calculated as the survival probability in [t0, tk+1):\nlcensored = S(k | Xi). (9) Finally, the loss function can be defined as:\nL =\u2212 Ci logS(k | Xi) \u2212 (1\u2212 Ci) logS(k \u2212 1 | Xi) \u2212 (1\u2212 Ci) log h(k | Xi).\n(10)"
        },
        {
            "heading": "Experimental Results",
            "text": ""
        },
        {
            "heading": "Datasets",
            "text": "We closely follow the data settings of PatchGCN (Chen et al. 2021a). Five public cancer types from TCGA are adopted: Bladder Urothelial Carcinoma (BLCA), Breast Invasive Carcinoma (BRCA), Glioblastoma&Lower Grade Glioma (GB&LG), Lung Adenocarcinoma (LUAD), Uterine Corpus Endometrial Carcinoma (UCEC). We take gastrointestinal tract cancer type into our experiment for a comprehensive comparison: Colon&Rectal Adenocarcinoma (CO&RE). Six public cancer datasets include 3,104 patients and 3,752 H&E diagnostic WSIs, whose specific information is summarized in Table 1."
        },
        {
            "heading": "Evaluation Metric and Implementation Details",
            "text": "This paper uses Concordance Index (C-Index) and Kaplan\u2013Meier (KM) estimator with a Log-rank test for evaluation metrics. For the dataset partition, we adopt 4-fold crossvalidation. For WSIs and follow-up labels, we follow the PatchGCN processing step. For the parameters and training of HVTSurv, the window size is 49, the number of sub-WSIs is 2, and the survival loss function Eq.(10) is adopted with the training batch size being 1. More details are in Appendix."
        },
        {
            "heading": "Results and Discussion",
            "text": "Performance comparisons for all methods are summarized in Table 2 and Appendix Fig. 3, including C-Index scores, P-Values and KM analysis. We also compare computational efficiency in Appendix Table 1. For the 4-fold crossvalidation C-Index results in Table 2, we present it as \u201caverage C-Indexstandard deviation\u201d. \u201cTCGA-Mean\u201d represents the average C-Index scores on the 6 TCGA datasets. Besides, we bold the best and underline the second best.\nCompared with WSI-level MIL methods such as DSMIL and TransMIL, the results of patient-level MIL methods including PatchGCN and HVTSurv show that hierarchically aggregating the patient-level features can make a better survival prediction. Compared with random sampling methods such as DeepAttnMISL and SeTranSurv, HVTSurv adopts the hierarchical processing framework that can handle more patch features. Moreover, local spatially correlated windows obtained by the feature rearrangement can help to achieve significantly better results. Compared with Transformerbased methods such as TransMIL, SeTranSurv, and ESAT, convolutional based and sin-cos based position encoding\nschemes pay more attention to global spatial information, which inevitably loses local prognostic information. HVTSurv creatively adopts the Manhattan distance to represent the relative position in the local window, which can correctly and effectively encode local prognostic information. Compared with GNN-based models such as DeepGraphSurv and PatchGCN, different from simply increasing model depth, HVTSurv adopts spatial shuffle for all the local windows, which can encode WSI-level interaction more efficiently. Compared with other methods in computational efficiency, HVTSurv benefits from the window attention method and has more efficient GPU Memory Costs in patient-level MIL task. Compared with other methods in Log-rank test, binary experiments show that low and high-risk patients have a statistically significant difference (P-Value < 0.05) over 6 cancer types. In summary, the average C-Index is 2.50-11.30% higher than all competitive models over 6 TCGA datasets."
        },
        {
            "heading": "Ablation and Effectiveness Analysis",
            "text": "We further conduct a series of ablation studies to determine the contribution of different modules and major components in HVTSurv and test the parameters used in this paper. We use the average C-Index score to measure the performance.\nIn Table 3 and Appendix Table 2, we test the effect of different modules, major components and different model structures in HVTSurv. The results show that both position encoding and spatial shuffle play a significant role in improving the performance of the model. WSI is a highresolution and irregularly shaped image after background removal, it is hard to directly encode accurate contextual interaction in the WSI-level bag. So we devise a two-step approach, i.e., local position encoding and WSI-level spatial shuffle. Local accurate spatial information is an essential basis for global information encoding, we find it has\na more significant effect on the performance improvement. Besides, we also perform ablation experiments on the three major components. In most cancers, combining the three interaction layers can better encode the spatial, contextual and hierarchical information in the patient-level bag. Due to the different prognostic features of various cancers, the importance of each interaction layer is slightly different. BLCA and UCEC are two similar cancer types whose prognosis depend more on global-level features such as the depth of tumor invasion in the myometrium and bladder wall, so the WSI-level interaction layer plays a more critical role in these cancer types. In Table 4, we test the effect of different window sizes in HVTSurv, we find that a moderate window size can help the model to learn the spatial interaction within the window more accurately and efficiently. Moreover, a relatively small window size can fully utilize the window method\u2019s high computational efficiency. In Table 5, we test the effect of different sub-WSI numbers for the random window masking strategy, it can be found that the training strategy of random window masking can help the model better adapt to the heterogeneity of cancer. Moreover, dividing a WSI into multiple sub-WSIs can further exploit the advantages of our hierarchical processing framework."
        },
        {
            "heading": "Interpretability and Attention Visualization",
            "text": "We further explore the interpretability of our HVTSurv model in the slide level and patch level, whose results are shown in Fig. 3 and Fig. 4, respectively. The details are in the Appendix. In Fig. 3, the attention to the three major components is gradually extended from local low-level information to global high-level information, such as cancer-related regions. Benefiting from the hierarchical network, the receptive field of the model can gradually become larger, then the hierarchical information in the patient-level bag can be fully explored. Besides, in Fig. 4, we show the number of tumorrelated patches in the high attention score area, which further explains from patch level statistical result for the whole CO&RE dataset. HVTSurv adopts a hierarchically designed network structure encoding interactions from local-level to WSI-level and further to patient-level, which can gradually discover the critical prognostic tissues like tumor-related tissues STR and TUM. We can also find that for high-risk patients, the number of tumor-related patches has significantly increased, which has been medically proven to be related to the prognosis of colorectal cancer (Abbet et al. 2020)."
        },
        {
            "heading": "Conclusion",
            "text": "In this work, we propose a hierarchical Vision Transformer named HVTSurv that progressively explores locallevel spatial interaction, WSI-level contextual interaction and patient-level hierarchical interaction in the patient-level survival prediction. Hierarchical processing framework effectively reduces the computational cost, which is suitable for patient-level MIL tasks. Inspired by this, we propose the local and shuffle window block to progressively obtain the WSI-level representation and an attention pooling layer to get patient-level hazard risk. Besides, we design feature preprocessing strategies, including feature rearrangement and random window masking to explore spatial, contextual, hierarchical information better. Compared to SOTA methods, we achieve a better average C-Index over the 6 TCGA datasets, with a performance gain of 2.50%. In KM analysis and Logrank test, the low and high-risk patients have a statistically significant difference over 6 TCGA cancer types. In the ablation study, we prove that adopting Manhattan distance based position encoding and spatial shuffle based long-range interaction can obtain better representation. Moreover, the ablation results of three interaction layers demonstrate the effectiveness of our hierarchical processing framework. The visualization of attention further confirms our conclusions."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by the National Natural Science Foundation of China (61922048&62031023), in part by the Shenzhen Science and Technology Project (JCYJ20200109142808034), and in part by Guangdong Special Support (2019TX05X187)."
        },
        {
            "heading": "Method",
            "text": ""
        },
        {
            "heading": "Feature Generation",
            "text": "To better reflect the local characteristics in both horizontal and vertical directions within a window, a feature rearrangement method is proposed to ensure the closeness in both directions of the 2D space after the window partition. We present quantitative and qualitative experimental results in Fig. 5 and Fig. 6, respectively."
        },
        {
            "heading": "Experimental Results",
            "text": ""
        },
        {
            "heading": "Evaluation Metric and Statistical Analysis",
            "text": "\u2022 Concordance Index For evaluation metric, Concordance\nIndex(C-Index) (Heagerty and Zheng 2005) is usually adopted in survival prediction. The C-Index is computed as follows:\nc = 1\nn \u2211 i:Ci=0 \u2211 j:Tj>Ti I [fi > fj ] , (11)\nwhere n is the number of comparable pairs, I[.] is the indicator function, and f. denotes the patient\u2019s predicted risk. The value of C-Index ranges from 0 to 1. The larger C-Index value means the model has a better prediction performance. This paper uses the average C-Index to represent cross-validated C-Index performance.\n\u2022 Kaplan\u2013Meier estimator For statistical analysis, Kaplan\u2013Meier (KM) estimator (Kaplan and Meier 1958) with Log-rank test is a common tool in survival prediction. KM estimator is a non-parametric statistic used to\nestimate the survival function S(t), which is defined as:\nS\u0302(t) = \u220f\ni:ti\u2264t\n( 1\u2212 di\nni\n) , (12)\nwhere ti denotes the time when at least one event happened, di represents the number of events (e.g., deaths) that happened at time ti, and ni denotes the individuals known to have survived (have not yet had an event or been censored) up to time ti. The Log-rank test is used to test for statistical significance (P-Value < 0.05) in survival distributions between low and high risk patients (Bland and Altman 2004). We divide high and low risk patients for each test fold according to out-of-sample risk predictions. Then we concatenate all test fold binary results to perform KM analysis and Log-rank test."
        },
        {
            "heading": "Implementation Details",
            "text": "For the dataset partition, we use 4-fold cross-validation to train and test all the models for each TCGA cancer type. We randomly split the patient data in the ratio of training: validation: test = 60: 15: 25. We use the StratifiedKFold (Pedregosa et al. 2011) method to ensure the training validation and test sets have similar censorship ratios. For each WSI, we follow CLAM (Lu et al. 2021) processing step to extract patch features, and each WSI contains approximately 12017 256\u00d7256 image patches at 20\u00d7 magnification, with some patients having up to 17 WSIs. For the random masking strategy, the number of sub-WSI is 2. For the follow-up label, we follow Patch-GCN (Chen et al. 2021a) processing step. For each cancer type, we use the quartiles of survival time for uncensored patients to divide the survival times of all patients into 4 non-overlapping intervals. For the parameters of HVTSurv, we set \u03b1 = 1.9, \u03b2 = 7.6, \u03b3 = 11.4, \u03bb = 7 in the piecewise function. We adopt the feature reduction for the Linear layer from 1024 to 512 dimensions. For each window, the window size is 49, i.e., there are 49 patch features within each window. For the sake of simplicity, the local window block and shuffle window block adopt the same window size. Besides, we instantiate one local-level interaction layer to process all sub-WSIs of a patient separately. For the training of HVTSurv, the Ranger optimizer (Wright 2019) is employed with a learning rate of 2e-4 and weight decay of 1e-5. The validation loss is used as the monitor metric, and the early stopping strategy is adopted, with the patience of 8. In addition, for a fair comparison, we employ the same survival loss function, i.e., cross entropy-based survival loss function as Eq.(6) in the main text, ResNet-50 feature embeddings, and training hyperparameters for all the compared methods."
        },
        {
            "heading": "Results and Discussion",
            "text": "To further illustrate the computational efficiency of HVTSurv, we compare FLOPs, Params, and GPU Memory Costs. The TCGA-12-0769 WSI image is selected to calculate FLOPs, and GB&LG data set is selected to calculate the GPU Memory Costs under half precision floating-point format, where each WSI contains 12421 patches on average. All experiments are performed on a GeForce RTX 3090, and\nthe results are shown in Table 6. It should be noted that \u201cXX / 24268 M\u201d in GPU Memory Cost means the GPU memory usage during model training, \u201c24268 M\u201d represents the total GPU memory of a GeForce RTX 3090. OOM means out of memory, (*) means the input feature dimension is reduced from 1024 to 128, and for others, the input feature dimension is reduced from 1024 to 512.\nIt can be seen that ESAT and SeTranSurv have many model parameters, and different operations are required to prevent out of memory during model training. Besides, for TransMIL and PatchGCN, it is still challenging to process high-dimensional data. Therefore, for the input features, the dimension is reduced from 1024 to 128 for subsequent feature aggregation. In particular, when the models of TransMIL and PatchGCN have no extra operations, and out of memory will inevitably happen due to large model complexity. Based on the window attention method, HVTSurv is devised to solve the GPU Memory Costs problem in patientlevel survival prediction. Under the same feature dimension, HVTSurv has fewer FLOPs, Params, and GPU Memory Costs than PatchGCN."
        },
        {
            "heading": "Ablation and Effectiveness Analyses",
            "text": "We show the effect of different model structures of HVTSurv in Table 7. The results show that the model structures of two WSI-level interaction layers and our HVTSurv model structure can achieve relatively good results. Since our method employs a two-step progressive encoding of global information, i.e. local accurate spatial information plus global contextual information interaction, the proposed network architecture can achieve more robustness on different cancer datasets compared to other schemes."
        },
        {
            "heading": "Interpretability and Attention Visualization",
            "text": "We further explore the interpretability of our HVTSurv model. For the slide level interpretability, we adopt the TCGA cancer annotation from Loeffler et al (Loeffler and Kather 2021). For the patch level interpretability, we adopt the nine tissue-class fully supervised dataset NCT-CRC-HE (Kather et al. 2019) on colorectal cancer for tissue classification of patches with high attention scores in high-risk patients and low-risk patients, respectively. Specifically, for the slide level interpretability, attention weights are re-scaled from min-max to [0, 1], and for better visualization, we drop 80% of the smaller attention value. For the patch level interpretability, all low-risk and high-risk patients were defined as those below and above the 25% and 75% predicted risk percentiles, respectively. For the WSI of low-risk and highrisk patients, 1% of high-interest patches were selected.\nWe briefly introduce the visualization. For the multi-head self-attention map Amultihead \u2208 Rw\u00d7w\u00d7head, where w denotes window size, we first utilize a mean pooling in head dimension to obtain Ahead \u2208 Rw\u00d7w. Then we employ a mean pooling in one w dimension to get the importance of each patch feature in the window, i.e., A \u2208 R1\u00d7w. For better visualization, we first drop 80% of the smaller attention value, and then splice the results of all the window attention. It should be noted that for the shuffle window block, the window attention obtained after shuffle needs to be restored to the original spatial position and then spliced. The visualization part also refers to the implementation in CLAM."
        }
    ],
    "title": "HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image",
    "year": 2023
}