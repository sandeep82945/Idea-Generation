{
    "abstractText": "Most Neural Topic Models (NTM) use a variational auto-encoder framework producing K topics limited to the size of the encoder\u2019s output. These topics are interpreted through the selection of the top activated words via the weights or reconstructed vector of the decoder that are directly connected to each neuron. In this paper, we present a model-free two-stage process to reinterpret NTM and derive further insights on the state of the trained model. Firstly, building on the original information from a trained NTM, we generate a pool of potential candidate \u201ccomposite topics\u201d by exploiting possible co-occurrences within the original set of topics, which decouples the strict interpretation of topics from the original NTM. This is followed by a combinatorial formulation to select a final set of composite topics, which we evaluate for coherence and diversity on a large external corpus. Lastly, we employ a user study to derive further insights on the reinterpretation process.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jia Peng Lim"
        },
        {
            "affiliations": [],
            "name": "Hady W. Lauw"
        }
    ],
    "id": "SP:15bdf6b7da1893629db92941176ebc0669ae2a03",
    "references": [
        {
            "authors": [
                "Rakesh Agrawal",
                "Ramakrishnan Srikant."
            ],
            "title": "Fast algorithms for mining association rules",
            "venue": "Proc. 20th Int. Conf. Very Large Data Bases, VLDB, pages 487\u2013499.",
            "year": 1994
        },
        {
            "authors": [
                "Nikolaos Aletras",
                "Mark Stevenson."
            ],
            "title": "Evaluating topic coherence using distributional semantics",
            "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS 2013) \u2013 Long Papers, pages 13\u201322, Potsdam, Germany.",
            "year": 2013
        },
        {
            "authors": [
                "Per Austrin",
                "Subhash Khot",
                "Muli Safra."
            ],
            "title": "Inapproximability of vertex cover and independent set in bounded degree graphs",
            "venue": "2009 24th Annual IEEE Conference on Computational Complexity.",
            "year": 2009
        },
        {
            "authors": [
                "Federico Bianchi",
                "Silvia Terragni",
                "Dirk Hovy."
            ],
            "title": "Pre-training is a hot topic: Contextualized document embeddings improve topic coherence",
            "venue": "3696",
            "year": 2021
        },
        {
            "authors": [
                "Federico Bianchi",
                "Silvia Terragni",
                "Dirk Hovy",
                "Debora Nozza",
                "Elisabetta Fersini."
            ],
            "title": "Cross-lingual contextualized topic models with zero-shot learning",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "David M. Blei",
                "Andrew Y. Ng",
                "Michael I. Jordan."
            ],
            "title": "Latent dirichlet allocation",
            "venue": "J. Mach. Learn. Res., 3:993\u20131022.",
            "year": 2003
        },
        {
            "authors": [
                "Gerlof Bouma."
            ],
            "title": "Normalized (pointwise) mutual information in collocation extraction",
            "venue": "Proceedings of the Biennial GSCL Conference 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Jin-Hee Cho",
                "Yating Wang",
                "Ing-Ray Chen",
                "Kevin S. Chan",
                "Ananthram Swami."
            ],
            "title": "A survey on modeling and optimizing multi-objective systems",
            "venue": "IEEE Communications Surveys & Tutorials, 19(3):1867\u20131901.",
            "year": 2017
        },
        {
            "authors": [
                "P.C. Chu",
                "J.E. Beasley."
            ],
            "title": "Genetic algorithm for the multidimensional knapsack problem",
            "venue": "Journal of Heuristics, 4(1):63\u201386.",
            "year": 1998
        },
        {
            "authors": [
                "Adji B. Dieng",
                "Francisco J.R. Ruiz",
                "David M. Blei."
            ],
            "title": "Topic modeling in embedding spaces",
            "venue": "Transactions of the Association for Computational Linguistics, 8:439\u2013453.",
            "year": 2020
        },
        {
            "authors": [
                "Thanh-Nam Doan",
                "Tuan-Anh Hoang."
            ],
            "title": "Benchmarking neural topic models: An empirical study",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4363\u20134368, Online.",
            "year": 2021
        },
        {
            "authors": [
                "Michael R. Garey",
                "David S. Johnson."
            ],
            "title": "Computers and Intractability: A Guide to the Theory of NP-Completeness",
            "venue": "USA.",
            "year": 1979
        },
        {
            "authors": [
                "Luis Gouveia",
                "Pedro Martins."
            ],
            "title": "Solving the maximum edge-weight clique problem in sparse graphs with compact formulations",
            "venue": "EURO Journal on Computational Optimization, 3(1):1\u201330.",
            "year": 2015
        },
        {
            "authors": [
                "Derek Greene",
                "Padraig Cunningham."
            ],
            "title": "Practical solutions to the problem of diagonal dominance in kernel document clustering",
            "venue": "ICML, volume 148 of ACM International Conference Proceeding Series, pages 377\u2013384.",
            "year": 2006
        },
        {
            "authors": [
                "Amulya Gupta",
                "Zhu Zhang."
            ],
            "title": "Vectorquantization-based topic modeling",
            "venue": "ACM Trans. Intell. Syst. Technol., 12(3).",
            "year": 2021
        },
        {
            "authors": [
                "Anna Hart."
            ],
            "title": "Mann-whitney test is not just a test of medians: Differences in spread can be important",
            "venue": "BMJ (Clinical research ed.), 323:391\u20133.",
            "year": 2001
        },
        {
            "authors": [
                "Matthew Hoffman",
                "Francis Bach",
                "David Blei."
            ],
            "title": "Online learning for latent dirichlet allocation",
            "venue": "Advances in Neural Information Processing Systems, volume 23.",
            "year": 2010
        },
        {
            "authors": [
                "Tushar Kalra",
                "Rogers Mathew",
                "Sudebkumar Prasant Pal",
                "Vijay Pandey."
            ],
            "title": "Maximum weighted independent sets with a budget",
            "venue": "CALDAM.",
            "year": 2017
        },
        {
            "authors": [
                "Subhash Khot."
            ],
            "title": "On the power of unique 2-prover 1-round games",
            "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing STOC '02.",
            "year": 2002
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling."
            ],
            "title": "AutoEncoding Variational Bayes",
            "venue": "2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings.",
            "year": 2014
        },
        {
            "authors": [
                "Soukaina Laabadi",
                "Mohamed Naimi",
                "Hassan El Amri",
                "Boujem\u00e2a Achchab."
            ],
            "title": "The 0/1 multidimensional knapsack problem and its variants: A survey of practical models and heuristic approaches",
            "venue": "American Journal of Operations Research, 08(05):395\u2013439.",
            "year": 2018
        },
        {
            "authors": [
                "Jey Han Lau",
                "Timothy Baldwin",
                "Trevor Cohn."
            ],
            "title": "Topically driven neural language model",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 355\u2013365, Vancouver, Canada.",
            "year": 2017
        },
        {
            "authors": [
                "Jey Han Lau",
                "David Newman",
                "Timothy Baldwin."
            ],
            "title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality",
            "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational",
            "year": 2014
        },
        {
            "authors": [
                "Kar Wai Lim",
                "Wray L. Buntine."
            ],
            "title": "Bibliographic analysis with the citation network topic model",
            "venue": "ACML, volume 39 of JMLR Workshop and Conference Proceedings.",
            "year": 2014
        },
        {
            "authors": [
                "Tianyi Lin",
                "Zhiyue Hu",
                "Xin Guo."
            ],
            "title": "Sparsemax and relaxed wasserstein for topic sparsity",
            "venue": "Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM \u201919, page 141\u2013149, New York, NY, USA.",
            "year": 2019
        },
        {
            "authors": [
                "H.B. Mann",
                "D.R. Whitney."
            ],
            "title": "On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other",
            "venue": "The Annals of Mathematical Statistics, 18(1):50 \u2013 60.",
            "year": 1947
        },
        {
            "authors": [
                "Silvano Martello",
                "Paolo Toth."
            ],
            "title": "Knapsack Problems: Algorithms and Computer Implementations",
            "venue": "USA.",
            "year": 1990
        },
        {
            "authors": [
                "Yu Meng",
                "Yunyi Zhang",
                "Jiaxin Huang",
                "Yu Zhang",
                "Chao Zhang",
                "Jiawei Han."
            ],
            "title": "Hierarchical topic mining via joint spherical tree and text embedding",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Yishu Miao",
                "Lei Yu",
                "Phil Blunsom."
            ],
            "title": "Neural variational inference for text processing",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1727\u20131736, New",
            "year": 2016
        },
        {
            "authors": [
                "Feng Nan",
                "Ran Ding",
                "Ramesh Nallapati",
                "Bing Xiang."
            ],
            "title": "Topic modeling with Wasserstein autoencoders",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6345\u20136381, Florence, Italy.",
            "year": 2019
        },
        {
            "authors": [
                "Shirui Pan",
                "Jia Wu",
                "Xingquan Zhu",
                "Chengqi Zhang",
                "Yang Wang."
            ],
            "title": "Tri-party deep network representation",
            "venue": "IJCAI, pages 1895\u20131901.",
            "year": 2016
        },
        {
            "authors": [
                "Radim \u0158eh\u016f\u0159ek",
                "Petr Sojka."
            ],
            "title": "Software Framework for Topic Modelling with Large Corpora",
            "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta, Malta.",
            "year": 2010
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "SentenceBERT: Sentence embeddings using Siamese BERTnetworks",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Michael R\u00f6der",
                "Andreas Both",
                "Alexander Hinneburg."
            ],
            "title": "Exploring the space of topic coherence measures",
            "venue": "WSDM, pages 399\u2013408.",
            "year": 2015
        },
        {
            "authors": [
                "Ashoka Savasere",
                "Edward Omiecinski",
                "Shamkant B. Navathe."
            ],
            "title": "An efficient algorithm for mining association rules in large databases",
            "venue": "Proceedings of the 21th International Conference on Very Large Data Bases, VLDB \u201995, page 432\u2013444, San Fran-",
            "year": 1995
        },
        {
            "authors": [
                "Dazhong Shen",
                "Chuan Qin",
                "Chao Wang",
                "Zheng Dong",
                "Hengshu Zhu",
                "Hui Xiong."
            ],
            "title": "Topic modeling revisited: A document graph-based neural network perspective",
            "venue": "Advances in Neural Information Processing Systems 34 - 35th Conference on Neural",
            "year": 2021
        },
        {
            "authors": [
                "Akash Srivastava",
                "Charles Sutton."
            ],
            "title": "Autoencoding variational inference for topic models",
            "venue": "ICLR (Poster).",
            "year": 2017
        },
        {
            "authors": [
                "Jie Tang",
                "Jing Zhang",
                "Limin Yao",
                "Juanzi Li",
                "Li Zhang",
                "Zhong Su."
            ],
            "title": "Arnetminer: extraction and mining of academic social networks",
            "venue": "KDD, pages 990\u2013998.",
            "year": 2008
        },
        {
            "authors": [
                "Silvia Terragni",
                "Elisabetta Fersini",
                "Bruno Giovanni Galuzzi",
                "Pietro Tropeano",
                "Antonio Candelieri"
            ],
            "title": "OCTIS: Comparing and optimizing topic models is simple",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Hannu Toivonen."
            ],
            "title": "Sampling large databases for association rules",
            "venue": "Proceedings of the 22th International Conference on Very Large Data Bases, VLDB \u201996, page 134\u2013145, San Francisco, CA, USA.",
            "year": 1996
        },
        {
            "authors": [
                "Ilya O. Tolstikhin",
                "Olivier Bousquet",
                "Sylvain Gelly",
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "Wasserstein autoencoders",
            "venue": "ICLR.",
            "year": 2018
        },
        {
            "authors": [
                "Wenlin Wang",
                "Zhe Gan",
                "Hongteng Xu",
                "Ruiyi Zhang",
                "Guoyin Wang",
                "Dinghan Shen",
                "Changyou Chen",
                "Lawrence Carin."
            ],
            "title": "Topic-guided variational auto-encoder for text generation",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Zhengjue Wang",
                "Zhibin Duan",
                "Hao Zhang",
                "Chaojie Wang",
                "Long Tian",
                "Bo Chen",
                "Mingyuan Zhou."
            ],
            "title": "Friendly topic assistant for transformer based abstractive summarization",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Liang Yang",
                "Fan Wu",
                "Junhua Gu",
                "Chuan Wang",
                "Xiaochun Cao",
                "Di Jin",
                "Yuanfang Guo."
            ],
            "title": "Graph attention topic modeling network",
            "venue": "Proceedings of The Web Conference 2020, WWW \u201920, page 144\u2013154, New York, NY, USA. Association for Com-",
            "year": 2020
        },
        {
            "authors": [
                "Ce Zhang",
                "Hady W. Lauw."
            ],
            "title": "Topic modeling on document networks with adjacent-encoder",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):6737\u20136745.",
            "year": 2020
        },
        {
            "authors": [
                "He Zhao",
                "Dinh Phung",
                "Viet Huynh",
                "Yuan Jin",
                "Lan Du",
                "Wray Buntine."
            ],
            "title": "Topic modelling meets deep neural networks: A survey",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4713\u20134720.",
            "year": 2021
        },
        {
            "authors": [
                "Renbo Zhao",
                "Vincent Tan",
                "Huan Xu."
            ],
            "title": "Online Nonnegative Matrix Factorization with General Divergences",
            "venue": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Re-",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3688 - 3703 December 7-11, 2022 \u00a92022 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "To help us understand the latent structures within a text corpus, topic models associate each document with \u201ctopics\u201d (Blei et al., 2003). In turn, each topic is associated with a set of words that frequently co-occur together in various documents, forming a semantically coherent grouping that fosters interpretability. Aside from the common applications in text analysis and classifications, topic models are also used in advanced downstream tasks such as in summarization (Wang et al., 2020), text generation (Wang et al., 2019), and language modelling (Lau et al., 2017). While earlier topic models are based on graphical models, more recent topic models are neural, with several based on the variational autoencoder framework (Kingma and Welling, 2014). Traditionally, what constitutes a topic is a neuron at the encoder\u2019s output. Its association with words is typically derived from a selection of the top activated words via the weights or reconstructed vector\nof the decoder connected to that neuron, forming what we now interpret as a topic-word distribution.\nMotivation. While such autoencoder-based topic models are adept at learning lowerdimensional representations of documents, we question the notion of one-to-one correspondence between a topic and a neuron. We postulate that this traditional view belies the natural working order of a neural model, whereby it is the joint activation of several neurons, rather than the singular activation of an independent neuron, that may be responsible for the generation or reconstruction of document semantics. Moreover, the traditional interpretation of only the top activations in the resultant topic-word distribution ignores the potential information that might be gleaned from the rest of the distribution. We therefore hypothesize that individual neurons are but components of a \u201ctopic\u201d that is inherently compositional in nature. And, to properly interpret an autoencoder-based topic model, we need to fully utilise the topic-word distribution space to uncover such compositions of neurons that frequently coactivate to collectively represent a semantic topic.\nApproach. Given a generic class of trained neural topic model (NTM) (to be defined in Section 3) with K component (original) topics, we seek to reinterpret the NTM by finding a new set of K compositional topics that are more attuned to wellaccepted measures of topic interpretability (also to be specified in Section 3). Each compositional topic is a linear combination of the original component topics. Inherently, the number of potential compositional topics are combinatorially explosive. Thus, we propose a two-stage process of candidate generation via mining the neural activations of various documents in the original corpus for frequently co-activated neurons, followed by candidate selection via solving optimization problems that map to classical algorithmic formulations with well-established computational properties.\nContributions. To our best knowledge, this is\n3688\nthe first work to seek a reinterpretation of an NTM via compositional topics. We reiterate that our objective is not to replace, but to derive further quantitative insights on the state of the trained model. This reinterpretation process is model-free, as validated on a number of base NTMs (see Section 7.1).\nSecondly, we propose an approach that aligns the mining of compositional topics to the objective of optimizing for well-accepted notions of topic interpretability. This approach is realized through principled formulations of frequent itemset mining for candidate generation (Section 5), as well as maximum independent sets and multi-dimensional knapsack for candidate selection (Section 6).\nThirdly, through quantitative measurements of interpretability on external large corpora, we show that the compositional topics tend to perform better than the original output of NTM\u2019s (Section 7).\nFinally, as our core thrust is topic interpretability, we employ a user study to derive additional insights from the reinterpretation process (Section 8).\nImplementation. Our gurobipy1 and an alternative CVXPY2 implementation can be found at github.com/PreferredAI/ReIntNTM."
        },
        {
            "heading": "2 Related Work",
            "text": "There are many neural topic models (NTMs), a comprehensive review can be found at Zhao et al. (2021). Primarily, the focus has been on creating better models, with numerous NTMs benchmarked in Doan and Hoang (2021). More detailed descriptions of our baseline NTMs used in the experiments can be found in Section 7.1. There are also notable research efforts to derive better interpretability of NTMs, such as through works focusing on topic sparsity (Lin et al., 2019; Gupta and Zhang, 2021), and through weakly supervised training (Meng et al., 2020).\nAnother popular approach to topic modelling involves using graph-based NTMs such as in Shen et al. (2021), Yang et al. (2020), and Zhang and Lauw (2020) which utilizes Graph Neural Networks, and/or, leveraging on graph representations of document/word/document-word relations and also through graph representations of higher-level entity metadata. The key distinction between our work and previous stand-alone graph-based NTMs is that our model-free approach is rooted in (nonneural network) classical selection problems with\n1www.gurobi.com 2www.cvxpy.org\nthe choices (component topics) represented in a graphical manner.\nFinally, there are other non-neural networkbased topic modelling approaches such as online mean-field variational inference (Hoffman et al., 2010) and Non-negative matrix factorization (Zhao et al., 2017)."
        },
        {
            "heading": "3 Preliminaries",
            "text": "Neural Topic Model (NTM). Let D denote a text corpus, K the desired number of topics, and N the vocabulary. An autoencoder-based NTM \u03c4 trained on D would produce a latent layer at the output of the encoder that we denote \u03b8. The ith neuron \u03b8i is referred to as an original or component topic. To associate \u03b8i with its topic words, we examine the topic-word decoder\u2019s weights or outputs due to the sole activation of \u03b8i. Considering the general case where a topic-word decoder has one of more hidden layers, we set \u03b8i = 1 with the other \u03b8j = 0 \u2200\u03b8j \u2208 {\u03b8 \\ \u03b8i}. Passing this input through the decoder, \u2200\u03b8i \u2208 \u03b8, creates a K \u00d7 |N | topicword relation matrix \u03b2. Taking the l top-activated words from each row in \u03b2 produces a topic set T = {Ti}i=1,...,K consisting of K number of lsized word sets Ti, using the top activated words in each row of \u03b2.\nNormalised Point-wise Mutual Information (NPMI). Introduced in Bouma (2009) and evaluated for texts in Aletras and Stevenson (2013) and Lau et al. (2014), this is a popular metric used for evaluating T . In R\u00f6der et al. (2015), it is shown that NPMI has a good correlation with human ratings and the least sensitive to changes in the windows size parameter. This metric ranges from -1, suggesting incoherence, to 1, suggesting coherence within the topic. Let n represent a word in vocabulary N .\nnpmi(ni, nj) = log\np(ni,nj) p(ni)p(nj)\n\u2212log(p(ni, nj)) (1)\nNPMI(T ) = 1\nK\n\u2211\nt\u2208T\n\u2211 ni\u2208t \u2211 nj\u2208t, nj \u0338=ni npmi(ni, nj)\nl(l \u2212 1)/2 (2)\nTopic Uniqueness (TU). We seek to obtain K diverse topics (each of which is coherent), rather than a repetition of the same coherent topics multiple times. An intuitive measure is to count how many unique words are collectively represented by\nthe K topics (more unique words means less repetition). TU is defined as a percentage of unique words in the topic set (Dieng et al., 2020; Bianchi et al., 2021a). This ranges from 1K to 1, with 1 implying that each topic is unique and each word occurs once in T .\nTU(T ) = | \u222at\u2208T {nt \u2208 t}|/(l \u00b7K) (3)"
        },
        {
            "heading": "4 Overview",
            "text": "Classically, the interpretation of NTM, after training on D, is as-is via \u03b2. This assumes independence within \u03b8 and that the \u03c4 \u2019s complexity is surface-deep. Since neurons work together in a composite manner to optimize a loss function, we believe that these composite interactions C within \u03b8 has the potential to produce a better interpretation of \u03c4 . As shown in Fig. 1, we seek to find a\nC \u2208 RL\u00d7K that interacts with \u03b2K\u00d7|N | to form a better reinterpretation \u03b2\u0302 to produce new topic set T\u0302 with K topics3. The sum of each row in C is constrained to 1, reflecting the components\u2019 weight in the composite topic, sufficiently representing all possible compositions.\nFor simplicity and without loss of generality, we consider the case where components are evenlyweighted in each composition. Modelling the compositions within \u03b2 results in a binary combinatorial search space 2KCK . The difficulty of selecting the best C is further increased as it involves optimizing for two potentially-diverging objectives as there exist solutions that result in high coherence with low diversity and vice versa. Common strategies to solve for multiple objectives include min-max and weighted-sum. Cho et al. (2017) has a comprehensive survey on solving Multi-Objective Systems. We employ \u03f5-programming, where we focus on NPMI objective while converting TU objective into a soft constraint.\nProblem 1 (Reinterpreting NTM). Given \u03b2 from a NTM \u03c4 . Find a K \u00d7 K composite matrix C that produces a new reinterpretation \u03b2\u0302 \u2208 RK\u00d7|N | where C \u00b7 \u03b2 = \u03b2\u0302. Where T\u0302 with K topics, T\u0302 = {top-l(b\u0302i)|b\u0302i \u2208 \u03b2\u0302, b\u0302i \u2208 R1\u00d7|N |}, is derived from \u03b2\u0302 and maximizes the primary objective NPMI(T\u0302 ) and secondary objective TU(T\u0302 ) with soft constraints \u03f5.\nProposed Approach. In Stage I, Topic Candidate Generation seeks to identify a pool of candidate topics V of feasible size m from the exponential number of possible compositions. In Stage II, Topic Selection uses several proposed formulations relying on \u03f5 to pick the final K composite topics, from V , to produce T\u0302 that has high NPMI and TU. We elaborate on each of these stages in the coming sections."
        },
        {
            "heading": "5 Stage I: Topic Candidate Generation Based on Neural Activation Profiles",
            "text": "We make the critical observation that which neurons tend to co-activate with one another can be\n3While in general, the new topic set could be larger or smaller, for parity in this paper we set them to be the same as the original number of topics. Hence, L = K.\nmined from the pattern of neural activations of the documents within a corpus. From Figure 3, the activation distribution of \u03c4 on D in layer \u03b8 is similar to a pareto distribution, with a only a few neurons being responsible for most of the activation strength. For practical purposes, we limit the size of compositions of up to five different component topics. Leveraging on D and \u03c4 , producing document-topic relations \u0398 \u2208 R|D|\u00d7K , we can find frequently occurring compositions in D.\nWe can transform our current search problem to the Frequent Itemset Mining problem (FIM) (Agrawal and Srikant, 1994). The input to FIM is a set of transactions, where each transaction is a basket of items. The objective is to output all frequent itemsets, i.e., subsets of items that occur in at least s (minimum support) of the transactions.\nIn our context, each transaction is a document, and each item is an activated neuron. We set the minimum activation threshold \u03ba to the fifth-largest mean activation value for \u0398. For each document d, we set its \u03b8d,k = 1 \u21d0\u21d2 \u03b8d,k > \u03ba else 0, creating boolean \u201citemsets\u201d (essentially baskets of co-activated neurons). Hyper-parameter minimum support s controls the size of V (setting larger values of s resulting in fewer candidates). While there are many solution approaches to FIM (Savasere et al., 1995; Toivonen, 1996), we leverage the Apriori algorithm4 (Agrawal and Srikant, 1994).\nThe resulting frequent itemsets C\u0302 (each itemset specifying a few co-activated neurons) generate candidate pool V = {top-l(b)|b \u2208 C\u0302 \u00b7\u03b2}, i.e., each v \u2208 V is a set of top-l words due to the correspond-\n4http://rasbt.github.io/mlxtend\ning composition of topics in an \u201citemset\u201d."
        },
        {
            "heading": "6 Stage II: Diversity-Constrained Coherence-Optimizing Topic Selection",
            "text": "We now seek to reduce V to find the final K composite topics that represent C, by optimizing for NPMI, as evaluated on D5. However, due to the way that diversity-oriented constraint \u03f5 could be formulated, this gives rise to a couple of formulation variants as outlined below."
        },
        {
            "heading": "6.1 Maximum-Weight Budget Independent Set (MWBIS)",
            "text": "Suppose that candidates V are vertices in a graph G(V,E). An edge (vi, vj) \u2208 E exists if the corresponding candidate topics have more than \u03f5 number of similar words. To ensure diversity, we seek an independent set of unconnected vertices in G. Because we could only accommodate K topics, the independent set must be budgeted or capped in size to K. Because there are many possible K-sized independent sets, we seek the one with the maximum weight, which is the coherence score NPMI.\nMixed Integer Program. Formulating it as a mixed integer problem (MIP), we have an objective (4) with budget constraints (5) to (7). Binary xv represents whether a topic v \u2208 V is selected, and wv, representing NPMI of v. Constraint (5) allows us to have negative weights. Constraint (7) restrict the number of times a word can appear in T\u0302 .\nmax v\n\u2211\nv\nxvwv (4)\ns.t V\u2211\nv\nxv = K (5)\nxi + xj \u2264 1, \u2200i, j \u2208 E (6)\n\u2211\nj=1,...,K|n\u2208Tj xj \u2264 \u03f5+ 1,\u2200n \u2208 N (7)\nThis formulation is essentially a maximumweighted budget independent set problem (MWBIS) Kalra et al. (2017), which is a variant of the well-established maximum-weighted independent set problem (MWIS), a known NP-hard problem for general graphs (Garey and Johnson, 1979). Even so, this could still be solvable for smaller\n5This is only for training. For testing, we evaluate NPMI based on large external corpora.\ngraphs, particularly with the help of numerical solvers capable of approximations.\nGreedy Algorithm. We introduce this simple approach, that mirrors the formulation of our MWBIS formulation, as a fallback approach when it is infeasible to use solvers. It employs two heuristics f and g. f ensures that each Ti \u2208 T\u0302 is no more than \u03f5-similar of each other. g ensures that each word occurs at most \u03f5 + 1 times in T\u0302 . The procedure iterates on V , sorted by NPMI, greedily choosing v, popped from V , if adding v to T\u0302 fulfils f and g. If we do not have K topics after a complete iteration, we increment \u03f5, bounded by l, and repeat iteration, terminating procedure upon selecting K topics.\nFrom Austrin et al. (2009), assuming unique games conjecture (Khot, 2002) and P \u0338= NP , they prove that there is no \u2126( log\n2 \u25b3 \u25b3 )-factor polyno-\nmial time approximation algorithm for MWIS in a degree-\u25b3 bounded graph when \u25b3 is sufficiently large. According to Kalra et al. (2017), the hardness result of MWIS applies to MWBIS as well. While we are unable to ensure optimal bounds for the greedy solution, it performs well empirically for a reasonable size of V (see Section 7)."
        },
        {
            "heading": "6.2 Multi-Dimensional Knapsack Problem (MDKP)",
            "text": "In addressing diversity, the previous formulation seeks to reduce overlap between pairs of candidates. An alternative diversity constraint could be to seek some minimum number of unique words among the selected topic candidates.\nAgain, we maximize the similar objective (4) with budget constraint (5) and treat the number of unique words as a budget to exceed in (8). For our experiments, we set \u03f5MDKP to the number of unique words in the original T , i.e., |{nv \u2208 v|v \u2208 T }|.\n| \u222av\u2208V |xv=1 {v}| \u2265 \u03f5MDKP (8) This formulation transforms our problem into a 0/1 Multi-dimensional Knapsack Problem (MDKP) (Martello and Toth, 1990), a NP-hard problem (Chu and Beasley, 1998). It is also noted in Laabadi et al. (2018) that available heuristics and metaheuristics approaches for MDKP did not ensure optimality."
        },
        {
            "heading": "7 Experiments",
            "text": "The primary objective of the following experiments is to investigate the efficacy of the terpretation pro-\ncess, i.e, whether the discovered composite topics via our methodology would outperform the component topics from the input NTMs (denoted Original in result tables) in terms of NPMI and TU."
        },
        {
            "heading": "7.1 Base Neural Topic Models",
            "text": "As previously asserted, our reinterpretation process is model-free, accommodating various NTMs. In this sub-section we describe the NTMs used in our experiments. There are 3 encoder parameters that we optimize for with respect to D: 1) Number and 2) Size of hidden encoder layers and 3) Dropout. For more information, refer to Appendix B.\nCTM (Bianchi et al., 2021b). We chose this model as it utilises S-BERT (Reimers and Gurevych, 2019) embeddings as an additional source of information to construct a topic model. Additionally, there are other models such as (Dieng et al., 2020) that leverage on word embeddings.\nNeuralLDA (Srivastava and Sutton, 2017). Introduced alongside ProdLDA, with its main difference is how its \u03b2 is interpreted. For its \u03b2, the decoder\u2019s weights are further processed via batchnormalisation and softmax.\nNVDM (Miao et al., 2016). It is widely used as a baseline comparison in topic modelling, and is shown to produce a topic set that has has a weaker coherence compared to other NTMs.\nProdLDA (Srivastava and Sutton, 2017). This NTM is a popular topic modelling baseline and is also used as a backbone model in CTM. Compared to NeuralLDA, ProdLDA\u2019s \u03b2 does not undergo addition processing steps.\nWTM (Nan et al., 2019). This model differs greatly from the other selected models as it uses Wasserstein auto-encoders (Tolstikhin et al., 2018) for topic modelling. We use the recommended hyper-parameters Dirichlet parameter of 0.1 and noise coefficient \u03b1 to 0.5."
        },
        {
            "heading": "7.2 Training Corpora",
            "text": "We use four English language corpora from OCTIS. For more details about the preparation of the cor-\npora, refer to Terragni et al. (2021). Aside from the quantifiable differences (Table 2), we also note that 20NewsGroup6 and BBC-news (Lim and Buntine, 2014) have vocabularies that are considered more general and broad compared to the specialized and technical vocabularies found in M10 (Greene and Cunningham, 2006) and DBLP (Tang et al., 2008; Pan et al., 2016).\nEach corpus has a predefined train/val/test split comprising of 70%/15%/15%. During the training phase, the models optimizes its loss function on the train set in an unsupervised manner. The val set is used to determine early stopping. The full corpus is used for coherence evaluation during the Topic Selection stage."
        },
        {
            "heading": "7.3 NPMI",
            "text": "For our NPMI calculation, we use the recommended window size of 10 to consider word cooccurrences. To score V , with l = 10, we evaluate for NPMI on D, using Gensim7 (R\u030cehu\u030ar\u030cek and Sojka, 2010) wrapper in OCTIS. These NPMI scores are then utilised to select T\u0302 in Topic Selection.\nFor a fairer evaluation against the original T , we conducted coherence evaluation on a external large corpora, using Palmetto8 (R\u00f6der et al., 2015), a coherence evaluation tool with its word co-occurrence index built from Wikipedia articles. We do not measure perplexity, because our reinterpretation process does not change the weights of \u03c4 , hence, \u03c4 \u2019s perplexity remains unchanged. As NPMI of topics within T\u0302 and T might not have a normal distribution, a one-sided Mann\u2013Whitney U test (Mann and Whitney, 1947) is suitable (Hart, 2001) to evaluate the significance of the difference in NPMI between T\u0302 and T ."
        },
        {
            "heading": "7.4 Results",
            "text": "Better composite topics can be found. In most experiment instances with results for K = 20, shown in Table 3, we are able to discover a set of composite topics T\u0302 that score better in NPMI and TU on the external reference corpus, suggesting that T\u0302 is more coherent and has a higher generality compared to T . The observations for K = 20 extends to when K = 50 (see Appendix C.1).\nInformation outside of top l words. To get a 6http://people.csail.mit.edu/jrennie/20Newsgro ups/ 7https://radimrehurek.com/gensim/models/coher encemodel.html 8https://aksw.org/Projects/Palmetto.html\nsense of how composite topics are different from the components, Table 4 shows several examples selected from ProdLDA (MDKP) on 20NewsGroup at K = 20. From the first example, \"medical\" did not appear in the top-10 words of the component surface topics. Combining all three component topics (2, 6, 12) could surface the word in this \"healthcare research\"-related topic. Furthermore, some words that are highly activated in the component topics, are suppressed in the composite topics. We believe this is caused by negative values in \u03b2, that may be informative. Experiments conducted with positively-constrained \u03b2 yields worse results compared to unconstrained \u03b2.\nReducing redundancy. We showcase the third example in Table 4 where two unique but similarthemed component topics combine to form a better composite topic. The two component topics are excluded from the final T\u0302 . By folding together two similar component topics, we could make room to surface other topics of other themes, improving the diversity of T\u0302 qualitatively.\nOn model collapse. When T contains similar topics, the composite combinations of these topics would also produce similar topics in T\u0302 . In Table 3b, while T of NVDM has similar topics, we still can improve NPMI and TU in T\u0302 , despite many candidate topics sharing similar words, However, if a topic model collapses to a single topic, it is unlikely that we can generate more topics.\nBetter topic set not guaranteed. This occurs when T\u0302 does not improve on T in both metrics, suggesting \u03b2\u0302 \u2248 \u03b2, such as in Table 3c, where MDKP for NeuralLDA unable to find a better T\u0302 . Consequently, this means that we are likely to be already evaluating the best topic set that can be interpreted from the topic model.\nImpact of \u03f5. Adjusting \u03f5 influences the solution space of T\u0302 , resulting in trade-off between uniqueness and coherence. Table 5 shows that as \u03f5 increases, NPMI increases while TU decreases. Since different \u03f5 produces different T\u0302 , we might have multiple solutions where T\u0302 is better than T .\nImpact of s. We tried three different ways of generating candidate pools (see Table 6) and find that in cases where |V | discovered by FIM (referred to as discovered) is low, adding composite pairs to V generated by Apriori algorithm is a nonexpensive method to increase |V |. However, overgenerating candidates might result in topics overfitted to the training corpus. Comparing modes\n\u2019pairs\u2019 (candidate topics must be composite of two components only) and \u2019add-pairs\u2019 (adding pairs to the discovered frequent itemsets), we can conclude that compositions of more than 2 topics can be meaningful. From our experiment results, a recommended target size |V | close to 1000 is reasonable for K = 20 and K = 50, and can be revised upwards for larger values of K."
        },
        {
            "heading": "7.5 Computational Practicability",
            "text": "In the hundreds of experiments (shown in Figure 4), a few could not be solved within time limit with MIP gap > 0.05. These involve large V exceeding 10,000 candidate topics with \u03f5 set to enforce tight uniqueness constraint, i.e. \u03f5 = 0. In Gouveia and Martins (2015), experiments on similar maximumweight clique problems suggest that solver may be impractical when both density of graph and vertex\ncount is high. However, setting reasonable \u03f5 and s to avoid such conditions, we find many feasible T\u0302 . In any case, the Greedy approach is always capable of producing a solution."
        },
        {
            "heading": "8 User Study",
            "text": "We have 29 valid responses to our user study, consisting of 30 questions (14 normal and 1 verification each for two tasks below). We excluded responses that failed verification questions9, ensuring responses of higher quality. Before starting, participants were given a short primer on coherence and reminded that there are no right or wrong answers.\nQuestions. Procedure of random question generation, with topics sorted alphabetically, and example questions can be found in Appendix A.\n9A verification question would contain a \u2019fake\u2019 topic, e.g., \u201canimal blood you should select this option for this question\u201d.\nFor Task I, participants are shown a pair of composite-component topics and asked to identify which of the two is more coherent. We split the 14 questions into two groups where half of the questions contains a component topic with NPMI strictly larger than its paired composite topic, with the other half having equal or less.\nFor Task II, participants are shown a group of topics consisting of one composite topic and its components and asked to check which topics they think are coherent. They may select multiple op-\ntions or none at all. Following which, they are asked if the composite topic is related to its components. Out of the 14 questions in Task II, 7 groups of topics will serve as control, with one of its component topic randomly swapped out with another.\nInsights. In Task I, we establish that NPMI indeed has a positive correlation (Pearson\u2019s r = 0.500\u2217\u2217\u2217)10 to participants\u2019 selection of their preferred topic, with greater participant\u2019s agreement in instances where NPMI difference is large. Additionally, despite the 50/50 split, in 60% of question instances, participants choose the composite topic over its component topic.\nIn Task II, we plot each topic shown as a point\n10We note that our r is slightly lower than the r = 0.653 reported in R\u00f6der et al. (2015)\nin Figure 5. On average, composite topics have a higher consistent agreement (%), amongst participants marking it as coherent, with a mean of 78%, compared to component topics, at 61%. Additionally, in terms of composite-component relevance, 5 out of 7 treatment groups have more than 75% of participants agreeing that the composite topic is relevant to the component topics, compared to 0 out of 7 control groups for the same criteria. This reveals that while majority of the composite topics are built out of related component topics, there are also instances when non-related component topics contribute to form composite topics."
        },
        {
            "heading": "9 Conclusion",
            "text": "Our proposed two-stage reinterpretation process strongly demonstrates the possibility of obtaining better topic sets. Its accompanying improvements, in both computational metrics and human evaluation, highlight the necessity to view the original topic model in a composite manner to reveal a deeper interpretation. Since auto-encoder frameworks are widely used on other tasks, future investigation is required to explore and determine if this methodology can be applied to other tasks as well.\nLimitations\nUsing composite topics for documents. We conducted a simple supervised classification task using supervised logistic regression to compare composite and original topic vectors. Classification accuracy for both vectors are very similar suggesting parity in information while being different in the interpretation of the information.\nEffect of \u03c4 \u2019s K on B\u0302 and T\u0302 . Given the scope of this paper, we have not explored comparing similar NTMs with different K, i.e., comparing T\u0302 from\na model with K = 20 against T from the same model type with K = 50 or higher values of K. Overcoming the current NTM\u2019s fixed K might help to generate better models tailored to evaluation for a specific number of topics and more investigation into this area is required. |V| generated. For the purposes of parity, we try to keep |V | at similar levels for experiments shown. However, the relationship between NTMs and generated V varies, some models might require a larger |V | to showcase its full potential.\nEthics Statement\nWe understand that some corpus might produce topics with group of words that might cause offense due to possible sensitiveness regarding politicallycharged affairs in the Middle-East. Hence, for our user study, we reviewed questions to remove or replace any topics that we think might be offensive. However, for the sake of transparency, these omitted topics are still included in the full set of topics that are listed in the Appendix D. The use of the reinterpretation process is largely dependent on the corpus that NTM \u03c4 is trained on."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-RP2021-020). We also extend our gratitude to our user study participants, as well as, our reviewers for their feedback."
        },
        {
            "heading": "A User Study Appendix",
            "text": "Task I. To select pairs, we shuffled T\u0302 and select the first 7 pairs of random topics made up of one composite topic and one of its component topic where the NPMI of the composite topic is more than its random component topic. We repeat the procedure\nto obtain another 7 pairs with NPMI of composite topic is lower than its random component topic. T\u0302 is from CTM on 20NewsGroup at K = 50. The options for each questions are randomized when displayed to the volunteer.\nTask II. We randomly select 14 groups of topics from T\u0302 made up of a single component and its component topics. T\u0302 is from ProdLDA on 20NewsGroup at K = 50. Of the 14 groups, we randomly choose 7 groups and replace one of its component with a random topic to create a control sample to test for composite-component similarity relations. The component topic is shown at the top of the list, followed by the component topics.\nParticipant recruitment. We recruited study participants from two groups of people. For the first group, we have 17 valid responses from graduates with a STEM background, physically located locally in our city. For the second group, we have 12 valid responses from a small online text-based role-playing game community, physically located around the world. On average, the responses from\nboth group are similar."
        },
        {
            "heading": "B Model Parameters and Optimization",
            "text": "For all NTMs, except WTM, we use OCTIS11 bayesian optimizer to search for encoder parameters with 30 optimization iterations and 3 model runs each with selected parameters in Table 7. For all NTMs, their decoder has no hidden layers. We adapted NVDM12 for OCTIS framework. For WTM 13, we use similar recommended parameters suggested in (Nan et al., 2019). We use default values for unmentioned parameters.\n11https://github.com/MIND-Lab/OCTIS 12referred to both https://github.com/YongfeiYan/Ne ural-Document-Modeling and https://github.com/ysm iao/nvdm\n13https://github.com/awslabs/w-lda"
        },
        {
            "heading": "C Additional Results Appendix",
            "text": "C.1 Experiment results for NTMs with K = 50 The tabled results for 20NewsGroup and BBC-news for NTMs with K = 50.\nC.2 Full results for ablation on s The extended tabled results for three different modes of generations for different s."
        },
        {
            "heading": "D Full Topic Set Examples",
            "text": "NPMI shown are evaluated on large external corpora in Palmetto. Each composite topic is shown in terms of a listing of the component topics, e.g., composite topic (1, 17) indicates that it has been derived from combining component topic 1 and topic 17. For each topic, we show the NPMI score, as well as a list of the top-10 words.\nTopic sets T\u0302 (MDKP) and T from ProdLDA on 20NewsGroup at K = 20 # NPMI Topics New composite topics in T\u0302 1, 17 0.03 people, road, town, kill, armenian, dead, soldier, woman, body, leave 3, 5 0.01 fire, compound, die, death, building, child, place, evil, tear, life 5, 8 0.07 agent, warrant, criminal, illegal, police, batf, federal, citizen, law, crime 7, 9 0.18 game, season, team, player, win, score, year, play, hockey, playoff 13, 15 0.14 drive, card, disk, work, scsi, problem, driver, hard, ide, controller 14, 19 0.07 file, window, image, color, format, display, convert, widget, program, set 2, 6, 12 0.16 research, medical, treatment, patient, disease, medicine, study, effect, health, fund 2, 11, 12 0.10 science, scientist, observation, objective, scientific, natural, theory, term, human, concept 2, 12, 16 0.08 sell, sale, price, pay, interested, cost, purchase, item, money, offer 10, 15, 16 0.04 speaker, external, connector, circuit, mhz, internal, apple, motherboard, parallel, cable 14, 16, 16 0.04 monitor, card, video, mouse, memory, meg, printer, ram, vga, resolution 15, 17, 18 0.07 engine, oil, brake, replace, car, battery, tire, plastic, shop, dealer 1, 2, 5, 11 0.06 moral, society, justify, matter, sexual, sex, defend, practice, prove, freedom 4, 6, 8, 19 0.16 internet, mail, network, address, email, privacy, access, message, newsgroup, information 4, 13, 14, 19 0.05 advance, code, compile, graphic, host, shareware, window, utility, library, application 5, 12, 17, 18 0.12 vehicle, gas, heavy, engine, tank, ride, foot, fuel, pound, weight Common component topics in T\u0302 and T 7 0.10 game, playoff, score, hockey, fan, goal, blue, period, season, shot 8 0.06 key, clipper, chip, secure, encrypt, encryption, escrow, security, algorithm, enforcement 11 0.12 homosexual, belief, religion, truth, interpretation, nature, meaning, homosexuality, christian, human 12 0.12 launch, satellite, year, mission, orbit, space, station, rocket, flight, system Excluded component topics from T\u0302 but in T 0 -0.03 powerful, frequently, consist, limited, earlier, deep, longer, numerous, compare, portion 1 0.10 muslim, people, israeli, genocide, village, population, turkish, jewish, government, armenian 2 0.03 medicine, literature, bias, article, research, blood, associate, treatment, poster, treat 3 0.01 people, make, time, thing, president, work, church, morning, pray, give 4 -0.02 advance, summary, reply, host, address, interested, domain, compile, email, print 5 0 batf, fire, compound, assault, knock, gas, warrant, crime, agent, criminal 6 0.04 firearm, people, gun, patient, drug, bill, health, amendment, law, weapon 9 0.07 good, year, player, make, time, point, season, average, league, team 10 -0.07 gather, pre, fair, remark, portion, critical, previously, chapter, frequently, limited 13 0.09 system, disk, work, run, backup, drive, memory, software, driver, card 14 0.05 window, screen, font, color, default, mouse, convert, event, display, problem 15 0.08 scsi, drive, card, ide, cable, speed, problem, fast, boot, connector 16 -0.04 sell, sale, price, offer, monitor, interested, shipping, video, card, condition 17 0.11 car, bike, engine, ride, tire, road, brake, start, floor, gear 18 -0.01 requirement, warning, consist, limited, submit, frequently, complaint, chain, oil, recommend 19 0.05 mail, internet, pub, site, graphic, email, file, send, format, list\nTopic sets T\u0302 (MDKP) and T from ProdLDA on 20NewsGroup at K = 50 # NPMI Topics New composite topics in T\u0302 0, 32 0.09 address, mail, email, mailing, paper, network, list, topic, internet, advance 0, 35 0.20 space, mission, orbit, shuttle, system, launch, satellite, solar, flight, rocket 1, 3 0.09 church, christian, passage, verse, scripture, word, father, teach, refer, doctrine 1, 11 0.08 love, sin, faith, life, good, make, eternal, doctrine, hate, give 4, 25 0.10 window, font, screen, manager, expose, button, display, default, event, app 6, 36 0.04 drive, problem, speed, buy, hard, cable, fast, scsi, power, ide 7, 12 0.10 people, armenian, turkish, massacre, genocide, village, muslim, population, organize, russian 7, 38 0.07 fire, shoot, officer, batf, bullet, incident, knock, gun, wound, tank 9, 12 0.08 israeli, people, arab, jewish, state, territory, occupy, land, civil, country 10, 34 0.09 game, blue, goal, score, play, penalty, back, shot, lead, circle 13, 22 0.14 effect, treat, blood, patient, medical, energy, cell, disease, animal, treatment 14, 46 0.10 card, monitor, port, video, board, slot, motherboard, pin, external, vga 15, 22 0.01 page, guide, email, mail, interested, software, computer, daily, volume, fax 15, 35 0 bag, annual, art, book, copy, element, cover, object, title, flight 17, 24 0.05 law, public, number, key, enforcement, agency, court, amendment, encrypt, license 18, 23 0.20 team, game, season, play, player, baseball, league, playoff, fan, win 19, 31 0.12 absolute, truth, atheism, belief, atheist, moral, definition, objective, statement, morality 22, 43 0.10 medical, patient, food, doctor, treatment, health, eat, year, high, disease 23, 34 0.11 game, team, playoff, play, cap, pen, score, goal, lose, wing 24, 44 0.03 key, secret, chip, algorithm, escrow, clipper, agency, enforcement, encryption, encrypt 25, 40 0.07 window, run, file, directory, problem, manager, menu, program, application, display 25, 46 0.04 mouse, driver, card, mode, problem, video, memory, fine, window, instal 27, 38 0.09 gun, crime, criminal, illegal, violent, drug, insurance, abuse, warrant, police 28, 47 0.10 noise, battery, cycle, frequency, circuit, voltage, heat, low, band, audio 33, 42 0.02 widget, export, motif, window, resource, set, subject, include, server, client 43, 47 0.10 water, oil, temperature, weight, air, battery, heat, fuel, pressure, bike 1, 19, 31 0.14 truth, belief, absolute, christian, christianity, true, religion, human, moral, nature 2, 8, 38 -0.03 fire, batf, compound, watch, tear, gas, building, hear, death, tank 2, 11, 34 0.14 team, game, baseball, fan, play, pitch, hit, ball, bad, player 2, 23, 34 0.05 game, fan, team, play, baseball, watch, playoff, hockey, ranger, pen 3, 9, 19 0.16 homosexual, sex, homosexuality, gay, sexual, male, relationship, behavior, christian, society 4, 32, 33 0.05 advance, print, code, printer, font, convert, draw, tool, character, library 6, 16, 26 0.10 lock, engine, bike, seat, front, owner, rear, chain, wheel, paint 7, 8, 9 0.13 people, kill, civilian, child, murder, woman, innocent, rape, man, israeli 9, 17, 38 0.03 gun, batf, weapon, crime, law, assault, firearm, state, citizen, armed 14, 16, 41 0.06 sale, sell, offer, condition, cheap, price, ship, company, shipping, brand 14, 29, 36 0.02 card, disk, board, ram, video, port, modem, drive, meg, bus 15, 33, 40 0.06 graphic, processing, package, mail, database, software, object, pub, send, analysis 28, 36, 46 0.11 drive, pin, cable, card, internal, connector, connect, board, port, controller Common component topics in T\u0302 and T 6 0.16 bike, car, drive, ride, tire, transmission, gear, engine, brake, shift 7 0.05 people, body, massacre, village, dead, town, bullet, escape, soldier, troop 8 0.02 people, time, neighbor, thing, afraid, building, mother, floor, parent, hospital 12 0.09 greek, turkish, muslim, genocide, century, jewish, armenian, international, territory, soviet 24 0.07 key, block, encrypt, secret, serial, bit, chip, session, generate, algorithm 28 0.13 audio, power, voltage, circuit, input, supply, wire, price, speaker, output 30 0.02 people, make, work, president, decision, morning, job, yesterday, talk, meeting 31 0.06 science, existence, objective, scientist, atheism, evidence, observation, exist, atheist, universe 37 0.04 vote, newsgroup, article, post, group, discussion, topic, propose, creation, response 44 0.01 government, ensure, technology, privacy, encryption, administration, industry, policy, escrow, conversation 48 0.03 quality, image, color, compression, scale, conversion, format, convert, file, shareware\nExcluded component topics from T\u0302 but in T 0 0.06 post, shuttle, space, mail, posting, usenet, list, email, internet, mailing 1 0.10 church, teach, sin, love, doctrine, soul, faith, life, christian, passage 2 -0.04 fan, lot, watch, doesn, food, guess, dream, baseball, hockey, ball 3 0.05 male, homosexuality, homosexual, cite, refer, historical, law, writer, term, tradition 4 0.04 font, character, print, convert, button, advance, window, expose, printer, attribute 5 0 extend, deep, impossible, originally, permission, spread, consist, huge, tip, frequently 9 0.05 israeli, people, gay, sex, arab, law, homosexual, civilian, society, sexual 10 0.01 time, back, car, people, walk, start, blue, work, year, make 11 0 good, win, love, make, life, faith, pitcher, year, sin, team 13 0.07 energy, effect, blood, bank, reduce, pain, treat, animal, brain, reaction 14 0.01 sale, offer, card, monitor, price, video, sell, interested, board, item 15 -0.01 copy, art, graphic, bag, daily, book, sale, annual, interested, price 16 0.07 sell, sale, company, market, engine, cost, condition, launch, satellite, firm 17 0.06 firearm, license, weapon, bill, file, gun, dangerous, section, amendment, assault 18 0.10 year, good, season, team, average, player, league, draft, game, excellent 19 0.05 truth, absolute, gay, relationship, moral, sex, belief, atheism, christian, agree 20 0.01 make, people, president, time, work, military, yesterday, government, meeting, russian 21 -0.01 domain, portion, pattern, guarantee, summary, greatly, frequently, host, permission, numerous 22 0.07 patient, page, medical, health, treatment, disease, child, volume, adult, internet 23 0.11 pen, team, fan, lose, cap, baseball, playoff, game, win, play 25 0.02 window, run, problem, win, menu, main, manager, file, directory, app 26 0.02 chain, lock, clean, cut, portion, originally, travel, stay, seat, tip 27 0.02 insurance, drug, private, people, canadian, make, cost, doctor, spend, government 29 -0.01 driver, card, run, problem, instal, mouse, screen, ram, video, memory 32 0 advance, address, paper, interested, domain, email, summary, mail, fax, reply 33 0.05 tool, platform, motif, analysis, processing, widget, graphic, export, data, filter 34 0.09 game, goal, lead, score, blue, wing, tie, period, team, play 35 0.03 planet, solar, surface, earth, orbit, moon, degree, sun, mission, dark 36 0.10 drive, scsi, ide, modem, problem, transfer, system, disk, internal, apple 38 -0.03 fire, batf, gun, compound, gas, cop, auto, knock, initial, agent 39 0 suit, portion, virtually, ball, frequently, apparently, joke, supposedly, numerous, suffer 40 0.06 file, database, system, package, workstation, run, graphic, mail, function, utility 41 -0.01 portion, originally, task, virtually, external, upgrade, frequently, sale, numerous, guarantee 42 0.01 resource, variable, client, window, widget, root, make, entry, include, set 43 0.04 pressure, water, car, air, food, engine, eat, day, temperature, good 45 -0.01 portion, popular, frequently, originally, complaint, collection, permission, virtually, successful, tip 46 0.06 card, port, drive, mouse, monitor, controller, board, video, driver, pin 47 0.07 water, heat, cycle, noise, oil, weight, ride, bike, temperature, effect 49 -0.01 numerous, essentially, tip, impossible, worry, complaint, virtually, portion, frequently, suit"
        }
    ],
    "title": "Towards Reinterpreting Neural Topic Models via Composite Activations",
    "year": 2022
}