{
    "abstractText": "The AI community has been pursuing algorithms known as artificial general intelligence (AGI) that apply to any kind of real-world problem. Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear. One may owe the dilemma to the fact that visual signals are more complex than language signals, yet we are interested in finding concrete reasons, as well as absorbing experiences from GPT and LLMs to solve the problem. In this paper, we start with a conceptual definition of AGI and briefly review how NLP solves a wide range of tasks via a chat system. The analysis inspires us that unification is the next important goal of CV. But, despite various efforts in this direction, CV is still far from a system like GPT that naturally integrates all tasks. We point out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world. We then imagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale, interactable environments, pre-trains it to predict future frames with respect to its action, and then fine-tunes it with instruction to accomplish various tasks. We expect substantial research and engineering efforts to push the idea forward and scale it up, for which we share our perspectives on future research directions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lingxi Xie"
        },
        {
            "affiliations": [],
            "name": "Longhui Wei"
        },
        {
            "affiliations": [],
            "name": "Xiaopeng Zhang"
        },
        {
            "affiliations": [],
            "name": "Kaifeng Bi"
        },
        {
            "affiliations": [],
            "name": "Xiaotao Gu"
        },
        {
            "affiliations": [],
            "name": "Jianlong Chang"
        }
    ],
    "id": "SP:1571915c859a2090143fe030b54cac9ee6d70b2a",
    "references": [
        {
            "authors": [
                "OpenAI"
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv preprint arXiv:2303.08774, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Bubeck",
                "V. Chandrasekaran",
                "R. Eldan",
                "J. Gehrke",
                "E. Horvitz",
                "E. Kamar",
                "P. Lee",
                "Y.T. Lee",
                "Y. Li",
                "S. Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "L. Ouyang",
                "J. Wu",
                "X. Jiang",
                "D. Almeida",
                "C. Wainwright",
                "P. Mishkin",
                "C. Zhang",
                "S. Agarwal",
                "K. Slama",
                "A. Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Wei",
                "X. Wang",
                "D. Schuurmans",
                "M. Bosma",
                "E. Chi",
                "Q. Le",
                "D. Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "F. Li",
                "S. Liu",
                "L. Zhang",
                "H. Su",
                "J. Zhu",
                "L.M. Ni",
                "H.-Y. Shum"
            ],
            "title": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection",
            "venue": "International Conference on Learning Representations, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "F. Li",
                "H. Zhang",
                "H. Xu",
                "S. Liu",
                "L. Zhang",
                "L.M. Ni",
                "H.-Y. Shum"
            ],
            "title": "Mask dino: Towards a unified transformer-based framework for object detection and segmentation",
            "venue": "Computer Vision and Pattern Recognition, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "Computer Vision and Pattern Recognition, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "International Conference on Learning Representations, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "International Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "D. Li",
                "S. Savarese",
                "S. Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "arXiv preprint arXiv:2301.12597, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "R. Rombach",
                "A. Blattmann",
                "D. Lorenz",
                "P. Esser",
                "B. Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "International Conference on Machine Learning, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Kirillov",
                "E. Mintun",
                "N. Ravi",
                "H. Mao",
                "C. Rolland",
                "L. Gustafson",
                "T. Xiao",
                "S. Whitehead",
                "A.C. Berg",
                "W.-Y. Lo"
            ],
            "title": "Segment anything",
            "venue": "arXiv preprint arXiv:2304.02643, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Chen",
                "S. Saxena",
                "L. Li",
                "D.J. Fleet",
                "G. Hinton"
            ],
            "title": "Pix2seq: A language modeling framework for object detection",
            "venue": "International Conference on Learning Representations, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Wang",
                "A. Yang",
                "R. Men",
                "J. Lin",
                "S. Bai",
                "Z. Li",
                "J. Ma",
                "C. Zhou",
                "J. Zhou",
                "H. Yang"
            ],
            "title": "Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework",
            "venue": "International Conference on Machine Learning, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "W. Wang",
                "Y. Cao",
                "C. Shen",
                "T. Huang"
            ],
            "title": "Images speak in images: A generalist painter for in-context visual learning",
            "venue": "Computer Vision and Pattern Recognition, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "D. Sur\u0131\u0301s",
                "S. Menon",
                "C. Vondrick"
            ],
            "title": "Vipergpt: Visual inference via python execution for reasoning",
            "venue": "arXiv preprint arXiv:2303.08128, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Shen",
                "K. Song",
                "X. Tan",
                "D. Li",
                "W. Lu",
                "Y. Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "H. Liu",
                "C. Li",
                "Q. Wu",
                "Y.J. Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Y. LeCun",
                "Y. Bengio",
                "G. Hinton"
            ],
            "title": "Deep learning",
            "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Savva",
                "A. Kadian",
                "O. Maksymets",
                "Y. Zhao",
                "E. Wijmans",
                "B. Jain",
                "J. Straub",
                "J. Liu",
                "V. Koltun",
                "J. Malik"
            ],
            "title": "Habitat: A platform for embodied ai research",
            "venue": "Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Deitke",
                "E. VanderBilt",
                "A. Herrasti",
                "L. Weihs",
                "J. Salvador",
                "K. Ehsani",
                "W. Han",
                "E. Kolve",
                "A. Farhadi",
                "A. Kembhavi"
            ],
            "title": "Procthor: Large-scale embodied ai using procedural generation",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International Conference on Machine Learning, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Bao",
                "L. Dong",
                "S. Piao",
                "F. Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "International Conference on Learning Representations, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A.A. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller",
                "A.K. Fidjeland",
                "G. Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "O. Vinyals",
                "I. Babuschkin",
                "W.M. Czarnecki",
                "M. Mathieu",
                "A. Dudzik",
                "J. Chung",
                "D.H. Choi",
                "R. Powell",
                "T. Ewalds",
                "P. Georgiev"
            ],
            "title": "Grandmaster level in starcraft ii using multiagent reinforcement learning",
            "venue": "Nature, vol. 575, no. 7782, pp. 350\u2013 354, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhu",
                "R. Mottaghi",
                "E. Kolve",
                "J.J. Lim",
                "A. Gupta",
                "L. Fei-Fei",
                "A. Farhadi"
            ],
            "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning",
            "venue": "International Conference on Robotics and Automation, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Das",
                "S. Datta",
                "G. Gkioxari",
                "S. Lee",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Embodied question answering",
            "venue": "Computer Vision and Pattern Recognition, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. Driess",
                "F. Xia",
                "M.S. Sajjadi",
                "C. Lynch",
                "A. Chowdhery",
                "B. Ichter",
                "A. Wahid",
                "J. Tompson",
                "Q. Vuong",
                "T. Yu"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "arXiv preprint arXiv:2303.03378, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "K. Kotar",
                "A. Walsman",
                "R. Mottaghi"
            ],
            "title": "Entl: Embodied navigation trajectory learner",
            "venue": "arXiv preprint arXiv:2304.02639, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. McCarthy"
            ],
            "title": "What is artificial intelligence",
            "venue": "2007.",
            "year": 2007
        },
        {
            "authors": [
                "S. Legg",
                "M. Hutter"
            ],
            "title": "A collection of definitions of intelligence",
            "venue": "Frontiers in Artificial Intelligence and applications, vol. 157, p. 17, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "S. Legg",
                "M. Hutter"
            ],
            "title": "Universal intelligence: A definition of machine intelligence",
            "venue": "Minds and Machines, vol. 17, pp. 391\u2013444, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "S. Legg",
                "M. Hutter"
            ],
            "title": "A formal measure of machine intelligence",
            "venue": "Annual Machine Learning Conference of Belgium and The Netherlands, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "B. Goertzel"
            ],
            "title": "The hidden pattern: A patternist philosophy of mind",
            "venue": "2006.",
            "year": 2006
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Artificial general intelligence: concept, state of the art, and future prospects",
            "venue": "Journal of Artificial General Intelligence, vol. 5, no. 1, p. 1, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "A. Turing"
            ],
            "title": "Computing Machinery and Intelligence",
            "year": 1950
        },
        {
            "authors": [
                "J.R. Searle"
            ],
            "title": "Minds, brains, and programs",
            "venue": "Behavioral and Brain Sciences, vol. 3, no. 3, pp. 417\u2013424, 1980.",
            "year": 1980
        },
        {
            "authors": [
                "R. Thoppilan",
                "D. De Freitas",
                "J. Hall",
                "N. Shazeer",
                "A. Kulshreshtha",
                "H.-T. Cheng",
                "A. Jin",
                "T. Bos",
                "L. Baker",
                "Y. Du"
            ],
            "title": "Lamda: Language models for dialog applications",
            "venue": "arXiv preprint arXiv:2201.08239, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Ji",
                "N. Lee",
                "R. Frieske",
                "T. Yu",
                "D. Su",
                "Y. Xu",
                "E. Ishii",
                "Y.J. Bang",
                "A. Madotto",
                "P. Fung"
            ],
            "title": "Survey of hallucination in natural language generation",
            "venue": "ACM Computing Surveys, vol. 55, no. 12, pp. 1\u201338, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "D. Silver",
                "S. Singh",
                "D. Precup",
                "R.S. Sutton"
            ],
            "title": "Reward is enough",
            "venue": "Artificial Intelligence, vol. 299, p. 103535, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Reed",
                "K. Zolna",
                "E. Parisotto",
                "S.G. Colmenarejo",
                "A. Novikov",
                "G. Barth-Maron",
                "M. Gimenez",
                "Y. Sulsky",
                "J. Kay",
                "J.T. Springenberg"
            ],
            "title": "A generalist agent",
            "venue": "Transactions on Machine Learning Research, 2022. MANUSCRIPT DRAFT: JUN 6TH, 2023 15",
            "year": 2022
        },
        {
            "authors": [
                "D. Zhu",
                "J. Chen",
                "X. Shen",
                "X. Li",
                "M. Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "venue": "arXiv preprint arXiv:2304.10592, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Radford",
                "K. Narasimhan",
                "T. Salimans",
                "I. Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "venue": "Advances in Neural Information Processing Systems, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
            "venue": "North American Chapter of the Association for Computational Linguistics, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P.F. Christiano",
                "J. Leike",
                "T. Brown",
                "M. Martic",
                "S. Legg",
                "D. Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in Neural Information Processing Systems, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "I. Kokkinos"
            ],
            "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
            "venue": "Computer Vision and Pattern Recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional networks",
            "venue": "Advances in Neural Information Processing Systems, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "K. He",
                "X. Chen",
                "S. Xie",
                "Y. Li",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "Computer Vision and Pattern Recognition, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "C. Sun",
                "A. Shrivastava",
                "S. Singh",
                "A. Gupta"
            ],
            "title": "Revisiting unreasonable effectiveness of data in deep learning era",
            "venue": "International Conference on Computer Vision, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Yu",
                "Z. Wang",
                "V. Vasudevan",
                "L. Yeung",
                "M. Seyedhosseini",
                "Y. Wu"
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "venue": "Transactions on Machine Learning Research, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "R. Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "International Conference on Computer Vision, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in Neural Information Processing Systems, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "International Conference on Computer Vision, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Redmon",
                "S. Divvala",
                "R. Girshick",
                "A. Farhadi"
            ],
            "title": "You only look once: Unified, real-time object detection",
            "venue": "Computer Vision and Pattern Recognition, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "W. Liu",
                "D. Anguelov",
                "D. Erhan",
                "C. Szegedy",
                "S. Reed",
                "C.-Y. Fu",
                "A.C. Berg"
            ],
            "title": "Ssd: Single shot multibox detector",
            "venue": "European Conference on Computer Vision, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "N. Carion",
                "F. Massa",
                "G. Synnaeve",
                "N. Usunier",
                "A. Kirillov",
                "S. Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "European Conference on Computer Vision, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.-Y. Lin",
                "M. Maire",
                "S. Belongie",
                "J. Hays",
                "P. Perona",
                "D. Ramanan",
                "P. Doll\u00e1r",
                "C.L. Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "European Conference on Computer Vision, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J. Long",
                "E. Shelhamer",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "Computer Vision and Pattern Recognition, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L.-C. Chen",
                "G. Papandreou",
                "I. Kokkinos",
                "K. Murphy",
                "A.L. Yuille"
            ],
            "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 4, pp. 834\u2013848, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "F. Milletari",
                "N. Navab",
                "S.-A. Ahmadi"
            ],
            "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation",
            "venue": "International Conference on 3D Vision (3DV), 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Wang",
                "K. Sun",
                "T. Cheng",
                "B. Jiang",
                "C. Deng",
                "Y. Zhao",
                "D. Liu",
                "Y. Mu",
                "M. Tan",
                "X. Wang"
            ],
            "title": "Deep high-resolution representation learning for visual recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 3349\u20133364, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Cheng",
                "A. Schwing",
                "A. Kirillov"
            ],
            "title": "Per-pixel classification is not all you need for semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "E. Xie",
                "W. Wang",
                "Z. Yu",
                "A. Anandkumar",
                "J.M. Alvarez",
                "P. Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "Advances in Neural Information Processing Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Zhou",
                "H. Zhao",
                "X. Puig",
                "S. Fidler",
                "A. Barriuso",
                "A. Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "Computer Vision and Pattern Recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Krishna",
                "Y. Zhu",
                "O. Groth",
                "J. Johnson",
                "K. Hata",
                "J. Kravitz",
                "S. Chen",
                "Y. Kalantidis",
                "L.-J. Li",
                "D.A. Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International Journal of Computer Vision, vol. 123, pp. 32\u201373, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "O. Vinyals",
                "A. Toshev",
                "S. Bengio",
                "D. Erhan"
            ],
            "title": "Show and tell: A neural image caption generator",
            "venue": "Computer Vision and Pattern Recognition, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "K. Xu",
                "J. Ba",
                "R. Kiros",
                "K. Cho",
                "A. Courville",
                "R. Salakhudinov",
                "R. Zemel",
                "Y. Bengio"
            ],
            "title": "Show, attend and tell: Neural image caption generation with visual attention",
            "venue": "International Conference on Machine Learning, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "L. Zhou",
                "H. Palangi",
                "L. Zhang",
                "H. Hu",
                "J. Corso",
                "J. Gao"
            ],
            "title": "Unified vision-language pre-training for image captioning and vqa",
            "venue": "AAAI Conference on Artificial Intelligence, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J.-B. Alayrac",
                "J. Donahue",
                "P. Luc",
                "A. Miech",
                "I. Barr",
                "Y. Hasson",
                "K. Lenc",
                "A. Mensch",
                "K. Millican",
                "M. Reynolds"
            ],
            "title": "Flamingo: a visual language model for few-shot learning",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Li",
                "D. Li",
                "C. Xiong",
                "S. Hoi"
            ],
            "title": "Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Ramesh",
                "P. Dhariwal",
                "A. Nichol",
                "C. Chu",
                "M. Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Razavi",
                "A. Van den Oord",
                "O. Vinyals"
            ],
            "title": "Generating diverse high-fidelity images with vq-vae-2",
            "venue": "Advances in Neural Information Processing Systems, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Ramesh",
                "M. Pavlov",
                "G. Goh",
                "S. Gray",
                "C. Voss",
                "A. Radford",
                "M. Chen",
                "I. Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "International Conference on Machine Learning, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Saharia",
                "W. Chan",
                "S. Saxena",
                "L. Li",
                "J. Whang",
                "E.L. Denton",
                "K. Ghasemipour",
                "R. Gontijo Lopes",
                "B. Karagol Ayan",
                "T. Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Yan",
                "H. Peng",
                "J. Fu",
                "D. Wang",
                "H. Lu"
            ],
            "title": "Learning spatiotemporal transformer for visual tracking",
            "venue": "International Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Meinhardt",
                "A. Kirillov",
                "L. Leal-Taixe",
                "C. Feichtenhofer"
            ],
            "title": "Trackformer: Multi-object tracking with transformers",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "P. Sun",
                "Y. Jiang",
                "D. Yu",
                "F. Weng",
                "Z. Yuan",
                "P. Luo",
                "W. Liu",
                "X. Wang"
            ],
            "title": "Bytetrack: Multi-object tracking by associating every detection box",
            "venue": "European Conference on Computer Vision, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xu",
                "J. Zhang",
                "Q. Zhang",
                "D. Tao"
            ],
            "title": "Vitpose: Simple vision transformer baselines for human pose estimation",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Gu",
                "T.-Y. Lin",
                "W. Kuo",
                "Y. Cui"
            ],
            "title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "venue": "International Conference on Learning Representations, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "B. Li",
                "K.Q. Weinberger",
                "S. Belongie",
                "V. Koltun",
                "R. Ranftl"
            ],
            "title": "Language-driven semantic segmentation",
            "venue": "International Conference on Learning Representations, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "P. Zhang",
                "X. Hu",
                "Y.-C. Chen",
                "L. Li",
                "X. Dai",
                "L. Wang",
                "L. Yuan",
                "J.-N. Hwang",
                "J. Gao"
            ],
            "title": "Glipv2: Unifying localization and vision-language understanding",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Tang",
                "L. Xie",
                "X. Zhang",
                "X. Hu",
                "Q. Tian"
            ],
            "title": "Visual recognition by request",
            "venue": "Computer Vision and Pattern Recognition, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Fritz"
            ],
            "title": "A multi-world approach to question answering about real-world scenes based on uncertain input",
            "venue": "Advances in neural information processing systems, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Malinowski",
                "M. Rohrbach",
                "M. Fritz"
            ],
            "title": "Ask your neurons: A neural-based approach to answering questions about images",
            "venue": "International Conference on Computer Vision, 2015. MANUSCRIPT DRAFT: JUN 6TH, 2023 16",
            "year": 2015
        },
        {
            "authors": [
                "J. Andreas",
                "M. Rohrbach",
                "T. Darrell",
                "D. Klein"
            ],
            "title": "Neural module networks",
            "venue": "Computer Vision and Pattern Recognition, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "C. Jia",
                "Y. Yang",
                "Y. Xia",
                "Y.-T. Chen",
                "Z. Parekh",
                "H. Pham",
                "Q. Le",
                "Y.-H. Sung",
                "Z. Li",
                "T. Duerig"
            ],
            "title": "Scaling up visual and visionlanguage representation learning with noisy text supervision",
            "venue": "International Conference on Machine Learning, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Huang",
                "L. Dong",
                "W. Wang",
                "Y. Hao",
                "S. Singhal",
                "S. Ma",
                "T. Lv",
                "L. Cui",
                "O.K. Mohammed",
                "Q. Liu"
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "arXiv preprint arXiv:2302.14045, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "K. Zhou",
                "J. Yang",
                "C.C. Loy",
                "Z. Liu"
            ],
            "title": "Learning to prompt for vision-language models",
            "venue": "International Journal of Computer Vision, vol. 130, no. 9, pp. 2337\u20132348, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wang",
                "Z. Zhang",
                "C.-Y. Lee",
                "H. Zhang",
                "R. Sun",
                "X. Ren",
                "G. Su",
                "V. Perot",
                "J. Dy",
                "T. Pfister"
            ],
            "title": "Learning to prompt for continual learning",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Du",
                "F. Wei",
                "Z. Zhang",
                "M. Shi",
                "Y. Gao",
                "G. Li"
            ],
            "title": "Learning to prompt for open-vocabulary object detection with visionlanguage model",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Xu",
                "Z. Zhang",
                "F. Wei",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "X. Bai"
            ],
            "title": "A simple baseline for open-vocabulary semantic segmentation with pre-trained vision-language model",
            "venue": "European Conference on Computer Vision, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Huynh",
                "J. Kuen",
                "Z. Lin",
                "J. Gu",
                "E. Elhamifar"
            ],
            "title": "Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Ding",
                "J. Wang",
                "Z. Tu"
            ],
            "title": "Open-vocabulary panoptic segmentation with maskclip",
            "venue": "arXiv preprint arXiv:2208.08984, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Jain",
                "J. Li",
                "M.T. Chiu",
                "A. Hassani",
                "N. Orlov",
                "H. Shi"
            ],
            "title": "Oneformer: One transformer to rule universal image segmentation",
            "venue": "Computer Vision and Pattern Recognition, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "L.H. Li",
                "P. Zhang",
                "H. Zhang",
                "J. Yang",
                "C. Li",
                "Y. Zhong",
                "L. Wang",
                "L. Yuan",
                "L. Zhang",
                "J.-N. Hwang"
            ],
            "title": "Grounded language-image pre-training",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhong",
                "J. Yang",
                "P. Zhang",
                "C. Li",
                "N. Codella",
                "L.H. Li",
                "L. Zhou",
                "X. Dai",
                "L. Yuan",
                "Y. Li"
            ],
            "title": "Regionclip: Region-based languageimage pretraining",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Cherti",
                "R. Beaumont",
                "R. Wightman",
                "M. Wortsman",
                "G. Ilharco",
                "C. Gordon",
                "C. Schuhmann",
                "L. Schmidt",
                "J. Jitsev"
            ],
            "title": "Reproducible scaling laws for contrastive language-image learning",
            "venue": "Computer Vision and Pattern Recognition, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Q. Sun",
                "Y. Fang",
                "L. Wu",
                "X. Wang",
                "Y. Cao"
            ],
            "title": "Eva-clip: Improved training techniques for clip at scale",
            "venue": "arXiv preprint arXiv:2303.15389, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Chen",
                "Z. Yang",
                "L. Zhang"
            ],
            "title": "Semantic segment anything",
            "venue": "https://github.com/fudan-zvg/Semantic-Segment-Anything, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Cen",
                "Z. Zhou",
                "J. Fang",
                "W. Shen",
                "L. Xie",
                "X. Zhang",
                "Q. Tian"
            ],
            "title": "Segment anything in 3d with nerfs",
            "venue": "arXiv preprint arXiv:2304.12308, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Yu",
                "R. Feng",
                "R. Feng",
                "J. Liu",
                "X. Jin",
                "W. Zeng",
                "Z. Chen"
            ],
            "title": "Inpaint anything: Segment anything meets image inpainting",
            "venue": "arXiv preprint arXiv:2304.06790, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Chen",
                "L. Zhu",
                "C. Ding",
                "R. Cao",
                "S. Zhang",
                "Y. Wang",
                "Z. Li",
                "L. Sun",
                "P. Mao",
                "Y. Zang"
            ],
            "title": "Sam fails to segment anything?\u2013samadapter: Adapting sam in underperformed scenes: Camouflage, shadow, and more",
            "venue": "arXiv preprint arXiv:2304.09148, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Ma",
                "B. Wang"
            ],
            "title": "Segment anything in medical images",
            "venue": "arXiv preprint arXiv:2304.12306, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "R. Deng",
                "C. Cui",
                "Q. Liu",
                "T. Yao",
                "L.W. Remedios",
                "S. Bao",
                "B.A. Landman",
                "L.E. Wheless",
                "L.A. Coburn",
                "K.T. Wilson"
            ],
            "title": "Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging",
            "venue": "arXiv preprint arXiv:2304.04155, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. He",
                "R. Bao",
                "J. Li",
                "P.E. Grant",
                "Y. Ou"
            ],
            "title": "Accuracy of segmentanything model (sam) in medical image segmentation tasks",
            "venue": "arXiv preprint arXiv:2304.09324, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "L. Tang",
                "H. Xiao",
                "B. Li"
            ],
            "title": "Can sam segment anything? when sam meets camouflaged object detection",
            "venue": "arXiv preprint arXiv:2304.04709, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Yang",
                "M. Gao",
                "Z. Li",
                "S. Gao",
                "F. Wang",
                "F. Zheng"
            ],
            "title": "Track anything: Segment anything meets videos",
            "venue": "arXiv preprint arXiv:2304.11968, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Liu",
                "Z. Zeng",
                "T. Ren",
                "F. Li",
                "H. Zhang",
                "J. Yang",
                "C. Li",
                "J. Yang",
                "H. Su",
                "J. Zhu"
            ],
            "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection",
            "venue": "arXiv preprint arXiv:2303.05499, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "X. Zou",
                "J. Yang",
                "H. Zhang",
                "F. Li",
                "L. Li",
                "J. Gao",
                "Y.J. Lee"
            ],
            "title": "Segment everything everywhere all at once",
            "venue": "arXiv preprint arXiv:2304.06718, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Chen",
                "S. Saxena",
                "L. Li",
                "T.-Y. Lin",
                "D.J. Fleet",
                "G.E. Hinton"
            ],
            "title": "A unified sequence interface for vision tasks",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "X. Zhang",
                "Y. Cao",
                "W. Wang",
                "C. Shen",
                "T. Huang"
            ],
            "title": "Seggpt: Segmenting everything in context",
            "venue": "arXiv preprint arXiv:2304.03284, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Kolesnikov",
                "A. Susano Pinto",
                "L. Beyer",
                "X. Zhai",
                "J. Harmsen",
                "N. Houlsby"
            ],
            "title": "Uvim: A unified modeling approach for vision with learned guiding codes",
            "venue": "Advances in Neural Information Processing Systems, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Yang",
                "A. Nagrani",
                "P.H. Seo",
                "A. Miech",
                "J. Pont-Tuset",
                "I. Laptev",
                "J. Sivic",
                "C. Schmid"
            ],
            "title": "Vid2seq: Large-scale pretraining of a visual language model for dense video captioning",
            "venue": "Computer Vision and Pattern Recognition, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Lu",
                "C. Clark",
                "R. Zellers",
                "R. Mottaghi",
                "A. Kembhavi"
            ],
            "title": "Unified-io: A unified model for vision, language, and multimodal tasks",
            "venue": "arXiv preprint arXiv:2206.08916, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhu",
                "J. Zhu",
                "H. Li",
                "X. Wu",
                "H. Li",
                "X. Wang",
                "J. Dai"
            ],
            "title": "Uniperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Lu",
                "B. Peng",
                "H. Cheng",
                "M. Galley",
                "K.-W. Chang",
                "Y.N. Wu",
                "S.-C. Zhu",
                "J. Gao"
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "venue": "arXiv preprint arXiv:2304.09842, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Yang",
                "L. Li",
                "J. Wang",
                "K. Lin",
                "E. Azarnasab",
                "F. Ahmed",
                "Z. Liu",
                "C. Liu",
                "M. Zeng",
                "L. Wang"
            ],
            "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
            "venue": "arXiv preprint arXiv:2303.11381, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Johnson",
                "B. Hariharan",
                "L. Van Der Maaten",
                "J. Hoffman",
                "L. Fei- Fei",
                "C. Lawrence Zitnick",
                "R. Girshick"
            ],
            "title": "Inferring and executing programs for visual reasoning",
            "venue": "International Conference on Computer Vision, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "R. Zellers",
                "Y. Bisk",
                "A. Farhadi",
                "Y. Choi"
            ],
            "title": "From recognition to cognition: Visual commonsense reasoning",
            "venue": "Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Antol",
                "A. Agrawal",
                "J. Lu",
                "M. Mitchell",
                "D. Batra",
                "C.L. Zitnick",
                "D. Parikh"
            ],
            "title": "Vqa: Visual question answering",
            "venue": "International Conference on Computer Vision, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Goyal",
                "T. Khot",
                "D. Summers-Stay",
                "D. Batra",
                "D. Parikh"
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "Computer Vision and Pattern Recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Johnson",
                "B. Hariharan",
                "L. Van Der Maaten",
                "L. Fei-Fei",
                "C. Lawrence Zitnick",
                "R. Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "Computer Vision and Pattern Recognition, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Wu",
                "S. Yin",
                "W. Qi",
                "X. Wang",
                "Z. Tang",
                "N. Duan"
            ],
            "title": "Visual chatgpt: Talking, drawing and editing with visual foundation models",
            "venue": "arXiv preprint arXiv:2303.04671, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "D. Marr"
            ],
            "title": "Vision: A computational investigation into the human representation and processing of visual information",
            "year": 1982
        },
        {
            "authors": [
                "H. Moravec"
            ],
            "title": "Mind children: The future of robot and human intelligence",
            "year": 1988
        },
        {
            "authors": [
                "R.A. Brooks"
            ],
            "title": "Elephants don\u2019t play chess",
            "venue": "Robotics and Autonomous Systems, vol. 6, no. 1-2, pp. 3\u201315, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M. Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International Journal of Computer Vision, vol. 115, pp. 211\u2013252, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Yuille",
                "D. Kersten"
            ],
            "title": "Vision as bayesian inference: analysis by synthesis?",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2006
        },
        {
            "authors": [
                "E. Kolve",
                "R. Mottaghi",
                "W. Han",
                "E. VanderBilt",
                "L. Weihs",
                "A. Herrasti",
                "M. Deitke",
                "K. Ehsani",
                "D. Gordon",
                "Y. Zhu"
            ],
            "title": "Ai2thor: An interactive 3d environment for visual ai",
            "venue": "arXiv preprint arXiv:1712.05474, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Wei",
                "Y. Tay",
                "R. Bommasani",
                "C. Raffel",
                "B. Zoph",
                "S. Borgeaud",
                "D. Yogatama",
                "M. Bosma",
                "D. Zhou",
                "D. Metzler"
            ],
            "title": "Emergent abilities of large language models",
            "venue": "arXiv preprint arXiv:2206.07682, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Wu",
                "Y. Xiong",
                "S.X. Yu",
                "D. Lin"
            ],
            "title": "Unsupervised feature learning via non-parametric instance discrimination",
            "venue": "Computer Vision and Pattern Recognition, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. v. d. Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748, 2018.",
            "year": 1807
        },
        {
            "authors": [
                "K. He",
                "H. Fan",
                "Y. Wu",
                "S. Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Computer Vision and Pattern Recognition, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Xie",
                "Z. Zhang",
                "Y. Cao",
                "Y. Lin",
                "J. Bao",
                "Z. Yao",
                "Q. Dai",
                "H. Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "venue": "Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J. Schulman",
                "F. Wolski",
                "P. Dhariwal",
                "A. Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J. Schulman",
                "S. Levine",
                "P. Abbeel",
                "M. Jordan",
                "P. Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "International Conference on Machine Learning, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D. Silver",
                "A. Huang",
                "C.J. Maddison",
                "A. Guez",
                "L. Sifre",
                "G. Van Den Driessche",
                "J. Schrittwieser",
                "I. Antonoglou",
                "V. Panneershelvam",
                "M. Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree search",
            "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013489, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "D. Silver",
                "J. Schrittwieser",
                "K. Simonyan",
                "I. Antonoglou",
                "A. Huang",
                "A. Guez",
                "T. Hubert",
                "L. Baker",
                "M. Lai",
                "A. Bolton"
            ],
            "title": "Mastering the game of go without human knowledge",
            "venue": "Nature, vol. 550, no. 7676, pp. 354\u2013359, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Hessel",
                "J. Modayil",
                "H. Van Hasselt",
                "T. Schaul",
                "G. Ostrovski",
                "W. Dabney",
                "D. Horgan",
                "B. Piot",
                "M. Azar",
                "D. Silver"
            ],
            "title": "Rainbow: Combining improvements in deep reinforcement learning",
            "venue": "AAAI Conference on Artificial Antelligence, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Ecoffet",
                "J. Huizinga",
                "J. Lehman",
                "K.O. Stanley",
                "J. Clune"
            ],
            "title": "First return, then explore",
            "venue": "Nature, vol. 590, no. 7847, pp. 580\u2013 586, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Smith",
                "M. Gasser"
            ],
            "title": "The development of embodied cognition: Six lessons from babies",
            "venue": "Artificial Life, vol. 11, no. 1-2, pp. 13\u201329, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "D. Pathak",
                "P. Agrawal",
                "A.A. Efros",
                "T. Darrell"
            ],
            "title": "Curiositydriven exploration by self-supervised prediction",
            "venue": "International Conference on Machine Learning, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "T. Chen",
                "S. Gupta",
                "A. Gupta"
            ],
            "title": "Learning exploration policies for navigation",
            "venue": "arXiv preprint arXiv:1903.01959, 2019.",
            "year": 1903
        },
        {
            "authors": [
                "D.S. Chaplot",
                "H. Jiang",
                "S. Gupta",
                "A. Gupta"
            ],
            "title": "Semantic curiosity for active visual learning",
            "venue": "European Conference on Computer Vision, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Anderson",
                "A. Chang",
                "D.S. Chaplot",
                "A. Dosovitskiy",
                "S. Gupta",
                "V. Koltun",
                "J. Kosecka",
                "J. Malik",
                "R. Mottaghi",
                "M. Savva"
            ],
            "title": "On evaluation of embodied navigation agents",
            "venue": "arXiv preprint arXiv:1807.06757, 2018.",
            "year": 1807
        },
        {
            "authors": [
                "D.S. Chaplot",
                "R. Salakhutdinov",
                "A. Gupta",
                "S. Gupta"
            ],
            "title": "Neural topological slam for visual navigation",
            "venue": "Computer Vision and Pattern Recognition, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D. Gordon",
                "A. Kembhavi",
                "M. Rastegari",
                "J. Redmon",
                "D. Fox",
                "A. Farhadi"
            ],
            "title": "Iqa: Visual question answering in interactive environments",
            "venue": "Computer Vision and Pattern Recognition, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM, vol. 65, no. 1, pp. 99\u2013106, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Yu",
                "V. Ye",
                "M. Tancik",
                "A. Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "Computer Vision and Pattern Recognition, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Zhang",
                "G. Riegler",
                "N. Snavely",
                "V. Koltun"
            ],
            "title": "Nerf++: Analyzing and improving neural radiance fields",
            "venue": "arXiv preprint arXiv:2010.07492, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "A. Petrenko",
                "E. Wijmans",
                "B. Shacklett",
                "V. Koltun"
            ],
            "title": "Megaverse: Simulating embodied agents at one million experiences per second",
            "venue": "International Conference on Machine Learning, 2021.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Computer Vision, Artificial General Intelligence, Foundation Models, Unification, Environments.\n\u2726"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "THE world is witnessing an epic odyssey towards arti-ficial general intelligence (AGI), where we follow the convention to define AGI as a computer algorithm that can replicate any intellectual task that human beings or other animals can1. Specifically, in natural language processing (NLP), computer algorithms have been developed to an extent that can solve a wide range of tasks via chat with humans [1]. Some researchers believed that such systems can be seen as early sparks of AGI [2]. These systems were mostly built upon large language models (LLMs) [3] and enhanced by instruct tuning [4]. Equipped with an external knowledge base and specifically designed modules, they can accomplish complex tasks such as solving mathematical questions, generating visual contents, etc., reflecting its strong ability to understand users\u2019 intentions and perform preliminary chain-of-thoughts [5]. Despite known weaknesses in some aspects (e.g., telling scientific facts and relationships between named people), these pioneering studies have shown a clear trend to unify most tasks in NLP into one system, which reflects the pursuit of AGI.\nCompared to the rapid progress of unification in NLP, the computer vision (CV) community is yet far from the target of unifying all tasks. The regular CV tasks, such as visual recognition, tracking, captioning, generation, etc., are mostly processed using largely different network architectures and/or specifically designed pipelines. Researchers look forward to a system like GPT that can deal with a wide\n\u2022 All authors, unless specified below, are with Huawei Inc., China. E-mail of the leading author (Lingxi Xie): 198808xc@gmail.com \u2022 Corresponding author: Qi Tian. E-mail: tian.qi1@huawei.com Manuscript received Month Date, 2023.\n1. https://en.wikipedia.org/wiki/Artificial general intelligence\nrange of CV tasks with a unified prompt mechanism, but there exists a tradeoff between achieving good practice in individual tasks and being generalized across a wide range of tasks. For example, to report high recognition accuracy in object detection and semantic segmentation, the best strategy is to design specific head modules [6], [7] upon strong backbones [8], [9], [10] for image classification, and such designs do not generally transfer to other problems such as image captioning [11] or visual content generation [12].\nClearly, unification is the trend in CV. In recent years, there are many efforts in this direction, and we roughly categorize them into five research topics, namely, (i) open-world visual recognition based on vision-language alignment [13], (ii) the Segment Anything task [14] for generic visual recognition, (iii) generalized visual encoding to unify vision tasks [15], [16], [17], (iv) LLM-guided visual understanding to enhance the logic in CV [18], [19], and (v) multimodal dialog to facilitate vision-language interaction [11], [20]. These works showed promise of unification, but yet, they cannot composite a system like GPT that can solve generic CV tasks in the real world.\nHence, two questions arise: (1) Why is unification so difficult in CV? (2) What can we learn from GPT and LLMs to achieve this goal? To answer them, we revisit GPT and understand it as establishing an environment in the text world and allowing an algorithm (or agent) to learn from interaction. The CV research lacks such an environment. Consequently, the algorithms cannot simulate the world, so they instead sample the world and learn to achieve good performance in the so-called proxy tasks. After an epic decade of deep learning [21], the proxy tasks are no longer meaningful to indicate the ability of CV algorithms; it becomes more and more apparent that continuing to pursue\nar X\niv :2\n30 6.\n08 64\n1v 1\n[ cs\n.C V\n] 1\n4 Ju\nn 20\n23\nhigh accuracy on them can drive us away from AGI. Based on the analysis above, we propose an imaginary pipeline towards AGI in CV. It involves three stages. The first stage is to establish a set of environments that are fidelitous, abundant, and interactable. The second stage aims to train an agent by forcing it to explore the environment(s) and predict future frames: this corresponds to the autoregressive pre-training stage in NLP [3]. The third stage involves teaching the agent to accomplish various tasks: it is likely that human instructions shall be introduced in this stage, corresponding to the instruct fine-tuning stage in NLP [4]. Optionally, the agent can be tuned to perform proxy tasks via simple and unified prompts. The idea is related to a few existing research topics, including 3D environment establishment [22], [23], visual pre-training [24], [25], reinforcement learning [26], [27], and embodied CV [28], [29]. But, existing works are mostly preliminary and we expect that substantial efforts [30], [31] are required to make it an effective paradigm to solve real-world problems.\nThe remainder of this paper is organized as follows. First, in Section 2, we briefly introduce the history and thoughts of AGI and inherit the definition that AGI is an algorithm to maximize the reward. It is followed by Section 3 where we show the ability of GPT, the state-ofthe-art NLP algorithm which was considered the spark of AGI. Then, in Section 4, based on the current status of CV research, we analyze why AGI is difficult in computer vision and point out that the essential difficulty lies in the outdated learning paradigm. The analysis leads to Section 5, where we imagine a pipeline that pushes CV closer to AGI, based on which we make some comments on future research directions. Finally, in Section 6, we conclude this paper and share our thoughts."
        },
        {
            "heading": "2 ARTIFICIAL GENERAL INTELLIGENCE",
            "text": "Artificial intelligence (AI) is a long-lasting battle to replicate human intelligence with a machine or a set of mathematical algorithms. Modern AI was formally proposed in the Dartmouth workshop, 1956, and the community has developed a large number of methodologies for this purpose. There are at least two different pathways to achieve AI: (i) the symbolic AI which tries to formulate the world into a symbolic system and uses logic algorithms to reason about it; (ii) the statistical AI which tries to establish a mathematical function to formulate the relationship between input and output, yet the function can be approximated or even nonexplainable. The past decade was dominated by the second path, in particular, the deep learning theory [21] which is part of the idea of the connectionist approach.\nAlthough artificial general intelligence (AGI) is the ultimate goal of AI. The added word, \u2018general\u2019, implies that the key of AGI is to improve the generalization ability of AI algorithms. Conceptually, AGI can be defined as a system that solves any task that human beings or animals can perform [32]2. In the modern era, there are a series of thoughts about AGI, resulting in verbal, psychological, and\n2. Throughout this paper, we limit the concept of AGI within the scope of problem-solving, and thus we will not talk about programs that exhibit sentience or consciousness.\nof course AI-based definitions of AGI, many of which were summarized in an early paper [33], including:\n\u2022 In [32], [34], the authors assumed that an AGI algorithm can do any task that humans or intelligent animals can do. This description is direct and anthropocentric, but it ignores the possibility that AGI can surpass real-world creatures, possibly by consuming more energy. \u2022 In [35], [36], the authors asked that AGI algorithms can apply to as many tasks and scenarios as possible. However, without any constraints, the definition seems difficult to distinguish an AGI algorithm from a set of individual algorithms designed for specific purposes. \u2022 In [37], the authors described typical characteristics of AGI algorithms, including being symbolic, emergentist, hybrid, and universalist.\nDespite the vast argument in the description of AGI, one conclusion is clear: human intelligence is multi-faceted and thus it is difficult to use one definition to cover all properties of AGI.\nIn the AI field, probably one of the most famous thought experiments is the Turing test [38] which claimed that a machine is considered to gain intelligence if a human evaluator cannot tell the machine from the human in textonly communications. After being pursued by researchers for decades, the Turing test has become part of AI culture, although there exist challenges to it, e.g., the Chinese room argument [39] which argued that AI algorithms might pass the Turing test without understanding what they are doing.\nAs far as we know, no AI algorithms have seriously passed the Turing test, because all of them exhibit clear patterns which make them easy to be discriminated from humans. This also includes the recently developed AI chatbots like LaMDA [40] and the GPT series [1]: they have shown strong abilities in chat and/or problem-solving, and some sources even advocated for them to pass the Turing test, but, for professional evaluators who are familiar with AI, they are still quite easy to be identified, not to mention that these chatbots are known to \u2018hallucination\u2019 [41] and humans often do not. This is an interesting signal that useful AGI systems may not necessarily mimic human behaviors.\nGoing beyond text-only systems, there are many more data modalities (e.g., speech, image, video, etc.) to be processed. To integrate them into one system, we follow [42], [43] to define the goal of AGI to be maximizing reward in an environment. Let there be an environment and an agent (the AGI algorithm) that can interact with it. The agent observes a sequence of states, S = {s1, . . . , sT }, and can choose from a set of actions, A = {a1, . . . ,aM}, to perform. There are two functions that define the transition between states and the obtained rewards, respectively. The goal of AGI is to learn a policy, denoted as \u03c0 : S 7\u2192 A, which maximizes the expected cumulative reward R = \u2211T t=1 r(st,at). When we set st and at to be different data modalities, it is the above formulation can cover a wide range of AI tasks. Specifically, the currently popular proxy tasks in computer vision such as image classification, object detection and segmentation, etc., are mostly weakened versions of the above formulation\nwhere the episode length T equals to 1, i.e., these tasks are not built upon interaction with some environments.\nIn brief, the AGI is to learn a generalized function a = \u03c0(s). Although the form is quite simple, it was very difficult for the old-fashioned AI algorithms to use the same methodology, algorithm, or even model to deal with them all. In the past decade, deep learning [21] offers an effective and unified methodology: one can train a deep neural network to approximate the function a = \u03c0(s) without knowing about the actual relationship between them. The emergence of powerful neural architectures such as the transformer [44] even enables the researcher to train one model for different data modalities [45].\nThere are enormous difficulties in achieving AGI, including but not limited to the following issues:\n\u2022 The complexity of data. Real-world data is multi-\nfaceted and rich. Some data modalities (e.g., images) can have quite a high dimensionality and the relationship between different modalities can be complex and latent. \u2022 The complexity of human intelligence. The goal of AGI is not only about problem-solving but also about planning, reasoning, reacting to different events, etc. Sometimes, the relationship between human behavior and the target is obscure and hard to represent in math forms. \u2022 Lack of neurological or cognitive theory. Humans do not yet understand how human intelligence is achieved. Currently, computer algorithms provide one pathway, yet more possibilities may arise with future research in neurology and/or cognition."
        },
        {
            "heading": "U His calm exterior _____ the militant fervor with which he threw himself into the campaign. delineated/reviled/falsified/ridiculed/belied",
            "text": ""
        },
        {
            "heading": "U Quantity A",
            "text": ""
        },
        {
            "heading": "3 GPT: SPARK OF AGI IN NLP",
            "text": "In the past year, ChatGPT3, GPT-4 [1], and other AI chatbots such as Vicuna4, made large progress towards AGI. They are computer algorithms developed for natural language processing (NLP). With a chat procedure with humans, they can understand the intention of humans and accomplish a wide range of tasks as long as they can be presented in pure texts. In particular, GPT-4 has a strong ability in generic problem-solving and was considered an early spark of AGI in the NLP field [2].\nWe briefly showcase the pure-text abilities of GPT-4. Throughout this part, we have used the May 12th version of GPT-4. The set of covered tasks includes the conventional NLP problems (e.g., translation, named entity recognition, question answering, etc., as shown in Figure 1) and other\n3. https://openai.com/blog/chatgpt 4. https://github.com/lm-sys/FastChat\ntext-based problems such as solving mathematical and logical problems (Figure 2), passing verbal exams (e.g., GRE, as shown in Figure 3), coding with debugging (Figure 4), and so on. Beyond these basic examples, GPT-4 also exhibits a strong ability in logic, which enables it to integrate clues collected from multiple rounds of dialog into the final answer (Figure 5). We refer the readers to a previous paper (i.e., the Sparks-of-AGI paper [2]) for a thorough analysis of the ability of GPT-4.\nAlthough GPT-4 has not yet opened the vision interface to the public, the official technical report [1] showed several fancy examples about multimodal dialog, i.e., chat based on an input image as reference. This implies that GPT-4 has been equipped with abilities of aligning language features with visual features, hence it can perform basic visual understanding tasks. As we shall see later (in Section 4.2.5), the vision community has developed several replacements [20], [46] for the same purpose, and the key lies in using ChatGPT"
        },
        {
            "heading": "U def replay(self, batch_size):",
            "text": ""
        },
        {
            "heading": "U Error message:",
            "text": "or GPT-4 to generate (instruct) training data. Additionally, with simple prompts, GPT-4 is also capable of calling external software (e.g., Midjourney, as shown in Figure 6) for image generation and external libraries (e.g., the HuggingFace libraries, as shown in [19]) for solving complex problems in computer vision.\nThese AI chatbots were trained in a two-stage procedure. In the first stage, a large language model (LLM), most of which are based on the transformer architecture [44], is pretrained on a large-scale text database with self-supervised learning [3], [47], [48]. In the second stage, the pre-trained\nLLM is supervised by human instructions [4] to accomplish specific tasks. If necessary, human feedback is collected and reinforcement learning is performed [49] to fine-tune the LLM towards better performance and higher data efficiency.\nLater in Section 4.3, we will revisit the above procedure and understand it as a natural choice for training an agent to interact with the text environment."
        },
        {
            "heading": "U Using the updated prompt on midjourney.",
            "text": ""
        },
        {
            "heading": "4 CV: THE NEXT BATTLEFIELD OF AGI",
            "text": "Humans perceive the world based on multiple data modalities. It is a common knowledge that about 85% of what we learn is through our vision system. Therefore, given that the NLP community has shown the promise of AGI, it is natural to consider computer vision (CV) or multimodality (which includes at least the vision and language domains) as the next battlefield of AGI.\nHere we provide two additional comments to complement the above statement. First, it is clear that CV is a superset of NLP, because humans read articles by first recognizing characters in the captured images and then understanding the contents. In other words, an AGI in CV (or multimodality) should cover all abilities of an AGI in NLP. Second, we argue that language alone is insufficient in many scenarios. For example, when one tries to find detailed information about an unknown object (e.g., animal, fashion, etc.), the best way is to capture an image and use it for online search; purely relying on text descriptions can introduce uncertainty and inaccuracy. As another case, as we shall see in Section 4.3, it is not always easy to refer to fine-grained semantics in a scene (for recognition or image editing), and it is more efficient to think in a vision-friendly manner, e.g., using a point or box to locate an object rather than saying something like \u2018the person who is wearing black jacket, standing in front of the yellow car, and talking to another person\u2019."
        },
        {
            "heading": "4.1 Ideal and Reality",
            "text": "We desire a CV algorithm that can solve generic tasks, possibly by interacting with the environment. Note that the requirement is not limited to recognizing everything or performing dialog based on an image or video clip. It shall be a holistic system that receives generic orders from humans and produces the desired results. But, the current status of CV is quite preliminary. As shown in Figure 7, the CV community has been using different modules and even systems for different vision tasks. Below, we list a few of them.\n\u2022 Image classification is one of the most fundamental tasks in CV, due to the simplicity of the setting and the cheapness of collecting training data. State-of-the-art image classification algorithms are based on deep neural networks including convolutional networks [8], [51] and vision transformers [9], [10]. A pre-training stage with either selfsupervised representation learning [25], [52] or largescale datasets (e.g., the full ImageNet [53] or even external datasets [54], [55]) is very helpful to improve the classification accuracy. \u2022 The models for object detection and instance segmentation are mostly fine-tuned from the models trained for image classification. Researchers designed specific modules (often referred to as the head) to use the image features extracted by the classification network (often referred to as the backbone) for object localization and recognition. The head modules can be roughly categorized into the twostage [56], [57], [58] and one-stage [59], [60] methods,\nand the transformer blocks have been used [61] and pushed the performance on real-world data [62] towards a higher level [6], [7]. \u2022 The semantic segmentation algorithms fine-tune models trained for image classification in another way. The early efforts involve the encoder-decoder architecture which first downsamples the original image to extract semantic features and then upsamples the features to the original resolution [63], [64]. The idea was also inherited to medical images [65] and generalized to 3D data [66]. It was shown that keeping high-resolution features improves the segmentation accuracy [67]. Vision transformers also offered new opportunities for more accurate segmentation models [68], [69], especially for more challenging datasets [70]. \u2022 The image captioning task [62], [71] is one of the early trials for cross-modal understanding. In the beginning, pre-trained vision models are equipped with a recurrent head such as LSTM [72] for generating captions [73], [74]. Recently, researchers developed an alternative solution for image captioning which involves fine-tuning foundation models that have connected vision to language [75], [76], [77]. \u2022 For text-to-image generation, state-of-the-art algorithms [12], [78] are based on the alignment between vision and language. For this purpose, a cross-modal pre-trained model such as CLIP [13] is inherited, based on which probabilistic models are used to decode sequential tokens into images [79], [80], [81] or denoising latent diffusion models [12], [78], [82].\nBesides, there exist algorithms for other vision tasks, including multiple object tracking [83], [84], [85], pose estimation [86], and many others. It is clear that the current status of CV (individual algorithms are used for different purposes) is far from what the GPT series has achieved in the NLP field."
        },
        {
            "heading": "4.2 Unification Is the Trend",
            "text": "Below, we summarize recent research topics towards unification in CV into five categories."
        },
        {
            "heading": "4.2.1 Open-world Visual Recognition",
            "text": "In a long period of time, most CV algorithms can only recognize the concepts that appear in the training data, leading to a \u2018closed-world\u2019 of visual concepts. In opposite, the concept of \u2018open-world\u2019 refers to the ability that a CV algorithm can recognize or understand any concept regardless whether it has appeared before. The open-world ability5 is often introduced by natural language since it is a natural way for humans to understand new concepts. This explains why language-related tasks such as image captioning [73], [74] and visual question answering [91], [92], [93] contributed to the earliest open-world settings for visual recognition.\nRecently, with the emergence of vision-language pretraining (e.g., CLIP [13] and ALIGN [94]), it becomes much\n5. Sometimes, \u2018open-world\u2019 is referred to as \u2018open-set\u2019 or \u2018opendomain\u2019, although these terminologies may have slightly different meanings.\neasier to align features in the vision and language domains. The unified feature space not only offers simpler pipelines for image captioning [75], [76], [77] and visual question answering [11], [76], [95], but also creates a new methodology [13] for conventional visual recognition tasks. For example, image classification can be done by simply matching the query image with a set of templates (also known as \u2018prompts\u2019) saying a photo of {something}, where something can be any (hence open-world) concept like cat or Siberian husky, and set the result to be the candidate with the highest matching score. Beyond the vanilla version, researchers developed algorithms [96], [97] named \u2018learning to prompt\u2019 to improve the classification accuracy. Later, the methodology was inherited from image classification to object detection [87], [98], semantic segmentation [88], [99], instance segmentation [100], panoptic segmentation [101], [102], and further extended to visual grounding [103] and composite visual recognition [90] tasks. These tasks can benefit from vision-language models pre-trained with enhanced localization [103], [104].\nOpen-world visual recognition is closely related to zeroshot visual recognition because both of them try to generalize the recognition ability to the concepts that have\nnot appeared in the training set. However, in the authors\u2019 opinion, it is yet unclear whether and how deep learning algorithms can recognize unseen concepts. Indeed, there are some special cases that zero-shot recognition can be achieved (e.g., the training data contains dog, cat, and dog\u2019s head, but it does not contain cat\u2019s head; it is possible that the algorithm can learn the concept of cat\u2019s head from composition without training data), but in most cases, the zero-shot ability was inherited from the pre-trained vision language model. Note that the original CLIP model and other variants (e.g., OpenCLIP [105] and EVA-CLIP [106]) were pre-trained on large-scale image-text pairs which may have contained the target concepts withheld from the downstream training set. Therefore, we argue that \u2018open-world\u2019 is a more precise description than \u2018zero-shot\u2019.\nAs language introduces flexibility to visual recognition, it also brings the drawback of referring to detailed semantics in complex scenes. For example, when a large number of same-class objects appear in an image, it is difficult for the model to ask about the position, shape, or attributes of a specific object. This issue is easily solved in vision itself, e.g., one can use a point to indicate the object of interest. We will get back to this issue in the part discussing multimodal dialog (see Section 4.2.5)."
        },
        {
            "heading": "4.2.2 The Segment Anything Task",
            "text": "The Segment Anything task [14] was introduced recently as a generalized module to cluster raw image pixels into groups, many of which correspond to basic visual units in the image. The proposed task supports several types of prompts including point, contour, text, etc., and produces a few masks as well as scores for each prompt or each combination of prompts. Trained on a large-scale dataset with about 10 million images, the derived model, SAM, was able to transfer to a wide range of segmentation tasks including medical image analysis [111], [112], [113], camouflaged object segmentation [110], [114], 3D object segmentation [108], object tracking [115], as well as application scenarios such as image inpainting [109]. SAM can also be used with state-\nof-the-art visual recognition algorithms, such as refining bounding boxes produced by visual grounding [116] algorithms into masks, and feeding the segmented units into open-set classification algorithms for image tagging [107], [117].\nTechnically, the keys of SAM lie in the prompting mechanism and data closure, i.e., closing the segmentation task with a small amount of feedback from labelers. The unified form of prompts makes SAM look like a part of the vision foundation model or pipeline, but there are still many unsolved issues. For example, it remains unclear about the upstream and downstream modules of SAM (if SAM is indeed part of the pipeline), and SAM can be severely impacted by pixel-level appearance, e.g., an arm can be segmented from the torso exactly on the border of clothes, implying that color is the dominant factor for segmentation. In general, it is likely that SAM has over-fitted to the Segment Anything task itself and hence weakened its ability of classification."
        },
        {
            "heading": "4.2.3 Generalized Visual Encoding",
            "text": "Another way to unify CV tasks is to provide a generalized visual encoding for them. There are several methodologies to achieve this goal.\nA key difficulty lies in the large variance between vision tasks, e.g., object detection requires a set of bounding boxes while semantic segmentation requires a dense prediction over the entire image, both of which are very different from a single label required by image classification. As all can understand it, natural language offers a unified form to represent everything. An early effort named pix2seq [15]\nshowed that object detection results (i.e., bounding boxes) can be formulated into natural language and coordinates and then converted into tokens as the output of vision models. In a later version, pix2seq-v2, they generalized the representation to unify the output of object detection, instance segmentation, keypoint detection, and image captioning. Similar ideas were also used for other image recognition [120], video recognition [121], and multimodal understanding [16], [122], [123] tasks.\nBesides using language, researchers also tried to use vision alone to unify everything. The idea was named incontext learning and was borrowed from the NLP community [3], suggesting that a pre-trained model can realize the intention of new tasks from a few demonstrations. This learning paradigm was first introduced into CV using natural language as prompts [76]. In [17], different vision tasks, including instance segmentation, keypoint detection, depth estimation, saliency detection, etc., were formulated into assigning different color patches or regions in the output image canvas, hence a single model named Painter can be trained to deal with them all. The framework was then extended into a more generalized form which also supports video segmentation [119].\nIn the backbone of the above algorithms lies the vision transformer [9], which offers strong data fitting ability in different modalities. The ability was verified by an earlier work which trained a generalist agent named Gato [45] to unify vision, language, and robotics tasks as long as the desired output can be encoded into a sequence of tokens.\nDespite the ability of unified representation, it is questionable how far the methodology has gone beyond multitask visual representation learning, where different tasks are integrated by incorporating multiple loss functions [50]. Recall that GPT applied in-context learning to unify NLP tasks, but CV does not necessarily follow the same direction: this is because CV tasks are mostly discrete (e.g., there is no intermediate task between segmentation and tracking) and thus there might not be a significant difference between individual and joint optimization strategies."
        },
        {
            "heading": "4.2.4 LLM-guided Visual Understanding",
            "text": "Visual recognition can be complex especially when it involves compositional concepts and/or relationships between visual instances. It is difficult for end-to-end models (vision-language pre-trained models for visual question\nanswering [11], [76], [95]) to produce answers following a procedure that is easily understood by humans.\nTo alleviate the issue, a practical methodology lies in generating explainable logic to assist visual recognition. The idea was not new. Several years ago, prior to the appearance of the transformer architecture, researchers proposed to use the long short-term memory (LSTM) model [72] to generate programs so that vision modules are invoked as modules for complex question answering [126]. At that time, the ability of LSTM largely limits the idea within the range of relatively simple and templated questions.\nRecently, the appearance of large language models (especially the GPT series) makes the conversion of arbitrary questions possible. Specifically, GPT can interact with humans in different ways. For example, it can summarize basic recognition results to the final answer [125] or generate code [18], [124] or natural language scripts [19] to call basic vision modules. As a result, visual questions can be decomposed into basic modules. This is especially effective for logical questions, e.g., that asking about the spatial relationship between objects or that depending on the number of objects.\nLLMs may understand the logic, but they have not yet showed the ability to assist fundamental visual recognition modules. That said, the answer will still be wrong once the basic recognition results are incorrect, e.g., the detection algorithm misses a few objects that are small and/or partially occluded. We expect an essential visual logic to be formulated in the future (e.g., the algorithm can follow a sequential algorithm to detect every object, or be guided by commonsense [127] to solve hard cases), possibly with the assistance of LLMs, so that fundamental visual recognition is boosted."
        },
        {
            "heading": "4.2.5 Multimodal Dialog",
            "text": "Multimodal dialog extends text-based dialog to the vision domain. The early efforts involved visual question answering in which various datasets with simple questions have been constructed [128], [129], [130]. With the rapid development of LLMs, multi-round question answering was made available by fine-tuning pre-trained vision and lan-\nguage models together [11], [95]6. It was also shown that a wide range of questions can be answered via in-context learning in multimodality [76] or using GPT as the logic controller [131].\nRecently, a novel paradigm developed in the GPT series, named instruct learning [4], has been inherited to enhance the quality of multimodal dialog [20], [46]. The idea was to provide a few reference data (e.g., objects, descriptions) from ground-truth annotation or recognition results and ask the GPT model to generate instruction data (i.e., enriched question-answer pairs). Fine-tuned with these data (without reference), the foundation models for vision and language can interact with each other via a lightweight network module (e.g., a Q-former [11]).\nMultimodal dialog offers a preliminary interactive benchmark for computer vision, but, as a language-guided task, it also has the weaknesses analyzed in the openworld visual recognition (see Section 4.2.1). We expect that enriching the form of queries (e.g., using generalized visual encoding methods, see Section 4.2.3) can push multimodal dialog to a higher level."
        },
        {
            "heading": "4.3 The Essential Difficulty",
            "text": "Indeed, the above efforts have largely advanced the progress of unification in CV. However, the community is still far from an algorithm that can solve a wide range of real-world tasks, in particular when interaction is needed. In this part, we discuss on the essential difficulty that leads to the current dilemma."
        },
        {
            "heading": "4.3.1 GPT Revisited",
            "text": "We start with recalling the definition of AGI (see Section 2, the definition was inherited from [42], [43]). In short, the goal of AGI is to maximize a reward in an interactable environment.\nGPT defined such an environment with the chat task. Note that, in a pure-text world, chat is the perfect task for an agent to learn from interaction (talking with humans and receiving feedback); meanwhile, any task can be defined by chat. In our opinion, establishing the environment (with the chat task) is the most important insight of GPT. The technical solutions (i.e., generative pre-training followed by instruct fine-tuning) can be derived from the chat task: generative pre-training is to memorize the distribution of the environment (world); instruct fine-tuning is to align the learned contribution with question-answer pairs for problem-solving. There are clear boundary between them, as the fine-tuning stage is driven by specific tasks while the pre-training stage is not.\nWe try to build the relationship between the basic elements of an environment and GPT. We find that the state, action, and reward in the environment correspond to the prompting mechanism, the desired target, and the feedback from users, respectively. We expect that CV algorithms are also trained in such an environment, or at least with the above factors clearly defined.\n6. GPT-4 [1] also showed examples of multimodal dialog, but it is unclear how they achieved the ability, especially for the cases with rich texts, e.g., solving a complex physical problem and understanding a joke which is mainly described in optical characters."
        },
        {
            "heading": "4.3.2 Why Not Establishing Environments for CV?",
            "text": "Conceptually, an AGI in CV should also be trained in environments. Back to the 1970s, David Marr pointed out that CV algorithms should construct a world model and learn abilities by interacting with the model [132]. Other pioneers in AI, including Hans Moravec and Rodney Brooks, also emphasized the importance of learning from environments [133], [134]. However, establishing environments for CV is never an easy task, unlike that for NLP which is quite straightforward.\nThere are mainly two options, namely, training agents in the real world or in virtual, simulated environments. The former option is closer to the final objective, but the overhigh costs and uncontrollable safety issues have constrained it in small-scale and toy scenarios (e.g., training robotic arms for object grasping). The latter option is relatively easy to achieve, but it suffers the fidelity issues (not only about 3D modeling and rendering, but also about the behavior of other agents) and thus has to alleviate a significant domain gap when being transferred to the real world.\nDue to the high difficulty of simulating the world (i.e., establishing environments), the CV community has taken an alternative solution which is to sample the world. It involves two major steps, namely, image/video data collection and semantic annotation. The first step is to perform a sparse sampling of the real world \u2013 note that, although their size has significantly increased during the past decades, all existing datasets are still orders of magnitude smaller than the real world. The second step is to expect what agents need in order to accomplish tasks and convert the requirements (e.g., detecting objects) into semantic annotations. From this point of view, we refer to them proxy tasks as they serve as surrogates to achieve the goal of AGI. Note that proxy tasks exist in almost all AI fields, e.g., in NLP, there are various such tasks including translation, named entity recognition, and others."
        },
        {
            "heading": "4.3.3 Proxy Is Dying!",
            "text": "The proxy tasks have been vastly improved in the past decade, thanks to the rapid development of deep learning. One of the most well-known examples lies in ImageNet1K classification [53], [135], where the best accuracy is under 50% prior to AlexNet [51], while the accuracy is higher than 90% as of today [55]. The odyssey was made possible by strong network architectures, effective optimization tricks, external training data, etc. Nevertheless, many research papers have been still pursuing higher accuracy on this dataset. Standing upon a high baseline (e.g., 85%), the improvement brought by the proposed algorithm is\noften small (e.g., 0.5%), leading to a weird situation in that implementation tricks contribute even more than the proposed algorithm itself.\nWe illustrate the situation in Figure 13. Let us assume that AGI of vision and perfect performance of proxy tasks are two goals in the space of algorithms. In the pre-deep learning era, CV algorithms are mostly weak, so setting the goal to be high proxy task performance is reasonable. As of today, CV algorithms have been much stronger than before; consequently, continuing to improve proxy tasks can drive us away from AGI. We refer the readers to what happened in NLP: GPT offered a unified solution and largely reduced the importance of the conventional proxy tasks (e.g., translation, named entity recognition, etc.)7."
        },
        {
            "heading": "5 FUTURE: LEARNING FROM ENVIRONMENT",
            "text": "The above analysis calls for a new paradigm for training strong agents for CV. In this section, we convert our opinions and insights into an imaginary pipeline, review existing works that are related to the pipeline, and make comments on future research directions based on the pipeline."
        },
        {
            "heading": "5.1 An Imaginary Pipeline",
            "text": "Figure 14 shows our idea. The pipeline comprises three stages: Stage 0 for establishing environments, Stage 1 for pre-training, and Stage 2 for fine-tuning. When necessary, the fine-tuned model can be prompted for conventional visual recognition tasks. Below, we describe each stage in detail.\n\u2022 Stage 0: establishing environments. As analyzed previously, high-quality environments are strongly required for AGI in CV. Here the concept of \u2018highquality\u2019 includes but is not limited to abundance (there should be sufficient and diversified environments), fidelity (visual appearance and other agents\u2019 behavior should be close to the real world), and richness in interaction (the agent can be asked to perform a wide range of tasks by interacting with the environments). \u2022 Stage 1: generative pre-training. The algorithm is asked to explore the environments and pre-trained to predict future frames. The biggest difference from the GPT task (predicting the next token) in NLP lies in that the future frames depend on the action of the agent (in NLP, the pre-trained text corpus remains unchanged), so the model is trying to learn a joint distribution of state and action. This strategy is especially useful when the set of established environments is insufficient to approximate the world\u2019s distribution. Note that, as CV is a superset of NLP (see the paragraph before Section 4.1), the size (e.g., number of parameters) of pre-trained CV models should be orders of magnitude larger than NLP models.\n7. Disclaimer: these tasks are still important in some real-world applications, but it is unlikely that they shall be studied in an oldfashioned manner. With the chat task, these tasks can be accomplished with simple prompts.\n\u2022 Stage 2: instruct fine-tuning. The pre-trained model is guided to accomplish real-world tasks, following human instructions. Intuitively, there are much more types of allowed interaction between the agent and environments, including exploration, navigation, using language, performing physical actions, and many others. A reasonable conjecture is that much more instruction data should be collected, which also corresponds to the size of foundation CV models. \u2022 Optional: downstream perception. We expect that CV algorithms can learn all required abilities of perception from the previous stage, e.g., in order to accomplish a very simple task, Buy me a cup of coffee, the model must at least learn to (i) explore around with safety, (ii) recognize where the coffee bar is, (iii) communicate with the shop assistant with language, and (iv) grasp the bought coffee. Such a model, when properly prompted, should output desired perception results, including tracking another agent (for not colliding with it), open-set visual recognition (for finding the bar and the bought coffee), and others. This is related to the idea of analysis by synthesis [136].\nTo sum up, we expect the agent to be task-oriented, i.e., focusing itself on accomplishing tasks in the established environments. The proxy tasks (see Section 4.3.3) are to be solved naturally with prompts."
        },
        {
            "heading": "5.2 Existing Works",
            "text": "We briefly review the existing works that are related to the imaginary pipeline."
        },
        {
            "heading": "5.2.1 3D Embodied Environments",
            "text": "There are typically two options for establishing virtual environments. The first option is to collect visual content\nfrom a real-world scenario and perform 3D reconstruction. For example, Habitat [22] released more than 200 scanned scenarios for visual navigation. The second option is to generate (render) scenes with 3D models. For example, ProcTHOR [23] provided a large set of 3D objects and a program to randomly generate room layouts, so that one can sample an arbitrary number of 3D scenes and perform various tasks including navigation, grasping, and reordering.\nHowever, the existing environments (including the above two and many others [28], [137]) have not yet validated the ability to scale up to the level of the real world. In particular, when more and more environments are sampled from ProcTHOR [23], the performance, either for embodied or downstream recognition tasks, can saturate quickly. Clearly, for any existing method, there is a tradeoff between abundance and fidelity. This is a major challenge before pretrained CV models can exhibit the scaling law [138] and emergent abilities [139] as NLP models did."
        },
        {
            "heading": "5.2.2 Pre-training Vision Models",
            "text": "In the past years, visual pre-training methods have been largely developed. Contrastive learning (CL) [24], [140], [141], [142] offered the first methodology to surpass supervised learning in downstream tasks, and masked image modeling (MIM) [25], [52], [143] pushed the performance of pre-trained models to a higher level. The major difference between them lies in the pre-training objective, where CL is discriminative and MIM is generative.\nBuilt upon a generative objective, MIM is closer to what we desire in the aforementioned pipeline. However, predicting future frames in environments is different from predicting missing contents in sampled images or videos, which is similar to the difference between masked language modeling [48] and generative pre-training [47]. Additionally, compared to text data, there can be much more re-\ndundant information in vision data. We conjecture that data compression is an important factor in the pre-training task."
        },
        {
            "heading": "5.2.3 Reinforcement Learning for Game Playing",
            "text": "Interacting with environments is closely related to game playing. The past decade has witnessed the integration of deep learning and reinforcement learning, resulting in a series of algorithms for playing various games, such as Atari 2600 games [26], [144], simulated robotics tasks [144], [145], Go and other chess [146], [147], StarCraft II [27], and so on. Effective algorithms were also designed for combining multiple reinforcement learning strategies [148] or completing very complex tasks [149].\nCompared to the above problems, the real world is much more complicated and involves actions from different aspects including language and robotics. A practical strategy is to first disable some types of interaction to simplify the tasks and add them back when the foundation models are sufficiently strong."
        },
        {
            "heading": "5.2.4 Embodied Computer Vision",
            "text": "Embodied AI refers to the research field in that agents learn from/for interacting with virtual environments. The motivation partly came from how humans learn as babies [150]. In the scope of CV, typical tasks include exploration [151], [152], [153] where the goal is simply to explore and reconstruct the world, visual navigation [28], [154], [155] where the agents are asked to explore the world for specific targets (e.g., an image or an object), and embodied question answering [29], [156] where the agents answer questions based on the interaction with the world.\nRecently, two works were brought to our attention. The first one is PaLM-E [30] where a general-purpose visionlanguage model is trained to perform a wide range of embodied tasks. These tasks are organized into a unified prompting system, thanks to the in introduction of LLMs. The second one is ENTL [31] where an end-to-end system was designed and different stages in embodied CV (including world modeling, localization, and imitation learning) were tokenized and integrated into a sequence prediction task. Both works pushed the unification in embodied CV forward from different directions.\nWe emphasize that, despite the existing efforts, the realworld scenario is much more complicated than what we have ever created or simulated (see Section 5.2.1). To achieve the goal of AGI, more interaction types should be supported, long-range and complex tasks should be designed, and instruction data should be collected. System design and engineering might be more important than one thinks."
        },
        {
            "heading": "5.3 Comments on Research Directions",
            "text": "As the final part, we make comments on future research directions. With the major goals migrated from the performance of proxy tasks to learning from environments, many popular research directions may have to adjust their goal. Here is a disclaimer: all the following statements are our personal opinions and may be wrong."
        },
        {
            "heading": "5.3.1 On Establishing Environments",
            "text": "A clear goal is to continue increasing the size, diversity, and fidelity of the virtual environments. There are multiple techniques that can help. For example, novel 3D representation forms (e.g., neural rendering field, NeRF [157], [158], [159]) may be more efficient in achieving a tradeoff between the reconstruction quality and overhead.\nAnother important direction lies in the richness of environments. It is a non-trivial task to define new, complex tasks and unify them into a prompting system. Also, AI algorithms can benefit much from a better simulation of other agents\u2019 behavior [160] because it can largely improve the abundance of environments and hence the robustness of the trained algorithms."
        },
        {
            "heading": "5.3.2 On Generative Pre-training",
            "text": "There are mainly two factors that affect the pre-training stage, namely, neural architecture design and proxy task design. The latter is clearly more important and the former shall be built upon the latter.\nAs analyzed in Section 5.2.2, existing pre-training tasks, including contrastive learning and masked image modeling, shall be modified to allow for efficient explorations in virtual environments. We expect the newly designed proxy to focus on data compression, because there is much heavier redundancy in vision data than in language data. The new pre-training proxy defines the requirement of neural architectures, e.g., to achieve a tradeoff between data compression and visual recognition, the designed architecture should be equipped with an ability to extract different levels (granularity) of visual features by request.\nAdditionally, cross-modality (e.g., text-to-image) generation will become a direct metric to measure the performance of pre-training. When a unified tokenization method is available, it can be formulated into a multimodal version of the reconstruction loss."
        },
        {
            "heading": "5.3.3 On Instruct Fine-tuning",
            "text": "We have not yet entered the scope of defining tasks in the new paradigm. As real-world tasks can be very complicated, we conjecture that some elementary tasks (e.g., exploration, fetching, interaction, etc.) can be defined and trained first, so that complex tasks can be decomposed into them. For this purpose, a unified prompting system should be designed and abundant human instructions should be collected. As a reasonable conjecture, the amount of instruction data can be orders of magnitude larger than what has been collected for training GPT and other chatbots.\nThis is a completely new story for CV. The road ahead is filled with unknown difficulties and uncertainty. We cannot see much at the current point, but clear paths will emerge in the future."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this paper, we discuss how to advance CV algorithms towards AGI. We start with reviewing the current status and recent efforts of CV for unification, and then we inherit ideas and insights from NLP, especially the GPT series. Our conclusion is that CV lacks a paradigm that allows learning from environments, for which we propose an imaginary\npipeline. We expect that substantial technical evolution is required to bring the pipeline to truth."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank many colleagues and collaborating researchers for instructive discussions."
        }
    ],
    "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models",
    "year": 2023
}