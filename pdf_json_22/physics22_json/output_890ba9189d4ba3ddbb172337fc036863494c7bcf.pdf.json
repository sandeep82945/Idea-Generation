{
    "abstractText": "Active learning (AL) has interesting features for parameter scans of new models. We show on a variety of models that AL scans bring large efficiency gains to the traditionally tedious work of finding boundaries for BSM models. In the MSSM, this approach produces more accurate bounds. In light of our prior publication, we further refine the exploration of the parameter space of the SMSQQ model, and update the maximum mass of a dark matter singlet to 48.4 TeV. Finally we show that this technique is especially useful in more complex models like the MDGSSM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mark D. Goodsella"
        },
        {
            "affiliations": [],
            "name": "Ari Jouryb"
        }
    ],
    "id": "SP:c07cfd4b64b9f8ffe0f67cc0fa7ec49b03c06950",
    "references": [
        {
            "authors": [
                "P. Slavich"
            ],
            "title": "Higgs-mass predictions in the MSSM and beyond",
            "venue": "Eur. Phys. J. C 81,",
            "year": 2021
        },
        {
            "authors": [
                "F. Staub",
                "SARAH 4"
            ],
            "title": "A tool for (not only SUSY) model builders",
            "venue": "Comput. Phys. Commun. 185, 1773",
            "year": 2014
        },
        {
            "authors": [
                "M. Goodsell",
                "K. Nickel",
                "F. Staub"
            ],
            "title": "Generic two-loop Higgs mass calculation from a diagrammatic approach",
            "venue": "Eur. Phys. J. C 75,",
            "year": 2015
        },
        {
            "authors": [
                "M.D. Goodsell",
                "K. Nickel",
                "F. Staub"
            ],
            "title": "The Higgs Mass in the MSSM at two-loop order beyond minimal flavour violation",
            "venue": "Phys. Lett. B 758,",
            "year": 2016
        },
        {
            "authors": [
                "J. Braathen",
                "M.D. Goodsell",
                "F. Staub"
            ],
            "title": "Supersymmetric and nonsupersymmetric models without catastrophic Goldstone bosons",
            "venue": "Eur. Phys. J. C 77,",
            "year": 2017
        },
        {
            "authors": [
                "M.D. Goodsell",
                "F. Staub"
            ],
            "title": "Unitarity constraints on general scalar couplings with SARAH",
            "venue": "Eur. Phys. J. C 78,",
            "year": 2018
        },
        {
            "authors": [
                "M.D. Goodsell",
                "R. Moutafis"
            ],
            "title": "How heavy can dark matter be? Constraining colourful unitarity with SARAH",
            "venue": "Eur. Phys. J. C 81,",
            "year": 2023
        },
        {
            "authors": [
                "W. Porod"
            ],
            "title": "SPheno, a program for calculating supersymmetric spectra, SUSY particle decays and SUSY particle production at e+ ecolliders",
            "venue": "Comput. Phys. Commun. 153,",
            "year": 2003
        },
        {
            "authors": [
                "W. Porod",
                "F. Staub"
            ],
            "title": "SPheno 3.1: Extensions including flavour, CPphases and models beyond the MSSM",
            "year": 2011
        },
        {
            "authors": [
                "G. B\u00e9langer",
                "F. Boudjema",
                "A. Goudelis",
                "A. Pukhov",
                "B. Zaldivar",
                "micrOMEGAs5.0"
            ],
            "title": "Freeze-in",
            "venue": "Comput. Phys. Commun. 231, 173",
            "year": 2018
        },
        {
            "authors": [
                "G. Belanger",
                "A. Mjallal",
                "A. Pukhov"
            ],
            "title": "Recasting direct detection limits within micrOMEGAs and implication for non-standard Dark Matter scenarios (2020)",
            "year": 2003
        },
        {
            "authors": [
                "P. Bechtle",
                "S. Heinemeyer",
                "O. St\u00e5l",
                "T. Stefaniak",
                "G. Weiglein",
                "HiggsSignals"
            ],
            "title": "Confronting arbitrary Higgs sectors with measurements at the Tevatron and the LHC",
            "venue": "Eur. Phys. J. C 74, 2711",
            "year": 2014
        },
        {
            "authors": [
                "P. Bechtle",
                "S. Heinemeyer",
                "T. Klingl",
                "T. Stefaniak",
                "G. Weiglein",
                "J. Wittbrodt",
                "HiggsSignals-2"
            ],
            "title": "Probing new physics with precision Higgs measurements in the LHC 13 TeV era",
            "venue": "Eur. Phys. J. C 81, 145",
            "year": 2021
        },
        {
            "authors": [
                "P. Bechtle",
                "O. Brein",
                "S. Heinemeyer",
                "G. Weiglein",
                "K.E. Williams",
                "HiggsBounds"
            ],
            "title": "Confronting Arbitrary Higgs Sectors with Exclusion Bounds from LEP and the Tevatron",
            "venue": "Comput. Phys. Commun. 181, 138",
            "year": 2010
        },
        {
            "authors": [
                "P. Bechtle",
                "D. Dercks",
                "S. Heinemeyer",
                "T. Klingl",
                "T. Stefaniak",
                "G. Weiglein",
                "J. Wittbrodt",
                "HiggsBounds-5"
            ],
            "title": "Testing Higgs Sectors in the LHC 13 TeV Era",
            "venue": "Eur. Phys. J. C 80, 1211",
            "year": 2020
        },
        {
            "authors": [
                "H. Bahl",
                "T. Biek\u00f6tter",
                "S. Heinemeyer",
                "C. Li",
                "S. Paasch",
                "G. Weiglein",
                "J. Wittbrodt"
            ],
            "title": "HiggsTools: BSM scalar phenomenology with new versions of HiggsBounds and HiggsSignals (2022)",
            "year": 2022
        },
        {
            "authors": [
                "J.E. Camargo-Molina",
                "B. O\u2019Leary",
                "W. Porod",
                "F. Staub",
                "Vevacious"
            ],
            "title": "A Tool For Finding The Global Minima Of One-Loop Effective Potentials With Many Scalars",
            "venue": "Eur. Phys. J. C 73, 2588",
            "year": 2013
        },
        {
            "authors": [
                "P. Athron",
                "J.-H. Park",
                "D. St\u00f6ckinger",
                "A. Voigt"
            ],
            "title": "FlexibleSUSY \u2013 a meta spectrum generator for supersymmetric models",
            "venue": "Nucl. Part. Phys. Proc. 273\u2013275,",
            "year": 2016
        },
        {
            "authors": [
                "P. Athron",
                "M. Bach",
                "D. Harries",
                "T. Kwasnitza",
                "J.-h. Park",
                "D. St\u00f6ckinger",
                "A. Voigt",
                "J. Ziebell",
                "FlexibleSUSY 2.0"
            ],
            "title": "Extensions to investigate the phenomenology of SUSY and non-SUSY models",
            "venue": "Comput. Phys. Commun.230, 145",
            "year": 2018
        },
        {
            "authors": [
                "P.Z. Skands et al.",
                "SUSY Les Houches accord"
            ],
            "title": "Interfacing SUSY spectrum calculators, decay packages, and event generators",
            "venue": "JHEP 07, 036",
            "year": 2004
        },
        {
            "authors": [
                "D.M. Straub"
            ],
            "title": "flavio: a Python package for flavour and precision phenomenology in the Standard Model and beyond, (2018)",
            "year": 2018
        },
        {
            "authors": [
                "G. Alguero",
                "J.Y. Araz",
                "S.B. Fuks"
            ],
            "title": "Kraml, Signal region combination with full and simplified likelihoods in MadAnalysis",
            "year": 2022
        },
        {
            "authors": [
                "F. Feroz",
                "M.P. Hobson",
                "M. Bridges",
                "MultiNest"
            ],
            "title": "an efficient and robust Bayesian inference tool for cosmology and particle physics",
            "venue": "Mon. Not. Roy. Astron. Soc. 398, 1601",
            "year": 2009
        },
        {
            "authors": [
                "E. Bagnaschi"
            ],
            "title": "Global Analysis of Dark Matter Simplified Models with Leptophobic Spin-One Mediators using MasterCode",
            "venue": "Eur. Phys. J. C 79,",
            "year": 2019
        },
        {
            "authors": [
                "GAMBIT",
                "P. Athron",
                "et al.",
                "GAMBIT"
            ],
            "title": "The Global and Modular Beyond-the-Standard-Model Inference Tool",
            "venue": "Eur. Phys. J. C 77, 784",
            "year": 2017
        },
        {
            "authors": [
                "GAMBIT Models Workgroup",
                "P. Athron",
                "et al.",
                "SpecBit",
                "DecayBit",
                "PrecisionBit"
            ],
            "title": "GAMBIT modules for computing mass spectra, particle decay rates and precision observables",
            "venue": "Eur. Phys. J. C78, 22",
            "year": 2018
        },
        {
            "authors": [
                "S. Bloor",
                "T.E. Gonzalo",
                "P. Scott",
                "C. Chang",
                "A. Raklev",
                "J.E. Camargo-Molina",
                "A. Kvellestad",
                "J.J. Renk",
                "P. Athron",
                "C. Bal\u00e1zs",
                "The GAMBIT Universal Model Machine"
            ],
            "title": "from Lagrangians to likelihoods",
            "venue": "Eur. Phys. J. C 81, 1103",
            "year": 2021
        },
        {
            "authors": [
                "GAMBIT",
                "G.D. Martinez",
                "J. McKay",
                "B. Farmer",
                "P. Scott",
                "E. Roebber",
                "A. Putze",
                "J. Conrad"
            ],
            "title": "Comparison of statistical sampling methods with ScannerBit, the GAMBIT scanning",
            "venue": "module. Eur. Phys. J. C 77,",
            "year": 2017
        },
        {
            "authors": [
                "J. Ren",
                "L. Wu",
                "J.M. Yang",
                "J. Zhao"
            ],
            "title": "Exploring supersymmetry with machine learning",
            "venue": "Nucl. Phys. B 943,",
            "year": 2019
        },
        {
            "authors": [
                "F. Staub"
            ],
            "title": "xBIT: an easy to use scanning tool with machine learning abilities, (2019)",
            "year": 1906
        },
        {
            "authors": [
                "J. Gawlikowski"
            ],
            "title": "a survey of uncertainty in deep neural networks. CoRR abs/2107.03342 (2021)",
            "year": 2021
        },
        {
            "authors": [
                "NNPDF",
                "R.D. Ball",
                "L. Del Debbio",
                "S. Forte",
                "A. Guffanti",
                "J.I. Latorre",
                "A. Piccione",
                "J. Rojo",
                "M. Ubiali"
            ],
            "title": "A Determination of parton distributions with faithful uncertainty estimation",
            "venue": "Nucl. Phys. B 809,",
            "year": 2009
        },
        {
            "authors": [
                "A. Butter"
            ],
            "title": "The Machine Learning landscape of top taggers",
            "venue": "SciPost Phys.7,",
            "year": 2019
        },
        {
            "authors": [
                "S. Bollweg",
                "M. Hau\u00dfmann",
                "G. Kasieczka",
                "M. Luchmann",
                "T. Plehn",
                "J. Thompson"
            ],
            "title": "Deep-Learning Jets with Uncertainties and More",
            "venue": "SciPost Phys.8,",
            "year": 2020
        },
        {
            "authors": [
                "S. Cheong",
                "A. Cukierman",
                "B. Nachman",
                "M. Safdari",
                "A. Schwartzman"
            ],
            "title": "Parametrizing the Detector Response with Neural Networks",
            "venue": "JINST 15,",
            "year": 2020
        },
        {
            "authors": [
                "CMS",
                "A.M. Sirunyan"
            ],
            "title": "A Deep Neural Network for Simultaneous Estimation of b Jet Energy and Resolution",
            "venue": "Com123 Eur. Phys. J. C (2023)",
            "year": 2020
        },
        {
            "authors": [
                "B.S. Kronheim",
                "M.P. Kuchera",
                "A.H.B. Prosper"
            ],
            "title": "Karbo, Bayesian Neural Networks for Fast SUSY Predictions",
            "venue": "Phys. Lett. B",
            "year": 2021
        },
        {
            "authors": [
                "J.Y. Araz",
                "M. Spannowsky",
                "Combine",
                "Conquer"
            ],
            "title": "Event Reconstruction with Bayesian Ensemble Neural Networks",
            "venue": "JHEP 04, 296",
            "year": 2021
        },
        {
            "authors": [
                "M. Bellagente",
                "M. Haussmann",
                "M. Luchmann",
                "T. Plehn"
            ],
            "title": "Understanding Event-Generation Networks via Uncertainties",
            "venue": "SciPost Phys. 13,",
            "year": 2022
        },
        {
            "authors": [
                "B. Kronheim",
                "M.P. Kuchera",
                "H.B. Prosper",
                "R. Ramanujan"
            ],
            "title": "Implicit Quantile Neural Networks for Jet Simulation and Correction (2021)",
            "year": 2021
        },
        {
            "authors": [
                "R. Gambhir",
                "B. Nachman",
                "J. Thaler",
                "Learning uncertainties the frequentist way"
            ],
            "title": "Calibration and correlation in high energy physics",
            "venue": "Phys. Rev. Lett. 129, 082001",
            "year": 2022
        },
        {
            "authors": [
                "S. Caron",
                "T. Heskes",
                "S. Otten",
                "B. Stienen"
            ],
            "title": "Constraining the parameters of high-dimensional models with active learning",
            "venue": "Eur. Phys. J. C 79,",
            "year": 2019
        },
        {
            "authors": [
                "J. Rocamonde",
                "L. Corpe",
                "G. Zilgalvis",
                "M. Avramidou",
                "J. Butterworth"
            ],
            "title": "Picking the low-hanging fruit: testing new physics at scale with active learning, (2022)",
            "year": 2022
        },
        {
            "authors": [
                "M. Feickert",
                "B. Nachman"
            ],
            "title": "A living review of machine learning for particle physics, (2021)",
            "year": 2021
        },
        {
            "authors": [
                "J. Carifio",
                "J. Halverson",
                "D. Krioukov",
                "B.D. Nelson"
            ],
            "title": "Machine Learning in the String Landscape",
            "venue": "JHEP 09,",
            "year": 2017
        },
        {
            "authors": [
                "F. Ruehle"
            ],
            "title": "Data science applications to string theory",
            "venue": "Phys. Rept. 839,",
            "year": 2020
        },
        {
            "authors": [
                "P. Berglund",
                "G. Butbaia",
                "T. H\u00fcbsch",
                "V. Jejjala",
                "D. Mayorga Pe na",
                "C. Mishra",
                "J. Tan"
            ],
            "title": "Machine Learned Calabi\u2013Yau Metrics and Curvature (2022)",
            "year": 2022
        },
        {
            "authors": [
                "K. Cranmer et al.",
                "Publishing statistical models"
            ],
            "title": "Getting the most out of particle physics experiments",
            "venue": "SciPost Phys. 12, 037",
            "year": 2022
        },
        {
            "authors": [
                "S.S. AbdusSalam"
            ],
            "title": "Simple and statistically sound recommendations for analysing physical theories",
            "venue": "Rept. Prog. Phys. 85,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xu",
                "R. Akella",
                "Y. Zhang"
            ],
            "title": "incorporating diversity and density in active learning for relevance feedback, in Advances in Information Retrieval, edited",
            "year": 2007
        },
        {
            "authors": [
                "J.R. Ellis",
                "K.A. Olive",
                "Y. Santoso"
            ],
            "title": "The MSSM parameter space with nonuniversal",
            "venue": "Higgs masses. Phys. Lett. B 539,",
            "year": 2002
        },
        {
            "authors": [
                "J. Ellis",
                "K.A. Olive",
                "P. Sandick"
            ],
            "title": "Update on the direct detection of dark matter in MSSM models with non-universal higgs masses",
            "venue": "New J. Phys. 11,",
            "year": 2009
        },
        {
            "authors": [
                "J. Ellis",
                "K.A. Olive"
            ],
            "title": "Revisiting the Higgs Mass and Dark Matter in the CMSSM",
            "venue": "Eur. Phys. J. C",
            "year": 2005
        },
        {
            "authors": [
                "M.K. Griest"
            ],
            "title": "Kamionkowski, Unitarity Limits on the Mass and Radius of Dark Matter Particles",
            "venue": "Phys. Rev. Lett. 64,",
            "year": 1990
        },
        {
            "authors": [
                "S. El Hedri",
                "W. Shepherd",
                "D.G.E. Walker"
            ],
            "title": "Perturbative Unitarity Constraints on Gauge Portals",
            "venue": "Phys. Dark Univ",
            "year": 2017
        },
        {
            "authors": [
                "B. von Harling",
                "K. Petraki"
            ],
            "title": "Bound-state formation for thermal relic dark matter and unitarity",
            "venue": "JCAP 12,",
            "year": 2014
        },
        {
            "authors": [
                "M. Cahill-Rowley",
                "S. El Hedri",
                "W. Shepherd",
                "D.G.E. Walker"
            ],
            "title": "Perturbative Unitarity Constraints on Charged/Colored Portals",
            "venue": "Phys. Dark Univ. 22,",
            "year": 2018
        },
        {
            "authors": [
                "F. Kahlhoefer",
                "K. Schmidt-Hoberg",
                "T. Schwetz",
                "S. Vogl"
            ],
            "title": "Implications of unitarity and gauge invariance for simplified dark matter models",
            "venue": "JHEP 02,",
            "year": 2016
        },
        {
            "authors": [
                "I. Baldes",
                "K. Petraki",
                "Asymmetric thermal-relic dark matter"
            ],
            "title": "Sommerfeld-enhanced freeze-out, annihilation signals and unitarity bounds",
            "venue": "JCAP 09, 028",
            "year": 2017
        },
        {
            "authors": [
                "S. El Hedri",
                "A. Kaminska",
                "M. de Vries",
                "J. Zurita"
            ],
            "title": "Simplified Phenomenology for Colored Dark Sectors",
            "venue": "JHEP 04,",
            "year": 2017
        },
        {
            "authors": [
                "S. El Hedri",
                "M. de Vries"
            ],
            "title": "Cornering Colored Coannihilation",
            "venue": "JHEP 10,",
            "year": 2018
        },
        {
            "authors": [
                "J. Harz",
                "K. Petraki"
            ],
            "title": "Radiative bound-state formation in unbroken perturbative non-Abelian theories and implications for dark matter",
            "venue": "JHEP 07,",
            "year": 2018
        },
        {
            "authors": [
                "A. Hektor",
                "K.A. Hryczuk"
            ],
            "title": "Kannike, Improved bounds on Z3 singlet dark matter",
            "venue": "JHEP 03,",
            "year": 2019
        },
        {
            "authors": [
                "K. Kannike",
                "K. Loos",
                "M. Raidal"
            ],
            "title": "Gravitational wave signals of pseudo-Goldstone dark matter in the Z3 complex singlet model",
            "venue": "Phys. Rev. D 101,",
            "year": 2020
        },
        {
            "authors": [
                "T. Alanne",
                "N. Benincasa",
                "M. Heikinheimo",
                "K. Kannike",
                "V. Keus",
                "N. Koivunen",
                "K. Tuominen",
                "Pseudo-Goldstone dark matter"
            ],
            "title": "gravitational waves and direct-detection blind spots",
            "venue": "JHEP 10, 080",
            "year": 2020
        },
        {
            "authors": [
                "B. Fuks",
                "M.D. Goodsell",
                "D.W. Kang",
                "P. Ko",
                "S.J. Lee",
                "M. Utsch"
            ],
            "title": "Heavy dark matter through the dilaton portal",
            "venue": "JHEP 10,",
            "year": 2020
        },
        {
            "authors": [
                "C. Espinoza",
                "E. Garc\u00e9s",
                "M. Mondrag\u00f3n",
                "H. Reyes-Gonz\u00e1lez",
                "An Inert Scalar In The S3 Symmetric Model. J. Phys"
            ],
            "title": "Conf",
            "venue": "Ser. 1586, 012025",
            "year": 2020
        },
        {
            "authors": [
                "J. Cao",
                "P. Wan",
                "J.M. Yang",
                "J. Zhu",
                "The SM extension with color-octet scalars"
            ],
            "title": "diphoton enhancement and global fit of LHC Higgs data",
            "venue": "JHEP 08, 009",
            "year": 2013
        },
        {
            "authors": [
                "X.-G. He",
                "H. Phoon",
                "Y. Tang",
                "G. Valencia"
            ],
            "title": "Unitarity and vacuum stability constraints on the couplings of color octet scalars",
            "venue": "JHEP 05,",
            "year": 2013
        },
        {
            "authors": [
                "L. Cheng",
                "O. Eberhardt",
                "C.W. Murphy"
            ],
            "title": "Novel theoretical constraints for color-octet scalar models",
            "venue": "Chin. Phys. C",
            "year": 2019
        },
        {
            "authors": [
                "A. Schuessler",
                "D. Zeppenfeld"
            ],
            "title": "Unitarity constraints on MSSM trilinear couplings",
            "venue": "SUSY 2007 Proceedings,",
            "year": 2007
        },
        {
            "authors": [
                "A. Sch\u00fcssler"
            ],
            "title": "Unitarit\u00e4ts-Schranken an triskalare Kopplungen im MSSM, Diplomarbeit, Institut f\u00fcr Theoretische Physik, Universit\u00e4t Karlsruhe(2005), KIT diploma server",
            "year": 2005
        },
        {
            "authors": [
                "F. Staub",
                "Theoretical Constraints on Supersymmetric Models"
            ],
            "title": "Perturbative Unitarity vs",
            "venue": "Vacuum Stability. Phys. Lett. B 789, 203",
            "year": 2019
        },
        {
            "authors": [
                "M.J. Baker",
                "R.R.P. Cox"
            ],
            "title": "Volkas, Has the Origin of the Third-Family Fermion Masses been Determined? (2020)",
            "year": 2012
        },
        {
            "authors": [
                "G. Chalons",
                "M.D. Goodsell",
                "S. Kraml",
                "H. Reyes-Gonz\u00e1lez",
                "S.L. Williamson"
            ],
            "title": "LHC limits on gluinos and squarks in the minimal Dirac gaugino model",
            "venue": "JHEP 04,",
            "year": 2019
        },
        {
            "authors": [
                "B.K. Miller",
                "A. Cole",
                "G. Louppe",
                "C. Weniger"
            ],
            "title": "Simulation-efficient marginal posterior estimation with swyft: stop wasting your precious time (2020)",
            "year": 2011
        },
        {
            "authors": [
                "S. Mohamed",
                "B. Lakshminarayanan"
            ],
            "title": "Learning in Implicit Generative Models (2016)",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "The Standard Model (SM) is a remarkably economic theory, making many predictions based on only a few parameters. In principle all of the parameters of the model can be fixed by observations, and then many more observables can be predicted. Once we add new particles and interactions to the SM\u2014the bread and butter of many theoretical particle physicists\u2014we necessarily add many new parameters, typically masses and couplings in a Lagrangian. It is then in general a very difficult task to fix as many of these by existing observations and then make predictions for new observations. In many cases these observations have not yet been made; for example, we usually cannot fix the mass of a particle we have not yet observed. In others, the relationship with the observable is complicated; for example the Higgs mass in supersymmetric models, which is highly sensitive to loop corrections (see e.g. [1]). Therefore the first task with a new model is generally to explore the parameter space for plausible allowed regions according to some basic list of observations, before a more detailed examination can be made.\na e-mail: goodsell@lpthe.jussieu.fr (corresponding author) b e-mail: ari@joury.eu\nThere now exists a whole chain of tools for computing the properties of general new theories. These compute observables for the model such as the spectrum of masses (there is a difference between the Lagrangian mass parameter and the pole mass), their widths and branching ratios, production cross-sections at colliders, dark matter relic density, dark matter dircect and indirect detection cross-sections, influence on rare decays of mesons, etc etc. The codes most relevant for this work are SARAH [2\u20138], SPheno [9,10] and MicrOmegas [11,12], but there exist many others, such as HiggsSignals [13,14], HiggsBounds [15,16] (and their new successor HiggsTools [17]), Vevacious/Vevacious++ [18] and FlexibleSUSY [19,20]. Thanks to SUSY Les Houches Accords [21,22], the output of these codes is standardised and can be passed from one to the other in the form of text files. However, the actual task of doing this job is generally left to the user, and of course there is then the task of choosing the strategy of exploring the parameter space of models.\nA small number of codes have been established to simplify the task of parameter space exploration. Simple grid or random scans of the model parameters suffice for very small numbers of parameters, especially if the running of the toolchain is fast, but for models with more than one new particle\u2014such as supersymmetric models (where even the CMSSM has five parameters), or the Two Higgs Doublet Model\u2014a more sophisticated approach is required to make sense of the landscape of possibilities. Almost universally the community has decided that the first step in designing such a strategy is to first compute a likelihood function based on the model\u2019s observables, and then find efficient and sophisticated methods of exploring the properties of that function over the parameter space of the model. In its simplest form, this involves assigning a gaussian or log likelihood to each observable, with a given mean and variance that may be related to experimental measurements and uncertainties (e.g. when selecting for observables such as \u03c1), or just\nreflect the uncertainty of the tools (e.g. taking an uncertainty of the Higgs mass of the order a few GeV). These likelihoods are generally combined assuming no correlations, with noteable exceptions being flavio [23], SmodelS [24] or MadAnalysis [25] and currently much work is being done to compute and include such correlations.\nArmed with a toolchain ending in a likelihood, techniques can be used that have been developed in other fields to sample the parameter space with a point density proportional to this. The physics has been completely abstracted into this function so the task can be easily separated. In its simplest guise, the search strategy is often a Markov Chain Monte Carlo (MCMC) based on the Metropolis-Hastings alogrithm; or other groups use more efficient versions based e.g on MultiNest [26]. See e.g. the MasterCode [27,28] or BayesFITS [29] collaborations.\nGAMBIT [30,31], especially with the advent of the GAMBIT Universal Machine [32], attempts to solve all problems for the user: the communication between tools and efficient running of them is solved by creating backends for common tools and taking over the task of interfacing between them. Likelihoods are computed by its dedicated internal routines and combined across all observables; and the user then has the option of certain included search strategies [33], which range from the simple, to toy MCMC, to MultiNest and the bespoke codeDiver [33]. This is an admirable and powerful tool and best for large-scale scans on a computing cluster.\nHowever, one of the key motivations for this paper is to note that making the choice of reconstructing a likelihood is not the only choice that can be made. We will not criticise this approach in general, but to highlight occasions when an alternative can be used. To start with, one problem that arises with a global likelihood function is that for many use cases in High Energy Physics (HEP) they vary sharply around a small or narrow region in high-dimensional parameter spaces. While in principle they are continuous and therefore contain useful information when exploring regions away from the experimentally allowed region, allowing a smart algorithm to guide itself towards more likely points, in practice, and especially when combining many unrelated observables, they are effectively step or delta functions. Thus an approach that relies on the continuity of the function may fail or spend large amounts of time in uninteresting regions. This was already noted in [34], where a different approach, named \u201cMachine Learning Scan,\u201d was proposed and later implemented in the tool xBit [35]. Their approach involves training a neural network directly on observables rather than on a likelihood function and this was then used to select points that would be in the \u201cgood\u201d region. The user can then reasonably rapidly find many points in the allowed region, again with sample density related to the likelihood in that region, reproducing\nthe results of MCMC-based strategies after sampling fewer points.\nAs stated above, in this work we shall be interested in a different goal, namely for users who merely want to find, potentially as a first step, the boundaries of the allowed region. As for the definition of the \u201cboundary\u201d we shall be deliberately vague: this could be the 1\u03c3 or 3\u03c3 limits on the parameters\u2014 potentially computed from a global likelihood\u2014or a much more arbitrary distinction between good and bad points (e.g. applying a criterion that masses should be above some threshold, or the dark matter relic density below the observed limits). Indeed, for many phenomenologists exploring a model for the first time, the most likely point or region is profoundly uninteresting; for many models they are more likely if we set new physics to arise with large masses or very weak couplings with very little difference in the likelihood in the direction of the decoupling regime. We are therefore not interested in reconstructing the whole likelihood function across the parameter space. With traditional MCMC-based sampling, the scanning routine samples the most points in the most likely regions, which may be a large waste of resources; alternative algorithms aiming to infer the posterior likelihood may be an improvement, but may still be more than we need, as we are not interested in the precise value away from the boundaries.\nAs a pertinent example, in our previous work [8] we were interested in finding the heaviest possible dark matter mass for a given model; this is a long way away from the most likely region, and we were interested there in exploring the decision boundary itself. There we adapted a simple MCMC algorithm by biasing the likelihood function to favour heavier masses. Here we shall revisit that computation and show more clearly the effect of such a bias on the sampling distribution. However, the main result of this work is a new algorithm using active learning to train a neural network discriminator and choose points near the decision boundary so as to best explore the limits of the allowed parameter space of a model.\nActive Learning (AL) (for a useful review see [36]) is the general name for machine learning where the algorithm chooses its own inputs. This means finding a measure of the uncertainty of the algorithm about its prediction for any given inputs, and then choosing points where the uncertainty is greatest. The uncertainty in the predictions of neural networks is hugely important and the object of significant study in the literature (see e.g. [37] for a recent review, or [38\u201346] for applications with discussions in the context of High Energy Physics). Proposed methods typically involve training ensembles of neural networks; but also Bayesian Inference; test time data augmentation (varying the dataset, including dropout); and some single network models with components representing both the model and uncertainty (see e.g. the recent proposal [47] which appeared after the first version of this work). A more obvious class of network to use\nis a random forest classifier, where the result is determined based on the accumulating the results of a set of decision trees, and then the uncertainty can be related to the differing predictions within the set in a natural way. Such classifiers have recently been applied in the HEP context [48,49]. In Sect. 2 of this work we note that a neural network classifier does have a natural notion of uncertainty (since the output of a layer of a single neuron passed to a sigmoid function can be interpreted as the probability of a given classification) and we propose an algorithm to use this to efficiently select points in high-dimensional parameter spaces. Since neural networks are much more sophisticated than random forests with unlimited potential for generalisation, for more complicated parameter spaces this should lead to more efficient sampling and discovery, and\u2014despite the extra overhead in training neural networks\u2014save time and give a more accurate description of the allowed range of a model. Indeed, once the model has been trained through active learning, the discriminator can then be used to describe the parameter space.\nThis paper is organised as follows. In Sect. 2 we describe our algorithm and setup. We then apply AL scans to several models of increasing complexity: first, simple toy models (see Sect. 3), then the CMSSM (Sect. 4), the SMSQQ [8] (Sect. 5), and finally the MDGSSM (Sect. 6). We also compare the performance to standard MCMC scans, and to vanilla neural networks and random forest classifiers (RFCs). We conclude with a brief discussion of the possible use cases of AL scans, and perspectives for future work. The code for this work is implemented in a general framework and so will be released publically along with an upcoming publication."
        },
        {
            "heading": "2 Active learning with a neural network",
            "text": "In this section we describe our AL algorithm to train a neural network. We will assume the reader is familiar with neural networks and deep learning; they are becoming ubiquitous in science, see e.g. the reference above or [50] for a comprehensive \u201cliving review\u201d of HEP applications, and [51\u201355] for some examples in the context of the string landscape. However, while it has become widely adopted for experimental, quantum and collider-oriented physics, there have been very few applications so far in BSM theory, with notable exceptions mentioned in the introduction, and so our presentation will be somewhat pedestrian.\nOur approach is based on data that can be simply categorised as \u201cgood\u201d or \u201cbad\u201d. This decision can be made on a given point by an \u201coracle\u201d which in a simple model could just be a formula, whereas in model building applications it will be based on ranges of observables computed using given tools. As mentioned above, such a distinction could be made based on a global likelihood for the model, if one is available via the tools at the user\u2019s disposal; while this is in princi-\nple available within tools such as GAMBIT, this might not be what is desired, perhaps because the user is then limited to a particular toolchain (in our case we are unable to use that tool) or perhaps because the the computation of a global likelihood for a physical theory is debatable and one could question either whether it is meaningful to combine different classes of observations (such as collider, dark matter and flavour data) into a single function, or the actual methodology for doing so. The importance of and challenges in constructing such a function are discussed e.g. in [56], Sect. 4.7. In particular, we will combine observational data with theoretical constraints\u2014from perturbative unitarity\u2014and it is clearly unreasonable to combine them statistically. In our applications, in order to build a new class of search algorithm capable of using the latest versions of the relevant codes we shall take our classification based on simple criteria and consider a point to be \u201cbad\u201d if it fails the test for one observable; this has been argued to be a poor choice for global scans [57] but is at least unambiguous and easy to implement, and should be a good approximation for the cases we are interested in.\nOur first step is to create an initial dataset, typically consisting of random points, and query the oracle to get the result. We then feed these points and the oracle results to a neural network and let it train on those. We describe the parameters of the networks used in the subsequent sections, but they all consist of an input layer feeding to a series of 2 to 5 hidden layers of large (order 100) size connected by ReLU activation functions, and a final ouput layer with one neuron whose output is mapped to a sigmoid function. The implementation is done in pytorch and we use the default weight initialisation.\nDuring training the neural network tries to minimize the loss function; we use the Binary Cross Entropy:\nloss = \u2212 1 N\nN\u2211 i=1 yi log(y\u0302i ) + (1 \u2212 yi ) log(1 \u2212 y\u0302i ), (2.1)\nwhere yi is the outcome of the data, and y\u0302i the fitted outcome by the neural network. In the case of what is studied in this work, y\u0302i is some number between 0 and 1, so we use a sigmoid function for the output of the final layer; yi is either 0 or 1 depending whether a point is bad or good. This loss function is minimized by influencing y\u0302i by changing the weights of the networks.\nAfter initial training, at each further step in the scanning/training cycle, our aim will be to choose K points to pass to the oracle, from L proposed to the discriminator. The results of the oracle\u2019s evaluation of the K points are then used to further train the discriminator. However, we have several choices about both how to propose the L points, and then how to select the batch of K . An initial strategy would be to choose the L points completely randomly (similar to the\nMLS approach [34]); this would help to randomly discover interesting regions, but is very inefficient in high-dimensional parameter spaces. Indeed, if we have n parameters with an interesting region a hypercube of side 0.1 of the parameter range, then a random scan would require O(10n) points to find it; if L = 100,000 and n \u2265 6, then subsequent passes would not be likely to pick a point near the region. As an alternative we can choose points \u201cnear\u201d good points that we have found, somewhat like the jumps during an MCMC (see appendix A for the our toy MCMC algorithm that we shall use as a comparison); however, in that case we do not want to spend time in uninteresting regions away from the boundary near many good points, if the \u201cgood\u201d region is large. Therefore we adopt a hybrid approach of choosing 10% of the L points purely randomly, and the remaining 90% from the vicinity of good points (with coordinates selected from a normal distribution located around the point, with variances specified for each variable).\nFurthermore, if the discriminator is not providing useful information then it is hardly worthwhile choosing points near its decision boundary. Hence the batch of K points contains a further proportion p of the K points selected randomly/within jumps of good points. We choose p based on the training score of the classifier: if q is the proportion of points incorrectly classified after the last training, then\np = 2 \u00d7 min(q, 0.5). In other words, if the discriminator is no better than a coin toss then we need to propose entirely random points.\nIn order to select the pK points to pass to the oracle we also need a prescription for scoring the batch. First we assign an uncertainty score si to each point:\nsi = y\u0302i (1 \u2212 y\u0302i ). (2.2)\nThis score si peaks at y\u0302i = 0.5 and reaches 0.25 there, and falls off to 0 for y\u0302i = 0 and y\u0302i = 1. It is essentially equivalent to the Binary Cross Entropy above for one variable, but simpler to calculate. An initial choice would be to select the pK points which score highest. However, this would potentially lead to clustered points and insufficient diversity. This is also a classic issue in AL, and is solved by introducing a diversity/distance measure [58] to quantify the distance between points in feature space. In random forest appraoches, naively the entropy of the predictions on the sample can be maximised; or the Kullback\u2013Leibler (KL) divergence can be used. In our case this is not possible, but we can instead use a score based on the physical distance in parameter space! We experimented with using a positive score for larger distances, but found that this simply drove the sampling to the boundaries, so instead we introduce a distance measure based on an electrostatic repulsion between points. If xi , x j are the vectors representing the input parameters for two points, and\nd2 = |xi \u2212 x j |2 then the \u201crepulsion\u201d is given by\nr(xi , x j ) = {\u2212 a\na+d2 , d < 0.01 0 d \u2265 0.01 (2.3)\nHere a is some constant that we take to be 0.0001. Of course, this requires that we have rescaled all of our input parameters to have range [0, 1]; the measure is not rescaling/reparameterisation invariant\u2014but that is actually the point, in our applications we have a concept of the separation of points in terms of distances in parameter space. We then start with the point that has the highest score si and remove it from the pool P and put it in the selected set pK . Then we iteratively add points until we have selected pK points as follows:\n1. Compute r j \u2261 \u2211i r(xi , x j ) for {xi } \u2208 pK , {x j } \u2208 P for each point x j 2. Compute the maximum total repulsion rmax = max({r j }) and the standard deviation \u03c3 of the uncertainty scores {si }. 3. Assign to each point a score:\nSi = (1 \u2212 \u03b1)si + \u03b1 ri \u03c3 \u00d7 1 4rmax\n4. Add the point with the highest score Si to the set pK and remove it from the pool P .\nNote that at each step it is not necessary to recompute the whole sum r j , since by storing the old scores we just need to add the scores for the total repulsion from the last point we added to our selected set. The parameter \u03b1 is a diversity weighting that can be adjusted depending on the scan: if we think that the sample contains only one small intersting region we may wish to set \u03b1 small. By using the standard deviation and rmax we make sure that the relative weight of the diversity measure and the neural network uncertainty have a weight that depends only on \u03b1.\nOnce we have our set of K points, we pass these to the oracle, and then train the discriminator on the outputs over a given number of epochs. We also have a choice as to whether to train the discriminator on the whole dataset or just on new points. The cost of training on the full dataset is time, especially once a large number of points have been accrued; however, only training on new data, especially when those points are chosen to be near the boundary, can be deleterious. Hence we train on the full dataset every fixed number of iterations of this procedure.\nThe reader should note that it is also necessary to balance the dataset for training the discriminator: in cases where we have many bad points and few good ones (especially initially) it is advantageous for the discriminator to just classify\neverything as bad. Hence in our training set we make copies of the underrepresented point set so that the discriminator is fed an equal number of good and bad points at each training epoch.\nThe algorithm should thus automatically find interesting points, i.e. points close to the boundaries; we shall see how well it does so in the following sections."
        },
        {
            "heading": "3 Active learning toy models",
            "text": "We begin by illustrating the principle of active learning on a variety of toy models in two dimensions. The results of this are depicted in Figs. 1 and 2. In these figures one can see a fair amount of randomly distributed points, which is intended in order to make sure that no potentially interesting region is missed. Crucially, though, we see how well the algorithm figuratively zooms into the interesting regions to determine where the border between good and bad points runs.\nTo obtain these results, we use a scan with settings as shown in Table 1. On all toy models, 20,000 points and 2000 training steps suffice. The settings work for a variety of shapes, including irregular ones like the bean and the squiggle (see Fig. 1) and ones with holes or multiple pieces, like the beams and the blobs (see Fig. 2). Its ability to correctly identify multiple regions is particularly notable as this sets it somewhat apart from an MCMC scan. The known danger in the latter is that it zooms into one good region and miss-\ning other good regions in the process. This can be mitigated with additional measures. But in active learning, this danger is circumvented automatically, so that parameter spaces with more than one good region are a lot easier to deal with.\nThere are a few exceptions where the settings of our scan needed to be adjusted to account for special features of a model. In the case of the donut, more initial random points were needed to identify the hole; without these random points the algorithm finds the outer border but misses the inner one. In the case of the pizza, one of the borders is not recognised without more initial random points. The straight line of the demicircle is not recognized with lower numbers of random points either.\nFor the donut and the demicircle, the algorithm also needs to train twice as often on the full dataset in order to prevent it from weighting pieces of new data too much. The diversity alpha, which ensures that new points have a certain distance from one another, is also increased for the pizza and the demicircle. This helps identify the pointy corners of these shapes. For the donut, this is not necessary as has no pointy edges.\nIt is plausible, albeit far from proven, that similar settings adjustments may be necessary for models which exhibit holes or pointed edges in their parameter space. Nevertheless, the fact that we were able to correctly identify 7 out of 10 shapes with the exact same settings illustrates the versatility of active learning scans without the need for extensive finetuning.\nAll figures show the results after 20,000 points for the sake of having a nice plot; however, the algorithm accurately identified all borders between good and bad points already after 10,000 points. It\u2019s quite plausible that an interpolation of a grid- or random scan with this amount of points could have led to a similar accuracy. Unless one can press this interpolation into an analytical form, however, the knowledge of the boundaries will be of limited use. In contrast, the active learning scan\u2014or any scan with a neural network, for this point\u2014returns a model in additon to a list of points. This allows us to do two additional things: First, we can query the model about another point, even if we already ran the scan and the point was not included in it. Second, we can refine this model by training it on more points if we so desire. Both these things would not be as straightforward with an interpolation.\nIn the following sections we shall examine what happens with higher-dimensional examples."
        },
        {
            "heading": "4 Active learning in the CMSSM",
            "text": "Having tested our algorithm on toy oracles, we will now apply it to a simple physics example: the valid dark matter parameter space of the constrained MSSM. This is a scenario where the masses and gauge couplings unify at the GUT scale, and leads to only five free parameters called m0,m1/2, A0, tan \u03b2 and sign(\u03bc); it is therefore used as a standard theoretical example, especially for exploration of the valid dark matter parameter space, e.g. [59\u201361]. To make the problem resemble the toy model we will only consider the m0/m1/2 plane, fixing tan \u03b2 = 10, A0 = 0 and sign(\u03bc) = 1. We shall scan over m0,m1/2 \u2208 [100 GeV, 2000 GeV] for both an example MCMC scan and our active learning algorithm (using the same codes of SARAH and MicrOmegas for both to give a fair comparison).\nThis is the same parameter plane considered in [35] and in one example in [61]; this is the reason for selecting this search in this section. As in the former reference we only impose a\nconstraint on the dark matter density and ignore all other constraints. The difference between those two references is in the lattitude allowed; the latter takes h2 = 0.112\u00b10.012 while in the former h2 < 0.2 is considered acceptable. We consider h2 < 0.12 to be a \u201cgood\u201d point and larger densities to be \u201cbad\u201d. In the MCMC scan we use a log likelihood for h2 with mean 0.112 and variance 0.05. This is peaked around the decision boundary and therefore should provide a simple alternative to our AL procedure to find points near it. In the AL scan we classify valid points as those with h2 < 0.12. We use L = 10,000, K = 100 and a variance of 200 in the steps around good points.\nWith those constraints, the parameter plane is not especially interesting: there is an acceptable region at very small m1/2 which eventually leads to a region at m0 > 1250 GeV for m1/2 up to 400 GeV where no points are generated by the spectrum generator because there is no electroweak symmetry breaking. On the left side of the plane at small values of m0 and large m1/2 there is a coannihilation region, but in reference [61] (and as we find with our constraint on h2) this disappears into a region roughly from (m1/2,m0) from (100, 540) GeV, up to (350, 1500) GeV where the LSP is a charged slepton.\nHence in our training we ignore the constraint on the charged LSP and just take the dark matter density from MicrOmegas\u2014which shows the density of the neutralino, overlaying the unphysical region on the plot aferwards. Since the points are not especially physical and the idea is to compare strategies, and with the results of the previous references this can therefore be regarded as a toy model.\nThe results can be seen in Fig. 3. In the MCMC scan, a large amount of points end up in the area where m0 \u2208 [100, 500] and m1/2 \u2208 [750, 2000]. This is inefficient because there are lots of would-be good points in that area, and because the rest of the parameter space remains relatively unexplored in consequence. In contrast, the AL scan clearly favours the regions which are on the border between good and bad. It nonetheless explores the rest of the parameter space in sufficient detail. The comparatively less explored bare band region roughly 10% away from the border region comes about as an artifact from the diversity measure which penalises points for being too close to those already chosen.\nIn these figures, we have also shown a discriminator line for both scans. This was achieved by retraining a neural network with the same settings as the original discriminator from the AL scan on either set of points. To make these two lines comparable to one another, not the original discriminator but the one retrained on the whole set of points was used to produce the line for the AL scan. Such a retraining of networks can of course in general lead to better discriminator performance. This is because the gradual introduction of more interesting points, i.e. points near the boundaries, can lead to distortions of the sort where the first points, which were less interesting, have a larger impact on the network than points which follow. With the points distributed as they are, it is not surprising that the line from the AL points traces more accurately the boundary between good and bad points. If we define \u201cinteresting\u201d points to be those near the decision boundary, so for m1/2 < 250 GeV, or all values to the left of a line from (100, 250) GeV to (350, 1500) GeV, then the\nactive learning scan delivered 42% of its points in this region compared to only 32% for the MCMC. The discriminator shows a few artifact islands in both plots which represent isolated mischaracterised points, which would be eliminated with more data/training.\nIn conclusion, even in a relatively simple and lowdimensional model like the CMSSM, the AL scan produces a superior choice of points than the toy MCMC scan. This has the advantage that one could use fewer points to find the boundaries, and we expect that the trained network on a set of points deliberately selected to locate the boundaries should have superior performance at locating the boundary; this can clearly be seen from the two plots. The cost of this, however, is that training the discriminator after each new set of points takes a certain amount of time. For such a simple scenario where we only take into account the dark matter density, there is no gain from using deep learning. However, if we were to include more constraints\u2014especially collider constraints\u2014we would expect that the time spent training networks to select points would be worth it.\nNevertheless, this example was essentially equivalent to the toy models of the previous section; we included it as a physics example that had also been studied using xBit. In the following we shall consider more interesting, higherdimensional, cases."
        },
        {
            "heading": "5 Parameter space of the SMSQQ model",
            "text": "5.1 Highest singlet mass\nWe now move to explore the SMSQQ, a model with colourful mediators, with an active learning scan. This model is par-\nticularly useful to illustrate how colourful unitarity bounds contribute to a mass limit for dark matter. In our previous paper [8], we had used an iterative approach using several MCMC scans, narrowing in on the region we were interested in. The likelihood function used for the MCMC was artificially weighted in a way that forced it to prioritise points with a higher mass. Doing so, we were able to establish an upper bound for the singlet mass.\nIn this work, we further refined the search for a highest singlet mass by choosing new, updated ranges. The old and new ranges can be seen in Table 4. The results are shown in Table 2. One can see that by limiting to a high but valid range, we were able to gain almost 1 TeV on the highest singlet mass. In this table, we also show the points with the highest singlet mass in an AL and an MCMC scan when we only generated 200k points and used looser ranges. One can see that these two masses are very close together, the one produced by the AL scan even being slightly higher than the one from the MCMC. This is remarkable because, unlike the MCMC, the AL scan is by no means skewed towards higher masses.\n5.2 Performance of the AL scan\nWe then move to the AL and the MCMC scan with 200k points each to explore their properties. The MCMC and AL scans have ranges as provided in Table 4. The settings of the AL scan can be found in Table 3.\nWe find that making an AL scan with many points is not completely straightforward though. Whether a network can handle a load of points depends at least in part on its number of parameters, i.e. its size. Their quantitative performance, and attempts to understand how good the dataset generated by\nthe AL process is, are discussed in in appendix B. An additional difficulty is that the relative scarcity of good points in the ranges we want to explore: In a random scan with these ranges, solely around 3/10,000 points are good. While an MCMC would be able to find at least one good region somewhere despite this scarcity, the AL would not be able to operate on such few points because it does not work with gradients of any kind (as of now). We therefore start with a smaller AL scan with 50k points on a range of which we sus-\npect that it contains more good points on average (see Table 4 for these ranges). We then feed these 50k points, of which some 23% are good, to a larger network. This network then generates the remaining 150k points based on the lessons it has already drawn after training on the first 50k points. These remaining points are on the loose ranges that the MCMC scan also works on.\nThe difference between the first points and last 10k points of the AL scan can be seen in Fig. 4. The first feature that meets the eye is the fact that the model really starts exploring the parameter space, and gets increasingly confident about areas it barely covered in the first 10k points. This is especially apparent in the \u03ba \u2212mS plane. From the summary plots in the last row of plots in the figure it is evident how much exploring the parameter space helps find a point with a higher singlet mass, too. We have cross-checked this phenomenon with AL scans of the same model which differed from the one shown here in various settings or point numbers.\nFigure 5 is similar to the last row in Fig. 4, except that now not the point density but the average discriminator value is shown. One can see that there are regions which the first 10k points already investigated sufficiently (in red), and regions which the last 10k points investigated and the discriminator is quite confident about (in blue). Grey regions are regions where the discriminator values of the first 10k points and the last 10k points are rougly equal, or where only one of the two exists and is close to zero. Note that the random points outside the region are grey with a blue border; however, this is an artifact from the grid interpolation. This can be verified\nFig. 4 Comparison of the first and last 10k points in the AL SMSQQ scan with 200k points. Left: the \u03ba \u2212 mS plane, right: the \u03ba \u2212 plane. The uppermost plot shows the point distribution of the first 10k points, the middle one that of the last 10k points. In these plots, the light blue star indicates the location of the point with the highest singlet mass of that subset of data, the dark blue star the location of the highest singlet mass overall. The lowest plots are a comparison and summary of the\ntwo above: In a 100-by-100 grid, in each bin the number of points in the first and last 10k points are subtracted from one another. The dark blue star indicates the location of the point with the highest singlet mass mS in the whole dataset, the red one that of the first 10k points and the light blue one that of the last 10k. Note how, again, the area of interest broadens with time as the discriminator explores the borders of the regions of good points\nFig. 5 Comparison of the discriminator results in the first versus the last 10k AL-generated points. The dark blue star indicates the location of the point with the highest singlet mass mS in the whole dataset, the red one that of the first 10k points and the light blue one that of the last 10k. Note how the discriminator explores the border as more points get introduced. In each cell of a 100-by-100 bin grid, the average discriminator value of the first 10k points and the last 10k points are taken, then\nsubtracted from one another. Thus, red or orange indicates areas where the discriminator was confident about finding good points among the first 10k points. Dark or light blue indicates areas where it was confident about finding good poitns among the last 10k. Grey areas indicate that the confidence in these points is roughly equal for the first and last 10k points\nby checking top rows of the Fig. 4 plot: Outside the region with good points, the discriminator is always close to zero.\nFigure 6 shows the discriminator values in all of the 200k points. One can clearly see that the discriminator only returns values significantly larger than zero inside the region with good points, with the exception of a few outliers around the fuzzy corners of these regions, and a single outlier at a high singlet mass. Two substructures in these distributions are an interesting byproduct: First, the discriminator returns around the fuzzy edges tend to be a bit blotchy. This could be due to the fact that a portion of points is sampled in the vicinity of already-known good points. Around the fuzzy edges there is a significant proportion of bad points and an uneven distribution of good points. This exacerbates the effect of sampling around good points. Second, there are almost straight lines of points that the discriminator deems good, often close to a hard border. This is in parts due to the fact that more points are sampled close to borders. It turns out, however, that these are areas where there are indeed less bad points. We have verified that such a stark dark discriminator line always appears in regions close to a constraint on the dark matter density [62]. It is therefore conceivable that the discriminator zeroes in on dark matter constraints particularly well, thus reducing the need to generate and evaluate bad points in those areas. This is rendered even more plausible by the fact that dark matter constaints tend to be pretty abrupt in comparison to those imposed by unitarity [8,63\u201377] or vacuum stability [78\u201384]. Gradient-agnostic discriminators like the one we are dealing with here are particularly well-suited for handling such\nconstraints, unlike MCMCs. However, the implementation of a gradient of sorts into the AL scan so that it handles less abrupt constraints similarly well is left for future work.\nFigure 7 shows the distribution of good and bad points and the contour line of where the discriminator thinks the border between good and bad points is. Comparing this to the previous plots, one can see that the discriminator does a pretty good job at identifying the borderline of good points. Marked in blue are points where the discriminator returned a value around 0.5, i.e. where it was not sure whether this point was good or bad. These points are not right on the borderline as they are in our two-dimensional toy models (see Figs. 1 and 2 for comparison). This is mostly due to the higher dimensionality of this model. It is noteworthy as well that there is no extreme amount of blue points in the regions where the discriminator showed a large confidence for good points, as shown in Fig. 6. There are some points; however, especially in the \u03ba \u2212mS plane one can see that their amount is not as large as we would assume had the discriminator not been so confident around the hard upper edge of the good region. One can also see that the point with maximum singlet mass is right on the borderline, as was expected.\n5.3 AL vs. MCMC\nFigure 8 shows a comparison of the points explored by the AL versus those of the MCMC. The first thing to jump to the eye is how much larger the region is that the MCMC covers. This does not speak in favor of the MCMC, how-\nFig. 6 Distribution of the discriminator values interpolated from a 100 \u00d7 100 grid. The dark purple star indicates the location of the point with the highest singlet mass mS . Take note of the substructures in the regions with good points: Along some borders, like e.g. the upper border in the \u03ba \u2212 mS plane, the discriminator is quite sure about finding\ngood points. Along other borders, for example the upper border in the mS \u2212 plane, it is not as sure. On \u201cfuzzy\u201d borders, like e.g. the upper border in themS\u2212mO plane, the discriminator tends to produce blotchy results\u2014as one would expect\never. While it does find 283 good points out of 200k total (versus 70 out of 200k for a random scan), the AL finds a whopping 85,291 good points. Not only is this an enormous difference in numbers; as we established earlier, the good points that the AL scan finds tend to be of a higher quality because they are more often than not close to the borders of the good regions and thus help finding boundaries better. The AL scan did fail to find a very small number of uninteresting isolated good points at \u03ba > 140,000 GeV and mS < 30,000 GeV which require fine-tuning of several other parameters in order to exist; we did not adjust the scan parameters to compensate for this as they correspond to smaller dark matter masses. In our algorithm they require an unlikely jump to discover, and would therefore be found through more data, by adding more random points to explore the rest of the param-\neter space, increasing the diversity measure, or generating more initial random points. We see therefore that the AL scan, while rather robust, can be sensitive to tuning parameters. We deem it feasible to build a similar scan which tunes relevant parameters automatically; however, we leave this for future work.\nOn the whole, the advantages of AL scans are quite apparent from this figure. The AL algorithm automatically found the tip of the region of highest singlet mass, without having to be specifically directed to aim for it (in contrast to our MCMC). In higher-dimensional parameter spaces where good points are scarce, boundaries are difficult to predict, and points take a long time to evaluate with established HEP tools, AL scans therefore propose a much more efficient way to explore parameter spaces.\nFig. 7 Scatter point distribution of good and bad AL-generated points in the SMSQQ. The bright green point indicates the location of the point of the highest singlet massmS . Note how the distribution of points where the discriminator is uncertain (blue points) matches the regions where there are good points and never goes to regions with bad points. Also\nnote how well the discriminator identifies the borderline between good and bad points even in this higher-dimensional model, as visualized here by an interpolation grid with 100 bins on each axis. The discriminator returns a value close to 1 where it suspects a good point, and close to 0 where it suspects a bad point"
        },
        {
            "heading": "6 Learning the Higgs mass in the MDGSSM",
            "text": "6.1 Setup and performance of the AL scan\nIn a final step, we apply the AL scan to the Minimal Dirac Gaugino Supersymmetric Standard Model (MDGSSM). This is a non-minimal supersymmetric scenario with many parameters at low energy. Collider constraints on strongly-coupled particles on this model were considered in [86]. Subsequently in [85] the constraints on electroweak-charged particles were considered from both dark matter and collider searches. As a consequence of the constraints on colourful particles, it is prudent to consider them heavy (of order 2\u20133 TeV) where their exact values do not significantly affect the phenomenology. This leaves six interesting parameters for the low energy theory that greatly affect the masses and phenomenology of the electroweak sector. To place constraints, it is therefore necessary to scan over these parameters. However, it was found in [85] that only a small proportion of parameter choices lead to an acceptable Higgs mass. Therefore a random forest classifier was first trained on a random dataset and used to filter proposed points in a larger MCMC scan over the dark matter parameter space.\nHere we shall examine whether AL can do a better job at this task, by selecting points to train a discriminator that decides whether a given point has a Higgs mass in the range 122\u2013128 GeV. One approach would be to attempt to learn the Higgs mass itself, using the continuous data available to us. However, the SARAH code for the MDGSSM produces a lot of null points, where the result is not a bad Higgs mass but rather that there is no valid spectrum (typically because of\nthe presence of tachyons). This is a classic issue with complicated models that is the manifestation of the little hierarchy problem: the inputs need to be fine-tuned to some extent to be compatible with the assumptions that are forced on it (electroweak expectation value below the scale of the new particles). While ideally the code would automatically implement this fine-tuning so that the user is always guaranteed a spectrum given a perhaps constrained set of inputs, that is beyond the scope of this work. Instead, it becomes immediately clear that a discriminator is a more natural choice, because it can treat both \u201cno spectrum\u201d and \u201cmass outside the given range\u201d as \u201cbad\u201d points. We can then also use our AL algorithm to improve the training of such a discriminator, which can then be used as a \u201cHiggs-gatekeeper\u201d able to predict with relative certainty whether a point has an acceptable Higgs mass and valid spectrum or not.\nWe proceed in a two-step fashion again, putting a smaller network to work on 20k points, then feeding those points to a larger network and generating 100k points in total. (We generate fewer points than for the SMSQQ for the simple reason that points in the MDGSSM take longer to generate.) As before, the larger network requires a smaller learning rate; in this model, a smaller stochastic gradient descent momentum stabilizes the larger network as well. The network parameters are listed in Table 5. The ranges of this scan are chosen such that they include all reference points listed in the literature [85]. They are listed in Table 6. Note that we need not make tighter ranges for the smaller scan in this model because there is a sufficient amount of good points in the chosen ranges (around 7% of randomly selected points are good). Nevertheless, the two-step approach is justified because, as we can\nsee in appendix B, feeding a generous amount of initial training points from the first run to the larger network brings better training results.\nFigure 9 shows the result of this scan in various planes. These distributions do not at all exhibit neat, spatially secluded regions of good points like we saw with the SMSQQ in the previous section. Instead, good and bad points are pretty much all jumbled together, with structures visible mostly in planes containing \u221a 2\u03bbT and very little structure otherwise. This makes it hard to separate good regions from bad ones in a spatial way, as illustrated by the large areas that the blue line in the plots encompasses. This occurs not as a fault of the scan, but as an intrinsic feature of the model: Several variables in the MDGSSM only have an indirect effect on the Higgs mass, such that their influence does not show in\na plot. Crucially, however, a neural network\u2014or an AL scan for that matter\u2014is still able to use these variables as a basis to estimate whether they give rise to a good or bad Higgs mass. This is demonstrated in the section below.\n6.2 AL vs. other networks\nIn the same fashion as with the SMSQQ, we now benchmark our AL scan against various neural networks and RFCs. The results are shown in Table 7. Regularly testing on a dataset of 20k random points, we find that the error rate drops below 5% after around 24k not-null points, i.e. after adding about 7 sets of K new points to the initial dataset of 20k points. This is a substantially better error rate than the one we get with the SMSQQ. We should not overinterpret this, however, because\nthe distributions of good and bad points are vastly different from those in the SMSQQ. Note that the AL scan that was retrained on another 24k points scored slightly more than 5% , which is due to statistics. This tells us that there is some uncertainty to all values shown in this table. Nevertheless, they give us an idea of where we are going with this.\nAs expected, the AL trained on 58k points outperforms the one that trained on 24k points only. This means that new interesting points add value to the overall performance of the network. The neural networks trained on the AL-generated points do similarly well as the AL networks. In the case of the 58k training points, the NN slightly outperforms the corresponding AL network. This highlights the importance of training the AL on its full dataset from time to time; this was not done with this model due to the fact that it requires further finetuning of the network to ensure stability in subsequent training rounds. Regularly retraining the AL network on its full dataset therefore is, as of now, only recommended for simpler models \u2013 we did this for the toy models in Sect. 3 but not in these more advanced ones. It is conceivable, how-\never, that this might be more feasible with an autotuning AL scan, which we have left for future work.\nUnlike the results for the SMSQQ (see Tables 8 and 9 in the appendix), a neural network trained on random points does similarly well to the one trained on AL points. Although all networks shown here handle the impact of indirectly influencing variables well, this shows a limitation to AL scans: when regions of good points are not clearly distinguishable from regions of bad points, the value of generating interesting points is of limited value. It is possible that a set of random points, fed to a sufficiently large neural network, will do a similarly good job as an AL scan in the role of a \u201cHiggsgatekeeper\u201d. There are two reasons why we would advise using an AL scan nevertheless: First, it is not always clear which spatially separated structures the AL might uncover anyways, which would allow it to generate at least some interesting points. As one can see from the fact that the error rates of tests on random points and AL-generated points do differ in a statistically significant way, the AL scan has managed to produce at least somewhat interesting points even with this model. Second, as we can see in appendix B, AL scans might allow us to get away with smaller networks by feeding it piecemeal portions of points at a time. Both reasons imply that AL scans are a more computational- and cost-efficient procedure.\nIn summary, even in fairly complex models like the MDGSSM, AL scans are a cost-efficient way of producing gatekeepers, be it for the Higgs mass or for another observable. This helps us save computing resources which, in other scans, might have been wasted using very effective but nevertheless time-consuming HEP tools. By pretraining a Higgs gatekeeper (which uses just one of the tools) we can save orders of magnitude of executions of the codes in a scan involving the full toolchain. Feeding the \u201cHiggs-gatekeeper\u201d we presented here to an MCMC- or other scan is left for future work. The standout feature we see from the performance analysis on the MDGSSM is that the concept of such gatekeepers from AL scans is quite generalisable and can be used for all kinds of models."
        },
        {
            "heading": "7 Conclusions",
            "text": "In this work we propose a novel approach to explore the parameter spaces of new models. We have demonstrated on a variety of different models that active learning scans provide a cost- and computationally-efficient way to find boundaries and identify areas with good points. We have also shown the use of this approach even with models that do not have a spatially secluded region of good points, as is the case in the MDGSSM. In such cases, AL scans can be used to produce so-called gatekeepers, i.e. networks which are able to predict whether the observables resulting from a set of\nTable 7 Benchmark of AL, RFC, and neural networks on the MDGSSM. There are 58k-63k non-null AL-generated points out of 100k total points, and 62k random points of 100k. The 14k points were selected based on the number of points after which the original AL (with 58k points total) reached an error below 5%. The fact that the 14k\nAL does not do as well is due to statistical fluctuations. One can see that while the RFC trails far behind, the AL and NN do similarly well on the AL-generated points. The NNs trained on random points yield similar results but become less reliable with RFC-misclassified points when the number of training points is large\nClassifier Training points Percent error testing on 62k random points 63k AL points 17k RFC-misclassified AL points\nRFC AL 58k 18.2 31.6 100\nRandom 62k 19.3 32.6 100\nAL AL 58k 3.3 10.4 13.8\nAL 24k 6.7 17.3 22.3\nNN AL 58k 2.9 5.6 9.3\nAL 24k 5.3 14.1 23.0\nRandom 62k 2.4 7.9 13.7\nRandom 24k 5.9 16.4 23.8\nvariables are good or bad with relative certainty. These can be plugged into subsequent MCMC- or other scans to avoid spending unnecessary time and resources evaluating HEP tools on points that are most likely bad anyway.\nIn comparison to MCMC scans, AL scans do a much better job at finding and identifying regions of good points\u2014exactly as we advertised. If the initial training set is not large enough or does not cover the entire parameter space, however, there is a risk of missing potential good regions (see Sect. 5.3). To further ensure finding all good regions, one can finetune AL parameters such as the diversity measure or the proportion of random points in each training set. This shows the importance (and tediousness) of finetuning at this point in time. We think that implementing an AL scan that automatically tunes its own parameters without the need of the user\u2019s intervention is feasible; however, we leave this task for future work.\nIn comparison to RFCs, AL scans vastly outperform in terms of accuracy. Vanilla neural networks can sometimes slightly outperform AL networks when trained on the same points (see appendix B and Sect. 6.2); however, such NNs cannot generate interesting points on their own. To combine the advantages of the two, one could retrain the AL network on its full dataset every so often. At this point in time, this is only feasible for small or simple models at this point, though, because retraining on larger models requires more finetuning of the network parameters. We also find that regular NNs sometimes fail to train with a large amount of training points (see appendix B). This might be due to suboptimal settings to train with such large datasets, or it could be that the network is fundamentally not able to cope with such many points at once. If it is a matter of network parameters, an autotuning AL scan might fix this problem in the future. One last drawback of AL scans is that we need a sufficiently large amount of good points in order to start training(see Sect. 5.2. This could be reduced in the future by introducing gradients to the selection of K from L training points. This, too, is left for future work.\nFinally, the code for this work will be released as part of a general framework for running simple scans along with a future publication. At that point it would also be interesting to consider more sophisticated deep learning and AL approaches such as those in the swyft library [87], and whether they can be used to improve the performance of tasks that we are interested in here. Similarly, other ML techniques could be employed to select parameter spaces, such as autotuning of the parmaters, or using a GAN [88,89], where two neural networks try to fool one another with increasingly interesting points. It would also likely be advantageous to develop an approach which takes advantage of gradients of a continuous likelihood function; if such an object is avaialable, it ought to be more efficient to zoom in on the boundaries by using more information. We leave all these open questions to further work.\nAcknowledgements MDG acknowledges support from the grants \u201cHiggsAutomator\u201d and \u201cDMwithLLPatLHC\u201d of the Agence Nationale de la Recherche (ANR) (ANR-15-CE31-0002), (ANR-21-CE31-0013). We thank Humberto Reyes and Sabine Kraml for collaboration on related topics and helpful discussions.\nData Availability Statement This manuscript has associated data or the data will not be deposited. [Authors\u2019 comment: The code (BSMArt) used for all scans in this paper will be made public and associated with a separate publication.]\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/. Funded by SCOAP3. SCOAP3 supports the goals of the International Year of Basic Sciences for Sustainable Development."
        },
        {
            "heading": "A Comment on our toy MCMC implementation",
            "text": "The Metropolis-Hastings algorithm is the most well-known method of exploring parameter spaces. However, since implementations vary somewhat we spell out here exactly what our codes do. We use a multi-core operation where each core runs a different chain, and we collect each chain together at the end and examine the total set of points. As initialisation we give the range allowed for each input parameter, and information to construct a (possibly biased) likelihood function (see [8] for more details about this for the SMSQQ model).\nEach chain performs the following:\n1. Initialise: pick an initial state x0 at random within the prescribed ranges. If the code fails to produce an output for this point, continue until a valid point is found. This point is the first accepted point. 2. Iterate until either we reach the desired total number of points tested by the code, or desired number of accepted points is found:\n(a) Propose a new point x \u2032. In our algorithm the values for each component of this point are selected randomly with a normal distribution around the last accepted point xn with variances specified at initialisation. We check that the point x \u2032 lies within the presecribed global ranges, and if not, select a different one.\n(b) Compute the likelihood L(x \u2032). If the code fails to produce an output the point is rejected and we return to the previous step. (c) Compute the acceptance probability A(x \u2032, xn) = max ( 1, L(x\n\u2032) L(xn)\n) .\n(d) Accept or reject: i. Generate a random number r \u2208 [0, 1]\nii. If r \u2264 A(x \u2032, xn) then accept the new point: xn+1 = x \u2032.\niii. If r > A(x \u2032, xn), then reject the new point: xn+1 = xn .\nIt is important that we choose jump probabilities from a normal distribution: in this way, the probability of jumping from xn to x\n\u2032 is equal to the probability of a jump in the reverse direction. If this were not true, we would have to reweight our function A(x \u2032, xn) by the ratio of these probabilities. An alternative choice for the jumps could just be completely random within the proscribed ranges, but this would be very inefficient for our purposes: this way, we are able to follow a trail along a curve of increasing probability. If, however, the jump variances are too small, then the code will only converge slowly to a highly likely region, and will not be able to jump out of local maxima.\nB Investigation of theAL-trained network in the SMSQQ model\nIn this appendix we wish to examine the performance of the neural network (NN) from Sect. 5 trained during the AL algorithm, compared to other classifiers trained on on either the same points generated during the AL scan, or just random\npoints. As comparison we shall use two NNs and a random forest classifier (RFC), since such networks are often used in AL.\nIn a final step, we benchmark how well our AL models do against other networks in the SMSQQ. We set out generating two new AL scans with 200k points each, but this time stopping the training only after the error rate has dropped below 0.5% (compared to 5% earlier). We then train two neural networks (NN) and a random forest classifier (RFC) with the exact same settings on these points and a set of random points. We also try to find the amount of points after which the error rate falls below 20%; however, this is already reached after 50k points because the network is larger than the one used to generate these 50k initial training points. It is for this reason that the neural network and the active learning scan, trained on 50k points, do identically well (see Table 8).\nWe also see that the error rates of the RFC, kept on its original settings of 150 estimators, are vastly worse than those of the AL and NN. It is worth mentioning that the NN fails to train on 200k points, regardless of whether they are random or AL-generated. This cannot be solely because of the amount of points, as the AL trains just fine on 200k points. Rather, it either fails because the network parameters are not ideal for getting all points at once, or it fails because being presented with all these points at once is fundamentally too much for a network this size. As is often the case in this area of research, it is difficult to prove the one or the other, though. The AL network does fine, however, by getting the points in a piecemeal fashion, which gives it the chance to adjust its weights gradually.\nAs expected, the NN trained on 50k random points does consistently worse than that trained on 50k AL-generated points because the quality of AL-generated points is higher\nTable 8 Benchmark comparison of RFC, AL, and various neural networks (NN). Each classifier was trained on one set of either 50k or 200k points which are either AL-generated or random. The 50k points that serve the AL as an initial dataset already lead to a very good result; therefore the active learning and the simple neural network are identical in this case. The neural network fails to train on 200k points; it stays at a 50% error throughout the process. The 24k RFC-misclassified points\ncome from the RFC trained on 50k active learning points and tested on 50k active learning points from a separate run. Note that the amount of RFC-misclassified points tallies up with the error rate of 44.0% on 200k test points because, after rebalancing the test data set to contain equal amounts of good and bad points, we\u2019re left with about 24k test points\nClassifier Training points Percent error testing on\n200k random points 200k AL points 24k RFC-misclassified AL points\nRFC AL 200k 18.7 27.2 100\nRandom 200k 4.8 44.0 100\nAL AL 200k 1.6 13.9 21.6\nAL 50k 1.6 28.4 38.5\nNN AL 200k \u2013 \u2013 \u2013\nAL 50k 1.6 28.4 38.5\nRandom 200k \u2013 \u2013 \u2013\nRandom 50k 15.8 42.9 49.1\nin the sense that there are more points in areas where there is a lot to learn. There is no difference between the AL trained on 50k and 200k points when tested only on random points, differences appear when testing with another set of ALgenerated points (which are presumably more interesting). This difference further manifests when tested on only those points which are AL-generated and an RFC, trained on ALgenerated points, misclassified (these points are potentially even more interesting).\nKeeping this in mind, we move to make two more AL scans of 50k points and a smaller network, with settings like the previous AL 50k scan (see Table 3). We find that the error rate of the AL network drops below 20% after about 13k points when tested on a set of 200k random points. The results are shown in Table 9. As before, the NN fails to train with this amount of points. Unsurprisingly, the NN trained on 13k random points does fairly poorly, especially on interesting points. Surprisingly, though, the NN trained on 13k AL-generated points does better than the AL model on these points, particularly for interesting points. This might be happening due to the fact that there are no initial points in the AL, meaning that the first round of K points might be getting an overly large amount of attention. There is a fairly easy fix to this, though: adding a sufficiently large amount of random points as initial training set into the AL should do the trick, like it was necessary for some of the toy models in Sect. 3. It is somewhat surprising that the RFC does comparably well when it is trained and tested on random points. This seeming superiority vanishes quickly, however, when tested on more interesting points. As before, the performance of the AL improves with more points. This is not a monolith, however: The AL trained on 50k points actually does worse than that trained on 13k points, when tested on random points. It is better, however, at judging more interesting points.\nOverall, this benchmarking effort demonstrates two advantages that can be generalised beyond the confines of this particular model: First, AL scans are very good at finding inter-\nesting points and at scoring well when tested on similarly interesting points. This makes them good for finding boundaries in new models. Second, by using the fact that AL scans feed points to their network in a piecemeal fashion, we are able to get away with smaller networks than we would if we somehow found interesting points and subsequently trained a network on these. These two advantages make AL scans particularly useful for working on new, complex, and higherdimensional models in a cost- and compute-efficient way. Nevertheless, there is one obvious limitation to AL scans because many parameters need to be tuned, which requires some know-how. As mentioned earlier, writing an autotuning AL scan is left for future work."
        }
    ],
    "title": "Active learning BSM parameter spaces",
    "year": 2023
}