{
    "abstractText": "The facility location problems (FLPs) are a typical class of NP-hard combinatorial optimization problems, which are widely seen in the supply chain and logistics. Many mathematical and heuristic algorithms have been developed for optimizing the FLP. In addition to the transportation cost, there are usually multiple conflicting objectives in realistic applications. It is therefore desirable to design algorithms that find a set of Pareto solutions efficiently without enormous search cost. In this paper, we consider the multi-objective facility location problem (MO-FLP) that simultaneously minimizes the overall cost and maximizes the system reliability. We develop a learning-based approach to predicting the distribution probability of the entire Pareto set for a given problem. To this end, the MO-FLP is modeled as a bipartite graph optimization problem and two graph neural networks are constructed to learn the implicit graph representation on nodes and edges. The network outputs are then converted into the probability distribution of the Pareto set, from which a set of non-dominated solutions can be sampled nonautoregressively. Experimental results on MO-FLP instances of different scales show that the proposed approach achieves a comparable performance to a widely used multi-objective evolutionary algorithm in terms of the solution quality while significantly reducing the computational cost for search.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shiqing Liu"
        },
        {
            "affiliations": [],
            "name": "Xueming Yan"
        },
        {
            "affiliations": [],
            "name": "Yaochu Jin"
        }
    ],
    "id": "SP:2bd815cbc7ea8c84ed8529d693113a1ea08060e1",
    "references": [
        {
            "authors": [
                "Y. Bengio",
                "A. Lodi",
                "A. Prouvost"
            ],
            "title": "Machine learning for combinatorial optimization: a methodological tour d\u2019horizon",
            "venue": "European Journal of Operational Research 290(2), 405\u2013421",
            "year": 2021
        },
        {
            "authors": [
                "X. Bresson",
                "T. Laurent"
            ],
            "title": "Residual gated graph convnets",
            "venue": "arXiv preprint arXiv:1711.07553",
            "year": 2017
        },
        {
            "authors": [
                "J. Bruna",
                "W. Zaremba",
                "A. Szlam",
                "Y. LeCun"
            ],
            "title": "Spectral networks and locally connected networks on graphs",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR)",
            "year": 2014
        },
        {
            "authors": [
                "Q. Cappart",
                "D. Ch\u00e9telat",
                "E. Khalil",
                "A. Lodi",
                "C. Morris",
                "P. Veli\u010dkovi\u0107"
            ],
            "title": "Combinatorial optimization and reasoning with graph neural networks",
            "venue": "arXiv preprint arXiv:2102.09544",
            "year": 2021
        },
        {
            "authors": [
                "D. Chakrabarty",
                "P. Goyal",
                "R. Krishnaswamy"
            ],
            "title": "The non-uniform k-center problem",
            "venue": "ACM Transactions on Algorithms (TALG) 16(4), 1\u201319",
            "year": 2020
        },
        {
            "authors": [
                "R. Cheng",
                "Y. Jin",
                "M. Olhofer",
                "B. Sendhoff"
            ],
            "title": "A reference vector guided evolutionary algorithm for many-objective optimization",
            "venue": "IEEE Transactions on Evolutionary Computation 20(5), 773\u2013791",
            "year": 2016
        },
        {
            "authors": [
                "K. Deb",
                "A. Pratap",
                "S. Agarwal",
                "T. Meyarivan"
            ],
            "title": "A fast and elitist multiobjective genetic algorithm: NSGA-II",
            "venue": "IEEE Transactions on Evolutionary Computation 6(2), 182\u2013197",
            "year": 2002
        },
        {
            "authors": [
                "M. Defferrard",
                "X. Bresson",
                "P. Vandergheynst"
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "Advances in Neural Information Processing Systems 29",
            "year": 2016
        },
        {
            "authors": [
                "R.Z. Farahani",
                "S. Fallah",
                "R. Ruiz",
                "S. Hosseini",
                "N. Asgari"
            ],
            "title": "Or models in urban service facility location: A critical review of applications and future developments",
            "venue": "European Journal of Operational Research 276(1), 1\u201327",
            "year": 2019
        },
        {
            "authors": [
                "S. Gar\u0107\u0131a",
                "A. Ma\u0155\u0131n"
            ],
            "title": "Covering location problems",
            "venue": "Location science, pp. 93\u2013114. Springer",
            "year": 2015
        },
        {
            "authors": [
                "M. Gasse",
                "D. Ch\u00e9telat",
                "N. Ferroni",
                "L. Charlin",
                "A. Lodi"
            ],
            "title": "Exact combinatorial optimization with graph convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems 32",
            "year": 2019
        },
        {
            "authors": [
                "J. Gilmer",
                "S.S. Schoenholz",
                "P.F. Riley",
                "O. Vinyals",
                "G.E. Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "Proceedings of the International Conference on Machine Learning. pp. 1263\u20131272. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "M. Gori",
                "G. Monfardini",
                "F. Scarselli"
            ],
            "title": "A new model for learning in graph domains",
            "venue": "Proceedings of the IEEE international Joint Conference on Neural Networks. vol. 2, pp. 729\u2013734",
            "year": 2005
        },
        {
            "authors": [
                "T.S. Hale",
                "C.R. Moberg"
            ],
            "title": "Location science research: a review",
            "venue": "Annals of Operations Research 123(1), 21\u201335",
            "year": 2003
        },
        {
            "authors": [
                "W.L. Hamilton"
            ],
            "title": "Graph representation learning",
            "venue": "Synthesis Lectures on Artifical Intelligence and Machine Learning 14(3), 1\u2013159",
            "year": 2020
        },
        {
            "authors": [
                "B. Hudson",
                "Q. Li",
                "M. Malencia",
                "A. Prorok"
            ],
            "title": "Graph neural network guided local search for the traveling salesperson problem",
            "venue": "arXiv preprint arXiv:2110.05291",
            "year": 2021
        },
        {
            "authors": [
                "Y. Jin"
            ],
            "title": "A comprehensive survey of fitness approximation in evolutionary computation",
            "venue": "Soft Computing 9(1), 3\u201312",
            "year": 2005
        },
        {
            "authors": [
                "C.K. Joshi",
                "T. Laurent",
                "X. Bresson"
            ],
            "title": "An efficient graph convolutional network technique for the travelling salesman problem",
            "venue": "arXiv preprint arXiv:1906.01227",
            "year": 2019
        },
        {
            "authors": [
                "C.K. Joshi",
                "T. Laurent",
                "X. Bresson"
            ],
            "title": "On learning paradigms for the travelling salesman problem",
            "venue": "arXiv preprint arXiv:1910.07210",
            "year": 2019
        },
        {
            "authors": [
                "E. Khalil",
                "H. Dai",
                "Y. Zhang",
                "B. Dilkina",
                "L. Song"
            ],
            "title": "Learning combinatorial optimization algorithms over graphs",
            "venue": "Advances in Neural Information Processing Systems 30",
            "year": 2017
        },
        {
            "authors": [
                "J. Kim",
                "T. Kim",
                "S. Kim",
                "C.D. Yoo"
            ],
            "title": "Edge-labeling graph neural network for fewshot learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11\u201320",
            "year": 2019
        },
        {
            "authors": [
                "W. Kool",
                "H. Van Hoof",
                "M. Welling"
            ],
            "title": "Attention, learn to solve routing problems",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR)",
            "year": 2019
        },
        {
            "authors": [
                "G. Laporte",
                "S. Nickel",
                "F. Saldanha-da Gama"
            ],
            "title": "Introduction to location science",
            "venue": "Location science, pp. 1\u201321. Springer",
            "year": 2019
        },
        {
            "authors": [
                "R. Levie",
                "F. Monti",
                "X. Bresson",
                "M.M. Bronstein"
            ],
            "title": "Cayleynets: Graph convolutional neural networks with complex rational spectral filters",
            "venue": "IEEE Transactions on Signal Processing 67(1), 97\u2013109",
            "year": 2018
        },
        {
            "authors": [
                "F. Monti",
                "D. Boscaini",
                "J. Masci",
                "E. Rodola",
                "J. Svoboda",
                "M.M. Bronstein"
            ],
            "title": "Geometric deep learning on graphs and manifolds using mixture model cnns",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5115\u20135124",
            "year": 2017
        },
        {
            "authors": [
                "M. Niepert",
                "M. Ahmed",
                "K. Kutzkov"
            ],
            "title": "Learning convolutional neural networks for graphs",
            "venue": "Proceedings of the International Conference on Machine Learning. pp. 2014\u20132023. PMLR",
            "year": 2016
        },
        {
            "authors": [
                "M. Rausand",
                "A. Hoyland"
            ],
            "title": "System reliability theory: models, statistical methods, and applications, vol",
            "venue": "396. John Wiley & Sons",
            "year": 2003
        },
        {
            "authors": [
                "F. Scarselli",
                "M. Gori",
                "A.C. Tsoi",
                "M. Hagenbuchner",
                "G. Monfardini"
            ],
            "title": "The graph neural network model",
            "venue": "IEEE transactions on neural networks 20(1), 61\u201380",
            "year": 2008
        },
        {
            "authors": [
                "A. Sperduti",
                "A. Starita"
            ],
            "title": "Supervised neural networks for the classification of structures",
            "venue": "IEEE Transactions on Neural Networks 8(3), 714\u2013735",
            "year": 1997
        },
        {
            "authors": [
                "I. Vasilyev",
                "A.V. Ushakov",
                "N. Maltugueva",
                "A. Sforza"
            ],
            "title": "An effective heuristic for large-scale fault-tolerant k-median problem",
            "venue": "Soft Computing 23(9), 2959\u20132967",
            "year": 2019
        },
        {
            "authors": [
                "N. Vesselinova",
                "R. Steinert",
                "D.F. Perez-Ramirez",
                "M. Boman"
            ],
            "title": "Learning combinatorial optimization on graphs: A survey with applications to networking",
            "venue": "IEEE Access 8, 120388\u2013120416",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "W. Wang",
                "Y. Liang",
                "Y. Cai",
                "J. Liu",
                "B. Hooi"
            ],
            "title": "Nodeaug: Semisupervised node classification with data augmentation",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 207\u2013217",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wu",
                "S. Pan",
                "F. Chen",
                "G. Long",
                "C. Zhang",
                "S.Y. Philip"
            ],
            "title": "A comprehensive survey on graph neural networks",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems 32(1), 4\u201324",
            "year": 2020
        },
        {
            "authors": [
                "Q. Zhang",
                "H. Li"
            ],
            "title": "Moea/d: A multiobjective evolutionary algorithm based on decomposition",
            "venue": "IEEE Transactions on Evolutionary Computation 11(6), 712\u2013731",
            "year": 2007
        },
        {
            "authors": [
                "S. Zhang",
                "H. Tong",
                "J. Xu",
                "R. Maciejewski"
            ],
            "title": "Graph convolutional networks: a comprehensive review",
            "venue": "Computational Social Networks 6(1), 1\u201323",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhou",
                "G. Cui",
                "S. Hu",
                "Z. Zhang",
                "C. Yang",
                "Z. Liu",
                "L. Wang",
                "C. Li",
                "M. Sun"
            ],
            "title": "Graph neural networks: A review of methods and applications",
            "venue": "AI Open 1, 57\u201381",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Keywords: Combinatorial optimization \u00b7 Multi-objective optimization \u00b7 Graph neural network."
        },
        {
            "heading": "1 Introduction",
            "text": "Multi-objective combinatorial optimization (MOCO) has received considerable attention over the past few decades due to its wide applications in the realworld. In MOCO, there are multiple conflicting objectives, and it is often nontrivial to optimize them simultaneously [1]. The multi-objective facility location problem (MO-FLP) is a typical NP-hard MOCO problem. It aims to determine an optimal set of facility locations that can satisfy all the customer demands within certain constraints, while minimizing the total costs and maximizing the\nar X\niv :2\n21 0.\n15 22\n0v 1\n[ cs\n.L G\n] 2\n7 O\nsystem reliability. Decisions made in facility location have a long-term impact on numerous operational and logistical strategies and are critical to both private and public firms [14].\nA lot of work has been devoted to developing mathematical methods or handcrafted heuristic algorithms for solving MOCO problems. An intuitive approach is to reduce a multi-objective problem to a single-objective problem by calculating the weighted sum of multiple objectives. However, assigning a suitable weight to each objective introduces an additional hyperparameter optimization problem. Evolutionary algorithms (EAs) have been successful in approximating the Pareto set of MOCOs by maintaining and updating a set of solutions [7,34,6]. However, EAs and other population-based methods often require a large number of function evaluations during the search process, incurring prohibitive computing overhead when the objective functions are expensive to evaluate [17]. Moreover, it is difficult to resue the knowledge about the optimal sets of solutions for other instances of the same problem that have already been solved.\nMost existing work considers MOCO as constrained mixed-integer linear programming, overlooking the highly structured nature of the combinatorial optimisation problems. For example in the facility location problem (FLP), the locations of all facilities and customers can be represented by a set of nodes separately, and the transport overhead is the weight of the edge connecting two nodes from different sets. Generally, permutation-based COPs can be formulated as sequential decision-making tasks on graphs [18], and matching-based COPs can be considered as node and edge classification or prediction tasks on graphs. Therefore, machine learning methods can be used to extract high-dimensional characteristics of the graph-based problems and learn optimal policies to solve COPs instead of relying on handcrafted heuristics [1,31]. Graph neural networks (GNNs) can exploit the message passing scheme to learn the structural information of nodes and edges efficiently according to the graph topology. Consequently, GNNs are well-suited for tackling the MOCO problems [16,11,4]. However, most existing methods focus on solving permutation-based problems and only consider one single objective, neglecting the study of more commonly seen matching-based multi-objective COPs [9].\nIn this paper, we propose a learning-based approach leveraging graph convolutional networks (GCNs) to approximate the Pareto set distribution of the multi-objective facility location problem. The overall framework is shown in Fig. 1. The problem is formulated as a bipartite graph with edge connections between two independent sets of nodes. The model consists of two different residual gated GCNs for node classification and edge prediction tasks, respectively. The model takes bipartite graphs as the input, and transforms the original node and edge features into high-dimensional embeddings. Several residual gated graph convolutional layers are used to learn the structural information from the graph topology and update the embeddings iteratively. The output of the first GCN is a prediction of the probability of each factory being selected in the Pareto optimal solutions. The output of the second GCN is a probabilistic model in the form of an adjacency matrix, denoting the probability of each customer being assigned to each selected factory. The output probability models can be sampled directly to generate a set of Pareto solutions in a one-shot manner. The two networks are\ntrained coordinately by supervised learning. The training data is a large set of MO-FLP instances with various Pareto optimal solutions generated by a multiobjective evolutionary algorithms, e.g., the fast elitism non-dominated sorting genetic algorithm (NSGA-II) [7]. The main contributions of this paper include:\n1. We formulate the MO-FLP as a bipartite graph optimization task and develop a novel learning-based combinatorial optimization method to directly approximate the Pareto set of new instances of the same problem without extra search. 2. We propose an end-to-end probabilistic prediction model based on two GCNs for node and edge predictions, respectively, and train the model with a supervised learning using data generated by a multi-objective evolutionary algorithm. 3. We demonstrate the efficiency of our proposed method for solving MOFLP instances with different scales. Our experimental results show that the learning-based approach can approximate a set of Pareto optimal solutions without additional search, significantly reducing the computational cost compared to population-based algorithms."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Facility Location Problem",
            "text": "FLPs are a typical class of NP-hard combinatorial problems in operations research [23]. FLPs consider choosing an optimal set of facilities among all the potential sites and determines an allocation scheme for all customers, under the constraints that all customer demands must be satisfied by the constructed facilities. A common objective of FLPs is to minimize the total costs, which consist of the transportation cost and the fixed cost.\nFLPs have several variants depending on different constraint settings and objective functions. Each candidate facility may have a limited or unlimited\nmaximum capacity, which classifies the problems into capacitated and uncapacitated facility location problems. When the number of established facilities is fixed to k, there are two variants, namely the k-median problem [30] and the kcenter problem [5]. The k-median problem minimizes the sum of distances from each customer to the closest facility, while the k-center problem minimizes the maximum value of a distance from a customer to the closest facility. Another category of variants is the covering problem, where the problems share a property that a customer can receive the service only if it is located with a certain distance from the nearest facility [10]. The set covering problem aims to find a set of facilities with the minimum number that can satisfy all customers\u2019 demands. The maximum covering problem intends to find a set of facilities with a fixed number to maximize the total demands it covers. From an objective perspective, FLPs and its variants can be divided into single- and multi-objective problems. In addition to the overall costs, multi-objective facility location problems may also include other practical objectives such as the system reliability in logistics, which is quite desirable in real-world applications [9]."
        },
        {
            "heading": "2.2 Graph Representation Learning",
            "text": "Graph-structured data is ubiquitous in daily life. Various kinds of data can be naturally expressed as graphs, such as social relationships, telecommunication networks, chemical molecules, and also combinatorial optimization tasks [33]. Generally, a graph is a collection of objects (nodes) along with a set of interactions (edges) between pairs of them [15]. With the development of machine learning techniques, graph representation learning has attracted increasing attention for in-depth analysis and effective utilization of graph data. Graph representation learning derives node and edge embeddings based on the graph topology for a variety of downstream tasks in machine learning, such as node classification [32], edge prediction [21], and graph clustering. The traditional graph representation methods neither use the node features nor share parameters in the encoder, and are not able to generalize to unseen nodes after training. To alleviate these limitations, graph neural networks are proposed to learn node embeddings in a more explainable way based on the topology and attributes of the input graph [33,36]. Early attempts made by Sperduti and Starita [29] dealt with arbitrary structured data as directed acyclic graphs with recursive neural networks. Gori [13] and Scarselli [28] generalized the recursive neural networks for other types of graph structures and introduced the concept of graph neural networks. With the compelling performance shown by convolutional neural networks in computer vision tasks, a lot of work has been devoted to the transfer of convolution operators to graph domain [35], which can be categorized into spectral-based methods [3,8,24] and spatial-based methods [12,26,25]. GNNs have been practically applied to various domains and achieved encouraging performance [18,16,11]."
        },
        {
            "heading": "2.3 Machine Learning for Combinatorial Optimization on Graphs",
            "text": "NP-hard combinatorial optimization problems are non-trivial to solve, but the instances are relatively easy to generate. In many practical scenarios, decisionmakers need to solve different instances of the same optimization task, where\nthe instances share the same problem structure and only differ in data [1,20,4]. Traditional heuristic methods require extensive expert knowledge and a huge computational cost, and their solutions cannot be transferred to other instances. To address this limitation, recent years have seen a surge in research on machine learning approaches to combinatorial optimization to automate the solution of different instances of combinatorial optimization problems [31,4]. Combinatorial optimization problems often depict a collection of entities and their relations, which are graph-structured data in essence. Therefore, many GNN-based machine learning methods are proposed to solve combinatorial optimization problems [18,19,20]. Kool et al. developed a GNN model in an encoder-decoder architecture based on attention layers, and trained it using REINFORCE for solving routing problems [22]. In addition to solving COPs directly, machine learning techniques can also be used to provide valuable information to operation research algorithms [11]. Although a lot of effort has been devoted to developing ML methods for COPs, most work has focused on single-objective permutationbased problems, and little research on multi-objective matching-based problems has been reported."
        },
        {
            "heading": "3 Problem Formulation",
            "text": "This section begins with a formal definition of the multi-objective uncapacitated facility location problem, which is mathematically formulated as integer linear programming. Subsequently, we discuss how to measure the logistics system reliability in facility location problems.\nMulti-objective uncapacitated facility location. Consider a set of candidate facility locations and a set of demand points (customers) with fixed locations. Every customer has its own quantity of demand to be satisfied. Each potential facility has its own fixed cost for construction, and there are different transportation costs between facilities and customers associated with their distances. Each customer should be served by only one facility, while each facility can serve multiple customers simultaneously. The target is to identify the selected collection of facilities for construction and assign an allocation plan for all customers, in order to minimize the total costs and maximize the system reliability.\nMathematical Formulation. With the minimization of the total costs (including fixed costs and transportation costs) and the maximization of the system reliability as the two objectives, the multi-objective uncapacitated facility location problem can be defined as follows:\nminCtotal = \u2211 i\u2208M fiXi + \u2211 i\u2208M \u2211 j\u2208N qjdijcijYij (1)\nmaxRsys =\n\u2211 i\u2208M \u2211 j\u2208N qjXi [ 1\u2212 FVij ( dij tj )] \u2211\nj\u2208N qj (2)\ns.t. \u2211 i\u2208M Yij = 1 j \u2208 N (3)\nXi, Yij = {0, 1}, Yij \u2264 Xi i \u2208M, j \u2208 N (4)\nIn the MO-FLP, assume there are m candidate facility locations denoted as a set M = {1, 2, . . . ,m}, and n customer points denoted as a set N = {1, 2, . . . , n}. fi \u2208 R+ denotes the fixed cost of constructing the facility at candidate location i (i \u2208M), and qj \u2208 R+ is the demand volume of customer j (j \u2208 N). dij \u2208 R+ and cij \u2208 R+ are the distance and the unit transportation cost between facility i and customer j respectively. Vij denotes the speed for vehicles travelling from facility i to customer j, and FVij (\u00b7) is a statistically regular velocity distribution. tj is the delivery timescale required by customer j.\nThe decision variables areXi \u2208 {0, 1}, which denotes whether facility location j is selected (Xi = 1) or not (Xi = 0), and Yij \u2208 {0, 1}, which denotes whether customer j is served by facility i (Yij = 1) or not (Yij = 0). The two objectives are the minimization of the total costs Ctotal and maximization of the system reliability Rsys.\nLogistics system reliability. Reliability is the probability that a system performs its intended function under the stated conditions [27]. Logistics system reliability is defined as the probability at which the system will successfully provide services to customers under certain conditions and within a specified time. System reliability is a common metric for assessing service levels in modern logistics. The service reliability Rij between factory i and customer j is defined as:\nRij = P (Tij \u2264 tj) = P ( dij Vij \u2264 tj ) = P ( Vij \u2265 dij tj ) = 1\u2212 FVij ( dij tj ) , (5)\nwhere Tij is the time cost for delivery from facility i to customer j, and FVij (\u00b7) is a statistically regular velocity distribution function that usually follows the characteristics of a normal distribution. Based on this, the logistics system reliability of facilities serving multiple customers is calculated by:\nRsys =\n\u2211 i\u2208M \u2211 j\u2208N qjRij\u2211\nj\u2208N qj =\n\u2211 i\u2208M \u2211 j\u2208N qj [ 1\u2212 FVij ( dij tj )] \u2211\nj\u2208N qj (6)"
        },
        {
            "heading": "4 Method",
            "text": "We first convert an MO-FLP instance to a bipartite graph based on its inherent structural properties, and then train a dual GCN-based model to directly output the probabilistic model of the Pareto optimal solutions for the given task. The proposed model consists of two graph convolutional networks GCNnode and GCNedge. GCNnode learns high-dimensional representations of nodes and outputs a probabilistic prediction for each node via a simple multi-layer perceptron (MLP) classifier. Meanwhile, GCNedge learns high-dimensional edge representations and predicts the probability of each edge appearing in the Pareto optimal solutions in the form of an adjacency matrix. The entire model is trained in an end-to-end manner by minimizing the loss between predictions and groundtruth labels. During the test, the output probabilistic model is sampled and\nconverted into a set of non-dominated solutions in a non-autoregressive manner, eliminating the requirement of further search when solving new instances."
        },
        {
            "heading": "4.1 Bipartite Optimization on MO-FLP",
            "text": "An instance of the MO-FLP is transformed into a bipartite graph G = (U, V,E), whose vertices are divided into two independent sets U (including all candidate facilities) and V (including all customers), and these two parts are connected by a set of edges E. Within the graph, each facility in U contains the information of its fixed cost, while each customer in V contains its demand and delivery timescale information. The features of each edge in E contain the Euclidean distance, the transportation cost and the reliability between the facility and the customer it connects. The aim of converting the MO-FLP into a bipartite optimization is to derive high-dimensional embeddings in the latent space through graph representation learning, in order to predict optimal solutions by means of machine learning."
        },
        {
            "heading": "4.2 The Dual GCN-based model",
            "text": "Overall framework. Note that a solution to an MO-FLP problem consists of two parts: X = {Xi | i \u2208 M} and Y = {Yij | i \u2208 M, j \u2208 N}. The decision variable X first determines a subset of facilities to be constructed from all candidate locations, then the decision variable Y identifies the allocation scheme between customers and the selected locations in X. According to the mathematical formulation in Section 3, the calculation of objective Ctotal in Equation 1 requires both X and Y as the decision variables, while the second objective Rsys in Equation 2 is only determined by X. Leveraging the structural properties of the MO-FLP problem discussed above, we propose to predict the two components X and Y by designing two GCN models, one for node prediction and the other for edge prediction.\nAs shown in Fig. 1, the proposed model consists of GCNnode and GCNedge, which take the same bipartite graph as their input. More specifically, GCNnode loads node and edge information and computes H-dimensional representations for each node via iterative graph convolution operators. The last graph convolution layer is followed by a multi-layer perceptron (MLP) classifier, where the updated node embeddings are taken as its inputs to compute the probability of each node being selected in decision variable X. The output of the classifier is represented as a probabilistic model P (X) \u2208 RM , where M is the number of all candidate facilities. Simultaneously, GCNedge takes the same node and edge information as input attributes and derives H-dimensional representations for each edge. A following edge classifier is used to predict the probability of each edge occurring in the Pareto optimal solutions in the form of a heat-map over the adjacency matrix P (Y ) \u2208 RM\u00d7N , where N is the number of all customers. The outputs of the two GCN models indicate the information of X and Y , respectively, which together constitute a prediction of the Pareto optimal solutions. The GCN architectures adopted in the proposed model consist of three building blocks: an embedding block, a graph convolution block and an MLP classifier.\nEmbedding block. The inputs to the embedding block are a set of original node features hn = {~u1, ~u2, . . . , ~uM , ~v1, ~v2, . . . , ~vN} , ~ui \u2208 RFu , ~vj \u2208 RFv\nand edge features he = {~w11, ~w12, . . . , ~wMN} , ~wij \u2208 RFe . M and N are the numbers of facilities and customers, and Fu, Fv and Fe are the numbers of features for different nodes and edges. The outputs of the embedding block are node embeddings n = {~n1, ~n2, . . . , ~nM+N} , ~ni \u2208 RH and edge embeddings e = {~e11, ~e12, . . . , ~eMN} , ~eij \u2208 RH , where H is the dimension of the hidden space.\nFor node embeddings, each feature a \u2208 R is first embedded in a d-dimensional vector ~\u03b1 \u2208 Rd by a learnable linear transformation to get adequate expressive power. Then all the feature vectors are concatenated together to get an embedding ~ni for node i:\n~ni = concat Fn k=1 ( ~\u03b1ki )\n(7)\nSimilarly, the edge embedding ~eij for the edge between node i and node j is the concatenation of all the edge feature vectors:\n~eij = concat Fe k=1 ( ~\u03b2kij ) (8)\nThe selection of node and edge features as the input to the embedding layers depends on the problem\u2019s characteristics, which should have a significant impact on the objective function values. For the MO-FLP problem investigated in this work, there are several candidate node features of the bipartite graph served as input: the node category of the binary classification (i.e., whether a node belongs to the facility set or the customer set), the demand volume of a customer, the fixed cost of constructing a facility, the transportation costs and the reliability of all edges connected to a node. And the input edge features include the adjacency matrix of the bipartite graph, the transportation cost, and the reliability of an edge.\nGraph convolution block. The message passing process mainly occurs in the graph convolution block by stacking several graph convolution layers sequentially. It leverages the structure and properties of the input graph in order to exchange information between neighbors and update node and edge embeddings without changing the connectivity. The graph convolution adopted in our model follows the framework of residual gated graph convolutional neural network [2], where additional edge features and residual gated operators are integrated to introduce heterogeneity in the message passing process.\nIn the graph convolution block, the inputs to the k-th layer are a set of node embeddings nk = { ~nk1 , ~n k 2 , . . . , ~n k M+N } and a set of edge embeddings ek ={\n~ek11, ~e k 12, . . . , ~e k MN } where ~ni, ~eij \u2208 RH . The k-th layer outputs an update set of both node and edge embeddings with the same dimension H. Let ~ekij denote the edge embedding between node i and node j at the kth GCN layer. In the message passing of ~eij (the superscript k is omitted for simplicity), we first gather the associated node embeddings ~ni and ~nj as neighborhood information, and aggregate all the messages as ~e \u2032ij . Then ~e \u2032 ij is passed through a batch normalization layer BN and the rectified linear unit ReLU, to form the updated edge embedding ~ek+1ij together with the original input ~e k ij : :\n~ek+1ij = ~e k ij + ReLU\n( BN ( U~ekij + V ( ~nki + ~n k j ))) , (9)\nwhere U,V \u2208 RH\u00d7H are linear transformations. Suppose ~nki denotes the node embedding of node i at the k-th layer. For updating ~ni, we first calculate the weight vector \u03c9ij of each neighbor node j as:\n\u03c9ij = \u03c3 (~eij)\u2211\nj\u2208Ni \u03c3 (~eij) + \u03b4 , (10)\nwhere Ni denotes all the first-order neighbors of node i. \u03c3 represents the sigmoid function, and \u03b4 > 0 is a small value. Then we gather the neighbor embeddings ~nj (j \u2208 Ni) and define the output of the k-th convolution layer as:\n~nk+1i = ~n k i + ReLU\n( BN ( P~ni + Q \u2211 j\u2208Ni \u03c9ij~nj )) , (11)\nwhere P,Q \u2208 RH\u00d7H are linear transformations. The stack of graph convolution layers enables neighborhood messages to be progressively transferred within the graph. The dimensionality of the embeddings remains the same, however, the representation of each node and edge contains more local information in addition to its original features.\nMLP classifier. The updated representations are taken as inputs to an MLP for classification tasks. For node prediction in GCNnode, we consider ~ni (i \u2208M) as the high-dimensional embedding of node i from the facility set M . For edge prediction in GCNedge, we consider ~eij (i \u2208M, j \u2208 N) as the embedding of edge between facility i and customer j. The probability p\u0302i \u2208 [0, 1] of node i being selected as a constructed facility and the probability p\u0302ij \u2208 [0, 1] of facility i serving customer j are predicted by:\np\u0302i = MLP(~ni), p\u0302ij = MLP(~eij) (12)\nThe weight parameters is trained in an end-to-end manner by minimizing the mean square error between the prediction P\u0302 (X) = {p\u0302i | i \u2208M} and the ground-truth label P (X) = {pi | i \u2208M} via gradient descent methods.\nSince each customer must be served by only one facility, we consider the edge prediction for each customer as a multi-class classification task and train the network parameters by minimizing the cross entropy loss between the prediction P\u0302 (Y ) = {p\u0302ij | i \u2208M, j \u2208 N} and the ground-truth label P (Y ) = {pij | i \u2208M, j \u2208 N}, where P (X) and P (Y ) are both derived from the Pareto optimal solutions.\nEnd-to-end training. The dataset for training and testing the proposed model is generated by a multi-objective evolutionary algorithm. We generate MO-FLP instances of different scales (i.e., various numbers of facility and customer nodes) and approximate their Pareto Fronts via the fast elitist nondominated sorting genetic algorithm (NSGA-II) [7]. Then the probabilistic distributions P (X) and P (Y ) for each instance are derived from a set of Pareto optimal solutions, which serve as ground-truth labels for training and evaluating the proposed model."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Dataset Generation and Hyperparameter Configurations",
            "text": "We consider MO-FLP problems with the following four different configurations: M\u00d7N are set to 20\u00d750, 20\u00d7100, 50\u00d7100, and 50\u00d7200. We randomly generate 1000 instances for each problem scale and optimize them using NSGA-II until convergence to approximate the true Pareto fronts. Then the 1000 instances for each scale are divided into a training dataset, a validation dataset and a test dataset with 700, 200 and 100 pairs of instances and ground-truth labels, respectively. During each training epoch, the training data is split into minibatches with a batch size B = 20 instances. The Adam optimizer is used to train the weights of the proposed model with an initial learning rate of \u03b3 = 0.001 and a maximum number of 300 epochs. Both GCNnode and GCNedge consist of lGCN = 3 graph convolutional layers and lMLP = 3 classification layers. The dimension of the hidden space is set to H = 128 for node and edge embeddings. During the test, we sample 200 solutions from the output prediction for each instance and calculate the hypervolume (HV) and IGD value of the obtained non-dominated solution sets as the performance indicators."
        },
        {
            "heading": "5.2 Experimental Results",
            "text": "There are two variants of our proposed model adopted in the experiments, named Dual A and Dual B with different input features. Dual A takes the node category, the customer demand and the fixed cost of each facility as the original node features, while Dual B also considers the transportation costs and the service reliability of all the edges associated with the node. Both architectures share the same edge features as inputs. To investigate the model performance on MO-FLP with various scales, we compare it to NSGA-II with different numbers of function evaluations (MFEs). We set the number of independent runs to 20 for the compared algorithm, and calculate the mean and standard deviation of HV and IGD values as the performance indicators. The population size is set to 100 for all experiments.\nTable 1 shows the performance of the proposed model compared to NSGA-II in different problem scales. We train the dual GCN-based models with different scales of the problem instances and evaluate them on test datasets. For each test case, 200 solutions are first sampled from the predicted probability distribution and evaluated by the objective functions to get a set of non-dominated solutions. Then we calculate the mean HV and IGD values. Finally, for HV and IGD values associated with each MFE configuration of NSGA-II, we count the percentage of the cases in which the proposed model performs better than NSGA-II out of the 100 test cases. The statistical results in Table 1 indicate that for an unseen instance, by only sampling 200 solutions from the model, the performance of the sampled solution set is already better than NSGA-II with more than 10000 function evaluations.\nFigure 2 depicts the differences between the HV values of the solution sets obtained by NSGA-II and the proposed model for different problem scales with different MFEs. A positive difference means that the proposed model performs\nbetter than NSGA-II. These results reveal that the proposed model outperforms NSGA-II when the MFEs is less than 40000 in most test cases for all scales. In some cases the model performance is even comparable to that of NSGA-II with 50000 MFEs."
        },
        {
            "heading": "5.3 Hyperparameter Sensitive Analysis",
            "text": "We investigate the influence of different graph convolution layers and hidden dimensions on the two performance indicators, HV and IGD. We train the proposed model with different numbers of GCN layers on the 20\u00d720 training dataset, and evaluate them on the test dataset with 100 unseen instances. The statistics\nof HV and IGD values are presented in the form of boxplots in Fig. 3(a). The results demonstrate that the increase in the number of GCN layers has a little impact on the model performance, and lGCN = 3 achieves a slightly better performance. Similarly, we train the proposed model for different dimensions of the hidden space and plot the statistical results of the two indicators in Fig. 3(b). The model performance improves as the hidden dimension increases from 32 to 128. Note that a larger number of hidden layers and more GCN layers also lead to higher computational costs in the training process."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "This paper proposes a learning-based approach to directly predicting a set of non-dominated solutions for multi-objective facility location. We convert the original combinatorial optimization problem into a bipartite graph, and train two GCN models for predicting Pareto optimal solutions for unseen instances by learning the distribution of Pareto optimal solutions in previously solved examples. Experimental results on different scales of MO-FLP instances demonstrate that by only sampling hundreds of solutions, the proposed dual GCN-based approach can achieve a performance comparable to NSGA-II using up to tens of thousands of function evaluations. Future work will focus on improving the model scalability and exploring the heterogeneity of the input graphs in order to generalize the proposed approach to more complex and realistic problems with conflicting objectives and multiple constraints.\nAcknowledgment This work was supported in part by the National Natural Science Foundation of China under Grant No. 62006053, in part by the Program of Science and Technology of Guangzhou under Grant No. 202102020878 and in part by a Ulucu PhD studentship. Y. Jin is funded by an Alexander von Humboldt Professorship for Artificial Intelligence endowed by the German Federal Ministry of Education and Research."
        }
    ],
    "title": "End-to-End Pareto Set Prediction with Graph Neural Networks for Multi-objective Facility Location",
    "year": 2022
}