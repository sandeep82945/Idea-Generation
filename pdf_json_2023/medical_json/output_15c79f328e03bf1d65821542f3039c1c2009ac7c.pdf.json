{
    "abstractText": "Inspired by Fechner\u2019s law, we propose a Fechner multiscale local descriptor (FMLD) for feature extraction and face recognition. Fechner\u2019s law is a well-known law in psychology, which states that a human perception is proportional to the logarithm of the intensity of the corresponding significant differences physical quantity. FMLD uses the significant difference between pixels to simulate the pattern perception of human beings to the changes of surroundings. The first round of feature extraction is performed in two local domains of different sizes to capture the structural features of the facial images, resulting in four facial feature images. In the second round of feature extraction, two binary patterns are used to extract local features on the obtained magnitude and direction feature images, and four corresponding feature maps are output. Finally, all feature maps are fused to form an overall histogram feature. Different from the existing descriptors, the FMLD\u2019s magnitude and direction features are not isolated. They are derived from the \u201cperceived intensity\u201d, thus there is a close relationship between them, which further facilitates the feature representation. In the experiments, we evaluated the performance of FMLD in multiple face databases and compared it with the leading edge approaches. The results show that the proposed FMLD performs well in recognizing images with illumination, pose, expression and occlusion changes. The results also indicate that the feature images produced by FMLD significantly improve the performance of convolutional neural network (CNN), and the combination of FMLD and CNN exhibits better performance than other advanced descriptors.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jinxiang Feng1 \u00b7 Jie Xu"
        },
        {
            "affiliations": [],
            "name": "Yizhi Deng1 \u00b7 Jun Gao"
        },
        {
            "affiliations": [],
            "name": "Jie Xu"
        }
    ],
    "id": "SP:a90cdeae69df730b944a877f8e7641271970a86d",
    "references": [
        {
            "authors": [
                "Y Bi",
                "B Xue",
                "M Zhang"
            ],
            "title": "Multi-objective genetic programming for feature learning in face recognition",
            "venue": "Appl Soft Comput",
            "year": 2021
        },
        {
            "authors": [
                "M Kas",
                "Y El-merabet",
                "Y Ruichek",
                "R Messoussi"
            ],
            "title": "A comprehensive comparative study of handcrafted methods for face recognition LBP-like and non LBP operators",
            "venue": "Multimed Tools Appl",
            "year": 2019
        },
        {
            "authors": [
                "T Ahonen",
                "A Hadid",
                "M Pietikainen"
            ],
            "title": "Face description with local binary patterns: Application to face recognition",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2006
        },
        {
            "authors": [
                "T Ojala",
                "M Pietikainen",
                "D Harwood"
            ],
            "title": "A comparative study of texture measures with classification based on feature distributions",
            "venue": "Pattern Recognit",
            "year": 1996
        },
        {
            "authors": [
                "A Manickam",
                "E Devarasan",
                "G Manogaran",
                "MK Priyan",
                "R Varatharajan",
                "CH Hsu",
                "R Krishnamoorthi"
            ],
            "title": "Score level based latent fingerprint enhancement and matching using SIFT feature",
            "venue": "Multimed Tools Appl",
            "year": 2019
        },
        {
            "authors": [
                "H Bay",
                "A Ess",
                "T Tuytelaars",
                "L Van Gool"
            ],
            "title": "Speeded-Up Robust Features (SURF)",
            "venue": "Comput Vis Image Underst",
            "year": 2008
        },
        {
            "authors": [
                "S Gornale",
                "R Kruthi",
                "Patil"
            ],
            "title": "A (2019) Fusion of local binary pattern and local phase quantization features set for gender classification using fingerprints",
            "venue": "Int J Comput Sci Eng",
            "year": 2019
        },
        {
            "authors": [
                "CJ Liu",
                "H Wechsler"
            ],
            "title": "Gabor feature based classification using the enhanced Fisher linear discriminant model for face recognition",
            "venue": "IEEE Trans Image Process",
            "year": 2002
        },
        {
            "authors": [
                "XY Tan",
                "B Triggs"
            ],
            "title": "Enhanced local texture feature sets for face recognition under difficult lighting conditions. IEEE Trans Image Process 19:1635\u20131650 19697 1 3 A Fechner multiscale local descriptor for face recognition",
            "year": 2010
        },
        {
            "authors": [
                "ZH Guo",
                "L Zhang",
                "D Zhang"
            ],
            "title": "Rotation invariant texture classification using LBP variance (LBPV) with global matching",
            "venue": "Pattern Recognit",
            "year": 2010
        },
        {
            "authors": [
                "ZH Guo",
                "L Zhang",
                "D Zhang"
            ],
            "title": "A completed modeling of local binary pattern operator for texture classification",
            "venue": "IEEE Trans Image Process",
            "year": 2010
        },
        {
            "authors": [
                "SC Liao",
                "XX Zhu",
                "Z Lei",
                "L Zhang",
                "SZ Li"
            ],
            "title": "Learning multi-scale block local binary patterns for face recognition",
            "venue": "In: International Conference on Biometrics,",
            "year": 2007
        },
        {
            "authors": [
                "L Wolf",
                "T Hassner",
                "Y Taigman"
            ],
            "title": "Effective unconstrained face recognition by combining multiple descriptors and learned background statistics",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2011
        },
        {
            "authors": [
                "Y Huang",
                "Y Wang",
                "T Tan"
            ],
            "title": "Combining statistics of geometrical and correlative features for 3D face recognition",
            "venue": "Proceedings of the British Machine Vision Conference",
            "year": 2006
        },
        {
            "authors": [
                "I Al Saidi",
                "M Rziza",
                "J Debayle"
            ],
            "title": "A new LBP variant: corner rhombus shape LBP (CRSLBP)",
            "venue": "J Imaging",
            "year": 2022
        },
        {
            "authors": [
                "D Huang",
                "C Zhu",
                "YH Wang",
                "LM Chen"
            ],
            "title": "HSOG: a novel local image descriptor based on histograms of the second-order gradients",
            "venue": "IEEE Trans Image Process",
            "year": 2014
        },
        {
            "authors": [
                "D Huang",
                "M Ardabilian",
                "YH Wang",
                "LM Chen"
            ],
            "title": "3-D face recognition Using eLBP-based facial description and local feature hybrid matching",
            "venue": "IEEE Trans Inf Forensic Secur",
            "year": 2012
        },
        {
            "authors": [
                "W Zhang",
                "S Shan",
                "G Wen",
                "X Chen",
                "H Zhang"
            ],
            "title": "Local Gabor binary pattern histogram sequence (LGBPHS): a novel non-statistical model for face representation and recognition",
            "venue": "Tenth IEEE International Conference on Computer Vision",
            "year": 2005
        },
        {
            "authors": [
                "Z Zhang",
                "M Wang"
            ],
            "title": "Multi-feature fusion partitioned local binary pattern method for finger vein recognition",
            "year": 2022
        },
        {
            "authors": [
                "BH Zhang",
                "SG Shan",
                "XL Chen",
                "W Gao"
            ],
            "title": "Histogram of Gabor phase patterns (HGPP): a novel object representation approach for face recognition",
            "venue": "IEEE Trans Image Process",
            "year": 2007
        },
        {
            "authors": [
                "GY Zhao",
                "M Pietikainen"
            ],
            "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2007
        },
        {
            "authors": [
                "TR Almaev",
                "MF Valstar"
            ],
            "title": "Local gabor binary patterns from three orthogonal planes for automatic facial expression recognition",
            "venue": "Biannual Conference of the Humaine-Association on Affective Computing and Intelligent Interaction (ACII),",
            "year": 2013
        },
        {
            "authors": [
                "CX Ding",
                "J Choi",
                "DC Tao",
                "LS Davis"
            ],
            "title": "Multi-directional multi-level dual-cross patterns for robust face recognition",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2016
        },
        {
            "authors": [
                "J Sun",
                "S Zhao",
                "Y Yu",
                "X Wang",
                "L Zhou"
            ],
            "title": "Iris recognition based on local circular Gabor filters and multi-scale convolution feature fusion network",
            "venue": "Multimed Tools Appl",
            "year": 2022
        },
        {
            "authors": [
                "A Fathi",
                "P Alirezazadeh",
                "F Abdali-Mohammadi"
            ],
            "title": "2016) A new Global-Gabor-Zernike feature descriptor and its application to face recognition",
            "venue": "J Vis Commun Image Represent",
            "year": 2016
        },
        {
            "authors": [
                "M Hazgui",
                "H Ghazouani",
                "W Barhoumi"
            ],
            "title": "Genetic programming-based fusion of HOG and LBP features for fully automated texture classification",
            "year": 2021
        },
        {
            "authors": [
                "H Ghazouani",
                "W Barhoumi"
            ],
            "title": "Genetic programming-based learning of texture classification descriptors from Local Edge Signature",
            "venue": "Expert Syst Appl",
            "year": 2020
        },
        {
            "authors": [
                "J Chen",
                "S Shan",
                "C He",
                "G Zhao",
                "M Pietikainen",
                "X Chen",
                "W Gao"
            ],
            "title": "WLD: a robust local image descriptor",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2010
        },
        {
            "authors": [
                "Z Xia",
                "C Yuan",
                "R Lv",
                "X Sun",
                "NN Xiong",
                "Y-Q Shi"
            ],
            "title": "A Novel Weber local binary descriptor for fingerprint liveness detection",
            "venue": "IEEE Trans Syst Man Cybernet Syst",
            "year": 2020
        },
        {
            "authors": [
                "W Yang",
                "X Zhang",
                "J Li"
            ],
            "title": "A local multiple patterns feature descriptor for face recognition",
            "year": 2020
        },
        {
            "authors": [
                "OM Parkhi",
                "A Vedaldi",
                "Zisserman"
            ],
            "title": "A (2015) Deep face recognition",
            "venue": "British Machine Vision Conference",
            "year": 2015
        },
        {
            "authors": [
                "Y Wen",
                "K Zhang",
                "Z Li",
                "Y Qiao"
            ],
            "title": "2016) A discriminative feature learning approach for deep face recognition",
            "venue": "European Conference on Computer Vision (ECCV), (Springer International Publishing Ag,",
            "year": 2016
        },
        {
            "authors": [
                "O Gupta",
                "R Dan",
                "R Raskar"
            ],
            "title": "Deep video gesture recognition using illumination invariants",
            "year": 2016
        },
        {
            "authors": [
                "P Ke",
                "M Cai",
                "H Wang",
                "J Chen"
            ],
            "title": "A novel face recognition algorithm based on the combination of LBP and CNN",
            "venue": "IEEE International Conference on Signal Processing (ICSP), (ieee, Beijing, PEOPLES R CHINA),",
            "year": 2018
        },
        {
            "authors": [
                "M Asif",
                "Y Gao",
                "J Zhou"
            ],
            "title": "Face recognition with multi-channel local mesh high-order pattern descriptor and convolutional neural network",
            "venue": "In: International Conference on Digital Image Computing - Techniques and Applications (DICTA),",
            "year": 2018
        },
        {
            "authors": [
                "X Wang",
                "L Qi",
                "Y Tie",
                "EQ Chen",
                "HJ Sun"
            ],
            "title": "Face recognition based on the band fusion of generalized phase spectrum of 2D-FrFT",
            "venue": "In: 8th International Conference on Graphic and Image Processing (ICGIP),",
            "year": 2016
        },
        {
            "authors": [
                "Y Li",
                "W Zheng",
                "Z Cui",
                "T Zhang"
            ],
            "title": "Face recognition based on recurrent regression neural network",
            "year": 2018
        },
        {
            "authors": [
                "L Zhang",
                "J Liu",
                "B Zhanga",
                "D Zhangb",
                "C Zhu"
            ],
            "title": "Deep cascade model-based face recognition: when deep-layered learning meets small data",
            "venue": "IEEE Trans Image Process",
            "year": 2019
        },
        {
            "authors": [
                "YT Luo",
                "LY Zhao",
                "B Zhang",
                "W Jia",
                "F Xue",
                "JT Lu",
                "YH Zhu",
                "BQ Xu"
            ],
            "title": "Local line directional pattern for palmprint recognition",
            "venue": "Pattern Recognit",
            "year": 2016
        },
        {
            "authors": [
                "J Yu",
                "H Liu",
                "X Zheng"
            ],
            "title": "Two-dimensional joint local and nonlocal discriminant analysis-based 2D image feature extraction for deep learning",
            "year": 2019
        },
        {
            "authors": [
                "D Bhattacharjee",
                "H Roy"
            ],
            "title": "Pattern of local gravitational force (PLGF): a novel local image descriptor",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2021
        },
        {
            "authors": [
                "R van den Berg",
                "AH Yoo",
                "WJ Ma"
            ],
            "title": "Fechner\u2019s law in metacognition: a quantitative model of visual working memory confidence",
            "venue": "Psychol Rev 124:197\u2013214",
            "year": 2017
        },
        {
            "authors": [
                "C Maes"
            ],
            "title": "Statistical mechanical foundation of weber-fechner laws",
            "venue": "J Stat Phys",
            "year": 2021
        },
        {
            "authors": [
                "J Wu",
                "F Zhang",
                "J Han",
                "Y Li",
                "Y Sun"
            ],
            "title": "Agent-based automated persuasion with adaptive concessions tuned by emotions",
            "venue": "J Ambient Intell Human Comput",
            "year": 2021
        },
        {
            "authors": [
                "SS Yang",
                "LW Zhang",
                "C Xu",
                "HQ Yu",
                "JQ Fan",
                "ZB Xu"
            ],
            "title": "Massive data clustering by multi-scale psychological observations",
            "venue": "Natl Sci Rev",
            "year": 2022
        },
        {
            "authors": [
                "N Kadakia",
                "T Emonet"
            ],
            "title": "Front-end Weber-Fechner gain control enhances the fidelity of combinatorial odor coding",
            "venue": "Elife",
            "year": 2019
        },
        {
            "authors": [
                "YZ Mao",
                "SY Tian",
                "YM Qin",
                "JZ Han"
            ],
            "title": "A new sensory sweetness definition and sweetness conversion method of five natural sugars, based on the Weber-Fechner Law",
            "venue": "Food Chem",
            "year": 2019
        },
        {
            "authors": [
                "CF Shan",
                "SG Gong",
                "PW McOwan"
            ],
            "title": "Facial expression recognition based on local binary patterns: a comprehensive study",
            "venue": "Image Vis Comput",
            "year": 2009
        },
        {
            "authors": [
                "KC Lee",
                "J Ho",
                "DJ Kriegman"
            ],
            "title": "Acquiring linear subspaces for face recognition under variable lighting",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2005
        },
        {
            "authors": [
                "T Sim",
                "S Baker",
                "M Bsat"
            ],
            "title": "The CMU pose, illumination, and expression database",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2003
        },
        {
            "authors": [
                "PJ Phillips",
                "H Moon",
                "SA Rizvi",
                "PJ Rauss"
            ],
            "title": "The FERET evaluation methodology for face-recognition algorithms",
            "venue": "IEEE Trans Pattern Anal Mach Intell",
            "year": 2000
        },
        {
            "authors": [
                "PJ Phillips"
            ],
            "title": "The facial recognition technology (FERET) database, http:// www. itl. nist. gov/ iad/ human id/ feret/ feret_ master",
            "year": 2004
        },
        {
            "authors": [
                "AM Martinez",
                "R Benavente"
            ],
            "title": "The AR face database, http:// rvl1",
            "venue": "ecn. purdue. edu/ ~aleix/ aleix_ face_ DB",
            "year": 2003
        },
        {
            "authors": [
                "A.M. Martinez",
                "R. Benavente"
            ],
            "title": "The AR Face Database, CVC Technical Report #24",
            "year": 1998
        },
        {
            "authors": [
                "HN Vu",
                "MH Nguyen",
                "C Pham"
            ],
            "title": "Masked face recognition with convolutional neural networks and local binary patterns",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Vol.:(0123456789)\n1\nKeywords Fechner\u2019s law\u00a0\u00b7 Face recognition\u00a0\u00b7 Feature extraction\u00a0\u00b7 Fechner multiscale local descriptor (FMLD)\n* Jie Xu xujie@gdut.edu.cn\n1 Guangdong University of\u00a0Technology, Guangzhou, China 2 Guangzhou Maritime University, Guangzhou, China\n19672 J.\u00a0Feng et al.\n1 3"
        },
        {
            "heading": "1 Introduction",
            "text": "With the continuous development of computer graphics and image processing capabilities, face recognition has become a hot research field recently. It has a wide range of applications in entertainment, authentication, monitoring systems and information security. However, face recognition remains a challenging task due to wide variations in pose, illumination, expression, resolution and occlusion [1]. In face recognition, feature extraction and classifier are two main parts. Many methods have been developed and achieved good face recognition performance [2].\nIn the technology of feature extractions, the methods based on local features are especially widely used. It has been heavily explored by researchers, most of which divide the whole image into multiple parts and analyze the features of each part independently. The local feature extraction methods show better robustness, such as the Local Binary Pattern (LBP) [3, 4], Scale Invariant Feature Transform (SIFT) [5], speeded-up robust feature (SURF) [6], Local Phase Quantization (LPQ) [7] and wavelet Gabor [8] etc. Among the local feature extraction methods, LBP is the most classic one. In LBP, neighboring pixels are compared with the center pixel to produce a binary pattern. Due to its simplicity and effectiveness, a large number of LBP variants have been produced, e.g., Local Ternary Patterns (LTP) [9], LBP variance (LBPV) [10], Completed LBP (CLBP) [11], multi-block LBP (MB-LBP) [12], three-patch LBP (TP-LBP) and four-path LBP (FP-LBP) [13], 3DLBP [14], Corner Rhombus Shape LBP (CRSLBP) [15] and other variants [16\u201319]. Better performance is achieved by ensembling Gabor wavelets and LBP, for example, local Gabor Binary Patterns (LGBP) [20], histogram of Gabor phase pattern (HGPP) [21], LBP-Top [22]. There are also combinations of Gabor and other methods, such as Dural-Cross Patterns (DCP) [23], Local Circular Gabor Filters (LCGF) [24], Global-Gabor-Zernike(GGZ) [25] etc. In addition, the method based on genetic programming (GP)-based, such as combining histograms of oriented gradients (HOG) and LBP [26], automatically generates texture descriptors through genetic programming [27]. These two methods also have good results in feature extraction and classification of textures.\nInspired by Weber\u2019s law, Chen et\u00a0al. proposed Weber local descriptor (WLD) [28]. WLD consists of differential excitation and orientation. The differential excitation component uses the ratio of the local neighborhood pixel difference value to the central pixel to describe local texture change. The orientation component is the spatial information that describes the gray level changes using the ratio of neighborhood gradient. WLD has achieved good texture representation. Xia et\u00a0 al. proposed Weber Local Binary Descriptor (WLBD) [29], which overcame the two defects that WLD could not fully reflect the local gradient orientations and that the positive difference and negative difference canceled each other in the calculation of differential excitation component. It performed well in fingerprint liveness detection. Yang et\u00a0 al. [30] developed a local multiple patterns (LMP) feature descriptor. They modified the Weber\u2019s ratio to include the change of direction, and quantized the modified Weber\u2019s ratio into several intervals to\n19673\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition\ngenerate multiple feature maps to describe different changes. WLD, WLBD and LMP are all descriptors designed based on Weber\u2019s law to imitate human perception of changes in the surrounding environment.\nIn recent years, feature learning in the context of deep learning has attracted extensive attention in the field of machine learning [31]. Many methods based on deep learning have been proposed to describe images [32\u201341]. Representative methods include VGGNet [33], ResNet [34], etc., which require a high number of training samples. On the basis of classical deep learning network, researchers have proposed many improved algorithms. P. Ke et\u00a0 al. proposed a new feature extraction method combining LBP and convolutional neural network (CNN) [35]. Inspired by the law of universal gravitation, Bhattacharjee et\u00a0al. proposed a descriptor based on Pattern of Local Gravitational Force (PLGF) and used PLGF transform images as input to improve the performance of CNN [42].\nFechner\u2019s law is a famous psychological law after Weber\u2019s law [43]. Fechner\u2019s law states that human perception is proportional to the logarithm of the intensity of the corresponding physical quantity, which is an improvement on Weber\u2019s law [44]. Fechner\u2019s law has been applied in areas related to human psychology and behavior [45\u201348]. Inspired by WLD, we imitate the Fechner\u2019s law and propose a feature descriptor, named Fechner multiscale local descriptor (FMLD). FMLD uses the difference between the neighboring pixel and the central pixel as the \u201cphysical quantity\u201d to calculate the \u201cperception intensity\u201d to describe an image. The \u201cperception intensity\u201d is result of FMLD imitating the human beings respond to the salient differences within an image. It is a logarithmic function of \u201cphysical quantity\u201d and matches a human being\u2019s perception well. The \u201cperception intensity\u201d is later divided into the horizontal and vertical components, so as to obtain the magnitude feature and direction feature. Then, to capture more structural information, two scale models were used for coding and four feature maps were obtained. Finally, all feature maps are fused to obtain a histogram feature. The main contributions of this paper can be summarized as follows:\n(1) Propose a feature extractor, named FMLD. FMLD uses the significant difference between pixels to simulate the pattern perception of human beings to the changes of surroundings. (2) FMLD extracts the feature twice. The extracted features are multiscale. The four output feature images are more discriminative, and can represent the face feature well. (3) Unlike most existing descriptors, the features of magnitude and direction in FMLD are derived from the \u201cperceived intensity\u201d, so there is a close relationship between them, which contributes to the feature representation. (4) FMLD has good performance in recognizing the images with the illumination, posture and occlusion changes. (5) The feature image does help in improving the performance of CNN.\nThe rest of the paper is organized as follows: Sect.\u00a0 2 reviews the work. In Sect.\u00a0 3, the proposed FMLD is described in detail. In Sect.\u00a0 4, we conduct\n19674 J.\u00a0Feng et al.\n1 3\nexperiments on multiple face data sets. In Sect.\u00a05, FMLD is combined with CNN for the experiment. Finally, the conclusion is given in Sect.\u00a06."
        },
        {
            "heading": "2 Related work",
            "text": ""
        },
        {
            "heading": "2.1 Local binary pattern (LBP)",
            "text": "LBP has been successfully used in the field of pattern recognition due to its simplicity and good performance. LBP mainly consists of the following steps. First, all the neighboring pixels in the 3 \u00d7 3 window are compared to the center pixel. Pixels with a pixel value greater than or equal to the center pixel are assigned a label 1, and 0 otherwise. Then converts 8-bit binary mode to a decimal number. Finally, after encoding the binary pattern for each pixel position, the LBP transform image is generated. The LBP conversion formula is as follows:\nwhere pc is the central pixel, pi is the ith neighboring pixel, and n is the number of neighboring pixels. Figure\u00a01 illustrates the calculation process of LBP."
        },
        {
            "heading": "2.2 Local multiple patterns (LMP)",
            "text": "Local multiple patterns (LMP) is developed based on the Weber\u2019s law [30]. The LMP process is as follows: (1) The intensity difference and the intensity sum of the neighboring pixels and the central pixel are calculated, so as to obtain the sign and absolute value of the intensity difference respectively; (2) The ratio of the absolute value of the intensity difference to the intensity sum is divided into several intervals to generate multiple feature maps.\nTo modify the ratio in Weber\u2019s law, the following formula is used to find the ratio between the intensity difference and the intensity sum:\nwhere pc is the central pixel, pi is the ith neighboring pixel, and N is the number of neighboring pixels. LMP considers the sign and absolute value of ri , both of which\n(1)LBP = n\u2211 i=1 S ( pi \u2212 pc ) 2i, S(x) =\n{ 1, x \u2265 0\n0, x < 0\n(2)ri = pi \u2212 pc\npi + pc , i = 1, 2,\u2026 ,N\n19675\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition\nare represented in binary mode. Firstly, using a binary bit \u2032i0\u2032 represents the symbol. Then, the absolute value of ri is divided into four intervals and two binary bits are used to represent the binary pattern, as follows:\nwhere 0 \u2264 t1 < t2 < t3 \u2264 1 are the thresholds, and are set the same as LMP[30]: t1 = 0.03, t2 = 0.08, t3 = 0.15. Figure\u00a02 shows the LMP process."
        },
        {
            "heading": "2.3 Weber\u2013Fechner law",
            "text": "Weber-Fechner law is divided into Weber\u2019s law and Fechner\u2019s law. Weber\u2019s law [28] holds that the ratio of the incremental threshold to the background intensity is a constant, which can be expressed as:\n(3)i0 = { 1, ri > t1\n0, else\n(4)i2i1 = \u23a7 \u23aa\u23aa\u23a8\u23aa\u23aa\u23a9 00, \ufffd\ufffdri\ufffd\ufffd \u2264 t1 01, t1 < \ufffd\ufffdri\ufffd\ufffd \u2264 t2 10, t2 < \ufffd\ufffdri\ufffd\ufffd \u2264 t3 11, \ufffd\ufffdri\ufffd\ufffd \u2265 t3\n19676 J.\u00a0Feng et al.\n1 3\nwhere \u0394I is the incremental threshold, I is the original stimulus intensity, and k is called Weber\u2019s ratio.\nFechner\u2019s law [43] is another famous psychological law after Weber\u2019s law, proposed by German psychologist Gustav Theodore Fechner in 1860. Fechner pointed out that above the absolute threshold, there is a logarithmic relationship between subjective sensation intensity and stimulus intensity [44]. All human senses, including vision, hearing, skin sensation, taste, smell, electric shock, etc., are proportional to the common logarithm of the corresponding physical quantity intensity. According to Fechner\u2019s law, human perception is not directly proportional to the intensity of the corresponding physical quantity, but rather to the logarithm of the intensity of the corresponding physical quantity., i.e.,\nwhere S is the sensory intensity, R is the stimulus intensity and K is a constant. That is, if the intensity of the stimulus increases geometrically the intensity of the sensation increases only arithmetically."
        },
        {
            "heading": "3 Fechner multiscale local descriptor",
            "text": ""
        },
        {
            "heading": "3.1 Feature extraction",
            "text": "According to Fechner\u2019s law, the human perception intensity is proportional to the logarithm of the intensity of the corresponding physical quantity. Imitate the \u201cperceived intensity\u201d produced by humans in response to significant differences in their surroundings, and we design a descriptor for image feature extraction, called the Fechner multiscale local descriptor (FMLD).\nFor a given central pixel in face image, the difference between the neighbor pixel and the central pixel is treated as the \u201cphysical quantity\u201d. According to the formula of Fechner\u2019s law, we have\nwhere Ic is central pixel intensity and Ii is the ith neighboring pixel intensity, n is the total number of neighboring pixels, \u2265 1 is a parameter, which is set to 1 in general to ensure Ri \u2265 1 . Take a similar way used in LMP [30], we set K in (6) to:\nWhen ||Ki|| is small, it means that the intensity difference is much smaller than the intensity sum, and there is almost no intensity change between the central pixel and neighboring pixel. When the intensity of the neighboring pixel is larger\n(5) \u0394I\nI = k\n(6)S = K lnR\n(7)Ri = ||Ii \u2212 Ic|| + ,i = 1, 2, ..., n\n(8)Ki = Ii \u2212 Ic\nIi + Ic .\n19677\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition\nthan that of the center pixel, Ki is positive, and when the intensity of the neighboring pixel is small than that of the center pixel, Ki is negative.\nCombining (6), (7) and (8), we have a rewritten Fechner descriptor:\nThe logarithmic transformation of the image is beneficial to extend the low intensity difference values and compress the high intensity difference values for image enhancement.\nWe consider that difference caused by the neighboring pixels in different positions will have different impacts on the central pixel. Therefore, we introduce an Angle i and decompose the amount of \u201cperceived intensity\u201d caused by different neighboring pixels on the central pixel into two components, namely the component in horizontal direction and the component in vertical direction, i.e.,\nwhere i is the angle between the central pixel and the ith neighborhood pixel with respect to the horizontal X-axis. So, the intensity difference caused by the pixel Ii on the central pixel Ic convert to a ternary vector ( \u0302Fix, F\u0302 i y , \ud835\udf03i ). Varying i = 1 to n , where n is the number of neighboring pixels, one has a set of the intensity difference vectors.\nSum all of the horizontal components of the intensity difference vectors, and we can obtain the total horizontal component of the intensity difference vector as follows:\nSimilarly, we have the total vertical component of the intensity difference vector as follows:\nIn order to capture more structure information of image, we use two different scales of grid, i.e., n1 = 8, n2 = 16 . The schematic diagram is shown in Fig.\u00a03.\nNow, combining the total horizontal component and total vertical component together, we calculate the magnitude and the direction of \u201cperception intensity\u201d of central pixel, which is caused by the difference between the surrounding 8 and 16 neighbor pixels and the central pixel, respectively. The magnitude feature of\n(9)Fi = Ii \u2212 Ic Ii + Ic ln (||Ii \u2212 Ic|| + )\n(10)F\u0302ix = I i \u2212 I c\nI i + I c\nln (||Ii \u2212 Ic|| + \ud835\udf06 ) cos \ud835\udf03 i\n(11)F\u0302iy = Ii \u2212 Ic\nIi + Ic ln\n(||Ii \u2212 Ic|| + \ud835\udf06 ) sin \ud835\udf03i\n(12)Fx = n\u2211 i=1 F\u0302i x\n(13)Fy = n\u2211 i=1 F\u0302i y\n1 3\nthe total \u201cperception intensity\u201d caused by the 8 or 16 neighbor pixels on the central pixel is as\nMoreover, arctangent function is used to calculate the total direction as\nThen, each central pixel is further converted into the features vector form of (Mc, c) , which consists of magnitude and direction features.\nTwo sizes of grids, i.e.,n1 = 8, n2 = 16 , have used in this paper. Therefore, the ith pixel as the central pixel has two groups of magnitude and direction feature, i.e., M1 ( ic )\nand 1 ( ic ) , M2 ( ic ) and 2 ( ic ) . Collect the same feature to form an image, and one can have 4 different feature images. To illustrate the difference among 4 feature images more clearly, we showed them in Fig.\u00a04.\nMultiscale descriptor is reflected in two aspects. One is that the first stage of feature extraction adopts two sizes of grids, namely 3 \u00d7 3 and 5 \u00d7 5. This is more conducive to extracting local domain features; The second aspect is that feature extraction was performed twice. In other words, based on the feature map obtained for the first time, the second feature extraction similar to LBP coding is carried out. It will be introduced in Sect.\u00a03.2 below. Although the second extraction used a 3 \u00d7 3 grid on the feature map, it was equivalent to extracting features with a larger range than 3 \u00d7 3 on the original image. This is a bit like a neural network, which can mine deeper features of the sample through multi-layer feature extraction.\n(14)Mc = \u221a F2 x + F2 y .\n(15) c = arctan ( Fy\nFx\n) .\n1 3"
        },
        {
            "heading": "3.2 FMLD binary pattern",
            "text": "Based on the Fechner\u2019s law, FMLD first extracts face features in the two different sizes of local domains, and generate 2 magnitude feature images and 2 direction feature images. To further capture the discriminative structure information, we move a 3 \u00d7 3 grid window on the obtained 4 feature images to generate the pseudorandom sequences. LBP can export the information in the image very quickly in a single scan of the image [49]. Here we use two LBP like binary patterns to further capture the structure information of magnitude and direction feature images respectively, and connect the results to the entire facial image."
        },
        {
            "heading": "3.2.1 FMLD magnitude encoding pattern",
            "text": "FMLD uses a coding method similar to neighbors to center difference binary pattern (NCDBP) [42] to capture the magnitude features. NCDBP sets two thresholds for the difference between neighboring points and the center pixel, one for positive differences and the other for negative differences. The NCDBP encoding method can not only capture face feature information well, but also has stronger robustness to noise.\nThe specific steps of our encoding model are: first find the positive mean using the positive difference between all neighboring points and the center pixel, and then find the negative mean using the negative difference. Finally, the positive mean is used to encode the positive difference values, and the negative mean is used to encode the negative difference values.\nMove a 3 \u00d7 3 grid window on the magnitude feature image of FMLD. Each central value \u2032mc\u2032 has 8 neighboring values \u2032mi\u2032 . The neighbor to center difference is expressed as follows:\n(16)DM = [ m1 \u2212 mc,m2 \u2212 mc,\u2026 ,m8 \u2212 mc ]\n19680 J.\u00a0Feng et al.\n1 3\nThen we can calculate the mean of the positive difference ( D+ mean ) and the mean of the negative difference ( D\u2212\nmean ) from the DM , respectively.\nwhere p1 is the number of DMi \u2265 0 in DM , p2 is the number of DMi < 0 in DM , and p1 + p2 = 8 . Finally, the positive difference values from DM are compared with the D+\nmean , and the negative difference values are compared with the D\u2212 mean . The encoded\ndecimal value is obtained using (19). Figure\u00a05 shows the coding processes."
        },
        {
            "heading": "3.2.2 FMLD direction encoding pattern",
            "text": "FMLD uses an LBP like threshold coding approach to further capture the direction features. More specifically, a 3 \u00d7 3 grid window is moved on the direction feature image of FMLD. Each central value \u2032 c\u2032 has 8 neighboring values \u2032 i\u2032 . The neighbor to center difference values are coded according to a predefined threshold \u2032T \u2032 . Figure\u00a06 shows the coding processes and the formula is as follows:\nwhere T is a threshold, and the optimal value of T will be discussed in Sect.\u00a04.3.\n(17)D+ mean =\n\u2211p1 i=1 DMi\np1\n(18)D\u2212 mean =\n\u2211p2 i=1 DMi\np2\n(19)\nBM = 8\u2211 i=1 S ( mi \u2212 mc ) 2i, S(x) =\n{ 1, x \u2265 0 and x > D+\nmean ,or x < 0 and x < D\u2212 mean\n0, othewrise\n(20)B = 8\u2211 i=1 S ( i \u2212 c ) 2i, S(x) =\n{ 1, x \u2265 T\n0, otherwise\n1 3"
        },
        {
            "heading": "3.3 Feature fusion",
            "text": "FMLD uses two types of patterns to encode the magnitude and direction feature images and generates four feature maps, i.e., BM1 , B 1 , BM2 and B 2 . All the four feature maps are formed into the histograms respectively, and then concatenated into an extended histogram to represent an image as\nwhere HM1 and HM2 are the histograms of BM1 and BM2 respectively, H 1 and H 2 are the histograms of B 1 and B 2 respectively. Before encoding, we divide each feature image into 6 \u00d7 6 regions. Then 256 binary bins are used in the encoding process, and finally the length of HFMLD is: 4 \u00d7 256 \u00d7 36 = 36,864."
        },
        {
            "heading": "3.4 Comparisons with\u00a0LMP",
            "text": "FMLD and LMP are developed based on two different famous psychological laws, i.e., the Weber\u2019s law and the Fechner\u2019s law. Both of them states the human perception to the external things. LMP is proposed based on the Weber\u2019s law and uses the Weber ration produced by the significant differences within an image to imitate the pattern perception of human beings. In Weber\u2019s law, the weber ratio is a liner function of stimulus intensity. The proposed FMLD also focuses the significant differences within an image, and uses them to find the \u201chuman perception\u201d to imitate the pattern perception of human beings. The difference is that \u201chuman perception\u201d is a logarithmic function of stimulus intensity. In fact, the changes in \u201chuman perception\u201d to the outside world cannot increase indefinitely with the increase in the number of external stimuli, but should tend to stabilize at a certain level. The curve of the logarithmic function conforms to this rule and it is more similar to the curve of the sigmoid function, which is always used in Neural Networks algorithms and can matches a human being\u2019s perception well [28].\n(21)HFMLD = [ HM1, H 1, HM2, H 2 ]\n19682 J.\u00a0Feng et al.\n1 3\nFrom this point of view, FMLD using the logarithmic function to represent the images seems to be more reasonable than LMP using the weber ratio.\nMoreover, LMP outputs 3 feature maps to capture more features. The proposed FMLD outputs 4 feature maps, i.e., 2 magnitude feature maps and 2 direction feature maps. Generally, with more feature maps, the descriptor can represent an image better [30]. From this point of view, FMLD seems to have an advantage over LMP.\nIn addition, many feature descriptors prefer to use magnitude features and direction features together to enhance the image presentation, such as LMP, WLD, WLBD, etc. However, we find that the relationship between magnitude features and direction features is loose in these descriptors. For example, if we choose a grid of 3 \u00d7 3 , there are 8 neighbors with 8 different orientations. Different neighbors have different pixel differences from the center pixel. Therefore, it is necessary to match between pairs of amplitude features and orientation features. FMLD overcomes this limitation, and finds a way to integrate two types of features. This way makes FMLD performed well in the following experiments."
        },
        {
            "heading": "3.5 Time complexity",
            "text": "Finally, we analyze the complexity of feature descriptors. Complexity usually refers to speed and storage requirements. The storage requirements of descriptors are mainly determined by the feature dimension, while the dimension of local histogram-based features is the product of the number of labels and the number of blocks.\nIn addition, the speed of computation tends to depend on the number of digits in the binary and the number of binary strings generated (labels), both of which are determined by the number of neighbors of the central pixel. Given an image of m \u00d7 n resolution, the time complexity of all descriptors is O(mn) . For further comparison, the time complexity of LBP, LMP and WLBD are:\nwhere C1 is the constant used to calculate each pixel of the descriptor, and the following formula is the same. The time complexity of WLD is:\nThe time complexity of FMLD is:\nIn the labeling step, since WLD needs to calculate the ratio of Weber and the orientation of pixels on the image, the time cost of WLD is much higher than other methods. Furthermore, since FMLD generates 4 feature maps, the computational cost of FMLD should be higher than that of LBP, WLBD and LMP. The number of feature maps generated by each method is shown in Table\u00a05.\n(22)O1 = C1O(m \u00d7 n)\n(23)O2 = C2O(m \u00d7 n)\n(24)O3 = C3O(m \u00d7 n)\n19683\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition"
        },
        {
            "heading": "4 Experiments",
            "text": "We conduct the experiments on Yale, Expended Yale B, CMU_PIE, FERET and AR face databases to compare the performance of WLD, WLBD, LBP, LMP and PLGF. Since the FMLD is represented by a histogram, the nearest neighbor with Chi-square distance is employed for distance metric. The software used in the experiment is MATLAB R2020a, and the hardware is Intel Core i7-9700\u00a0K CPU @ 3.60\u00a0GHz, 16\u00a0GB RAM, NVIDIA GeForce RTX 2060ti graphics card.\n1 3"
        },
        {
            "heading": "4.1 Experiments on\u00a0extended Yale B and\u00a0CMU_PIE face databases",
            "text": "The proposed FMLD is developed from human\u2019s perception of signal stimulus changes. In this section, we will compare 3 related image descriptors that also mimic the human perception of changes in the surrounding environment, i.e., WLD, WLBD and LMP on Extended Yale B and CMU_PIE face database.\nThe Extended Yale B face database contains 38 individuals with 64 different images per person and captures in different illumination conditions [50]. The image size is 168 \u00d7 192 pixels. Some samples of different illumination variations for a subject on Extended Yale face database are shown in Fig.\u00a07.\nThe CMU_PIE face database was created by researchers at Carnegie Mellon University [51]. The original database contains pictures of 68 people taken under 13 poses, 43 light conditions, and 4 expressions, and the total number of photos is 40,000. In this paper, the Pose05 sub-dataset in the CMU_PIE face database was tested. 49 images of each person were included in one sample set, and the size of the image is 64 \u00d7 64 pixels. Figure\u00a08 shows some images of the same subject from CMU_PIE face database.\nOn Extended Yale B face database, the first l(l = 5,15,20,30,40) samples per person were used for training, while the rest was used as testing set. On CMU_PIE face database, the first l(l = 5,10,15,20,25) samples were taken for training, and the remaining samples were tested. The experimental results are shown in Figs.\u00a09 and 10, respectively.\nObserving these results, we find that: (1) FMLD is superior to other methods irrespective of the variation in the size of the training set in each class on both databases. (2) The data in both databases are highly variable in illumination, but the recognition rates of FMLD are much higher than other methods, especially in the Extended Yale B face database. It suggests that FMLD has strong robustness to illumination changes. (3) WLD, WLBD, LMP and FMLD are all descriptors developed based on the human\u2019s perception of signal stimulus changes. FMLD achieves the best results, which further prove that the way FMLD used to capture the significant\n1 3\ndifferences within an image can represent the image well. This way is effective and feasible.\n1 3"
        },
        {
            "heading": "4.2 Experiments on\u00a0FERET face database",
            "text": "Face Recognition Technology (FERET) engineering was launched by the US Department of Defense\u2019s Counterdrug Technology Transfer Program (CTTP) [52, 53]. The FERET database includes a general face library and universal test standards, in which variations in facial expressions, illuminations, gestures, and ages are all represented by 1400 images of 200 people (7 images each person). Each image in this database is cropped and resized to 80 \u00d7 80 in our experiments. Seven images of one person are shown in Fig.\u00a011.\nOn FERET database, we compared the performance of 3 types of images descriptors. One is the classical local descriptor LBP; the second kind is the descriptors developed from the psychological laws, such as WLD, WLBD, LMP and FMLD; the third kind is the one developed based on the Gravitational Force PLGF [42]. In the first experiment, the first l(l = 1,2,3,4,5,6) images of each class were used for training, and the rest was used as testing samples. The results of different methods are shown in Fig.\u00a0 12. After that, l(l = 1,2,3,4,5,6) images of each class were\n19687\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition\nrandomly chosen as training samples and the rest was used as testing samples. We ran the systems 10 times. The average results are shown in Fig.\u00a013.\nObserving the results in Figs.\u00a012 and 13, we have the following findings: FMLD outperforms other comparisons. Both WLD and WLBD were developed based on the well-known Weber\u2019s law, but they do not perform as well as expected. The possible reason is that their initial recognition target is not a human face, which has a unique symmetric structure, and this structural information is not well captured in their design."
        },
        {
            "heading": "4.3 Experiments on\u00a0Yale face database",
            "text": "The Yale Face database was csreated by Yale University. It consists of 11 images for each of the 15 different people, who were shot with different poses, expressions, and light conditions. Each image is 100 \u00d7 80 pixels in size. Figure\u00a014 shows the original face images of one person in Yale face database.\nIn this part, we conduct some experiments on Yale face database to compare the performance of WLD, WLBD, LBP, LMP and PLGF. At first, the first l(l = 2,3,4,5) images of each class were used as the training set, and the rest was used as testing set. The results of different methods are shown in Table\u00a01. After that, the random experiments were conducted l(l = 2,3,4,5) images of each class were randomly chosen as training samples and the rest was used as testing samples. Ran the systems 10 times. The average results are shown in Table\u00a02.\u00a0We use bold for the recognition rate of the methods that perform best under the same training samples.\n1 3\nIn order to explore the setting of threshold \u2032T \u2032 in Sect.\u00a03.2.2 of our method, we continue the experiments on the Yale face database. The first 3 images of each individual were used for training and the rest for testing. Vary the T value from \u2212 \u22154 to \u22154 with the different intervals to observe the recognition results, which are listed in Table\u00a03. The results were very close, both above 98%. The best results even reached 100%. This shows that the value of T is not too sensitive, but we can find a more suitable T by a similar search method or a cross-validation method. Here the T is set to be \u221516.\nNext, we explore the performance of the algorithms in recognizing the images with obstructions. Different sizes of square black blocks are randomly placed at different locations of the face images of the Yale Face Dataset. The size of the black block is set up with 20 \u00d7 20, 40 \u00d7 40, and 60 \u00d7 60. Some obscured images are shown in Fig.\u00a015. The first 5 original images in each class were used for training, and the remaining images which covered with black block were tested. The test results are reported in Table\u00a04. To show the changes of the recognition rates when the square of black blocks is increasing, we calculate the decrease rate of the recognition rate of each method and show them in Fig.\u00a016. In addition, the number of feature maps of each method is listed in Table\u00a05.\nObserving Tables\u00a01\u20132, we can conclude that (1) the recognition rates of all methods are improving as the training samples increase, with the recognition accuracy of FMLD even reaching 100% in some cases. (2) FMLD performs better than other methods. Moreover, the standard variance of FMLD was small in the 10 random experiments. It indicates that the performance of FMLD is more stable than others. (3) PLGF is suboptimal to FMLD in the recognition accuracy. As can be seen from Table\u00a05, both PLGF and FMLD produce four feature maps, which is more than the other descriptors. Generally, the more feature maps a descriptor produces, the better it performs [30].\nThe results in Table\u00a04 and Fig.\u00a016 show us that: FMLD is robust to occlusion. Obviously, the recognition rate of each method decreases as the masking area increases, but the recognition rate of FMLD is much higher than the other methods. Even when the occlusion area is increased to 60 \u00d7 60, the recognition rate of FMLD remains at 90%. In this case, the occlusion rate reaches 45%. The recognition rate of other most comparisons are less than 30%, except PLGF. It further proves that output of multiple feature maps really can facilitate feature extraction.\n19689\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition"
        },
        {
            "heading": "4.4 Experiments on\u00a0AR face database",
            "text": "The face images of the AR face database reach the number of 4000, grabbed from 126 people (70 men and 56 women) [54, 55]. Each individual consists of 26 images. These images contain significant variations in illumination, facial expressions and occlusions. The images of most persons were captured in two different temporal sequences at an interval of two weeks. In our experiments, the images with the full face of the 120 individuals were used. The facial region in the image is manually cropped and normalized with 50 \u00d7 40 resolution [30]. Each sequence contains 13 images per person, including one neutral face, three different expressions such as smile, anger and screaming, three different illumination conditions (left light on, right one on and all side lights on), three different wearing sunglasses under different illuminations and three wearing scarf under different illuminations. The two sequences are shown in Fig.\u00a017.\nMethod Training image number\n2 3 4 5\nMethod Training image number 2 3 4 5\n1 3\nIn order to explore the performance of FMLD in recognizing the images with the expression and illumination changes, we performed following experiments on AR face database.\nFirst, we conduct experiments on a subset consisting of first 4 expression images of two sequences to evaluate the performance of FMLD for expression recognition. Each time we leave 2 same expression images out to form the training set (720 images in all), and the removed 2 expression images for testing (240 images in all). The results are shown in Table\u00a06. FMLD has achieved a high recognition rate for all facial expressions, and its average recognition rate reaches 99.37 percent.\nThen, the experiments are conducted on the data consisting of the 1st, 5th,6th and 7th images of two sequences to evaluate the performance of FMLD in recognizing the images with the illumination changes. Each time 2 same illumination images were left out to form the training set (720 images in all), and the removed 2 illumination images for testing (240 images in all). The results are shown in Table\u00a07.\nMethod Different occlusion areas\n0 \u00d7 0 20 \u00d7 20 40 \u00d7 40 60 \u00d7 60\n1 3\nFMLD also achieved a high recognition rate for all illumination changes, and its average recognition rate reaches 99.86 percent.\nFinally, the five most relevant descriptors were compared with FMLD in the AR face database to evaluate their performance in recognizing images under expression, illumination, and occlusion by wearing sunglasses or scarves. In the two sequences of the AR face database, two neutral images for each person, i.e., the first image, are used as training samples, and the remaining three images of different cases are used for testing.\nThe results of different methods are shown in Fig.\u00a0 18. In Fig.\u00a0 18, FMLD performed well in representing images with expressions and illumination changes. However, the FMLD decreased when the facial covering was sunglasses or a scarf. This is justified because the large area of occlusion makes fewer features obtainable. Even so, the recognition rate of FMLD is still one of the highest among all comparisons."
        },
        {
            "heading": "4.5 Summary",
            "text": "In this chapter, we conduct experiments on Extended Yale B face database, CMU_ PIE face database, FERET face database, Yale face database and AR face database. The experimental results prove that FMLD has strong robustness to illumination changes and facial occlusion, and also has good effect on expression recognition."
        },
        {
            "heading": "5 Performance evaluation with\u00a0convolutional neural networks",
            "text": "In this section, the Convolutional Neural Network (CNN): Resnet 18 was used to evaluate the performance of descriptors in feature description. The feature images produced by WLD, LBP, LMP, PLGF and FMLD were used to train in CNN. The number of output feature images varies for different descriptors. In this way, the operator with more feature images has more training samples than the operator with fewer feature images.\nSet up the learning networks as \u201cWLD + CNN\u201d, \u201cLBP + CNN\u201d, \u201cLMP + CNN\u201d, \u201cPLGF + CNN\u201d and \u201cFMLD + CNN\u201d. In addition, CNN was conducted on the original images, named as \u201cOriginal pixel + CNN\u201d learning network. The corresponding experiments are conducted on Extended Yale B face database, CASIA-FaceV5 database and COMASK20 dataset.\n19693\n1 3\nA Fechner multiscale local descriptor for\u00a0face recognition"
        },
        {
            "heading": "5.1 Experiments on\u00a0extended yale B face database",
            "text": "Two experiments were conducted on the Extended Yale B face database. In the first experiment, 30% of the images were randomly selected for testing, and the rest was tested. The average results were obtained by running 10 times each learning system, as shown in Table\u00a08. In the second experiment, we used a disk filter to blur the images and the effect images are shown in the Fig.\u00a019. The filtered images were used for training and testing, and the experimental settings were the same as the first experiment. The results are shown in Table\u00a08.\n\"FMLD + CNN\" has achieved the best performance among all the combinations. FMLD can produce 4 more feature images for each original image. In this way, the number of training samples becomes four times larger. In fact, CNN requires a large number of training samples to achieve good accuracy. \"PLGF + CNN\" also produce four times of training samples, but its recognition rate is not as good as \"FMLD + CNN\". Among all the comparisons, LBP is the worse one. LBP can only output one feature image for each image. The limited \"training samples\" lead to worse results.\nWhen blurred noise is added to the face image, the feature information of the face is greatly reduced, which makes the image difficult to recognize. The recognition rate of all methods decreases a lot, but FMLD is affected the least and remains the best method. This indicates that FMLD has good robustness to fuzzy noise."
        },
        {
            "heading": "5.2 Experiments on\u00a0COMASK20 dataset and\u00a0CASIA\u2011FaceV5 database",
            "text": "Nowadays, people need to wear masks because of the COVID-19 pandemic. Therefore, whether the face can be recognized after wearing a mask is also a\n19694 J.\u00a0Feng et al.\n1 3\nproblem that needs to be considered. We evaluated the performance of the proposed descriptor on COMASK20 dataset [56]. Each type of images is taken from 3 to 5\u00a0s of video taken by each person. In order to simulate the registration and prediction situations that might occur in real life, videos are made under different backgrounds, head scales and lighting conditions. This dataset\u00a0contains 2824 facial images of 312 subjects. Some samples of a subject on COMASK20 dataset are shown in Fig.\u00a020.\nCASIA Face Image Database Version 5.0 (or CASIA-FaceV5) contains 500 subjects with 5 different facial images per person, a total of 2500 color images, and the image size is 640 \u00d7 480 [57]. We convert the images into grayscale images, as shown in Fig.\u00a021.\nIn the experiments on COMASK20 dataset, 70% of the images were randomly selected for training, and the rest was tested. For the CASIA-FaceV5 database, 60% of the images were randomly selected for training, and the rest was tested. We run each system 10 times and report the average results in Table\u00a09. In Table\u00a09, the experimental results on the COMASK20 database show that the FMLD + CNN can maintain a good recognition performance for faces wearing masks. It shows that the proposed method can extract the effective feature from the images with large occlusion areas.\nAlthough the CASIA-FaceV5 database contains 2500 images, there are only 5 images per person, so it is still not easy to get a good accuracy rate. FMLD + CNN obtained a recognition rate of 92.67% in this database, which is the highest one among all the combinations. This means that the feature images generated by FMLD contain more discriminative features than others, and are more suitable for CNN.\nTable 8 Average recognition rates (%) of different methods on Extended Yale B face database\nBold values indicate the recognition rate of the methods that perform best under the same training samples\nMethod Original pixel + CNN WLD + CNN LBP + CNN LMP + CNN PLGF + CNN FMLD + CNN\nYale B 89.05 87.58 80.78 93.07 94.63 98.37 Yale B(noise) 65.06 39.31 58.23 44.28 76.70 84.53\nFig.19 Some samples from the Extended Yale B face database with blurred noise\n1 3"
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, inspired by Fechner\u2019s law, we propose a feature descriptor for face recognition, the Fechner Multiscale Local Descriptor (FMLD). FMLD exploits significant differences within images to mimic human pattern perception to the external changes. After performing two rounds of feature extraction, we obtained 4 face feature images with multi-scale features. These feature images contain more discriminative features. Many experiments have been conducted on Extended Yale B face database, CMU_PIE face database, FERET face database, Yale face database and AR face database. The results show that FMLD performs better than other descriptors when dealing with various complex facial changes, including illumination changes, occlusion, and expression changes. Moreover, the image feature maps of the images in the COMASK20 dataset and CASIA-FaceV5 dataset are used to train the CNN. The results indicate that the feature maps obtained by FMLD can greatly\nTable 9 Average recognition rates (%) for different methods on COMASK20 dataset and CASIA-FaceV5 database\nBold values indicate the recognition rate of the methods that perform best under the same training samples\nMethod Original pixel + CNN WLD + CNN LBP + CNN LMP + CNN PLGF + CNN FMLD + CNN\nCOMASK20 94.52 88.24 92.16 93.38 95.83 96.08 CAISA 73.48 50.62 39.48 82.27 91.90 92.67\n19696 J.\u00a0Feng et al.\n1 3\nimprove the performance of CNN. This further proves that it is feasible to use this \u201cperceived intensity\u201d to represent features.\nOur proposed descriptor FMLD still has areas for optimization, including parameter reduction and runtime. In addition, we will try to verify the feasibility of FMLD in other scenarios, such as texture, iris and medical imaging. What\u2019s more, I hope to develop more interesting feature extractors using knowledge from more different research areas.\nAuthor contributions JF wrote the main part of the manuscript and performed the experiments. JX made more suggestions on the model of the algorithm. YD and JG participated in the discussion and provided some suggestions that helped a lot in finding the optimal solution of the model. All authors reviewed the manuscript.\nFunding This work was partially supported by the National Natural Science Foundation of China under Grant no. 61773128.\nData availability The datasets used and analyzed during the current study are available from the corresponding author on reasonable request.\nDeclarations\nConflict of interest The authors declared no potential conflicts of interest with respect to the research, author- ship, and publication of this article.\nEthical approval The written informed consent for publication of this paper was approved by Guangdong University of Technology, Guangzhou Maritime University and all the authors."
        }
    ],
    "title": "A Fechner multiscale local descriptor for face recognition",
    "year": 2023
}