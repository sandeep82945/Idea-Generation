{
    "abstractText": "It is known that the multiplication of an N \u00d7M matrix with an M \u00d7P matrix can be performed using fewer multiplications than what the naive NMP approach suggests. The most famous instance of this is Strassen\u2019s algorithm for multiplying 2 \u00d7 2 matrices in 7 instead of 8 multiplications. This gives rise to the constraint satisfaction problem of fast matrix multiplication, where a set of R < NMP multiplication terms must be chosen and combined such that they satisfy correctness constraints on the output matrix. Despite its highly combinatorial nature, this problem has not been exhaustively examined from that perspective, as evidenced for example by the recent deep reinforcement learning approach of AlphaTensor. In this work, we propose a simple yet novel Constraint Programming approach to find algorithms for fast matrix multiplication or provide proof of infeasibility otherwise. We propose a set of symmetry-breaking constraints and valid inequalities that are particularly helpful in proving infeasibility. On the feasible side, we find that exploiting solver performance variability in conjunction with a sparsity-based problem decomposition enables finding solutions for larger (feasible) instances of fast matrix multiplication. Our experimental results using CP Optimizer demonstrate that we can find fast matrix multiplication algorithms for matrices up to 3 \u00d7 3 with R = 23 in a short amount of time. 2012 ACM Subject Classification Mathematics of computing",
    "authors": [
        {
            "affiliations": [],
            "name": "Arnaud Deza"
        },
        {
            "affiliations": [],
            "name": "Chang Liu"
        },
        {
            "affiliations": [],
            "name": "Pashootan Vaezipoor"
        },
        {
            "affiliations": [],
            "name": "Elias B. Khalil"
        }
    ],
    "id": "SP:343e4ee61de508fbc3b01a2d30ab4a902000ca25",
    "references": [
        {
            "authors": [
                "Austin R. Benson",
                "Grey Ballard"
            ],
            "title": "A framework for practical parallel fast matrix multiplication",
            "venue": "ACM SIGPLAN Notices,",
            "year": 2015
        },
        {
            "authors": [
                "Markus Blaser"
            ],
            "title": "On the complexity of the multiplication of matrices of small formats",
            "venue": "Journal of Complexity,",
            "year": 2003
        },
        {
            "authors": [
                "Markus Bl\u00e4ser"
            ],
            "title": "Fast Matrix Multiplication. Number 5 in Graduate Surveys",
            "venue": "Theory of Computing Library,",
            "year": 2013
        },
        {
            "authors": [
                "Roger W Brockett",
                "David Dobkin"
            ],
            "title": "On the optimal evaluation of a set of bilinear forms",
            "venue": "In Proceedings of the fifth annual ACM symposium on Theory of computing,",
            "year": 1973
        },
        {
            "authors": [
                "Hans F de Groote"
            ],
            "title": "On varieties of optimal algorithms for the computation of bilinear mappings ii. optimal algorithms for 2\u00d7 2-matrix multiplication",
            "venue": "Theoretical Computer Science,",
            "year": 1978
        },
        {
            "authors": [
                "Alhussein Fawzi",
                "Matej Balog",
                "Aja Huang",
                "Thomas Hubert",
                "Bernardino Romera-Paredes",
                "Mohammadamin Barekatain",
                "Alexander Novikov",
                "Francisco J R Ruiz",
                "Julian Schrittwieser",
                "Grzegorz Swirszcz"
            ],
            "title": "Discovering faster matrix multiplication algorithms with reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Matteo Fischetti",
                "Michele Monaci"
            ],
            "title": "Exploiting erraticism in search",
            "venue": "Operations Research,",
            "year": 2014
        },
        {
            "authors": [
                "Ambros Gleixner",
                "Gregor Hendel",
                "Gerald Gamrath",
                "Tobias Achterberg",
                "Michael Bastubbe",
                "Timo Berthold",
                "Philipp Christophel",
                "Kati Jarck",
                "Thorsten Koch",
                "Jeff Linderoth"
            ],
            "title": "Miplib 2017: data-driven compilation of the 6th mixed-integer programming library",
            "venue": "Mathematical Programming Computation,",
            "year": 2021
        },
        {
            "authors": [
                "Andrea Lodi",
                "Andrea Tramontani"
            ],
            "title": "Performance variability in mixed-integer programming",
            "venue": "In Theory driven by influential applications,",
            "year": 2013
        },
        {
            "authors": [
                "Alexey V. Smirnov"
            ],
            "title": "The bilinear complexity and practical algorithms for matrix multiplication",
            "venue": "Computational Mathematics and Mathematical Physics,",
            "year": 2013
        },
        {
            "authors": [
                "Laurent Sorber",
                "Marc Van Barel"
            ],
            "title": "A mixed-integer linear program formulation for fast matrix multiplication, 2017",
            "year": 2017
        },
        {
            "authors": [
                "David Speck",
                "Paul H\u00f6ft",
                "Daniel Gnad",
                "Jendrik Seipp"
            ],
            "title": "Finding matrix multiplication algorithms with classical planning",
            "venue": "Proceedings of the Thirty-Third International Conference on Automated Planning and Scheduling (ICAPS",
            "year": 2023
        },
        {
            "authors": [
                "Volker Strassen"
            ],
            "title": "Gaussian elimination is not optimal",
            "venue": "Numerische mathematik,",
            "year": 1969
        },
        {
            "authors": [
                "Shmuel Winograd"
            ],
            "title": "On the number of multiplications necessary to compute certain functions",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 1970
        }
    ],
    "sections": [
        {
            "text": "2012 ACM Subject Classification Mathematics of computing\nKeywords and phrases fast matrix multiplication, computer-assisted proofs, constraint programming, constraint satisfaction problem\nDigital Object Identifier 10.4230/LIPIcs.CP.2023.26\nSupplementary Material Software (Source Code): https://github.com/khalil-research/MatrixMult-CP\n1 Introduction\nMatrix multiplication is a fundamental operation in linear algebra with applications in virtually every computational domain. As a result, extensive research has been dedicated to the development of faster matrix multiplication algorithms.\nThe elementary way of multiplying two N \u00d7 N matrices requires N3 multiplications. For example, multiplying two 2 \u00d7 2 matrices naively requires a total of 23 = 8 multiplications. In 1969, Strassen [13] constructed an algorithm that finds the product of two 2 \u00d7 2 matrices in only 7 multiplications. This discovery has had significant implications as it opened up the door for potentially faster algorithms for large-scale matrix or tensor computations.\n1 These authors contributed equally.\n\u00a9 Arnaud Deza, Chang Liu, Elias B. Khalil, Pashootan Vaezipoor; licensed under Creative Commons License CC-BY 4.0\n29th International Conference on Principles and Practice of Constraint Programming (CP 2023). Editor: Roland H. C. Yap; Article No. 26; pp. 26:1\u201326:15\nLeibniz International Proceedings in Informatics Schloss Dagstuhl \u2013 Leibniz-Zentrum f\u00fcr Informatik, Dagstuhl Publishing, Germany\nar X\niv :2\n30 6.\n01 09\n7v 2\n[ cs\n.A I]\n1 7\nJu l 2\n02 3\nStrassen\u2019s algorithm has later been proved to be both canonical [4] (no smaller rank exists) and essentially unique [5] (all other solutions of the same rank are equivalent up to symmetry).\nCurrently, the best-known algorithm for multiplying 3 \u00d7 3 matrices requires R = 23 multiplications, compared to the naive elementary method that requires 27 multiplications. A known theoretical lower bound of R = 19 exists [2], however, it remains unclear whether 19 \u2264 R \u2264 22 is truly attainable. This is a testament to the difficulty of the fast matrix multiplication (FMM) problem, which has been intractable for existing methods even for tiny matrices.\nIn the literature, the general approach to finding FMM algorithms starts by representing matrix multiplication as a tensor operation using the multiplication tensor TN followed by finding exact or approximate low-rank decompositions that represent TN . The factor matrices that are used in the low-rank decomposition encode FMM algorithms. A rank-7 decomposition (i.e., a multiplication algorithm that uses 7 multiplication operations) of a 2 \u00d7 2 matrix multiplication using Strassen\u2019s algorithm is shown in Figure 1. Existing methods for finding such factor matrices have several limitations. The most successful and common methods include local search [10] techniques for low-rank approximation, which cannot guarantee optimality. A more recent successful approach [6] searches for low-rank decomposition using reinforcement learning (RL) and was successful in finding faster algorithms for N = 4. However, this method is not exhaustive and hence cannot prove the infeasibility of a given rank.\nIn this work, we propose a novel approach to finding FMM algorithms by formulating the tensor decomposition problem, for the first time, as a constraint satisfaction problem (CSP) that is solved using Constraint Programming (CP). We believe that this is a very natural formulation of this highly combinatorial problem. CP is advantageous for FMM in that it is a flexible framework that can bring to bear a wide range of search and logical inference techniques that have been developed over the last few decades. It provides the ability to prove infeasibility when it is not possible to multiply two matrices using a given number of multiplications.\nBesides a base CP formulation for FMM, we propose a set of symmetry-breaking constraints and valid inequalities that are useful for infeasibility proofs. On the feasible side, we show that \u201cperformance variability\u201d w.r.t. solver random seeds can be exploited in conjunction with a sparsity-based decomposition of FMM for faster solving. Our experimental results, while limited to matrices of size up to 3 \u00d7 3, demonstrate the effectiveness of the aforementioned constraints and techniques. The CP approach to FMM is uniquely positioned to close open questions such as whether it is possible to multiply two 3 \u00d7 3 matrices in 19 to 22 multiplications. While we do not yet resolve this or other open questions, our work opens up the potential for further enhancements to the CP formulation and search such as customized branching strategies and CP-based heuristics.\n2 Fast Matrix Multiplication: Problem Statement\nThe multiplication of two matrices A and B of sizes N \u00d7M and M \u00d7P , respectively, results in a product matrix C of size N \u00d7 P . This operation can be represented by a binary third-order tensor TNMP (TN for square matrices A and B of size N \u00d7 N). An entry Ti,j,k of this tensor is equal to 1 if and only if the kth entry in the output matrix C uses the scalar product of the ith entry of A and the jth entry of B. Here, i, j, and k are indices of a matrix entry starting with 1 in the first row and column; and proceeding entry by entry, left to right, top to bottom. For example, for N = M = P = 2, it must be that T2,3,1 = 1 since the first entry\nof C, c1, is equal to a1b1 + a2b3. Similarly, T1,2,1 = 0 must hold since a1b2 is not part of c1. Figures 1a and 1b show a complete example of the indexing and tensor representation.\nThe FMM problem for a given tensor TNMP , rank R \u2208 Z+, and field F (e.g., F = {\u22121, 0, 1}) asks: can each entry Ti,j,k of TNMP be expressed as the sum of exactly R trilinear terms involving the factor matrices U \u2208 FN \u00b7M\u00d7R, V \u2208 FM \u00b7P \u00d7R, and W \u2208 FN \u00b7P \u00d7R, as follows:\nTi,j,k = R\u2211\nr=1 Ui,r \u00b7 Vj,r \u00b7 Wk,r \u2200i \u2208 {1, . . . , N \u00b7 M}, j \u2208 {1, . . . , M \u00b7 P}, k \u2208 {1, . . . , N \u00b7 P}\nNote that we use the notation FL\u00d7Q to refer to the set of matrices of dimension L \u00d7 Q and entries in F. The CSP is to find factor matrices with entries in F that produce the tensor TNMP for a given rank R.\nThis decomposition is also referred to as the polyadic decomposition and its associated rank is the minimal R needed. The rank can be interpreted as the number of multiplications required to compute the product. For example, for 2 \u00d7 2 matrices, the rank of the decomposition using Strassen\u2019s algorithm is 7. Figure 1 walks through an example of the low-rank decomposition of a 2 \u00d7 2 matrix multiplication using Strassen\u2019s algorithm. The matrix multiplication of the two 2 \u00d7 2 matrices can be seen in Figure 1a, its associated tensor representation TN in Figure 1b, the low-rank decomposition in Figure 1c, and the factor matrices U , V , and W in Figure 1d.\n3 Related Work\nSince Strassen\u2019s discovery [13], there has been substantial research on finding faster algorithms for matrix multiplication. Mathematicians have discovered such algorithms manually over the years for a variety of matrix dimensions and ranks. In this section, however, we will focus on automated methods for discovering such algorithms and briefly discuss some of the existing methods. A recent survey on the topic can be found in [3]."
        },
        {
            "heading": "3.1 Continuous Local Search Methods",
            "text": "The most common approach in the literature to compute the factor matrices U , V , and W is to use (heuristic, continuous) local search methods for low-rank tensor decomposition. The stateof-the-art local method [10] uses alternating least squares with regularization. This method has been the most successful in finding fast algorithms whilst remaining computationally tractable and has been scaled up to N = M = P = 4, R = 492. However, this approach has limitations which include getting stuck at local minima, facing ill-conditioned linear leastsquares problems, and solutions being only adequate up to machine precision. Additionally, these methods are not exhaustive and hence cannot be used to provide a proof of infeasibility for a given rank R."
        },
        {
            "heading": "3.2 AlphaTensor",
            "text": "More recently, DeepMind released AlphaTensor [6], a deep RL method that searches this large combinatorial space by playing a single-player game, the TensorGame, formulated as\n2 Note that this particular result is not very useful as an R = 49 solution can be obtained by applying Strassen\u2019s R = 7 algorithm for 2 \u00d7 2 matrices on the four 2 \u00d7 2 blocks of the 4 \u00d7 4 matrices.\nCP 2023\n( c1 c2 c3 c4 ) = ( a1 a2 a3 a4 ) \u00b7 ( b1 b2 b3 b4 ) (a) Multiplication of two 2 \u00d7 2 matrices. We highlight the term c1 = a1b1 + a2b3.\nT:,:,1 = ( 1 0 0 0\n0 0 1 0 0 0 0 0 0 0 0 0\n) T:,:,2 = ( 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 ) T:,:,3 = ( 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 ) T:,:,4 = ( 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 ) (b) Tensor representation of the 2 \u00d7 2 matrix multiplication operation. T:,:,1 represents c1, the entry T2,3,1 (in yellow) is set to 1 because the product a2b3 is required to compute c1 (similarly for T1,1,1 in red).\nm1 = (a1 + a4)(b1 + b4) m5 = (a1 + a2)(b4)\nm2 = (a3 + a4)(b1) m6 = (a3 \u2212 a1)(b1 + b2) m3 = (a1)(b2 \u2212 b4) m7 = (a2 \u2212 a4)(b3 + b4) m4 = (a4)(b3 \u2212 b1)\nc1 = m1 + m4 \u2212 m5 + m7\n= (a1 + a4)(b1 + b4) + (a4)(b3 \u2212 b1) \u2212 (a1 + a2)(b4) + (a2 \u2212 a4)(b3 + b4)\n= a1b1 + a1b4 + a4b1 + a4b4 + a4b3 \u2212 a4b1 \u2212 a1b4 \u2212 a2b4 + a2b3 + a2b4 \u2212 a4b3 \u2212 a4b4 = a1b1 + a2b3\nc2 = m3 + m5 c3 = m2 + m4 c4 = m1 \u2212 m2 + m3 + m6\n(c) A low-rank decomposition of the 2 \u00d7 2 matrix multiplication using Strassen\u2019s algorithm. The m terms are the multiplication terms and the c terms represent the entries in the product matrix. Here c1 = m1 + m4 \u2212 m5 + m7 gives c1 = a1b1 + a2b3 after expansion.\nm1 m2 m3 m4 m5 m6 m7\nU =  1 0 1 0 1 -1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 1 0 0 -1  a1 a2 a3 a4\nV =  1 1 0 -1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 -1 0 1 0 1  b1 b2 b3 b4\nW =  1 0 0 1 -1 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 -1 1 0 0 1 0  c1 c2 c3 c4\n(d) The factor matrices U , V , and W for Strassen\u2019s algorithm. The columns in U and V represent the coefficient of the a and b terms in each m. Each row in W represents the coefficient of the m terms in one c term.\nFigure 1 A low-rank decomposition of a 2 \u00d7 2 matrix multiplication using Strassen\u2019s algorithm.\na Markov decision process (MDP). At every step t of this MDP, the state is characterized by a tensor St which is initially set to the target multiplication tensor, i.e., S0 = TN . An action at at iteration t corresponds to the player selecting a triplet of vectors (u(t), v(t), w(t)) which in turn will provide the next state St = St\u22121 \u2212 u(t) \u2297 v(t) \u2297 w(t) where \u2297 denotes the outer tensor product. The goal of the player is to reach the zero tensor St = 0 in the fewest number of steps possible. This is done by providing a reward of \u22121 to the player after every non-terminal state whereas a large negative reward \u2212\u03b3(SRlimit) is given to the player if the number of steps Rlimit is met, where \u03b3(SRlimit) upper bounds the rank of the tensor at iteration Rlimit. If the agent successfully reaches the zero tensor, the sequence of actions taken constitutes a valid low-rank decomposition of TN , and hence an FMM algorithm is found with the rank R corresponding to the number of steps taken by the agent.\nThis approach is the first to directly incorporate learning into the search which resulted in the discovery of new minimal ranks for certain non-trivial cases. The largest case tackled by this method is N = M = P = 5, R = 98. The sole focus of this purely heuristic method is to find lower ranks than currently best-known ranks but it cannot prove the infeasibility of a given rank. Additionally, rather complex architectures and multiple training phases were required for successful learning. It is worth noting that AlphaTensor was trained for one week on 64 Tensor Processing Units (TPUs), Google\u2019s proprietary chip. The paper [6] does not provide any estimates of the amount of computation required to produce the reported results, namely how long the trained \u201cagent\u201d must be run to discover FMM algorithms. Our CP runs use much fewer resources while leveraging thread parallelism in the CP solver on readily-available CPU machines."
        },
        {
            "heading": "3.3 Integer Programming",
            "text": "The work that is the most related to our approach tackles this problem through a mixedinteger linear program (MILP) formulation in an unpublished technical report [11]. The goal of this methodology is to linearize the trilinear products in the low-rank decomposition of TN to a MILP that aims to 1) maximize the sparsity of the integer decision variables representing factor matrices U , V , and W and 2) minimize the reconstruction loss (L1 norm) from the input TN and the multiplication tensor attained by the decision variables representing factor matrices. The report [11] focuses solely on presenting the MILP formulation for square matrices but does not include any computational experiments. However, the MILP formulations for N \u2208 {2, 3} are benchmark problems in MIPLIB 2017 [8]3. The linearization of the trilinear products likely leads to a weak linear programming relaxation as well as an explosion in the number of integer variables and constraints, which might explain why the MILP approach to FMM has not picked up significant interest. A CP formulation is more natural and compact, as we will show in this paper."
        },
        {
            "heading": "3.4 Classical AI Planning",
            "text": "Very recently, AI planning techniques were used for FMM [12]. They use a similar state space as AlphaTensor but use various planning tools (with and without exhaustive search) to solve this problem. They compared a number of heuristic and exact planning methods from the literature on matrices of size up to 3 \u00d7 3. However, the experiments show that planning approaches are severely limited, even failing to find Strassen\u2019s algorithm for the 2 \u00d7 2 case\n3 See https://miplib.zib.de/instance_details_fastxgemm-n3r21s3t6.html for example.\nCP 2023\n(see Table 1 in [12]). We will show that our CP approach is significantly more effective as we are able to attack the 3 \u00d7 3 case with R = 23, matching the known upper bound from the literature.\n4 Constraint Programming for Fast Matrix Multiplication\nIn the FMM problem, all variables have the same domain F = {\u22121, 0, 1}4. Since the variable domains are small and this problem is highly structured, CP is a promising solution paradigm.\nThe base CP model for FMM is given in Equation (1). Let U denote the set {1, . . . , N \u00b7M}, V denote the set {1, . . . , M \u00b7 P}, W denote the set {1, . . . , N \u00b7 P}, and R denote the set {1, . . . , R}. The CP model uses three sets of variables: ui,r where i \u2208 U , vj,r where j \u2208 V , and wk,r where k \u2208 W; r \u2208 R in all three cases. Each variable ui,r, vj,r and wk,r represents the value of the i/j/kth row and rth column of the matrices U , V , and W . The domain of all variables is {\u22121, 0, 1}. The set of constraints presented here requires that the decomposition algorithm\u2019s output matches the original tensor multiplication TNMP . Therefore the input to the CSP model is 4 integers: (N, M, P ) and R. The model then reads as:\nR\u2211 r=1 (ui,r \u00b7 vj,r \u00b7 wk,r) = Ti,j,k, \u2200i \u2208 U , j \u2208 V, k \u2208 W\nui,r, vj,r, wk,r \u2208 {\u22121, 0, 1}, \u2200i \u2208 U , j \u2208 V, k \u2208 W, r \u2208 R (1)\nThe search space for this (NP-complete) problem grows very quickly with increasing matrix sizes N, M, P and rank R. With only one set of equality constraints, a CP solver may struggle with constraint propagation, thus failing to scale with increasing N, M, P . To that end, we will introduce additional valid constraints to help CP prune and propagate more efficiently."
        },
        {
            "heading": "4.1 Symmetry Breaking",
            "text": "There are many symmetric solutions to the FMM problem. We can reduce the search space of our problem significantly by prohibiting symmetries."
        },
        {
            "heading": "4.1.1 Permutation Symmetry",
            "text": "Since addition is commutative, i.e., (a1 + a2) = (a2 + a1), there are many equivalent solutions to the tensor decomposition problem. Therefore, any permutation of the columns of matrices U , V , and W produces an equivalent solution. If we consider Strassen\u2019s solution for the 2 \u00d7 2 case, Figure 2 provides an example of two equivalent solutions.\nIn order to break this symmetry, we introduce a lexicographic-strict5 constraint on the ui,r and vj,r variables. When applied to two variable arrays x and y, the lexicographic ordering constraint enforces that x is strictly less than y in the defined lexicographic order. Because of the strictness, this also enforces that the two variable arrays must be different. This set of symmetry-breaking constraints is modelled as follows:\nlexicographic-strict([u:,r; v:,r], [u:,r+1; v:,r+1]), \u2200r \u2208 R\n4 One can consider bigger fields such as {\u22122, \u22121, 0, 1, 2} but the bulk of the work in the literature has been with {\u22121, 0, 1}. 5 https://www.ibm.com/docs/en/icos/22.1.0?topic=variables-lexicographic-constraint\nwhere [u:,r; v:,r] represents the vector concatenating the rth column of the matrix U and V . In Figure 2, sol2 satisfies the lexicographic-strict constraint."
        },
        {
            "heading": "4.1.2 Sign Symmetry",
            "text": "For the multiplicative mi terms, one can easily see that multiplying both sets of terms from A and B by \u22121 will result in the same solution. For example, (a1 + a4)(b1 + b4) = (\u2212a1 \u2212 a4)(\u2212b1 \u2212 b4), where we could multiply any subset of columns of U and V by \u22121 to achieve the same solution. We call this symmetry the sign symmetry. In order to break it, we introduce the following constraints:\nu1,r \u2264 0 ui,r \u2264 i\u22121\u2211 i\u2032=1 |ui\u2032,r| \u2200r \u2208 R, i > 1, i \u2208 U\nThe main idea of these constraints is to enforce that the first non-zero entry in a column of U can only take on the value of \u22121, enforcing that the first entry of the columns is either 0 or \u22121. The subsequent constraints ensure that for any column r, an entry in row i > 1 can only be 1 if there has been an entry in the same column in an earlier row with value \u22121. This set of constraints applies to the concatenation of the columns in U and V , however, in modelling, it only needs to be applied to the columns of the U matrix as none of the columns can be zero, so the leading \u22121 must appear in the U matrix. By applying these constraints, we make sure that {\u2212ui,r} is infeasible for any feasible {ui,r}. Employing this sign symmetry breaking constraint to sol2 from Figure 2, we arrive at sol3 shown in Figure 3.\nsol3: U = ( -1 0 0 0 -1 -1 -1\n0 0 0 -1 0 0 -1 1 0 -1 0 0 0 0 0 -1 -1 1 0 -1 0\n) V = ( 1 1 -1 0 0 -1 0 1 0 0 0 -1 0 0 0 -1 0 -1 0 0 0 0 0 0 -1 1 -1 -1 ) W = ( 0 1 0 1 0 1 -1 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 0 -1 0 1 1 0 )\nFigure 3 sol3 is the solution derived from enforcing the sign symmetry constraints on sol2.\nSimilarly, the same type of sign symmetry-breaking constraints can be applied to the W factor matrix as follows:\nw1,r \u2264 0 wk,r \u2264 k\u22121\u2211 k\u2032=1 |wk\u2032,r| \u2200r \u2208 R, k > 1, k \u2208 W.\nThe interpretation is as follows. In Figure 1c, consider c4 = m1 \u2212 m2 + m3 + m6, and notice that one can redefine m6 = (a3 \u2212 a1)(b1 + b2) to become m6 = (a3 \u2212 a1)(\u2212b1 \u2212 b2) and then\nCP 2023\nrewrite c4 as c4 = m1 \u2212 m2 + m3 \u2212 m6. Recall that the coefficients of the m terms in an output entry ck are the entries of row k of factor matrix W . The transformation we just performed produces two equivalent solutions and is an instance of \u201cvalue symmetry\u201d that is broken by the above constraint set as it forces the first non-zero entry of a column of W to be \u22121."
        },
        {
            "heading": "4.2 Valid Inequalities",
            "text": "Based on the structure of this problem, we can also introduce a series of valid inequalities that could potentially help a CP solver with propagation.\nFirst, for the W matrix, we know that each multiplicative term mr must be used at least once for sufficiently small R (i.e., for non-trivial cases of the FMM problem where R \u2264 NMP ). This means that the sum of each column in W must be at least one:\u2211\nk\u2208W\n|wk,r| \u2265 1, \u2200r \u2208 R.\nEach result term cl must use at least M terms. This is due to a basic fact in algebraic complexity theory which states that the dot-product of two vectors of size M requires at least M multiplications [14]. This means that the sum of each row of W must be greater or equal to M :\u2211\nr\u2208R |wk,r| \u2265 M, \u2200k \u2208 W.\nEach result term cl must differ in at least two mr terms; a simple proof by contradiction is omitted for brevity. This can be modelled as follows:\u2211\nr\u2208R |wk,r \u2212 wk\u2032,r| \u2265 2, \u2200k \u0338= k\u2032 \u2208 W.\nEach term in the A and B matrices must appear in at least one of the multiplicative terms mr. This translates to each row of U and V having at least one non-zero term as shown in the constraints below:\u2211\nr\u2208R |ui,r| \u2265 1, \u2200i \u2208 U\u2211 r\u2208R |vj,r| \u2265 1, \u2200j \u2208 V.\nFurthermore, each valid product of two terms from the A and B matrices, e.g., a2b3 for 2 \u00d7 2 matrices, must appear in at least one of the R multiplication terms. For a2b3 appears in c1 and c2, see Figure 1a. This can be modelled as follows:\u2211\nr\u2208R |ui,r \u00b7 vj,r| \u2265 1, \u2200 valid i, j."
        },
        {
            "heading": "4.3 Full CP Model",
            "text": "Finally, the full CP model is presented in Figure 4. The constraints in Equation (2) ensure that the output matches the original multiplication tensor and thus the validity of an assignment as a matrix multiplication algorithm. We enforce permutation symmetry-breaking with Equation (3) and sign symmetry-breaking with Equations (4)\u2013(7). The valid inequalities are modelled through Equations (8)\u2013(13).\n\u2211 r\u2208R (ui,r \u00b7 vj,r \u00b7 wk,r) = Ti,j,k, \u2200i \u2208 U , j \u2208 V, k \u2208 W (2)\nlexicographic-strict([u:,r; v:,r], [u:,r+1; v:,r+1]), \u2200r \u2208 R (3) u1,r \u2264 0 \u2200r \u2208 R (4) ui,r \u2264 i\u22121\u2211 i\u2032=1 |ui\u2032,r|, \u2200r \u2208 R, i > 1, i \u2208 U (5)\nw1,r \u2264 0 \u2200r \u2208 R (6) wk,r \u2264 k\u22121\u2211 k\u2032=1\n|wk\u2032,r| \u2200r \u2208 R, k > 1, k \u2208 W (7)\u2211 k\u2208W\n|wk,r| \u2265 1, \u2200r \u2208 R (8)\u2211 r\u2208R\n|wk,r| \u2265 M, \u2200k \u2208 W (9)\u2211 r\u2208R\n|wk,r \u2212 wk\u2032,r| \u2265 2, \u2200k \u0338= k\u2032 \u2208 W (10)\u2211 r\u2208R\n|ui,r| \u2265 1, \u2200i \u2208 U (11)\u2211 r\u2208R\n|vj,r| \u2265 1, \u2200j \u2208 V (12)\u2211 r\u2208R |ui,r \u00b7 vj,r| \u2265 1, \u2200 valid i, j (13)\nFigure 4 Full CP Model with symmetry-breaking constraints and valid inequalities."
        },
        {
            "heading": "4.4 Sparsity-based Problem Decomposition",
            "text": "Given that the factor matrices that have been found for known decompositions tend to be sparse, we introduce some inexact inequalities to induce sparsity and trim candidate assignments that have a high likelihood to be infeasible or that are unnecessarily dense. For example, observe that Strassen\u2019s solution in Figure 1c leads to many zeros in the factor matrices; no m term uses more than 2 out of 4 of the a or b terms, no c term uses more than 4 out of the 7 m terms. It has been observed that as the matrix sizes grow, the best solutions become even sparser.\nWe first introduce a constraint limiting the number of active (i.e., nonzero) terms in each column r (i.e., multiplication term) of U and V . This constraint is written as:\u2211\ni\u2208U |ui,r| + \u2211 j\u2208V |vj,r| \u2264 K1, \u2200r \u2208 R.\nA similar constraint can be imposed on W , by restricting that each output must use at most K2 multiplication terms. This constraint is written as:\u2211\nr\u2208R |wk,r| \u2264 K2, \u2200k \u2208 W.\nBased on these constraints, K1 has an upper bound of (NM +MP ) and K2 is upper bounded by R. By observing decompositions for small to medium-scale matrices, we can estimate K1\nCP 2023\nand K2. For example, for 3 \u00d7 3 matrices with R = 23, we observe that K1 = 9 and K2 = 10 is the safest estimate possible compared to the upper bounds of 18 and 23, respectively, which could restrict the CP search dramatically. Note that one could start with any such estimates of the decomposition parameters K1 and K2, iteratively increasing them if the restricted instances are found to be infeasible by the CP solver, eventually resulting in a complete resolution of the original problem."
        },
        {
            "heading": "4.5 Cyclic Invariant Formulation",
            "text": "In contrast to the symmetries of the factor matrices discussed in Section 4.1, there exists well-known cyclic symmetry for the multiplication tensors TN of square matrices. More precisely, it is known that Ti,j,k = Tj,k,i = Tk,i,j . The authors in [1] proposed to leverage this cyclic symmetry property and parameterize FMM algorithms with cyclic invariant factor matrices: U = [ABCD], V = [ADBC], W = [ACDB] with A \u2208 {\u22121, 0, 1}N2\u00d7S and B, C, D \u2208 {\u22121, 0, 1}N2\u00d7T corresponding to a rank R = S + 3T .\nAlthough this parametrization reduces the number of integer variables by a factor of three, helping with the combinatorial nature of the problem, there is no guarantee that the minimal rank decomposition corresponds to solutions that exhibit cyclic symmetry. That being said, Strassen\u2019s solution of R = 7 for N = 2, which is optimal, exhibits such a symmetry (S \u2208 {1, 4}), as does the best-known rank of 23 for N = 3 (S \u2208 {2, 5, 11}). Performing two steps of Strassen\u2019s algorithm for N = 2 yields a rank 49 cyclic invariant solution for N = 4. It is currently unknown whether a solution of rank less than 49 exists for N = 4, let alone one exhibiting cyclic invariance.\nWe implement Ballard and Benson\u2019s cyclic invariant reduction [1] of the FMM problem for square matrices by reducing the decision variables of our CP formulation as required and imposing the invariant structure on the factor matrices.\n5 Experiments\nIn this section, we present our experimental results starting by showing how our CP approach can recover the best-known upper bounds on the rank in a small amount of time on multiplication problems ranging from the trivial (N, M, P ) = (1, 1, 1) case all the way up to the much harder (2, 2, 4) and (3, 3, 3) cases. We then present results for the infeasible cases for (2, 2, 2). We used IBM\u2019s CP Optimizer (CPO) 22.1.06 to solve our CP models. We ran our experiments on a compute cluster of AMD Ryzen Threadripper 2990WX cores with 128 GB of RAM per node."
        },
        {
            "heading": "5.1 Experimental Setup",
            "text": "To ensure the reproducibility and robustness of our results, all our experiments are run with multiple random seeds. This accounts for the often observed performance variability in combinatorial search; this is documented for example in MILP [9]. To that end, we ran each experiment with 10 different seeds. We assigned 8 cores (CPO\u2019s Workers parameter) to the solver for each run (except for more compute-intensive experiments in Section 5.4 where we assigned 20 cores) and timed out the experiments after 2 hours.\n6 https://www.ibm.com/products/ilog-cplex-optimization-studio/cplex-cp-optimizer"
        },
        {
            "heading": "5.2 Evaluation Metrics",
            "text": "We will report the solver runtime and the number of branches during the solution process for completed runs (i.e., runs that returned a feasible solution or a proof of infeasibility). Given that each problem is attempted with multiple random seeds, the shifted geometric mean with a shift of 0.00017, median, minimum, and maximum of the time in seconds and the number of branches will be reported for a complete picture of the results. Runs that terminated due to the time or memory limits will be discussed where applicable."
        },
        {
            "heading": "5.3 Feasible Cases: Searching for Solutions with the Base CP Model",
            "text": "Table 1 shows the time and number of branches (Num Branches) required by the base CP model (i.e., without symmetry breaking or valid inequalities) to find solution for a range of problems. Our approach was able to find Strassen\u2019s solution for the 2 \u00d7 2 matrix multiplication in less than a second whereas the AlphaTensor paper [6] reports a few minutes of model inference to find that solution. Performance variability. In Table 1, we can see that for (2, 2, 3) with R = 11, the worst seed took 245 seconds to find a feasible solution compared to 0.98 seconds for the best seed. This drastic difference in time (and ultimately the number of branches) is an indication that minute parameters such as the seed can significantly impact the CP search. For feasible instances, this phenomenon can be seen as a blessing rather than a curse if one has access to multiple cores: the randomness can be exploited by running multiple copies of the solver, terminating as soon as the first successful run is completed. This has been done in MILP [7].\n7 The shifted geometric mean of a set of n values t1, . . . , tn is defined as ( \u220fn i=1 [ti + shift] ) 1\nn \u2212 shift. Compared to the arithmetic mean, it is less sensitive to large variations in the values.\nCP 2023"
        },
        {
            "heading": "5.4 Feasible Cases: Sparsity Constraints and Cyclic Invariance Help",
            "text": "Solving the base CP formulation, with our current time and memory budgets, does not yet yield feasible decompositions for dimensions higher than (2, 2, 3) or (2, 3, 2). However, after increasing both the time and memory limits, we were able to find a solution to the problem for dimension (2, 2, 4) with R = 14 in 19.6 hours using the inexact inequalities (K1 = 11 and K2 = 7) developed in Section 4.2. Furthermore, our cyclic invariant formulation (with S = 5) with inexact inequalities (K1 = 9 and K2 = 10) was able to find a solution for (3, 3, 3), R = 23. More specifically, we ran the cyclic invariant formulation for 10 hours with 5 different seeds and observed that two seeds produced a feasible solution within one hour whereas the other three seeds hit the time limit. Once again, this indicates that performance variability in the CP search is significant for our problem. Additionally, we ran the base CP formulation for (3, 3, 3) without inexact inequalities for 5 seeds which all hit the time limit of 10 hours, demonstrating the benefit of the reduction of variables for the cyclic invariant formulation and sparsity constraints. We have yet to check whether using only inexact inequalities can help the base formulation for (3, 3, 3). A similar result was observed for the (2, 2, 2) case in which the cyclic invariant formulation (S = 4) with inexact inequalities (K1 = 6 and K2 = 4) produced an average solution time of 0.05 seconds across 10 seeds whereas the base CP model has an average of 1.46 seconds. Our current implementation is not able to find cyclic invariant solutions for N = 4 with R = 48, but we have hope that this approach is a promising tool for the search for new cyclic invariant solutions for square matrix multiplication."
        },
        {
            "heading": "5.5 Infeasible Cases: The Importance of Symmetry Breaking",
            "text": "Since CP performs an exhaustive search, it can provide a proof of infeasibility if a given rank R is not achievable for certain matrix dimensions. As expected, the runtime to prove infeasibility significantly increases as we approach the known minimum rank; this can be seen in Table 2 for the (2,2,2) case. It is also apparent that the addition of symmetry-breaking constraints helps tremendously when proving infeasibility given that they reduce the search space significantly. More specifically, for R = 6 in Table 2, it is not even currently possible to prove infeasibility without symmetry-breaking constraints in 2 hours whereas the CP model with symmetry-breaking constraints (B+S) requires around 7 minutes. These results highlight the importance of symmetry-breaking constraints when looking to prove infeasibility.\n6 Conclusion\nWe have proposed a novel CP approach to solve the fast matrix multiplication problem. We have provided a set of constraints for breaking permutation and sign symmetries as well as a set of valid inequality constraints to help CP prune and propagate more efficiently. We provide a decomposition framework that is beneficial for finding feasible solutions for the largest case we have attempted, i.e., 3 \u00d7 3 matrix multiplication. Based on our experimental results, we have been able to solve small instances of this problem within a reasonable amount of time. This is in contrast to some existing search-based approaches (MILP, planning) that seem to struggle. In contrast to the AlphaTensor approach [6], the CP model is far more natural for this combinatorial task and is uniquely positioned to provide proof of infeasibility for some open problems in this space.\nWhile the results of our approach are promising given the limited amount of computing used, there are several limitations that we aim to address in future work. First, our algorithm\nTable 2 Runtime results for the base CP model and variants to prove infeasibility of R < 7 for\n(2,2,2). \u201cgeo mean\u201d refers to the shifted geometric mean as described in Section 5.2; \u201cmed\u201d refers to the median and \u201cmin\u201d/\u201cmax\u201d to the minimum and maximum, respectively. Overall, the use of symmetry-breaking constraints (denoted by the letter \u201cS\u201d) on top of the base CP formulation (\u201cB\u201d) is crucial for efficient proofs of infeasibility. \u201cV\u201d refers to the valid inequalities of Section 4.2 which sometimes complement symmetry-breaking but are not always needed for the fastest results.\nTime (sec) Num Branches R Method geo mean (min, med, max) geo mean (min, med, max)"
        },
        {
            "heading": "1 B 0.01 (0.00, 0.01, 0.02) 1.08\u00d7103 (1.06\u00d7103, 1.08\u00d7103, 1.10\u00d7103)",
            "text": "B+S 0.00 (0.00, 0.01, 0.01) 1.00\u00d710\u22124 (0.00, 0.00, 0.00) B+V 0.00 (0.00, 0.00, 0.01) 1.00\u00d710\u22124 (0.00, 0.00, 0.00) B+V+S 0.00 (0.00, 0.00, 0.00) 1.00\u00d710\u22124 (0.00, 0.00, 0.00) 2 B 0.01 (0.00, 0.01, 0.03) 4.38\u00d7103 (3.72\u00d7103, 4.41\u00d7103, 5.30\u00d7103) B+S 0.01 (0.01, 0.01, 0.02) 1.08\u00d7103 (1.08\u00d7103, 1.08\u00d7103, 1.08\u00d7103) B+V 0.00 (0.00, 0.01, 0.02) 1.33\u00d7103 (1.31\u00d7103, 1.32\u00d7103, 1.34\u00d7103) B+V+S 0.01 (0.01, 0.01, 0.03) 1.09\u00d7103 (1.07\u00d7103, 1.08\u00d7103, 1.10\u00d7103) 3 B 0.21 (0.17, 0.21, 0.28) 1.70\u00d7105 (1.42\u00d7105, 1.70\u00d7105, 1.95\u00d7105) B+S 0.02 (0.01, 0.02, 0.03) 4.13\u00d7103 (3.06\u00d7103, 4.30\u00d7103, 5.32\u00d7103) B+V 0.20 (0.13, 0.21, 0.25) 1.38\u00d7105 (1.14\u00d7105, 1.35\u00d7105, 1.78\u00d7105) B+V+S 0.02 (0.01, 0.02, 0.03) 3.61\u00d7103 (2.52\u00d7103, 3.67\u00d7103, 4.92\u00d7103) 4 B 43.79 (31.33, 41.89, 66.93) 3.85\u00d7107 (3.06\u00d7107, 3.80\u00d7107, 5.00\u00d7107) B+S 0.12 (0.07, 0.13, 0.18) 8.50\u00d7104 (6.94\u00d7104, 8.54\u00d7104, 1.03\u00d7105) B+V 53.49 (39.10, 48.87, 69.35) 3.70\u00d7107 (3.18\u00d7107, 3.69\u00d7107, 4.23\u00d7107) B+V+S 0.15 (0.11, 0.15, 0.20) 8.53\u00d7104 (7.34\u00d7104, 8.10\u00d7104, 1.06\u00d7105) 5 B T.O. (N/A, N/A, N/A) 6.03\u00d7109 (5.56\u00d7109, 5.75\u00d7109, 7.14\u00d7109) B+S 3.06 (2.28, 3.02, 4.15) 2.22\u00d7106 (1.89\u00d7106, 2.19\u00d7106, 2.67\u00d7106) B+V T.O. (N/A, N/A, N/A) 5.57\u00d7109 (3.97\u00d7109, 5.83\u00d7109, 6.16\u00d7109) B+V+S 2.98 (2.56, 2.94, 3.44) 2.14\u00d7106 (1.91\u00d7106, 2.12\u00d7106, 2.56\u00d7106) 6 B T.O. (N/A, N/A, N/A) 5.99\u00d7109 (4.53\u00d7109, 5.79\u00d7109, 6.93\u00d7109) B+S 429.26 (333.88, 441.63, 528.61) 3.28\u00d7108 (2.94\u00d7108, 3.31\u00d7108, 3.76\u00d7108) B+V T.O. (N/A, N/A, N/A) 4.67\u00d7109 (3.82\u00d7109, 4.73\u00d7109, 5.48\u00d7109) B+V+S 517.33 (414.07, 522.81, 640.65) 3.35\u00d7108 (2.97\u00d7108, 3.28\u00d7108, 3.95\u00d7108)\nCP 2023\nstruggles to scale for larger matrix dimensions or ranks due to the quick increase in the number of variables of the CP model. Secondly, we have found that the base CP model outperforms the addition of symmetry constraints and valid inequalities in the case of feasible solutions, likely due to the latter\u2019s tendency to prune symmetric solutions early in the tree search. However, we believe that our experiment\u2019s small matrix dimensions may have skewed these results and valid inequalities may be crucial for larger sizes. Moving forward, we propose several areas for further exploration and improvement:\nConduct larger-scale experiments using larger compute clusters to take advantage of the parallelizability of the CP solver\u2019s search procedure. Analyze the highly structured nature of this problem to develop more valid inequalities that can further reduce the search space of our CP model, including inexact inequalities that may not hold for all matrix multiplication dimensions but help for some cases. Explore solver parameter tuning, particularly for branching strategies and other important search-related decisions. Further investigate the idea of sparsity-based problem decomposition as a means of improving the scalability and performance of our approach.\nReferences 1 Austin R. Benson and Grey Ballard. A framework for practical parallel fast matrix multi-\nplication. ACM SIGPLAN Notices, 50(8):42\u201353, jan 2015. URL: https://doi.org/10.1145% 2F2858788.2688513, doi:10.1145/2858788.2688513. 2 Markus Blaser. On the complexity of the multiplication of matrices of small formats. Journal of Complexity, 19(1):43\u201360, 2003. 3 Markus Bl\u00e4ser. Fast Matrix Multiplication. Number 5 in Graduate Surveys. Theory of Computing Library, 2013. URL: http://www.theoryofcomputing.org/library.html, doi: 10.4086/toc.gs.2013.005. 4 Roger W Brockett and David Dobkin. On the optimal evaluation of a set of bilinear forms. In Proceedings of the fifth annual ACM symposium on Theory of computing, pages 88\u201395, 1973. 5 Hans F de Groote. On varieties of optimal algorithms for the computation of bilinear mappings ii. optimal algorithms for 2\u00d7 2-matrix multiplication. Theoretical Computer Science, 7(2):127\u2013 148, 1978. 6 Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, and Grzegorz Swirszcz. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47\u201353, 2022. 7 Matteo Fischetti and Michele Monaci. Exploiting erraticism in search. Operations Research, 62(1):114\u2013122, 2014. 8 Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael Bastubbe, Timo Berthold, Philipp Christophel, Kati Jarck, Thorsten Koch, Jeff Linderoth, et al. Miplib 2017: data-driven compilation of the 6th mixed-integer programming library. Mathematical Programming Computation, 13(3):443\u2013490, 2021. 9 Andrea Lodi and Andrea Tramontani. Performance variability in mixed-integer programming. In Theory driven by influential applications, pages 1\u201312. INFORMS, 2013. 10 Alexey V. Smirnov. The bilinear complexity and practical algorithms for matrix multiplication. Computational Mathematics and Mathematical Physics, 53:1781 \u2013 1795, 2013. 11 Laurent Sorber and Marc Van Barel. A mixed-integer linear program formulation for fast matrix multiplication, 2017. 12 David Speck, Paul H\u00f6ft, Daniel Gnad, and Jendrik Seipp. Finding matrix multiplication algorithms with classical planning. In Sven Koenig, Roni Stern, and Mauro Vallati, editors, Proceedings of the Thirty-Third International Conference on Automated Planning and Scheduling (ICAPS 2023). AAAI Press, 2023. 13 Volker Strassen. Gaussian elimination is not optimal. Numerische mathematik, 13(4):354\u2013356, 1969. 14 Shmuel Winograd. On the number of multiplications necessary to compute certain functions. Communications on Pure and Applied Mathematics, 23(2):165\u2013179, 1970.\nCP 2023"
        }
    ],
    "title": "Fast Matrix Multiplication Without Tears:  A Constraint Programming Approach",
    "year": 2023
}