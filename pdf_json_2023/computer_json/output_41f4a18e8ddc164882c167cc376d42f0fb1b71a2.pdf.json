{
    "abstractText": "Breast cancer is the most common and deadly type of cancer in the world. Based onmachine learning algorithms such as XGBoost, random forest, logistic regression, and K-nearest neighbor, this paper establishes diferent models to classify and predict breast cancer, so as to provide a reference for the early diagnosis of breast cancer. Recall indicates the probability of detecting malignant cancer cells in medical diagnosis, which is of great signifcance for the classifcation of breast cancer, so this article takes recall as the primary evaluation index and considers the precision, accuracy, and F1-score evaluation indicators to evaluate and compare the prediction efect of eachmodel. In order to eliminate the infuence of diferent dimensional concepts on the efect of themodel, the data are standardized. In order to fnd the optimal subset and improve the accuracy of the model, 15 features were screened out as input to the model through the Pearson correlation test. Te K-nearest neighbor model uses the cross-validation method to select the optimal k value by using recall as an evaluation index. For the problem of positive and negative sample imbalance, the hierarchical sampling method is used to extract the training set and test set proportionally according to diferent categories. Te experimental results show that under diferent dataset division (8 : 2 and 7 : 3), the prediction efect of the same model will have diferent changes. Comparative analysis shows that the XGBoost model established in this paper (which divides the training set and test set by 8 : 2) has better efects, and its recall, precision, accuracy, and F1-score are 1.00, 0.960, 0.974, and 0.980, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hua Chen"
        },
        {
            "affiliations": [],
            "name": "Nan Wang"
        },
        {
            "affiliations": [],
            "name": "Xueping Du"
        },
        {
            "affiliations": [],
            "name": "Kehui Mei"
        },
        {
            "affiliations": [],
            "name": "Yuan Zhou"
        },
        {
            "affiliations": [],
            "name": "Guangxing Cai"
        }
    ],
    "id": "SP:c87974da24328cf45b18cda065872f16a0f874c5",
    "references": [
        {
            "authors": [
                "V. Fotedar",
                "S. Fotedar",
                "P. Takur",
                "S. Vats",
                "A. Negi",
                "L. Chanderkant"
            ],
            "title": "Knowledge of breast cancer risk factors and methods for its early detection among the primary health-care workers in shimla",
            "venue": "himachal pradesh,\u201d Journal of Education and Health Promotion, vol. 8, p. 265",
            "year": 2019
        },
        {
            "authors": [
                "M.S. Simon",
                "T.A. Hastert"
            ],
            "title": "A",
            "venue": "Barac et al., \u201cCardiometabolic risk factors and survival after cancer in the women\u2019s health initiative,\u201d Cancer, vol. 127, no. 4, pp. 598\u2013608",
            "year": 2021
        },
        {
            "authors": [
                "H.F. Pasha",
                "R.H. Mohamed",
                "M.M. Toam"
            ],
            "title": "and A",
            "venue": "M. Yehia, \u201cGenetic and epigenetic modifcations of adiponectin gene: p,\u201d Te Journal of Gene Medicine, vol. 21, no. 10, p. e3120",
            "year": 2019
        },
        {
            "authors": [
                "D. Hong",
                "A.J. Fritz"
            ],
            "title": "S",
            "venue": "K. Zaidi et al., \u201cEpithelial to mesenchymal transition and cancer stem cells contribute to breast cancer heterogeneity,\u201d Journal of Cellular Physiology, vol. 233, no. 12, pp. 9136\u20139144",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhong",
                "D. Sun"
            ],
            "title": "W",
            "venue": "Wei et al., \u201cContrast-enhanced ultrasound-guided fne-needle aspiration for sentinel lymph node biopsy in early-stage breast cancer,\u201d Ultrasound in Medicine and Biology, vol. 44, no. 7, pp. 1371\u20131378",
            "year": 2018
        },
        {
            "authors": [
                "K. Babic",
                "C. Siguan-Bell",
                "M. Hee"
            ],
            "title": "and S",
            "venue": "C. Lin, \u201cOcular g: ultrasound B-scan assessment of retained surgical sponge after ahmed valve surgery: a case r,\u201d Journal of Glaucoma, vol. 26, no. 10, pp. e239\u2013e241",
            "year": 2017
        },
        {
            "authors": [
                "A. Yala",
                "P.G. Mikhael"
            ],
            "title": "F",
            "venue": "Strand et al., \u201cToward robust mammography-based models for breast cancer risk,\u201d Science Translational Medicine, vol. 13, no. 578, Article ID eaba4373",
            "year": 2021
        },
        {
            "authors": [
                "G. Zheng",
                "X. Liu"
            ],
            "title": "and G",
            "venue": "Han, \u201cMedical image computeraided detection and diagnosis system review,\u201d Journal of Software, vol. 29, no. 5, pp. 1471\u20131514",
            "year": 2018
        },
        {
            "authors": [
                "E.Y. Huang",
                "S. Knight"
            ],
            "title": "C",
            "venue": "R. Guetter et al., \u201cTelemedicine and telementoring in the surgical specialties: a narrative review,\u201d 8 Computational Intelligence and Neuroscience Te American Journal of Surgery, vol. 218, no. 4, pp. 760\u2013766",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wu",
                "B.T. Wang",
                "H.R. Rao",
                "Y. Jiang"
            ],
            "title": "and C",
            "venue": "Liu, \u201cBreast cancer and benign disease cells form metrology research,\u201d Journal of Anhui Medical University, vol. 31, no. 02, pp. 91\u201393",
            "year": 1996
        },
        {
            "authors": [
                "Q. Shen",
                "F. Shao"
            ],
            "title": "and R",
            "venue": "Sun, \u201cPrediction model of breast cancer based on xgboost,\u201d Journal of Qingdao University (Natural Science Edition), vol. 32, no. 1",
            "year": 2019
        },
        {
            "authors": [
                "Z. Deng",
                "B. Su"
            ],
            "title": "and K",
            "venue": "Zhan.g, \u201cBreast cancer classifcation based on ensemble learning,\u201d China Medical Devices, vol. 35, no. 12",
            "year": 2020
        },
        {
            "authors": [
                "M. Monirujjaman Khan",
                "S. Islam"
            ],
            "title": "S",
            "venue": "Sarkar et al., \u201cMachine learning based comparative analysis for breast cancer prediction,\u201d Journal of Healthcare Engineering, vol. 2022, Article ID 4365855, 15 pages",
            "year": 2022
        },
        {
            "authors": [
                "A. Bhardwaj",
                "H. Bhardwaj",
                "A. Sakalle",
                "Z. Uddin",
                "M. Sakalle"
            ],
            "title": "andW",
            "venue": "Ibrahim, \u201cTree-based and machine learning algorithm analysis for breast cancer classifcation,\u201d Computational Intelligence and Neuroscience, vol. 2022, Article ID 6715406, 6 pages",
            "year": 2022
        },
        {
            "authors": [
                "H. Dong",
                "L. Ma"
            ],
            "title": "Prediction model of triple negative breast cancer based on machine learning,",
            "venue": "Journal of Yunnan University,",
            "year": 2017
        },
        {
            "authors": [
                "H. Wang",
                "B. Zheng",
                "S.W. Yoon"
            ],
            "title": "and H",
            "venue": "S. Ko, \u201cA support vector machine-based ensemble algorithm for breast cancer diagnosis,\u201d European Journal of Operational Research, vol. 267, no. 2, pp. 687\u2013699",
            "year": 2018
        },
        {
            "authors": [
                "B. Zheng",
                "S.W. Yoon"
            ],
            "title": "and S",
            "venue": "S. Lam, \u201cBreast cancer diagnosis based on feature extraction using a hybrid of k-means and support vector machine algorithms,\u201d Expert Systems with Applications, vol. 41, no. 4, pp. 1476\u20131482",
            "year": 2014
        },
        {
            "authors": [
                "X.S. Jia",
                "X. Sun"
            ],
            "title": "and X",
            "venue": "Zhang, \u201cBreast cancer identifcation using machine learning,\u201d Mathematical Problems in Engineering, vol. 2022, Article ID 8122895, 8 pages",
            "year": 2022
        },
        {
            "authors": [
                "S. Singh",
                "S.K. Jangir"
            ],
            "title": "M",
            "venue": "Kumar et al., \u201cFeature importancescore-based functional link artifcial neural networks for breast cancer classifcation,\u201d BioMed Research International, vol. 2022, Article ID 2696916, 8 pages",
            "year": 2022
        },
        {
            "authors": [
                "T.R. Mahesh",
                "V. Vinoth Kumar",
                "V. Muthukumaran",
                "H.K. Shashikala",
                "B. Swapna"
            ],
            "title": "and S",
            "venue": "Guluwadi, \u201cPerformance analysis of xgboost ensemble methods for survivability with the classifcation of breast cancer,\u201d Journal of Sensors, vol. 2022, Article ID 4649510, 8 pages",
            "year": 2022
        },
        {
            "authors": [
                "O.L. Mangasarian",
                "W.H. Wolberg"
            ],
            "title": "Cancer Diagnosis Via Linear Programming",
            "venue": "University of Wisconsin-Madison Department of Computer Sciences, Madison, WI, USA",
            "year": 1990
        },
        {
            "authors": [
                "M.K. Das",
                "A. Chaudhary",
                "A. Bryan",
                "M.H. Wener",
                "S.L. Fink"
            ],
            "title": "and C",
            "venue": "Morishima, \u201cRapid screening evaluation of sars-cov-2 igg assays using z-scores to standardize results,\u201d Emerging Infectious Diseases, vol. 26, no. 10, pp. 2501\u20132503",
            "year": 2020
        },
        {
            "authors": [
                "X. Su",
                "Y. Wang"
            ],
            "title": "Feature selection algorithm for highdimensional unbalanced medical data,",
            "venue": "Small Microcomputer Systems,",
            "year": 2022
        },
        {
            "authors": [
                "P. Chen",
                "F. Li"
            ],
            "title": "and C",
            "venue": "Wu, \u201cResearch on intrusion detection method based on Pearson correlation coefcient feature selection algorithm,\u201d Journal of Physics: Conference Series, vol. 1757, no. 1, Article ID 012054",
            "year": 2021
        },
        {
            "authors": [
                "C. Lu",
                "H. Shen"
            ],
            "title": "Css: handling imbalanced data by improved clustering with stratifed sampling,",
            "venue": "Concurrency and Computation: Practice and Experience,",
            "year": 2020
        },
        {
            "authors": [
                "L. Sol",
                "Z. Sophia",
                "H. Solveig",
                "A.L. Seidler",
                "D. Bernardi"
            ],
            "title": "and K",
            "venue": "Laqng, \u201cAn individual participant data meta-analysis of breast cancer detection and recall rates for digital breast tomosynthesis versus digital mammography population screening,\u201d Clinical Breast Cancer, vol. 22, no. 5",
            "year": 2022
        },
        {
            "authors": [
                "M. Hossin",
                "S.Mn"
            ],
            "title": "A review on evaluationmetrics for data classifcation evaluations,",
            "venue": "International Journal of Data Mining & Knowledge Management Process (IJDKP),",
            "year": 2015
        },
        {
            "authors": [
                "F. Rahmad",
                "K. Ramli"
            ],
            "title": "and Y",
            "venue": "Suryanto, \u201cPerformance comparison of anti-spam technology using confusion matrix classifcation,\u201d IOP Conference Series: Materials Science and Engineering, vol. 879, no. 1, Article ID 012076",
            "year": 2020
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "Xgboost: a scalable tree boosting system,",
            "venue": "Proceedings of the 22nd ACM Sigkdd International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "H. Li",
                "Y. Zhu"
            ],
            "title": "Optimization of xgboost algorithm based on gradient distribution adjustment strategy,",
            "venue": "Computer Applications,",
            "year": 2020
        },
        {
            "authors": [
                "L. Breiman"
            ],
            "title": "Random forests,",
            "venue": "Machine Learning,",
            "year": 2001
        },
        {
            "authors": [
                "B. Pes"
            ],
            "title": "Learning from high-dimensional and class-imbalanced datasets using random forests,",
            "venue": "Information, vol. 12,",
            "year": 2021
        },
        {
            "authors": [
                "L. Breiman"
            ],
            "title": "Bagging predictors,",
            "venue": "Machine Learning,",
            "year": 1996
        },
        {
            "authors": [
                "S.H.Walker",
                "D.B. Duncan"
            ],
            "title": "Estimation of the probability of an event as a function of several independent variables,",
            "venue": "Biometrika, vol. 54,",
            "year": 1967
        },
        {
            "authors": [
                "M. Nourelahi",
                "Z. Ali",
                "A. Talei"
            ],
            "title": "and S",
            "venue": "Tahmasebi, \u201cA model to predict breast cancer survivability using logistic regression,\u201d Middle East Journal of Cancer, vol. 10, no. 2, pp. 132\u2013138",
            "year": 2019
        },
        {
            "authors": [
                "T. Cover",
                "P. Hart"
            ],
            "title": "Nearest neighbor pattern classifcation,",
            "venue": "IEEE Transactions on InformationTeory,",
            "year": 1967
        },
        {
            "authors": [
                "S.H. Rukmawan",
                "F.R. Aszhari",
                "Z. Rustam"
            ],
            "title": "and J",
            "venue": "Pandelaki, \u201cCerebral infarction classifcation using the k-nearest neighbor and naive bayes classifer,\u201d Journal of Physics: Conference Series, vol. 1752, no. 1, Article ID 012045",
            "year": 2021
        },
        {
            "authors": [
                "W.A. Yousef"
            ],
            "title": "Estimating the standard error of cross-validation-based estimators of classifer performance,",
            "venue": "Pattern Recognition Letters,",
            "year": 2021
        },
        {
            "authors": [
                "G. Hu",
                "K.C.S. Kwok"
            ],
            "title": "Predicting wind pressures around circular cylinders using machine learning techniques,",
            "venue": "Journal of Wind Engineering and Industrial Aerodynamics,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "Research Article",
            "text": ""
        },
        {
            "heading": "Classification Prediction of Breast Cancer Based on",
            "text": ""
        },
        {
            "heading": "Machine Learning",
            "text": "Hua Chen , Nan Wang , Xueping Du, Kehui Mei , Yuan Zhou , and Guangxing Cai\nSchool of Science, Hubei University of Technology, Wuhan 430000, China\nCorrespondence should be addressed to Hua Chen; 20070002@hbut.edu.cn\nReceived 19 May 2022; Revised 8 December 2022; Accepted 2 January 2023; Published 11 January 2023\nAcademic Editor: U\u0308mit Ag\u0306bulut\nCopyright \u00a9 2023 Hua Chen et al. Tis is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nBreast cancer is the most common and deadly type of cancer in the world. Based onmachine learning algorithms such as XGBoost, random forest, logistic regression, and K-nearest neighbor, this paper establishes diferent models to classify and predict breast cancer, so as to provide a reference for the early diagnosis of breast cancer. Recall indicates the probability of detecting malignant cancer cells in medical diagnosis, which is of great signifcance for the classifcation of breast cancer, so this article takes recall as the primary evaluation index and considers the precision, accuracy, and F1-score evaluation indicators to evaluate and compare the prediction efect of eachmodel. In order to eliminate the infuence of diferent dimensional concepts on the efect of themodel, the data are standardized. In order to fnd the optimal subset and improve the accuracy of the model, 15 features were screened out as input to the model through the Pearson correlation test. Te K-nearest neighbor model uses the cross-validation method to select the optimal k value by using recall as an evaluation index. For the problem of positive and negative sample imbalance, the hierarchical sampling method is used to extract the training set and test set proportionally according to diferent categories. Te experimental results show that under diferent dataset division (8 : 2 and 7 : 3), the prediction efect of the same model will have diferent changes. Comparative analysis shows that the XGBoost model established in this paper (which divides the training set and test set by 8 : 2) has better efects, and its recall, precision, accuracy, and F1-score are 1.00, 0.960, 0.974, and 0.980, respectively."
        },
        {
            "heading": "1. Introduction",
            "text": "In the past ten years, the incidence of breast cancer in China has increased by 47%, and the incidence is increasing year by year, and the incidence of breast cancer is gradually younger [1]. Te pathogenesis of breast cancer is related to personal hormones, family history, marriage, and childbearing history [2]. Breast cancer is not easy to detect in the early stage, and has the characteristics of the early age of onset but late presentation [3, 4]. At present, the main diagnosis of breast cancer is based on three methods: puncture cytology [5], ultrasound scan [6], and mammogram X-ray [7]. If a patient is caught early in breast cancer, the more likely it is to be cured and the better the prognosis. Terefore, regular examination and early diagnosis are very necessary for the prevention and timely detection of breast cancer.\nIn the medical feld, the establishment of models through machine learning methods can assist doctors to improve the\ndetection rate of cancer, so as to achieve the purpose of early detection and early treatment. Machine learning methods have yielded good results in the diagnosis of cancer [8, 9]. Wu et al. [10] observed the cell morphology under the microscope and found that there were obvious diferences between breast cancer cells and normal healthy cell parameters. Tis fnding provides a theoretical basis for many studies. While there are many machine learning methods currently applied to breast cancer cell classifcation, no single algorithm can be applied to all problems. Each type of machine learning algorithm has its own areas of expertise, so the choice of algorithm is diferent in diferent scenarios.\nShen et al. [11] used the XGBoost model to classify and predict breast cancer, and the accuracy reached 97.86%, and the recall reached 95.83%. Deng et al. [12] used the XGBoost algorithm to classify and predict breast cancer with an accuracy of 0.96 and a recall of 0.97. Monirujjaman Khan et al. [13] used multiple machine learning models to identify\nHindawi Computational Intelligence and Neuroscience Volume 2023, Article ID 6530719, 9 pages https://doi.org/10.1155/2023/6530719\nbreast cancer, and random forest, decision tree, K-nearest neighbor, and logistic regression were the algorithms with higher F1-score, 96%, 95%, 90%, and 98%, respectively. Bhardwaj et al. [14] used multilayer perceptron (MLP), K-nearest neighbor (KNN), genetic algorithm (GP), and random forest (RF) to classify benign and malignant breast cancer cells, and the experimental results showed that the optimal classifer was RF with a classifcation accuracy of 96.24%. Dong and Ma [15] studied the possible markers of triple-negative breast cancer, and machine learning algorithms were used to predict whether people had triplenegative breast cancer. Te results show that the accuracy of the support vector machine (SVM) classifcation prediction model reaches 97.8%. In order to improve the accuracy of breast cancer identifcation methods and improve machine learning algorithms, Wang et al. [16] proposed a weighted AUC ensemble learning model based on SVM for breast cancer diagnosis, using C-SVM and V-SVM with 6 kernel functions to increase the diversity of the base model set and comparing diferent decision results with the Area Integration (WAUCE) model under the weighted receiving working characteristic curve. Te results show that on the small dataset, the proposed WAUCE structure reduces the variance of the diagnostic accuracy by up to 69.23% and improves the accuracy by 0.94%. Zheng et al. [17] tested the Wisconsin Breast Cancer (WDBC) dataset according to the K-means and support vector machine hybrid algorithm extracts tumor features and diagnoses breast cancer, and the results show that the hybrid algorithm improves the accuracy to 97.38%. Jia et al. [18] proposed a new population optimization algorithm, Whale Optimization Algorithm (WOA), which intelligently adjusts the parameters of the SVM model, and the experimental results show that the performance of the WOA-SVM model is signifcantly better than that of the traditional breast cancer recognition model, with an accuracy of 97.5%. In order to solve the problem of overftting of machine learning techniques in breast cancer classifcation, Singh et al. [19] proposed a functionally connected artifcial neural network (FLANN) and experimentally found that the model has high accuracy for early diagnosis of breast cancer. Mahesh et al. [20] propose a breast cancer prediction XGBoost ensemble technique based on known feature patterns, frst using synthetic minority oversampling technology (SMOTE) to deal with data imbalance and noise problems and then using na\u0131\u0308ve Bayes classifer, decision tree classifer, and random forest, respectively, combined with XGBoost and classifying the data. According to experimental analysis, XGBoost-Random Forest ensemble classifer has an accuracy rate of 98.20% in the early detection of breast cancer.\nBased on XGBoost, random forest, logistic regression, K-nearest neighbor, and other machine learning methods, this paper establishes diferent models to classify and predict breast cancer, which provides a reference for early diagnosis of breast cancer. When most studies apply machine learning models to breast cancer cell diagnosis, they focus on using the precision, accuracy, and F1-score of the model as indicators to evaluate the quality of the model, while ignoring the medical diagnostic signifcance of the recall of the model,\nwhich indicates the proportion of malignant breast cancer cells that are predicted, and the higher the recall, the greater the probability of malignant cells being predicted in breast cancer cells. Terefore, this article takes recall as the primary index and considers precision, accuracy, and F1-score to evaluate the model used.\nIn the modeling process, data preprocessing is a very important part, and the efect of the predictive model is diferent depending on the processing method. In order to eliminate the infuence of diferent dimensional concepts on the efect of the model, the data is standardized. In order to fnd the optimal subset and improve the accuracy of the model, feature selection was made according to the Pearson correlation coefcient between the feature variable and the target variable. For the problem of positive and negative sample imbalance, the hierarchical sampling method is used to extract the training set and test set proportionally according to diferent categories. Considering that the prediction efect of machine learning models varies under diferent dataset divisions, this paper will use diferent dataset divisions (8 : 2 and 7 : 3) as two sets of experiments to observe the prediction efect of the model established in this paper."
        },
        {
            "heading": "2. Data Preprocessing",
            "text": "2.1. Data Introduction. Te data set used in this paper is the breast cancer data in the UCI data set, which was provided by the famous Dr. William from the Clinical Medicine Research Institute of the University of Wisconsin [21]. Features are computed from a digitized image of a fne needle aspirate (FNA) of a breast mass. Tey describe the characteristics of the cell nuclei present in the image. Te data set contained 569 experimental samples, including 357 benign samples and 212 malignant samples of breast cancer. For the cells extracted from each experimental object, the following ten features of its nucleus are mainly collected: radius (mean of the distance from center to points on the perimeter), perimeter, smoothness (local variation in radius lengths), area, compactness (perimeter\u2217 \u2217 2/area-1.0), concavity (severity of concave portions of the contour), symmetry, texture (standard deviation of gray-scale values), concave points (number of concave portions of the contour), and fractal_dimension (\u201ccoastline approximation\u201d-1). Te mean, standard error, and \u201cworst\u201d or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. Te classifcation label represents the type of breast cancer. Terefore, the sample data set contains a total of 30 features and one sample label feature (malignant and benign).\n2.2. Data Standardization. By observing the value range of each feature, it is found that the data values of diferent features difer greatly. In some models, diferent dimensions have a great infuence on the prediction efect. For example, the k-nearest neighbor algorithm based on distance division needs to keep the data dimension consistent, so the data need to be standardized before modeling. However, some\nmodels are less afected by dimensionality, such as the random forest algorithm. In order to make the experiment comparative, the data of diferent models are treated in the same way.\nFor problems with diferent sample data dimensions, the commonly used dimensionless processing methods include data standardization, and data standardization methods include Min-max standardization and Z-score standardization. Among them, when the data used have outliers outside the value range, or the maximum and minimum values of some indicators are unknown, the Z-score standardization can be used.\nIn this paper, according to the characteristics of the WDBC breast cancer dataset, the Z-score standardization was selected to process the data.Te data processed by the Zscore standardization [22] follows a standard normal distribution, that is, the mean is 0 and the variance is 1. Te formula for Z-score standardization is as follows:\nx \u2217 \ufffd x \u2212 mean\nstd , (1)\nwhere mean is the mean of sample characteristic data and std is the standard deviation of sample characteristic data. Te data standardization results are shown in Table 1.\n2.3. Feature Selection. As an important part of the data preprocessing process, feature selection is to fnd the optimal subset, feature selection can reduce redundant and useless features to improve the accuracy of the model. Te feature selection method is generally divided into the overthinking method, the encapsulation method, and the embedding method. Te fltering method can be independent of the algorithm used later in the study and has high computational efciency and strong generalization ability [23], so the feature selection method in this paper uses the flter method, and the general method summary in the fltering method is shown in Table 2.\nTe breast cancer data after 0 mean normalization meet the requirements of the Pearson correlation coefcient test in the fltering method; so in this paper, Pearson\u2019s correlation coefcient [24] is used to test the correlation between each feature and the target variable. Pearson\u2019s correlation coefcient formula is as follows:\n\u03c1X1X2 \ufffd Cov X1, X2( \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd DX1 \u2217DX2  \ufffd EX1X2 \u2212 EX1 \u2217EX2\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd DX1 \u2217DX2  , (2)\nwhere \u03c1X1X2 represents the correlation coefcient between two variables, Cov(X1, X2) represents the covariance between two variables, EX1 represents the expectation of variables, and DX1 represents the variance of variables.\nAccording to the Pearson correlation coefcient, there are 15 features whose absolute value of the correlation coefcient with the target variable is greater than or equal to 0.5. Tese 15 feature variables are used for model construction, and the 15 feature and target variables are shown in Table 3."
        },
        {
            "heading": "3. Model Construction",
            "text": "In this paper, the categories of breast cancer are predicted based on XGBoost, random forest, logistic regression, and K-nearest Neighbor model, respectively. Malignant breast cancer is regarded as a positive sample, while benign breast cancer is regarded as a negative sample.\nTo solve the problem of sample imbalance, this paper uses a stratifed sampling method [25] to extract the training set and test set in proportion to all kinds of sample data. Stratifed sampling is also called type sampling. Te sample population is divided into subpopulations that are independent of each other. Random sampling was carried out in proportion in each subpopulation. Stratifed sampling draws a more representative sample and is more suitable for unbalanced samples.\nDiferent data set partitioning may lead to diferent model efects. Terefore, this paper carries out two groups of experiments according to a diferent division of the sample data set. Te frst group divided the data set into a training set and test set in a ratio of 8 : 2, and the second group divided the data set into a training set and test set in a ratio of 7 : 3. Observe the model performance of the four algorithms under diferent data set partitioning.\n3.1. Evaluation Indicators. In this study, accuracy, precision, recall, and F1-score [26, 27] were used to evaluate the prediction efect of the model. Considering the particularity of medical diagnosis, it is expected that all malignant breast cancer can be predicted. Terefore, recall is taken as an important evaluation index here. Te higher the recall is, the higher the proportion of malignant breast cancer that can be predicted. Te model classifcation results can generate a confusion matrix [28], as shown in Table 4.\nHere, TP is a true positive, indicating the number of positive samples predicted as positive samples. TN is a true negative, indicating the number of negative samples predicted as negative samples. FP is a false positive, indicating the number of positive samples predicted from negative samples, which is called type 1 error. FN is a false negative, indicating the number of positive samples predicted as negative samples, which is called type 2 error.\nPrecision, abbreviated as P. For the predicted results, precision represents how many of the positive predicted samples are really positive samples, and the formula is\nPrecision \ufffd TP\nTP + FP . (3)\nRecall is also known as the true positive rate. For the original samples, the recall represents how many positive samples in the samples are predicted correctly, and the formula is\nRecall \ufffd TP\nTP + FN . (4)\nAccuracy, referred to as A, refers to the proportion of all correctly predicted samples (including positive samples and negative samples) in the total sample. Te formula is\nAccuracy \ufffd TP + TN\nTP + TN + FP + FN . (5)\nF1-score is obtained by the weighted harmonic average of precision and recall due to the contradiction between the two evaluation indexes. F1-score is a comprehensive\nevaluation index of external methods, and a higher value indicates that the classifcation results are more efective.Te formula of index F1-score is\nF1 \ufffd 2PR\n(P + R) . (6)"
        },
        {
            "heading": "3.2. Prediction Model of Breast Cancer Based on XGBoost.",
            "text": "XGBoost, short for extreme gradient boosting, is a Boosting algorithm [29]. Both XGBoost and random forest are integration algorithms based on the decision tree. Diferent from the Bagging algorithm, Boosting algorithm builds weak learners one by one, accumulating multiple weak learners through continuous iteration [30]. Te objective function is\nobj \ufffd  m\ni\ufffd1 l yi, yi(  + \nK\nk\ufffd1 \u03a9 fk( , (7)\nwhere i represents the ith sample, m represents the sample size corresponding to the kth decision tree, and K represents the currently established weak learner. Te frst part of the objective function is the loss function, which measures the diference between the predicted value and the true value. Te second part of the function represents the complexity of the model.\nIn order to optimize the tree after the t-th iteration, Taylor expansion is performed on the objective function. Ten, the objective function can be converted into\nobj \ufffd  m\ni\ufffd1 ft xi( gi +\n1 2 ft xi( (  2 hi  +\u03a9 ft( , (8)\nwhere gi is the frst derivative of loss function l(yti , y (t\u22121) i ) with respect to y(t\u22121)i , and hi is the second derivative of loss function l(yti , y (t\u22121) i ) with respect to y (t\u22121) i . Te regularization expression of L2 in the formula is\n\u03a9 ft(  \ufffd c T + 1 2 \u03bb\u2016\u03c9\u20162. (9)\nTe addition of regular term can reduce the variance of the model and make the model obtained by the training set more simple, so as to prevent the occurrence of overftting.\nTe XGBoost algorithm is used to train the model on the training data set. In the process of model training, the parameters are adjusted to obtain a better set of parameters, and fnally the optimal prediction model is obtained. Te model was used to predict breast cancer categories on the test set.\nWhen the data set was divided into a training set and test set by 8 : 2, the accuracy, precision, recall, and F1-score of the XGBoost model were 0.974, 0.960, 1.00, and 0.980, respectively. When the XGBoost model was divided into a training set and test set by 7 : 3, the accuracy, precision, recall, and F1-score of the XGBoost model were 0.959, 0.946, 0.991, and 0.968, respectively. Te results show that the XGBoost model has better prediction performance when the data set is divided by 8 : 2. Te recall rate of 1 indicates that the XGBoost model correctly predicted all malignant breast cancers in the sample, which is very important for medical diagnosis.\n3.3. Prediction Model of Breast Cancer Based on Random Forest. Random forest is a supervised learning algorithm that integrates multiple trees through the Bagging idea [31\u201333].Te bootstrap method is used to extract the training sample set from the original sample data, and the\ncorresponding decision tree model is trained for each training set. Finally, all base classifers are voted on, and the one with the most votes is the fnal category.\nWhen the data set was divided into a training set and test set by 8 : 2, the accuracy, precision, recall, and F1-score of the random forest model were 0.965, 0.947, 1.00, and 0.973, respectively. When the data set was divided into a training set and test set by 7 : 3, accuracy, precision, recall, and F1score were 0.953, 0.946, 0.981, and 0.963, respectively. Te results show that the random forest model has better prediction performance when the data set is divided by 8 : 2.Te recall rate of this model was also 1, indicating that the random forest model also correctly predicted all malignant breast cancer.\n3.4. Prediction Model of Breast Cancer Based on Logistic Regression. LR, Logistic Regression, is one of the most widely used methods in medical data analysis [34, 35]. Logistic regression is a sigmoid function nested on the basis of a multiple linear regression model. Te basic form is\ny(x) \ufffd exp \u03b80 + \u03b81x1 + . . . + \u03b8kxk( \n1 + exp \u03b80 + \u03b81x1 + . . . + \u03b8kxk(  , (10)\nwhere \u03b80, \u03b81 . . . \u03b8k is similar to the regression coefcient in multiple linear regression.\nWhen the data set was divided into a training set and test set by 8 : 2, accuracy, precision, recall, and F1-score of the logistic regression model were 0.947, 0.923, 1.00, and 0.960, respectively. When the data set was divided into a training set and test set by 7 : 3, accuracy, precision, recall, and F1score were 0.947, 0.922, 1.00, and 0.960, respectively. It can be seen from the results that the prediction efect of the logistic regression model is consistent under the two partitioning conditions. Te recall was also 1, indicating that all malignant breast cancer was correctly predicted by the logistic regression model.\n3.5. Prediction Model of Breast Cancer Based on K-Nearest Neighbor. Te K-nearest neighbor algorithm [36, 37] projects samples into higher dimensional space according to variable values. Similar samples show spatial aggregation in higher dimensional space. Euclidean distance is commonly used to measure distances in k-nearest neighbors, and the calculation method is as follows:\nd xi, xj  \ufffd  n\nl\ufffd1 x\nl i \u2212 x l j   2 \u239b\u239d \u239e\u23a0\n1/2\n, (11)\nwhere xi, xj represents two diferent samples, xli represents the value of sample i on attribute l, and xlj represents the value of sample j on attribute l.\nTe three basic elements of the k-nearest neighbor algorithm are distance measurement, k-value selection, and classifcation decision rule.\nFor the problem of K value selection in the k-nearest neighbor algorithm, this paper uses the tenfold cross-validation method [38, 39] and takes the recall rate as the model\nevaluation index to select an appropriate k value. Let the value range of k be 1\u201340, and for each k, the cross-validation of tenfold is performed.Te k value with themaximum recall rate is the optimal k value. Te recall of diferent k values under the cross-validation of tenfold is shown in Figure 1.\nIt can be seen from Figure 1 that as k value increases, the recall decreases. When k\ufffd 3 and k\ufffd 5, the recall is the largest. Because the k value is set too small, it is easy to overft, so the k value is set as 5 here.\nWhen the data set was divided into a training set and test set by 8 : 2, the accuracy, precision, recall, and F1-score of the k-nearest Neighbor model were 0.912, 0.888, 0.986, and 0.934, respectively. When the data set was divided into a training set and test set by 7 : 3, accuracy, precision, recall, and F1-score were 0.930, 0.906, 0.991, and 0.946, respectively. Te results show that the k-nearest neighbor model has better prediction performance when the data set is divided by 7 : 3."
        },
        {
            "heading": "4. Comparison and Analysis",
            "text": "In order to better understand the performance of the model established in this paper, this paper is based on the Python 3.9.7 development environment and uses the breast cancer data provided by Dr. William of the University of Wisconsin Clinical Medical Research Institute for experiments.\nTe experimental environment is Windows 11 operating system, the processor is Intel(R) Core(TM) i5-1155G7@ 2.50GHz 2.50GHz, and the memory is 8.00GB.\nTe experimental parameters of eachmodel are shown in Table 5, and the following three comparative analysis results will be carried out: (1) performance comparison of each model in this paper when the data set is divided into training set and test set in 8 : 2. (2) Performance comparison of each model in this paper when the data set is divided into a training set and test set in 7 : 3. (3) Comparison with some models in the literature [11\u201314].\n(1) When the data set is divided into a training set and test set by 8 : 2, the performance of each model is shown in Table 6. As can be seen fromTable 4, when dividing the training set and test set in a ratio of 8 : 2, the accuracy of the four machine learning methods is above 0.9, among which XGBoost and RF are above 0.95. Te prediction accuracy of XGBoost is 0.974, indicating a high prediction accuracy. For precision, the precision of the KNN model is not high, below 0.9, only 0.888. XGBoost has the highest precision, which is 0.960. As for the recall, the recall of XGBoost, random forest, and logistic regression algorithms are all 1. K-nearest neighbor algorithm has the lowest recall, but it is also above 0.95. For this study, recall rates represent the proportion of malignant breast cancer samples that were correctly diagnosed. In medicine, it is very important for a disease to be diagnosed.Te consequence of not being diagnosed is delayed treatment, whichmay result in patients missing the best time for treatment. Tis is much more serious than being diagnosed with a\ndisease without having one. Terefore, the recall rate is a very important indicator in the feld of disease diagnosis. Here, the recall of XGBoost, random forest, and logistic regression algorithm are all 1, indicating that all malignant breast cancers in the samples have been diagnosed. For F1-score, it can be seen that the F1-score of XGBoost, random forest, and logistic regression are all above 0.95. Te F1-score of the XGBoost algorithm is the highest, reaching 0.980. Te K-nearest neighbor model has the lowest F1-score of 0.934. Taking the four indicators into consideration, it can be said that when the data set is divided into a training set and test set by 8 : 2, the overall model efect of the XGBoost algorithm is better than the other three models. XGBoost not only achieved a recall of 1 but also achieved a recall of 0.95 ormore for the other three metrics.\n(2) When the data set is divided into a training set and test set in 7 : 3, the performance of each model is shown in Table 7. As can be seen from Table 7, when the training set and test set are divided by 7 : 3, the model efect of XGBoost and RF is obviously not as good as that of 8 : 2. First of all, in terms of the important index recall, when dividing the data set by 8 : 2, the recall of XGBoost and RF were both 1, but now they have decreased to 0.991 and 0.981, respectively. Te two models also have slightly decreased in the other three indexes. However, the change of the prediction efect of logistic regression and the K-nearest neighbor model is diferent from these two algorithms. For the logistic regression model, the four indicators barely changed when the training set and test set were divided by 7 : 3 and 8 : 2, respectively.Tis shows that the logistic regressionmodel is almost not afected by diferent data set partition. In the case of a 7 : 3 split between the training set and the test set, logistic regression showed lower accuracy, precision, and F1score than XGBoost and random forest. However, the recall of the logistic regression model is 1, indicating that all malignant breast cancer has been predicted, which is of great signifcance for disease diagnosis. Terefore, it can be said that the prediction efect of the logistic regression model is better than the other three algorithms. For the KNNmodel, when the training set and test set were divided by 7 : 3, all four indexes increased. Te recall increased to 0.991, which was higher than that of the random forest. Te accuracy, precision, and F1-score increased from 0.912, 0.887, and 0.934 to 0.930, 0.906, and 0.946, respectively. Terefore, the KNN model performs better in the case of the training set and test set divided by 7 : 3 than the model divided by 8 : 2. Te analysis shows that the division of data sets is not fxed. For diferent models, diferent divisions bring diferent changes in the model prediction efect.\n(3) According to the comparative analysis of Tables 6 and 7, the XGBoost model established in this paper\n(dividing the training set and test set by 8 : 2) has the best efect, and the following compares it with the model performance in the literature [11\u201314], and the specifc results are shown in Table 8.\nAs can be seen fromTable 8, the better performingmodels of the fve models are the Logistic regression model of the literature [13] and the XGBoost model established in this paper. Te recall and accuracy of the model in the literature [13] are 0.99 and 0.98, respectively, and the recall and accuracy of the model in this paper are 1.00 and 0.974, respectively, compared with the literature [13], the recall of the model is high, and the recall inmedical diagnosis indicates the probability of detecting malignant cancer cells, which is of\ngreat signifcance for the classifcation of breast cancer cells, so the XGBoost model established in this paper has a better prediction efect and can be used as a medical tool to assist doctors to make treatment plans for breast cancer patients.\nTable 6: Te model efect of dividing the dataset by 8 : 2.\nAccuracy Precision Recall F1-score XGBoost 0.974 0.960 1.00 0.980 RF 0.965 0.947 1.00 0.973 LR 0.947 0.923 1.00 0.960 KNN 0.912 0.888 0.986 0.934\nTable 7: Te model efect of dividing the dataset by 7 : 3.\nAccuracy Precision Recall F1-score XGBoost 0.959 0.946 0.991 0.968 RF 0.953 0.946 0.981 0.963 LR 0.947 0.922 1.00 0.960 KNN 0.930 0.906 0.991 0.946"
        },
        {
            "heading": "5. Conclusion",
            "text": "Tis paper mainly predicted the categories of breast cancer, from data preprocessing to feature selection, and then to the establishment of the model. Finally, the prediction results were compared and analyzed from many aspects.\nIn this paper, recall is taken as an important index to predict malignant breast cancer samples as accurately as possible. Te original data set contained 30 features, and 15 features were selected as the input of the model through the Pearson correlation test. Before model construction, data were standardized to eliminate the impact of diferent dimensions on model efects. For the problem of unbalanced positive and negative samples, the stratifed sampling method is used to extract training sets and test sets proportionally according to diferent categories of data. When selecting the optimal k value in the k-nearest neighbor, the recall is used as the model evaluation index, so that the k value with the highest recall rate is the optimal value.\nTe models are compared and analyzed from three aspects. Te results are shown as follows:\n(1) In the case of dividing the training set and the test set by 8 : 2, the recall of XGBoost, random forest, and logistic regression is 1, which can predict all malignant breast cancer, and the K-nearest neighbor recall is slightly lower than 0.986 compared with the other three models. For the prediction accuracy, precision, and F1-score of the model, the results of the XGBoost model are better than the results of random forest and logistic regression, which are 0.974, 0.96, and 0.98, respectively, so the XGboost model is selected as the fnal prediction model under the condition of 8 : 2 division of the training set and the test set.\n(2) In the case of dividing the training set and the test set at 7 : 3, the values of the four evaluation indicators for XGBoost and random forest decreased, while the values of the four evaluation indicators for the K-nearest neighbor model were improved, but for the recall, only the recall of the logistic regression model was 1, and the other models were above 0.98, so the model prediction efect of logistic regression was the best, and the prediction accuracy, precision, and F1-score of logistic regression were 0.947, 0.922, and 0.96, respectively.\n(3) It can be seen from experiments that under diferent divisions, the prediction efect of the model has diferent changes. Comparing the optimal models in the two sets of diferent experiments, it can be seen that the prediction accuracy, precision, and F1-score of the XGBoost model (which divides the training set and the test set by 8 : 2) are higher than that of the logistic regression model (which divides the training set and the test set by 7 : 3) when the recall is 1, so the XGBoost model (which divides the training set and the test set by 8 : 2) works best in the model established in this paper. In addition, compared with the models in the literature [11\u201314], the XGBoost model\nestablished in this paper has a better efect and can accurately identify malignant breast cancer cells. However, this research is limited to numerical datasets, and in the future, we will try to use deep learning algorithms to apply various feature extraction techniques to image data (such as X-ray images) to obtain better classifcation results."
        },
        {
            "heading": "Data Availability",
            "text": "Te data that support the fndings of this study are openly available in UCI Machine Learning Repository at https:// archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+ %28Diagnostic%29."
        },
        {
            "heading": "Conflicts of Interest",
            "text": "Te authors declare that they have no conficts of interest."
        },
        {
            "heading": "Acknowledgments",
            "text": "Tis work was supported by the National Natural Science Foundation of China (Grant no. 61502156), Teaching and Research Project of Hubei Education Committee (Grant no. 282), and Doctoral Foundation of Hubei University of Technology (Grant no. BSQD13051)."
        }
    ],
    "title": "Classification Prediction of Breast Cancer Based on Machine Learning",
    "year": 2023
}