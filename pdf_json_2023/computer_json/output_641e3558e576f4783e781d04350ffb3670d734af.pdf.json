{
    "abstractText": "Vision transformers (ViT) have been of broad interest in recent theoretical and empirical works. They are state-of-the-art thanks to their attention-based approach, which boosts the identification of key features and patterns within images thanks to the capability of avoiding inductive bias, resulting in highly accurate image analysis. Meanwhile, neoteric studies have reported a \u201csparse double descent\u201d phenomenon that can occur in modern deep-learning models, where extremely overparametrized models can generalize well. This raises practical questions about the optimal size of the model and the quest over finding the best trade-off between sparsity and performance is launched: are Vision Transformers also prone to sparse double descent? Can we find a way to avoid such a phenomenon? Our work tackles the occurrence of sparse double descent on ViTs. Despite some works that have shown that traditional architectures, like Resnet, are condemned to the sparse double descent phenomenon, for ViTs we observe that an optimally-tuned l2 regularization relieves such a phenomenon. However, everything comes at a cost: optimal lambda will sacrifice the potential compression of the ViT.",
    "authors": [
        {
            "affiliations": [],
            "name": "Enzo Tartaglione"
        }
    ],
    "id": "SP:9c736c0675540045a6ad0b0d6646cb63e0c1d7c7",
    "references": [
        {
            "authors": [
                "C.A. Barbano",
                "E. Tartaglione",
                "C. Berzovini",
                "M. Calandri",
                "M. Grangetto"
            ],
            "title": "A twostep radiologist-like approach for covid-19 computer-aided diagnosis from chest xray images",
            "venue": "Image Analysis and Processing\u2013ICIAP 2022: 21st International Conference, Lecce, Italy, May 23\u201327, 2022, Proceedings, Part I. pp. 173\u2013184. Springer",
            "year": 2022
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems 33, 1877\u20131901",
            "year": 2020
        },
        {
            "authors": [
                "H.A.H. Chaudhry",
                "R. Renzulli",
                "D. Perlo",
                "F. Santinelli",
                "S. Tibaldi",
                "C. Cristiano",
                "M. Grosso",
                "A. Fiandrotti",
                "M. Lucenteforte",
                "D. Cavagnino"
            ],
            "title": "Lung nodules segmentation with deephealth toolkit",
            "venue": "Image Analysis and Processing. ICIAP 2022 Workshops: ICIAP International Workshops, Lecce, Italy, May 23\u201327, 2022, Revised Selected Papers, Part I. pp. 487\u2013497. Springer",
            "year": 2022
        },
        {
            "authors": [
                "Z. Dai",
                "H. Liu",
                "Q.V. Le",
                "M. Tan"
            ],
            "title": "Coatnet: Marrying convolution and attention for all data sizes",
            "venue": "Advances in Neural Information Processing Systems 34, 3965\u2013 3977",
            "year": 2021
        },
        {
            "authors": [
                "M. Dehghani",
                "J. Djolonga",
                "B. Mustafa",
                "P. Padlewski",
                "J. Heek",
                "J. Gilmer",
                "A. Steiner",
                "M. Caron",
                "R. Geirhos",
                "I Alabdulmohsin"
            ],
            "title": "Scaling vision transformers to 22 billion parameters",
            "venue": "arXiv preprint arXiv:2302.05442",
            "year": 2023
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net",
            "year": 2021
        },
        {
            "authors": [
                "P. Esser",
                "R. Rombach",
                "B. Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 12873\u201312883",
            "year": 2021
        },
        {
            "authors": [
                "T. Gale",
                "E. Elsen",
                "S. Hooker"
            ],
            "title": "The state of sparsity in deep neural networks",
            "venue": "arXiv preprint arXiv:1902.09574",
            "year": 2019
        },
        {
            "authors": [
                "S. Han",
                "J. Pool",
                "J. Tran",
                "W. Dally"
            ],
            "title": "Learning both weights and connections for efficient neural network",
            "venue": "Advances in neural information processing systems 28",
            "year": 2015
        },
        {
            "authors": [
                "Z. He",
                "Z. Xie",
                "Q. Zhu",
                "Z. Qin"
            ],
            "title": "Sparse double descent: Where network pruning aggravates overfitting",
            "venue": "International Conference on Machine Learning. pp. 8635\u20138659. PMLR",
            "year": 2022
        },
        {
            "authors": [
                "S. Khan",
                "M. Naseer",
                "M. Hayat",
                "S.W. Zamir",
                "F.S. Khan",
                "M. Shah"
            ],
            "title": "Transformers in vision: A survey",
            "venue": "ACM Comput. Surv. 54(10s)",
            "year": 2022
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 9992\u201310002",
            "year": 2021
        },
        {
            "authors": [
                "X. Ma",
                "H. Huang",
                "Y. Wang",
                "S. Romano",
                "S. Erfani",
                "J. Bailey"
            ],
            "title": "Normalized loss functions for deep learning with noisy labels",
            "venue": "International conference on machine learning. pp. 6543\u20136553. PMLR",
            "year": 2020
        },
        {
            "authors": [
                "X. Ma",
                "Y. Wang",
                "M.E. Houle",
                "S. Zhou",
                "S. Erfani",
                "S. Xia",
                "S. Wijewickrema",
                "J. Bailey"
            ],
            "title": "Dimensionality-driven learning with noisy labels",
            "venue": "International Conference on Machine Learning. pp. 3355\u20133364. PMLR",
            "year": 2018
        },
        {
            "authors": [
                "P.L. Mazzeo",
                "E. Frontoni",
                "S. Sclaroff",
                "C. Distante"
            ],
            "title": "Image Analysis and Processing",
            "venue": "ICIAP 2022 Workshops: ICIAP International Workshops, Lecce, Italy, May 23\u201327, 2022, Revised Selected Papers, Part I, vol. 13373. Springer Nature",
            "year": 2022
        },
        {
            "authors": [
                "P. Nakkiran",
                "G. Kaplun",
                "Y. Bansal",
                "T. Yang",
                "B. Barak",
                "I. Sutskever"
            ],
            "title": "Deep double descent: Where bigger models and more data hurt",
            "venue": "International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "P. Nakkiran",
                "P. Venkat",
                "S.M. Kakade",
                "T. Ma"
            ],
            "title": "Optimal regularization can mitigate double descent",
            "venue": "International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "V. Qu\u00e9tu",
                "E. Tartaglione"
            ],
            "title": "Dodging the sparse double descent",
            "venue": "arXiv preprint arXiv:2303.01213",
            "year": 2023
        },
        {
            "authors": [
                "V. Qu\u00e9tu",
                "E. Tartaglione"
            ],
            "title": "Can we avoid double descent in deep neural networks",
            "year": 2023
        },
        {
            "authors": [
                "J.T. Springenberg",
                "A. Dosovitskiy",
                "T. Brox",
                "M.A. Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings",
            "year": 2015
        },
        {
            "authors": [
                "S. Sukhbaatar",
                "J. Bruna",
                "M. Paluri",
                "L. Bourdev",
                "R. Fergus"
            ],
            "title": "Training convolutional networks with noisy labels",
            "venue": "arXiv preprint arXiv:1406.2080",
            "year": 2014
        },
        {
            "authors": [
                "H. Touvron",
                "M. Cord",
                "M. Douze",
                "F. Massa",
                "A. Sablayrolles",
                "H. J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "International conference on machine learning. pp. 10347\u201310357. PMLR",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems 30",
            "year": 2017
        },
        {
            "authors": [
                "J. Wei",
                "Z. Zhu",
                "H. Cheng",
                "T. Liu",
                "G. Niu",
                "Y. Liu"
            ],
            "title": "Learning with noisy labels revisited: A study using real-world human annotations",
            "venue": "International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "F.F. Yilmaz",
                "R. Heckel"
            ],
            "title": "Regularization-wise double descent: Why it occurs and how to eliminate it",
            "venue": "2022 IEEE International Symposium on Information Theory (ISIT). pp. 426\u2013431. IEEE",
            "year": 2022
        },
        {
            "authors": [
                "F. Yu",
                "K. Huang",
                "M. Wang",
                "Y. Cheng",
                "W. Chu",
                "L. Cui"
            ],
            "title": "Width & depth pruning for vision transformers",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence. vol. 36, pp. 3143\u20133151",
            "year": 2022
        },
        {
            "authors": [
                "L. Yuan",
                "Y. Chen",
                "T. Wang",
                "W. Yu",
                "Y. Shi",
                "Z.H. Jiang",
                "F.E. Tay",
                "J. Feng",
                "S. Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "Proceedings of the IEEE/CVF international conference on computer vision. pp. 558\u2013567",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Sparse double descent, transformers, pruning, deep learning"
        },
        {
            "heading": "1 Introduction",
            "text": "Deep neural networks (DNNs) have revolutionized the field of computer vision by achieving state-of-the-art results in tasks such as segmentation [3], classification [1], and object detection [16]. DNNs outperform conventional machine learning algorithms on many visual recognition tasks as they can automatically learn feature representations from raw input [6]. In addition, they can process a lot of data and generalize well to novel, unseen examples. For a long time, convolutional neural architectures (CNN) like VGG and ResNet models have been dominant in computer vision, thanks to their ability to learn, simultaneously, feature extraction (typically handled by convolutional layers) and classification\nArticle accepted for publication at the 22nd International Conference on Image Analysis and Processing (ICIAP23).\nar X\niv :2\n30 7.\n14 25\n3v 1\n[ cs\n.C V\n] 2\n6 Ju\nl 2 02\n(by multi-layer perceptrons, or in some cases even by convolutional layers themselves, like in ALL-CNN [21]). A new, deep architecture called Transformer has been pioneered by [24], and it has, at first, been conceived for natural language processing tasks [2], resulting in a break-through for the community. Given its big potential, the computer vision world has recently begun to adopt it [13]. Vision Transformers (ViT), which are Transformer architectures adapted for computer vision, quickly became state-ofthe-art for many tasks, out of which we cite generative models [8]. However, the lack of strong inductive biases causes them to be even more data-hungry than traditional CNN architectures, and this poses severe performance drops when noisy data are available to train such a model. In image classification tasks, noisy labels are a frequent issue that can negatively impact the performance of deep learning models [22]. Incorrect labels in the training data can mislead the model during the learning process and result in sub-optimal performance. Various approaches have been proposed to address this issue, including label smoothing, data augmentation, and robust loss functions [14]. The expected behavior, in such cases, is that the higher the noise in the data, the higher the overfit the model will suffer. As opposed to the traditional bias-variance trade-off, a phenomenon has been recently discovered, called Double Descent (DD) [17]. Namely, enlarging the model size in the overfitting regime worsens the performance of an over-parametrized network; then, the trend reverses. DD represents an important challenge in finding the optimal set of parameters since it shows that it is possible to potentially improve generalization in an over-parametrized regime, but without real indicators on the best model\u2019s size to adopt. This behavior is observed in various architectures, stretching from machine learning models to DNNs, such as standard CNNs and ResNet [26]. Analogously, a sparse double descent (SDD) phenomenon is observed when moving the model from an over-parametrized towards a sparser regime [11], via parameter pruning. In this paper, we show that ViTs also suffer from the SDD phenomenon: besides the burden of the lack of an inductive bias that could help these models to generalize, the occurrence of SDD makes the performance even worse in intermediate compression regimes, when only a part of the parameters is removed. This is a possible explanation for the fact that typical ViT architectures can not be pruned to similar extreme rates as traditional CNNs [27]. However, contrarily to what is suggested for traditional CNNs [19], ViTs can avoid SDD with the optimal tuning of \u21132 regularization, a result which was suggested by the theory [18]. We postulate this is possible thanks to the lack of an inductive bias embedded in the architecture, which favors the strong regularization necessary to avoid SDD. Such a discovery enables back all the traditional compression mechanisms, having as a stop criterion a worsening in performance on the validation set. Everything, however, comes at a cost: we observe that optimally regularized models are significantly less compressible, due to the strong prior we impose to avoid SDD. We summarize, here below, our key messages and contributions.\n\u2013 To the best of our knowledge, this is the first paper raising concerns on the potential occurrence of the sparse double descent phenomenon in ViT models. Through this work, we compare the behavior of ViT and ResNet in the typically employed test scenarios [17], also including a test on real annotated data (CIFAR-100N), observing SDD also on ViT. \u2013 We propose a quantitative study over the \u21132 regularization parameter, supported by the theory [18] but already proven as inapplicable to traditional CNNs [20]. We observe that, in ViT models, avoiding SDD is possible with a properly tuned value for the regularization, which nicely imposes a strong prior on the model\u2019s parameters, impossible in traditional architectures suffering from inductive bias. \u2013 We interestingly observe a trade-off between avoidance of SDD and compressibility of the model. More specifically, to avoid SDD (to employ typical pruning schemes with a stop criterion once the performance on a validation set worsens below some given threshold) we want to have a strong \u21132 regularization, which however makes the model less compressible as a higher number of parameters will have a similar relevance. Depending on what we are targeting (high performance or high compressibility) we might want or not want to avoid SDD."
        },
        {
            "heading": "2 Background on Vision Transformers",
            "text": "Vision Transformers [7], presented in Fig. 1a, use self-attention mechanisms to capture the relationships between the elements of an input image. They essentially consist of four key elements: patch embedding, positional encoding, the transformer encoder, shown in Fig. 1b, and the classification head. Patch embedding. The input image is divided into a grid of patches (which can or can not be non-overlapping), each containing a fixed number of pixels. These patches are then linearly projected to a lower-dimensional embedding space. This process converts the spatial information of the image into a sequence of patch embeddings, mimicking the process of text embedding. Positional encoding. To preserve the positional information of the image patches, positional encoding is added to the patch embeddings: this allows to distinguish different patches and capture their global positions in the image. The transformer encoder. The patch embeddings, along with their positional encodings, are fed into a stack of transformer encoder layers. Each encoder layer consists of two sub-layers: a multi-head self-attention mechanism, and a multilayer perception (MLP). The self-attention mechanism enables the model to capture global interactions between patches by attending to all patches and aggregating information accordingly, thus performing feature aggregation. More specifically, self-attention operations determine the attention output a based on the relevance of one item to others. This is iteratively refined and computed using keys k and queries q, which have the same dimension d, and values v [24]. The keys are the indices of the hidden states of the encoded input items, and each key ki has some associated value vi. Each query qi represents an output\ncoming from the encoded target item (class). The attention is computed as the softmax of the product between q and k, and then multiplied by v: the model learns to prioritize important input features and capture more informative representations of the input data. Many attention heads are hence concatenated, to form the multi-head, to obtain contextualized representations that include both local and global information. Finally, an MLP carries out feature transformation. The classification head. At the end of the transformer encoder stack, a classification head is attached to the output of the final transformer layer. The classification head can take various forms, such as a simple fully connected layer or a combination of linear and softmax layers. It maps the aggregated representation of the patches to class probabilities, in order to perform image classification. Training. ViT models require pretraining on enormous datasets (such as JFT300M, consisting of approximately 300 million images) due to their lack of strong inductive bias, which is present in other architectures like CNNs [5]. Despite recent advances in learning on smaller datasets using distillation approaches or optimizing models with smaller sizes, transformers still have larger model architectures compared to CNN-based models and require large datasets for optimal performance, despite requiring less computational resources [7]. However, [12] concludes that scaling up Transformer models improves performance, but with current designs, it is computationally expensive and necessitates efficient designs. Beyond traditional ViT. In the last few years, many different Vision Transformer designs have been proposed to improve the performance of computer vision tasks. One of the most popular is SWIN [13] which proposes shifted windows to create overlapping receptive fields, cascaded stages to mimic a multiresolution approach, tokenization of windows, and token shifts across the stages. As it is possible to imagine, this architecture already goes in the direction of cus-\ntomizing the Transformer architecture to process images. Other newly proposed transformers variants include, among others, CoaT [4], TNT [28], and DeiT [23]."
        },
        {
            "heading": "3 Sparse Double Descent and ViT",
            "text": "In this section, we will discuss the background for Double Descent and Sparse Double Descent, moving then to the potential impact on ViT architectures. Double descent. It is known that when comparing a model performance (on unseen data) and model complexity, as the complexity grows (from right to left), we observe a first region where the performance improves (under-fitting - blue region in Fig. 2) and then, at some point, a trend inversion where the performance decreases while increasing the model\u2019s complexity (over-fitting). When exposed to real-world noisy data, however, neural networks tend to exhibit the DD phenomenon [17]: instead of being monotonous in that region, the performance inverts, at some point, its trend (critical region - orange in Fig. 2), and starts back decreasing (overfit region - green in Fig. 2). DD has been observed in regression tasks and successfully averted with optimallytuned \u21132 regularization [18]. However, for classification tasks, this problem is not easily mitigated. It has been shown that the more challenging the dataset and classification task, the harder it is to avoid DD [20]. The authors in [17] demonstrate the DD not only depending on the model width but also depending on the number of epochs during training. Similarly to DD, an SDD phenomenon happens in the transition from the complex model toward the sparse, pruned model (as illustrated in Fig. 2) [11]. SDD has implications for model selection, regularization techniques, and understanding the behavior of complex models in high-dimensional settings, as the presence of SDD makes many criteria, like when to stop the pruning, unclear. Addressing the Sparse Double Descent. We introduce here Alg. 1, de-\nAlgorithm 1 Iterative algorithm to detect Sparse Double Descent.\n1: procedure DETECT SDD (winit, \u039e, \u03bb, \u03b6 iter,\u03b6end) 2: w \u2190 Train(winit, \u039etrain, \u03bb) 3: pi\u22121 \u2190 Performance(w,\u039eval) 4: prev increasing, prev decreasing, already increased, already decreased \u2190 False 5: SDD \u2190 False 6: while Sparsity(w,winit) < \u03b6end do 7: w \u2190 Prune(w, \u03b6 iter) 8: w \u2190 Train(w,\u039etrain, \u03bb) 9: pi \u2190 Performance(w,\u039eval)\n10: if (pi < pi\u22121 and already decreased and not prev decreasing) or 11: (pi > pi\u22121 and already increased and not prev increasing) then 12: SDD \u2190 True 13: end if 14: if pi \u0338= pi\u22121 then 15: prev decreasing \u2190 pi < pi\u22121; prev increasing \u2190 pi > pi\u22121 16: already decreasing \u2190already decreasing or pi < pi\u22121 17: already increasing \u2190already increasing or pi > pi\u22121 18: pi \u2190 pi\u22121 19: end if 20: end while 21: Return SDD 22: end procedure\nsigned to demonstrate the eventual occurrence of the sparse double descent phenomenon. The algorithm begins by training the model on the learning task \u039e for the first time, incorporating \u21132 regularization weighted by \u03bb (line 2). Following this initial training step, a magnitude pruning stage is set up (line 7). Neural network pruning aims to reduce the size of a large network while maintaining its accuracy by removing irrelevant weights, filters, or other structures. As in [11], we use in this algorithm an unstructured pruning method called magnitudebased pruning, popularized by [10], in which a fixed amount of weights below some specific threshold, are pruned (line 7). Here, every time we prune, a fixed \u03b6 iter fraction of parameters from the model is removed. We highlight that more complex pruning approaches exist, but magnitude-based pruning shows its competitiveness despite very low complexity [9]. The accuracy of the model typically decreases after pruning. To improve the performance of the model, we retrain it using the same original learning policy (line 8). Recent works have shown that this approach leads to the best performance at the highest sparsities [19]. This approach allows us to determine whether a sparsely-parameterized model, starting from its initialization, has the potential to successfully learn a given target task. We end our pruning procedure once we reach a sparsity \u03b6end (line 6).\nViT and Sparse Double Descent. The number of parameters in ViT architectures is proportional to the model depth and quadratic function of the width. There is a tendency to scale these models even further, to increase their performance [5], even though this is becoming very computationally expensive.\nLooking from that perspective, the understanding of the comportment of the ViT models becomes essential. Having a completely different learning architecture from other models, like CNNs, it is not easy to predict the behavior of ViT when pruning is applied. Our work addresses this issue and performs an extensive study with different levels of label noise and various model sparsity levels. In the next section, we will conduct a quantitative study on ViT, determining whether SDD is a real threat to ViT as it is to CNNs or not."
        },
        {
            "heading": "4 Experiments",
            "text": "Setup. For the experimental setup, we follow the same approach as He et al. [11]. The first model we train is a ResNet-18, trained on CIFAR-10 & CIFAR-100, for 160 epochs, optimized with SGD, having momentum 0.9, a learning rate of 0.1 decayed by a factor 0.1 at milestones 80 and 120, batch size 128 and \u03bb 10\u22124. The second model is a ViT with 4 patches, 8 heads, and 512 embedding dimensions, trained on CIFAR-10 and CIFAR-100 for 200 epochs, optimized with Adam, having a learning rate of 10\u22124 with a cosine annealing schedule and \u03bb 0.03.\nFor each dataset, a percentage \u03b5 of symmetric, noisy labels are introduced: the labels of a given proportion of training samples are flipped to one of the other class labels, selected with equal probability [15]. In our experiments, we test with \u03b5 \u2208 {10%, 20%, 50%}. Moreover, as synthetic noise has clean structures which greatly enabled statistical analyses but often fails to model real-world noise patterns, we also conducted experiments without adding synthetic noise. With the same architectures and learning policies presented above, we carried out experiments on CIFAR-100N, which is formed by the CIFAR-100 training dataset with human-annotated real-world noisy labels collected from Amazon Mechanical Turk [25]. In all experiments, we set \u03b6 iter = 20% and \u03b6end = 99.99%.1\nOccurrence of sparse double descent. Fig. 3 displays the results of ResNet18 and ViT, on CIFAR-10 and CIFAR-100. As in He et al. [11] work, the double descent consists of 4 phases. First, at low sparsities, the network is overparameterized, thus pruned network can still reach similar accuracy to the dense model. The second phase is a phase near the \u201cinterpolation threshold\u201d, where the test accuracy is about to first decrease and then increase as sparsity grows. The third phase is located at high sparsities, where test accuracy is rising. The final phase happens when both training and test accuracy drop significantly. For every value of \u03b5, whether on CIFAR-10 or CIFAR-100, the sparse double descent phenomenon occurs both for ResNet and ViT. We observe a similar phenomenon as in the simulated \u03b5 also in the human-annotated CIFAR-100N.\nStudy on \u03bb. In the previous experiments in Fig. 3, ViTs were trained with a \u21132-regularization hyper-parameter equal to 0.03, which is typically used in other\n1 The code is available at https://github.com/VGCQ/SDD_ViT\nworks. However, it has been recently shown that, for certain linear regression models with isotropic data distribution, optimally-tuned \u21132 regularization can achieve monotonic test performance as either the sample size or the model size is grown. Nakkiran et al. [18] demonstrated it analytically and established that optimally-tuned \u21132 regularization can mitigate double descent for general models, including neural networks like Convolutional Neural Networks. Moreover, a recent study showed that \u21132 regularization is positively contributing to the avoidance of sparse double descent in an image classification context, but is not the antidote to \u201cdodge\u201d it [20]. Hence, we propose in Fig. 4 a quantitative study over \u03bb for ViT on CIFAR-10 with \u03b5 = 10%. With small values of \u03bb, i.e. below 1, the sparse double descent is empirically noticeable. The increment of \u03bb pushes the occurrence of the phenomenon towards smaller sparsity values. Looking at the loss, increasing \u03bb smoothens the bump of the test loss and at some point, i.e. \u03bb = 1, the test loss becomes flat and behaves monotonically: the sparse double descent is avoided. For \u03bb > 1, the phenomenon also results avoided, but the performance worsens (lighter blue region at the bottom right corner) since the regularization is stronger. Note that with higher \u03bb, performances are better but the maximum sparsity achievable is not as high as for lower values of \u03bb.\nAvoidance of the Sparse Double Descent. As \u03bb = 1 seems to be an optimal value enabling dodging SDD on CIFAR-10 with \u03b5 = 10%, we try to use this value for other setups. Fig. 5 displays the results of ViT on CIFAR-10/CIFAR-100 with \u03b5 \u2208 {10%, 20%, 50%} and \u03bb = 1. For small noise rates, i.e. \u03b5 \u2264 20%, the phenomenon vanishes and performance is enhanced. However, for higher noise rates, like \u03b5 = 50%, SDD is mitigated, but still present. Even if it already helps, it seems that the strength of the regularization is not high enough to completely avoid SDD. Indeed, with a higher \u03bb, i.e. 3, the performance becomes monotonic. Trade-off between SDD and compressibility. Fig. 4, supported also by the experiments displayed in Fig. 5, suggests that at high regularization regimes,\nwhere we avoid SDD, the ability to compress the model is harmed. This is due to the strong prior we impose over the distribution of the parameters of the model: the stronger this is, the least we are indeed able to remove degrees of freedom from our system. As a visual example, Fig. 6 displays the distribution of the parameters for one of the considered training configurations, for \u03bb = 0.03 and 1, without pruning and after two pruning steps. We observe that despite removing the same quantity of parameters, with higher regularization the parameters have less variance, which has the dual effect of both making them more robust to injected noise (due to the strong regularization) but, at the same time, this distribution is more sensitive to compression by pruning. Hence, we conclude that, in case we wish to have a robust, well-generalizing model, we wish to avoid SDD and employ strong \u21132 regularization; on the contrary, if we target compressibility, we would like to favor SDD, as the better generalizing region is pushed to highly compressed regions."
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper investigates the occurrence of Sparse Double Descent in the Vision Transformer architecture. SDD is a phenomenon carefully explored due to its influence on determining the optimal model size necessary for maintaining the performance of over-parametrized models. We observe that, indeed, ViT is also susceptible to SDD. Moreover, we study different values for \u21132 regularization and discover that, unlike for other CNN architectures like ResNet, we can find the optimal value and completely avoid SDD. However, the regularization comes at a price - at the same time, it renders the model less compressible, because of the strong enforced prior. We postulate that this is possible due to the lack of strong inductive bias in ViT, which enables strong regularization regimes, impossible for CNNs. Finally, we inspect the trade-off between avoiding SDD (enhancing\nhence model\u2019s performance) and favoring the model compressibility, observing that, for the second one, we would like to favor SDD. This study hopes to inform the community about the risk of SDD ViT models might incur, which depending on the final scope of the trained model can be a real or a phantom threat. Acknowledgments. This project was provided with computer and storage resources by GENCI at IDRIS thanks to the grant 2022-AD011013930 on the supercomputer Jean Zay\u2019s the V100 partition."
        }
    ],
    "year": 2023
}