 Here are 5 possible future research ideas after brainstorming the given paper:

1. Develop new alignment methods between text and knowledge graphs to enable more targeted and relevant knowledge injection. The paper suggests current alignment methods may not be optimal.

2. Explore different representations of knowledge (e.g. conceptual vs. factual) and their impact on knowledge injection. The paper showed conceptual knowledge worked better.

3. Study why large pre-trained models fail to properly utilize injected knowledge during fine-tuning. More analysis on the model internals could provide insights. 

4. Design more sophisticated methods to prune or filter knowledge bases for high quality knowledge selection prior to injection. The paper demonstrated the value of pruning.

5. Conduct further experiments on very large models like GPT-3 and ChatGPT to better understand the impact of knowledge injection at scale. The paper was limited to smaller BERT models.