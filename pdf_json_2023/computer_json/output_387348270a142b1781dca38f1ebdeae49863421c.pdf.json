{
    "abstractText": "Image classifiers are information-discarding machines, by design. Yet, how these models discard information remains mysterious. We hypothesize that one way for image classifiers to reach high accuracy is to zoom to the most discriminative region in the image and then extract features from there to predict image labels, discarding the rest of the image. Studying six popular networks ranging from AlexNet to CLIP, we find that proper framing of the input image can lead to the correct classification of 98.91% of ImageNet images. Furthermore, we uncover positional biases in various datasets, especially a strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insights into the potential of zooming, we propose a test-time augmentation (TTA) technique that improves classification accuracy by forcing models to explicitly perform zoom-in operations before making predictions. Our method is more interpretable, accurate, and faster than MEMO, a state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark that challenges SOTA classifiers including large vision-language models even when optimal zooming is allowed.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mohammad Reza Taesiri"
        },
        {
            "affiliations": [],
            "name": "Giang Nguyen"
        }
    ],
    "id": "SP:c863d56c1f1a2657fa52d67d40d0b122c7b657fe",
    "references": [
        {
            "authors": [
                "M.A. Alcorn",
                "Q. Li",
                "Z. Gong",
                "C. Wang",
                "L. Mai",
                "Ku",
                "W.-S",
                "A. Nguyen"
            ],
            "title": "Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "M.S. Ayhan",
                "P. Berens"
            ],
            "title": "Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks",
            "venue": "In Medical Imaging with Deep Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Bahat",
                "G. Shakhnarovich"
            ],
            "title": "Classification confidence estimation with test-time data-augmentation",
            "venue": "arXiv e-prints, pp. arXiv\u20132006,",
            "year": 2020
        },
        {
            "authors": [
                "A. Barbu",
                "D. Mayo",
                "J. Alverio",
                "W. Luo",
                "C. Wang",
                "D. Gutfreund",
                "J. Tenenbaum",
                "B. Katz"
            ],
            "title": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "D. Bau",
                "B. Zhou",
                "A. Khosla",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Network dissection: Quantifying interpretability of deep visual representations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "L. Beyer",
                "O.J. H\u00e9naff",
                "A. Kolesnikov",
                "X. Zhai",
                "Oord",
                "A. v. d"
            ],
            "title": "Are we done with imagenet",
            "venue": "arXiv preprint arXiv:2006.07159,",
            "year": 2006
        },
        {
            "authors": [
                "H. Chefer",
                "I. Schwartz",
                "L. Wolf"
            ],
            "title": "Optimizing relevance maps of vision transformers improves robustness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "P. Chen",
                "C. Agarwal",
                "A. Nguyen"
            ],
            "title": "The shape and simplicity biases of adversarially robust imagenettrained cnns",
            "venue": "arXiv preprint arXiv:2006.09373,",
            "year": 2020
        },
        {
            "authors": [
                "S. Chen",
                "X. Huang",
                "Z. He",
                "C. Sun"
            ],
            "title": "Damagenet: A universal adversarial dataset",
            "venue": "arXiv preprint arXiv:1912.07160,",
            "year": 1912
        },
        {
            "authors": [
                "S. Chun",
                "J.Y. Lee",
                "J. Kim"
            ],
            "title": "Cyclic test time augmentation with entropy weight method",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "T. De Vries",
                "I. Misra",
                "C. Wang",
                "L. Van der Maaten"
            ],
            "title": "Does object recognition work for everyone",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2019
        },
        {
            "authors": [
                "J. Donnelly",
                "A.J. Barnett",
                "C. Chen"
            ],
            "title": "Deformable protopnet: An interpretable image classifier using deformable prototypes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "J. Fu",
                "H. Zheng",
                "T. Mei"
            ],
            "title": "Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "R. Geirhos",
                "P. Rubisch",
                "C. Michaelis",
                "M. Bethge",
                "F.A. Wichmann",
                "W. Brendel"
            ],
            "title": "Imagenettrained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "R. Geirhos",
                "Jacobsen",
                "J.-H",
                "C. Michaelis",
                "R. Zemel",
                "W. Brendel",
                "M. Bethge",
                "F.A. Wichmann"
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "G. Goh",
                "N. Cammarata",
                "C. Voss",
                "S. Carter",
                "M. Petrov",
                "L. Schubert",
                "A. Radford",
                "C. Olah"
            ],
            "title": "Multimodal neurons in artificial neural networks",
            "venue": "Distill, 6(3):e30,",
            "year": 2021
        },
        {
            "authors": [
                "A. Gupta",
                "P. Dollar",
                "R. Girshick"
            ],
            "title": "Lvis: A dataset for large vocabulary instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "D. Hendrycks",
                "T. Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "arXiv preprint arXiv:1903.12261,",
            "year": 1903
        },
        {
            "authors": [
                "D. Hendrycks",
                "N. Mu",
                "E.D. Cubuk",
                "B. Zoph",
                "J. Gilmer",
                "B. Lakshminarayanan"
            ],
            "title": "Augmix: A simple data processing method to improve robustness and uncertainty",
            "venue": "arXiv preprint arXiv:1912.02781,",
            "year": 1912
        },
        {
            "authors": [
                "D. Hendrycks",
                "S. Basart",
                "N. Mu",
                "S. Kadavath",
                "F. Wang",
                "E. Dorundo",
                "R. Desai",
                "T. Zhu",
                "S. Parajuli",
                "M Guo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "D. Hendrycks",
                "K. Zhao",
                "S. Basart",
                "J. Steinhardt",
                "D. Song"
            ],
            "title": "Natural adversarial examples",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "A. Howard",
                "M. Sandler",
                "G. Chu",
                "Chen",
                "L.-C",
                "B. Chen",
                "M. Tan",
                "W. Wang",
                "Y. Zhu",
                "R. Pang",
                "V Vasudevan"
            ],
            "title": "Searching for mobilenetv3",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. Van Der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "M. Jaderberg",
                "K. Simonyan",
                "A Zisserman"
            ],
            "title": "Spatial transformer networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "C. Jin",
                "R. Tanno",
                "T. Mertzanidou",
                "E. Panagiotaki",
                "D.C. Alexander"
            ],
            "title": "Learning to downsample for segmentation of ultra-high resolution images",
            "venue": "arXiv preprint arXiv:2109.11071,",
            "year": 2021
        },
        {
            "authors": [
                "M. Kang",
                "Zhu",
                "J.-Y",
                "R. Zhang",
                "J. Park",
                "E. Shechtman",
                "S. Paris",
                "T. Park"
            ],
            "title": "Scaling up gans for text-to-image synthesis",
            "venue": "arXiv preprint arXiv:2303.05511,",
            "year": 2023
        },
        {
            "authors": [
                "I. Kim",
                "Y. Kim",
                "S. Kim"
            ],
            "title": "Learning loss for test-time augmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "F. Kong",
                "R. Henao"
            ],
            "title": "Efficient classification of very large images with tiny objects",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "J. Krause",
                "H. Jin",
                "J. Yang",
                "L. Fei-Fei"
            ],
            "title": "Fine-grained recognition without part annotations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "B. Leung",
                "Ho",
                "C.-H",
                "A. Persekian",
                "D. Orozco",
                "Y. Chang",
                "E. Sandstrom",
                "B. Liu",
                "N. Vasconcelos"
            ],
            "title": "Oowl500: Overcoming dataset collection bias in the wild",
            "venue": "arXiv preprint arXiv:2108.10992,",
            "year": 2021
        },
        {
            "authors": [
                "B. Li",
                "F. Wu",
                "Lim",
                "S.-N",
                "S. Belongie",
                "K.Q. Weinberger"
            ],
            "title": "On feature normalization and data augmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "J. Li",
                "C. Xiong",
                "S.C. Hoi"
            ],
            "title": "Learning from noisy data with robust representation learning",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "X. Li",
                "J. Li",
                "T. Dai",
                "J. Shi",
                "J. Zhu",
                "X. Hu"
            ],
            "title": "Rethinking natural adversarial examples for classification models",
            "venue": "arXiv preprint arXiv:2102.11731,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "H. Mao",
                "Wu",
                "C.-Y",
                "C. Feichtenhofer",
                "T. Darrell",
                "S. Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "A.S. Luccioni",
                "D. Rolnick"
            ],
            "title": "Bugs in the data: How imagenet misrepresents biodiversity",
            "venue": "arXiv preprint arXiv:2208.11695,",
            "year": 2022
        },
        {
            "authors": [
                "A. Lyzhov",
                "Y. Molchanova",
                "A. Ashukha",
                "D. Molchanov",
                "D. Vetrov"
            ],
            "title": "Greedy policy search: A simple baseline for learnable test-time augmentation",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "N. Ma",
                "X. Zhang",
                "Zheng",
                "H.-T",
                "J. Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "S. Marcel",
                "Y. Rodriguez"
            ],
            "title": "Torchvision the machine-vision package of torch",
            "venue": "In Proceedings of the 18th ACM international conference on Multimedia,",
            "year": 2010
        },
        {
            "authors": [
                "M. Minderer",
                "A. Gritsenko",
                "A. Stone",
                "M. Neumann",
                "D. Weissenborn",
                "A. Dosovitskiy",
                "A. Mahendran",
                "A. Arnab",
                "M. Dehghani",
                "Z Shen"
            ],
            "title": "Simple open-vocabulary object detection with vision transformers",
            "venue": "arXiv preprint arXiv:2205.06230,",
            "year": 2022
        },
        {
            "authors": [
                "A. Nguyen",
                "J. Yosinski",
                "J. Clune"
            ],
            "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks",
            "venue": "arXiv preprint arXiv:1602.03616,",
            "year": 2016
        },
        {
            "authors": [
                "T. Pang",
                "K. Xu",
                "J. Zhu"
            ],
            "title": "Mixup inference: Better exploiting mixup to defend adversarial attacks",
            "venue": "arXiv preprint arXiv:1909.11515,",
            "year": 1909
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "M. Raghu",
                "T. Unterthiner",
                "S. Kornblith",
                "C. Zhang",
                "A. Dosovitskiy"
            ],
            "title": "Do vision transformers see like convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A. Recasens",
                "P. Kellnhofer",
                "S. Stent",
                "W. Matusik",
                "A. Torralba"
            ],
            "title": "Learning to zoom: a saliency-based sampling layer for neural networks",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "B. Recht",
                "R. Roelofs",
                "L. Schmidt",
                "V. Shankar"
            ],
            "title": "Do imagenet classifiers generalize to imagenet",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "J. Robinson",
                "L. Sun",
                "K. Yu",
                "K. Batmanghelich",
                "S. Jegelka",
                "S. Sra"
            ],
            "title": "Can contrastive learning avoid shortcut solutions? Advances in neural information processing",
            "year": 2021
        },
        {
            "authors": [
                "R. Rs",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "International Journal of Computer Vision, 128,",
            "year": 2020
        },
        {
            "authors": [
                "O. Russakovsky",
                "J. Deng",
                "H. Su",
                "J. Krause",
                "S. Satheesh",
                "S. Ma",
                "Z. Huang",
                "A. Karpathy",
                "A. Khosla",
                "M Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International Journal of Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "M. Sandler",
                "A. Howard",
                "M. Zhu",
                "A. Zhmoginov",
                "Chen",
                "L.-C"
            ],
            "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "I. Sato",
                "H. Nishimura",
                "K. Yokoi"
            ],
            "title": "Apac: Augmented pattern classification with neural networks",
            "venue": "arXiv preprint arXiv:1505.03229,",
            "year": 2015
        },
        {
            "authors": [
                "D. Shanmugam",
                "D. Blalock",
                "G. Balakrishnan",
                "J. Guttag"
            ],
            "title": "Better aggregation in test-time augmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "P. Slav\u00edk"
            ],
            "title": "A tight analysis of the greedy algorithm for set cover",
            "venue": "In Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing,",
            "year": 1996
        },
        {
            "authors": [
                "L. Smith",
                "Y. Gal"
            ],
            "title": "Understanding measures of uncertainty for adversarial example detection",
            "venue": "arXiv preprint arXiv:1803.08533,",
            "year": 2018
        },
        {
            "authors": [
                "A. Steiner",
                "A. Kolesnikov",
                "X. Zhai",
                "R. Wightman",
                "J. Uszkoreit",
                "L. Beyer"
            ],
            "title": "How to train your vit? data, augmentation, and regularization in vision transformers",
            "venue": "arXiv preprint arXiv:2106.10270,",
            "year": 2021
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "M.R. Taesiri",
                "G. Nguyen",
                "A. Nguyen"
            ],
            "title": "Visual correspondence-based explanations improve ai robustness and human-ai team accuracy",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M. Tan",
                "Q. Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "C. Thavamani",
                "M. Li",
                "F. Ferroni",
                "D. Ramanan"
            ],
            "title": "Learning to zoom and unzoom",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "D. Tsipras",
                "S. Santurkar",
                "L. Engstrom",
                "A. Ilyas",
                "A. Madry"
            ],
            "title": "From imagenet to image classification: Contextualizing progress on benchmarks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Tu",
                "H. Talebi",
                "H. Zhang",
                "F. Yang",
                "P. Milanfar",
                "A. Bovik",
                "Y. Li"
            ],
            "title": "Maxvit: Multi-axis vision transformer",
            "venue": "In European conference on computer vision,",
            "year": 2022
        },
        {
            "authors": [
                "B. Uzkent",
                "S. Ermon"
            ],
            "title": "Learning when and where to zoom with deep reinforcement learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "D. Wang",
                "E. Shelhamer",
                "S. Liu",
                "B. Olshausen",
                "T. Darrell"
            ],
            "title": "Tent: Fully test-time adaptation by entropy minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "H. Wang",
                "S. Ge",
                "Z. Lipton",
                "E.P. Xing"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Wei",
                "X.-S",
                "Song",
                "Y.-Z",
                "O. Mac Aodha",
                "J. Wu",
                "Y. Peng",
                "J. Tang",
                "J. Yang",
                "S. Belongie"
            ],
            "title": "Finegrained image analysis with deep learning: A survey",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "R. Wightman",
                "H. Touvron",
                "H. J\u00e9gou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "venue": "arXiv preprint arXiv:2110.00476,",
            "year": 2021
        },
        {
            "authors": [
                "K.Y. Xiao",
                "L. Engstrom",
                "A. Ilyas",
                "A. Madry"
            ],
            "title": "Noise or signal: The role of image backgrounds in object recognition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Q. Xie",
                "Luong",
                "M.-T",
                "E. Hovy",
                "Q.V. Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "S. Xie",
                "R. Girshick",
                "P. Doll\u00e1r",
                "Z. Tu",
                "K. He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "S. Yun",
                "S.J. Oh",
                "B. Heo",
                "D. Han",
                "J. Choe",
                "S. Chun"
            ],
            "title": "Re-labeling imagenet: from single to multi-labels, from global to localized labels",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "M. Zhang",
                "S. Levine",
                "C. Finn"
            ],
            "title": "Memo: Test time robustness via adaptation and augmentation",
            "venue": "arXiv preprint arXiv:2110.09506,",
            "year": 2021
        },
        {
            "authors": [
                "H. Zheng",
                "J. Fu",
                "Zha",
                "Z.-J",
                "J. Luo"
            ],
            "title": "Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Zhong",
                "J. Yang",
                "P. Zhang",
                "C. Li",
                "N. Codella",
                "L.H. Li",
                "L. Zhou",
                "X. Dai",
                "L. Yuan",
                "Y Li"
            ],
            "title": "Regionclip: Region-based language-image pretraining",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Since the release of AlexNet in 2012 [38], deep neural networks have set many ImageNet (IN) [59] accuracy records [38, 23]. While many papers reported improved learning algorithms or architectures, little is known about how the inner workings of image classifiers actually evolve. The success is often attributed to a network\u2019s ability to detect more objects [9] and a variety of facets of each object (i.e., invariance to style, pose, and form changes) [50, 21]. By aggregating the information from all the visual cues in a scene, a classifier somehow chooses a better label for the image. For example, Figs. 13\u201314 in [58] show that a model detects both dogs and cats in the same image and only discards the dog features right before the classification layer to arrive at a tiger cat prediction.\nWhen processing an image, a network may implicitly zoom in or out (defined in Sec. 3) to the most discriminative image region ignoring the rest of the image (Fig. 1a), and then extract that localized region\u2019s features to predict image labels. We hypothesize that the improved image classification may largely be due to the networks accurately zooming to the discriminative areas (e.g., junco and magpie birds in Fig. 1a) rather than more accurately describing them (i.e. generating better features of these two birds). In this work, we present supporting evidence for our zooming hypothesis.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\nar X\niv :2\n30 4.\n05 53\n8v 4\n[ cs\n.C V\n] 8\nWe conduct a thorough study to test the effects of zooming in and out on the classification accuracy of six network architectures on six ImageNet-scale benchmarks. Our main findings also include:\n1. A major, surprising finding is that state-of-the-art, IN-trained models can accurately predict up to 98.91% of ImageNet samples when an optimally-zoomed image is provided. The remaining few hundred IN images (0.39%) that are never correctly labeled by any model (despite optimal zooming) include mostly ill-posed and rare images (Sec. 4.1). 2. ImageNet-A [27] and ObjectNet [86] both exhibit a substantial center bias. For example, by only upsampling and center-cropping each ImageNet-A image, ResNet-50\u2019s accuracy increases dramatically from 0.09% to 14.58% (Sec. 4.2). 3. Zooming can be leveraged as an inductive bias at test time to improve ImageNet classification accuracy. That is, integrating zoom transformations into MEMO [83], a leading test-time augmentation method, yields consistently higher accuracy than the baseline ResNet-50 models and also MEMO with default transformations on multiple datasets (Sec. 4.3).\nOur findings show that the accuracy of image classifiers can be improved by finding an optimal zoom setting first and then classifying that crop alone (Fig. 1a). Motivated by this insight, we build ImageNet-Hard1, a new 1000-way classification benchmark that challenges state-of-theart (SOTA) classifiers despite the application of optimal zooming (Sec. 4.4). In other words, we collect images from seven existing ImageNet-scale benchmarks where OpenAI\u2019s CLIP ViT-L/14 [53] misclassifies even when allowed to try 324 zooming settings. Interestingly, SOTA classifiers that operate at 224\u00d7224 resolution perform poorly on ImageNet-Hard (below 19% accuracy). Analyzing misclassifications on ImageNet-Hard reveals a major remaining challenge in the era of SOTA classifiers of Transformers [17], EfficientNets [69, 36], and large vision-language models (Sec. 4.5)."
        },
        {
            "heading": "2 Related Work",
            "text": "Learning to Zoom in image classification Leveraging zoom-in or crops of an image has a long history of improving fine-grained image classification with approaches varying from combining multiple crops at different resolutions [18, 73], using multiple crops of the object (i.e., part-based classification) [16, 37, 84, 68, 84, 36] to warping the input image [55, 31, 32]. We note that a common\n1Code and data are available on https://taesiri.github.io/ZoomIsAllYouNeed.\nprior definition of \u201czoom\u201d [55, 31, 32, 70] is to first divide an input image into a grid and then warp the image, distorting the aspect ratio of the objects in the image. In contrast, our zoom procedure utilizes only two functions: resize and crop, maintaining the original aspect ratio.\nFurthermore, to our knowledge, we are the first to perform a zoom study on ImageNet-scale datasets (ImageNet [59], ImageNet-A [27], ObjectNet [8], etc) while prior zoom approaches [16, 37, 84, 68, 84, 36, 32] exclusively focus on non-ImageNet, fine-grained classification (e.g. classifying birds or dogs). Due to such differences in the image distribution of interests, prior works mostly study zooming in (which benefits fine-grained classification) while we study both zooming in and out.\nTest-time data augmentation (TTA) is a versatile technique that could help estimate uncertainty [65, 7, 6] and improve classification accuracy [38, 67, 23, 52, 46, 62, 34, 14]. When test inputs are sampled from unseen, non-training distributions, augmenting the data often improve a model\u2019s generalization to new domains [74, 83]. A simple TTA method is 10-crop evaluation [38] in which 5 patches of 224\u00d7 224 px along with their horizontal reflections (resulting in 10 patches) are extracted from the original image. An alternative way to leverage the marginal output distributions over augmented data is to use them as gradient signals to update the classifier\u2019s parameters [75, 83]. We employ this approach to update the model during test time, with patches being zoom-based augmentations.\nBiases in ImageNet and datasets Resize-then-center-crop has been a pre-processing standard for ImageNet classification since AlexNet [38]. This pre-processing exploits the known center bias of ImageNet. While ImageNet has been shown to contain a variety of biases in image labels [10, 71], object poses [5], image quality [39], we are the first to examine the positional biases of the out-ofdistribution (OOD) benchmarks for ImageNet classifiers and find a strong center bias in ImageNet-A and ObjectNet that could affect how the community interprets progress on these OOD benchmarks.\nPerhaps the closest to us is a preprint by Li et al. [42] that shows that cropping to the main object can improve model accuracy on ImageNet-A [27]. Yet, unlike [42], we study six ImageNet-scale datasets, both zooming in and zooming out, and we propose a new dataset of ImageNet-Hard."
        },
        {
            "heading": "3 Method",
            "text": "Zoom definition To zoom in or out of the image, we only use resize and crop operations. Initially, we uniformly resize the test image so that the smaller dimension matches the target scale of S. Then, we define a 3\u00d7 3 grid on the image to divide it into 9 patches. We perform a CenterCrop operation at the center (\u2022 in Fig. 1b) of each patch to extract a 224\u00d7 224 px crop from each of the nine locations (see Python code in Appendix A.1). In the CenterCrop step, zero-padding is used when the content to be cropped is smaller than 224\u00d7 224. Overall, at each target scale S, we generate 9 crops (Fig. 1b). We test 36 different values of S ranging from 10 to 1024 px, resulting in a total of 36 \u00d7 9 = 324 different zoomed versions for each image. Based on initial scale factor S, we define three groups: (1) zoom-out group contains all augmented crops where S < 224; (2) zoom-in group contains all augmented crops where S > 224; and (3) zoom-224 group contains the 9 patches where S = 224.\nBenchmark datasets We use the ImageNet (IN) [59] dataset with both the original and ImageNetReaL [10] (ReaL) labels. For each IN image, we use the union of the IN and ReaL labels (IN+ReaL) to complement each other to reduce noise in IN labels. We further examine the effects of zoom-based transformations on four popular OOD benchmarks: (a) natural adversarials (ImageNet-A [27]), (b) image renditions (ImageNet-R [26]), (c) black-and-white sketches (ImageNet-Sketch [75]), and (d) viewpoint-and-background-controlled samples (ObjectNet [8]). We refer to these as benchmarks as IN-A, IN-R, IN-S, and ON, respectively, in the rest of the paper.\nClassifiers We study the effects of zoom-based transformations on six popular image classifiers in the last decade: AlexNet [38], VGG-16 [63], ResNet-18 & ResNet-50 [23], ViT-B/32 [17], and OpenAI\u2019s CLIP-ViT-L/14 [53]. The inclusion of the 11-year-old AlexNet provides a baseline for the power of deep features (when given the right region to look at). Predicted labels from CLIP-ViT-L/14 are acquired using its standard zero-shot classification setup (Appendix A.5)."
        },
        {
            "heading": "4 Experimental Results",
            "text": ""
        },
        {
            "heading": "4.1 Zooming has the potential to substantially improve image classification accuracy",
            "text": "To understand the potential of zooming in improving image classification accuracy, first, we establish an upper-bound accuracy (i.e. when an \u201coptimal\u201d zoom is given). That is, we apply 36 scales \u00d7 9 anchors = 324 zoom transformations (Sec. 3) to each image to generate 324 zoomed versions of the input. We then feed all N = 324 versions to each network and label an image \u201ccorrectly classified given the optimal zoom\u201d if at least 1 of the 324 is correctly labeled. We call such maximum possible top-1 accuracy \u201cupper-bound accuracy\u201d in Tab. 1. Our experiment also informs the community of the type of image that cannot be correctly labeled even with an optimal zooming strategy.\nOOD benchmarks still pose a significant challenge to IN-trained models even with optimal zooming (i.e., all upper-bound accuracy scores < 80%).\nmicrophone four poster reel\noxcart shopping basket plate, soup bowl\nsliding door goldfish, plastic bag canon\npitcher swab, broom reflex camera\n(b )\nRa re\nc as es (c ) Ill us io\nns (d ) M an y ob je ct s\nResults Table 1 shows upper-bound accuracy over different values of N = {1, 36, 324}. First, the random baselines (given N = 324 attempts per image) are at 32.4% for 1,000 classes (IN, ReaL, IN+ReaL, and IN-Sketch), and 100% for 200 and 313 classes (IN-A and ON, respectively). Yet, the accuracy of models with optimal zooming is far from random\u2014e.g. ResNet-50 largely outperforms not only the random baseline but also the 1-crop baseline on IN (96.78% vs. 75.75%; Tab. 1a vs. c).\nOn the IN, ReaL, and IN+ReaL datasets, there is a substantial gap for all models (around +20 to +35 points) between the 1-crop and the optimal zooming setting (Tab. 1a vs. c). Surprisingly, given optimal zooming, the 11-year-old AlexNet actually can correctly label over 90% of IN images, which is roughly the 1-crop accuracy (87.8%) of the 2022 state-of-the-art ConvNexts [44]. This result is consistent with our hypothesis: One way for state-of-the-art classifiers to obtain their current accuracy is to simply learn how to zoom on top of the same, old feature extractors (e.g. that of AlexNet).\nUnclassifiable IN images Interestingly, even with optimal zooming, no model reaches 100% on IN images. We find that 0.39% of the IN+ReaL images were not classified correctly by any of the IN-trained classifiers and these images are similar to natural adversarial images (Fig. 2) and can be categorized into four groups:\n1. Lack of information (Fig. 2a): Images lack adequate signals for classification due to low light, occlusion, blurriness, or noise.\n2. Rare cases (Fig. 2b): Images depict the primary object but in an uncommon form, pose, or rendition.\n3. Illusions (Fig. 2c): Images have misleading elements, like a shadow appearing as a staircase, leading to misclassification.\n4. Many objects (Fig. 2d): Images displaying several classes of objects but not all classes are listed in the set of groundtruth labels.\nOOD datasets pose a significant challenge to IN-trained models despite optimal zooming. Across IN-A, IN-R, and ON, all IN-trained models perform far below the 324-crop random baseline (100%) with the highest score being 79.28% (Tab. 1). In contrast, CLIP reaches far better scores than IN-trained models (98.45% on IN-A and 99.20% on IN-R; Tab. 1). Our result suggests that OOD images (e.g. objects in unusual poses or renditions) require a more robust feature extractor to recognize besides zooming. And that CLIP was trained on an Internet-scale dataset [53] and thus is much more familiar with a variety of poses, styles, and shapes of objects [21].\nAmong the 324 zoom transformations, for each (classifier, dataset) pair, we initially construct a bipartite graph connecting transforms to images based on their correct classification. With this graph, we employ the iterative, greedy minimum-set cover algorithm [64, 35] to compute the minimum set of transforms required to achieve the upper-bound accuracy detailed in Sec. 4.1. Through this process, we discover that, on average, only 70% of the transforms are essential. Furthermore, we identify the top-36 zoom transforms most important to classification (see visualizations in Appendix D.1). More details on this process can be found in Appendix B.4.\nThe upper-bound accuracy using 36 crops (Tab. 1b) is only slightly lower than that when using all 324 crops but is substantially higher than (1) the standard 1-crop, e.g. 85.19% vs. 56.16% for AlexNet on IN (Tab. 1b); and (2) the random baseline (i.e. 3.6% for IN). Our result confirms that these 36 zoom transforms are indeed important to classification (not because models are given 36 random trials per image) and that studying them might reveal interesting insights into the datasets.\nAs our 324 transforms include both zoom-in and zoom-out, we further analyze the contribution of each zoom type to each dataset. We find that, across 7 datasets, zoom-in is more useful than zoom-out. And that zoom-out is the most important to abstract images i.e., of IN-R and IN-S (Appendix B.6)."
        },
        {
            "heading": "4.2 ImageNet-A and ObjectNet suffer from a severe center bias",
            "text": "The standard image pre-processing for IN-trained models involves resizing the image so its smaller dimension is 256, then taking the center 224\u00d7 224 crop of the resized image [2, 38]. While suitable for ImageNet, this pre-processing may not be optimal for every OOD dataset, not allowing a model to fully utilize off-center visual cues (which optimal zooming could). Leveraging the minimum set of transforms obtained in Appendix B.4, we quantify which spatial locations (out of 9 anchors; Fig. 1b) contain the most discriminative features in each dataset. That is, we compute the upper-bound accuracy for each of the 9 anchor points per dataset and discover biases in some benchmarks.\nExperiment For each image, we have 9 anchors (Fig. 1b) and the originally K = 36 zoomed versions per anchor as defined in Sec. 3. Yet, after reducing to the minimum set (Appendix B.4), K averages at 25, over all datasets, and 10 \u2264 K \u2264 31. Here, we count the probability that the K zoomed versions per anchor lead to a correct prediction. In other words, we compute the upper-bound accuracy as in Sec. 4.1 but for each anchor separately.\nResults First, as expected, the upper-bound accuracy for each anchor (Fig. 3) is consistently lower than when all 9 anchors are allowed (Tab. 1c). Second, across all 6 datasets, the center anchor consistently achieves the highest upper-bound accuracy versus the other 8 locations (Fig. 3 and Appendix B.2), indicating a center bias in all datasets. However, we find this bias is small in IN, IN-R, and IN-S but large in IN-A and ON (i.e. the largest difference between center accuracy and the lowest off-center accuracy is around -25 and -23 points, respectively; whereas for other datasets, it is around (-1) to (-5) points, as shown in Fig. 3).\nThe center bias in ObjectNet can be explained by the fact that the images were captured using smartphones with aspect ratios of 2:3 or 9:16 (Appendix D.3.3). Overall, such strong center bias in IN-A and ON may not be desirable since improvements on these two benchmarks may be attributed to learning to zoom to the center as opposed to the intended quest of recognizing objects in unusual forms (IN-A) or poses (ON). By merely upscaling the image and center cropping, we can achieve higher accuracy using nearly all the same models on these two datasets (Figs. A14 and A17).\nWe also find that, during test time, center-zooming (Appendix B.5) increases the top-1 accuracy of all IN-trained models but not CLIP, even on IN-A and ON images. This observation is intriguing considering these OOD datasets contain more distracting objects than ImageNet images (Appendix C.2) and therefore, center-zooming should de-clutter the scene for more accurate classification. However, CLIP prefers a specific zoom scale that provides sufficient background for object recognition\u2014it struggles to identify a single object in a tightly-cropped image [85]. Future research should examine whether this \u201czoom bias\u201d of CLIP is due to its image- or text-encoder, or both.\n4.3 Test-time augmentation of MEMO with only zoom-in transforms improves accuracy\nAggregating model predictions over zoom-in versions of the input image during test time leads to higher top-1 accuracy on IN, IN-ReaL, IN-A and ON, but lower accuracy on IN-R and IN-S (Appendix B.7). However, interestingly, always zooming out on IN-R and IN-S abstract images also hurts accuracy, suggesting that an adaptive zooming strategy might be a better approach.\nHere, we test building such an adaptive test-time zooming strategy by modifying MEMO [83], a SOTA test-time augmentation method that finetunes a pre-trained classifier at test time to achieve a more accurate prediction. Specifically, MEMO finds a network that produces a low-entropy predicted label over a set of K = 16 augmented versions of the test image I and then runs this finetuned model on I again to produce the final prediction. It does this by applying different augmentations to the test point I to get augmented points I1, . . . , IK , passing these through the model to obtain predictive distributions, and updating the model parameters by minimizing the entropy of the averaged marginal distribution over predictions. While improving accuracy, MEMO requires a pre-defined set of diverse augmentation transforms (e.g. sheer, rotate, and solarize in AugMix [26]). Yet, the hyperparameters for each type of transform are hard-coded, and the contribution of each transform to improved classification accuracy is unknown.\nWe improve MEMO\u2019s accuracy and interpretability by replacing AugMix transforms with only zoom-in functions. Intuitively, a model first looks at all zoomed-in frames of the input image (at different zoom scales and locations) and then decides to achieve the most confident prediction.\nExperiment MEMO relies on AugMix [25], which applies a set of 13 image transforms, such as translation, rotation, and color distortion, to an original image at varying intensities, and then chains them together to create K = 16 new augmented images (examples in Appendix D.5).\nWe replace AugMix with RandomResizedCrop [4] (RRC), which takes a random crop of the input image (i.e. at a random location, random rectangular area, and a random aspect ratio) and then resizes it to the fixed 224\u00d7224 (i.e. the network input size). RRC basically implements a random zoom-in function (examples in Appendix D.5).\nWe compare the original MEMO [83] (which uses AugMix) and our version that uses RRC on five benchmarks (IN, IN-A, IN-R, IN-S, and ON). We follow the same experimental setup as in [83] (e.g. K = 16). Specifically, we test three ResNet-50 variants that were pre-trained using distinct augmentation techniques.2\n2The ResNet-50 model used as a baseline in this Sec. 4.3 is different from that in our other (non-MEMO) sections of the paper.\nWe utilize Grad-Cam [58] to understand the impact of MIMO on the network\u2019s attentions within the final layer, both before and after modification. Specifically, our investigation seeks test our hypothesis concerning the model\u2019s focus on the regions of interest within an image.\nResults Both our MEMO + RRC and the original MEMO + AugMix [83] consistently outperform the baseline models, which do not use MEMO, on all five datasets (Tab. 2). That is, when combined with MEMO, zoom-in transforms implemented via RRC are also helpful in classifying IN-S and IN-R images\u2014where we previously find zoom-in to not help in mean/max aggregation (Appendix B.7).\nOn average, over all three models and five datasets, our RRC outperforms AugMix by +0.28 points, with a larger impact on IN-A, where it achieves a mean improvement of +1.10 points (Tab. 2). Our results show that zoom-in alone can be a useful inductive bias, helping improve downstream image classification. In contrast, some of the transformations among the 13 transform functions in AugMix may not be essential to the results of Zhang et al. [83] (no ablation studies of transformations were provided in [83]) and are less effective than our zoom-in.\nFigure 4 shows Grad-CAM visualizations for three samples, providing evidence of how the network\u2019s behavior changes before and after the MEMO update. For an image of a pug, the network initially focused on a kitchen appliance, failing to detect the object correctly. After applying the MEMO modification, it refocused on the dog, classifying it accurately. Similarly, in an image of a fox squirrel, the network initially had a diffuse focus but refocused on the fox squirrel after the update. These results demonstrate the effectiveness of the MEMO modification in guiding the network\u2019s attention or encouraging the model to perform an implicit zoom on the regions of interest, thereby improving its classification performance."
        },
        {
            "heading": "4.4 ImageNet-Hard: A benchmark with images that remain unclassifiable, even after 324 zoom attempts",
            "text": "Existing ImageNet-scale benchmarks followed one of the following three construction methods: (1) perturbing real images with the aim of making them harder for models to classify (e.g., ImageNetC [24] and DAmageNet [13]); (2) collecting the real images that models misclassify (e.g., IN-A, ImageNet-O [27]); or (3) setting up a highly-controlled data collection process (e.g., IN-S and ON). Yet, none of such benchmarks explicitly challenge models on the ability to recognize a well-framed object in an image (i.e., no zooming required). For example, ON is supposed to test the recognition of objects in unusual poses but the cluttered background in ON images is actually a major reason for misclassification (Sec. 4.2). Furthermore, the results in Tab. 1 suggest that given optimal zooming, these existing benchmarks only challenge IN-trained models but not the Internet-scale vision-language models (e.g. CLIP) anymore. We propose ImageNet-Hard, a novel ImageNet-scale benchmark that challenges existing and future SOTA models. ImageNet-Hard is a collection of images that the SOTA CLIP-ViT-L/14 fails to correctly classify even when 324 zooming attempts are provided."
        },
        {
            "heading": "4.4.1 ImageNet-Hard construction",
            "text": "Initial data collection We take CLIP-ViT-L/14 (the highest-performing model in Tab. 1) and run the zooming procedure to find \u201cUnclassifiable images\u201d (defined in Sec. 4.1) from the following six datasets: IN-V2 [56], IN-ReaL, IN-A, IN-R, IN-S, and ON. That is, for each image x, we generate 324 zoomed versions of x and feed them into CLIP-ViT-L/14. We add x to ImageNet-Hard only if none of the 324 versions are correctly classified.\nAdding ImageNet-C The original IN-C [24] are the original IN images but center-cropped to 224 \u00d7 224 px, which significantly makes the classification task unnecessarily more ill-posed (e.g., by adding Gaussian noise to a crop where the main object is already removed).\nTo find a subset of IN-C images for adding into ImageNet-Hard, we first re-generate ImageNet-C by adding the 19 types of corruption noise to IN without resizing the original IN images. Second, we run CLIP-ViT-L/14 on all 19 corruption types and manually select a subset of six diverse and lowest-accuracy corruption groups: Impulse Noise, Frost, Fog, Snow, Brightness, and Zoom Blur. We repeat the initial data collection process for these 6 image sets of IN-C.\nGroundtruth labels After the above procedure, our dataset contains 13,925 images collected from IN+ReaL, IN-V2, IN-A, IN-C, IN-R, IN-S, and ON (see the distribution in Appendix E.1). ImageNetHard presents a 1000-way classification task where the 1000 classes are from ImageNet. We manually inspect all images and remove 295 samples that are obviously ill-posed (e.g. an entirely black image but labeled great white shark in IN-S Fig. A60), arriving at a total of 13,630 ImageNet-Hard images. A sample contains only one groundtruth label from its original datasets except for IN and IN-C images, which have a set of IN+ReaL labels. Each IN or IN-C image is considered correctly labeled by a model if its top-1 predicted label is among the groundtruth labels.\nRefining groundtruth labels via human feedback Label noise is still present in IN and OOD benchmarks despite cleaning efforts [10, 56, 81]. Since ImageNet-Hard contains images misclassified by CLIP-ViT-L/14, our manual inspection confirms many misclassified images have debatable labels.\nTo ameliorate the issue, we orchestrate a human feedback study for eliminating images with inaccurate labels. First, the first author examine every image and flag 3,133 images as ambiguous and needs verification. Then, we have two groups of annotators to help verify the labels (by choosing Accept, Reject, or Not Sure). Group A is composed of three students, each examine all 3,133 images where Group B is composed of 38 students, each examine 50 randomly-selected images. Our inter-annotator aggregation procedure merges labels from both groups and results in 2,280 images removed (out of 3,133 originally flagged), leaving ImageNet-Hard at a total of 11,350 images.\nThat is, we accept an image x if one of the two conditions is satisfied: (1) when all 3/3 group-A annotators accept x; or (2) when 2/3 group-A annotators accept x and all group-B reviewers of x accept x (assuming at least 1 group-B annotator reviews x; otherwise x will be rejected).\nInspired by IN-ReaL [10], we further clean up the labels by eliminating 370 images associated with the labels sunglass, sunglasses, tub, bathtub, cradle, bassinet, projectile, and missile, i.e., the classes that often contain similar images that belong to more than one class. After this refinement, the final ImageNet-Hard dataset contains a total of 10,980 images.\n4K version We utilize GigaGAN [33] to upscale every image in our final dataset and construct ImageNet-Hard-4K, which is aimed to facilitate future research into how a super-resolution step may improve image classification results (e.g., to classify an object when the image is blurry).\nRelease ImageNet-Hard and ImageNet-Hard-4K are released on HuggingFace (see samples in Fig. A49) under MIT License. Code for evaluating models on ImageNet-Hard is on GitHub."
        },
        {
            "heading": "4.4.2 ImageNet-Hard challenges SOTA classifiers, especially those operating at 224\u00d7224",
            "text": "Here, we evaluate the standard 1-crop, top-1 accuracy of SOTA classifiers on ImageNet-Hard. We use the image pre-processing function defined by each classifier. In addition to the 6 models in Sec. 4.1, we also test CLIP-ViT-L/14@336px [53], EfficientNet (B0@224px and B7@600px) [69], and EfficientNet-L2@800px [1]. CLIP-ViT-L/14@336px, EfficientNet-B7@600px, and EfficientNetL2@800px are state-of-the-art models that operate at high resolutions of 336\u00d7336, 600\u00d7600, and 800\u00d7800 respectively. In addition, our evaluation includes models from the OpenCLIP family [30].\nResults Tab. 3 shows fairly low top-1 accuracy by various classifiers on ImageNet-Hard. First, all well-known IN-trained classifiers that operate at 224\u00d7224 perform poorly between 7.34% (AlexNet) and 18.52% accuracy (ViT-B/32).\nSince ImageNet-Hard is based on a collection of images that OpenAI\u2019s CLIP ViT-L/14@224px mislabels, this classifier\u2019s accuracy on our dataset is only 1.86%. Yet, interestingly, CLIP-ViTL/14@336px also performs poorly at 2.02% (Tab. 3). Furthermore, all 68 tested OpenCLIP models perform poorly, with an accuracy below 16% (see details in Appendix E.6).\nSeparately, we observe a trend that models operating at a higher resolution tend to perform better on ImageNet-Hard with EfficientNet-L2@800px scoring highest at 39.00% (compared to 88.40% [79] on the original ImageNet). Overall, all models perform substantially worse on ImageNet-Hard (Tab. 3) than on other ImageNet-scale datasets (see Tab. A6; 1-crop). This result is expected because ImageNet-Hard is a set of hard cases collected from those OOD benchmarks.\nImageNet-Hard-4K We find that when upsampling images to 4K using GigaGAN [33] and downsampling them back to the resolution of each classifier does not help but even hurt the accuracy slightly (Tab. A13). Given that GigaGAN performs remarkably well, this result suggests ImageNetHard is different from typical fine-grained animal classification where improving the texture details increases classification accuracy [76]. The next section (Sec. 4.5) sheds light on model failures on ImageNet-Hard, revealing challenges posed to future SOTA models."
        },
        {
            "heading": "4.5 Analysis of Image Classification Errors",
            "text": "Motivated by the fact that EfficientNet-L2 is the best classifier on ImageNet-Hard, we qualitatively analyze its failure cases to characterize the challenge posed by our benchmark. Specifically, we provide gpt-3.5-turbo [51] with a pair of EfficientNet-L2\u2019s top-1 (incorrect) label and the groundtruth label and ask it to categorize the error into \u201ccommon\u201d or \u201crare\u201d based on the labels\u2019 semantic similarity (see Appendix E.3 for full details). For instance, mislabeling bucket into barrel is common (as two objects are quite related) while mislabeling cloak into jigsaw puzzle is rare.\nResults See Appendix E.4 for samples of wrong labels that EfficientNet-L2 most frequently misclassifies into. We find that 39.4% of EfficientNet-L2\u2019s misclassifications on the ImageNet-Hard dataset are \u201ccommon\u201d, while 60.6% are \u201crare\u201d.\nA. Common group captures model confusion between two related classes (e.g. two fish species: clownfish and rock beauty; Fig. 5a). Yet, another source of problem for these \u201cerrors\u201d is the debatable groundtruth labels, which may require domain-expert annotators to verify and rectify [45].\nB. Rare group captures errors where the model confusion is between two semantically distant classes (e.g., llama \u2192 plectrum; Fig. 5b). This often happens with abstract images or objects in unusual\nposes [5] or forms [15]. Classifying this group of images is challenging and sometimes requires a strong understanding of context and reasoning capabilities."
        },
        {
            "heading": "5 Discussion and Conclusion",
            "text": "Limitations By manual inspection, we estimate 14.7% of labeling noise, which ImageNet-Hard inherits from the source datasets.\nOur study rigorously analyzed the zooming effect on six known classifiers and image classification benchmarks. We first demonstrate that previous state-of-the-art classifiers, as old as AlexNet [38], could potentially achieve near 90% accuracy with optimal zooming. This sparks the intriguing question of whether image classifiers\u2019 evolution over the past ten years is about mastering where and at what scale to zoom (instead of enhancing feature extractors, a.k.a. representation learning [3]). Through another lens, we probe the evolution by analyzing the implicit zooming mechanisms that deep classifiers apply to input images. This perspective diverges from [54], which studied the progression of representation learning from CNNs to ViTs.\nWe are the first to document the spatial biases of existing benchmarks. Notably, IN-A and ON contain a large center bias and simply zooming to the center will de-clutter the scene and yield a high accuracy (24.69% for ViT-B/32 on IN-A; Tab. A5), which is competitive with state-of-the-art trained models (e.g. 24.1% of Robust ViT [11]) and much higher than state-of-the-art TTA techniques (e.g. 11.21% of MEMO [83]; Tab. 2). Our simple, but strong zoom-in baselines on IN-A and ON motivate future research into better-controlled benchmarks that more explicitly test models on a set of pre-defined properties. Our proposed TTA method with zoom-in transforms (MEMO + RRC) is not only more accurate but also more interpretable and faster to run (Tab. A7) than the original MEMO.\nFinally, we introduce ImageNet-Hard (Sec. 4.4), a new challenging dataset for SOTA IN-trained and vision-language classifiers."
        },
        {
            "heading": "Acknowledgement",
            "text": "GN is supported by Auburn University PGRF Fellowship. AN was supported by a NSF CAREER award No. 2145767, and donations from NaphCare Foundation, and Adobe Research. We greatly appreciate David Seunghyun Yoon\u2019s help in generating the ImageNet-Hard-4K (Sec. 4.4.2) version using GigaGAN. We also thank those students at Auburn University and Alberta University who participated in our experiment for cleaning up the labels of ImageNet-Hard images."
        },
        {
            "heading": "ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification",
            "text": "A Implementation details\nIn this section, we provide a detailed description of our experimental setup, including the Python code for our zoom transform, the classifiers we employed, and the setup we used for zero-shot classification.\nA.1 Sample Python code for zoom-based transform\nfrom PIL import Image import torchvision.transforms.functional as fv import torchvision.transforms as transforms from functools import partial\ndef crop_at(size , slice_x , slice_y): def slice_crop(image , size , slice_x , slice_y):\nwidth , height = image.size tile_size_x = width // 3 tile_size_y = height // 3 anchor_x = (slice_y * tile_size_x) + (tile_size_x // 2) anchor_y = (slice_x * tile_size_y) + (tile_size_y // 2) return fv.crop(\nimage , anchor_y - (size // 2), anchor_x - (size // 2), size , size ,\n) return partial(slice_crop , size=size , slice_x=slice_x , slice_y= slice_y)\nzoom_scale = 255 zoom_transform = transforms.Compose(\n[ transforms.Resize(\nzoom_scale , interpolation=transforms.InterpolationMode\n.BICUBIC , max_size=None , antialias=None ,\n), crop_at (224, i, j),\n] )\nFigure A1: Sample python code."
        },
        {
            "heading": "A.2 Datasets\u2019 licenses",
            "text": "Dataset Name License ImageNet Custom license, non-commercial ImageNet-A License ImageNet-R MIT License ImageNet-Sketch MIT License ImageNet-C MIT License ObjectNet Custom license derived from Creative Commons Attribution 4.0 ImageNet-V2 MIT License\nTable A1: Dataset Licenses"
        },
        {
            "heading": "A.3 Zoom Scales used",
            "text": "In our experiments, we tried the following zoom scales:\n10, 16, 32, 48, 64, 96, 122, 128, 192, 224, 235, 240, 256, 288, 320, 348, 384, 448, 460, 512,\n573, 576, 640, 664, 672, 680, 686, 690, 700, 720, 768, 798, 832, 896, 911, 1024."
        },
        {
            "heading": "A.4 Model selection",
            "text": "We use the official OpenAI\u2019s official CLIP for all CLIP-related experiments. All IN-trained models are retrieved from the torchvision [48] library. For models from the OpenCLIP family, we utilize the OpenCLIP library version 2.20.0. In the case of the EfficientNet-B family, we use the Hugging Face Transformers library. Lastly, for EfficientNet-L2, we use the implementation from the timm library."
        },
        {
            "heading": "A.5 Zero-shot classification using CLIP",
            "text": "For CLIP, we follow the standard zero-shot classification. This involves creating a text template for each class in the dataset, which contains a generic description of an image featuring an object from that class. Then, we use CLIP\u2019s text encoder to obtain embeddings for these templates and then average them to obtain a final vector that represents the class. To classify an image, we calculate the cosine similarity between its embedding and the text vectors for each class and then select the class with the highest value.\nA.6 Zoom-based transform\n(a)\nwool tennis ball 3.73% wool 83.23% wool 77.49%\nknot 55.13% wool 61.55% wool 77.53% spindle 45.44%\nZoom scale 512\nStandard center crop wool 89.71% wool 34.45% tick 4.0%\n(b)\nFigure A2: (a) Making a 3-by-3 uniform grid out of the image. We pick the center point in each region as the anchor. (b) Sample image showing how our zoom transform is applied to an image."
        },
        {
            "heading": "B Additional Results",
            "text": "In this section, we provide additional results for our experiments.\nB.1 Zooming out is needed for a small portion of the datasets In our approach, we leverage the power of both zoom-in and zoom-out transforms, and Tab. 1 results indicate that this combined zooming approach can be effective in classifying images from diverse datasets. Zooming in enhances texture patterns while zooming out provides a better perspective of the object\u2019s shape. The question we aim to answer is which dataset and model pairs require which type of zoom, and whether zooming is always necessary. Additionally, we investigate which types of networks are less reliant on explicit zooming, as they implicitly focus on the main object in the image.\nExperiment We separate zoom transforms into three groups and report the maximum possible accuracy as defined in Sec. 3. We use transforms in the minimum set covers (as shown in Fig. A10) for each dataset and classifier pair. We then report the number of images that can only be classified using transforms in each group separately.\nhyena\nfrench horn 15.65% hyena 8.31% snow leopard 10.41%\ntriceratops 31.22% american alligator 6.17% hyena 18.64% snow leopard 21.78%\nZoom scale 122\nStandard center crop web site 5.47% hyena 15.64% snow leopard 11.52%\nFigure A3: A sample image from the ImageNet-Sketch dataset that can only be solved by zooming out. For this image, with the standard ImageNet transform, the entire body of the animal is not visible. Instead, zooming out of the image helps you see the whole body of the animal. More samples can be found in Appendix D.3.\nResults In general, we find that zooming in is more effective than zooming out. Zooming in provides two benefits: (1) it helps the model to focus on the key region where the target object is located, and (2) the model can extract features from the target object at a higher resolution. Across all methods and datasets, we can see a certain percentage of images are only classifiable using transforms of the zoom-out group. In particular, for ImageNet-R and ImageNet-Sketch, between 1.2% \u2212 3% (Table A2) of the entire dataset can only be solved using a transform in the zoom-out group. This is especially true for drawings, where the texture may lack distinguishable features, and zooming out allows us to better perceive the shape.\nTable A2: Breakdown of maximum possible accuracy by different zoom groups. In each dataset, certain images necessitate a specific zoom group for correct classification regardless of the model being used. However, CLIP performs well overall without depending heavily on a particular zoom level. On average, the percentage of datasets that can only be solved with a specific zoom group is very small for this model.\nDataset Model zoom-inSolve zoom-out Solves zoom-224 Solves Only zoom-in Solves Only zoom-out Solves Only zoom-224 Solves\nImageNet\nResNet-18 94.57 79.49 81.16 10.59 0.43 0.08 ResNet-50 96.30 85.84 86.39 7.59 0.40 0.04 ViT-B/32 96.83 86.18 85.12 7.59 0.30 0.02 VGG-16 94.60 82.11 83.08 8.92 0.58 0.07 AlexNet 89.17 62.92 67.98 18.01 0.65 0.18 CLIP-ViT-L/14 95.82 90.80 87.04 4.81 0.83 0.05\nImageNet ReaL ResNet-18 97.37 86.10 87.62 7.38 0.27 0.07 ResNet-50 98.22 91.07 91.87 4.65 0.25 0.04 ViT-B/32 98.50 90.79 88.06 4.92 0.18 0.03 VGG-16 97.38 88.43 89.40 6.02 0.38 0.07 AlexNet 93.15 69.58 74.85 15.47 0.45 0.19 CLIP-ViT-L/14 98.05 94.44 91.69 3.20 0.55 0.04\nImageNet+ReaL ResNet-18 97.16 85.51 86.77 7.72 0.28 0.05 ResNet-50 98.25 91.10 91.77 4.60 0.24 0.03 ViT-B/32 98.70 91.00 90.95 4.92 0.14 0.02 VGG-16 97.12 87.88 89.09 6.25 0.42 0.06 AlexNet 92.79 68.65 73.93 16.25 0.47 0.16 CLIP-ViT-L/14 98.24 95.09 92.41 2.75 0.47 0.04\nImageNet-A\nResNet-18 63.66 47.95 45.37 13.97 2.75 0.21 ResNet-50 65.28 52.36 48.59 12.05 3.13 0.22 ViT-B/32 73.07 56.34 54.84 14.20 2.04 0.27 VGG-16 56.67 44.95 39.35 11.80 3.85 0.24 AlexNet 52.69 32.86 31.95 17.15 2.34 0.30 CLIP-ViT-L/14 98.35 96.71 93.57 1.70 0.69 0.04\nImageNet-R\nResNet-18 57.07 12.19 10.07 40.67 0.92 0.19 ResNet-50 64.52 12.95 10.36 48.72 1.00 0.23 ViT-B/32 76.71 18.57 21.92 51.75 0.85 0.15 VGG-16 56.59 13.15 13.27 38.24 0.93 0.29 AlexNet 39.91 10.39 9.11 26.27 1.08 0.36 CLIP-ViT-L/14 97.99 81.32 77.03 12.01 0.44 0.05\nImageNet-Sketch ResNet-18 41.14 27.06 27.41 11.83 1.77 0.36 ResNet-50 44.72 32.80 31.45 10.99 2.23 0.24 ViT-B/32 53.45 37.43 37.38 13.11 1.83 0.36 VGG-16 36.20 27.20 24.59 9.47 2.97 0.28 AlexNet 27.71 13.84 15.11 11.26 1.22 0.33 CLIP-ViT-L/14 86.20 80.67 73.94 6.64 2.38 0.12\nObjectNet\nResNet-18 68.98 38.52 37.23 25.76 1.93 0.25 ResNet-50 74.16 51.56 47.79 19.68 2.16 0.30 ViT-B/32 77.66 44.49 42.65 27.43 1.34 0.20 VGG-16 69.19 41.72 39.49 23.34 2.27 0.31 AlexNet 56.76 23.45 22.59 28.85 2.27 0.33 CLIP-ViT-L/14 91.28 82.22 77.60 8.37 1.38 0.15\nAverage\nResNet-18 74.28 53.83 53.66 16.85 1.19 0.17 ResNet-50 77.35 59.67 58.32 15.47 1.34 0.16 ViT-B/32 82.13 60.69 60.13 17.70 0.95 0.15 VGG-16 72.54 55.06 54.04 14.86 1.63 0.19 AlexNet 64.60 40.24 42.22 19.04 1.21 0.26 CLIP-ViT-L/14 95.13 88.75 84.75 5.64 0.95 0.07\nB.2 Anchor-based analysis of Center bias in ImageNet and OOD datasets\n81.72 (-7.06) 85.25 (-3.53) 81.62 (-7.16)\n84.88 (-3.90) 88.78 84.70 (-4.08)\n81.91 (-6.87) 85.35 (-3.43) 81.76 (-7.02)\n(a) ImageNet-ReaL\n15.93 (-9.58) 17.24 (-8.27) 15.00 (-10.51)\n19.01 (-6.50) 25.51 17.51 (-8.00)\n16.79 (-8.72) 18.89 (-6.62) 16.20 (-9.31)\n(b) ImageNet-A\n40.39 (-6.83) 43.67 (-3.55) 40.20 (-7.02)\n43.05 (-4.17) 47.22 42.97 (-4.25)\n39.67 (-7.55) 42.88 (-4.34) 39.58 (-7.64)\n(c) ImageNet-R\n20.64 (-4.50) 23.07 (-2.07) 20.64 (-4.50)\n22.26 (-2.88) 25.14 22.20 (-2.94)\n19.73 (-5.41) 21.88 (-3.26) 19.70 (-5.44)\n(d) ImageNet-Sketch\n21.42 (-26.74) 22.49 (-25.67) 21.42 (-26.74)\n38.20 (-9.96) 48.16 38.35 (-9.81)\n21.15 (-27.01) 22.42 (-25.74) 21.08 (-27.08)\n(e) ObjectNet\nFigure A4: AlexNet\n92.77 (-2.75) 94.20 (-1.32) 92.72 (-2.80)\n94.13 (-1.39) 95.52 94.02 (-1.50)\n92.83 (-2.69) 94.41 (-1.11) 93.04 (-2.48)\n(a) ImageNet-ReaL\n21.83 (-17.53) 26.35 (-13.01) 21.95 (-17.41)\n26.97 (-12.39) 39.36 26.19 (-13.17)\n22.03 (-17.33) 27.37 (-11.99) 21.99 (-17.37)\n(b) ImageNet-A\n50.04 (-4.24) 51.98 (-2.30) 49.86 (-4.42)\n51.76 (-2.52) 54.28 51.71 (-2.57)\n49.40 (-4.88) 51.35 (-2.93) 49.31 (-4.97)\n(c) ImageNet-R\n32.43 (-3.48) 34.25 (-1.66) 32.13 (-3.78)\n33.92 (-1.99) 35.91 33.63 (-2.28)\n31.60 (-4.31) 33.30 (-2.61) 31.35 (-4.56)\n(d) ImageNet-Sketch\n37.42 (-25.03) 38.70 (-23.75) 37.42 (-25.03)\n54.41 (-8.04) 62.45 54.66 (-7.79)\n37.74 (-24.71) 39.07 (-23.38) 37.86 (-24.59)\n(e) ObjectNet\nFigure A5: VGG16\n91.58 (-3.63) 93.49 (-1.72) 91.74 (-3.47)\n93.33 (-1.88) 95.21 93.46 (-1.75)\n91.87 (-3.34) 93.71 (-1.50) 92.03 (-3.18)\n(a) ImageNet-ReaL\n19.35 (-19.74) 22.21 (-16.88) 19.21 (-19.88)\n23.87 (-15.22) 39.09 23.41 (-15.68)\n20.20 (-18.89) 23.21 (-15.88) 19.88 (-19.21)\n(b) ImageNet-A\n54.42 (-5.72) 57.41 (-2.73) 54.66 (-5.48)\n56.70 (-3.44) 60.14 56.89 (-3.25)\n53.78 (-6.36) 56.52 (-3.62) 53.84 (-6.30)\n(c) ImageNet-R\n34.47 (-4.87) 37.05 (-2.29) 34.62 (-4.72)\n36.29 (-3.05) 39.34 36.43 (-2.91)\n33.71 (-5.63) 36.04 (-3.30) 33.77 (-5.57)\n(d) ImageNet-Sketch\n34.15 (-28.51) 35.87 (-26.79) 34.34 (-28.32)\n53.14 (-9.52) 62.66 53.39 (-9.27)\n35.11 (-27.55) 36.66 (-26.00) 35.06 (-27.60)\n(e) ObjectNet\nFigure A6: ResNet-18\n94.53 (-2.24) 95.82 (-0.95) 94.82 (-1.95)\n95.58 (-1.19) 96.77 95.91 (-0.86)\n94.65 (-2.12) 95.92 (-0.85) 94.94 (-1.83)\n(a) ImageNet-ReaL\n21.17 (-25.32) 26.77 (-19.72) 21.59 (-24.90)\n27.57 (-18.92) 46.49 26.57 (-19.92)\n22.52 (-23.97) 27.61 (-18.88) 22.31 (-24.18)\n(b) ImageNet-A\n57.55 (-4.97) 60.28 (-2.24) 57.59 (-4.93)\n59.49 (-3.03) 62.52 59.62 (-2.90)\n57.09 (-5.43) 59.60 (-2.92) 57.19 (-5.33)\n(c) ImageNet-R\n38.88 (-4.04) 41.08 (-1.84) 39.01 (-3.91)\n40.57 (-2.35) 42.92 40.71 (-2.21)\n38.29 (-4.63) 40.38 (-2.54) 38.37 (-4.55)\n(d) ImageNet-Sketch\n46.81 (-22.49) 47.96 (-21.34) 46.85 (-22.45)\n62.25 (-7.05) 69.30 62.53 (-6.77)\n48.42 (-20.88) 49.84 (-19.46) 48.55 (-20.75)\n(e) ObjectNet\nFigure A7: ResNet-50\nB.3 Distribution of the minimum set cover per classifier and dataset In this section, we provide details on the distribution of minimum set cover size.\nB.4 Only 70% of all transforms are needed to reach maximum possible accuracy In Sec. 4.1, we first pre-define all 324 zoom transforms and then compute the maximum possible accuracy to ensure the predicted labels were the results of models looking at a controlled zoomed region (i.e. not because a model was given 324 arbitrary trials per image). Here, we aim to compute the minimum number of zoom settings required for a model to reach the same upper-bound accuracy.\n94.45 (-2.41) 95.55 (-1.31) 94.14 (-2.72)\n95.61 (-1.25) 96.86 95.41 (-1.45)\n94.70 (-2.16) 95.80 (-1.06) 94.26 (-2.60)\n(a) ImageNet-ReaL\n32.72 (-27.20) 38.48 (-21.44) 33.21 (-26.71)\n40.28 (-19.64) 59.92 39.49 (-20.43)\n34.51 (-25.41) 40.13 (-19.79) 33.13 (-26.79)\n(b) ImageNet-A\n63.91 (-5.54) 66.87 (-2.58) 64.06 (-5.39)\n65.79 (-3.66) 69.45 65.96 (-3.49)\n62.81 (-6.64) 65.31 (-4.14) 62.82 (-6.63)\n(c) ImageNet-R\n46.29 (-4.55) 48.88 (-1.96) 46.29 (-4.55)\n47.86 (-2.98) 50.84 47.80 (-3.04)\n44.83 (-6.01) 46.91 (-3.93) 44.73 (-6.11)\n(d) ImageNet-Sketch\n40.16 (-29.36) 41.54 (-27.98) 40.40 (-29.12)\n59.80 (-9.72) 69.52 59.93 (-9.59)\n41.95 (-27.57) 43.88 (-25.64) 42.02 (-27.50)\n(e) ObjectNet\nFigure A8: ViT-B/32\n92.42 (-2.02) 93.67 (-0.77) 92.76 (-1.68)\n93.64 (-0.80) 94.44 93.70 (-0.74)\n92.59 (-1.85) 93.86 (-0.58) 92.62 (-1.82)\n(a) ImageNet-ReaL\n77.31 (-15.00) 85.04 (-7.27) 77.53 (-14.78)\n85.41 (-6.90) 92.31 84.65 (-7.66)\n77.60 (-14.71) 84.49 (-7.82) 77.29 (-15.02)\n(b) ImageNet-A\n94.57 (-2.50) 95.92 (-1.15) 94.64 (-2.43)\n95.87 (-1.20) 97.07 95.80 (-1.27)\n94.15 (-2.92) 95.42 (-1.65) 94.26 (-2.81)\n(c) ImageNet-R\n74.57 (-4.86) 77.28 (-2.15) 74.65 (-4.78)\n77.08 (-2.35) 79.43 77.19 (-2.24)\n73.61 (-5.82) 76.24 (-3.19) 73.62 (-5.81)\n(d) ImageNet-Sketch\n66.81 (-20.16) 70.03 (-16.94) 67.88 (-19.09)\n82.06 (-4.91) 86.97 81.87 (-5.10)\n68.32 (-18.65) 71.38 (-15.59) 68.46 (-18.51)\n(e) ObjectNet\nFigure A9: CLIP-ViT-L/14\nTable A3: Distribution of the minimum set cover per classifier and dataset. (ZI: zoom-in, ZO: zoom-out, ZL: zoom-224)\nReaL IN-A IN-R IN-Sketch ON\nZI ZO ZL Total ZI ZO ZL Total ZI ZO ZL Total ZI ZO ZL Total ZI ZO ZL Total\nResNet-18 160 33 8 201 174 31 6 211 204 65 9 278 209 51 9 269 191 54 9 254 ResNet-50 136 33 9 178 165 42 7 214 200 62 9 271 216 56 9 281 187 63 9 259 ViT-B/32 134 30 4 168 167 19 7 193 196 52 9 257 218 46 9 273 206 58 9 273 VGG-16 158 34 9 201 181 33 8 222 214 66 9 289 210 54 9 273 198 52 9 259 AlexNet 191 40 8 239 170 33 9 212 212 51 9 272 217 49 9 275 201 58 9 268 CLIP-ViT-L/14 141 48 8 197 75 14 4 93 76 33 5 114 142 61 9 212 205 66 9 280\nEvaluating this minimum set may reveal spatial biases of a dataset (Sec. 4.2) as well as the implicit zoom operation that a state-of-the-art model (e.g. CLIP) may have learned. Experiment Given a (dataset, classifier) pair, we constructed a bipartite graph G = (N,E), where N = A \u222aB, A represents the set of transforms, and B represents the set of images. The edges E are defined as follows:\nE = {(ni, nj) | ni \u2208 A,nj \u2208 B, and transform ni leads to the correct classification of image nj}\nWe aim to find a minimum set cover [64, 35] in this graph, synonymous with finding a minimum subset of transforms among the 324 that lead to the correct prediction for all classifiable images in Sec. 4 (i.e. those that make up the accuracy scores in Tab. 1c), without unnecessary transforms. The resulting subset of transforms from the process leads to the correct prediction for all classifiable images without sacrificing accuracy. During each iteration of the greedy minimum set cover algorithm, the transform that yields the highest number of correct classifications for the remaining images is selected. This process continues until all of the images have been \u201ccovered,\u201d i.e. all images have connected to a transform with at least one edge. The result aligns with our initial goal to remove unnecessary zoom transforms while maintaining the maximum possible accuracy, as outlined in Sec. 4 (i.e., those that make up the accuracy scores in Tab. 1c). The outline of the algorithm can be seen in Algorithm 1. Results Fig. A10 shows the minimum number of transforms per dataset required to reach the maximum possible accuracy. Although this number varies depending on the dataset and classifier, on average, the size of the minimum cover is 229, which is \u223c70% of all 324 pre-defined transforms. We evaluate the maximum possible accuracy using the top 36 transforms, the same number as the number of zoom scales and report the results in Tab. 1b. This set of transforms is achieved by stopping the algorithm after 36 iterations, which provided us with 36 high-performing transforms. The maximum possible accuracy using only 36 crops is only slightly lower than that when using all 324 crops but is substantially higher than the standard 1-crop, e.g. 85.19% vs. 56.16% for AlexNet on IN (Tab. 1b). Also, the upper-bound accuracy for 36 crops being much higher than the\nAlgorithm 1 Greedy Minimum Set Cover for Transforms 1: Initialization: C = \u2205 (Covered set of images), T = \u2205 (Selected transforms) 2: while C \u0338= B do 3: Find ni \u2208 A \\ T that maximizes |nj \u2208 B \\ C | (ni, nj) \u2208 E| 4: C = C \u222a {nj | (ni, nj) \u2208 E} 5: T = T \u222a {ni} 6: end while 7: Result: The subset of transforms corresponding to T can classify images without sacrificing\naccuracy.\nFigure A10: The minimum number of zoom transforms (out of 324) required to achieve the maximum possible accuracy scores reported in Tab. 1c.\nIN ReaL IN+ReaL IN-A IN-R IN-S ON \u00b5\nAlexNet 255 239 246 212 272 275 268 252 VGG-16 242 201 201 222 289 273 259 241 ResNet-18 250 201 208 211 278 269 254 239 ResNet-50 234 178 183 214 271 281 259 231 ViT-B/32 233 168 173 193 257 273 273 224 CLIP-ViT-L/14 251 197 186 93 114 280 212 190\nrandom baseline (i.e. 3.6% for IN) confirms that the pre-defined zoom transforms are important to classification (not because models are given 36 random trials per image). The top-36 zoom transforms for ResNet-50 on ImageNet contain zooms at various locations in the image (see the visualizations in Appendix D.1). Remarkably, CLIP requires 190 transforms on average, which is fewer than every other model (Fig. A10; \u00b5 column). This can be attributed to either the implicit zoom power of CLIP or the fact it has a stronger feature extractor.\nB.5 Center-zooming increases the accuracy of all ImageNet-trained models but not CLIP Previously, we have found that CLIP obtains the best accuracy on all six datasets (Tab. 1a) and also requires the smallest minimum set of zoom transforms to obtain the upper-bound accuracy (Appendix B.4). It is important to understand what classification strategy a CLIP classifier internally performs to classify better. Here, we test the hypothesis that the state-of-the-art CLIP is already performing an implicit zoom on images. If that is true, directly zooming to the center, exploiting the strong center bias of ImageNet-A and ObjectNet, will not improve CLIP accuracy. Experiment We evaluate the accuracy of all models when center-zooming on IN-A and ON images at 11 different scales S \u2208 {128, 160, 192, ..., 448} (Fig. A11). That is, center-zooming at S first resizes the input image so that the smaller dimension becomes S and then takes a 224\u00d7224 center crop (zero-padding is applied when necessary). Results In Fig. A11, we show the changes in the top-1 accuracy (1-crop) when varying the centerzoom scales away from the default ImageNet transform scale (S = 256) for both ImageNet-A and ObjectNet. While IN-trained networks exhibit consistent improvement as the zoom scale increases, CLIP shows a monotonic decrease in performance (Fig. A11; yellow curves decreasing on both sides of S = 256). This result is surprising but consistent with our hypothesis that CLIP internally performs implicit zooming to reach its peak accuracy and therefore manually zooming (either in or out) at the center mostly ruins its performance.\nB.6 Zoom-in is more useful than zoom-out, which is most important to abstract images Zooming in enhances texture patterns while zooming out provides a better perspective of the object\u2019s shape, which is known to be useful to image classification [12, 19]. Results in Sec. 4.1 and Appendix B.4 indicate that this combined zooming approach can be effective in classifying images from diverse datasets. Here, we test which dataset and model pairs require which type of zoom, and whether zooming in or out is always necessary. Experiment To better understand the effectiveness of each zoom group, we calculate the maximum possible accuracy using all nine locations and different zoom scales S to show per-dataset trends. Additionally, we examined the percentage of images within each dataset that required a specific zoom group to be accurately classified. This analysis allowed us to gain a more comprehensive\nunderstanding of the role that each zoom group played in reaching the maximum possible accuracy reported in Tab. 1. Results The maximum possible accuracy for different zoom scales reveals a clear trend for each dataset. For instance, a slight zoom-out enhances accuracy for abstract image datasets like IN-Sketch (Fig. A12a). Conversely, for adversarial image datasets such as IN-A, zooming in improves accuracy (Fig. A12b) This pattern is also evident in evaluations using standard 1-crop accuracy (Appendix B.9). Furthermore, the percentage of images that are exclusively classifiable with the zoom-in group is consistently higher than the other two groups, i.e. using ViT-B/32 51.75% on IN-A, and 13.11% on IN-S (Tab. A4a). This shows that most datasets necessitate focusing on the object of interest in the image to both see texture patterns better and reduce background clutter (see Tab. A2 for full results). However, we also find that the zoom-out group is also necessary for the correct classification of a small portion of each dataset. For instance, 1.22% \u2212 2.97% of IN-S images (Tab. A4b) require a zoom-out transform to be correctly labeled (i.e. zoom-in does not help at all).\nTable A4: % of images in the entire dataset that require a particular zoom group to be classified correctly. See Tab. A2 for full results.\nzoom-in (a) zoom-out (b) zoom-224 (d) IN-A IN-S IN-A IN-S IN-A IN-S\nResNet-18 40.67 11.83 0.92 1.77 0.19 0.36 ResNet-50 48.72 10.99 1.00 2.23 0.23 0.24 ViT-B/32 51.75 13.11 0.85 1.83 0.15 0.36 VGG-16 38.24 9.47 0.93 2.97 0.29 0.28 AlexNet 26.27 11.26 1.08 1.22 0.36 0.33 CLIP-ViT-L/14 12.01 6.64 0.44 2.38 0.05 0.12\nB.7 Simple aggregation of the zoom transforms can improve accuracy on some datasets but not all\nSec. 4.1 and Appendix B.5 show that using the same feature extractors (even as old as AlexNet), it is possible to achieve higher image classification accuracy if we know where to zoom and at which scale. A practical follow-up question is: How to build a classifier that knows how to zoom given\na test image? In this section, we establish simple baselines that aggregate predictions over a set of zoom transforms. Experiment We employ the mean method from prior work [61, 46], and the max method to aggregate output marginal distributions. For a given image, we get N output distributions over classes from a classifier, in which N is the total number of used transforms. The aggregation process combines these N distributions and outputs a final prediction for the given image. In the aggregation step, we use the mean or max method to infer the final confidence for each class along N distributions. Finally, we select the class that has the highest confidence score. Additionally, we test 5-crop and 10-crop evaluation [38, 63, 23] and compare them with our methods. We use the transforms in the minimum set found for IN-ReaL to evaluate the remaining datasets. The purpose is to reduce the number of augmentations and prevent training on OOD benchmarks. Results max aggregation of zoom-in transforms results in the largest improvements on ImageNetA. That is, on IN-A, ViT-B/32 reaches a top-1 accuracy of 24.69% (+15.05) (Tabs. A5 and A6) and a ResNet-50 accuracy increases by +13.03 points from 16.62% to 29.65% (Appendix C.3)\u2013a surprisingly strong baseline for future studies. On ObjectNet, max aggregation of zoom-in transforms also yields +1.99 improvement over the 1-crop ViT-B/32 baseline. On the other hand, mean aggregation results in smaller but more consistent improvements over the 1-crop baseline for many datasets (+3.56 on IN, +4.08 on ReaL, +4.65 on IN-A, and +3.03 on ON; Tab. A5). mean aggregation (Tab. A5b) also outperforms the standard 5-crop and 10-crop [38, 23] aggregation on these four datasets (Tab. A5e\u2013f). In contrast, for all 6 datasets, aggregating zoom-out and zoom-224 transforms consistently worsen the performance over the 1-crop baseline (Tab. A5c\u2013d). That is, we find that for a few dozen images (e.g. sketches and abstract visuals; Fig. 1ac), interestingly, only zooming out can lead to a correct classification (Appendix B.6), yet for most images in these 6 benchmarks, zooming out hurts the accuracy. In summary, based on the insights from Sec. 4.1, showing that zooming could help classification, we find that simple methods for aggregating zoom-in transforms at test-time can directly improve model accuracy over the 1-crop and zoom-224 baselines on four benchmarks, i.e. all except IN-R and IN-S, which contain abstract images.\nTable A5: Top-1 accuracy (%) of aggregation methods on an IN-trained ViT-B/32 model. Compared to the 1-crop baseline, aggregating zoom-in transforms consistently yields improved accuracy on IN-A, ON but worse accuracy on IN-R and IN-S. zoom-224 refers to the set of zoom transforms at S = 224. See Tab. A6 for more results.\n(a) (b) zoom-in \u00df (c) zoom-out \u00de (d) zoom-224 (e) 5-crop (f) 10-crop [38]\nDataset 1-crop max mean max mean max mean max mean max mean IN 75.75 74.35 (-1.40) 79.31 (+3.56) 71.48 69.47 72.66 73.67 77.33 77.73 77.30 77.87 ReaL 81.89 80.22 (-1.67) 85.97 (+4.08) 77.95 76.28 79.25 80.31 83.24 83.80 83.17 83.87 IN-A 9.64 24.69 (+15.05) 14.29 (+4.65) 7.79 5.48 8.12 7.39 12.19 9.88 12.32 9.67 IN-R 41.29 39.90 (-1.39) 40.06 (-1.23) 39.05 36.21 39.52 39.28 43.90 43.17 44.31 43.28 IN-S 26.83 19.74 (-7.09) 20.89 (-5.94) 22.37 19.25 25.06 25.21 28.72 28.66 28.94 28.76 ON 30.89 32.88 (+1.99) 33.92 (+3.03) 22.56 19.51 22.75 22.72 26.96 24.98 27.14 24.97\nTable A6: Performance of various aggregating methods (%) \u2013 The bold numbers show maximum accuracy per model/dataset. CLIP strongly and consistently favors 10-crop over other settings.\n(a) (b) zoom-in \u00df (c) zoom-out \u00de (d) zoom-224 (e) 5-crop (f) 10-crop [38] Dataset 1-crop Max Mean Max Mean Max Mean Max Mean Max Mean\nR es\nN et\n-1 8 IN 69.45 68.45 (-1.00) 71.45 (+2.00) 60.33 56.79 67.85 68.70 70.61 71.32 70.83 71.85 ReaL 76.94 76.33 (-0.61) 79.94 (+3.00) 67.64 63.92 75.73 76.74 78.26 79.01 78.42 79.46 IN-A 1.37 11.68 (+10.31) 5.48 (+4.11) 2.44 2.19 3.41 2.69 3.16 2.13 3.28 1.87 IN-R 32.14 30.60 (-1.54) 28.95 (-3.19) 29.08 27.28 32.29 32.54 33.99 33.38 34.59 33.83 IN-S 19.41 14.86 (-4.55) 14.34 (-5.07) 14.48 11.49 17.80 17.83 20.83 20.70 21.39 21.06 ON 27.59 28.21 (+0.62) 25.92 (-1.67) 16.11 14.10 22.82 22.86 24.77 20.91 25.47 21.03\nR es\nN et\n-5 0 IN 75.75 73.24 (-2.51) 77.30 (+1.55) 69.06 66.42 74.45 75.39 76.67 77.13 76.89 77.43 ReaL 82.63 80.36 (-2.27) 84.68 (+2.05) 76.35 73.85 81.96 82.85 83.67 84.06 83.82 84.31 IN-A 0.21 16.11 (+15.9) 6.23 (+6.02) 2.79 2.19 3.04 2.11 2.28 0.95 2.43 1.00 IN-R 35.39 33.58 (-1.81) 32.73 (-2.66) 35.85 33.22 36.64 36.44 37.47 36.50 38.23 36.86 IN-S 22.91 16.89 (-6.02) 17.80 (-5.11) 19.51 17.12 21.60 21.66 24.71 24.51 24.94 24.74 ON 36.18 34.56 (-1.62) 34.22 (-1.96) 27.10 25.32 31.78 31.98 33.34 29.58 33.93 29.86\nV iT\n-B /3\n2 IN 75.75 74.35 (-1.40) 79.31 (+3.56) 71.48 69.47 72.66 73.67 77.33 77.73 77.30 77.87 ReaL 81.89 80.22 (-1.67) 85.97 (+4.08) 77.95 76.28 79.25 80.31 83.24 83.80 83.17 83.87 IN-A 9.64 24.69 (+15.05) 14.29 (+4.65) 7.79 5.48 8.12 7.39 12.19 9.88 12.32 9.67 IN-R 41.29 39.90 (-1.39) 40.06 (-1.23) 39.05 36.21 39.52 39.28 43.90 43.17 44.31 43.28 IN-S 26.83 19.74 (-7.09) 20.89 (-5.94) 22.37 19.25 25.06 25.21 28.72 28.66 28.94 28.76 ON 30.89 32.88 (+1.99) 33.92 (+3.03) 22.56 19.51 22.75 22.72 26.96 24.98 27.14 24.97\nV G\nG -1\n6 IN 71.37 69.60 (-1.77) 72.46 (+1.09) 64.75 59.95 69.51 70.48 72.31 73.09 72.67 73.53 ReaL 78.90 77.23 (-1.67) 80.59 (+1.69) 72.55 67.68 77.48 78.58 79.80 80.42 80.13 80.80 IN-A 2.69 11.55 (+8.86) 6.24 (+3.55) 3.33 2.77 4.69 3.87 4.87 3.19 5.09 3.19 IN-R 26.98 26.18 (-0.80) 24.74 (-2.24) 28.01 25.62 27.76 27.78 28.75 27.95 29.23 28.35 IN-S 16.78 13.30 (-3.48) 13.05 (-3.73) 15.18 13.37 15.82 15.97 17.80 17.63 18.28 17.92 ON 28.32 26.96 (-1.36) 26.15 (-2.17) 19.88 16.42 23.47 23.60 26.21 21.65 26.52 21.80\nA le\nxN et IN 56.16 54.74 (-1.42) 56.98 (+0.82) 40.78 27.09 51.80 51.50 57.86 58.60 58.26 59.11 ReaL 62.67 61.46 (-1.21) 64.35 (+1.68) 45.84 30.58 58.25 58.16 64.53 65.39 64.98 65.94 IN-A 1.75 4.65 (+2.90) 3.27 (+1.52) 1.56 1.23 2.31 1.97 2.53 2.04 2.64 2.03 IN-R 21.10 20.65 (-0.45) 17.97 (-3.13) 15.72 11.25 19.91 19.55 22.79 21.86 23.26 22.16 IN-S 10.05 7.94 (-2.11) 6.54 (-3.51) 5.82 2.72 8.29 7.39 10.84 10.65 11.20 10.80 ON 14.23 14.91 (+0.68) 11.80 (-2.43) 6.11 3.75 9.65 9.01 12.63 9.57 12.84 9.58\nC L\nIP -V\niT -L\n/1 4 IN 75.03 70.01 (-5.02) 74.45 (-0.58) 72.01 72.21 74.45 76.04 76.77 76.91 76.72 77.00\nReaL 80.68 76.37 (-4.31) 81.31 (+0.63) 78.28 78.93 81.45 82.05 82.26 82.55 82.26 82.55 IN-A 71.28 76.57 (+5.29) 68.16 (-3.12) 60.71 49.51 71.69 70.04 77.80 76.61 78.25 76.83 IN-R 87.74 84.12 (-3.62) 83.54 (-4.20) 86.84 86.29 88.12 88.24 89.64 89.66 90.01 89.94 IN-S 58.23 51.88 (-6.35) 56.06 (-2.17) 57.14 57.43 59.00 59.90 61.28 61.61 61.59 62.07 ON 66.32 60.20 (-6.12) 58.10 (-8.22) 56.57 58.11 62.44 62.65 66.70 64.88 66.87 64.97"
        },
        {
            "heading": "B.8 Runtime analysis of MEMO",
            "text": "Another benefit of RRC compared to AugMix is faster inference time. Table A7 shows the runtime analysis of MEMO. Typically, TTA methods suffer from slow runtime due to augmentation and test-time training processes. We find that MEMO + RRC consistently leads to an average 1.6\u00d7 speed-up compared to MEMO + AugMix (Tab. A7; 0.65s / image vs. 1.15s / image), providing more evidence to support this transformation as a viable option for test-time augmentations.\nB.9 1-crop accuracy with different zoom scales\nIn this section, we demonstrate the performance of various models when zooming in or out of an image. In other words, we utilize the standard 1-crop ImageNet transform while altering the initial scale of the image.\nTable A7: Average runtime per query image (in seconds). Using RandomResizedCrop in MEMO speed ups the runtime by an average factor of 1.6\u00d7.\nRuntime (in seconds) IN IN-A IN-R IN-S ON MEMO + AugMix [83]\nResNet-50 [23] 1.24 1.12 1.12 1.32 1.51 DeepAug+AugMix [26] 1.19 1.07 1.12 1.23 1.55 MoEx+CutMix [40] 1.15 1.16 1.11 1.31 1.53 MEMO + RRC (Ours) ResNet-50 [23] 0.64 0.60 0.65 0.88 1.19 DeepAug+AugMix [26] 0.62 0.62 0.64 0.87 1.18 MoEx+CutMix [40] 0.65 0.62 0.66 0.88 1.19\nIn this section, we are conducting experiments using the following models: AlexNet [38], ConvNext (Base, Large, Small, Tiny) [44], DenseNet-161 [29], EfficientNet-B7 [69], MobileNet (V2, V3 Large) [60, 28], ResNet (50, 101) [23], ResNeXt-50 (32x4d) [80], ShuffleNet V2 x1.0 [47], VGG19 [63], Vision Transformer (ViT-B/16, ViT-B/32, ViT-L/16, ViT-L/32) [17], and Wide ResNet-502 [82].\nB.10 Distribution of the Top 36 performing transforms. In this section, we provide more details about the distribution of the top-36 performing transforms. Our results suggest that, on average, 26.65% of all top-36 performing transforms belong to the center at varying scales.\nB.11 Background occlusion in ImageNet dataset Sample images for images with and without occlusion.\n(a) A sample image of the Tank class without occlusion.\n(b) Image with heavy background occlusion.\n(c) A clean sample image of the Four Poster class.\n(d) A low-quality image with background occlusion.\nFigure A24: Background occlusion examples."
        },
        {
            "heading": "C Additional Experiments",
            "text": "In this section, we provide additional experiments with the proposed zoom-based transform.\nC.1 Zooming is similarly important to the foreground and background contents Background pixels, despite often being neglected in image classification, can contain predictive signals [86, 78, 20, 57]. It has remained largely unknown how much the image context (background) could contribute to the model performance. While Zhu et al. [86] disentangle the predictiveness of background (BG) and foreground (FG) via model training, we directly measure how pretrained models perceive these two signals.\nExperiment Using bounding-box annotations provided by Russakovsky et al. [59], we create two dataset variations of ImageNet: FGSet and BGSet, following Zhu et al. [86]. We mask all the background for FGSet as in Fig. A25b, and for BGSet we mask all the main objects, as depicted in Fig. A25d & Fig. A25f. After that, we compute the accuracy of these two sets with all tested classifiers using ImageNet and ImageNet-ReaL labels as in Tab. A8.\nResults Our results suggest that zooming is important to ImageNet regardless of whether foreground or background features are used, with the difference for FGSet and BGSet on average being similar (Tab. A8). Additionally, when only the background features were available, almost half of ImageNet images (45.23%) could be correctly classified if optimal Zoom was used. Finally, we found that with only foreground information, ViT-B/32 could achieve a maximum possible accuracy of 95.50% given an optimal zooming method, suggesting that only 98.75% \u2212 95.50% = 3.25% of images (Tab. 1) required the background information. These findings suggest that both foreground and background features are important for ImageNet classification, but that an optimal zooming method can considerably improve performance even in the absence of one of these feature sets.\nC.2 Adversarial datasets contain more objects compared to ImageNet So far, our findings indicate that if we apply the zoom-in operation to the two datasets of ImageNetA and ObjectNet, the performance of conventional vision models improves consistently up to a certain threshold (Sec. 4 and Appendix B.5). This suggests that the initial images contain distracting elements that impede the model from correctly identifying the object of interest. Both ImageNet-A and ObjectNet are considered out-of-distribution datasets, which are specifically designed to evaluate a vision model\u2019s ability to withstand natural adversarial and pose attacks. We hypothesize that the primary reason that these datasets are hard can be attributed to background clutter, multiple objects, and the presence of a positional bias in these images.\nExperiment We use OWL-ViT [49], an open vocabulary object detection model, to quantify the number of objects present in three datasets of ImageNet, ImageNet-A, and ObjecNet. The OWL-ViT expects an input image with a set of object names and will determine if any object instances are present in the image. To specify object names, we use LVIS vocabulary [22], which encompasses a\nTable A8: ImageNet classification from object-only and background-only signals. Numbers show the maximum possible top-1 accuracy (%) using zoom-based transforms for minimum set covers in Appendix B.4. We discover that background signals potentially hold significance for image classification. The bold numbers show the highest possible accuracy per dataset and group.\n1-crop Max possible using zooming\nFGSet BGSet FGSet BGSet\nIN ReaL IN ReaL IN ReaL IN ReaL\nResNet-18 59.77 64.97 4.91 7.84 89.89(+30.12) 92.04(+27.07) 25.81(+20.90) 31.33(+23.49) ResNet-50 68.02 72.90 6.18 9.83 93.45(+25.43) 94.89(+21.99) 30.30(+24.12) 35.98(+26.15) ViT-B/32 67.46 71.78 9.72 13.38 94.40(+26.94) 95.50(+23.72) 39.70(+29.98) 45.23(+31.85) VGG-16 63.78 69.09 5.36 8.59 91.01(+27.23) 92.91(+23.82) 26.98(+21.62) 32.62(+24.03) AlexNet 42.38 46.54 3.66 5.46 80.20(+37.82) 83.25(+36.71) 22.02(+18.36) 27.04(+21.58) CLIP-ViT-L/14 74.46 78.62 9.49 13.80 96.14(+21.68) 97.35(+18.73) 36.85(+27.36) 42.51(+28.71) mean 62.65 67.32 6.55 9.82 90.85 (+28.20) 92.66 (+25.34) 30.28 (+23.73) 35.79 (+25.97)\ncomprehensive list of 1203 distinct objects. The OWL-ViT model includes a threshold parameter that reflects its confidence level in its predictions. To assess whether different threshold values would affect our results, we conducted our experiment using both 0.1 and 0.05 as threshold values. After calculating the distribution of the number of objects in images, we perform a Mann-Whitney U test to determine whether there is a statistically significant difference in this distribution between datasets. As each dataset has a different number of classes, we limited our analysis to shared classes between any two datasets.\nResults The results of our study reveal a contrast between ImageNet and ImageNet-A, as well as ImageNet and ObjectNet. This finding implies a dissimilarity between the images in the original ImageNet dataset and its OOD datasets that might arise from the presence of background clutter. Specifically, on average, images in ImageNet-A and ObjectNet datasets tend to feature more objects, which can pose more significant distractions for image classification models. The results of the Mann-Whitney U test also reflect this finding, the p-value for both thresholds was found to be less than 0.05, which is statistically significant at the 95% confidence level (Tab. A9).\nC.2.1 p-values for Mann Whitney U test\nTable A9: The result of the Mann-Whitney U test to compare ImageNet with ImageNet-A and ObjectNet\nT = 0.05 T = 0.01\nImageNet-A 6.27E-265 1.71E-235 ObjectNet 1.80E-02 3.66E-02\nC.3 Zooming further improves robustified models on ImageNet-A Intensive data augmentations have been proven to significantly boost CNNs\u2019 performance [77, 66] on ImageNet. Motivated by these previous successes and the fact that neural networks trained on diverse augmentations are able to learn robust representations [41], we want to know if robustified pretrained models (i.e. trained with intensive augmentations) could reach higher accuracy on ImageNet-A using zooming in.\nExperiment We test 4 different ResNet-50 classifier versions that have been trained with different data augmentation procedures. From the the torchvision library, we select two sets of model weights; trained with (V23) and without (V14) data augmentations. We also take two other models trained with DeepAugmentation+AugMix [26] and MoEx+CutMix [40]. The second column in Tab. A10 represents the accuracy of models using 1-crop.\nResults Zooming in consistently helps ResNet-50 networks, with improvements varying from +13 to +24 points. The best-performing network is torchvision-V2 which uses the max aggregator and achieves 29.65%. These results suggest that simple aggregation over the proposed zoom transform is effective for datasets that have dominant center bias.\nTable A10: The results of different aggregation functions on four ResNet-50 variants when tested on ImageNet-A (%). Each model has been trained using different training-time augmentation techniques. Improvements values in parentheses are with respect to the 1-crop baseline.\nResNet-50 Baseline Max Mean\ntorchvision V1 0.21 16.11 (+15.90) 6.23 MoEx+CutMix [40] 8.60 24.72 (+16.12) 15.32 DeepAug+AugMix [26] 3.94 27.93 (+23.99) 13.16 torchvision V2 16.62 29.65 (+13.03) 22.08\n3ResNet50_Weights.IMAGENET1K_V2 4ResNet50_Weights.IMAGENET1K_V1\nD Visualization In this section, we provide several visualizations of zooming transforms.\nD.1 Visualizations for 36 top performing zoom transforms\nstethoscope 66.02 Chihuahua 57.99 golden_retriever 42.50 stethoscope 99.49 hand_blower 31.78 golden_retriever 77.68\nstethoscope 84.86 maraca 9.43 golden_retriever 25.41 golden_retriever 53.12 Chihuahua 83.43 stethoscope 39.02\nChihuahua 27.77 stethoscope 90.66 stethoscope 27.31 golden_retriever 63.55 hand_blower 46.10 golden_retriever 20.16\nstethoscope 92.62 Chihuahua 45.02 hand_blower 24.99 stethoscope 74.40 stethoscope 52.77 Chihuahua 96.98\nmaraca 10.31 golden_retriever 51.76 stethoscope 92.74 hand_blower 38.98 studio_couch 26.20 stethoscope 94.44\nenvelope 39.45 stethoscope 49.90 stethoscope 97.62 Chihuahua 38.00 stethoscope 94.04 stethoscope 64.81\nFigure A30: Different framing of an image of a stethoscope according to 36 high-performing transforms of a ResNet-50 model\nD.2 Overview of 324 transforms The visualizations below illustrate the transforms that result in the correct prediction of the query image, using ViT-B/32 [17] and CLIP-ViT-L/14 [53]. Each circle represents a transform, with the initial zoom scale indicated in the accompanying text. The green circles represent the transformations that lead to correct classification, while the red circles indicate incorrect ones.\nD.3 Only zoom-out solves Sample images that required zooming out to be classified correctly.\nD.3.1 ImageNet-Sketch\nmatchstick folding chair\nplate rack 56.2% matchstick\nStandard center crop matchstick\nZoom scale 96\n16.57% folding chair\n19.67% matchstick\n19.51% matchstick\n17.34% folding chair 14.69%\n27 .29% matchstick 18.38%\n22.72% matchstick 17.67%\nmicrophone microphone 91.54% microphone 91.98% microphone 92.48%\nC C . . 111c PholD--153730\nsax 43.73% microphone 96.65% microphone 97 .17% microphone 97.52%\nStandard center crop microphone 85.4 7% microphone 87 .62% microphone 88.71%\nZoom scale 96\nwater Tower water tower 18.65% water tower 44.72% water tower 14.72%\n\"\"\" JJJl11 ) I\nguillotine 18.88% coffeepot 7.8% water tower 12.96% coffeepot 11.47%\nStandard center crop folding chair 18.63% crutch 13.23% crutch 13.07%\nZoom scale 122\nice lolly ice lolly 71.77% ice lolly 72.11 % ice lolly 73.97%\nshovel 18.61% ice lolly 89.15% ice lolly 90.24% ice lolly 88.92%\nStandard center crop ice lolly 87 .12% ice lolly 88.39% ice lolly 84.71%\nZoom scale 96\nFigure A40: ImageNet-Sketch images that can only be solved using zoom-out. Predictions are from a ResNet-50 classifier.\nD.3.2 ImageNet-R"
        },
        {
            "heading": "D.3.3 ObjectNet",
            "text": "D.4 Only zoom-in solves Sample images that required zooming in to be classified correctly."
        },
        {
            "heading": "D.4.1 ObjectNet",
            "text": "D.5 AugMix and RandomResizedCrop\nE ImageNet-Hard In this section, we provide details about the ImageNet-hard dataset.\nE.1 Distribution\nE.2 Samples images"
        },
        {
            "heading": "E.3 Analysis of wrong predictions",
            "text": "We used gpt-3.5-turbo to categorize each misprediction made by EfficientNet-L2 into two classes: plausible and implausible, based on the semantic distance between the groundtruth label and the predicted label.\nSystem Message:\nUser:\nAssistant:\nYou are a helpful assistant tasked with evaluating an image classifier by reviewing its predictions.\nI am looking at a picture of a `curly coated retriever`. The model predicted it is a `flat coated retriever`. Can you categorize this prediction as plausible mistake or implausible mistake? provide a one line description.\nThis prediction can be categorized as a plausible mistake, as curly coated and flat coated retrievers can look similar to an untrained eye.\nFigure A50: Sample prompt and response of gpt-3.5-turbo for a plausible classification. The text in the Assistant block is the generated response."
        },
        {
            "heading": "E.4 Confusing classes",
            "text": "In this section, we present a selection of examples highlighting the errors made by our highestperforming model, EfficientNet-L2."
        },
        {
            "heading": "E.5 Common and rare misclassification",
            "text": "This section shows some sample misclassification for EfficientNet-L2 and OpenCLIP\u2019s ViT-bigG-14 classifiers."
        },
        {
            "heading": "E.6 Evaluating OpenCLIP models\u2019 performance on ImageNet-Hard",
            "text": "All the models in this section are downloaded and used from the OpenCLIP library version 2.20.0.\nconvnext_base laion400m_s13b_b51k 4.74 convnext_base_w laion2b_s13b_b82k 6.09 convnext_base_w laion2b_s13b_b82k_augreg 7.25 convnext_base_w laion_aesthetic_s13b_b82k 5.57 convnext_base_w_320 laion_aesthetic_s13b_b82k 5.50 convnext_base_w_320 laion_aesthetic_s13b_b82k_augreg 7.14 convnext_large_d laion2b_s26b_b102k_augreg 10.39 convnext_large_d_320 laion2b_s29b_b131k_ft 10.69 convnext_large_d_320 laion2b_s29b_b131k_ft_soup 11.20 convnext_xxlarge laion2b_s34b_b82k_augreg 14.27 convnext_xxlarge laion2b_s34b_b82k_augreg_rewind 14.23 convnext_xxlarge laion2b_s34b_b82k_augreg_soup 14.68 coca_ViT-B/32 laion2b_s13b_b90k 5.83 coca_ViT-B/32 mscoco_finetuned_laion2b_s13b_b90k 0.20 coca_ViT-L/14 laion2b_s13b_b90k 10.79 coca_ViT-L/14 mscoco_finetuned_laion2b_s13b_b90k 9.28"
        },
        {
            "heading": "E.7 Evaluating classifiers on ImageNet-Hard-4K",
            "text": "E.8 Obviously ill-posed samples from ImageNet-Sketch\nn01484850/7064.JPEG\nn01514859/7079.JPEG n01537544/7125.JPEG n01537544/7149.JPEG n01560419/7174.JPEG\nn01560419/7184.JPEG n01664065/7332.JPEG\nn01667114/7353.JPEG\nn01693334/7527.JPEG n01693334/7544.JPEG\nn01697457/7625.JPEG n01697457/7628.JPEG\nn01734418/7794.JPEG n01734418/7801.JPEG n01734418/7809.JPEG\nn01734418/7810.JPEG\nn01742172/7862.JPEG n01768244/8064.JPEG n01773549/8102.JPEG n01774750/8162.JPEG\nn01796340/8179.JPEG n01796340/8183.JPEG n01797886/8202.JPEG\nn01798484/8231.JPEG n01798484/8243.JPEG\nFigure A60: Sample images from ImageNet-Sketch that are completely black."
        },
        {
            "heading": "F Additional Details",
            "text": "F.1 Additional results for performance of classifiers using maximum possible accuracy In this section, we present the delta values for Tab. 1, which represent the difference in relation to the 1-crop accuracy for each cell.\nTable A14: On in-distribution data (IN & ReaL) there exists a substantial improvement when models are provided with an optimal zoom, either selected from 36 (b) or 324 pre-defined zoom crops (c). In\ncontrast, OOD benchmarks still pose a significant challenge to IN-trained models even with optimal zooming (i.e., all upper-bound accuracy scores < 80%). Improvements are respected to the standard 1-crop accuracy.\nIN ReaL IN+ReaL IN-A IN-R IN-S ON (a) Standard top-1 accuracy based on N = 1 crop AlexNet 56.16 62.67 61.76 1.75 21.10 10.05 14.23 VGG-16 71.37 78.90 78.52 2.69 26.98 16.78 28.32 ResNet-18 69.45 76.94 76.47 1.37 32.14 19.41 27.59 ResNet-50 75.75 82.63 82.97 0.21 35.39 22.91 36.18 ViT-B/32 75.75 81.89 82.59 9.64 41.29 26.83 30.89 CLIP-ViT-L/14 75.03 80.68 81.95 71.28 87.74 58.23 66.32 (b) Upper-bound accuracy using N = 36 crops AlexNet 85.19 (+29.03) 90.30 (+27.63) 89.74 (+27.98) 31.37 (+29.62) 47.04 (+25.94) 24.40 (+14.35) 49.17 (+34.94) VGG-16 92.30 (+20.93) 96.08 (+17.18) 95.81 (+17.29) 46.69 (+44.00) 52.86 (+25.88) 34.34 (+17.56) 62.94 (+34.62) ResNet-18 92.08 (+22.63) 95.97 (+19.03) 95.73 (+19.26) 47.48 (+46.11) 58.85 (+26.71) 37.91 (+18.50) 63.08 (+35.49) ResNet-50 94.46 (+18.71) 97.36 (+14.73) 97.40 (+14.43) 55.68 (+55.47) 61.42 (+26.03) 41.71 (+18.80) 69.60 (+33.42) ViT-B/32 95.05 (+19.30) 97.61 (+15.72) 97.88 (+15.29) 68.43 (+58.79) 68.77 (+27.48) 49.10 (+22.27) 70.30 (+39.41) CLIP-ViT-L/14 94.19 (+19.16) 97.32 (+16.64) 97.56 (+15.61) 97.16 (+25.88) 98.60 (+10.86) 83.77 (+25.54) 89.59 (+23.27) (c) Upper-bound accuracy using N = 324 crops AlexNet 90.03 (+33.87) 93.85 (+31.18) 93.48 (+31.72) 42.23 (+40.48) 55.52 (+34.42) 29.53 (+19.48) 59.65 (+45.42) VGG-16 95.30 (+23.93) 97.90 (+19.00) 97.66 (+19.14) 58.27 (+55.58) 60.88 (+33.90) 39.90 (+23.12) 71.85 (+43.53) ResNet-18 95.15 (+25.70) 97.76 (+20.82) 97.55 (+21.08) 58.87 (+57.50) 66.89 (+34.75) 43.68 (+24.27) 71.44 (+43.85) ResNet-50 96.78 (+21.03) 98.62 (+15.99) 98.57 (+15.60) 66.68 (+66.47) 68.84 (+33.45) 47.64 (+24.73) 76.83 (+40.65) ViT-B/32 97.19 (+21.44) 98.75 (+16.86) 98.91 (+16.32) 78.03 (+68.39) 75.58 (+34.29) 55.99 (+29.16) 79.28 (+48.39) CLIP-ViT-L/14 96.78 (+21.75) 98.69 (+18.01) 98.80 (+16.85) 98.45 (+27.17) 99.20 (+11.46) 89.00 (+30.77) 93.13 (+26.81)\nF.2 Comparing modern architectures with regard to their maximum possible accuracy. In this section, we repeated the experiment presented in Tab. 1 in order to include a broader range of architectures and conduct a rank analysis. Specifically, we added MaxViT [72], Swin Transformer [43], ConvNext [44], EfficientNet [69], and MobileNetV3 [28]."
        },
        {
            "heading": "F.3 Error-analysis for MEMO results",
            "text": "In this section, we repeated the MEMO experiment using various random seeds to evaluate the consistency of the results. Our findings demonstrate a consistent trend between MEMO and RRC, indicating that this relationship holds steady regardless of variations in individual runs.\nF.4 Grad-CAM visualizations for MEMO + RRC"
        },
        {
            "heading": "G Datasheet for ImageNet-Hard",
            "text": ""
        },
        {
            "heading": "G.1 Motivation",
            "text": "The questions in this section are primarily intended to encourage dataset creators to clearly articulate their reasons for creating the dataset and to promote transparency about funding interests. The latter may be particularly relevant for datasets created for research purposes.\n\u2022 For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. The ImageNet-Hard is a new benchmark to test the robustness of state-of-the-art image classifiers. It comprises an array of challenging images collected from six validation datasets of ImageNet. This dataset challenges state-of-the-art image classification models because even by perfectly localizing the key objects, the state-of-the-art classifiers still fail to correctly recognize.\n\u2022 Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? The dataset was created in collaboration efforts between the University of Alberta, Canada, and Auburn University, USA; mostly by, Mohammad Reza Taesiri and Anh Nguyen.\n\u2022 Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number. Anh Nguyen was supported by NSF Grant No. 2145767, and donations from NaphCare Foundation, and Adobe Research\n\u2022 Any other comments? No."
        },
        {
            "heading": "G.2 Composition",
            "text": "Dataset creators should read through these questions prior to any data collection and then provide answers once data collection is complete. Most of the questions in this section are intended to provide dataset consumers with the information they need to make informed decisions about using the dataset for their chosen tasks. Some of the questions are designed to elicit information about compliance with the EU\u2019s General Data Protection Regulation (GDPR) or comparable regulations in other jurisdictions. Questions that apply only to datasets that relate to people are grouped together at the end of the section. We recommend taking a broad interpretation of whether a dataset relates to people. For example, any dataset containing text that was written by people relates to people.\n\u2022 What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description. Each instance of the ImageNet-Hard dataset corresponds to an image and at least one groundtruth label that will be used to assess image classifiers.\n\u2022 How many instances are there in total (of each type, if appropriate)? There are 10,980 images in this dataset.\n\u2022 Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable). ImageNet-Hard is a combination of various publicly available datasets. We tried multiple refinement steps to make sure to get the best possible samples for the intended purpose. Then, it is not representative of any larger sets but a selective combination of multiple sets.\n\u2022 What data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images) or features? In either case, please provide a description. The dataset contains both raw and processed images. The processed images come from ImageNet-C. Details can be found in Sec. 4.4.\n\u2022 Is there a label or target associated with each instance? If so, please provide a description. Yes. Each sample has the label that is the folder name the image belongs to. Basically, we follow the structure of the ImageNet paper [59].\n\u2022 Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text. No.\n\u2022 Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social network links)? If so, please describe how these relationships are made explicit. No. The individual instances has no relationships.\n\u2022 Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them. No. This dataset is created for the testing purposes.\n\u2022 Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description. To the best of our knowledge, No. We tried our best efforts to filter any errors, sources of noise, or redundancies to create the ImageNet-Hard dataset.\n\u2022 Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a dataset consumer? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate. Yes. It does link and inherits from existing image datasets and was detailed in Sec. 4.4.\n\u2022 Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals\u2019 non-public communications)? If so, please provide a description. No.\n\u2022 Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why. No.\nIf the dataset does not relate to people, you may skip the remaining questions in this section.\n\u2022 Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. N/A.\n\u2022 Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how. N/A.\n\u2022 Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. N/A.\n\u2022 Any other comments? No."
        },
        {
            "heading": "G.3 Collection Process",
            "text": "As with the questions in the previous section, dataset creators should read through these questions prior to any data collection to flag potential issues and then provide answers once collection is complete. In addition to the goals outlined in the previous section, the questions in this section are designed to elicit information that may help researchers and practitioners to create alternative datasets with similar characteristics. Again, questions that apply only to datasets that relate to people are grouped together at the end of the section.\n\u2022 How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? If the data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. The dataset is linked from other 6 datasets. Please find the contribution of original daatasets in Appendix E.1)\n\u2022 What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or sensors, manual human curation, software programs, software APIs)? How were these mechanisms or procedures validated? We used both algorithm and human efforts to collect the data. Algorithms were used to choose hard samples from various datasets. We then used two human groups and their agreement to make sure the high quality of the process. Details for the human validation in Sec. 4.4. Finally, we removed samples that have debatable labels (e.g. sunglass vs. sunglasses).\n\u2022 If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? No, the dataset was not a subset of a larger set.\n\u2022 Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? In the data collection process, we involved students who voluntarily participated.\n\u2022 Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created. Feedback data was collected from April 20 2023 \u2013 May 4 2023.\n\u2022 Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. N/A. In this study, humans are not the subjects. Their voluntary feedback, however, is used to filter out incorrectly labelled samples from the original 6 datasets."
        },
        {
            "heading": "If the dataset does not relate to people, you may skip the remaining questions in this section.",
            "text": "\u2022 Did you collect the data from the individuals in question directly, or obtain it via third\nparties or other sources (e.g., websites)? We only involved individuals in the label verification step (i.e. 3133 samples). The answers from individuals directly affect if one of those 3133 samples will be kept or not.\n\u2022 Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. N/A. In this study, humans are not the subjects. Their voluntary feedback, however, is used to filter out incorrectly labelled samples from the original 6 datasets.\n\u2022 Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. N/A. In this study, humans are not the subjects. Their voluntary feedback, however, is used to filter out incorrectly labelled samples from the original 6 datasets.\n\u2022 If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). N/A. In this study, humans are not the subjects. Their voluntary feedback, however, is used to filter out incorrectly labelled samples from the original 6 datasets.\n\u2022 Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. N/A. In this study, humans are not the subjects. Their voluntary feedback, however, is used to filter out incorrectly labelled samples from the original 6 datasets.\n\u2022 Any other comments? No."
        },
        {
            "heading": "G.4 Preprocessing/cleaning/labeling",
            "text": "Dataset creators should read through these questions prior to any preprocessing, cleaning, or labeling and then provide answers once these tasks are complete. The questions in this section are intended to provide dataset consumers with the information they need to determine whether the \u201craw\u201d data has been processed in ways that are compatible with their chosen tasks. For example, text that has been converted into a \u201cbag-of-words\u201d is not suitable for tasks involving word order.\n\u2022 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remaining questions in this section. Yes, we did cleaning up the data. We removed 370 images associated with the labels sunglass, sunglasses, tub, bathtub, cradle, bassinet, projectile, and missile, i.e., the classes that often contain similar images that belong to more than one class. Also, the agreement human setup in Sec. 4.4 helps us remove bad samples.\n\u2022 Was the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d data. No, we did not save. However, the \u201craw\u201d data (i.e. six image datasets) can be found on the Internet.\n\u2022 Is the software that was used to preprocess/clean/label the data available? If so, please provide a link or other access point. Yes, we provided the source code for the algorithms that can be found at here.\n\u2022 Any other comments? No."
        },
        {
            "heading": "G.5 Uses",
            "text": "The questions in this section are intended to encourage dataset creators to reflect on the tasks for which the dataset should and should not be used. By explicitly highlighting these tasks, dataset creators can help dataset consumers to make informed decisions, thereby avoiding potential risks or harms.\n\u2022 Has the dataset been used for any tasks already? If so, please provide a description. The dataset was used for image classification in this paper.\n\u2022 Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. The papers that use the dataset could be found at this paperwithcode repository.\n\u2022 What (other) tasks could the dataset be used for? Studying the effects of upscaling on image classification.\n\u2022 Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a dataset consumer might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other risks or harms (e.g., legal risks, financial harms)? If so, please provide a description. Is there anything a dataset consumer could do to mitigate these risks or harms? No.\n\u2022 Are there tasks for which the dataset should not be used? If so, please provide a description. No.\n\u2022 Any other comments? No."
        },
        {
            "heading": "G.6 Distribution",
            "text": "Dataset creators should provide answers to these questions prior to distributing the dataset either internally within the entity on behalf of which the dataset was created or externally to third parties.\n\u2022 Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. The dataset will be shared with the common public.\n\u2022 How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)? It has a GitHub repository and Hugging Face page. It has no DOI.\n\u2022 When will the dataset be distributed? Until June 5 2023, the dataset had been already distributed.\n\u2022 Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions. No.\n\u2022 Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions. There are no such restrictions.\n\u2022 Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation. N/A.\n\u2022 Any other comments? No."
        },
        {
            "heading": "G.7 Maintenance",
            "text": "As with the questions in the previous section, dataset creators should provide answers to these questions prior to distributing the dataset. The questions in this section are intended to encourage dataset creators to plan for dataset maintenance and communicate this plan to dataset consumers.\n\u2022 Who will be supporting/hosting/maintaining the dataset? The authors of the paper will be supporting/hosting/maintaining the dataset.\n\u2022 How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Please reach out to mtaesiri@gmail.com and anh.ng8@gmail.com.\n\u2022 Is there an erratum? If so, please provide a link or other access point. No, there is no erratum.\n\u2022 Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to dataset consumers (e.g., mailing list, GitHub)? Yes, we anticipate either correcting the labels or removing some images entirely to reduce the noise in the dataset. We manage the versioning through the Hugging Face Dataset platform.\n\u2022 If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. N/A\n\u2022 Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to dataset consumers. No, the older versions will not be updated. Users are encouraged to use the latest version.\n\u2022 If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so, please provide a description. Please reach out to mtaesiri@gmail.com or anhng8@gmail.com.\n\u2022 Any other comments? No."
        }
    ],
    "title": "ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification",
    "year": 2023
}