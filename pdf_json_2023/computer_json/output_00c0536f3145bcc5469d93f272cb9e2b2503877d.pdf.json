{
    "abstractText": "With the continuous development of the communication industry, there is a shift in real-time services from 4G networks to Delay Tolerable (DT) services in the context of 5G/B5G networks. Additionally, energy consumption control poses significant challenges in the current communication industry. Therefore, we study algorithms and schemes to improve the Energy Efficiency (EE) of DT services in the context of Non-Orthogonal Multiple Access (NOMA) downlink two-user communication system.First, we transformed the EE enhancement problem into a convex optimization problem based on transmission power by derivation. Secondly, we propose to use Approximate Statistical Dynamic Programming (ASDP) algorithm, Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) to solve the problem that convex optimization cannot be decided in real time. Finally, we perform an interpretability analysis on whether the decision schemes of the agents trained by the DDPG algorithm and the PPO algorithm are reasonable. The simulation results show that the decisions made by the agent trained by the DDPG algorithm perform better compared to the ASDP algorithm and the PPO algorithm. INDEX TERMS Energy Efficiency, Delay Tolerable, Approximate Statistical Dynamic Programming algorithm, Deep Deterministic Policy Gradient, Proximal Policy Optimization.",
    "authors": [
        {
            "affiliations": [],
            "name": "MENGMENG BAI"
        },
        {
            "affiliations": [],
            "name": "RUI ZHU"
        },
        {
            "affiliations": [],
            "name": "JIANXIN GUO"
        },
        {
            "affiliations": [],
            "name": "FENG WANG"
        },
        {
            "affiliations": [],
            "name": "LIPING WANG"
        },
        {
            "affiliations": [],
            "name": "HANGJIE ZHU"
        },
        {
            "affiliations": [],
            "name": "LEI HUANG"
        },
        {
            "affiliations": [],
            "name": "YUSHUAI ZHANG"
        }
    ],
    "id": "SP:0e25e85d90acb2548bc442f57da85db056f3d40a",
    "references": [
        {
            "authors": [
                "J. Wu",
                "Y. Zhang",
                "M. Zukerman"
            ],
            "title": "and E",
            "venue": "K.-N. Yung, \u2018\u2018Energy-efficient base-stations sleep-mode techniques in green cellular networks: A survey,\u2019\u2019 IEEE communications surveys & tutorials, vol. 17, no. 2, pp. 803\u2013826",
            "year": 2015
        },
        {
            "authors": [
                "S. Buzzi",
                "I. Chih-Lin",
                "T.E. Klein",
                "H.V. Poor",
                "C. Yang"
            ],
            "title": "and A",
            "venue": "Zappone, \u2018\u2018A survey of energy-efficient techniques for 5g networks and challenges ahead,\u2019\u2019 IEEE Journal on Selected Areas in Communications, vol. 34, no. 4, pp. 697\u2013709",
            "year": 2016
        },
        {
            "authors": [
                "O. Arnold",
                "F. Richter",
                "G. Fettweis",
                "O. Blume"
            ],
            "title": "Power consumption modeling of different base station types in heterogeneous cellular networks,\u2019",
            "venue": "Future Network & Mobile Summit. IEEE, Conference Proceedings,",
            "year": 2010
        },
        {
            "authors": [
                "J. Wu",
                "S. Zhou"
            ],
            "title": "and Z",
            "venue": "Niu, \u2018\u2018Traffic-aware base station sleeping control and powermatching for energy-delay tradeoffs in green cellular networks,\u2019\u2019 IEEE Transactions on Wireless Communications, vol. 12, no. 8, pp. 4196\u2013 4209",
            "year": 2013
        },
        {
            "authors": [
                "R. Li",
                "Z. Zhao",
                "X. Chen",
                "J. Palicot"
            ],
            "title": "and H",
            "venue": "Zhang, \u2018\u2018Tact: A transfer actor-critic learning framework for energy saving in cellular radio access networks,\u2019\u2019 IEEE transactions on wireless communications, vol. 13, no. 4, pp. 2000\u20132011",
            "year": 2014
        },
        {
            "authors": [
                "F.E. Salem",
                "Z. Altman",
                "A. Gati",
                "T. Chahed",
                "E. Altman"
            ],
            "title": "Reinforcement learning approach for advanced sleep modes management in 5g networks,\u2019",
            "venue": "IEEE 88th Vehicular Technology Conference (VTC- Fall). IEEE, Conference Proceedings,",
            "year": 2018
        },
        {
            "authors": [
                "J. Liu",
                "B. Krishnamachari",
                "S. Zhou"
            ],
            "title": "and Z",
            "venue": "Niu, \u2018\u2018Deepnap: Data-driven base station sleeping operations through deep reinforcement learning,\u2019\u2019 IEEE Internet of Things Journal, vol. 5, no. 6, pp. 4273\u20134282",
            "year": 2018
        },
        {
            "authors": [
                "M. Kalil",
                "A. Al-Dweik",
                "M.F.A. Sharkh",
                "A. Shami"
            ],
            "title": "and A",
            "venue": "Refaey, \u2018\u2018A framework for joint wireless network virtualization and cloud radio access networks for next generation wireless networks,\u2019\u2019 IEEE Access, vol. 5, pp. 20 814\u201320 827",
            "year": 2017
        },
        {
            "authors": [
                "I. Ullah",
                "S. Ahmad",
                "F. Mehmood"
            ],
            "title": "and D",
            "venue": "Kim, \u2018\u2018Cloud based iot network virtualization for supporting dynamic connectivity among connected devices,\u2019\u2019 Electronics, vol. 8, no. 7, p. 742",
            "year": 2019
        },
        {
            "authors": [
                "D.M. Casas-Velasco",
                "O.M.C. Rendon"
            ],
            "title": "and N",
            "venue": "L. da Fonseca, \u2018\u2018Intelligent routing based on reinforcement learning for software-defined networking,\u2019\u2019 IEEE Transactions on Network and ServiceManagement, vol. 18, no. 1, pp. 870\u2013881",
            "year": 2020
        },
        {
            "authors": [
                "G. Kaur",
                "P. Chanak"
            ],
            "title": "and M",
            "venue": "Bhattacharya, \u2018\u2018Energy-efficient intelligent routing scheme for iot-enabled wsns,\u2019\u2019 IEEE Internet of Things Journal, vol. 8, no. 14, pp. 11 440\u201311 449",
            "year": 2021
        },
        {
            "authors": [
                "S. Zeadally",
                "F.K. Shaikh",
                "A. Talpur"
            ],
            "title": "andQ",
            "venue": "Z. Sheng, \u2018\u2018Design architectures for energy harvesting in the internet of things,\u2019\u2019 Renewable and Sustainable Energy Reviews, vol. 128, p. 109901",
            "year": 2020
        },
        {
            "authors": [
                "H. Yuan",
                "J. Bi",
                "M. Zhou",
                "Q. Liu"
            ],
            "title": "and A",
            "venue": "C. Ammari, \u2018\u2018Biobjective task scheduling for distributed green data centers,\u2019\u2019 IEEE Transactions on Automation Science and Engineering, vol. 18, no. 2, pp. 731\u2013742",
            "year": 2020
        },
        {
            "authors": [
                "A. Patil",
                "S. Iyer",
                "O.L. Lopez",
                "R.J. Pandya",
                "A. Kalla",
                "R. Kallimani"
            ],
            "title": "\u2018A comprehensive survey on spectrum sharing techniques for 5g/b5g intelligent wireless networks: Opportunities",
            "venue": "challenges and future research directions,\u2019\u2019 arXiv preprint arXiv:2211.08956",
            "year": 2022
        },
        {
            "authors": [
                "R. Zhu",
                "G. Jianxin",
                "W. Feng",
                "B. Lin"
            ],
            "title": "and Y",
            "venue": "Huang, \u2018\u2018Energy efficient transmission power control policy of the delay tolerable communication service,\u2019\u2019 IEEE Access, vol. 8, pp. 175 815\u2013175 826",
            "year": 2020
        },
        {
            "authors": [
                "Z. Yang",
                "Z. Ding",
                "P. Fan"
            ],
            "title": "and G",
            "venue": "K. Karagiannidis, \u2018\u2018On the performance of non-orthogonal multiple access systems with partial channel information,\u2019\u2019 IEEE Transactions on Communications, vol. 64, no. 2, pp. 654\u2013667",
            "year": 2015
        },
        {
            "authors": [
                "C. He",
                "Y. Hu",
                "Y. Chen"
            ],
            "title": "and B",
            "venue": "Zeng, \u2018\u2018Joint power allocation and channel assignment for noma with deep reinforcement learning,\u2019\u2019 IEEE Journal on Selected Areas in Communications, vol. 37, no. 10, pp. 2200\u20132210",
            "year": 2019
        },
        {
            "authors": [
                "H.M. Al-Obiedollah",
                "K. Cumanan",
                "J. Thiyagalingam",
                "A.G. Burr",
                "Z. Ding"
            ],
            "title": "and O",
            "venue": "A. Dobre, \u2018\u2018Energy efficient beamforming design for miso nonorthogonal multiple access systems,\u2019\u2019 IEEE Transactions on Communications, vol. 67, no. 6, pp. 4117\u20134131",
            "year": 2019
        },
        {
            "authors": [
                "H. Al-Obiedollah",
                "H.B. Salameh",
                "S. Abdel-Razeq",
                "A. Hayajneh",
                "K. Cumanan"
            ],
            "title": "and Y",
            "venue": "Jararweh, \u2018\u2018Energy-efficient opportunistic multicarrier noma-based resource allocation for beyond 5g (b5g) networks,\u2019\u2019 Simulation Modelling Practice and Theory, vol. 116, p. 102452",
            "year": 2022
        },
        {
            "authors": [
                "Z. Chen",
                "Z. Ding",
                "P. Xu"
            ],
            "title": "and X",
            "venue": "Dai, \u2018\u2018Optimal precoding for a qos optimization problem in two-user miso-noma downlink,\u2019\u2019 IEEE Communications Letters, vol. 20, no. 6, pp. 1263\u20131266",
            "year": 2016
        },
        {
            "authors": [
                "J. Zhu",
                "J. Wang",
                "Y. Huang",
                "S. He",
                "X. You"
            ],
            "title": "and L",
            "venue": "Yang, \u2018\u2018On optimal power allocation for downlink non-orthogonal multiple access systems,\u2019\u2019 IEEE Journal on Selected Areas in Communications, vol. 35, no. 12, pp. 2744\u2013 2757",
            "year": 2017
        },
        {
            "authors": [
                "S. Boyd",
                "S.P. Boyd",
                "L. Vandenberghe"
            ],
            "title": "Convex optimization",
            "venue": "Cambridge university press",
            "year": 2004
        },
        {
            "authors": [
                "S. Ghadimi",
                "G. Lan"
            ],
            "title": "and H",
            "venue": "Zhang, \u2018\u2018Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization,\u2019\u2019Mathematical Programming, vol. 155, no. 1-2, pp. 267\u2013305",
            "year": 2016
        },
        {
            "authors": [
                "T.M. Moerland",
                "J. Broekens",
                "A. Plaat"
            ],
            "title": "and C",
            "venue": "M. Jonker, \u2018\u2018Model-based reinforcement learning: A survey,\u2019\u2019 Foundations and Trends\u00ae in Machine Learning, vol. 16, no. 1, pp. 1\u2013118",
            "year": 2023
        },
        {
            "authors": [
                "A.B. Adam",
                "L. Lei",
                "S. Chatzinotas"
            ],
            "title": "and N",
            "venue": "U. R. Junejo, \u2018\u2018Deep convolutional self-attention network for energy-efficient power control in noma networks,\u2019\u2019 IEEE Transactions on Vehicular Technology, vol. 71, no. 5, pp. 5540\u20135545",
            "year": 2022
        },
        {
            "authors": [
                "S. Jo",
                "C. Jong",
                "C. Pak"
            ],
            "title": "and H",
            "venue": "Ri, \u2018\u2018Multi-agent deep reinforcement learning-based energy efficient power allocation in downlink mimo-noma systems,\u2019\u2019 IET Communications, vol. 15, no. 12, pp. 1642\u20131654",
            "year": 2021
        },
        {
            "authors": [
                "R. Pal",
                "N. Gupta",
                "A. Prakash",
                "R. Tripathi"
            ],
            "title": "and J",
            "venue": "J. Rodrigues, \u2018\u2018Deep reinforcement learning based optimal channel selection for cognitive radio vehicular ad-hoc network,\u2019\u2019 IET Communications, vol. 14, no. 19, pp. 3464\u20133471",
            "year": 2020
        },
        {
            "authors": [
                "X. Wang",
                "Y. Zhang",
                "R. Shen"
            ],
            "title": "Y",
            "venue": "Xu, and F.-C. Zheng, \u2018\u2018Drl-based energyefficient resource allocation frameworks for uplink noma systems,\u2019\u2019 IEEE Internet of Things Journal, vol. 7, no. 8, pp. 7279\u20137294",
            "year": 2020
        },
        {
            "authors": [
                "A. Alajmi",
                "W. Ahsan"
            ],
            "title": "An efficient actor critic drl framework for resource allocation in multi-cell downlink noma,\u2019",
            "year": 2022
        },
        {
            "authors": [
                "A.D. Cobb",
                "B. Jalaian",
                "N.D. Bastian"
            ],
            "title": "and S",
            "venue": "Russell, \u2018\u2018Toward safe decision-making via uncertainty quantification in machine learning,\u2019\u2019 Systems Engineering and Artificial Intelligence, pp. 379\u2013399",
            "year": 2021
        },
        {
            "authors": [
                "B. Kompa",
                "J. Snoek"
            ],
            "title": "and A",
            "venue": "L. Beam, \u2018\u2018Second opinion needed: communicating uncertainty in medical machine learning,\u2019\u2019 NPJ Digital Medicine, vol. 4, no. 1, p. 4",
            "year": 2021
        },
        {
            "authors": [
                "A. Adadi",
                "M. Berrada"
            ],
            "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (xai),\u2019",
            "venue": "IEEE access,",
            "year": 2018
        },
        {
            "authors": [
                "A. Madsen",
                "S. Reddy"
            ],
            "title": "and S",
            "venue": "Chandar, \u2018\u2018Post-hoc interpretability for neural nlp: A survey,\u2019\u2019 ACM Computing Surveys, vol. 55, no. 8, pp. 1\u201342",
            "year": 2022
        },
        {
            "authors": [
                "D.V. Carvalho",
                "E.M. Pereira"
            ],
            "title": "and J",
            "venue": "S. Cardoso, \u2018\u2018Machine learning interpretability: A survey on methods and metrics,\u2019\u2019 Electronics, vol. 8, no. 8, p. 832",
            "year": 2019
        },
        {
            "authors": [
                "G. Raja",
                "S. Anbalagan",
                "S. Senthilkumar",
                "K. Dev"
            ],
            "title": "and N",
            "venue": "M. F. Qureshi, \u2018\u2018Spas: Smart pothole-avoidance strategy for autonomous vehicles,\u2019\u2019 IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 10, pp. 19 827\u201319 836",
            "year": 2022
        },
        {
            "authors": [
                "Q. Liu",
                "Z. Liu"
            ],
            "title": "B",
            "venue": "Xiong,W. Xu, and Y. Liu, \u2018\u2018Deep reinforcement learningbased safe interaction for industrial human-robot collaboration using intrinsic reward function,\u2019\u2019 Advanced Engineering Informatics, vol. 49, p. 101360",
            "year": 2021
        },
        {
            "authors": [
                "H. Wang",
                "Y. Ye",
                "J. Zhang"
            ],
            "title": "and B",
            "venue": "Xu, \u2018\u2018A comparative study of 13 deep reinforcement learning based energy management methods for a hybrid electric vehicle,\u2019\u2019 Energy, vol. 266, p. 126497",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Energy Efficiency, Delay Tolerable, Approximate Statistical Dynamic Programming algorithm, Deep Deterministic Policy Gradient, Proximal Policy Optimization.\nI. INTRODUCTION\nW ITH the development of 5G/B5G, the communicationindustry has shown a rapid increase in energy consumption. And global climate deterioration has been particularly evident in recent years. So the concept of green communications was introduced in the development of the 5G standard [1], [2]. The aim is to meet user requirements for highquality communications while reducing the further impact of the communications industry on climate degradation. Energy consumption includes four main areas: base stations, networks, terminal equipment, and data centers. Among them, the 5G/B5G base station relative to the 4G base station energy consumption is reflected in the deployment of more intensive, larger bandwidth, access to massive services, etc. Therefore, many scholars have proposed many strategies for 5G/B5G base stations to achieve energy consumption reduction [3]. Some scholars believe that energy saving can be achieved\nthrough the strategy of base station dormancy [4]\u2013[8]. Some scholars have considered the use of network virtualization and cloud-based technologies to achieve green communication [9], [10]. Of course, intelligent routing [11], [12], energy recovery [13], [14], and the adoption of green energy [15] and other technologies have also become the hot direction of scholars\u2019 attention. In this paper, unlike these enhancements of Energy Efficiency (EE) for real-time services, the object of our study is to enhance the EE of Delay Tolerable (DT) services. The emergence of delayed tolerance services is also dependent on the development of 5G. Compared to 4G, 5G can reach a maximum data rate of 10 Gbps, which is about 10 times higher than 4G [16]. Therefore, some real-time services in the context of 4G become DT services in 5G [17], such as video-on-demand, music playback, email sending and receiving, etc. Thus, we can improve the EE of DT services based on the idea of time-domain water-filling method.\nVOLUME 11, 2023 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThis paper focuses on the energy-saving scheme for DT services in downlink NOMA communication systems. In 5G, NOMA is a very important fundamental technology. It is mainly through the superposition of multiple users\u2019 signals in the same frequency and time domain to transmit information, so that the communication system capacity and the number of loaded users can be further enhanced in the 5G age. The NOMA system differentiates users according to the power allocated to them so that each user can receive their own information [18]. The implementation of NOMA system functions also depends on the maturity and development of Successive Interference Cancellation (SIC) technologies [19]. In recent years, many scholars have conducted extensive research on the resource allocation and EE of NOMA system-based realtime services. Some scholars have achieved EE improvement by solving the downlink beamforming problem of NOMA system [20].Haitham Al-Obiedollah et al [21]considered the interplay between NOMA and opportunistic cognitive radio (CR)-based OFMA technologies as a promising solution to achieve large-scale access to future B5G wireless communication systems. Therefore, they proposed an EE resource allocation technique for the multi-carrier NOMA CR-based system, which greatly improved the EE of the system. In addition, there were those who considered the EE of NOMA systems while taking into account the Quality of Service (QoS) performance [22]. Based on this, some other scholars joined the consideration of resource allocation based on maximum equity to enhance EE [23].\nHowever, it is worth noting that the allocation of resources to improve EE in NOMA systems generally encounters more complex nonconvex problems. Many scholars have considered converting nonconvex problems into convex or nearly convex problems to solve them [24], [25]. With the development of Reinforcement Learning (RL) [26] and Deep Learning (DL), more complex mathematical problems can be solved by approximating the corresponding functions through neural networks. In [27], Abuzar B. M. Adam et al. considered the use of DL approach to solve the problem of boosting EE in NOMA networks, and the experimental results showed that the performance is closer to the optimum compared to the traditional algorithms. In recent years, there has been a focus on using Deep Reinforcement Learning (DRL) for resource allocation [28].Raghavendra Pal et al. proposed to use DRL algorithm for channel selection to achieve on-demand resource allocation [29]. Wang Xiaoming et al. used the DRL framework to solve the non-convex problem encountered in joint sub-channel assignment and power allocation in NOMA uplink and verified that it can improve EE to a great extent [30]. Of course, other scholars have considered the improvement of the DL algorithm and found that the improved DRL algorithm has a better performance in terms of long-term sum rate compared to the traditional RL and DRL algorithms through validation [31].\nBut as artificial intelligence has evolved, it has become clear that evaluating the trained neural networks or agents solely by their results may lead to greater uncertainty in future\napplications [32], [33]. And, when AI becomes sufficiently widespread [34], the unexpected occurrence of uncertainty may have irreversible consequences [35]. As a result, many experts and scholars have made interpretability analysis of the trained DL models recently [36], [37].The emergence of interpretability analysis allows people to understand the models intuitively and to evaluate them more scientifically. To better evaluate the soundness of our algorithm design, we also perform an interpretability analysis of our DRL model in this paper. The main contributions of this paper can be summarized as follows.\n\u25b6 First, we formulate the EE problem of DT service in NOMA downlink communication systems, which is considered a non-convexity problem, as a convex problem and obtain the corresponding closed-form solution. \u25b6 Second, we use Approximate Statistical Dynamic Programming (ASDP), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) algorithms to train agents that can perform reasonable resource allocation under non-complete Channel State Information (CSI). \u25b6 Finally, this paper also provides an interpretability analysis of the decisions made by the agents trained by the DDPG algorithm as well as the PPO algorithm.\nII. SYSTEM MODEL AND PROBLEM FORMULATION A. SYSTEM MODEL The communication scenario we considered is a cellular downlink NOMA scenario with one base station and K users. It should be mentioned that in this article we assume that a single transmit antenna is installed on both the base station and user side and operates in time division duplex (TDD) mode. The following Fig.1 provides a detailed description of our communication system.\nThe signal received by the k-th user in the i-th time slot can be expressed as\nyk,i = hk,i N\u2211 k=1 \u221a Pt,k,ixk,i + wk,i, (1)\n2 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nwhere hk,i denotes the channel gain andwk,i denotes the Gaussian white noise. And, xk,i denotes the transmission signal and Pk,i denotes the transmission power allocated by the base station to the k-th user. But the signals are subject to both small-scale and large-scale fading in the transmission links. Then the channel gain hk,i can be expressed as\nhk,i = gk,id \u2212 b2 k , (2)\nwhere gk,i is the channel fading coefficient, dk denotes the distance between the k-th user and the base station, and b is the path loss index. In this paper, the channel fading coefficient gk,i obeys the Rayleigh distribution, and the Probability Density Function (PDF) can be expressed as\nf (g) = g \u03c32 e\u2212 g2 2\u03c32 . (3)\nWithout loss of generality, assume that |h1,i| > \u00b7 \u00b7 \u00b7 > |hk,i| > \u00b7 \u00b7 \u00b7 > |hN ,i|.\nB. PROBLEM FORMULATION For point-to-point communication models, we usually express the EE as\nE = C\nW (Pl + Pt) , (4)\nwhere C and W represent the channel rate and bandwidth respectively. The Pl is the static power and the Pt is the transmission power.\nFor DT services, such as video-on-demand, sending emails, and playing music, this means transmitting the whole service in a fixed time. This concept is based on 5G highspeed communication and aims to improve the EE for nonreal-time communication services by rationalizing the allocation of resources. Therefore, deform (4) as follows\nE = \u03d5 W \u03c4 N\u2211 i=1 (Pl + Pt,i) , (5)\nwhere the total number of services is \u03d5 = WC\u03c4N , \u03c4 is the time slot interval and N is the number of time slots. Thus we consider that the EE of a multi-user communication model based on a downlink NOMA communication system can be expressed as\nE = \u03d51 + \u00b7 \u00b7 \u00b7+ \u03d5N W \u03c4 K\u2211 k=1 ( NPl,k + N\u2211 i=1 Pt,k,i ) , (6) where \u03d5k denotes the total service of the k-th user, Pl,k denotes the static power of the k-th user, and Pt,k,i denotes the dynamic power of the i-th time slot of the k-th user. And,k = 1, 2, \u00b7 \u00b7 \u00b7 ,K . The analysis reveals that in DT services, the EE is only related to the total dynamic power, and the EE ismaximumwhen the total dynamic power is minimal. However, in the communication model of this paper, for the allocation scheme of the transmitted power in each time slot, we consider the idea of introducing a time-domain based water-filling method as shown in Fig.2.\nIn Fig. 2 the horizontal axis represents the time interval and the vertical axis represents the transmission power distribution model. And the EE maximization problem can be transformed into a total dynamic power minimization problem, as follows\nmin Pt,k,i : K\u2211 k=1 N\u2211 i=1 Pt,k,i. (7)\nAnd its constraint can be expressed as\ns.t.  0 \u2264 K\u2211 k=1\nPt,k,i \u2264 Pmax N\u2211 i=1 log2 (1 + Pt,k,i\u03b1k,i) \u2265 \u03d5k , (8)\nwhere \u03b1k,i is the channel state of the k-th user at the i-th time slot. In this paper, we study the downlink-based NOMA systemmulti-user communication boosting EE problem, so users with weaker channel gain will be interfered by users with strong channel gain. So the channel state can be expressed as\n\u03b1k,i = |hk,i|2 |hk,i|2 K\u2211\nj=1,|hj,i|2>|hk,i|2 Pt,j,i + \u03c32\u03c9\n. (9)\nAccording to (7), (8) and (9), we can conclude that the problem is a non-convex optimization problem. However, it can be transformed into a convex optimization problem through variable substitution.\nmin ri : N\u2211 i=1 Pt,i (ri), (10)\ns.t.  0 \u2264 Pt,i (ri) \u2264 Pmax N\u2211 i=1 rk,i \u2265 \u03d5k , (k = 1, 2, \u00b7 \u00b7 \u00b7 ,K ) , (11)\nwhere rk,i can be expressed as the amount of information actually transmitted by the k-th user at the i-th time slot. And Pt,i (ri) is expressed as the power function transmitted at the i-th time slot, where the independent variable is the amount of information transmitted at the i-th time slot. The rk,i can be expressed as\nrk,i = log2 (1 + Pt,k,i\u03b1k,i) . (12)\nVOLUME 11, 2023 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThen (11) can obviously be proved to be a convex set. In summary, we can prove that (10) and (11) are convex optimization problems. And we can obtain the optimal allocation scheme of transmission power for all users in the downlink NOMA communication system according to (10) and (11). Proof: See the Appendix A. In reality, we have limited knowledge about the future CSI, so we use Markov Decision Processes (MDP) in this paper to solve the problem of EE transmission power allocation for MOMAbased communication systems. The MDP generally consists of four tuples, state S, action A, action transfer probability H, and reward \u03a5. And, we can express the state set in this article as A = { [s1,i, s2,i, \u00b7 \u00b7 \u00b7 , sK ,i]i } , the action set\nas A = { [r1,i, r2,i, \u00b7 \u00b7 \u00b7 , rK ,i]i } , the action transfer proba-\nbility H = { [\u03b71,i, \u03b72,i, \u00b7 \u00b7 \u00b7 , \u03b7K ,i]i } , and the reward \u03a5 ={\n[\u03b31,i, \u03b32,i, \u00b7 \u00b7 \u00b7 , \u03b3K ,i]i } . Where, i = 1, 2, \u00b7 \u00b7 \u00b7 ,N , and the transfer probability of the k-th user in the i-th time slot can be expressed as\n\u03b7k,i {sk,i+1 |sk,i, rk,i } = {\n1, ck,i+1 = ck,i \u2212 rk,i 0, other . (13)\nwhere ck,i is the amount of remaining information displayed by the k-th user at the i-th time slot. And, we can express the state value function as (14), where \u03bb is the discount factor. And the action state value function can be expressed as (15). When the k-th user arrives at the last time slot, the state value function and action state value function can be expressed as\nV (sk,N ) = \u03b3k,i (sk,N ) , (16)\nQ (sk,N , rk,N ) = \u03b3k,N (sk,N , rk,N ) . (17)\nOf course, we also need to note that the state of the k-th user at the i-th time slot in this article is composed of the channel state and the amount of remaining information, which can be expressed as\nsk,i = (\u03b1k,i, ck,i) . (18)\nIII. POWER ALLOCATION ALGORITHM BASED ON REINFORCEMENT LEARNING Since the solution of the optimal solution to the convex optimization problem takes much time, RL algorithms are invoked to reduce the computational complexity and improve the computational efficiency. In the general case, we only know the state distribution of the future channel, so the reward value of the k-th user at the i-th time slot can be expressed as an expectation, and then (14) and (15) can be converted into the following equation (19) and (20). Similarly, equations (16) and (17) can be expressed as\nV (sk,N ) = E [\u03b3k,i (sk,N )] , (21)\nQ (sk,N , rk,N ) = E [\u03b3k,N (sk,N , rk,N )] . (22)\nThe reward for the k-th user of the i-th time slot can be expressed as\nE [\u03b3k,i] = E [ Pt,k,i ] = \u222b (2rk,i \u2212 1) \u03b1k,i (g) f (g) dg. (23)\nWe design the amount of information that may be transmitted by the k-th user as follows\nrk,i = \u03d5k \u00d7 \u03b81 N , (24)\nwhere, \u03b81 = 1, 2, \u00b7 \u00b7 \u00b7 , \u03b82, \u03d5k denotes the total amount of information to be transmitted by the k-th user andN denotes the maximum delay of transmission. The amount of information remaining for the k-th user of each time slot can be expressed as\nck,i = \u03d5k \u00d7 \u03b82 N , (25)\nwhere, \u03b82 = 1, 2, \u00b7 \u00b7 \u00b7 ,N . The flow of the ASDP algorithm based on the NOMA system is shown below.\nIV. POWER ALLOCATION ALGORITHM BASED ON DEEP REINFORCEMENT LEARNING Although the ASDP algorithm compensates the problem of not being able to find the optimal closed-form solution under non-complete CSI conditions, it has corresponding drawbacks. The first one is that if the accuracy of action division is not sufficient, it consumes a little extra power when transmitting the same amount of information. It is also important to note that when the accuracy of the action is controlled precisely, the time to make the corresponding decision is multiplied. It can even generate curse of dimensionality due to too precise control action accuracy, and thus the ASDP algorithm cannot get the corresponding policy. With the development of DL and RL, DRL was born. And it can be a good remedy for the problem that the RL cannot make a decision because of the curse of dimensionality.\nA. DDPG ALGORITHM BASED ON NOMA SYSTEM The curse of dimensionality caused by sufficiently accurate actions in the ASDP algorithm are actually problems caused by the shift from discrete to continuous actions. For the DDPG algorithm is a good solution to the problem of curse of dimensionality caused by making decisions in the continuous action space. So we introduced the DDPG algorithm to solve the resource allocation problem for NOMA-based systems. The DRL algorithm differs from the RL algorithm in that it uses a neural network to approximate the state action value function and make decisions. For the DDPG algorithm, it makes the corresponding decisions through the actor network and calculates the state action values through the critic network. Meanwhile, for the stability of network update, both actor network and critic network have corresponding target networks. So we can express the loss value function of DDPG can be expressed as (26) and (27), where Q\u03c0 (sj, rj |\u03c9Actor , \u03c9critic ) denotes the state action value estimated by the critic network. \u03c9Actor ,\u03c9critic,\u03c9tarActor ,\u03c9 tar critic represent actor network weight, critic network weight and their target network weight, respectively. G denotes the number of samples. And Qtar (sj, rj |\u03c9tarActor , \u03c9tarcritic ) denotes the state action value fitted by the target critic network and reward, it can be expressed as (28).\n4 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nV (sk,i) = \u03b3k,i (sk,i) + \u03bb \u2211 sk,i\u2208S \u03b7k,i (sk,i+1 |sk,i )V (sk,i+1). (14)\nQ (sk,i, rk,i) = \u03b3k,i (sk,i, rk,i) + \u03bb \u2211\nsk,i+1\u2208S \u03b7k,i+1 (rk,i |sk,i+1, rk,i )V (sk,i+1, rk,i+1). (15)\nV (sk,i) = E [\u03b3k,i (sk,i)] + \u03bb \u2211 sk,i\u2208S \u03b7k,i (sk,i+1 |sk,i )V (sk,i+1). (19)\nQ (sk,i, rk,i) = E [\u03b3k,i (sk,i, rk,i)] + \u03bb \u2211\nsk,i+1\u2208S \u03b7k,i+1 (rk,i |sk,i+1, rk,i )V (sk,i+1, rk,i+1). (20)\nLDDPGActor = \u2212 1\nG G\u2211 j=1 (Q\u03c0 (sj, rj |\u03c9Actor , \u03c9critic )). (26)\nLDDPGcritic = 1\nG G\u2211 j=1 (Qtar (sj, rj |\u03c9tarActor , \u03c9tarcritic )\u2212 Q\u03c0 (sj, rj |\u03c9Actor , \u03c9critic )) 2 . (27)\nQtar (sj, rj |\u03c9tarActoe, \u03c9tarcritic ) = \u03beQtarmax (sj+1, rj+1 |\u03c9tarActor , \u03c9tarcritic ) + Rj. (28)\nVOLUME 11, 2023 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAnd, \u03be, Rj, sj and rj denote the discount factor, reward, state and action of the j-th group of samples, respectively. Based on the model in this paper, we can express sj and rj as\nsj = { [s1,i, s2,i, \u00b7 \u00b7 \u00b7 , sK ,i]i,j } , (29)\nrj = { [r1,i, r2,i, \u00b7 \u00b7 \u00b7 , rK ,i]i,j } , (30)\nwhere, i = 1, 2, \u00b7 \u00b7 \u00b7 ,N , j = 1, 2, \u00b7 \u00b7 \u00b7 ,G. Updating the critic network is to minimize the loss value of (27), updating the actor network needs to be based on gradient ascent, and we can express the policy gradient function as (31).\nThe weight update for the actor and critic target networks can be expressed as\n\u03c9tarActor \u2190 \u03ba\u03c9Actor + (1\u2212 \u03ba)\u03c9tarActor , (32)\n\u03c9tarcritic \u2190 \u03ba\u03c9critic + (1\u2212 \u03ba)\u03c9tarcritic, (33)\nwhere \u03ba is the soft update factor. Of course, what deserves more attention is the design of the reward function, which has been shown in many simulation experiments [38], [39] using the DDPG algorithm to be related to the goodness of the final decision.So we formulate the reward function as (34) and (35),where \u03b2, \u03b61, \u03b62, \u03b63 and \u03b64 represent the coefficients and their specific values will be set according to different conditions.\nRj = \u03b2\u03b3K ,i,j, i\u2211\n.l=1\nrk,l = \u03d5k . (34)\nAnd, R0 = \u03b3N ,0,1 = 0. The flow of the DDPG algorithm based on the NOMA system is shown below.\nB. PPO ALGORITHM BASED ON NOMA SYSTEM The PPO algorithm is also a DRL algorithm for continuous state and continuous action spaces. Compared with the DDPG algorithm, the PPO algorithm can effectively control a more stable update of the policy [40]. So we introduced the PPO algorithm to solve the resource allocation problem of NOMA-based communication systems. Therefore, the advantage function of the PPO algorithm in this paper can be expressed as\nDi = i\u2211\nl=1\n\u03c2N\u2212i+1\u03b3K ,l + \u03c2 N\u2212iV (si+1)\u2212 V (si) , (36)\nwhere \u03c2 is the discount factor of the final reward for each time slot, while V (si+1) and V (si) are the state values of the i+1th time slot and the i-th time slot, respectively. And, here the state values are obtained through the critic network. And, the update of the critic network depends on the dominance value function, so the critic loss value function can be expressed as\nLPPOcritic = 1\nN N\u2211 i=1 (Di) 2 . (37)\nAnd the actions are obtained by sampling the normal distribution calculated by the actor neural network, while we can also obtain the probabilities corresponding to the actions.\nHowever, old-actor network and new-actor network are set up in order to make the actor network update more stable in PPO algorithm. The normal distributions they obtained can then be denoted as N ( \u00b5old,i, \u03c3 2 old,i ) and N ( \u00b5new,i, \u03c3 2 new,i ) , respectively. And the probabilities corresponding to the actions obtained by sampling can be denoted as \u03c1old,i and \u03c1new,i, respectively. We can then obtain the loss value function for updating the new-actor network, which can be expressed as (38), where \u03b5 is the clipping factor. And, \u03c1new (ri |si ) and \u03c1old (ri |si ) denote the probability that the new actor network and the old actor network in the i-th time slot give out the action as ri based on the channel stat si, respectively.Since the PPO algorithm introduces a clipping factor, it is easier to converge and update the network more stable than the DDPG algorithm. For the update of the old-actor network, it depends on the update of the new-actor network, and in this article we set it to update the old-actor network after the C-step of the new-actor update. Here, we also need to pay attention to the acquisition of reward values. The reward value of the PPO algorithm in this article is similar to that of the DDPG algorithm, but there will be differences in the setting of the coefficients, which will be presented in the analysis of the results. The flow of the PPO algorithm based on the NOMA system is shown below.\nV. PERFORMANCE EVALUATION The number of end-users set in our simulation experiment is 2. Moreover, they receive 10 bits messages and 5 bits messages, respectively. And they have a maximum delay time of 8 seconds.The settings of the specific simulation experiment parameters are shown in the following table.\nAs it was found during the simulation experiments that when the channel state of multiple time slots is extremely poor, it may lead to the information not being transmitted in full with the maximum transmission power limit. Thus, we consider the introduction of outage probabilities and evaluate the performance of several algorithms by using equalizing power. The equalizing power equation can be expressed as\nP\u0304t =\nK\u2211 k N\u2211 i=1 Pt,k,i\n1\u2212 \u00b5 , (39)\nwhere Pt,k,i denotes the transmission power required by the k-th user of the i-th time slot and \u00b5 denotes the outage probability. For DRL algorithms, the network structure, reward settings, and hyperparameter settings all affect the performance of the trained agents. Therefore, the network structures of the\n6 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n\u2207\u03c9Actor J (V ) = 1\nG \u2211 j [ \u2207rQ (sj, rj |\u03c9Actor , \u03c9critc ) \u2223\u2223 r=V (sj|\u03c9Actor )\u2207\u03c9ActorV (sj |\u03c9Actor ) ] . (31)\n\u03b3k,i,j = \u03b61Rj\u22121 + \u03b62\u03b3K ,i\u22121,j + \u03b63 i\u22121\u2211 l=1 Pt,k,l + \u03b64 k\u2211 v=1 Pt,v,l . (35)\nLPPOactor = E [ min ( \u03c1new (ri |si ) \u03c1old (ri |si ) Di ) , clip ( \u03c1new (ri |si ) \u03c1old (ri |si ) , 1\u2212 \u03b5, 1 + \u03b5 ) Di ] . (38)\nVOLUME 11, 2023 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\naction network and the critic network of the DDPG algorithm and the PPO algorithm in the simulation experiments of this paper will be shown below, respectively.\nThe setting of hyperparameters in the training agent using the DRL algorithm also has an impact on the performance of the agent. In the following table, the more important hyperparameters in this simulation experiment are listed.\nThe RV referred to in Tables 3 and 4 represent action correction deviation values. Specifically, it can be expressed as:\nRV = |rk,i \u2212 rk,i,max| , (40)\nwhere rk,i denotes the transmission policy given by the agent to the k-th user in the i-th time slot and rk,i,max denotes the maximum action value that can be transmitted by the k-th user in the i-th time slot. Since the action given by the agent is likely not to be transmitted within the limit of maximum transmit power, RV needs to be introduced to measure the deviation between the policy given by the agent and the actual amount of information that can be transmitted. And the reward value plays a key role in the training direction of the agent, so the RV is used as an important indicator to influence the reward value. The maximum transferable action rk,i,max can be expressed as (41).\nFinally, it is important to note that the SNR in this paper\u2019s simulation experiment can be expressed as\nSNR = 10log10\n( 1\n\u03c32\n) . (42)\nA. SELECTION OF AGENTS AND MAXIMUM TRANSMISSION POWER We found through simulation experiments that agents trained under different maximum transmission power conditions also\nhave superior and inferior performance when performing resource allocation. The first thing that deserves our attention is the robustness of the agents. We test the robustness of the agents by perturbing the maximum transmit power in the test environment. As for measuring robustness, we consider using the stability of each agent\u2019s equalizing power consumption to transmit equal amounts of information as the metric. We then plotted a heat map to measure the robustness of the agent, as shown in Fig. 8. In Fig. 8, (a) shows the heat map measuring the robustness of the agents trained using the DDPG algorithm. (b) shows the heat map measuring the robustness of the agents trained by the PPO algorithm. The horizontal coordinate represents the maximum transmit power value set in the test environment, and the vertical coordinate represents the maximum transmit power value set during the training agent. We should note that when training agents, different maximum transmitting power will get different agents. Moreover, the color blocks of different colors indicate the equalized power that all users need to consume to transmit all messages. First, we focus on Fig. 8 (a). We can analyze that the robustness of the agents trained at a maximum power of 1(W) and a maximum power of 3 (W) is better according to the color block color. We then further analyzed the agents obtained by training under these two maximum transmitting power conditions. The bars in Fig. 9(a) represent the total transmission power consumed by all users to successfully transmit all messages. The line graph in Fig.9(b) represents the outage probability when transmitting a message. We can see that the agent trained at a maximum transmission power of 3 (W) needs to consume less total power compared to another agent for the most part. In terms of disruption probability, we can see that although the agent trained with a maximum transmission power of 1 (W) has a smoother performance, the disruption probability is higher in the test environment with a maximum transmission power below 10 (W) and above 15 (W) than the agent trained with a maximum transmission power equal to 3 (W). Therefore, we prefer to use the agent trained at the maximum transmission power equal to 3W for subsequent simulation experiments. According to Fig. 8(a) and Fig. 9, we consider the outage probability and the equalizing power of the agent, so we set the maximum transmission power to 19 (W) for the subsequent simulation, because under this condition, the equalizing power is minimal and the outage probability is close to 0. This reflects our basic idea of improving EE while satisfying the basic requirements of user communication. We use the same approach to measure the robustness of the agents trained by the PPO algorithm, as shown in Fig. 8 (b). We can see that there are also agents with similar robustness, so we performed the same analysis as in Fig. 9, which is shown in Fig. 10. Fig. 8 (b) shows that among all the algorithms trained by PPO, the robustness of the agents trained at the maximum transmit power of 1 (W), 5 (W), and 16 (W) performs better.\n8 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nrk,i,max = log2\n( 1 + max ( Pt,i,max \u2212\nk\u22121\u2211 l=1 Pt,l,i, 0\n) \u03b1k,i ) . (41)\nVOLUME 11, 2023 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n10 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFurther combined with Fig. 10, it can be analyzed that the agent trained with the maximum transmit power set to 5 (W) performs best. Its outage probability of transmitting information is close to that of the agent trained at the maximum transmit power of 1 (W) and lower than that of the agent trained at the maximum transmit power of 16 (W). And its total power required to transmit all users\u2019 information is less than that of the agent trained with the maximum transmission power of 1 (W).\nTherefore, in our subsequent simulation experiments, for the PPO algorithm, we use the agent trained in a maximum transmission power of 5 (W), and the maximum transmission power is set to 20 (W).\nB. PERFORMANCE ANALYSIS OF ALGORITHMS BASED ON EQUALIZING POWER In this section, the performance of all algorithms is compared under different SNR conditions based on our choice of agents and the maximum transmit power constraint. We still use the equalizing power that needs to be consumed to transmit all the information as a measure of good or bad algorithm performance. So, we need to first focus on the outage probability of these algorithms.\nIn Fig. 11, the horizontal coordinate represents the SNR and the vertical coordinate represents the outage probability. The yellow diamonds represent the outage probability curve of the greedy algorithm, the purple crosses represent the PPO algorithm, and the black triangles represent the DDPG algorithm. For the ASDP algorithm and the optimal lower\nlimit, their outage probability is 0 as long as they can carry out the transmission of information, so we do not plot them in Fig. 11. We further analyze Fig. 11 and find that the greedy algorithm experiences outage probability, which is higher than the DRL algorithm. We consider in depth the reasons for the occurrence of this phenomenon. This paper is based on NOMAdownlink two-user communication for simulation experiments. Therefore, in the greedy algorithm, it is specified that the near-end user communicates first, and then the far-end\nVOLUME 11, 2023 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nuser communicates when all the near-end user information has been transmitted. Such a fixed resource allocation may lead to not transmitting all the information within a maximum tolerable delay time, thus resulting in a high outage probability. This reflects one of the advantages of using the DRL algorithm, which is that we can make reasonable resource allocations according to the CSI and improve the QoS of the users when they communicate. We further focus on the outage probability curves of the DDPG algorithm and the PPO algorithm. We can see that the outage probability of the DDPG algorithm is significantly lower than that of the PPO algorithm. However, this does not mean that the DDPG algorithm is better than the PPO algorithm. We also need to consider the performance of the algorithms to improve EE, i.e., which algorithm needs to consume less equalization power to transmit the full information.\nThe curves of the greedy algorithm and the two DRL algorithms in Fig. 12 are represented in the same way as in Fig. 11. Note that the ASDP algorithm is represented by green dots and the optimal lower bound is represented by red squares.\nIn Fig. 12, it can be seen that the algorithmswe used, ASDP algorithm, DDPG algorithm, and PPO algorithm, are above the lowest lower limit and below the greedy algorithm. This shows that our adoption of RL and DRL algorithms to solve the EE problem for DT services is effective. Further analysis shows that the two DRL algorithms require little difference in the equalizing transmission power consumed to transmit all the information, and both are smaller than the ASDP algorithm. However, it is worth noting that as the SNR increases, the total equalizing power consumed by the three algorithms to transmit all the information gradually approaches. The ASDP algorithm is based on the distribution of the entire channel state to obtain the transmit power corresponding to the transmission of different amounts of information. While the SNR affects the channel state, then the SNR must affect the accuracy of the ASDP algorithm to estimate the power\nconsumption required to transmit a certain amount of information. However, as the SNR value increases, the channel state value is gradually reduced by the SNR influence, and the ASDP algorithm gradually approaches the decision made by the DRL algorithm, which also becomes reasonable. This shows the advantage of the DRL algorithm in regulating how much information is delivered based on the channel state. It selects the transmission strategy based only on the channel state, thus controlling the energy consumptionmore precisely. To better compare the performance of ASDP algorithm, DDPG algorithm, and PPO algorithm. We plotted their performance improvement percentage relative to the greedy algorithm in terms of transmitting the same information consuming equalizing power performance. This is shown in the figure below.\nFrom Fig. 13, it can be seen that ASDP has the fastest performance percentage improvement relative to the greedy algorithm as the SNR increases. Secondly, it can be seen that the percentage performance improvement of DDPG and PPO algorithms with increasing SNR does not change much with respect to the greedy algorithm, which more favorably confirms that the DRL algorithm designed in our paper will only select the policy based on the channel state and will not be affected by the SNR too much. When we further analyze theDDPG algorithm and the PPO algorithm, it is clear that the DDPG algorithm performs better than the PPO algorithm.\nVI. EXPLAINABLE ANALYSIS The DRL algorithm uses deep neural networks as function approximators to solve complex decision problems in continuous channel state space or action space. The trained agent is equivalent to a black box, and although it achieves intelligent and fast decisions with good performance, we are not sure whether the trained agent really solves the DT service enhancement EE problem according to our time-domain based water-filling method. Therefore, we visualized the decision making process of the agents trained by DDPG algorithm\n12 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nand the agents trained by PPO algorithm, and performed interpretable analysis.\nA. EXPLAINABLE ANALYSIS BASED ON ACTION SELECTION We first focused on the transmission decisions made by the agent trained by the DDPG algorithm for the two users of the NOMA downlink according to the different user channel states. This is shown in the following Fig. 14.\nIn Fig. 14, we use the square black lines to represent the action selection of the agent trained by the DDPG algorithm. And, we use the bars to represent the channel state countdown for this time slot. The left and right plots in Fig. 14 depict the near-end user (user 1) and the far-end user (user 2), respectively. One thing to note is that we are using the inverse of the channel state to describe the channel state, whichmeans that when the channel state is poor, the bar values in the graph will be higher, and conversely when the channel state is good, the bar values will be lower.\nAs can be seen from the graph on the left, in the 1st time slot the channel state is relatively good and the agent chooses to transmit the information, while in the 2nd time slot, the agent considers the channel state to be very good and transmits all the remaining information. Interestingly, in time slots 3 to 6, the first user also has a good channel state, but does not choose to transmit information. However, the diagram on the right gives a reasonable explanation for the appearance of this phenomenon. The channel state in the 1st time slot in the right-hand figure behaves poorly, so the agent selects transmit information in time slots 3 to 8. It is not difficult to conclude that the agent has done a good job of choosing the appropriate action based on the CSI of both users.\nWe further look at the action choices made by the agent trained by the PPO algorithm based on the channel state.\nThe lines in Figs. 15 and 14 have the same meaning. By further analysis, it can be seen that the agent trained by the PPO algorithm also combines the CSI of both users to make a reasonable action selection. However, it is important to know that the equalization power consumed by the agent trained by the DDPG algorithm is less than that of the PPO algorithm. So we visualize the power allocated by the agent to transmit the information as well.\nB. EXPLAINABLE ANALYSIS BASED ON EQUALIZING POWER We first focus on the case where the agent trained by the DDPG algorithm allocates transmission power for both users based on the channel state.\nIn Fig. 16, the purple area indicates the transmission power assigned by the agent trained by the DDPG algorithm according to the channel state, and the yellow area indicates the inverse of the channel state.\nWe can clearly see from Fig. 16 that the agent allocates a reasonable transmission power considering both channel states simultaneously. For example, if user 1 has a good channel state in the 1st time slot and user 2 has a poor channel\nstate in the 1st time slot, the agent will tilt the resources toward user 1. And when the channel state of user 1 is poor, but the channel state of user 2 is good, the agent again tilts the resources toward user 2. We further compare the agent trained by the PPO algorithm for resource allocation based on channel state. In Fig. 17, the agent trained by the PPO algorithm is represented to allocate the transmission power according to the channel state. The meaning of the different color blocks in the figure is the same as in Fig. 16. Further analysis shows that the agent trained by the PPO algorithm has a performance gap in allocating transmission power according to the channel state compared to the agent trained by the DDPG algorithm. In Fig. 17, user 1 has a better channel state in the 3rd time slot than in the 2nd time slot, while the difference between the 3rd and 4th time slot channel states for user 2 is not significant. If the agent chooses to transmit the same amount of information in the 3rd time slot as the present 2nd time slot for the user 1 channel state, and the agent chooses to transmit the same amount of information in the 4th time slot as the present 3rd time slot for the user 2 channel state, then according to Eq. (12) its transmission power must be smaller than the present strategy. In summary, the relationship between the channel state and the amount of transmission information can reflect to some extent the reasonableness of the decisions made by the agents trained by the DRL algorithm. However, the relationship between the transmission power and the channel state needs to be further visualized in order to understand more intuitively the problem that the agent\u2019s decision is not optimal.\nVII. CONCLUSIONS In order to solve the problem of DT service enhancement EE for two users in NOMA downlink, we can easily obtain the following conclusions based on the simulation results. First, of the three algorithmswe proposed, ASDP algorithm, DDPG algorithm and PPO algorithm, DDPG algorithm to solve the NOMA downlink two-user resource allocation problem has the best performance. Second, the performance of the decisions made by the agents trained by the DRL algorithm is related to the maximum transmission power settings in the training and testing environments. For both the DDPG and PPO algorithms, the agents with better decision performance have a lower maximum transmission power value setting for training and a relatively higher maximum transmission power setting for testing. However, it is worth noting that a larger maximum transmission power at test time is not better. Finally, we found that visualizing the relationship between the power allocation and the reciprocal of the channel state, as opposed to visualizing the relationship between the agent\u2019s decisions and the reciprocal of the channel state, can help us analyze what is wrong with the decisions made by the trained agent. Such an interpretation provides strong evidence to judge whether the decisions are reasonable, to analyze the reasons why the agent\u2019s decisions are not optimal, and to explain whether our evaluation of the agent is reasonable.\nVOLUME 11, 2023 13\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n14 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThere are more research directions we can take for the EE problem of NOMA-based downlink two-user enhancement of DT services. First, we can further focus on the adjustment of hyperparameters in DRL algorithm and the setting scheme of the reward function. In addition, we can further focus on the clustering problem of NOMA downlink multi-users. Thus, the theoretical level research can be further advanced to the practical application level. Finally, we can also consider the same idea to address the possibility of enhancing the EE of DT services in different communication scenarios.\nAPPENDIX A PROPERTY 1 The proofs for proving (10) and (11) as convex optimization problems are as follows. According to (12), we can obtain the transmitted power of the i-th time slot of the k-th user as follows\nPt,k,i = (2rk,i \u2212 1)\n\u03b1k,i . (43)\nAccording to (9) and (43) the expression for the transmitted power of the k-th user in a time slot can be obtained as follows\nPt,\u03c0(k) =\n(2r\u03c0(k) \u2212 1) ( \u03c32\u03c9 + \u2223\u2223h\u03c0(k)\u2223\u22232 k\u22121\u2211 j=1 Pt,\u03c0(j) ) \u2223\u2223h\u03c0(k)\u2223\u22232 = (2r\u03c0(k) \u2212 1)  \u03c32\u03c9\u2223\u2223h\u03c0(k)\u2223\u22232 + k\u22121\u2211 j=1 Pt,\u03c0(j)\n (44) To facilitate the calculation, sk is introduced to denote the power sum of the first k users of a time slot, which can be expressed as\nsk = k\u2211 j=1 Pt,\u03c0(j)\n= Pt,\u03c0(k) + sk\u22121 = 2r\u03c0(k) \u00d7 sk\u22121 + (2r\u03c0(k) \u2212 1) \u03c32\u03c9\u2223\u2223h\u03c0(k)\u2223\u22232 .\n(45)\nAnd the s0 = 0. In order to be able to derive (44) as a convex function, we also introduce the variable ck .\nck = 2\nK\u2211 j=k+1\nr\u03c0(j) . (46)\nAnd the cK = 1. Thus (17) can be obtained from (45) and\n(46), as follows\ncksk = 2 r\u03c0(k)+\nK\u2211 j=k+1\nr\u03c0(j) \u00d7 sk\u22121\n+ 2r\u03c0(k)+ K\u2211j=k+1 r\u03c0(j) \u2212 ck  \u03c32\u03c9\u2223\u2223h\u03c0(k)\u2223\u22232 = ck\u22121 \u00d7 sk\u22121 + (ck\u22121 \u2212 ck) \u03c32\u03c9\u2223\u2223h\u03c0(k)\u2223\u22232 .\n(47)\nLet k = K , so that (48) can be derived from (47) as follows\ncK sK = sK = cK\u22121 \u00d7 sK\u22121 + (cK\u22121 \u2212 cK ) \u03c3 2 \u03c9\n|h\u03c0(K)|2\n= cK\u22122 \u00d7 sK\u22122 + (cK\u22122 \u2212 cK\u22121) \u03c3 2 \u03c9\n|h\u03c0(K\u22121)|2\n+(cK\u22121 \u2212 cK ) \u03c3 2 \u03c9\n|h\u03c0(K)|2 = c0 \u00d7 s0 + K\u2211 k=1 (ck\u22121 \u2212 ck) \u03c3 2 \u03c9 |h\u03c0(k)|2 .\n(48)\nVOLUME 11, 2023 15\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nThus the power sum of K users of a time slot can be obtained.\nPt,i (ri) = K\u2211 k=1 2 K\u2211j=k r\u03c0i (j) \u2212 2 K\u2211j=k+1 r\u03c0i (j)  \u03c32\u03c9 |h\u03c0i(k)| 2\n= 2 K\u2211j=1 r\u03c0i (j) \u2212 2 K\u2211j=2 r\u03c0i (j)  \u03c32\u03c9 |h\u03c0i(1)| 2 + K\u2211 k=2 2 K\u2211j=k r\u03c0i (j) \u2212 2 K\u2211j=k+1 r\u03c0i (j)  \u03c32\u03c9 |h\u03c0i(k)| 2\n= 2\nK\u2211 j=1\nr\u03c0i (j) \u03c32\u03c9 |h\u03c0i(1)| 2\n+ K\u2211 k=2 2 K\u2211 j=k r\u03c0i (j) ( \u03c32\u03c9 |h\u03c0i(k)| 2 \u2212 \u03c3 2 \u03c9 |h\u03c0i(k\u22121)| 2 ) \u2212 \u03c3 2 \u03c9 |h\u03c0i(K)| 2\n(49)\nSince |hk\u22121| > |hk | , we can obtain\nPt,i\u2032\u2032 (ri) \u2265 0 (50)\nSo it can be proved that Pt,i (ri)is a convex function. And (11) is obviously a convex set, so the optimization problem consisting of (10) and (11) can be proved to be a convex optimization problem.\nREFERENCES [1] GreenTouch, \u2018\u2018Reducing the net energy consumption in communications\nnetworks by up to 98 [2] J. Wu, Y. Zhang, M. Zukerman, and E. K.-N. Yung, \u2018\u2018Energy-efficient\nbase-stations sleep-mode techniques in green cellular networks: A survey,\u2019\u2019 IEEE communications surveys & tutorials, vol. 17, no. 2, pp. 803\u2013826, 2015. [3] S. Buzzi, I. Chih-Lin, T. E. Klein, H. V. Poor, C. Yang, and A. Zappone, \u2018\u2018A survey of energy-efficient techniques for 5g networks and challenges ahead,\u2019\u2019 IEEE Journal on Selected Areas in Communications, vol. 34, no. 4, pp. 697\u2013709, 2016. [4] O. Arnold, F. Richter, G. Fettweis, and O. Blume, \u2018\u2018Power consumption modeling of different base station types in heterogeneous cellular networks,\u2019\u2019 in 2010 Future Network & Mobile Summit. IEEE, Conference Proceedings, pp. 1\u20138. [5] J. Wu, S. Zhou, and Z. Niu, \u2018\u2018Traffic-aware base station sleeping control and powermatching for energy-delay tradeoffs in green cellular networks,\u2019\u2019 IEEE Transactions on Wireless Communications, vol. 12, no. 8, pp. 4196\u2013 4209, 2013. [6] R. Li, Z. Zhao, X. Chen, J. Palicot, and H. Zhang, \u2018\u2018Tact: A transfer actor-critic learning framework for energy saving in cellular radio access networks,\u2019\u2019 IEEE transactions on wireless communications, vol. 13, no. 4, pp. 2000\u20132011, 2014. [7] F. E. Salem, Z. Altman, A. Gati, T. Chahed, and E. Altman, \u2018\u2018Reinforcement learning approach for advanced sleep modes management in 5g networks,\u2019\u2019 in 2018 IEEE 88th Vehicular Technology Conference (VTCFall). IEEE, Conference Proceedings, pp. 1\u20135. [8] J. Liu, B. Krishnamachari, S. Zhou, and Z. Niu, \u2018\u2018Deepnap: Data-driven base station sleeping operations through deep reinforcement learning,\u2019\u2019 IEEE Internet of Things Journal, vol. 5, no. 6, pp. 4273\u20134282, 2018. [9] M. Kalil, A. Al-Dweik, M. F. A. Sharkh, A. Shami, and A. Refaey, \u2018\u2018A framework for joint wireless network virtualization and cloud radio access networks for next generation wireless networks,\u2019\u2019 IEEE Access, vol. 5, pp. 20 814\u201320 827, 2017. [10] I. Ullah, S. Ahmad, F. Mehmood, and D. Kim, \u2018\u2018Cloud based iot network virtualization for supporting dynamic connectivity among connected devices,\u2019\u2019 Electronics, vol. 8, no. 7, p. 742, 2019. [11] D. M. Casas-Velasco, O. M. C. Rendon, and N. L. da Fonseca, \u2018\u2018Intelligent routing based on reinforcement learning for software-defined networking,\u2019\u2019 IEEE Transactions on Network and ServiceManagement, vol. 18, no. 1, pp. 870\u2013881, 2020.\n[12] G. Kaur, P. Chanak, and M. Bhattacharya, \u2018\u2018Energy-efficient intelligent routing scheme for iot-enabled wsns,\u2019\u2019 IEEE Internet of Things Journal, vol. 8, no. 14, pp. 11 440\u201311 449, 2021. [13] D. K. Sah and T. Amgoth, \u2018\u2018Renewable energy harvesting schemes in wireless sensor networks: A survey,\u2019\u2019 Information Fusion, vol. 63, pp. 223\u2013 247, 2020. [14] S. Zeadally, F. K. Shaikh, A. Talpur, andQ. Z. Sheng, \u2018\u2018Design architectures for energy harvesting in the internet of things,\u2019\u2019 Renewable and Sustainable Energy Reviews, vol. 128, p. 109901, 2020. [15] H. Yuan, J. Bi, M. Zhou, Q. Liu, and A. C. Ammari, \u2018\u2018Biobjective task scheduling for distributed green data centers,\u2019\u2019 IEEE Transactions on Automation Science and Engineering, vol. 18, no. 2, pp. 731\u2013742, 2020. [16] A. Patil, S. Iyer, O. L. Lopez, R. J. Pandya, A. Kalla, and R. Kallimani, \u2018\u2018A comprehensive survey on spectrum sharing techniques for 5g/b5g intelligent wireless networks: Opportunities, challenges and future research directions,\u2019\u2019 arXiv preprint arXiv:2211.08956, 2022. [17] R. Zhu, G. Jianxin, W. Feng, B. Lin, and Y. Huang, \u2018\u2018Energy efficient transmission power control policy of the delay tolerable communication service,\u2019\u2019 IEEE Access, vol. 8, pp. 175 815\u2013175 826, 2020. [18] Z. Yang, Z. Ding, P. Fan, and G. K. Karagiannidis, \u2018\u2018On the performance of non-orthogonal multiple access systems with partial channel information,\u2019\u2019 IEEE Transactions on Communications, vol. 64, no. 2, pp. 654\u2013667, 2015. [19] C. He, Y. Hu, Y. Chen, and B. Zeng, \u2018\u2018Joint power allocation and channel assignment for noma with deep reinforcement learning,\u2019\u2019 IEEE Journal on Selected Areas in Communications, vol. 37, no. 10, pp. 2200\u20132210, 2019. [20] H. M. Al-Obiedollah, K. Cumanan, J. Thiyagalingam, A. G. Burr, Z. Ding, and O. A. Dobre, \u2018\u2018Energy efficient beamforming design for miso nonorthogonal multiple access systems,\u2019\u2019 IEEE Transactions on Communications, vol. 67, no. 6, pp. 4117\u20134131, 2019. [21] H. Al-Obiedollah, H. B. Salameh, S. Abdel-Razeq, A. Hayajneh, K. Cumanan, and Y. Jararweh, \u2018\u2018Energy-efficient opportunistic multicarrier noma-based resource allocation for beyond 5g (b5g) networks,\u2019\u2019 Simulation Modelling Practice and Theory, vol. 116, p. 102452, 2022. [22] Z. Chen, Z. Ding, P. Xu, and X. Dai, \u2018\u2018Optimal precoding for a qos optimization problem in two-user miso-noma downlink,\u2019\u2019 IEEE Communications Letters, vol. 20, no. 6, pp. 1263\u20131266, 2016. [23] J. Zhu, J. Wang, Y. Huang, S. He, X. You, and L. Yang, \u2018\u2018On optimal power allocation for downlink non-orthogonal multiple access systems,\u2019\u2019 IEEE Journal on Selected Areas in Communications, vol. 35, no. 12, pp. 2744\u2013 2757, 2017. [24] S. Boyd, S. P. Boyd, and L. Vandenberghe, Convex optimization. Cambridge university press, 2004. [25] S. Ghadimi, G. Lan, and H. Zhang, \u2018\u2018Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization,\u2019\u2019Mathematical Programming, vol. 155, no. 1-2, pp. 267\u2013305, 2016. [26] T. M. Moerland, J. Broekens, A. Plaat, and C. M. Jonker, \u2018\u2018Model-based reinforcement learning: A survey,\u2019\u2019 Foundations and Trends\u00ae in Machine Learning, vol. 16, no. 1, pp. 1\u2013118, 2023. [27] A. B. Adam, L. Lei, S. Chatzinotas, and N. U. R. Junejo, \u2018\u2018Deep convolutional self-attention network for energy-efficient power control in noma networks,\u2019\u2019 IEEE Transactions on Vehicular Technology, vol. 71, no. 5, pp. 5540\u20135545, 2022. [28] S. Jo, C. Jong, C. Pak, and H. Ri, \u2018\u2018Multi-agent deep reinforcement learning-based energy efficient power allocation in downlink mimo-noma systems,\u2019\u2019 IET Communications, vol. 15, no. 12, pp. 1642\u20131654, 2021. [29] R. Pal, N. Gupta, A. Prakash, R. Tripathi, and J. J. Rodrigues, \u2018\u2018Deep reinforcement learning based optimal channel selection for cognitive radio vehicular ad-hoc network,\u2019\u2019 IET Communications, vol. 14, no. 19, pp. 3464\u20133471, 2020. [30] X. Wang, Y. Zhang, R. Shen, Y. Xu, and F.-C. Zheng, \u2018\u2018Drl-based energyefficient resource allocation frameworks for uplink noma systems,\u2019\u2019 IEEE Internet of Things Journal, vol. 7, no. 8, pp. 7279\u20137294, 2020. [31] A. Alajmi and W. Ahsan, \u2018\u2018An efficient actor critic drl framework for resource allocation in multi-cell downlink noma,\u2019\u2019 in 2022 Joint European Conference on Networks and Communications & 6G Summit (EuCNC/6G Summit). IEEE, Conference Proceedings, pp. 77\u201382. [32] A. D. Cobb, B. Jalaian, N. D. Bastian, and S. Russell, \u2018\u2018Toward safe decision-making via uncertainty quantification in machine learning,\u2019\u2019 Systems Engineering and Artificial Intelligence, pp. 379\u2013399, 2021. [33] B. Kompa, J. Snoek, and A. L. Beam, \u2018\u2018Second opinion needed: communicating uncertainty in medical machine learning,\u2019\u2019 NPJ Digital Medicine, vol. 4, no. 1, p. 4, 2021.\n16 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[34] A. Adadi and M. Berrada, \u2018\u2018Peeking inside the black-box: a survey on explainable artificial intelligence (xai),\u2019\u2019 IEEE access, vol. 6, pp. 52 138\u2013 52 160, 2018. [35] U. Bhatt, A. Xiang, S. Sharma, A. Weller, A. Taly, Y. Jia, J. Ghosh, R. Puri, J. M. Moura, and P. Eckersley, \u2018\u2018Explainable machine learning in deployment,\u2019\u2019 in Proceedings of the 2020 conference on fairness, accountability, and transparency, Conference Proceedings, pp. 648\u2013657. [36] A. Madsen, S. Reddy, and S. Chandar, \u2018\u2018Post-hoc interpretability for neural nlp: A survey,\u2019\u2019 ACM Computing Surveys, vol. 55, no. 8, pp. 1\u201342, 2022. [37] D. V. Carvalho, E. M. Pereira, and J. S. Cardoso, \u2018\u2018Machine learning interpretability: A survey on methods and metrics,\u2019\u2019 Electronics, vol. 8, no. 8, p. 832, 2019. [38] G. Raja, S. Anbalagan, S. Senthilkumar, K. Dev, and N. M. F. Qureshi, \u2018\u2018Spas: Smart pothole-avoidance strategy for autonomous vehicles,\u2019\u2019 IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 10, pp. 19 827\u201319 836, 2022. [39] Q. Liu, Z. Liu, B. Xiong,W. Xu, and Y. Liu, \u2018\u2018Deep reinforcement learningbased safe interaction for industrial human-robot collaboration using intrinsic reward function,\u2019\u2019 Advanced Engineering Informatics, vol. 49, p. 101360, 2021. [40] H. Wang, Y. Ye, J. Zhang, and B. Xu, \u2018\u2018A comparative study of 13 deep reinforcement learning based energy management methods for a hybrid electric vehicle,\u2019\u2019 Energy, vol. 266, p. 126497, 2023.\nMENGMENG BAI was born in Shijiazhuang, Hebei province, China in 1996. She received the B.B.A degree in marketing from the University of Xijing, in 2020. And she became a master\u2019s student in electronic information at Xijing University in 2021. Her research interests include green communications and deep reinforcement learning. She received postgraduate scholarships from Xijing University in 2021 and 2022. And she won the second prize in the 19th China Post-Graduate\nMathematical Contest in Modeling in 2022.\nRUI ZHU received the B.S. and M.S. degrees in electronic engineering from the PLA University of Science and Technology, Nanjing, China, in 2002 and 2005,respectively. And he received the Ph.D. degree in electronic engineering from Tsinghua University, Beijing, China, in 2014. From 2005 to 2007, he was a Research Assistant with the Telecommunication Engineering Institute of the Air Force. From 2008 to 2010, he was a lecturer with the Telecommunication Engineering Institute\nof the Air Force. From 2014 to 2017, he was a lecturer with the Information and Navigation Institute, Air Force Engineering University. Since 2018, he has been an assistant professor with the Information Engineering School, Xijing University. He is the author of more than 40 articles,. His research interests include green communication, cognitive radio network and wireless resource allocation.\nJIANXIN GUO received the B. S. degree in Communications Engineering from Telecommunications Engineering Institute of the Air Force, Xi\u2019an, China, in 1997, the M. S. degree in Information and Communications Engineering from Air Force Engineering University, Xi\u2019an, China, in 2000, and the Ph.D. degree in Information and Communications Engineering from the PLA Information Engineering University, Zheng Zhou, China, in 2004. He is now a Professor at the School of Information\nEngineering, Xijing University in Xi\u2019an China. His current research interests include cognitive radio technologies, MIMO-OFDM wireless communications and their implementation in GNU radio.\nFENG WANG received the B. S. degree in Communications Engineering from Air Force Engineering University (AFEU), Xi\u2019an, China, in 2001, and the Ph.D. degree in Information and Communications Engineering from the Xidian University Xi\u2019an, China, in 2016. From 2004 to 2018, hewas a lecturer with theAFEU. Since 2018, he has been an assistant professor with Information Engineering School, Xijing University. His research interests include compressed sensing, wireless localization\nand optimization theory.\nLIPING WANG received the M.S. degree in Electronic Science and technology form Xi\u2019an Shiyou University, China, in 2003. She is an assistant professor in the school of information engineering , Xijing University. Her research area is system automatic test and control, mainly including multisource sensor data acquisition, signal processing and system closed-loop control research. Facility: School of Information Engineering, Xijing University, Xi\u2019an, 710123, China. E-mail: wanglip-\ning@xijing.edu.cn.\nHANGJIE ZHU was born in Shengzhou, Zhejiang province, China in 1998. She received the B.E. degree in Electronic Information Engineering from the Jiangxi science and technology normal university, in 2021. And he became a master\u2019s student in electronic information at Xijing University in 2021. His research interests include communication signal modulation recognition and deep learning. He received postgraduate scholarships from Xijing University in 2021 and 2022. And he won\nthe second prize in the 19th China Post-Graduate Mathematical Contest in Modeling in 2022.\nVOLUME 11, 2023 17\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nLEI HUANG received the B.E. degree from Jiaxing Nanhu University, Jiaxing, Zhejiang, China, in 2021. He is currently pursuing the M.E. degree at Xijing University, Xi\u2019an, China. His current research interests include deep learning and computer vision.\nYUSHUAI ZHANG was born in Yan\u2019an, Shaanxi province, China in 1998. he received the B.S. degree in Electronic Information Engineering from Xijing University, in 2019. he received the M.S.degree in Control Engineering from Xijing University, in 2021. And he became a doctoral student in Disaster Prevention and Reduction Engineering and Protective Engineering at Institute of Defense Engineering, AMS, PLA, in 2021. His research interests include operations research.\n18 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "Energy-Efficient Transmission Strategy for Delay Tolerable Services in NOMA-Based Downlink with Two Users",
    "year": 2023
}