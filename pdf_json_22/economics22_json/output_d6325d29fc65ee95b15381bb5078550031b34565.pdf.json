{
    "abstractText": "We propose Variational Heteroscedastic Volatility Model (VHVM) an end-to-end neural network architecture capable of modelling heteroscedastic behaviour in multivariate financial time series. VHVM leverages recent advances in several areas of deep learning, namely sequential modelling and representation learning, to model complex temporal dynamics between different asset returns. At its core, VHVM consists of a variational autoencoder to capture relationships between assets, and a recurrent neural network to model the time-evolution of these dependencies. The outputs of VHVM are time-varying conditional volatilities in the form of covariance matrices. We demonstrate the effectiveness of VHVM against existing methods such as Generalised AutoRegressive Conditional Heteroscedasticity (GARCH) and Stochastic Volatility (SV) models on a wide range of multivariate foreign currency (FX) datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zexuan Yin"
        },
        {
            "affiliations": [],
            "name": "Danial Saef"
        },
        {
            "affiliations": [],
            "name": "Paolo Barucca"
        }
    ],
    "id": "SP:7e4d587fe044ff350529ee3ab0bd2f6d7f9047cb",
    "references": [
        {
            "authors": [
                "O. Aguilar",
                "M. West"
            ],
            "title": "Bayesian dynamic factor models and portfolio allocation",
            "venue": "Journal of Business and Economic Statistics,",
            "year": 2000
        },
        {
            "authors": [
                "A. Aydemir"
            ],
            "title": "Volatility Modelling in Finance, 1998 (Butterworth-Heinemann: Oxford)",
            "year": 1998
        },
        {
            "authors": [
                "K. Bandara",
                "P. Shi",
                "C. Bergmeir",
                "H. Hewamalage",
                "Q. Tran",
                "B. Seaman",
                "Sales"
            ],
            "title": "Demand Forecast in E-commerce Using a Long Short-Term Memory Neural Network Methodology",
            "venue": "In Proceedings of the Neural Information Processing,",
            "year": 2019
        },
        {
            "authors": [
                "L. Bauwens",
                "S. Laurent",
                "J.V. Rombouts"
            ],
            "title": "Multivariate GARCH models: A survey",
            "venue": "Journal of Applied Econometrics,",
            "year": 2006
        },
        {
            "authors": [
                "J. Bayer",
                "C. Osendorfer"
            ],
            "title": "Learning Stochastic Recurrent Networks",
            "venue": "arXiv preprint,",
            "year": 2014
        },
        {
            "authors": [
                "D.M. Blei",
                "A. Kucukelbir",
                "J.D. McAuliffe"
            ],
            "title": "Variational Inference: A Review for Statisticians",
            "venue": "Journal of the American Statistical Association,",
            "year": 2017
        },
        {
            "authors": [
                "T. Bollerslev"
            ],
            "title": "Generalised Autoregressive Conditional Heteroskedasticity",
            "venue": "Journal of Econometrics,",
            "year": 1986
        },
        {
            "authors": [
                "T. Bollerslev",
                "R.F. Engle",
                "J.M. Wooldridge"
            ],
            "title": "A Capital Asset Pricing Model with Time-Varying Covariances",
            "venue": "Journal of Political Economy,",
            "year": 1988
        },
        {
            "authors": [
                "J.H. B\u00f6se",
                "V. Flunkert",
                "J. Gasthaus",
                "T. Januschowski",
                "D. Lange",
                "D. Salinas",
                "S. Schelter",
                "M. Seeger",
                "Y. Wang"
            ],
            "title": "Probabilistic Demand Forecasting at Scale",
            "venue": "Proc. VLDB Endow., 2017,",
            "year": 2017
        },
        {
            "authors": [
                "J.C. Chan",
                "A.L. Grant"
            ],
            "title": "Modeling energy price dynamics: GARCH versus stochastic volatility",
            "venue": "Energy Economics, 2016,",
            "year": 2016
        },
        {
            "authors": [
                "K. Cho",
                "B. van Merrienboer",
                "D. Bahdanau",
                "Y. Bengio"
            ],
            "title": "On the Properties of Neural Machine Translation: Encoder\u2013Decoder Approaches",
            "venue": "In Proceedings of the In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8),",
            "year": 2014
        },
        {
            "authors": [
                "G.A. Christodoulakis",
                "S.E. Satchell"
            ],
            "title": "Correlated ARCH (CorrARCH): Modelling the time-varying conditional correlation between financial asset returns",
            "venue": "European Journal of Operational Research,",
            "year": 2002
        },
        {
            "authors": [
                "P.F. Christoffersen",
                "F.X. Diebold"
            ],
            "title": "How Relevant is Volatility Forecasting for Financial Risk Management ",
            "venue": "The Review of Economics and Statistics,",
            "year": 2000
        },
        {
            "authors": [
                "J. Chu",
                "S. Chan",
                "S. Nadarajah",
                "J. Osterrieder"
            ],
            "title": "GARCH Modelling of Cryptocurrencies",
            "venue": "Journal of Risk and Financial Management,",
            "year": 2017
        },
        {
            "authors": [
                "J. Chung",
                "K. Kastner",
                "L. Dinh",
                "K. Goel",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "A recurrent latent variable model for sequential data",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "E. Denton",
                "R. Fergus"
            ],
            "title": "Stochastic Video Generation with a Learned Prior",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning, ICML 2018,",
            "year": 2018
        },
        {
            "authors": [
                "F.X. Diebold",
                "M. Nerlove"
            ],
            "title": "The dynamics of exchange rate volatility: A multivariate latent factor ARCH model",
            "venue": "Journal of Applied Econometrics,",
            "year": 1989
        },
        {
            "authors": [
                "G. Dorta",
                "S. Vicente",
                "L. Agapito",
                "N.D. Campbell",
                "I. Simpson"
            ],
            "title": "Structured Uncertainty Prediction Networks",
            "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "X. Du",
                "C.L. Yu",
                "D.J. Hayes"
            ],
            "title": "Speculation and volatility spillover in the crude oil and agricultural commodity markets: A Bayesian analysis",
            "venue": "Energy Economics,",
            "year": 2011
        },
        {
            "authors": [
                "R. Engle"
            ],
            "title": "Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation",
            "venue": "Econometrica, 1982,",
            "year": 1982
        },
        {
            "authors": [
                "R. Engle"
            ],
            "title": "Dynamic Conditional Correlation",
            "venue": "Journal of Business and Economic Statistics,",
            "year": 2002
        },
        {
            "authors": [
                "R. Engle",
                "K. Kroner"
            ],
            "title": "Multivariate Simultaneous Generalized Arch",
            "venue": "Econometric Theory, 1995,",
            "year": 1995
        },
        {
            "authors": [
                "R.F. Engle",
                "A.J. Patton"
            ],
            "title": "What good is a volatility model",
            "venue": "Forecasting Volatility in the Financial Markets,",
            "year": 2007
        },
        {
            "authors": [
                "M. Escobar-Anel",
                "M. Gollart",
                "R. Zagst"
            ],
            "title": "Closed-form portfolio optimization under GARCH models",
            "venue": "Operations Research Perspectives, 2022,",
            "year": 2022
        },
        {
            "authors": [
                "O. Fabius",
                "J.R. van Amersfoort"
            ],
            "title": "Variational recurrent auto-encoders",
            "venue": "3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings,",
            "year": 2015
        },
        {
            "authors": [
                "Fama",
                "E.F"
            ],
            "title": "The Behavior of Stock-Market Prices Author",
            "venue": "Eugene F . Fama Published by : The University of Chicago Press Stable. The Journal of Business,",
            "year": 1965
        },
        {
            "authors": [
                "M. Fraccaro",
                "S.K. S\u00f8nderby",
                "U. Paquet",
                "O. Winther"
            ],
            "title": "Sequential neural models with stochastic layers",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "J.Y. Franceschi",
                "E. Delasalles",
                "M. Chen",
                "S. Lamprier",
                "P. Gallinari"
            ],
            "title": "Stochastic latent residual video prediction",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "A. Galanos"
            ],
            "title": "Chapter title. rmgarch: Multivariate GARCH models",
            "venue": "R package version",
            "year": 2022
        },
        {
            "authors": [
                "A. Galanos",
                "T. Kley"
            ],
            "title": "Chapter title. rugarch: Univariate GARCH Models R package version",
            "year": 2022
        },
        {
            "authors": [
                "L.R. Glosten",
                "R. Jagannathan",
                "D.E. Runkle"
            ],
            "title": "On the Relation between the Expected Value and the Volatility of the Nominal Excess",
            "venue": "Return on Stocks. The Journal of Finance,",
            "year": 1993
        },
        {
            "authors": [
                "P.R. Hansen",
                "A. Lunde"
            ],
            "title": "A forecast comparison of volatility models: Does anything beat a GARCH(1,1)",
            "venue": "Journal of Applied Econometrics,",
            "year": 2005
        },
        {
            "authors": [
                "S.A. Hassan",
                "F. Malik"
            ],
            "title": "Multivariate GARCH modeling of sector volatility transmission",
            "venue": "Quarterly Review of Economics and Finance,",
            "year": 2007
        },
        {
            "authors": [
                "D. Hosszejni",
                "G. Kastner"
            ],
            "title": "Modeling Univariate and Multivariate Stochastic Volatility in R with stochvol and factorstochvol",
            "venue": "Journal of Statistical Software,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Huang",
                "W. Su",
                "X. Li"
            ],
            "title": "Comparison of BEKK GARCH and DCC GARCH Models: An Empirical Study",
            "venue": "In Proceedings of the Advanced Data Mining and Applications,",
            "year": 2010
        },
        {
            "authors": [
                "H. Ismail Fawaz",
                "G. Forestier",
                "J. Weber",
                "L. Idoumghar",
                "P.A. Muller"
            ],
            "title": "Deep learning for time series classification: a review",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2019
        },
        {
            "authors": [
                "E. Jacouier",
                "N.G. Polson",
                "P.E. Rossl"
            ],
            "title": "Bayesian analysis of stochastic volatility models",
            "venue": "Journal of Business and Economic Statistics,",
            "year": 1994
        },
        {
            "authors": [
                "K. Jebran",
                "S. Chen",
                "I. Ullah",
                "S.S. Mirza"
            ],
            "title": "Does volatility spillover among stock markets varies from normal to turbulent periods? Evidence from emerging markets of Asia",
            "venue": "Journal of Finance and Data Science,",
            "year": 2017
        },
        {
            "authors": [
                "M. Karl",
                "M. Soelch",
                "J. Bayer",
                "P. Van Der Smagt"
            ],
            "title": "Deep variational Bayes filters: Unsupervised learning of state space models from raw data",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, ii,",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In Proceedings of the 2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings,",
            "year": 2014
        },
        {
            "authors": [
                "R.G. Krishnan",
                "U. Shalit",
                "D. Sontag"
            ],
            "title": "Structured inference networks for nonlinear state space models",
            "venue": "In Proceedings of the 31st AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Li",
                "R. Yu",
                "C. Shahabi",
                "Y. Liu"
            ],
            "title": "Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting",
            "venue": "In Proceedings of the 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "H.V. Long",
                "H.B. Jebreen",
                "I. Dassios",
                "D. Baleanu"
            ],
            "title": "On the statistical garch model for managing the risk by employing a fat-tailed distribution in finance",
            "venue": "Symmetry, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A.K. Malik"
            ],
            "title": "European exchange rate volatility dynamics: An empirical investigation",
            "venue": "Journal of Empirical Finance,",
            "year": 2005
        },
        {
            "authors": [
                "D. Nelson"
            ],
            "title": "Conditional Heteroskedasticity in Asset Returns",
            "venue": "A New Approach. Econometrica,",
            "year": 1991
        },
        {
            "authors": [
                "M.K. Pitt",
                "Shephard"
            ],
            "title": "N.In Time varying covariances: a factor stochastic volatility approach, pp. 547\u2013570 ((edited by j.m. bernardo, j.o",
            "venue": "berger, a.p. dawid and a.f.m smith) edn),",
            "year": 1999
        },
        {
            "authors": [
                "K. Platanioti",
                "E. McCoy",
                "D. Stephens"
            ],
            "title": "A Review of Stochastic Volatility: Univariate and Multivariate Models [online",
            "year": 2005
        },
        {
            "authors": [
                "V. Rankovi\u0107",
                "M. Drenovak",
                "B. Urosevic",
                "R. Jelic"
            ],
            "title": "Mean-univariate GARCH VaR portfolio optimization: Actual portfolio approach",
            "venue": "Computers and Operations Research,",
            "year": 2016
        },
        {
            "authors": [
                "G. Schwert"
            ],
            "title": "Why does stock market volatility change with time",
            "venue": "The Journal of Finance,",
            "year": 1989
        },
        {
            "authors": [
                "Y. Shapovalova"
            ],
            "title": "Exact and approximate methods for bayesian inference: stochastic volatility case study",
            "year": 2021
        },
        {
            "authors": [
                "Y.K. Tse",
                "A.K. Tsui"
            ],
            "title": "A multivariate generalized autoregressive conditional heteroscedasticity model with time-varying correlations",
            "venue": "Journal of Business and Economic Statistics,",
            "year": 2002
        },
        {
            "authors": [
                "R. Van Der Weide"
            ],
            "title": "GO-GARCH: A multivariate generalized orthogonal GARCH model",
            "venue": "Journal of Applied Econometrics,",
            "year": 2002
        },
        {
            "authors": [
                "Y. Wu",
                "J.M.H. Lobato",
                "Z. Ghahramani"
            ],
            "title": "Dynamic covariance models for multivariate financial time series",
            "venue": "In Proceedings of the 30th International Conference on Machine Learning, ICML 2013,",
            "year": 2013
        },
        {
            "authors": [
                "Z. Yin",
                "P. Barucca"
            ],
            "title": "Neural Generalised AutoRegressive Conditional Heteroskedasticity",
            "venue": "arXiv preprint,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Variational Heteroscedastic Volatility Model\nZexuan Yin\u2217\u2020, Danial Saef\u2021 and Paolo Barucca\u2020\n\u2020Department of Computer Science, University College London, WC1E 7JE, United Kingdom \u2021International Research Training Group, Humboldt Universita\u0308t zu Berlin, Group 1792,\nSpandauer Str.1, 10178 Berlin, Germany \u2020p.barucca@ucl.ac.uk, \u2021danial.saef@hu-berlin.de\n(v1.1 released April 2022)\nWe propose Variational Heteroscedastic Volatility Model (VHVM) - an end-to-end neural network architecture capable of modelling heteroscedastic behaviour in multivariate financial time series. VHVM leverages recent advances in several areas of deep learning, namely sequential modelling and representation learning, to model complex temporal dynamics between different asset returns. At its core, VHVM consists of a variational autoencoder to capture relationships between assets, and a recurrent neural network to model the time-evolution of these dependencies. The outputs of VHVM are time-varying conditional volatilities in the form of covariance matrices. We demonstrate the effectiveness of VHVM against existing methods such as Generalised AutoRegressive Conditional Heteroscedasticity (GARCH) and Stochastic Volatility (SV) models on a wide range of multivariate foreign currency (FX) datasets.\nKeywords: Multivariate heteroscedasticity; Recurrent neural networks; Variational inference; Volatility forecasting\nJEL Classification: C32, C45, C53,"
        },
        {
            "heading": "1. Introduction",
            "text": "Financial time series is known to exhibit heteroscedastic behaviour - time-varying conditional volatility (Engle and Patton 2007, Poon and Granger 2003). Being able to model and predict this behaviour is of practical importance to professionals in finance for the purpose of risk management (Christoffersen and Diebold 2000, Long et al. 2020), derivative pricing (J.Duan 1995), and portfolio optimisation (Rankovic\u0301 et al. 2016, Escobar-Anel et al. 2022).\nIt is well documented in literature that (conditional) volatility is forecastable on hourly or daily frequencies (Christoffersen and Diebold 2000). On a univariate level, this involves predicting time-varying variances of asset returns; this predictability can be attributed to the so-called volatility clustering phenomenon: large (small) changes in asset price are often followed by further large(small) changes (Engle and Patton 2007, Fama 1965, Schwert 1989). On a multivariate level (involving a portfolio of assets), volatility forecasting involves estimating conditional covariances between asset pairs in addition to conditional variances of the assets. One could argue that some of this predictability comes from the so-called spillover effect: the transfer of shock between different financial markets (Jebran et al. 2017, Hassan and Malik 2007, Du et al. 2011). An effective multivariate volatility model therefore needs to capture both intra and inter-time series dynamics.\nTraditional volatility forecasting models can be divided into two main categories: Generalised AutoRegressive Conditional Hertoscedasitcity (GARCH) (Bollerslev 1986, Nelson 1991, Glosten\n\u2217Corresponding author. Email: zexuan.yin.20@ucl.ac.uk 1\nar X\niv :2\n20 4.\n05 80\n6v 1\n[ q-\nfi n.\nST ]\n1 1\nA pr\n2 02\n2\net al. 1993) and Stochastic Volatility (SV) (Jacouier et al. 1994, Chan and Grant 2016) models. The two competing classes of models rely on different underlying assumptions (Luo et al. 2018). GARCH models describe a deterministic relationship between future conditional volatility and past conditional volatility and squared returns. SV models assumes that conditional volatility follows a latent autoregressive process. Although there is no general consensus that GARCH is always superior to SV (or vice versa), there is some evidence that SV models are more flexible in modelling the characteristics of asset returns (Chan and Grant 2016, Shapovalova 2021). Nonetheless, the popularity of GARCH models seems to surpass that of SV models due to several reasons. Firstly, GARCH models are easier to fit than SV models. The parameters of GARCH are obtained using maximum likelihood estimation, whereas for SV models one needs to obtain samples from an intractible posterior distribution using methods such as Markov chain Monte Carlo (MCMC), which works well when the number of parameters is small, however the convergence can be slow in larger models (Shapovalova 2021). Secondly, there is an abundance of open-source software (packages) for GARCH models, such as fGarch (Wuertz et al. 2017) and rugarch (Galanos and Kley 2022) in the programming language R, and arch (Sheppard et al. 2022) in Python. For SV models however, there was no go-to package for model estimation until the release of stochvol and factorstochvol (Hosszejni and Kastner 2021) in R.\nIt is worth mentioning that GARCH models too suffer from the curse of dimensionality. For a portfolio of n assets, the computational complexity of GARCH models scales with O(n5), which makes it impossible fit to beyond a portfolio of roughly 5 assets (Wu et al. 2013).\nIn recent years, the application of deep learning models for time series forecasting has achieved state of the art performances in many domains (Bandara et al. 2019, Bo\u0308se et al. 2017, Li et al. 2018). Since our observations consist of multiple asset returns time series, a natural research direction would be to investigate whether deep learning models can capture complex dependencies between different assets across time. There are two main obstacles in this task. Firstly, the conditional volatility (covariance matrix) is a latent variable and must be inferred using observational data (Luo et al. 2018). Secondly, for a matrix to be a valid covariance matrix, it must be symmetric and positive definite (Engle and Kroner 1995). How to impose these constraints on a neural network such that its outputs are valid covariance matrices is a challenging task.\nTo tackle the first challenge, we adopt a recent trend that combines a variational autoencoder (VAE) and a recurrent neural network (RNN) (a VRNN) to allow efficient structured inference over a sequence of continuous latent random variables (Chung et al. 2015, Bayer and Osendorfer 2014, Krishnan et al. 2017, Fabius and van Amersfoort 2015, Fraccaro et al. 2016, Karl et al. 2017). The use of a VAE (and hence variational inference) translates posterior approximation into an optimisation task which can be solved using a neural network trained with stochastic gradient descent. The use of an RNN allows information from previous steps to be used in the modelling and forecasting of the latent variable in future steps. In fact, this promising framework has been explored in Yin and Barucca (2022) and Luo et al. (2018). In Yin and Barucca (2022), the authors proposed a neural network adaptation of the GARCH model which showed improved performance over traditional GARCH models. However, this model also suffers from the curse of dimensionality as it is still a GARCH model by design. Since the focus of this paper is on multivariate volatility, not being able to scale up to higher dimensions (beyond at least 5) is a big limitation. The authors in Luo et al. (2018) proposed a purely data driven approach to volatility forecasting under the VRNN framework which we used as a baseline model in our results section.\nTo tackle the second challenge, one possible approach is to combine traditional econometrics models with deep learning models. Traditional econometrics models have well understood statistical properties and neural networks can be used to enhance the predictive power of the model. In Yin and Barucca (2022), the authors use a neural network to parameterise the time-varying coefficients of the BEKK(1,1) model, which is a multivariate GARCH model proposed by Engle and Kroner (1995). The BEKK model produces symmetric and positive definite covariance matrices by design and therefore no other constraints need to be applied to the neural network output. As mentioned previously however, many econometric models suffer from curse of dimensionality. Instead, we\nfollow the approach in Dorta et al. (2018) and design our neural network such that it outputs the Cholesky decomposition of the precision matrix (inverse of the covariance matrix) at every time step. We will show later on how this guarantees symmetry and positive definiteness. (Engle and Kroner 1995)\nOur main contribution is therefore an end-to-end neural network architecture capable of forecasting valid covariance matrices. We show in the results section that VHVM consistently outperforms existing GARCH and SV baselines on a range of multivariate FX portfolios.\nThe rest of the paper is structured as follows: in section 2 we present a summary of the field of volatility forecasting and introduce some of the popular models; also discuss how they are related to our proposed model. In section 3 we formally introduce VHVM: its generative and inference components, and model training and forecasting. In section 4 we outline the experiments and data used to show the effectiveness of VHVM against existing baselines from both traditional econometrics and the field of deep learning."
        },
        {
            "heading": "2. Related Work",
            "text": "For a financial asset with price St at time t, its log returns are computed as rt = log(St)\u2212log(St\u22121). The returns process rt can be assumed to have a conditional mean E[rt|It\u22121] = 0 and a conditional variance E[rt2|It\u22121] = \u03c3t2, in other words rt|It\u22121 \u223c N (0, \u03c32t ). The information set It describes all relevant available at time t: It = {r1:t, \u03c31:t2}. The time variation of conditional variance \u03c3 is known as heteroscedasticity and the aim of volatility forecasting is to model this behaviour (Engle and Kroner 1995).\n2.1. Generalise Autoregressive Conditional Heteroscedasticity\nARCH (Engle 1982) and GARCH (Bollerslev 1986) models have been dominating the field of volatility forecasting since the late 1900s due to their simple model form, explainability, and ease of estimation. Many GARCH model variants have since been proposed to account for well-known stylised facts about financial time series such as volatility clustering and leverage effect (Engle and Patton 2007). The EGARCH (Nelson 1991) and GJR-GARCH (Glosten et al. 1993) models for example, were designed to specifically accommodate the leverage effect. The most general GARCH(p,q) model proposed by Bollerslev (1986) describes a deterministic relationship between future conditional volatility, past conditional volatility and squared returns:\n\u03c32t = \u03c9 + p\u2211 i=1 \u03b1ir 2 t\u2212i + q\u2211 j=1 \u03b2j\u03c3 2 t\u2212j , (1)\nwhere p and q are lag orders of the ARCH and GARCH terms, under which the returns process rt has an unconditional mean E[rt] = 0 and unconditional variance E[r2t ] = \u03c91\u2212\u03b1\u2212\u03b2 .\nIn total, there are many hundreds of GARCH model variants, however there exists little consensus on when to use which GARCH models as their performances tend to vary with the nature and behaviour of the time series being modelled, for example, the leverage effect is frequently observed in stock returns but rarely seen in foreign exchange currency returns (Engle and Patton 2007). In Hansen and Lunde (2005) the authors compared the performances of 330 GARCH variants on Deutsche Mark-US Dollar exchange rates and IBM returns and found that the GARCH(1,1) was not outperformed by any other model in the foreign exchange analysis. In the IBM stock returns analysis however, the authors found that GARCH(1,1) was inferior to models that explicitly accounted for the leverage effect. It has thus become common practice to explore various GARCH variants for the same task (Chan and Grant 2016, Chu et al. 2017, Malik 2005).\nFor a portfolio of assets, models tend to be multivariate generalisations of the univariate GARCH\nmodel. In additional to modelling conditional variances for each asset, one also needs to model time varying covariances between different asset pairs. The output for a multivariate GARCH model is a time-varying covariance matrix which describes the instantaneous intra and inter-asset relationships. Notable examples of multivariate GARCH models include the VEC model (Bollerslev et al. 1988), the BEKK model (Engle and Kroner 1995), the GO-GARCH model (Van Der Weide 2002), and the DCC-GARCH model (Christodoulakis and Satchell 2002, Tse and Tsui 2002, Engle 2002). For our analysis, we used the DCC-GARCH (dynamic conditional correlation) model as a multivariate GARCH baseline. The key difference between DCC-GARCH and BEKK (a popular multivariate GARCH) is that BEKK assumes constant conditional correlation between assets, i.e. the change in the covariance between two assets with time is due to the changes in the two variances (but the conditional correlation is constant) (Huang et al. 2010). The constant conditional correlation (CCC) assumption is rather crude since during different market regimes one would expect the correlation between assets to vary. The DCC-GARCH is a generalisation of a CCCGARCH that accounts for dynamic correlation. During the estimation procedure, various univariate GARCH models are fit for the assets, followed by estimations of the parameters for conditional correlation.\nWhen fitting a multivariate GARCH model under the assumption of normal innovations (rt \u223c N (0,\u03a3t)), we seek to maximise the multivariate normal log likelihood function (Bauwens et al. 2006):\nL(\u03b8) = \u22121 2 T\u2211 t=1 (log|\u03a3t|+ rTt \u03a3\u22121t rt), (2)\nwhich becomes computationally expensive in higher dimensions since we are required (for a portfolio of n assets) to invert an n\u00d7n covariance matrix \u03a3t for every time step. One solution to alleviate this burden is to directly work with the precision matrix (inverse covariance matrix) instead: P = \u03a3\u22121. Setting a neural network output to be a precision matrix rather than a covariance matrix allows us to compute the log likelihood straightaway; hence bypassing the expensive matrix inversion step during model training. When the actual covariance matrix is required during the testing phase, one could simply invert the precision matrix to obtain the covariance matrix (Luo et al. 2018, Dorta et al. 2018).\n2.2. Stochastic Volatility\nStochastic Volatility is an alternative class of models that rely on assumption that the log conditional variance follows a non-deterministic autoregressive AR(p) (usually p = 1) process (Shapovalova 2021):\nln\u03c32t+1 = \u00b5+ \u03c6ln\u03c3 2 t + \u03c3\u03b7\u03b7t+1, (3)\nwhere \u03b7t \u223c N(0, 1) describes the innovation of the log variance process. For the rest of the section we refer the log volatility ln\u03c32t as ht such that rt = exp(ht/2) t where t \u223c N(0, 1).\nIn a multivariate setting, we seek to simultaneously model the volatility movements of a group of assets (Platanioti et al. 2005). Related movements between different asset classes, financial markets or exchange rates are often observed due to them being infuenced by common unobserved drivers (or factors) (Aydemir 1998). Diebold and Nerlove (1989) investigated the behaviour of seven dollar exchange rates for a period of 12 years and found that the seven series showed similarities in volatility behaviour in response to actions taken by the US government such as intervention efforts. Since stochastic volatility models are defined in terms of the log volatility process, it is harder to generalise a univariate model to its multivariate counterpart than a GARCH model (Platanioti\net al. 2005). In this paper we take as baseline the factor model independently proposed by Pitt and Shephard (1999) and Aguilar and West (2000). An open-source package (factorstochvol) was developed in the programming language R by Hosszejni and Kastner (2021) which we used to run the baseline SV model in our analysis. The factor volatility model (Hosszejni and Kastner 2021) for a portfolio of n assets assumes m latent common factors where m < n. We have that:\nrt|f t \u223c N (\u039bf t, \u03a3\u0304t),\nf t \u223c N (0, \u03a3\u030ct), (4)\nwhere f t = (ft1, ..., ftm) T is the vector of m factors, \u039b \u2208 Rn\u00d7m is a matrix of factor loadings.\nThe covariance matrices \u03a3\u0304t and \u03a3\u030ct are diagonal and are defined as:\n\u03a3\u0304t = diag(exp(h\u0304t1), ..., exp(h\u0304tn)),\n\u03a3\u030ct = diag(exp(h\u030ct1), ..., exp(h\u030ctm)), (5)\nwhere h\u0304 and h\u030c are the log variances of the n assets and m latent factors defined as follows (the AR(1) process given in (3)):\nh\u0304ti \u223c N (\u00b5\u0304i + \u03c6\u0304i(h\u0304t\u22121,i), \u03c3\u03042i ), i = 1, ..., n,\nh\u030ctj \u223c N (\u00b5\u030cj + \u03c6\u0304j(h\u030ct\u22121,j), \u03c3\u030c2j ), j = 1, ...,m. (6)\nGiven the above, the multivariate returns process follows a 0 mean multivariate normal distribution with rt \u223c N (0,\u03a3t), where \u03a3t = \u039b\u03a3\u030ct\u039bT + \u03a3\u0304t. We see that the factor volatility model in (4) is by nature a state space model with a random walk latent transition process and a linear emission process rt|f t. We will show later on that our end-to-end neural network architecture follows the same theoretical framework: a neural network (an RNN) that models the non-linear latent transition process f t|f t\u22121, a neural network (VAE) that infers the latent factors from observational data f t|r1:t, and a neural work (multilayer perceptron (MLP)) that parameterises the emission distribution rt|f t.\n2.3. Amortised Variational Inference\nFor a state space model with latent variable zt and observations rt, we are interested in the posterior distribution P (zt|r1:t) - also known as the filtering distribution in time series literature. Note that in the previous section we denoted the latent factors of a SV model as f t as this is a common choice of notation in SV literature. From this section onwards we will use zt instead of f t since zt is more commonly used to represent latent variables in machine learning literature.\nThe variational autoencoder (VAE) (Kingma and Welling 2014) is a neural network architecture trained with stochastic gradient descent. VAE consists of an encoder neural network that parameterises the posterior distribution P (z|r), and a decoder neural network that parameterises the emission distribution P (r|z). Here we have emitted the subscript t since VAEs were traditionally designed to work in a static setting and has been used extensively as generative models in computer vision (Dorta et al. 2018). Our aim is maximise the marginal log likelihood logP\u03b8(r) where the latent variable z has been integrated out and \u03b8 represents the model parameters we are optimising over. This task however involves an intractable integral.\nlogP\u03b8(r) = log \u222b P\u03b8(r|z)P\u03b8(z) dz. (7)\nWith variational inference (Blei et al. 2017), we use a much simpler distribution q\u03c6(z|r) to approximate the actual intractable posterior P\u03b8(z|r). We can express the marginal log likelihood in terms of an lower bound - evidence lower bound (or ELBO) - and the Kullback Leibler divergence between our variational approximation q\u03c6(z|r) and actual posterior P\u03b8(z|r):\nlogP\u03b8(r) = ELBO(\u03b8, \u03c6) +KL(q\u03c6(z|r)||P\u03b8(z|r)); (8)\nsince logP\u03b8(r) depends only on \u03b8, minimisingKL(q\u03c6(z|r)||P\u03b8(z|r)) with respect to \u03c6 is equivalent to maximising ELBO(\u03b8, \u03c6) w.r.t. \u03c6. Mximising ELBO(\u03b8, \u03c6) w.r.t. \u03b8 corresponds to maximising logP\u03b8(r).\nIn a variational autoencoder (Kingma and Welling 2014) the ELBO is expressed as:\nELBO(\u03b8, \u03c6) = Ez\u223cq\u03c6(z|r)[logP\u03b8(r|z)]\u2212KL(q\u03c6(z|r)||P\u03b8(z)), (9)\nwhere |P\u03b8(z) is the prior distribution over z, which is usually set to an uninformative N (0, I). We seek to maximise the ELBO(\u03b8, \u03c6) using the encoder neural network q\u03c6(z|r) and decoder neural network P\u03b8(z|r):\n{\u03b8\u2217, \u03c6\u2217} = argmax \u03b8,\u03c6 ELBO(\u03b8, \u03c6). (10)\nWe follow a recent trend which adapts a VAE from a static model to a sequential model (Chung et al. 2015, Bayer and Osendorfer 2014, Krishnan et al. 2017, Fabius and van Amersfoort 2015, Fraccaro et al. 2016, Karl et al. 2017). This usually involves using another neural network to parameterise a learned prior distribution P\u03b8(zt|zt\u22121) to replace the uninformative prior N (0, I). This learned prior describes the latent transition dynamics of the state space model, similar to f t of the stochastic volatility model in (4) but conditioned on f t\u22121 as opposed to a random walk. The decoder neural network parameterises P\u03b8(rt|zt), which corresponds to the emission mechanism rt|f t in (4). The posterior approximated by the encoder netowrk becomes P\u03b8(zt|r1:t) as opposed to P\u03b8(z|r) in a static setting. This requires a sequence to serve as input into an inference model, which is carried by an RNN since the hidden state ht is a summary of the sequence r1:t.\nThe VAE-RNN (or VRNN) framework has achieved state of the art performance in sequence modelling tasks such as video prediction (Franceschi et al. 2020, Denton and Fergus 2018); hence we also leveraged this framework to design our volatility model."
        },
        {
            "heading": "3. Materials and Methods",
            "text": "3.1. Covariance Matrix Parameterisation\nA covariance matrix is required to be both symmetric and positive definite (Engle and Kroner 1995). Under the assumption that the returns time series follows a multivariate normal distribution rt \u223c N (0,\u03a3t), we wish to evaluate the log determinant log|\u03a3t| and Mahalanobis distance rTt \u03a3\u22121t rt from the log likelihood (2). We follow the parameterisation scheme in Dorta et al. (2018) and perform a Cholesky decomposition on the precision matrix:\nP t = \u03a3 \u22121 t = LtL T t , (11)\nwhich ensures P t is symmetric by construction. To ensure positive definiteness, we require that the diagonal entries of Lt to be strictly positive; this could achieved by applying a Softplus function on top of the neural network output. We see from the log likelihood (2) that the covariance matrix needs to be inverted before evaluating the Mahalanobis distance; this process is costly at higher dimensions. Working with the precision matrix allows us to bypass the inversion during model training as the Mahalanobis distance is simply rTt LtL T t rt. To evaluate the log determinant, we\nhave log|\u03a3t| = \u22122 \u2211n i=1 log(lii,t), where lii,t is the i th element in the diagonal of Lt. Under this scheme, for a portfolio of n assets, the output of our neural network is simply a vector of size n(n+1)2 (denoted zt thereon). We convert the vector zt into a lower triangular matrix in a deterministic\nway using the torch.tril indices() method in PyTorch (e.g. f([a, b, c]T ) = [ a 0 b c ] ). We then apply a Softplus function to the diagonal elements of this matrix (to ensure positive definiteness) and the resulting matrix is the lower Cholesky matrix Lt. This procedure is carried out at every time step to produce time-varying precision matrices. When the actual covariance matrix is required, for example in the test set to evaluate model performance, matrix P t is inverted to obtain \u03a3t\n3.2. Generative Model\nThe generative model defines the joint distribution P\u03b8(r1:T ,L1:T , z1:T ), where rt is the multivariate returns process; Lt is the lower Cholesky decomposition of the precision matrix P t; vector zt is the neural network output of size n(n+1)2 , which is the latent variable that we try to infer using observational data. We factorise the joint distribution as follows:\nP\u03b8(r1:T ,L1:T , z1:T ) = T\u220f t=1 P\u03b8(rt|Lt)P\u03b8(Lt|zt)P\u03b8(zt|r1:t\u22121), (12)\nwhere P\u03b8(zt|r1:t\u22121) is a learned prior distribution which describes the transition dynamics of the latent variable zt. Information about the sequence r1:t\u22121 is carried by an RNN known as the gated recurrent unit (GRU) (Cho et al. 2014) with hidden state ht such that:\nP\u03b8(zt|r1:t\u22121) = P\u03b8(zt|ht\u22121) = N (\u00b5z,t,\u03a3z,t). (13)\nThe prior distribution is parameterised by a multilayer perceptron (MLP):\n{\u00b5z,t,\u03a3z,t}prior = MLPGen(ht\u22121). (14)\nP\u03b8(Lt|zt) is a delta distribution centered on the output of the deterministic function: torch.tril indices followed by a Softplus function on the diagonal elements, which converts neural network output vector zt into Lt. The emission distribution P\u03b8(rt|Lt) (the decoder) describes the 0 mean multivariate normal likelihood given in (2) since P\u03b8(rt|Lt) = P\u03b8(rt|(LtLTt )\u22121 = \u03a3t). A graphical presentation of the generative model is given in Fig 1. We refer to the parameters of the generative model collectively as \u03b8, and the parameters of the inference model as \u03c6, which are jointly optimised using stochastic gradient variational Bayes.\nThere are various ways to design the prior distribution P\u03b8(zt|It\u22121), where It\u22121 = {r1:t\u22121, z1:t\u22121,\u03a31:t\u22121} represents all available information up to time t \u2212 1. We tested other design schemes such as P\u03b8(zt|r1:t\u22121, z1:t\u22121) and P\u03b8(zt|r1:t\u22121,\u03a31:t\u22121), and found that in general the temporal dynamics of the latent variable could be well predicted using past returns alone; hence we decided on P\u03b8(zt|r1:t\u22121). Choosing the prior this way keeps the number of neural network parameters lower than the other two specifications, which reduces overfitting; also we do not need to evaluate the covariance matrix during training.\n3.3. Inference Model\nThe inference model defines the joint distribution q\u03c6(L1:T , z1:T |r1:T ) which we factorise as follows:\nq\u03c6(L1:T , z1:T |r1:T ) = T\u220f t=1 q\u03c6(Lt|zt)q\u03c6(zt|r1:t), (15)\nwhere the posterior distribution over latent variable q\u03c6(zt|r1:t) is parameterised by the encoder (MLP) of the VAE:\n{\u00b5z,t,\u03a3z,t}post = MLPInf (ht); (16)\nthis represents the filtering distribution, which is our inference of zt given the most up-to-date observational data r1:t. Since q\u03c6(zt|r1:t) is our variational approximation of the actual posterior P\u03b8(zt|r1:t), we see from (8) that maximising the ELBO(\u03b8, \u03c6) w.r.t. \u03c6 is equivalent to minimising the KL divergence between the variational posterior and the actual posterior. The deterministic function to obtain Lt given zt is the same as in the generative model: i.e. q\u03c6(Lt|zt) = P\u03b8(Lt|zt), since this simply torch.tril indices() followed by Softplus.\nTo summarise, VHVM is consisted of three neural networks: (1) an MLP (MLPGen) for the prior prediction model P\u03b8(zt|r1:t\u22121), also known as the decoder of the VAE which models the transition of the latent variable; (2) an MLP (MLPInf ) for the variational posterior q\u03c6(zt|r1:t), which is the encoder the VAE; (3) a GRU with hidden states ht to carry sequential information about the multivariate returns process {r1:T } and is shared by the generative and inference models.\n3.4. Model Training and Prediction\nTo perform variational inference we seek to maximise the ELBO(\u03b8, \u03c6) w.r.t. \u03b8 and \u03c6 jointly (Kingma and Welling 2014). The expression for the evidence lower bound is given in (17). VHVM is designed to output one step-ahead volatility prediction. When new observations become available, we update the hidden state ht of the GRU, which serves as the input to the prediction network MLPGen to predict the next period lower Cholesky matrix.\nELBO(\u03b8, \u03c6) = T\u2211 n=1 Ezt\u223cq\u03c6 [logP\u03b8(rt|zt)]\u2212KL(q\u03c6(zt|r1:t)||P\u03b8(zt|r1:t\u22121)), (17)"
        },
        {
            "heading": "4. Experiments",
            "text": "We test VHVM on foreign exchange data obtained from the Trading Academy website (eatradingacademy.com). We compute daily log returns using data from the period 24/01/2012 to 23/01/2022 (a total of 3653 observations), from which we remove weekend readings where the change in asset price was 0. We constructed various portfolios using our collection of FX series for n = 5, 10, 20, and 50. For model construction, we used a train:validation:test ratio of 80:10:10 and training for 50 epoches.\nFor model benchmarking, we compared VHVM against three benchmarks: (1) the DCC-GARCH model (Engle 2002), (2) a factor SV model with MCMC sampling (Hosszejni and Kastner 2021), and (3) Neural Stochastic Volatility model (Luo et al. 2018). The three baselines are representative models from the current approaches to volatility forecasting: GARCH models, SV models, and deep learning based models. We have chosen DCC-GARCH due to its ability to model dynamic conditional correlation between assets; we implemented the model in R using the package \u201drmgarch\u201d (Galanos 2022). For the factor SV model with MCMC sampler (MCMC-SV), we used the recently developed \u201dfactorstochvol\u201d package in R (Hosszejni and Kastner 2021).\nThe Neural Stochastic Volatility model (NSVM) Luo et al. (2018) is perhaps most relevant to our work since it was also designed under the VRNN framework. NSVM uses four recurrent neural networks to model temporal dynamics: one for the observed returns series rt and another for the latent factor zt in the generative model; similarly for the inference model. In our model however, we attempted to keep the number of model parameters low by using only one RNN but inputting the hidden state at different time steps to perform prediction and inference. Another key difference between NSVM and VHVM is that the output of NSVM is a low-rank approximation of the time-varying covariance matrix, whereas VHVM outputs the full covariance matrix. A low rank approximation may offer faster computations for higher dimensional portfolios, however we\nshow that VHVM is consistently better in terms of performance. For model evaluation, we perform one step-ahead covariance matrix forecasting on the test set, and following Wu et al. (2013) and Luo et al. (2018) use the log likelihood (2) as our performance metric since it describes the likelihood of the observed data falling under our estimated distribution. We have also included the hyperparameters of our model and the baselines in the Appendix for reproduction purposes."
        },
        {
            "heading": "5. Results and Discussion",
            "text": "In Table 1 to 5 we show the performance of VHVM against the three baseline models: NSVM, DCC-GARCH, and MCMC-SV on various 5 dimensional FX portfolios. For every time step we forecast a 5\u00d7 5 covariance matrix and in the tables we report the cumulative log likelihood of the test set. We have highlighted in bold the best performing model in terms of log likelihood (higher is better). We observe that VHVM performs best in 17 out of the 20 constructed portfolios. The neural network baseline NSVM however performs best in only one of the portfolios. As previously mentioned, the two key difference between VHVM and NSVM are: (1) VHVM uses a single RNN to carry information about r1:t and the hidden state at different time steps is used for forecasting (ht\u22121)/inference (ht), whereas NSVM uses four separate RNNs to model zt and rt in generation and inference; (2) NSVM outputs low rank approximations of the covariance matrix whereas VHVM outputs estimates of the full covariance matrix. We believe the simpler structure (fewer parameters) of our model has helped to reduce overfitting, and parameterising the full covariance matrix is more expressive than a low rank approximation at the expense of computational complexity (O(n2) vs O(n))\nTo better gauge the relative performances of the four models, we follow Ismail Fawaz et al. (2019) and plot a critical difference (CD) diagram showing the average ranking of the four model in Figure 3. Within a CD diagram, two models without a statistically significant difference (s.s.d.) in average ranking are connected with a horizontal line; the absence of such lines in Figure 3 indicates that the four models are s.s.d. in performance across the 20 5 dimensional experiments. According to Figure 3 VHVM has the best overall average ranking (1.25), followed by MCMCSV(2.2), DCC-GARCH(3), and NSVM(3.55). The fact that MCMC-SV performs slightly better\nthan DCC-GARCH is in accordance with claims that SV models are more flexible at modelling heteroscedastic behaviour in financial time series (Shapovalova 2021).\nIn Table 6 we show the experimental results for larger portfolios (10, 20, and 50 dimensions). In these experiments we only compare VHVM and NSVM since both DCC-GARCH and MCMC-SV have difficulties scaling up to higher dimensions. The list of currencies included in each portfolio is included in the Appendix. We observe that VHVM performs better than NSVM across all higher dimensional portfolios, consistent with what we observe in the 5 dimensional experiments."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper we propose Variational Heteroscedastic Volatility model (VHVM): an end-to-end neural network architecture capable of forecasting one step-ahead covariance matrices. VHVM outputs the lower Cholesky decomposition of a time-varying conditional precision matrix, which enforces two necessary constraints of a covariance matrix: symmetry and postive definiteness. Furthermore,\nby setting the neural network to output the precision matrix, we bypass the computationally expensive matrix conversion step in the evaluation of the multivariate normal log likelihood function. We demonstrated the effectiveness of VHVM against GARCH, SV, and deep learning baseline models and we observed that VHVM consistently outperformed its competitors."
        },
        {
            "heading": "Acknowledgement",
            "text": "The authors would like to thank Fabio Caccioli, Department of Computer Science, University College London, for proofreading the manuscript and providing feedback."
        }
    ],
    "title": "Variational Heteroscedastic Volatility Model",
    "year": 2022
}