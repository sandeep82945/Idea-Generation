{
    "abstractText": "This paper explores text classification on quantum computers. Previous results have achieved perfect accuracy on an artificial dataset of 100 short sentences, but at the unscalable cost of using a qubit for each word. This paper demonstrates that an amplitude encoded feature map combined with a quantum support vector machine can achieve 62% average accuracy predicting sentiment using a dataset of 50 actual movie reviews. This is still small, but considerably larger than previously-reported results in",
    "authors": [
        {
            "affiliations": [],
            "name": "Aaranya Alexander"
        },
        {
            "affiliations": [],
            "name": "Dominic Widdows"
        }
    ],
    "id": "SP:31a6c2a63fb24b70416a4406c59cb86daa87c5be",
    "references": [
        {
            "authors": [
                "C.J. Van Rijsbergen"
            ],
            "title": "The Geometry of Information Retrieval",
            "year": 2004
        },
        {
            "authors": [
                "D. Widdows"
            ],
            "title": "Geometry and meaning",
            "venue": "CSLI Publications,",
            "year": 2004
        },
        {
            "authors": [
                "D. Aerts",
                "T. Durt",
                "A. Grib",
                "B. Van Bogaert",
                "R. Zapatrin"
            ],
            "title": "Quantum structures in macroscopic reality",
            "venue": "International Journal of Theoretical Physics, vol. 32, no. 3, pp. pp\u2013489, 1993.",
            "year": 1993
        },
        {
            "authors": [
                "B. Coecke",
                "M. Sadrzadeh",
                "S. Clark"
            ],
            "title": "Mathematical foundations for a compositional distributional model of meaning",
            "venue": "CoRR, vol. abs/1003.4394, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "D. Widdows",
                "K. Kitto",
                "T. Cohen"
            ],
            "title": "Quantum mathematics in artificial intelligence",
            "venue": "Journal of Artificial Intelligence Research, vol. 72, pp. 1307\u20131341, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Guarasci",
                "G. De Pietro",
                "M. Esposito"
            ],
            "title": "Quantum natural language processing: Challenges and opportunities",
            "venue": "Applied Sciences, vol. 12, no. 11, 2022. [Online]. Available: https://www.mdpi.com/2076-3417/12/11/5651",
            "year": 2022
        },
        {
            "authors": [
                "R. Lorenz",
                "A. Pearson",
                "K. Meichanetzidis",
                "D. Kartsaklis",
                "B. Coecke"
            ],
            "title": "QNLP in practice: Running compositional models of meaning on a quantum computer",
            "venue": "arXiv preprint arXiv:2102.12846, 2021. [Online]. Available: https://arxiv.org/abs/2102.12846",
            "year": 2021
        },
        {
            "authors": [
                "D. Widdows",
                "D. Zhu",
                "C. Zimmerman"
            ],
            "title": "Near-term advances in quantum natural language processing",
            "venue": "arXiv preprint arXiv:2206.02171, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Wright",
                "K.M. Beck",
                "S. Debnath",
                "J. Amini",
                "Y. Nam",
                "N. Grzesiak",
                "J.-S. Chen",
                "N. Pisenti",
                "M. Chmielewski",
                "C. Collins"
            ],
            "title": "Benchmarking an 11-qubit quantum computer",
            "venue": "Nature communications, vol. 10, no. 1, pp. 1\u20136, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "IonQ Benchmarking"
            ],
            "title": "Algorithmic qubits: A better single-number metric",
            "venue": "2022, https://ionq.com/posts/ february-23-2022-algorithmic-qubits, accessed 2022-09-19.",
            "year": 2022
        },
        {
            "authors": [
                "S. Arunachalam",
                "V. Gheorghiu",
                "T. Jochym-O\u2019Connor",
                "M. Mosca",
                "P.V. Srinivasan"
            ],
            "title": "On the robustness of bucket brigade quantum ram",
            "venue": "New Journal of Physics, vol. 17, no. 12, p. 123010, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. G\u00e9ron"
            ],
            "title": "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems",
            "venue": "O\u2019Reilly Media,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Shan",
                "J. Guo",
                "X. Ding",
                "X. Zhou",
                "J. Wang",
                "H. Lian",
                "Y. Gao",
                "B. Zhao",
                "J. Xu"
            ],
            "title": "Demonstration of breast cancer detection using QSVM on IBM quantum processors",
            "venue": "2022. [Online]. Available: https://doi.org/10.21203/rs.3.rs-1434074/v1",
            "year": 2022
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint, vol. arXiv:1301.3781, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint, vol. arXiv:1810.04805, 2018.",
            "year": 1810
        },
        {
            "authors": [
                "D. Kartsaklis",
                "I. Fan",
                "R. Yeung",
                "A. Pearson",
                "R. Lorenz",
                "A. Toumi",
                "G. de Felice",
                "K. Meichanetzidis",
                "S. Clark",
                "B. Coecke"
            ],
            "title": "lambeq: An Efficient High-Level Python Library for Quantum NLP",
            "venue": "arXiv preprint arXiv:2110.04236, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.L. Maas",
                "R.E. Daly",
                "P.T. Pham",
                "D. Huang",
                "A.Y. Ng",
                "C. Potts"
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: Association for Computational Linguistics, June 2011, pp. 142\u2013150. [Online]. Available: http://www.aclweb.org/anthology/P11-1015",
            "year": 2011
        },
        {
            "authors": [
                "B.C. Boehmke",
                "B.M. Greenwell"
            ],
            "title": "Hands-on machine learning with R",
            "venue": "Support Vector Machines. Github, 2021. [Online]. Available: https://bradleyboehmke.github.io/HOML/svm.html",
            "year": 2021
        },
        {
            "authors": [
                "V. Salunkhe"
            ],
            "title": "Support vector machine (SVM)",
            "venue": "Jul 2021. [Online]. Available: https://medium.com/@viveksalunkhe80/ support-vector-machine-svm-88f360ff5f38",
            "year": 2021
        },
        {
            "authors": [
                "V. Havlicek",
                "A. Corcoles",
                "K. Temme",
                "other authors."
            ],
            "title": "Supervised learning with quantum-enhanced feature spaces",
            "venue": "Nature, vol. 567, no. 7747, pp. 212\u2013567, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M.S. ANIS",
                "Abby-Mitchell",
                "H. Abraham",
                "AduOffei",
                "R. Agarwal",
                "G. Agliardi",
                "other authors"
            ],
            "title": "Qiskit: An open-source framework for quantum computing",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Orazi"
            ],
            "title": "Quantum machine learning: development and evaluation of the multiple aggregator quantum algorithm",
            "venue": "Ph.D. dissertation, University of Bologna, 2022. [Online]. Available: http://amslaurea. unibo.it/25062/",
            "year": 2022
        },
        {
            "authors": [
                "I.F. Araujo",
                "D.K. Park",
                "F. Petruccione",
                "A.J. da Silva"
            ],
            "title": "A divide-and-conquer algorithm for quantum state preparation",
            "venue": "Nature Scientific Reports, vol. 11, no. 1, p. 6329, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Li",
                "X. Zhao",
                "X. Wang"
            ],
            "title": "Quantum self-attention neural networks for text classification",
            "venue": "2022. [Online]. Available: https: //arxiv.org/abs/2205.05625",
            "year": 2022
        },
        {
            "authors": [
                "D. Maheshwari",
                "D. Sierra-Sosa",
                "B. Garcia-Zapirain"
            ],
            "title": "Variational quantum classifier for binary classification: Real vs synthetic dataset",
            "venue": "IEEE Access, vol. 10, pp. 3705\u20133715, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "This paper demonstrates that an amplitude encoded feature map combined with a quantum support vector machine can achieve 62% average accuracy predicting sentiment using a dataset of 50 actual movie reviews. This is still small, but considerably larger than previously-reported results in quantum NLP.\nIndex Terms\u2014Quantum NLP, Quantum Kernels, QSVM"
        },
        {
            "heading": "1. Introduction, Motivation, and Outline",
            "text": "Quantum natural language processing (QNLP) is a new field, which in 2022 is theoretically advanced and nascent in quantum implementation. Quantum mathematical models have been used since the early 2000\u2019s in language-related fields such as information retrieval [1], [2] and cognitive science [3], and by 2010 deliberately quantum-theoretical approaches to combining logical and distributional semantics led to the development of the Distributed Compositional Categorical model of [4] (subsequently shortened to DisCoCat). This and related techniques including tensor networks, Frobenius algebras, and density matrices have been used to design systems that have demonstrated quantitative successes on various language tasks (for surveys see [5], [6]). As a source of scientific models and theories that have successfully been implemented, QNLP has become quite advanced.\nBy contrast, only in the past two or three years has it been possible to run QNLP programs on actual quantum computers. In 2020, experiments were run using 6-qubits [7], demonstrating successful compilation of short sentences into quantum circuits to give 83.3% classification accuracy. Since then, less sophisticated but more accurate classification results have been obtained at the cost of using more qubits (8 or 11) [8], and this work has also demonstrated small examples of circuits used as part of natural language generation and disambiguation. So QNLP is nascent, in that\nthe programs implemented on quantum computers are very small and young.\nSimilar situations are typical today in quantum information processing: the field has produced striking theoretical results since the 1980s and 1990s, and quantum computers that can implement these ideas are only just becoming available. The pace of development is fast \u2014 for example, at IonQ alone, systems have progressed from the 11 qubit machine evaluated by [9] to regularly running jobs with 20+ qubits [10]. The key scaling property of quantum memory is that the number of variables doubles with each additional qubit: so in theory, 10 qubits corresponds to a kilobyte, 20 to a megabyte, and 30 to a gigabyte, and it is easy to see that with 50 to 100 addressable qubits, the gap in scale between quantum and classical NLP could be closed.\nHowever, adapting even relatively simple NLP models to use these resources requires work. One strategy would be to wait for theoretical systems like the \u2018bucket brigade\u2019 protocol of [11] to be fully available, at which point implementing memory-intensive processes should be much simpler \u2014 but many opportunities may be forgone in the meantime, including the opportunity to influence the design of memory access, and to deliver useful intermediate-scale systems. An alternative more hands-on approach is to try and get the best results we can with current systems, to use this process to inform the design of useful intermediate-scale systems as soon as possible, and to cooperate directly with hardware engineering to enhance both machines and applications together. So far this approach has demonstrated that accurate classification can be performed using quantum hardware on a very small dataset, but with the memory requirement of at least one qubit for each salient word [8]. This method would not scale to a significant vocabulary without thousands of qubits \u2014 but if a more space-efficient method can be used to store and combine word-topic weights, the memory requirement could be much smaller.\nThis motivates the search for more space-efficient text encoding techniques to use in QNLP tasks, which is the topic of this paper.\nThis paper outlines the results of the preliminary study into space-efficient quantum encodings for binary text classification. The new research focuses on the implementation of\nar X\niv :2\n30 1.\n03 71\n5v 1\n[ qu\nan t-\nph ]\n9 J\nan 2\nencodings to enhance the Quantum Support Vector Machine (QSVM) classification method. First, the performance of different quantum classifiers are revisited, and the theory behind the quantum enhancement to the QSVM is explored. Further sections compare results across different encodings of text data in both quantum and classical methods."
        },
        {
            "heading": "2. Background: Quantum Machine Learning and Classification",
            "text": ""
        },
        {
            "heading": "2.1. Supervised and Quantum Machine Learning",
            "text": "The experiments in this paper follow the pattern of many machine-learning approaches to classification tasks. The task is to determine whether a piece of text is about a particular topic (e.g. food vs. computing), or demonstrates a particular sentiment (e.g. good vs. bad). The system is given training examples where the correct label is provided, and then evaluated by seeing how often it assigns the correct label to test data not used in training. The use of annotated training and test data makes this a supervised learning approach [12].\nQuantum machine learning has risen in potential in recent years due to development of quantum algorithms with space-efficiency and speedup compared to their classical counterparts. As described in [13], \u2018quantum machine learning\u2019 could refer to the use of quantum models on classical hardware with classical data, and this category covers most of the successes of QNLP to date. The new area for quantum computing is the opportunity to apply quantum processing to classical data, and the experiments in this paper fall into this category.\nQuantum algorithms make necessary trade-offs between space-efficiency, accuracy, and code complexity. For example, encoding data densely in a quantum state may bring difficulty in extracting information without disturbing the system, and often requires complicated circuits to perform data manipulation. However, space-efficient quantum algorithms in the realms of image classification and statistical datasets have shown to be promising in limiting extreme trade-offs between efficiency, accuracy and space [14] [15]. This also emphasizes the need for investigation of such methods in text datasets."
        },
        {
            "heading": "2.2. Featurizers and Classifiers",
            "text": "Many machine learning systems for classification today follow the pattern of featurizing then classifying. Various featurizers and classifiers are used in this paper, and describing them in the way can help to understand system architectures more easily."
        },
        {
            "heading": "2.3. Featurizers",
            "text": "A featurizer (sometimes called an encoder) takes an input such as an image or text file and maps it to a list of weights of salient features, which can be seen as a vector. These could be explicit features, such as the proportions\nof red, green, and blue in an area of an image, or implicit learned features, such as coordinates produced by a principal component analysis. There are many ways to map text datasets to feature vectors, ranging from counting the number of times each word occurs in each document (used since at least the 1960\u2019s), to using a neural network trained to map text to vectors (with a goal of predicting as many missing words as possible). The family of neural network methods has become large and includes word-based models such as Word2Vec [16] and contextual methods such as BERT [17] that featurize and combine fragments of words. Such textto-vector featurizers are often referred to as encoders, and their output vectors are often called embeddings."
        },
        {
            "heading": "2.4. Classifiers",
            "text": "A classifier takes an input and maps it to a class label (or several). That is a very general description, and could be implemented in many ways. The benefit of the featurizer / classifier patter is that the input to a classifier is typically reduced to vectors: in terms of types and interfaces, instead of mapping anything to a class label, a classifier can be implemented that maps a vector of floating point values to a class label."
        },
        {
            "heading": "2.5. Datasets Used in this Paper",
            "text": "Lambeq Dataset. The first dataset used in this work is the 70 training and 30 development and test sentences used for topic classification experiments by [7], and subsequently released as an open source package called Lambeq [18]. The sentences are artificially generated to use a small fixed vocabulary, to follow predictable syntactic patterns, and each comes with a binary topic label, \u20180\u2019 indicating computing and \u20181\u2019 indicating food, as in these examples:\n1 man prepares meal . 0 skillful woman debugs program .\nThe vocabulary used is too small to be representative of natural language, but this is still a useful and appreciated contribution to the QNLP community, because it enables use to quantitatively evaluate results on a dataset that is small enough to be loaded on today\u2019s quantum hardware.\nIMDB Dataset. The second, more complex set is taken from 50,000 archived IMDB movie reviews to be classified as either positive or negative reviews [19]. The average number of words in a review is between 228 and 229. Sentiment classification for this dataset is more difficult, because it is real user-generated text with varying degrees of good and bad, and many different aspects of a movie, some of which might be described positively and others negatively. Nonetheless, the reviews are published in positive and negative directories, and thus each review comes with a binary sentiment label."
        },
        {
            "heading": "2.6. Text Preprocessing",
            "text": "The datasets used are all English language and were preprocessed using simple methods. The Lambeq dataset can be perfectly tokenized just by splitting on whitespace characters, while the IMDB dataset has more of the vagaries of normal natural language. All of the experiments in this paper that used the IMDB dataset used a version of Word2Vec [16] to encode words as vectors, which also provides basic elements of tokenizing and normalizing English, in this case including splitting on whitespace, removing punctuation, though not automatically lowercasing."
        },
        {
            "heading": "3. Bag of Words Approach",
            "text": "One of the simplest families of classification methods are Bag of Words (BoW) approaches. In such methods, the features for each word are just added together \u2014 other conditional dependencies between features (such as those arising from word-order) are ignored. For a set of training documents with a known classification, the BoW classifier keeps a score per encountered word, where the score is proportional to the frequency of a word in each topic. These scores can be stored classically in a word-topic matrix. For new training documents, the classifier sums the scores of the words in the training set, and the topic with the highest score is deemed the class of the document. In the naive quantum implementation (Figure 1), the relationship between a word and a topic is encoded with single qubit rotations in the training phase. Then, a common phase adding circuit is performed on \u201ccombined\u201d topic qubits to sum the scores of words, and measured to classify the sample.\nThis can be seen as a featurizer / classifier pattern, with two very simple parts. The featurizer uses classical memory to map a text to vector for each topic, by mapping each (word, topic) pair to a particular qubit. The classifier adds the coordinates for each qubit belonging to the corresponding topic into a single combined value for that topic.\nThis quantum BoW circuit achieved 100% accuracy for the Lambeq dataset, the first such result reported on a\nquantum computer. However, it is extremely unscalable as it requires a qubit per (word, topic) combination, and additional qubits to hold each overall topic score. It follows that a goal in overcoming the bottleneck of quantum scalability is to improve the qubit encoding of the words in addition to the means of classification."
        },
        {
            "heading": "4. Support Vector Machines",
            "text": "Support Vector Machines (SVMs) classify arbitrary vectors by mapping training vectors to higher-dimensional spaces, and determining a class boundary which is used to classify new test vectors [20]. Figure 2 demonstrates three important cases of vector datasets that can be helped by the SVM. In a well-separated, linear, binary problem, classical fitting methods can confidently find an optimal hyperplane between the classes. However, as complexity of the data increases, exact nonlinear boundaries with large margins are difficult to achieve. Thus, the SVM first maps the data onto an enlarged feature space with a feature map, so the classes may be more easily distinguished. After application of the feature map, a kernel matrix is generated to store the relationship of support vectors with each other before being passed into the SVM for fitting.\nQuantum SVM classifiers have been proposed to harness higher-dimensional Hilbert spaces as feature spaces, and use known quantum computation methods to calculate the kernel [22]. The division of labor between classical and quantum processes is described in detail by [13]. The key insight is based on recognizing a similar structure between kernel methods and quantum processes. Kernel method bring a key optimization to the process by computing a predicted similarity between two high dimensional vectors by applying an appropriate kernel similarity function to lower-dimensional counterparts, thus avoiding the need to calculated the mapping into the higher dimensional space explicitly. Quantum circuits work well for exploring this higher-dimensional space, finding the pairwise similarities between different training instances that are then used as entries in the kernel matrix, which can then be used to train an SVM classifier using entirely classical computing. As such, the QSVM describes a hybrid quantum-classical method, where classical vectors are mapped to quantum states with a quantum feature map.\nThe quantum kernel can be efficiently calculated from the encoded quantum vectors, and then passed into the\nclassical SVM. The feature map is applied to the |0\u3009n state as a function of ~x, and then its conjugate is applied as a function of ~y. The kernel value is estimated by executing this circuit over a number of shots, R. The fraction of occurrences where the \u20180\u2019 string is measured corresponds to the estimated kernel value.\n\u03ba(~y, ~x) = | \u30080n|U\u2020\u03c6(~y)U\u03c6(~x) |0 n\u3009 |2 = | \u3008 ~\u03c6(y)| ~\u03c6(x)\u3009 |2 (1)\nFor example, a 7-dimensional amplitude feature map can be encoded with the 3-qubit circuit of Figure 3. Such an operator for ~x is then concatenated with a corresponding inverse operator for ~y, and the estimated kernel value is given by the proportion of |000\u3009 outcomes measured. (These circuits were constructed and run using the Qiskit package [23].)\nThe QSVM has been successfully implemented for classification of non-text datasets, with comparable accuracy to its fully classical analog [15] [14]. The main limitation of the QSVM concerns the design of the quantum feature map. For an n-qubit feature map, the accessible enlarged feature space is of size 2n, which is exponentially greater than what is possible with n-bits of classical space [22]. This quantum map must also manipulate the feature vectors so that the kernel matrix elements represent a classifiable relationship between vectors, and have reasonable complexity to be executed over many shots."
        },
        {
            "heading": "4.1. Simple QSVM for Text Classification",
            "text": "In an initial experiment using the Lambeq dataset, a QSVM classifier was coupled with the use of Word2Vec embeddings as features (described above). Sentence vectors are generated for each sentence by computing a Word2Vec embedding for each word, then averaging all the words in a sentence. This part is all done classically. These feature vectors were then passed to the QSVM for training, and used by the QSVM for classifying new sentences. Using 8 dimensions and one qubit for each dimension, an accuracy of 90% was achieved [8]. The quantum memory requirement is thus one qubit per dimension rather than one qubit per word. As a general rule, embedding dimensions tend to be a few hundred, ranging from default values of 300 for Word2Vec and 768 for BERT."
        },
        {
            "heading": "5. Text Encoding in Feature Maps",
            "text": ""
        },
        {
            "heading": "5.1. One-Hot Encoding Feature Map",
            "text": "The Pauli Gate feature maps are the most widely used in QSVM experiments for their low depth and adequate complexity. The second-order Pauli-Z evolution circuit (ZZ Feature Map) performs a non-linear mapping from n features to n qubits, analogous to a \u201cone-hot\u201d encoding [24]. In this case, it uses minimum space-efficiency achievable for a quantum feature map (n-to-n mapping), but obtains a quantum advantage due to the inability to simulate the ZZ feature map classically at larger scales.\nAn initial smaller-scale experiment was carried out using the IonQ simulator. For the Lambeq dataset, the classification accuracy peaked at 97% for 7-dimensional feature embeddings (Figure 5). The exponential increase in processing time with the number of qubits is infeasible for realistic NLP data; seven qubits for each inner product calculation scales poorly given the simplicity and low size of the Lambeq dataset compared to real text datasets."
        },
        {
            "heading": "5.2. Amplitude Encoded Feature Map",
            "text": "Quantum computing is appealing for several applications due to the potential dense encoding schemes for data into a qubit. Recent work in theoretical formalisms for dense encodings and computations are abundant, ranging from amplitude encodings to simultaneous amplitude and phase combined states. Most encoding schemes have few lowdepth state preparation methods, although it is a primary focus of current research. Moreover, the ability to extract or manipulate encoded features to perform a text classification task becomes more complicated the denser the encoding.\nTo investigate the performance of dense encodings in QNLP classification, this work takes a baseline approach with the simplest amplitude encoding scheme for a binary classification task (Equation 2).\n|word\u3009 = p1 |class1\u3009+ p2 |class2\u3009 (2)\nBuilding off of the one-hot encoding approach, it is possible to densely encode the feature elements to represent an n-dimensional Word2Vec vector in log2(n) qubits. Using the feature vector encoding as a feature map focuses specifically on developing an idea of text classification performance with a full dense encoding implementation.\nTo map the Word2Vec vectors to the quantum state in Equation 4, the divide and conquer state preparation method developed by [25] was implemented. This amplitude encoding feature map consists of a series of controlled-Y rotations applied on the |0log2(n)\u3009 state. Execution of this feature map for kernel estimation will be comparable to a classical linear kernel that takes the dot product of the each pair of vectors. It is emphasized that results from this map may not provide a quantum advantage, but acts as a means to draw conclusions on classification of densely encoded text data.\nU\u03c6(~x) = CRy(f(x8))...CRy(f(x1)) (3)\nU\u03c6(~x) |000\u3009 = x1 |000\u3009+ x2 |001\u3009 ...+ x8 |111\u3009 (4)"
        },
        {
            "heading": "6. Classification Results",
            "text": "The results described here were obtained using the IonQ simulator. The results of the encoded feature map in the QSVM are outlined in Figure 6, for the Lambeq dataset. The amplitude encoding not only exceeds the classification accuracy of the ZZ feature map, but it also reached 100% classification in just four qubits and lower processing time.\nPerforming further testing with the IMDB dataset, the classification accuracy varied considerably with each iteration. There remained cases with both the ZZ feature map and the amplitude encoded map where classification did not improve at all with increased features, where some sets peaked at 75% accuracy for three or four qubits. To corroborate the poor performance, analysis of the IMDB random dataset with the classical SVM (CSVM) and Bag of Words was performed with over several iterations and varied file size. Both failed to achieve greater than 80% accuracy and overall achieved under 60% accuracy for datasets with greater than 100 files (Figure 7).\nTo model a more targeted scenario, smaller Movies collections were made by taking reviews from the minimum number of movies to meet the desired test and train set size, in an attempt to reduce niche vocabulary and improve the quality of the training. The Movies dataset filtered for smaller training and testing vocabulary achieved more consistent trends with different sizes of file sets. Improvements in classification accuracy merited increase in kernel estimation shots; the effect of the different shot numbers on the kernel accuracy is shown in Figure 8.\nFor 50 files, 10,000 shots were sufficient to achieve decent results, where the systems were trained using 40 files, and tested with 10 files. Average performances for the CSVM and two QSVM maps are outlined in Table 1 for 20 different 50-file samples. The Amplitude Encoded QSVM correctly classified up to 8, 9 and 10 test files on select samples using three, four and five qubits respectively. This is a leading result for classification of real case text samples, of such a feature space, and of this complexity. Over all of the samples, a high classification score (70% or higher) was attained 26% of the time for both QSVM maps, and 36% of the time for the CSVM. Moreover, the Amplitude Encoded feature map outperformed the alternatives 80% of the time when it attained a high classification score. This indicates that the densely encoded feature map is often the best choice for the select samples it performs well on."
        },
        {
            "heading": "7. Discussion",
            "text": "The results on the Lambeq set are promising for the functionality of incorporating denser encodings into QSVM feature maps. There are several key factors behind the overall poor accuracy on the IMDB reviews, first and foremost being that the reviews were real text samples incorporating sentiment and colloquial language structure. Conversely, the\nLambeq dataset uses controlled skeleton sentences with basic grammar, and 200x lower word to sentence ratio than the reviews datasets. Other variational-based quantum methods, like a quantum self-attention neural network from [26] report similar trends with sentiment classification, and word to sample ratios. Such a neural network achieves up to 85% classification on a separate IMDB sentiment analysis set with no more than 12 words per sample, and 100% reached only on synthetic datasets.\nFurther, QSVM kernel estimation changes the results with several iterations. This is an expected result due to the probabilistic nature of performing computations and measurements with quantum circuits. It introduces more questions with regard to the feasibility of using delicately encoded quantum states, and the necessary shots needed to achieve accurate kernel estimates. The shot increase for the Movies dataset with only 50 files achieves a decent result and processing time, but this cost may become more apparent as quantum text datasets become larger. We can compare the work of [27] that investigated using amplitude encodings in a variational quantum circuit for binary classification. Accuracies of 75% and 67% were reported for datasets on diabetes patient specifications and sonar signal data, respectively. Such results were achievable with five iterations of the feature map and 100 epochs, and still underperformed compared to the traditional variational quantum classifier. This suggests that competitive results coming from densely encoded data suffers still from increased complexity and reduced accuracy across many datasets.\nThe non-linear trends in embedding size to classification accuracy highlights the potential unpredictability of classifying natural language datasets versus artificial or non-text datasets. Optimum performance may be unique to specific text samples and datasets, as well as the type of task.\nQNLP research is at the beginning of even generating\nthese questions, and the varied results from each refined dataset is encouraging to develop a systematic approach to improving quantum classification further."
        },
        {
            "heading": "8. Conclusions and Further Work",
            "text": "The intricacy of completing quantum text classification tasks stems from the nuances of natural language, and the complicated challenge of encoding this in quantum memory. The results demonstrated in this paper are a preliminary attempt to investigate the functionality of denser encodings in QSVMs, and the connections between word-vector maps and classification possibility. So far we have been able to achieve 100% accuracy on the Lambeq text classification dataset, and 62% average accuracy on a more realistic dataset of 50 movie reviews.\nIt is clear that a quantum encoded feature map can classify text sets in simulation, but so far for small and carefully targeted samples. The most obvious next steps include running these workflows on QPU resources: these experiments are underway (and are expected to be included when this paper is presented).\nThe focus on binary classification is another simplification that has made this work more tractable initially but is clearly a limiting restriction. Standard methods for extending SVMs to m classes include building m one-vsrest classifiers, or other collections of pairwise classifiers that can be combined to make m-way decisions. This may be a natural next step for quantum classification work. A more imaginative conjecture would be that the geometry of particular quantum feature spaces lends itself to new opportunities for separating the space into m topological components, an avenue we have yet to pursue.\nAs stated, the text preprocessing was so far simple, and typically carried out during the initial word vector encoding step. The impact of these choices on results has so far not been investigated. A more ambitious proposal would be to find ways to perform more of the distributional vector encoding on quantum computers, as begun by [7], [8].\nThere are many interesting avenues to pursue that can push towards quicker improvement of QNLP and quantum machine learning methods. This work addresses primarily space-efficiency, and aims to incentivize work regarding text encoding schemes, robust state preparation methods and algorithms for working with text data. Moving forward, investigations are in progress to determine patterns between feature map construction and its effect on text data mapping, with error analysis both in and out of simulation. Further work to develop a complete view of the trade-off between space, simplicity, and processing time is necessary to make an objective opinion on the potential of classification in QNLP."
        }
    ],
    "title": "Quantum Text Encoding for Classification Tasks",
    "year": 2023
}