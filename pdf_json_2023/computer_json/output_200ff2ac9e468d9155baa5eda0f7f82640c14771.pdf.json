{
    "abstractText": "Reinforcement learning(RL) algorithms face the challenge of limited data efficiency, particularly when dealing with high-dimensional state spaces and large-scale problems. Most of RL methods often rely solely on state transition information within the same episode when updating the agent\u2019s Critic, which can lead to low data efficiency and sub-optimal training time consumption. Inspired by human-like analogical reasoning abilities, we introduce a novel mesh information propagation mechanism, termed the \u2019Imagination Mechanism (IM)\u2019, designed to significantly enhance the data efficiency of RL algorithms. Specifically, IM enables information generated by a single sample to be effectively broadcasted to different states across episodes, instead of simply transmitting in the same episode. This capability enhances the model\u2019s comprehension of state interdependencies and facilitates more efficient learning of limited sample information. To promote versatility, we extend the IM to function as a plug-and-play module that can be seamlessly and fluidly integrated into other widely adopted RL algorithms. Our experiments demonstrate that IM consistently boosts four mainstream SOTA RL algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin, ultimately leading to superior performance than before across various tasks. For access to our code and data, please visit https://github.com/OuAzusaKou/IM.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zihang Wang"
        },
        {
            "affiliations": [],
            "name": "Maowei Jiang"
        }
    ],
    "id": "SP:1e1065d686153081fe4997ea5d879f9442796695",
    "references": [
        {
            "authors": [
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "Bilal Piot",
                "Steven Kapturowski",
                "Pablo Sprechmann",
                "Alex Vitvitskyi",
                "Zhaohan Daniel Guo",
                "Charles Blundell"
            ],
            "title": "Agent57: Outperforming the atari human benchmark",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher Berner",
                "Greg Brockman",
                "Brooke Chan",
                "Vicki Cheung",
                "Przemys\u0142aw D\u0119biak",
                "Christy Dennison",
                "David Farhi",
                "Quirin Fischer",
                "Shariq Hashme",
                "Chris Hesse"
            ],
            "title": "Dota 2 with large scale deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1912.06680,",
            "year": 2019
        },
        {
            "authors": [
                "Erik Cambria",
                "Bebo White"
            ],
            "title": "Jumping nlp curves: A review of natural language processing research",
            "venue": "IEEE Computational intelligence magazine,",
            "year": 2014
        },
        {
            "authors": [
                "Gabriel Dulac-Arnold",
                "Daniel Mankowitz",
                "Todd Hester"
            ],
            "title": "Challenges of real-world reinforcement learning",
            "venue": "arXiv preprint arXiv:1904.12901,",
            "year": 2019
        },
        {
            "authors": [
                "Luciano Floridi",
                "Massimo"
            ],
            "title": "Chiriatti. Gpt-3: Its nature, scope, limits, and consequences",
            "venue": "Minds and Machines,",
            "year": 2020
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "arXiv preprint arXiv:1912.01603,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Matteo Hessel",
                "Joseph Modayil",
                "Hado Van Hasselt",
                "Tom Schaul",
                "Georg Ostrovski",
                "Will Dabney",
                "Dan Horgan",
                "Bilal Piot",
                "Mohammad Azar",
                "David Silver"
            ],
            "title": "Rainbow: Combining improvements in deep reinforcement learning",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Lukasz Kaiser",
                "Mohammad Babaeizadeh",
                "Piotr Milos",
                "Blazej Osinski",
                "Roy H Campbell",
                "Konrad Czechowski",
                "Dumitru Erhan",
                "Chelsea Finn",
                "Piotr Kozakowski",
                "Sergey Levine"
            ],
            "title": "Model-based reinforcement learning for atari",
            "year": 1903
        },
        {
            "authors": [
                "Kacper Piotr Kielak"
            ],
            "title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency? 2019",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Michael Laskin",
                "Aravind Srinivas",
                "Pieter Abbeel"
            ],
            "title": "Curl: Contrastive unsupervised representations for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Misha Laskin",
                "Kimin Lee",
                "Adam Stooke",
                "Lerrel Pinto",
                "Pieter Abbeel",
                "Aravind Srinivas"
            ],
            "title": "Reinforcement learning with augmented data",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alex X Lee",
                "Anusha Nagabandi",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kevin Leonard",
                "Parisa Sepehri",
                "Breanna Cheri",
                "Debbie M Kelly"
            ],
            "title": "Relational complexity influences analogical reasoning",
            "venue": "ability. Iscience,",
            "year": 2023
        },
        {
            "authors": [
                "Peizhao Li",
                "Hongfu Liu"
            ],
            "title": "Achieving fairness at no utility cost via data reweighing with influence",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "James MacGlashan",
                "Mark K Ho",
                "Robert Loftin",
                "Bei Peng",
                "Guan Wang",
                "David L Roberts",
                "Matthew E Taylor",
                "Michael L Littman"
            ],
            "title": "Interactive learning from policy-dependent human feedback",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Max Schwarzer",
                "Ankesh Anand",
                "Rishab Goel",
                "R Devon Hjelm",
                "Aaron Courville",
                "Philip Bachman"
            ],
            "title": "Data-efficient reinforcement learning with self-predictive representations",
            "venue": "arXiv preprint arXiv:2007.05929,",
            "year": 2020
        },
        {
            "authors": [
                "Max Schwarzer",
                "Nitarshan Rajkumar",
                "Michael Noukhovitch",
                "Ankesh Anand",
                "Laurent Charlin",
                "R Devon Hjelm",
                "Philip Bachman",
                "Aaron C Courville"
            ],
            "title": "Pretraining representations for data-efficient reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "David Silver",
                "Guy Lever",
                "Nicolas Heess",
                "Thomas Degris",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "David Silver",
                "Aja Huang",
                "Chris J Maddison",
                "Arthur Guez",
                "Laurent Sifre",
                "George Van Den Driessche",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Veda Panneershelvam",
                "Marc Lanctot"
            ],
            "title": "Mastering the game of go with deep neural networks and tree",
            "venue": "search. nature,",
            "year": 2016
        },
        {
            "authors": [
                "Robert J Sternberg"
            ],
            "title": "Intelligence, information processing, and analogical reasoning: The componential analysis of human abilities",
            "venue": "Lawrence Erlbaum,",
            "year": 1977
        },
        {
            "authors": [
                "Robert J Sternberg",
                "Bathsheva Rifkin"
            ],
            "title": "The development of analogical reasoning processes",
            "venue": "Journal of experimental child psychology,",
            "year": 1979
        },
        {
            "authors": [
                "Richard S Sutton"
            ],
            "title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding",
            "venue": "Advances in neural information processing systems,",
            "year": 1995
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Hado Van Hasselt",
                "Arthur Guez",
                "David Silver"
            ],
            "title": "Deep reinforcement learning with double qlearning",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Hado P Van Hasselt",
                "Matteo Hessel",
                "John Aslanides"
            ],
            "title": "When to use parametric models in reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoying Xing",
                "Hongfu Liu",
                "Chen Chen",
                "Jundong Li"
            ],
            "title": "Fairness-aware unsupervised feature selection",
            "venue": "In Proceedings of the 30th ACM International Conference on Information & Knowledge Management,",
            "year": 2021
        },
        {
            "authors": [
                "Denis Yarats",
                "Rob Fergus",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Mastering visual continuous control: Improved data-augmented reinforcement learning",
            "venue": "arXiv preprint arXiv:2107.09645,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Reinforcement learning(RL) algorithms face the challenge of limited data efficiency, particularly when dealing with high-dimensional state spaces and large-scale problems. Most of RL methods often rely solely on state transition information within the same episode when updating the agent\u2019s Critic, which can lead to low data efficiency and sub-optimal training time consumption. Inspired by human-like analogical reasoning abilities, we introduce a novel mesh information propagation mechanism, termed the \u2019Imagination Mechanism (IM)\u2019, designed to significantly enhance the data efficiency of RL algorithms. Specifically, IM enables information generated by a single sample to be effectively broadcasted to different states across episodes, instead of simply transmitting in the same episode. This capability enhances the model\u2019s comprehension of state interdependencies and facilitates more efficient learning of limited sample information. To promote versatility, we extend the IM to function as a plug-and-play module that can be seamlessly and fluidly integrated into other widely adopted RL algorithms. Our experiments demonstrate that IM consistently boosts four mainstream SOTA RL algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin, ultimately leading to superior performance than before across various tasks. For access to our code and data, please visit https://github.com/OuAzusaKou/IM."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Data efficiency has been a fundamental and long-standing problem in the field of reinforcement learning (RL), especially when dealing with high-dimensional state spaces and large-scale problems. While RL has shown great promise in solving complex problems, it often requires a large amount of data, making it impractical or costly in many real-world applications. The sample complexity of such state-of-the-art agents is often incredibly high: MuZero (Schrittwieser et al., 2020) and Agent-57 (Badia et al., 2020) use 10-50 years of experience per Atari game, and (Berner et al., 2019) uses 45,000 years of experience to accomplish its remarkable performance. This is clearly impractical: unlike easily-simulated environments such as video games, collecting interaction data for many real-world tasks is extremely expensive. Therefore, improving data efficiency is crucial for RL algorithms (Dulac-Arnold et al., 2019).\nTo deal with data efficiency challenge. RAD (Laskin et al., 2020b)introduces two new data augmentation methods: random translation for image-based input and random amplitude scaling for proprioceptive input. CURL (Laskin et al., 2020a) performs contrastive learning simultaneously with an off-policy RL algorithm to improve data efficiency over prior pixel-based methods. (Schwarzer et al., 2021) addresses the challenge of data efficiency in deep RL by proposing a method that uses unlabeled data to pretrain an encoder, which is then finetuned on a small amount of task-specific data.\nEqual Contribution.\nar X\niv :2\n30 9.\n14 24\n3v 2\n[ cs\n.L G\n] 2\n7 Se\np 20\nWhile these achievements are truly impressive, it\u2019s important to note that current RL algorithms acquire information from state transition samples through interactions with the environment and then this information still can only be propagated and utilized within the same episode(as shown in Figure.1), by Temporal Difference (TD) updates (Sutton & Barto, 2018). This may result in the inability to propagate and utilize information contained in states from different episodes, thereby lowering the data efficiency of RL algorithms.\nn , a\nE[k]\nn ) can be broadcasted to different Q\nE[j]\ni\nacross episodes through the utilization of the IM, where i \u2208 (0, 1, 2, . . . , n) and j \u2208 (0, 1, 2, . . . , k). As shown in the figure, for instance, instead of merely transmitting within the same episode, the information QE[1]1 associated with (s E[1] 1 , a E[1] 1 ) and Q E[2] 2 associated with (s E[2] 2 , a E[2] 2 ) can propagate to other Critic value QE[j]i across episodes. The propagation paths are represented by the yellow and blue dashed lines, respectively.\nHuman beings can leverage their experiences in one task to improve their performance in another task through analogical reasoning. Inspired by human-like analogical reasoning abilities (Leonard et al., 2023; Sternberg, 1977; Sternberg & Rifkin, 1979), we propose an IM that enables the mutual\npropagation of information contained in states across different episodes(as shown in Figure.2). Specifically, we introduce a similarity-based difference inference module, which infers the Critic difference between two states based on the similarity of them. After updating the Critic value of one state, we employ this module to broadcast the Critic\u2019s change values to other states\u2019 Critic, thereby enhancing the estimation efficiency of the value function.\nTo this end, we propose IM based on a Similarity Calculation Network and a Difference Inference Network, and we make it a general module that can be theoretically applied in almost any deep RL algorithms. Furthermore, we conduct extensive experiments to validate our concept. The contributions of this paper are summarized as follows:\n\u2022 We propose IM, consisting of a Similarity Calculation Network and a Difference Inference Network. IM enables mutual information propagation across episodes, significantly enhancing data efficiency in various tasks. To the best of our knowledge, our mechanism has not been employed in prior work.\n\u2022 To promote versatility, we extend IM to function as a plug-and-play module that can be seamlessly and fluidly integrated into other widely adopted RL methods.\n\u2022 Extensive experiments show that IM consistently boosts four mainstream RL-algorithms, such as SAC, PPO, DDPG, and DQN, by a considerable margin, ultimately leading to superior performance(SOTA) than before in terms of data efficiency across various tested environments."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 MAINSTREAM RL ALGORITHMS",
            "text": "DQN (Van Hasselt et al., 2016) is a classic RL algorithm based on value functions used to solve discrete control problems. It employs experience replay and target networks to stabilize training. DDPG (Silver et al., 2014) is a classic Actor-Critic architecture method designed for continuous action space. In contrast to the off-policy methods mentioned above, on-policy methods sacrifice sample efficiency but enhance training stability. Techniques like TRPO (Schulman et al., 2015) utilize trust regions to limit the size of policy updates and leverage importance sampling to improve training stability. PPO (Schulman et al., 2017) provides a more concise way to restrict policy updates and has shown better practical results. SAC (Haarnoja et al., 2018) is a relatively newer off-policy method that introduces an entropy regularization term to encourage policy exploration in uncharted territories. It also uses the minimum value from dual Q-networks to estimate Q-values, avoiding overestimation to enhance training stability. In our work, we use these four RL algorithms as baselines to validate the effectiveness of IM."
        },
        {
            "heading": "2.2 DATA EFFICIENCY",
            "text": "A significant amount of research has been conducted to enhance the data efficiency in RL. SiMPLe (Kaiser et al., 2019) develops a pixel-level transition model for Atari games to generate simulated training data, achieving remarkable performance in the 100k frame setting. However, this approach demands several weeks of training. Variants of Rainbow (Hessel et al., 2018), namely DER (Van Hasselt et al., 2019) and OTRainbow (Kielak, 2019), are introduced with a focus on improving data efficiency. In the realm of continuous control, multiple studies (Hafner et al., 2019; Lee et al., 2020) suggest the utilization of a latent-space model trained with a reconstruction loss to boost data efficiency. Recently, in the field of RL, DrQ (Yarats et al., 2021) and RAD (Laskin et al., 2020b) observe that the application of mild image augmentation can significantly enhance data efficiency, yielding superior results to previous model-based approaches. CURL (Laskin et al., 2020a) proposed a combination of image augmentation and a contrastive loss to improve data efficiency for RL. SPR (Schwarzer et al., 2020) trains an agent to predict its own latent state representations multiple steps into the future. SGI (Schwarzer et al., 2021) improves data efficiency by using unlabeled data to pretrain an encoder which is then finetuned on a small amount of task-specific data.\nMost of the work mentioned above primarily focuses on improving data efficiency in pixel-based RL, which has certain limitations and is not a universally applicable method for enhancing data efficiency. Additionally, the methods mentioned earlier still rely on TD updates, we argue that using\nTD updates alone to update the Critic can result in inadequate information utilization, even leading to catastrophic learning failures, as previously discussed. To address these issues, we propose a general mesh information propagation mechanism, termed as the \u2019Imagination Mechanism (IM),\u2019 designed to significantly enhance the data efficiency of RL algorithms."
        },
        {
            "heading": "3 METHOD",
            "text": "Traditional RL algorithms update the Critic using the TD updates. We contend that TD updates can only propagate state transition information to previous states within the same episode, thereby limiting the data efficiency of RL algorithms, as demonstrated in Figure.1. It is well known that human beings can utilize their experiences from one task to another through analogical reasoning. Therefore, motivated by human-like analogical reasoning abilities, we propose IM that employs a sequential process of comparison followed by inference. Specifically, (1) we utilize a similarity calculation network(SCN) to compare states, yielding their respective similarity scores. (2) We design a difference inference network(DIN) to perform inference based on the outcomes of the comparison(similarity scores). (3) Finally, leveraging a mesh structure for information propagation, we can transmit information from a state transition sample to any state across episodes to update the Critic, as presented in Figure.2. The following subsections will cover SCN, DIN, and the application of IM in other RL algorithms, respectively."
        },
        {
            "heading": "3.1 SIMILARITY CALCULATION NETWORK",
            "text": "The intention of the SCN is to calculate the similarity feature. As shown in the dashed black rectangle in Figure.3. For any two given pairs (s, a), (s \u2032 , a \u2032 ), they are processed through multi-head encoder fi, to extract features q and q \u2032 , where i \u2208 1, 2, . . . , k. Subsequently, we calculate the similarity between q and q \u2032\nusing function Sim to obtain similarity vector v. The Sim function can be employed with various similarity methods, such as bi-linear inner-product or cosine similarity. The specific process is as follows:\nqi = fi(s, a) , q \u2032 i = fi(s \u2032, a\u2032) (1)\nvi = Sim(qi, q \u2032 i) (2)\nIn our work, we use cosine similarity as \"Sim\"."
        },
        {
            "heading": "3.2 DIFFERENCE INFERENCE NETWORK",
            "text": "The purpose of the Difference Inference Network(DIN) is to infer difference d between the Critics of two state-action pairs (s, a) and (s \u2032 , a \u2032 ) based on similarity vector v. As shown in the dashed orange rectangle in Figure.3. Specifically, DIN takes v as input and employs MLP to calculate the difference d between the Critics for the two different state-action pairs. Finally, we utilize Q(s, a) along with d(s, a, s\u2032, a\u2032) to infer Q(s \u2032 , a \u2032 ). The specific process is as follows:\nd(s, a, s\u2032, a\u2032) = MLP (v) , v = [v1, v2, . . . , vk] (3)\nQ(s\u2032, a\u2032)\u2190 Q(s, a) + d(s, a, s\u2032, a\u2032) (4)"
        },
        {
            "heading": "3.3 THE PROCESS OF APPLYING IM IN RL ALGORITHMS",
            "text": "Firstly, RL algorithm interacts with the environment to collect sample data. Next, these sample data are used to update the Critic or Actor. Afterward, we employ IM to update Critic. Finally, the training process is accomplished through iterations of the aforementioned steps. Below is a pseudocode example using the SAC algorithm.\nAlgorithm 1 Soft Actor-Critic with Imagination Mechanism Initialize actor, critic and replay buffer. for each iteration do\nfor each environment step do at \u223c \u03c0\u03d5(at|st) st+1 \u223c p(st+1|st,at) D \u2190 D \u222a {(st,at, r(st,at), st+1)} end for for each gradient step do\nupdate Actor and Critic() update Critic by IM()\nend for end for\nIM pseudocode process: (1) Given the input (s, a), we randomly sample from the replaybuffer to obtain (sn, an). (2) Next, we feed both (s, a) and (sn, an) into the feature encoder to obtain features (q, qn). (3) Then, we use the similarity function Sim to obtain similarity vector v. (4)Similarity vector v is passed to the DIN for Critic difference inference, resulting in the difference value d. (5) Finally, we use the Mean Squared Error(MSE) between \u2019Critic(s, a) + d\u2019 and \u2019Critic(s\u2032, a\u2032)\u2019 as the loss function. The Adam optimizer (He et al., 2020) is employed to update the parameters of fc,\nFC, and the Critic, while the momentum\u2212 averaged optimizer (Kingma & Ba, 2014) is used for updating the parameters of fd.\n1 # s,a: cuurent state and action 2 # s_n,a_n: other state and action 3 # f_c, f_d: feature encoder networks 4 # Sim: similarity method 5 # FC: full-connected layer 6 # loader: minibatch sampler from ReplayBuffer 7 # m: momentum, e.g. 0.95 8 # k: head num for Sim 9 # feature_dim: feature dimension\n10 # q,q_n: shape: [B,k*feature_dim] 11 # Critic: State-Action function 12\n13 f_c.params = f_d.params 14\n15 for s_n,a_n in loader: # load minibatch from replay buffer 16\n17 q = f_c.forward(s,a) 18 q_n = f_d.forward(s_n,a_n) 19 q_n = q_n.detach() # stop gradient 20 for i in range(k): 21\n22 v[i] = Sim(q[i*feature_dim:(i+1)*feature_dim], q_n[i* feature_dim:(i+1)*feature_dim])\n23\n24 d = FC(v) 25\n26 loss = MSE(Critic(s,a) + d, Critic(s_n,a_n)) 27 loss.backward() 28 update(f_c.params) # Adam 29 update(FC.params) # Adam 30 update(Critic.params) # Adam 31 f_d.params = m*f_d.params+(1-m)*f_c.params # momentum averaged\nListing 1: IM Learning Pseudocode(Pytorch-like)"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EVALUATION",
            "text": "We measure our proposed method on five different tasks for both data efficiency and performance, using two different environment step sizes: 100k and 500k steps. We use a 500k step size because most environments reach asymptotic performance at this point. The 100k step size is used to assess the initial learning speed of the algorithms.\nWe evaluate (i) sample efficiency by measuring the number of steps it takes for the best-performing baselines to reach the same performance level as baselines+IM within a fixed T (100k) steps (as illustrated in Figure.5), and (ii) performance by calculating the ratio of episode returns achieved by baselines+IM compared to the vanilla baseline at T steps(as presented in Table 1)."
        },
        {
            "heading": "4.2 ENVIRONMENTS",
            "text": "The primary objective of IM is to facilitate RL methods to be more data-efficient and effective, with broad applicability across a range of environments. We conduct evaluations using both discrete and continuous environments. Specifically, the continuous environments include \"Ant\", \"Half Cheetah\", and \"Pendulum\" from Mujoco (Todorov et al., 2012), while the discrete environments include \"Lunar Lander\" and \"Acrobot\" (Sutton, 1995) from the Gym (Brockman et al., 2016)."
        },
        {
            "heading": "500K STEP SCORES Half Cheetah-V3 Ant-V3 Pendulum-V0 Acrobot-V1 Lunar Lander-V2",
            "text": "Continuous environment : Existing model-free RL algorithms often exhibit poor data efficiency when applied to Mujoco tasks, primarily due to the high-dimensional state spaces. Consequently, we embed IM into these baseline methods in three continuous control tasks within Mujoco: Half Cheetah, Ant, and Pendulum, with the aim of improving the data efficiency of these baselines.\nDiscrete environment : We evaluate DQN in discrete control tasks, such as Acrobot and Lunar Lander, to ensure that our approach maintains generality in discrete control tasks as well."
        },
        {
            "heading": "4.3 BASELINES FOR BENCHMARKING DATA EFFICIENCY",
            "text": "As shown in the Table.1, we conduct comparisons with a total of four baseline methods, namely SAC, PPO, DDPG, and DQN, to validate IM in improving both the data efficiency and performance of RL algorithms. DQN is a classic RL algorithm based on value functions used to solve discrete control problems. It employs experience replay and target networks to stabilize training. DDPG is a classic Actor-Critic architecture method designed for continuous action spaces. In contrast to the off-policy methods mentioned above, on-policy methods sacrifice sample efficiency but enhance training stability. Techniques like TRPO utilize trust regions to restrict the size of policy updates and leverage importance sampling to improve training stability. PPO provides a more concise way to restrict policy updates and has shown better practical results. SAC is a relatively newer off-policy method that introduces an entropy regularization term to encourage policy exploration in uncharted territories. It also uses the minimum value from dual Q-networks to estimate Q-values, avoiding overestimation to enhance training stability."
        },
        {
            "heading": "4.4 RESULTS",
            "text": "Table.1 demonstrates the performance of four mainstream RL algorithms before and after the integration of IM at fixed steps T (100k or 500k). Specifically, in comparison to vanilla baselines, in three continuous tasks at the 100k setting, SAC+IM exhibits improvements of (27.24%, 86.79%, 1.22%), PPO+IM shows similar improvements of (24.55%, 95.56%, 1.29%), and DDPG+IM demonstrates parallel improvements of (30.30%, 87.00%, 3.26%). At the 500k setting, SAC+IM achieves enhancements of (15.88%, 51.45%, 1.22%) across all three environments, DDPG+IM presents\nimprovements of (12.61%, 63.82%, 0.86%), and PPO+IM demonstrates parallel improvements of (17.39%, 53.21%, 2.02%). In two discrete tasks at the 100k setting, DQN+IM displays enhancements of (4.01%, 2.14%), and at the 500k setting, DQN+IM shows improvements of (1.17%, 2.50%). The results demonstrate that our approach yields conspicuous improvements across a spectrum of tasks. The significant improvement observed in tasks Ant and Half-Cheetah can be attributed to its high-dimensional state space. It is evident that IM proves particularly effective in tackling challenges associated with such large-scale state spaces. In contrast, in environments such as Pendulum and Arcobot, characterized by relatively modest dimensional state spaces, the integration of IM does not yield substantial improvements. In scenarios such as Pendulum and Acrobot, vanilla RL algorithms have already reached high-performance levels, leaving little room for significant further improvement. A 500k step size is chosen since many environments reach their asymptotic performance at this point, while the 100k step size is employed to evaluate the initial learning speed of the algorithms (as mentioned in Sec 4.1).\nIn complex, high-dimensional observation space scenarios, as mentioned earlier, the imagination mechanism can significantly enhance data efficiency and performance during training, as shown in Figure.4 (a) and Figure.4 (b).\nAs illustrated in Figure 5, in the task \u2019Ant\u2019, SAC+IM achieves the performance of vanilla SAC in approximately 0.3x the number of T steps. Similarly, in the \u2019Hal cheetah\u2019 task, SAC+IM accomplishes this in roughly 0.4x the number of T steps. In the \u2019Pendulum\u2019 task, due to the modest dimensional state\nspaces, RL algorithms can reach high-performance levels, leaving little room for further improvement. Therefore, the impact of IM on such tasks is not obvious."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We contend that TD updates can only propagate state transition information to previous states within the same episode, thereby limiting data efficiency of RL algorithms. Inspired by humanlike analogical reasoning abilities, we propose a simple plug-and-play module(IM) designed to significantly enhance the performance and data efficiency of any RL method. Our implementation is straightforward, plug-and-play, efficient, and has been open-sourced. We hope that IM\u2019s performance improvements, ease of implementation, and efficiency in real-world time utilization will become valuable assets for advancing research in data-efficient and generalizable RL methods."
        },
        {
            "heading": "6 BROADER IMPACT",
            "text": "While significant progress has been made in fields such as Large Language Models (LLMs) (Floridi & Chiriatti, 2020), Computer Vision (CV) (He et al., 2016), Natural Language Processing (NLP) (Cambria & White, 2014), Reinforcement Learning (RL) (Silver et al., 2016), and Recommender Systems (RC) (Cambria & White, 2014), in terms of hardware (e.g., GPU acceleration from Nvidia) and software algorithms (e.g., ResNet, BERT, Transformer), there are pressing concerns associated with the increasing data costs and the environmental impact of state-of-the-art models.\nFrom the perspective of data cost, IM can be integrated into the training of LLMs(For the reason that RLHF (MacGlashan et al., 2017) is a part of LLMs). Given the high dimensionality and complex distribution of data in training LLMs, compared to the same data level, the introduction of IM may significantly improve training efficiency. This, in turn, leads to higher-quality answers in LLM-based Question and Answer (Q&A) systems, rather than relying solely on the continuous expansion of data volume. IM is therefore accessible to a broad range of researchers (even those without access to large datasets) and leaves a smaller carbon footprint. Technically, IM can be applied in multiagent algorithms to facilitate the propagation of information across agents, thereby enhancing the coordination of multi-agent systems. For instance, in the field of autonomous driving, vehicles can simulate the driving behavior of other vehicles to better coordinate traffic flow, reduce congestion, and accidents.\nFurthermore, IM is similar to an attention mechanism and can be embedded into multiple layers of DNN. It leverages the inter-sample relationships to enhance the utilization efficiency of samples. It enables the network to learn the correlation information among samples in the training set, which remains consistent in the test set, thereby improving performance and generalization on the test set.\nIt\u2019s fair to acknowledge that, despite the findings in this paper, we are still a long way from making Deep RL practical for solving complex real-world robotics problems. However, we believe that this work represents progress toward that objective.\nIf the system\u2019s administrator sets an unfair reward for vulnerable groups (or possibly even unintentionally harming themselves), the stronger the RL capabilities, the more unfavorable the results become. That\u2019s why, alongside improving algorithms for achieving cutting-edge performance, it\u2019s essential to also focus on additional research regarding fairness (Li & Liu, 2022; Xing et al., 2021)."
        }
    ],
    "title": "ENHANCING DATA EFFICIENCY IN REINFORCEMENT LEARNING: A NOVEL IMAGINATION MECHANISM BASED ON MESH INFORMATION PROPAGATION",
    "year": 2023
}