{
    "abstractText": "Large-scale pretrained models using self-supervised learning have reportedly improved the performance of speech antispoofing. However, the attacker side may also make use of such models. Also, since it is very expensive to train such models from scratch, pretrained models on the Internet are often used, but the attacker and defender may possibly use the same pretrained model. This paper investigates whether the improvement in anti-spoofing with pretrained models holds under the condition that the models are available to attackers. As the attacker, we train a model that enhances spoofed utterances so that the speaker embedding extractor based on the pretrained models cannot distinguish between bona fide and spoofed utterances. Experimental results show that the gains the anti-spoofing models obtained by using the pretrained models almost disappear if the attacker also makes use of the pretrained models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aoi Ito"
        },
        {
            "affiliations": [],
            "name": "Shota Horiguchi"
        }
    ],
    "id": "SP:7df5a36527a57f41144d588317cd4f97ddd37648",
    "references": [
        {
            "authors": [
                "Z. Wu",
                "J. Yamagishi",
                "T. Kinnunen",
                "C. Hanil\u00e7i",
                "M. Sahidullah",
                "A. Sizov",
                "N. Evans",
                "M. Todisco",
                "H. Delgado"
            ],
            "title": "ASVspoof: the automatic speaker verification spoofing and countermeasures challenge",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 4, pp. 588\u2013604, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "H. Tak",
                "J. Patino",
                "M. Todisco",
                "A. Nautsch",
                "N. Evans",
                "A. Larcher"
            ],
            "title": "End-to-end anti-spoofing with RawNet2",
            "venue": "Proc. ICASSP, 2021, pp. 6369\u20136373.",
            "year": 2021
        },
        {
            "authors": [
                "F. Alegre",
                "A. Amehraye",
                "N. Evans"
            ],
            "title": "A one-class classification approach to generalised speaker verification spoofing countermeasures using local binary patterns",
            "venue": "Proc. BTAS, 2013, pp. 1\u20138.",
            "year": 2013
        },
        {
            "authors": [
                "M. Todisco",
                "H. Delgado",
                "N. Evans"
            ],
            "title": "Constant q cepstral coefficients: A spoofing countermeasure for automatic speaker verification",
            "venue": "Computer Speech & Language, vol. 45, pp. 516\u2013535, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C.-I. Lai",
                "N. Chen",
                "J. Villalba",
                "N. Dehak"
            ],
            "title": "ASSERT: Antispoofing with squeeze-excitation and residual networks",
            "venue": "Proc. INTERSPEECH, 2019, pp. 1013\u20131017.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhang",
                "J. Cheng",
                "Y. Gu",
                "H. Wang",
                "J. Ma",
                "S. Wang",
                "J. Xiao"
            ],
            "title": "Improving replay detection system with channel consistency densenext for the asvspoof 2019 challenge.",
            "venue": "in Proc. IN- TERSPEECH,",
            "year": 2020
        },
        {
            "authors": [
                "A. Baevski",
                "Y. Zhou",
                "A. Mohamed",
                "M. Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Proc. NeurIPS, vol. 33, 2020, pp. 12 449\u201312 460.",
            "year": 2020
        },
        {
            "authors": [
                "W.-N. Hsu",
                "B. Bolte",
                "Y.-H.H. Tsai",
                "K. Lakhotia",
                "R. Salakhutdinov",
                "A. Mohamed"
            ],
            "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
            "venue": "IEEE/ACM TASLP, vol. 29, pp. 3451\u20133460, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Chen",
                "C. Wang",
                "Z. Chen",
                "Y. Wu",
                "S. Liu",
                "Z. Chen",
                "J. Li",
                "N. Kanda",
                "T. Yoshioka",
                "X. Xiao"
            ],
            "title": "WavLM: Large-scale selfsupervised pre-training for full stack speech processing",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Baevski",
                "M. Auli",
                "A. Mohamed"
            ],
            "title": "Effectiveness of selfsupervised pre-training for speech recognition",
            "venue": "Proc. ICASSP, 2019, pp. 7694\u20137698.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Huang",
                "S. Watanabe",
                "S.-w. Yang",
                "P. Garc\u0131\u0301a",
                "S. Khudanpur"
            ],
            "title": "Investigating self-supervised learning for speech enhancement and separation",
            "venue": "Proc. ICASSP, 2022, pp. 6837\u20136841.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Fan",
                "M. Li",
                "S. Zhou",
                "B. Xu"
            ],
            "title": "Exploring wav2vec 2.0 on speaker verification and language identification",
            "venue": "Proc. INTER- SPEECH, 2021, pp. 1509\u20131513.",
            "year": 2021
        },
        {
            "authors": [
                "L. Pepino",
                "P. Riera",
                "L. Ferrer"
            ],
            "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
            "venue": "Proc. INTERSPEECH, 2021, pp. 3400\u20133404.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Xie",
                "Z. Zhang",
                "Y. Yang"
            ],
            "title": "Siamese network with wav2vec feature for spoofing speech detection.",
            "venue": "in Proc. INTERSPEECH,",
            "year": 2021
        },
        {
            "authors": [
                "X. Wang",
                "J. Yamagishi"
            ],
            "title": "Investigating self-supervised front ends for speech spoofing countermeasures",
            "venue": "Proc. Odyssey, 2022, pp. 100\u2013106.",
            "year": 2022
        },
        {
            "authors": [
                "H. Tak",
                "M. Todisco",
                "X. Wang",
                "J.-w. Jung",
                "J. Yamagishi",
                "N. Evans"
            ],
            "title": "Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation",
            "venue": "Proc. Odyssey, 2022, pp. 112\u2013119.",
            "year": 2022
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "Proc. ICLR, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Papernot",
                "P. McDaniel",
                "I. Goodfellow",
                "S. Jha",
                "Z.B. Celik",
                "A. Swami"
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "Proc. ACM ASIACCS, 2017, pp. 506\u2013519.",
            "year": 2017
        },
        {
            "authors": [
                "R.K. Das",
                "X. Tian",
                "T. Kinnunen",
                "H. Li"
            ],
            "title": "The attacker\u2019s perspective on automatic speaker verification: An overview",
            "venue": "Proc. INTERSPEECH, 2020, pp. 4213\u20134217.",
            "year": 2020
        },
        {
            "authors": [
                "H. Wu",
                "B. Zheng",
                "X. Li",
                "X. Wu",
                "H.-Y. Lee",
                "H. Meng"
            ],
            "title": "Characterizing the adversarial vulnerability of speech self-supervised learning",
            "venue": "Proc. ICASSP, 2022, pp. 3164\u20133168.",
            "year": 2022
        },
        {
            "authors": [
                "K. Kurita",
                "P. Michel",
                "G. Neubig"
            ],
            "title": "Weight poisoning attacks on pretrained models",
            "venue": "Proc. ACL, 2020, pp. 2793\u20132806.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhang",
                "Z. Jiang",
                "J. Villalba",
                "N. Dehak"
            ],
            "title": "Black-box attacks on spoofing countermeasures using transferability of adversarial examples.",
            "venue": "in Proc. INTERSPEECH,",
            "year": 2020
        },
        {
            "authors": [
                "T. Nakamura",
                "Y. Saito",
                "S. Takamichi",
                "Y. Ijima",
                "H. Saruwatari"
            ],
            "title": "V2S attack: building DNN-based voice conversion from automatic speaker verification",
            "venue": "Proc. ISCA SSW, 2019, pp. 161\u2013 165.",
            "year": 2019
        },
        {
            "authors": [
                "M. Ravanelli",
                "Y. Bengio"
            ],
            "title": "Speaker recognition from raw waveform with SincNet",
            "venue": "Proc. SLT, 2018, pp. 1021\u20131028.",
            "year": 2018
        },
        {
            "authors": [
                "N. Vaessen",
                "D.A. Van Leeuwen"
            ],
            "title": "Fine-tuning wav2vec2 for speaker recognition",
            "venue": "Proc. ICASSP, 2022, pp. 7967\u20137971.",
            "year": 2022
        },
        {
            "authors": [
                "J. Deng",
                "J. Guo",
                "J. Yang",
                "N. Xue",
                "I. Kotsia",
                "S. Zafeiriou"
            ],
            "title": "ArcFace: Additive angular margin loss for deep face recognition",
            "venue": "IEEE TPAMI, vol. 44, no. 10, pp. 5962\u20135979, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Luo",
                "N. Mesgarani"
            ],
            "title": "Conv-TasNet: Surpassing ideal time\u2013 frequency magnitude masking for speech separation",
            "venue": "IEEE/ACM TASLP, vol. 27, no. 8, pp. 1256\u20131266, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Todisco",
                "X. Wang",
                "V. Vestman",
                "M. Sahidullah",
                "H. Delgado",
                "A. Nautsch",
                "J. Yamagishi",
                "N. Evans",
                "T. Kinnunen",
                "K.A. Lee"
            ],
            "title": "Asvspoof 2019: Future horizons in spoofed and fake audio detection",
            "venue": "Proc. INTERSPEECH, 2019, pp. 1008\u20131012.",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "J. Yamagishi",
                "M. Todisco",
                "H. Delgado",
                "A. Nautsch",
                "N. Evans",
                "M. Sahidullah",
                "V. Vestman",
                "T. Kinnunen",
                "K.A. Lee"
            ],
            "title": "ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech",
            "venue": "Computer Speech & Language, vol. 64, p. 101114, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Yamagishi",
                "X. Wang",
                "M. Todisco",
                "M. Sahidullah",
                "J. Patino",
                "A. Nautsch",
                "X. Liu",
                "K.A. Lee",
                "T. Kinnunen",
                "N. Evans",
                "H. Delgado"
            ],
            "title": "ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection",
            "venue": "Proc. ASVspoof Challenge, 2021, pp. 47\u201354.",
            "year": 2021
        },
        {
            "authors": [
                "N.M. M\u00fcller",
                "F. Dieckmann",
                "P. Czempin",
                "R. Canals",
                "K. B\u00f6ttinger",
                "J. Williams"
            ],
            "title": "Speech is silver, silence is golden: What do ASVspoof-trained models really learn?",
            "venue": "in Proc. ASVspoof Challenge,",
            "year": 2021
        },
        {
            "authors": [
                "Y.-Y. Yang",
                "M. Hira",
                "Z. Ni",
                "A. Astafurov",
                "C. Chen",
                "C. Puhrsch",
                "D. Pollack",
                "D. Genzel",
                "D. Greenberg",
                "E.Z. Yang"
            ],
            "title": "TorchAudio: Building blocks for audio and speech processing",
            "venue": "Proc. ICASSP, 2022, pp. 6982\u20136986.",
            "year": 2022
        },
        {
            "authors": [
                "V. Panayotov",
                "G. Chen",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "LibriSpeech: An ASR corpus based on public domain audio books",
            "venue": "Proc. ICASSP, 2015, pp. 5206\u20135210.",
            "year": 2015
        },
        {
            "authors": [
                "J. Kahn",
                "M. Riviere",
                "W. Zheng",
                "E. Kharitonov",
                "Q. Xu",
                "P.-E. Mazar\u00e9",
                "J. Karadayi",
                "V. Liptchinsky",
                "R. Collobert",
                "C. Fuegen"
            ],
            "title": "Libri-Light: A benchmark for ASR with limited or no supervision",
            "venue": "Proc. ICASSP, 2020, pp. 7669\u20137673.",
            "year": 2020
        },
        {
            "authors": [
                "G. Chen",
                "S. Chai",
                "G. Wang",
                "J. Du",
                "W.-Q. Zhang",
                "C. Weng",
                "D. Su",
                "D. Povey",
                "J. Trmal",
                "J. Zhang"
            ],
            "title": "GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio",
            "venue": "Proc. INTERSPEECH, 2021, pp. 3670\u20133674.",
            "year": 2021
        },
        {
            "authors": [
                "C. Wang",
                "M. Riviere",
                "A. Lee",
                "A. Wu",
                "C. Talnikar",
                "D. Haziza",
                "M. Williamson",
                "J. Pino",
                "E. Dupoux"
            ],
            "title": "VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
            "venue": "Proc. ACL- IJCNLP, 2021, pp. 993\u2014-1003.",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "Proc. ICLR, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Nagrani",
                "J.S. Chung",
                "W. Xie",
                "A. Zisserman"
            ],
            "title": "VoxCeleb: Large-scale speaker verification in the wild",
            "venue": "Computer Speech & Language, vol. 60, p. 101027, 2020.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Automatic speaker verification (ASV) is becoming a possible choice for secure authentication with the recent progress in its performance. However, ASV systems are exposed to the menaces of malicious attacks using speech synthesis, voice conversion, and replaying of recorded voice. To protect systems from such attacks, anti-spoofing methods are widely studied along with the growth of the community led by the ASVspoof challenges [1]. Although the boundaries are vague due to end-toend modeling [2], anti-spoofing models generally consist of a feature extraction part and a classification part. As the input feature, hand-crafted features such as linear frequency cepstral coefficients [3] and constant Q cepstral coefficients [4] or features from neural networks such as SENet [5], DenseNet [6], and RawNet2 [2] are used in the literature.\nThe quality of features extracted from audio using neural networks is rapidly advancing with the self-supervised learning (SSL) paradigm; a lot of models have been proposed in the last few years such as wav2vec 2.0 [7], HuBERT [8], and WavLM [9]. They have shown greatly improved performance on various speech-related tasks such as speech recognition [10], speech enhancement [11], speaker identification [12], and emotion recognition [13]. Likewise, it has been reported that ASV anti-spoofing can also benefit from SSL models [14, 15, 16].\nAlthough SSL models are powerful, training them from scratch consumes a lot of computational resources. For example, wav2vec 2.0, HuBERT, and WavLM have reportedly been\n\u2217The authors equally contributed to this work. This work has been done during Aoi Ito\u2019s internship at Hitachi, Ltd.\ntrained using 64, 32, and 16 NVIDIA V100 GPUs, respectively, even for the smallest BASE model of each. These computing environments are not necessarily on a scale that is readily available to everyone. Therefore, it is a common practice to finetune publicly available pretrained models on the Internet when using them for one\u2019s own applications.\nHere, although the accuracy of ASV anti-spoofing has indeed been reportedly improved by SSL models, it is easy to imagine that the attacker side can also take advantage of the power of these models. There is no previous investigation of the extent to which anti-spoofing loses the gains obtained by SSL models when attackers also use them. Also, if an ASV antispoofing system is developed using a publicly available pretrained model, an attacker can also access the same model. The security risks from the same pretrained models being used in attacks against anti-spoofing systems have also not been explored before.\nThis paper aims to provide answers to two questions: i) if an attacker leverages SSL models, will they maintain the anti-spoofing performance reportedly improved by them, and ii) is there any disadvantage to anti-spoofing if the attacker uses the same pretrained model on the Internet as the defender even when it is finetuned? For this purpose, we propose a method for training the attacker, as illustrated in Figure 1. In the method, a speaker embedding extractor based on SSL models is first trained. Then, an enhancement model that improves the deception ability of spoofed utterances is trained to make the speaker embeddings extracted from bona fide and enhanced spoofed utterances closer. The proposed method is evaluated using various combinations of attacker-defender pairs on three ASVspoof challenge datasets.\nar X\niv :2\n30 5.\n15 51\n8v 1\n[ ee\nss .A\nS] 2\n4 M\nay 2\n02 3"
        },
        {
            "heading": "2. Related work",
            "text": "If the attacker has prior knowledge of an anti-spoofing model, adversarial attacks can be attempted: white-box attacks when the parameters of the model are known [17] and black-box attacks when only input-output pairs of the model are known [18] (please refer to [19] for a literature review). The performance of ASV with SSL models was also degraded by adversarial attacks [20], but the models were not finetuned. When a model is fine-tuned, it is not possible to directly calculate such adversarial attacks. One possible attack in such a case is when a pretrained model has a backdoor in it due to weight poisoning [21], but this is not considered in this paper because we assume the commonly used SSL model. Another possibility is to utilize the transferability of adversarial attacks; successful attacks on a certain model are also likely to fool other models [22]. While attacks by adversarial samples are an important issue for any machine learning model, the scope of this paper is to verify whether the attacker can also benefit from the power of SSL models, so generating such samples artificially is out of scope for this paper.\nThe paradigm used in this paper is highly related to verification-to-synthesis (V2S) [23], in which a voice conversion model is trained to fool a pretrained speaker classification model. There are three differences from V2S: i) we enhance utterances that are already made to spoof target speakers, ii) we do not apply any regularization to enhanced utterances to preserve their phonetic properties, and iii) we directly compare a pair of speaker embeddings instead of the output from the classifier and the desired speaker label, which makes the method applicable to any speaker."
        },
        {
            "heading": "3. Method",
            "text": "We assume the situation depicted in Figure 1. The defender side builds an anti-spoofing model by using an SSL model on the Internet as a frontend. The anti-spoofing model performs a two-way classification that distinguishes whether an input is a bona fide or spoofed utterance. The attacker side aims to obtain a spoofing model that transforms an already spoofed piece of audio by, for example, voice conversion or text-to-speech synthesis, to enhance its spoofing ability. With a speaker embedding extractor based on the SSL model, the spoofing model is trained so that the embedding extracted from a spoofed input is indistinguishable from that from a bona fide input. We will describe the implementation of each side in detail in the following subsections."
        },
        {
            "heading": "3.1. Defender side",
            "text": "For the anti-spoofing model, we used a simplified version of the RawNet2-based architecture [16], in which the SincNet frontend [24] was replaced by an SSL model. Table 1 shows the detailed configuration of the model. Given 64,600 samples of audio (\u223c4 seconds), the SSL frontend first extracts 201-length 768-dimensional speech representations. The following dimensionality reduction layer and six-stacked residual blocks further convert the representations to perform two-way classification, i.e., spoofed vs. bona fide, in the last layer. The model was trained using the standard cross entropy loss. Note that the parameters of the SSL frontend were initialized with those of the pretrained model and jointly optimized with the backend in an end-to-end manner.\nSSL frontend (1, 201, 768) wav2vec 2.0/HuBERT/WavLM\nDimensionality reduction\n(1, 201, 128) 128-dim fully connected\n(1, 67, 42) 3\u00d7 3 max pooling (1, 67, 42) BN & SeLU\nResidual block (32, 67, 42) 3x3 conv, 32-chBN & SeLU 3x3 conv, 32-ch \u00d7 2 Residual block (64, 67, 42)\n3x3 conv, 64-chBN & SeLU 3x3 conv, 64-ch \u00d7 4 Classification\n64 67\u00d7 42 global average pooling 2 2-dim fully connected"
        },
        {
            "heading": "3.2. Attacker side",
            "text": "The attacker side attempts to transform spoofed recordings into ones that the SSL model cannot distinguish from bona fide recordings. The training of the spoofing model is two-staged. First, a speaker embedding extractor is constructed on the basis of the model pretrained using SSL, and then an enhancement model to improve the input\u2019s spoofing ability is trained using the extractor. If the enhancement model can be trained to be able to fool the extractor, then an anti-spoofing model based on the same SSL model could be fooled as well.\nThe speaker embedding extractor fembed was trained to classify utterances on the basis of their speaker IDs. Given an input utterance, the SSL frontend first computes frame-level embeddings, which are then aggregated by average pooling along the time axis to obtain an utterance-level embedding. Following the conventional study [25], the entire network was optimized using additive angular margin (AAM) softmax loss [26].\nWith the well-trained speaker embedding extractor, the enhancement model fenh was trained to convert spoofed utterances to ones whose speaker embeddings are not distinguishable from those extracted from bona fide utterances. During training, a pair of bona fide and spoofed utterances is used to train the model. A spoofed utterance xspoof is first fed to the spoofing model to convert it to an enhanced one:\nxenh = fenh (xspoof) . (1)\nFor the enhancement model fenh, we used Conv-TasNet [27] to convert the input audio in the time domain. Then, speaker embeddings are extracted from each bona fide and enhanced spoofed utterances. The network is optimized to minimize the angle between those embeddings by using the following loss:\nL = 1\u2212 cos (fembed (xenh) , fembed (xbonafide)) , (2) where xbonafide is a bona fide utterance of the speakers who the spoofed utterance xspoof is pretending to be and cos (\u00b7, \u00b7) denotes the cosine similarity between two arguments. Note that the parameters of the speaker embedding extractor were frozen during the training of the enhancement model; otherwise, it will fall into a trivial solution that, for example, always outputs the same embedding regardless of the input.\nDuring evaluations, enhanced spoofed utterances obtained using (1) are fed to the anti-spoofing models described in Section 3.1 instead of the original spoofed utterances."
        },
        {
            "heading": "4. Experimental settings",
            "text": ""
        },
        {
            "heading": "4.1. Dataset",
            "text": "We consider two scenarios in this paper. The first scenario is that an attacker and a defender use a different dataset for the training of each side\u2019s model. To meet this purpose, each of the training and development sets of the ASVspoof 2019 logical access (LA) database [28, 29] was divided into two portions to train spoofing models and anti-spoofing models, respectively. For the bona fide utterances, we assigned half of each speaker\u2019s utterances to the attacker and the other half to the defender. For the spoofed utterances, assuming that an attacker does not have prior knowledge of the specific method that a defender is taking into consideration, we divided them based on their systems: A01, A03, A05 for the attacker and A02, A04, A06 for the defender1. The second scenario is that an attacker has access to some of the defender\u2019s data, e.g., the defender makes use of publicly available datasets. In this scenario, the defender uses the whole ASVspoof 2019 LA database, while the attacker uses the same portion as the first scenario.\nFor the evaluation, we used the test set of the ASVspoof 2019 LA database as a clean dataset, in which spoofing attacks are based on speech synthesis and voice conversion. We also used the ASVspoof 2021 LA and deepfake (DF) databases [30] for more noisy and realistic trials. The 2021 LA database is based on the same attack algorithms as the 2019 LA database, but the effects of encoding and transmission over the telephone are also taken into account. The 2021 DF database focuses on the distortion caused by compressing and restoring audio through various codecs. Although there is some discussion about the appropriateness of the original ASVspoof datasets [31], we used them as they are. The models\u2019 performance was evaluated using an equal error rate (EER)."
        },
        {
            "heading": "4.2. Model configuration",
            "text": "As the SSL models, we used wav2vec 2.0 BASE [7], HuBERT BASE [8], WavLM BASE, and WavLM BASE+ [9] models. For simplicity, the word \u201cBASE\u201d will be omitted hereafter. The TorchAudio [32] implementations of wav2vec 2.0 and HuBERT and the official implementation of WavLMs2 were used in our experiments. Each model has approximately 95 million parameters. The wav2vec2.0, HuBERT, and WavLM were trained with the concatenation of train-clean-100, train-clean360, and train-other-500 from the LibriSpeech dataset [33], and WavLM+ was trained with the Libri-Light [34], GigaSpeech [35], and VoxPopuli [36] datasets.\nThe anti-spoofing models were trained on the four types of SSL models above. They were trained using the Adam optimizer [37] with a fixed learning rate of 1 \u00d7 10\u22126 for at most 100 epochs. The batch size was set to 32. As the baseline without an SSL frontend, we also used the official RawNet2 recipe from the ASVspoof 2021 baseline3. No data augmentation techniques were applied during training. The inputs to each model\n1This split was to avoid giving unfair advantages to the attacker since the spoofing systems used for A04 and A06 were also used in the test set.\n2https://github.com/microsoft/unilm/tree/ master/wavlm\n3https://github.com/asvspoof-challenge/2021\nwere aligned to 64,600 samples by cropping and/or repeating the original utterances.\nFor speaker embedding extractors, we simply finetuned the wav2vec 2.0 and HuBERT models using VoxCeleb2 [38] and evaluated them using VoxCeleb1. No extra embedding layer as the backend was introduced; thus, the dimensionality of the speaker embedding was 768. Each model was optimized to minimize the AAM softmax loss with a margin of 0.3 and a scale of 15 using the Adam optimizer for 6 epochs (\u223c100k iterations). The learning rate was linearly increased from zero to 1 \u00d7 10\u22125 for the first 10% of iterations, kept unchanged for the next 40% of iterations, and then linearly decayed to zero for the final 50% of iterations. Here also, we did not apply any data augmentation techniques during training.\nThe spoofing models based on Conv-TasNet were trained using each speaker embedding extractor as a backend. We used a Conv-TasNet model that consists of three repetitions of eightstacked convolutional blocks with different dilations, which was the best configuration in the original paper [27]. The training was conducted for 300 epochs using the Adam optimizer with a fixed learning rate of 1 \u00d7 10\u22125 and a batch size of 8 without data augmentation."
        },
        {
            "heading": "5. Results",
            "text": ""
        },
        {
            "heading": "5.1. Preliminary results on attacker side",
            "text": "Before discussing the main results, we report the performance of the model trained for the attacker side. Table 2 shows the EERs of the speaker embedding extractors on the VoxCeleb1 dataset. We use three cleaned splits of the dataset: the original test set (VoxCeleb1 in Table 2), the extended test set (VoxCeleb1-E), and the hard test set (VoxCeleb-H). Although the quality of the speaker embeddings is not the main focus of this paper, we note that these values are comparable, though not the best, to previously reported values [25]."
        },
        {
            "heading": "5.2. Main results",
            "text": "Table 3 shows the EERs for the cases when the anti-spoofing models were trained using the whole ASVspoof 2019 LA training set. Without spoofing enhancement models, the antispoofing models with the SSL frontend significantly outperformed the RawNet2 baseline on all the evaluation datasets, consistent with the previous study [16]. From the results on the 2019 LA test set (Table 3a), the absolute EERs of RawNet2based anti-spoofing were significantly increased by the spoofing enhancement, while those of the SSL-based anti-spoofing were hardly affected. This suggests that SSL models are not fooled by spoofing enhancement in clean conditions where the training and evaluation data do not diverge. On the other hand, the results on the 2021 LA and DF test sets in Tables 3b and 3c show that not only RawNet2-based anti-spoofing but also SSLbased anti-spoofing were degraded by spoofing enhancement. The performance gains that the defender side obtained by using SSL models were almost completely lost when the attacker side also used the SSL models. Even so, since the RawNet2 base-\nAnti-spoofing None wav2vec 2.0 HuBERT\nAnti-spoofing None wav2vec 2.0 HuBERT\nAnti-spoofing None wav2vec 2.0 HuBERT\n(a) ASVspoof 2019 LA test set\nSpoofing enhancement\nAnti-spoofing None wav2vec 2.0 HuBERT\nRawNet2 10.93 24.28 24.62 wav2vec 2.0 0.82 0.60 0.67 HuBERT 1.70 2.12 2.09 WavLM 0.94 3.05 2.95 WavLM+ 0.73 0.30 0.28\n(b) ASVspoof 2021 LA test set\nSpoofing enhancement\nAnti-spoofing None wav2vec 2.0 HuBERT\nRawNet2 11.64 29.31 29.45 wav2vec 2.0 8.28 20.05 19.51 HuBERT 5.40 15.25 13.90 WavLM 21.20 38.06 38.00 WavLM+ 10.76 29.20 28.10\n(c) ASVspoof 2021 DF test set\nSpoofing enhancement\nAnti-spoofing None wav2vec 2.0 HuBERT\nRawNet2 24.47 49.76 48.24 wav2vec 2.0 10.03 21.16 22.17 HuBERT 15.07 38.40 38.16 WavLM 14.78 42.30 42.48 WavLM+ 10.56 31.95 30.49\nline also suffers from performance degradation due to spoofing enhancement, the use of the SSL model for defense is recommended. From the results, no particular performance degradation was observed when using the same pretrained model for the attacker and the defender, e.g., a 16.57% EER on the 2021 LA test set when both sides used wav2vec 2.0. This means that it is a possible option to use publicly available SSL models on the Internet for defense.\nTable 4 shows the EERs for the case when the anti-spoofing models were trained using the portion of the 2019 LA training set. The trend in the results is the same as in Table 3; spoofing enhancement did not affect the results for the clean condition (2019 LA) that much but degraded those for the realistic conditions (2021 LA and DF).\nFigure 2 shows the cumulative distribution of the bona fide score, which was obtained as the logit of the bona fide class, for the 2021 LA set. Both the anti-spoofing and spoofing enhancement models are based on wav2vec 2.0. It is clearly observed that the spoofing enhancement shifted the distribution of the spoofed utterances to the right, i.e., increased the bona fide score.\nFigure 3 shows examples of the original spoofed utterances and their conversions of which the bona fide scores were largely\n0.0 0.5 1.0 0k\n4k\n8k\n0.0 0.5 1.0 0k\n4k\n8k\n0.0 0.5 1.0 0k\n4k\n8k\n0.0 0.5 1.0 0k\n4k\n8k\n(a) LA E 8161161 (\u22125.7 \u2192 25.5)\n0.0 0.5 1.0 0k\n4k\n8k\n(b) LA E 5770469 (\u22125.6 \u2192 23.7)\n0.0 0.5 1.0 0k\n4k\n8k\n(c) LA E 4822201 (\u22125.6 \u2192 21.1)\nFigure 3: Examples of spoofed utterances in ASVspoof 2021 LA test set before (top) and after (bottom) enhancement. Horizontal and vertical axes correspond to time and frequency, respectively. Anti-spoofing and enhancement models are both based on wav2vec 2.0. Values in brackets show how bona fide score was changed by enhancement.\nincreased by the enhancement model based on wav2vec 2.0. While spoofing enhancement makes utterances fool an antispoofing model, their harmonic structures are rarely preserved. Spoofing enhancement that also preserves the naturalness of transformed utterances (like V2S [23]) is left to future work."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this study, we investigated whether the performance improvement of an anti-spoofing model obtained by using an SSL model is real, under the condition that the attacker also has access to the SSL model. Experiments revealed that the attacker could also benefit from SSL models, thereby eliminating most of the benefits the defender gains from them. We also found no significant EER degradation for the attacker side from using the same pretraining model as the defender side, indicating that both sides should simply use a stronger SSL model. Future work will include a countermeasure for attackers in which pretrained SSL models are utilized."
        },
        {
            "heading": "7. References",
            "text": "[1] Z. Wu, J. Yamagishi, T. Kinnunen, C. Hanilc\u0327i, M. Sahidullah,\nA. Sizov, N. Evans, M. Todisco, and H. Delgado, \u201cASVspoof: the automatic speaker verification spoofing and countermeasures challenge,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 4, pp. 588\u2013604, 2017.\n[2] H. Tak, J. Patino, M. Todisco, A. Nautsch, N. Evans, and A. Larcher, \u201cEnd-to-end anti-spoofing with RawNet2,\u201d in Proc. ICASSP, 2021, pp. 6369\u20136373.\n[3] F. Alegre, A. Amehraye, and N. Evans, \u201cA one-class classification approach to generalised speaker verification spoofing countermeasures using local binary patterns,\u201d in Proc. BTAS, 2013, pp. 1\u20138.\n[4] M. Todisco, H. Delgado, and N. Evans, \u201cConstant q cepstral coefficients: A spoofing countermeasure for automatic speaker verification,\u201d Computer Speech & Language, vol. 45, pp. 516\u2013535, 2017.\n[5] C.-I. Lai, N. Chen, J. Villalba, and N. Dehak, \u201cASSERT: Antispoofing with squeeze-excitation and residual networks,\u201d in Proc. INTERSPEECH, 2019, pp. 1013\u20131017.\n[6] C. Zhang, J. Cheng, Y. Gu, H. Wang, J. Ma, S. Wang, and J. Xiao, \u201cImproving replay detection system with channel consistency densenext for the asvspoof 2019 challenge.\u201d in Proc. INTERSPEECH, 2020, pp. 4596\u20134600.\n[7] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Proc. NeurIPS, vol. 33, 2020, pp. 12 449\u201312 460.\n[8] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHuBERT: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM TASLP, vol. 29, pp. 3451\u20133460, 2021.\n[9] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavLM: Large-scale selfsupervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\n[10] A. Baevski, M. Auli, and A. Mohamed, \u201cEffectiveness of selfsupervised pre-training for speech recognition,\u201d in Proc. ICASSP, 2019, pp. 7694\u20137698.\n[11] Z. Huang, S. Watanabe, S.-w. Yang, P. Garc\u0131\u0301a, and S. Khudanpur, \u201cInvestigating self-supervised learning for speech enhancement and separation,\u201d in Proc. ICASSP, 2022, pp. 6837\u20136841.\n[12] Z. Fan, M. Li, S. Zhou, and B. Xu, \u201cExploring wav2vec 2.0 on speaker verification and language identification,\u201d in Proc. INTERSPEECH, 2021, pp. 1509\u20131513.\n[13] L. Pepino, P. Riera, and L. Ferrer, \u201cEmotion recognition from speech using wav2vec 2.0 embeddings,\u201d in Proc. INTERSPEECH, 2021, pp. 3400\u20133404.\n[14] Y. Xie, Z. Zhang, and Y. Yang, \u201cSiamese network with wav2vec feature for spoofing speech detection.\u201d in Proc. INTERSPEECH, 2021, pp. 4269\u20134273.\n[15] X. Wang and J. Yamagishi, \u201cInvestigating self-supervised front ends for speech spoofing countermeasures,\u201d in Proc. Odyssey, 2022, pp. 100\u2013106.\n[16] H. Tak, M. Todisco, X. Wang, J.-w. Jung, J. Yamagishi, and N. Evans, \u201cAutomatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation,\u201d in Proc. Odyssey, 2022, pp. 112\u2013119.\n[17] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus, \u201cIntriguing properties of neural networks,\u201d in Proc. ICLR, 2014.\n[18] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami, \u201cPractical black-box attacks against machine learning,\u201d in Proc. ACM ASIACCS, 2017, pp. 506\u2013519.\n[19] R. K. Das, X. Tian, T. Kinnunen, and H. Li, \u201cThe attacker\u2019s perspective on automatic speaker verification: An overview,\u201d in Proc. INTERSPEECH, 2020, pp. 4213\u20134217.\n[20] H. Wu, B. Zheng, X. Li, X. Wu, H.-Y. Lee, and H. Meng, \u201cCharacterizing the adversarial vulnerability of speech self-supervised learning,\u201d in Proc. ICASSP, 2022, pp. 3164\u20133168.\n[21] K. Kurita, P. Michel, and G. Neubig, \u201cWeight poisoning attacks on pretrained models,\u201d in Proc. ACL, 2020, pp. 2793\u20132806.\n[22] Y. Zhang, Z. Jiang, J. Villalba, and N. Dehak, \u201cBlack-box attacks on spoofing countermeasures using transferability of adversarial examples.\u201d in Proc. INTERSPEECH, 2020, pp. 4238\u20134242.\n[23] T. Nakamura, Y. Saito, S. Takamichi, Y. Ijima, and H. Saruwatari, \u201cV2S attack: building DNN-based voice conversion from automatic speaker verification,\u201d in Proc. ISCA SSW, 2019, pp. 161\u2013 165.\n[24] M. Ravanelli and Y. Bengio, \u201cSpeaker recognition from raw waveform with SincNet,\u201d in Proc. SLT, 2018, pp. 1021\u20131028.\n[25] N. Vaessen and D. A. Van Leeuwen, \u201cFine-tuning wav2vec2 for speaker recognition,\u201d in Proc. ICASSP, 2022, pp. 7967\u20137971.\n[26] J. Deng, J. Guo, J. Yang, N. Xue, I. Kotsia, and S. Zafeiriou, \u201cArcFace: Additive angular margin loss for deep face recognition,\u201d IEEE TPAMI, vol. 44, no. 10, pp. 5962\u20135979, 2022.\n[27] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013 frequency magnitude masking for speech separation,\u201d IEEE/ACM TASLP, vol. 27, no. 8, pp. 1256\u20131266, 2019.\n[28] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Delgado, A. Nautsch, J. Yamagishi, N. Evans, T. Kinnunen, and K. A. Lee, \u201cAsvspoof 2019: Future horizons in spoofed and fake audio detection,\u201d in Proc. INTERSPEECH, 2019, pp. 1008\u20131012.\n[29] X. Wang, J. Yamagishi, M. Todisco, H. Delgado, A. Nautsch, N. Evans, M. Sahidullah, V. Vestman, T. Kinnunen, K. A. Lee et al., \u201cASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech,\u201d Computer Speech & Language, vol. 64, p. 101114, 2020.\n[30] J. Yamagishi, X. Wang, M. Todisco, M. Sahidullah, J. Patino, A. Nautsch, X. Liu, K. A. Lee, T. Kinnunen, N. Evans, and H. Delgado, \u201cASVspoof 2021: accelerating progress in spoofed and deepfake speech detection,\u201d in Proc. ASVspoof Challenge, 2021, pp. 47\u201354.\n[31] N. M. Mu\u0308ller, F. Dieckmann, P. Czempin, R. Canals, K. Bo\u0308ttinger, and J. Williams, \u201cSpeech is silver, silence is golden: What do ASVspoof-trained models really learn?\u201d in Proc. ASVspoof Challenge, 2021.\n[32] Y.-Y. Yang, M. Hira, Z. Ni, A. Astafurov, C. Chen, C. Puhrsch, D. Pollack, D. Genzel, D. Greenberg, E. Z. Yang et al., \u201cTorchAudio: Building blocks for audio and speech processing,\u201d in Proc. ICASSP, 2022, pp. 6982\u20136986.\n[33] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibriSpeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015, pp. 5206\u20135210.\n[34] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazare\u0301, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen et al., \u201cLibri-Light: A benchmark for ASR with limited or no supervision,\u201d in Proc. ICASSP, 2020, pp. 7669\u20137673.\n[35] G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang et al., \u201cGigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio,\u201d in Proc. INTERSPEECH, 2021, pp. 3670\u20133674.\n[36] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. ACLIJCNLP, 2021, pp. 993\u2014-1003.\n[37] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR, 2015.\n[38] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, \u201cVoxCeleb: Large-scale speaker verification in the wild,\u201d Computer Speech & Language, vol. 60, p. 101027, 2020."
        }
    ],
    "title": "Spoofing Attacker Also Benefits from Self-Supervised Pretrained Model",
    "year": 2023
}