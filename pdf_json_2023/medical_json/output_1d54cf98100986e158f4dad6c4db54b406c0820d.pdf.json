{
    "abstractText": "Alzheimer\u2019s disease (AD) has a complex and multifactorial etiology, which requires integrating information about neuroanatomy, genetics, and cerebrospinal fluid biomarkers for accurate diagnosis. Hence, recent deep learning approaches combined image and tabular information to improve diagnostic performance. However, the black-box nature of such neural networks is still a barrier for clinical applications, in which understanding the decision of a heterogeneous model is integral. We propose PANIC, a prototypical additive neural network for interpretable AD classification that integrates 3D image and tabular data. It is interpretable by design and, thus, avoids the need for post-hoc explanations that try to approximate the decision of a network. Our results demonstrate that PANIC achieves state-of-the-art performance in AD classification, while directly providing local and global explanations. Finally, we show that PANIC extracts biologically meaningful signatures of AD, and satisfies a set of desirable desiderata for trustworthy machine learning. Our implementation is available at https://github.com/ai-med/PANIC.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tom Nuno Wolf"
        },
        {
            "affiliations": [],
            "name": "Sebastian P\u00f6lsterl"
        },
        {
            "affiliations": [],
            "name": "Christian Wachinger"
        }
    ],
    "id": "SP:42656e5d0652ff73ac08d1d7cc9f7d7ef369dbe7",
    "references": [
        {
            "authors": [
                "R. Agarwal",
                "L. Melnick",
                "N Frosst"
            ],
            "title": "Neural Additive Models: Interpretable Machine Learning with Neural Nets",
            "venue": "NeurIPS. vol. 34, pp. 4699\u20134711",
            "year": 2021
        },
        {
            "authors": [
                "C. Chen",
                "O. Li",
                "D Tao"
            ],
            "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition",
            "venue": "NeurIPS. vol. 32",
            "year": 2019
        },
        {
            "authors": [
                "J. Donnelly",
                "A.J. Barnett",
                "C. Chen"
            ],
            "title": "Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes",
            "venue": "CVPR. pp. 10265\u201310275",
            "year": 2022
        },
        {
            "authors": [
                "S. El-Sappagh",
                "T. Abuhmed",
                "S.M.R. Islam",
                "K.S. Kwak"
            ],
            "title": "Multimodal multitask deep learning model for Alzheimer\u2019s disease progression detection based on time series data",
            "venue": "Neurocomputing 412, 197\u2013215",
            "year": 2020
        },
        {
            "authors": [
                "S. Esmaeilzadeh",
                "D.I. Belivanis",
                "K.M. Pohl",
                "E. Adeli"
            ],
            "title": "End-to-end Alzheimer\u2019s disease diagnosis and biomarker identification",
            "venue": "MLMI. pp. 337\u2013345",
            "year": 2018
        },
        {
            "authors": [
                "L.S. Hesse",
                "A.I.L. Namburete"
            ],
            "title": "INSightR-Net: Interpretable Neural Network for Regression Using Similarity-Based Comparisons to Prototypical Examples",
            "venue": "MICCAI. pp. 502\u2013511",
            "year": 2022
        },
        {
            "authors": [
                "D.P. Hibar",
                "J.L. Stein",
                "Renteria",
                "M.E"
            ],
            "title": "Common Genetic Variants Influence Human Subcortical Brain Structures",
            "venue": "Nature 520(7546), 224\u20139",
            "year": 2015
        },
        {
            "authors": [
                "I. Ilanchezian",
                "D. Kobak",
                "H Faber"
            ],
            "title": "Interpretable Gender Classification from Retinal Fundus Images Using BagNets",
            "venue": "MICCAI. pp. 477\u2013487",
            "year": 2021
        },
        {
            "authors": [
                "Jack",
                "C.R"
            ],
            "title": "The Alzheimer\u2019s disease neuroimaging initiative (ADNI): MRI methods",
            "venue": "J Magn Reson Imaging 27(4), 685\u2013691",
            "year": 2008
        },
        {
            "authors": [
                "W. Jagust"
            ],
            "title": "Imaging the evolution and pathophysiology of Alzheimer disease",
            "venue": "Nat Rev Neurosci 19(11), 687\u2013700",
            "year": 2018
        },
        {
            "authors": [
                "E. Kim",
                "S. Kim",
                "M. Seo",
                "S. Yoon"
            ],
            "title": "XProtoNet: Diagnosis in Chest Radiography With Global and Local Explanations",
            "venue": "CVPR. pp. 15719\u201315728",
            "year": 2021
        },
        {
            "authors": [
                "P.J. Kindermans",
                "S. Hooker",
                "J Adebayo"
            ],
            "title": "The (Un)reliability of Saliency Methods",
            "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267\u2013280",
            "year": 2019
        },
        {
            "authors": [
                "J.C. Lambert",
                "C.A. Ibrahim-Verbaas",
                "D Harold"
            ],
            "title": "Meta-Analysis of 74,046 Individuals Identifies 11 New Susceptibility Loci for Alzheimer\u2019s Disease",
            "venue": "Nat Genet 45(12), 1452\u20138",
            "year": 2013
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR",
            "year": 2019
        },
        {
            "authors": [
                "Y. Lou",
                "R. Caruana",
                "J. Gehrke"
            ],
            "title": "Intelligible Models for Classification and Regression",
            "venue": "SIGKDD. pp. 150\u2013158",
            "year": 2012
        },
        {
            "authors": [
                "F.D. Martino",
                "F. Delmastro"
            ],
            "title": "Explainable AI for clinical and remote health applications: a survey on tabular and time series data",
            "venue": "Artif Intell Rev",
            "year": 2022
        },
        {
            "authors": [
                "T. Miyato",
                "T. Kataoka",
                "M. Koyama",
                "Y. Yoshida"
            ],
            "title": "Spectral normalization for generative adversarial networks",
            "venue": "ICLR",
            "year": 2018
        },
        {
            "authors": [
                "M. Narazani",
                "I. Sarasua",
                "S P\u00f6lsterl"
            ],
            "title": "Is a PET All You Need? A Multimodal Study for Alzheimer\u2019s Disease Using 3D CNNs",
            "venue": "MICCAI. pp. 66\u201376",
            "year": 2022
        },
        {
            "authors": [
                "H.D. Nguyen",
                "M. Cl\u00e9ment",
                "B. Mansencal",
                "P. Coup\u00e9"
            ],
            "title": "Interpretable Differential Diagnosis for Alzheimer\u2019s Disease and Frontotemporal Dementia",
            "venue": "MICCAI. pp. 55\u201365",
            "year": 2022
        },
        {
            "authors": [
                "E. Nichols",
                "J.D. Steinmetz",
                "Vollset",
                "S.E"
            ],
            "title": "Estimation of the global prevalence of dementia in 2019 and forecasted prevalence in 2050: an analysis for the global burden of disease study 2019",
            "venue": "Lancet Public Health 7(2), e105\u2013e125",
            "year": 2022
        },
        {
            "authors": [
                "Y. Oba",
                "T. Tezuka",
                "M. Sanuki",
                "Y. Wagatsuma"
            ],
            "title": "Interpretable Prediction of Diabetes from Tabular Health Screening Records Using an Attentional Neural Network",
            "venue": "DSAA. pp. 1\u201311",
            "year": 2021
        },
        {
            "authors": [
                "C. Rudin"
            ],
            "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
            "venue": "Nat Mach Intell 1(5), 206\u2013215",
            "year": 2019
        },
        {
            "authors": [
                "M.R. Sabuncu"
            ],
            "title": "The Dynamics of Cortical and Hippocampal Atrophy in Alzheimer Disease",
            "venue": "Arch Neurol 68(8), 1040",
            "year": 2011
        },
        {
            "authors": [
                "A. Sarica",
                "A. Quattrone",
                "A. Quattrone"
            ],
            "title": "Explainable Boosting Machine for Predicting Alzheimer\u2019s Disease from MRI Hippocampal Subfields",
            "venue": "Brain Informatics. pp. 341\u2013350",
            "year": 2021
        },
        {
            "authors": [
                "P. Scheltens",
                "K. Blennow",
                "Breteler",
                "M.M.B"
            ],
            "title": "Alzheimer\u2019s disease",
            "venue": "The Lancet 388(10043), 505\u2013517",
            "year": 2016
        },
        {
            "authors": [
                "L.N. Smith"
            ],
            "title": "Cyclical learning rates for training neural networks",
            "venue": "WACV. pp. 464\u2013472",
            "year": 2017
        },
        {
            "authors": [
                "C. Wang",
                "Y. Chen",
                "Y Liu"
            ],
            "title": "Knowledge Distillation to Ensemble Global and Interpretable Prototype-Based Mammogram Classification Models",
            "venue": "MICCAI. pp. 14\u201324",
            "year": 2022
        },
        {
            "authors": [
                "J Wen"
            ],
            "title": "Convolutional neural networks for classification of Alzheimer\u2019s disease: Overview and reproducible evaluation",
            "venue": "Med Image Anal 63, 101694",
            "year": 2020
        },
        {
            "authors": [
                "T.N. Wolf",
                "S. P\u00f6lsterl",
                "C. Wachinger"
            ],
            "title": "DAFT: A Universal Module to Interweave Tabular Data and 3D Images in CNNs",
            "venue": "NeuroImage p. 119505",
            "year": 2022
        },
        {
            "authors": [
                "H.A. Yi",
                "C. M\u00f6ller",
                "N Dieleman"
            ],
            "title": "Relation between subcortical grey matter atrophy and conversion from mild cognitive impairment to Alzheimer\u2019s disease",
            "venue": "J Neurol Neurosurg Psychiatry 87(4), 425\u2013432",
            "year": 2015
        },
        {
            "authors": [
                "C. Yin",
                "S. Liu",
                "R. Shao",
                "P.C. Yuen"
            ],
            "title": "Focusing on Clinically Interpretable Features: Selective Attention Regularization for Liver Biopsy Image Classification",
            "venue": "MICCAI. pp. 153\u2013162",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "It is estimated that the number of people suffering from dementia worldwide will reach 152.8 million by 2050, with Alzheimer\u2019s disease (AD) accounting for approximately 60\u201380% of all cases [20]. Due to large studies, like the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI; [9]), and advances in deep learning, the disease stage of AD can now be predicted relatively accurate [28]. In particular, models utilizing both tabular and image data have shown performance superior to unimodal models [4,5,29]. However, they are considered black-box models, as their decision-making process remains largely opaque. Explaining decisions of Convolutional Neural Networks (CNN) is typically achieved with post-hoc techniques in the form of saliency maps. However, recent studies showed that different post-hoc techniques lead to vastly different explanations of the same model [12]. Hence, post-hoc methods do not mimic the true model accurately and have low fidelity [22]. Another drawback of post-hoc techniques is that they ? T.N. Wolf and S. P\u00f6lsterl \u2013 These authors contributed equally to this work. ar X iv :2\n30 3.\n07 12\n5v 2\n[ cs\n.L G\n] 1\n4 M\nprovide local interpretability only, i.e., an approximation of the decision of a model for a specific input sample, which cannot explain the overall decisionmaking of a model. Rudin [22] advocated to overcome these shortcomings with inherently interpretable models, which are interpretable by design. For instance, a logistic regression model is inherently interpretable, because one can infer the decision-making process from the weights of each feature. Moreover, inherently interpretable models do provide both local and global explanations. While there has been progress towards inherently interpretable unimodal deep neural networks (DNNs) [1,11], there is a lack of inherently interpretable heterogeneous DNNs that incorporate both 3D image and tabular data.\nIn this work, we propose PANIC, a Prototypical Additive Neural Network for Interpretable Classification of AD, that is based on the Generalized Additive Model (GAM). PANIC consists of one neural net for 3D image data, one neural net for each tabular feature, and combines their outputs via summation to yield the final prediction (see Fig. 1). PANIC processes 3D images with an inherently interpretable CNN, as proposed in [11]. The CNN is a similarity-based classifier that reasons by comparing latent features of an input image to a set of classrepresentative latent features. The latter are representations of specific images from the training data. Thus, its decision-making can be considered similar to the way humans reason. Finally, we show that PANIC is fully transparent, because it is interpretable both locally and globally, and achieves state-of-the-art performance for AD classification."
        },
        {
            "heading": "2 Related Work",
            "text": "Interpretable Models for Tabular Data. Decision trees and linear models, such as logistic regression, are inherently interpretable and have been applied widely [16]. In contrast, multi-layer perceptrons (MLPs) are non-parametric and non-linear, but rely on post-hoc techniques for explanations. A GAM is a non-linear model that is fully interpretable, as its prediction is the sum of the outputs of univariate functions (one for each feature) [15]. Explainable Boosting Machines (EBMs) extend GAMs by allowing pairwise interaction of features. While this may boost performance compared to a standard GAM, the model is harder to interpret, because the number of functions to consider grows quadratically with the number of features. EBMs were used in [24] to predict conversion to AD.\nInterpretable Models for Medical Images. ProtoPNet [2] is a case-based interpretable CNN that learns class-specific prototypes and defines the prediction as the weighted sum of the similarities of features, extracted from a given input image, to the learned prototypes. It has been applied in the medical domain for diabetic retinopathy grading [6]. One drawback is that prototypes are restricted by the size of local patches: For example, it cannot learn a single prototype to represent hippocampal atrophy, because the hippocampus appears in the left and right hemisphere. As a result, the number of prototypes needs to be increased to learn a separate prototype for each hemisphere. The Deformable ProtoPNet [3]"
        },
        {
            "heading": "3D Image Data Tabular Data",
            "text": "1 ,pMCI2 , as seen on\nthe left. For each categorical feature, such as gender, a linear function is learned. Each continuous feature, such as A\u03b2 is processed with its own MLP. The final prediction is the sum of the outputs of the submodules plus a bias term.\nallows for multiple fine-grained prototypical parts to extract prototypes, but is bound to a fixed number of prototypical parts that represent a prototype. XProtoNet [11] overcomes this limitation by defining prototypes based on attention masks rather than patches; it has been applied for lung disease classification from radiographic images. Wang et al. [27] used knowledge distillation to guide the training of a ProtoPNet for mammogram classification. However, their final prediction is uninterpretable, because it is the average of the prediction of a ProtoPNet and a black-box CNN. The works in [8,19,21,27,31] proposed interpretable models for medical applications, but in contrast to ProtoPNet, XProtoNet and GAMs, they do not guarantee that explanations are faithful to the model prediction [22]."
        },
        {
            "heading": "3 Methods",
            "text": "We propose a prototypical additive neural network for interpretable classification (PANIC) that provides unambiguous local and global explanations for tabular and 3D image data. PANIC leverages the transparency of GAMs by adding functions that measure similarities between an input image and a set of classspecific prototypes, that are latent representations of images from the training data [11].\nLet the input consist ofN tabular features xn \u2208 R (n \u2208 {1, . . . , N}), and a 3D grayscale image I \u2208 R1\u00d7H\u00d7D\u00d7W . PANIC is a GAM comprising N univariate functions fn to account for tabular features, and an inherently interpretable CNN g to account for image data [11]. The latter provides interpretability by learning a set of K \u00d7 C class-specific prototypes (C classes, K prototypes per\nclass c \u2208 {1, . . . , C}). During inference, the model seeks evidence for the presence of prototypical parts in an image, which can be visualized and interpreted in the image domain. Computing the similarities of prototypes to latent features representing the presence of prototypical parts allows to predict the probability of a sample belonging to class c:\np(c |x1, . . . , xN , I) = softmax (\u00b5c) , \u00b5c = \u03b2c0+ \u2211N n=1 f c n(xn)+ \u2211K k=1 g c k(I), (1)\nwhere \u03b2c0 \u2208 R denotes a bias term, f cn(xn) the class-specific output of a neural additive model for feature n, and gck(I) the similarity between the k-th prototype of class c and the corresponding feature extracted from an input image I. We define the functions f cn and gck below."
        },
        {
            "heading": "3.1 Modeling Tabular Data",
            "text": "Tabular data often consists of continuous and discrete-valued features, such as age and genetic alterations. Therefore, we model feature-specific functions f cn depending on the type of feature n. If it is continuous, we estimate f cn nonparametrically using a multi-layer perceptron (MLP), as proposed in [1]. This assures full interpretability while allowing for non-linear processing of each feature n. If feature n is discrete, we estimate f cn parametrically using a linear model, in which case f cn is a step function, which is fully interpretable too. Moreover, we explicitly account for missing values by learning a class-conditional missing value indicator scn. To summarize, f cn is defined as\nf cn(xn) =  scn, if xn is missing, \u03b2cnxn, with \u03b2cn \u2208 R, if xn is categorical MLPcn(xn), otherwise.\n(2)\nPredicting a class with the sum of such univariate functions f cn was proposed in [1] as Neural Additive Model (NAM). Following [1], we apply an `2 penalty on the outputs of f cn(xn):\nLTab(x1, . . . , xn) = 1C \u2211C c=1 \u2211N n=1[f c n(xn)] 2.\nWe want to emphasize that NAMs retain global interpretability by plotting each univariate function f cn over its domain (see Fig. 2). Local interpretability is achieved by evaluating f cn(xn), which equals the contribution of a feature xn to the prediction of a sample, as defined in equation (1) (see Fig. 3 on the left)."
        },
        {
            "heading": "3.2 Modeling Image Data",
            "text": "We model 3D image data by defining the function gck(I) in equation (1) based on XProtoNet [11], which learns prototypes that can span multiple, disconnected regions within an image. In XProtoNet, an image is classified based on the\ncosine similarity between a latent feature vector zpck and learned class-specific prototypes pck, as depicted in the top part of Fig. 1:\ngck(I) = sim(pck, zpck) = pck\u00b7zpck \u2016pck\u2016\u2016zpck\u2016 . (3)\nA latent feature vector zpck is obtained by passing an image I into a CNN backbone U : R1\u00d7H\u00d7D\u00d7W \u2192 RR\u00d7H\u2032\u00d7D\u2032\u00d7W \u2032 , where R is the number of output channels. The result is passed into two separate modules: (i) the feature extractor V : RR\u00d7H\u2032\u00d7D\u2032\u00d7W \u2032 \u2192 RL\u00d7H\u2032\u00d7D\u2032\u00d7W \u2032 maps the feature map to the dimensionality of the prototype space L; (ii) the occurrence module Oc : RR\u00d7H\u2032\u00d7D\u2032\u00d7W \u2032 \u2192 RK\u00d7H\u2032\u00d7D\u2032\u00d7W \u2032 produces K class-specific attention masks. Finally, the latent feature vector zpck is defined as\nzpck = GAP[sigmoid(O c(U(I))k) softplus(V(U(I)))], (4)\nwhere denotes the Hadamard product, and GAP global average pooling.\nIntuitively, zpck represents the GAP-pooled activation maps that a prototype pck would yield if it were present in that image. For visualization, we can upsample the occurrence map Oc(U(I))k to the input dimensions and overlay it on the input image. The same can be done to visualize prototype pck (see Fig 3).\nRegularization. Training XProtoNet requires regularization with respect to the occurrence module and prototype space [11]: An occurrence and affine loss enforce sparsity and spatial fidelity of the attention masks Oc with respect to the image domain:\nLocc(I) = \u2211C c=1\u2016Oc(U(I))\u20161, Laffine(I) = \u2016A(Oc(U(I)))\u2212Oc(U(A(I)))\u20161,\nwith A a random affine transformation. Additionally, latent vectors zpck of an image I with true class label y should be close to prototypes of their respective class, and distant to prototypes of other classes:\nLclst(I) = \u2212maxk,c=y gck(I), Lsep(I) = maxk,c 6=y gck(I)."
        },
        {
            "heading": "3.3 PANIC",
            "text": "As stated in equation (1), PANIC is a GAM comprising functions f c1 , . . . , f cN for tabular data, and functions gc1, . . . , gcK for 3D image data (see equations (2) and (3)). Tabular features contribute to the overall prediction in equation (1) in terms of the values f c1(x1), . . . , f cN (xN ), while the image contributes in terms of the cosine similarity between the class-specific prototype pck and the latent feature vector zpck . By restricting prototypes to contribute to the prediction of a specific class only, we encourage the model to learn discriminative prototypes for each class.\nTo interpret PANIC locally, we simply consider the outputs of the functions f cn, and sum the image-based similarity scores over all prototypes: \u2211K k=1 g c k(I). To interpret the contributions due to the 3D image in detail, we examine the attention map of each prototype, the attention map of the input image, and the similarity score between each prototype and the image (see Fig. 3 on the right). To interpret PANIC globally, we compute the absolute contribution of each function to the per-class logits in equation (1), and average it over all samples in the training set, as seen in Fig. 4. In addition, we can directly visualize the function f cn learned from the tabular data in terms of the log odds ratio\nlog [ p(c |x1, . . . , xn, . . . , xN , I) p(CN |x1, . . . , xn, . . . , xN , I) / p(c |, x1, . . . , x\u2032n, . . . , xN , I) p(CN |x1, . . . , x\u2032n, . . . , xN , I) ] ,\nwhere x\u2032n is the mean value of feature n across all samples for continuous features, and zero for categorical features. As an example, let us consider the AD class. If the log odds ratio for a specific value xn is positive, it indicates that the odds of being diagnosed as AD, compared to CN, increases. Conversely, if it is negative, the odds of being diagnosed as AD decreases.\nwhere LCE is the cross-entropy loss, \u03bb1,...,5 are hyper-parameters, y the true class label, and y\u0302 the prediction of PANIC."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Overview",
            "text": "Dataset. Data used in this work was obtained from the ADNI database.3 We select only baseline visits to avoid data leakage, and use FDG-PET images following the processing pipeline described in [18]. Tabular data consists of the continuous features age, education; the cerobrospinal fluid markers A\u03b2, Tau, pTau; the MRI-derived volumes of the left/right hippocampus and thickness of the left/right entorhinal cortex. The categorical features are gender and 31 ADrelated genetic variants identified in [7,13] and having a minor allele frequency of \u2265 5%. Tabular data was standardized using the mean and standard deviation across the training set. Table 1 summarizes our data.\nImplementation Details. We train PANIC with the loss in equation (5) with AdamW [14] and a cyclic learning rate scheduler [26], with a learning rate of 0.002, and weight decay of 0.0005. We choose a 3D ResNet18 for the CNN backbone U with R = 256 channels in the last ResBlock. The feature extractor V and the occurrence module O are CNNs consisting of 1\u00d7 1\u00d7 1 convolutional layers with ReLU activations. We set K = 2, L = 64, \u03bb1 = 0.01 and \u03bb2,...,5 = 0.5, and norm the length of each prototype vector to one. For each continuous tabular 3 https://adni.loni.usc.edu/ [9]\nfeature n, f cn is a MLP that shares parameters for the class dependent outputs in (2). Thus, each MLP has 2 layers with 32 neurons, followed by an output layer with C neurons. As opposed to [1], we found it helpful to replace the ExU activations by ReLU activations. We apply spectral normalization [17] to make MLPs Lipschitz continuous. We add dropout with a probability of 0.4 to MLPs, and with a probability of 0.1 to all univariate functions in equation (1). The set of affine transformations A comprises all transformations with a scale factor \u2208 [0.8; 1.2] and random rotation \u2208 [\u2212180\u25e6; 180\u25e6] around the origin. We initialize the weights of U from a pre-trained model that has been trained on the same training data on the classification task for 100 epochs with early stopping. We cycle between optimizing all parameters of the network and optimizing the parameters of f cn only. We only validate the model directly after prototypes pck have been replaced with their closest latent feature vector zpck of samples from the training set of the same class. Otherwise, interpretability on an image level would be lost.\nWe perform 5-fold cross-validation, based on a data stratification strategy that accounts for sex, age and diagnosis. Each training set is again split such that 64% remain training and 16% are used for hyper-parameter tuning (validation set). We report the mean and standard deviation of the balanced accuracy (BAcc) of the best hyper-parameters found on the validation sets."
        },
        {
            "heading": "4.2 Classification Performance",
            "text": "PANIC achieves 64.0\u00b14.5% validation BAcc and 60.7\u00b14.4% test BAcc. We compare PANIC to a black-box model for heterogeneous data, namely DAFT [29]. We carry out a random hyper-parameter search with 100 configurations for learning rate, weight decay and the bottleneck factor of DAFT. DAFT achieves a validation and test BAcc of 60.9\u00b10.7% and 56.2\u00b14.5%, respectively. This indicates that interpretability does not necessitate a loss in prediction performance."
        },
        {
            "heading": "4.3 Interpretability",
            "text": "PANIC is easy to interpret on a global and local level. Figure 4 summarizes the average feature importance over the training set. It shows that FDG-PET has on average the biggest influence on the prediction, but also that importance can vary across classes. For instance, the SNP rs429358, which is located in the ApoE gene, plays a minor role for the controls, but is highly important for the AD class. This is reassuring, as it is a well known risk factor in AD [25]. The overall most important SNP is rs62097986, which is suspected to influence brain volumes early in neurodevelopment [7].\nTo get a more detailed inside into PANIC, we visualize the log odds ratio with respect to the function f cn across the domain of the nine most important tabular features in Fig. 2. We can easily see that PANIC learned that atrophy of the left hippocampus increases the odds of being diagnosed as AD. The volume of the right hippocampus is utilized similarly. For MCI, it appears as PANIC has overfit on outliers with very low right hippocampus volume. Overall, the results\nfor left/right hippocampus agree with the observation that the hippocampus is among the first structures experiencing neurodegeneration in AD [23]. The function for left entorhinal cortex thickness agrees with previous results too [23]. An increase in A\u03b2 measured in CSF is associated with a decreased risk of AD [25], which our model captured correctly. The inverse relationship holds for Tau [25], which PANIC confirmed too. The function learned for age shows a slight decrease in the log odds ratio of AD and MCI, except for patients around 60 years of age, which is due to few data samples for this age range in the training data. We note that the underlying causes that explain the evolution from normal aging to AD remain unknown, but since age is considered a confounder one should control for it [10]. Overall, we observe that PANIC learned a highly non-linear function for the continuous features hippocampus volume, entorhinal thickness, A\u03b2, Tau, and age, which illustrates that estimating the functions f cn via MLPs is effective. In our data, males have a higher incidence of AD (see Tab. 1), which is reflected in the decision-making of the model too. Our result that rs28834970 decreases the odds for AD does not agree with previous results [13]. However, since PANIC is fully interpretable, we can easily spot this misconception.\nAdditionally, we can visualize the prototypes by upscaling the attention map specific to each prototype, as produced by the occurrence module, to the input image size and highlighting activations of more than 30% of the maximum value, as proposed in [11] (see Fig. 3 on the right). The axial view of pAD1 shows attention towards the occipital lobe and pAD2 towards one side of the occipital lobe. Atrophy around the ventricles can be seen in FDG-PET [25] and both prototypes pAD1 and pAD2 incorporate this information, as seen in the coronal views. The sagittal views show, that pAD2 focuses on the cerebellum and parts of the occipital lobe. The parietal lobe is clearly activated by the prototype in the sagittal view of pAD1 and was linked to AD previously [25].\nWe can interpret the decision-making of PANIC for a specific subject by evaluating the contribution of each function with respect to the prediction (see Fig. 3 on the left). The patient was correctly classified as AD, and most evidence supports this (red arrows). The only exceptions are the SNPs rs4147929 and rs6087771, which the models treats as evidence against AD. Hippocampus volume contributed most to the prediction, i.e., was most important. Since, the subject\u2019s left hippocampus volume is relatively low (see Fig. 2), this increases our trust in the model\u2019s prediction. The subject is heterozygous for rs429358 (ApoE), a well known marker of AD, which the model captured correctly [25]. The four variants rs9331896, rs10498633, rs4147929, and rs4147929 have been associated with nucleus accumbens volume [7], which is involved in episodic memory function [30]. Atrophy of the nucleus accumbens is associated with cognitive impairment [30]. FDG-PET specific image features present in the image show a similarity of 0.44 to the features of prototype pAD1 . It is followed by minor evidence of features similar to prototype pAD2 . During the prediction of the test subject, the network extracted prototypical parts from similar regions. As seen in the axial view, both parts zpAD1 and zpAD2 contain parts of the occipital lobe. Additionally, they cover a large part of the temporal lobe, which has been linked to AD [25].\nIn summary, the decision-making of PANIC is easy to comprehend and predominantly agrees with current knowledge about AD."
        },
        {
            "heading": "5 Desiderata for Machine Learning Models",
            "text": "We now show that PANIC satisfies four desirable desiderata for machine learning (ML) models, based on the work in [22].\nExplanations must be faithful to the underlying model (perfect fidelity). To avoid misconception, an explanation must not mimic the underlying model, but equal the model. The explanations provided by PANIC are the values provided by the functions f c1 , . . . , f cN , g c 1, . . . , g c K in equation (1). Since PANIC is a GAM, the sum of these values (plus bias) equals the prediction. Hence, explanations of PANIC are faithful to how the model arrived at a specific prediction. We can plot these values to gain local interpretability, as done in Fig. 3.\nExplanations must be detailed. An explanation must be comprehensive such that it provides all information about what and how information is used by a model (global interpretability). The information learned by PANIC can be described precisely. Since PANIC is based on the sum of univariate functions, we can inspect individual functions to understand what the model learned. Plotting the functions f cn over their domain explains the change in odds when the value xn changes, as seen in Fig. 2. For image data, PANIC uses the similarity between the features extracted from the input image and the K class-specific prototypes. Global interpretability is achieved by visualizing the training image a prototype was mapped to, and its corresponding attention map 3.\nA machine learning model should help to improve the knowledge discovery process. For AD, the precise cause of cognitive decline remains\nelusive. Therefore, ML should help to identify biomarkers and relationships, and inform researchers studying the biological causes of AD. PANIC is a GAM, which means it provides full global interpretability. Therefore, the insights it learned from data are directly accessible, and provide unambiguous feedback to the knowledge discovery process. For instance, we can directly infer what PANIC learned about FDG-PET or a specific genetic mutation, as in Figs. 2 and 4. This establishes a feedback loop connecting ML researchers and researchers studying the biological causes of AD, which will ultimately make diagnosis more accurate.\nML models must be easy to troubleshoot. If an ML model produces a wrong or unexpected result, we must be able to easily troubleshoot the model. Since PANIC provides local and global interpretability, we can easily do this. We can use a local explanation (see Fig. 3) to precisely determine what the deciding factor for the prediction was and whether it agrees with our current understanding of AD. If we identified the culprit, we can inspect the function f cn, in the case of a tabular feature, or the prototypes, in the case of the image data: Suppose, the age of the patient in Fig. 3 was falsely recorded as 30 instead of 71. The contribution of age fADage would increase from \u22120.138 to 3.28, thus, dominate the prediction by a large margin. Hence, the local explanation would reveal that something is amiss and prompt us to investigate the learned function fADage (see Fig. 2), which is ill-defined for this age."
        },
        {
            "heading": "6 Conclusion",
            "text": "We proposed an inherently interpretable neural network for tabular and 3D image data, and showcased its use for AD classification. We used local and global interpretability properties of PANIC to verify that the decision-making of our model largely agrees with current knowledge about AD, and is easy to troubleshoot. Our model outperformed a state-of-the-art black-box model and satisfies a set of desirable desiderata that establish trustworthiness in PANIC.\nAcknowledgements This research was partially supported by the Bavarian State Ministry of Science and the Arts and coordinated by the bidt, the BMBF (DeepMentia, 031L0200A), the DFG and the LRZ."
        }
    ],
    "title": "Don\u2019t PANIC: Prototypical Additive Neural Network for Interpretable Classification of Alzheimer\u2019s Disease",
    "year": 2023
}