{
    "abstractText": "Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many realworld applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conformalized intervals with reduced length compared to existing approaches, while maintaining the same certainty level.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daniele Foffano"
        },
        {
            "affiliations": [],
            "name": "Alessio Russo"
        },
        {
            "affiliations": [],
            "name": "Alexandre Proutiere"
        }
    ],
    "id": "SP:a36226a73f27eebbe9ab14fd833afd761aac325b",
    "references": [
        {
            "authors": [
                "L\u00e9on Bottou",
                "Jonas Peters",
                "Joaquin Qui\u00f1onero-Candela",
                "Denis X Charles",
                "D Max Chickering",
                "Elon Portugaly",
                "Dipankar Ray",
                "Patrice Simard",
                "Ed Snelson"
            ],
            "title": "Counterfactual reasoning and learning systems: The example of computational advertising",
            "venue": "Journal of Machine Learning Research,",
            "year": 2013
        },
        {
            "authors": [
                "Thomas G Dietterich",
                "Jesse Hostetler"
            ],
            "title": "Conformal prediction intervals for markov decision process trajectories",
            "venue": "arXiv preprint arXiv:2206.04860,",
            "year": 2022
        },
        {
            "authors": [
                "Yaqi Duan",
                "Zeyu Jia",
                "Mengdi Wang"
            ],
            "title": "Minimax-optimal offpolicy evaluation with linear function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Peter W Glynn"
            ],
            "title": "Importance sampling for monte carlo estimation of quantiles",
            "venue": "Proceedings of the 2nd St. Petersburg Workshop on Simulation,",
            "year": 1996
        },
        {
            "authors": [
                "Josiah Hanna",
                "Peter Stone",
                "Scott Niekum"
            ],
            "title": "Bootstrapping with models: Confidence intervals for off-policy evaluation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Nan Jiang",
                "Jiawei Huang"
            ],
            "title": "Minimax value interval for off-policy evaluation and policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Kallus",
                "Masatoshi Uehara"
            ],
            "title": "Double reinforcement learning for efficient off-policy evaluation in markov decision processes",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Kallus",
                "Masatoshi Uehara"
            ],
            "title": "Efficiently breaking the curse of horizon in off-policy evaluation with double reinforcement learning",
            "venue": "Operations Research,",
            "year": 2022
        },
        {
            "authors": [
                "Ilja Kuzborskij",
                "Claire Vernade",
                "Andras Gyorgy",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Confident off-policy evaluation and selection through self-normalized importance weighting",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Hoang Le",
                "Cameron Voloshin",
                "Yisong Yue"
            ],
            "title": "Batch policy learning under constraints",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jing Lei",
                "Larry Wasserman"
            ],
            "title": "Distribution-free prediction bands for non-parametric regression",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 2014
        },
        {
            "authors": [
                "Lihua Lei",
                "Emmanuel J Cand\u00e8s"
            ],
            "title": "Conformal inference of counterfactuals and individual treatment effects",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2021
        },
        {
            "authors": [
                "Lars Lindemann",
                "Matthew Cleaveland",
                "Gihyun Shim",
                "George J Pappas"
            ],
            "title": "Safe planning in dynamic environments using conformal prediction",
            "venue": "arXiv preprint arXiv:2210.10254,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Lindh",
                "Anders Karl\u00e9n",
                "Ulf Norinder"
            ],
            "title": "Predicting the rate of skin penetration using an aggregated conformal prediction framework",
            "venue": "Molecular Pharmaceutics,",
            "year": 2017
        },
        {
            "authors": [
                "Qiang Liu",
                "Lihong Li",
                "Ziyang Tang",
                "Dengyong Zhou"
            ],
            "title": "Breaking the curse of horizon: Infinite-horizon off-policy estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Charles Lu",
                "Ken Chang",
                "Praveer Singh",
                "Jayashree Kalpathy-Cramer"
            ],
            "title": "Three applications of conformal prediction for rating breast density in mammography",
            "year": 2022
        },
        {
            "authors": [
                "Valery Manokhin"
            ],
            "title": "Awesome conformal prediction, April 2022. \"If you use Awesome Conformal Prediction, please cite it as below.",
            "year": 2022
        },
        {
            "authors": [
                "Harris Papadopoulos",
                "Kostas Proedrou",
                "Volodya Vovk",
                "Alex Gammerman"
            ],
            "title": "Inductive confidence machines for regression",
            "venue": "In European Conference on Machine Learning,",
            "year": 2002
        },
        {
            "authors": [
                "Doina Precup"
            ],
            "title": "Eligibility traces for off-policy policy evaluation",
            "venue": "Computer Science Department Faculty Publication Series,",
            "year": 2000
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov decision processes: discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "Yaniv Romano",
                "Evan Patterson",
                "Emmanuel Candes"
            ],
            "title": "Conformalized quantile regression",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Chengchun Shi",
                "Runzhe Wan",
                "Victor Chernozhukov",
                "Rui Song"
            ],
            "title": "Deeply-debiased off-policy interval estimation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Chengchun Shi",
                "Sheng Zhang",
                "Wenbin Lu",
                "Rui Song"
            ],
            "title": "Statistical inference of the value function for reinforcement learning in infinitehorizon settings",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Faaiz Taufiq",
                "Jean-Francois Ton",
                "Rob Cornish",
                "Yee Whye Teh",
                "Arnaud Doucet"
            ],
            "title": "Conformal off-policy prediction in contextual bandits",
            "venue": "arXiv preprint arXiv:2206.04405,",
            "year": 2022
        },
        {
            "authors": [
                "Philip Thomas",
                "Georgios Theocharous",
                "Mohammad Ghavamzadeh"
            ],
            "title": "High-confidence off-policy evaluation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Philip Thomas",
                "Georgios Theocharous",
                "Mohammad Ghavamzadeh"
            ],
            "title": "High confidence policy improvement",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Robert J Tibshirani",
                "Bradley Efron"
            ],
            "title": "An introduction to the bootstrap",
            "venue": "Monographs on statistics and applied probability,",
            "year": 1993
        },
        {
            "authors": [
                "Ryan J Tibshirani",
                "Rina Foygel Barber",
                "Emmanuel Candes",
                "Aaditya Ramdas"
            ],
            "title": "Conformal prediction under covariate shift",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Masatoshi Uehara",
                "Chengchun Shi",
                "Nathan Kallus"
            ],
            "title": "A review of off-policy evaluation in reinforcement learning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Vladimir Vovk",
                "Alexander Gammerman",
                "Glenn Shafer"
            ],
            "title": "Algorithmic learning in a random world",
            "venue": "Springer Science & Business Media,",
            "year": 2005
        },
        {
            "authors": [
                "Wojciech Wisniewski",
                "David Lindsay",
                "Sian Lindsay"
            ],
            "title": "Application of conformal prediction interval estimations to market makers\u2019 net positions",
            "venue": "In Conformal and Probabilistic Prediction and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Zepu Xi",
                "Xuebin Zhuang",
                "Hongbo Chen"
            ],
            "title": "Conformal prediction for hypersonic flight vehicle classification",
            "venue": "PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Xianghao Zhan",
                "Zhan Wang",
                "Meng Yang",
                "Zhiyuan Luo",
                "You Wang",
                "Guang Li"
            ],
            "title": "An electronic nose-based assistive diagnostic prototype for lung cancer detection with conformal prediction",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nIn this work, we consider the problem of off-policy evaluation (OPE) in finite time-horizon Markov Decision Processes (MDPs). This problem is concerned with the task of learning the expected cumulative reward of a target policy from data gathered under a different behavior policy. In fact, OPE has attracted a lot of attention recently [10], [23], [19], [25], [5], [15] since it is particularly relevant in real-world scenarios where the learner is not allowed to experiment and deploy the target policy to infer its value. In these scenarios, testing a new policy in an online manner can be indeed too risky or unethical (e.g., in finance or healthcare).\nThe main challenge in OPE algorithms stems from the distribution shift of the target and behavior policies. To address this issue, researchers have developed various solutions, often based on Importance Sampling methods (refer to \u00a7II and to [29] for a recent survey). Lastly, while existing OPE algorithms sometimes enjoy asymptotic convergence properties, most of them do not come with accuracy and certainty guarantees [25], [26], [7].\nTo that aim, we are concerned with devising OPE estimators that enjoy non-asymptotic performance guarantees. We leverage techniques from Conformal Prediction (CP) [30], [28], [21], which, directly from the data, allow to build conformalized sets that provably includes the true value of the quantity to be estimated with a prescribed level of certainty. Furthermore, CP is a distribution-free method,\n\u2217 Equal contribution Daniele Foffano, Alessio Russo and Alexandre Proutiere are in the Division of Decision and Control Systems of the EECS School at KTH Royal Institute of Technology, Stockholm, Sweden. {foffano,alessior,alepro}@kth.se\nthus circumventing the burden of estimating a model while providing non-asymptotic guarantees. Due to these desirable properties, CP has been applied with success in many fields, including medicine [14], [33], [16], aerospace engineering [32], finance [31] and safe motion planning [13].\nNevertheless, standard CP assumes to be trained on i.i.d. data, and that at test time the data comes from the same distribution from which the training data was drawn (a.k.a. as distribution/covariates shift). This latter assumption is violated in OPE problems, since the training data is gathered using a policy than is different from the target policy to be evaluated. A solution to address the distribution shift is to leverage the concept of weighted exchangeability [28], [12].\nBy exploiting the concept of weighted exchengeability, we study the conformalized OPE problem for Markov Decision Processes (MDPs). Our method builds on top of the technique described in [24], which introduces conformalized OPE for contextual bandit models (which can be seen as MDPs with i.i.d. states). Compared to [24], we have to handle additional difficulties, including the inherent dependence in the data (which consists of trajectories of a controlled Markov chain) and the statistical hardness of dealing with the distribution shift when the time horizon grows large.\nContribution-wise, we present and empirically evaluate CP algorithms that yield conformalized intervals with reduced length compared to existing approaches, while maintaining the same certainty level. These algorithms are based on the two following new components. (i) Asymmetric score functions: existing CP approaches use symmetric score functions and hence, for our problem, would output conformalized intervals centered on the value of the behavior policy. We introduce asymmetric score functions, so that the CP algorithm yields an interval that efficiently moves its center to follow the distribution shift. In turn, CP with asymmetric score functions results in intervals of smaller size. (ii) We propose methods to address the distribution shift in MDPs.\nWe finally illustrate the performance of our algorithms numerically on the classical inventory control problem [20]. The experiments demonstrate that indeed our algorithms achieve smaller interval lengths than existing approaches, while retaining the same certainty guarantees."
        },
        {
            "heading": "II. RELATED WORK",
            "text": ""
        },
        {
            "heading": "A. Off-Policy Evaluation (OPE)",
            "text": "There are mainly three classes of OPE algorithms in the literature: Direct, Importance Sampling and Doubly Robust Methods. Direct Methods (DMs) learn a model of the system [10], [23] and then evaluate the policy against it. DMs can lead to biased estimators due to a mismatch between the\nar X\niv :2\n30 4.\n02 57\n4v 2\n[ cs\n.L G\n] 1\n9 Se\np 20\n23\nmodel and the true system. Importance Sampling (IS) is a well-known method [19], [25], [5], [15] used to correct the distribution mismatch caused by the discrepancies between the target and the behavior policies by re-weighting the sampled rewards. Still, IS-based algorithms suffer from high variance in long-horizon problems. Doubly Robust (DR) methods combine DMs and IS to obtain more robust estimators [7], [6]. [15] introduce Marginalized Importance Sampling, reducing the variance by applying IS directly on the stationary statevisitation distribution.\nThe aforementioned approaches only provide an accurate point-wise estimate of the policy value, without quantifying its uncertainty. [1] derived confidence intervals (CIs) using the Central Limit Theorem. In [25], [9], the authors leveraged concentration inequalities to estimate good CIs, which, however, tend to be overly-conservative. For short-horizon problems, [26], [5] approximate CIs for OPE can also be found by means of bootstrapping. [22] derives a non-asymptotic CI using concentration bounds on a kernel-based Q-function.\nIn [8], the authors derive an asymptotic CI using Double Reinforcement Learning (DRL), also addressing the curse of the horizon. However, the DRL method might not converge in high-dimensional RL tasks, resulting in an asymptotically biased estimator. [3], [23] derive non-asymptotic and asymptotic CIs by approximating the value function with linear functions, but their approaches might lead to a biased estimator if the model assumption is incorrect. [7] derived a CI that involves solving a linear program, but they assume the observations to be i.i.d., whereas transitions are time-dependent in many RL problems."
        },
        {
            "heading": "B. Conformal Prediction (CP)",
            "text": "CP is a frequentist technique to derive CIs with a specified coverage (i.e., confidence) and a finite number of i.i.d. samples (we refer the reader to [17] for a comprehensive list of CP-related papers). The advantage of CP with respect to other methods is that the provided coverage guarantees are distribution-free and non-asymptotic.\nCP for off-policy evaluation has been recently applied to the contextual bandit setting [24], which, in contrast to our work, has no dynamics and no time-dependent data. To address the distribution shift, the authors in [24] use of the weighted exchangeability property, which was previously introduced in [28]. In [2], the authors apply CP to predict the expected value of MDPs trajectories. They consider an online setting where they do not have to deal with the distribution shift."
        },
        {
            "heading": "III. PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "A. Off-policy evaluation in Markov Decision Processes",
            "text": "We consider finite-time horizon MDPs [20]. Such an MDP is defined by a tuple M = \u27e8X ,A, T, q, p,H\u27e9, where X and A are the (finite) state and action spaces, respectively. For all (x, a) \u2208 X\u00d7A, T (\u00b7|x, a) and q(\u00b7|x, a) denote the distributions of the next state and of the instantaneous reward given that the current state is x and that the decision maker selects action a (for simplicity, we assume that the transition probabilities and the reward distributions are stationary; our results can be\neasily generalized to non-stationary dynamics and rewards). Finally, p \u2208 \u2206(S) denotes the distribution of the initial state, and H the time horizon.\nIn off-policy evaluation, we gather data using a behavior policy \u03c0b, and we wish to estimate the value function of different policy \u03c0. Here again for simplicity, we consider stationary policies: both \u03c0b and \u03c0 are mappings between the state space and the set \u2206(A) of distributions over actions. The value function of \u03c0 maps the initial state x to the expected reward gathered under \u03c0 when starting in x: V \u03c0H(x) = E\u03c0[ \u2211H t=1 rt|x1 = x], where rt \u223c q(\u00b7|xt, at), at \u223c \u03c0(\u00b7|xt), and xt+1 \u223c T (\u00b7|xt, at) for t = 1, . . . ,H ."
        },
        {
            "heading": "B. Standard Conformal Prediction",
            "text": "Conformal Prediction (CP) is a method for distributionfree uncertainty quantification of learning methods, see e.g. [30], [18], [11]. To illustrate how CP works, we consider classical supervised learning tasks and restrict our attention to split CP where the pre-training and the calibration phases are conducted on different datasets. The learner starts with a pretrained model f\u0302 : X \u2192 Y that maps inputs to predicted labels (this model may also consist of upper and lower estimated quantiles if the pre-training procedure corresponds to quantile regression). She also has i.i.d. calibration data Dcal = {Xi, Yi}ni=1\ni.i.d.\u223c PX,Y . From f\u0302 and Dcal, CP constructs for each possible input x a subset C\u0302n(x) of possible labels. More precisely, the method proceeds as follows: (i) first a score function s : X \u00d7 Y \u2192 R is constructed from the model f\u0302 (e.g., it could be the residuals |y\u2212 f\u0302(x)| if Y \u2282 R); (ii) the scores of the various calibration samples are computed Vi = s(Xi, Yi), and (iii) the confidence set is built according to C\u0302n(x) = {y \u2208 Y : s(x, y) \u2264 \u03b7}, where \u03b7 = Quantile1\u2212\u03b1 ( 1\nn+1 (\u2211n i=1 \u03b4Vi + \u03b4{\u221e} )) . If\n(X1, Y1), . . . , (Xn+1, Yn+1) are exchangeable, this construction ensures coverage with certainty level 1\u2212 \u03b1:\n1\u2212 \u03b1 \u2264 P(Y \u2208 C\u0302n(X)) \u2264 1\u2212 \u03b1+ 1\nn+ 1 . (1)"
        },
        {
            "heading": "IV. CONFORMALIZED OFF-POLICY EVALUATION",
            "text": "Our objective is to get conformalized predictions for the value function of a policy \u03c0, based on training and calibration data gathered under a different behavior policy \u03c0b. We address this distribution shift by extending and improving the techniques developed in [28], [24]. We apply the CP formalism where the input X corresponds to the initial state, and the output Y to V \u03c0H(X). Our method is illustrated in Figure 1. Next, we describe its components in detail. Specifically, (i) we explain how the aforementioned distribution shift can be addressed by weighing scores; (ii) we then discuss the important choice of the score function."
        },
        {
            "heading": "A. Weighted conformal prediction",
            "text": "As suggested [28], [24], we can handle the distribution shift by weighing the scores using estimates of the likelihood ratio\nw(x, y) := dP\u03c0X,Y\ndP\u03c0 b\nX,Y\n(x, y) = dP\u03c0Y |X\ndP\u03c0 b Y |X (y|x),\nSplit CP Scores re-weighting\nFig. 1. Conformal prediction for off-policy evaluation. The dataset D is collected using a behavior policy \u03c0b, which is then split into the training Dtr and calibration Dcal datasets. When evaluating a different policy \u03c0, there is a shift in the data distribution, and we need to learn a likelihood ratios w\u0302 to compensate for this shift. The training data is used to learn estimates of the weights w\u0302 and a model f\u0302 used in the computation of the scores. The estimated weights are used as plug-in estimates to re-weight the cumulative distribution function of the scores F\u0302x,yn , which is then used to compute the conformalized intervals C\u0302n(x).\nwhere for any policy \u03c0\u2032 \u2208 {\u03c0, \u03c0b}, P\u03c0\u2032X,Y (x, y) = P\u03c0 \u2032\nY |X(y|x)p(x) denotes the distribution of the observation (X,Y ) under \u03c0\u2032 (P\u03c0 \u2032\nY |X is this distribution given X), and p(x) is the initial state distribution, which is the same in both cases. The value of a given trajectory \u03c4 = {x1, a1, r1, . . . , xH , aH , rH} is y = \u2211H t=1 rt. For any policy \u03c0\u2032 \u2208 {\u03c0, \u03c0b}, the probability of observing \u03c4 under \u03c0\u2032 given the initial state x1 = x is:\nP\u03c0 \u2032 (\u03c4 |x) =\u03c0\u2032(a1|x)q(r1|x, a1) H\u220f t=2 \u03c0\u2032(at|xt)\n\u00d7 T (xt|xt\u22121, at\u22121)q(rt|xt, at).\nHence the weights can be written as:\nw(x, y) =\n\u222b 1{y= \u2211H t=1 rt}\nP\u03c0(\u03c4 |x)d\u03c4\u222b 1{y= \u2211H t=1 rt} P\u03c0b(\u03c4 |x)d\u03c4 .\nWe make the following assumption to guarantee that the above weights are always well defined, and that the calibration data is i.i.d.\nAssumption 1: We assume throughout the paper that P\u03c0(\u00b7|x) is absolutely continuous w.r.t. P\u03c0b(\u00b7|x) for all x \u2208 X . We further assume that calibration data Dcal provides n i.i.d. samples (Xi, Yi) \u223c P\u03c0 b\nX,Y . Then, we can compute the scores Vi = s(Xi, Yi). For each possible pair (x, y), using the normalized weights, we form the distribution F\u0302 x,yn := \u2211n i=1 p w i (x, y)\u03b4Vi + p w n+1(x, y)\u03b4\u221e, with\npwi (x, y) =  w (Xi, Yi)\u2211n j=1 w (Xj , Yj) + w(x, y) if i \u2264 n,\nw (x, y)\u2211n j=1 w (Xj , Yj) + w(x, y) if i = n+ 1,\n(2) and the conformalized set\nC\u0302n(x) := { y \u2208 R : s(x, y) \u2264 Quantile1\u2212\u03b1 ( F\u0302 x,yn )} . (3)\nProposition 1: Under Assumption 1, for any score function s and any \u03b1 \u2208 (0, 1),\nP\u03c0 b,\u03c0 [ Y \u2208 C\u0302n(X) ] \u2265 1\u2212 \u03b1, (4)\nwhere P\u03c0b,\u03c0 accounts for the randomness of (X,Y ) \u223c P\u03c0X,Y and that of the data Dcal = {Xi, Yi}ni=1 (with for all i \u2208 [n], (Xi, Yi) \u223c P\u03c0 b X,Y ).\nProof: The proof follows that in [24, Proposition 4.1]. The idea relies on the fact that {(Xi, Yi)}ni=1\u222a(Xn+1, Yn+1) are weighted exchangeable (see Lemma 1 in the appendix), where (Xn+1, Yn+1) is sampled according to P\u03c0X,Y and {(Xi, Yi)}ni=1 according to P\u03c0 b\nX,Y . Then, assume for simplicity that V1, . . . , Vn+1 are distinct almost surely. We define f as the joint distribution of the random variables {Xi, Yi}n+1i=1 . We also denote Ez as the event of {Z1, . . . , Zn+1} = {z1, . . . , zn+1} (where the equality refers to the equality between sets) and let vi = s (zi) = s (xi, yi). Then, for each i :\nP [Vn+1 = vi | Ez] = P [Zn+1 = zi | Ez] ,\n=\n\u2211 \u03c3:\u03c3(n+1)=i f ( z\u03c3(1), . . . , z\u03c3(n+1) )\u2211 \u03c3 f ( z\u03c3(1), . . . , z\u03c3(n+1)\n) Now using the fact that Z1, . . . , Zn+1 are weighted exchangeable, as in [24] we find that P [Zn+1 = zi | Ez] = pwi (zn+1) .\nNext, just as in [28] we can view:\n{Vn+1 = vi | Ez} \u223c n+1\u2211 i=1 pwi (zn+1) \u03b4vi\nwhich implies that:\nP [ Vn+1 \u2264 Quantile1\u2212\u03b1 ( n+1\u2211 i=1 pwi (Xn+1) \u03b4vi ) | Ez ] \u2265 1\u2212\u03b1.\nMarginalizing over Ez concludes the proof. Proposition 1 shows that, in absence of data from the target policy, we can still use a shifted CDF of the scores to assess the target policy. The result however relies on the assumption that the weights w(x, y) are known. In practice, we could use the training data to learn these weights, refer to Section V for details. The next proposition quantifies the impact of the error in this estimation procedure on the coverage. Its proof follows the same arguments as those in [24].\nProposition 2: Assume that the conformalized sets (3) are defined using estimated the weights w\u0302(x, y) satisfying E\u03c0b [w\u0302(X,Y )r] \u2264 Mrr < \u221e for some r \u2265 2. Define \u2206w = 1 2E \u03c0b |w\u0302(X,Y )\u2212 w(X,Y )|. Then\nP\u03c0 b,\u03c0 [ Y \u2208 C\u0302n(X) ] \u2265 1\u2212 \u03b1\u2212\u2206w, (5)\nIf, in addition, the non-conformity scores {Vi}ni=1 have no ties almost surely, then we also have\nP\u03c0 b,\u03c0 [ Y \u2208 C\u0302n(X) ] \u2264 1\u2212 \u03b1+\u2206w + cn1/r\u22121,\nfor some positive constant c depending on Mr and r only. Proof: The proof is omitted for brevity, since it follows mutatis mutandis from that in [24, Proposition 4.2]."
        },
        {
            "heading": "B. Selecting the score function",
            "text": "The choice of the score function critically impacts the size and center of the conformalized sets C\u0302n(x). In previous work [21], [24], the pre-training procedure outputs some estimated quantiles q\u03b1lo(x) and q\u03b1hi(x) for the value of the behavior policy with initial state x, and the use of the symmetric score function\ns(x, y) = max(q\u03b1lo(x)\u2212 y, y \u2212 q\u03b1hi(x)), (6)\nis advocated. This choice yields a set C\u0302n(x) centered q\u0304\u03c0b(x) = (q\u03b1lo(x) + q\u03b1hi(x))/2. Indeed, in view of (3) and (6), there is \u03b7(x) \u2208 R such that C\u0302n(x) = [q\u0304\u03c0b(x) \u2212 \u03b7(x), q\u0304\u03c0b(x) + \u03b7(x)] (note that when n grows large, \u03b7(x) becomes independent of x). Having C\u0302n(x) centered on the estimated median value for \u03c0b is of course very problematic when the values of \u03c0b and \u03c0 significantly differ. In this case, the length of C\u0302n(x) becomes unnecessarily large. Next we propose methods and score functions that efficiently re-center C\u0302n(x) around the value of \u03c0 (instead of \u03c0b), and that in turn yield much smaller conformalized sets.\n1) Double-quantile score: a first idea is to break the symmetry of the score function used in [24] by considering the following confidence set\nC\u0302n(x) := { y \u2208 R : q\u03b1lo(x)\u2212 y \u2264 Quantile1\u2212\u03b1/2 ( F\u0302 x,yn,0 )} \u2229 { y \u2208 R : y \u2212 q\u03b1hi(x) \u2264 Quantile1\u2212\u03b1/2 ( F\u0302 x,yn,1 )} ,\n(7) where F\u0302 x,yn,0 := \u2211n i=1 p w i (x, y)\u03b4Vi,0 + p w n+1(x, y)\u03b4\u221e and\nF\u0302 x,yn,1 := \u2211n i=1 p w i (x, y)\u03b4Vi,1 + p w n+1(x, y)\u03b4\u221e, with Vi,0 = q\u03b1lo(Xi) \u2212 Yi and Vi,1 = Yi \u2212 q\u03b1hi(Xi). In essence, we separately look at the lower and upper quantiles of the shifted distribution of the scores. A graphical illustration is provided in Fig. 2. The new construction of C\u0302n(x) does not affect coverage guarantees:\nProposition 3: Under Assumption 1, for \u03b1 \u2208 (0, 1) the sets C\u0302n(x) in (7) satisfies\nP\u03c0 b,\u03c0 [ Y \u2208 C\u0302n(X) ] \u2265 1\u2212 \u03b1. (8)\nProof: The proof follows from that of Proposition 1. Assume for simplicity that for a fixed j \u2208 {0, 1} the values {Vi,j}ni=1 are distinct almost surely and let s0(x, y) = q\u03b1lo(x)\u2212 y, s1(x, y) = y \u2212 q\u03b1hi(x). As before, we define f as the joint distribution of the random variables {Xi, Yi}n+1i=1 . Recall that Zi = (Xi, Yi), then, we denote by Ez the event that {Z1, . . . , Zn+1} = {z1, . . . , zn+1} and let vi,j = sj (zi) = sj (xi, yi) for j \u2208 {0, 1}. First, following the previous proposition, we observe that\n{Vn+1,j = vi,j | Ez} \u223c n+1\u2211 i=1 pwi (zn+1) \u03b4vi,j , j \u2208 {0, 1},\n(9) and, similarly, P [ Vn+1,j \u2264 \u03b71\u2212\u03b1/2(Zn+1) | Ez ] \u2265 1 \u2212 \u03b1/2, j \u2208 {0, 1}, where\n\u03b71\u2212\u03b1/2(Zn+1) = Quantile1\u2212\u03b1/2 ( n+1\u2211 i=1 pwi (Zn+1) \u03b4vi,j ) .\nLet C\u0302n,j = { y \u2208 R : sj(x, y) \u2264 Quantile1\u2212\u03b1/2 ( F\u0302 x,y\u03b7,j )} ,\nthen P\u03c0b,\u03c0 [ Y /\u2208 C\u0302n,j(X)|Ez ] \u2264 \u03b1/2, j \u2208 {0, 1}, from which follows (through a union bound) P\u03c0b,\u03c0 [ Y /\u2208 C\u0302n(X)|Ez ] \u2264 \u03b1. We get the claim after marginalizing over Ez . We also obtain the following guarantees in case w(x, y) is replaced by w\u0302(x, y).\nProposition 4: Let C\u0302n(x) be as in (7) with weights w(x, y) replaced by w\u0302(x, y). Under the same assumptions as in Proposition 2, we have\nP\u03c0 b,\u03c0 [ Y \u2208 C\u0302n(X) ] \u2265 1\u2212 \u03b1\u2212\u2206w.\nIf, in addition, non-conformity scores {Vi,0}ni=1 and {Vi,1} n i=1 have no ties almost surely, then we also have\nP\u03c0 b,\u03c0 [ Y \u2208 C\u0302n(X) ] \u2264 1\u2212 \u03b1+\u2206w + cn1/r\u22121,\nfor some positive constant c depending only on Mr and r. Proof: We take inspiration from [24, Proposition 4.2]. For the sake of notation, we denote the test point by (X \u2032, Y \u2032) instead of (Xn+1, Yn+1). We also denote by C\u0302n(x) the confidence set C\u0302n(x) := C0,n(x) \u2229 C1,n(x), with\nC\u03020,n(x) := { y \u2208 R : q\u03b1lo(x)\u2212 y \u2264 Quantile1\u2212\u03b1/2 ( F\u0302 x,y\u03b7,0 )} ,\nC\u03021,n(x) := { y \u2208 R : y \u2212 q\u03b1hi(x) \u2264 Quantile1\u2212\u03b1/2 ( F\u0302 x,y\u03b7,1 )} where F\u0302 x,yn,0 := \u2211n i=1 p w\u0302 i (x, y)\u03b4Vi,0 + p w\u0302 n+1(x, y)\u03b4\u221e and\nF\u0302 x,yn,1 := \u2211n i=1 p w i (x, y)\u03b4Vi,1 + p w n+1(x, y)\u03b4\u221e, with Vi,0 and Vi,1 are as before. We first prove the lower bound. Let P\u0303\u03c0X,Y be a probability measure with dP\u0303\u03c0X,Y (x, y) = w\u0302(x, y)dP \u03c0b\nX,Y (x, y). Further, let TV(P,Q) be the total variation distance between two distributions P,Q, and observe that\nTV(P\u0303\u03c0, P\u03c0) = 1\n2\n\u222b |w\u0302(x, y)\u2212 w(x, y)|dP\u03c0 b\n(x, y) = \u2206w.\nFirst, note that from an application of Proposition 3 we have that P(X,Y )\u223cP\u0303\u03c0X,Y [(Y \u2208 C\u0302n(X)] \u2265 1\u2212 \u03b1. Then, we can use the total variation to bound the difference in probability\n|P(X,Y )\u223cP\u0303\u03c0X,Y [Y \u2208 C\u0302n(X)]\u2212 P(X,Y )\u223cP\u03c0X,Y [Y \u2208 C\u0302n(X)]| \u2264 \u2206w.\nUsing the triangle inequality we find the lower bound:\nP(X,Y )\u223cP\u03c0X,Y [Y \u2208 C\u0302n(X)] \u2265 1\u2212 \u03b1\u2212\u2206w.\nWe now prove the upper bound. The moment assumption on w\u0302(x, y) guarantees that w\u0302 is bounded a.s. under P\u03c0 b\n. Then, W.l.o.g., assume E\n(X,Y )\u223cP\u03c0bX,Y [w\u0302(X,Y )] = 1. As shown in\n[24, Proposition 4.2], we have E(X,Y )\u223cP\u0303\u03c0X,Y [w\u0302(X,Y )] \u2264 M 2 r . Then\nP[Y \u2032 \u2208C\u0302n(X \u2032)] = P[Y \u2032 \u2208 C\u03020,n(X \u2032) \u2229 Y \u2032 \u2208 C\u03021,n(X \u2032)], \u2264 min [ P(Y \u2032 \u2208 C\u03020,n(X \u2032)),P(Y \u2032 \u2208 C\u03021,n(X \u2032)) ] .\nThe rest of the proof follows as in [24, Proposition 4.2], where we note that P[Y \u2032 \u2208 C\u0302i,n(X \u2032)] \u2264 1\u2212\u03b1+cn1/r\u22121, i \u2208 {1, 2}, and thus P(X,Y )\u223cP\u0303\u03c0X,Y [Y\n\u2032 \u2208 C\u0302n(X \u2032)] \u2264 1 \u2212 \u03b1 + cn1/r\u22121. Using the triangle inequality on\n|P(X,Y )\u223cP\u0303\u03c0X,Y [Y \u2208 C\u0302n(X)]\u2212 P(X,Y )\u223cP\u03c0X,Y [Y \u2208 C\u0302n(X)]| \u2264 \u2206w.\nwe conclude that P(X,Y )\u223cP\u03c0X,Y [Y \u2032 \u2208 C\u0302n(X \u2032)] \u2264 1 \u2212 \u03b1 + \u2206w + cn 1/r\u22121.\n2) Shifted values: a second idea is to simply shift the values of the behavior policy \u03c0b using the likelihood ratios w(x, y), as one would in important sampling methods. This can be done by simply using s(x, y) = y. This choice of score function makes sense intuitively: if we are interested in the value of the target policy \u03c0, then we may look at the shifted distribution of the values of the behavior policy.\nWe may also combine this choice with the double-quantile idea and construct C\u0302n(x) as\nC\u0302n(x) = C\u0302n,0(x) \u2229 C\u0302n,1(x), (10) where C\u0302n,0 = { y \u2208 R : y \u2265 Quantile\u03b1/2 ( F\u0302 x,yn )} and\nC\u0302n,1 = { y \u2208 R : y \u2264 Quantile1\u2212\u03b1/2 ( F\u0302 x,yn )} . Propositions 3 and 4 also hold for this choice."
        },
        {
            "heading": "V. OFFLINE ESTIMATION OF THE LIKELIHOOD RATIOS",
            "text": "In this section, we present various ways to estimate the likelihood ratios w(x, y), and discuss their pros and cons."
        },
        {
            "heading": "A. Monte-Carlo method",
            "text": "To estimate w(x, y), we need to compute P\u03c0X,Y (x, y) and P\u03c0 b\nX,Y (x, y). Recall that the likelihood ratio is equal to\nw(x, y) =\n\u222b 1{y= \u2211H t=1 rt}\nP\u03c0(\u03c4 |x)d\u03c4\u222b 1{y= \u2211H t=1 rt} P\u03c0b(\u03c4 |x)d\u03c4 ,\nwhere \u03c4 is a trajectory of length H . Since P\u03c0(\u03c4 |x) (sim. P\u03c0 b\n(\u03c4 |x)) depends on the transition kernel T and the reward distribution q, one needs to estimate these distributions from the data. We may proceed as follows:\n1) We use the training data Dtr to compute an estimate (T\u0302 , q\u0302) of (T, q) (through maximum likelihood). 2) Compute an estimate of w\u0302(x, y) through Monte-Carlo sampling:\nw\u0302(x, y) = (1/h)\n\u2211h k=1 1{y= \u2211H t=1 r (k) t }\n(1/h) \u2211h\nk=1 1{y= \u2211H\nt=1 r (k)\u2032 t }\n, (11)\nwhere r(k)t and r (k)\u2032\nt are sequences of rewards generated, respectively, by starting in x and following \u03c0 and \u03c0b, and h is the number of Monte Carlo samples. These trajectories are generated using T\u0302 and q\u0302, estimated in the previous step.\nThis approach has various shortcomings. First it requires us to estimate the model (T, q). Then it forces us to generate a large number of trajectories, which is heavy computationally. Finally, the term 1{y=\u2211Ht=1 rt} is going to be 0 most of the times. A possible way to alleviate this issue consists in not including the last reward in the trajectory \u03c4 . This implies that we replace 1{y=\u2211Ht=1 rt} by q\u0302(y \u2212\u2211H\u22121n=1 rn|xH , aH). As it turns out, this naive Monte-Carlo method, used with success in simple scenarios (contextual bandits [24]), does not work in MDPs."
        },
        {
            "heading": "B. Empirical and gradient-based methods",
            "text": "Next we present an alternative and more scalable way to estimate the weights w(x, y) from the training dataset Dtr. We make use of the following simple rewriting of the likelihood ratio (also suggested in [24]):\nw(x, y) = P\u03c0X,Y (x, y)\nP\u03c0 b X,Y (x, y) ,\n=\n\u222b P\u03c0X,Y (x, y)\nP\u03c0 b\nX,Y (x, y)\nP\u03c0 b\n\u03c4 |X,Y (\u03c4 |x, y) P\u03c0 b \u03c4 |X,Y (\u03c4 |x, y) P\u03c0\u03c4 |X,Y (\u03c4 |x, y)d\u03c4,\n=\n\u222b P\u03c0X,Y,\u03c4 (x, y, \u03c4)\nP\u03c0 b X,Y,\u03c4 (x, y, \u03c4) P\u03c0\nb\n\u03c4 |X,Y (\u03c4 |x, y)d\u03c4,\n= E \u03c4\u223cP\u03c0b\n\u03c4|X=x,Y =y\n[ P\u03c0X,Y,\u03c4 (x, y, \u03c4)\nP\u03c0 b\nX,Y,\u03c4 (x, y, \u03c4)\n] .\nNext, observe that:\nP\u03c0X,Y,\u03c4 (x, y, \u03c4) P\u03c0 b X,Y,\u03c4 (x, y, \u03c4) = P (y|x, \u03c4)P\u03c0(\u03c4 |x) P (y|x, \u03c4)P\u03c0b(\u03c4 |x) = \u220fH t=1 \u03c0(at|xt)\u220fH t=1 \u03c0 b(at|xt) .\nHence, learning w amounts to learning the following expectation:\nw(x, y) = E \u03c4\u223cP\u03c0b\n\u03c4|X=x,Y =y [ \u220fH t=1 \u03c0(at|xt)\u220fH t=1 \u03c0 b(at|xt) ] . (12)\nTo this aim, we propose the following two approaches.\n1) Empirical estimator: this method applies to the case x and y belong to some finite spaces X and Y only. In this case, we can directly estimate w(x, y) from the training data Dtr by simply computing\nw\u0302(x, y) = 1\nN(x, y) \u2211 \u03c4 i\u2208Dtr(x,y) \u220fH t=1 \u03c0(a (i) t |x (i) t )\u220fH t=1 \u03c0 b(a (i) t |x (i) t ) , (13)\nwhere the training data Dtr consists of m trajectories generated under \u03c0b, the i-th trajectory in this dataset is \u03c4i = (x (i) t , a (i) t , r (i) t ) H t=1, Dtr(x, y) are trajectories with initial state and the accumulated reward x and y, respectively, and N(x, y) = |Dtr(x, y)|. When the likelihood ratios are bounded, we can quantify the accuracy of the above estimates using standard concentration results: Proposition 5: Let (\u03b5, \u03b4) \u2208 (0, 1). Assume the ratio\u220fH t=1 \u03c0(at|xt)/ \u220fH t=1 \u03c0\nb(at|xt) to be bounded in [m,M ] for all possible trajectories of horizon H generated under \u03c0b. If minx,y N(x, y) \u2265 (M\u2212m) 2 2\u03b52 ln 2|X ||Y| \u03b4 , then\nP\u03c0 b [|w\u0302(X,Y )\u2212 w(X,Y )| > \u03b5] < \u03b4.\nFurthermore, we also have \u2206w \u2264 (M\u2212m)|X ||Y| \u221a \u03c0\n2 \u221a 2minx,y N(x,y) .\nProof: The proof is a simple application of Hoeffding\u2019s inequality: P[|w\u0302(X,Y ) \u2212 w(X,Y )| > \u03b5] <\u2211\nx,y 2e \u22122N(x,y)\u03b52 (M\u2212m)2 \u2264 2|X ||Y|e \u22122\u03b52 minx,y N(x,y)\n(M\u2212m)2 , where we made use also of a union bound over X \u00d7 Y . Then, if we choose minx,y N(x, y) \u2265 (M\u2212m) 2 2\u03b52 ln 2|X ||Y|\n\u03b4 then P[|w\u0302(X,Y ) \u2212 w(X,Y )| > \u03b5] < \u03b4. Regarding the inequality, we find that E[|w\u0302(X,Y ) \u2212 w(X,Y )|] \u2264\n2|X ||Y| \u222b\u221e 0 e \u22122\u03b52 minx,y N(x,y) (M\u2212m)2 d\u03b5 = (M\u2212m)|X ||Y| \u221a \u03c0\u221a\n2minx,y N(x,y) , and\nthus \u2206w \u2264 (M\u2212m)|X ||Y| \u221a \u03c0\n2 \u221a 2minx,y N(x,y) .\nThe quantities M and m are usually function of the horizon H (in general one can choose m = 0). For example, in case A is finite, we obtain:\n\u2022 If \u03c0b is uniform over A, then an upper bound M is given by |A|H , and m = (|A|minx,a \u03c0(a|x))H . \u2022 In case \u03c0b and \u03c0 are convex mixtures of a uniform distribution with another deterministic policy \u03c0\u0302, for example \u03c0(a|x) = \u03f5|A| + (1 \u2212 \u03f5)1{a=\u03c0\u0302(x)} (sim. \u03c0 b\nwith \u03f5b), for some \u03f5, \u03f5b \u2265 0, then one can choose M,m\nAlgorithm 1 Conformal Off-Policy Evaluation in MDPs Require: Datasets Dtr,Dcal; target coverage \u03b1; policies\n(\u03c0b, \u03c0); score function s; test input xtest. 1: Use Dtr to learn the quantiles q\u03b1lo(x) and q\u03b1hi(x), as\nwell as the weight w\u0302(x, y) using either the empirical estimator or the gradient-based method. 2: Compute F\u0302 x,yn and the conformalized set C\u0302n(x test) using\nw\u0302(x, y) and the scores derived from the dataset Dcal using either (3) or (7) or (10). Return C\u0302n(xtest)\nas\nM1/H = max\n( \u03f5\n\u03f5b , (1\u2212 \u03f5) + \u03f5/|A| (1\u2212 \u03f5b) + \u03f5b/|A|\n) ,\nm1/H = min\n( \u03f5\n\u03f5b , (1\u2212 \u03f5) + \u03f5/|A| (1\u2212 \u03f5b) + \u03f5b/|A|\n) .\nSee also Fig. 3 for an example of the scaling of M \u2212m. In general, we see that the dependency on H is mild when \u03c0 and \u03c0b that are somehow similar. As a future research direction, we could investigate possible ways to alleviate the impact of H (for example, by looking at the stationary rewards of the MDP, as in [15]).\n2) Gradient method: an alternative approach is to notice that w, as suggested in [24], can be seen as the solution of a MSE minimization problem. Indeed, w solves the following problem:\nmin f E (X,Y,\u03c4)\u223cP\u03c0bX,Y,\u03c4 ( \u220fHt=1 \u03c0(at|xt)\u220fH t=1 \u03c0 b(at|xt) \u2212 f(X,Y ) )2 . (14) Therefore, given some function approximator f\u03b8 parametrized by \u03b8, we can minimize over \u03b8 the following empirical risk:\n1\nm \u2211 \u03c4 i\u2208Dtr\n( \u220fH t=1 \u03c0(a (i) t |x\n(i) t )\u220fH\nt=1 \u03c0 b(a (i) t |x (i) t )\n\u2212 f\u03b8 ( x (i) 1 , H\u2211 t=1 r (i) t ))2 .\nAs one would expect, this method still suffers from a large variance. For large horizons, it becomes quite difficult to learn the ratio of probabilities, especially when the two policies are extremely different. In fact for large H , in case the two policies are different, then it is likely that the ratio of action probabilities is 0 most of the time, with very few values different from 0 that tend to be extremely large. This makes the training procedure difficult, since most function approximators will just learn to output 0."
        },
        {
            "heading": "C. Algorithm",
            "text": "To conclude this section, we present a generic sketch of our proposed algorithm, see Algorithm 1 for a pseudocode. Following the split conformal prediction method, the algorithm first leverages the training data Dtr to estimate the quantiles of the value of \u03c0b and the weights w. It then uses the calibration data Dcal to compute the non-conformity scores. Using w\u0302 as a plug-in estimate in the re-weighted\nscores distribution F\u0302 x,yn , the algorithm can finally build the conformal prediction set C\u0302n(xtest)."
        },
        {
            "heading": "VI. NUMERICAL RESULTS",
            "text": "We evaluate our algorithms on the inventory problem [20], which can be modelled as an MDP with finite state and action spaces. We assume the behavior and target policies (\u03c0, \u03c0b) to be known, and to be (\u03f5, \u03f5b)\u2212greedy with respect to the optimal policy \u03c0\u22c6. For example, for \u03c0, this means that for all (x, a),\n\u03c0(a|x) = \u03f5 |A| + (1\u2212 \u03f5)1{a=\u03c0\u22c6(x)},\nand similarly for \u03c0b with \u03f5b. The optimal policy \u03c0\u22c6 was computed by solving the infinite time-horizon discounted MDP, with discount factor \u03b3 = 0.99. For each method, we evaluate the prediction interval for the cumulative return of the target policy \u03c0 with different values of \u03f5, while the behavior policy \u03c0b has \u03f5b = 0.4. By considering different values of \u03f5 for the target policy, we are able to observe how the coverage and interval length vary with respect to the distance between the target and the behavior policies."
        },
        {
            "heading": "A. Environment",
            "text": "The inventory control problem is modelled as follows: an agent manages an inventory of size N while facing a stochastic demand for what is stored in it. At each round, the agent must choose how many items to buy to meet the upcoming order for the next day. The action set is the same for every state, i.e. A = [0, N ]. We define the cost of buying a items as k1{a>0} + c(min(N, xt + a) \u2212 xt), where k > 0 is the fixed cost for a single order and c > 0 is the cost of a single unit bought. At each round, the agent earns a quantity pl, where p is the price of a single item and l is the number of items sold. Finally, the agent has to pay a cost zn for storing n > 0 items, with z > 0 and p > z. The order ot is sampled from a Poisson distribution with rate \u03bb. The next state is computed according to xt+1 = max(0,min(N, xt + at)\u2212 ot+1), while the reward is the sum of the costs and earnings listed above, i.e., r(xt, at, xt+1) = \u2212k1{at>0} \u2212 zxt \u2212 c(min(N, xt + at) \u2212 xt) + pmax(0,min(N, xt + at) \u2212 xt+1). Note that here, the rewards are deterministic but depend on the next state \u2013 we can easily verify that all our results naturally extend to this setting. We considered two instances of the inventory environment when evaluating our algorithm. In the first one we chose the following parameters: N = 10, k = 1, c = 2, z = 2, p = 4, \u03bb = 10. For the second one, we modified the parameters such that: k = 3, \u03bb = 6. So in the second instance, the agent was penalized more for making a single order (i.e., higher k) and the demand rate \u03bb was decreased."
        },
        {
            "heading": "B. Algorithm details",
            "text": "We consider three different implementations of our algorithm: the first using the classical pinball score function [21], [24], the second using the double quantile method and finally the shifted values method with double quantile.\n1) Pinball score function: this method adapts the algorithm presented in [24] to our setting (which is described in Section IV-B). We use the training dataset Dtr to also learn two quantile networks q\u0302\u03b1lo and q\u0302\u03b1hi , with \u03b1lo = \u03b1/2, \u03b1hi = 1 \u2212 \u03b1/2 (where \u03b1 is the coverage parameter). The two functions are estimated using quantile regression and are modelled using two neural networks with two hidden layers of 64 nodes and ReLU activation functions. For this approach, the score function used is s(x, y) = max (y \u2212 q\u0302\u03b1hi , q\u0302\u03b1lo \u2212 y) . Once we have computed the empirical CDF of the scores F\u0302 x,yn , the confidence set is obtained using (3).\n2) Double Quantile (DQ) method: Here we apply the method in IV-B.1. In this method, we introduce two score functions\ns0(x, y) = q\u0302\u03b1lo(x)\u2212 y s1(x, y) = y \u2212 q\u0302\u03b1hi(x),\nwhere q\u0302\u03b1lo and q\u0302\u03b1hi are the same networks as in the previous method. Lastly, the confidence set is computed using (7).\n3) Shifted Values (SV) with double quantile method: Here we consider a score function that allows us to shift the values of the behavior policy s(x, y) = y, as explained in IV-B.2, and compute the confidence set according to (10)."
        },
        {
            "heading": "C. Baseline: Quantile Estimation through Importance Sam-",
            "text": "pling with Bootstrap (QIS-Bootstrap)\nWe compare the conformal prediction method developed in this work to quantile estimation through importance sampling [4] with bootstrap. Importance sampling (IS) has been widely used as a variance reduction technique in statistical methods, but in our case it can be used to perform off-policy evaluation as in [19], [26]. However, compared to [19], [26] that try to estimate the mean value of the target policy \u03c0, we use the IS technique to estimate the (\u03b1lo, \u03b1hi)-quantiles of the value of \u03c0. The key insight is that q\u03c0\u03b1(x), the \u03b1-quantile of \u03c0 in x, can be estimated using the calibration data Dcal and the likelihood ratio w(x, y) through the following expression\nq\u03c0\u03b1(x) = Quantile\u03b1  \u2211 y\u2208I(x) w (x, y)\u2211 y\u2032\u2208I(x) w (x, y \u2032) \u03b4y  , where I(x) = {y \u2208 Dcal : x1 = x}, i.e., we only consider the cumulative rewards of the trajectories in Dcal that start in x.\nThe inner term can be seen as an empirical estimator of F\u03c0x (y) = EY\u223cP\u03c0(Y |X=x)[1{Y\u2264y}] = E Y\u223cP\u03c0b (Y |X=x)[w(x, Y )1{Y\u2264y}], the CDF of the values of \u03c0 in x (note that the normalization factor does not affect the outcome, see also [4]). Since w(x, y) is unknown, we replace it by w\u0302(x, y).\nNext, rather than using the estimate q\u03c0\u03b1 directly, to obtain a better estimate we use bootstrapping [27] to estimate a confidence interval around the \u03b1-quantile, obtaining a highconfidence interval (q\u03c0\u03b1\u2212, q \u03c0 \u03b1+) and then taking the median point q\u0304\u03b1(x) := (q\u03c0\u03b1\u2212 + q \u03c0 \u03b1+)/2. Finally, the confidence set for the value of \u03c0 is simply given by\nC\u0302n(x) = [q\u0304\u03b1lo(x), q\u0304\u03b1hi(x)]. (15)\nIt is important to remember that there is no coverage guarantees for this set C\u0302n(x)."
        },
        {
            "heading": "D. Results and discussion",
            "text": "In Figure 4, we show the results of our methods in the Inventory Problem for horizons 20 and 40 (for both the problem instances defined in VI-A), where results are averaged over 30 runs. Recall that the policy \u03c0b is \u03f5b\u2212greedy w.r.t. \u03c0\u22c6, with \u03f5b = 0.4, while \u03c0 is \u03f5-greedy w.r.t \u03c0\u22c6, with \u03f5 varying in [0.15, 0.65]. The target level of coverage was chosen as 1\u2212\u03b1 = 90% (depicted as the dashed black line in the plots of the second column). We evaluated our algorithms using the empirical estimate of w\u0302 (see V-B.1) against the QIS-Bootstrap baseline method in Section VI-C.\n1) Conformalized intervals: the boxplots illustrate the conformalized interval obtained for each method. For each run, method, and value of \u03f5, we evaluated the confidence interval across 2000 tests-points xtest sampled from p(x), and averaged the corresponding minimum and maximum values of the confidence set C\u0302n(xtest). The whiskers indicate 95% confidence interval for the minimum and the maximum. As mentioned in Section IV-B, we observe that the pinball method yields an interval that enlarges/shrinks symmetrically around a fixed point. As a consequence, the interval becomes larger to maintain the desired coverage when the target policy \u03c0 becomes really different than \u03c0b (i.e., \u03f5 is different than \u03f5b = 0.4). Instead, with the proposed double quantile method, the interval is shifted depending on how far the target policy \u03c0 is w.r.t. \u03c0b, leading to smaller intervals even when the policies are far from each other. The intervals estimated by the QISBootstrap method match the ones of our new score functions when \u03c0 is close to \u03c0b. However, when the policies are far from each other, the estimated interval is too conservative (i.e., too small and off-centred), which reflects in the coverage level of the algorithm, quickly degrading as \u03c0 moves away from \u03c0b, for both horizons.\n2) Coverage: the line plots illustrate the achieved coverage, averaged over 30 runs (bars indicate 95% confidence interval). All the proposed conformalized methods achieved better levels of coverage than QIS-Bootstrap, as one would expect. For horizon H = 20, the pinball method can maintain the desired level of coverage for all the epsilons at the expense of the interval length, while the new methods achieve a better level of coverage with a smaller interval size. For a larger horizon (H = 40), we can see that the coverage of the QIS-Bootstrap method degrades very rapidly, maintaining the desired level only for \u03c0 \u2248 \u03c0b.\n3) Discussion and future work: Some of the methods discussed to estimate the likelihood ratio w(x, y) were not used in our numerical experiments. This is mostly due to computational challenges: as we previously mentioned, the computational complexity of the Monte-Carlo method vastly exceeds the complexity of the other methods (empirical estimator and gradient method), while the gradient method has several difficulties in learning the likelihood ratios for values of (\u03f5, \u03f5b) that greatly differ. We plan to investigate how to efficiently learn the likelihood ratios using neural networks. Finally, we note that one may try conformalize the QIS-Bootstrap method in Section VI-C to have a more fair comparison."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this work, we considered the offline off-policy evaluation problem in finite time-horizon Markov Decision Processes. Using Conformal Prediction (CP) techniques, we developed methods to construct conformalized intervals that include the true reward of the target policy with a prescribed level of certainty. Some of the challenges addressed in this paper include dealing with time-dependent data, as well as addressing the distribution shift between the behavior policy and the target policy. Furthermore, we proposed improved CP methods that allow to obtain intervals with significantly reduced length when compared to existing CP methods,\nwhile retaining the same certainty guarantees. We conclude with numerical results on the inventory control problem that demonstrated the efficiency of our methods. Several interesting research directions have been mentioned in the text, of which, the most significant, consists in improving the estimation of the likelihood ratio characterizing the distribution shift."
        }
    ],
    "title": "Conformal Off-Policy Evaluation in Markov Decision Processes",
    "year": 2023
}