{
    "abstractText": "We tackle the challenging task of unsupervised object localization in this work. Recently, transformers trained with self-supervised learning have been shown to exhibit object localization properties without being trained for this task. In this work, we present Multiple Object localization with Self-supervised Transformers (MOST) that uses features of transformers trained using self-supervised learning to localize multiple objects in real world images. MOST analyzes the similarity maps of the features using box counting; a fractal analysis tool to identify tokens lying on foreground patches. The identified tokens are then clustered together, and tokens of each cluster are used to generate bounding boxes on foreground regions. Unlike recent state-of-theart object localization methods, MOST can localize multiple objects per image and outperforms SOTA algorithms on several object localization and discovery benchmarks on PASCAL-VOC 07, 12 and COCO20k datasets. Additionally, we show that MOST can be used for self-supervised pretraining of object detectors, and yields consistent improvements on fully, semi-supervised object detection and unsupervised region proposal generation.Our project is publicly available at this website.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sai Saketh Rambhatla"
        },
        {
            "affiliations": [],
            "name": "Ishan Misra"
        },
        {
            "affiliations": [],
            "name": "Rama Chellappa"
        },
        {
            "affiliations": [],
            "name": "Abhinav Shrivastava"
        }
    ],
    "id": "SP:d2abbdf64ffe108d40aee427f757d4c11d9aaca6",
    "references": [
        {
            "authors": [
                "Cewu Lu",
                "Ranjay Krishna",
                "Michael Bernstein",
                "Li Fei- Fei"
            ],
            "title": "Visual relationship detection with language priors",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A. Shamma",
                "Michael S. Bernstein",
                "Li Fei-Fei"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International Journal of Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Ankan Bansal",
                "Sai Saketh Rambhatla",
                "Abhinav Shrivastava",
                "Rama Chellappa"
            ],
            "title": "Detecting human-object interactions via functional generalization",
            "venue": "Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Masato Tamura",
                "Hiroki Ohashi",
                "Tomoaki Yoshinaga"
            ],
            "title": "Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Tiancai Wang",
                "Tong Yang",
                "Martin Danelljan",
                "Fahad Shahbaz Khan",
                "X. Zhang",
                "Jian Sun"
            ],
            "title": "Learning human-object interaction detection using interaction points",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Georgia Gkioxari",
                "Ross B. Girshick",
                "Piotr Doll\u00e1r",
                "Kaiming He"
            ],
            "title": "Detecting and recognizing human-object interactions",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Danfei Xu",
                "Yuke Zhu",
                "Christopher Bongsoo Choy",
                "Li Fei-Fei"
            ],
            "title": "Scene graph generation by iterative message passing",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Nicolai Wojke",
                "Alex Bewley",
                "Dietrich Paulus"
            ],
            "title": "Simple online and realtime tracking with a deep association metric",
            "venue": "IEEE International Conference on Image Processing (ICIP),",
            "year": 2017
        },
        {
            "authors": [
                "Zhongdao Wang",
                "Liang Zheng",
                "Yixuan Liu",
                "Shengjin Wang"
            ],
            "title": "Towards real-time multi-object tracking",
            "venue": "The European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "LVIS: A dataset for large vocabulary instance segmentation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Akshay Dhamija",
                "Manuel Gunther",
                "Jonathan Ventura",
                "Terrance Boult"
            ],
            "title": "The overlooked elephant of object detection: Open set",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2020
        },
        {
            "authors": [
                "Oriane Sim\u00e9oni",
                "Gilles Puy",
                "Huy V. Vo",
                "Simon Roburin",
                "Spyros Gidaris",
                "Andrei Bursuc",
                "Patrick P\u00e9rez",
                "Renaud Marlet",
                "Jean Ponce"
            ],
            "title": "Localizing objects with self-supervised transformers and no labels",
            "venue": "In Proceedings of the British Machine Vision Conference (BMVC), November 2021",
            "year": 2021
        },
        {
            "authors": [
                "Yangtao Wang",
                "Xi Shen",
                "Shell Xu Hu",
                "Yuan Yuan",
                "James Crowley",
                "Dominique Vaufreydaz"
            ],
            "title": "Self-supervised transformers for unsupervised object discovery using normalized cut",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yong Jae Lee",
                "Kristen Grauman"
            ],
            "title": "Learning the easy things first: Self-paced visual category discovery",
            "venue": "In CVPR,",
            "year": 2011
        },
        {
            "authors": [
                "Alvaro Collet Romea",
                "Bo Xiong",
                "Corina Gurau",
                "Martial Hebert",
                "Siddhartha Srinivasa"
            ],
            "title": "Herbdisc: Towards lifelong robotic object discovery",
            "venue": "In Proceedings of International Journal of Robotics Research (IJRR),",
            "year": 2014
        },
        {
            "authors": [
                "Y.J. Lee",
                "K. Grauman"
            ],
            "title": "Object-graphs for context-aware category discovery",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2010
        },
        {
            "authors": [
                "J. Sivic",
                "B.C. Russell",
                "A.A. Efros",
                "A. Zisserman",
                "W.T. Freeman"
            ],
            "title": "Discovering objects and their location in images",
            "venue": "In Tenth IEEE International Conference on Computer Vision (ICCV\u201905) Volume 1,",
            "year": 2005
        },
        {
            "authors": [
                "J.R.R. Uijlings",
                "K.E.A. van de Sande",
                "T. Gevers",
                "A.W.M. Smeulders"
            ],
            "title": "Selective search for object recognition",
            "venue": "International Journal of Computer Vision,",
            "year": 2013
        },
        {
            "authors": [
                "Santiago Manen",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Prime object proposals with randomized prim\u2019s algorithm",
            "venue": "IEEE International Conference on Computer Vision, pages 2536\u20132543,",
            "year": 2013
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2015
        },
        {
            "authors": [
                "Huy V. Vo",
                "Patrick P\u00e9rez",
                "Jean Ponce"
            ],
            "title": "Toward unsupervised, multi-object discovery in large-scale image collections",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Huy V. Vo",
                "Francis Bach",
                "Minsu Cho",
                "Kai Han",
                "Yann LeCun",
                "Patrick P\u00e9rez",
                "Jean Ponce"
            ],
            "title": "Unsupervised image matching and object discovery as optimization",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Huy V. Vo",
                "Elena Sizikova",
                "Cordelia Schmid",
                "Patrick P\u00e9rez",
                "Jean Ponce"
            ],
            "title": "Large-scale unsupervised object discovery",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Sai Saketh Rambhatla",
                "Ramalingam Chellappa",
                "Abhinav Shrivastava"
            ],
            "title": "The pursuit of knowledge: Discovering and localizing novel categories using dual memory",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher K.I. Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International Journal of Computer Vision,",
            "year": 2010
        },
        {
            "authors": [
                "Jing Z. Liu",
                "Luduan Zhang",
                "Guang H. Yue"
            ],
            "title": "Fractal dimension in human cerebellum measured by magnetic resonance imaging",
            "venue": "Biophysical journal,",
            "year": 2003
        },
        {
            "authors": [
                "Yen-Chang Hsu",
                "Zhaoyang Lv",
                "Zsolt Kira"
            ],
            "title": "Deep image category discovery using a transferred similarity function",
            "venue": "CoRR, abs/1612.01253,",
            "year": 2016
        },
        {
            "authors": [
                "Yen-Chang Hsu",
                "Zhaoyang Lv",
                "Zsolt Kira"
            ],
            "title": "Learning to cluster in order to transfer across domains and tasks",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yen-Chang Hsu",
                "Zhaoyang Lv",
                "Joel Schlosser",
                "Phillip Odom",
                "Zsolt Kira"
            ],
            "title": "Multi-class classification without multiclass labels",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Kai Han",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Learning to discover novel visual categories via deep transfer clustering",
            "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Kai Han",
                "Sylvestre-Alvise Rebuffi",
                "Sebastien Ehrhardt",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Automatically discovering and learning new visual categories with ranking statistics",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Krishna Kumar Singh",
                "Utkarsh Ojha",
                "Yong Jae Lee"
            ],
            "title": "Finegan: Unsupervised hierarchical disentanglement for fine-grained object generation and discovery",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Minsu Cho",
                "Suha Kwak",
                "Cordelia Schmid",
                "Jean Ponce"
            ],
            "title": "Unsupervised Object Discovery and Localization in the Wild: Part-based Matching with Bottom-up Region Proposals",
            "venue": "In CVPR - IEEE Conference on Computer Vision & Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Suha Kwak",
                "Minsu Cho",
                "Ivan Laptev",
                "Jean Ponce",
                "Cordelia Schmid"
            ],
            "title": "Unsupervised Object Discovery and Tracking in Video Collections",
            "venue": "In ICCV - IEEE International Conference on Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Carl Doersch",
                "Abhinav Gupta",
                "Alexei A. Efros"
            ],
            "title": "Context as supervisory signal: Discovering objects with predictable context",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "X. Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Debidatta Dwibedi",
                "Yusuf Aytar",
                "Jonathan Tompson",
                "Pierre Sermanet",
                "Andrew Zisserman"
            ],
            "title": "With a little help from my friends: Nearest-neighbor contrastive learning of visual representations",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jianbo Shi",
                "Jitendra Malik"
            ],
            "title": "Normalized cuts and image segmentation",
            "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 1997
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Christian Rupprecht",
                "Iro Laina",
                "Andrea Vedaldi"
            ],
            "title": "Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "ICLR, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Roy E Plotnick",
                "Robert H Gardner",
                "William W Hargrove",
                "Karen Prestegaard",
                "Martin Perlmutter"
            ],
            "title": "Lacunarity analysis: a general technique for the analysis of spatial patterns",
            "venue": "Physical review E,",
            "year": 1996
        },
        {
            "authors": [
                "Jian Li",
                "Qian Du",
                "Caixin Sun"
            ],
            "title": "An improved boxcounting method for image fractal dimension estimation",
            "venue": "Pattern Recognit.,",
            "year": 2009
        },
        {
            "authors": [
                "T.G. Smith",
                "G.D. Lange",
                "W.B. Marks"
            ],
            "title": "Fractal methods and results in cellular morphology \u2014 dimensions, lacunarity and multifractals",
            "venue": "URL https://doi. org/10.1016/s0165-0270(96)00080-5",
            "year": 1996
        },
        {
            "authors": [
                "Martin Ester",
                "Hans-Peter Kriegel",
                "J\u00f6rg Sander",
                "Xiaowei Xu"
            ],
            "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
            "venue": "In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining,",
            "year": 1996
        },
        {
            "authors": [
                "Amir Bar",
                "Xin Wang",
                "Vadim Kantorov",
                "Colorado J Reed",
                "Roei Herzig",
                "Gal Chechik",
                "Anna Rohrbach",
                "Trevor Darrell",
                "Amir Globerson"
            ],
            "title": "Detreg: Unsupervised pretraining with region priors for object detection, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable {detr}: Deformable transformers for end-to-end object detection",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ville Satopaa",
                "Jeannie Albrecht",
                "David Irwin",
                "Barath Raghavan"
            ],
            "title": "Finding a \u201dkneedle\u201d in a haystack: Detecting knee points in system behavior",
            "venue": "In 2011 31st International Conference on Distributed Computing Systems Workshops,",
            "year": 2011
        },
        {
            "authors": [
                "Jianping Shi",
                "Qiong Yan",
                "Li Xu",
                "Jiaya Jia"
            ],
            "title": "Hierarchical image saliency detection on extended cssd",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "Wang Lijun",
                "Lu Huchuan",
                "Wang Yifan",
                "Feng Mengyang",
                "Wang Dong",
                "Yin Baocai",
                "Ruan Xiang"
            ],
            "title": "Learning to detect salient objects with image-level supervision",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Yang Chuan",
                "Zhang Lihe",
                "Lu Huchuan Ruan Xiang",
                "Yang Ming-Hsuan"
            ],
            "title": "Saliency detection via graph-based manifold ranking",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2013
        },
        {
            "authors": [
                "Duc Tam Nguyen",
                "Maximilian Dax",
                "Chaithanya Kumar Mummadi",
                "Thi-Phuong-Nhung Ngo",
                "Thi Hoai Phuong Nguyen",
                "Zhongyu Lou",
                "Thomas Brox"
            ],
            "title": "Deepusps: Deep robust unsupervised saliency prediction with selfsupervision",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Andrey Voynov",
                "Stanislav Morozov",
                "Artem Babenko"
            ],
            "title": "Object segmentation without labels with large-scale generative models",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Welinder",
                "Steve Branson",
                "Takeshi Mita",
                "Catherine Wah",
                "Florian Schroff",
                "Serge Belongie",
                "Pietro Perona"
            ],
            "title": "Caltech-ucsd birds 200",
            "venue": "Technical Report CNS-TR-201, Caltech,",
            "year": 2010
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Object detectors are important components of several computer vision systems such as visual relationship detection [1, 2], human-object interaction detection [3\u20136], scene graph generation [7] and object tracking [8, 9] etc. Performance of object detectors is heavily reliant on the availability of training data. However, annotating large object detection datasets can be expensive and time consuming [10, 11]. Additionally, the vocabulary of object detectors is limited by the training datasets and such detectors often fail to generalize to new categories [12]. This strategy is not scal-\nWork done while at UMD.\nable and a more effective approach is warranted. Object discovery is one such task that has the potential to address these concerns. Object discovery is the problem of identifying and grouping objects/parts in a large collection of images without human intervention [15\u201318]. The first step in object discovery is to obtain region proposals and subsequently group them semantically. Previous works on discovery used Selective Search [19], randomized Prim\u2019s [20] or a region proposal network (RPN) [21] to get object proposals. [22\u201324] used inter-image similarity to construct a graph and performed optimization or ranking, to localize objects without any supervision. Such methods are computationally expensive and often fail to scale to datasets larger than 20000 images. [25] used region proposals from an RPN and proposed a never ending learning approach and is the first method shown to scale to \u223c100000 images. However, these region proposal methods are often of low quality, and therefore reduce the performance of discovery systems. Recently, LOST [13] and TokenCut [14] leveraged the object segmentation properties of transformers [26] trained using self-supervised learning (DINO [27]) to obtain high quality object proposals. They demonstrate significant improvements over state-of-the-art on object discovery, salient ar X iv :2 30 4.\n05 38\n7v 2\n[ cs\n.C V\n] 2\n6 A\nug 2\nobject detection and weakly supervised object localization benchmarks.\nHowever, both LOST [13] and TokenCut [14] assume the presence of a single salient object per image and hence, can localize only one object as shown in Fig 1 (top). This assumption may hold for object centric datasets like ImageNet [28] but is not true for scene-centric real world datasets like PASCAL-VOC [29] and COCO [10]. In this work, we address the problem of localizing multiple objects per image and demonstrate the effectiveness of our approach for the task of unsupervised object localization and discovery on several standard benchmarks.\nWe propose a new object localization method called \u201cMultiple Object localization with Self-supervised Transformers\u201d (MOST) which is capable of localizing multiple objects per image without using any labels. We use the features extracted from a transformer [26] network trained with DINO [27]. Our method is based on two empirical observations; 1) Patches within foreground objects have higher correlation with each other than the ones on the background [13] and 2) The similarity map computed using the features of a foreground object with all the features in the image is usually more localized and less noisier than the one computed using the feature of a background. Our algorithm analyzes the similarities between patches exhaustively using a fractal analysis tool called box counting [30]. This analysis picks a set of patches that most likely lie on foreground objects. Next, we perform clustering on the patch locations to group patches belonging to a foreground object together. Each of these clusters is called pools. A binary mask is then computed for each pool and a bounding box is extracted. This capability enables the algorithm to extract multiple bounding boxes per image as shown in Fig.1 (bottom). We demonstrate that without any training, our method can outperform state-of-the-art object localization methods that train class agnostic detectors to detect multiple objects. To prove the effectiveness of MOST, we demonstrate results on several object localization and discovery benchmarks. On self-supervised pre-training for object detectors, using MOST yields consistent improvement across multiple downstream tasks using 6\u00d7 fewer boxes. When compared against other self-supervised transformer-based localization methods, MOST achieves higher recall with and without additional training. We summarize the contributions of our work below.\n\u2022 We propose MOST, an effective method to localize and discover multiple objects per image without supervision using transformers trained with DINO.\n\u2022 We perform exhaustive experiments to assess the performance of MOST on several localization and discovery benchmarks and show significant improvements over the baselines.\nThe paper is organized as follows. In Section 2 we discuss related works on object localization and discovery. We describe our approach in detail in Section 3. We describe our experimental setup and present results in Section 4 and conclude in Section 5."
        },
        {
            "heading": "2. Related Works",
            "text": "Unsupervised Object Localization and Discovery: Object category discovery can be broadly segregated into image-based [31\u201336] and region-based methods [13\u201315, 17, 22\u201325, 37\u201339]. Region-based methods start by generating object proposals and later group them into coherent semantic groups. Image-based approaches on the other hand, assume the localization task to be solved and focus solely on the semantic grouping. Our current method is closer to the former. Vo et al., [22\u201324] localize regions in images by constructing an inter-image similarity graph between regions and partitioning it using optimization or ranking. Due to the quadratic nature of the graph, these methods cannot scale to large datasets beyond tens of thousands of images. Our current work does not compute inter-image similarity between regions and scales linearly with number of images. Lee et al., [15] propose object graphs that incorporates knowledge about a few known categories to facilitate the discovery of novel categories. MOST doesn\u2019t assume any prior knowledge and has the ability to discover objects from scratch. Lee et al., [17] extend object graphs to a curriculum based discovery pipeline, that learns to discover easy objects first and progressively proceeds to discover the harder ones. Along similar lines, Rambhatla et al., [25] propose a large scale discovery pipeline, similar to [15] that leverages prior knowledge about a few categories. Authors of [25] use an RPN [21] trained on known categories as the localization method. In contrast to that, MOST localizes objects in images using features of a transformer [26] trained with DINO [27]. Object localization using self-supervised networks: Recently, CNNs [40] and Transformers [26] trained in a selfsupervised fashion, have been shown to exhibit object localization/segmentation properties [27, 41]. This property is of particular interest as it has the potential to facilitate research on unsupervised localization, detection and segmentation. [13] is a simple method, based on the observation that the key features of the last self attention layer of a transformer, trained using DINO, has certain affinity. They construct a map of inverse degree to extract bounding boxes on objects in an unsupervised fashion. This method is shown to outperform recent state-of-the-art methods by a significant margin. [14] propose an alternate method for localizing objects using self-supervised transformers, based on normalized cut [42]. TokenCut [14] construct an undirected graph based on token feature similarities and use normalized cut to cluster foreground and background patches. Spectral clus-\ntering is used to solve the graph-cut and they show that the eigen vector corresponding to the second smallest eigenvalue provides a good cutting solution. TokenCut outperforms LOST on unsupervised object discovery. In addition to discovery, [14] also demonstrate impressive results on unsupervised saliency detection and weakly supervised object localization. Kyriazi et al., [43] propose deep spectral methods, that performs normalized cut [42] but using an affinity matrix computed using semantic and color features. Since this method is very similar to TokenCut and achieves lower performance, we only compare with the latter in this work.\nHowever, one limitation of LOST and TokenCut is that they can localize only one object per image. Our proposed method, MOST can automatically localize multiple objects per image and outperforms LOST and TokenCut on standard discovery benchmarks."
        },
        {
            "heading": "3. Approach: MOST",
            "text": "MOST can be used to localize multiple objects in an image. Our approach, illustrated in Fig. 3, first identifies a set of tokens that is computed from patches on foreground objects. These tokens are then clustered and each cluster, named pool, is used to obtain a bounding box. Next, we discuss a few preliminaries in Section 3.1 followed by the motivation and proposed solution in Section 3.2."
        },
        {
            "heading": "3.1. Preliminaries",
            "text": "Box Counting: Box counting is a method of analyzing a pattern by breaking and analyzing it at smaller scales. This is done by performing a raster scan of the pattern at different scales and computing appropriate metrics to assess its fractal nature. In this work, we use a sliding window scan. Vision Transformers: ViTs [44] operate on learned embeddings, called tokens, generated from non-overlapping image patches of resolution P\u00d7P (typically P=8/16) that form a sequence. To be precise, an image I of shape H \u00d7 W \u00d7 3 is first divided into non-overlapping patches of resolution P \u00d7 P\u00d73, generating a total of N = HW/P 2 patches. Next, each patch is embedded via a trainable linear layer to generate a token of dimension d to form a sequence of patches. An extra [CLS] token [45] is added to this se-\nquence, whose purpose is to aggregate the information from the tokens of the sequence. Typically, a projection head is attached to the [CLS] to train the network for classification. DINO: DINO [27] combines self-training and knowledge distillation without labels for self supervised learning. DINO constructs two global views and several local views of lower resolution, from an image. DINO consists of a teacher and a student network. The student processes all the crops while the teacher is operated only on the global crops. The teacher network then distills its dark knowledge to the student. This encourages the student network to learn local to global correspondences. In contrast to other knowledge distillation methods, DINO\u2019s teacher network is updated dynamically during training using exponential moving average. DINO was shown to perform on par or better than several baselines on several tasks of image retrieval, copy detection, instance segmentation etc. The property of importance to the current work, is the ability of DINO to localize foreground regions of semantic entities in an image. [13, 14] leverage this property to localize the salient object in an image by using the key features from the last self-attention layer. Similar to [13, 14], we concatenate the key features of all the heads in the last self-attention layer to obtain the input to MOST."
        },
        {
            "heading": "3.2. Multiple object localization",
            "text": "Motivation: Consider the example shown in Fig. 2. We show three examples of the similarity maps of a token (shown in red) picked on the background (column 2) and foreground (columns 3, 4). Tokens within foreground patches have higher correlation than the ones on background [13]. This results in the similarity maps of foreground patches being less \u201cspatially\u201d random than the ones on the background. The task then becomes to analyze the similarity maps and identify the ones with less spatial randomness. Box counting [46, 47] is a popular technique in fractal analysis that analyzes spatial patterns at different scales to extract desired properties. Hence, we adopt box counting for our case and since, we are interested in randomness, we adopt entropy as the metric. Preprocessing: The input to our method is a d-dimensional feature F \u2208 RN\u00d7d, extracted from an image using a neural network. Here, N denotes the number of spatial locations in the feature map, in case of a CNN, or number of tokens, in case of a transformer network. The aim is to identify subsets of tokens, which we call pools, that can be used to localize all the objects in an image. We do not make any assumption on the number of objects present in the image. Given the feature F , we compute an outer product matrix A = FFT \u2208 RN\u00d7N . Row i of matrix A, i.e., A[i, :] encodes a similarity map of a token at location i with all the other tokens in F . Next, each row of A is processed by the Entropy-based Box Analysis (EBA) module.\nEntropy-based Box Analysis (EBA): The proposed entropy based box analysis module performs a fractal analysis method, called box counting to segregate similarity maps of tokens on foreground patches from those of background. As shown in Fig. 3, we perform a raster scan with increasing box (used interchangeably with kernel in this work) sizes on each map. Traditionally, measures like lacunarity [48] are computed within each box to analyze the pattern. In this work, we average the elements within each box. This can be implemented efficiently using pooling operations. The resulting downsampled map is flattened and the entropy is computed using the pmf computed as follows: p(x = xi) = \u03a3 h.w i=1 1(fi==xi) h.w , where fi is the i\nth index in the feature map. A downsampled map belongs to a token on a foreground patch if its entropy is less than a threshold \u03c4 . Using K boxes in the EBA module results in K entropy values ek(k \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,K}). Finally, we perform a majority voting among the entropies of all the downsampled maps, i.e., \u03a3Ki=1 1(ei\u2264\u03c4) K > 0.5, to decide if the original similarity map belongs to a token on a foreground patch. A map of dimension n \u00d7 n has a maximum entropy of log(n2). We use a threshold of the form \u03c4 = a + blog(n2) (we use a = 1, b = 0.5 in this work). We do not consider \u03c4 as a hyperparameter and we pick a value that is mid-way between the minimum and maximum permissible value (b=0.5). To prevent a threshold of 0 for n = 1, we add a constant (a=1).\nClustering: The EBA module, identifies a set S = {p|p \u2208\n{1, 2, \u00b7 \u00b7 \u00b7 , N}}, that contains the spatial locations of tokens computed from foreground patches. Often, highly redundant neighboring tokens are identified. We group neighboring tokens with the help of a clustering step to obtain pools. We convert the linear index p of the tokens to cartesian coordinates (x, y), and use that as the feature for clustering. Manhattan distance is used as the dissimilarity metric with a threshold \u03f5 (\u03f5 = 2 i.e. Moore neighborhood). Since, the number of pools is not known a-priori, we use a densitybased clustering method, DBSCAN [49] which automatically identifies the number of clusters from the data. Pools identified by the clustering step are then post-processed to obtain bounding boxes on foreground objects. Post-processing: The clusters, called pools, obtained from the clustering step are then post-processed to obtain one box per pool. Consider M pools identified by the clustering step, i.e. Ci, where i \u2208 {1, 2, \u00b7 \u00b7 \u00b7M}. Each pool Ci is a set of token locations Ci = {pi|pi \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , N}}. We leverage the first observation mentioned above to obtain a bounding box from the pool as follows. First, we build a binary similarity matrix A\u0302 = A > 0. Next, within the tokens in the pool, we identify the one with lowest degree in A\u0302, called the core token, c\u2217.\nc\u2217 = argmin c\u2208Ci dc where dc = N\u2211 j=1 A\u0302[c, j]\nAuthors of LOST [13] report that tokens with low degrees\nmost likely fall within an object. Next, we remove the tokens from the pool that do not correlate positively with c\u2217 to form a reduced pool C\u2217i . This ensures that all the tokens in the current pool lie on the same foreground object. Next, a binary mask is constructed by computing the sum of similarities of token features in C\u2217i with the features of all the tokens, i.e. mik = 1( \u2211 c\u2208C\u2217i\nfTk fc \u2265 0). Finally, connected component analysis is performed on the binary mask and the bounding box of the island that contains c\u2217 is selected as the region containing the object. We repeat this process for all the M pools to generate M bounding boxes per image. Note that, M is not assumed to be known a-priori and is decided automatically by our method. Additionally, we remove trivial boxes i.e., boxes which have area less than than a threshold (256) or cover the whole image. Implementation Details: For all our experiments, we use the ViT-S/16 and ViT-B/8 [44] models trained with DINO [27] to extract the features. We concatenate the key features of all the heads from the last self-attention layer to use as the input to our method."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section we describe, in detail, the experimental setup used for evaluation. We evaluate our method on two setups, namely the localization setup, and the discovery setup. We begin by describing the datasets and metrics in Sec. 4.1. We describe the evaluation setups in Sec. 4.2. Sec. 4.3 compares our method against contemporary work. We then describe ablation experiments in Sec. 4.4 and show qualitative results in Sec. 4.6."
        },
        {
            "heading": "4.1. Datasets and Metrics",
            "text": "We use the PASCAL-VOC [29] (2007, 2012 splits) and the COCO [10] (COCO20k [22] and COCO splits) datasets in our experiments. The PASCAL VOC [29] 2007 and 2012\ntrainval sets consists of 5011, 11540 images respectively, spanning twenty objects. The PASCAL VOC [29] test set consists of 4952 images. The COCO [10] 2014 train set consists of \u223c110k images containing over eighty objects and the COCO minival set consists of 5000 images. We do not use any class or bounding box annotations for our method except for evaluation.\nFor the localization setup, we use the average precision at different thresholds ([0.5:0.95], 0.5 and 0.75), average recall (AR1, AR10 and AR100) and Correct Localization (CorLoc) metrics for evaluation. CorLoc is defined as the fraction of the images in which atleast one object is localized with an IoU greater than a threshold (0.5 in this work). AP, AR are defined in the usual way. For the object discovery setup, we report both the PASCAL VOC style AP50 and COCO style AP[50:95] metrics along with area under the purity-coverage plots [25, 39]. We refer the interested readers to [25] for definitions of purity and coverage."
        },
        {
            "heading": "4.2. Setups",
            "text": "Localization setup: This setup evaluates the localization performance of methods. We evaluate models on a) unsupervised pre-training, b) Multiple Object Localization, and c) single object localization. For unsupervised pre-training, localization methods are used to train object detectors in an unsupervised fashion and their performance is evaluated on the downstream task of object detection. In this work, we use the recently proposed DETReg [50] as the pre-training strategy which uses a Deformable DeTR [51] architecture. DETReg uses an object localization method and pre-trains an object detector in an unsupervised fashion. We evaluate the pre-trained model on the downstream tasks of semisupervised, fully-supervised and class-agnostic object proposal generation. In the semi-supervised setting, models are trained on the PASCAL-VOC(07+12) and COCO train\nsets without labels and are fine-tuned on k% of labeled data similar to [50]. In the fully supervised setting, pre-trained models are fine-tuned on the full PASCAL-VOC and COCO dataset using all the labels. For the class-agnostic object proposal generation, models are pre-trained on COCO dataset without labels and the generated object proposals are evaluated on the COCO validation set similar to [50].\nWe follow the settings used in [24] for multiple-object localization and evaluate on PASCAL-VOC 2007 and COCO20k. For single-object localization, we follow the settings in [13, 14] and evaluate on PASCAL-VOC 2007, PASCAL-VOC 2012 and COCO20k. Discovery setup: This setup evaluates the object discovery performance. Similar to [13] we use the regions obtained by our localization method, to perform K-means clustering and use the resulting cluster labels to train Faster-RCNN object detectors on PASCAL-VOC 2007, 2012 trainval and COCO20k train sets. We report results of these experiments on the PASCAL-VOC 2007 test and COCO minival sets respectively. In addition to this, we report the performance of our discovery method on COCO train set, similar to the large scale discovery in [25]."
        },
        {
            "heading": "4.3. Comparison with contemporary methods",
            "text": "In this section we compare our method against contemporary works the localization and discovery setups."
        },
        {
            "heading": "4.3.1 Localization setup",
            "text": "Unsupervised Pre-training: Table 1 compares the results of all the localization methods on unsupervised pre-training of object detectors. We use average precision at different IoU thresholds ([0.50:0.95]: AP, 0.5: AP0.50, 0.75: AP0.75) for evaluation. On the semi-supervised setting, on VOC 07+12 (k = 10%), the self-supervised transformer based methods (LOST, TokenCut and MOST) outperform SS [19] with fewer boxes per image. In particular, TokenCut (denoted as TCut in Table 1) which outputs only one box per image, outperforms SS, using ten boxes per image, by \u223c0.4\npoints on mAP. MOST which outputs an average of 4.65 boxes per image outperforms TokenCut (the best performing self-supervised transformer based method) by 1.89, 2.7 and 2.26 percentage points on AP, AP50, and AP75 respectively. This can be attributed to the ability of MOST to output multiple foreground regions resulting in more samples for pre-training which is not possible in the case of TokenCut. MOST outperforms SS, that outputs 30 boxes per image, by 0.91, 2.09 and 0.9 points on AP, AP50, and AP75 respectively using almost 6\u00d7 fewer boxes per image and this can be attributed to the ability of MOST to generate high quality proposals. On COCO, MOST outperforms TokenCut by 0.8 and 0.86 on the 1% and 2% setting of semisupervised learning. MOST with a ViT-B/8 backbone, that outputs 13.09 boxes on average per image, outperforms SS (with 30 boxes per image) by 0.36, 0.07 points on k=1% and k=2% few shot splits of COCO respectively.\nOn the fully supervised setting, MOST outperforms LOST and TokenCut by 0.76 and 0.55 (AP) percentage points respectively on VOC 07+12. On COCO, MOST outperforms them by 0.50 and 1 points respectively. On VOC 07+12, MOST using ViT-B/8 (13.09 boxes per image) outperforms SS (with 30 boxes per image) by 0.40. On the much harder COCO dataset, MOST outperforms SS 1.20 (AP) percentage points using 2\u00d7 fewer boxes per image.\nIn Table 2 we report the class agnostic object proposal evaluation of DETReg trained using different localization methods. We report average precision at different IoU thresholds (AP, AP50, AP75) and recall @ 1, 10 and 100 proposals per image (denoted as R1, R10, and R100) to evaluate the quality of region proposals. Note that the numbers in the table are low because of the unsupervised nature of training. All the self-supervised transformer-based methods achieve performance better than SS with far fewer boxes. In recall, TokenCut and MOST perform on par with each other and outperform rest of the methods with significant improvements. MOST achieves the highest performance on average precision among all the methods. It can achieve\nhigher precision and recall because of its ability to output multiple high quality regions per image. While LOST and TokenCut output high quality boxes, they cannot output more than one box per image. SS on the other hand, outputs multiple boxes but with poor quality. Multiple Object Localization: We compare with LOD, the state-of-the-art method on the multi-object localization benchmark proposed by LOST using the code released by authors. On VOC2007, we attain an odAP[0.5:0.95] of 6.43 compared to 5.35 attained by LOD, an improvement of 1.09 percentage points. On the COCO20k dataset, we attain a performance of 1.70 (compared to 1.53 achieved by LOD) on the harder odAP[0.50:0.95] metric. Note, we do not compare with rOSD [22] as LOD [24] outperforms it. Single Object Localization: Table 4 compares the results of our method on single object localization with recent methods on PASCAL VOC 2007, 2012 and COCO20k respectively. We use the CorLoc metric to evaluate methods. Note that MOST is a multiple object localization method and this setup evaluates the ability of methods to output a single region. Since MOST outputs multiple boxes, we use the heuristic, average best overlap (for evaluating object proposals in [19]), to select one region per image. The numbers reported for MOST in this table are the \u201cbest\u201d case scenario. We outperform LOST by 12.9, 13.4 and 16.4 percentage points on VOC 2007, 2012 and COCO20k respectively. We outperform TokenCut [14] by 6, 5.3 and 8.3 percentage points on the three datasets respectively. To obtain multiple regions per image, authors of LOST train a foreground object detector using the regions obtained by their method as supervision, called LOST+CAD [13]. This method can output multiple boxes per image and from Table 4, even without any training, our method outperforms LOST+CAD and TokenCut+CAD by 9.1, 7, 9.6 and 3.4, 2.1, 4.5 percentage points on VOC 2007, 2012 and COCO20k respectively.\nDiscovery Setup: This setup evaluates the true object discovery performance as the localized boxes are used to discover semantic groups. Following LOST [13], we first cluster the features of the localized objects using K-means clustering. For VOC 2007 and 2007+2012 trainval splits, we use 20, 30 and 40 clusters. We use 80, 90 and 100 clusters for COCO20k train split. We report the results of experiments on VOC 2007, 07+12 trainval sets on VOC 2007 test set. For experiments on COCO20k, we report results on the COCO validation set. Results are tabulated in Table 3. MOST outperforms LOST in most settings with the margin of improvement higher for more number of clusters and more cluttered datasets like COCO. For more details on clustering and training refer to supplementary.\nFinally, we evaluate the performance of MOST on largescale object discovery setup introduced in [25]. For this setup, we use the area under the purity coverage plot as the metric. [25] automatically identifies the number of clusters and obtains an AuC@0.5 of 3.6% on the COCO 2014 train set. We extract the DINO [CLS] token features of regions obtained from MOST for K-Means clustering. To avoid specifying the number of clusters manually, we employ the \u201ckneedle\u201d method [52] to get the optimal number of clusters (more details in supplementary). Next, we randomly sample 10000 features from the whole dataset and cluster them using K-means with the optimal number of clusters. This subsampling avoids loading all the features into mem-\nory. MOST + optimal K-means achieves an AuC@0.5 of 8.74% on COCO 2014 train set. We use the cluster labels to train an object detector on the COCO train set and achieve an AP/AP50 of 3.9/9.5% compared to 5.2% AP50 obtained by [25] on COCO validation set. For more experiments on unsupervised saliency detection and weakly supervised localization, refer to the supplementary."
        },
        {
            "heading": "4.4. Ablation Experiments",
            "text": "Effect of kernel size: The EBA module performs box analysis in a sliding window fashion using boxes (or kernels) of different sizes. We implement this efficiently using a pooling operation. We visualize the effect of the size of pooling kernels on the final output in Fig. 4. We observe that the majority voting performed in EBA, helps in removing noisy predictions in the first triplet, where a box identified by kernel of size 1 is eliminated by majority voting of kernels with larger receptive field. In the second triplet in Fig. 4, an object which was missed by the lower order kernels (k=[1,4]), can be picked up with a higher order kernel (k=5). For all our experiments, we use kernels of sizes upto 5 (i.e. k=[1,5]). For quantitative results on the effect of kernel sizes, refer to the supplementary. Effect of clustering: MOST performs clustering with the token locations as features to obtain pools. Each pool contains tokens belonging to a foreground object. We show the effect of clustering qualitatively in Fig. 5. We observe that each pool focuses on one foreground object and illustrate the bounding boxes extracted from each pool.\nEach pool contains tokens belonging to a foreground object. We show the effect of forming pools qualitatively in Fig. 6. The first image in each pair shows the output of MOST without clustering to form pools and the second image shows the output with the formation. We observe that, without clustering, each token can generate a bounding box resulting in noisier outputs. Recall of boxes: To analyze the object localization perfor-\nmance of MOST, we compare its recall with LOST and TokenCut on VOC 07, 12 and COCO20k datasets in Fig. 7. The x-axis represents the maximum number of boxes allowed per image and the y axis plots the recall. LOST and TokenCut generate only one box per image and hence have fixed recall in all the plots. MOST can generate more boxes and hence have higher recall than LOST and TokenCut. [13] trains a class agnostic detector (CAD) to output multiple boxes per image using the output of LOST as supervision. Without a single step of training, MOST performs competitively against LOST+CAD on all the datasets. With a class agnostic detector, MOST+CAD outperforms LOST, TokenCut and their CAD counterparts comfortably on all the datasets. On COCO20k, a much harder dataset, MOST+CAD outperforms all the methods with a significant margin demonstrating its superior localization abilities. Effect of EBA: We study the effect of EBA on single-object localization. The task of the EBA module is to identify tokens on foreground instances from similarity maps. We replace the EBA module with the strategy used by LOST [13], effectively giving LOST the ability to localize multiple objects. We use top-100 patches and this system achieves a CorLoc of 63.66 (compared to 74.84 of MOST). The EBA module can automatically pick the right tokens, unlike LOST to localize multiple objects. This experiment demonstrates the benefit of the proposed EBA module. Timing analysis: MOST can localize multiple objects per image and does so by analyzing the similarity maps of all the tokens which is computationally more expensive. To understand this requirement, we perform a timing analysis. On average, LOST takes 0.008s per image while MOST takes 0.3s. MOST obtains a recall of 0.19, 0.21, and 0.08 on VOC07, VOC12, and COCO20k respectively (compared to 0.13, 0.15, and 0.03 of LOST). This translates to localizing an additional 750, 2400, and 7200 instances than LOST on VOC07, VOC12, and COCO20k respectively. We believe the additional time taken by MOST is justified by the improvement in recall performance which is essential for an object localization method. Effect of backbone: We study the effect of backbone on MOST in Table 5. We observe that backbones with a smaller patch size can localize more objects, especially smaller ones (refer to supplementary for qualitative results)\nresulting in a higher CorLoc. This comes at a cost of producing noisier outputs. We refer interested readers to the supplementary for more analysis and qualitative results on the effect of backbones and patch sizes."
        },
        {
            "heading": "4.5. Additional Experiments",
            "text": "Unsupervised Saliency Detection: MOST can easily be extended to unsupervised saliency detection. We experiment with ECSSD [53], DUTS [54] and DUT-OMRON [55] datasets and use the metrics used by [13, 14] for evaluation. All these datasets require methods to segment the salient object in an image. Hence, naively using MOST doesn\u2019t perform well. To extend MOST for saliency detection, we select the largest pool and use the similarity map computed using its tokens as the saliency map. In Table 6, we compare MOST with TokenCut and LOST on the three datasets. MOST outperforms LOST comfortably on all the metrics, datasets and fares competitively against TokenCut. We believe that the ability to detect multiple objects in images is a good tradeoff for a slight drop in performance on saliency detection. We refer interested readers to the supplementary for qualitative results on saliency detection.\nWeakly supervised localization: We evaluate MOST on weakly supervised object localization on CUB-200 [58] and Imagenet [28] datasets respectively, and achieve a CorLoc of 92.42 (vs. 91.8 of TokenCut) on CUB-200 and a CorLoc of 71.4 (vs. 65.4 of TokenCut)."
        },
        {
            "heading": "4.6. Qualitative Results:",
            "text": "We illustrate qualitative results of MOST on VOC2007, 2012 and COCO datasets in Fig. 8. Fig. 8a shows results on VOC 2007 and 2012 and Fig. 8b shows results on COCO dataset. MOST is capable of localizing fairly complex scenes in all the three datasets. We observe that, such unsupervised localization methods are not limited by the categories annotated by humans but can localize regions of \u201cstuff\u201d like sign boards (third image in the first row of Fig. 8a right), rocks (last image of last row in Fig. 8b), and water bodies (first image in second row of Fig. 8b)."
        },
        {
            "heading": "5. Conclusion",
            "text": "We present MOST, an effective method for localizing multiple objects in complex images without a single annotation. MOST leverages object segmentation properties of transformers trained using DINO [27]. We show that the ability of MOST to localize multiple objects in an image is very effective on several object localization and discovery benchmarks. In particular, MOST outperforms recent stateof-the-art methods that train a class agnostic detector, on the task of single object localization, without any training. Further, we show that MOST achieves higher recall and covers more ground truth objects for a fixed set of boxes than LOST [13], a contemporary work on object localization. Finally, we extend MOST to the task of unsupervised saliency detection and report competitive results with recent works. Acknowledgement. This project was partially supported by DARPA SemaFor (HR001119S0085) and DARPA SAIL-ON (W911NF2020009), and Amazon Research Award to Abhinav Shrivastava. Rama Chellappa was supported by an ONR MURI (N00014-20-1-2787)."
        }
    ],
    "title": "MOST: Multiple Object localization with Self-supervised Transformers for object discovery",
    "year": 2023
}