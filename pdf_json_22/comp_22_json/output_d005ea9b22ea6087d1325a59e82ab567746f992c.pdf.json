{
    "abstractText": "In classification problems, as the number of classes increases, correctly classifying a new instance into one of them is assumed to be more challenging than making the same decision in the presence of fewer classes. The essence of the problem is that using the learning algorithm on each decision boundary individually is better than using the same learning algorithm on several of them simultaneously. However, why and when it happens is still not well-understood today. This work\u2019s main contribution is to introduce the concept of heterogeneity of decision boundaries as an explanation of this phenomenon. Based on the definition of heterogeneity of decision boundaries, we analyze and explain the differences in the performance of state of the art approaches to solve multi-class classification. We demonstrate that as the heterogeneity increases, the performances of all approaches, except one-vs-one, decrease. We show that by correctly encoding the knowledge of the heterogeneity of decision boundaries in a decomposition of the multi-class problem, we can obtain better results than state of the art decompositions. The benefits can be an increase in classification performance or a decrease in the time it takes to train and evaluate the models. We first provide intuitions and illustrate the effects of the heterogeneity of decision boundaries using synthetic datasets and a simplistic classifier. Then, we demonstrate how a real dataset exhibits these same principles, also under realistic learning algorithms. In this setting, we devise a method to quantify the heterogeneity of different decision boundaries, and use it to decompose the multi-class problem. The results show significant improvements over state-of-the-art decompositions that do not take the heterogeneity of decision boundaries into account. INDEX TERMS Classification complexity, Heterogeneity of decision boundaries, Multi-class classification",
    "authors": [
        {
            "affiliations": [],
            "name": "PABLO DEL MORAL"
        },
        {
            "affiliations": [],
            "name": "S\u0142AWOMIR NOWACZYK"
        },
        {
            "affiliations": [],
            "name": "SEPIDEH PASHAMI"
        }
    ],
    "id": "SP:9198fb8edce38aa0833baeda012f7184903575d8",
    "references": [
        {
            "authors": [
                "Anna Bosch",
                "Andrew Zisserman",
                "Xavier Mu\u00f1oz"
            ],
            "title": "Image classification using random forests and ferns",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2007
        },
        {
            "authors": [
                "Ram\u00f3n D\u00edaz-Uriarte",
                "Sara Alvarez de Andr\u00e9s"
            ],
            "title": "Gene selection and classification of microarray data using random forest",
            "venue": "BMC Bioinformatics,",
            "year": 2006
        },
        {
            "authors": [
                "Tin Kam Ho",
                "Mitra Basu"
            ],
            "title": "Complexity measures of supervised classification problems",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2002
        },
        {
            "authors": [
                "Vitalik Melnikov",
                "Eyke H\u00fcllermeier"
            ],
            "title": "On the effectiveness of heuristics for learning nested dichotomies: an empirical analysis",
            "venue": "Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Mikel Galar",
                "Alberto Fern\u00e1ndez",
                "Edurne Barrenechea",
                "Humberto Bustince",
                "Francisco Herrera"
            ],
            "title": "An overview of ensemble methods for binary classifiers in multi-class problems: Experimental study on onevs-one and one-vs-all schemes",
            "venue": "Pattern Recognition,",
            "year": 2011
        },
        {
            "authors": [
                "Pablo del Moral",
                "Slawomir Nowaczyk",
                "Anita Sant\u2019Anna",
                "Sepideh Pashami"
            ],
            "title": "Pitfalls of assessing extracted hierarchies for multi-class classification, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Rifkin",
                "Aldebaro Klautau"
            ],
            "title": "In defense of one-vs-all classification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2004
        },
        {
            "authors": [
                "Rohit Babbar",
                "Ioannis Partalas",
                "Eric Gaussier",
                "Massih Reza Amini"
            ],
            "title": "On flat versus hierarchical classification in large-scale taxonomies",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Leo Breiman",
                "Jerome H. Friedman",
                "Richard A. Olshen",
                "Charles J. Stone"
            ],
            "title": "Classification and regression",
            "year": 2017
        },
        {
            "authors": [
                "T.G. Dietterich",
                "G. Bakiri"
            ],
            "title": "Solving Multiclass Learning Problems via Error-Correcting Output Codes",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 1995
        },
        {
            "authors": [
                "Eun Bae Kong",
                "Thomas G Dietterich"
            ],
            "title": "Error- Correcting Output Coding Corrects Bias and Variance",
            "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,",
            "year": 1995
        },
        {
            "authors": [
                "Samy Bengio",
                "J Weston",
                "David Grangier"
            ],
            "title": "Label embedding trees for large multi-class tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2010
        },
        {
            "authors": [
                "Peter W Frey",
                "David J Slate"
            ],
            "title": "Letter recognition using holland-style adaptive classifiers",
            "venue": "Machine learning,",
            "year": 1991
        },
        {
            "authors": [
                "Achim Hoffmann",
                "Rex Kwok",
                "Paul Compton"
            ],
            "title": "Using subclasses to improve classification learning",
            "venue": "Machine Learning: ECML",
            "year": 2001
        },
        {
            "authors": [
                "Chulhee Lee",
                "Seongyoun Woo"
            ],
            "title": "Linear classifier design in the weight space",
            "venue": "Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Weiwei Liu",
                "Ivor W. Tsang",
                "Klaus Robert M\u00fcller"
            ],
            "title": "An easy-to-hard learning paradigm for multiple classes and multiple labels",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Volkan Vural",
                "Jennifer G. Dy"
            ],
            "title": "A hierarchical method for multi-class support vector machines",
            "venue": "Proceedings, Twenty-First International Conference on Machine Learning,",
            "year": 2004
        },
        {
            "authors": [
                "J Friedman"
            ],
            "title": "Multivariate Adaptive Regression Splines",
            "venue": "Annals of Statistics,",
            "year": 1991
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Classification complexity, Heterogeneity of decision boundaries, Multi-class classification\nI. INTRODUCTION Multi-class classification is the task of classifying a new instance into one among at least three classes. Multi-class problems are ubiquitous in many domains such as image classification [1], text classification [2], microarray classification [3], etc. It is widely accepted that these problems become more challenging as the number of classes increases; however, why they are more difficult is an unanswered question. We claim that this extra layer of complexity in multi-class classification problems can be explained, at least partially, by the heterogeneity of decision boundaries. This paper formalizes that notion and provides justification for that claim.\nA decision boundary is the manifold that separates the region of the feature space labeled with one class from the region of the space labeled with another one. We regard the multi-class problem as the problem of solving each of its classes\u2019 one-to-one decision boundaries. As the number of classes increases, so does the number of decision boundaries a learning algorithm has to solve. Intuition and experimental\nresults tell us that as the number of decision boundaries increases, so does the problem\u2019s difficulty. However, experiments also show that if we divide the multi-class problem into smaller ones, the performance of our models can increase.\nA fundamental question is \"why?\". Why is multi-class classification hard? What are the characteristics of the decision boundaries that make the classification problem harder or easier? Where does the intrinsic difficulty of multi-class classification problems lay?\nThe difficulty of classification problems has been extensively studied for the binary case. In [4], the authors characterize the complexity of a binary classification problem based on geometrical features of the decision boundary between the two classes. No similar work exists on multi-class classification. In this paper, we argue that the total difficulty of the multi-class classification problem is not just the combination of the geometrical complexities of the one-to-one decision boundaries; there exists an additional source of difficulty coming from the interaction of different decision boundaries.\nVOLUME 4, 2016 1\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nWe identify the heterogeneity of decision boundaries as a source of this interaction and an ultimate cause of the difficulty of multi-class classification problems.\nThere exist different approaches to solve multi-class classification problems. Some learning algorithms (CART, Neural Networks, K-NN) can naturally handle outputs with more than two classes. In general, when these algorithms are used, little attention is paid to the structure between classes. Some other algorithms can only handle binary outputs natively (SVM, perceptron, logistic regression, AdaBoost,...). In these cases, binary classifiers need to be adapted to a setting that can address multi-class classification problems. The adaptation runs in three steps: the original problem is divided into binary problems, independent models are trained for each one, and the results are aggregated.\nThere are multiple ways to decompose a multi-class classification problem to use binary classifiers [5, 6, 7]. A very interesting common conclusion is how, using the same learning algorithm, results can vary significantly depending on the decomposition chosen [8]. In this scenario, finding the best multi-class classifier means finding the best binary classifiers as well as finding the best decomposition.\nThis fact points back to our claim: there exists an extra layer of difficulty in multi-class classification problems. A layer of difficulty that the learning algorithm often cannot unravel itself, a layer of complexity that can only be dealt with by a smart decomposition. However, [9] shows that some learning algorithms are not affected by how we decompose some particular problems. These results lead us to more questions: What are the characteristics of the learning algorithm that make it sensitive to the different decompositions? How do the characteristics of the learning algorithm and the decision boundaries relate to each other?\nThere are many works dedicated to finding the best possible decompositions of multi-class classification problems. However, there has not been a similar effort into understanding why some decompositions are better than others. In [10], the authors compare flat and hierarchical approaches. They conclude that grouping decision boundaries is better for balanced problems, whereas for unbalanced problems, separating decision boundaries yields better results.\nOur work aims to show how the heterogeneity of decision boundaries can affect the classification performance of different multi-class classification approaches. We will first present a set of illustrative experiments performed on synthetic datasets specifically designed to isolate the effects of the heterogeneity of decision boundaries. We will analyze how different approaches towards solving multi-class classification are affected by the varying degrees of heterogeneity in decision boundaries.\nAfter the examples that build up the intuition, we will present a method to measure the heterogeneity of decision boundaries on a real dataset. We will show how the heterogeneity of decision boundaries affects the classification performance using a state of the art classification algorithm (nonlinear SVM). Finally, we will use the knowledge of\nthe heterogeneity of decision boundaries to decompose the multi-class problem.\nOur contributions can be summarized as: \u2022 We define formally the concept of heterogeneity of\ndecision boundaries. \u2022 We quantify the overall negative effect of the hetero-\ngeneity of decision boundaries on the performance of classification tasks. \u2022 We show how the heterogeneity of decision boundaries can explain the differences in classification performance of several approaches to solving multi-class problems. \u2022 We propose new decomposition approaches that take the heterogeneity of decision boundaries into account and analyze the improvements with respect to approaches that do not. \u2022 We showcase the above phenomena with both intuitive and educational examples, as well as a real dataset using a state of the art classifier."
        },
        {
            "heading": "II. DEFINITIONS AND BACKGROUND",
            "text": "In this section, we will establish the basic concepts to define heterogeneity of decision boundaries.\nDefinition: A learning algorithm receives as input a training set S, sampled from an unknown distribution D and labeled by some target function f : X \u2212\u2192 Y , and should output a classifier h : X \u2212\u2192 Y . The goal of the algorithm is to find the classifier h that minimizes a given loss function with respect to the unknowns D and f .\nIdeally, we would like to find h from the set of all possible functionsHmapping the feature space X into the label space Y . In practice, we restrict the size of H, by either selecting a smaller set of functions (e.g., support vector machines can only find linear functions); or by choosing a heuristic to reduce the search automatically (e.g., CART algorithm to build decision trees; here we assume that a tree of arbitrary depth can represent any function in a discrete space). Reducing the space of hypothesis H by either selecting a family of functions or following a heuristic is called inductive bias.\nThe typical definition of a learning algorithm denotes, in fact, a family of learning algorithms differing in some hyperparameter configuration. With CART, for example, we need to fix hyperparameters related to the depth of the tree, the pruning complexity parameter, the minimal number of instances in each leaf of the tree, and so on; with SVM\u2019s we can select the type of kernel, its characteristics, the regularization parameter, and more; with neural networks, we can choose the structure, number of neurons, activation functions, etc.\nWhen facing a classification task, the standard practice is to choose one (or several) of these families of learning algorithms. Then, through validation schemes like crossvalidation, we estimate the performance of each individual learning algorithm with its hyperparameter configuration. Fixing these hyperparameters can be regarded as a second"
        },
        {
            "heading": "2 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1: On the left, the two underlying decision boundaries are illustrated, and, on the right, the corresponding training set used for this experiment.\nlayer of learning, where instead of choosing the classifier that obtains the best performance, we choose from a pool of learning algorithms, the one that produces the classifier that results in the lowest classification error.\nDefinition: we define a ground truth decision boundary as the manifold that delimits regions of the feature space labeled with one class from those labeled with another, as defined by the unknowns distribution D and labeling function f .\nThe final output of a classifier can be interpreted as a decision boundary approximating the unknown ground truth one. Given a binary classification problem, the role of learning can be regarded as finding the decision boundary that best matches the unknown ground truth one. How well the classifier matches the decision boundary is computed through the estimated classification performance.\nIn practice, different learning algorithms can lead to classifiers with the same classification performance. This can be due to those algorithms\u2019 output being the same decision boundary or different classifiers finding different decision boundaries that result in the same classification performance.\nThe inherent characteristics of a ground truth decision boundary will affect the classification performance of the classifiers trained to solve it. Given a family of learning algorithms, we can look at the space of its possible hyperparameter configurations and measure the classification performance of the corresponding classifiers. We can regard every learning algorithm defined by its hyperparameters as a measurement instrument and the classification performance as the final measure: the different classification performances are a reflection of the inherent characteristics of the ground truth decision boundary.\nGiven a learning algorithm, with its corresponding hyperparameters to tune, in a case with more than one decision boundary, the regions of maximum classification performance in the hyperparameter space of each decision boundary may not overlap. In such a case, there is no single configuration that allows the learning algorithm to learn all decision boundaries as well as if it was solving each decision\nboundary independently. If, on the other hand, those regions overlap, it means that there exists a parameter configuration that allows the learning algorithm to solve both of them at once and still achieve maximum classification performance.\nDefinition:\ngiven two or more decision boundaries and a family of learning algorithms defined by their hyperparameter configurations, we say that these decision boundaries are heterogeneous if their regions of maximum classification performance in the hyperparameter space do not overlap.\nOur definition of heterogeneity is dependent on the characteristics of the data and the characteristics of the learning algorithm chosen. For different algorithms, the geometrical characteristics of the decision boundaries that make them heterogeneous will be different.\nThis paper will study how the heterogeneity of decision boundaries can diminish the classification performance when two or more decision boundaries are solved simultaneously."
        },
        {
            "heading": "III. PROOF OF CONCEPT",
            "text": "We are going to illustrate the concept behind heterogeneous decision boundaries with a toy example. As a family of learning algorithms, we use a decision tree learner, CART [11] (as implemented in the package rpart in R). In our setting, we will only tune the depth of the tree and keep the rest of the parameters constant. The advantage of just tuning the depth of the tree is that we can measure the classifier\u2019s complexity \u2013 the shallower the tree, the simpler the model.\nIn the left picture Fig. 1, we have exemplified two decision boundaries. We will solve two different problems with CART. First, we will classify all the classes simultaneously. Then, we will independently solve the diagonal decision boundary and the horizontal one. The CART algorithm can only make splits parallel to the axis. Therefore, the diagonal decision boundary needs a very deep (or complex) tree, while the parallel one needs a very shallow (or simple) tree. In this way, the decision boundaries become heterogeneous for the selected learning algorithm.\nVOLUME 4, 2016 3\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2: Results of a single classifier with tree depths 2, 6, 10 and 16 respectively.\nIn Fig. 1, on the right, we can see an example of the data generated for this proof of concept. To obtain it, we sample 10000 points from each of the classes denoted by the colors. We add Gaussian noise to the x and y components for the vertical decision boundary to force the learning algorithm to be penalized by overfitting. We perform 5-fold crossvalidation to evaluate the behavior of our classifiers under different hyperparameter configurations.\nIn Fig. 2, we can see the behavior of a single classifier solving both decision boundaries simultaneously for different tree depths. The CART algorithm is presented with the four classes denoted by the colors. As the depth of the tree increases, the classifier matches the diagonal decision boundary better. However, we observe the opposite behavior in the horizontal decision boundary.\nAs a comparison, in Fig. 3, we can see the results of training two different classifiers, separately for each decision boundary. With two classifiers, the decisions match almost perfectly the original distribution of the data. At the same\ntime, for a single classifier, there is no possible depth of the tree where the CART algorithm can achieve optimal performance. By optimal, we mean the best this learning algorithm can do; it is capable of doing better if it solves each decision boundary independently, as demonstrated in Fig. 3.\nIn a way, the two decision boundaries are interacting with each other through the learning algorithm. The global optimization of the depth of the tree forces the learning algorithms to find a compromise between the two decision boundaries.\nIn Fig. 4, we can see the details of the error of the different classifiers as a function of the depth of the tree. To obtain an estimation of the error, we have sampled 10 different training sets from the original distribution.\nFor the vertical decision boundary, the optimal depth of the tree should be 1, and it starts to overfit after depth 4. For the diagonal decision boundary, it underfits until the depth of the tree is 7. The optimal region in the hyperparameter space"
        },
        {
            "heading": "4 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 5: On the left, the training set of the first iteration (no noise). On the right, the training for the last iteration with noise in all inner triangles.\nfor the vertical decision boundary is from 1 to 4; the optimal region for the diagonal decision boundary goes from 7 to 30. When training a single classifier, the optimal depth of the tree is somewhere between 5 and 7. A tree of this depth overfits for the vertical decision boundary.\nTheoretically, there clearly exists a tree where the best performance can be achieved \u2013 but our family of learning algorithms is unable to find it. The inductive bias rooting from selecting the learning algorithm and its hyperparameters makes it impossible for it to solve both decision boundaries with the best possible performance simultaneously.\nOverfitting is of crucial importance to understand this toy experiment. For more advanced algorithms, those able to avoid overfitting, there is no penalization for the most complex trees: in Fig. 4, the red line would be flat, and the optimal region in the hyperparameter space will go from 1 to 30. By design, this experiment has one simple and one complex decision boundaries that do not overlap in the hyperparameter space. However, as we will show later in the paper, model complexity and overfitting are just one possible reason for the heterogeneity; in high dimensional hyperparameter spaces, we can expect more complex sets of heterogeneous decision boundaries.\nObviously, the family of learning algorithms chosen defines the optimal region in the hyperparameter space. Different families will have different parameters to optimize. This illustrative example is specific to decision trees, where we only tune the depth of the tree; other algorithms will exhibit similar behavior, albeit in other scenarios.\nIV. EXPERIMENTS A. MULTI-CLASS CLASSIFICATION AND HETEROGENEITY OF DECISION BOUNDARIES In this experiment, we are going to evaluate how stateof-the-art approaches are affected by the heterogeneity of decision boundaries. In particular, we are going to evaluate a single classifier, One-vs-All decomposition, and One-vs-One decomposition.\nA multi-output classifier can natively deal with more than two classes. CART from [11] is an example of this type of\nlearning algorithm. One-vs-All trains one binary classifier for each class, versus all the data points from the rest of the classes. All binary classifiers need to be evaluated, and the final prediction is built by integrating all the binary predictions through voting. If the output of the classifier is probabilistic, the classifier that outputs the highest probability defines the prediction, cf. [7].\nOne-vs-One trains one binary classifier for each pair of classes. To output a prediction, all binary classifiers are evaluated, and the final prediction is decided via voting. If the output of the classifier is probabilistic, there are many different approaches to combine the outputs, cf. [7]. In our experiment, for simplicity, we choose to combine the Onevs-One classifiers via voting.\nWe will again use synthetic datasets based on diagonal and parallel to the axis decision boundaries. Using our definition of heterogeneity of decision boundaries, we will start with a set of homogeneous decision boundaries, and iteratively we will add heterogeneity. Our dataset will consist of 16 classes. In Fig. 5, we can see an example of all the decision boundaries without noise, and all of the inner classes with noise. We control the heterogeneity by adding noise: without it there is no overfitting, parallel to the axis and diagonal decision boundaries are homogeneous; with noise, very complex trees are penalized when solving parallel to the axis decision boundaries. At each iteration, we will add noise to\nVOLUME 4, 2016 5\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 7: Decomposition of the original multi-class problem into two 4-class problems. On the left, parallel to the axis decision boundaries. On the right, diagonal decision boundaries.\none of the inner triangles. Our learning algorithm will again be CART, and we will tune the pruning parameter through 10-Fold cross-validation.\nFor all our experiments, we will sample 50 training sets and report the accuracy on the original distribution without noise. In this way, we make sure that we correctly evaluate how well the classifier matches all underlying decision boundaries.\nThe results of the comparison of the three approaches can be seen in Fig. 6. In the first iterations, there are no significant differences between the approaches. When decision boundaries are fully homogeneous, the performance of the multi-output classifier is indistinguishable from OVO, and only slightly higher than OVA. However, as we inject more and more heterogeneity through noise, obviously the performance of all the classifiers degrades \u2013 however, both OVA and the multi-output classifier suffer more, and perform increasingly worse in comparison to OVO.\nThe key observation here is that OVO is treating every decision boundary individually, finding the optimal hyperparameter configuration for each of them. Thus, the only difference in performance comes from the problem becoming inherently harder with more noise. The other two approaches group together decision boundaries, including heterogeneous ones, which necessarily leads to additional performance loss, as demonstrated in the toy example in Section III.\nB. EXTENDING THE MULTI-OUTPUT CLASSIFIER TO DEAL WITH HETEROGENEITY.\nGenerally, when a learning algorithm that can deal with more than two classes is chosen, usually no attention is paid to the structure of the classes and possible decompositions of the problem. On the other hand, we have shown how a single multi-output classifier is adversely affected by the heterogeneity of decision boundaries. Based on these finding, however, we will now demonstrate how knowing what type of heterogeneities we are dealing with allows for designing efficient, even if ad-hoc, solutions.\nFor example, in the case of Fig. 5, since we have two types of decision boundaries, we can decompose our 16- classes classification problem into two 4-classes classification problems. The decomposition is visualized in Fig. 7. The first classifier distinguishes between the four meta-classes on the left figure, the second classifier distinguishes between the four meta-classes on the right figure. In this way, every individual class in the Figure is identified unambiguously.\nIn Fig. 8, we can see the comparison of a single multioutput classifier with the 2-fold decomposed multi-output classifier. The graph shows the error compared to OVO, since that is the gold-standard approach that obtains the best results. We knew from before that a single multi-output classifier performs worse and worse as the heterogeneity increases. Here, we are showing that a decomposition that separates the decision boundaries in homogeneous groups obtains results equivalent to those of OVO."
        },
        {
            "heading": "C. EXTENDING BINARY CLASSIFIERS TO DEAL WITH HETEROGENEITY",
            "text": "CART can natively deal with multi-class classification problems; however, other learning algorithms can only deal with binary outputs. Now, we are going to introduce two binary decompositions that incorporate the information about the heterogeneity."
        },
        {
            "heading": "6 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nMinimal Decomposition: we decompose the 16-classes classification problem into 4-binary classification problems. A combination of the 4 classifiers will uniquely define each class. The decomposition can be easily understood with help of Fig. 9\nHeterogeneity Aware Hierarchy (HAH): we are going to decompose the 16-classes classification problem into a hierarchy of 15 binary problems. At each node of the hierarchy, a binary classifier is trained to discern between the instances labeled with the classes on the left branch, and those labeled with the classes on the right branch. In Fig. 10, we have a description of how the hierarchical decomposition has been made.\nIn Fig. 11, we can see the comparison of the two approaches, together with OVA, with respect to OVO. Again, those decompositions that separate decision boundaries into homogeneous groups obtain classification performances similar to OVO, and significantly outperform approaches that do not take into account this information, like OVA.\nD. COMPARISON OF HAH AND RANDOM HIERARCHIES Our hierarchy has been manually created. In this subsection, we will compare our handcrafted hierarchy with a state of the\nart method to find good hierarchies. The method presented in [6] takes a number of hierarchies sampled uniformly at random. Through cross-validation on the training set, we pick the hierarchy that yields the best performance. Then, we use this hierarchy to train the final models.\nFor this experiment, we will calculate the best hierarchy from a pool of 1, 5, 10, 30, 50, and 100 randomly generated hierarchies.\nThe results of the different approaches are presented in Fig. 12. HAH clearly outperforms the rest of the hierarchies. With 16 classes, the number of possible hierarchies is huge compared to the number of hierarchies that effectively separate all heterogeneous decision boundaries.\nAs the number of random hierarchies to choose from increases, the performance slightly increases. However, even for 100 sampled hierarchies, the performance is significantly worse than that of HAH.\nGiven our synthetic dataset, finding hierarchies that separate all heterogeneous decision boundaries into homogeneous groups is a complex task. Each decision boundary does not overlap with many of the other decision boundaries; it is very unlikely to find a suitable decomposition randomly. For simpler cases, where the set of decision boundaries that do not overlap in the hyperparameter space is small, it is more likely to find a good decomposition by sampling random hierarchies."
        },
        {
            "heading": "E. COMPARISON OF HAH WITH ECOCS",
            "text": "In the literature, there are two ways to try to improve the multi-class classification performance based on binary classifiers. The first one, shown previously, tries to find a structure between the classes. The second one makes use of ensembles. In this section, we will compare our handcrafted hierarchy with the Error Correcting Output Code (ECOC) method. ECOC, originally presented in [12], trains a number of binary classifiers, each of them separating a group of classes from another.\nThere are different heuristics to group classes for each binary classifier. The key of ECOC is redundancy and diversity.\nVOLUME 4, 2016 7\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nSeveral classifiers learn each decision boundary. In this way, if one of the classifiers fails, but the rest is correct, they will recover the error. This phenomenon will happen if the errors of the different classifiers are uncorrelated.\nIn ECOC, each binary classifier assigns one of two labels to all the classes of the original problem. In this way, each class is defined by a unique code based on the different classifiers.\nWe will try different lengths of codes (8, 12, 24, 64, 256). To create the codes, we follow the randomized hill-climbing heuristic presented in the original paper.\nThe results are presented in Fig. 13. When presented with the most homogeneous setup, HAH outperforms some of the simpler ECOC variants, but significantly underperforms compared to the most complex. As we inject heterogeneity,\nthe difference with respect to the simpler ECOCs increases. In fact, some of the ECOCs that outperformed the hierarchy for the homogeneous case, underperform in the more heterogeneous one (ECOC 24). The most complex codes (ECOC 64 and 256), consistently outperform the hierarchical approach.\nAt the first glance, these results contradict our hypothesis. Heterogeneity of decision boundaries is not being taken into account in the design of the codes; however, ECOC seems to outperform our HAH, when the number of codes is big enough.\nTo understand this, we will decompose the error in bias and variance, as done in [13]. In Fig. 14, we can see this decomposition. Interestingly, the bias error of the hierarchical decomposition is increasingly lower as the heterogeneity increases when compared to all ECOC versions. However, the more complex ECOC approaches consistently have lower variance error.\nThese results should remind us about the traditional biasvariance trade-off; however, we can look deeper into what type of bias errors are made by the two different approaches. In Fig. 15, we can track where the bias errors are produced. We already stated that the HAH has a lower bias error. This is obvious in two regions: the union of decision boundaries and the parallel to the axis decision boundaries.\nThe first case can be explained through the voting and the error correction mechanisms, the same mechanisms that lower the variance error. However, the bias error in the parallel to the axis decision boundaries is different. The root of this bias error is in the heterogeneity of decision boundaries. Given our experiment setup, most of the 256 classifiers are mixing diagonal and parallel to the axis decision boundaries; most of them are suffering from the heterogeneity of decision boundaries.\nEnsemble approaches like ECOC can significantly increase the classification performance when compared to nonensemble approaches. However, they are not immune to the problems rooting from the heterogeneity of decision boundaries."
        },
        {
            "heading": "F. A HYBRID SOLUTION COMBINING ECOC AND HETEROGENEITY KNOWLEDGE",
            "text": "For the proposed hybrid approach, we are going to create a variation of ECOC that combines the error-correcting power of ECOC and the decomposition based on the knowledge about the heterogeneity of decision boundaries. We will create specific classifiers to separate the four different regions of the left image in Fig. 7. These four regions combine all parallel to the axis decision boundaries. We will exhaust every possible combination of those 4 regions to train 12 different classifiers.\nWe will add these 12 classifiers to classifiers created like in ECOC, with the constraint that they are only concerned with diagonal decision boundaries. In total, we are creating 12 codes to separate the parallel to the axis decision boundaries, and 84 to separate the rest."
        },
        {
            "heading": "8 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 14: On the left, Bias error of ECOC wrt HAH. On the right, Variance error of ECOC wrt HAH.\nThe results of the comparison can be seen in Fig. 16. The hybrid approach performs increasingly better than the ECOC-256 approach, even if the number of codes is much higher in the latter. The reason behind this can only be the heterogeneity of decision boundaries. By manipulating the codes so that parallel to the axis and diagonal decision boundaries are never mixed, we are decreasing the bias of all the base classifiers. In addition, we are using the errorcorrecting properties of this approach to reduce the part of the error rooting in the variance, clearly outperforming the hierarchical decomposition.\nG. NUMBER OF CLASSES, NUMBER OF CODES, AND TIME. One of the main challenges of multi-class classification problems is the time it takes to train models and output predictions [14]. In this section, we will report the training and cross-validation, and testing times of some of the proposed approaches with a different number of classes: 16, 32, and 64. The total number of instances for training and testing will be the same so that we can isolate the effect of the number of classes. In Table 1, we can see the results of this comparison.\nOVO is the standard \"brute-force\" solution. It achieves higher accuracy than one-vs-all or the multi-output classifier.\nThis increase in accuracy comes at the expense of training and testing time, especially if the number of classes is high. The multi-output classifier is the fastest of them, but also the most affected by the heterogeneity in terms of accuracy, together with OVA.\nMinimal decomposition and HAH achieve significantly better results than the multi-output classifier or OVA approaches. In these cases, the benefits of taking into account the heterogeneity of decision boundaries can be seen in the classification performance of our models. When compared to OVO, these approaches achieve similar classification performance. In these cases, the benefit of taking into account the heterogeneity of decision boundaries can be noted in the training and testing times. Note that the testing time of HAH could be further reduced by only evaluating the classifiers connecting the root of the hierarchy and the leaf corresponding to the final prediction [14].\nWith long enough codes, ECOC achieves the best performances among the off-the-shelf approaches. It does so at the expense of training a high number of classifiers on the whole dataset; it requires more time to both train and test. With the hybrid approach, adding information about the heterogeneity of decision boundaries boosts the classification performance significantly, without further harming the com-\nVOLUME 4, 2016 9\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTraining and cv Testing 16 classes 32 classes 64 classes 16 classes 32 classes 64 classes\nMulti-output 3.1 4.6 8.5 0.2 0.5 1.2 OVA 9.8 13.6 23.6 1.8 3.6 11.8 2-Fold MO 3.9 4 5.5 1.1 2.1 5.6 OVO 4.8 8 87 5.5 10.8 15.7 HAH 2.4 2.4 3.1 1.7 3.7 15 ECOC 96 103 103 104 19 55 177 Hybrid 96 102 104 102 18 57 177\nTABLE 1: Time comparison in seconds of different approaches and different number of classes. The approaches are sorted (ascending) by the accuracy in classifying the most heterogeneous cases. Double line separation means significant difference in accuracy.\nputational time.\nH. CONCLUSIONS AND DISCUSSIONS ABOUT THE EXPERIMENT We have showcased, with ad-hoc decompositions, how a heterogeneity-aware decomposition can improve existing algorithms for multi-class classification. In our case, the problem was explicitly designed so that we have control over the modes of heterogeneity.\nAnother aspect to take into account is the simplicity of our synthetic datasets. We have only three types of decision boundaries: diagonals, perpendicular to the axes, and not contiguous (these do not have any effect on the heterogeneity discussion since they can never overfit). The two \u201crelevant\u201d types of decision boundaries can be easily clustered in homogeneous groups with the minimal or the 2-Fold Multi-output decomposition. Problems with more modes of heterogeneity might not be as easily handled with such straightforward approaches.\nHowever, our synthetic dataset is also, in another sense, quite complex. All classes end up having heterogeneous decision boundaries with respect to the rest of the classes. In this setup, methods based on randomness like the method presented in [6], and the ECOC approach presented in [12] are going to underperform when compared with HAH and the hybrid codes approach respectively. If the total number of heterogeneous decision boundaries is smaller, we expect these methods to be more competitive."
        },
        {
            "heading": "V. MEASURING THE HETEROGENEITY AND APPLICATION TO A REAL DATASET",
            "text": "In this section, we are going to apply our ideas of heterogeneity of decision boundaries to practical use in a real dataset, using state of the art learning algorithms. We are going to use SVM as implemented in the package liquidsvm. We define our learning task as the one where we want to find the best SVM model with gaussian kernel, fine tuning the width of the gaussian \u03b3 and the regularization parameter C (in the liquidsvm, the parameter \u03bb is used instead of C, \u03bb is inversely proportional to C and normalized to the number of instances).\nAs an example, we are going to use the letter dataset [15], primarily for its interpretability. The dataset has 20 000 instances and 16 features consisting on descriptors of distorted letters from different fonts, corresponding to the 26 letters in English language. Our aim with these experiments is to show that heterogeneity of decision boundaries does indeed happen in real life situations, both in terms of realistic data and realistic learning algorithms. Moreover, this phenomenon can be measured, and can be exploited.\nIn Figure 17, we show four examples of how the SVM classifier with different hyper-parameter configurations behaves when solving individual decision boundaries. On the top row, we have an example of two heterogeneous decision boundaries, where the regions of maximum performance do not overlap. On the bottom row, we have an example of two homogeneous decision boundaries, where their regions of maximum performance do indeed overlap. In Table 2, we show the results of solving two different decision boundaries with different learning setups. In the first one, we solve the two decision boundaries individually. In the second and third setup, we mix the classes so that the decision boundaries are combined. We run a 10-fold cross-validation and run a paired t-test. In the case of the heterogeneous decision boundaries,"
        },
        {
            "heading": "10 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nwe observer a significant difference in favor of solving the decision boundaries individually. In the case of the homogeneous decision boundaries, we observe small differences in favor of solving the decision boundaries individually, however these differences are not significant.\nA. MEASURING THE HETEROGENEITY So far in the paper, we have discussed heterogeneity qualitatively. In our synthetic datasets, we were creating sets of heterogeneous decision boundaries and we increased or decreased the heterogeneity by increasing or decreasing the number of sets of heterogeneous decision boundaries. In a non-synthetic dataset, we do not have this information a priori, and instead need to find a way to measure the heterogeneity.\nFor each decision boundary, we are going to compare the accuracy of a classifier with different hyper parameter configurations. Let AiS be the array of accuracies of a family of classifiers solving the decision boundary i over the hyperparameter space S. The plots in Figure 17 are in fact the visualization of different AiS , where we have sampled a grid of hyper-parameter configurations. We measure the amount, or degree, of heterogeneity between two decision boundaries i and j with the equation:\nHDB(i, j) = max(AiS)+max(A j S)\u2212max(A i+Aj)S (1)\nThe last term of the equation corresponds to the selection of the hyper-parameter configuration that produces the highest accuracy of both decision boundaries, as they are combined and solved together. If the two decision boundaries\nshare a region of the hyper-parameter space where they achieve the maximum accuracy, the value of HDB is 0, i.e., the decision boundaries are homogeneous. If they do not share the region of maximum performance, the maximum value of the combined (Ai +Aj)S will not coincide with the individual maxima of AiS and A j S . In this case, the value of HDB would be bigger than 0, indicating that the decision boundaries are heterogeneous, and capturing the degree of the heterogeneity. This measure can be easily expanded to more than two decision boundaries:\nHDB(1, .., n) = n\u2211 i=1 max(AiS)\u2212max( n\u2211 i=1 Ai)S (2)\nIn the next set of experiments, we are going to demonstrate that the heterogeneity of decision boundaries, as measured using HDB, indeed has an effect on classification performance. This will establish that our intuitions and results from Section 4 also apply in this more realistic scenario. For the first experiment, we are going to repeatedly split randomly the letters of the alphabet into two groups. Using a crossvalidation scheme, we are going to evaluate the HDB of every split using the training set, then we are going to train a classifier (SVM with the best hyper-parametrization) to distinguish between the two groups of letters. We will name this classifier \"split classifier\". On the validation set, we are going to measure the accuracy of the split classifier and compare it with the accuracy of an OVO classifier trained with all 26 classes, but evaluated on the two groups only. We use OVO as a benchmark, since it treats every decision boundary independently, and we have shown before that it is not affected by the presence of heterogeneous decision boundaries.\nOn the left of Fig. 18, we can see the relationship between HDB and the difference in accuracy between the OVO classifier and split classifier. HDB is an absolute measure, i.e., it is not normalized to the number of decision boundaries. To explore the effect of the number of classes, we have sampled 30 different splits of the letters, with different number of classes in each split. In the right plot of Fig. in 18, we see the difference in accuracies as a function of the number of classes (counting the majority group in every split).\nThere is a clear correlation between the HDB and the difference in accuracy. The more heterogeneous the decision boundaries in the split are, the higher the difference in accuracies between an approach that deals with the heterogeneous boundaries separately (OVO) and the approach that deals with them simultaneously (split classifier). There is also an obvious correlation between how heterogeneous the decision boundaries are and the number of classes in the majority group of each split. This is an expected result, since we assume that the set of heterogeneous decision boundaries is uniformly distributed. The more decision boundaries are present in the split, the higher is the expected number of heterogeneous decision boundaries, and thus the higher value\nVOLUME 4, 2016 11\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 18: On the left, the difference in the accuracy between an OVO classifier and a classifier trained on the two groups of letters as a function of the HDB measure. On the right, the difference in the accuracy between an OVO classifier and a classifier trained on the two groups of letters as a function of the number of classes in the majoritary group.\nof HDB. The extreme case is when we have one class against the rest, where the difference in accuracies approaches 0.\nIn principle, our notion of heterogeneity of decision boundaries seems to contradict the conclusions of [9]: that there are no significant differences in performance between OVA and OVO. However, this last result points to a phenomenological reason that can reconcile both stances. In the Letters dataset, classes are quite compact. Every individual class is \u201cequally\u201d dissimilar from the rest of the classes. It is only when we merge decision boundaries that do not share classes, that we see heterogeneous decision boundaries. In datasets like letters, the OVA decomposition clusters the decision boundaries into homogeneous groups, therefore OVO and OVA can achieve a similar performance in many real world problems. However, we believe that this situation is not universal.\nAccording to this experiment, the difference in accuracy between the OVO classifier and the split classifier is not only correlated to the measure of HDB, but also to the different number of classes in each side of the split. For the next experiment, we want to isolate the effect of the heterogeneity of decision boundaries independently of the number of classes in the split. To this effect, we are going to select the splits that separate the 26 letters into two groups of 13. There are 10 400 600 different possible splits, so exhaustive analysis is clearly infeasible, and we will sample just 150 randomly. Again, we will measure the HDB value and compare it with the difference in accuracies between OVO and the split classifier.\nIn Fig. 19, we can see the relation between the HDB measured and the difference between the accuracies of approaches that take into account the heterogeneity of decision boundaries and those that not. We can confirm our hypothesis: independently of the number of classes in the split, the more heterogeneity, the bigger the difference between the accuracies of approaches that take into account the heterogeneity of decision boundaries and those that not. We measure the Pearson correlation between these two variables\nand obtain a value of 0.3. We test against the null hypothesis that the two variables are uncorrelated and obtain a p-value 0.01. There exist a clear relation between the HDB measured and the difference in accuracies."
        },
        {
            "heading": "B. EXPLOITING HDB",
            "text": "In this section, we want to find the split of the 26 classes into two groups of 13 where the performance of the split classifier is as close as possible to the performance of the OVO classifier. Evaluating every possible split is not feasible, so we will use the correlation between HDB and the difference in accuracies presented in the previous experiment. Still, we need to evaluate the HDB of every possible split. To avoid this process we are going to use an off-the-shelf genetic algorithm. As a fitness function, we use the HDB of the split measured with the training set in each fold of the cross-validation scheme. We weigh this value with the number of classes, so that the best splits have 13 classes in each group, but the rest of the splits are not discarded in the search process and help the algorithm to find the best possible"
        },
        {
            "heading": "12 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nsplit. To evaluate our approach, we will benchmark our candidate split against 150 random splits. In Fig. 20, we can observe the distribution of the difference in the performance of the 1000 random splits we have selected. Highlighted in green is the position of the split found by our genetic algorithm. The split optimized for HDB ranks 14th out of 150 in terms of accuracy difference.\nUsing this methodology of finding the best split, we are going to create a binary hierarchy by greedily selecting the best possible splits in a top-bottom manner. We decide to keep the hierarchy as balanced as possible. Given that, for 26 classes, we need a hierarchy with 5 levels, we will allow the first level to be unbalanced, as long as the splits at the rest of the levels are balanced.\nTo have a benchmark, we compare the results against a randomly sampled hierarchy and OVO. Results are presented in Table 3. We perform paired t-test comparisons between the different approaches. The hierarchical approach using HDB measurements to create the hierarchy is significantly better than the hierarchical approach using a random hierarchy, with a p-value of 0.04. There is no significant difference between the One-vs-One approach and the hierarchy using HDB (with a p-value of 0.22).\nVI. LITERATURE REVIEW What makes classification hard has been extensively studied, but only in the binary case. In [4], the difficulty of binary classification problems is described by several factors, studying geometrical properties of the decision boundary. It is indicated that the study of the difficulty of multi-class classification problems could be done through the summary\nof individual class-vs-class decision boundaries. The findings of our work imply that the total complexity of the multi-class problem is not simply the addition of the complexities of the individual decision boundaries but also depends on the heterogeneity of decision boundaries. The total complexity can be more than the sum of the parts, i.e., of the complexities of the individual problems.\nIn [10], the authors provide a theoretical explanation of whether to choose \"flat\" or \"hierarchical\" approaches in multi-class classification. In their formalism, this means eliminating or creating intermediate nodes in the hierarchy, i.e., whether to group or not decision boundaries. Their discussion is based on the study of error generalization bounds. Analyzing the unbalance of classes, the authors suggest to use shallow hierarchies for well-balanced problems and use deeper hierarchies for unbalanced cases, i.e., grouping decision boundaries when the classes have similar number of instances and separate them otherwise.\nAlthough not directly, the unbalance of classes can be related to the heterogeneity of decision boundaries as we have described it in this paper. Many hyperparameters can be related to the number of instances per class (number of neighbors in KNN, number of instances per leaf in CART, slack variables in support vector machines, and so on). On the other hand, in our experiments, we have shown an example of a completely shallow hierarchy (multi-output classifier) performing significantly worse than a deep hierarchy (HAH) for a completely balanced problem.\nIn [16], the authors observe that the performance of C4.5 on multi-class classification problems decreases if the classes are grouped. The authors attribute this behavior to the fact that the instances of the grouped classes may lie in different areas of the feature space. It would be very interesting to to analyze this problem from the point of view of heterogeneity of decision boundaries. In addition, the datasets where this effect is more acute happen to be those with higher imbalance of classes.\n[17] presents a novel method to train linear classifier. Instead of learning a model on the data space, they propose to learn models in the weight space. There is a clear correspondence between the weight space in their work and our hyper-parameter space. In their case, the weights are directly related to the final model; in our case, the hyper-parameters define the classifier created by a learning algorithm.\nIn the literature, ensemble methods like bagging, stacking, or boosting are always among the most competitive methods. The core idea behind the success of ensemble methods is to train diverse classifiers and ensemble their outputs. ECOC [12], is an ensemble method tailored specifically for multiclass classification. A number of binary classifiers are trained to separate a group of classes from the rest, i.e., grouping decision boundaries in different ways.\nIn our work, we have shown that ensemble approaches have benefits that are independent of the heterogeneity of decision boundaries; however, ECOC is not immune to the adverse effects of the heterogeneity of decision boundary. If\nVOLUME 4, 2016 13\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nmost of the binary classifiers are affected, the final ensemble will as well.\nEnsemble methods are one of the ways to improve multiclass classification performance. The other direction is finding a structure in the classes. Examples of these types of approaches are [18] and [14, 19]. In [18], the authors build a chain of classifiers, while in [14, 19], the authors build a hierarchy of classes. The approaches that we have discussed in this paper fall into this category.\nIn our work, we have based our decomposition strategies on the knowledge about the heterogeneity of decision boundaries. In the literature, these approaches have been based on the similarity of classes. There are two main ways of measuring the similarity of classes: geometrical distance of centroids [19], and classifier based distance [14]. The latter consists of first training a classifier on the data, and estimate which classes are similar or dissimilar based on the classification performance. None of these similarities are directly linked to the heterogeneity of decision boundaries, although we speculate that the classifier based similarity might be correlated.\nIn the field of regression, heterogeneity has already been treated. [20] presents MARS, a regression method that uses splines to create different fits in different parts of the feature space.\nVII. CONCLUSIONS AND DISCUSSIONS The main contribution of this paper is the identification and description of a previously untreated phenomenon: how heterogeneity of decision boundaries can affect the performance of multi-class classification performance.\nThe concept of heterogeneity in decision boundaries speaks about the relationship of these decision boundaries through the lens of a learning algorithm. Two decision boundaries can be heterogeneous under one learning algorithm and homogeneous under another. Heterogeneity of decision boundaries is not a direct result of the geometrical properties of the decision boundaries, but how those properties relate to each other through the learning algorithm. If two decision boundaries are heterogeneous, a learning algorithm will solve them better individually than simultaneously.\nWe have identified the inductive bias introduced by the hyperparameter tuning as a source of heterogeneity. Hyperparameter tuning is generally done with the help of validation schemes like cross-validation. This is a strong example of inductive bias: among all possible hypotheses, we are only going to select the ones that can be found by the particular learning algorithm and hyperparameter configuration chosen. If the hyperparameter configurations that achieve maximum performance for two or more decision boundaries do not overlap, it will result in a performance loss.\nWe have shown how one part of the classification problem can negatively affect another part of the problem in terms of classification performance. To the best of our knowledge, this is the first comprehensive description of pure algorithmic bias mechanism in classification. Some decision boundaries\nare incorrectly estimated by the presence of other decision boundaries; this is especially true when we compare the same learning algorithms acting individually on the same decision boundaries. Research of fairness in machine learning and AI generally focuses on bias rooting from the quality of the data [21]. Heterogeneity of decision boundaries looks a promising direction to study fairness in AI.\nWe have studied how the heterogeneity of decision boundaries affects different approaches to solve multi-class classification problems. The main lesson we have learned is that whenever a learning algorithm has to solve two or more heterogeneous decision boundaries, the classification performance decreases.\nWe have also shown that the information about the heterogeneity of decision boundaries can be used to devise approaches that obtain better classification performances. The information about the heterogeneity of decision boundaries can also be used to devise approaches that allow us to get similar classification performance at a lower computational cost as compared to approaches that do not use information about heterogeneity. In the field of multi-class classification, this is important, since generally computation time increases significantly with the number of classes.\nIn our work, we have designed the synthetic dataset with only two types of decision boundaries so that we could easily identify and group them in homogeneous groups. Finding the same structures in real-life problems might not be as simple. Some of the proposed decompositions, like the minimal or 2- Fold multi-output decompositions, are very restrictive; more complex structures in the heterogeneity might not be as easily represented with these schemes. We believe that other approaches, such as ECOC or hierarchical decompositions, are potentially more practical since they are flexible enough to represent complex hidden structures among decision boundaries.\nWe have applied our findings about the heterogeneity of decision boundaries on a real dataset using a competitive learning algorithm like SVM. We have proposed a way to measure the degree of heterogeneity of decision boundaries and have corroborated its effects on classification performance. Finally, we have used the measurement of the heterogeneity of decision boundaries to extract a hierarchy of classes that outperforms other hierarchies that do not take this information into account.\nWe have used multi-class classification as a natural field to showcase the heterogeneity of decision boundaries. However, indirectly, we have also shown the effects on binary classification by comparing OVA and OVO. As a hypothesis, one could further decompose a binary problem into a collection of smaller binary problems defining new decision boundaries that might be heterogeneous. How to do it is not obvious. However, positive results in ensemble methods for binary classification problems indicates the potential of this idea, especially if we consider the computational time to output a new prediction for ensemble methods.\nAn interesting final remark is how we have used learning"
        },
        {
            "heading": "14 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nalgorithms to describe decision boundaries. Generally, the complexity of data is too big for humans to understand and visualize its characteristics easily. In quantum mechanics, nature is described by interacting with it through the process of measure. Here, we have taken a similar approach, describing the nature of the data by how it interacts with our measuring instruments (performance of classifiers learned by a particular algorithm). In addition, we have described how decision boundaries can interact with another through the effects of learning.\nREFERENCES [1] Anna Bosch, Andrew Zisserman, and Xavier Mu\u00f1oz.\nImage classification using random forests and ferns. In Proceedings of the IEEE International Conference on Computer Vision, 2007. [2] Fabrizio Sebastiani. Machine Learning in Automated Text Categorization, 2002. [3] Ram\u00f3n D\u00edaz-Uriarte and Sara Alvarez de Andr\u00e9s. Gene selection and classification of microarray data using random forest. BMC Bioinformatics, 2006. [4] Tin Kam Ho and Mitra Basu. Complexity measures of supervised classification problems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(3):289\u2013300, 2002. [5] Johannes Fuernkranz. Round Robin Classification. Journal of Machine Learning Research, 2:721\u2013747, 2002. [6] Vitalik Melnikov and Eyke H\u00fcllermeier. On the effectiveness of heuristics for learning nested dichotomies: an empirical analysis. Machine Learning, 107(8- 10):1537\u20131560, 2018. [7] Mikel Galar, Alberto Fern\u00e1ndez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. An overview of ensemble methods for binary classifiers in multi-class problems: Experimental study on onevs-one and one-vs-all schemes. Pattern Recognition, 44(8):1761\u20131776, 2011. [8] Pablo del Moral, Slawomir Nowaczyk, Anita Sant\u2019Anna, and Sepideh Pashami. Pitfalls of assessing extracted hierarchies for multi-class classification, 2021. [9] Ryan Rifkin and Aldebaro Klautau. In defense of one-vs-all classification. Journal of Machine Learning Research, 5:101\u2013141, 2004. [10] Rohit Babbar, Ioannis Partalas, Eric Gaussier, and Massih Reza Amini. On flat versus hierarchical classification in large-scale taxonomies. Advances in Neural Information Processing Systems, pages 1\u20139, 2013. [11] Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. Classification and regression trees. 2017. [12] T. G. Dietterich and G. Bakiri. Solving Multiclass Learning Problems via Error-Correcting Output Codes. Journal of Artificial Intelligence Research, 2:263\u2013286, 1995.\n[13] Eun Bae Kong and Thomas G Dietterich. ErrorCorrecting Output Coding Corrects Bias and Variance. In Proceedings of the Twelfth International Conference on Machine Learning, 1995. [14] Samy Bengio, J Weston, and David Grangier. Label embedding trees for large multi-class tasks. Advances in Neural Information Processing Systems, 23(1):163\u2013 171, 2010. [15] Peter W Frey and David J Slate. Letter recognition using holland-style adaptive classifiers. Machine learning, 6(2):161\u2013182, 1991. [16] Achim Hoffmann, Rex Kwok, and Paul Compton. Using subclasses to improve classification learning. In Luc De Raedt and Peter Flach, editors, Machine Learning: ECML 2001, pages 203\u2013213, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg. [17] Chulhee Lee and Seongyoun Woo. Linear classifier design in the weight space. Pattern Recognition, 88:210\u2013 222, 2019. [18] Weiwei Liu, Ivor W. Tsang, and Klaus Robert M\u00fcller. An easy-to-hard learning paradigm for multiple classes and multiple labels. Journal of Machine Learning Research, 18:1\u201338, 2017. [19] Volkan Vural and Jennifer G. Dy. A hierarchical method for multi-class support vector machines. Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004, (2):831\u2013838, 2004. [20] J Friedman. Multivariate Adaptive Regression Splines. Annals of Statistics, 19(1):1\u2013141, 1991. [21] David Danks and Alex John London. Algorithmic bias in autonomous systems. IJCAI International Joint Conference on Artificial Intelligence, (August 2017):4691\u2013 4697, 2017.\nPABLO DEL MORAL is a PhD candidate in Data Mining at Center for Applied Intelligent Systems Research, Halmstad University, Sweden. He has a Masters degree in Data Science from University of Granada, and a Masters degree in Nuclear, Particle and Astrophysics from Technical University of Munich.\nVOLUME 4, 2016 15\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nS\u0142AWOMIR NOWACZYK is Professor in Machine Learning, working at Center for Applied Intelligent Systems Research, Halmstad University, Sweden. He has received his MSc degree from Poznan University of Technology in 2002 and his PhD degree from the Lund University of Technology in 2008. During the last decade his research focused on knowledge representation, data mining and self-organizing systems, especially in large and distributed data streams, including unsuper-\nvised modelling. He is a board member for the Swedish AI Society, and a research leader for the School of Information Technology at Halmstad University. Slawomir has led multiple research projects related to applying Artificial Intelligence and Machine Learning in many different domains, such as transport and automotive, energy, smart cities as well as healthcare. In most cases, this research was done in collaboration with industry and public administration organizations \u2013 inspired by practical challenges and leading to tangible results and deployed solutions.\nSEPIDEH PASHAMI is a senior researcher at RISE and a lecturer at the Center for Applied Intelligent Systems Research, Halmstad University. She received her Ph.D. degree from AASS Research Centre, \u00d6rebro University, Sweden, in 2016. Her research interests include predictive maintenance, interactive machine learning, causal inference, and representation learning. She has been involved as a researcher and research leader in several projects (e.g. EVE, In4Uptime, ARISE\nand HEALTH) together with Volvo Group AB, applying machine learning techniques for predictive maintenance of heavy-duty vehicles."
        },
        {
            "heading": "16 VOLUME 4, 2016",
            "text": "This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        }
    ],
    "title": "Why is Multi-Class Classification Hard?",
    "year": 2022
}