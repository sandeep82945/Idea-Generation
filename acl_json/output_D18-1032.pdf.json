{
    "abstractText": "Multilingual knowledge graphs (KGs) such as DBpedia and YAGO contain structured knowledge of entities in several distinct languages, and they are useful resources for cross-lingual AI and NLP applications. Cross-lingual KG alignment is the task of matching entities with their counterparts in different languages, which is an important way to enrich the crosslingual links in multilingual KGs. In this paper, we propose a novel approach for crosslingual KG alignment via graph convolutional networks (GCNs). Given a set of pre-aligned entities, our approach trains GCNs to embed entities of each language into a unified vector space. Entity alignments are discovered based on the distances between entities in the embedding space. Embeddings can be learned from both the structural and attribute information of entities, and the results of structure embedding and attribute embedding are combined to get accurate alignments. In the experiments on aligning real multilingual KGs, our approach gets the best performance compared with other embedding-based KG alignment approaches.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhichun Wang"
        },
        {
            "affiliations": [],
            "name": "Qingsong Lv"
        },
        {
            "affiliations": [],
            "name": "Xiaohan Lan"
        },
        {
            "affiliations": [],
            "name": "Yu Zhang"
        }
    ],
    "id": "SP:336a4dbf6a9f0f01480c27b456a66d0d64d2d93b",
    "references": [
        {
            "authors": [
                "Christian Bizer",
                "Jens Lehmann",
                "Georgi Kobilarov",
                "S\u00f6ren Auer",
                "Christian Becker",
                "Richard Cyganiak",
                "Sebastian Hellmann."
            ],
            "title": "Dbpedia-a crystallization point for the web of data",
            "venue": "Web Semantics:",
            "year": 2009
        },
        {
            "authors": [
                "Antoine Bordes",
                "Nicolas Usunier",
                "Alberto GarciaDuran",
                "Jason Weston",
                "Oksana Yakhnenko."
            ],
            "title": "Translating embeddings for modeling multirelational data",
            "venue": "Proceedings of Advances in neural information processing systems (NIPS2013),",
            "year": 2013
        },
        {
            "authors": [
                "Joan Bruna",
                "Wojciech Zaremba",
                "Arthur Szlam",
                "Yann Lecun."
            ],
            "title": "Spectral networks and locally connected networks on graphs",
            "venue": "Proceedings of International Conference on Learning Representations (ICLR2014).",
            "year": 2014
        },
        {
            "authors": [
                "Muhao Chen",
                "Yingtao Tian",
                "Mohan Yang",
                "Carlo Zaniolo."
            ],
            "title": "Multilingual knowledge graph embeddings for cross-lingual knowledge alignment",
            "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (AAAI2017),",
            "year": 2017
        },
        {
            "authors": [
                "Michael Defferrard",
                "Xavier Bresson",
                "Pierre Vandergheynst."
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "Proceedings Advances in Neural Information Processing Systems (NIPS2016), pages 3844\u20133852.",
            "year": 2016
        },
        {
            "authors": [
                "Yanchao Hao",
                "Yuanzhe Zhang",
                "Shizhu He",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "A joint embedding method for entity alignment of knowledge bases",
            "venue": "Proceedings of China Conference on Knowledge Graph and Semantic Computing (CCKS2016), pages 3\u201314.",
            "year": 2016
        },
        {
            "authors": [
                "Mikael Henaff",
                "Joan Bruna",
                "Yann LeCun."
            ],
            "title": "Deep convolutional networks on graph-structured data",
            "venue": "arXiv preprint arXiv:1506.05163.",
            "year": 2015
        },
        {
            "authors": [
                "Guoliang Ji",
                "Shizhu He",
                "Liheng Xu",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Knowledge graph embedding via dynamic mapping matrix",
            "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
            "year": 2015
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling."
            ],
            "title": "Semisupervised classification with graph convolutional networks",
            "venue": "Proceedings of International Conference on Learning Representations (ICLR2017).",
            "year": 2017
        },
        {
            "authors": [
                "Yankai Lin",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Yang Liu",
                "Xuan Zhu."
            ],
            "title": "Learning entity and relation embeddings for knowledge graph completion",
            "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI2015), volume 15,",
            "year": 2015
        },
        {
            "authors": [
                "Roberto Navigli",
                "Simone Paolo Ponzetto."
            ],
            "title": "Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
            "venue": "Artificial Intelligence, 193:217\u2013 250.",
            "year": 2012
        },
        {
            "authors": [
                "Maximilian Nickel",
                "Kevin Murphy",
                "Volker Tresp",
                "Evgeniy Gabrilovich."
            ],
            "title": "A review of relational machine learning for knowledge graphs",
            "venue": "Proceedings of the IEEE, 104(1):11\u201333.",
            "year": 2016
        },
        {
            "authors": [
                "Thomas Rebele",
                "Fabian M. Suchanek",
                "Johannes Hoffart",
                "Joanna Asia Biega",
                "Erdal Kuzey",
                "Gerhard Weikum."
            ],
            "title": "Yago: A multilingual knowledge base from wikipedia, wordnet, and geonames",
            "venue": "Proceedings of the Fifteenth International Semantic",
            "year": 2016
        },
        {
            "authors": [
                "Michael Schlichtkrull",
                "Thomas N Kipf",
                "Peter Bloem",
                "Rianne van den Berg",
                "Ivan Titov",
                "Max Welling."
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1703.06103.",
            "year": 2017
        },
        {
            "authors": [
                "Fabian M Suchanek",
                "Gjergji Kasneci",
                "Gerhard Weikum."
            ],
            "title": "Yago: A large ontology from wikipedia and wordnet",
            "venue": "Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203\u2013217.",
            "year": 2008
        },
        {
            "authors": [
                "Zequn Sun",
                "Wei Hu",
                "Chengkai Li."
            ],
            "title": "Cross-lingual entity alignment via joint attributepreserving embedding",
            "venue": "Proceedings of the Sixteenth International Semantic Web Conference (ISWC2017), pages 628\u2013644.",
            "year": 2017
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "CoRR, abs/1710.10903.",
            "year": 2017
        },
        {
            "authors": [
                "Quan Wang",
                "Zhendong Mao",
                "Bin Wang",
                "Li Guo."
            ],
            "title": "Knowledge graph embedding: A survey of approaches and applications",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 29(12):2724\u2013 2743.",
            "year": 2017
        },
        {
            "authors": [
                "Zhen Wang",
                "Jianwen Zhang",
                "Jianlin Feng",
                "Zheng Chen."
            ],
            "title": "Knowledge graph embedding by translating on hyperplanes",
            "venue": "Proceedings of the Twentyeighth AAAI Conference on Artificial Intelligence (AAAI2014), volume 14, pages 1112\u20131119.",
            "year": 2014
        },
        {
            "authors": [
                "Hao Zhu",
                "Ruobing Xie",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Iterative entity alignment via joint knowledge embeddings",
            "venue": "Proceedings of the Twenty-sixth International Joint Conference on Artificial Intelligence (IJCAI2017), pages 4258\u20134264.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 349\u2013357 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n349"
        },
        {
            "heading": "1 Introduction",
            "text": "Knowledge graphs (KGs) represent human knowledge in the machine-readable format, are becoming the important basis of many applications in the areas of artificial intelligence and natural language processing. Multilingual KGs such as DBpedia (Bizer et al., 2009), YAGO (Suchanek et al., 2008; Rebele et al., 2016), and BabelNet (Navigli and Ponzetto, 2012) are especially valuable if cross-lingual applications are to be built. Besides the knowledge encoded in each distinct language, multilingual KGs also contain rich cross-lingual links that match the equivalent entities in different languages. The cross-lingual links play an important role to bridge the language gap in a multilingual KG; however, not all the equivalent entities\nare connected by cross-lingual links in most multilingual KGs. Therefore, increasingly more research work studies the problem of cross-lingual KG alignment, aiming to match entities in different languages in a multilingual KG automatically.\nTraditional cross-lingual KG alignment approaches either rely on machine translation technique or defining various language-independent features to discover cross-lingual links. Most recently, several embedding-based approaches have been proposed for cross-lingual KG alignment, including MTransE (Chen et al., 2017) and JAPE (Sun et al., 2017). Given two KGs and a set of pre-aligned entities of them, embedding-based approaches project entities into low-dimensional vector spaces; entities are matched based on the computations on their vector representations. Following very similar ideas as above, JE (Hao et al., 2016) and ITransE (Zhu et al., 2017) are embedding-based approaches for matching entities between heterogeneous KGs, and they can also work for the problem of cross-lingual KG alignment. The above embedding-based approaches can achieve promising performance without machine translation or feature engineering.\nHowever, we find that the above approaches all try to jointly model the cross-lingual knowledge and the monolingual knowledge in one unified optimization problem. The loss of two kinds of knowledge has to be carefully balanced during the optimization. For example, JE, MTransE, and ITransE all use hyper-parameters to weight the loss of entity alignments in the loss functions of their models; JAPE uses the pre-aligned entities to combine two KGs as one, and adds weight to the scores of negative samples in its loss function. In the above approaches, entities\u2019 embeddings have to encode both the structural information in KGs and the equivalent relations of entities. Further-\nmore, the attributes of entities (e.g., the age of a people, the population of a country) have not been fully utilized in the existing models. MTransE and ITransE cannot use attributional information in KGs; although JAPE includes the attribute types in the model, the attribute values of entities are ignored. We believe that considering the attribute values can further improve the results of KG alignment.\nHaving the above observations, we propose a new embedding-based KG alignment approach which directly models the equivalent relations between entities by using graph convolutional networks (GCNs). GCN is a kind of convolutional network which directly operates on graphstructured data; it generates node-level embeddings by encoding information about the nodes\u2019 neighborhoods. The adjacencies of two equivalent entities in KGs usually contain other equivalent entities, so we choose GCNs to generate neighborhood-aware embeddings of entities, which are used to discover entity alignments. Our approach can also provide a simple and effective way to include entities\u2019 attribute values in the alignment model. More specifically, our approach has the following advantages:\n\u2022 Our approach uses the entity relations in each KG to build the network structure of GCNs, and it only considers the equivalent relations between entities in model training. Our approach has small model complexity and can achieve encouraging alignment results.\n\u2022 Our approach only needs pre-aligned entities as training data, and it does not require any pre-aligned relations or attributes between KGs.\n\u2022 Entity relations and entity attributes are effectively combined in our approach to improve the alignment results.\nIn the experiments on aligning real multilingual KGs, our approach gets the best performance compared with the baseline methods.\nThe rest of this paper is organized as follows, Section 2 reviews some related work, Section 3 introduces some background knowledge, Section 4 describes our proposed approach, Section 5 presents the evaluation results, Section 6 is the conclusion and future work."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 KG Embedding",
            "text": "In the past few years, much work has been done on the problem of KG embedding. KG embedding models embed entities and relations in a KG into a low-dimensional vector space while preserving the original knowledge. The embeddings are usually learned by minimizing a global loss function of all the entities and relations in a KG, which can be further used for relation prediction, information extraction, and some other tasks. TransE is a representative KG embedding approach (Bordes et al., 2013), which projects both entities and relations into the same vector space; if a triple (h, r, t) holds, TransE wants that h + r \u2248 t. The embeddings are learned by minimizing a margin-based ranking criterion over the training set. TransE model is simple but powerful, and it gets promising results on link prediction and triple classification problems. To further improve TransE, several enhanced models based on it have been proposed, including TransR (Lin et al., 2015), TransH (Wang et al., 2014) and TransD (Ji et al., 2015) etc. By introducing new representations of relational translation, later approaches achieve better performance at the cost of increasing model complexity. There are many other KG embedding approaches, recent surveys (Wang et al., 2017; Nickel et al., 2016) give detailed introduction and comparison."
        },
        {
            "heading": "2.2 Embedding-based KG Alignment",
            "text": "Here we introduce the KG Alignment approaches most related to ours, and discuss the main differences between our approach and them.\nJE (Hao et al., 2016) jointly learns the embeddings of multiple KGs in a uniform vector space to align entities in KGs. JE uses a set of seed entity alignments to connect two KGs, and then learns the embeddings by using a modified TransE model, which adds a loss of entity alignments in its global loss function.\nMTransE (Chen et al., 2017) encodes entities and relations of each KG in a separated embedding space by using TransE; it also provides transitions for each embedding vector to its crosslingual counterparts in other spaces. The loss function of MTransE is the weighted sum of two component models\u2019 loss (i.e., knowledge model and alignment model). To train the alignment model, MTransE needs a set of aligned triples of two KGs.\nJAPE (Sun et al., 2017) combines structure embedding and attribute embedding to match entities in different KGs. Structure embedding follows the TransE model, which learns vector representations of entities in the overlay graph of two KGs. Attribute embedding follows the Skip-gram model, which aims to capture the correlations of attributes. To get desirable results, JAPE needs the relations and attributes of two KGs to be aligned in advance.\nITransE (Zhu et al., 2017) is a joint knowledge embedding approach for multiple KGs, which is also suitable for the cross-lingual KG alignment problem. ITransE first learns both entity and relation embeddings following TransE; then it learns to map knowledge embeddings of different KGs into a joint space according to a set of seed entity alignments. ITransE performs iterative entity alignment by using the newly discovered entity alignments to update joint embeddings of entities. ITransE requires all relations being shared among KGs.\nThe above approaches follow the similar framework to match entities in different KGs. They all rely on TransE model to learn entity embeddings, and then define some kinds of transformation between embeddings of aligned entities. Compared with these approaches, our approach uses an entirely different framework; it uses GCNs to embed entities in a unified vector space, where aligned entities are expected to be as close as possible. Our approach only focuses on matching entities in two KGs, and it does not learn embeddings of relations. MTransE, JAPE, and ITransE all require relations being aligned or shared in KGs; our approach does not need this kind of prior knowledge."
        },
        {
            "heading": "3 Problem Formulation",
            "text": "KGs represent knowledge about real-world entities as triples. Here we consider two kinds of triples in KGs: relational triples, and attributional triples. Relational triples represents relations between entities, and it has the form \u3008entity1, relation, entity2\u3009. Attributional triples describe attributes of entities, and it has the form \u3008entity, attribute, value\u3009. For example in the data of YAGO, graduatedFrom is a relation, and (Albert Einstein, graduatedFrom, ETH Zurich) is a relational triple; diedOnDate is an attribute, and (Albert Einstein, diedOnDate, 1955) is an attributional triple. Both relational and attributional\ntriples describe important information about entities, we will take both of them into account in the task of cross-lingual KG alignment.\nFormally, we represent a KG as G = (E,R,A, TR, TA), where E,R,A are sets of entities, relations and attributes, respectively; TR \u2282 E \u00d7 R \u00d7 E is the set of relational triples, TA \u2282 E\u00d7A\u00d7V is the set of attributional triples, where V is the set of attribute values.\nLet G1 = (E1, R1, A1, TR1 , T A 1 ) and G2 =\n(E2, R2, A2, T R 2 , T A 2 ) be two KGs in different languages, and S = {(ei1 , ei2)|ei1 \u2208 E1, ei2 \u2208 E2}mi=1 be a set of pre-aligned entity pairs between G1 andG2. We define the task of cross-lingual KG alignment as finding new entity alignments based on the existing ones. In multilingual KGs such as DBpedia and YAGO, the cross-lingual links in them can be used to build the sets of pre-aligned entity pairs. The already known entity alignments are used as seeds or training data in the process of KG alignment."
        },
        {
            "heading": "4 The Proposed Approach",
            "text": "The framework of our proposed approach is shown in Figure 1. Given two KGsG1 andG2 in different languages, and a set of known aligned entity pairs S = {(ei1 , ei2)}mi=1 between them, our approach automatically find new entity alignments based on GCN-based entity embeddings. The basic idea of our approach is to use GCNs to embed entities from different languages into a unified vector space, where equivalent entities are expected to be as close as possible. Entity alignments are predicted by applying a pre-defined distance function to entities\u2019 GCN-representations."
        },
        {
            "heading": "4.1 GCN-based Entity Embedding",
            "text": "GCNs (Bruna et al., 2014; Henaff et al., 2015; Defferrard et al., 2016; Kipf and Welling, 2017) are a type of neural network that directly operates on graph data. GCNs allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The inputs of a GCN are feature vectors of nodes and the structure of the graph; the goal of a GCN is to learn a function of features on the input graph and produces a nodelevel output. GCNs can encode information about the neighborhood of a node as a real-valued vector, which was usually used for classification or regression. When solving the problem of KG alignment, we assume that (1) equivalent entities tend\nto have similar attributes, and (2) equivalent entities are usually neighbored by some other equivalent entities. GCNs can combine the attribute information and the structure information together, therefore our approach uses GCNs to project entities into low-dimensional vector space, where equivalent entities are close to each other.\nA GCN-model consists of multiple stacked GCN layers. The input to the l-th layer of the GCN model is a vertex feature matrix, H(l) \u2208 Rn\u00d7d(l) , where n is the number of vertices and d(l) is the number of features in the l-th layer. The output of the l-th layer is a new feature matrixH(l+1) by the following convolutional computation:\nH(l+1) = \u03c3 ( D\u0302\u2212 1 2 A\u0302D\u0302\u2212 1 2H(l)W (l) ) (1)\nwhere \u03c3 is an activation function;A is a n\u00d7n connectivity matrix that represents the structure information of the graph; A\u0302 = A+ I , and I is the identity matrix; D\u0302 is the diagonal node degree matrix of A\u0302; W (l) \u2208 Rd(l)\u00d7d(l+1) is the weight matrix of the l-th layer in the GCN, d(l+1) is the dimensionality of new vertex features.\nStructure and Attribute Embedding. In our approach, GCNs are used to embed entities of two KGs in a unified vector space. To utilize both structure and attribute information of entities, our approach assigns two feature vectors to each entity in GCN layers, structure feature vector hs and attribute feature vector ha. In the input layer, h (0) s is randomly initialized and updated during the training process; h(0)a is the attribute vectors of entities and it is fixed during the model training. Let Hs and Ha be the structure and attribute feature matrices of all the entities, we redefine the convolu-\ntional computation as:\n[H(l+1)s ;H (l+1) a ] = \u03c3 ( D\u0302\u2212 1 2 A\u0302D\u0302\u2212\n1 2 [H(l)s W (l) s ;H (l) a W (l) a ] ) (2)\nwhere W (l)s and W (l) a are the weight matrices for structure features and attribute features in the l-th layer, respectively; [ ; ] denotes the concatenation of two matrices. The activation function \u03c3 is chosen as ReLU(\u00b7) = max(0, \u00b7). Model Configuration. More specifically, our approach uses two 2-layer GCNs, and each GCN processes one KG to generate embeddings of its entities. As defined in Section 3, we denote two KGs as G1 = (E1, R1, A1, TR1 , T A 1 ) and G2 = (E2, R2, A2, T R 2 , T A 2 ); and let their corresponding GCN models be denoted as GCN1 and GCN2. As for the structure feature vectors of entities, we set the dimensionality of feature vectors to ds in all the layers of GCN1 and GCN2; and two GCN models share the weight matrices W (1)s and W (2) s for the structure features in two layers. As for the attribute vectors of entities, we set the dimensionality of output feature vectors to da. Because two KGs may have different number of attributes (i.e. |A1| 6= |A2|), the dimensionalities of the input attribute feature vectors in two GCN models are different. The first layer of each GCN model transforms the input attribute feature vectors into vectors of size da; and two GCN-models generate attribute embeddings of the same dimensionality. Table 1 outlines the parameters of two GCNs in our approach. The final outputs of two GCNs are (ds + da)-dimensional embeddings of entities, which are further used to discover entity alignments.\nComputation of Connectivity Matrix. In a GCN model, the connectivity matrix A defines the neighborhoods of entities in the convolutional computation. For an undirected graph, the adjacency matrix can be directly used as As. But KGs are relational multi-graphs, entities are connected by typed relations. Therefore, we design a particular method for computing A of a KG; we let aij \u2208 A indicate to what extent the information of alignments propagates from the i-th entity to the j-th entity. The probability of two entities being equivalent differs greatly considering they connect to aligned entities by different relations (e.g., hasParent vs. hasFriend). Therefore, we compute two measures, which are called functionality and inverse functionality, for each relation:\nfun(r) = #Head Entities of r\n#Triples of r (3)\nifun(r) = #Tail Entities of r\n#Triples of r (4)\nwhere #Triples of r is the number of triples of relation r; #Head Entities of r and #Tail Entities of r are the numbers of head entities and tail entities of r, respectively. To measure the influence of the i-th entity over the j-the entity, we set aij \u2208 A as:\naij = \u2211\n\u3008ei,r,ej\u3009\u2208G\nifun(r)+ \u2211\n\u3008ej ,r,ei\u3009\u2208G\nfun(r) (5)"
        },
        {
            "heading": "4.2 Alignment prediction",
            "text": "Entity alignments are predicted based on the distances between entities from two KGs in the GCNrepresentation space. For entities ei in G1 and vj\ninG2, we compute the following distance measure between them:\nD(ei, vj) =\u03b2 f(hs(ei),hs(vj)\nds\n+ (1\u2212 \u03b2)f(ha(ei),ha(vj)) da\n(6)\nwhere f(x,y) =\u2016 x \u2212 y \u20161, hs(\u00b7) and ha(\u00b7) denote the structure embedding and attribute embedding of an entity, respectively; ds and da are dimensionalities of structure embeddings and attribute embeddings; \u03b2 is a hyper-parameter that balances the importance of two kinds of embeddings.\nThe distance is expected to be small for equivalent entities and large for non-equivalent ones. For a specific entity ei in G1, our approach computes the distances between ei and all the entities in G2, and returns a list of ranked entities as candidate alignments. The alignment can be also performed from G2 to G1. In the experiments, we report the results of both directions of KG alignment."
        },
        {
            "heading": "4.3 Model Training",
            "text": "To enable GCNs to embed equivalent entities as close as possible in the vector space, we use a set of known entity alignments S as training data to train GCN models. The model training is performed by minimizing the following margin-based ranking loss functions:\nLs = \u2211\n(e,v)\u2208S \u2211 (e\u2032,v\u2032)\u2208S\u2032\n(e,v)\n[f(hs(e),hs(v)) + \u03b3s\n\u2212 f(hs(e\u2032),hs(v\u2032)]+ (7)\nLa = \u2211\n(e,v)\u2208S \u2211 (e\u2032,v\u2032)\u2208S\u2032\n(e,v)\n[f(ha(e),ha(v)) + \u03b3a\n\u2212 f(ha(e\u2032),ha(v\u2032)]+ (8)\nwhere [x]+ = max{0, x}, S\u2032(e,v) denotes the set of negative entity alignments constructed by corrupting (e, v), i.e. replacing e or v with a randomly chosen entity in G1 or G2; \u03b3s, \u03b3a > 0 are margin hyper-parameters separating positive and negative entity alignments. Ls and La are loss functions for structure embedding and attribute embedding, respectively; they are independent of each other and hence are optimized separately. We adopt stochastic gradient descent (SGD) to minimize the above loss functions."
        },
        {
            "heading": "5 Experiment",
            "text": ""
        },
        {
            "heading": "5.1 Datasets",
            "text": "We use the DBP15K datasets in the experiments, which were built by Sun et al. (2017). The datasets were generated from DBpedia, a large-scale multilingual KG containing rich inter-language links between different language versions. Subsets of Chinese, English, Japanese and French versions of DBpedia are selected following certain rules. Table 2 outlines the detail information of the datasets. Each dataset contains data two KGs in different languages and 15 thousand interlanguage links connecting equivalent entities in two KGs. In the experiments, the known equivalent entity pairs are used for model training and testing."
        },
        {
            "heading": "5.2 Experiment Settings",
            "text": "In the experiments, we compared our approach with JE, MTransE and JAPE. We also build JAPE\u2032, a variant of JAPE which does not use pre-aligned relations and attributes. Because the approach ITransE performs iterative alignment and it requires two KGs sharing the same relations, we do not include it in the comparison. The interlanguage links in each dataset are used as the gold standards of entity alignments. For all the compared approaches, we use 30% of inter-language links for training and 70% of them for testing; the split of training and testing are the same for all approaches. We use Hits@k as the evaluation measure to assess the performance of all the\napproaches. Hits@k measures the proportion of correctly aligned entities ranked in the top k candidates. For the parameters of our approach, we set ds = 1, 000, da = 100; the margin \u03b3s = \u03b3a = 3 in the loss function, and \u03b2 in the distance measure is emperically set to 0.9."
        },
        {
            "heading": "5.3 Results",
            "text": "Table 3 shows the results of all the compared approaches on DBP15K datasets. We report Hits@1, Hits@10 and Hits@50 of approaches on each dataset. Because we use the same datasets as in (Sun et al., 2017), the results of JE, MTransE, and JAPE are obtained from (Sun et al., 2017). For JAPE and JAPE\u2032, each of them has three variants: Structure Embedding without negative triples (SE w/o neg.), Structure Embedding (SE), Structure and attribute joint embedding (SE+AE). We use GCN(SE) and GCN(SE+AE) to denote two variants of our approach: one only uses relational triples to perform structure embedding, and the other uses both relational and attributional triples to perform structure and attribute embedding.\nGCN(SE) vs. GCN(SE+AE) We first compare the results of GCN(SE) and GCN(SE+AE) to see whether the attributional information is helpful in the KG alignment task. According to the results, adding attributes in our approach do lead to slightly better results. The improvements range from 1% to 10%, which are very similar to the improvements of JAPE(SE) over JAPE(SE+AE). It shows that the KG alignment mainly relays on the structural information in KGs, but the attributional information is still useful. Our approach uses the same framework for embedding structure and attribute information, the combination of two kinds of embeddings works effectively.\nGCN(SE+AE) vs. Baselines On the dataset of DBP15KZH\u2212EN , JAPE(SE+AE) performs best and gets five best Hits@k values; our approach GCN(SE+AE) gets the best Hits@1 in the alignment direction of ZH\u2192EN. The results of GCN(SE+AE) and JAPE gets very close results regarding Hits@1 and Hits@10 in the direction of ZH\u2192EN. In the alignment direction of EN\u2192ZH, JAPE(SE+AE) outperforms GCN(SE+AE) by about 2-3%. But it should be noticed that JAPE uses additional aligned relations and attributes as its inputs,\nwhile our approach does not use these kinds of prior knowledge. If compared with JAPE\u2032,\nGCN(SE+AE) performs better than it regrading Hits@1 and Hits@10. While compared with\nJE and MTransE, GCN(SE+AE) outperform them by more than 10% in most cases. On the datasets of DBP15KJA\u2212EN and DBP15KFR\u2212EN , GCN(SE+AE) outperforms all the compared approaches regarding all the Hits@k measures. Even compared with JAPE which uses extra relation and attribute alignments, GCN(SE+AE) still gets better results than it.\nComparing with all the baselines, both GCN(SE) and GCN(SE+AE) outperform JE and MTransE significantly. Among all the baselines, JAPE is the strongest one; it might due to its ability of using both relational and attributional triples, and the extra alignments of relations and attributes that it consumes. Our approach achieves better results than JAPE on two datasets; Although JAPE performs better than our approach, the differences between their results are small. If there are no existing relation and attribute alignments between two KGs, our approach will have distinct advantage over JAPE.\nGCN vs. JAPE using different sizes of training data\nTo investigate how the size of training set affects the results of our approach, we further compare our approach with JAPE by using different number of pre-aligned entities as training data. For JAPE, the pre-aligned entities are used as seeds to make their vectors overlapped. In our approach, all the pre-aligned entities are used to train GCN models. Intuitively, the more pre-aligned entities used, the better results should be obtained by both GCN and JAPE.\nHere we use different proportions of pre-aligned entities as training data, which ranges 10% to 50% with step 10%; all the rest of pre-aligned entities are used for testing. Figure 2 shows the Hits@1 of two approaches in three datasets. It shows that\nboth approaches perform better as the size of training data increases. And our approach always outperforms JAPE except using 40% pre-aligned entities as training data in Figure 2(a). Especially in the tasks of aligning Japanese to English and French to English, our approach has a distinct advantage over JAPE."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "This paper presents a new embedding-based KG alignment approach which discovers entity alignments based on the entity embeddings learned by GCNs. Our approach can make use of both the relational and the attributional triples in KGs to discover the entity alignments. We evaluate our method on the data of real multilingual KGs, and the results show the advantages of our approach over the compared baselines.\nIn the future work, we will explore more advanced GCN models for KG alignment task, such as Relational GCNs (Schlichtkrull et al., 2017) and Graph Attention Networks (GATs) (Velickovic et al., 2017). Furthermore, how to iteratively discover new entity alignments in the framework of our approach is another interesting direction that we will study in the future."
        },
        {
            "heading": "Acknowledgments",
            "text": "The work is supported by the National Natural Science Foundation of China (No. 61772079) and the National Key R&D Program of China (No. 2017YFC0804004)."
        }
    ],
    "title": "Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks",
    "year": 2018
}