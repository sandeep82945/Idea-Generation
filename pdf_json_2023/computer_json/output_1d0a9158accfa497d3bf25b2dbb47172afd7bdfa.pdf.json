{
    "abstractText": "Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that na\u0131\u0308ve mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on different downstream tasks with significant efficiency. Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9 AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base. Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance discrimination, while accelerating training by 2\u00d7. To our best knowledge, this is the very first work to consider mixing for MIM from the perspective of pretext task design. Code will be made available.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kai Chen"
        },
        {
            "affiliations": [],
            "name": "Zhili Liu"
        },
        {
            "affiliations": [],
            "name": "Lanqing Hong"
        },
        {
            "affiliations": [],
            "name": "Hang Xu"
        },
        {
            "affiliations": [],
            "name": "Zhenguo Li"
        },
        {
            "affiliations": [],
            "name": "Dit-Yan Yeung"
        }
    ],
    "id": "SP:8f390140c62ca5360b3789c33105c2892e7857e7",
    "references": [
        {
            "authors": [
                "Alexei Baevski",
                "Wei-Ning Hsu",
                "Qiantong Xu",
                "Arun Babu",
                "Jiatao Gu",
                "Michael Auli"
            ],
            "title": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
            "venue": "arXiv preprint arXiv:2202.03555,",
            "year": 2022
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "arXiv preprint arXiv:2106.08254,",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101 \u2013 mining discriminative components with random forests",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Nuno Vasconcelos"
            ],
            "title": "Cascade R-CNN: delving into high quality object detection",
            "venue": "In CVPR, 2018",
            "year": 2018
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "arXiv preprint arXiv:2006.09882,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Chen",
                "Lanqing Hong",
                "Hang Xu",
                "Zhenguo Li",
                "Dit-Yan Yeung"
            ],
            "title": "Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2003
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "arXiv preprint arXiv:2104.02057,",
            "year": 2057
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In CVPR,",
            "year": 2014
        },
        {
            "authors": [
                "Kevin Clark",
                "Minh-Thang Luong",
                "Quoc V Le",
                "Christopher D Manning"
            ],
            "title": "Electra: Pre-training text encoders as discriminators rather than generators",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Carl Doersch",
                "Abhinav Gupta",
                "Alexei A Efros"
            ],
            "title": "Unsupervised visual representation learning by context prediction",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoyi Dong",
                "Jianmin Bao",
                "Ting Zhang",
                "Dongdong Chen",
                "Weiming Zhang",
                "Lu Yuan",
                "Dong Chen",
                "Fang Wen",
                "Nenghai Yu"
            ],
            "title": "Peco: Perceptual codebook for bert pre-training of vision transformers",
            "venue": "arXiv preprint arXiv:2111.12710,",
            "year": 2021
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Alaaeldin El-Nouby",
                "Gautier Izacard",
                "Hugo Touvron",
                "Ivan Laptev",
                "Herv\u00e9 Jegou",
                "Edouard Grave"
            ],
            "title": "Are large-scale datasets necessary for self-supervised pre-training",
            "venue": "arXiv preprint arXiv:2112.10740,",
            "year": 2021
        },
        {
            "authors": [
                "Linus Ericsson",
                "Henry Gouk",
                "Timothy M Hospedales"
            ],
            "title": "How well do self-supervised models transfer",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "In ICCV,",
            "year": 2010
        },
        {
            "authors": [
                "Yuxin Fang",
                "Li Dong",
                "Hangbo Bao",
                "Xinggang Wang",
                "Furu Wei"
            ],
            "title": "Corrupted image modeling for self-supervised visual pre-training",
            "venue": "arXiv preprint arXiv:2202.03382,",
            "year": 2022
        },
        {
            "authors": [
                "Spyros Gidaris",
                "Praveer Singh",
                "Nikos Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "arXiv preprint arXiv:1803.07728,",
            "year": 2018
        },
        {
            "authors": [
                "Xavier Glorot",
                "Yoshua Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "In AIS- TATS,",
            "year": 2010
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre H Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Daniel Guo",
                "Mohammad Gheshlaghi Azar",
                "others"
            ],
            "title": "Bootstrap your own latent: A new approach to self-supervised learning",
            "venue": "arXiv preprint arXiv:2006.07733,",
            "year": 2006
        },
        {
            "authors": [
                "Jianhua Han",
                "Xiwen Liang",
                "Hang Xu",
                "Kai Chen",
                "Lanqing Hong",
                "Chaoqiang Ye",
                "Wei Zhang",
                "Zhenguo Li",
                "Xiaodan Liang",
                "Chunjing Xu"
            ],
            "title": "Soda10m: Towards large-scale object detection benchmark for autonomous driving",
            "venue": "arXiv preprint arXiv:2106.11118,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "arXiv preprint arXiv:2111.06377,",
            "year": 2021
        },
        {
            "authors": [
                "Olivier J H\u00e9naff",
                "Skanda Koppula",
                "Evan Shelhamer",
                "Daniel Zoran",
                "Andrew Jaegle",
                "Andrew Zisserman",
                "Jo\u00e3o Carreira",
                "Relja Arandjelovi\u0107"
            ],
            "title": "Object discovery and representation networks",
            "venue": "arXiv preprint arXiv:2203.08777,",
            "year": 2022
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Ioannis Kakogeorgiou",
                "Spyros Gidaris",
                "Bill Psomas",
                "Yannis Avrithis",
                "Andrei Bursuc",
                "Konstantinos Karantzalos",
                "Nikos Komodakis"
            ],
            "title": "What to hide from your students: 9 Attention-guided masked image modeling",
            "venue": "arXiv preprint arXiv:2203.12719,",
            "year": 2022
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Krause",
                "Jia Deng",
                "Michael Stark",
                "Li Fei-Fei"
            ],
            "title": "Collecting a large-scale dataset of fine-grained cars",
            "venue": "In Workshop on Fine-Grained Visual Categorization,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Master\u2019s thesis, University of Tront,",
            "year": 2009
        },
        {
            "authors": [
                "Fei-Fei Li",
                "Fergus Rob",
                "Perona Pietro"
            ],
            "title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories",
            "venue": "CVPRW,",
            "year": 2004
        },
        {
            "authors": [
                "Kaican Li",
                "Kai Chen",
                "Haoyu Wang",
                "Lanqing Hong",
                "Chaoqiang Ye",
                "Jianhua Han",
                "Yukuai Chen",
                "Wei Zhang",
                "Chunjing Xu",
                "Dit-Yan Yeung"
            ],
            "title": "Coda: A real-world road corner case dataset for object detection in autonomous driving",
            "venue": "arXiv preprint arXiv:2203.07724,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Jihao Liu",
                "Xin Huang",
                "Yu Liu",
                "Hongsheng Li"
            ],
            "title": "Mixmim: Mixed and masked image modeling for efficient visual representation learning",
            "venue": "arXiv preprint arXiv:2205.13137,",
            "year": 2022
        },
        {
            "authors": [
                "Zhili Liu",
                "Jianhua Han",
                "Kai Chen",
                "Lanqing Hong",
                "Hang Xu",
                "Chunjing Xu",
                "Zhenguo Li"
            ],
            "title": "Task-customized selfsupervised pre-training with scalable dynamic routing",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "In ICLR,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Maria-Elena Nilsback",
                "Andrew Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "In Indian Conference on Computer Vision, Graphics & Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In CVPR,",
            "year": 2012
        },
        {
            "authors": [
                "Deepak Pathak",
                "Philipp Krahenbuhl",
                "Jeff Donahue",
                "Trevor Darrell",
                "Alexei A Efros"
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqiang Shen",
                "Zechun Liu",
                "Zhuang Liu",
                "Marios Savvides",
                "Trevor Darrell",
                "Eric Xing"
            ],
            "title": "Un-mix: Rethinking image mixtures for unsupervised visual representation learning",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Yuge Shi",
                "N Siddharth",
                "Philip HS Torr",
                "Adam R Kosiorek"
            ],
            "title": "Adversarial masking for self-supervised learning",
            "venue": "arXiv preprint arXiv:2201.13100,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Chen Wei",
                "Haoqi Fan",
                "Saining Xie",
                "Chao-Yuan Wu",
                "Alan Yuille",
                "Christoph Feichtenhofer"
            ],
            "title": "Masked feature prediction for self-supervised visual pre-training",
            "venue": "arXiv preprint arXiv:2112.09133,",
            "year": 2021
        },
        {
            "authors": [
                "Fangyun Wei",
                "Yue Gao",
                "Zhirong Wu",
                "Han Hu",
                "Stephen Lin"
            ],
            "title": "Aligning pretraining for detection via object-level contrastive learning",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jianxiong Xiao",
                "James Hays",
                "Krista A Ehinger",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Sun database: Large-scale scene recognition from abbey to zoo",
            "venue": "In CVPR,",
            "year": 2010
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zheng Zhang",
                "Yue Cao",
                "Yutong Lin",
                "Jianmin Bao",
                "Zhuliang Yao",
                "Qi Dai",
                "Han Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "venue": "arXiv preprint arXiv:2111.09886,",
            "year": 2021
        },
        {
            "authors": [
                "Haohang Xu",
                "Xiaopeng Zhang",
                "Hao Li",
                "Lingxi Xie",
                "Wenrui Dai",
                "Hongkai Xiong",
                "Qi Tian"
            ],
            "title": "Seed the views: Hierarchical semantic alignment for contrastive representation learning",
            "venue": "In PAMI,",
            "year": 2022
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Sanghyuk Chun",
                "Seong Joon Oh",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "CutMix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ],
            "title": "MixUp: Beyond empirical risk minimization",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "LIU Zhili",
                "Kai Chen",
                "Jianhua Han",
                "HONG Lanqing",
                "Hang Xu",
                "Zhenguo Li",
                "James Kwok"
            ],
            "title": "Task-customized masked autoencoder via mixture of cluster-conditional experts",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Tete Xiao",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Semantic understanding of scenes through the ade20k dataset",
            "venue": "In IJCV, 2019",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Self-supervised learning (SSL) has become one of the most popular pre-training paradigm due to its independence of human annotation. Previous literature mainly focuses on the handcrafted pretext task design [16,23,46] and instance discrimination [7,11], while with the development of Vision Transformer [18], masked image modeling (MIM), deeply motivated by masked language modeling [15], has started to demonstrate more superior effectiveness by firstly masking some patches of the input images and then reconstructing the masked patches from visible ones by predicting certain\ntargets generated by masked patches. In order to complete reconstruction, the encoder is expected to generate highly semantic representation which can be better transferred to downstream tasks [26, 36, 37, 62] for superior performance.\nExisting MIM works mainly concentrate on the design of the reconstruction targets (e.g., visual tokenizers [3, 17], pixels [27,57], graphical features [53] and instance discrimination [2, 19, 63]) and masking strategies (e.g., random [3, 27], attention-guide [31] and sample-dependent [49]). See more detailed discussions in Sec. 2. Despite the superior performance, we observe that the input augmentations for MIM have been seldom explored. Specifically, adding color jittering, an essential augmentation technique of contrastive learning [9], with MAE [27] even degrades transfer results, suggesting that MIM might possess a different preference for data augmentations, and the effective data augmentation strategies for MIM are still an open question.\nIn this paper, we explore the usage of image mixing, a commonly used technique in both supervised [59, 60] and contrastive learning [48, 58], with MAE [27]. We start by\nar X\niv :2\n30 3.\n17 15\n2v 2\n[ cs\n.C V\n] 7\nJ un\n2 02\nconstructing a simple baseline to adopt mixing with MAE directly, which, different from in supervised and contrastive learning, would instead ease the reconstruction pretext by increasing the mutual information between the model input and reconstruction target due to the usage of image mixing with global self-attention as proved in Sec. 3.1. To address this issue, we propose homologous recognition, an auxiliary pretext task to enforce each patch to recognize homologous patches explicitly according to attention distributions before reconstruction, and build our Mixed Autoencoder network (MixedAE) in Sec. 3.3. Moreover, we demonstrate that our simple yet effective method can not only achieve significant performance improvement, but also conduct object-aware SSL pre-training without any specifically designed modules for better downstream dense perception results in Sec. 3.4.\nConcurrently, MixMIM [38] also considers mixing with MAE, but different from ours, 1) Purpose: MixMIM uses mixing to recover the 2D structure after random masking for an efficient implementation to conduct MAE-style pretraining on hierarchy Vision Transformers [40], while ours utilizes mixing to conduct object-aware SSL pre-training for better representation learning. 2) Method: MixMIM uses masked self-attention to only perform attention within patches from the same images given the mixing masks as input, sharing the exactly same pretext task with MAE, while ours requires explicit homologous recognition given mixing masks as target, actively emerging mixing into the pretext design. 3) Formulation: The mixing ratio r is limited to 0.5 in MixMIM, which instead can be flexibly selected from (0, 0.5] in our formulation. See more details in Sec. 3.\nThe main contributions of this work contain three parts:\n1. We propose the Mixed Autoencoder (MixedAE), a simple yet effective approach to conduct object-aware pretraining without introducing any specifically designed modules. With extensive experiments, we demonstrate that MixedAE can achieve the state-of-the-art transfer performance on various downstream tasks including image classification, semantic segmentation and object detection, while maintaining significant efficiency.\n2. We theoretically demonstrate the underlying design differences between MIM and previous supervision with mixing (e.g., supervised and contrastive learning).\n3. To our best knowledge, this is the first work to consider mixing as an effective data augmentation strategy for MIM from the perspective of pretext design with a pure autoencoder-based architecture."
        },
        {
            "heading": "2. Related Work",
            "text": "Self-supervised learning aims at learning a transferable representation without human annotation. Previous works mainly focus on handcrafted pretext design [16, 23, 46] and\ninstance discrimination [9, 10, 25]. Mask image modeling (MIM), inspired by the mask language modeling [15], has achieved significant performance with superior pre-training efficiency by firstly masking portion of an image, and then reconstructing the masked part based on the visible one.\nReconstruction target. BEiT [3] pioneeringly proposes to predict visual tokens generated by a pre-trained visual tokenizer [47], which is simplified by SimMIM [57] to use pixel values as the reconstruction target directly. MAE [27] proposes an asymmetric encoder-decoder architecture for better efficiency. Besides pixels as the target, MaskFeat [53] utilizes HOG features, while PeCo [17] enhances the BEiT tokenizer with an additional perceptual loss. Recent works combine the idea of instance discrimination [9] with MIM. iBOT [63] considers MIM as a self-distillation process with a Siamese architecture, and data2vec [2] proposes a unified framework to conduct masked reconstruction pre-training for speech, images and languages. SplitMask [19] divides an image into two equal partitions and performs contrastive learning and MIM in the multi-task manner. In this paper, we build MixedAE based on MAE due to its efficiency and effectiveness, while the improvement brought by MixedAE is complementary to more advanced reconstruction targets.\nMasking strategy. Instead of random masking [3, 27], AttMask [31] proposes a novel attention-guided masking strategy by masking according to the attention map of the final Transformer layer, while ADIOS [49] introduces an adversarial objective between masking and reconstruction to generate learnable masks for MIM pre-training. In this paper, we utilize random masking for MixedAE following MAE due to its simplicity and effectiveness.\nInput augmentation instead has been less explored for MIM. Instead of masking, CIM [22] adopts a small BEiT as the generator to corrupt an image, which is further taken as input to an enhancer to reconstruct the corrupted patches or distinguish the corrupted patches from the uncorrupted ones. Concurrently, MixMIM [38] considers mixing with MAE also, but different from ours, MixMIM uses masked self-attention to only perform attention within patches from the same images given the mixing masks as input, which is exactly the same with MAE from the perspective of pretext design, while ours utilizes mixing as part of the pretext task actively to conduct object-aware SSL pre-training."
        },
        {
            "heading": "3. Method",
            "text": "In this section, we start by adopting mixing in MAE [27] with a simple baseline in Sec. 3.1, which, as we can prove, will instead ease the reconstruction pretext task. Then, we propose a novel auxiliary pretext task to formulate our final MixedAE, which can not only alleviate the ease of reconstruction, but also achieve object-aware SSL pre-training without specifically designed modules in Secs. 3.2 to 3.4."
        },
        {
            "heading": "3.1. Mixing: A Simple Baseline",
            "text": "Given a unlabeled dataset, we randomly sample a clean data batch with size B, which are later divided into nonoverlapping patch sequences {xi}Bi=1 (xi \u2208 RL\u00d7(P\n2\u00b7C)) following ViT [18], where L is the sequence length, P is the patch size, and C is the image channel dimension.\nMixing. The data batch is further separated into groups {{xji} 1/r i=1}Brj=1, and each group will generate a single mixed image, where r \u2208 (0, 0.5] is the mixing ratio representing the ratio of patches each clean image contributes to within a single mixed sample. Different from MixMIM [38], r is not restricted to 0.5 in our formulation. Therefore, the mixing process for the j-th group can be represented as,\nx\u0302j = \u03c3mix({xji},M j) = 1/r\u2211 i=1 1(M j = i)xji , (1)\nwhere 1(\u00b7) is indicator function and M j \u2208 {1, 2, ..., 1/r}L represents a random mixing mask independently generated for the j-th group, which satisfies,\nL\u2211 l=1 1(M jl = i) = rL, \u2200i \u2208 {1, 2, ..., 1/r}. (2)\nSo, M j determines the source patch in each position of x\u0302j , while maintaining the mixing ratio for each clean image equal with r (i.e. symmetric mixing). The mixed images x\u0302j are further fed into the encoder for feature extraction, which can be represented as z\u0302j = ENC(x\u0302j).\nUnmixing. Following MAE [27], z\u0302j is then \u201cunmixed\u201d to recover the input batch before mixing by inserting a special [MASK] token with M j . For \u2200i \u2208 {1, 2..., 1/r}, we have,\nzji = 1(M j = i)z\u0302j + [1\u2212 1(M j = i)][MASK]. (3)\nThe \u201cunmixed\u201d group {zji} 1/r i=1 is then taken as input to the decoder for pixel reconstruction, as yji = DEC(z j i ). Finally, the reconstruction loss can be formulated as,\nLrecon = 1/r\u2211 i=1 L\u2211 l=1 [1\u2212 1(M jl = i)](y j i,l \u2212 x j i,l) 2. (4)\nSo far, we have built a simple baseline to adopt mixing for MAE, which, however, performs even worse than MAE, as demonstrated in Tab. 3f. In the following, we provide a theoretical explanation to prove that this na\u0131\u0308ve incorporation will actually ease the reconstruction pretext task.\nMutual information analysis. Without loss of generality, we take r = 0.5 as an example. Denote X1,X2 as two random variables representing two input images, and X1 is further considered as the reconstruction target (symmetric for X2). Then, we can prove that the mutual information (MI) between the mixed input \u03c3mix({X1,X2},M) and the target X1 is no smaller than that between the MAE input \u03c3MAE(X1,M) and X1 as (see proofs in Appendix A),\nI(\u03c3mix({X1,X2},M);X1) \u2265 I(\u03c3MAE(X1,M);X1). (5) Therefore, different from masking, which is introduced to decrease the mutual information between the model input\n4\nand the reconstruction target due to the redundancy of image signals [27], na\u0131\u0308ve mixing will instead increase the MI, and thus, ease the reconstruction pretext task. Verification experiment is conducted in Appendix C.\nAlso note that the MI increasement brought by mixing is target-invariant, suggesting that Eq. (5) also holds when the target is semantic labels for supervised learning or positive samples for contrastive learning, for which MI increasement is appealing. This might explain why na\u0131\u0308ve mixing without auxiliary supervision is beneficial for supervised [59, 60] and contrastive learning [48, 58], but not for MAE."
        },
        {
            "heading": "3.2. Recognition: Homologous Recognition",
            "text": "Another indispensable factor to achieve MI increasement is the usage of global self-attention in ViT, with which each query patch will inevitably attend to heterologous patches from other images. Due to the uncertainty of generative modeling, heterologous patches might provide a shortcut to complete reconstruction (e.g., the green color of cucumbers is a \u201cvaluable\u201d cue to reconstruct the forest behind the fox in Fig. 3a). To address, we propose a novel auxiliary pretext task called homologous recognition to enforce each query to explicitly recognize and only attend to homologous patches.\nHomologous attention recognizes homologous patches on-the-fly by enforcing each query patch to only attend to key patches with the highest attention mass using a TopK(\u00b7) sampling operation. Specifically, the homologous attention can be formulated as,\nAHomoAtt = softmax(TopK(qk T / \u221a Dh)), (6)\nwhere q is the query patch, k are the key patches and Dh is the feature dimension. By default, all the self-attention operations in ViT are replaced with homologous attention except the very first layer. See comparisons in Tab. 3e.\nHomologous contrastive aims at verifying the TopK(\u00b7) sampling accuracy by encouraging the encoder features of homologous patches to be similar, while heterologous ones to be dissimilar in the supervised contrastive manner [32]. The homologous contrastive loss can be formulated as,\nLHomoCon = \u2212 L\u2211\nl=1 \u2211 l+ log exp(cos(z\u0302jl , z\u0302 j l+)/\u03c4)\u2211L l\u2032 \u0338=l exp(cos(z\u0302 j l , z\u0302 j l\u2032)/\u03c4) ,\n(7) where \u03c4 is the temperature and cos(\u00b7, \u00b7) is the cosine similarity. As demonstrated in Fig. 6, the TopK(\u00b7) sampling accuracy is significantly improved and stabilized with the usage of the homologous contrastive loss LHomoCon.\nSegment embedding. Beside the positional embeddings, we add another segment embedding to the mixed sequence x\u0302j following BERT [15] to provide necessary information to complete homologous recognition, due to the uncertainty of generative modeling. The segment embedding is shared for patches from the same image, while different for patches from different images, as demonstrated in Fig. 3b.\nMixing mode. For a fair comparison under different training overheads, two mixing modes are adopted for MixedAE, 1) Compose: each group generates a single mixed sample following Eq. (1), and the effective encoder batch size is Br; 2) Full: each group generates 1/r mixed samples by sampling M j for 1/r times independently, and the effective encoder batch size is B. An example is provided in Fig. 4 when r = 0.5. As shown in Tab. 1, MixedAE achieves the SoTA performance under different training overheads. If no otherwise specified, compose mixing is adopted by default."
        },
        {
            "heading": "3.3. Reconstruction: Loss Function",
            "text": "Loss function. We formulate MixedAE in the multi-task learning manner and the final loss function is a weighted sum of the reconstruction loss Lrecon and the homologous contrastive loss LHomoCon as,\nLMixedAE = Lrecon + \u03bbLHomoCon, (8)\nwhere the balanced weight \u03bb is set to be 0.1 by default."
        },
        {
            "heading": "3.4. Discussion: Object-aware Pre-training",
            "text": "With the usage of mixing, we observe that MixedAE can achieve object-aware self-supervised pre-training without any specifically designed components, such as K-means [8], selective search [54] and object discovery network [29], because homologous recognition requires each query patch to recognize all homologous patches within a mixed image. Due to the single-centric-object guarantee [8] of ImageNet, that most images are pre-processed to guarantee only one object in the center part of them, the mixed image can be considered as a \u201cpseudo\u201d multi-instance image, and given a query patch, the process of recognizing all patches from the same image within a mixed sample is exactly recognizing all patches from the same object within a given \u201cpseudo\u201d multi-instance image. Therefore, the awareness of object existence and completeness is enhanced in the leant representation of our MixedAE.\nIn Fig. 5, we visualize the attention maps of MAE and MixedAE by averaging all attention heads of the last layer, taken the [CLS] token as query and patch tokens as keys. Compared with MAE which mainly focuses on the most discriminative patches (e.g., boundaries and corners), our MixedAE can successfully discover the foreground object patches more precisely and completely, which might also\nexplain why MixedAE improves more significantly when transferred to dense perception tasks, such as semantic segmentation [62] and object detection [37], as shown in Tab. 1."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Implementation Details",
            "text": "Architecture. We mainly use the standard ViT-Base [18] as the backbone architecture, and further provide ViT-Large experiments in Appendix C. The input images are resized to 224 \u00d7 224, resulting in a total sequence length L = 196 with the patch size being 16\u00d716. Following MAE [27], the decoder consists of 8 Transformer layers with the hidden dimension as 512 by default. For a fair comparison with BEiT [3], we additionally build a MixedAE in full mixing mode with a lightweight decoder made up of 2 Transformer layers and the hidden dimension as 256, as shown in Tab. 1.\nThe mixing ratio r is set to be 0.25 (i.e., corresponding to the 0.75 masking ratio in MAE [27]) by default, and the threshold K in the TopK(\u00b7) operation is therefore set to be 0.25L. Following common practices [7, 11], we adopt a linear projector with the output dimension as 128 right before the homologous contrastive loss, and the temperature coefficient \u03c4 is set to be 0.25.\nOptimization. We pre-train MixedAE on the ImageNet1K [14] training set with the AdamW [42] optimizer and a cosine learning rate schedule with a linear warm-up of 40 epochs. The batch size is set to be 4096 for the compose mixing, while 1024 for the full mode. The base learning rate is set to be 7.5e\u22125, which will scale linearly with the batch size (lr = lrbase\u00d7bs/256). Only standard random cropping and flipping are utilized for data augmentation. The remaining hyperparameters are all maintained the same with MAE for a fair comparison (see Appendix B for more details)."
        },
        {
            "heading": "4.2. Transfer Results on ImageNet-1K",
            "text": "Setup. We consider the fully fine-tuning performance on ImageNet-1K for 100 epochs and report the Top-1 accuracy. Following MAE [27], we average all the patch tokens after the final Transformer layer, which is taken as input to a linear head for classification. See more details in Appendix B.\nComparison with MAE. As shown in Tab. 1, MixedAE obtains consistent improvements over MAE under different pre-training epochs with only 3% additional overhead. The 300-epoch pre-trained MixedAE with full mixing acquires even better accuracy than the 1600-epoch pre-trained MAE, demonstrating the efficiency of data utilization.\nComparison with other MIM augmentations. Our MixedAE with the lightweight decoder and the full mixing mode obtains 83.7% Top-1 accuracy, 0.5% and 0.4% higher than MixMIM [38] and CIM [22] respectively, meanwhile saving at least 23.4% computational overhead, revealing the simplicity of our MixedAE.\nComparison with other SSL approaches. Our MixedAE obtains consistent improvements under various pre-training epochs and overheads, and achieves a better trade-off between pre-training overhead and transfer performance, as shown in Fig. 1. Specifically, MixedAE pre-trained for 1600 epochs achieves 83.9% accuracy, constructing a new stateof-the-art result with a pure autoencoder-based framework. Requiring only 53.4% of the training overhead, MixedAE\nsurpasses the strong iBOT [63], demonstrating remarkable efficiency. Moreover, the improvement brought by mixing is orthogonal to the usage of more advanced reconstruction targets [2, 17, 53] and masking strategies [31, 49]."
        },
        {
            "heading": "4.3. Transfer Results on Downstream Tasks",
            "text": "We further consider three downstream settings to evaluate the learnt representation, and more details about the different transfer procedures are included in Appendix B.\nSemantic segmentation. We utilize the UperNet [56] to perform semantic segmentation on ADE20K [62] following BEiT [3]. As reported in Tab. 1, our 800-epoch MixedAE achieves 48.7 mIoU, even surpassing the MAE pre-trained for 1600 epochs by 0.6 mIoU, and our 1600-epoch MixedAE further improves to 49.8 mIoU, outperforming all baseline methods by a non-trivial margin, which is more significant than the improvement on ImageNet-1K (1.7 vs. 0.3), thanks to the object-aware pre-training, as discussed in Sec. 3.4.\nObject detection and instance segmentation. We utilize Cascade Mask R-CNN [5, 28] to produce bounding boxes and instance masks simultaneously on COCO [37]. As demonstrated in Tab. 1, MixedAE consistently outperforms MAE under different epochs (0.6/0.9/0.9 & 0.5/0.6/0.7). Similarly with ADE20K, more significant improvements are observed due to the high-quality attention maps learnt by the object-aware pre-training, as demonstrated in Fig. 5.\nDownstream classification. Following [20, 39, 61], we study transfer performance on 11 downstream classification datasets, including both fine-grained (e.g., Cars [33]) and coarse-grained ones (e.g., CIFAR100 [34]) in Tab. 2. Our MixedAE achieves consistent improvement over MAE on all 11 downstream tasks with an average accuracy of 86.9%, outperforming all counterparts as demonstrated in Tab. 2."
        },
        {
            "heading": "4.4. Ablation Study",
            "text": "Setup. We conduct 300-epoch pre-training with a base learning rate of 1.5e\u22124 for all ablation studies on MixedAE with compose mixing. By default, we report the fine-tuning accuracy on ImageNet-1K [14] and mIoU on ADE20K [62]. See more detailed settings and results in Appendix C.\nMixing ratio r. Different from MixMIM [38], the mixing ratio r in our formulation can be flexibly selected from the range of (0, 0.5]. As shown in Tab. 3a, r = 0.25 works better, while requiring less pre-training overhead (since the effective encoder batch size scales linearly with r, as shown in Eq. (1)). Note that r = 0.25 is also corresponding to the default 0.75 masking ratio in MAE.\nPosition of homologous contrastive. We study whether the encoder features before or after the final Layer Normalization ([LN]) [1] of ViT [18] achieves better performance as input to the homologous contrastive loss in Tab. 3b. The latter achieves consistent improvements on both ImageNet1K and ADE20K, suggesting that the features after [LN] are more suitable for homologous recognition.\nPositives of homologous contrastive. In Eq. (7), given a query patch, all homologous patches are considered as positive samples but taken separately to calculate LHomoCon in the supervised contrastive manner [32]. We further ablate to utilize the average of both the query and all its homologous patches as its positive, which, however, performs slightly worse than the separate manner (-0.2 mIoU on ADE20K).\nThreshold K of homologous attention. We study the threshold number K in AHomoAtt in Tab. 3d, where all the global self-attention operations in the ViT are replaced with our homologous attention. Compared with the no sampling baseline, the usage of homologous attention obtains consistent improvement on ImageNet-1K, while the best achieves at K = 0.25L, which is consistent with the mixing ratio r in Tab. 3a specifically.\nPosition of homologous attention. As shown in Fig. 6a, homologous attention cannot achieve promising accuracy in early Transformer layers without sufficient information engagement. Thus, we further explore to maintain the global self-attention in early layers unchanged in Tab. 3e, and observe empirically that utilizing global self-attention in the very first layer only achieves the best performance.\nHomologous recognition. In Tab. 3f, we further compare the effectiveness of different components of homologous recognition. Without the homologous contrastive loss for verification, utilizing homologous attention only obtains a significant drop of 0.5 mIoU on ADE20K. Although achieving improvement, utilizing the homologous contrastive only still suffers from the ease of reconstruction brought by MI increasement, as previously discussed in Eq. (5). Finally, the best performance is achieved when using homologous attention and contrastive loss simultaneously."
        },
        {
            "heading": "4.5. Analysis",
            "text": "Effectiveness of TopK sampling. To further observe the effectiveness of TopK sampling in homologous attention, we visualize the sampling accuracy with respect to different layers and pre-training epochs in Fig. 6. As demonstrated in Fig. 6a, the na\u0131\u0308ve usage of homologous attention only cannot achieve promising sampling accuracy, which, therefore, suffers from a significant performance drop in Tab. 3f. Specifically, neither the sampling accuracy of the first two\nnor final two layers exceed 60%, and accuracy of the very final layer even maintains under 40% throughout the whole pre-training process, as shown in Fig. 6b (the green curve).\nHowever, with the usage of the homologous contrastive loss for verification, the sampling accuracy is significantly improved and stabilized around 70% to 80% for all layers except the first two, as in Fig. 6a. The sampling accuracy of the final layer rapidly increases to around 70% when pretrained only for 20 epochs, maintaining stable throughout the remaining pre-training, as in Fig. 6b (the orange curve).\nComparison with existing MIM methods combined with contrastive. Although also utilizing a \u201ccontrastive loss\u201d with reconstruction, MixedAE differs from existing MIM works [19,63] combined with contrastive learning from two perspectives, 1) Purpose: existing works utilize contrastive loss to perform instance discrimination simultaneously with MIM, while our homologous contrastive is only utilized to guarantee the sampling accuracy. Therefore, homologous contrastive performs more like a regularization term instead of an individual self-supervision in [19, 63]. To verify, we pre-train a MixedAE with LHomoCon only without Lrecon, which cannot achieve reasonable performance, as reported\nin Appendix C. 2) Efficiency: given a single input, existing works require forward propagation at least twice for several augmented views to conduct instance discrimination, while only once for our homologous contrastive, resulting in the significant efficiency. Specifically, our MixedAE surpasses iBOT [63] with only 53.4% of its computational overhead."
        },
        {
            "heading": "5. Conclusion",
            "text": "This paper explores the usage of image mixing for MAE. Different from in supervised and contrastive learning, we first theoretically demonstrate na\u0131\u0308ve mixing might instead ease the reconstruction pretext task. To address that, our MixedAE with the proposed homologous recognition as the auxiliary supervision can not only achieve state-of-the-art performance with a better trade-off between transfer results and pre-training overhead, but also conduct object-aware pre-training without any specifically designed modules. We hope our simple yet effective method can bring researchers\u2019 attention to more effective data augmentations for MIM.\nAcknowledgments. We gratefully acknowledge support of MindSpore, CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research."
        },
        {
            "heading": "A. Proof of Eq. (5)",
            "text": "In this section, we prove Eq. (5) with the terminologies maintained consistent with Sec. 3. We start to prove when r = 0.5 (i.e., two clean images within a single group). Denote X1,X2 as two random variables representing two input images, and M as the random mask, which can be considered as a constant here since it is independently generated with X1 and X2. Then, according to Eq. (1), the mixed input can be represented as,\n\u03c3mix({X1,X2},M) = 1(M = 1)X1 + 1(M = 2)X2, (9)\nwhile the MAE input can be represented as,\n\u03c3MAE(X1,M) = {X1,l|Ml = 1} = 1(M = 1)X1 + 1(M = 2) \u2212\u2192 0 . (10)\nTherefore, given X1 as the reconstruction target, we can represent the mutual information (MI) between the mixed input and the reconstruction target X1 as,\nI(\u03c3mix({X1,X2},M);X1) = I(1(M = 1)X1 + 1(M = 2)X2;X1) = H(X1)\u2212H(X1|1(M = 1)X1 + 1(M = 2)X2)\n= H(X1)\u2212H(X1|1(M = 1)X1 + 1(M = 2) \u2212\u2192 0 + 1(M = 1) \u2212\u2192 0 + 1(M = 2)X2) = H(X1)\u2212H(X1|1(M = 1)X1 + 1(M = 2) \u2212\u2192 0 ,1(M = 1) \u2212\u2192 0 + 1(M = 2)X2) \u2265 H(X1)\u2212H(X1|1(M = 1)X1 + 1(M = 2) \u2212\u2192 0 )\n= I(\u03c3MAE(X1,M);X1), (11)\nwhere H(\u00b7) is the entropy. The conclusion above also holds when X2 is considered as the reconstruction target symmetrically. Note that although independent in the data space, X1 and X2 are not independent in the feature space, because the global self-attention would introduce inevitable information leakage from X2 to X1, which would be enhanced for \u201crelevant\u201d X2 (e.g., the green cucumber in Fig. 3a), while restrained for \u201cirrelevant\u201d ones (e.g., the blue sky) by assigning different attention weights. As shown in Fig. 6, the TopK(\u00b7) sampling accuracy converges to around 80%, suggesting that the information leakage does exist in practice. Therefore, the independence is affected, and the equality in Eq. (11) does not hold.\nFor r \u2208 (0, 0.5), there are more than two images within a single group {X1,X2, ...,X1/r}. Considering X1 as the reconstruction target, we can first mix all images except X1 to generate a pseudo X\u03022 as,\nX\u03022 = \u03c3mix({ \u2212\u2192 0 ,X2,X3, ...,X1/r},M) = 1(M = 1) \u2212\u2192 0 + 1/r\u2211 i=2 1(M = i)Xi, (12)\nwhich is then mixed with X1 following Eq. (9). Therefore, the conclusion in Eq. (11) can still be satisfied. As discussed above, the usage of the global self-attention in ViT [18] is another indispensable factor to achieve the MI increasement in Eq. (11). Therefore, in this paper, we propose the homologous attention to replace the global self-attention together with the homologous contrastive loss as verification for our MixedAE."
        },
        {
            "heading": "B. More Implementation Details",
            "text": "Pre-training. The default settings are provided in Tab. 4. We use xavier uniform [24] to initialize all Transformer layers, following the official code of ViT [18]. Normalized pixels [27] are utilized as the reconstruction target, and the mask patch size is set to be 32 \u00d7 32, following [38, 57]. In practice, we utilize the sine-cosine encodings [52] for segment embeddings, which is added to the input of each Transformer layer following [40]. If no otherwise specified, the compose mixing mode is by default adopted to generate a single mixed sample for each image group.\nImageNet classification. The default settings are provided in Tab. 5. We mainly adopt the fully fine-tuning transfer setting to fine-tune the parameters of the backbone and the classification head simultaneously. We utilize the layer-wise learning rate decay strategy [13] following [3]. In practice, we sweep the decay ratio in {0.65, 0.7, 0.75} following [38, 63].\nADE20K semantic segmentation. We use UperNet [56] as the segmentor following BEiT [3]. The input resolution is 512 \u00d7 512, and the batch size is set to be 16. The learning rate is set to be 3e\u22124 with the layer-wise learning rate decay ratio as 0.65 for ViT-Base and 0.75 for ViT-Large. We conduct fine-tuning for 160K iterations, and evaluate the performance without the multi-scale augmentation.\nCOCO object detection and instance segmentation. We utilize the Cascade Mask R-CNN [5, 28] following iBOT [63]. Multi-scale training is adopted with the shorted side randomly resized between 480 and 800 while the longer side no larger than 1333. The batch size is 16, the initial learning rate is 1e\u22124, and the layer-wise learning rate decay ratio is set to be 0.75. We adopt the standard 1\u00d7 schedule to train for 12 epochs, and decrease the learning rate by 10\u00d7 at epoch 9 and 11.\nDownstream classification. We mainly follow the setups in [39,61] to evaluate the transfer performance on 11 downstream classification datasets, including both the fine-grained datasets (e.g., Aircraft [43], Cars [33], Flowers [44], Food [4], Pets [45] and SUN397 [55]), and the coarse-grained ones (e.g., Caltech101 [35], CIFAR10 [34], CIFAR100 [34], DTD [12] and VOC2007 [21]). Specifically, we adopt a linear classification head upon the pre-trained ViT backbone and fully fine-tune the whole model simultaneously for 5000 iterations. The SGD optimizer and the cosine learning rate schedule are adopted. We grid search the optimal learning rate among {1e\u22123, 3e\u22123, 1e\u22122, 3e\u22122}, and set the weight decay to be 0."
        },
        {
            "heading": "C. More Experiments",
            "text": "Scaling property of MixedAE. We further build MixedAE with the standard ViT-Large [18] as the backbone architecture, and pre-train for 1600 epochs on ImageNet-1K [14] following the same optimization recipe with ViT-Base as in Appendix B. As demonstrated in Tab. 6, MixedAE still outperforms MAE consistently with ViT-Large, especially on downstream dense perception tasks [26, 36, 37, 62] thanks to the object-aware pre-training, revealing the scalability of our proposed MixedAE.\nMoreover, we further report the performance of the 1500-epoch pre-trained MixedAE with ViT-Large to maintain similar computational overhead with MAE in Tab. 6. MixedAE still obtains consistent improvements over MAE, while outperforming iBOT with a 1.8\u00d7 acceleration, suggesting that MixedAE can achieve a better trade-off between the computational overhead and the transfer performance regardless of the architecture size.\nAblation settings in Tab. 3. We conduct the ablation of new components based on previous results. Starting from the na\u0131\u0308ve baseline in Sec. 3.1, we first ablate the mixing ratio r in Tab. 3a. Accordingly, we fix r = 0.25 to explore the position and positives of the homologous contrastive in Tabs. 3b and 3c. Similarly, we ablate homologous attention in Tabs. 3d and 3e based on results of Tab. 3c, and finally summarize all components in Tab. 3f.\nMain cause of performance degradation of na\u0131\u0308ve mixing. To verify that the information leakage brought by the mutual information increasement, as proved in Appendix A, is indeed the main cause of performance degradation of the na\u0131\u0308ve mixing\nMasked SA acc mIoU MAE 82.7 46.1 Na\u0131\u0308ve 82.4 45.0\nMixing \u2713 82.6 45.9\n(a) Main cause of performance degradation is indeed the information leakage brought by na\u0131\u0308ve mixing without homologous recognition.\nLrecon LHomoCon acc mIoU \u2713 82.4 45.0\n\u2713 7.8 8.3 \u2713 \u2713 82.7 46.4\n(b) Functionality of the LHomoCon. When adopting LHomoCon alone, MixedAE cannot even achieve reasonable transfer performance.\nMixing SE acc mIoU \u2713 82.2 44.9\n\u2713 81.1 42.9 \u2713 \u2713 82.7 46.4\n(c) Necessity of the mixing and segment embeddings. The best transfer performance is achieved when both are adopted.\nTable 7. More MixedAE ablation experiments with ViT-B/16. Default settings are marked in gray .\nbaseline introduced in Sec. 3.1 instead of the optimization difficulty, we further build a mixing baseline by: 1) applying the masked self-attention to perform self-attention only within homologous patches to prevent information leakage without homologous recognition (i.e., neither homologous TopK(\u00b7) attention nor homologous contrastive loss); 2) adopting exactly the same optimization recipe with MAE. As demonstrated in Tab. 7a, the model achieves 82.6% accuracy and 45.9 mIoU, comparably with MAE, suggesting that the information leakage is definitely the culprit here.\nFunctionality of the LHomoCon. To verify that the homologous contrastive loss performs more like a regularization term instead of an individual self-supervision as in [19, 63], we further pre-train a MixedAE with LHomoCon only following the settings in Sec. 4.4. As demonstrated in Tab. 7b, MixedAE performs well when the reconstruction loss Lrecon is utilized only or together with the homologous contrastive loss LHomoCon. However, when adopting LHomoCon only, MixedAE cannot even achieve reasonable transfer performance, suggesting that LHomoCon cannot work alone without Lrecon.\nNecessity of mixing. To verify to necessity of adopting mixing augmentation in our MixedAE, we extend MAE with the homologous contrastive LHomoCon by applying Eq. (7) to patches across different images in groups of 4 for a fair comparison with MixedAE, which achieves 81.1% accuracy and 42.9 mIoU as demonstrated in Tab. 7c (2nd & 3rd rows), significantly worse than our default MixedAE, revealing the necessity of using mixing augmentation.\nNecessity of segment embeddings. As shown in Tab. 7c (1st & 3rd rows), we build a MixedAE without segment embeddings and achieve 82.2% accuracy and 44.9 mIoU, significantly worse than our default MixedAE, suggesting the importance of adopting segment embeddings to provide necessary information for homologous recognition."
        },
        {
            "heading": "D. More Analysis",
            "text": "Exploration for other augmentations. As discussed in Sec. 3.1, based on mixing, we observe that common augmentation strategies will increase mutual information (MI) between the model input and the reconstruction target, suggesting that data augmentations like random augmentation and color jittering might not be suitable or require specific designs for MIM, which will be explored in the future work.\nLimitations. Although demonstrating significant performance, we notice that the TopK(\u00b7) sampling accuracy in homologous attention still cannot achieve 100% as shown in Fig. 6, for which there exist several potential reasons accounting: 1) The background patches might be included during random cropping inevitably, which are difficult for attention-based methods to recognize. 2) There is still further improvement space for MixedAE. For example, more strict verification than homologous contrastive loss is an appealing future work direction."
        },
        {
            "heading": "E. More Visualizations",
            "text": "We provide more visualizations of the attention mapes learnt by MAE and MixedAE on images from ImageNet-1K [14], ADE20K [62] and Microsoft COCO [37] datasets in Fig. 7. As demonstrated, our MixedAE can generate more reasonable and discriminative attention maps, revealing the effectiveness of MixedAE to conduct object-aware pre-training without any specifically designed modules (e.g., K-means [8] and object discovery network [29])."
        }
    ],
    "title": "Mixed Autoencoder for Self-supervised Visual Representation Learning",
    "year": 2023
}