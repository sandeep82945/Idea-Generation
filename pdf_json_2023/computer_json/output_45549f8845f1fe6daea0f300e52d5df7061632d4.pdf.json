{
    "abstractText": "DDIM inversion has revealed the remarkable potential of real image editing within diffusion-based methods. However, the accuracy of DDIM reconstruction degrades as larger classifier-free guidance (CFG) scales being used for enhanced editing. Null-text inversion (NTI) optimizes null embeddings to align the reconstruction and inversion trajectories with larger CFG scales, enabling real image editing with cross-attention control. Negative-prompt inversion (NPI) further offers a training-free closed-form solution of NTI. However, it may introduce artifacts and is still constrained by DDIM reconstruction quality. To overcome these limitations, we propose proximal guidance and incorporate it to NPI with cross-attention control. We enhance NPI with a regularization term and inversion guidance, which reduces artifacts while capitalizing on its trainingfree nature. Additionally, we extend the concepts to incorporate mutual self-attention control, enabling geometry and layout alterations in the editing process. Our method provides an efficient and straightforward approach, effectively addressing real image editing tasks with minimal computational overhead1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ligong Han"
        },
        {
            "affiliations": [],
            "name": "Song Wen"
        },
        {
            "affiliations": [],
            "name": "Qi Chen"
        },
        {
            "affiliations": [],
            "name": "Zhixing Zhang"
        },
        {
            "affiliations": [],
            "name": "Kunpeng Song"
        },
        {
            "affiliations": [],
            "name": "Mengwei Ren"
        },
        {
            "affiliations": [],
            "name": "Ruijiang Gao"
        },
        {
            "affiliations": [],
            "name": "Anastasis Stathopoulos"
        },
        {
            "affiliations": [],
            "name": "Xiaoxiao He"
        },
        {
            "affiliations": [],
            "name": "Yuxiao Chen"
        },
        {
            "affiliations": [],
            "name": "Di Liu"
        },
        {
            "affiliations": [],
            "name": "Qilong Zhangli"
        },
        {
            "affiliations": [],
            "name": "Jindong Jiang"
        },
        {
            "affiliations": [],
            "name": "Zhaoyang Xia"
        },
        {
            "affiliations": [],
            "name": "Akash Srivastava"
        },
        {
            "affiliations": [],
            "name": "Dimitris Metaxas"
        }
    ],
    "id": "SP:0e76f3668c4fb948d538521169ebcd70c0a15991",
    "references": [
        {
            "authors": [
                "Rameen Abdal",
                "Peihao Zhu",
                "John Femiani",
                "Niloy Mitra",
                "Peter Wonka"
            ],
            "title": "Clip2stylegan: Unsupervised extraction of stylegan edit directions",
            "venue": "In ACM SIGGRAPH 2022 Conference Proceedings,",
            "year": 2022
        },
        {
            "authors": [
                "Rameen Abdal",
                "Peihao Zhu",
                "Niloy J Mitra",
                "Peter Wonka"
            ],
            "title": "Styleflow: Attribute-conditioned exploration of stylegangenerated images using conditional continuous normalizing flows",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2021
        },
        {
            "authors": [
                "Omri Avrahami",
                "Kfir Aberman",
                "Ohad Fried",
                "Daniel Cohen- Or",
                "Dani Lischinski"
            ],
            "title": "Break-a-scene: Extracting multiple concepts from a single image",
            "venue": "arXiv preprint arXiv:2305.16311,",
            "year": 2023
        },
        {
            "authors": [
                "Stephen Boyd",
                "Neal Parikh",
                "Eric Chu",
                "Borja Peleato",
                "Jonathan Eckstein"
            ],
            "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
            "venue": "Foundations and Trends\u00ae in Machine learning,",
            "year": 2011
        },
        {
            "authors": [
                "Manuel Brack",
                "Felix Friedrich",
                "Dominik Hintersdorf",
                "Lukas Struppek",
                "Patrick Schramowski",
                "Kristian Kersting"
            ],
            "title": "Sega: Instructing diffusion using semantic dimensions",
            "venue": "arXiv preprint arXiv:2301.12247,",
            "year": 2023
        },
        {
            "authors": [
                "Mingdeng Cao",
                "Xintao Wang",
                "Zhongang Qi",
                "Ying Shan",
                "Xiaohu Qie",
                "Yinqiang Zheng"
            ],
            "title": "Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing",
            "venue": "arXiv preprint arXiv:2304.08465,",
            "year": 2023
        },
        {
            "authors": [
                "Huiwen Chang",
                "Han Zhang",
                "Jarred Barber",
                "AJ Maschinot",
                "Jose Lezama",
                "Lu Jiang",
                "Ming-Hsuan Yang",
                "Kevin Murphy",
                "William T Freeman",
                "Michael Rubinstein"
            ],
            "title": "Muse: Text-to-image generation via masked generative transformers",
            "venue": "arXiv preprint arXiv:2301.00704,",
            "year": 2023
        },
        {
            "authors": [
                "Hila Chefer",
                "Oran Lang",
                "Mor Geva",
                "Volodymyr Polosukhin",
                "Assaf Shocher",
                "Michal Irani",
                "Inbar Mosseri",
                "Lior Wolf"
            ],
            "title": "The hidden language of diffusion models",
            "venue": "arXiv preprint arXiv:2306.00966,",
            "year": 2023
        },
        {
            "authors": [
                "Hong Chen",
                "Yipeng Zhang",
                "Xin Wang",
                "Xuguang Duan",
                "Yuwei Zhou",
                "Wenwu Zhu"
            ],
            "title": "Disenbooth: Identitypreserving disentangled tuning for subject-driven text-toimage generation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yuxiao Chen",
                "Jianbo Yuan",
                "Yu Tian",
                "Shijie Geng",
                "Xinyu Li",
                "Ding Zhou",
                "Dimitris N. Metaxas",
                "Hongxia Yang"
            ],
            "title": "Revisiting multimodal representation in contrastive learning: from patch and token embeddings to finite discrete tokens",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Giannis Daras",
                "Alexandros G Dimakis"
            ],
            "title": "Multiresolution textual inversion",
            "venue": "arXiv preprint arXiv:2211.17115,",
            "year": 2022
        },
        {
            "authors": [
                "Raman Dutt",
                "Linus Ericsson",
                "Pedro Sanchez",
                "Sotirios A Tsaftaris",
                "Timothy Hospedales"
            ],
            "title": "Parameter-efficient finetuning for medical image analysis: The missed opportunity",
            "venue": "arXiv preprint arXiv:2305.08252,",
            "year": 2023
        },
        {
            "authors": [
                "Jeffrey A. Fessler"
            ],
            "title": "Lecture notes on proximal methods",
            "venue": "https://web.eecs.umich.edu/ \u0303fessler/ course/598/l/n-05-prox.pdf,",
            "year": 2021
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen- Or"
            ],
            "title": "An image is worth one word: Personalizing text-toimage generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "Shijie Geng",
                "Jianbo Yuan",
                "Yu Tian",
                "Yuxiao Chen",
                "Yongfeng Zhang"
            ],
            "title": "Hiclip: Contrastive language-image pretraining with hierarchy-aware attention",
            "venue": "arXiv preprint arXiv:2303.02995,",
            "year": 2023
        },
        {
            "authors": [
                "Shuyang Gu",
                "Dong Chen",
                "Jianmin Bao",
                "Fang Wen",
                "Bo Zhang",
                "Dongdong Chen",
                "Lu Yuan",
                "Baining Guo"
            ],
            "title": "Vector quantized diffusion model for text-to-image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ligong Han",
                "Ruijiang Gao",
                "Mun Kim",
                "Xin Tao",
                "Bo Liu",
                "Dimitris Metaxas"
            ],
            "title": "Robust conditional gan from uncertaintyaware pairwise comparisons",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ligong Han",
                "Yinxiao Li",
                "Han Zhang",
                "Peyman Milanfar",
                "Dimitris Metaxas",
                "Feng Yang"
            ],
            "title": "Svdiff: Compact parameter space for diffusion fine-tuning",
            "venue": "arXiv preprint arXiv:2303.11305,",
            "year": 2023
        },
        {
            "authors": [
                "Ligong Han",
                "Martin Renqiang Min",
                "Anastasis Stathopoulos",
                "Yu Tian",
                "Ruijiang Gao",
                "Asim Kadav",
                "Dimitris N Metaxas"
            ],
            "title": "Dual projection generative adversarial networks for conditional image generation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ligong Han",
                "Sri Harsha Musunuri",
                "Martin Renqiang Min",
                "Ruijiang Gao",
                "Yu Tian",
                "Dimitris Metaxas"
            ],
            "title": "Ae-stylegan: Improved training of style-based auto-encoders",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Ligong Han",
                "Jian Ren",
                "Hsin-Ying Lee",
                "Francesco Barbieri",
                "Kyle Olszewski",
                "Shervin Minaee",
                "Dimitris Metaxas",
                "Sergey Tulyakov"
            ],
            "title": "Show me what and tell me how: Video synthesis via multimodal conditioning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ligong Han",
                "Anastasis Stathopoulos",
                "Tao Xue",
                "Dimitris Metaxas"
            ],
            "title": "Unbiased auxiliary classifier gans with mine",
            "venue": "arXiv preprint arXiv:2006.07567,",
            "year": 2020
        },
        {
            "authors": [
                "Erik H\u00e4rk\u00f6nen",
                "Aaron Hertzmann",
                "Jaakko Lehtinen",
                "Sylvain Paris"
            ],
            "title": "Ganspace: Discovering interpretable gan controls",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Amir Hertz",
                "Kfir Aberman",
                "Daniel Cohen-Or"
            ],
            "title": "Delta denoising score",
            "venue": "arXiv preprint arXiv:2304.07090,",
            "year": 2023
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Prompt-to-prompt image editing with cross attention control",
            "venue": "arXiv preprint arXiv:2208.01626,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen- Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Yihao Huang",
                "Qing Guo",
                "Felix Juefei-Xu"
            ],
            "title": "Zero-day backdoor attack against text-to-image diffusion models via personalization",
            "venue": "arXiv preprint arXiv:2305.10701,",
            "year": 2023
        },
        {
            "authors": [
                "Ruixiang Jiang",
                "Can Wang",
                "Jingbo Zhang",
                "Menglei Chai",
                "Mingming He",
                "Dongdong Chen",
                "Jing Liao"
            ],
            "title": "Avatarcraft: Transforming text into neural human avatars with parameterized shape and pose control",
            "venue": "arXiv preprint arXiv:2303.17606,",
            "year": 2023
        },
        {
            "authors": [
                "Nupur Kumari",
                "Bingliang Zhang",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu"
            ],
            "title": "Multi-concept customization of text-to-image diffusion",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Seung Hyun Lee",
                "Sieun Kim",
                "Innfarn Yoo",
                "Feng Yang",
                "Donghyeon Cho",
                "Youngseo Kim",
                "Huiwen Chang",
                "Jinkyu Kim",
                "Sangpil Kim"
            ],
            "title": "Soundini: Sound-guided diffusion for natural video editing",
            "venue": "arXiv preprint arXiv:2304.06818,",
            "year": 2023
        },
        {
            "authors": [
                "Pengzhi Li",
                "QInxuan Huang",
                "Yikang Ding",
                "Zhiheng Li"
            ],
            "title": "Layerdiffusion: Layered controlled image editing with diffusion models",
            "venue": "arXiv preprint arXiv:2305.18676,",
            "year": 2023
        },
        {
            "authors": [
                "Senmao Li",
                "Joost van de Weijer",
                "Taihang Hu",
                "Fahad Shahbaz Khan",
                "Qibin Hou",
                "Yaxing Wang",
                "Jian Yang"
            ],
            "title": "Stylediffusion: Prompt-embedding inversion for text-based editing",
            "venue": "arXiv preprint arXiv:2303.15649,",
            "year": 2023
        },
        {
            "authors": [
                "Yuheng Li",
                "Haotian Liu",
                "Qingyang Wu",
                "Fangzhou Mu",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Chunyuan Li",
                "Yong Jae Lee"
            ],
            "title": "Gligen: Open-set grounded text-to-image generation",
            "venue": "arXiv preprint arXiv:2301.07093,",
            "year": 2023
        },
        {
            "authors": [
                "Yanyu Li",
                "Huan Wang",
                "Qing Jin",
                "Ju Hu",
                "Pavlo Chemerys",
                "Yun Fu",
                "Yanzhi Wang",
                "Sergey Tulyakov",
                "Jian Ren"
            ],
            "title": "Snapfusion: Text-to-image diffusion model on mobile devices within two seconds",
            "venue": "arXiv preprint arXiv:2306.00980,",
            "year": 2023
        },
        {
            "authors": [
                "Daiki Miyake",
                "Akihiro Iohara",
                "Yu Saito",
                "Toshiyuki Tanaka"
            ],
            "title": "Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2305.16807,",
            "year": 2023
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Null-text inversion for editing real images using guided diffusion models",
            "venue": "arXiv preprint arXiv:2211.09794,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Mou",
                "Xintao Wang",
                "Liangbin Xie",
                "Jian Zhang",
                "Zhongang Qi",
                "Ying Shan",
                "Xiaohu Qie"
            ],
            "title": "T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.08453,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Byong Mok Oh",
                "Max Chen",
                "Julie Dorsey",
                "Fr\u00e9do Durand"
            ],
            "title": "Image-based modeling and photo editing",
            "venue": "In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,",
            "year": 2001
        },
        {
            "authors": [
                "Hadas Orgad",
                "Bahjat Kawar",
                "Yonatan Belinkov"
            ],
            "title": "Editing implicit assumptions in text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2303.08084,",
            "year": 2023
        },
        {
            "authors": [
                "Or Patashnik",
                "Zongze Wu",
                "Eli Shechtman",
                "Daniel Cohen-Or",
                "Dani Lischinski"
            ],
            "title": "Styleclip: Text-driven manipulation of stylegan imagery",
            "venue": "arXiv preprint arXiv:2103.17249,",
            "year": 2021
        },
        {
            "authors": [
                "Chenyang Qi",
                "Xiaodong Cun",
                "Yong Zhang",
                "Chenyang Lei",
                "Xintao Wang",
                "Ying Shan",
                "Qifeng Chen"
            ],
            "title": "Fatezero: Fusing attentions for zero-shot text-based video editing",
            "venue": "arXiv preprint arXiv:2303.09535,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "arXiv preprint arXiv:2102.12092,",
            "year": 2021
        },
        {
            "authors": [
                "Mengwei Ren",
                "Mauricio Delbracio",
                "Hossein Talebi",
                "Guido Gerig",
                "Peyman Milanfar"
            ],
            "title": "Image deblurring with domain generalizable diffusion models",
            "venue": "arXiv preprint arXiv:2212.01789,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "arXiv preprint arXiv:2208.12242,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Jing Shi",
                "Wei Xiong",
                "Zhe Lin",
                "Hyun Joon Jung"
            ],
            "title": "Instantbooth: Personalized text-to-image generation without testtime finetuning",
            "venue": "arXiv preprint arXiv:2304.03411,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Nataniel Ruiz",
                "Kimin Lee",
                "Daniel Castro Chin",
                "Irina Blok",
                "Huiwen Chang",
                "Jarred Barber",
                "Lu Jiang",
                "Glenn Entis",
                "Yuanzhen Li"
            ],
            "title": "Styledrop: Text-to-image generation in any style",
            "venue": "arXiv preprint arXiv:2306.00983,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Ming Tao",
                "Hao Tang",
                "Songsong Wu",
                "Nicu Sebe",
                "Xiao-Yuan Jing",
                "Fei Wu",
                "Bingkun Bao"
            ],
            "title": "Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis",
            "venue": "arXiv preprint arXiv:2008.05865,",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Tibshirani"
            ],
            "title": "Lecture notes on proximal gradient descent. https://www.stat.cmu.edu/ \u0303ryantibs/ convexopt/lectures/prox-grad.pdf, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Narek Tumanyan",
                "Michal Geyer",
                "Shai Bagon",
                "Tali Dekel"
            ],
            "title": "Plug-and-play diffusion features for text-driven image-to-image translation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Andrey Voynov",
                "Qinghao Chu",
                "Daniel Cohen-Or",
                "Kfir Aberman"
            ],
            "title": "p+: Extended textual conditioning in text-toimage generation",
            "venue": "arXiv preprint arXiv:2303.09522,",
            "year": 2023
        },
        {
            "authors": [
                "Ruichen Wang",
                "Zekang Chen",
                "Chen Chen",
                "Jian Ma",
                "Haonan Lu",
                "Xiaodong Lin"
            ],
            "title": "Compositional text-to-image synthesis with attention map control of diffusion models",
            "venue": "arXiv preprint arXiv:2305.13921,",
            "year": 2023
        },
        {
            "authors": [
                "Zhendong Wang",
                "Yifan Jiang",
                "Huangjie Zheng",
                "Peihao Wang",
                "Pengcheng He",
                "Zhangyang Wang",
                "Weizhu Chen",
                "Mingyuan Zhou"
            ],
            "title": "Patch diffusion: Faster and more data-efficient training of diffusion models",
            "venue": "arXiv preprint arXiv:2304.12526,",
            "year": 2023
        },
        {
            "authors": [
                "Rundi Wu",
                "Ruoshi Liu",
                "Carl Vondrick",
                "Changxi Zheng"
            ],
            "title": "Sin3dm: Learning a diffusion model from a single 3d textured shape",
            "venue": "arXiv preprint arXiv:2305.15399,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Xu",
                "Pengchuan Zhang",
                "Qiuyuan Huang",
                "Han Zhang",
                "Zhe Gan",
                "Xiaolei Huang",
                "Xiaodong He"
            ],
            "title": "Attngan: Finegrained text to image generation with attentional generative adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Hui Ye",
                "Xiulong Yang",
                "Martin Takac",
                "Rajshekhar Sunderraman",
                "Shihao Ji"
            ],
            "title": "Improving text-to-image synthesis using contrastive learning",
            "venue": "arXiv preprint arXiv:2107.02423,",
            "year": 2021
        },
        {
            "authors": [
                "Jiahui Yu",
                "Yuanzhong Xu",
                "Jing Yu Koh",
                "Thang Luong",
                "Gunjan Baid",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Alexander Ku",
                "Yinfei Yang",
                "Burcu Karagol Ayan"
            ],
            "title": "Scaling autoregressive models for content-rich text-to-image generation",
            "venue": "arXiv preprint arXiv:2206.10789,",
            "year": 2022
        },
        {
            "authors": [
                "Fangneng Zhan",
                "Yingchen Yu",
                "Rongliang Wu",
                "Jiahui Zhang",
                "Shijian Lu",
                "Lingjie Liu",
                "Adam Kortylewski",
                "Christian Theobalt",
                "Eric Xing"
            ],
            "title": "Multimodal image synthesis and editing: A survey",
            "venue": "arXiv preprint arXiv:2112.13592,",
            "year": 2021
        },
        {
            "authors": [
                "Han Zhang",
                "Jing Yu Koh",
                "Jason Baldridge",
                "Honglak Lee",
                "Yinfei Yang"
            ],
            "title": "Cross-modal contrastive learning for text-toimage generation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaxin Zhang",
                "Kamalika Das",
                "Sricharan Kumar"
            ],
            "title": "On the robustness of diffusion inversion in image manipulation",
            "venue": "In ICLR 2023 Workshop on Trustworthy and Reliable Large- Scale Machine Learning Models,",
            "year": 2023
        },
        {
            "authors": [
                "Yuxin Zhang",
                "Weiming Dong",
                "Fan Tang",
                "Nisha Huang",
                "Haibin Huang",
                "Chongyang Ma",
                "Tong-Yee Lee",
                "Oliver Deussen",
                "Changsheng Xu"
            ],
            "title": "Prospect: Expanded conditioning for the personalization of attribute-aware image generation",
            "venue": "arXiv preprint arXiv:2305.16225,",
            "year": 2023
        },
        {
            "authors": [
                "Yuechen Zhang",
                "Jinbo Xing",
                "Eric Lo",
                "Jiaya Jia"
            ],
            "title": "Realworld image variation by aligning diffusion inversion chain",
            "venue": "arXiv preprint arXiv:2305.18729,",
            "year": 2023
        },
        {
            "authors": [
                "Zhongping Zhang",
                "Jian Zheng",
                "Jacob Zhiyuan Fang",
                "Bryan A Plummer"
            ],
            "title": "Text-to-image editing by image information removal",
            "venue": "arXiv preprint arXiv:2305.17489,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Diffusion-based methods have emerged as popular approaches for real image editing, with many of these methods utilizing DDIM inversion (a deterministic inversion method proposed in Denoising Diffusion Implicit Models [55]). DDIM inversion is known to yield accurate reconstructions when using null embeddings or source prompts with a classifier-free guidance [27] (CFG) scale of 1. However, in order to achieve better editing capabilities, it is often necessary to use a CFG scale significantly larger than 1. Unfortunately, this scaling can lead to inaccurate reconstructions of the source image, which hinders the editing quality. This phenomenon is also observed in prompt-\n1Code: https://github.com/phymhan/prompt-to-prompt.\nto-prompt [25] editing scenarios. To address this limitation, Null-text inversion [38] (NTI) was introduced as a solution. NTI employs pivotal inversion by optimizing the null embedding(s), ensuring that the reconstruction trajectory aligns with the inversion trajectory even under a larger CFG scale. While NTI has a lightweight parameter set, it requires per-image optimization, which can be time-consuming. To eliminate the need for optimization in\nar X\niv :2\n30 6.\n05 41\n4v 3\n[ cs\n.C V\n] 6\nJ ul\n2 02\n3\nNTI, Negative-prompt inversion [37] (NPI) offers a closedform solution. By assuming equal predicted noises between consecutive timesteps of the diffusion model, NPI elegantly demonstrated that the solver of NTI is equivalent to the source prompt embedding. However, NPI may occasionally introduce artifacts due to its underlying assumptions.\nBuilding upon the remarkable results of NPI, we enhance it by incorporating a regularization term to improve the reconstruction of the source image. Moreover, we recognize that NPI is still constrained by the reconstruction quality of DDIM inversion, unable to correct errors introduced during the reconstruction process. To overcome this, we introduce a inversion guidance technique that performs one-step gradient descent on the current latent, aligning it with the inversion latents. The resulting algorithm offers a straightforward approach with negligible computational overhead.\nFurthermore, as NTI and NPI are primarily designed for Cross-Attention Control [25], which focuses on texture and appearance changes, we extend our method to integrate proximal guidance into the Mutual Self-Attention Control framework [6]. This integration allows for geometry and layout alterations in real image editing tasks. In summary, our proposed method combines the benefits of NPI, inversion guidance, and a regularization term to provide an effective and efficient optimization-free solution for real image editing. We demonstrate its applications in NPI with Cross-Attention Control and Mutual Self-Attention Control, showcasing its versatility and potential impact."
        },
        {
            "heading": "2. Related Work",
            "text": "Image generation with text guidance has been wellexplored in image synthesis field [1, 2, 10, 15, 17\u201323, 42, 44, 47, 48, 59, 66\u201368, 70, 75]. Recent development of textto-image (T2I) diffusion models [7, 16, 26, 41, 53, 55\u201358] introduced new solutions to this task. In particular, T2I diffusion models trained with large-scale image-caption pairs\nhave shown impressive generation ability [40, 46, 49, 51]. The development of large-scale T2I models provides a giant and flexible design space for image manipulation methods leveraging the pre-trained model. Recent works propose novel controlling mechanisms tailored for these T2I models [3, 8, 12, 29, 30, 32, 33, 36, 63\u201365, 69, 71, 74].\nDiffusion-based image editing. Many recent works finetune the pre-trained T2I models with a few personalized images to keep the context information [9,31,34,43,52,54,72]. Wide design choices have been explored in this direction. Textual-Inversion [11, 14, 62]-based methods propose finetuning the text embedding. Dreambooth [50] fine-tunes the whole model. [31] fine-tunes the cross-attention layers in the UNet of Stable-Diffusion model. These methods require hundreds of iterations at the fine-tuning stage to capture the identity information. For better efficiency, more techniques [18, 28, 35, 39] are developed by reducing the number of parameters optimized at fine-tuning stage. While fine-tuning the pre-trained T2I model shows extraordinary results, the test-time efficiency of these methods remains a great challenge. SEGA [5] discovers that target concept can be encoded using latent dimensions falling into the upper and lower tail of the distribution.\nInversion-based image editing. DDIM inversion [55] is widely adopted in editing tasks by deterministically encoding the original image into a latent noise that can be accurately reconstructed. Leveraging pivotal inversion on null-text embeddings, Null-text Inversion [38] improved the identity preservation of the edit. However, all these methods rely on optimization at test-time for accurate reconstruction. Negative-prompt inversion (NPI) [37] further avoided the computation cost for the optimization while achieves competitive results as null-text inversion. One can also interpret NPI as performing Delta Denoising Score (DDS) [24] on the same noisy image.\nAlgorithm 1 Proximal Negative-Prompt Inversion Input: Given source original sample z0, source condition"
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Background",
            "text": "DDIM inversion. DDIM is a widely used deterministic sampling (if chosen to be) of DDPM. While DDPM follows a stochastic differential equation (SDE) process, DDIM corresponds to its ordinary differential equation (ODE) counterpart. The reverse DDIM process can be written as\nzt\u22121 = \u221a \u03b1t\u22121\u221a \u03b1t zt+ (1)\n\u221a \u03b1t\u22121\n(\u221a 1\n\u03b1t\u22121 \u2212 1\u2212\n\u221a 1\n\u03b1t \u2212 1\n) \u03f5\u03b8(zt, t, C),\nwhere C is the given conditioning. To invert the given image, the latent variables can be estimated by reversing the above discrete ODE sampling process. By rearranging\nEq. (1), we have\nzt = \u221a \u03b1t\u221a\n\u03b1t\u22121 zt\u22121+ (2)\n\u221a \u03b1t\n(\u221a 1\n\u03b1t \u2212 1\u2212\n\u221a 1\n\u03b1t\u22121 \u2212 1\n) \u03f5\u03b8(zt, t, C).\nNote that zt appears at both sides. A common technique is to approximate \u03f5\u03b8(zt, t, C) with \u03f5\u03b8(zt\u22121, t \u2212 1, C), such that the inversion process can be solved by Euler method. Then, denote the sequence of latent variables from z0 via DDIM inversion as {z\u2217t }Tt=1, we have\nz\u2217t = \u221a \u03b1t\u221a\n\u03b1t\u22121 z\u2217t\u22121+ (3)\n\u221a \u03b1t\n(\u221a 1\n\u03b1t \u2212 1\u2212\n\u221a 1\n\u03b1t\u22121 \u2212 1\n) \u03f5\u03b8(z \u2217 t\u22121, t\u2212 1, C).\nNull-text inversion. Using the classifier-free guidance (CFG [27]), the noise is estimated by\n\u03f5\u0303\u03b8(zt, t, C, \u2205) = w\u03f5\u03b8(zt, t, C) + (1\u2212 w)\u03f5\u03b8(zt, t, \u2205) (4)\nIf w > 1, the accumulated error on DDIM inversion will affect reconstruction accuracy. To address the problem, nulltext inversion [38] (NTI) optimizes a set of per-timestep null-text embeddings {\u2205t} to track the DDIM inversion trajectory even under a large w. It first computes {z\u2217t }Tt=1 using DDIM inversion with w = 1. Then, after initializing z\u0304T = z \u2217 T , null-text inversion solves \u2205t by performing the following optimizations for t = T, . . . , 1:\nmin \u2205t ||zt\u22121(z\u0304t, \u2205t, C)\u2212 z\u2217t\u22121||22. (5)\nNegative-prompt inversion (NPI [37]) overcomes the limitation of per-image optimization in null-text inversion by providing a closed-form solution, \u2205t = C, with minimal approximation. NPI validates this solution through induction: if \u2205t = C and z\u0304t = z\u2217t hold for timestep t, they also hold for timestep t \u2212 1, by assuming \u03f5\u03b8(z\u2217t , t, C) \u2248 \u03f5\u03b8(z \u2217 t\u22121, t \u2212 1, C). We can verify that with \u2205t = C, NPI reconstruction with w > 1 recovers the DDIM reconstruction,\n\u03f5\u0303\u03b8(zt, t, C, C) = w\u03f5\u03b8(zt, t, C) + (1\u2212 w)\u03f5\u03b8(zt, t, C) = \u03f5\u03b8(zt, t, C). (6)\nIn fact, as demonstrated in Fig. 2 and the subsequent remark, NPI provides an exact solution without any approximation when tracking the DDIM reconstruction trajectory instead of the inversion trajectory:\nRemark 3.1. Negative-prompt inversion is the exact closed-form solution if we solve null-text inversion optimizations to track the DDIM reconstruction trajectory {z\u0302t}."
        },
        {
            "heading": "3.2. Proximal Negative-Prompt Inversion",
            "text": "Negative-prompt inversion provides an elegant closedform solution for computing null-text inverted nullembeddings, \u2205t = C. This solution intuitively aligns with the DDIM reconstruction process. Fig. 3 illustrates a single inference step using classifier-free guidance (CFG) with a scale parameter w = 2. Initially, all methods employ DDIM inversion [55] with the source prompt (and w = 1). In Fig. 3(a), we depict a baseline approach where direct sampling is performed using the target prompt. Fig. 3(b)\ndemonstrates the inference step of negative-prompt inversion, where CFG amplifies the editing direction of (\u03f5\u0303tar \u2212 \u03f5\u0303src). Intuitively, when the target prompt is close to the source prompt, the inference trajectory for editing should closely resemble the DDIM reconstruction trajectory. In fact, when the target condition C \u2032 = C, negative-prompt inversion exactly recovers DDIM reconstruction. However, we observe that negative-prompt inversion occasionally over-amplifies the editing direction (\u03f5\u0303tar \u2212 \u03f5\u0303src). To address this, we propose the addition of an extra loss term\nthat encourages the CFG noise \u03f5\u0303 to align with \u03f5\u0303src.\nTo accomplish this, we draw inspiration from the proximal gradient method [13, 60] and introduce a regularization term to constrain (\u03f5\u0303tar \u2212 \u03f5\u0303src). This regularization is achieved through the use of a proximal function,\nprox\u03bb,Lp(x) = argmin z\n1 2 \u2225z \u2212 x\u222522 + \u03bb\u2225z\u2225p. (7)\nwhich encourages desired properties in the editing process. When p = 1 (corresponding to L1 regularization), the\nsolver takes the form of a soft-thresholding function,\n[prox\u03bb,L1(x)]i = [S\u03bb(x)]i =  xi \u2212 \u03bb if xi > \u03bb0 if \u2212\u03bb \u2264 xi \u2264 \u03bb xi + \u03bb if xi < \u2212\u03bb\n(8)\nwith [ \u00b7 ]i denoting the i-th component. if p = 0 (representing L0 regularization), the solver takes the form of a hard-thresholding function,\n[prox\u03bb,L0(x)]i = { xi if |xi| > \u221a 2\u03bb\n0 otherwise. (9)\nSince the value range of (\u03f5\u0303tar\u2212 \u03f5\u0303src) does not follow a standard Gaussian distribution, we employ a dynamic threshold\nrather than a fixed one by selecting a quantile of the absolute values |\u03f5\u0303tar \u2212 \u03f5\u0303src|. It is worth noting that when using hard thresholding with quantile thresholds, this approach is similar to SEGA [5]. Fig. 3(c) provides a visualization of a 2-D case when soft-thresholding is employed. If all values are clamped to zero, our method reduces to DDIM reconstruction. Conversely, when all values are retained after thresholding, our method simplifies to negative-prompt inversion.\nInversion guidance. As discussed previously, NPI is still upper-bounded by the quality of DDIM reconstruction. Even if \u03f5\u0303 converges to \u03f5src, it cannot correct errors in cases where DDIM reconstruction is imperfect. On the other hand, NTI tracks the DDIM inversion trajectory and thus does not have such limitation. This motivates us to introduce a inversion guidance by performing a single step of gradient descent on the current latent z\u0303t\u22121, aiming to align it with the inversion latent z\u2217t\u22121. This gradient descent step is applied only to the \u201cunedited\u201d region identified by the mask M = |\u03f5\u0303tar \u2212 \u03f5\u0303src| \u2264 \u03bb, where we\nreuse the notation \u03bb to represent the threshold value. By choosing a step size \u03b7, the update can be expressed as z\u0303t\u22121 \u2190 z\u0303t\u22121 \u2212 \u03b7M \u2299 (z\u0303t\u22121 \u2212 z\u2217t\u22121), where \u03b7 = 1 corresponds to a complete replacement. The complete algorithm is outlined in Algorithm 1. The algorithm can be thought of as an ADMM [4] type of method that solves NTI on the DDIM inversion trajectory:\nmin \u2205t ||zt\u22121(z\u0304t, \u2205t, C)\u2212 z\u0302t\u22121||22 s.t. z\u0302t\u22121 = z\u2217t\u22121 (10)\nwhere the objective is solved by NPI (see Remark 3.1) and the constraint is enforced by inversion guidance."
        },
        {
            "heading": "3.3. Proximal Mutual Self-Attention Control",
            "text": "Both NTI and NPI are designed to be used with CrossAttention Control (or Prompt-to-Prompt [25]) for real image editing. While Cross-Attention Control primarily focuses on changing the texture or appearance of a subject, recent methods have explored self-attention controlling mechanisms for manipulating geometry or layout [6, 45, 61, 73]. MasaCtrl [6] proposes a Mutual Self-Attention Control\nmechanism that queries image content from the source input image. In this section, we aim to integrate proximal guidance into the MasaCtrl framework.\nAlthough NTI/NPI and MasaCtrl operate through different mechanisms, they share the same goal of preserving specific content from the source image. Initially, we observe that directly incorporating NPI with MasaCtrl by substituting the null embedding with the source prompt embedding can lead to artifacts. This occurs because, without any cross-attention or self-attention control, this is equivalent to setting the source prompt as negative prompt. As illustrated in an example in Fig. 4, using the source prompt as the negative prompt (\u201cNPI w/o MasaCtrl\u201d) generates a jumping person unrelated to the source image. When combined with MasaCtrl (\u201cNPI w/ MasaCtrl\u201d), the model is compelled to query cat features to render the same jumping person. Therefore, we propose using NPI solely in the\nreconstruction branch while retaining the null embedding in the synthesis branch:\n\u03f5\u0302 = \u03f5\u0302src + 1 \u00b7 (\u03f5\u0302src \u2212 \u03f5\u0302src), [reconstruction] \u03f5\u0303 = \u03f5\u0303null + w \u00b7 prox\u03bb(\u03f5\u0303tar \u2212 \u03f5\u0303null). [synthesis]\n(11)\nHere, we introduce proximal guidance to the term (\u03f5\u0303tar \u2212 \u03f5\u0303null). During model forward passes, the MasaCtrl mechanism forces both \u03f5\u0303null and \u03f5\u0303tar to query features from \u03f5\u0302src (in the original MasaCtrl, the unconditional part \u03f5\u0303null queries from \u03f5\u0302null in the reconstruction branch). By setting \u03bb to the 100% quantile, \u03f5\u0303 converges to \u03f5\u0303null \u2248 \u03f5\u0302src, degrading to a DDIM reconstruction. Hence, similar to ProxNPI, the introduced proximal guidance here also controls the proximity of the synthesized image to the source image."
        },
        {
            "heading": "4. Experiment",
            "text": ""
        },
        {
            "heading": "4.1. Cross-Attention Control",
            "text": "In this section, we present qualitative comparisons among Null-text inversion (NTI) [38], Negative-prompt inversion (NPI) [37], and our proposed method (ProxNPI), as illustrated in Fig. 5. Each row in the figure showcases the reconstruction results (columns 2-4) and editing results (columns 5-7) for each method. It is worth noting that NPI reconstruction is equivalent to DDIM reconstruction [55], as discussed previously. For examples (c-e) in Fig. 5, we utilize inversion guidance since DDIM reconstruction still introduces minor errors. These errors or artifacts are highlighted using red circles or boxes.\nIn Fig. 5, we observe that NPI fails to retain the shower head in the mirror (a), while both NTI and NPI alter the\nleaves on the tree to flowers (b). In (c), both NTI and NPI exhibit imperfect reconstruction of the input image, whereas the missing detail is recovered through the inversion guidance incorporated in our method. Additionally, in (d), NPI introduces reconstruction errors, while our method demonstrates superior preservation of the background in edited images compared to NTI and NPI. Lastly, in (e), both NTI and NPI display reconstruction errors on the chair. Additional visual results. We present more visual editing results in Fig. 6, along with the corresponding prompts provided underneath the images. Among the eight examples,\ninversion guidance is employed for examples (c), (e), (f), and (g) due to imperfections in DDIM reconstructions for these specific cases."
        },
        {
            "heading": "4.2. Mutual Self-Attention Control",
            "text": "We conducted qualitative comparisons between MasaCtrl [6] and our proposed ProxMasaCtrl, as shown in Fig. 7. Our ProxMasaCtrl incorporates proximal guidance to address the issues of instability and undesired changes occasionally observed with MasaCtrl. As shown, MasaCtrl can introduce slight color shifts in the main subject(s) and background, as demonstrated in examples (a), (b), (c), and (f). However, with proximal guidance, we achieve better preservation of the background and intended details. For instance, in example (a), the two steel bowls at the bottom and the person holding a phone at the right edge are preserved. Similarly, in example (b), the fence on the upper right and the dog\u2019s mouth are retained with improved fidelity. Additionally, in example (c), the reins on the horse\u2019s head are better maintained, and in example (d), the cup and vase in the background are better preserved. Comparing with Prompt-to-Prompt. As mentioned earlier, Prompt-to-Prompt [25] is designed to alter the texture of a subject, whereas Mutual Self-Attention Control [6] targets geometry and layout modifications. Fig. 8 shows a visual comparison between these two attention controlling mechanisms for geometry editing. This comparison aims to illustrate their respective behaviors rather than establish the superiority of one over the other. Notably, in Fig. 8(c),\nPrompt-to-Prompt introduces a new texture on the cake, resembling its cross-section that deviates from the source image, while MasaCtrl preserves the original appearance. Additionally, Prompt-to-Prompt confines the rendered cake to a square shape within the cross-attention map of the original round cake, whereas MasaCtrl allows rendering outside the boundaries of the original cake. Note that the base of the cake is also changed to square. Simultaneous texture and geometry editing. We extend our approach by sequentially applying ProxNPI and ProxMasaCtrl to enable simultaneous editing of both texture and geometry, as shown in Fig. 9. While this represents a preliminary exploration, we leave a more effective integration of these two controlling mechanisms for future research."
        },
        {
            "heading": "4.3. Ablations",
            "text": "Thresholding. In Fig. 10 we present visual results obtained by setting the threshold \u03bb to the 60%, 70%, ..., 95% quantiles of the absolute values of the noise difference. The first row illustrates the case of hard-thresholding (labeled as L0), while the second and third rows display the case of softthresholding (labeled as L1). As anticipated, we observe that soft-thresholding tends to introduce fewer changes to the edited images compared to hard-thresholding. Alternatively, we can increase the CFG scale, such as using w = 15, to enhance the prominence of the target attribute. However, this approach may lead to an amplified contrast ratio and shifted color tone. We empirically find using hardthresholding with quantile 0.7 usually gives good results. Inversion guidance. In Fig. 11 we present visual results obtained by varying the stepsize \u03b7 for the inversion guidance, applied when t < Tinv . As observed, when the guidance strength \u03b7 and Tinv are small, the reconstruction of chop-\nsticks is incomplete, and the pattern on the bowl is missing. We empirically find that setting Tinv \u2265 600 and \u03b7 \u2265 0.2 generally yields satisfactory results.\nMasaCtrl ablation. In Fig. 12, we conduct ablations of different mutual self-attention feature injection strategies in MasaCtrl. The synthesis branch utilizes the \u201cnull embedding\u201d denoted as C = interp(\u03b1,Csrc, Cnull), where the default setting uses the original null embedding with \u03b1 = 1. We explore the visual effects by varying \u03b1 and querying different feature sets, including \u201csource\u201d (the default strategy), \u201cjoint\u201d (querying from both branches), and \u201cnone\u201d (no feature injection). While \u03b1 = 1 with \u201csource\u201d generally produces good results for both unconditional and conditional noises, we observe that using \u201cjoint\u201d or \u201cnone\u201d for the unconditional noise occasionally improves the outcomes."
        },
        {
            "heading": "5. Discussion and Conclusion",
            "text": "In this paper, we introduced proximal guidance, a versatile technique for enhancing diffusion-based tuning-free real image editing. We applied this technique to two concurrent frameworks: Negative-prompt inversion (NPI) and Mutual Self-Attention Control. The resulting algorithms, ProxNPI and ProxMasaCtrl, addressed limitations and achieved high-quality editing while maintaining computational efficiency. However, there are still considerations, as the performance of proximal guidance can be sensitive to hyperparameters. Exploring heuristics or automated methods for parameter selection could enhance the usability and generalizability of the proposed method. Our work demonstrates the potential of proximal guidance and opens avenues for further research in tuning-free real image editing."
        },
        {
            "heading": "A. Proof of Remark 3.1",
            "text": "Remark A.1. Negative-prompt inversion is the exact closed-form solution if we solve null-text inversion optimizations to track the DDIM reconstruction trajectory {z\u0302t}, with z\u0304T initialized as z\u0302T = z\u2217T ,\nC = argmin \u2205t\n||zt\u22121(z\u0304t, \u2205t, C)\u2212 z\u0302t\u22121||22. (12)\nProof. Following negative-prompt inversion [37], we prove this by induction. Suppose at timestep t, \u2205t = C and z\u0304t = z\u0302t hold, then we derive z\u0304t\u22121 for timestep t\u2212 1. By definition (Eq. (1) with classifier-free guidance),\nz\u0304t\u22121 = zt\u22121(z\u0304t, t, C, \u2205t) = \u221a \u03b1t\u22121\u221a \u03b1t z\u0304t + \u221a \u03b1t\u22121\n(\u221a 1\n\u03b1t\u22121 \u2212 1\u2212\n\u221a 1\n\u03b1t \u2212 1\n) \u03f5\u0303\u03b8(z\u0304t, t, C, \u2205t). (13)\nSince z\u0304t = z\u0302t and by Eq. (2),\nz\u0304t = z\u0302t = \u221a \u03b1t\u221a\n\u03b1t\u22121 z\u0302t\u22121 +\n\u221a \u03b1t\n(\u221a 1\n\u03b1t \u2212 1\u2212\n\u221a 1\n\u03b1t\u22121 \u2212 1\n) \u03f5\u03b8(z\u0302t, t, C). (14)\nSubstituting the above into Eq. (13), we have\nz\u0304t\u22121 = z\u0302t\u22121 + \u221a \u03b1t\u22121\n(\u221a 1\n\u03b1t\u22121 \u2212 1\u2212\n\u221a 1\n\u03b1t \u2212 1\n) (\u03f5\u0303\u03b8(z\u0302t, t, C, \u2205t)\u2212 \u03f5\u03b8(z\u0302t, t, C)) . (15)\nSince \u03f5\u0303\u03b8(z\u0302t, t, C, \u2205t)\u2212 \u03f5\u03b8(z\u0302t, t, C) = (w \u2212 1) (\u03f5\u03b8(z\u0302t, t, C)\u2212 \u03f5\u03b8(z\u0302t, t, \u2205t)), we have z\u0304t\u22121 = z\u0302t\u22121 if \u2205t = C."
        },
        {
            "heading": "B. Reconstruction Guidance",
            "text": "Algorithm 2 Proximal Negative-Prompt Inversion with reconstruction guidance Input: Given source original sample z0, source condition C, target condition C \u2032, denoising model \u03f5\u03b8, proximal function\nprox\u03bb(\u00b7). 1: z\u0304T = DDIMInvert(z0, C, w = 1) 2: z\u0303T = z\u0304T 3: for t = T to 1 do 4: \u03f5\u0303src = \u03f5\u03b8(z\u0303t, t, C) 5: \u03f5\u0303tar = \u03f5\u03b8(z\u0303t, t, C\n\u2032) 6: \u03f5\u0303 = \u03f5\u0303src + w \u00b7 prox\u03bb(\u03f5\u0303tar \u2212 \u03f5\u0303src) 7: M = |\u03f5\u0303tar \u2212 \u03f5\u0303src| \u2264 \u03bb 8: z\u03030 =\n1\u221a \u03b1t z\u0303t \u2212 \u221a 1 \u03b1t \u2212 1\u03f5\u0303\n9: if reconstruction guidance and t < Trec then 10: z\u03030 = z\u03030 \u2212 \u03b7M \u2299 (z\u03030 \u2212 z0) 11: end if 12: z\u0303t\u22121 = \u221a \u03b1t\u22121z\u03030 + \u221a 1\u2212 \u03b1t\u22121\u03f5\u0303 13: if inversion guidance and t < Tinv then 14: z\u0303t\u22121 = z\u0303t\u22121 \u2212 \u03b7M \u2299 (z\u0303t\u22121 \u2212 z\u2217t\u22121) 15: end if 16: end for 17: return z\u03030\nWe have introduced the concept of \u201creconstruction guidance\u201d as an additional solution to address the issue of imperfect DDIM reconstruction. Another straight-forward solution is reconstruction guidance. To do so, we perform one step of gradient descent on the current predicted original sample z\u03030 to align it with the source sample z0. Similarly, this gradient descent step is applied to the \u201cunedited\u201d region identified by the mask M = |\u03f5\u0303tar \u2212 \u03f5\u0303src| \u2264 \u03bb. The update can be expressed\nas z\u03030 \u2190 z\u03030 \u2212 \u03b7M \u2299 (z\u03030 \u2212 z0). The algorithm with reconstruction guidance is outlined in Algorithm 2. In Fig. 13, we present visual results obtained by varying the stepsize \u03b7. The guidance is applied when t < Trec. As observed, when the guidance strength is small (with a small \u03b7), the reconstruction of chopsticks is incomplete. Increasing Trec results in accurate reconstruction of the chopsticks, however, a large \u03b7 may introduce artifacts such as an over-amplified contrast ratio. Based on empirical findings, we generally set Trec = 400 and \u03b7 = 0.1, although inversion guidance is still preferred over reconstruction guidance."
        }
    ],
    "title": "Improving Tuning-Free Real Image Editing with Proximal Guidance",
    "year": 2023
}