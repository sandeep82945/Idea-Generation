{
    "abstractText": "To keep pace with the explosive growth of Artificial Intelligence (AI) and Machine Learning (ML)-dominated applications, distributed intelligence solutions are gaining momentum, which exploit cloud facilities, edge nodes and end-devices to increase the overall computational power, meet application requirements, and optimize performance. Despite the benefits in terms of data privacy and efficient usage of resources, distributing intelligence throughout the cloud-to-things continuum raises unprecedented challenges to the network design. DistributedAI/ML components need high-bandwidth, low-latency connectivity to execute learning and inference tasks, while ensuring high-accuracy and energy-efficiency. This paper aims to explore the new challenging distributed intelligence scenario by extensively and critically scanning the main research achievements in the literature. In addition, starting from them, the main building blocks of a network ecosystem that can enable distributed intelligence are identified and the authors\u2019 views are dissected to provide guidelines for the design of a \u2018\u2018future network for distributed Intelligence\u2019\u2019. INDEX TERMS Artificial intelligence, cloud continuum, distributed intelligence, machine learning, network.",
    "authors": [
        {
            "affiliations": [],
            "name": "CLAUDIA CAMPOLO"
        },
        {
            "affiliations": [],
            "name": "ANTONIO IERA"
        },
        {
            "affiliations": [],
            "name": "ANTONELLA MOLINARO"
        }
    ],
    "id": "SP:a3e94c20ba49afe7e3a541348c89b1fe683bac92",
    "references": [
        {
            "authors": [
                "J. Wan",
                "J. Yang",
                "Z. Wang",
                "Q. Hua"
            ],
            "title": "Artificial intelligence for cloudassisted smart factory",
            "venue": "IEEE Access, vol. 6, pp. 55419\u201355430, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Huynh-The",
                "Q.-V. Pham",
                "X.-Q. Pham",
                "T.T. Nguyen",
                "Z. Han",
                "D.-S. Kim"
            ],
            "title": "Artificial intelligence for the metaverse: A survey",
            "venue": "Eng. Appl. Artif. Intell., vol. 117, Jan. 2023, Art. no. 105581.",
            "year": 2023
        },
        {
            "authors": [
                "K. Muhammad",
                "A. Ullah",
                "J. Lloret",
                "J.D. Ser",
                "V.H.C. de Albuquerque"
            ],
            "title": "Deep learning for safe autonomous driving: Current challenges and future directions",
            "venue": "IEEE Trans. Intell. Transp. Syst., vol. 22, no. 7, pp. 4316\u20134336, Jul. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Rong",
                "A. Mendez",
                "E. Bou Assi",
                "B. Zhao",
                "M. Sawan"
            ],
            "title": "Artificial intelligence in healthcare: Review and prediction case studies,\u2019\u2019Engineering",
            "venue": "vol. 6,",
            "year": 2020
        },
        {
            "authors": [
                "J. Verbraeken"
            ],
            "title": "A survey on distributed machine learning",
            "venue": "ACM Comput. Surv., vol. 53, no. 2, pp. 1\u201333, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.S. Salem",
                "G. Castellano",
                "G. Neglia",
                "F. Pianese andA. Araldo"
            ],
            "title": "Towards inference delivery networks: Distributing machine learning with optimality guarantees",
            "venue": "Proc. 19th Medit. Commun. Comput. Netw. Conf. (MedComNet), Jun. 2021, pp. 1\u20138.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Cheng",
                "X. Fan",
                "M. Liwang",
                "M. Min",
                "X. Wang",
                "X. Du"
            ],
            "title": "Hybrid architectures for distributed machine learning in heterogeneous wireless networks",
            "venue": "2022, arXiv:2206.01906.",
            "year": 2022
        },
        {
            "authors": [
                "D. Rosendo",
                "A. Costan",
                "P. Valduriez",
                "G. Antoniu"
            ],
            "title": "Distributed intelligence on the edge-to-cloud continuum: A systematic literature review",
            "venue": "J. Parallel Distrib. Comput., vol. 166, pp. 71\u201394, Aug. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Matsubara",
                "M. Levorato",
                "F. Restuccia"
            ],
            "title": "Split computing and early exiting for deep learning applications: Survey and research challenges",
            "venue": "ACM Comput. Surveys, vol. 55, no. 5, pp. 1\u201330, May 2023.",
            "year": 2023
        },
        {
            "authors": [
                "S. Talwar",
                "N. Himayat",
                "H. Nikopour",
                "F. Xue",
                "G.Wu",
                "V. Ilderem"
            ],
            "title": "6G: Connectivity in the era of distributed intelligence",
            "venue": "IEEE Commun. Mag., vol. 59, no. 11, pp. 45\u201350, Nov. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F.R. Yu"
            ],
            "title": "From information networking to intelligence networking: Motivations, scenarios, and challenges",
            "venue": "IEEE Netw., vol. 35, no. 6, pp. 209\u2013216, Nov. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Pan",
                "L. Cai",
                "S. Yan",
                "X.S. Shen"
            ],
            "title": "Network for AI and AI for network: Challenges and opportunities for learning-oriented networks",
            "venue": "IEEE Netw., vol. 35, no. 6, pp. 270\u2013277, Nov. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Li",
                "R. Xie",
                "F.R. Yu",
                "T. Huang",
                "Y. Liu"
            ],
            "title": "Advancing softwaredefined service-centric networking toward in-network intelligence",
            "venue": "IEEE Netw., vol. 35, no. 5, pp. 210\u2013218, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K.B. Letaief",
                "Y. Shi",
                "J. Lu",
                "J. Lu"
            ],
            "title": "Edge artificial intelligence for 6G: Vision, enabling technologies, and applications",
            "venue": "IEEE J. Sel. Areas Commun., vol. 40, no. 1, pp. 5\u201336, Jan. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Chen",
                "D. G\u00fcnd\u00fcz",
                "K. Huang",
                "W. Saad",
                "M. Bennis",
                "A.V. Feljan",
                "H.V. Poor"
            ],
            "title": "Distributed learning in wireless networks: Recent progress and future challenges",
            "venue": "IEEE J. Sel. Areas Commun., vol. 39, no. 12, pp. 3579\u20133605, Dec. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Ouyang",
                "D. Dong",
                "Y. Xu",
                "L. Xiao"
            ],
            "title": "Communication optimization strategies for distributed deep neural network training: A survey",
            "venue": "J. Parallel Distrib. Comput., vol. 149, pp. 52\u201365, Mar. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "X. Cao",
                "T. Basar",
                "S. Diggavi",
                "Y.C. Eldar",
                "K.B. Letaief",
                "H.V. Poor",
                "J. Zhang"
            ],
            "title": "Communication-efficient distributed learning: An overview",
            "venue": "IEEE J. Sel. Areas Commun., vol. 41, no. 4, pp. 851\u2013873, Apr. 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Zhang",
                "C. Chang",
                "H. Lin",
                "Y. Wang",
                "R. Arora",
                "X. Jin"
            ],
            "title": "Is network the bottleneck of distributed training?\u2019",
            "venue": "in Proc. Workshop Netw. Meets AI ML,",
            "year": 2020
        },
        {
            "authors": [
                "M.C. Luizelli",
                "R. Canofre",
                "A.F. Lorenzon",
                "F.D. Rossi",
                "W. Cordeiro",
                "O.M. Caicedo"
            ],
            "title": "In-network neural networks: Challenges and opportunities for innovation",
            "venue": "IEEE Netw., vol. 35, no. 6, pp. 68\u201374, Nov. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Sun",
                "M. Peng",
                "Y. Zhou",
                "Y. Huang",
                "S.Mao"
            ],
            "title": "Application ofmachine learning in wireless networks: Key techniques and open issues",
            "venue": "IEEE Commun. Surveys Tuts., vol. 21, no. 4, pp. 3072\u20133108, 4th Quart., 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Hu",
                "X. Chen",
                "W. Ni",
                "E. Hossain",
                "X. Wang"
            ],
            "title": "Distributed machine learning for wireless communication networks: Techniques, architectures, and applications",
            "venue": "IEEE Commun. Surveys Tuts., vol. 23, no. 3, pp. 1458\u20131493, 3rd Quart., 2021. VOLUME 11, 2023 52857 C. Campolo et al.: Network for Distributed Intelligence: A Survey and Future Perspectives",
            "year": 2021
        },
        {
            "authors": [
                "H. Yang",
                "A. Alphones",
                "Z. Xiong",
                "D. Niyato",
                "J. Zhao",
                "K. Wu"
            ],
            "title": "Artificial-intelligence-enabled intelligent 6G networks",
            "venue": "IEEE Netw., vol. 34, no. 6, pp. 272\u2013280, Nov. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D.M. Gutierrez-Estevez",
                "M. Gramaglia",
                "A.D. Domenico",
                "G. Dandachi",
                "S. Khatibi",
                "D. Tsolkas",
                "I. Balan",
                "A. Garcia-Saavedra",
                "U. Elzur",
                "Y. Wang"
            ],
            "title": "Artificial intelligence for elastic management and orchestration of 5G networks",
            "venue": "IEEE Wireless Commun., vol. 26, no. 5, pp. 134\u2013141, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Zhang",
                "D. Zhu"
            ],
            "title": "Towards artificial intelligence enabled 6G: State of the art, challenges, and opportunities",
            "venue": "Comput. Netw., vol. 183, Dec. 2020, Art. no. 107556.",
            "year": 2020
        },
        {
            "authors": [
                "M. Giordani",
                "M. Polese",
                "M. Mezzavilla",
                "S. Rangan",
                "M. Zorzi"
            ],
            "title": "Toward 6G networks: Use cases and technologies",
            "venue": "IEEE Commun. Mag., vol. 58, no. 3, pp. 55\u201361, Mar. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Dean",
                "G. Corrado",
                "R.Monga",
                "K. Chen",
                "M. Devin",
                "M.Mao",
                "M. Ranzato",
                "A. Senior",
                "P. Tucker",
                "K. Yang",
                "Q. Le",
                "A. Ng"
            ],
            "title": "Large scale distributed deep networks",
            "venue": "Proc. Adv. Neural Inf. Process. Syst., vol. 25, 2012, pp. 1\u201312.",
            "year": 2012
        },
        {
            "authors": [
                "J. Chen",
                "X. Ran"
            ],
            "title": "Deep learning with edge computing: A review",
            "venue": "Proc. IEEE, vol. 107, no. 8, pp. 1655\u20131674, Aug. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "Y. Han",
                "V.C.M. Leung",
                "D. Niyato",
                "X. Yan",
                "X. Chen"
            ],
            "title": "Convergence of edge computing and deep learning: A comprehensive survey",
            "venue": "IEEE Commun. Surveys Tuts., vol. 22, no. 2, pp. 869\u2013904, 2nd Quart., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Chen",
                "K. Li",
                "Q. Deng",
                "K. Li",
                "P.S. Yu"
            ],
            "title": "Distributed deep learning model for intelligent video surveillance systems with edge computing",
            "venue": "IEEE Trans. Ind. Informat., early access, Apr. 4, 2019, doi: 10.1109/TII.2019.2909473.",
            "year": 2019
        },
        {
            "authors": [
                "O. Gupta",
                "R. Raskar"
            ],
            "title": "Distributed learning of deep neural network over multiple agents",
            "venue": "J. Netw. Comput. Appl., vol. 116, pp. 1\u20138, Aug. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "E. Li",
                "Z. Zhou",
                "X. Chen"
            ],
            "title": "Edge intelligence: On-demand deep learning model co-inference with device-edge synergy",
            "venue": "Proc. Workshop Mobile Edge Commun., Aug. 2018, pp. 31\u201336.",
            "year": 2018
        },
        {
            "authors": [
                "J. Shao",
                "J. Zhang"
            ],
            "title": "Communication-computation trade-off in resource-constrained edge inference",
            "venue": "IEEE Commun. Mag., vol. 58, no. 12, pp. 20\u201326, Dec. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Li",
                "C. Hu",
                "J. Jiang",
                "Z. Wang",
                "Y. Wen",
                "W. Zhu"
            ],
            "title": "JALAD: Joint accuracy-and latency-aware deep structure decoupling for edgecloud execution",
            "venue": "Proc. IEEE 24th Int. Conf. Parallel Distrib. Syst. (ICPADS), Dec. 2018, pp. 671\u2013678.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Zhou",
                "X. Chen",
                "E. Li",
                "L. Zeng",
                "K. Luo",
                "J. Zhang"
            ],
            "title": "Edge intelligence: Paving the last mile of artificial intelligence with edge computing",
            "venue": "Proc. IEEE, vol. 107, no. 8, pp. 1738\u20131762, Aug. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "B. McMahan",
                "E. Moore",
                "D. Ramage",
                "S. Hampson",
                "B.A.Y. Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "Proc. Artif. Intell. Statist., 2017, pp. 1273\u20131282.",
            "year": 2017
        },
        {
            "authors": [
                "M. Aledhari",
                "R. Razzak",
                "R.M. Parizi",
                "F. Saeed"
            ],
            "title": "Federated learning: A survey on enabling technologies, protocols, and applications",
            "venue": "IEEE Access, vol. 8, pp. 140699\u2013140725, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shi",
                "Y. Zhou",
                "Y. Shi"
            ],
            "title": "Over-the-air decentralized federated learning",
            "venue": "Proc. IEEE Int. Symp. Inf. Theory (ISIT), Jul. 2021, pp. 455\u2013460.",
            "year": 2021
        },
        {
            "authors": [
                "E.T.M. Beltr\u00e1n",
                "M.Q. P\u00e9rez",
                "P.M.S. S\u00e1nchez",
                "S.L. Bernal",
                "G. Bovet",
                "M.G. P\u00e9rez",
                "G.M. P\u00e9rez",
                "A.H. Celdr\u00e1n"
            ],
            "title": "Decentralized federated learning: Fundamentals, state-of-the-art, frameworks, trends, and challenges",
            "venue": "2022, arXiv:2211.08413.",
            "year": 2022
        },
        {
            "authors": [
                "L. Barbieri",
                "S. Savazzi",
                "M. Brambilla",
                "M. Nicoli"
            ],
            "title": "Decentralized federated learning for extended sensing in 6G connected vehicles",
            "venue": "Veh. Commun., vol. 33, Jan. 2022, Art. no. 100396.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xiao",
                "Y. Ye",
                "S. Huang",
                "L. Hao",
                "Z. Ma",
                "M. Xiao",
                "S. Mumtaz",
                "O.A. Dobre"
            ],
            "title": "Fully decentralized federated learning-based on-board mission for UAV swarm system",
            "venue": "IEEE Commun. Lett., vol. 25, no. 10, pp. 3296\u20133300, Oct. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Thapa",
                "P.C.M. Arachchige",
                "S. Camtepe",
                "L. Sun"
            ],
            "title": "SplitFed:When federated learning meets split learning",
            "venue": "Proc. AAAI Conf. Artif. Intell., vol. 36, no. 8, 2022, pp. 8485\u20138493.",
            "year": 2022
        },
        {
            "authors": [
                "A. Sufian",
                "A. Ghosh",
                "A.S. Sadiq",
                "F. Smarandache"
            ],
            "title": "A survey on deep transfer learning to edge computing for mitigating the COVID-19 pandemic",
            "venue": "J. Syst. Archit., vol. 108, Sep. 2020, Art. no. 101830.",
            "year": 2020
        },
        {
            "authors": [
                "L. Valerio",
                "A. Passarella",
                "M. Conti"
            ],
            "title": "Accuracy vs. traffic trade-off of learning IoT data patterns at the edge with hypothesis transfer learning",
            "venue": "Proc. IEEE 2nd Int. Forum Res. Technol. Soc. Ind. Leveraging Better Tomorrow (RTSI), Sep. 2016, pp. 1\u20136.",
            "year": 2016
        },
        {
            "authors": [
                "K.I. Wang",
                "X. Zhou",
                "W. Liang",
                "Z. Yan",
                "J. She"
            ],
            "title": "Federated transfer learning based cross-domain prediction for smart manufacturing",
            "venue": "IEEE Trans. Ind. Informat., vol. 18, no. 6, pp. 4088\u20134096, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.J. Pan",
                "Q. Yang"
            ],
            "title": "A survey on transfer learning",
            "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345\u20131359, Oct. 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Wang",
                "M. Damani",
                "P. Wang",
                "Y. Cao",
                "G. Sartoretti"
            ],
            "title": "Distributed reinforcement learning for robot teams: A review",
            "venue": "2022, arXiv:2204.03516.",
            "year": 2022
        },
        {
            "authors": [
                "F. Venturini",
                "F. Mason",
                "F. Pase",
                "F. Chiariotti",
                "A. Testolin",
                "A. Zanella",
                "M. Zorzi"
            ],
            "title": "Distributed reinforcement learning for flexible and efficient UAV swarm control",
            "venue": "IEEE Trans. Cognit. Commun. Netw., vol. 7, no. 3, pp. 955\u2013969, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Spryn",
                "A. Sharma",
                "D. Parkar",
                "M. Shrimal"
            ],
            "title": "Distributed deep reinforcement learning on the cloud for autonomous driving",
            "venue": "Proc. IEEE/ACM 1st Int. Workshop Softw. Eng. for AI Auto. Syst. (SEFAIAS), May 2018, pp. 16\u201322.",
            "year": 2018
        },
        {
            "authors": [
                "T. Chen",
                "K. Zhang",
                "G.B. Giannakis",
                "T. Basar"
            ],
            "title": "Communicationefficient policy gradient methods for distributed reinforcement learning",
            "venue": "IEEE Trans. Control Netw. Syst., vol. 9, no. 2, pp. 917\u2013929, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Tonellotto",
                "A. Gotta",
                "F.M. Nardini",
                "D. Gadler",
                "F. Silvestri"
            ],
            "title": "Neural network quantization in federated learning at the edge",
            "venue": "Inf. Sci., vol. 575, pp. 417\u2013436, Oct. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T.B. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A. Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "inProc. Adv. Neur. Inf. Process. Sys., vol. 33, 2020, pp. 1877\u20131901.",
            "year": 2020
        },
        {
            "authors": [
                "A. Imteaj",
                "U. Thakker",
                "S. Wang",
                "J. Li",
                "M.H. Amini"
            ],
            "title": "A survey on federated learning for resource-constrained IoT devices",
            "venue": "IEEE Internet Things J., vol. 9, no. 1, pp. 1\u201324, Jan. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T. Zeng",
                "O. Semiari",
                "M. Chen",
                "W. Saad",
                "M. Bennis"
            ],
            "title": "Federated learning on the road autonomous controller design for connected and autonomous vehicles",
            "venue": "IEEE Trans. Wireless Commun., vol. 21, no. 12, pp. 10407\u201310423, Dec. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu",
                "J. Nie",
                "X. Li",
                "S.H. Ahmed",
                "W.Y.B. Lim",
                "C.Miao"
            ],
            "title": "Federated learning in the sky: Aerial-ground air quality sensing framework with UAV swarms",
            "venue": "IEEE Internet Things J., vol. 8, no. 12, pp. 9827\u20139837, Jun. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Xie",
                "Y. Peng"
            ],
            "title": "Improving the quality of inference for applications using chained DNN models during edge server handover",
            "venue": "Proc. IEEE/ACM 7th Symp. Edge Comput. (SEC), Dec. 2022, pp. 516\u2013520.",
            "year": 2022
        },
        {
            "authors": [
                "E. Ramos",
                "R. Morabito",
                "J. Kainulainen"
            ],
            "title": "Distributing intelligence to the edge and beyond [research Frontier",
            "venue": "IEEE Comput. Intell. Mag., vol. 14, no. 4, pp. 65\u201392, Nov. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "X. Qi",
                "C. Liu"
            ],
            "title": "Enabling deep learning on IoT edge: Approaches and evaluation",
            "venue": "Proc. IEEE/ACM Symp. Edge Comput. (SEC), Oct. 2018, pp. 367\u2013372.",
            "year": 2018
        },
        {
            "authors": [
                "C. Mwase",
                "Y. Jin",
                "T. Westerlund",
                "H. Tenhunen",
                "Z. Zou"
            ],
            "title": "Communication-efficient distributed AI strategies for the IoT edge",
            "venue": "Future Gener. Comput. Syst., vol. 131, pp. 292\u2013308, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Wu",
                "F. Wu",
                "L. Lyu",
                "Y. Huang",
                "X. Xie"
            ],
            "title": "Communication-efficient federated learning via knowledge distillation",
            "venue": "Nature Commun., vol. 13, no. 1, pp. 1\u20138, Apr. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Shi",
                "K. Yang",
                "T. Jiang",
                "J. Zhang",
                "K.B. Letaief"
            ],
            "title": "Communicationefficient edge AI: Algorithms and systems",
            "venue": "IEEE Commun. Surveys Tuts., vol. 22, no. 4, pp. 2167\u20132191, 4th Quart., 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Park",
                "S. Samarakoon",
                "A. Elgabli",
                "J. Kim",
                "M. Bennis",
                "S. Kim",
                "M. Debbah"
            ],
            "title": "Communication-efficient and distributed learning over wireless networks: Principles and applications",
            "venue": "Proc. IEEE, vol. 109, no. 5, pp. 796\u2013819, May 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Liu",
                "G. Zhu",
                "J. Zhang",
                "K. Huang"
            ],
            "title": "Data-importance aware user scheduling for communication-efficient edge machine learning",
            "venue": "IEEE Trans. Cognit. Commun. Netw., vol. 7, no. 1, pp. 265\u2013278, Mar. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Liu",
                "G. Zhu",
                "Q. Zeng",
                "J. Zhang",
                "K. Huang"
            ],
            "title": "Wireless data acquisition for edge learning: Data-importance aware retransmission",
            "venue": "IEEE Trans. Wireless Commun., vol. 20, no. 1, pp. 406\u2013420, Jan. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. He",
                "J. Ren",
                "G. Yu",
                "J. Yuan"
            ],
            "title": "Importance-aware data selection and resource allocation in federated edge learning system",
            "venue": "IEEE Trans. Veh. Technol., vol. 69, no. 11, pp. 13593\u201313605, Nov. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H.H. Yang",
                "A. Arafa",
                "T.Q.S. Quek",
                "H. Vincent Poor"
            ],
            "title": "Agebased scheduling policy for federated learning in mobile edge networks",
            "venue": "Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2020, pp. 8743\u20138747. 52858 VOLUME 11, 2023 C. Campolo et al.: Network for Distributed Intelligence: A Survey and Future Perspectives",
            "year": 2020
        },
        {
            "authors": [
                "H. Sun",
                "X. Ma",
                "R.Q. Hu"
            ],
            "title": "Adaptive federated learning with gradient compression in uplink NOMA",
            "venue": "IEEE Trans. Veh. Technol., vol. 69, no. 12, pp. 16325\u201316329, Dec. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X.Ma",
                "H. Sun",
                "R.Q. Hu"
            ],
            "title": "Scheduling policy and power allocation for federated learning in NOMA based MEC",
            "venue": "Proc. GLOBECOM IEEE Global Commun. Conf., Dec. 2020, pp. 1\u20137.",
            "year": 2020
        },
        {
            "authors": [
                "P.S. Bouzinis",
                "P.D. Diamantoulakis",
                "G.K. Karagiannidis"
            ],
            "title": "Wireless federated learning (WFL) for 6G networks\u2014Part II: The computethen-transmit NOMA paradigm",
            "venue": "IEEE Commun. Lett., vol. 26, no. 1, pp. 8\u201312, Jan. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "I. Mrad",
                "R. Hamila",
                "A. Erbad",
                "M. Gabbouj"
            ],
            "title": "Joint learning and optimization for federated learning in NOMA-based networks",
            "venue": "Pervas. Mobile Comput., vol. 89, Feb. 2023, Art. no. 101739.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wu",
                "Y. Song",
                "T. Wang",
                "L. Qian",
                "T.Q.S. Quek"
            ],
            "title": "Non-orthogonal multiple access assisted federated learning via wireless power transfer: A cost-efficient approach",
            "venue": "IEEE Trans. Commun., vol. 70, no. 4, pp. 2853\u20132869, Apr. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Song",
                "T. Wang",
                "Y. Wu",
                "L. Qian",
                "Z. Shi"
            ],
            "title": "Non-orthogonal multiple access assisted federated learning for UAV swarms: An approach of latency minimization",
            "venue": "Proc. Int. Wireless Commun. Mobile Comput. (IWCMC), Jun. 2021, pp. 1123\u20131128.",
            "year": 2021
        },
        {
            "authors": [
                "K. Yang",
                "T. Jiang",
                "Y. Shi",
                "Z. Ding"
            ],
            "title": "Federated learning via overthe-air computation",
            "venue": "IEEE Trans. Wireless Commun., vol. 19, no. 3, pp. 2022\u20132035, Mar. 2020.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yang",
                "Z. Zhang",
                "Y. Tian",
                "Z. Yang",
                "C. Huang",
                "C. Zhong",
                "K. Wong"
            ],
            "title": "Over-the-air split machine learning in wireless MIMO networks",
            "venue": "IEEE J. Sel. Areas Commun., vol. 41, no. 4, pp. 1007\u20131022, Apr. 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Q.N. Le",
                "L. Bariah",
                "O.A. Dobre",
                "S.Muhaidat"
            ],
            "title": "Reconfigurable intelligent surface-enabled federated learning for power-constrained devices",
            "venue": "IEEE Commun. Lett., vol. 26, no. 11, pp. 2725\u20132729, Nov. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Liu",
                "X. Yuan",
                "Y.A. Zhang"
            ],
            "title": "Reconfigurable intelligent surface enabled federated learning: A unified communication-learning design approach",
            "venue": "IEEE Trans. Wireless Commun., vol. 20, no. 11, pp. 7595\u20137609, Nov. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "K. Yang",
                "Y. Shi",
                "Y. Zhou",
                "Z. Yang",
                "L. Fu",
                "W. Chen"
            ],
            "title": "Federated machine learning for intelligent IoT via reconfigurable intelligent surface",
            "venue": "IEEE Netw., vol. 34, no. 5, pp. 16\u201322, Sep. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z.Wang",
                "J. Qiu",
                "Y. Zhou",
                "Y. Shi",
                "L. Fu",
                "W. Chen",
                "K.B. Letaief"
            ],
            "title": "Federated learning via intelligent reflecting surface",
            "venue": "IEEE Trans. Wireless Commun., vol. 21, no. 2, pp. 808\u2013822, Feb. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Ni",
                "Y. Liu",
                "Z. Yang",
                "H. Tian",
                "X. Shen"
            ],
            "title": "Federated learning in multi-RIS-aided systems",
            "venue": "IEEE Internet Things J., vol. 9, no. 12, pp. 9608\u20139624, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Ni",
                "Y. Liu",
                "H. Tian",
                "Y.C. Eldar",
                "K. Huang"
            ],
            "title": "SemiFL: Semi-federated learning empowered by simultaneously transmitting and reflecting reconfigurable intelligent surface",
            "venue": "Proc. IEEE Int. Conf. Commun., May 2022, pp. 5104\u20135109.",
            "year": 2022
        },
        {
            "authors": [
                "S. Wang",
                "M. Lee",
                "S. Hosseinalipour",
                "R. Morabito",
                "M. Chiang",
                "C.G. Brinton"
            ],
            "title": "Device sampling for heterogeneous federated learning: Theory, algorithms, and implementation",
            "venue": "Proc. INFOCOM IEEE Conf. Comput. Commun., May 2021, pp. 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "X. Cai",
                "X. Mo",
                "J. Chen",
                "J. Xu"
            ],
            "title": "D2D-enabled data sharing for distributed machine learning at wireless network edge",
            "venue": "IEEE Wireless Commun. Lett., vol. 9, no. 9, pp. 1457\u20131461, Sep. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Savazzi",
                "M. Nicoli",
                "V. Rampa"
            ],
            "title": "Federated learning with cooperating devices: A consensus approach for massive IoT networks",
            "venue": "IEEE Internet Things J., vol. 7, no. 5, pp. 4641\u20134654, May 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Huang",
                "X. Qiao",
                "W. Lai",
                "S. Dustdar",
                "J. Zhang",
                "J. Li"
            ],
            "title": "Enabling DNN acceleration with data and model parallelization over ubiquitous end devices",
            "venue": "IEEE Internet Things J., vol. 9, no. 16, pp. 15053\u201315065, Aug. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Yemini",
                "R. Saha",
                "E. Ozfatura",
                "D. G\u00fcnd\u00fcz",
                "A.J. Goldsmith"
            ],
            "title": "Semi-decentralized federated learning with collaborative relaying",
            "venue": "2022, arXiv:2205.10998.",
            "year": 2022
        },
        {
            "authors": [
                "M. Fu",
                "Y. Shi",
                "Y. Zhou"
            ],
            "title": "Federated learning via unmanned aerial vehicle",
            "venue": "2022, arXiv:2210.10970.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Lin",
                "H. Liu",
                "Y.A. Zhang"
            ],
            "title": "Relay-assisted cooperative federated learning",
            "venue": "IEEE Trans. Wireless Commun., vol. 21, no. 9, pp. 7148\u20137164, Sep. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. G\u00fcnd\u00fcz",
                "P. de Kerret",
                "N.D. Sidiropoulos",
                "D. Gesbert",
                "C.R. Murthy",
                "M. van der Schaar"
            ],
            "title": "Machine learning in the air",
            "venue": "IEEE J. Sel. Areas Commun., vol. 37, no. 10, pp. 2184\u20132199, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "D. Shome",
                "O. Waqar",
                "W.U. Khan"
            ],
            "title": "Federated learning and next generation wireless communications: A survey on bidirectional relationship",
            "venue": "Trans. Emerg. Telecommun. Technol., vol. 33, no. 7, p. e4458, Jul. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Sorour",
                "U. Mohammad",
                "A. Abutuleb",
                "H. Hassanein"
            ],
            "title": "Returning the favor: What wireless networking can offer to AI and edge learning",
            "venue": "2020, arXiv:2006.07453.",
            "year": 2020
        },
        {
            "authors": [
                "A. Kosta",
                "N. Pappas",
                "V. Angelakis"
            ],
            "title": "Age of information: A new concept, metric, and tool",
            "venue": "Found. Trends Netw., vol. 12, no. 3, pp. 162\u2013259, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "E. Calvanese Strinati",
                "S. Barbarossa"
            ],
            "title": "6G networks: Beyond Shannon towards semantic and goal-oriented communications",
            "venue": "Comput. Netw., vol. 190, May 2021, Art. no. 107930.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Ding",
                "X. Lei",
                "G.K. Karagiannidis",
                "R. Schober",
                "J. Yuan",
                "V.K. Bhargava"
            ],
            "title": "A survey on non-orthogonal multiple access for 5G networks: Research challenges and future trends",
            "venue": "IEEE J. Sel. Areas Commun., vol. 35, no. 10, pp. 2181\u20132195, Oct. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Sahin",
                "R. Yang"
            ],
            "title": "A survey on over-the-air computation",
            "venue": "IEEE Commun. Surveys Tuts., early access, Apr. 5, 2023, doi: 10.1109/COMST.2023.3264649.",
            "year": 2023
        },
        {
            "authors": [
                "Y. Liu",
                "X. Liu",
                "X. Mu",
                "T. Hou",
                "J. Xu",
                "M. Di Renzo",
                "N. Al-Dhahir"
            ],
            "title": "Reconfigurable intelligent surfaces: Principles and opportunities",
            "venue": "IEEE Commun. Surveys Tuts., vol. 23, no. 3, pp. 1546\u20131577, 3rd Quart., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Huang",
                "R. Mo",
                "C. Yuen"
            ],
            "title": "Reconfigurable intelligent surface assisted multiuser MISO systems exploiting deep reinforcement learning",
            "venue": "IEEE J. Sel. Areas Commun., vol. 38, no. 8, pp. 1839\u20131850, Aug. 2020.",
            "year": 1839
        },
        {
            "authors": [
                "F. Jameel",
                "Z. Hamid",
                "F. Jabeen",
                "S. Zeadally",
                "M.A. Javed"
            ],
            "title": "A survey of device-to-device communications: Research issues and challenges",
            "venue": "IEEE Commun. Surveys Tuts., vol. 20, no. 3, pp. 2133\u20132168, 3rd Quart., 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Lin",
                "H. Liu",
                "Y.-J.A. Zhang"
            ],
            "title": "Relay-assisted over-the-air federated learning",
            "venue": "Proc. IEEE GlobecomWorkshops (GCWkshps), Dec. 2021, pp. 1\u20137.",
            "year": 2021
        },
        {
            "authors": [
                "W. Ren",
                "Y. Qu",
                "C. Dong",
                "Y. Jing",
                "H. Sun",
                "Q. Wu",
                "S. Guo"
            ],
            "title": "A survey on collaborative DNN inference for edge intelligence",
            "venue": "2022, arXiv:2207.07812.",
            "year": 2022
        },
        {
            "authors": [
                "G. Durisi",
                "T. Koch",
                "P. Popovski"
            ],
            "title": "Toward massive, ultrareliable, and low-latency wireless communication with short packets",
            "venue": "Proc. IEEE, vol. 104, no. 9, pp. 1711\u20131726, Sep. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "T. Nishio",
                "R. Yonetani"
            ],
            "title": "Client selection for federated learning with heterogeneous resources in mobile edge",
            "venue": "Proc. IEEE Int. Conf. Commun. (ICC), May 2019, pp. 1\u20137.",
            "year": 2019
        },
        {
            "authors": [
                "X. Chen",
                "G. Zhu",
                "Y. Deng",
                "Y. Fang"
            ],
            "title": "Federated learning over multihop wireless networks with in-network aggregation",
            "venue": "IEEE Trans. Wireless Commun., vol. 21, no. 6, pp. 4622\u20134634, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.T. Vu",
                "H. Quoc Ngo",
                "T.L. Marzetta",
                "M. Matthaiou"
            ],
            "title": "How does cell-free massive MIMO support multiple federated learning groups?\u2019",
            "venue": "in Proc. IEEE 22nd Int. Workshop Signal Process. Adv. Wireless Commun. (SPAWC),",
            "year": 2021
        },
        {
            "authors": [
                "Y. Lin",
                "S. Luo"
            ],
            "title": "Poster: Accelerate cross-device federated learning with semi-reliable model multicast over the air",
            "venue": "Proc. IEEE 29th Int. Conf. Netw. Protocols (ICNP), Nov. 2021, pp. 1\u20132.",
            "year": 2021
        },
        {
            "authors": [
                "V.K. Shrivastava",
                "S. Baek",
                "Y. Baek"
            ],
            "title": "5G evolution for multicast and broadcast services in 3GPP release 17",
            "venue": "IEEE Commun. Standards Mag., vol. 6, no. 3, pp. 70\u201376, Sep. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "E.F. Pupo",
                "C.C. Gonz\u00e1lez",
                "L. Atzori",
                "M. Murroni"
            ],
            "title": "Dynamic multicast access technique in SC-PTM 5G networks: Subgrouping with OM/NOM",
            "venue": "Proc. IEEE Int. Symp. Broadband Multimedia Syst. Broadcast. (BMSB), Jun. 2022, pp. 1\u20136.",
            "year": 2022
        },
        {
            "authors": [
                "E. Seo",
                "D. Niyato",
                "E. Elmroth"
            ],
            "title": "Auction-based federated learning using software-defined networking for resource efficiency",
            "venue": "Proc. 17th Int. Conf. Netw. Service Manage. (CNSM), Oct. 2021, pp. 42\u201348.",
            "year": 2021
        },
        {
            "authors": [
                "S.K.P. Tam",
                "S. Math"
            ],
            "title": "Efficient resource slicing scheme for optimizing federated learning communications in software-defined IoT networks",
            "venue": "J. Internet Comput. Services, vol. 22, no. 5, pp. 27\u201333, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "V. Balasubramanian",
                "M. Aloqaily",
                "M. Reisslein",
                "A. Scaglione"
            ],
            "title": "Intelligent resource management at the edge for ubiquitous IoT: An SDN-based federated learning approach",
            "venue": "IEEE Netw., vol. 35, no. 5, pp. 114\u2013121, Sep. 2021. VOLUME 11, 2023 52859 C. Campolo et al.: Network for Distributed Intelligence: A Survey and Future Perspectives",
            "year": 2021
        },
        {
            "authors": [
                "D. Aguiari",
                "A. Ferlini",
                "J. Cao",
                "S. Guo",
                "G. Pau"
            ],
            "title": "Poster abstract: Ccontinuum: Edge-to-cloud computing for distributed AI",
            "venue": "Proc. IEEE Conf. Comput. Commun. Workshops (INFOCOM WKSHPS), Apr. 2019, pp. 1053\u20131054.",
            "year": 2019
        },
        {
            "authors": [
                "C. Campolo",
                "G. Lia",
                "M. Amadeo",
                "G. Ruggeri",
                "A. Iera",
                "A. Molinaro"
            ],
            "title": "Towards namedAI networking: Unveiling the potential of NDN for edge AI",
            "venue": "Proc. AdHocNow, 2020, pp. 16\u201322.",
            "year": 2020
        },
        {
            "authors": [
                "M. Amadeo",
                "C. Campolo",
                "A. Iera",
                "A. Molinaro",
                "G. Ruggeri"
            ],
            "title": "Client discovery and data exchange in edge-based federated learning via named data networking",
            "venue": "Proc. IEEE Int. Conf. Commun., May 2022, pp. 2990\u20132995.",
            "year": 2022
        },
        {
            "authors": [
                "S. Bano",
                "N. Tonellotto",
                "P. Cassara",
                "A. Gotta"
            ],
            "title": "KafkaFed: Two-tier federated learning communication architecture for Internet of Vehicles",
            "venue": "Proc. IEEE Int. Conf. Pervasive Comput. Commun. Workshops Other Affiliated Events (PerCom Workshops), Mar. 2022, pp. 515\u2013520.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xiong",
                "N. Zilberman"
            ],
            "title": "Do switches dream of machine learning? Toward in-network classification",
            "venue": "Proc. 18th ACM Workshop Hot Topics Netw., Nov. 2019, pp. 25\u201333.",
            "year": 2019
        },
        {
            "authors": [
                "D. Sanvito",
                "G. Siracusano",
                "R. Bifulco"
            ],
            "title": "Can the network be the AI accelerator?\u2019",
            "venue": "in Proc. Morning Workshop In-Network Comput.,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Li",
                "J. Park",
                "M. Alian",
                "Y. Yuan",
                "Z. Qu",
                "P. Pan",
                "R.Wang"
            ],
            "title": "A networkcentric hardware/algorithm co-design to accelerate distributed training of deep neural networks",
            "venue": "IEEE/ACM MICRO, Oct. 2018, pp. 175\u2013188.",
            "year": 2018
        },
        {
            "authors": [
                "M. Saquetti",
                "R. Canofre",
                "A.F. Lorenzon",
                "F.D. Rossi",
                "J.R. Azambuja",
                "W. Cordeiro",
                "M.C. Luizelli"
            ],
            "title": "Toward in-network intelligence: Running distributed artificial neural networks in the data plane",
            "venue": "IEEE Commun. Lett., vol. 25, no. 11, pp. 3551\u20133555, Nov. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B.M. Xavier",
                "R.S. Guimar\u00e3es",
                "G. Comarela",
                "M. Martinello"
            ],
            "title": "Programmable switches for in-networking classification",
            "venue": "Proc. IEEE Conf. Comput. Commun., May 2021, pp. 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "A. Sapio",
                "M. Canini",
                "C.-Y. Ho",
                "J. Nelson",
                "P. Kalnis",
                "C. Kim",
                "A. Krishnamurthy",
                "M. Moshref",
                "D.R.K. Ports",
                "P. Richt\u00e1rik"
            ],
            "title": "Scaling distributed machine learning with in-network aggregation",
            "venue": "2019, arXiv:1903.06701.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "I. Liu",
                "Y. Yuan",
                "D. Chen",
                "A. Schwing",
                "J. Huang"
            ],
            "title": "Accelerating distributed reinforcement learning with in-switch computing",
            "venue": "Proc. ACM/IEEE 46th Annu. Int. Symp. Comput. Archit. (ISCA), Jun. 2019, pp. 279\u2013291.",
            "year": 2019
        },
        {
            "authors": [
                "J. He",
                "H. Wu",
                "X. Xiao",
                "R. Bassoli",
                "F.H.P. Fitzek"
            ],
            "title": "Functional split of in-network deep learning for 6G: A feasibility study",
            "venue": "IEEE Wireless Commun., vol. 29, no. 5, pp. 36\u201342, Oct. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Kreutz",
                "F.M.V. Ramos",
                "P.E. Ver\u00edssimo",
                "C.E. Rothenberg",
                "S. Azodolmolky",
                "S. Uhlig"
            ],
            "title": "Software-defined networking: A comprehensive survey",
            "venue": "Proc. IEEE, vol. 103, no. 1, pp. 14\u201376, Jan. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Xie",
                "F.R. Yu",
                "T. Huang",
                "R. Xie",
                "J. Liu",
                "C. Wang",
                "Y. Liu"
            ],
            "title": "A survey of machine learning techniques applied to software defined networking (SDN): Research issues and challenges",
            "venue": "IEEE Commun. Surveys Tuts., vol. 21, no. 1, pp. 393\u2013430, 1st Quart., 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Islam",
                "N. Muslim",
                "J.W. Atwood"
            ],
            "title": "A survey on multicasting in software-defined networking",
            "venue": "IEEE Commun. Surveys Tuts., vol. 20, no. 1, pp. 355\u2013387, 1st Quart., 2018.",
            "year": 2018
        },
        {
            "authors": [
                "D. King",
                "A. Farrel"
            ],
            "title": "A survey of semantic internet routing techniques",
            "venue": "Internet Eng. Task Force (IETF), Draft-King-Irtf-Semantic- Survey-01, Tech. Rep., 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P. Bosshart",
                "D. Daly",
                "G. Gibb",
                "M. Izzard",
                "N. McKeown",
                "J. Rexford",
                "C. Schlesinger",
                "D. Talayco",
                "A. Vahdat",
                "G. Varghese",
                "D. Walker"
            ],
            "title": "P4: Programming protocol-independent packet processors",
            "venue": "ACM SIG- COMM Comput. Commun. Rev., vol. 44, no. 3, pp. 87\u201395, Jul. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "O. Michel",
                "R. Bifulco",
                "G. R\u00e9tv\u00e1ri",
                "S. Schmid"
            ],
            "title": "The programmable data plane: Abstractions, architectures, algorithms, and applications",
            "venue": "ACM Comput. Surveys, vol. 54, no. 4, pp. 1\u201336, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "T. Mai",
                "S. Garg",
                "H. Yao",
                "J. Nie",
                "G. Kaddoum",
                "Z. Xiong"
            ],
            "title": "In-network intelligence control: Toward a self-driving networking architecture",
            "venue": "IEEE Netw., vol. 35, no. 2, pp. 53\u201359, Mar. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G. Siracusano",
                "R. Bifulco"
            ],
            "title": "In-network neural networks",
            "venue": "2018, arXiv:1801.05731.",
            "year": 2018
        },
        {
            "authors": [
                "L. Zhang",
                "A. Afanasyev",
                "J. Burke",
                "V. Jacobson",
                "K. Claffy",
                "P. Crowley",
                "C. Papadopoulos",
                "L. Wang",
                "B. Zhang"
            ],
            "title": "Named data networking",
            "venue": "SIGCOMM Comput. Commun. Rev., vol. 44, no. 3, pp. 66\u201373, Jul. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "G. Xylomenos",
                "C.N. Ververidis",
                "V.A. Siris",
                "N. Fotiou",
                "C. Tsilopoulos",
                "X. Vasilakos",
                "K.V. Katsaros",
                "G.C. Polyzos"
            ],
            "title": "A survey of information-centric networking research",
            "venue": "IEEE Commun. Surveys Tuts., vol. 16, no. 2, pp. 1024\u20131049, 2nd Quart., 2014.",
            "year": 2014
        },
        {
            "authors": [
                "M. Amadeo",
                "C. Campolo",
                "A. Molinaro",
                "G. Ruggeri"
            ],
            "title": "IoT data processing at the edge with named data networking",
            "venue": "Proc. Eur. Wireless 24th Eur. Wireless Conf., May 2018, pp. 1\u20136.",
            "year": 2018
        },
        {
            "authors": [
                "A. Rahman",
                "D. Trossen",
                "D. Kutscher",
                "andR. Ravindran",
                "Deployment"
            ],
            "title": "Considerations for Information-Centric Networking (ICN), document RFC",
            "year": 2020
        },
        {
            "authors": [
                "S. Shahhosseini",
                "D. Seo",
                "A. Kanduri",
                "T. Hu",
                "S.-S. Lim",
                "B. Donyanavard",
                "A.M. Rahmani",
                "N. Dutt"
            ],
            "title": "Online learning for orchestration of inference in multi-user end-edge-cloud networks",
            "venue": "ACM Trans. Embedded Comput. Syst., vol. 21, no. 6, pp. 1\u201325, Nov. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Malandrino",
                "C.F. Chiasserini",
                "G. di Giacomo"
            ],
            "title": "Efficient distributed DNNs in the mobile-edge-cloud continuum",
            "venue": "IEEE/ACM Trans. Netw., early access, Nov. 24, 2022, doi: 10.1109/TNET.2022.3222640.",
            "year": 2022
        },
        {
            "authors": [
                "E. Samikwa",
                "A.D.Maio",
                "T. Braun"
            ],
            "title": "ARES: Adaptive resource-aware split learning for Internet of Things,\u2019\u2019Comput",
            "venue": "Netw., vol",
            "year": 2022
        },
        {
            "authors": [
                "S. Wang",
                "X. Zhang",
                "H. Uchiyama",
                "H. Matsuda"
            ],
            "title": "HiveMind: Towards cellular native machine learning model splitting",
            "venue": "IEEE J. Sel. Areas Commun., vol. 40, no. 2, pp. 626\u2013640, Feb. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M.M. Wadu",
                "S. Samarakoon",
                "M. Bennis"
            ],
            "title": "Joint client scheduling and resource allocation under channel uncertainty in federated learning",
            "venue": "IEEE Trans. Commun., vol. 69, no. 9, pp. 5962\u20135974, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Yu",
                "R. Albelaihi",
                "X. Sun",
                "N. Ansari",
                "M. Devetsikiotis"
            ],
            "title": "Jointly optimizing client selection and resource management in wireless federated learning for Internet of Things",
            "venue": "IEEE Internet Things J., vol. 9, no. 6, pp. 4385\u20134395, Mar. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Ko",
                "J. Lee",
                "S. Seo",
                "S. Pack",
                "V.C.M. Leung"
            ],
            "title": "Joint client selection and bandwidth allocation algorithm for federated learning",
            "venue": "IEEE Trans. Mobile Comput., vol. 22, no. 6, pp. 3380\u20133390, Jun. 2023.",
            "year": 2023
        },
        {
            "authors": [
                "J. Xu",
                "H. Wang",
                "L. Chen"
            ],
            "title": "Bandwidth allocation for multiple federated learning services in wireless edge networks",
            "venue": "IEEE Trans. Wireless Commun., vol. 21, no. 4, pp. 2534\u20132546, Apr. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Campolo",
                "G. Genovese",
                "A. Iera",
                "A. Molinaro"
            ],
            "title": "Virtualizing AI at the distributed edge towards intelligent IoT applications",
            "venue": "J. Sensor Actuator Netw., vol. 10, no. 1, p. 13, Feb. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "C. Campolo",
                "G. Genovese",
                "G. Singh",
                "A. Molinaro"
            ],
            "title": "Scalable and interoperable edge-based federated learning in IoT contexts",
            "venue": "Comput. Netw., vol. 223, Mar. 2023, Art. no. 109576.",
            "year": 2023
        },
        {
            "authors": [
                "T. Yang",
                "M. Qin",
                "N. Cheng",
                "W. Xu",
                "L. Zhao"
            ],
            "title": "Liquid software-based edge intelligence for future 6G networks",
            "venue": "IEEE Netw., vol. 36, no. 1, pp. 69\u201375, Jan. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Abdulrahman",
                "H. Tout",
                "A. Mourad",
                "C. Talhi"
            ],
            "title": "FedMCCS: Multicriteria client selection model for optimal IoT federated learning",
            "venue": "IEEE Internet Things J., vol. 8, no. 6, pp. 4723\u20134735, Mar. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Wu",
                "C. Zhou",
                "M. Li",
                "H. Wu",
                "H. Zhou",
                "N. Zhang",
                "X.S. Shen",
                "W. Zhuang"
            ],
            "title": "AI-native network slicing for 6G networks",
            "venue": "IEEE Wireless Commun., vol. 29, no. 1, pp. 96\u2013103, Feb. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Kanduri",
                "S. Shahhosseini",
                "E. Kasaeyan Naeini",
                "H. Alikhani",
                "P. Liljeberg",
                "N. Dutt",
                "A.M. Rahmani"
            ],
            "title": "Edge-centric optimization of multimodal ML-driven eHealth applications",
            "venue": "2022, arXiv:2208.02597.",
            "year": 2022
        },
        {
            "authors": [
                "M. Nitti",
                "V. Pilloni",
                "G. Colistra",
                "L. Atzori"
            ],
            "title": "The virtual object as a major element of the Internet of Things: A survey",
            "venue": "IEEE Commun. Surveys Tuts., vol. 18, no. 2, pp. 1228\u20131240, 2nd Quart., 2016.",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS Artificial intelligence, cloud continuum, distributed intelligence, machine learning, network.\nI. INTRODUCTION Fueled by the increasing amount of data, generated by massively deployed connected devices (up to 29.3 billions by 2023 according to Cisco\u2019s annual report [1]), Artificial Intelligence (AI) algorithms are significantly advancing decision making in many real-world application domains, ranging from smart manufacturing [2] to immersive experience [3], from autonomous driving [4] to healthcare [5].\nAs a branch of AI, Machine Learning (ML), relies on two main phases: (i) learning, which trains a model based on an input dataset, and (ii) inference, which provides knowledge/prediction upon the trained model. The conventional approach so far was to execute both training and inference in the cloud, by leveraging the computing capabilities of high-performing data centers. Nevertheless, the increasing demand for running training [6] and inference procedures [7]\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Hosam El-Ocla .\nis outpacing the increase in computation power of existing centralized (cloud) infrastructures. Moreover, supporting frequent transmission of huge amount of training data towards the cloud is a challenging task even for wired links [8].\nSuch circumstances coupled with the responsiveness, security and privacy demands of a large set of ML-based applications, are pushing towards distributed intelligence solutions, leveraging computing resources spread from the cloud to the edge, and even extending to the deep edge, encompassing (resource-constrained) embedded devices [9]. For instance, model training can run in parallel over multiple distributed heterogeneous devices [6], and the execution of inference models can be sequentially split over multiple chained nodes [10].\nA. RELEVANT CHALLENGES Distributing AI workloads entails deciding how many computing resources to dedicate and where to allocate them; such\n52840 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 11, 2023\ndecisions cannot overlook the status of communication links and the overall network dynamics. Indeed, (big) amount of data, e.g., huge raw datasets and (portion of) ML models as well as small-sized results of inference need to be moved across the network and possibly transformed to readily construct and distribute knowledge.\nSuch a new context raises unprecedented challenges to the network design; in fact, the network behaviour can highly affect the performance of applications built upon distributed intelligence.\nOn the one hand, the network may be a bottleneck, e.g., in case of distributed training [19], when the reliability and the latency of huge data exchanges among learning nodes over multiple iterations may be undermined by lossy and lowbandwidth channels. On the other hand, if properly designed, the network can boost distributed AI performance. Indeed, entities hosting AI-related components (e.g., data, models) can be chained to ensure reliable, low-latency and efficient data exchanges. Programmable network nodes on the path towards end-host applications may perform in-network Artificial Neural Network (ANN) processing [20], while reducing the amount of moved data and its latency.\nB. CONTRIBUTIONS OF THIS SURVEY From these considerations, a strongly felt need clearly emerges to develop future network solutions to better support distributed intelligence applications.\nNonetheless, so far, studies that relate AI and ML with next-generation networks mostly consider intelligence as a key enabler to improve network performance, in a perspective of \u2018\u2018AI for networks\u2019\u2019, e.g., [21], [22], [23], [24], and [25]. Reason for this is undoubtedly the expected tremendous complexity of sixth-generation (6G) systems, whichwill likely be much denser (i.e., in terms of number of access points, users and devices), more heterogeneous (in terms of technologies), and will support a variety of fascinating applications with stricter performance requirements w.r.t. the fifthgeneration (5G) [26]. Thus, solutions utilizing sophisticated AI/ML techniques are being designed to enable the cognitive management of network functionalities and to dynamically and promptly adapt offered services in an automated fashion, based on changes in user needs, environmental conditions, and business goals [11].\nOnly a few recent pioneering works recognized the need for a shift from a \u2018\u2018network of information\u2019\u2019 to a \u2018\u2018network of intelligence\u2019\u2019. They promote new communication primitives and network functionalities aimed to meet accuracy and latency constraints of AI/ML workloads [12], thus matching the scope of the so-called \u2018\u2018network for AI\u2019\u2019 vision. To the best of our knowledge, the literature is still missing a comprehensive analysis of the key design issues for future networks supporting AI, with special focus on the key requirements of emerging distributed intelligence solutions. Recently, some works have been published which address such topics, among which [11], [12], [13], [14], [15], [16], [17], [18]. However, as summarized in Table 1, the existing\nworks either focus on specific network domains, e.g., wireless access [16], [18], edge domain [15], or they consider few communication (transport) protocols [17] and few radio resource allocation management approaches (i.e., power and bandwith allocation) [18], or miss a detailed overview of the state-of-the-art [11], [12], [13], [14].\nIn particular, taking a cue from the works summarized in Table 1 and going further, we aim to provide an endto-end perspective, by identifying network design principles that can effectively support distributed intelligence over both the radio access and core network segments, in agreement with the vision of fifth generation (5G) and upcoming 6G systems. Such principles and the relevant scanned literature solutions span from radio resource allocation and innovative physical layer techniques, to both evolutionary and disruptive routing and forwarding mechanisms in the core network domain and orchestration and management approaches. To this aim, a comprehensive survey of the literature about network-related solutions to support distributed intelligence is shared along with our visions. We expect our investigation to fuel research efforts towards the design of a new network ecosystem aimed both at supporting distributed intelligence by design and at actively contributing to its widespread adoption.\nC. ORGANIZATION OF THIS SURVEY The remainder of this work is organized as follows. Section II scans some representative distributed intelligence solutions. The relevant network requirements and issues are dissected in Section III together with some solutions to the above issues coming from the AI community. Sections IV, V, VI discuss the technologies we deem relevant as key enablers of a future end-to-end network supporting distributed intelligence, in the wireless access domain, in the core network and at the orchestration layer, respectively. Section VII reviews the latest progress of the industry standardization and projects on developing network solutions to support the deployment of distributed intelligence. Then, Section VIII summarizes the main findings of the surveyed literature, provides guidelines for future research directions as well as pinpoints additional open issues. Conclusive remarks are reported in Section IX.\nII. DISTRIBUTED INTELLIGENCE: REPRESENTATIVE IMPLEMENTATIONS Several options have been devised for distributing training and inference workloads across multiple devices. In the targeted context, such devices are not limited tomachines within data centers, but may span the cloud-to-things continuum, as graphically sketched in Fig. 1. In this section, some of the most representative solutions, which we deem may impact the network performance and be affected, in their turn, by the network performance, are recalled.\nA. PARALLEL TRAINING To speed up training and cope with increasing complexity and sizes of Deep Neural Network (DNN) models and training\nVOLUME 11, 2023 52841\nTABLE 1. Differences between this manuscript and the closest related works.\ndatasets, full advantage of parallel nodes can be taken by partitioning the data, the model, or a combination of them [6], [27]. The data-parallel approach assumes to partition the data and feed the different portions to a set of distributed nodes, all implementing the same model. Alternatively, a modelparallel approach can be applied in which the entire dataset is used by each node that operates on different parts of the model, and the final model results from the aggregation of the various parts.\nInitially proposed for data centers with multiple workers, the same approach has been recently borrowed in the edge computing domain, where several nodes can enforce the aforementioned parallel tasks [28], [29]. For instance, in [30], such techniques are applied to Convolutional Neural Networks (CNNs) models for video surveillance tasks.\nB. MODEL SPLITTING A further distributed framework, that can be applied to both training and inference, is model splitting, in which different (at least two) portions of a complex ML model are executed sequentially in different processing nodes [31]. The peculiarity of this type of distributed learning is that each node does not train an instance of the whole model and model layers are processed sequentially. In the envisioned context, for instance, end-devices may run the initial computation-friendlymodel layers of theDNN and then, send\nthe intermediate results to edge nodes and cloud facilities to feed the remaining computation-heavier layers and produce the final outputs [10], [32], [33].\nThe model splitting idea builds upon the fact that the data size at some intermediate layers of a DNN is significantly smaller than that of raw input data. Hence, on the one hand, it is possible to reduce the transmission latency and the incurred amount of exchanged data compared to cloud-only DNN implementations. On the other hand, model splitting oversteps the limitations in terms of computing and storage capabilities of edge devices, which are usually not able to fully deploy and run large deep network models (e.g., containing up to millions of parameters) [34]. In addition, model splitting protects user privacy by transmitting partially processed data rather than transmitting raw data [35].\nHow to split the model is a critical decision, since it affects the resulting computational cost and communication overhead. If not properly selected, it can cause the data amplification effect [34], according to which the size of intermediate output data of the DNN can be larger than that of the input data.\nC. FEDERATED LEARNING Rather than sending raw data to a remote server, in Federated Learning (FL) the model is (entirely, unlike in Model Splitting) locally trained on their own data by distributed devices,\n52842 VOLUME 11, 2023\nreferred to as clients. The latter ones then share the relevant parameters (e.g., NNweights) with a server, which aggregates the model and pushes it back to the clients. The procedure is reiterated over several rounds until the desired accuracy level is achieved [36], [37].\nInitially proposed by Google [36], Federated Learning allows minimizing user privacy leakage, since raw data are not required for the training at the Federated Learning aggregator. As a further benefit, latency due to data offloading is reduced compared to a centralized learning approach and network resources for data exchange saved.\nThe single point of failure (the single server) is one of the main weaknesses of the Federated Learning approach. Indeed, the central aggregator may not always be available and reliable when the number of edge devices is large, and especially in some application scenarios, e.g., cooperative driving and robotics [38]. Hence, decentralized server-less Federated Learning solutions, also referred to as swarm learning, are gaining momentum [39], [40], [41]. In such decentralized solutions, devices interact with their neighbors through device-to-device (D2D) communications, and consensus mechanisms are needed to ensure the achievement of a common learning goal.\nFinally, coupling Federated and Split Learning may be contemplated when resource-constrained clients are involved and training and deployment of the full model is poorly feasible [42].\nD. TRANSFER LEARNING In particular usage scenarios, e.g., the medical domain [43], accessing the massive data necessary to train Deep Learning models is very expensive, or even impossible. In other cases, data may available in a number of disjoint physical locations, e.g., those collected from users\u2019 personal devices for the sake of activity recognition [44] or from smart devices in smart manufacturing context [45].\nBy mimicking human behavior in which it is possible to apply knowledge learned in one domain to solve problems in a different domain, Transfer Learning enables the transfer of knowledge and learning between devices [46]. Although not fully equivalent to the distributed approaches mentioned so far, Transfer Learning also entails the distribution of data/models among different nodes spanning different domains.\nE. DISTRIBUTED REINFORCEMENT LEARNING Reinforcement Learning involves a sequential decisionmaking procedure, where a learner takes (possibly randomized) actions in a stochastic environment over a sequence of time steps, and aims to maximize the long-term cumulative rewards received from the interacting environment according to a given policy.\nAlthough initially conceived for the single-learner tasks, multiple learners can be foreseen which are coordinated by a central controller, whenever scalability becomes an issue,\nVOLUME 11, 2023 52843\ne.g., in robotics [47], drones-based [48] and autonomous vehicles applications [49]. To this aim, the latter one must exchange information with all learners, by collecting their rewards and local observations, or, broadcasting the policy to them [50].\nIII. NETWORK REQUIREMENTS AND ISSUES FOR DISTRIBUTED INTELLIGENCE When considering distributed intelligence spanning different nodes, achieving high accuracy does not only matter. Indeed, its achievement also depends on the specific network deployment and can be affected by the peculiar network requirements of each distributed intelligence solution.\nA. NETWORK REQUIREMENTS Whatever the distributed intelligence approach and the motivation to implement it, it is essential to properly connect the nodes that perform the different tasks in order to streamline operations. It is possible to opt for systems with a single central point of aggregation, or based on tree structures, or even systems leveraging fully distributed nodes, possibly organized into clusters, without hierarchical relationships. The different architectural models and their relevant deployed topology differently impact on: the obtainable accuracy, the system resilience and security/privacy degree, the overall computing load of nodes, and, what we are most interested in, the communication footprint and network performance.\nTable 2 reports the network requirements in terms of communication footprint (bandwidth) and latency to support specific data exchange (e.g., dataset, entire model, model weights, etc.) in representative distributed intelligence solutions."
        },
        {
            "heading": "1) BANDWIDTH",
            "text": "Distributed intelligence solutions have been mainly conceived to partition the computing load and/or reduce the communication footprint. Some of the aforecited approaches may still require to transfer bandwidth-hungry datasets among different nodes, as in the case of parallel training solutions. However, also when datasets are not exchanged, heavy\nmodels demanding high bandwidth may be transferred, ranging in size from hundreds of MBytes to several GBytes [51], for instance in the case of Transfer and Federated Learning.\nAdditionally, model updates may also imply a huge network load, if frequently (e.g., at every iteration) and massively transmitted (e.g., by multiple nodes), as for Distributed Reinforcement Learning and Federated Learning solutions. Each model update may have as many parameters as the model itself. Natural Language Processing models usually have hundreds of millions of parameters. For instance, the well-known GPT-3 model has 175 billion parameters corresponding to over 350 GBytes [52]. Training such kind of models by exchanging the aforementioned amount of data per each communication round is challenging, especially over wireless channels.\nIn case of model splitting, data outputs of intermediate DNN layers could overwhelm the network and deteriorate the learning/inference performance due to a wrong choice of the splitting points."
        },
        {
            "heading": "2) LATENCY",
            "text": "In distributed intelligence solutions, latency is composed of two main contributions: the communication latency due to exchanges of data (whatever the type, as per Table 2) over the network, and the computing latency, due to the execution of (portions of) training/inference models.\nIn general, keeping a short latency is more crucial for inference compared to training operations. Indeed, inference often needs to be executed in near real-time to provide prompt reaction to events, e.g., 200 ms to get predictions from ML models for voice assistants, or below 10 ms when tactile Internet and autonomous driving operations are considered.\nB. MAIN ISSUES Compared to centralized solutions, distributed intelligence solutions exhibit specific issues which need to be addressed."
        },
        {
            "heading": "1) HETEROGENEITY OF DEVICES",
            "text": "Devices to be involved in the distributed training and inference procedures may span the cloud continuum, hence\n52844 VOLUME 11, 2023\nencompassing Internet of Things (IoT) devices as well as edge/cloud nodes. Heterogeneity may concern the computing/battery capabilities, as well as the experienced connectivity conditions. In practical distributed learning setups, some clients are stragglers and cannot send their updates regularly, either because: (i) they cannot finish their computation within a prescribed deadline due to limitations of their computing capabilities, or (ii) they experience poor and/or intermittent connectivity.\nOn the one hand, stragglers may significantly deteriorate the convergence of distributed learning procedures as the computed local updates become stale. On the other hand, if they are not selected to contribute to the training procedures, the model quality could be extremely low, especially if their number is high [53]."
        },
        {
            "heading": "2) MOBILITY OF DEVICES",
            "text": "In several upcoming applications, learning algorithms will rely on data provided/generated by either terrestrial [54] or aerial mobile devices [55]. Raw data retrieval under high mobility conditions may face poor robustness due to fast fading and may be hindered by short-lived connectivity. The mobility of clients could become a concern also for Federated Learning, when model updates need to be iteratively exchanged over multiple rounds. The issue is less exacerbated in the case of inference, where time dynamics are typically smaller. However, it could happen that inference split may be interrupted during the movement of mobile users from one edge access point to another [56]."
        },
        {
            "heading": "3) LACK OF INTEROPERABILITY",
            "text": "Different distributed AI components should be able to transparently collaborate to serve the same purpose, in case of parallelization and serialization of training and inference workloads. However, unlike in centralized AI deployments, interoperability is severely hindered by fragmented and mainly application-specific solutions [57].\nA typical approach to achieving the targeted requirements and solving some of these issues comes from the AI community and consists in considering the network as a possible bottleneck and the solution sought is to adapt the distributed learning techniques to make themwork better despite the limits imposed by the network. Most of the solutions of this type in the literature try to act on the data, models, and information managed by learning/inference algorithms in such a way as to overcome the issues experienced by distributed training and inference solutions over the continuum.\nModel compression techniques, such as pruning and quantization, help reducing the computational complexity of DNNs [58], [59], to enable their execution even in constrained devices at the deep edge, while reducing latency and energy consumption, although at the expenses of a lower accuracy.\nMore in detail, quantisation strategies provide a low precision representation of weights, gradients or activations to reduce the total number of bits transmitted in each\nupdate and thus, reduce the incurred communication footprint and latency transfer [59]. Sparsification techniques prevent irrelevant updates from being transmitted by, for example, removing redundant information and only transmitting the important values from local estimates. Knowledge distillation techniques transfer knowledge learnt from a larger model or ensemble of models, such as output predictions, feature activations, or correlations between feature maps, to train a smaller model. Knowledge distillation techniques can be applied in Federated Learning to send soft-label predictions, instead of heavier updated models or gradients [60].\nThe analysis of the aforementioned communicationefficient techniques is extensively surveyed in the literature, for instance in [18], [59], [61], and [62]. As stated in [19], such solutions require changes to applications and may hurt the accuracy performance of models. For this very reason, the design of solutions coming from the network community is advocated as the real game changer to enable distributed intelligence solutions.\nIn alignment with such a perspective, one must start from understanding the actual impact of the aforementioned requirements and issues on the network design, and consequently move from the current concept of networks that already support distributed services well towards that of networks for distributed AI-driven services, which will surely characterize future 6G ecosystems.\nTo this aim, in the following Sections we will discuss potential network enablers to adequately support distributed intelligence. For each technical enabler, we briefly discuss potentials, by scanning relevant representative solutions in the literature, whenever possible, and then, outline possible limitations and open issues.\nIV. ENABLERS AND SOLUTIONS IN THE RADIO ACCESS Intense research activities have been recently carried out to exploit ML for optimizing various procedures in wireless communication networks (e.g., handover, radio resource allocation) [21], [22]. Less attention instead, has been devoted to assess the impact of ML techniques in practical wireless communication systems. For instance, whenever learning is offloaded to the edge/cloud, the transfer of huge datasets could easily burden the uplink air interface. The same holds for model updates sent by mobile devices acting as Federated Learning clients.\nCommunication solutions aimed to make the best of the limited radio resources are strongly needed to enable effective distributed intelligence in upcoming 6G systems [16], [88], [89].\nHowever, only very recently efforts have been devoted to explore how adapting, optimizing, and arranging wireless networks can contribute to implementing ML techniques [90]. Some of these solutions are discussed in the following, by scanning some representative works in the literature. The solutions analysed in this Section and summarized in Table 3 refer to the Radio Access Network (RAN) of future wireless systems.\nVOLUME 11, 2023 52845\nA. SEMANTIC COMMUNICATIONS Conventional communication techniques assume data bits being of equal importance. For learning, instead, some samples within a training dataset may be more important than others. Therefore, data can be delivered more efficiently over wireless links by differentiating the usefulness of training data samples. In this context, Age of Information [91] is a further possible metric to consider for assessing information significance.\nSemantic communications appear extremely promising in such a context, when informative messages need to be transmitted for training/inference procedures. They refer to a paradigm shift for the wireless system design from data-oriented communication (i.e., maximizing communication rate or reliability based on Shannon theory) to goal-oriented communications targeting effective task execution among distributed network nodes [92]."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "The work in [63] proposes a data-importance aware user scheduling algorithm for communication-efficient edge machine learning. A classifier is trained at the edge server by utilizing the data distributed at multiple edge devices. The scheduling decision is based on a data importance indicator, which both incorporates the signal-to-noise ratio and data uncertainty. Likewise, re-transmission is devised in [64], which selectively re-transmits a data sample based on its uncertainty, which helps learning. Besides raw data collection, a similar philosophy can be applied for the acquisition of learning relevant information in Federated Learning as in [65]. Similarly, in [66], based on a metric termed the Age of Update, a scheduling policy is proposed to improve the Federated Learning efficiency that jointly accounts for the staleness of the received parameters and the instantaneous channel qualities."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Some of the aforementioned solutions, although promising to reduce the bandwidth pressure over wireless links and to improve accuracy and latency performance, entail further advancements towardsmore sophisticated and powerful wireless transceivers. Semantic extraction is necessary to understand what information is of interest before transmitting it; this, in turn, may require AI models.\nB. NON-ORTHOGONAL MULTIPLE ACCCESS The conventional orthogonal multi-access (OMA) schemes are inefficient to handle massive learners competing for radio resources, as in the case of Federated Learning. The required radio resources linearly scale with the number of edge devices that participate in the learning process. On the contrary, Non-Orthogonal Multiple Access (NOMA) allows multiple devices to transmit simultaneously on the same channel, so that the data rate is increased and the communication\nlatency is reduced [93]. This is crucial to make training convergence faster and to improve communication efficiency."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "The seminal work in [67] proposes NOMA for model update in Federated Learning. Clients are capable of transmitting their trained parameters simultaneously, while the base station decodes the users\u2019 messages by utilizing Successive Interference Cancellation (SIC). There, it shown that NOMA outperforms a traditional time division multiplexing access approach. The proposal also adaptively compresses gradient values according to either sparsification or quantization. A scheduling policy and power allocation scheme using NOMA is proposed in [68] in order to maximize the weighted sum data rate under practical constraints during the entire learning process.\nThe Compute-then-Transmit NOMA (CT-NOMA) protocol is introduced and optimized in [69]. According to CT-NOMA, users terminate concurrently the local model training and then, after a fixed time, simultaneously transmit the trained parameters to the central aggregator. The work in [70] formulates a multi-objective optimization problem adopted to minimize the convergence round and to maximize another crucial metric in the NOMA domain, namely the user access fairness.\nIn [71] NOMA is applied coupled with wireless power transfer, which is adopted by the base station to power the end-devices. An optimization problem is formulated with the aim of minimizing a system-wise cost that includes the total energy consumption and the overall latency for the Federated Learning convergence.\nIn [72] a group of unmanned aerial vehicles (UAVs) use their collected data to train their respective local models, which are then aggregated into a global model. There, NOMA enables the follower-UAVs to send their local models to the leader-UAV simultaneously over a same resource block."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Despite the neat advantages of NOMA to improve uplink transmissions, its concrete implementation is still an open issue. Existing works mainly consider ideal SIC, which is not always the case. Moreover, more sophisticated access solutions, such as hybrid NOMA/OMA configurations, which could possibly enhance the scalability of Federated Learning, may definitely be a subject matter of future work.\nC. AirComp AirComp [94] has been recently developed as a new airinterface solution, which merges the concurrent data transmission from multiple devices and performs \u2018\u2018over-the-air\u2019\u2019 data aggregation, by exploiting the inherent waveform superposition property of a multi-access channel."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "In [73] AirComp is proposed for fast model update aggregation. The Federated Learning server directly receives\nVOLUME 11, 2023 52847\nthe aggregated version of analog modulated local models/gradients simultaneously transmitted by devices. Such a scheme allows simultaneous access and hence, can dramatically reduce multi-access latency compared to the OMA schemes, better scaling with the number of clients, without significant loss of the learning accuracy. AirComp coupled with MIMO is applied to split learning in [74]."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Accurate channel estimation and strict synchronization of participating devices, although hard to achieve undermobility and highly dynamic channel conditions, are mandatory for an effective AirComp implementation.\nAlthough the AirComp approach can significantly improve the performance of model aggregation for Federated Learning, it may still suffer from unfavorable signal propagation conditions over the wireless links, such as deep fading. Since all local parameters are uploaded via noisy concurrent transmissions, the unfavorable propagation error can prevent from achieving a high accuracy of the aggregated global model. Hence, it is typically not deployed in a stand-alone manner, but instead coupled with other techniques, as discussed in the following.\nD. RECONFIGURABLE INTELLIGENT SURFACES Thanks to their capability of proactively modifying the wireless communication environment, Reconfigurable Intelligent Surfaces (RISs) have become a prominent technology to mitigate a wide range of challenges encountered in wireless networks [95]. More in detail, the large number of low-cost passive reflecting elements of a RIS can adjust the phase shift of the incident signal and thus, altering the propagation of the reflected signal. The signal reflected by the RIS can be constructively superposed with the signal over the direct link to boost the received signal power, by compensating for the power loss over long distances and/or obstructed propagation paths. Moreover, compared with conventional active relays, RISs usually do not require dedicated energy supplies for operation."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "RISs have been extensively leveraged to improve the Federated Learning performance. Battery constraints drive the design of the proposal in [75] where the total transmit power minimization of clients is targeted with the assistance of the RIS, for a sustainable Federated Learning implementation. RISs can also help counteracting the AirComp limitations in a Federated Learning context. The work in [76] proposes a unified framework to jointly optimize RIS configurations and client selection.\nA novel simultaneous access scheme empowered by RIS is proposed in [77] to develop a smart radio environment and hence, to boost the performance of model aggregation. In [78] a RIS is deployed to mitigate the signal magnitude misalignment of AirComp during model aggregation at the server, due\nto a unfavorable propagation environment. Similarly, in [79] multiple geo-distributed RISs are deployed to enhance the parameter aggregation from IoT devices to the base station in an efficient manner.\nA hybrid learning approach is proposed in [80]. There, devices with high computing capabilities are selected to learn locally, whereas the others upload their datasets to the base station for remote aggregation on behalf of them. To enhance spectrum efficiency, both the updatedmodels and the raw data are transmitted concurrently over the simultaneous transmitting and reflecting RIS-assisted multiple access channels."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "The RIS design and deployment per se is still a challenging topic. RISs comprise a large number of reconfigurable phase shifts to be optimized, as well as of devices\u2019 transmit beamformers and of the base station\u2019s receive beamformer, also under imperfect CSI and under fast-varying channel environments. Deep Reinforcement Learning can be applied to properly adapt the RIS configuration by learning about the environment [96].\nE. D2D COMMUNICATIONS AND RELAYING D2D communications have emerged as a promising technology for optimizing spectral efficiency in future cellular networks [97]. They exploit the proximity of devices for efficient utilization of available radio resources, improving data rates, unburdening the network infrastructure and reducing latency and energy consumption. In addition, devices experiencing poor connectivity (e.g., those at the edge of the cell) can forward their transmissions to the base station by establishing a D2D link with a device in proximity acting as a relay. In addition, in the envisioned context, resource-constrained devices can offload training/inference tasks to resource-rich devices in proximity through D2D communications [35]."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "In [81] nodes can exchange small data samples with trusted neighbors, calculate similarities among datasets locally, and report them to the Federated Learning aggregator, so to improve the quality of data spread among clients. The authors in [82] propose a solution aiming to minimize the total delay for the FL model training, by optimizing the radio resource allocation for both D2D data sharing and distributed model training. The work in [83] leverages the cooperation of devices that perform data operations inside the network by iterating local computations and mutual interactions via consensus-based methods. In [38] AirComp is adopted to facilitate the local model consensus in a D2D communication manner.\nInstead of designing a client selection mechanism for Federated Learning, or optimizing resource allocation to balance client participation, the authors in [85] introduce a relaying mechanism that takes into account the nature of individual clients\u2019 connectivity to the aggregator and ensures that,\n52848 VOLUME 11, 2023\nin case of poor connectivity, their local updates are delivered to the aggregator with the help of their neighboring clients acting as relays. A similar approach is foreseen in [87] and [98], where a relay-assisted over-the-air Federated Learning scheme is proposed to counteract the communication straggler issue. With similar purposes, the work in [86] proposes to leverage a UAV as a flying aggregator when no terrestrial base station is available. There, the authors propose to jointly design UAV trajectory and device scheduling, in order to ensure that all devices (also those experiencing poor channel conditions) have the opportunity to successfully participate in distributed training.\nCollaboration among devices in proximity is also beneficially exploited for inference splitting in [84], where close devices exchange data via the Web real-time communication protocol. Similar approaches are surveyed in [99] and references therein."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Despite the huge literature leveraging D2D communications for distributed intelligence solutions, to the best of our knowledge, most of them fail to investigate how D2D communications can actually be designed to match distributed inference needs.\nMoreover, as for other applications, still the exploitation of direct short-range communications, relaying and task offloading among devices in proximity entail the definition of proper incentive mechanisms.\nF. SHORT-PACKET COMMUNICATIONS Control commands and sensor status updates with ultrareliable and low-latency communications requirements are normally conveyed in short packets, in the order of hundreds of bits. Short-packet communications can be also leveraged to transfer inference results, which need to be promptly transmitted to feed into the decision-making process.\nIn short-packet communications, the decoding error probability at a receiver is not negligible, due to a finite block length, and to the fact that both the thermal noise and the channel distortion are not easily averaged. Moreover, the associated packet control information is not negligible compared to the short payload.\nDespite the recent advancements in information theory, several issues still need to be addressed to make the transmission of short packets efficient and reliable [100] and no solution still exists specifically meant to address inference delivery.\nG. MULTICASTING Point-to-multipoint communications are expected to support distributed intelligence. Multiple devices may, in fact, simultaneously need to be queried or to receive data. For instance, multiple clients are instructed by the Federated Learning server to locally perform model training, thus becoming the simultaneous recipients of both initial global model\nand updated parameters. Furthermore, multiple devices, e.g., surveillance cameras in a smart city, may be the simultaneous recipients of updated inference models (e.g., for face recognition).\nProper network primitives (e.g., multicast) are required to efficiently and effectively forward data over radio links towards the nodes involved in the learning/inference process.\nSo far, broadcast communications are mainly assumed, with the exception of a few works that specifically mention multicast interactions, e.g., [101], [102], [103], and [104]. In [101] and [102], a global model is sent in multicast to a set of Federated Learning clients. There, a basic multicasting scheme is envisioned according to which the throughput is assumed to be bounded by that of the client experiencing the worst channel conditions. No further details are provided about how the multicast group is formed and maintained, and how transmissions are actually performed over the radio interface (e.g., over which channel, with which periodicity).\nEfforts should be devoted to practically implement the above procedures, also in compliance with the Third Generation Partnership Project (3GPP) 5G Multicast Broadcast Services (MBS) specifications in Release 17 and beyond [105]. Moreover, improvements over the basic legacy multicasting procedures can be envisioned, e.g., through dynamic subgrouping [106] applied to the Federated Learning clients, in order to speed-up the global model delivery.\nV. ENABLERS AND SOLUTIONS IN THE CORE NETWORK Datasets, models, intermediate/final inference results may need to traverse the continuum, hence entailing a huge traffic to be routed beyond the wireless edge domain across the core network, when properly steered across different end-points.\nIn the following, solutions addressing these issues on top of existing infrastructures are discussed and summarized in Table 4. Some of them, widely known in the 5G context, shall likely be re-engineered to handle distributed intelligence. Others are emerging as key innovations of beyond5G systems and we believe that can serve the aforementioned purpose.\nA. SOFTWARE-DEFINED NETWORKING Routing and forwarding protocols have undergone a deep transformation in recent years owing to the Software-Defined Networking (SDN) paradigm [122]. By decoupling the control plane from the data plane, and moving the former to a logically centralized entity, the Controller, SDN allows abstracting network functions (e.g., routing, load balancing) from the underlying network nodes, which become simple forwarding elements. Thanks to the network-wide view of the Controller about link status and network nodes under its control, sophisticated mechanisms for traffic control and resource management can be more flexibly deployed.\nHuge research efforts have been devoted to improve SDN performance through ML techniques, as surveyed e.g., in [123]. However, only recently, interesting research works have started to address how a programmable control\nVOLUME 11, 2023 52849\nplane, through the SDN paradigm, can support functionalities related to distributed intelligence applications. Some of them are scanned below."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "In [107], the authors devise a solution for Federated Learning, in which the server acts as the auction buyer while the clients function as sellers. There, the SDN Controller is used to create an overlay network in which server and clients can perform auction bidding and product provision. Also, in [108] the potential of SDN Controllers are exploited to orchestrate the optimal forwarding graph for several slices in order to optimize communications associated with Federated Learning in software-defined IoT networks. Instead, the authors of [109] address two key aspects of a mobile IoT network, i.e., security and seamless connectivity for data delivery. They propose to exploit an SDN-assisted Federated Learning approach to predict the users\u2019 demands for a particular content in order to improve content placement decisions. SDN facilitates privacy in the communication channels between Federated Learning controllers."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "The aforementioned works are among the first examples that are emerging from the literature, in which SDN Controllers are designed to support distributed intelligence applications. There is still much to do, since their role in orchestrating the overall distributed intelligence implementation is still marginal. By way of example, two areas are identified in which studies of this kind show great potential.\nThe first field of investigation considers the role of an SDN controller to support a Federated Learning server in the client selection phase. By leveraging SDN, (i) not only the memory and computation capabilities of the clients, but also the delays on the core network path that divides them from the server can be taken into account during the selection phase, and (ii) the load conditions on the core network links can be continuously monitored and dynamically adapted in the view of an improvement in the entire Federated Learning performance.\nThe second area of research considers the capability of the SDN Controller to set multicast forwarding rules inside the switches\u2019 forwarding tables. This will facilitate the\n52850 VOLUME 11, 2023\ndelivery of the same model updates to several clients, whenever point-to-multipoint communications encompass wired links besides the radio interface. Indeed, in recent years, huge efforts have concentrated on designing multicast supported controllers, and the related interfaces to program them, for distributing many types of contents [124]. The full potential of such techniques to support collective communications for distributed intelligence solutions still needs to be unveiled.\nAnother interesting application area is that of in-network learning, in which an SDNController could dynamically support ensemble learning implemented via the deployment of various Weak Learners (WL) within programmable switches. As known, a WL produces a classifier which is only slightly more accurate than random classification. The role of the Controller could be the wise and dynamic routing of data flows between WLs, in order to optimally distribute the learning tasks across a programmable data plane of the core network, see Section V-C.\nB. SEMANTIC ROUTING Making the core network aware of the type of data traffic exchanged among players of the various distributed intelligent processes could make it more supportive for these processes. In this respect, the network could differentiate the handling of packets, exchanged during learning and inference procedures, based on the nature of the carried content and the originating applications. In the literature, there are several solutions to manage packets in a differentiated way. Some of them are designed to be implemented in small and private Internet Protocol (IP) domains, others to be used acrossmultiple domains over the Internet. Some require clean-slate solutions, others can be supported via current Internet extensions or hybrid solutions.\nIn particular, semantic routing represents a promising direction to explore for the case of data exchange related to distributed intelligence applications. It is intended as the process of routing packets that contain IP addresses with additional semantics, possibly using that information to perform policy-based routing or other enhanced routing functions [125].\nDifferent techniques have been proposed which allow flexibly modifying the packet treatment behaviour, i.e., the forwarding decisions. This can be done by either adding information into IP packet headers to adequately instruct network nodes, or by modifying addresses or even interpreting them differently. Several methods are being studied to extend the semantics of IP headers [125]. Unfortunately, to the best of our knowledge, despite the potential advantages deriving from using this approach to identify and optimally treat packets related to distributed intelligence applications, literature on this subject is still lacking, being the semantic routing technology a brand new topic.\nIt is our opinion that IP packets originated by any given distributed intelligence service, if embedded with differentiated semantic information, can bring great benefits to the\nservice itself. For instance, packets belonging to a given dataset can be forwarded towards a node, which is equipped with the right capabilities to run the training procedure upon them. Similarly, an inference output, which is needed for fast decision making, can be sent over a low-latency path; instead, a huge dataset can be carried over a low-congested and highbandwidth path. In the case of model splitting, for example, the instructions contained in the packets\u2019 header could appropriately guide intermediate data from one node to the next, in order to execute different model portions sequentially.\nWhat we believe should be pursued is to create adequate semantics, specific to flexibly support the traffic associated with distributed intelligence, and accordingly redirect it to the optimal endpoint or over the path meeting its service quality requirements [125].\nC. PROGRAMMABLE DATA PLANE The flexibility addressed in the previous two subsections calls for data plane programmability. It entails a network device to expose the low-level packet processing logic to the control plane, through standardized Applications Programming Interfaces (APIs), to be systematically, rapidly, and comprehensively reconfigured.We support the idea that good results could come from a wise joint use of network control plane virtualization (via SDN) and data plane virtualization techniques, for example by using switches or Network Interface Cards (NICs) that are programmable, e.g., via the P4 language [126], [127].\nProgrammable network switches, like other network devices and NICs, have been identified among the main enablers for the transition of intelligence into the data plane [114], [128]. They can play a key role for applications like real-time flow classification and detection of network traffic anomalies, thus acting as on-pathNeural Network (NN) accelerators, which avoid additional data transfer towards purpose-built off-path dedicated hardware."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "In this view, there is a wide body of literature starting from initial works [115], [129], which hypothesized the implementation of NNs and the execution of in-network inference within programmable network devices. Most of the works deal with ML models that run entirely on single network devices [114], [116]. In [114], the authors introduce a framework for in-network classification, in which they map both supervised and unsupervised algorithms to a match-action pipeline, and discuss the applicability of such implementations.\nThe obvious risk is overloading the devices themselves and subtracting resources usually dedicated to packet processing and forwarding, thus reducing their performance levels. For this reason, other interesting works are beginning to appear, such as [117], which proposes distributed in-network intelligence based on distributed NNs, and addresses the challenges of neuron specification, placement, and chaining in\nVOLUME 11, 2023 52851\nswitches, by considering smart network telemetry as a use case. In [118], the focus is on deploying ML trained models for classification, showing how to express them into the P4 language\u2019s primitives and focusing on in-network classifier for an Intrusion Detection System as a proof-of-concept.\nThe work in [119] addresses the network bottleneck issue due to the heavy exchange of model updates in parallel training. Distributed workers send their model updates over the network, where an (integer) aggregation primitive implemented in a programmable data plane switch, sums the updates and distributes only the resulting value. A P4 program distributes aggregation across multiple stages of the switch ingress pipeline. An in-switch aggregation accelerator is also proposed in [120], but to reduce the gradient aggregation overhead in Distributed Reinforcement Learning training.\nIn [121] the authors propose to enhance the data plane of a 3GPP network with the aim to enable in-transit Deep Learning inference services over extended User Plane Functions (UPFs). Required extensions of the control plane and the management plane to support the envisioned data plane modifications are also discussed, e.g., in terms of interfaces and service discovery."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Despite the inherent benefits of in-network acceleration, a particularly relevant open issue is related to the choice of how to split a trained DNN model so that it can best fit data plane functions along a forwarding path. This is addressed by a recent paper [121], which deals with the problem of finding valid strategies for splitting and distributing DNNs within programmable network devices in the light of the growing complexity of large-scale NN models. The research in this field is in its infancy and efforts in this direction are still required.\nD. INFORMATION-CENTRIC NETWORKING Departing from the host-centric IP model that is oblivious of the content of exchanged packets, Named Data Networking (NDN) [130], one of the most prominent Information-centric networking (ICN) instantiations [131], conveys semantic-rich names and attributes in exchanged packets, natively enabling a more conscious data delivery. NDN also natively implements in-network caching and, if properly extended [132], can support in-network processing, thus enabling in-network intelligence at a wide extent.\nOverall, such features make NDN a candidate networking solution to support distributed intelligence workloads. Clearly, the support of distributed intelligence requires additional functionalities in the NDN data plane, besides forwarding."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "In [110] it was early argued that through native in-network caching and name-based forwarding, NDN can facilitate the orchestration of distributed AI components. The potential benefits are more extensively discussed in [111].\nIn-network caching can play a crucial role for both training and inference phases. Inference results can be cached if deemed of interest for several end-points, e.g., a given traffic sign on a driving lane that needs to be detected by multiple vehicles. Also data for training or intermediate trained results (e.g., in case of model splitting) can be cached to quickly recover from packet losses, which may occur over unreliable and/or congested links.\nThe authors in [112] propose NDN to improve client discovery and data exchange procedures in Federated Learning. An expressive and flexible naming scheme allows declaring the capabilities of heterogeneous clients in a uniform semantic-rich manner. Moreover, it is proven that multicast data delivery and in-network caching save precious network resources. This is especially useful when exchanging huge global models over potentially congested and/or bandwidthlimited lossy backhaul links. The potential of ICN in Federated Learning is also investigated in [113], where it is coupled with the Kafka publish/subscribe framework to support Internet of Vehicles applications.\nFinally, ICN can be combined with SDN to realize a service-centric architecture for in-network intelligence orchestration, as argued in [14]."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Initial attempts to effectively support distributed intelligence applications by leveraging disruptive technologies to enhance the IP data plane exist. However, it is well known that the practical large-scale deployment of disruptive ICN-based solutions is still far from being a reality [133], unless to consider greenfield environments.\nMoreover, there is much room for further theoretical investigations. For example, model popularity can be used to support caching decisions. In this respect, a popular trained model can be cached into those network nodes that are more likely to be traversed by input data, and the model placement can be dynamically updated based on requests [7]. Overall, novel caching policies should be defined tomatch the delivery requirements of distributed inference and learning tasks.\nVI. END-TO-END ORCHESTRATION AND MANAGEMENT The design of effective network ecosystems meeting the requirements of distributed intelligence goes well beyond the scope of the radio access and core network solutions scanned in the previous sections, and entails further actions. Whenever an intelligent application, built upon AI/ML learning/inference, requests the execution of a workload, its lifecycle management (i.e., initial configuration, placement, maintenance, update, delete, etc.) must be performed.\nSome instantiations of such solutions are discussed in this Section and summarized in Table 5.\nA. COMPUTING AND COMMUNICATION CO-DESIGN & ORCHESTRATION Policies are needed to jointly orchestrate and manage computing, caching, and communication (3C) resources across\n52852 VOLUME 11, 2023\nthe cloud-to-things continuum in order to optimize distributed AI/ML workloads [11], [59].\nJoint 3C orchestration for distributedMLworkloads unveil specific challenges compared to more traditional orchestration problems. On the one hand, the seamless support of endto-end distributed pipelines of AI components cannot rely on the conventional orchestration of the computing infrastructure, designed orthogonally to the network architecture. In fact, if data exchange occurs in myopic manner, then\nthe network risks becoming the bottleneck for distributed intelligence. On the other hand, the policies meant to decide where to place a given intelligence task should not only minimize data collection latency, computation times and/or energy consumption, but also provide the targeted accuracy in training and inference.\nIn this regard, a key issue is the choice of the proper model instance to perform a given learning/inference task, among the multiple ones that may be available throughout the\nVOLUME 11, 2023 52853\ncontinuum. Lighter model versions can be run by constrained end-devices at the expenses of lower accuracy, while more sophisticated models can be available at large data centers.\nAdditionally, according to the deployed distributed intelligence application, the following possible decisions may need to be taken, e.g., when and where a model (re-)training procedure shall be triggered; which clients shall be selected for a Federated Learning task; among which nodes and where an inference/training model shall be split."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "The work in [134] proposes to employ online reinforcement learning to orchestrate Deep Learning services for multi-users over the end-edge-cloud system, in order to better understand the system dynamics. The offloading policy aims to minimize response time while providing sufficient accuracy.\nIn [7] the authors propose the concept of inference delivery networks, i.e., networks of computing nodes that coordinate to perform inference tasks. The proposal targets the following problems: (i) where placing the models for serving a certain inference task among a set of nodes, and (ii) how selecting their size/complexity among the available alternatives, while achieving the best trade-off between latency and accuracy. Similarly, the work in [135] proposes a framework allowing to jointly decide (i) which data using for learning, (ii)which DNN structure employing, and (iii)which physical nodes, and resources therein, using. The proposal aims at minimizing the energy consumption while meeting a target maximum learning time and desired learning quality. Unlike the work in [7], the latter targets the more challenging and resource-intensive learning phase, and splits a model across multiple devices.\nThe authors in [136] propose a solution, which aims to jointly accelerate model training time and minimize energy consumption in resource-constrained IoT devices, through the proper selection of model splitting points. It accounts for the time-varying network throughput and the computing resources of involved devices. Model splitting is also the focus in [137], where an inference model is partitioned in a 5G infrastructure across the end devices, multiple edge server and the cloud, in order to minimize the inference latency.\nOrchestrating computing and communication resources is also crucial for Federated Learning. Indeed, client selection is critical to determine the training time and model accuracy, and it cannot be performed by overlooking (wireless) channel conditions and its dynamics and clients\u2019 computing/battery capabilities. Several client selection schemes have been proposed, e.g., [101] and [145], which consider the aforementioned criteria. However, tighter coupling is required between client selection and related resource assignment (e.g., transmission power, radio resource blocks), which should be performed in a joint manner. In [138], a joint client selection and resource allocation policy is proposed for Federated Learning under communication limitations and imperfect CSI.\nIn general, selecting more clients may reduce the overall training latency. Optimizing the tradeoff betweenmaximizing the number of clients and minimizing the overall energy consumption is the target of the study in [139]. There, appropriate resources, in terms of CPU frequency and transmission power, are allocated to the selected clients. An algorithm is proposed in [140] that jointly selects the best clients and allocates the right amount of bandwidth at each round, by considering their data, computing power, and channel gain. The algorithm can be implemented as an application in the open radio access network (O-RAN) architecture, to collect information about the data (e.g., the numbers of data and data classes) as well as the channel gain, and the available computing power of potential clients.\nIn [141] the co-existence is considered of multiple Federated Learning services sharing common wireless resources. A two-level resource allocation framework is proposed, which aims to minimize the round length by optimizing bandwidth allocation among the clients of each Federated Learning service, and distributing bandwidth resources among multiple simultaneous services."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Despite the aforementioned works, in the recent literature the coupling of distributed intelligence orchestration with the network data plane is still loose. For instance, in case ofmodel splitting, decisions shall be taken alongwith the configuration of the proper routing path across the nodes selected to host the different NN layers. The interplay of orchestration mechanisms with SDN routing policies is highly recommended.\nA more holistic approach for jointly orchestrating network and computing resources can build upon network slicing, with a slice tailored to the specific demands of distributed AI applications, while sharing the (programmable) network infrastructure with other services [146].\nAnother interesting perspective is early discussed in [147]. Understanding the quality of input data (e.g., subject to anomalies, discrepancies, and noisy components) can be exploited for selective sensing, in order to reduce the input data volume to be transferred over the network. There, in the healthcare context, the potentials of a cross-layered sense-compute co-optimization are presented to jointly improve sensing, computation, and communication aspects of ML-based applications over the end-edge-cloud continuum.\nB. VIRTUALIZATION The aforementioned decisions cannot be taken without collecting sufficient information about capabilities of end-devices and network nodes, as well as about AI/ML models. For instance, to enable smart selection of the best Federated Learning clients [101], [145], candidate devices can be asked by the aggregator to provide information about their capabilities as well as the experienced link quality. The same holds for the identification of the nodes which should host a partitioned model in case of splitting.\n52854 VOLUME 11, 2023\nVirtualization techniques, initially proposed in the Information Technology domain, can serve this purpose, being recently inherited within the IoT context [148] and evolving towards the broader digital twin concept."
        },
        {
            "heading": "1) STATE-OF-THE-ART",
            "text": "In [142] a virtualization layer hosted at the network edge is proposed, which is in charge of the semantic description of AI-embedded IoT devices\u2019 capabilities. The virtual replicas expose and augment the cognitive capabilities of the corresponding (potentially constrained) physical devices, in order to feed intelligent IoT applications. Properly customized data models defined according to the Open Mobile Alliance (OMA) LightweightMachine-to-Machine (LwM2M) semantics [149] are leveraged to this purpose. The choice falls into OMA LwM2M being it specifically conceived for resourceconstrained devices. A similar approach is followed in [143] for the semantic description of the client capabilities in an edge-based Federated Learning context involving IoT devices as learners.\nThe authors in [59] propose the use of digital twins as virtual replica of a distributed training system. They can provide a platform for real-time analysis to learn the peculiarities of the system, and target inefficiencies in communication.\nThe need for joint 3C and learning (3C-L) resource allocation approaches is theoretically discussed in [144], where a liquid model for 6G is proposed. According to the vision discussed there, 3C-L resources provided by edge nodes may be virtualized, also with the support of digital twins, with the aim of improving resource pooling."
        },
        {
            "heading": "2) LESSONS LEARNT AND ROAD AHEAD",
            "text": "Different players involved in the network design and deployment for supporting distributed intelligence should achieve a consensus on the proper data models/semantics to adopt. Furthermore, although they foster the crucial issue of interoperability and enable more judicious orchestration policies, digital replicas of AI components still may increase the communication load to keep the state of their physical counterparts in real-time. Thus, their instantiation and whole lifecycle management need to be carefully planned.\nVII. STANDARDIZATION INITIATIVES AND PROJECTS Synergies among network-related standardization activities and AI/ML forums are advocated to accelerate advancements in both domains.\nThe International Telecommunications Unit (ITU) Focus Group on Technologies for Network 2030 (FG-NET2030) pioneeringly identified the pervasive distribution of AI as a crucial use case for future networks [150].\nEfforts are underway within the Internet Research Task Force (IRTF) COmputing in the Network Research Group (coinrg) to push new network protocol designs to efficiently federate decentralized computing resources, also to support emerging AI/ML workloads [151].\nThe one6G Association outlines distributed federated AI among the key enabling technologies that constitute the pillars of the evolution towards 6G [152].\nFurther initiatives are encouraged which recognize the need for a novel network design to satisfy the AI demands and to optimize the AI performance, and not the other way around only. Indeed, the investigation of AI to optimize the network performance are targeted, for instance, by ITU [153] and 3GPP [154].\nAlong with the advocated path, 3GPP prospective work items for the upcoming 5G Release 18 are aimed to support AI applications. For instance, the document in [155] covers use cases and potential requirements for 5G system to support AI/ML model distribution and transfer (download, upload, updates, etc.). This is an interesting contribution coming from a standardization group, but still too germinal.\nDynamic distribution of intelligence is the subjectmatter of several recent projects funded by the European Commission. For instance, this is the case of the DAIS [156] and DEDICAT 6G [157] projects. However, a few of them, among which the AI@EDGE project [158], claims to address network design for AI.\nVIII. MAIN FINDINGS, GUIDELINES AND OPEN ISSUES A. FINDINGS AND GUIDELINES The conducted analysis unveils that, despite the infancy of the distributed intelligence topic, a huge amount of works have been published in the literature. Starting from it, the following main considerations can be summarized."
        },
        {
            "heading": "1) RESEARCH SOLUTIONS IN THE RAN DEFINITELY OUTNUMBER THOSE IN THE CORE NETWORK",
            "text": "It can be observed that the design of solutions specifically targeting the wireless (edge) domain mainly catalyzed the interest of the networking community. This trend was somehow expected, given the wireless channel dynamics and the resource-constrained nature of end-devices. Moreover, this is also due to the fact that most of the distributed intelligence deployments consider a two-layer architecture involving enddevices and an edge server only. However, in the near future the compute continuum will become a reality and even network nodes could contribute to (portion of) training and inference procedures. Hence, additional efforts are required, on the one hand, to improve distributed intelligence-related traffic steering, and on the other hand, to allow in-network NN execution, without penalizing forwarding procedures."
        },
        {
            "heading": "2) COUPLING OF DIFFERENT COMMUNICATION/NETWORKING TECHNIQUES IS HIGHLY ADVISED",
            "text": "It can be further stated that to effectively overstep the limitations of wireless communications, different techniques need to be blended to improve the performance of distributed intelligence solutions over the RAN. This is the case of AirComp coupled with either RIS or D2D communications\nVOLUME 11, 2023 52855\nand relaying. Synergies between SDN and ICN appear also extremely promising."
        },
        {
            "heading": "3) NETWORK SOLUTIONS SO FAR MAINLY TARGETED FEDERATED LEARNING AND, MORE IN GENERAL, LEARNING PROCEDURES",
            "text": "We noticed that the majority of works focused on the Federated Learning approach, given the inherent benefits it promises and, likely, the push from the industries. However, although less computation- and bandwidth-hungry, also inference will be massively and frequently requested, and it would likely be pervasive for the network, likely as IoT was in the last decade. Hence, inference orchestration and network optimization solutions for it should be conceived."
        },
        {
            "heading": "4) SEVERAL SOLUTIONS, ALTHOUGH PROMISING, ARE STILL GERMINAL",
            "text": "Not all the solutions have the samematurity level, as shown in Fig. 2, where the identified enablers are graphically sketched to provide an end-to-end perspective of a future network supporting distributed intelligence.\nIn the RAN, there is much room for the design of D2D communication techniques specifically treating distributed intelligence-related data. The same comment holds for shortpacket, multicast and semantic communications.\nFurthermore, robust solutions against mobility of devices are needed in the RAN segment. For instance, predicting the mobility of source devices may be crucial to improve data (i.e., datasets, model updates) collection, making it reliable also through opportunistic procedures [13].\nIn the core network segment, we identified semantic routing as a prominent enabler for distributed intelligence, but to the best of our knowledge, nowork is available yet.Moreover, despite the maturity of the SDN paradigm per se, its potential in dealing with distributed intelligence is still overlooked.\nOverall, whatever the network segment, a more conscious distributed intelligence-related data delivery and network procedures adaptation (including \u2018casting\u2019 primitives) to their peculiar needs and features would make the difference."
        },
        {
            "heading": "5) CO-DESIGN OF DISTRIBUTED INTELLIGENCE AND NETWORK OPERATIONS IS MANDATORY",
            "text": "The model splitting point decision, as well as the Federated Learning client selection cannot disregard the (wireless) network dynamics. The behaviour of radio resource allocation schemes and routing protocols should be tightly coupled with the decision concerning the distributed intelligence deployment to appropriately trade-off among accuracy, latency and bandwidth performance."
        },
        {
            "heading": "6) REALISTIC AND COMPREHENSIVE EVALUATION FRAMEWORKS ARE MISSING",
            "text": "A further limitation identified while scanning the literature is the lack of evaluation platforms which couple accurate and realistic simulators, focusing on link/network-layer performance, with the validation of model quality through realistic datasets. Results are typically achieved separately or by loosing in accuracy of the achieved results. A few works target experimental test-beds; although being quite representative of realistic deployments, they barely scale to up to\n52856 VOLUME 11, 2023\nhundreds/thousands of devices, which represent the more likely case for several distributed intelligence (especially learning) solutions.\nB. ADDITIONAL OPEN ISSUES For the sake of completeness, some socio-economic aspects are also worth discussing which deserve further investigations, although outside the main scope of the manuscript."
        },
        {
            "heading": "1) SECURITY",
            "text": "Distributed AI services further challenge the design of security and privacy-preservingmechanisms. Therefore, to ensure a broad social acceptability of the paradigm, mutual trustworthiness among involved entities, proper access control and authentication schemes for end-users, and secure routing mechanisms need to be enforced, considering that raw (sensitive) data, model (parameters) and intermediate outputs need to be exchanged."
        },
        {
            "heading": "2) SUSTAINABILITY",
            "text": "It is important to consider implications related to environmental sustainability. The overall AI lifecycle should be devised to be distributed throughout the continuum with an eye to the carbon footprint reduction [159]. To this aim, for instance, orchestration should opportunistically deploy AI workloads where green sources are available, increasingly leveraged by telco and cloud providers to power their infrastructures."
        },
        {
            "heading": "3) BUSINESS MODELS",
            "text": "As for business-related issues, the provisioning of distributed AI services on top of existing networks also entails revising the traditional value chain. Either providers of AI-based applications may interact with network operators and cloud/edge providers to offer the aforementioned services, or new operators may enter the scene to offer such kind of distributed and possibly green AI services. Solid business models should be conceived accordingly, which may provide new revenue opportunities and stimulate cooperation among all players in the envisioned ecosystem.\nIX. CONCLUSIVE REMARKS In this paper, we first identified the most representative distributed intelligence solutions and the main relevant issues, with special focus on networking aspects. Then, we provided a comprehensive and end-to-end analysis of the key enablers, from the RAN to the core, for the design of a future network supporting distributed intelligence. Despite the infancy of the topic, several research works can be found in the literature. This paper focused on classifying the most representative solutions for each identified enabler, without having the claim to be exhaustive, but with the aim to provide a valuable support to newcomers to the topic as well as to experienced researchers both from the AI and the networking communities.\nIndeed, the intriguing challenges for building future network ecosystems supporting distributed intelligence are\nmultidisciplinary and span different research areas having different maturity levels. Hence, synergies among different communities need to be fostered.\nREFERENCES [1] Cisco Annual Internet Report 2018\u20132023, Cisco, San Jose, CA, USA,\n2020. [2] J. Wan, J. Yang, Z. Wang, and Q. Hua, \u2018\u2018Artificial intelligence for cloud-\nassisted smart factory,\u2019\u2019 IEEE Access, vol. 6, pp. 55419\u201355430, 2018. [3] T. Huynh-The, Q.-V. Pham, X.-Q. Pham, T. T. Nguyen, Z. Han, and\nD.-S. Kim, \u2018\u2018Artificial intelligence for the metaverse: A survey,\u2019\u2019 Eng. Appl. Artif. Intell., vol. 117, Jan. 2023, Art. no. 105581. [4] K. Muhammad, A. Ullah, J. Lloret, J. D. Ser, and V. H. C. de Albuquerque, \u2018\u2018Deep learning for safe autonomous driving: Current challenges and future directions,\u2019\u2019 IEEE Trans. Intell. Transp. Syst., vol. 22, no. 7, pp. 4316\u20134336, Jul. 2021. [5] G. Rong, A. Mendez, E. Bou Assi, B. Zhao, and M. Sawan, \u2018\u2018Artificial intelligence in healthcare: Review and prediction case studies,\u2019\u2019Engineering, vol. 6, no. 3, pp. 291\u2013301, Mar. 2020. [6] J. Verbraeken, \u2018\u2018A survey on distributed machine learning,\u2019\u2019 ACM Comput. Surv., vol. 53, no. 2, pp. 1\u201333, 2020. [7] T. S. Salem, G. Castellano, G. Neglia, F. Pianese andA. Araldo, \u2018\u2018Towards inference delivery networks: Distributing machine learning with optimality guarantees,\u2019\u2019 in Proc. 19th Medit. Commun. Comput. Netw. Conf. (MedComNet), Jun. 2021, pp. 1\u20138. [8] Z. Cheng, X. Fan, M. Liwang, M. Min, X. Wang, and X. Du, \u2018\u2018Hybrid architectures for distributed machine learning in heterogeneous wireless networks,\u2019\u2019 2022, arXiv:2206.01906. [9] D. Rosendo, A. Costan, P. Valduriez, and G. Antoniu, \u2018\u2018Distributed intelligence on the edge-to-cloud continuum: A systematic literature review,\u2019\u2019 J. Parallel Distrib. Comput., vol. 166, pp. 71\u201394, Aug. 2022. [10] Y. Matsubara, M. Levorato, and F. Restuccia, \u2018\u2018Split computing and early exiting for deep learning applications: Survey and research challenges,\u2019\u2019 ACM Comput. Surveys, vol. 55, no. 5, pp. 1\u201330, May 2023. [11] S. Talwar, N. Himayat, H. Nikopour, F. Xue, G.Wu, and V. Ilderem, \u2018\u20186G: Connectivity in the era of distributed intelligence,\u2019\u2019 IEEE Commun. Mag., vol. 59, no. 11, pp. 45\u201350, Nov. 2021. [12] F. R. Yu, \u2018\u2018From information networking to intelligence networking: Motivations, scenarios, and challenges,\u2019\u2019 IEEE Netw., vol. 35, no. 6, pp. 209\u2013216, Nov. 2021. [13] J. Pan, L. Cai, S. Yan, and X. S. Shen, \u2018\u2018Network for AI and AI for network: Challenges and opportunities for learning-oriented networks,\u2019\u2019 IEEE Netw., vol. 35, no. 6, pp. 270\u2013277, Nov. 2021. [14] X. Li, R. Xie, F. R. Yu, T. Huang, and Y. Liu, \u2018\u2018Advancing softwaredefined service-centric networking toward in-network intelligence,\u2019\u2019 IEEE Netw., vol. 35, no. 5, pp. 210\u2013218, Sep. 2021. [15] K. B. Letaief, Y. Shi, J. Lu, and J. Lu, \u2018\u2018Edge artificial intelligence for 6G: Vision, enabling technologies, and applications,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 40, no. 1, pp. 5\u201336, Jan. 2022. [16] M. Chen, D. G\u00fcnd\u00fcz, K. Huang, W. Saad, M. Bennis, A. V. Feljan, and H. V. Poor, \u2018\u2018Distributed learning in wireless networks: Recent progress and future challenges,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 39, no. 12, pp. 3579\u20133605, Dec. 2021. [17] S. Ouyang, D. Dong, Y. Xu, and L. Xiao, \u2018\u2018Communication optimization strategies for distributed deep neural network training: A survey,\u2019\u2019 J. Parallel Distrib. Comput., vol. 149, pp. 52\u201365, Mar. 2021. [18] X. Cao, T. Basar, S. Diggavi, Y. C. Eldar, K. B. Letaief, H. V. Poor, and J. Zhang, \u2018\u2018Communication-efficient distributed learning: An overview,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 41, no. 4, pp. 851\u2013873, Apr. 2023. [19] Z. Zhang, C. Chang, H. Lin, Y. Wang, R. Arora, and X. Jin, \u2018\u2018Is network the bottleneck of distributed training?\u2019\u2019 in Proc. Workshop Netw. Meets AI ML, Aug. 2020, pp. 8\u201313. [20] M. C. Luizelli, R. Canofre, A. F. Lorenzon, F. D. Rossi, W. Cordeiro, and O. M. Caicedo, \u2018\u2018In-network neural networks: Challenges and opportunities for innovation,\u2019\u2019 IEEE Netw., vol. 35, no. 6, pp. 68\u201374, Nov. 2021. [21] Y. Sun,M. Peng, Y. Zhou, Y. Huang, and S.Mao, \u2018\u2018Application ofmachine learning in wireless networks: Key techniques and open issues,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 21, no. 4, pp. 3072\u20133108, 4th Quart., 2019. [22] S. Hu, X. Chen, W. Ni, E. Hossain, and X. Wang, \u2018\u2018Distributed machine learning for wireless communication networks: Techniques, architectures, and applications,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 23, no. 3, pp. 1458\u20131493, 3rd Quart., 2021.\nVOLUME 11, 2023 52857\n[23] H. Yang, A. Alphones, Z. Xiong, D. Niyato, J. Zhao, and K. Wu, \u2018\u2018Artificial-intelligence-enabled intelligent 6G networks,\u2019\u2019 IEEE Netw., vol. 34, no. 6, pp. 272\u2013280, Nov. 2020. [24] D. M. Gutierrez-Estevez, M. Gramaglia, A. D. Domenico, G. Dandachi, S. Khatibi, D. Tsolkas, I. Balan, A. Garcia-Saavedra, U. Elzur, and Y. Wang, \u2018\u2018Artificial intelligence for elastic management and orchestration of 5G networks,\u2019\u2019 IEEE Wireless Commun., vol. 26, no. 5, pp. 134\u2013141, Oct. 2019. [25] S. Zhang and D. Zhu, \u2018\u2018Towards artificial intelligence enabled 6G: State of the art, challenges, and opportunities,\u2019\u2019 Comput. Netw., vol. 183, Dec. 2020, Art. no. 107556. [26] M. Giordani, M. Polese, M. Mezzavilla, S. Rangan, and M. Zorzi, \u2018\u2018Toward 6G networks: Use cases and technologies,\u2019\u2019 IEEE Commun. Mag., vol. 58, no. 3, pp. 55\u201361, Mar. 2020. [27] J. Dean, G. Corrado, R.Monga, K. Chen, M. Devin, M.Mao,M. Ranzato, A. Senior, P. Tucker, K. Yang, Q. Le, and A. Ng, \u2018\u2018Large scale distributed deep networks,\u2019\u2019 in Proc. Adv. Neural Inf. Process. Syst., vol. 25, 2012, pp. 1\u201312. [28] J. Chen and X. Ran, \u2018\u2018Deep learning with edge computing: A review,\u2019\u2019 Proc. IEEE, vol. 107, no. 8, pp. 1655\u20131674, Aug. 2019. [29] X. Wang, Y. Han, V. C. M. Leung, D. Niyato, X. Yan, and X. Chen, \u2018\u2018Convergence of edge computing and deep learning: A comprehensive survey,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 22, no. 2, pp. 869\u2013904, 2nd Quart., 2020. [30] J. Chen, K. Li, Q. Deng, K. Li, and P. S. Yu, \u2018\u2018Distributed deep learning model for intelligent video surveillance systems with edge computing,\u2019\u2019 IEEE Trans. Ind. Informat., early access, Apr. 4, 2019, doi: 10.1109/TII.2019.2909473. [31] O. Gupta and R. Raskar, \u2018\u2018Distributed learning of deep neural network over multiple agents,\u2019\u2019 J. Netw. Comput. Appl., vol. 116, pp. 1\u20138, Aug. 2018. [32] E. Li, Z. Zhou, and X. Chen, \u2018\u2018Edge intelligence: On-demand deep learning model co-inference with device-edge synergy,\u2019\u2019 in Proc. Workshop Mobile Edge Commun., Aug. 2018, pp. 31\u201336. [33] J. Shao and J. Zhang, \u2018\u2018Communication-computation trade-off in resource-constrained edge inference,\u2019\u2019 IEEE Commun. Mag., vol. 58, no. 12, pp. 20\u201326, Dec. 2020. [34] H. Li, C. Hu, J. Jiang, Z. Wang, Y. Wen, and W. Zhu, \u2018\u2018JALAD: Joint accuracy-and latency-aware deep structure decoupling for edgecloud execution,\u2019\u2019 in Proc. IEEE 24th Int. Conf. Parallel Distrib. Syst. (ICPADS), Dec. 2018, pp. 671\u2013678. [35] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, \u2018\u2018Edge intelligence: Paving the last mile of artificial intelligence with edge computing,\u2019\u2019 Proc. IEEE, vol. 107, no. 8, pp. 1738\u20131762, Aug. 2019. [36] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Y. Arcas, \u2018\u2018Communication-efficient learning of deep networks from decentralized data,\u2019\u2019 in Proc. Artif. Intell. Statist., 2017, pp. 1273\u20131282. [37] M. Aledhari, R. Razzak, R. M. Parizi, and F. Saeed, \u2018\u2018Federated learning: A survey on enabling technologies, protocols, and applications,\u2019\u2019 IEEE Access, vol. 8, pp. 140699\u2013140725, 2020. [38] Y. Shi, Y. Zhou, and Y. Shi, \u2018\u2018Over-the-air decentralized federated learning,\u2019\u2019 in Proc. IEEE Int. Symp. Inf. Theory (ISIT), Jul. 2021, pp. 455\u2013460. [39] E. T. M. Beltr\u00e1n, M. Q. P\u00e9rez, P. M. S. S\u00e1nchez, S. L. Bernal, G. Bovet, M. G. P\u00e9rez, G. M. P\u00e9rez, and A. H. Celdr\u00e1n, \u2018\u2018Decentralized federated learning: Fundamentals, state-of-the-art, frameworks, trends, and challenges,\u2019\u2019 2022, arXiv:2211.08413. [40] L. Barbieri, S. Savazzi, M. Brambilla, and M. Nicoli, \u2018\u2018Decentralized federated learning for extended sensing in 6G connected vehicles,\u2019\u2019 Veh. Commun., vol. 33, Jan. 2022, Art. no. 100396. [41] Y. Xiao, Y. Ye, S. Huang, L. Hao, Z. Ma, M. Xiao, S. Mumtaz, and O. A. Dobre, \u2018\u2018Fully decentralized federated learning-based on-board mission for UAV swarm system,\u2019\u2019 IEEE Commun. Lett., vol. 25, no. 10, pp. 3296\u20133300, Oct. 2021. [42] C. Thapa, P. C. M. Arachchige, S. Camtepe, and L. Sun, \u2018\u2018SplitFed:When federated learning meets split learning,\u2019\u2019 in Proc. AAAI Conf. Artif. Intell., vol. 36, no. 8, 2022, pp. 8485\u20138493. [43] A. Sufian, A. Ghosh, A. S. Sadiq, and F. Smarandache, \u2018\u2018A survey on deep transfer learning to edge computing for mitigating the COVID-19 pandemic,\u2019\u2019 J. Syst. Archit., vol. 108, Sep. 2020, Art. no. 101830. [44] L. Valerio, A. Passarella, and M. Conti, \u2018\u2018Accuracy vs. traffic trade-off of learning IoT data patterns at the edge with hypothesis transfer learning,\u2019\u2019 in Proc. IEEE 2nd Int. Forum Res. Technol. Soc. Ind. Leveraging Better Tomorrow (RTSI), Sep. 2016, pp. 1\u20136.\n[45] K. I. Wang, X. Zhou, W. Liang, Z. Yan, and J. She, \u2018\u2018Federated transfer learning based cross-domain prediction for smart manufacturing,\u2019\u2019 IEEE Trans. Ind. Informat., vol. 18, no. 6, pp. 4088\u20134096, Jun. 2022. [46] S. J. Pan and Q. Yang, \u2018\u2018A survey on transfer learning,\u2019\u2019 IEEE Trans. Knowl. Data Eng., vol. 22, no. 10, pp. 1345\u20131359, Oct. 2009. [47] Y. Wang, M. Damani, P. Wang, Y. Cao, and G. Sartoretti, \u2018\u2018Distributed reinforcement learning for robot teams: A review,\u2019\u2019 2022, arXiv:2204.03516. [48] F. Venturini, F. Mason, F. Pase, F. Chiariotti, A. Testolin, A. Zanella, and M. Zorzi, \u2018\u2018Distributed reinforcement learning for flexible and efficient UAV swarm control,\u2019\u2019 IEEE Trans. Cognit. Commun. Netw., vol. 7, no. 3, pp. 955\u2013969, Sep. 2021. [49] M. Spryn, A. Sharma, D. Parkar, and M. Shrimal, \u2018\u2018Distributed deep reinforcement learning on the cloud for autonomous driving,\u2019\u2019 in Proc. IEEE/ACM 1st Int. Workshop Softw. Eng. for AI Auto. Syst. (SEFAIAS), May 2018, pp. 16\u201322. [50] T. Chen, K. Zhang, G. B. Giannakis, and T. Basar, \u2018\u2018Communicationefficient policy gradient methods for distributed reinforcement learning,\u2019\u2019 IEEE Trans. Control Netw. Syst., vol. 9, no. 2, pp. 917\u2013929, Jun. 2022. [51] N. Tonellotto, A. Gotta, F.M. Nardini, D. Gadler, and F. Silvestri, \u2018\u2018Neural network quantization in federated learning at the edge,\u2019\u2019 Inf. Sci., vol. 575, pp. 417\u2013436, Oct. 2021. [52] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, and A. Askell, \u2018\u2018Language models are few-shot learners,\u2019\u2019 inProc. Adv. Neur. Inf. Process. Sys., vol. 33, 2020, pp. 1877\u20131901. [53] A. Imteaj, U. Thakker, S. Wang, J. Li, and M. H. Amini, \u2018\u2018A survey on federated learning for resource-constrained IoT devices,\u2019\u2019 IEEE Internet Things J., vol. 9, no. 1, pp. 1\u201324, Jan. 2022. [54] T. Zeng, O. Semiari, M. Chen, W. Saad, and M. Bennis, \u2018\u2018Federated learning on the road autonomous controller design for connected and autonomous vehicles,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 21, no. 12, pp. 10407\u201310423, Dec. 2022. [55] Y. Liu, J. Nie, X. Li, S. H. Ahmed,W. Y. B. Lim, and C.Miao, \u2018\u2018Federated learning in the sky: Aerial-ground air quality sensing framework with UAV swarms,\u2019\u2019 IEEE Internet Things J., vol. 8, no. 12, pp. 9827\u20139837, Jun. 2021. [56] A. Xie and Y. Peng, \u2018\u2018Improving the quality of inference for applications using chained DNN models during edge server handover,\u2019\u2019 in Proc. IEEE/ACM 7th Symp. Edge Comput. (SEC), Dec. 2022, pp. 516\u2013520. [57] E. Ramos, R. Morabito, and J. Kainulainen, \u2018\u2018Distributing intelligence to the edge and beyond [research Frontier],\u2019\u2019 IEEE Comput. Intell. Mag., vol. 14, no. 4, pp. 65\u201392, Nov. 2019. [58] X. Qi and C. Liu, \u2018\u2018Enabling deep learning on IoT edge: Approaches and evaluation,\u2019\u2019 in Proc. IEEE/ACM Symp. Edge Comput. (SEC), Oct. 2018, pp. 367\u2013372. [59] C. Mwase, Y. Jin, T. Westerlund, H. Tenhunen, and Z. Zou, \u2018\u2018Communication-efficient distributed AI strategies for the IoT edge,\u2019\u2019 Future Gener. Comput. Syst., vol. 131, pp. 292\u2013308, Jun. 2022. [60] C. Wu, F. Wu, L. Lyu, Y. Huang, and X. Xie, \u2018\u2018Communication-efficient federated learning via knowledge distillation,\u2019\u2019 Nature Commun., vol. 13, no. 1, pp. 1\u20138, Apr. 2022. [61] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, \u2018\u2018Communicationefficient edge AI: Algorithms and systems,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 22, no. 4, pp. 2167\u20132191, 4th Quart., 2020. [62] J. Park, S. Samarakoon, A. Elgabli, J. Kim, M. Bennis, S. Kim, and M. Debbah, \u2018\u2018Communication-efficient and distributed learning over wireless networks: Principles and applications,\u2019\u2019 Proc. IEEE, vol. 109, no. 5, pp. 796\u2013819, May 2021. [63] D. Liu, G. Zhu, J. Zhang, and K. Huang, \u2018\u2018Data-importance aware user scheduling for communication-efficient edge machine learning,\u2019\u2019 IEEE Trans. Cognit. Commun. Netw., vol. 7, no. 1, pp. 265\u2013278, Mar. 2021. [64] D. Liu, G. Zhu, Q. Zeng, J. Zhang, and K. Huang, \u2018\u2018Wireless data acquisition for edge learning: Data-importance aware retransmission,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 20, no. 1, pp. 406\u2013420, Jan. 2021. [65] Y. He, J. Ren, G. Yu, and J. Yuan, \u2018\u2018Importance-aware data selection and resource allocation in federated edge learning system,\u2019\u2019 IEEE Trans. Veh. Technol., vol. 69, no. 11, pp. 13593\u201313605, Nov. 2020. [66] H. H. Yang, A. Arafa, T. Q. S. Quek, and H. Vincent Poor, \u2018\u2018Agebased scheduling policy for federated learning in mobile edge networks,\u2019\u2019 in Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), May 2020, pp. 8743\u20138747.\n52858 VOLUME 11, 2023\n[67] H. Sun, X. Ma, and R. Q. Hu, \u2018\u2018Adaptive federated learning with gradient compression in uplink NOMA,\u2019\u2019 IEEE Trans. Veh. Technol., vol. 69, no. 12, pp. 16325\u201316329, Dec. 2020. [68] X.Ma, H. Sun, and R. Q. Hu, \u2018\u2018Scheduling policy and power allocation for federated learning in NOMA based MEC,\u2019\u2019 in Proc. GLOBECOM IEEE Global Commun. Conf., Dec. 2020, pp. 1\u20137. [69] P. S. Bouzinis, P. D. Diamantoulakis, and G. K. Karagiannidis, \u2018\u2018Wireless federated learning (WFL) for 6G networks\u2014Part II: The computethen-transmit NOMA paradigm,\u2019\u2019 IEEE Commun. Lett., vol. 26, no. 1, pp. 8\u201312, Jan. 2022. [70] I. Mrad, R. Hamila, A. Erbad, and M. Gabbouj, \u2018\u2018Joint learning and optimization for federated learning in NOMA-based networks,\u2019\u2019 Pervas. Mobile Comput., vol. 89, Feb. 2023, Art. no. 101739. [71] Y. Wu, Y. Song, T. Wang, L. Qian, and T. Q. S. Quek, \u2018\u2018Non-orthogonal multiple access assisted federated learning via wireless power transfer: A cost-efficient approach,\u2019\u2019 IEEE Trans. Commun., vol. 70, no. 4, pp. 2853\u20132869, Apr. 2022. [72] Y. Song, T. Wang, Y. Wu, L. Qian, and Z. Shi, \u2018\u2018Non-orthogonal multiple access assisted federated learning for UAV swarms: An approach of latency minimization,\u2019\u2019 in Proc. Int. Wireless Commun. Mobile Comput. (IWCMC), Jun. 2021, pp. 1123\u20131128. [73] K. Yang, T. Jiang, Y. Shi, and Z. Ding, \u2018\u2018Federated learning via overthe-air computation,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 19, no. 3, pp. 2022\u20132035, Mar. 2020. [74] Y. Yang, Z. Zhang, Y. Tian, Z. Yang, C. Huang, C. Zhong, and K. Wong, \u2018\u2018Over-the-air split machine learning in wireless MIMO networks,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 41, no. 4, pp. 1007\u20131022, Apr. 2023. [75] Q. N. Le, L. Bariah, O. A. Dobre, and S.Muhaidat, \u2018\u2018Reconfigurable intelligent surface-enabled federated learning for power-constrained devices,\u2019\u2019 IEEE Commun. Lett., vol. 26, no. 11, pp. 2725\u20132729, Nov. 2022. [76] H. Liu, X. Yuan, and Y. A. Zhang, \u2018\u2018Reconfigurable intelligent surface enabled federated learning: A unified communication-learning design approach,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 20, no. 11, pp. 7595\u20137609, Nov. 2021. [77] K. Yang, Y. Shi, Y. Zhou, Z. Yang, L. Fu, and W. Chen, \u2018\u2018Federated machine learning for intelligent IoT via reconfigurable intelligent surface,\u2019\u2019 IEEE Netw., vol. 34, no. 5, pp. 16\u201322, Sep. 2020. [78] Z.Wang, J. Qiu, Y. Zhou, Y. Shi, L. Fu,W. Chen, and K. B. Letaief, \u2018\u2018Federated learning via intelligent reflecting surface,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 21, no. 2, pp. 808\u2013822, Feb. 2022. [79] W. Ni, Y. Liu, Z. Yang, H. Tian, and X. Shen, \u2018\u2018Federated learning in multi-RIS-aided systems,\u2019\u2019 IEEE Internet Things J., vol. 9, no. 12, pp. 9608\u20139624, Jun. 2022. [80] W. Ni, Y. Liu, H. Tian, Y. C. Eldar, and K. Huang, \u2018\u2018SemiFL: Semi-federated learning empowered by simultaneously transmitting and reflecting reconfigurable intelligent surface,\u2019\u2019 in Proc. IEEE Int. Conf. Commun., May 2022, pp. 5104\u20135109. [81] S. Wang, M. Lee, S. Hosseinalipour, R. Morabito, M. Chiang, and C. G. Brinton, \u2018\u2018Device sampling for heterogeneous federated learning: Theory, algorithms, and implementation,\u2019\u2019 in Proc. INFOCOM IEEE Conf. Comput. Commun., May 2021, pp. 1\u201310. [82] X. Cai, X. Mo, J. Chen, and J. Xu, \u2018\u2018D2D-enabled data sharing for distributed machine learning at wireless network edge,\u2019\u2019 IEEE Wireless Commun. Lett., vol. 9, no. 9, pp. 1457\u20131461, Sep. 2020. [83] S. Savazzi, M. Nicoli, and V. Rampa, \u2018\u2018Federated learning with cooperating devices: A consensus approach for massive IoT networks,\u2019\u2019 IEEE Internet Things J., vol. 7, no. 5, pp. 4641\u20134654, May 2020. [84] Y. Huang, X. Qiao, W. Lai, S. Dustdar, J. Zhang, and J. Li, \u2018\u2018Enabling DNN acceleration with data and model parallelization over ubiquitous end devices,\u2019\u2019 IEEE Internet Things J., vol. 9, no. 16, pp. 15053\u201315065, Aug. 2022. [85] M. Yemini, R. Saha, E. Ozfatura, D. G\u00fcnd\u00fcz, and A. J. Goldsmith, \u2018\u2018Semi-decentralized federated learning with collaborative relaying,\u2019\u2019 2022, arXiv:2205.10998. [86] M. Fu, Y. Shi, and Y. Zhou, \u2018\u2018Federated learning via unmanned aerial vehicle,\u2019\u2019 2022, arXiv:2210.10970. [87] Z. Lin, H. Liu, and Y. A. Zhang, \u2018\u2018Relay-assisted cooperative federated learning,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 21, no. 9, pp. 7148\u20137164, Sep. 2022. [88] D. G\u00fcnd\u00fcz, P. de Kerret, N. D. Sidiropoulos, D. Gesbert, C. R. Murthy, and M. van der Schaar, \u2018\u2018Machine learning in the air,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 37, no. 10, pp. 2184\u20132199, Oct. 2019.\n[89] D. Shome, O. Waqar, and W. U. Khan, \u2018\u2018Federated learning and next generation wireless communications: A survey on bidirectional relationship,\u2019\u2019 Trans. Emerg. Telecommun. Technol., vol. 33, no. 7, p. e4458, Jul. 2022. [90] S. Sorour, U. Mohammad, A. Abutuleb, and H. Hassanein, \u2018\u2018Returning the favor: What wireless networking can offer to AI and edge learning,\u2019\u2019 2020, arXiv:2006.07453. [91] A. Kosta, N. Pappas, and V. Angelakis, \u2018\u2018Age of information: A new concept, metric, and tool,\u2019\u2019 Found. Trends Netw., vol. 12, no. 3, pp. 162\u2013259, 2017. [92] E. Calvanese Strinati and S. Barbarossa, \u2018\u20186G networks: Beyond Shannon towards semantic and goal-oriented communications,\u2019\u2019 Comput. Netw., vol. 190, May 2021, Art. no. 107930. [93] Z. Ding, X. Lei, G. K. Karagiannidis, R. Schober, J. Yuan, and V. K. Bhargava, \u2018\u2018A survey on non-orthogonal multiple access for 5G networks: Research challenges and future trends,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 35, no. 10, pp. 2181\u20132195, Oct. 2017. [94] A. Sahin and R. Yang, \u2018\u2018A survey on over-the-air computation,\u2019\u2019 IEEE Commun. Surveys Tuts., early access, Apr. 5, 2023, doi: 10.1109/COMST.2023.3264649. [95] Y. Liu, X. Liu, X. Mu, T. Hou, J. Xu, M. Di Renzo, and N. Al-Dhahir, \u2018\u2018Reconfigurable intelligent surfaces: Principles and opportunities,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 23, no. 3, pp. 1546\u20131577, 3rd Quart., 2021. [96] C. Huang, R. Mo, and C. Yuen, \u2018\u2018Reconfigurable intelligent surface assisted multiuser MISO systems exploiting deep reinforcement learning,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 38, no. 8, pp. 1839\u20131850, Aug. 2020. [97] F. Jameel, Z. Hamid, F. Jabeen, S. Zeadally, and M. A. Javed, \u2018\u2018A survey of device-to-device communications: Research issues and challenges,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 20, no. 3, pp. 2133\u20132168, 3rd Quart., 2018. [98] Z. Lin, H. Liu, and Y.-J. A. Zhang, \u2018\u2018Relay-assisted over-the-air federated learning,\u2019\u2019 in Proc. IEEE GlobecomWorkshops (GCWkshps), Dec. 2021, pp. 1\u20137. [99] W. Ren, Y. Qu, C. Dong, Y. Jing, H. Sun, Q. Wu, and S. Guo, \u2018\u2018A survey on collaborative DNN inference for edge intelligence,\u2019\u2019 2022, arXiv:2207.07812. [100] G. Durisi, T. Koch, and P. Popovski, \u2018\u2018Toward massive, ultrareliable, and low-latency wireless communication with short packets,\u2019\u2019 Proc. IEEE, vol. 104, no. 9, pp. 1711\u20131726, Sep. 2016. [101] T. Nishio and R. Yonetani, \u2018\u2018Client selection for federated learning with heterogeneous resources in mobile edge,\u2019\u2019 in Proc. IEEE Int. Conf. Commun. (ICC), May 2019, pp. 1\u20137. [102] X. Chen, G. Zhu, Y. Deng, and Y. Fang, \u2018\u2018Federated learning over multihop wireless networks with in-network aggregation,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 21, no. 6, pp. 4622\u20134634, Jun. 2022. [103] T. T. Vu, H. Quoc Ngo, T. L. Marzetta, and M. Matthaiou, \u2018\u2018How does cell-free massive MIMO support multiple federated learning groups?\u2019\u2019 in Proc. IEEE 22nd Int. Workshop Signal Process. Adv. Wireless Commun. (SPAWC), Sep. 2021, pp. 401\u2013405. [104] Y. Lin and S. Luo, \u2018\u2018Poster: Accelerate cross-device federated learning with semi-reliable model multicast over the air,\u2019\u2019 in Proc. IEEE 29th Int. Conf. Netw. Protocols (ICNP), Nov. 2021, pp. 1\u20132. [105] V. K. Shrivastava, S. Baek, and Y. Baek, \u2018\u20185G evolution for multicast and broadcast services in 3GPP release 17,\u2019\u2019 IEEE Commun. Standards Mag., vol. 6, no. 3, pp. 70\u201376, Sep. 2022. [106] E. F. Pupo, C. C. Gonz\u00e1lez, L. Atzori, and M. Murroni, \u2018\u2018Dynamic multicast access technique in SC-PTM 5G networks: Subgrouping with OM/NOM,\u2019\u2019 in Proc. IEEE Int. Symp. Broadband Multimedia Syst. Broadcast. (BMSB), Jun. 2022, pp. 1\u20136. [107] E. Seo, D. Niyato, and E. Elmroth, \u2018\u2018Auction-based federated learning using software-defined networking for resource efficiency,\u2019\u2019 in Proc. 17th Int. Conf. Netw. Service Manage. (CNSM), Oct. 2021, pp. 42\u201348. [108] S. K. P. Tam and S. Math, \u2018\u2018Efficient resource slicing scheme for optimizing federated learning communications in software-defined IoT networks,\u2019\u2019 J. Internet Comput. Services, vol. 22, no. 5, pp. 27\u201333, 2021. [109] V. Balasubramanian, M. Aloqaily, M. Reisslein, and A. Scaglione, \u2018\u2018Intelligent resource management at the edge for ubiquitous IoT: An SDN-based federated learning approach,\u2019\u2019 IEEE Netw., vol. 35, no. 5, pp. 114\u2013121, Sep. 2021.\nVOLUME 11, 2023 52859\n[110] D. Aguiari, A. Ferlini, J. Cao, S. Guo, and G. Pau, \u2018\u2018Poster abstract: Ccontinuum: Edge-to-cloud computing for distributed AI,\u2019\u2019 in Proc. IEEE Conf. Comput. Commun. Workshops (INFOCOM WKSHPS), Apr. 2019, pp. 1053\u20131054. [111] C. Campolo, G. Lia, M. Amadeo, G. Ruggeri, A. Iera, and A. Molinaro, \u2018\u2018Towards namedAI networking: Unveiling the potential of NDN for edge AI,\u2019\u2019 in Proc. AdHocNow, 2020, pp. 16\u201322. [112] M. Amadeo, C. Campolo, A. Iera, A. Molinaro, and G. Ruggeri, \u2018\u2018Client discovery and data exchange in edge-based federated learning via named data networking,\u2019\u2019 in Proc. IEEE Int. Conf. Commun., May 2022, pp. 2990\u20132995. [113] S. Bano, N. Tonellotto, P. Cassara, and A. Gotta, \u2018\u2018KafkaFed: Two-tier federated learning communication architecture for Internet of Vehicles,\u2019\u2019 in Proc. IEEE Int. Conf. Pervasive Comput. Commun. Workshops Other Affiliated Events (PerCom Workshops), Mar. 2022, pp. 515\u2013520. [114] Z. Xiong and N. Zilberman, \u2018\u2018Do switches dream of machine learning? Toward in-network classification,\u2019\u2019 in Proc. 18th ACM Workshop Hot Topics Netw., Nov. 2019, pp. 25\u201333. [115] D. Sanvito, G. Siracusano, and R. Bifulco, \u2018\u2018Can the network be the AI accelerator?\u2019\u2019 in Proc. Morning Workshop In-Network Comput., Aug. 2018, pp. 20\u201325. [116] Y. Li, J. Park, M. Alian, Y. Yuan, Z. Qu, P. Pan, and R.Wang, \u2018\u2018A networkcentric hardware/algorithm co-design to accelerate distributed training of deep neural networks,\u2019\u2019 IEEE/ACM MICRO, Oct. 2018, pp. 175\u2013188. [117] M. Saquetti, R. Canofre, A. F. Lorenzon, F. D. Rossi, J. R. Azambuja, W. Cordeiro, and M. C. Luizelli, \u2018\u2018Toward in-network intelligence: Running distributed artificial neural networks in the data plane,\u2019\u2019 IEEE Commun. Lett., vol. 25, no. 11, pp. 3551\u20133555, Nov. 2021. [118] B. M. Xavier, R. S. Guimar\u00e3es, G. Comarela, and M. Martinello, \u2018\u2018Programmable switches for in-networking classification,\u2019\u2019 in Proc. IEEE Conf. Comput. Commun., May 2021, pp. 1\u201310. [119] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy, M. Moshref, D. R. K. Ports, and P. Richt\u00e1rik, \u2018\u2018Scaling distributed machine learning with in-network aggregation,\u2019\u2019 2019, arXiv:1903.06701. [120] Y. Li, I. Liu, Y. Yuan, D. Chen, A. Schwing, and J. Huang, \u2018\u2018Accelerating distributed reinforcement learning with in-switch computing,\u2019\u2019 in Proc. ACM/IEEE 46th Annu. Int. Symp. Comput. Archit. (ISCA), Jun. 2019, pp. 279\u2013291. [121] J. He, H. Wu, X. Xiao, R. Bassoli, and F. H. P. Fitzek, \u2018\u2018Functional split of in-network deep learning for 6G: A feasibility study,\u2019\u2019 IEEE Wireless Commun., vol. 29, no. 5, pp. 36\u201342, Oct. 2022. [122] D. Kreutz, F. M. V. Ramos, P. E. Ver\u00edssimo, C. E. Rothenberg, S. Azodolmolky, and S. Uhlig, \u2018\u2018Software-defined networking: A comprehensive survey,\u2019\u2019 Proc. IEEE, vol. 103, no. 1, pp. 14\u201376, Jan. 2015. [123] J. Xie, F. R. Yu, T. Huang, R. Xie, J. Liu, C. Wang, and Y. Liu, \u2018\u2018A survey of machine learning techniques applied to software defined networking (SDN): Research issues and challenges,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 21, no. 1, pp. 393\u2013430, 1st Quart., 2019. [124] S. Islam, N. Muslim, and J. W. Atwood, \u2018\u2018A survey on multicasting in software-defined networking,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 20, no. 1, pp. 355\u2013387, 1st Quart., 2018. [125] D. King and A. Farrel, \u2018\u2018A survey of semantic internet routing techniques,\u2019\u2019 Internet Eng. Task Force (IETF), Draft-King-Irtf-SemanticSurvey-01, Tech. Rep., 2022. [126] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown, J. Rexford, C. Schlesinger, D. Talayco, A. Vahdat, G. Varghese, and D. Walker, \u2018\u2018P4: Programming protocol-independent packet processors,\u2019\u2019 ACM SIGCOMM Comput. Commun. Rev., vol. 44, no. 3, pp. 87\u201395, Jul. 2014. [127] O. Michel, R. Bifulco, G. R\u00e9tv\u00e1ri, and S. Schmid, \u2018\u2018The programmable data plane: Abstractions, architectures, algorithms, and applications,\u2019\u2019 ACM Comput. Surveys, vol. 54, no. 4, pp. 1\u201336, 2021. [128] T. Mai, S. Garg, H. Yao, J. Nie, G. Kaddoum, and Z. Xiong, \u2018\u2018In-network intelligence control: Toward a self-driving networking architecture,\u2019\u2019 IEEE Netw., vol. 35, no. 2, pp. 53\u201359, Mar. 2021. [129] G. Siracusano and R. Bifulco, \u2018\u2018In-network neural networks,\u2019\u2019 2018, arXiv:1801.05731. [130] L. Zhang, A. Afanasyev, J. Burke, V. Jacobson, K. Claffy, P. Crowley, C. Papadopoulos, L. Wang, and B. Zhang, \u2018\u2018Named data networking,\u2019\u2019 SIGCOMM Comput. Commun. Rev., vol. 44, no. 3, pp. 66\u201373, Jul. 2014. [131] G. Xylomenos, C. N. Ververidis, V. A. Siris, N. Fotiou, C. Tsilopoulos, X. Vasilakos, K. V. Katsaros, and G. C. Polyzos, \u2018\u2018A survey of information-centric networking research,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 16, no. 2, pp. 1024\u20131049, 2nd Quart., 2014.\n[132] M. Amadeo, C. Campolo, A. Molinaro, and G. Ruggeri, \u2018\u2018IoT data processing at the edge with named data networking,\u2019\u2019 in Proc. Eur. Wireless 24th Eur. Wireless Conf., May 2018, pp. 1\u20136. [133] A. Rahman, D. Trossen, D. Kutscher, andR. Ravindran,Deployment Considerations for Information-Centric Networking (ICN), document RFC 8763, 2020. [134] S. Shahhosseini, D. Seo, A. Kanduri, T. Hu, S.-S. Lim, B. Donyanavard, A. M. Rahmani, and N. Dutt, \u2018\u2018Online learning for orchestration of inference in multi-user end-edge-cloud networks,\u2019\u2019 ACM Trans. Embedded Comput. Syst., vol. 21, no. 6, pp. 1\u201325, Nov. 2022. [135] F. Malandrino, C. F. Chiasserini, and G. di Giacomo, \u2018\u2018Efficient distributed DNNs in the mobile-edge-cloud continuum,\u2019\u2019 IEEE/ACM Trans. Netw., early access, Nov. 24, 2022, doi: 10.1109/TNET.2022.3222640. [136] E. Samikwa, A. D.Maio, and T. Braun, \u2018\u2018ARES: Adaptive resource-aware split learning for Internet of Things,\u2019\u2019Comput. Netw., vol. 218, Dec. 2022, Art. no. 109380. [137] S. Wang, X. Zhang, H. Uchiyama, and H. Matsuda, \u2018\u2018HiveMind: Towards cellular native machine learning model splitting,\u2019\u2019 IEEE J. Sel. Areas Commun., vol. 40, no. 2, pp. 626\u2013640, Feb. 2022. [138] M. M. Wadu, S. Samarakoon, and M. Bennis, \u2018\u2018Joint client scheduling and resource allocation under channel uncertainty in federated learning,\u2019\u2019 IEEE Trans. Commun., vol. 69, no. 9, pp. 5962\u20135974, Sep. 2021. [139] L. Yu, R. Albelaihi, X. Sun, N. Ansari, and M. Devetsikiotis, \u2018\u2018Jointly optimizing client selection and resource management in wireless federated learning for Internet of Things,\u2019\u2019 IEEE Internet Things J., vol. 9, no. 6, pp. 4385\u20134395, Mar. 2022. [140] H. Ko, J. Lee, S. Seo, S. Pack, and V. C. M. Leung, \u2018\u2018Joint client selection and bandwidth allocation algorithm for federated learning,\u2019\u2019 IEEE Trans. Mobile Comput., vol. 22, no. 6, pp. 3380\u20133390, Jun. 2023. [141] J. Xu, H. Wang, and L. Chen, \u2018\u2018Bandwidth allocation for multiple federated learning services in wireless edge networks,\u2019\u2019 IEEE Trans. Wireless Commun., vol. 21, no. 4, pp. 2534\u20132546, Apr. 2022. [142] C. Campolo, G. Genovese, A. Iera, and A. Molinaro, \u2018\u2018Virtualizing AI at the distributed edge towards intelligent IoT applications,\u2019\u2019 J. Sensor Actuator Netw., vol. 10, no. 1, p. 13, Feb. 2021. [143] C. Campolo, G. Genovese, G. Singh, and A. Molinaro, \u2018\u2018Scalable and interoperable edge-based federated learning in IoT contexts,\u2019\u2019 Comput. Netw., vol. 223, Mar. 2023, Art. no. 109576. [144] T. Yang, M. Qin, N. Cheng, W. Xu, and L. Zhao, \u2018\u2018Liquid software-based edge intelligence for future 6G networks,\u2019\u2019 IEEE Netw., vol. 36, no. 1, pp. 69\u201375, Jan. 2022. [145] S. Abdulrahman, H. Tout, A. Mourad, and C. Talhi, \u2018\u2018FedMCCS: Multicriteria client selection model for optimal IoT federated learning,\u2019\u2019 IEEE Internet Things J., vol. 8, no. 6, pp. 4723\u20134735, Mar. 2021. [146] W. Wu, C. Zhou, M. Li, H. Wu, H. Zhou, N. Zhang, X. S. Shen, and W. Zhuang, \u2018\u2018AI-native network slicing for 6G networks,\u2019\u2019 IEEE Wireless Commun., vol. 29, no. 1, pp. 96\u2013103, Feb. 2022. [147] A. Kanduri, S. Shahhosseini, E. Kasaeyan Naeini, H. Alikhani, P. Liljeberg, N. Dutt, and A. M. Rahmani, \u2018\u2018Edge-centric optimization of multimodal ML-driven eHealth applications,\u2019\u2019 2022, arXiv:2208.02597. [148] M. Nitti, V. Pilloni, G. Colistra, and L. Atzori, \u2018\u2018The virtual object as a major element of the Internet of Things: A survey,\u2019\u2019 IEEE Commun. Surveys Tuts., vol. 18, no. 2, pp. 1228\u20131240, 2nd Quart., 2016. [149] Open Mobile Alliance, Lightweight Machine to Machine Technical Specification Core; v1_1-20180612-c, Open Mobile Alliance (OMA), 2018. [150] Focus Group on Technologies for Network 2030, Additional Representative Use Cases and Key Network Requirements for Network 2030, document ITU FG-NET2030, Jun. 2020. Accessed: May 29, 2023. [Online]. Available: https://datatracker.ietf.org/rg/coinrg/about/ [151] IETF Computing in the Network Research Group (COINRG). Accessed: May 29, 2023. [Online]. Available: https://datatracker. ietf.org/rg/coinrg/about/ [152] One6G White Paper: 6G Technology Overview. Accessed: May 29, 2023. [Online]. Available: https://www.itu.int/en/ITU-T/ focusgroups/ml5g/Pages/default.aspx [153] ITU-T Focus Group on Machine Learning for Future Networks including 5G. [154] Architecture Enhancements for 5G System (5GS) to Support Network Data Analytics Services, document TS 23.288, V16.2.0, Rel. 16, 3GPP, Dec. 2019. [155] Technical Specification Group Services and System Aspects; Study on Traffic Characteristics and Performance Requirements for AI/ML Model Transfer in 5GS; Release 18, document TR 22.784, V17.1.1, 3GPP, Dec. 2021. Accessed: Feb. 28, 2023.\n52860 VOLUME 11, 2023\n[156] DAIS, Distributed Artificial Intelligent System. Accessed: Feb. 28, 2023. [Online]. Available: https://dais-project.eu/ [157] DEDICAT6G, Dynamic Coverage Extension and Distributed Intelligence for Human Centric Applications With Assured Security, Privacy, and Trust: From 5G to 6G. Accessed: Feb. 28, 2023. [Online]. Available: https://dedicat6g.eu/ [158] AI@edge, A Secure and Reusable Artificial Intelligence Platform for Edge Computing in Beyond 5G Networks. Accessed: Feb. 28, 2023. [Online]. Available: https://aiatedge.eu/ [159] C.-J. Wu, \u2018\u2018Sustainable AI: Environmental implications, challenges and opportunities,\u2019\u2019 in Proc. Mach. Learn. Syst., vol. 4, 2022, pp. 795\u2013813.\nCLAUDIA CAMPOLO (Senior Member, IEEE) is currently an Associate Professor of telecommunications with the University Mediterranea of Reggio Calabria, Italy. Her research interests include vehicular networking, 5G/6G, and future internet architectures.\nANTONIO IERA (Senior Member, IEEE) is currently a Full Professor of telecommunications with theUniversity of Calabria, Italy. His research interests include next generation mobile and wireless systems and the Internet of Things.\nANTONELLA MOLINARO (Senior Member, IEEE) is currently a Full Professor of telecommunications with the UniversityMediterranea of Reggio Calabria, Italy, and Universit\u00e9 Paris-Saclay, France. Her current research interests include 5G and beyond networks, connected vehicles, and the future internet.\nOpen Access funding provided by \u2018Universit\u00e0 della Calabria\u2019 within the CRUI CARE Agreement\nVOLUME 11, 2023 52861"
        }
    ],
    "title": "Network for Distributed Intelligence: A Survey and Future Perspectives",
    "year": 2023
}