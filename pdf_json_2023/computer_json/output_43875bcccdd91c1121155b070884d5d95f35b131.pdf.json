{
    "abstractText": "We study the cost of overfitting in noisy kernel ridge regression (KRR), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an \u201cagnostic\u201d view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the RKHS. We analyze the cost of overfitting under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting (qv Mallinar et al. 2022).",
    "authors": [],
    "id": "SP:d441cefc6d9700194327c3598f7f0623519d6a15",
    "references": [
        {
            "authors": [
                "Belkin",
                "Mikhail",
                "Daniel Hsu",
                "Siyuan Ma",
                "Soumik Mandal"
            ],
            "title": "Reconciling modern machine learning practice and the bias-variance trade-off.",
            "venue": "Proceedings of the National Academy of Sciences",
            "year": 2019
        },
        {
            "authors": [
                "Billingsley",
                "Patrick"
            ],
            "title": "Probability and Measure",
            "year": 1995
        },
        {
            "authors": [
                "Canatar",
                "Abdulkadir",
                "Blake Bordelon",
                "Cengiz Pehlevan"
            ],
            "title": "Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks.",
            "venue": "Nature Communications",
            "year": 2021
        },
        {
            "authors": [
                "Donhauser",
                "Konstantin",
                "Nicolo Ruggeri",
                "Stefan Stojanovic",
                "Fanny Yang"
            ],
            "title": "Fast rates for noisy interpolation require rethinking the effects of inductive bias.",
            "year": 2022
        },
        {
            "authors": [
                "Ghorbani",
                "Behrooz",
                "Song Mei",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Linearized two-layers neural networks in high dimension.",
            "venue": "The Annals of Statistics",
            "year": 2021
        },
        {
            "authors": [
                "Hastie",
                "Trevor",
                "Andrea Montanari",
                "Saharon Rosset",
                "Ryan J Tibshirani"
            ],
            "title": "Surprises in high-dimensional ridgeless least squares interpolation.",
            "venue": "Annals of Statistics",
            "year": 2019
        },
        {
            "authors": [
                "Haussler",
                "David"
            ],
            "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications.",
            "venue": "Information and computation",
            "year": 1992
        },
        {
            "authors": [
                "Hu",
                "Hong",
                "Yue M. Lu"
            ],
            "title": "Universality Laws for High-Dimensional Learning With Random Features.",
            "venue": "IEEE Transactions on Information Theory",
            "year": 2023
        },
        {
            "authors": [
                "Jacot",
                "Arthur",
                "Berfin Simsek",
                "Francesco Spadaro",
                "Cl\u00e9ment Hongler",
                "Franck Gabriel"
            ],
            "title": "Kernel Alignment Risk Estimator: Risk Prediction from Training Data.",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Koehler",
                "Frederic",
                "Lijia Zhou",
                "Danica J. Sutherland",
                "Nathan Srebro"
            ],
            "title": "Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds and Benign Overfitting.",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Loureiro",
                "Bruno",
                "Cedric Gerbelot",
                "Hugo Cui",
                "Sebastian Goldt",
                "Florent Krzakala",
                "Marc Mezard",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Learning curves of generic features maps for realistic datasets with a teacher-student model.",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Mei",
                "Song",
                "Theodor Misiakiewicz",
                "Andrea Montanari"
            ],
            "title": "Generalization error of random feature and kernel methods: Hypercontractivity and kernel matrix concentration.",
            "venue": "Applied and Computational Harmonic Analysis",
            "year": 2022
        },
        {
            "authors": [
                "Mei",
                "Song",
                "Andrea Montanari"
            ],
            "title": "The generalization error of random features regression: Precise asymptotics and the double descent curve.",
            "venue": "Communications on Pure and Applied Mathematics",
            "year": 2022
        },
        {
            "authors": [
                "Mel",
                "Gabriel C",
                "Surya Ganguli"
            ],
            "title": "A Theory of High Dimensional Regression with Arbitrary Correlations between Input Features and Target Functions: Sample Complexity, Multiple Descent Curves and a Hierarchy of Phase Transitions.",
            "venue": "International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "Mercer",
                "James"
            ],
            "title": "Functions of positive and negative type, and their connection the theory of integral equations.",
            "venue": "Philosophical Transactions of the Royal Society of London",
            "year": 1909
        },
        {
            "authors": [
                "Minh",
                "Ha Quang",
                "Partha Niyogi",
                "Yuan Yao"
            ],
            "title": "Mercer\u2019s theorem, feature maps, and smoothing.",
            "venue": "International Conference on Computational Learning Theory",
            "year": 2006
        },
        {
            "authors": [
                "Misiakiewicz",
                "Theodor"
            ],
            "title": "Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression.",
            "year": 2022
        },
        {
            "authors": [
                "Muthukumar",
                "Vidya",
                "Kailas Vodrahalli",
                "Vignesh Subramanian",
                "Anant Sahai"
            ],
            "title": "Harmless interpolation of noisy data in regression.",
            "venue": "IEEE Journal on Selected Areas in Information Theory",
            "year": 2020
        },
        {
            "authors": [
                "Neyshabur",
                "Behnam",
                "Ryota Tomioka",
                "Nathan Srebro"
            ],
            "title": "In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.",
            "venue": "International Conference on Learning Representations \u2013 Workshop",
            "year": 2015
        },
        {
            "authors": [
                "Panchenko",
                "Dmitry"
            ],
            "title": "Some Extensions of an Inequality of Vapnik and Chervonenkis.",
            "venue": "Electronic Communications in Probability",
            "year": 2002
        },
        {
            "authors": [
                "Richards",
                "Dominic",
                "Jaouad Mourtada",
                "Lorenzo Rosasco"
            ],
            "title": "Asymptotics of Ridge(less) Regression under General Source Condition.",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "year": 2021
        },
        {
            "authors": [
                "Shalev-Shwartz",
                "Shai",
                "Shai Ben-David"
            ],
            "title": "Understanding machine learning: From theory to algorithms",
            "year": 2014
        },
        {
            "authors": [
                "Simon",
                "James B",
                "Madeline Dickens",
                "Dhruva Karkada",
                "Michael R. DeWeese"
            ],
            "title": "The Eigenlearning Framework: A Conservation Law Perspective on Kernel Regression and Wide Neural Networks.",
            "year": 2021
        },
        {
            "authors": [
                "Srebro",
                "Nathan",
                "Karthik Sridharan",
                "Ambuj Tewari"
            ],
            "title": "Optimistic Rates for Learning with a Smooth Loss.",
            "year": 2010
        },
        {
            "authors": [
                "Tsigler",
                "Alexander",
                "Peter L. Bartlett"
            ],
            "title": "Benign overfitting in ridge regression.",
            "year": 2020
        },
        {
            "authors": [
                "Vapnik",
                "Vladimir",
                "Alexey Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events to their probabilities.",
            "venue": "Theory of Probability and its applications",
            "year": 1971
        },
        {
            "authors": [
                "Wang",
                "Guillaume",
                "Konstantin Donhauser",
                "Fanny Yang"
            ],
            "title": "Tight bounds for minimum l1norm interpolation of noisy data.",
            "year": 2021
        },
        {
            "authors": [
                "Zhang",
                "Chiyuan",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization.",
            "venue": "International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "Zhou",
                "Lijia",
                "Frederic Koehler",
                "Pragya Sur",
                "Danica J. Sutherland",
                "Nathan Srebro"
            ],
            "title": "A Non-Asymptotic Moreau Envelope Theory for High-Dimensional Generalized Linear Models.",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Zhou",
                "Lijia",
                "Frederic Koehler",
                "Danica J. Sutherland",
                "Nathan Srebro"
            ],
            "title": "Optimistic Rates: A Unifying Theory for Interpolation Learning and Regularization in Linear Regression.",
            "venue": "ACM / IMS Journal of Data Science",
            "year": 2021
        },
        {
            "authors": [
                "Zhou",
                "Lijia",
                "Danica J. Sutherland",
                "Nathan Srebro"
            ],
            "title": "On Uniform Convergence and Low-Norm Interpolation Learning.",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "2021) avoids any hidden multiplicative constant and logarithmic factor present in previous works and can be used to establish benign overfitting. However, their proof techniques depend on the Gaussian Minimax Theorem (GMT) and are limited to the setting of Gaussian features. We recover their result in Theorem 7 here with a (non-rigorous) calculation that extends beyond the Gaussian case",
            "year": 2021
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "The proof concludes by plugging in Lemma 1. Finally, we can plug in the norm bound of Theorem 9 into Theorem 7 to establish benign overfitting, as in Koehler et al",
            "venue": "For any l \u2208 N \u222a {\u221e} and k \u2208 N",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 6.\n13 18\n5v 1\n[ st\nat .M\nL ]"
        },
        {
            "heading": "1 Introduction",
            "text": "The ability of large neural networks to generalize, even when they overfit to noisy training data (Neyshabur et al. 2015; Zhang et al. 2017; Belkin et al. 2019), has significantly challenged our understanding of the effect of overfitting. A starting point for understanding overfitting in deep learning is to understand the issue in kernel methods, possibly viewing deep learning through their kernel approximation (Jacot et al. 2020). Indeed, there is much progress in understanding the effect of overfitting in kernel ridge regression and ridge regression with Gaussian data. It has been shown that the test error of the minimal norm interpolant can approach Bayes optimality and so overfitting is \"benign\" (Bartlett et al. 2020; Muthukumar et al. 2020; Koehler et al. 2021; Wang et al. 2021; Donhauser et al. 2022). In other situations such as Laplace kernels and ReLU neural tangent kernels, the interpolating solution is not consistent but also not \"catastrophically\" bad, which falls into an intermediate regime called \"tempered\" overfitting (Mallinar et al. 2022).\nHowever, the perspective taken in this line of work differs from the agnostic view of statistical learning. These results typically focus on asymptotic behavior and consistency of a well-specified model, asking how the limiting behavior of interpolating learning rules compares to the Bayes error (the smallest risk attainable by any measurable function of the feature x). In contrast, the agnostic PAC model (Vapnik and Chervonenkis 1971; Haussler 1992; Shalev-Shwartz and Ben-David 2014) does not require any assumption on the conditional distribution of the label y. In particular, the conditional expectation E[y|x] is not necessarily a member of the hypothesis class and it does not need to have small Hilbert norm in the Reproducing Kernel Hilbert Space (RKHS). Instead, the learning rule is asked to find a model whose test risk can compete with the smallest risk within the hypothesis class, which can be quite high if the sample size is not large enough for consistency or when no predictor in the hypothesis class can even attain the Bayes error. In these situations, the agnostic PAC model can still provide a meaningful learning guarantee.\nFurthermore, we would like to isolate the effect of overfitting (i.e. underregularizing, and choosing to use a predictor that fits the noise, instead of compromising on empirical fit and choosing a predictor that balances empirical fit with complexity or norm) from the difficulty of the learning problem and appropriateness of the model irrespective of overfitting (i.e. even if we were to not overfit and instead optimally balance empirical fit and norm, as in ridge regression). A view which considers only the risk of the overfitting rule (e.g. Mallinar et al. 2022) confounds these two issues. Instead, we would like to study the direct effect of overfitting: how much does it hurt to overfit and use ridgeless regression compared to optimally tuned ridge regression.\nIn this paper, we take an agnostic view to the direct effect of overfitting in (kernel) ridge regression. Rather than comparing the asymptotic risk of the interpolating ridgeless model to the Bayes error, we compare it to the best ridge model in terms of population error as a function of sample size, and we measure the cost of overfitting as a ratio. We show that the cost of overfitting can be bounded by using only the sample size and the effective ranks of the covariance, even when the risk of the optimally-tuned model is high relative to the Bayes error. Our analysis applies to any target function (including ones with unbounded RKHS norm) and recovers the matching upper and lower bounds from Bartlett et al. 2020, which allows us to have a more refined understanding of the benign overfitting. In addition to benign overfitting, we show that the amount of \"tempered\" overfitting can also be understood using the cost of interpolation, and we derive the necessary and sufficient condition for \"catastrophic\" overfitting (Mallinar et al. 2022). Combining these results leads to a refined notion of benign, tempered, and catastrophic overfitting (focusing on the difference versus the optimally tuned predictor), and a characterization as a function of sample size n based on computing the effective rank rk at some index k. We further apply our results to the setting of inner product kernels in the polynomial regime (Ghorbani et al. 2021; Mei et al. 2022; Misiakiewicz 2022) and recover the multiple descent curve."
        },
        {
            "heading": "2 Problem Formulation",
            "text": "Let X be an abstract input space and K : X \u00d7 X \u2192 R a positive semi-definite kernel1."
        },
        {
            "heading": "2.1 Bi-criterion Optimization in KRR",
            "text": "Given a data set Dn consisting of (x1, y1), ..., (xn, yn) \u2208 X \u00d7 R sampled from some unknown joint distribution D, in order to find a predictor with good test error R(f), we solve the bi-criterion optimization:\nmin f\u2208H\nR\u0302(f), \u2016f\u2016H (1)\nwhere \u2016f\u2016H is the Hilbert norm in the RKHS and the test error and training error (in square loss) of a predictor f is given by\nR(f) := E [ (f(x)\u2212 y)2 ] and R\u0302(f) := 1\nn\nn \u2211\ni=1\n(f(xi)\u2212 yi)2. (2)\nThe Pareto-frontier of the bi-criterion problem (1) corresponds to the regularization path {f\u0302\u03b4}\u03b4\u22650 given by the sequence of problems:\nf\u0302\u03b4 = argmin f\u2208H\nR\u0302(f) + \u03b4\nn \u2016f\u20162H. (3)\nBy the representation theorem, f\u0302\u03b4 has the explicit closed form:\nf\u0302\u03b4(x) = K(Dn, x) T (K(Dn, Dn) + \u03b4In) \u22121 Y (4)\nwhere K(Dn, x) \u2208 Rn,K(Dn, Dn) \u2208 Rn\u00d7n, Y \u2208 Rn are given by [K(Dn, x)]i = K(xi, x), [K(Dn, Dn)]i,j = K(xi, xj) and [Y ]i = yi. The interpolating \"ridgeless\" solution (minimal norm interpolant) is the extreme Pareto point and obtained by taking \u03b4 \u2192 0+:\nf\u03020 = argmin f\u2208H:R\u0302(f)=0\n\u2016f\u2016H. (5)\n1i.e.: (i) \u2200x, x\u2032 \u2208 X , K(x, x\u2032) = K(x\u2032, x), and (ii) \u2200n \u2208 N, x1, ..., xn \u2208 X , c1, ..., cn \u2208 R, it holds that\u2211n i=1 \u2211n j=1 cicjK(xi, xj) \u2265 0.\nEven though f\u03020 has the minimal norm among all interpolants, the norm of f\u03020 will still be very large because it needs to memorize all the noisy training labels. In this paper, we are particularly interested\nin the generalization performance of the ridgeless solution f\u03020, which minimizes the training error in the bi-criterion problem (1) too much."
        },
        {
            "heading": "2.2 Mercer\u2019s Decomposition",
            "text": "Though the setting for KRR is very generic, we can understand it as (linear) ridge regression. By Mercer\u2019s theorem (Mercer 1909), the kernel admits the decomposition\nK(x, x\u2032) = \u2211\ni\n\u03bbi\u03c6i(x)\u03c6(x \u2032) (6)\nwhere \u03c6i : X \u2192 R satisfies Ex[\u03c6i(x)\u03c6j(x)] = 1 if i = j and 0 otherwise, and the expectation is taken with respect to the marginal distribution of x given by D. For example, if X = {x1, ..., xM} has finite cardinalityM and x is uniformly distributed over X , then (6) can be found by the spectral decomposition of the matrix K(X ,X ) \u2208 RM\u00d7M given by [K(X ,X )]i,j = K(xi, xj). When x is uniformly distributed over the sphere in Rd or the boolean hypercube {\u22121, 1}d, then {\u03c6i} can be taken to be the spherical harmonics or the Fourier-Walsh (parity) basis. In the case that K is the Gaussian kernel or polynomial kernel, the eigenvalues {\u03bbi} has closed-form expression in terms of the modified Bessel function or the Gamma function (Minh et al. 2006).\nTherefore, instead of viewing the feature x as an element of X , we can consider the potentially infinite-dimensional real-valued vector \u03c8(x) = ( \u221a \u03bb1\u03c61(x), \u221a \u03bb2\u03c62(x), ...) and denote the design matrix \u03a8 = [\u03c8(x1), \u03c8(x2), ...] T . Then we can write K(x, x\u2032) = \u3008\u03c8(x), \u03c8(x\u2032)\u3009 and understand the predictor in (4) as\nf\u0302\u03b4(x) = \u03c8(x) T\u03a8T (\u03a8\u03a8T + \u03b4In) \u22121Y\n= \u3008w\u0302\u03b4, \u03c8(x)\u3009 (7)\nwhere w\u0302\u03b4 = \u03a8 T (\u03a8\u03a8T +\u03b4In) \u22121Y is simply the ridge regression estimate with respect to the data set (\u03a8, Y ). For a predictor f of the form f(x) = \u3008w,\u03c8(x)\u3009, its Hilbert norm is given by \u2016f\u2016H = \u2016w\u20162."
        },
        {
            "heading": "2.3 Closed-form Risk Estimate",
            "text": "Many prior works (Hastie et al. 2019; Wu and Xu 2020; Jacot et al. 2020; Canatar et al. 2021; Loureiro et al. 2021; Mel and Ganguli 2021; Richards et al. 2021; Simon et al. 2021) have characterize the test risk R(f\u0302\u03b4) under the well-specified model assumption y = f \u2217(x) + \u03be, where f\u2217 is usually a function inside the RKHS and \u03be is some independent noise. However, we show that it is unnecessarily restrictive. In particular, the predictions from Simon et al. 2021 can be easily extended to arbitrary distributions in the following way.\nGiven any distribution D, we can always write y = f\u2217(x, \u03be) for some appropriate choice of f\u2217 and noise \u03be. Therefore, we can treat (x, \u03be) as the feature x. Of course, we do not observe \u03be in practice, but we can simply let the kernel to ignore the noise and so the estimator in (4) is the same as if we use only x as our feature. As a result, the eigenfunction of K remains the same and is only a function of x. Extending the eigenfunctions {\u03c6i} to be a basis for \u21132 functions over (x, \u03be), which we denote as {\u03c6i} and {\u03c6\u2032j}, we observe that\nK((x, \u03be), (x\u2032, \u03be\u2032)) = \u2211\ni\n\u03bbi\u03c6i(x)\u03c6i(x \u2032) +\n\u2211\nj\n0 \u00b7 \u03c6\u2032j(x, \u03be)\u03c6\u2032j(x\u2032, \u03be\u2032) (8)\nand we can expand\nf\u2217(x, \u03be) = \u2211\ni\nvi\u03c6i(x) + \u2211\nj\nv\u2032j\u03c6 \u2032 j(x, \u03be). (9)\nIn other words, the general case can be understood as the noiseless case with additional zero eigenvalues, in which the calculation from Simon et al. 2021 still applies. Given any sample size n, spectrum {\u03bbi} and target coefficients {vi} and {v\u2032j} , to compute the test errorR(f\u0302\u03b4) for any \u03b4 \u2265 0, we can first solve for the effective regularization \u03ba\u03b4 defined by\n\u2211\ni\n\u03bbi \u03bbi + \u03ba\u03b4 + \u03b4 \u03ba\u03b4 = n. (10)\nUsing \u03ba\u03b4, we can define\nLi,\u03b4 = \u03bbi\n\u03bbi + \u03ba\u03b4 and E\u03b4 =\nn\nn\u2212 \u2211 i L2i,\u03b4 , (11)\nthen Simon et al. 2021 predicts that the test error approximately equals\nR(f\u0302\u03b4) = E\u03b4\n\n\n\u2211\ni\n(1\u2212 Li,\u03b4)2v2i + \u2211\nj\n(v\u2032j) 2\n\n (12)\nbecause the eigenvalues associated with v\u2032j are zero and Lj,\u03b4 = 0. From now on, we will denote \u03c32 = \u2211\nj(v \u2032 j) 2. In particular, if K is an universal kernel and {\u03c6i} spans all \u21132 functions of x, then since conditional expectation is a projection (Billingsley 1995), it holds that \u2211\ni vi\u03c6i(x) = E[y|x] and \u03c32 = E [ (y \u2212 \u2211\ni vi\u03c6i(x)) 2 ]\nis simply the Bayes error of D. Note that under the well-specified model assumption, equation (12) recovers the expression in Simon et al. 2021.\nThough the result from Simon et al. 2021 is non-rigorous, rigorous version of (12) can be proven with random matrix theory, which requires that the features are a linear transformation of random vectors whose coordinates are independent and have bounded high-order moments (e.g., Hastie et al. 2019; Wu and Xu 2020). Despite that the features in KRR do not satisfy this assumption, many works have established universality results showing that KRR is asymptotically equivalent, in terms of the test and training error, to a Gaussian model with matching covariance matrix (Goldt et al. 2020; Mei and Montanari 2022; Misiakiewicz 2022; Hu and Lu 2023). Numerical experiments (e.g., Jacot et al. 2020; Simon et al. 2021) also suggest that the predictions based on statistical mechanics should hold more generally."
        },
        {
            "heading": "3 Cost of Overfitting",
            "text": "The sensible and traditional approach to learning using a complexity penalty, such as the Hilbert norm \u2016f\u2016H, is to use a Pareto point (point on the regularization path) of the bi-criteria problem (1) that minimizes some balanced combination of the empirical risk and penalty (the \u201cstructural risk\u201d) so as to ensure small population risk. Assumptions about the problem can help us choose which Pareto optimal point, i.e. what value of the tradeoff parameter \u03b4, to use. Simpler and safer is to choose this through validation: calculate the Pareto frontier (aka regularization path) on half the training data set, and choose among these Pareto points by minimizing the \u201cvalidation error\u201d on the held-out half of the training set. Here we do not get into these details, and simply compare to the best Pareto point:\nR(f\u0302\u03b4\u2217) = inf \u03b4\u22650 R(f\u0302\u03b4). (13)\nAlthough we cannot find f\u0302\u03b4\u2217 exactly empirically, it is useful as an oracle, and studying the gap versus this ideal Pareto point provides an upper bound on the gap versus any possible Pareto point (i.e. with any amount of \u201cideal\u201d regularization). And in practice, as well as theoretically, a validation approach\nas described above will behave very similar to f\u0302\u03b4\u2217 . We therefore define the cost of overfitting as the (multiplicative) gap between the interpolating predictor f\u03020 and the optimaly regularized f\u0302\u03b4\u2217 : Definition 1. Given any data distribution D over X \u00d7 R and sample size n \u2208 N, we define the cost of overfitting as\nC(D, n) := R(f\u03020) inf\u03b4\u22650R(f\u0302\u03b4) . (14)\nIt is possible to directly analyzeR(f\u03020) andR(f\u0302\u03b4\u2217) in order to study the cost of overfitting. However, any bound onR(f\u03020) orR(f\u0302\u03b4\u2217) will necessarily depend on the target function. Instead, we show that there is a much simpler argument to bound the cost of overfitting.\nTheorem 1. Consider E0 defined in (11) with \u03b4 = 0, then it holds that\nC(D, n) \u2264 E0. (15)\nProof. Observe that\nR(f\u0302\u03b4\u2217) = inf \u03b4\u22650\nE\u03b4 ( \u2211\ni\n(1\u2212 Li,\u03b4)2v2i + \u03c32 )\n\u2265 inf \u03b4\u22650\n\u2211\ni\n(1 \u2212 Li,\u03b4)2v2i + \u03c32\n= \u2211\ni\n(1\u2212 Li,0)2v2i + \u03c32\nwhere we use the fact that (1 \u2212 Li,\u03b4)2 decreases as \u03ba\u03b4 decreases, and \u03ba\u03b4 decreases as \u03b4 decreases. The proof concludes by observing \u2211\ni(1\u2212 Li,0)2v2i + \u03c32 = R(f\u03020)/E0.\nIndeed, equations (10) and (11) used to define E0 does not depend on the target coefficients. It is also straightforward to check that if vi = 0, then R(f\u03020) = E0\u03c32 and R(f\u0302\u03b4\u2217) = \u03c32 by choosing \u03b4\u2217 = \u221e, and C(D, n) = E0 for any n. This shows that (15) is the tightest agnostic bound on the cost of overfitting:\n\u2200P (x) E0 = sup P (y|x) C(D, n) (16)\nwhere E0 on the left-hand-side depends only on the marginal P (x), while C(D, n) depends on both the marginal P (x) and the conditional P (y|x).\nMore generally, it is clear that we have the lower bound C(D, n) \u2265 E0 \u03c3 2\nR(f\u0302\u03b4\u2217 ) due to the non-\nnegativity of v2i in (12). Therefore, it is also possible to show per-instance lower bound by analyzing R(f\u0302\u03b4\u2217), which should be close to \u03c3 2 in well-specified settings. Furthermore, the argument in the proof of Theorem 1 shows something more: for any \u03b4 \u2264 \u03b4\u2217, it holds that R(f\u0302\u03b4) \u2264 E\u03b4R(f\u0302\u03b4\u2217) \u2264 E0R(f\u0302\u03b4\u2217). Therefore, the quantity E0 bounds the cost of overfitting not only for the interpolating solution, but also for any ridge model with a sufficiently small regularization parameter \u03b4. Consequently, if E0 is close to one, then the risk curve will become flat once all of the signal is fitted (for example, see Figure 1 of Zhou et al. 2021), exhibiting the double descent phenomenon instead of the classical U-shape curve (Belkin et al. 2019). Similar results on the flatness of the generalization curve are proven in Tsigler and Bartlett (2020) and Zhou et al. (2021)."
        },
        {
            "heading": "3.1 Benign Overfitting",
            "text": "In this section, we discuss when E0 can be close to 1 and so overfitting is benign. Note that the target coefficients play no role at all in our analysis. To further upper bound the cost of overfitting, we will introduce the notion of effective rank (Bartlett et al. 2020). Definition 2. The effective ranks of a covariance matrix with eigenvalues {\u03bbi}\u221ei=1 in descending order are defined as\nrk =\n\u2211\ni>k \u03bbi\n\u03bbk+1 and Rk :=\n( \u2211 i>k \u03bbi )2\n\u2211\ni>k \u03bb 2 i\n. (17)\nThe two effective ranks are closely related to each other by the relationship rk \u2264 Rk \u2264 r2k and are equal if \u03a3 is the identity matrix (Bartlett et al. 2020). Roughly speaking, the minimal norm interpolant can approximate the target in the span of top k eigenfunctions and use the remaining components of x to memorize the residual. A large effective rank ensures that the small eigenvalues of \u03a3 are roughly equal to each other and so it is possible to evenly spread out the cost of overfitting into many different directions. More precisely, we show the following finite-sample bound on E0, which decreases to 1 as n increases if k = o(n) and Rk = \u03c9(n):\nTheorem 2. For any k < n, it holds that\nE0 \u2264 ( 1\u2212 k n\n)\u22122(\n1\u2212 n Rk\n)\u22121\n+\n. (18)\nThe conditions that k = o(n) and Rk = \u03c9(n) are two key conditions for benign overfitting in linear regression (Bartlett et al. 2020). They require an additional assumption that r0 = o(n) for\nconsistency, which is sufficient for the consistency of the optimally tuned model when the target is well-specified. Our Theorem 2 provides a more refined understanding of benign overfitting: at a finite sample n, if we can choose a small k such that Rk is large relative to n, then the interpolating ridgeless solution is nearly as good as the optimally tuned model, regardless of whether the optimally tuned model can learn the target. Furthermore, we also recover a version of the matching lower bound of Theorem 4 in Bartlett et al. (2020), though our proof technique is completely different and simpler since we have a closed-form expression. Since E0 = ( 1\u2212 1n \u2211 i L2i,0 )\u22121 , it suffices to lower bound 1n \u2211\ni L2i,0. Theorem 3. Fix any b > 0. If there exists k < n such that n \u2264 k + brk, then let k be the first such integer. Otherwise, pick k = n. It holds that\n1\nn\n\u2211\ni\nL2i,0 \u2265 max {\n1\n(b + 1)2\n(\n1\u2212 k n\n)2 n\nRk ,\n(\nb\nb+ 1\n)2 k\nn\n}\n. (19)\nFor simplicity, we can take b = 1 in the lower bound above. We see that E0 cannot be close to 1 unless k is small relative to n. Even if k is small, the first term in (19) requires n/Rk to be small. Conversely, if both k/n and n/Rk are small, then we can apply Theorem 2 to show that E0 is close to 1 and we have identify the necessary and sufficient condition for E0 \u2192 1. Corollary 1. For any n \u2208 N, let kn be the first integer k < n such that n \u2264 k + rk. Then E0 \u2192 1 if and only if\nlim n\u2192\u221e kn n = 0 and lim n\u2192\u221e n Rkn = 0. (20)\nThough Corollary 1 is stated as an asymptotic result, the spectrum is allowed to change with the sample size n and the target function plays no role in condition (20). Next, we apply our results to some canonical examples where overfitting is benign.\nExample 1 (Benign covariance from Bartlett et al. (2020)).\n\u03bbi = i \u22121 log\u2212\u03b1 i for some \u03b1 > 0.\nIn this case, we can estimate\n\u2211\ni>k\n\u03bbi \u2265 \u222b \u221e\nk+1\n1\nx log\u03b1 x dx =\n1\n(\u03b1\u2212 1) log\u03b1\u22121(k + 1) \u2211\ni>k\n\u03bb2i \u2264 1\nk + 1\n\u222b \u221e\nk\n1\nx log2\u03b1 x dx =\n1\n(k + 1)(2\u03b1\u2212 1) log2\u03b1\u22121(k)\nand so\nRk \u2265 (k + 1)(2\u03b1\u2212 1) log2\u03b1\u22121(k) (\u03b1\u2212 1)2 log2\u03b1\u22122(k + 1) = \u0398 (k log k) .\nThen by choosing k = \u0398 (\nn\u221a logn\n)\n, we have k = o(n) and Rk = \u03c9(n) because Rk n = \u0398(log 1/2 n).\nExample 2 (Junk features from Zhou et al. (2020)).\n\u03bbi =\n\n\n\n1 if i \u2264 dS 1 dJ\nif dS + 1 \u2264 i \u2264 dS + dJ 0 if i > dS + dJ .\nIn this case, it is routine to checkRk = dJ by choosing k = dS . Letting dS = o(n) and dJ = \u03c9(n), Theorem 2 shows that E0 \u2192 1.\nFinally, we show our bound (18) also applies to isotropic features in the proportional regime even though overfitting is not necessarily benign.\nExample 3 (Isotropic features in the proportional regime).\n\u03bbi =\n{\n1 if i \u2264 d 0 otherwise\nfor d = \u03b3n and \u03b3 > 1.\nIn this case, it is easy to check that rk = d \u2212 k and so k + rk = d > n and kn = 0. The first condition in (20) holds because kn/n = 0. However, the second condition in (20) does not hold because Rk = d\u2212 k and n/Rkn = 1/\u03b3 > 0. Plugging in k = 0 to Theorem 2, we obtain\nE0 \u2264 ( 1\u2212 n d )\u22121 = \u03b3 \u03b3 \u2212 1 .\nThe above upper bound is tight when vi = 0 because it is well-known that in the proportional regime (for example, see Hastie et al. (2019) and Zhou et al. (2021)), it holds that\nlim n\u2192\u221e\nR(f\u03020) = \u03c3 2 \u03b3\n\u03b3 \u2212 1 ."
        },
        {
            "heading": "3.2 Tempered Overfitting",
            "text": "Theorem 2 allows us to understand the cost of overfitting when it is benign. However, it is not informative when no k < n satisfies Rk > n. In Theorem 4 below, we provide an estimate for the amount of \"tempered\" overfitting based on the ratio k/rk over a finite range of indices. Theorem 4. Fix any \u01eb > 0 and consider kl, ku \u2208 N given by kl :=max {k \u2265 0 : k + \u01ebrk \u2264 n} ku :=min {k \u2265 0 : k + rk \u2265 (1 + \u01eb\u22121)n}. (21)"
        },
        {
            "heading": "Then it holds that",
            "text": "E0 \u2264 (1 + \u01eb)2 \u00b7 max kl\u2264k<ku\n(\n\u03bbk+1 \u03bbk+2 + 1 \u01eb k + 1 rk \u2212 1\n)\n. (22)\nTo interpret (22), we first suppose that the spectrum {\u03bbi} does not change with n and has infinitely many non-zero eigenvalues (which is the case in Example 1, 4 and 5 below). For any fixed \u01eb > 0, kl must increases as n increases. If k is large, then it is usually the case that \u03bbk+1 \u2248 \u03bbk or the ratio is bounded. Letting \u01eb = 1, we can understand (22) as E0 . 1 + krk . In particular, if rk = \u2126(k), then E0 is bounded and overfitting cannot be catastrophic. Conversely, we show that overfitting is catastrophic when rk = o(k) in section 3.3 below. Therefore, the condition limk\u2192\u221e k/rk = \u221e is both necessary and sufficient for catastrophic overfitting: E0 \u2192 \u221e. Furthermore, we argue that (22) is also sufficient for benign overfitting in some settings: if limk\u2192\u221e k/rk = 0, then we have limn\u2192\u221e E0 \u2264 (1 + \u01eb)2 for any \u01eb > 0, and thus E0 \u2192 1. Example 4 (Power law decay from Mallinar et al. (2022)).\n\u03bbi = i \u2212\u03b1 for some \u03b1 > 1.\nIn this case, we can estimate\n1 (\u03b1\u2212 1)(k + 1)\u03b1\u22121 = \u222b \u221e\nk+1\nx\u2212\u03b1 dx \u2264 \u2211\ni>k\n\u03bbi \u2264 \u222b \u221e\nk\nx\u2212\u03b1 dx = 1\n(\u03b1 \u2212 1)k\u03b1\u22121\n1 (2\u03b1\u2212 1)(k + 1)2\u03b1\u22121 = \u222b \u221e\nk+1\nx\u22122\u03b1 dx \u2264 \u2211\ni>k\n\u03bb2i \u2264 \u222b \u221e\nk\nx\u22122\u03b1 dx = 1\n(2\u03b1\u2212 1)k2\u03b1\u22121\nand so (\nk\nk + 1\n)\n(\u03b1\u2212 1) \u2264 k rk \u2264 ( k k + 1 )\u03b1\u22121 (\u03b1\u2212 1).\nTherefore, we have limk\u2192\u221e k/rk = \u03b1\u2212 1 and so E0 . \u03b1, which agrees with Mallinar et al. 2022."
        },
        {
            "heading": "3.3 Catastrophic Overfitting",
            "text": "We first state a generic non-asymptotic lower bound on E0 = ( 1\u2212 1n \u2211 i L2i,0 )\u22121\nand then discuss the implication for catastrophic overfitting as n increases. Theorem 5. For any k \u2265 n, it holds that 1\nn\n\u2211\ni\nL2i,0 \u2265 n\nk\n(\nk \u2212 n k \u2212 n+ rk\n)2\n. (23)\nFor any \u01eb > 0, if rk = o(k) and we consider k = (1 + \u01eb)n, then it is straightforward from (23) that limn\u2192\u221e 1 n \u2211 i L2i,0 \u2265 (1+ \u01eb)\u22121. Since the choice of \u01eb is arbitrary, we have limn\u2192\u221e 1n \u2211\ni L2i,0 = 1 and so E0 \u2192 \u221e. Example 5 (Exponential decay).\n\u03bbi = e \u2212i.\nIn this case, we can estimate \u2211\ni>k\n\u03bbi \u2264 \u222b \u221e\nk\ne\u2212x dx = e\u2212k\nand rk \u2264 e and rk/k \u2192 0. Theorem 5 implies that overfitting is catastrophic, as expected from Mallinar et al. 2022.\nSince Theorem 3, 4 and 5 are agnostic and non-asymptotic, we can use them to obtain an elegant characterization of whether overfitting is benign, tempered, or catastrophic, resolving an open problem2 raised by Mallinar et al. (2022): Theorem 6. Suppose that the spectrum {\u03bbi} is fixed as n increases and contains infinitely many non-zero eigenvalues.\n(a) If limk\u2192\u221e k/rk = 0, then overfitting is benign: limn\u2192\u221e E0 = 1. (b) If limk\u2192\u221e k/rk \u2208 (0,\u221e), then overfitting is tempered: limn\u2192\u221e E0 \u2208 (1,\u221e). (c) If limk\u2192\u221e k/rk = \u221e, then overfitting is catastrophic: limn\u2192\u221e E0 = \u221e."
        },
        {
            "heading": "4 Application: Inner-Product Kernels in the Polynomial Regime",
            "text": "In this section, we consider KRR with inner-product kernels in the polynomial regime (Ghorbani et al. 2021; Mei et al. 2022; Misiakiewicz 2022). Let\u2019s take the distribution of x to be uniformly distributed over the hypersphere in Rd or the boolean hypercube. Denote V\u2264l\u22121 to be the subspace of all polynomials of degree \u2264 l\u22121 andB(d, l) = \u0398d(dl) to be the dimension of the subspace Vl of degree-l polynomials orthogonal to V\u2264l\u22121. Moreover, denote P\u2264\u230al\u230b to be the projection onto V\u2264\u230al\u230b and P>\u230al\u230b to be the projection onto its complement. Let {Yks}k\u22650,s\u2208[B(d,k)] be the polynomial basis with respect to D (e.g. spherical harmonics or parity functions).\nInner-product kernels. Consider kernels of the form K(x, x\u2032) = hd(\u3008x, x\u2032\u3009/d), then it admits the eigendecompositon in the polynomial basis:\nK(x, x\u2032) = \u221e \u2211\nk=0\n\u2211\ns\u2208[B(d,k)]\n\u00b5d,k(h) B(d, k) Yks(x)Yks(x \u2032). (24)\nWe also expand the target according to (9) and define f\u2217(x) := \u2211\u221e\nk=0\n\u2211\ns\u2208[B(d,k)] vksYks(x). Interestingly, the eigenvalues of K with respect to D has a block diagonal structure. The block diagonal structure is a consequence of the rotation-invariance of the distribution of x.\nPolynomial regime. Consider the regime n \u224d dl where l is not an integer. We will choose k in Theorem 2 to include the first \u230al\u230b blocks. Then\nk =\n\u230al\u230b \u2211\nk=0\nB(d, k) = \u0398\n\n\n\u230al\u230b \u2211\nk=0\ndk\n\n = \u0398 ( d\u230al\u230b ) = o(n). (25)\nand\nRk =\n(\n\u2211 k>\u230al\u230b \u2211 s\u2208[B(d,k)] \u00b5d,k(h) B(d,k)\n)2\n\u2211 k>\u230al\u230b \u2211 s\u2208[B(d,k)]\n(\n\u00b5d,k(h) B(d,k)\n)2 \u2265\n(\n\u2211 k>\u230al\u230b \u00b5d,k(h) )2 \u2211\nk>\u230al\u230b \u00b5d,k(h) 2\n\u00b7B(d, \u2308l\u2309)\n\u2265 B(d, \u2308l\u2309) = \u2126(d\u2308l\u2309) = \u03c9(n).\n(26)\n2See footnote 11 in their paper. The settings they consider (e.g., clause (a) of Theorem 3.1 with \u03b4 > 0)\nalways satisfy R(f\u0302\u03b4\u2217) = \u03c3 2 and so limn\u2192\u221e R(f\u03020) = limn\u2192\u221e E0 \u00b7 \u03c3 2.\nHence, the cost of overfitting is small when l is bounded away from the integers. To obtain a bound on the error of the ridgeless solution, it suffices to analyze the error of the optimally regularized model, which can be easily done with uniform convergence. Using the predictions from Simon et al. 2021, we can also recover a type of uniform convergence known as \"optimistic rate\" (Panchenko 2002; Srebro et al. 2010; Zhou et al. 2021), which is suitable for the square loss.\nTheorem 7. Fix any k \u2208 N and let \u01eb = \u221a (k2 + 2kn)/n2. For any \u03b4 \u2265 0, it holds that\n(1\u2212 \u01eb) \u221a R(f\u0302\u03b4)\u2212 \u221a R\u0302(f\u0302\u03b4) \u2264\n\u221a\n( \u2211 i>k \u03bbi)\u2016f\u0302\u03b4\u20162H n . (27)\nNote that the error of the predictor P\u2264\u230al\u230bf \u2217 is approximately\n\u03c32 + \u2211\nk>\u230al\u230b\n\u2211\ns\u2208[B(d,k)] v2i = \u03c3 2 + \u2016P>\u230al\u230bf\u2217\u20162. (28)\nand we can tune \u03b4\u2217 to match the training error of f\u0302\u03b4\u2217 to (28) and the Hilbert norm satisfies \u2016f\u0302\u03b4\u2016H \u2264 \u2016P\u2264\u230al\u230bf\u2217\u2016H because f\u0302\u03b4 is Pareto-optimal. Moreover, the expected norm of the feature is\n\u2211\nk>\u230al\u230b\n\u2211\ns\u2208[B(d,k)]\n\u00b5d,k(h)\nB(d, k) = \u2211\nk>\u230al\u230b \u00b5d,k(h), (29)\nand so if \u2016P\u2264\u230al\u230bf\u2217\u20162H \u00b7 ( \u2211 k>\u230al\u230b \u00b5d,k(h) ) = o(n), then limn\u2192\u221eR(f\u0302\u03b4\u2217) \u2264 \u03c32 + \u2016P>\u230al\u230bf\u2217\u20162. In Ghorbani et al. (2021) and Mei et al. (2022), it is shown that the above is not just an upper bound. In fact, it holds that limn\u2192\u221eR(f\u03020) = \u03c32 + \u2016P>\u230al\u230bf\u2217\u20162 and our application is tight in this case."
        },
        {
            "heading": "5 Conclusion",
            "text": "Understanding the effect of overfitting is a fundamental problem in statistical learning theory. Contrary to the traditional intuition, prior works have shown that predictors that interpolate noisy training labels can achieve nearly optimal test error when the data distribution is well-specified. In this paper, we extend these results to the agnostic case and we use them to develop a more refined understanding of benign, tempered, and catastrophic overfitting. To the best of our knowledge, our work is the first to connect the complex closed-form risk predictions and the effective rank introduced by Bartlett et al. 2020 to establish simple and interpretable learning guarantee for KRR. As we can see in Corollary 1 and Theorem 6, the effective ranks play a crucial role in the analysis and tightly characterize the cost of overfitting in many settings.\nSince our results are based on the non-rigorous predictions from Simon et al. 2021, an important future direction is to recover the sharp characterization of the cost of overfitting using rigorous techniques. It is also interesting to ask whether our results extend to other settings, such as kernel SVM, since our theory is agnostic to the target. We hope that the theory of KRR and ridge regression with Gaussian features can lead us toward a better understanding of generalization in neural networks."
        },
        {
            "heading": "A Supplemental Proofs",
            "text": "In the appendix, we give proofs of all results from the main text. Our proofs are very self-contained and only use elementary results such as the Cauchy-Schwarz inequality."
        },
        {
            "heading": "A.1 Upper Bounds",
            "text": "The main challenge for analyzing E0 from equation (11) is that the effective regularization \u03ba0 is defined by the non-linear equation (10), which does not have a simple closed-form solution. However, the following lemma can provide an estimate for \u03ba0 in terms of the effective rank.\nLemma 1. For any k \u2208 N, it holds that\n\u03ba0 \u2265 ( 1\u2212 n Rk\n) \u2211\ni>k \u03bbi\nn and \u03ba0 \u2265 \u03bbk+1\n(\nk + rk n \u2212 1 ) . (30)\nMoreover, for any k < n, it holds that\n\u03ba0 <\n(\n1\u2212 k n\n)\u22121 \u2211 i>k \u03bbi\nn . (31)\nProof. From the Cauchy-Schwarz inequality, we show that\n(\n\u2211\ni>k\n\u03bbi\n)2\n=\n(\n\u2211\ni>k\n\u221a\n\u03bbi \u03bbi + \u03ba0 \u221a \u03bbi(\u03bbi + \u03ba0)\n)2\n\u2264 ( \u2211\ni>k\n\u03bbi \u03bbi + \u03ba0\n)(\n\u2211\ni>k\n\u03bbi(\u03bbi + \u03ba0)\n)\n\u2264 ( \u2211\ni\n\u03bbi \u03bbi + \u03ba0\n)(\n\u2211\ni>k\n\u03bbi(\u03bbi + \u03ba0)\n)\n= n\n(\n\u2211\ni>k\n\u03bb2i + \u03ba0 \u2211\ni>k\n\u03bbi\n)\n.\nRearranging in terms of \u03ba0 proves the first inequality. Moreover, it holds that\nn = \u2211\ni\u2264k\n\u03bbi \u03bbi + \u03ba0 + \u2211\ni>k\n\u03bbi \u03bbi + \u03ba0\n\u2265 k\u03bbk+1 \u03bbk+1 + \u03ba0 +\n\u2211\ni>k \u03bbi\n\u03bbk+1 + \u03ba0 .\nwhich can be rearranged to the second lower bound. Finally, observe that\nn = \u2211\ni\n\u03bbi \u03bbi + \u03ba0 < k +\n\u2211\ni>k \u03bbi\n\u03ba0\nand rearranging concludes the proof of the last inequality.\nIn particular, when there exists k such that k = o(n) and Rk = \u03c9(n), then \u03ba0 \u2248 \u2211\ni>k \u03bbi/n. Using lemma 1, we can show Theorem 2.\nTheorem 2. For any k < n, it holds that\nE0 \u2264 ( 1\u2212 k n\n)\u22122(\n1\u2212 n Rk\n)\u22121\n+\n. (18)\nProof. For any \u03b4 \u2265 0, by the definition (10), we have\nn\u2212 \u03b4 \u03ba\u03b4 = \u2211\ni\n\u03bbi \u03bbi + \u03ba\u03b4\n\u2264 \u2211\ni\u2264k\n\u03bbi \u03bbi + \u03ba\u03b4 + \u2211\ni>k\n\u221a \u03bbi\n\u03bbi + \u03ba\u03b4\n\u221a\n\u03bbi\n\u2264 k + \u221a \u2211\ni>k\n\u03bbi (\u03bbi + \u03ba\u03b4)2 \u2211\ni>k\n\u03bbi.\nRearranging, we get (\nn\u2212 k \u2212 \u03b4\u03ba\u03b4 )2\n\u2211 i>k \u03bbi \u2264 \u2211\ni>k\n\u03bbi (\u03bbi + \u03ba\u03b4)2 . (32)\nAt the same time, we can use the definition (10) again and (32) to show that\n1\u2212 1 n \u2211\ni\nL2i,\u03b4 = 1\nn\n\u2211\ni\n[\n\u03bbi \u03bbi + \u03ba\u03b4 \u2212 ( \u03bbi \u03bbi + \u03ba\u03b4\n)2 ]\n+ \u03b4\nn\u03ba\u03b4\n= \u03ba\u03b4 n \u2211\ni\n\u03bbi (\u03bbi + \u03ba\u03b4)2 + \u03b4 n\u03ba\u03b4\n\u2265 \u03ba\u03b4 n\n( n\u2212 k \u2212 \u03b4\u03ba\u03b4 )2\n\u2211 i>k \u03bbi +\n\u03b4\nn\u03ba\u03b4 .\n(33)\nPlugging in \u03b4 = 0 and Lemma 1, we have\nE0 = (\n1\u2212 1 n \u2211\ni\nL2i,0\n)\u22121\n\u2264 (\n\u03ba0 n\n(n\u2212 k)2 \u2211\ni>k \u03bbi\n)\u22121\n=\n(\n1\u2212 k n\n)\u22122(\n1\u2212 n Rk\n)\u22121\nprovided that Rk > n.\nUsing the second part of equation (30), we can show a similar bound that depends rk, which is smaller than Rk, but has a better dependence on k.\nTheorem 8. For any k < n, it holds that\nE0 \u2264 ( 1\u2212 k n\n)\u22121(\n1\u2212 n k + rk\n)\u22121\n+\n. (34)\nProof. For i \u2265 k + 1, it holds that \u03bbi \u2264 \u03bbk+1 and so by Lemma 1, we have \u03ba0\n\u03bbi + \u03ba0 \u2265 \u03ba0 \u03bbk+1 + \u03ba0 \u2265 k+rk n \u2212 1 k+rk n = 1\u2212 n k + rk .\nFinally, by equation (10), we have\nE\u221210 = 1\nn\n\u2211\ni\n\u03bbi \u03bbi + \u03ba0 \u03ba0 \u03bbi + \u03ba0\n\u2265 1 n \u2211\ni\u2265k+1\n\u03bbi \u03bbi + \u03ba0 \u03ba0 \u03bbi + \u03ba0\n\u2265 ( 1\u2212 k n )( 1\u2212 n k + rk ) .\nTaking the inverse on both hand side concludes the proof.\nFinally, we prove Theorem 4. The proof goes through a different argument to avoid the dependence on 1\u2212 k/n because we might need to choose k = \u2126(n) when overfitting is tempered.\nTheorem 4. Fix any \u01eb > 0 and consider kl, ku \u2208 N given by kl :=max {k \u2265 0 : k + \u01ebrk \u2264 n} ku :=min {k \u2265 0 : k + rk \u2265 (1 + \u01eb\u22121)n}.\n(21)"
        },
        {
            "heading": "Then it holds that",
            "text": "E0 \u2264 (1 + \u01eb)2 \u00b7 max kl\u2264k<ku\n(\n\u03bbk+1 \u03bbk+2 + 1 \u01eb k + 1 rk \u2212 1\n)\n. (22)\nProof. If \u01eb \u2264 n/r0, then it is clear that k = 0 satisfies k + \u01ebrk \u2264 n. It is also clear that choosing k \u2265 (1+\u01eb\u22121)n satisfies k+rk \u2265 (1+\u01eb\u22121)n because rk \u2265 0. Then both kl and ku are well-defined. To show that both are finite, we observe that kl \u2264 kl + \u01ebrkl \u2264 n by definition and ku \u2264 (1 + \u01eb\u22121)n because it is defined as the minimum k.\nNext, let k\u2217 be the smallest integer such that \u03bbk\u2217 \u2264 \u01eb\u03ba0. We will show that k\u2217 is also well defined and k\u2217 \u2208 [kl + 2, ku + 1]. Note that for any k < n, we can apply Lemma 1 to show\n\u01eb\u03ba0 < \u01eb\n\u2211\ni>k \u03bbi n\u2212 k = \u01ebrk n\u2212 k\u03bbk+1.\nTherefore, by our definition of kl and k \u2217, it holds that \u03bbkl+1 > \u01eb\u03ba0 \u2265 \u03bbk\u2217 . Since the eigenvalues are sorted, it must hold that k\u2217 > kl + 1. On the other hand, for any k \u2208 N, we also apply Lemma 1 to show\n\u01eb\u03ba0 \u2265 \u03bbk+1\u01eb ( k + rk n \u2212 1 )\nBy our definition of ku and k \u2217, it holds that \u03bbku+1 \u2264 \u01eb\u03ba0 and so k\u2217 \u2264 ku + 1. Finally, since we have \u03bbi \u2264 \u03bbk\u2217 \u2264 \u01eb\u03ba0 for all i \u2265 k\u2217 and \u03bbk\u2217\u22121 > \u01eb\u03ba0, we can check that\nE\u221210 = 1\u2212 1\nn\n\u2211\ni\nL2i,0 = \u03ba0 n \u2211\ni\n\u03bbi (\u03bbi + \u03ba0)2\n\u2265 \u03ba0 n \u2211\ni\u2265k\u2217\n\u03bbi (\u03bbi + \u03ba0)2\n\u2265 1 (1 + \u01eb)2 1\nn\u03ba0\n\u2211 i\u2265k\u2217 \u03bbi >\n\u01eb (1 + \u01eb)2 1 n\n\u2211\ni\u2265k\u2217\u22121 \u03bbi \u2212 \u03bbk\u2217\u22121 \u03bbk\u2217\u22121\n= \u01eb (1 + \u01eb)2 rk\u2217\u22122 \u2212 1 n .\nRecall that k\u2217 \u2212 1 \u2265 kl + 1 and so by definition of kl, we have k\u2217 \u2212 1 + \u01ebrk\u2217\u22121 > n. Therefore, it holds that\nE0 < (1 + \u01eb)2\n\u01eb\nk\u2217 \u2212 1 + \u01ebrk\u2217\u22121 rk\u2217\u22122 \u2212 1\n= (1 + \u01eb)2 [ \u03bbk\u2217\u22121 \u03bbk\u2217 + 1 \u01eb (k\u2217 \u2212 2) + 1 rk\u2217\u22122 \u2212 1 ] .\nwhere in the last step we use\nrk\u2217\u22122 \u2212 1 = \u2211 i>k\u2217\u22122 \u03bbi\n\u03bbk\u2217\u22121 \u2212 1 =\n\u2211\ni>k\u2217\u22121 \u03bbi \u03bbk\u2217\u22121\n= \u03bbk\u2217\n\u03bbk\u2217\u22121 rk\u2217\u22121.\nThe rest follows from the fact that k\u2217 \u2212 2 \u2208 [kl, ku \u2212 1]."
        },
        {
            "heading": "A.2 Lower Bounds",
            "text": "We will now prove two lower bound for E0.\nTheorem 3. Fix any b > 0. If there exists k < n such that n \u2264 k + brk, then let k be the first such integer. Otherwise, pick k = n. It holds that\n1\nn\n\u2211\ni\nL2i,0 \u2265 max {\n1\n(b + 1)2\n(\n1\u2212 k n\n)2 n\nRk ,\n(\nb\nb+ 1\n)2 k\nn\n}\n. (19)\nProof. First, suppose that there exists k < n such that n \u2264 k+ brk and let k be the first such integer. Then we can rearrange n \u2264 k + brk into\n\u03bbk+1 \u2264 b \u2211 i>k \u03bbi\nn\u2212 k ,\nand since \u03bbi \u2264 \u03bbk+1 for i > k, we apply the above and equation (30) of Lemma 1 to show that \u2211\ni\nL2i,0 \u2265 \u2211\ni>k\n(\n\u03bbi \u03bbi + \u03ba0\n)2\n\u2265 \u2211 i>k \u03bb 2 i\n( b \u2211 i>k \u03bbi n\u2212k + \u2211 i>k \u03bbi n\u2212k\n)2 = n\n(b+ 1)2\n(\n1\u2212 k n\n)2 n\nRk .\nMoreover, by the definition of k, we must have n > k \u2212 1 + brk\u22121 which can be rearranged to\n\u03bbk > b\n\u2211\ni>k\u22121 \u03bbi\nn\u2212 k + 1 \u2265 b\u03ba0 by equation (30) of Lemma 1 again. Then for any i \u2264 k, we have \u03bbi \u2265 \u03bbk > b\u03ba0 and so \u03ba0 < \u03bbi/b. Therefore, we have\n\u2211\ni\nL2i,0 \u2265 \u2211\ni\u2264k\n(\n\u03bbi \u03bbi + \u03ba0\n)2\n\u2265 k ( b\nb+ 1\n)2\n.\nFinally, if there is no such k, then the first inequality is trivial. Moreover, we have n > n\u22121+brn\u22121 which rearranges to \u03bbn \u2265 b \u2211\ni>n\u22121 \u03bbi > b\u03ba0. Therefore, by all i \u2264 n, we have \u03bbi \u2265 \u03bbn > b\u03ba0 and the rest of the proof is the same.\nTheorem 5. For any k \u2265 n, it holds that 1\nn\n\u2211\ni\nL2i,0 \u2265 n\nk\n(\nk \u2212 n k \u2212 n+ rk\n)2\n. (23)\nProof. By the Cauchy-Schwarz inequality, we have\nn = \u2211\ni>k\n\u03bbi \u03bbi + \u03ba0 + \u2211\ni\u2264k\n\u03bbi \u03bbi + \u03ba0\n\u2264 \u2211 i>k \u03bbi \u03ba0 + \u221a k\n\u221a \u221a \u221a \u221a \u2211\ni\u2264k\n(\n\u03bbi \u03bbi + \u03ba0\n)2\n.\nBy Lemma 1, we have \u03ba0 \u2265 \u03bbk+1 ( k+rk n \u2212 1 ) . Combine with above, we obtain\nn \u2264 nrk k + rk \u2212 n\n+ \u221a k\n\u221a \u221a \u221a \u221a \u2211\ni\u2264k\n(\n\u03bbi \u03bbi + \u03ba0\n)2\n.\nRearranging gives us\nn\u221a k k \u2212 n k + rk \u2212 n \u2264\n\u221a \u221a \u221a \u221a \u2211\ni\u2264k\n(\n\u03bbi \u03bbi + \u03ba0\n)2\n,\nwhich implies that\n1\nn\n\u2211\ni\nL2i,0 \u2265 1\nn\n\u2211\ni\u2264k\n(\n\u03bbi \u03bbi + \u03ba0\n)2\n\u2265 n k\n(\nk \u2212 n k + rk \u2212 n\n)2\n."
        },
        {
            "heading": "A.3 Taxonomy of Overfitting",
            "text": "Theorem 6. Suppose that the spectrum {\u03bbi} is fixed as n increases and contains infinitely many non-zero eigenvalues.\n(a) If limk\u2192\u221e k/rk = 0, then overfitting is benign: limn\u2192\u221e E0 = 1. (b) If limk\u2192\u221e k/rk \u2208 (0,\u221e), then overfitting is tempered: limn\u2192\u221e E0 \u2208 (1,\u221e). (c) If limk\u2192\u221e k/rk = \u221e, then overfitting is catastrophic: limn\u2192\u221e E0 = \u221e.\nProof. We will show each clause separately.\n(a) For any \u01eb > 0, we can pick k = \u01ebn in Theorem 2 and obtain the following:\nE0 \u2264 1 (1\u2212 \u01eb)2 ( 1\u2212 1 \u01eb k\nRk\n)\u22121 . (35)\nSince we have \u2211\ni>k\n\u03bb2i \u2264 \u03bbk+1 \u2211\ni>k\n\u03bbi =\u21d2 Rk \u2265 rk,\nwe can send n\u2192 \u221e and k/Rk \u2264 k/rk \u2192 0. Therefore, it holds that\nlim n\u2192\u221e\nE0 \u2264 1\n(1\u2212 \u01eb)2 .\nSince the choice of \u01eb > 0 can be made arbitrarily small, we have the desired conclusion by taking \u01eb\u2192 0.\n(b) If {k/rk} converges to a non-zero constant, then the sequence must be bounded. In particular, there exists M > 0 such that rk < kM for all k. If we let b = 1/(3M) in Theorem 3, then for all k \u2264 n/2, it holds that\nk + brk < k(1 + bM) \u2264 1 + bM 2 n \u2264 2n 3 < n.\nThen we need to choose k > n/2 in Theorem 3 and\n1\nn\n\u2211\ni\nL2i,0 \u2265 1\n2(1 + 3M)2\nand so limn\u2192\u221e E0 > 1. Similarly, there also exists m > 0 such that rk > mk for all k. Then by choosing k = \u221a\n1 1+mn and Theorem 8, we have\nE0 \u2264 ( 1\u2212 k n\n)\u22121(\n1\u2212 1 1 +m n k\n)\u22121 = (\n1\u2212 1\u221a 1 +m\n)\u22122 <\u221e. (36)\n(c) We will apply Theorem 5. For any \u01eb > 0, choose k = (1 + \u01eb)n, we get\n1\nn\n\u2211\ni\nL2i,0 \u2265 1\n1 + \u01eb\n(\n1\u2212 rk k 1 + \u01eb \u01eb\n)2\nTherefore, if rk = o(k), then\nlim n\u2192\u221e\n1\nn\n\u2211\ni\nL2i,0 \u2265 1\n1 + \u01eb\nHowever, since the choice of \u01eb is arbitrary, then we can send \u01eb\u2192 0. The desired conclusion follows by E0 = ( 1\u2212 1n \u2211 i L2i,0 )\u22121 .\nRemark 1. As mentioned in the main text, it is also possible to use Theorem 4 to show the upper bounds in the proof of Theorem 6 above. For simplicity, we use a different argument here by applying Theorem 2 and 8."
        },
        {
            "heading": "B Uniform Convergence",
            "text": "In this appendix, we show that the predictions from Simon et al. 2021 can establish a type of uniform convergence guarantee known as \"optimistic rate\" (Panchenko 2002; Srebro et al. 2010) along the ridge path, which maybe of independent interest. We briefly mention the uniform convergence result in section 4 of the main text.\nIn particular, the tight result from Zhou et al. (2021) avoids any hidden multiplicative constant and logarithmic factor present in previous works and can be used to establish benign overfitting. However, their proof techniques depend on the Gaussian Minimax Theorem (GMT) and are limited to the setting of Gaussian features. We recover their result in Theorem 7 here with a (non-rigorous) calculation that extends beyond the Gaussian case."
        },
        {
            "heading": "B.1 Formula for Training Error and Hilbert Norm",
            "text": "We first provide closed-form expression for the training error and Hilbert norm of f\u0302\u03b4. By the predictions from Simon et al. 2021, we know that\nR\u0302(f\u0302\u03b4) = \u03b42\nn2\u03ba2\u03b4 R(f\u0302\u03b4) (37)\nand we can use section 4.1 of Simon et al. 2021 to compute the expected Hilbert norm:\nE \u2016f\u0302\u03b4\u20162H = \u2211\ni\nE[v\u03022i ]\n\u03bbi = \u2211\ni\nE[v\u0302i] 2 +Var[v\u0302i]\n\u03bbi\n= \u2211\ni\nL2i,\u03b4v2i + L2i,\u03b4R(f\u0302\u03b4) n\n\u03bbi\n= \u2211\ni\nL2i,\u03b4v2i \u03bbi + R(f\u0302\u03b4) n \u2211\ni\nL2i,\u03b4 \u03bbi .\nTherefore, we will just use the expression:\n\u2016f\u0302\u03b4\u20162H = \u2211\ni\n\u03bbiv 2 i\n(\u03bbi + \u03ba\u03b4)2 + R(f\u0302\u03b4) n \u2211\ni\n\u03bbi (\u03bbi + \u03ba\u03b4)2 . (38)"
        },
        {
            "heading": "B.2 Optimistic Rate",
            "text": "Theorem 7. Fix any k \u2208 N and let \u01eb = \u221a (k2 + 2kn)/n2. For any \u03b4 \u2265 0, it holds that\n(1\u2212 \u01eb) \u221a R(f\u0302\u03b4)\u2212 \u221a R\u0302(f\u0302\u03b4) \u2264\n\u221a\n( \u2211 i>k \u03bbi)\u2016f\u0302\u03b4\u20162H n . (27)\nProof. Applying equation (12) and (10), we can write the difference\n\u221a R(f\u0302\u03b4)\u2212 \u221a R\u0302(f\u0302\u03b4) =\n(\n1\u2212 \u03b4 n\u03ba\u03b4\n) \u221a\nR(f\u0302\u03b4)\n\u2264 ( 1\nn\n\u2211\ni\n\u03bbi \u03bbi + \u03ba\u03b4\n)\n\u221a\nR(f\u0302\u03b4).\nBy the Cauchy-Schwarz inequality, for any k \u2208 N, we have (\n\u2211\ni\n\u03bbi \u03bbi + \u03ba\u03b4\n)2\n\u2264 ( k + \u2211\ni>k\n\u03bbi \u03bbi + \u03ba\u03b4\n)2\n= k2 + 2k\n(\n\u2211\ni>k\n\u03bbi \u03bbi + \u03ba\u03b4\n)\n+\n(\n\u2211\ni>k\n\u221a \u03bbi\n\u03bbi + \u03ba\u03b4\n\u221a\n\u03bbi\n)2\n\u2264 k2 + 2kn+ ( \u2211\ni>k\n\u03bbi (\u03bbi + \u03ba\u03b4)2\n)(\n\u2211\ni>k\n\u03bbi\n)\nBy the expression (38), we have\n( \u221a\nR(f\u0302\u03b4)\u2212 \u221a R\u0302(f\u0302\u03b4)\n)2\n\u2264 k 2 + 2kn\nn2 R(f\u0302\u03b4) +\n(\nR(f\u0302\u03b4)\nn\n\u2211\ni>k\n\u03bbi (\u03bbi + \u03ba\u03b4)2\n)(\n1\nn\n\u2211\ni>k\n\u03bbi\n)\n\u2264 k 2 + 2kn\nn2 R(f\u0302\u03b4) +\n\u2016f\u0302\u03b4\u20162H( \u2211 i>k \u03bbi)\nn\nthen using \u221a x+ y \u2264 \u221ax+\u221ay, we show that\n\u221a R(f\u0302\u03b4)\u2212 \u221a R\u0302(f\u0302\u03b4) \u2264\n\u221a\nk2 + 2kn\nn2 R(f\u0302\u03b4) +\n\u2016f\u0302\u03b4\u20162H( \u2211 i>k \u03bbi)\nn\n\u2264 \u221a k2 + 2kn\nn2 R(f\u0302\u03b4) +\n\u221a\n\u2016f\u0302\u03b4\u20162H( \u2211 i>k \u03bbi)\nn .\nRearranging concludes the proof."
        },
        {
            "heading": "B.3 Norm Analysis",
            "text": "Theorem 9. For any l \u2208 N \u222a {\u221e} and k \u2208 N such that Rk > n, it holds that\n\u2016f\u03020\u20162H \u2264 \u2211\ni\u2264l\nv2i \u03bbi +\n(\n1\u2212 n Rk\n)\u22121 n ( \u03c32 + \u2211\ni>l v 2 i\n)\n\u2211 i>k \u03bbi . (39)\nProof. When \u03b4 = 0, it holds that\nn E0 = n\u2212 \u2211\ni\nL2i,0 = \u2211\ni\n\u03bbi \u03bbi + \u03ba0 \u2212 \u03bb 2 i (\u03bbi + \u03ba0)2\n= \u03ba0\n(\n\u2211\ni\n\u03bbi (\u03bbi + \u03ba0)2\n)\nby applying (11) and (10). Therefore, the second term in (38) can be simplified as\nR(f\u03020)\nn\n\u2211\ni\n\u03bbi (\u03bbi + \u03ba0)2\n= E0 ( \u2211 i(1 \u2212 Li,0)2v2i + \u03c32 )\nn\nn\nE0\u03ba0\n= \u2211\ni\n(1\u2212 Li,0)2 \u03ba0 v2i + \u03c32 \u03ba0\n= \u2211\ni\n\u03ba0 (\u03bbi + \u03ba0)2 v2i + \u03c32 \u03ba0\nby the definition in (12). Plugging in, we arrive at\n\u2016f\u03020\u20162H = \u2211\ni\nv2i \u03bbi + \u03ba0 + \u03c32 \u03ba0 (40)\nTo handle situations where f\u2217 is not in the RKHS, observe that for any l, we have\n\u2211\ni\nv2i \u03bbi + \u03ba0 = \u2211\ni\u2264l\nv2i \u03bbi + \u03ba0 + \u2211\ni>l\nv2i \u03bbi + \u03ba0\n\u2264 \u2211\ni\u2264l\nv2i \u03bbi + 1 \u03ba0 \u2211\ni>l\nv2i\nand so\n\u2016f\u03020\u20162H \u2264 \u2211\ni\u2264l\nv2i \u03bbi + 1 \u03ba0\n(\n\u03c32 + \u2211\ni>l\nv2i\n)\n. (41)\nThe proof concludes by plugging in Lemma 1.\nFinally, we can plug in the norm bound of Theorem 9 into Theorem 7 to establish benign overfitting, as in Koehler et al. (2021) and Zhou et al. (2022).\nCorollary 2. For any l \u2208 N \u222a {\u221e} and k \u2208 N such that (k/n)2 + 2(k/n) < 1 and Rk > n. Let \u01eb = \u221a (k2 + 2kn)/n2, then it holds that\n(1 \u2212 \u01eb)2R(f\u03020) \u2264 ( \u2211 i>k \u03bbi )\n(\n\u2211 i\u2264l v2i \u03bbi\n)\nn +\n(\n1\u2212 n Rk\n)\u22121(\n\u03c32 + \u2211\ni>l\nv2i\n)\n. (42)"
        }
    ],
    "year": 2023
}