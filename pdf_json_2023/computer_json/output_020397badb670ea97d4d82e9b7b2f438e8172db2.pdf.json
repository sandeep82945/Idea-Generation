{
    "abstractText": "How can one meaningfully make a measurement, if the meter does not conform to any standard and its scale expands or shrinks depending on what is measured? In the present work it is argued that current evaluation practices for machine-learning classifiers are affected by this kind of problem, leading to negative consequences when classifiers are put to real use; consequences that could have been avoided. It is proposed that evaluation be grounded on Decision Theory, and the implications of such foundation are explored. The main result is that every evaluation metric must be a linear combination of confusion-matrix elements, with coefficients \u2013 \u2018utilities\u2019 \u2013 that depend on the specific classification problem. For binary classification, the space of such possible metrics is effectively two-dimensional. It is shown that popular metrics such as precision, balanced accuracy, Matthews Correlation Coefficient, Fowlkes-Mallows index, F1-measure, and Area Under the Curve are never optimal: they always give rise to an in-principle avoidable fraction of incorrect evaluations. This fraction is even larger than would be caused by the use of a decision-theoretic metric with moderately wrong coefficients.",
    "authors": [
        {
            "affiliations": [],
            "name": "K. Dyrland"
        },
        {
            "affiliations": [],
            "name": "<kjetil.dyrland gmail.com"
        },
        {
            "affiliations": [],
            "name": "A. S. Lundervold"
        }
    ],
    "id": "SP:94d3217da3309da46c48b3075aa0f114bf80f047",
    "references": [
        {
            "authors": [
                "H. Alt",
                "L.J. Guibas"
            ],
            "title": "Discrete geometric shapes: matching, interpolation, and approximation",
            "year": 2000
        },
        {
            "authors": [
                "S.G. Baker",
                "P.F. Pinsky"
            ],
            "title": "A proposed design and analysis for comparing digital and analog",
            "year": 2001
        },
        {
            "authors": [
                "J.O. Berger"
            ],
            "title": "Statistical Decision Theory and Bayesian Analysis, 2nd ed",
            "year": 1985
        },
        {
            "authors": [
                "K.H. Brodersen",
                "C.S. Ong",
                "K.E. Stephan",
                "J.M. Buhmann"
            ],
            "title": "The balanced accuracy",
            "year": 2010
        },
        {
            "authors": [
                "C.F. Camerer",
                "H. Kunreuther"
            ],
            "title": "Decision processes for low probability events: policy implications",
            "venue": "J. Policy Anal. Manag",
            "year": 1989
        },
        {
            "authors": [],
            "title": "Measures of the amount of ecologic association between species",
            "venue": "Ecology 263,",
            "year": 1945
        },
        {
            "authors": [
                "K. Dyrland",
                "A.S. Lundervold",
                "P.G.L. Porta Mana"
            ],
            "title": "A probability transducer",
            "year": 2022
        },
        {
            "authors": [
                "N. Fenton",
                "M. Neil"
            ],
            "title": "Risk Assessment and Decision Analysis with Bayesian Networks",
            "year": 2019
        },
        {
            "authors": [
                "T.L. Fine"
            ],
            "title": "Theories of Probability: An Examination of Foundations",
            "year": 1973
        },
        {
            "authors": [
                "R.A. Fisher"
            ],
            "title": "Statistical Methods for Research Workers",
            "venue": "rev. 13th ed. (Hafner,",
            "year": 1963
        },
        {
            "authors": [
                "J.L. Fleiss"
            ],
            "title": "Measuring agreement between two judges on the presence or absence of a trait",
            "venue": "Biometrics 313,",
            "year": 1975
        },
        {
            "authors": [
                "E.B. Fowlkes",
                "C.L. Mallows"
            ],
            "title": "A method for comparing two hierarchical clusterings",
            "venue": "J. Am. Stat. Assoc. 78383,",
            "year": 1983
        },
        {
            "authors": [
                "T. Gilovich",
                "D. Griffin",
                "D. Kahneman",
                "eds"
            ],
            "title": "Heuristics and Biases: The Psychology",
            "year": 2009
        },
        {
            "authors": [
                "I.J. Good",
                "G.H. Toulmin"
            ],
            "title": "Coding theorems and weight of evidence",
            "venue": "IMA J. Appl. Math",
            "year": 1968
        },
        {
            "authors": [
                "D. Hand",
                "P. Christen"
            ],
            "title": "A note on using the F-measure for evaluating record linkage algorithms",
            "venue": "Stat. Comput. 283,",
            "year": 2018
        },
        {
            "authors": [
                "R.A. Howard"
            ],
            "title": "On making life and death",
            "year": 1980
        },
        {
            "authors": [
                "J.B. Wong",
                "P.P. Glasziou"
            ],
            "title": "Decision Making in Health and Medicine: Integrating",
            "year": 2014
        },
        {
            "authors": [
                "E.T. Jaynes"
            ],
            "title": "Probability Theory: The Logic of Science",
            "year": 2003
        },
        {
            "authors": [
                "R.C. Jeffrey"
            ],
            "title": "The Logic of Decision",
            "year": 1965
        },
        {
            "authors": [
                "L.A. Jeni",
                "J.F. Cohn",
                "F. De La Torre"
            ],
            "title": "Facing imbalanced data: recommendations",
            "year": 2013
        },
        {
            "authors": [
                "D. Kahneman"
            ],
            "title": "Thinking, Fast and Slow",
            "venue": "(Farrar, Straus and Giroux,",
            "year": 2011
        },
        {
            "authors": [
                "D. Kahneman",
                "P. Slovic",
                "A. Tversky"
            ],
            "title": "Judgment under uncertainty: Heurist",
            "year": 2008
        },
        {
            "authors": [
                "H. Kim",
                "H.R. Markus"
            ],
            "title": "Deviance or uniqueness, harmony or conformity? A cultural analysis",
            "venue": "J. Pers. Soc. Psychol",
            "year": 1999
        },
        {
            "authors": [
                "D.V. Lindley"
            ],
            "title": "Making Decisions, 2nd ed",
            "venue": "(Wiley, London). First publ",
            "year": 1988
        },
        {
            "authors": [
                "J.M. Lobo",
                "A. Jim\u00e9nez-Valverde",
                "R. Real"
            ],
            "title": "AUC: a misleading measure of the performance of predictive distribution models",
            "venue": "Glob. Ecol. Biogeogr",
            "year": 2008
        },
        {
            "authors": [
                "A. Lundervold"
            ],
            "title": "An overview of deep learning in medical imaging",
            "year": 2019
        },
        {
            "authors": [],
            "title": "A tutorial introduction to decision theory",
            "venue": "IEEE Trans. Syst. Sci. Cybern",
            "year": 1968
        },
        {
            "authors": [],
            "title": "Decision Analysis: Introductory Lectures on Choices under Uncertainty",
            "year": 1970
        },
        {
            "authors": [
                "First publ."
            ],
            "title": "Ramsey, F",
            "venue": "P. (1926): Truth and probability. In: Ramsey (1950): ch.VII:156\u2013198. Repr. in",
            "year": 1961
        },
        {
            "authors": [
                "Kyburg",
                "Smokler"
            ],
            "title": "The Foundations of Mathematics: and other Logical Essays",
            "venue": "Written",
            "year": 1980
        },
        {
            "authors": [
                "First publ."
            ],
            "title": "Schwing, R",
            "venue": "C., Albers Jr., W. A., eds. (1980): Societal Risk Assessment: How Safe is Safe",
            "year": 1954
        },
        {
            "authors": [
                "M. Self",
                "P.C. Cheeseman"
            ],
            "title": "Enough? (Springer, New York)",
            "year": 1987
        },
        {
            "authors": [
                "C.E. arXiv.1304.2717. Shannon"
            ],
            "title": "A mathematical theory of communication",
            "venue": "Bell Syst. Tech. J. 273,",
            "year": 1948
        },
        {
            "authors": [],
            "title": "On the correctness and reasonableness of Cox\u2019s theorem for finite domains",
            "year": 1998
        },
        {
            "authors": [
                "G. eb026584. Varoquaux",
                "V. Cheplygina"
            ],
            "title": "Machine learning for medical imaging: methodological",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Does the evaluation stand up to evaluation?"
        },
        {
            "heading": "A first-principle approach to the evaluation of classifiers",
            "text": "K. Dyrland <kjetil.dyrland gmail.com>"
        },
        {
            "heading": "A. S. Lundervold \u2020",
            "text": "<alexander.selvikvag.lundervold hvl.no>\nP.G.L. Porta Mana <pgl portamana.org>\n(listed alphabetically)\nDept of Computer science, Electrical Engineering and Mathematical Sciences, Western Norway University of Applied Sciences, Bergen, Norway\n\u2020& Mohn Medical Imaging and Visualization Centre, Dept of Radiology, Haukeland University Hospital, Bergen, Norway\n27 May 2022; updated 21 February 2023\nHow can one meaningfully make a measurement, if the meter does not conform to any standard and its scale expands or shrinks depending on what is measured? In the present work it is argued that current evaluation practices for machine-learning classifiers are affected by this kind of problem, leading to negative consequences when classifiers are put to real use; consequences that could have been avoided. It is proposed that evaluation be grounded on Decision Theory, and the implications of such foundation are explored. The main result is that every evaluation metric must be a linear combination of confusion-matrix elements, with coefficients \u2013 \u2018utilities\u2019 \u2013 that depend on the specific classification problem. For binary classification, the space of such possible metrics is effectively two-dimensional. It is shown that popular metrics such as precision, balanced accuracy, Matthews Correlation Coefficient, Fowlkes-Mallows index, \ud835\udc391-measure, and Area Under the Curve are never optimal: they always give rise to an in-principle avoidable fraction of incorrect evaluations. This fraction is even larger than would be caused by the use of a decision-theoretic metric with moderately wrong coefficients.\n0 Prologue: a short story\nThe manager of a factory which produces a sort of electronic component wishes to employ a machine-learning classifier to assess the durability of each produced component. The durability determines whether the component will be used in one of two possible kinds of device. The\n1 Th is\ndo cu\nm en\nti s\nde si\ngn ed\nfo rs\ncr ee\nn re\nad in\ng an\nd tw\noup\npr in\ntin g\non A\n4 or\nLe tte\nrp ap\ner\nar X\niv :2\n30 2.\n12 00\n6v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n23\nclassifier should take some complex features of the component as input, and output one of the two labels \u20180\u2019 for \u2018long durability\u2019, or \u20181\u2019 for \u2018short durability\u2019, depending on the component type.\nTwo candidate classifiers, let us call them A and B, are trained on available training data. When employed on a separate evaluation set, they yield the following confusion matrices, written in the format\ncl as\nsifi er\nou tp ut 1 0\ntrue class 0 1[\nTrue 0 False 0 False 1 True 1 ] and normalized over the total number of evaluation data:\nclassifier A: [ 0.27 0.15 0.23 0.35 ] , (1)\nclassifier B: [ 0.43 0.18 0.07 0.32 ] . (2)\nThese matrices show that the factory produces, on average, 50% shortand 50% long-durability components.\nThe confusion matrices above lead to the following values of common evaluation metrics1 for the two classifiers. Class 0 is \u2018positive\u2019, 1 \u2018negative\u2019. Blue bold indicates the classifier favoured by the metric, red the disfavoured:\nThe majority of these metrics favour classifier B, some of them by quite a wide relative difference. Only the true-negative rate favours classifier A, but only by a relative difference of 9%.\n1 Balanced accuracy: Brodersen et al. 2010; \ud835\udc391 measure: van R\u0133sbergen 1974; Matthews correlation coefficient: Matthews 1975; Fowlkes-Mallows index: Fowlkes & Mallows 1983.\nThe developers of the classifiers therefore recommend the employment of classifier B.\nThe factory manager does not fully trust these metrics, asking, \u201chow do I know they are appropriate?\u201d. The developers assure that these metrics are widely used. The manager (of engineering background) comments, \u201cI don\u2019t remember \u2018widely used\u2019 being a criterion of scientific correctness \u2013 not after Galileo at least\u201d, and decides to employ both classifiers for a trial period, to see which factually leads to the best revenue. The two classifiers are integrated into two separate but otherwise identical parallel production lines.\nDuring the trial period, the classifiers perform according to the classification statistics of the confusion matrices (1) and (2) above. At the end of this period the factory manager finds that the average net gains per assessed component yielded by the two classifiers are2\nclassifier A classifier B 3.5 $ \u22123.5 $ (3)\nThat is, classifierB actually led to a loss of revenue. The manager therefore decides to employ classifier A, commenting with a smug smile that it is always unwise to trust the recommendations of developers, unacquainted with the nitty-gritty reality of a business.\nThe average gains above are easy to calculate from some additional information. The final net gains caused by the correct or incorrect classification of one electronic component are as follows:\ncla ss\nifi er\nou tp ut 1 0\ntrue class 0 1[\n15 $ \u2212335 $ \u221235 $ 165 $\n] (4)\nThe reason behind these values is that short-durability components (class 1) provide more power and are used in high-end, costly devices; but they cause extreme damage and consequent repair costs and refunds if used in devices that require long-durability components (class 0). Long-durability components provide less power and are used in lowend, cheaper devices; they cause some damage if used in devices that require short-durability components, but with lower consequent costs.\n2 \u2018$\u2019 represents a generic currency or value unit; this is why it is not written in front of the gains.\nTaking the sum of the products of the gains above by the respective percentages of occurrence \u2013 that is, the elements of the confusion matrix \u2013 yields the final average gain. The final average gain returned by the use of classifier A, for example, is\n15 $ \u00d7 0.27 \u2212 335 $ \u00d7 0.15 \u2212 35 $ \u00d7 0.23 + 165 $ \u00d7 0.35 = 3.5 $ . In the present case, the confusion matrices (1) and (2) lead to the amounts (3) found by the manager."
        },
        {
            "heading": "1 Issues in the evaluation of classifiers",
            "text": "The story above illustrates several well-known issues of currently popular evaluation procedures for machine-learning classifiers:\n(a) We are swept by an avalanche of possible evaluation metrics. Often it is not clear which is the most compelling. In the story above, for example, one could argue that the true-negative rate was the appropriate metric, in view of the great difference in gains between correct and wrong classification for class 1, compared with that for class 0. But at which point does this qualitative reasoning fail? Imagine that the net gains had been as follows instead:\ncl as\nsifi er\nou tp ut 1 0\ntrue class 0 1[\n45 $ \u2212335 $ \u221265 $ 165 $\n] . (5)\nAlso in this case one could argue that there is a greater economic difference between correct and wrong classification for class 1 than for class 0. The true-negative rate should, therefore, again be the appropriate metric. Yet a simple calculation analogous to the one of \u00a7 0 shows that classifier B actually leads to the best average revenue: 7.3 $/component, vs 4.7 $/component for classifier A. Hence the true-negative rate is not the appropriate metric in this case: our qualitative reasoning failed us.\n(b) A classifier favoured by the majority of available metrics can still turn out not to be the best one in practice.\n(c) Most popular metrics are introduced by intuitive reasoning, ad hoc mathematical operations, special assumptions (such as Gaussianity3\n3 e.g. Fisher 1963 \u00a7 31 p. 183 for the Matthews correlation coefficient.\nor other statistical assumptions), and an analysis of special cases only. Unfortunately this kind of derivations does not guarantee generalization to all cases, nor that the proposed metric is uniquely determined by the chosen assumptions, nor that it satisfies more general consistency requirements. By contrast, consider the kind of derivation that starts from specific qualitative requirements and mathematically proves the uniqueness of a particular formula satisfying them. Examples are the derivation of the Shannon entropy as the unique metric universally satisfying a set of basic requirements for the amount of information4. Or the derivation of the probability calculus as the unique set of rules satisfying general rational requirements for inductive reasoning, learning, and prediction5. Or the derivation of decision theory as the unique framework guaranteeing a rational and optimal decision under uncertainty6.\n(d) Let us assume that some of the popular metrics identify the best algorithm \u2018in the majority of cases\u2019 \u2013 although it is difficult to statistically define such a majority, and no real surveys have ever been conducted to back up this assumption. Yet, do we expect the end-user to simply hope not to belong to the unlucky minority? Is such uncertainty inevitable? We cannot have a cavalier attitude towards this problem: life and death can depend on it in some machine-learning applications7. Imagine a story analogous to the factory one, but in a medical setting instead. The classifiers should distinguish between two tumour types, requiring two different types of medical intervention. The confusion matrices are the same (1) and (2). Correct and incorrect classification lead to the following expected remaining life lengths for patients in a specific age range:8\ncl as\nsifi er\nou tp ut 1 0\ntrue class 0 1[\n350 months 0 months 300 months 500 months\n] . (6)\n4 Shannon 1948; Woodward 1964 \u00a7 3.2; also Good & Toulmin 1968. 5 Cox 1946; Fine 1973; Halpern 1999; Snow 1998; 2001; Jaynes 2003 chs 1\u20132; see also Self & Cheeseman 1987; Cheeseman 1988; Russell & Norvig 2022 ch. 12. 6 Russell & Norvig 2022 \u00a7 15.2; von Neumann & Morgenstern 1955 chs 2\u20133. 7 cf. Howard 1980. 8 cf. the discussion in Sox et al. 2013 \u00a7 11.2.9.\nThese values might arise in several scenarios. For example, tumours of class 0 and 1 may require very different kinds of treatment. If a class 0 tumour is misdiagnosed and not properly treated, it leads to immediate death (0 months); if correctly diagnosed, its treatment is usually successful, leading to high life expectancy (500 months). Class 0 tumours can be treated, but they lead to a shorter life expectancy (350 months). If they are misdiagnosed as class 1, however, the damage caused by class 1 treatment shortens this life expectancy even further (300 months).\nThis matrix above is numerically equivalent to (4) up to a common additive constant of 335, so the final net gains are also shifted by this amount. It is easy to see that the metrics are exactly as in Table 1, the majority favouring classifier B. And yet the use of classifier A leads to a more than six-month longer expected remaining life than classifier B.\n(e) Often it is not possible to temporarily deploy all candidate classifiers, as our fictitious manager did, in order to observe which factually leads to the best results. Or it may even be unethical: consider a situation like the medical one above, where a classifier may lead to a larger number of immediate deaths than another.\n(f) Finally, all issues listed above are not caused by class imbalance (the occurrence of one class with a higher frequency than another). In our story, for example, the two classes were perfectly balanced. Class imbalance can make all these issues worse9.\nBut our story also points to a possible solution for all these issues. The \u2018metric\u2019 that ultimately proved to be relevant to the manager was the average net monetary gain obtained by using a candidate classifier. In the medical variation discussed in issue (d) above, it was the average life expectancy. In either case, such metric could have been easily calculated beforehand, upon gathering information about the average gains and losses of correct and incorrect classification, collected in the matrix (4) or (6), and combining these with statistics collected in the confusion matrix associated with the classifier. Denoting the former kind of matrix by (\ud835\udc48\ud835\udc56 \ud835\udc57) and the confusion matrix by (\ud835\udc36\ud835\udc56 \ud835\udc57), where \ud835\udc56 indexes the classifier\n9 Jeni et al. 2013; Zhu 2020.\noutputs (rows) and \ud835\udc57 the true classes (columns), such a metric would have the formula \u2211\n\ud835\udc56 , \ud835\udc57\n\ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc36\ud835\udc56 \ud835\udc57 (7)\nthe sum extending to all matrix elements.\nIn the present work, we argue that formula (7) is indeed the only acceptable metric for evaluating and comparing the performance of two or more classifiers, each with its own confusion matrix (\ud835\udc36\ud835\udc56 \ud835\udc57) collected on relevant test data. The coefficients \ud835\udc48\ud835\udc56 \ud835\udc57 , called utilities, are problemdependent. This formula is the utility yield of a classifier having confusion matrix (\ud835\udc36\ud835\udc56 \ud835\udc57).\nOur argument is based on Decision Theory, an overview of which is given in \u00a7 2.\nThe utility yield (7) is a linear combination of the confusion-matrix elements, with coefficients independent of the elements themselves. In \u00a7 3 we explore some properties of this formula and of the space of such metrics for binary classification. We also show that some common metrics such as precision, \ud835\udc391-measure, Matthews correlation coefficient, balanced accuracy, and Fowlkes-Mallows index cannot be written as a linear combination of this kind (or a one-one function thereof). This impossibility has two consequences. First, it means that these metrics are likely affected by some kind of cognitive bias. Second, there exists no classification problem for which these metrics can correctly rank the performance of all pairs of classifiers. Using any one of these metrics leaves open the possibility that the evaluation is incorrect a priori. In \u00a7 5 we show that this is also true for the Area Under the Curve of the Receiver Operating Characteristic, and we offer some additional remarks about it from the standpoint of decision theory.\nOn the other hand, metrics such as accuracy, true-positive rate, truenegative rate can be written in the form (7). Consequently, each one has a set of classification problems in which it correctly ranks the performance of all pairs of classifiers.\nWhat happens if we are uncertain about the utilities appropriate to a classification problem? And what happens if the utilities are incorrectly assessed? We show in \u00a7 4 that uncertainty about utilities still leads to a metric of the form (7). We also show that an evaluation using incorrect utilities, even with relative errors as large as 20% of the maximal utility,\nstill leads to a higher amount of correctly ranked classifiers than the use of any of the popular metrics mentioned above.\nWe summarize and discuss our results in the final \u00a7 6."
        },
        {
            "heading": "2 Brief overview of decision theory",
            "text": ""
        },
        {
            "heading": "2.1 References",
            "text": "Here we give a brief overview of decision theory. We only focus on the notions relevant to the problem of evaluating classifiers, and simply state the rules of the theory. These rules are quite intuitive, but it must be remarked that they are constructed in order to be logically and mathematically self-consistent: see the following references. For a presentation of decision theory from the point of view of artificial intelligence and machine learning, see Russell & Norvig 2022 ch. 15. Simple introductions are given by North 1968; Raiffa 1970; Lindley 1988; Tribus 1969 ch. 8; Jeffrey 1965; and a discussion of its foundations and history by Steele & Stef\u00e1nsson 2020. For more thorough expositions see Raiffa & Schlaifer 2000; Fenton & Neil 2019; Berger 1985; Savage 1972; and Sox et al. 2013; Hunink et al. 2014 for a medical perspective. See also Ramsey\u2019s 1926 insightful and charming pioneering discussion."
        },
        {
            "heading": "2.2 Decisions and classes",
            "text": "Decision theory makes a distinction between \u2022 the possible situations we are uncertain about: in our case, the\npossible classes; \u2022 the possible decisions we can make.\nThis distinction is important because it prevents the appearance of various cognitive biases10 in evaluating the probabilities and frequencies of the possible situations on the one hand, and the values of our decisions on the other. Examples are the scarcity bias11 \u201cthis class is rare, therefore its correct classification must lead to high gains\u201d, and plain wishful thinking: \u201cthis event leads to high gains, therefore it is more probable\u201d.\nOften even the number of classes and the number of decisions differ. But in using machine-learning classifiers, one typically considers\n10 Kahneman et al. 2008; Gilovich et al. 2009; Kahneman 2011. 11 Camerer & Kunreuther 1989; Kim & Markus 1999; Mittone & Savadori 2009.\nsituations where the set of available decisions and the set of possible classes have some kind of natural correspondence and equal cardinality. In a \u2018cat vs dog\u2019 image classification, for example, the classes are \u2018cat\u2019 and \u2018dog\u2019, and the decisions could be \u2018put into folder Cats\u2019 vs \u2018put into folder Dogs\u2019. In a medical application the classes could be \u2018ill\u2019 and \u2018healthy\u2019 and the decisions \u2018treat\u2019 vs \u2018dismiss\u2019. As already mentioned, most of our discussions and examples focus for simplicity on binary classification."
        },
        {
            "heading": "2.3 Utilities and maximization of expected utility",
            "text": "To each decision we associate several utilities, depending on which of the possible classes is actually true. A utility may, for instance, equal a gain or loss in money, energy, number of customers, life expectancy, or quality of life, measured in appropriate units; or it may equal a combination of such quantities.\nThese utilities are collected into a utility matrix (\ud835\udc48\ud835\udc56 \ud835\udc57), like the ones shown in formulae (4), (5), (6). The component \ud835\udc48\ud835\udc56 \ud835\udc57 is the utility of the decision corresponding to class \ud835\udc56 if class \ud835\udc57 is true, or simply the utility of class \ud835\udc56 conditional on class \ud835\udc57.\nIn an individual classification instance, if we know which class is true, then the optimal decision is the one having maximal utility among those conditional on the true class. If, on the other hand, we are uncertain about which class is true, with probability \ud835\udc5d \ud835\udc57 for class \ud835\udc57 such that \u2211 \ud835\udc57 \ud835\udc5d \ud835\udc57 = 1, then decision theory states that the optimal decision is the one having maximal expected utility ?\u0304?\ud835\udc56 , defined as the expected value of the utility of decision \ud835\udc56 with respect to the probabilities of the various classes:\n?\u0304?\ud835\udc56 := \u2211 \ud835\udc57 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc5d \ud835\udc57 . (8)\nIn formulae, this principle of maximization of expected utility is\nchoose class \ud835\udc56\u2217 = arg max \ud835\udc56 {?\u0304?\ud835\udc56} \u2261 arg max \ud835\udc56 {\u2211 \ud835\udc57 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc5d \ud835\udc57 } . (9)\nA very important result in decision theory is that basic requirements of rational decision-making imply that there must be a set of utilities underlying the decisions of a rational agent, and the decisions must obey the principle of maximization of expected utility12.\n12 Russell & Norvig 2022 \u00a7 15.2; von Neumann & Morgenstern 1955 chs 2\u20133.\nHow are utilities determined? They are obviously problem-specific and cannot be given by the theory (which would otherwise be a model rather than a theory). Utilities can be obvious in decision problems involving gains or losses of measurable quantities such as money or energy (the utility of money is usually not equal to the amount of money, the relationship between the two being somewhat logarithmic13). In medical problems they can correspond to life expectancy and quality of life; see for example Sox et al. 2013 esp. ch. 8 and \u00a7 11.2.9 and Hunink et al. 2014 esp. ch. 4 on how such health factors are transformed into utilities.\nThe final utility of a single classification instance may depend, in some cases, on a sequence of further uncertain events and further decisions. In the story of \u00a7 0, for instance, the misclassification of a short-durability component as a long-durability one leads the final device to break only in a high fraction of cases, and in such cases the end customer requires a refund in a high fraction of subcases; the refunded amount may even depend on further circumstances. The negative utility \ud835\udc4801 = \u2212335 $ in table (4) comes from a statistical average of the losses in all these possible end results. This is the topic of so-called decision networks or influence diagrams14. The decision-theory subfield of utility theory gives rules that guarantee the mutual consistency of a set of utilities in single decisions or decision networks. For simple introductions to utility theory see Russell & Norvig 2022 \u00a7 15.2, North 1968 pp. 201\u2013205, and the references given at the beginning of the present section.\nIn the present work, we do not worry about such rules in order not to complicate the discussion: they should be approximately satisfied if the utilities of a problem have been carefully assessed."
        },
        {
            "heading": "3 Evaluation of classifiers from a decision-theoretic perspective",
            "text": ""
        },
        {
            "heading": "3.1 Admissible evaluation metrics for classification problems",
            "text": "Maximization of expected utility is the ground rule for rational decision making15. In the present work we focus on the stage where a large number of classifications have already been made by a classifier on a test\n13 e.g. North 1968 pp. 203\u2013204; Raiffa 1970 ch. 4. 14 Besides the general references already given: Russell & Norvig 2022 \u00a7 15.5; Howard & Matheson 2005; for a step-by-step tutorial: Raiffa 1970. 15 We discuss and use it in our companion work Dyrland et al. 2022.\ndataset with \ud835\udc41 data. Denote by \ud835\udc39\ud835\udc56 \ud835\udc57 the number of instances in which the classifier chose class \ud835\udc56 and the true class was \ud835\udc57. Then (\ud835\udc39\ud835\udc56 \ud835\udc57) is the confusion matrix of the classifier on this particular test set. For all instances in which the classifier chose class \ud835\udc56 and the true class was \ud835\udc57, a utility \ud835\udc48\ud835\udc56 \ud835\udc57 is eventually gained. The total utility yielded by the classifier on the test set is therefore \u2211 \ud835\udc56 \ud835\udc57 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc39\ud835\udc56 \ud835\udc57 . Dividing by \ud835\udc41 we obtain the average utility per datum, which we call the utility yield; it can be written as\n\u2211 \ud835\udc56 \ud835\udc57 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc36\ud835\udc56 \ud835\udc57 (10)\nwhere \ud835\udc36\ud835\udc56 \ud835\udc57 := \ud835\udc39\ud835\udc56 \ud835\udc57/\ud835\udc41 is the relative frequency of choice \ud835\udc56 and true class \ud835\udc57, and (\ud835\udc36\ud835\udc56 \ud835\udc57) is the normalized confusion matrix.\nThe utility yield, formula (10), is therefore the natural metric to evaluate and compare the performance of classifiers on a test set for a classification problem characterized by the utility matrix (\ud835\udc48\ud835\udc56 \ud835\udc57).\nNote how the utilities \ud835\udc48\ud835\udc56 \ud835\udc57 do depend on the frequencies \ud835\udc39\ud835\udc56 \ud835\udc57 or \ud835\udc36\ud835\udc56 \ud835\udc57 . If they did, it would mean that we had waited until all classification instances had been made in order to assess the value of each single instance. In virtually all classification problem we can think of, this would be a source of evaluation bias, such as the scarcity bias mentioned in \u00a7 2.2. It would, moreover, be an impossible procedure in contexts where the consequence of a single classification is manifest before the next classification is made.\nIf we modify the elements of a utility matrix by a common additive constant or by a common positive multiplicative constant,\n\ud835\udc48\ud835\udc56 \ud835\udc57 \u21a6\u2192 \ud835\udc4e \ud835\udc48\ud835\udc56 \ud835\udc57 + \ud835\udc4f \ud835\udc4e > 0 , (11)\nthen the final utilities yielded by a classifier with a particular confusion matrix are modified by the same constants. The ranking of any set of classifiers will therefore be the same. After all, an additive constant or a positive factor represent only changes in the zero or the measurement unit of our utility scale16. Such changes should not affect a decision problem. Indeed, the fact that they do not is another example of the logical consistency of decision theory.\n16 cf. Russell & Norvig 2022 \u00a7 15.2.2."
        },
        {
            "heading": "3.2 Space of utility matrices for binary classification",
            "text": "Let us consider a problem of binary classification. It is characterized by a matrix of 2\u00d7 2 utilities. We suppose that they are not all equal; the choice of class would be immaterial otherwise, and the classification problem trivial. We can use the freedom of choosing a zero and measurement unit to bring the utility matrix to a standard form. Let us choose them such that the maximum utility is 1 and the minimum utility is 0 (note that this value may still correspond to an actual monetary loss, for example). That is, we are effecting the transformation\n\ud835\udc48\ud835\udc56 \ud835\udc57 \u21a6\u2192 \ud835\udc48\ud835\udc56 \ud835\udc57 \u2212 min(\ud835\udc48\ud835\udc56 \ud835\udc57)\nmax(\ud835\udc48\ud835\udc56 \ud835\udc57) \u2212 min(\ud835\udc48\ud835\udc56 \ud835\udc57) . (12)\nWith this convention, it is clear that we only have two degrees of freedom in choosing the utility matrix of a binary-classification problem. As a consequence, the space (more precisely: manifold) of possible evaluation metrics for binary classifications is two-dimensional. In order to evaluate candidate classifiers for a binary-classification problem, we must choose a point from this space.\nWe can represent this space as in fig. 1. The centre is the utility matrix with equal maximum utilities for correct classification and equal minimum utilities for incorrect classification; we shall see later that it corresponds to the use of accuracy as the evaluation metric. Moving to the left from the centre, the utility for correct classification of class 1 decreases with respect to class 0; vice versa moving to the right. Moving upwards from the centre, the utility for misclassification of class 1 increases; moving downwards, the utility for misclassification of class 0 increases. We have excluded utility matrices in which misclassification has a higher utility than correct classification (although they may occur in some situations); they would appear in the missing upper-left and lower-right corners. Fixing (\ud835\udc65, \ud835\udc66) axes through the centre of the set, a utility matrix has coordinates[\n1 \u2212 \ud835\udc65 \u03b4(\ud835\udc65 > 0) \ud835\udc66 \u03b4(\ud835\udc66 > 0) \u2212\ud835\udc66 \u03b4(\ud835\udc66 < 0) 1 + \ud835\udc65 \u03b4(\ud835\udc65 < 0)\n] , |\ud835\udc65 |, |\ud835\udc66 | \u2a7d 1 . (13)\nNote that this representation is not meant to reflect any convex or metric properties, however. No metric or distance is defined in the space of utility matrices. Convex combination is defined if we drop the normalization (12) but it is not correctly reflected in the representation of fig. 1."
        },
        {
            "heading": "3.3 Relationship with common metrics",
            "text": "In \u00a7 3.1 we found that the most general evaluation metric according to decision theory must be a linear combination of the confusion-matrix elements. The coefficients of this linear combination do not depend on the confusion-matrix elements themselves; such a dependence usually reflects some sort of cognitive bias. Which common popular metrics adhere to this mathematical form? We want to answer this question in the binary-classification case while giving as much allowance as possible in the typical context in which popular metrics are used.\nConsider the case in which we are comparing several classifiers on the same test set. The number of data \ud835\udc41 and the relative frequencies \ud835\udc530 , \ud835\udc531 with which the two classes \u20180\u2019, \u20181\u2019 occur in the test set are fixed and constant for all classifiers under evaluation.\nA classifier yields a normalized confusion matrix (\ud835\udc36\ud835\udc56 \ud835\udc57) which we write in the format\ncla ss\nifi er\nou tp ut 1 0 true class 0 1[ \ud835\udc3600 \ud835\udc3601 \ud835\udc3610 \ud835\udc3611 ] .\nOwing to the constraints \ud835\udc3600 + \ud835\udc3610 \u2261 \ud835\udc530 and \ud835\udc3601 + \ud835\udc3611 \u2261 \ud835\udc531 we can always make two elements of the confusion matrix appear or disappear from any formula, replacing them with expressions involving the remaining two elements and the class frequencies. To avoid ambiguities in interpreting the functional form of mathematical formulae, let us agree to always express them in terms of \ud835\udc3600 and \ud835\udc3611 only, making the replacements \ud835\udc3610 = \ud835\udc530 \u2212 \ud835\udc3600, \ud835\udc3601 = \ud835\udc531 \u2212 \ud835\udc3611 wherever necessary.\nRecall that, given a utility matrix, we can always modify its elements by a common positive multiplicative constant \ud835\udc4e and by a common additive constant \ud835\udc4f, eq. (11), because such a modification corresponds to a change of unit and zero of the utility scale. With such a modification the evaluation metric (10) takes the equivalent form\n\ud835\udc4e \u2211 \ud835\udc56 \ud835\udc57 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc36\ud835\udc56 \ud835\udc57 + \ud835\udc4f (14)\nbecause \u2211\n\ud835\udc56 \ud835\udc57 \ud835\udc36\ud835\udc56 \ud835\udc57 \u2261 1. Writing the sum explicitly and rewriting the elements \ud835\udc3610 , \ud835\udc3601 in terms of \ud835\udc3600 , \ud835\udc3611 as discussed above, this formula becomes\n\ud835\udc4e (\ud835\udc4800 \u2212\ud835\udc4810) \ud835\udc3600 + \ud835\udc4e (\ud835\udc4811 \u2212\ud835\udc4801) \ud835\udc3611 + \ud835\udc4e \ud835\udc530 \ud835\udc4810 + \ud835\udc4e \ud835\udc531 \ud835\udc4801 + \ud835\udc4f . (15)\nSince in the present context \ud835\udc41, \ud835\udc530 , \ud835\udc531 are constants, we are free to construct the arbitrary constants \ud835\udc4e > 0 and \ud835\udc4f from them in any way we please: \ud835\udc4e = \ud835\udc4e(\ud835\udc41, \ud835\udc530 , \ud835\udc531) > 0 , \ud835\udc4f = \ud835\udc4f(\ud835\udc41, \ud835\udc530 , \ud835\udc531) . (16) We can also use this freedom to include the term \ud835\udc4e \ud835\udc530 \ud835\udc4810 + \ud835\udc4e \ud835\udc531 \ud835\udc4801 into \ud835\udc4f in the formula above. We conclude that an evaluation metric for binary classification complies with decision theory if and only if it can be written in the general form\n\ud835\udc4e(\ud835\udc41, \ud835\udc530 , \ud835\udc531) \ud835\udc4b \ud835\udc3600 + \ud835\udc4e(\ud835\udc41, \ud835\udc530 , \ud835\udc531) \ud835\udc4c \ud835\udc3611 + \ud835\udc4f(\ud835\udc41, \ud835\udc530 , \ud835\udc531) (17)\nwhere \ud835\udc4b,\ud835\udc4c are real constants that do not depend on \ud835\udc3600 , \ud835\udc3611 , \ud835\udc41 , \ud835\udc530 , \ud835\udc531; and \ud835\udc4e( \u00b7 ) > 0, \ud835\udc4f( \u00b7 ) are arbitrary functions of \ud835\udc41, \ud835\udc530 , \ud835\udc531 only.\nA monotonic function (such as an exponential) of the expression above is also admissible if we only require a comparison score to rank several classifiers from best to worst.\nLet us examine some common evaluation metrics for binary classification from this point of view. We write their formulae in terms of \ud835\udc3600 , \ud835\udc3611.\nThe following metrics are particular instances of formula (17):\n\u2713 Accuracy: \ud835\udc3600 + \ud835\udc3611. We have \ud835\udc4e = 1, \ud835\udc4b = \ud835\udc4c = 1, \ud835\udc4f = 0. Indeed it corresponds to the utility yield based on the identity utility matrix (\ud835\udc48\ud835\udc56 \ud835\udc57) = [ 1 0 0 1 ] (or equivalently a utility matrix that assigns the same\nutility to the correct classification of any class, and the same, lower utility to the misclassification of any class).\n\u2713 True-positive rate (recall): \ud835\udc3600/ \ud835\udc530. Here \ud835\udc4e = 1/ \ud835\udc530, \ud835\udc4b = 1, \ud835\udc4c = 0, \ud835\udc4f = 0. It corresponds to using the utility matrix [ 1 0 0 0 ] . \u2713 True-negative rate (specificity): \ud835\udc3611/ \ud835\udc531. Here \ud835\udc4e = 1/ \ud835\udc531, \ud835\udc4b = 0, \ud835\udc4c = 1, \ud835\udc4f = 0. It corresponds to using the utility matrix [ 0 0 0 1 ] .\nThe following metrics instead cannot be written in the form (17), nor as monotonic functions of that form:\n\u2717 Precision: \ud835\udc3600/(\ud835\udc3600 \u2212 \ud835\udc3611 + \ud835\udc531). Non-linear in \ud835\udc3600 , \ud835\udc3611. \u2717 \ud835\udc391-measure: 2\ud835\udc3600/(\ud835\udc3600 \u2212 \ud835\udc3611 + 1). Non-linear in \ud835\udc3600 , \ud835\udc3611. The same\nis true for the more general \ud835\udc39\u03b2 -measures.\n\u2717 Matthews correlation coefficient: \ud835\udc531 \ud835\udc3600+ \ud835\udc530 \ud835\udc3611\u221a \ud835\udc530 \ud835\udc531 ( \ud835\udc531+\ud835\udc3600\u2212\ud835\udc3611) ( \ud835\udc530+\ud835\udc3611\u2212\ud835\udc3600) . Non-\nlinear in \ud835\udc3600 , \ud835\udc3611. \u2717 Fowlkes-Mallows index: \ud835\udc3600/ \u221a \ud835\udc530 ( \ud835\udc531 + \ud835\udc3600 \u2212 \ud835\udc3611). Non-linear in\n\ud835\udc3600 , \ud835\udc3611.\n\u2717 Balanced accuracy: \ud835\udc3600/(2 \ud835\udc530) + \ud835\udc3611/(2 \ud835\udc531). Despite being linear in \ud835\udc3600 , \ud835\udc3611 and an average of two metrics (true-positive and truenegative rate) that are instances of formula (17), it is not an instance of that formula, because the two averaged metrics involve different \ud835\udc4e( \u00b7 ) functions.\nWe see that many popular evaluation metrics do not comply with the principles of decision theory. Any such metric suffers from two problems.\nFirst, as discussed in \u00a7 2, the metric involves an interdependence of utilities and classification frequencies, which typically implies some form of cognitive bias17.\nSecond, the ranking of confusion matrices yielded by the metric does not fully agree with that yielded by any utility matrix \u2013 a full agreement would otherwise imply that the metric could be written in the form (17). Some confusion matrices must therefore be incorrectly ranked. Since any rational classification problem is characterized by some underlying utility matrix, this means that the non-compliant metric will always lead to some wrong evaluations. By contrast, compliant metrics such as the accuracy give completely correct rankings for all pairs of confusion matrices in specific sets of classification problems.\nThe second phenomenon is illustrated in the plots of figs 2\u20133. Each blue dot in a plot represents a hypothetical confusion matrix obtained from a test dataset in a binary classification. The dot\u2019s coordinates are the utility yield of that confusion matrix according to a particular utility matrix underlying the classification problem, and the score of the confusion matrix according to another metric. The underlying utility matrix is [ 1 0 0 1 ] for all plots in the left column, and [ 1 0 0 0 ] for all plots in the right column. The other metrics considered, one for each row of plots, are accuracy, true-positive rate (recall, class 0 being \u2018positive\u2019), \ud835\udc391-measure, Matthews correlation coefficient.\nThe confusion matrices are selected by first fixing a proportion of classes in the dataset, which is 50%/50% (balanced dataset) for all plots in fig. 2 and 90%/10% (imbalanced dataset) for all plots in fig. 3. Then a true-positive rate and a true-negative rate are independently selected from the range [1/2, 1], with a probability linearly increasing in the rate (median of 0.85, lower and upper quartiles at 0.75 and 0.93; see side plot). These confusion matrices therefore represent the classification statistics produced by classifiers that tend to have high true-negative and true-positive rates \u2013 as is clear from the fact that the points tend to accumulate on the upper-right corners of the plots. 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 true\u2212positive/negative rate pr ob ab ili ty d en si ty\n17 Hand & Christen 2018 discuss such biases regarding the \ud835\udc391-measure.\nWe see that the accuracy (first-row plots) always gives correct relative evaluations of all confusion matrices when the underlying utility matrix is equivalent to [ 1 0 0 1 ] (left column): the y-coordinate is a monotonically increasing function \u2013 in fact a linear function \u2013 of the x-coordinate. Accuracy is indeed the utility yield corresponding to the identity utility matrix. The true-positive rate (second-row plots) always gives correct relative evaluations (provided the test set is the same) when the underlying utility matrix is equivalent to [ 1 0 0 0 ] (right column).\nOn the other hand, if any of these two metrics is used for a problem having a different underlying utility matrix, then there is no deterministic relationship between the metric\u2019s score and the actual utility yield. In this case it is always possible to find two or more confusion matrices for which the metric gives completely reversed evaluations with respect to the actual utility yield. In other words, the confusion matrix \u2013 and associated algorithm \u2013 which is worst according to the true utility, is ranked best by the metric; and vice versa. Pairs of red triangular shapes in a plot are examples of such confusion matrices wrongly ranked by the y-axis metric.\nMetrics such as accuracy and true-positive rate, complying with formula (17), thus require us to rely on evaluation luck only when they are used in the wrong classification problem.\nThe plots for the \ud835\udc391-measure (third-row plots) and Matthews correlation coefficient (fourth-row plots) show that these two metrics do not have any functional relationship with the actual utility yield. It is again always possible to find two or more confusion matrices for which either metric gives completely reversed evaluations with respect to the actual utility yield. But for these two metrics, unlike accuracy and true-positive rate, cases of incorrect evaluation will always occur in every classification problem.\nMetrics such as \ud835\udc391-measure and Matthews correlation coefficient, not complying with formula (17), thus always require us to rely on luck in our evaluations. There are no classification problems for which these metrics lead to always correct evaluations.\nA metric non-compliant with decision theory can lead to a large number of correct results for some classification problems and test sets. The bottom-left plot of fig. 2, for instance, shows that the Matthews correlation coefficient is almost a monotonically increasing deterministic\nfunction of the utility yield when the underlying utility matrix is the identity and the dataset is balanced (but it is not when the underlying utility matrix is [ 1 0 0 0 ] or the dataset is imbalanced; see corresponding plots). Such an occasional partial agreement is useless, however. Knowledge of the utility matrix is a prerequisite for relying on such partial agreement\u2013 but given such knowledge we can directly use the actual utility yield instead, which has an exact agreement and is easier to compute."
        },
        {
            "heading": "4 Unknown or incorrect utilities",
            "text": "So far, we have argued that the natural evaluation metric for a classifier is the utility yield of its confusion matrix, according to the utilities underlying the classification problem of interest. We have also argued that many popular metrics, those not complying with formula (17), must always a priori lead to instances of incorrect evaluation. Our arguments are based on the principles of decision theory.\nSeveral interrelated questions spring from our arguments, though:\n\u2022 What to do when we are uncertain about the utilities underlying a classification problem?\n\u2022 What happens if the utilities we use are actually wrong, that is, not the true ones underlying the problem?\n\u2022 How often do uncompliant metrics such as \ud835\udc391-measure or Matthews correlation coefficient lead to incorrect results, on average?\nIn fact, if a small error in the assessment of the utilities led to a large number of wrong evaluations, while non-compliant metrics led to a small number of wrong evaluations on average, then all the rigorousness of decision-theoretic metrics would be useless in practice, and noncompliant metrics would be best for real applications.\nThis is not the case, however. We now discuss how to deal with uncertainty about the utilities and present an important result: Using wrong utilities, even with relative errors almost as large as 20% of the maximum utility, still leads to fewer incorrect relative evaluations on average than using many currently popular metrics."
        },
        {
            "heading": "4.1 Unknown utilities; average performance on several classification problems",
            "text": "Dealing with unknown utilities is straightforward. Suppose we are uncertain whether the utility matrix appropriate to a classification problem is U(1) \u2261 ( \ud835\udc48\n(1) \ud835\udc56 \ud835\udc57\n) , or U(2), or U(3), and so on, where the number of\nalternatives can even be infinite or continuous. Each alternative U(\ud835\udc4e) has a probability \ud835\udc5e\ud835\udc4e , or probability density \ud835\udc5e(\ud835\udc4e) d\ud835\udc4e in the continuous case. Then for this classification problem, we should use the expected utility matrix\nU\u0302 := \ud835\udc5e1 U(1) + \ud835\udc5e2 U(2) + \ud835\udc5e3 U(3) + \u00b7 \u00b7 \u00b7 (18) or U\u0302 := \u222b \ud835\udc5e(\ud835\udc4e)U(\ud835\udc4e) d\ud835\udc4e in the continuous case.\nWe only give a sketch of the proof of this intuitive result18. If we are uncertain about the utility matrix, then we have a double decision problem: choosing the optimal utility and choosing the optimal class. If the true utility matrix is, for instance, U(2) \u2261 ( \ud835\udc48\n(2) \ud835\udc56 \ud835\udc57\n) , and the true class is\nclass 0, then choosing class 1 would yield a utility \ud835\udc48 (2)10 , choosing class 0 would yield a utility\ud835\udc48 (2)00 , and so on. Our double decision problem is thus characterized by a rectangular utility matrix that is the row-concatenation of the utility matrices U(\ud835\udc4e). We make the realistic judgement that the probabilities \ud835\udc5e\ud835\udc4e of the utility matrices and the probabilities \ud835\udc5d \ud835\udc57 of the classes are independent, so that \ud835\udc5e\ud835\udc4e \u00b7 \ud835\udc5d \ud835\udc57 is the probability that the true utility matrix is U(\ud835\udc4e) and the true class is \ud835\udc57. The principle of maximum expected utility, \u00a7 2.3 eq. (9), then leads to the maximization of the expected utilities\n?\u0304?\ud835\udc56 := \u2211 \ud835\udc57 ,\ud835\udc4e \ud835\udc48 (\ud835\udc4e) \ud835\udc56 \ud835\udc57 \ud835\udc5e\ud835\udc4e \u00b7 \ud835\udc5d \ud835\udc57 \u2261 \u2211 \ud835\udc57 [\u2211 \ud835\udc4e\n\ud835\udc5e\ud835\udc4e \ud835\udc48 (\ud835\udc4e) \ud835\udc56 \ud835\udc57\ufe38 \ufe37\ufe37 \ufe38\nU\u0302\n] \ud835\udc5d \ud835\udc57 (19)\nin which the expected utility matrix (18) appears as an \u2018effective\u2019 utility matrix to be used for the class-decision problem alone.\nIf our uncertainty is symmetric with respect to the utilities conditional on the different classes \u2013 for instance, our uncertainty about the utilities conditional on class 0 is the same as on class 1 \u2013 then the expected utility matrix is equivalent to the identity matrix. The utility yield is\n18 see e.g. Raiffa 1970 esp. ch. 3.\nin this case equal to the accuracy. The accuracy is therefore the natural evaluation metric to use if we are in a complete state of uncertainty regarding the underlying utilities. This fact is indeed reflected in some results discussed in \u00a7 4.2.\nFor binary classification the set of possible utility matrices can be represented as in fig. 1, as discussed in \u00a7 3.2. Our uncertainty about the true underlying utility matrix corresponds to a discrete or continuous distribution of probability over this set. Note, however, that the expected utility matrix (18) does not correspond to the mass-centre of the distribution, because of the peculiar coordinate system used in that figure. The actual mass-centre is obtained by representing the set of utility matrices as a two-dimensional surface (a tetrahedron) in three-dimensional space. For brevity we do not discuss this representation in the present work.\nThe procedure of averaging utilities, formula (18), also applies if we want to evaluate how a classifier performs on average on several classification problems, which differ in their utility matrices. Again, what we need to use is the average of their utility matrices."
        },
        {
            "heading": "4.2 Consequences of wrong utility assessments and comparison with common metrics",
            "text": "It may happen that our assessment of the utility matrix of a classification problem is incorrect, especially if it has been made on semi-quantitative grounds owing to a lack of information. Then our comparative evaluations of classifiers may also end up being incorrect. What is the probability of an incorrect comparative evaluation, on average, in such cases? and how does it depend on the amount of error in the utilities? Is it higher than the probability of incorrect evaluation by other metrics?\nA precise answer to these questions is extremely difficult if not impossible because to define \u2018on average\u2019 we would need to conduct a survey of classification problems of any kind, collecting statistics about their underlying utility matrices, about the confusion matrices of candidate classification algorithms for their solution, and about the errors committed in assessing utilities. We try to give a cursory answer to the questions above for the binary-classification case, based on the following assumptions and judgements:\n(i) Two possible distributions of true utility matrices on the set of fig. 1 (in that coordinate system): 1. a uniform distribution; 2. a bivariate\n0 1\n]\nand\n[ 1 0\n0.5 0.5\n]\n(black diamonds).\n(truncated) gaussian distribution centred on the identity matrix[ 1 0 0 1 ] and with standard deviation 1/3 in the \ud835\udc65 and \ud835\udc66 coordinates of eq. (13), illustrated in fig. 4.\n(ii) A distribution of confusion matrices for which the fraction of one class is uniformly distributed in [0, 1], and the true-positive and true-negative rates are independently distributed in [0.5, 1] with linearly increasing probabilities (median of 0.85, lower and upper quartiles at 0.75 and 0.93; see side plot on p. 16). This means that we consider problems with highly imbalanced data to be as common as problems with balanced data (a realistic assumption, according to our experience), and candidate classifiers to be generally good.\n(iii) A truncated gaussian distribution of error around each true utilitymatrix element, centred on the true utility value. We consider standard deviations ranging from 0 to 0.3. The gaussian must be truncated because each true utility has a value between 0 and 1, and because we require the utilities of correct classifications to be larger than those of incorrect ones. Figure 5 illustrates the extent of such an error in the space of utility matrices, for standard deviations equal to 0.1 (blue triangles) and 0.2 (red squares).\nUnder these assumptions, we calculate how often a pair of classifiers, having two confusion matrices with the same class proportions, is\nevaluated in reverse order, with respect to their true utility yield, when an incorrect utility matrix or another metric is used for the evaluation. This calculation is an integration problem that we solve by Monte Carlo sampling. The procedure is intuitive:\n1. Select a \u2018true\u2019 utility matrix according to the distribution (i). 2. Select errors around the elements of the true utility matrix, accord-\ning to the distribution (iii), and add them to it. 3. Select a class proportion and then two confusion matrices having\nthat class proportion (the class proportion must be the same since the matrices are obtained from the same data), according to the distributions (ii).\n4. Calculate the signed difference between the true utility yield of the second confusion matrix and that of the first confusion matrix, using the true utility from step 1. If this difference is positive, then the second confusion matrix has higher utility than the first; if negative, then the first confusion matrix has higher utility than the second.\n5. a. Consider several metrics (precision, Matthews correlation coefficient, and so on). For each, calculate the signed difference between the score it gives to the second confusion matrix, and the score it gives to the first.\nb. Consider the erroneous utility matrix from step 2. Calculate the signed difference between the utility yield of the second confusion matrix and that of the first confusion matrix, using this erroneous utility matrix.\nIn either case, a positive difference means that the second confusion matrix is ranked \u2018best\u2019 and the second \u2018worst\u2019, and vice versa for a negative difference.\n6. Now go through the signed differences obtained in step 5, and compare them, in turn, with the signed difference obtained in step 4. If the difference from step 5 has opposite sign to that of step 4, then the two confusion matrices are oppositely and incorrectly ranked by the corresponding metric or by the erroneous utility matrix.\nThe results of this sampling procedure for the case of uniform distribution of true utility matrices, several metrics, and utilities affected by errors with 0.1 standard deviation, are shown in fig. 6. Each point\nrepresents a pair of confusion matrices (step 3); its coordinates are the true utility yield and either the score given by a metric or (last plot) the yield according to the incorrect utility matrix. The red or yellow triangular points in the II and IV quadrants (discordant signs) are incorrectly ranked pairs. The percentages of incorrect rankings are calculated from 106 samples, giving slightly more than one decimal significant digit; fewer samples are shown in the plots.\nThe plots are displayed in order (left-right, top-bottom) of decreasing percentages of incorrect rankings. The accuracy metric proves to be the best among the ones considered, leading to 8.7% incorrect pairwise rankings. But we see that a utility matrix affected by gaussian errors with 0.1 standard deviation is even better, yielding 4% incorrect pairwise rankings.\nThe dependence of the fraction of incorrect rankings on the standard deviation of the error affecting the utilities is shown in the plots of fig. 7, for the case of uniform distribution (top plot) and gaussian distribution (bottom plot) of true utility matrices. It is approximately linear. The plots also report the fractions of incorrect rankings for the other metrics. We see that evaluations based on a utility matrix affected by errors with standard deviation up to 0.15 or even 0.25 are still more reliable than evaluations based on the other reported metrics. This is a remarkable fact, considering that errors with such standard deviations are quite large, as was shown in fig. 5.\nA utility error with standard deviations around 0.25 covers the whole space of utility matrices almost uniformly (cf fig. 5). Such a large error means that we are almost completely uncertain about the utilities to start with. It therefore makes sense that the accuracy, equivalent to using the identity utility matrix, becomes a more reliable metric when this error level is reached: as we saw in \u00a7 4.1, the identity utility matrix is the natural one to use in a state of complete uncertainty about the utilities. This result is just another example of the internal consistency of Decision Theory."
        },
        {
            "heading": "5 What about the area under the curve of the receiver operating characteristic?",
            "text": "Another very common metric for evaluating binary classifiers is the Area Under the Curve of the Receiver Operating Characteristic, or \u2018area\nunder the curve\u2019 for short. This metric can only be used for particular classifying algorithms, and its meaning is different from that of the metrics reviewed so far. For these reasons, we leave a full discussion of it to future works and only offer a couple of remarks here.\nThe area under the curve can only be computed for classifiers that output a continuous variable rather than a class. A threshold for this variable determines whether its value predicts one class or the other. Different choices of threshold lead to different pairs of false-positive rate \ud835\udc53 (which is 1 \u2212 true-negative rate) and true-positive rate \ud835\udc61 in a given test set. These pairs can be plotted as a curve \ud835\udc53 \u21a6\u2192 \ud835\udc61( \ud835\udc53 ) on a graph with corresponding axes. Given the proportion of classes in the test set, every point on the curve corresponds to a possible confusion matrix \ud835\udc36\ud835\udc56 \ud835\udc57( \ud835\udc53 ) that the classifier can produce depending on the threshold chosen. The area subtended by the curve is a weighted average of true-positive rates with a peculiar choice of weights; the weights are uniform as a function of the false-positive rate, but generally not uniform as a function of the threshold, for example. The meaning and proper use of the receiver operating characteristic are discussed in a classic by Metz 1978, see especially p. 290.\nFrom the standpoint of decision theory, two remarks can be made19. First, according to the principle of maximum expected utility, \u00a7 2.3, we should choose a threshold and corresponding false-positive rate \ud835\udc53 \u2217 such as to maximize the utility yield, given by eq. (10):\nchoose \ud835\udc53 \u2217 = arg max \ud835\udc53 { 1\u2211 \ud835\udc56 , \ud835\udc57=0 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc36\ud835\udc56 \ud835\udc57( \ud835\udc53 ) } . (20)\nAny other values of \ud835\udc53 and of the threshold are irrelevant. Averages over \ud835\udc53 values are therefore irrelevant as well. Second, suppose our goal is to evaluate the average performance over several possible classification problems. In that case, the quantities to be averaged are the utility matrices of those classification problems, as discussed in \u00a7 4.1, yielding a unique expected utility matrix. Once this is computed, we go back to a single choice of \ud835\udc53 according to our first remark.\nOwing to these issues, the area under the curve suffers from the same problems as the non-compliant metrics discussed in \u00a7 3.3: in every classification problem, it always leads to cases of incorrect evaluation. 19 similar points are made by Baker & Pinsky 2001; Lobo et al. 2008.\nA correct use of the receiver-operating-characteristic curve \ud835\udc61( \ud835\udc53 ) can be made, however. It is explained in Metz 1978 section Cost/Benefit Analysis p. 295, and in Sox et al. 2013 \u00a7 5.7.4 (curiously Sox et al. also mention the generally erroneous criterion of the area under the curve).\nDenote the proportion of class 0 (positive) in the test set by \ud835\udc35. The confusion matrix as a function of \ud835\udc53 is then[\n\ud835\udc3600( \ud835\udc53 ) \ud835\udc3601( \ud835\udc53 ) \ud835\udc3610( \ud835\udc53 ) \ud835\udc3611( \ud835\udc53 )\n] = [ \ud835\udc35 \ud835\udc61( \ud835\udc53 ) (1 \u2212 \ud835\udc35) \ud835\udc53\n\ud835\udc35 [1 \u2212 \ud835\udc61( \ud835\udc53 )] (1 \u2212 \ud835\udc35) (1 \u2212 \ud835\udc53 )\n] . (21)\nThe sum in formula (20) above can then be explicitly written, rearranging some terms,\n1\u2211 \ud835\udc56 , \ud835\udc57=0 \ud835\udc48\ud835\udc56 \ud835\udc57 \ud835\udc36\ud835\udc56 \ud835\udc57( \ud835\udc53 ) \u2261 (\ud835\udc4800 \u2212\ud835\udc4810) \ud835\udc35 \ud835\udc61( \ud835\udc53 ) \u2212 (\ud835\udc4811 \u2212\ud835\udc4801) (1 \u2212 \ud835\udc35) \ud835\udc53 +\n\ud835\udc4810 \ud835\udc35 +\ud835\udc4811 (1 \u2212 \ud835\udc35) . (22) The principle of maximum expected utility (20) is then equivalent to the following condition, obtained using the explicit sum above but dropping the constant term on the second line for simplicity:\nchoose \ud835\udc53 \u2217 = arg max \ud835\udc53\n{ (\ud835\udc4800 \u2212\ud835\udc4810) \ud835\udc35 \ud835\udc61( \ud835\udc53 ) \u2212 (\ud835\udc4811 \u2212\ud835\udc4801) (1\u2212 \ud835\udc35) \ud835\udc53 } . (23)\nThe function in braces is monotonically increasing because \ud835\udc61( \ud835\udc53 ) is (we assume, as always, that the utility of correct classification of a class is higher than that of misclassification, so \ud835\udc4800 \u2212\ud835\udc4810 \u2a7e 0 and \ud835\udc4811 \u2212\ud835\udc4801 \u2a7e 0). Its maximum can thus be found by setting its derivative to zero:\nchoose \ud835\udc53 \u2217 such that \ud835\udc61\u2032( \ud835\udc53 \u2217) = (\ud835\udc4811 \u2212\ud835\udc4801) (1 \u2212 \ud835\udc35)(\ud835\udc4800 \u2212\ud835\udc4810) \ud835\udc35 . (24)\nIf we have several classifiers, each with its own curve \ud835\udc61( \ud835\udc53 ), then the best is the one tangent to the line\n\ud835\udc61 = (\ud835\udc4811 \u2212\ud835\udc4801) (1 \u2212 \ud835\udc35)\n(\ud835\udc4800 \u2212\ud835\udc4810) \ud835\udc35 \ud835\udc53 + const. (25)\nthat has the highest intercept. From this criterion it can be seen geometrically that if a classifier has its curve \ud835\udc61( \ud835\udc53 ) completely above the curve of another classifier, then it must have a higher utility yield. But nothing, in general, can be said if the curves of the two classifiers cross. It is the tangent of a receiver-operatingcharacteristic curve that matters, not its subtended area. Figure 8 shows an example of this."
        },
        {
            "heading": "6 Summary and discussion",
            "text": "The evaluation and ranking of classification algorithms is a critical stage in their development and deployment. Without such evaluation we cannot even say whether an algorithm is better than another, or whether a set of parameter values for a specific algorithm is better than another set.\nAnd yet, at present, we have not an evaluation theory but only an evaluation folklore: different procedures, proposed only out of intuition and of analysis of special cases, with fuzzy criteria to decide which should be used, and without rigorous theoretical foundations that should guarantee uniqueness, universality properties, and absence of biases. We believe that some of the surprising failures of machine learning in actual applications20 come not only from biases in the choice of test datasets and\n20 see e.g. Varoquaux & Cheplygina 2022.\nother similar biases, but also from the use of wrong evaluation metrics in the development stage.\nIn the present work, we have argued that theoretical foundations for the evaluation process are available in Decision Theory. Its main notions and principle \u2013 utilities and their maximization \u2013 are very intuitive, as shown (we hope) by the introductory story.\nThese are the main results of the application of decision theory to the evaluation of classifiers:\n\u2022 The evaluation metric must depend on the specific classification problem.\n\u2022 Such metric is completely defined by \ud835\udc5b2 parameters, called utilities, collected in a utility matrix; \ud835\udc5b is the number of classes. Two parameters are arbitrary and represent a zero and measurement unit of the utility scale. In the binary-classification case, this means that we have a twodimensional set of possible metrics.\n\u2022 The score of a classifier on a test set is simply given by its utility yield: the grand sum of the products of the elements of the utility matrix and the confusion matrix of the classifier. It is a simple linear expression in the confusion-matrix elements.\n\u2022 A utility matrix, obtained from an average, is also used when we are uncertain about the utilities underlying a classification problem or when we want to consider the average performance over several classification problems.\n\u2022 Some popular metrics such as precision, balanced accuracy, Matthews correlation coefficient, Fowlkes-Mallows index, \ud835\udc391-measure, and area under the receiver-operating-characteristic curve do not comply with decision theory. As a consequence, they always lead to some erroneous comparative evaluations of classifiers in every classification problem, even when all utilities and frequencies are correctly assessed; are they are likely affected by cognitive biases.\n\u2022 Using a utility matrix with incorrectly assessed utilities still leads, on average, to fewer wrong comparative evaluations than using other popular metrics.\nWe believe that the decision-theoretic evaluation of classifiers also has remarkable advantages:\nFirst, it translates the fuzzy problem \u201cwhich of the numerous scores should I rely on?\u201d into a more structured, thus easier to confront, one: to assess, at least semi-quantitatively, how many times more valuable, desirable, or useful is the correct classification of a class than its incorrect classification, than the correct classification of another class, and so on. Such utilities usually have a more immediate, problem-dependent interpretation than other metrics.\nSecond, it leads to a mathematically simple, computationally convenient metric: a linear combination of confusion-matrix elements \u2013 no need for non-linear functions or integration of curves.\nThird, the principles of the underlying theory guide us if we have to face new peculiar problems. Imagine, for instance, a classification problem where we cannot say, in general, whether true positives are more important than true negatives and so on, because such valuation can vary from one tested item to another. Decision theory, in this case, requires an item-wise assessment of utilities, and still provides an item-wise score, which can be accumulated across items to obtain a total evaluation score for the performance of candidate classifiers.\nThe theory, remarks, and results of the present work generalize beyond classification: to regression and more complex classification-like problems such as image segmentation, with important applications in medicine21. It would be interesting to examine whether popular metrics in the latter field, such as Dice score22 and Hausdorff distance23, comply with decision-theoretic principles, and which alternatives could be used otherwise.\nIn a companion work24 we apply the general ideas presented here to improve the performance of machine-learning classifiers.\nAuthor contributions\nAll authors have contributed equally to the present work.\n21 Lundervold & Lundervold 2019. 22 Dice 1945; Fleiss 1975; Z\u0133denbos et al. 1994. 23 Alt & Guibas 2000. 24 Dyrland et al. 2022.\nThanks\nKD and ASL acknowledge support from the Trond Mohn Research Foundation, grant number BFS2018TMT07, and PGLPM from The Research Council of Norway, grant number 294594.\nKD would like to thank family for endless support; partner Synne for constant love, support, and encouragement; and the developers and maintainers of Python, FastAi, PyTorch, scikit-learn, NumPy and RDKit for free open source software and for making the experiments possible.\nPGLPM thanks Iv\u00e1n Davidovich for useful comments on previous drafts of this work; Maja, Mari, Miri, Emma for continuous encouragement and affection; Buster Keaton and Saitama for filling life with awe and inspiration; and the developers and maintainers of LATEX, Emacs, AUCTEX, Open Science Framework, R, Python, Inkscape, LibreOffice, Sci-Hub for making a free and impartial scientific exchange possible."
        }
    ],
    "title": "Does the evaluation stand up to evaluation? A first-principle approach to the evaluation of classifiers",
    "year": 2023
}