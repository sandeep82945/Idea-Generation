{
    "abstractText": "The rapid evolution of coronaviruses in respiratory diseases, including severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), poses a significant challenge for deep learning models to accurately detect and adapt to new strains. To address this challenge, we propose a novel Continuous Learning approach, CoroTrans-CL, for the diagnosis and prevention of various coronavirus infections that cause severe respiratory diseases using chest radiography images. Our approach is based on the Swin Transformer architecture and uses a combination of the Elastic Weight Consolidation (EWC) and Herding Selection Replay (HSR) methods to mitigate the problem of catastrophic forgetting. We constructed an informative benchmark dataset containing multiple strains of coronaviruses and present the proposed approach in five successive learning stages representing the epidemic timeline of different coronaviruses (SARS, MERS, wild-type SARS-CoV-2, and the Omicron and Delta variants of SARS-CoV-2) in the real world. Our experiments showed that the proposed CoroTrans-CL model achieved a joint training accuracy of 95.34%, an F1 score of 92%, and an average accuracy of 83.40% while maintaining a balance between plasticity and stability. Our study demonstrates that CoroTransCL can accurately diagnose and detect the changes caused by new mutant viral strains in the lungs without forgetting existing strains, and it provides an effective solution for the ongoing diagnosis of mutant SARS-CoV-2 virus infections.",
    "authors": [
        {
            "affiliations": [],
            "name": "Cheng Siong Chin"
        },
        {
            "affiliations": [],
            "name": "Kalyana C. Veluvolu"
        },
        {
            "affiliations": [],
            "name": "Boyuan Wang"
        },
        {
            "affiliations": [],
            "name": "Zonggui Tian"
        }
    ],
    "id": "SP:b4e3994e8f258dea6dfbe4276dd9c1d461bd61ad",
    "references": [
        {
            "authors": [
                "C. Huang",
                "Y. Wang",
                "X. Li",
                "L. Ren",
                "J. Zhao",
                "Y. Hu",
                "L. Zhang",
                "G. Fan",
                "J. Xu",
                "X Gu"
            ],
            "title": "Clinical features of patients infected with 2019 novel coronavirus in Wuhan, China",
            "year": 2020
        },
        {
            "authors": [
                "A. Das"
            ],
            "title": "Adaptive UNet-based Lung Segmentation and Ensemble Learning with CNN-based Deep Features for Automated COVID-19 Diagnosis",
            "venue": "Multimed. Tools Appl",
            "year": 2022
        },
        {
            "authors": [
                "S. Park",
                "G. Kim",
                "Y. Oh",
                "J.B. Seo",
                "S.M. Lee",
                "J.H. Kim",
                "S. Moon",
                "J.K. Lim",
                "J.C. Ye"
            ],
            "title": "Multi-task vision transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity",
            "venue": "quantification. Med. Image Anal",
            "year": 2022
        },
        {
            "authors": [
                "C.S. Guan",
                "Z.B. Lv",
                "S. Yan",
                "Y.N. Du",
                "H. Chen",
                "L.G. Wei",
                "R.M. Xie",
                "B.D. Chen"
            ],
            "title": "Imaging Features of Coronavirus disease",
            "venue": "Evaluation on Thin-Section CT. Acad. Radiol",
            "year": 2020
        },
        {
            "authors": [
                "M. Shorfuzzaman",
                "M. Masud",
                "H. Alhumyani",
                "D. Anand",
                "A. Singh"
            ],
            "title": "Artificial Neural Network-Based Deep Learning Model for COVID-19 Patient Detection Using X-Ray Chest Images",
            "venue": "J. Healthc. Eng",
            "year": 2021
        },
        {
            "authors": [
                "D. Yang",
                "C. Martinez",
                "L. Visu\u00f1a",
                "H. Khandhar",
                "C. Bhatt",
                "J. Carretero"
            ],
            "title": "Detection and analysis of COVID-19 in medical images using deep learning techniques",
            "venue": "Sci. Rep. 2021,",
            "year": 1963
        },
        {
            "authors": [
                "A. Vaswani",
                "N.M. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is All you Need",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "J. Jumper",
                "R. Evans",
                "A. Pritzel",
                "T. Green",
                "M. Figurnov",
                "O. Ronneberger",
                "K. Tunyasuvunakool",
                "R. Bates",
                "A. Zidek",
                "A Potapenko"
            ],
            "title": "Highly accurate protein structure prediction with AlphaFold",
            "venue": "Nature",
            "year": 2021
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S Gelly"
            ],
            "title": "An Image is Worth 16 \u00d7 16 Words: Transformers for Image Recognition at Scale",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A.K. Mondal",
                "A. Bhattacharjee",
                "P. Singla",
                "A.P. Prathosh"
            ],
            "title": "xViTCOS: Explainable Vision Transformer Based COVID-19 Screening Using Radiography",
            "venue": "IEEE J. Transl. Eng. Health Med",
            "year": 2022
        },
        {
            "authors": [
                "C. Zhang",
                "M. Zhang",
                "S. Zhang",
                "D. Jin",
                "Q. Zhou",
                "Z. Cai",
                "H. Zhao",
                "X. Liu",
                "Z. Liu"
            ],
            "title": "Delving Deep Into the Generalization of Vision Transformers Under Distribution Shifts",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA,",
            "year": 2021
        },
        {
            "authors": [
                "R. Chen CF",
                "Q. Fan",
                "R. Panda"
            ],
            "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, QC, Canada,",
            "year": 2021
        },
        {
            "authors": [
                "C.C. Ukwuoma",
                "Z. Qin",
                "M.B.B. Heyat",
                "F. Akhtar",
                "A. Smahi",
                "J.K. Jackson"
            ],
            "title": "Automated Lung-Related Pneumonia and COVID19 Detection Based on Novel Feature Extraction Framework and Vision Transformer Approaches Using Chest X-ray Images",
            "venue": "Bioengineering 2022,",
            "year": 2022
        },
        {
            "authors": [
                "R. Hadsell",
                "D. Rao",
                "A.A. Rusu",
                "R. Pascanu"
            ],
            "title": "Embracing Change: Continual Learning in Deep Neural Networks",
            "venue": "Trends Cogn. Sci",
            "year": 2020
        },
        {
            "authors": [
                "M. McCloskey",
                "N.J. Cohen"
            ],
            "title": "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
            "venue": "In Psychology of Learning and Motivation;",
            "year": 1989
        },
        {
            "authors": [
                "W.C. Abraham",
                "A. Robins"
            ],
            "title": "Memory retention\u2013the synaptic stability versus plasticity dilemma",
            "venue": "Trends Neurosci",
            "year": 2005
        },
        {
            "authors": [
                "G.I. Parisi",
                "R. Kemker",
                "J.L. Part",
                "C. Kanan",
                "S. Wermter"
            ],
            "title": "Continual lifelong learning with neural networks: A review",
            "venue": "Neural Netw",
            "year": 2019
        },
        {
            "authors": [
                "T. Lesort",
                "V. Lomonaco",
                "A. Stoian",
                "D. Maltoni",
                "D. Filliat",
                "N. D\u00edaz-Rodr\u00edguez"
            ],
            "title": "Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges",
            "venue": "Inf. Fusion",
            "year": 2020
        },
        {
            "authors": [
                "A. Prabhu",
                "P.H.S. Torr",
                "P.K. Dokania"
            ],
            "title": "GDumb: A Simple Approach that Questions Our Progress in Continual Learning",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV",
            "year": 2020
        },
        {
            "authors": [
                "S. Rebuffi",
                "A. Kolesnikov",
                "G. Sperl",
                "C.H. Lampert"
            ],
            "title": "iCaRL: Incremental Classifier and Representation Learning",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "R. Aljundi",
                "F. Babiloni",
                "M. Elhoseiny",
                "M. Rohrbach",
                "T. Tuytelaars"
            ],
            "title": "Memory Aware Synapses: Learning What (not) to Forget",
            "venue": "In Proceedings of the Computer Vision\u2014ECCV 2018, Cham, Switzerland,",
            "year": 2018
        },
        {
            "authors": [
                "J. Kirkpatrick",
                "R. Pascanu",
                "N. Rabinowitz",
                "J. Veness",
                "G. Desjardins",
                "A.A. Rusu",
                "K. Milan",
                "J. Quan",
                "T. Ramalho",
                "A GrabskaBarwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proc. Natl. Acad. Sci. USA",
            "year": 2017
        },
        {
            "authors": [
                "E. Erg\u00fcn",
                "B.U. T\u00f6reyin"
            ],
            "title": "Sparse Progressive Neural Networks for Continual Learning",
            "venue": "In Proceedings of the International Conference on Computational Collective Intelligence, Cham, Switzerland,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Li",
                "D. Hoiem"
            ],
            "title": "Learning without Forgetting",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 2018
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "venue": "arXiv 2014,",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L.V.D. Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely Connected Convolutional Networks",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "C. Szegedy",
                "L. Wei",
                "J. Yangqing",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "F. Chollet"
            ],
            "title": "Xception: Deep Learning with Depthwise Separable Convolutions",
            "venue": "In Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA,",
            "year": 2017
        },
        {
            "authors": [
                "A.G. Howard",
                "M. Zhu",
                "B. Chen",
                "D. Kalenichenko",
                "W. Wang",
                "T. Weyand",
                "M. Andreetto",
                "H. Adam"
            ],
            "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "M. Tan",
                "Q.V. Le"
            ],
            "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "L. Brunese",
                "F. Mercaldo",
                "A. Reginelli",
                "A. Santone"
            ],
            "title": "Explainable Deep Learning for Pulmonary Disease and Coronavirus COVID-19 Detection from X-rays",
            "venue": "Comput. Methods Programs Biomed",
            "year": 2020
        },
        {
            "authors": [
                "A. Narin",
                "C. Kaya",
                "Z. Pamuk"
            ],
            "title": "Automatic detection of coronavirus disease (COVID-19) using X-ray images and deep convolutional neural networks",
            "venue": "Pattern Anal. Appl",
            "year": 2021
        },
        {
            "authors": [
                "G. Wang",
                "X. Liu",
                "J. Shen",
                "C. Wang",
                "Z. Li",
                "L. Ye",
                "X. Wu",
                "T. Chen"
            ],
            "title": "A deep-learning pipeline for the diagnosis and discrimination of viral, non-viral and COVID-19 pneumonia from chest",
            "venue": "X-ray images. Nat. Biomed. Eng. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "A. Shamila Ebenezer",
                "S. Deepa Kanmani",
                "M. Sivakumar",
                "S. Jeba Priya"
            ],
            "title": "Effect of image transformation on EfficientNet model for COVID-19 CT image classification",
            "venue": "Mater. Today Proc",
            "year": 2022
        },
        {
            "authors": [
                "S. Toraman",
                "T.B. Alakus",
                "I. Turkoglu"
            ],
            "title": "Convolutional capsnet: A novel artificial neural network approach to detect COVID-19 disease from X-ray images using capsule networks",
            "venue": "Chaos Solitons Fractals",
            "year": 2020
        },
        {
            "authors": [
                "S. Sabour",
                "N. Frosst",
                "G.E. Hinton"
            ],
            "title": "Dynamic Routing Between Capsules",
            "venue": "arXiv 2017,",
            "year": 2023
        },
        {
            "authors": [
                "H. Gunraj",
                "L. Wang",
                "A. Wong"
            ],
            "title": "COVIDNet-CT: A Tailored Deep Convolutional Neural Network Design for Detection of COVID-19 Cases From Chest CT Images",
            "venue": "Front. Med",
            "year": 2020
        },
        {
            "authors": [
                "M. Ghaderzadeh",
                "M.A. Eshraghi",
                "F. Asadi",
                "A. Hosseini",
                "R. Jafari",
                "D. Bashash",
                "H. Abolghasemi"
            ],
            "title": "Efficient Framework for Detection of COVID-19 Omicron and Delta Variants",
            "venue": "Based on Two Intelligent Phases of CNN Models. Comput. Math. Methods Med",
            "year": 2022
        },
        {
            "authors": [
                "A.M. Tahir",
                "Y. Qiblawey",
                "A. Khandakar",
                "T. Rahman",
                "U. Khurshid",
                "F. Musharavati",
                "M.T. Islam",
                "S. Kiranyaz",
                "S. Al-Maadeed",
                "M.E.H. Chowdhury"
            ],
            "title": "Deep Learning for Reliable Classification of COVID-19, MERS, and SARS from Chest X-ray Images",
            "year": 2022
        },
        {
            "authors": [
                "A.M. Tahir",
                "M.E.H. Chowdhury",
                "A. Khandakar",
                "T. Rahman",
                "Y. Qiblawey",
                "U. Khurshid",
                "S. Kiranyaz",
                "N. Ibtehaz",
                "M.S. Rahman",
                "S Al-Maadeed"
            ],
            "title": "COVID-19 infection localization and severity grading from chest",
            "venue": "X-ray images. Comput. Biol. Med",
            "year": 2021
        },
        {
            "authors": [
                "D.S. Kermany",
                "M. Goldbaum",
                "W. Cai",
                "C.C.S. Valentim",
                "H. Liang",
                "S.L. Baxter",
                "A. McKeown",
                "G. Yang",
                "X. Wu",
                "F Yan"
            ],
            "title": "Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning",
            "venue": "Cell 2018,",
            "year": 2018
        },
        {
            "authors": [
                "G. Merlin",
                "V. Lomonaco",
                "A. Cossu",
                "A. Carta",
                "D. Bacciu"
            ],
            "title": "Practical Recommendations for Replay-Based Continual Learning Methods",
            "venue": "In Proceedings of the Image Analysis and Processing, ICIAP 2022 Workshops, Cham, Switzerland,",
            "year": 2022
        },
        {
            "authors": [
                "A.V. Robins"
            ],
            "title": "Catastrophic Forgetting, Rehearsal and Pseudorehearsal",
            "venue": "Connect. Sci",
            "year": 1995
        },
        {
            "authors": [
                "Y. Zhou",
                "S. Zhang",
                "X. Sun",
                "F. Ma",
                "F. Zhang"
            ],
            "title": "SAR Target Incremental Recognition Based on Hybrid Loss Function and Class-Bias Correction",
            "venue": "Appl. Sci",
            "year": 2022
        },
        {
            "authors": [
                "V. Lomonaco",
                "L. Pellegrini",
                "A. Cossu",
                "A. Carta",
                "G. Graffieti",
                "T.L. Hayes",
                "M.D. Lange",
                "M. Masana",
                "J. Pomponi",
                "G.M.v.d Ven"
            ],
            "title": "Avalanche: An End-to-End Library for Continual Learning",
            "venue": "In Proceedings of the 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "S. Grossberg"
            ],
            "title": "A Path Toward Explainable AI and Autonomous Adaptive Intelligence: Deep Learning, Adaptive Resonance, and Models of Perception, Emotion, and Action",
            "venue": "Front. Neurorobot",
            "year": 2020
        },
        {
            "authors": [
                "S.K. Amalapuram",
                "A. Tadwai",
                "R. Vinta",
                "S.S. Channappayya",
                "B.R. Tamma"
            ],
            "title": "Continual Learning for Anomaly based Network Intrusion Detection",
            "venue": "In Proceedings of the 2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS), Bangalore, India,",
            "year": 2022
        },
        {
            "authors": [
                "M. Biesialska",
                "K. Biesialska",
                "M.R. Costa-juss\u00e0"
            ],
            "title": "Continual Lifelong Learning in Natural Language Processing: A Survey",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "I.O. Tolstikhin",
                "N. Houlsby",
                "A. Kolesnikov",
                "L. Beyer",
                "X. Zhai",
                "T. Unterthiner",
                "J. Yung",
                "D. Keysers",
                "J. Uszkoreit",
                "M Lucic"
            ],
            "title": "MLP-Mixer: An all-MLP Architecture for Vision",
            "venue": "Adv. Neural Inf. Process. Syst. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Peng",
                "L. Dong",
                "H. Bao",
                "Q. Ye",
                "F. Wei"
            ],
            "title": "BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers",
            "venue": "arXiv 2022,",
            "year": 2022
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR), San Diego, CA, USA,",
            "year": 2015
        },
        {
            "authors": [
                "M.A.R. David Lopez-Paz"
            ],
            "title": "Gradient Episodic Memory for Continual Learning",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems (NIPS), Long Beach, CA,",
            "year": 2017
        },
        {
            "authors": [
                "A. Chaudhry",
                "M.A. Ranzato",
                "M. Rohrbach",
                "M. Elhoseiny"
            ],
            "title": "Efficient Lifelong Learning with A-GEM",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "V. Lomonaco",
                "D. Maltoni",
                "L. Pellegrini"
            ],
            "title": "Rehearsal-Free Continual Learning over Small Non-I.I.D. Batches",
            "venue": "In Proceedings of the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Seattle, WA, USA,",
            "year": 2020
        },
        {
            "authors": [
                "L. Van der Maaten",
                "G. Hinton"
            ],
            "title": "Viualizing data using t-SNE",
            "venue": "J. Mach. Learn. Res. 2008,",
            "year": 2605
        }
    ],
    "sections": [
        {
            "text": "Citation: Wang, B.; Zhang, D.; Tian,\nZ. CoroTrans-CL: A Novel\nTransformer-Based Continual Deep\nLearning Model for Image\nRecognition of Coronavirus\nInfections. Electronics 2023, 12, 866.\nhttps://doi.org/10.3390/\nelectronics12040866\nAcademic Editors: Cheng Siong Chin,\nKalyana C. Veluvolu, Mazdak\nZamani and Len Gelman\nReceived: 14 January 2023\nRevised: 31 January 2023\nAccepted: 4 February 2023\nPublished: 8 February 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: continual learning; coronaviruses; swin transformer"
        },
        {
            "heading": "1. Introduction",
            "text": "Coronavirus disease 2019 (COVID-19) has become the most widespread respiratory infectious disease of the 21st century [1], infecting more than 657,060,111 people and causing 6,669,951 deaths worldwide as of 17 December 2022 [2]. The rapid mutation and emergence of immune escape variants such as Delta and Omicron have made testing for the virus a challenging task for public health workers. Currently, real-time reverse transcription-polymerase chain reaction (RT-PCR) testing have a false-negative rate in experimental testing, requiring repeat testing to reduce misdiagnosis [3,4]. Chest computed tomography (CT) can be used to improve sensitivity in diagnosing COVID-19 cases [5,6], with the main findings in chest CT being ground-glass opacities, pulmonary consolidation and \u2018leaving stone\u2019 signs after SARS-CoV-2 infection [7]. These findings, together with RT-PCR results, clinical symptoms, and epidemiological history, are the sole basis for the diagnosis or exclusion of COVID-19 pneumonia. To achieve automatic early warning for COVID-19, some studies have attempted to develop models that can automatically identify COVID-19 patients by learning lesion characteristics using artificial intelligence technology. Most of these studies have used convolutional neural networks (CNNs) to automatically identify COVID-19 patients based on chest CT images [8,9]. Although CNNs have demonstrated their ability to solve various clas-\nElectronics 2023, 12, 866. https://doi.org/10.3390/electronics12040866 https://www.mdpi.com/journal/electronics\nElectronics 2023, 12, 866 2 of 21\nsification problems, they are not ideal for tasks requiring high-level categorization where global features such as patterns, multiplicity and distribution need to be considered [6]. Since 2017, there have been significant advances in deep learning algorithms, applications and technologies, such as the Transformer architecture proposed by Ashish et al. [10], which has become a highlight in the field of deep learning and deep neural networks. The Transformer architecture has evolved and expanded beyond its original language tasks to other domains such as bioinformatics, where it is the key technology for DeepMind\u2019s protein structure prediction model, AlphaFold [11]. More recently, the Transformer architecture entered the field of computer vision with the proposal of the Vision Transformer (ViT) by Dosovitskiy et al. [12], which has replaced CNNs in many complex tasks. Unlike the traditional convolution of CNNs, the main architecture of ViT consists of multiple stacks of Transformer blocks based on a self-attention mechanism. The multi-headed attention mechanism in the Transformer architecture can establish long-range dependencies on a target to extract more powerful features and capture global context information, making it suitable for disease detection in complex environments [13,14]. In addition, ViT\u2019s closer alignment with human cognitive features allows it to outperform CNNs in generalization under most distributed transformations (DS), with a top-1 correct rate that is 5\u201310% higher than the corresponding CNNs for the same or fewer number of parameters [15]. Other Transformer-based computer vision models include the Swin Transformer [16] and CrossViT [17]. Some recent studies using these architectures have achieved better classification results [13,18]. CNNs and ViT have demonstrated high performance in COVID-19 detection, outperforming general radiologists in certain tasks. However, many of these models are primarily based on fixed datasets and static environments that do not consider the gradual provision of information over time and therefore cannot adapt or learn new knowledge. In some cases, these models completely fail or show significant performance degradation on previously learned tasks, leading to problems of catastrophic forgetting [19] and limited intelligence. McCloskey and Cohen [20] were the first to identify catastrophic forgetting in neural network models. They found that when neural networks are trained on a new task or category, they often forget the information learned in the previous training task. The weights of the latest task can overwrite the weights of the previous task, leading to a decline in model performance, which is known as the stability\u2013plasticity dilemma [21]. In contrast to neural networks, the human learning capacity consists of a rich set of neurocognitive and brain memory mechanisms that facilitate the development of learning skills and the consolidation of long-term memory [22]. Inspired by cognitive science, Continual Learning, also referred to as Lifelong Learning, is a research area that aims to address such problems in artificial intelligence [23]. Its goal is to increase the adaptive capacity of models so that they can learn different tasks at different times (plasticity) without forgetting the characteristics of previous tasks (stability), as well as to make trained models more general. Based on the method of historical information retention, Continual Learning methods can be classified into three categories: rehearsal methods, regularization methods, and parameter isolation methods. Rehearsal approaches work by retaining some historical data or high-level representations. When learning a new task, the old task data are simultaneously replayed to reduce model forgetting [24,25]. Regularization methods constrain the optimization direction of the model on the new task in a way that minimises catastrophic forgetting. This includes adding distillation losses to the old model as the target, optimizing constraints on essential model parameters, and projecting the gradient direction of the parameters [26,27]. Parameter isolation methods extend the old model to new tasks by isolating the parameters of the old and new models to reduce the occurrence of catastrophic forgetting [28,29]. Currently, there is a lack of research and application of Continual Learning methods for artificial intelligence-based solutions in COVID-19 detection based on CT and X-ray images. To address these issues, we undertook this study and summarise our main contributions as follows:\nElectronics 2023, 12, 866 3 of 21\n1. We established a benchmark dataset of CT and chest X-ray images of pneumonia for Continuous Learning in medical image classification tasks. The dataset included normal, wild-type SARS-CoV-2, SARS-CoV-2 Omicron and Delta variant, and other viral pneumonia infection CT images, as well as normal, MERS, SARS, wild-type SARS-CoV-2, SARS-CoV-2Omicron and Delta variant, other viral pneumonia, and bacterial pneumonia infection chest X-ray images. 2. We designed a five-stage incremental learning task based on the real-world epidemic timeline of different coronaviruses (SARS, MERS, wild-type SARS-CoV-2, and the Omicron and Delta variants of SARS-CoV-2). We compared several models using three deep learning architectures (CNNs, ViT and MLP) and found that the Swin Transformer model with the highest classification accuracy on learning was the most suitable feature extraction backbone for our study. We also propose a novel imaging approach, CoroTrans-CL, based on the Swin Transformer architecture and the Continuous Learning strategies of regularization-based and rehearsal-based learning approaches to recognise CT and chest X-ray images of coronavirus infections, which cause severe respiratory infections, and to mitigate the problem of catastrophic forgetting and performance stagnation. 3. To address the issue of representative sample selection, we used the Herding Selection strategy to minimise the feature centre distance of sub-samples from the full dataset. We also conducted extensive ablation experiments to compare the effects of different Continual Learning methods and different sample selection strategies on the results in order to verify the effectiveness of each key component.\nThis paper is structured as follows. We review some of the literature on artificial intelligence in the fight against the COVID-19 pandemic in Section 2, and we describe the sources and construction methods for our datasets and our proposed approach, as well as performance evaluation metrics and model parameter settings for our experimental study, in Section 3. The experimental results and discussion are presented in Sections 4 and 5, respectively. Finally, in Section 6, we conclude the paper with comments on future work."
        },
        {
            "heading": "2. Related Works",
            "text": "In this section, we review the primary research methods used for current COVID-19 case detection. CNNs are the most commonly used approaches for automated COVID-19 diagnosis. Previous studies have mainly used pre-trained networks such as variants of Very Deep Convolutional Networks (VGG) [30], Residual Network (ResNet) [31], Densely Connected Convolutional Networks (DenseNet) [32], Google Inception Network (Inception) [33], Deep Learning with Depth-Wise Separable Convolutions (Xception) [34], Efficient Convolutional Neural Networks for Mobile Vision Applications (MobileNet) [35], and Rethinking Model Scaling for Convolutional Neural Networks (EfficientNet) [36] as deep learning frameworks. These models adapt to the new task of COVID-19 patient detection and classification by modifying or adding custom layers and by transferring knowledge from previous experience. For example, Brunese et al. [37] proposed two models using the VGG-16 network as a backbone model based on transfer learning. The first network is used to identify whether the target is healthy or has pneumonia. If the first network gives a positive prediction, the second network is used to identify COVID-19. The VGG-16 network achieved 98% accuracy for three-class classification. ResNet is another common CNN architecture that avoids gradient disappearance problems compared with earlier architectures such as VGG. Using a Residual Network, Narin et al. [38] classified COVID-19 cases and healthy cases with ResNet-50, achieving the highest accuracy (98%) for binary classification. Other studies have used more efficient architectures such as DenseNet and EfficientNet. Wang et al. [39] developed a COVID-19 pneumonia classification pipeline using DenseNet-121, which achieved an AUC with an overall performance of 0.88\u20130.99 across different datasets. Shamila et al. [40] used the EfficientNet architecture to build a classification model with 95% accuracy and a 93% F1 score on the test set.\nElectronics 2023, 12, 866 4 of 21\nAlthough CNNs are effective for image classification in deep learning, they have some conceptual limitations. During maximum pooling, CNNs lose information about the location of entities. They also do not consider some spatial relationships between simple objects and require a large receptive field to capture long-range dependencies, which leads to the development of large kernels or highly massive networks and results in a complex model that is difficult to train [13]. To overcome these limitations of CNNs, some researchers have used other architectures such as capsule neural networks (Capsnets) [41] and ViT [12] for COVID-19 classification. Sabour et al. proposed Capsnets [42], a new neural network architecture that uses location and orientation information to perform object recognition, to address the shortcomings of CNNs. Toraman et al. [41] proposed a five-convolutional layer Capsnets model with 16, 32, 64 and 128 kernels in the first four layers and 32 capsules in the fifth layer. After 10-fold cross-validation and 50 epochs of training, the model achieved 84.22% accuracy for multiclassification. Recent research in COVID-19 detection has focused on the Transformer architecture [10]. Dosovitskiy et al. [12] applied the standard Transformer architecture to image recognition and proposed the use of self-attention in ViT to approach or outperform the state-of-the-art (SOTA) model on several image recognition benchmarks. A few studies have proposed the use of ViT in COVID-19 recognition algorithms. Shome et al. [14] created a dataset of 30,000 images and trained the ViT model on it, achieving 92% accuracy and 98% AUC, outperforming CNNs such as EfficientNet-B0, Inception-V3 and ResNet-50 in multiclassification. Mondal et al. [13] proposed a network based on the ViT-B/16 architecture and achieved the highest accuracy of 98.1%, outperforming most existing methods."
        },
        {
            "heading": "3. Materials and Methods",
            "text": ""
        },
        {
            "heading": "3.1. Datasets",
            "text": "The evaluation of our proposed approach involved the creation of a comprehensive benchmark image dataset, CL-COVIDset, specifically designed for continuous learning in medical image classification tasks. This dataset included a variety of images, including CT scans and X-rays, representing different types of coronavirus infections and other viral and bacterial pneumonia infections. The datasets used to create the CL-COVIDset are publicly available. The CT images in the CL-COVIDset consist of normal scans [43] and scans showing infections caused by the wild-type SARS-CoV-2 strain [43] and its Omicron and Delta variants [44]. The dataset also included CT images of other viral pneumonia infections [43]. X-ray images in the CL-COVIDset also included normal scans and those showing infections caused by MERS [45], SARS [45], wild-type SARS-CoV-2 [45], the Omicron and Delta variants [44], other viral pneumonia infections [46,47], and bacterial pneumonia [48] infections. The CL-COVIDset consisted of three sets: a training set, a validation set, and an evaluation set (see Table 1). The use of the validation set allowed us to fine-tune our model, and the model was finally tested for performance on the evaluation set. The performance of the model on the evaluation set was assessed using observed and unobserved data and distributions. To facilitate the use of CL-COVIDset by the wider research community, we have made it publicly available on the Kaggle platform at the following link: https://www. kaggle.com/datasets/mustai/continual-learning-of-covid19 (accessed on 31 January 2023).\nElectronics 2023, 12, 866 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images Dataset\nTrain and Val Evaluation Total\nCT\nNormal\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class I\nataset \\ Train and Val Ev n Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n954 1239\nWild-type SARS-CoV-2\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage y es Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n450 570\nOmicron and Delta variants of SARS-CoV-2\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron a d Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n538 729\nOther pneumonias\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n447 565\nX-ray\nNormal\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n470 530\nBacterial pneumonia\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\ncterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n283 343\nSARS\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n106 133\nMERS\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nWild-type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n135 184\nWild-type SARS-CoV-2\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nil -type SARS-CoV-2\n205 52 257\nOmicron and Delta variants of SARSCoV-2\n111 50 161\n205 257\nOmicron and Delta variants of SARS-CoV-2\nElectronics 2023, 12, x FOR PEER REVIEW 5 of 21\nTable 1. Details of the CL-COVIDset dataset.\nImage Types Class Images\nDataset \\ Train and Val Evaluation Total\nCT\nNormal\n954 285 1239\nWild-type SARS-CoV-2\n450 120 570\nOmicron and Delta variants of SARSCoV-2\n538 191 729\nOther pneumonias\n447 118 565\nX-Ray\nNormal\n470 60 530\nBacterial pneumonia\n283 60 343\nSARS\n106 27 133\nMERS\n135 49 184\nild-type SARS-CoV-2\n205 52 257\nOmicron a d Delta variants of SARSCoV-2\n111 50 161 11 161\nOther viral pneumonias\nElectronics 2023, 12, x FOR PEER REVIEW 6 of 21\ner viral pneumonias\n240 60 300\nTotal 3939 1072 5011\n3.2. Methods 3.2.1. Methodology of CoroTrans-CL\nFigure 1 illustrates the proposed methodology, CoroTrans-CL, for disease detection. It is divided into three components: data augmentation, the CoroTrans model with a feature extraction backbone and a classification head, and the Continual Learning strategy. The data augmentation module is used to increase the diversity of the input data by applying various transformations to the images, including random rotation, cropping, blurring and noise addition. This helps to improve the generalizability of the model by exposing it to a wider range of data variations. To further increase the randomness of the data, the module applies a random order command to shuffle the order in which these transformations are applied. The resulting augmented dataset is then normalised using the mean and standard deviation. The CoroTrans model is an artificial intelligence system designed to perform disease classification tasks. The architecture of the model consists of two main layers, namely, a feature extraction backbone network layer and a disease classifier layer. The feature extraction backbone network layer is responsible for encoding the input data into a feature representation, which is then used as input for the disease classifier layer.\nFigure 1. The CoroTrans-CL framework: a Continual Learning approach utilizing a Swin Transformer network for disease detection.\nThis layer can be thought of as the core or foundation of the model, providing a basic structure upon which additional functionality can be built, similar to the concept of a \u2018backbone\u2019 in a network. The disease classifier layer, on the other hand, uses the feature representation generated by the feature extraction backbone network layer to perform the actual disease classification. This layer can be implemented using various techniques, such as using a neural network as the classifier, where the neural network can be trained\n240 300\nTotal 3939 1072 5011\nElectronics 2023, 12, 866 6 of 21"
        },
        {
            "heading": "3.2. Methods",
            "text": "3.2.1. Methodology of CoroTrans-CL\nFigure 1 illustrates the proposed methodology, CoroTrans-CL, for disease detection. It is divided into three components: data augmentation, the CoroTrans model with a feature extraction backbone and a classification head, and the Continual Learning strategy.\nElectronics 2023, 12, x FOR PEER REVIEW 6 of 21\nThe data augmentation module is used to increase the diversity of the input data by\napplying various transformations to the images, including random rotation, cropping,\nblurring and noise addition. This helps to improve the generalizability of the model by\nexposing it to a wider range of data variations. To further increase the randomness of the\ndata, the module applies a random order command to shuffle the order in which these\ntransformations are applied. The resulting augmented dataset is then normalised using\nthe mean and standard deviation.\nThe CoroTrans model is an artificial intelligence system designed to perform disease\nclassification tasks. The a chitecture of the model consists of two main layers, namely, a\nfeature extraction backbone network layer and a disease classifier layer. The feature ex-\ntraction backbone network layer is responsible for encoding the input data into a feature\nrepresentation, which is then used as input for the disease classifier layer.\nThis layer can be thought of as the core or foundation of the model, providing a basic\nstructure upon which additional functionality can be built, similar to the concept of a\n\u2018backbone\u2019 in a network. The disease classifier layer, on the other hand, uses the feature\nrepresentation generated by the feature extraction backbone network layer to perform the\nactual disease classification. This layer can be implemented using various techniques,\nsuch as using a neural network as the classifier, where the neural network can be trained\nto classify the input based on the feature representation generated by the feature extrac-\ntion layer.\n\u2022 Feature Extraction Backbone\nThe encoder backbone is a hierarchical structure consisting of four stages. The patch\npartition stage divides the input RGB image into non-overlapping patches, each of which\nis treated as a token. These patches are then processed through multiple Swin Transformer\nblocks [16] that consist of interconnected Window and Shift Window Multi-Head Self-\nAttention (W-MSA and SW-MSA)-based Transformer blocks. These blocks enhance the\ncomputational performance of the Window and Shift Window methods and are governed\nby computational Equations (1) to (4), including the LayerNorm, Window Attention,\nShifted Window Attention, and MLP modules. The Swin Transformer is a novel attention-\nbased transform architecture specifically designed for the efficient processing of image\ni . CoroTrans-CL framework: a Conti ual Learning approach utilizing a Swin Tra sformer network for disease detection.\ne data augm ntation module is used t i crease the diversity of the i put data by applying various transformations to the images, including random rotation, cropping, blurring and noise addition. This helps to improve the generalizability of the model by exposing it to a wider range of data variations. To further increase the randomness of the data, the module applies a random order command to shuffle the order in which these transformations are applied. The resulting augmented dataset is then normalised using the mean and standard deviation. The CoroTrans model is an artificial intelligence system designed to perform disease classification tasks. The architecture of the model consists of two main layers, namely, a feature extraction backbone network layer and a disease classifier layer. The feature extraction backbone network layer is responsible for encoding the input data into a feature representation, which is then used as input for the disease classifier layer. This layer can be thought of as the core or foundation of the model, providing a basic structure upon which additional functionality can be built, similar to the concept of a \u2018backbone\u2019 in a etwork. The disease classifier layer, on t e other hand, uses t feature representation generated by the feature extraction backb ne network layer to perf rm the actual disease classification. This layer can be implemente using vari us techniques, such as using a neural network a the classifie , where the neural network can\ne trained to classify th input based on the feature rep esentation gene at d by the featur extraction layer.\n\u2022 Feature Extraction Backbone The encoder backbone is a hierarchical structure consisting of four stages. The patch partition stage divides the input RGB image into non-overlapping patches, each of which is treated as a token. These patches are then processed through multiple Swin Transformer blocks [16] that consist of interconnected Window and Shift Window Multi-Head Self-\nElectronics 2023, 12, 866 7 of 21\nAttention (W-MSA and SW-MSA)-based Transformer blocks. These blocks enhance the computational performance of the Window and Shift Window methods and are governed by computational Equations (1) to (4), including the LayerNorm, Window Attention, Shifted Window Attention, and MLP modules. The Swin Transformer is a novel attention-based transform architecture specifically designed for the efficient processing of image data. It exploits the local structure of images by partitioning them into patches and only applying self-attention within each patch rather than over the entire image. This allows the Swin Transformer to effectively model the long-range dependencies present in images while maintaining a high degree of computational efficiency. In CoroTrans-CL, the patch size is 4 \u00d7 4 and the feature dimension of each patch is 4 \u00d7 4 \u00d7 3, resulting in patch tokens with a (H/4, W/4, 4 \u00d7 4 \u00d7 channel) shape. The use of the Swin Transformer has been shown to significantly improve the performance of image classification tasks compared with other Transformer architectures. This is due in part to its ability to effectively model local structure and long-range dependencies, as well as its high computational efficiency. The block design of the Swin Transformer, consisting of interconnected Window and Shift Window Multi-Head Self-Attention-based Transformer blocks, has also been shown to be an effective method for improving performance, particularly in the context of image processing.\nx\u0302i = W \u2212MSA ( LN(xi\u22121) ) + xi\u22121, (1)\nxi = MLP ( LN(x\u0302i) ) + x\u0302i, (2)\nx\u0302i+1 = SW \u2212MSA ( LN(xi\u22121) ) + xi, (3)\nxi+1 = MLP ( LN(x\u0302i+1) ) + x\u0302i+1, (4)\nwhere x\u0302i is (S)W-MSA\u2019s output, xi is the output of MLP, and i represents the block\u2019s position. The output shapes of the tokens are (224/8, 224/8, 2C), (224/16, 224/16, 4C), and (224/32, 224/32, 8C) for stages 2, 3, and 4, respectively. The resolution of the output features is 7 \u00d7 7, and the channel has 768 dimensions, as does the output of the encoder stage.\n\u2022 Disease Classification Head The CoroTrans model features a disease classification head that was specifically designed for the identification of various pathologies present in chest X-ray and CT images. The classifier is implemented as a multi-layer perceptron consisting of several linear layers. The input to the classifier is a one-dimensional feature vector of 768 dimensions derived from the image data, which is transformed by the linear layers to predict the target pathology among 11 classes. The final output of the classifier represents the prediction of the model, providing a diagnostic tool for physicians. Continual Learning, also known as Lifelong Learning, refers to the ability of artificial intelligence models to sequentially adapt and learn new tasks without forgetting previously trained tasks [22]. This approach is particularly relevant in addressing the challenges posed by rapidly evolving environments, such as the COVID-19 pandemic, where new information and data are constantly being generated. Continual Learning strategies aim to improve the adaptability of AI models by allowing them to gain new knowledge for new tasks without forgetting previous information. Formally, sequential learning can be defined as the ability of a model to learn individual distributions D1,. . . , Dn, at T1, T2 . . . , moments while being tested on a set containing all distributions. The goal of Continual Learning is to collect data from the new distribution Dn+1 at Tn+1 and to update the model parameters \u03b8 while the model simultaneously adapts to all distributions D1, . . . , Dn+1. In this study, we employed HSR [25] and EWC [27] as a combined COVID-19 Continual Learning strategy (HSR-EWC):\nElectronics 2023, 12, 866 8 of 21\n\u2022 Herding Selection Replay. The classical replay strategy is a widely used technique to address the challenge of forgetting in Continuous Learning scenarios [49,50]. It involves storing and replaying relevant samples from previous training sets as experience, with the aim of improving the adaptability of the model to learn new tasks without forgetting previously acquired knowledge. A specific implementation of the classical replay strategy is the HSR method [25,51], which involves selecting representative examples that are as close as possible to the centre of the feature space. Algorithm 1 describes the representative image selection process of the HSR strategy.\nAlgorithm 1: Herding Selection Replay.\nInput: Image set of CL-COVIDset, set \u03b1 = {\u03b11,\u03b12, . . . ,\u03b1k} of class label y; The number of classes is n; Maximum size of the replay memory buffer; Swin Transformer backbone model S for feature extraction;\nTraining model S to obtain the feature maps \u03b8 and feature function \u03d5 of the extracted CT and X-ray images;\nInitialise L to an empty list. Calculate the mean \u00b5 of the samples in the class by \u03b8; for 1, 2, . . . , m do\nLm \u2190 arg min \u03b1\u2208\u03b1\n\u2016\u00b5\u2212 \u03d5(\u03b1)\u2212\u2211 m\u22121 i=1 \u03d5(Li) n \u2016\nend for L\u2190 (L1, . . . , Lm); Output: Exemplar buffer list L;\nThe HSR method involves up-sampling using the feature extraction backbone during the training phase, followed by calculating the mean of the samples in each class. The distance of each individual sample to the class mean is then calculated and the closest distance ranking is generated. For each class, the top n most representative samples are selected based on this ranking to form a representative subset of samples that are stored in memory. The total number of representative samples in memory (set to N = 200 in this study) is equally divided between the learned classes, with the number of classes being dynamically adjusted according to the learning process. An advantage of the HSR method is that the final sample means are close to the actual class means, allowing the samples to better represent the classes to which they belong [52]. This contrasts with other methods, which can result in sample means that are far from the actual class means, leading to the less effective representation of the classes. The HSR approach has been shown to be effective in improving the Continuous Learning capabilities of artificial intelligence models.\n\u2022 Elastic Weight Consolidation. EWC enables Continual Learning by reducing the plasticity of synapses important to previous tasks [27]. As shown in Figure 2, the parameters (weights and biases) of tasks TA and TB are denoted as \u03b8A and \u03b8B, respectively, and the sets of parameters that reduce the errors for tasks A and B are \u0398\u2217A and \u0398 \u2217 B, respectively. It is possible to find a solution with \u03b8A \u2208 \u0398A and \u03b8B \u2208 \u0398B. Under a Bayesian perspective, if the data are divided into two independent parts, the DA of TA and DB of TB, the posterior distribution p(\u03b8|D) is estimated with (5).\nlog p(\u03b8|D) = log p(DB|\u03b8) + log p(\u03b8|DA)\u2212 log p(DB), (5)\nAs it is not possible to compute the true posterior probability, the EWC assumes that it is a Gaussian distribution with a mean given by the parameter \u03b8\u2217A and the Fisher\nElectronics 2023, 12, 866 9 of 21\ninformation matrix F is used to estimate diagonal precision [53]. The loss function is built in (6).\nl(\u03b8) = lB(\u03b8) + \u2211 i\n\u03bb 2 zi ( \u03b8i \u2212 \u03b8\u2217A, i )2, (6) where lB(\u03b8) is the loss function specific to task TB, \u3009 is the weight vector of the index, \u03b8\u2217A, i denotes the parameter after learning task TA, F denotes the Fisher information matrix, and \u03bb is the parameter which determines the relative importance of the old and new tasks. [54]. When task B arrives, EWC uses parameter \u03b8 close to \u03b8\u2217A in Equation (6), and when a third task Tc arrives, EWC continues to make the parameter \u03b8 close to \u03b8\u2217AB, where \u03b8\u2217A,B is the parameter learned from tasks TA and TB. Extending to all T tasks, the optimization objectives of EWC are given in (7).\n\u03b8\u2217T = argmin \u03b8\n{ \u2212 log p(DT |\u03b8)\u2212\n1 2 \u2211i (\u2211 t<T (\u03bbtzt,i)(\u03b8i \u2212 \u03b8\u2217T\u22121, i) 2\n} (7) Electronics 2023, 12, x FOR PEER REVIEW 9 of 21\nFigure 2. Search space for Elastic Weight Consolidation strategies.\nUnder a Bayesian perspective, if the data are divided into two independent parts, the\n\ud835\udc37\ud835\udc34 of \ud835\udcaf\ud835\udc34 and \ud835\udc37\ud835\udc35 of \ud835\udcaf\ud835\udc35, the posterior distribution \ud835\udc5d(\u03b8|\ud835\udc37) is estimated with (5).\nlog \ud835\udc5d(\u03b8|\ud835\udc37) = log \ud835\udc5d(\ud835\udc37\ud835\udc35|\u03b8) + log \ud835\udc5d(\u03b8|\ud835\udc37\ud835\udc34) \u2212 log \ud835\udc5d(\ud835\udc37\ud835\udc35), (5)\nAs it is not possible to compute the true posterior probability, the EWC assumes that\nit is a Gaussian distribution with a mean given by the parameter \ud835\udf03\ud835\udc34 \u2217 and the Fisher infor-\nmation matrix F is used to estimate diagonal precision [53]. The loss function is built in (6).\n\u2113(\u03b8) = \u2113B(\u03b8) + \u2211 \ud835\udf06\n2 \ud835\udc56\n\u03dc\ud835\udc56(\u03b8\ud835\udc56 \u2212 \u03b8\ud835\udc34,\ud835\udc56 \u2217 )2, (6)\nwhere \u2113B(\u03b8) is the loss function specific to task \ud835\udcaf\ud835\udc35, \ud835\udcbe is the weight vector of the index, \ud835\udf03\ud835\udc34,\ud835\udc56 \u2217 denotes the parameter after learning task \ud835\udcaf\ud835\udc34, F denotes the Fisher information matrix, and\n\u03bb is the parameter which determines the relative importance of the old and new tasks. [54].\nWhen task B arrives, EWC uses parameter \ud835\udf03 close to \ud835\udf03\ud835\udc34 \u2217 in Equation (6), and when\na third task \ud835\udcaf\ud835\udc50 arrives, EWC continues to make the parameter \ud835\udf03 close to \ud835\udf03\ud835\udc34\ud835\udc35 \u2217 , where \ud835\udf03\ud835\udc34,\ud835\udc35 \u2217\nis the parameter learned from tasks \ud835\udcaf\ud835\udc34 and \ud835\udcaf\ud835\udc35. Extending to all \ud835\udcaf tasks, the optimization\nobjectives of EWC are given in (7).\n\u03b8T \u2217 = argmin\n\u03b8 {\u2212log \ud835\udc5d(\ud835\udc37\ud835\udcaf|\u03b8) \u2212\n1 2 \u2211(\u2211(\ud835\udf06t\u03dc\ud835\udc61,\ud835\udc56\n\ud835\udc61<\ud835\udc47\n)\n\ud835\udc56\n(\u03b8\ud835\udc56 \u2212 \u03b8\ud835\udc47\u22121,\ud835\udc56 \u2217 )2} (7)\n3.2.2. Performance Evaluation Metrics\nIn this study, we used a number of Continual Learning evaluation metrics to assess\nthe performance of the models. One such metric was average accuracy [55], which\nmeasures the average accuracy of a model after class-incremental training for the first task\nup to T. This metric is calculated with formula (8). Average accuracy is a widely used\nmetric in the Continual Learning literature, as it provides a comprehensive view of a\nmodel\u2019s performance across all tasks. Unlike other metrics, such as per-task accuracy,\nwhich only consider performance on individual tasks, average accuracy takes a model\u2019s\nperformance on all tasks into account, providing a more comprehensive assessment of a\nmodel\u2019s ability to continuously learn new tasks without forgetting previously acquired\nknowledge.\n\ud835\udc34\ud835\udc63\ud835\udc52\ud835\udc5f\ud835\udc4e\ud835\udc54\ud835\udc52 \ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66 = 1\n\ud835\udcaf \u2211 \ud835\udc5d\ud835\udcaf,\ud835\udc56\n\ud835\udcaf\n\ud835\udc56=1\n, (8)\n3.2.2. Performance Evaluation Metrics\nIn this study, we used a number of Continual Learning evaluation metrics to assess the performance of the models. One such metric was average accuracy [55], which measures the average accuracy of a model after class-incremental training for the first task up to T. This metric is calculated with formula (8). Average accuracy is a widely used metric in the Continual Learning literature, as it provides a comprehensive view of a model\u2019s performance across all tasks. Unlike other metrics, such as per-task accuracy, which only consider performance on individual tasks, average accuracy takes a model\u2019s performance on all tasks into account, providing a more comprehensive assessment of a model\u2019s ability to continuously learn new tasks without forgetting previously acquired knowledge.\nAverage Accuracy = 1 T T \u2211 i=1 pT , i, (8)\nElectronics 2023, 12, 866 10 of 21\nwhere pj, i is the Top1 accuracy of the model on the held-out test set of task Ti after the model is trained on task Tj. The precision, sensitivity and F1 score are defined as:\nPrecision = True Positives\nTrue Positives + False Positive , (9)\nSensitivity = True Positives\nTrue Positives + False Negative (10)\nF1\u2212 score = 2\u00d7 (Sensitivity\u00d7 Precision) Sensitivity + Precision\n(11)\nTo assess the overall performance of the models in this study, the average of each metric was calculated using the MACRO method [39]. The confusion matrix is a valuable tool for analysing the error types of a model, as it provides a breakdown of the number of true positive, true negative, false positive and false negative predictions made by a model. This information is useful for identifying patterns in a model\u2019s errors and for developing strategies to improve a model\u2019s performance.\n3.2.3. Experimental Setup\nIn this study, we compared the performance of several state-of-the-art deep learning models as the backbones for CT and chest X-ray image classifiers. The models used for comparison included an all-MLP Architecture for Vision (MLP-Mixer) B/16 [56], a multilayer perceptron model with a mixer block architecture (ResNet-50) [31], a widely used convolutional neural network (CNN) model with a residual architecture (Efficientnetb4) [36], a CNN model with an efficient architecture designed to improve performance while reducing the number of parameters and computational complexity (ViT-S/16) [12], a vision Transformer model with a small patch size and BERT Pre-Training of Image Transformers (BeiT) v2 [57], and a hybrid Transformer model that combines the strengths of both CNNs and Transformers. In addition to these models, we also included our own model for comparison. To evaluate the classification ability of our proposed model (CoroTrans) and the other models, we performed a joint learning experiment. The experiment involved simultaneously training all models on all classes of images. In this setup, each model is trained for a certain number of iterations, called epochs. In our experiment, the models were trained for 30 epochs. The training process for the models involved updating their parameters to minimise the difference between the predicted output and the actual output. This process as conducted using an optimisation algorithm. In our experiment, we used the Adaptive Moment Estimation Decoupling Weight Decay (AdamW) optimiser. The Adam optimiser [58] is a popular optimisation algorithm that is widely used in deep learning and is particularly well-suited to training large neural networks. It is a combination of two other optimisation techniques, namely, the Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). The Adam optimiser has several advantages over other optimisation techniques, including the ability to adaptively adjust the learning rate on a per-parameter basis. This means that the optimiser can adjust the learning rate for different parameters based on the past gradients of the parameters, resulting in faster convergence. In addition, Adam has a momentum term that helps to smooth the gradients, allowing the optimiser to converge faster and more stably. AdamW is a modified version of the Adam stochastic optimisation algorithm that improves upon the traditional implementation of weight decay. This is achieved by decoupling the weight decay calculation from the gradient update operations, allowing the weights to be updated in a more effective and efficient manner. The learning rate is another important hyperparameter in the training process. It controls the step size at which the optimiser updates the model parameters. A high learning rate causes the model to make rapid updates to the parameters and the training process can quickly converge, though with the risk of overshooting the optimal solution, while a low learning rate causes the model to make smaller updates, leading to\nElectronics 2023, 12, 866 11 of 21\nslower convergence but with less risk of overshooting. In our experiment, we used an initial learning rate of 3 \u00d7 10\u22125. To further assess the performance of each model, we used a 5-fold cross-validation strategy to derive the average scoring accuracy of each model on the validation set, providing a robust assessment of their performance. We then conducted a series of class incremental learning experiments, as shown in Figure 3, in which the models were trained on successive sets of images corresponding to different types of coronavirus infection. To this end, we conducted experiments on five levels of Continuous Learning that accurately reflected the epidemic timeline of different coronaviruses (namely, SARS, MERS, wild-type SARS-CoV-2, and the Omicron and Delta variants of SARS-CoV-2) in the real world. The results of these experiments were compared with the baseline results obtained by training the models on all previous classes in addition to the new class each time (the upper bound). The aim of these experiments was to assess the ability of the models to adapt to new information without forgetting previously learned knowledge.\nElectronics 2023, 12, x FOR PEER REVIEW 11 of 21\nthe optimal solution, while a low learning rate causes the model to make smaller updates,\nleading to slower convergence but with less risk of overshooting. In our experiment, we\nused an initial learning rate of 3 \u00d7 10\u22125. To further assess the performance of each model,\nwe used a 5-fold cross-validation strategy to derive the average scoring accuracy of each\nmodel on the validation set, providing a robust assessment of their performance.\nWe then conducted a series of class incremental learning experiments, as shown in\nFigure 3, in which the models were trained on successive sets of images corresponding to\ndifferent types of coronavirus infection. To this end, we conducted experiments on five\nlevels of Continuous Learning that accurately reflected the epidemic timeline of different\ncoronaviruses (namely, SARS, MERS, wild-type SARS-CoV-2, and the Omicron and Delta\nvariants of SARS-CoV-2) in the real world. The results of these experiments were com-\npared with the baseline results obtained by training the models on all previous classes in\naddition to the new class each time (the upper bound). The aim of these experiments was\nto assess the ability of the models to adapt to new information without forgetting previ-\nously learned knowledge.\nFigure 3. Using AI to simulate the discovery of coronaviruses: a contrast between joint and classincremental learning experiments: (a) joint learning; (b) class-incremental learning experiment in which trials were conducted in five successive learning phases reflecting the actual epidemic timeline of different coronaviruses (namely, SARS, MERS, wild-type SARS-CoV-2, and the Omicron and Delta variants of SARS-CoV-2) in the real world.\nFinally, we compared several state-of-the-art Continual Learning methods for their\nperformance in adapting to new information without forgetting previously learned\nknowledge. The methods we considered included Gradient Episodic Memory (GEM) [59],\na method that stores past gradients and adjusts the learning rate for each example based\non the distance between the current gradient and the past gradients; GDumb [24], a\nmethod that stores a fixed number of examples from each task and performs gradient\ndescent on these examples at the beginning of each task; Average Gradient Episodic\nMemory (AGEM) [60], a method that stores and averages previous slopes to provide a\ncontinuous representation of the understanding acquired in previous tasks\u2014this repre-\nsentation is then used to streamline the learning process for the current task, ensuring that\nthe network retains essential data from previous tasks while avoiding harmful interfer-\nence; CopyWeights with Re-init Star (CWRStar) [61], a rehearsal-free Continuous Learn-\ning approach in deep learning, which is a notable way of dealing with forgetting in the\nSingle-Incremental Task\u2013New Classes situation that involves the implementation of a\ndouble memory in the fully connected layer; Cumulative (the upper bound), a method\nthat stores all past examples and trains on them in addition to the current task; and Ran-\ndom Replay, a method that randomly samples past examples to train on at the beginning\nof each task. These methods represent three different strategies for Continuous Learning:\ngradient-based, example-based, and random sampling. We compared the performance of\nFigure 3. Using AI to simulate the discovery of coronaviruses: a contrast between joint and classincre ental learning experi ents: (a) joint learning; (b) class-incre ental learning experi ent in i t i l re conducted in five suc es ive learning phases reflecting the actual epidemic timeline of dif erent coronaviruses (namely, SARS, MERS, wild-type SARS-CoV 2, and the Omicron and Delta variants of SARS-CoV-2) in the real world.\n, e compared several sta e-of-the-art Conti ual Learning methods for their pe - formance in adapting to new information with ut forgetting previously learned knowledge. The m thods w considered included Gradient Episodic Memory (GE ) [59], a method that stor s past gradient and adjusts the learning rate for each example based on the dist nce between the current gradient and the past gradie ts; GDumb [24], a method that stores fixed number of examples from each task and erforms gradient descent on these examples at the beginning of each task; Average Gradient Episodic Memory (AGEM) [60], a method that stores and averages previous slopes to provide a continuous representation of the understanding acquired in previous tasks\u2014this representation is then used to streamline the learning process for the current task, ensuring that the network retains essential data from previous tasks while avoiding harmful interference; CopyWeights with Re-init Star (CWRStar) [61], a rehearsal-free Continuous Learning approach in deep learning, which is a notable way of dealing with forgetting in the Single-Incremental Task\u2013New Classes situation that involves the implementation of a double memory in the fully connected layer; Cumulative (the upper bound), a method that stores all past examples and trains on them in addition to the current task; and Random Replay, a method that randomly samples past examples to train on at the beginning of each task. These methods represent three different strategies for Continuous Learning: gradient-based, example-based, and random sampling. We compared the performance of these methods using a five-step incremental learning task involving the sequential learning of different coronavirus classes. The capacity\nElectronics 2023, 12, 866 12 of 21\nhyperparameter of the buffer for all replay methods was uniformly set to store a maximum of 200 samples. The experiments were conducted using the Tesla A100, V100 and P100 GPU graphics cards, and they were implemented using PyTorch (https://pytorch.org/ (accessed on 31 January 2023)), the PyTorch image model library (https://github.com/fastai/timmdocs/ (accessed on 31 January 2023)), and the PyTorch Continuous Learning framework (https:// avalanche.continualai.org/ (accessed on 30 January 2023)) [52]. The use of GPU technology has become increasingly common in deep learning due to its ability to increase the efficiency of the training and inference process through parallel processing. PyTorch, a widely used open-source deep learning framework, provides a high-level interface for training and deploying deep learning models. The timmdocs library developed by fastai is a suite of PyTorch utilities and callbacks for training image classification models. In addition, the avalanche framework is a PyTorch-based toolkit for the design and evaluation of Continuous Learning algorithms."
        },
        {
            "heading": "4. Results",
            "text": "4.1. Joint Training Results 4.1.1. Accuracy Results\nThe results presented in Table 2 show the superiority of our approach, CoroTrans, a Swing Transformer Network-based model, over the other evaluated models in terms of precision, recall, F1 score and accuracy as performance metrics in joint training. Our approach, CoroTrans, showed exceptional performance with a precision of 97.18%; in addition, our approach achieved an accuracy of 95.34%, further highlighting the effectiveness of CoroTrans in identifying coronavirus-infected respiratory diseases in medical images in joint training. Notably, the performance gain of CoroTrans over other models was significant, with average improvements of 20% in precision, 20% in recall, 20% in F1 score and 16% in accuracy over other models. These results illustrate the potential of our proposed approach to provide a comprehensive and accurate prediction model for Coronavirus-infected respiratory disease.\n4.1.2. Various Diseases Classification Results\nThe results, as shown in Figure 4 and Table 3, demonstrate the robustness and validity of our proposed model, CoroTrans, for the classification of different diseases. The F1 scores for all diseases were higher than 92%, highlighting the model\u2019s ability to accurately discriminate between different disease classes. In particular, CoroTrans showed a superior ability to detect wild-type SARS-CoV-2 and other viral pneumonias in both CT and Xray images. The model achieved F1 scores of 98.32% for CT images and 100% for X-rays for the identification of wild-type SARS-CoV-2, a significant improvement over the other compared models. In addition, the model achieved high F1 scores of 98.73% and 100% for CT and X-ray images, respectively, for the identification of other viral pneumonias. Furthermore, the model\u2019s performance in identifying the Omicron and Delta variants of SARS-CoV-2 (with F1 scores of 89.06% and 96.97% respectively), while not as high as its performance in\nElectronics 2023, 12, 866 13 of 21\nother disease classes, demonstrates its ability to accurately identify these variants, which are known to be more difficult to diagnose.\nElectronics 2023, 12, x FOR PEER REVIEW 13 of 21 and X-ray images, respectively, for the identification of other viral pneumonias. Furthermore, the model\u2019s performance in identifying the Omicron and Delta variants of SARSCoV-2 (with F1 scores of 89.06% and 96.97% respectively), while not as high as its performance in other disease classes, demonstrates its ability to accurately identify these variants, which are known to be more difficult to diagnose. Table 3. Results of CoroTrans classification for different diseases. Image Types Class Evaluation Metrics Precision Sensitivity F1 Score CT Normal 0.9261 0.9228 0.9244 Wild-type SARS-CoV-2 0.9915 0.9750 0.9832 Omicron and Delta Variants of SARS-CoV-2 0.8860 0.8953 0.8906\nOther Pneumonias 0.9832 0.9915 0.9873\nX-Ray\nNormal 0.9677 1.0000 0.9836\nWild-type SARS-CoV-2 1.0000 1.0000 1.0000\nOmicron and Delta Variants of SARS-CoV-2 0.9796 0.9600 0.9697\nSARS 0.9630 0.9630 0.9630\nMERS 0.9600 0.9796 0.9697\nOther Viral Pneumonias 1.0000 1.0000 1.0000\nBacterial Pneumonia 1.0000 1.0000 1.0000\n(a) (b) (c)\n(d) (e) (f)\nFigure 4. Confusion matrix illustrating the classification results of the models: (a) MLP-Mixer B/16; (b) Efficientnet-b4; (c) ResNet-50; (d) ViT-B/16; (e) BeiT-v2; (f) CoroTrans (ours).\n4.1.3. Comparison of Model Feature Extraction\nFigure 4. Confusion matrix illustrating the classification results of the models: (a) MLP-Mixer B/16; (b) Efficientnet-b4; (c) ResNet-50; (d) ViT-B/16; (e) BeiT-v2; (f) CoroTrans (ours).\n4.1.3. Comparison of Model Feature Extraction\nt-Distributed Stochastic Neighbour Embedding (t-SNE) [62] is a dimensionality reduction and visualization technique for high-dimensional data that is particularly useful for visualizing the structure of complex datasets and has been widely used in the field of machine learning to visualise the representations learned by deep neural networks. In our study, we used t-SNE to visualise the disease features learned by our proposed model, CoroTrans, as well as other models based on three major deep learning architectures: CNNs, ViT, and MLP. We extracted the output of the last layer of the feature extractor in each model to obtain a multidimensional feature vector and projected it into a two-dimensional space using t-SNE. The resulting scatter plots, shown in Figure 5, indicate that the feature distributions obtained by the MLP-Mixer B/16, EfficientNet-b4 and ResNet-50 models did not result in a clear boundary between different disease classes.\nElectronics 2023, 12, 866 14 of 21\nElectronics 2023, 12, x FOR PEER REVIEW 14 of 21 We extracted the output of the last layer of the feature extractor in each model to obtain a multidimensional feature vector and projected it into a two-dimensional space using t-SNE. The resulting scatter plots, shown in Figure 5, indicate that the feature distributions obtained by the MLP-Mixer B/16, EfficientNet-b4 and ResNet-50 models did\nnot result in a clear boundary between different disease classes.\nIn contrast, the feature distributions obtained by the ViT-B/16, BeiT-v2 and Coro-\nTrans models had clear boundaries between different classes of features, with CoroTrans\nshowing the clearest separation of each class compared with other techniques. These re-\nsults indicate that our model effectively captured the relevant features of the images, lead-\ning to improved classification performance.\n(a) (b) (c)\n(d) (e) (f)\nFigure 5. Results of t-SNE visualization for high-dimensional data clustering: (a) MLP-Mixer B/16; (b) Efficientnet-b4; (c) ResNet-50; (d) ViT-B/16; (e) BeiT-v2; (f) CoroTrans (ours)."
        },
        {
            "heading": "4.2. Continual Learning Results",
            "text": ""
        },
        {
            "heading": "4.2.1. Average Accuracy and Confusion Matrix Results",
            "text": "The results presented in Table 4 demonstrate the effectiveness of our proposed\nmethod, CoroTrans-CL, in mitigating catastrophic forgetting in Continuous Learning\ntasks. Using the HSR-EWC strategy, our method achieved superior performance com-\npared with other evaluated strategies such as AGEM, CWRStar, GEM, GDumb and Ran-\ndom Replay. In particular, when applied to the Swin Transformer backbone, our method\nachieved an accuracy of 83.40%, while the next best strategy, Random Replay, achieved\nan accuracy of 66.04%. This result indicated a significant improvement in performance,\naround 30% better than other models. In addition, it is worth noting that the average\nFigure 5. Results of t-SNE visualization for high-dimensional data clustering: (a) MLP-Mixer B/16; (b) Efficientnet-b4; (c) ResNet-50; (d) ViT-B/16; (e) BeiT-v2; (f) CoroTrans (ours).\nIn contrast, the feature distributions obtained by the ViT-B/16, BeiT-v2 and CoroTrans models had clear boundaries between different clas es of features, with CoroTrans showing the clearest separation of each class compared with other techniques. These results indicate that our model effectively captured the relevant features of the images, leading to improved classification performance."
        },
        {
            "heading": "4.2. Continual Learning Results",
            "text": "4.2.1. Average Accuracy and Confusion Matrix Results\nThe results presented in Table 4 demonstrate the effectiveness of our proposed method, CoroTrans-CL, in mitigating catastrophic forgetting in Continuous Learning tasks. Using the HSR-EWC strategy, our method achieved superior performance compared with other evaluated strategies such as AGEM, CWRStar, GEM, GDumb and Random Replay. In particular, when applied to the Swin Transformer backbone, our method achieved an accuracy of 83.40%, while the next best strategy, Random Replay, achieved an accuracy of 66.04%. This result indicated a significant improvement in performance, around 30% better than other models. In addition, it is worth noting that the average accuracy of our method was the closest to the upper performance limit of the Cumulative method of all evaluated methods. These results highlight the effectiveness of our proposed approach in dealing with catastrophic forgetting and maintaining performance in Continuous Learning tasks.\nElectronics 2023, 12, 866 15 of 21\nTable 4. A comparison of the average accuracy of different strategies.\nArchitecture\nBaseline Evaluation Strategies\nCumulative (the Upper Bound) AGEM CWRStar GEM GDumb\nRandom Replay\nHSR-EWC (Our CL Strategy)\nResNet-50 0.4776 0.2323 0.3209 0.2155 0.3256 0.3269 0.2323 Efficientnet-b4 0.4813 0.2668 0.3218 0.2780 0.0914 0.2351 0.3461\nBeiT-v2 0.8834 0.2267 0.3256 0.4646 0.6772 0.3414 0.5718 MLP-Mixer B/16 0.8657 0.3461 0.3358 0.3563 0.4039 0.5765 0.6604\nViT-B/16 0.8983 0.2257 0.3619 0.7453 0.6922 0.653 0.7724 CoroTrans (Our Model) 0.9375 0.2304 0.3479 0.4403 0.5373 0.694 0.8340\n4.2.2. Incremental Learning Processes Results\nThe results of our study, as depicted in Figure 6, illustrate the incremental learning process of the CoroTrans-CL across several Continual Learning strategies. As seen in the figure, other methods such as AGEM, CWRStar, GEM, GDumb, and Random Replay experienced sharp declines in average accuracy, indicating catastrophic forgetting. This is particularly problematic in the context of a medical deep learning model, as it renders the model ineffective in adapting to new, unseen classes of data. On the other hand, our proposed CoroTrans-CL approach, the Swin Transformer backbone combined with the HSR strategy, exhibited a slow degradation of performance, similar to the way the human brain learns. This slow degradation in accuracy maintained the model\u2019s ability to recognise new, unseen classes of data without compromising performance on previous classes. This ability to continuously learn and adapt to new virus strains without forgetting previous strains is critical in the fight against coronavirus mutations and pandemics, as it allows for real-time adaptation to new strains while maintaining performance on previous strains.\nElectronics 2023, 12, x FOR PEER REVIEW 15 of 21 accuracy of our method was the closest to the upper performance limit of the Cumulative\nmethod of all evaluated methods. These results highlight the effectiveness of our pro-\nposed approach in dealing with catastrophic forgetting and maintaining performance in\nContinuous Learning tasks.\nTable 4. A comparison of the average accuracy of different strategies.\nArchitecture\nBaseline Evaluation Strategies\nCumulative\n(the Upper Bound)\nAGEM CWRStar GEM GDumb\nRandom\nReplay\nHSR-EWC\n(Our CL Strategy)\nResNet-50 0.4776 0.2323 0.3209 0.2155 0.3256 0.3269 0.2323\nEfficientnet-b4 0.4813 0.2668 0.3218 0.2780 0.0914 0.2351 0.3461\nBeiT-v2 0.8834 0.2267 0.3256 0.4646 0.6772 0.3414 0.5718\nMLP-Mixer B/16 0.8657 0.3461 0.3358 0.3563 0.4039 0.5765 0.6604\nViT-B/16 0.8983 0.2257 0.3619 0.7453 0.6922 0.653 0.7724\nCoroTrans\n(Our Model) 0.9375 0.2304 0.3479 0.4403 0.5373 0.694 0.8340\n4.2.2. Incremental Learning Processes Results\nThe results of our study, as depicted in Figure 6, illustrate the incremental learning\nprocess of the CoroTrans-CL across several Continual Learning strategies. As seen in the\nfigure, other methods uch as AGEM, CWRStar, GEM, GDumb, and Random Replay ex-\nperienced sharp declines in average accuracy, indicating catast ophic forgetting. This is\nparticularly probl matic in the context of a medical deep le rni g model, s it renders th\nmodel ineffective in adapting to new, unseen classes of data. On the other hand, our pro-\nposed CoroTrans-CL approach, the Swin Transformer backbone combined with the HSR\nstrategy, exhibited a slow degradation of performance, similar to the way the human brain\nlearns. This slow degradation in accuracy maintained the model\u2019s ability to recognise\nnew, unseen classes of data without compromising performance on previous classes. This\nability to continuously learn and dapt to new virus strains without forgetting previous\nstrains is critical in the figh against coronavirus m tations a d pandemics, as it allows for\nreal-time adaptation to new strains while maintaini g performance on previous strains.\nFigure 6. Accuracy on CoroTrans-learnt classes with different strategies.\nFigure 6. Accuracy on CoroTrans-learnt classes with different strategies.\n4.2.3. Ablation Experiments\nIn our study, we conducted ablation experiments to evaluate the contribution of each component of our proposed method, which combined EWC and HSR to mitigate catastrophic forgetting. Specifically, we compared the performance of three different combinations of methods:\nHybrid 1, where the model was trained using only EWC.\nElectronics 2023, 12, 866 16 of 21\nHybrid 2, where the model was trained using EWC and Replay. Our method, where the model was trained using EWC, Replay, and Herding Selection. These ablation experiments were conducted to gain insight into the individual contri-\nbutions of each component of our proposed method and to determine which combination of methods resulted in the most effective performance. The results presented in Table 5 are essential for understanding the architecture of the model. Understanding the precise impact of each component of the proposed method on incremental learning performance is crucial for making informed decisions about the structure and design of the model.\nHybrid 1, using only EWC, was able to maintain the weights of the previously learned classes. However, as the gap between the new and old classes increased, so did the confusion between the old and new classes, leading to a significant decrease in the final incremental average accuracy. Hybrid 2, which combined EWC and Replay, showed improved performance over Hybrid 1, as the increased storage of example samples helped to reduce confusion between old and new classes. However, as the selection of examples was random, the samples lacked feature representativeness and recognition accuracy was not high after several stages of incremental training. Our method, which included the addition of the Herding Selection method, improved on Hybrid 2 by using a Herding Selection algorithm to select representative samples near the mean feature after averaging the features extracted by the backbone network. This resulted in a limited number of subsamples that effectively represented the entire sample and led to significantly improved performance and slow forgetting during incremental learning, resulting in the highest final incremental average accuracy.\n4.2.4. Comparison of Different Sample Selection Strategies\nTo analyse the effects of different exemplar selection strategies on incremental learning performance, we compared three different exemplar selection strategies. Hybrid 1: Random Exemplar Selection strategy, which randomly selected the exemplars in the dataset. Hybrid 2: Closest to Centre strategy, which is a greedy algorithm that selected the remaining exemplar that is closest to the centre of the feature space based on the already selected elements. Our method: Herding Selection strategy, which selected the remaining exemplar that brought the centre of the already selected exemplars as close as possible to the overall centre of the feature space by iteratively adjusting the selection criteria. The results of our experiments, shown in Figure 7, show that the Herding Selection strategy was the most effective method for selecting exemplars in incremental learning tasks. The strategy, which is based on a Herd Selection algorithm, minimised the distance between the feature centres of the selected exemplars and the overall feature centres of the sample. This resulted in a better representation of the replayed samples, which consequently led to a higher average accuracy in the final increment. In contrast, the Random Exemplar Selection strategy, which used stochastic sampling to retain information from old classes, suffered from a lack of representativeness as the gap between the random sub-sample space and the total space gradually increased with increment. The Closest to Centre strategy performed well in the initial stages, but as the incremental training stage gradually increased, the representativeness of the samples selected by the algorithm decreased and recognition accuracy accordingly decreased.\nElectronics 2023, 12, 866 17 of 21\nElectronics 2023, 12, x FOR PEER REVIEW 17 of 21 consequently led to a higher average accuracy in the final increment. In contrast, the Ran-\ndom Exemplar Selection strategy, which used stochastic sampling to retain information\nfrom old classes, suffered from a lack of representativeness as the gap between the ran-\ndom sub-sample space and the total space gradually increased with increment. The Clos-\nest to Centre strategy performed well in the initial stages, but as the incremental training\nstage gradually increased, the representativeness of the samples selected by the algorithm\ndecreased and recognition accuracy accordingly decreased.\nOur proposed Continuous Learning approach, CoroTrans-CL, based on the Swin\nTransformer architecture, was developed for the diagnosis and prevention of coronavirus\ninfection using chest radiography images. During the joint training phase, we employed\na robust evaluation strategy by using 5-fold cross-validation, a widely accepted method\nfor evaluating the performance of machine learning models. In this approach, the training\ndata were divided into five equally sized subsets called \u2018folds\u2019, and the model was trained\non four of these folds, with the remaining fold used as a validation set. This process was\nrepeated five times, with each fold being used as a validation set only once. The advantage\nof this method, especially for smaller datasets, is that it allows for a more comprehensive\nevaluation of a model\u2019s performance. The performance of our proposed model CoroTrans\nwas validated by a 5-fold cross-validation on the evaluation set, achieving an accuracy of\n95.34% in the conclusive results. This outstanding result demonstrates the superior feature\nextraction capability of our model.\nThe results presented in Table 2 show that the CoroTrans-CL model had high preci-\nsion and recall values, with a precision of 97.18%, a recall of 97.16%, and an F1 score of\n97.16%, indicating low numbers of both false positive and false negative predictions.\nThese results demonstrate the model\u2019s ability to accurately identify infected images while\nminimizing the number of uninfected images misclassified as infected. In addition, Table 3\nillustrates the performance of the CoroTrans model for different diseases, such as normal\nscans, wild-type SARS-CoV-2, the Omicron and Delta variants of SARS-CoV-2, and other\npneumonia infections. The F1 scores for all diseases were greater than 92%, indicating the\nability of the model to accurately discriminate between different disease classes and to\nminimise the number of false positive and false negative predictions. It is worth noting\nthat the high performance of the CoroTrans-CL model in terms of precision, recall and\naccuracy suggests that it is capable of effectively identifying coronavirus-infected images\nwhile minimizing the number of false positive and false negative predictions. Our"
        },
        {
            "heading": "5. Discussion",
            "text": "Our proposed Continuous Learning approach, CoroTrans-CL, based on the Swin Transformer architecture, was developed for the diagnosis and prevention of coronavirus infection using chest radiography images. During the joint training phase, we employed a robust evaluation strategy by using 5-fold cross-validation, a widely accepted method for evaluating the performance of machine learning models. In this approach, the training dat were divided int five equally sized subsets called \u2018folds\u2019, a d the model was rained on four of these folds, with the rema ning fold u ed as a vali ation set. This process was repeated five times, with each fold being used as a validation set only once. The advantage of this method, especially for smaller datasets, is that it allows for a more comprehensive evaluation of a model\u2019s performance. The performance of our proposed model CoroTrans was validated by a 5-fold cross-validation on the evaluation set, achieving an accuracy of 95.34% in the conclusive results. This outstanding result demonstrates the superior feature extraction capability of our model. The results presented in Table 2 show that the CoroTrans-CL model had high precision and recall values, with a precision of 97.18%, a recall of 97.16%, and an F1 score of 97.16%, indicating low numbers of both fa se positive and false negative predictions. T ese results demonstrate the model\u2019s ability to accurately identify infected images while minimizing the number of uninfected images misclassified as infected. In addition, Table 3 illustrates the performance of the CoroTrans model for different diseases, such as normal scans, wildtype SARS-CoV-2, the Omicron and Delta variants of SARS-CoV-2, and other pneumonia infections. The F1 scores for all diseases were greater than 92%, indicating the ability of the model to accurately discriminate between different disease classes and to minimise the number of false positive and false negative predictions. It is worth noting that the high perform nce of the CoroTrans-CL model in terms of precision, call and accuracy suggests that it is capable of effectively identifying coronavirus-infected images while minimizing the number of false positive and false negative predictions. Our confusion matrix results further demonstrate the robustness and validity of CoroTrans for the classification of various diseases. The utilization of the ViT architecture in medical image analysis has been found to be particularly advantageous in this task, as it allows for the processing of input images of arbitrary resolution. The self-attention mechanism enables the model to focus on the most informative regions of the input images, leading to better performance. Additionally,\nElectronics 2023, 12, 866 18 of 21\nthe Transformer architecture allows for the utilization of a vast number of parameters, leading to improved representation capability. A comparison of the ability of model feature extraction (t-sne) indicated that our model outperformed the comparative CNNs and MLP model architectures in terms of feature extraction, as evidenced by the t-SNE visualization. This emphasises the significance of using Swin Transformer as a feature extraction backbone in our model, as its hierarchical architecture enables the extraction of features at multiple scales, thus leading to improved performance in medical image recognition tasks. These factors collectively contributed to the superior performance of our model CoroTrans in identifying images of different coronavirus infections. The results of our proposed method in the context of Continual Learning, as presented in Section 4.2, reveal its superior performance compared with other comparative approaches. In particular, the mean accuracy of 83.40% achieved by our proposed method in the Continual Learning setting highlights its ability to effectively adapt to new data and task variations while maintaining its performance on previously learned tasks. In addition, to gain insight into the influence of image resolution on the precision of our proposed approach, we evaluated the classification results of Continuous Learning using a standard resolution image (224 \u00d7 224) and an enhanced resolution image obtained by adapting it to a larger resolution image (384 \u00d7 384). As expected, the higher resolution resulted in a further increase in the overall accuracy of the model recognition, reaching an overall accuracy of 84.70% after five stages of Continuous Learning, an increase of 1.3 percentage points compared with the mean accuracy for the standard resolutions. Rehearsal-based methods, such as our proposed HSR strategy, have been found to be more effective in addressing the issue of catastrophic forgetting as they actively store and replay examples from previous tasks, allowing models to retain previous knowledge. In contrast, regularization-based methods, such as EWC, primarily focus on constraining a model\u2019s parameters to prevent excessive changes but do not actively store previous knowledge. The Random Replay strategy, while being a rehearsal-based approach, still suffers from severe forgetting due to the random selection of examples that may not be representative of the previous tasks. Our HSR approach, on the other hand, utilises a Herding Selection algorithm, which selects a small representative sample of examples based on their similarity to the previous tasks, thus allowing the model to effectively retain previous knowledge and minimise forgetting during the learning process. Furthermore, the results of the incremental learning processes presented in Section 4.2.2 further support the effectiveness of our proposed method in addressing the issue of catastrophic forgetting. The gradual decline in performance observed in our proposed method, in contrast to the severe forgetting exhibited by other comparative methods, aligns with the process of memory learning in the human brain and helps to maintain a balance of plasticity and stability. The results of our ablation experiments and comparison of different sample selection strategies demonstrate the effectiveness of our proposed method, which combines EWC, Replay, and Herding Selection to mitigate catastrophic forgetting in incremental learning tasks. Our study shows that a hybrid approach using multiple strategies is more effective in addressing the problem of catastrophic forgetting than relying on a single strategy alone. In particular, our proposed method improved on the performance of other studied methods by using a Herding Selection algorithm to select representative samples that effectively represent the entire sample. This led to significantly improved performance and slow forgetting during incremental learning, resulting in the highest final average incremental accuracy. Furthermore, the comparison of different sample selection strategies showed that HSR was a more effective method for selecting exemplars in incremental learning tasks, as it minimised the distance between the feature centres of the selected exemplars and the feature centres of the whole sample. The results of our study provide valuable insights into the design and structure of incremental learning models and demonstrate the potential of our proposed method to address the challenge of catastrophic forgetting in such models.\nElectronics 2023, 12, 866 19 of 21"
        },
        {
            "heading": "6. Conclusions",
            "text": "In this paper, we propose a novel approach to detect lung CT and X-ray images of different coronaviruses that cause major respiratory diseases, such as SARS, MERS, wild-type SARS-CoV-2, and the Omicron and Delta variants of SARS-CoV-2, using a Transformer-based deep learning model. We combined regularization-based and rehearsalbased methods to address the challenge of Continuous Learning. Our approach achieved impressive performance, with a joint training accuracy of 0.9534, an F1 score of over 92%, and an average accuracy of 83.40% in the Continuous Learning environment. The proposed approach is a promising solution to address the challenges posed by continuously mutating viruses. In future work, we plan to further investigate the segmentation and lesion detection tasks in the CT and X-ray imaging of coronavirus-infected lungs based on the Continuous Learning methods proposed in this paper.\nAuthor Contributions: Conceptualization, B.W. and D.Z.; methodology, B.W. and Z.T.; software, B.W.; validation, Z.T.; resources, B.W.; writing\u2014original draft preparation, B.W.; writing\u2014review and editing, D.Z. and Z.T.; visualization, B.W.; supervision, D.Z.; project administration, B.W.; funding acquisition, D.Z. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was partially funded by the Science and Technology Development Fund, Macao SAR, under the Macao Funding Scheme for Key R&D Projects, grant number 0025/2019/AKP.\nData Availability Statement: The datasets utilised during the present investigation are accessible on Kaggle, https://www.kaggle.com/datasets/mustai/continual-learning-of-covid19 (accessed on 23 December 2022).\nAcknowledgments: This research was supported by the Research Centre for Intelligent Prediction and Warning System for Mass Epidemics at Macau University of Science and Technology.\nConflicts of Interest: The authors declare no conflict of interest."
        }
    ],
    "title": "CoroTrans-CL: A Novel Transformer-Based Continual Deep Learning Model for Image Recognition of Coronavirus Infections",
    "year": 2023
}