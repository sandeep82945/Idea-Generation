{
    "abstractText": "Monocrystalline silicon is an important raw material in the semiconductor and photovoltaic industries. In the Czochralski (CZ) method of growing monocrystalline silicon, various factors may cause node loss and lead to the failure of crystal growth. Currently, there is no efficient method to detect the node loss of monocrystalline silicon at industrial sites. Therefore, this paper proposed a monocrystalline silicon node-loss detection method based on multimodal data fusion. The aim was to explore a new data-driven approach for the study of monocrystalline silicon growth. This article first collected the diameter, temperature, and pulling speed signals as well as two-dimensional images of the meniscus. Later, the continuous wavelet transform was used to preprocess the one-dimensional signals. Finally, convolutional neural networks and attention mechanisms were used to analyze and recognize the features of multimodal data. In the article, a convolutional neural network based on an improved channel attention mechanism (ICAM-CNN) for one-dimensional signal fusion as well as a multimodal fusion network (MMFN) for multimodal data fusion was proposed, which could automatically detect node loss in the CZ silicon single-crystal growth process. The experimental results showed that the proposed methods effectively detected node-loss defects in the growth process of monocrystalline silicon with high accuracy, robustness, and real-time performance. The methods could provide effective technical support to improve efficiency and quality control in the CZ silicon single-crystal growth process.",
    "authors": [
        {
            "affiliations": [],
            "name": "Lei Jiang"
        },
        {
            "affiliations": [],
            "name": "Rui Xue"
        },
        {
            "affiliations": [],
            "name": "Ding Liu"
        }
    ],
    "id": "SP:75305a133fca53ff28e212752b48af34567b24e4",
    "references": [
        {
            "authors": [
                "W. Heywang",
                "K.H. Zaininger"
            ],
            "title": "Silicon: The semiconductor material",
            "year": 2004
        },
        {
            "authors": [
                "G.D. Zhang",
                "S.Q. Zhai",
                "H.W. Cui",
                "J.C. Liu"
            ],
            "title": "Study on Dislocation in Growing proless of Semiconductor Single Crystals",
            "venue": "J. Synth. Cryst",
            "year": 2007
        },
        {
            "authors": [
                "I. Yonenaga"
            ],
            "title": "Nitrogen effects on generation and velocity of dislocations in Czochralski-grown silicon",
            "venue": "J. Appl. Phys",
            "year": 2005
        },
        {
            "authors": [
                "K. Kajiwara",
                "K. Torigoec",
                "K. Harada",
                "M. Hourai",
                "S.I. Nishizawa"
            ],
            "title": "Oxygen concentration dependence of as-grown defect formation in nitrogen-doped Czochralski silicon single crystals",
            "venue": "J. Cryst. Growth",
            "year": 2021
        },
        {
            "authors": [
                "J. Zhang",
                "H. Liu",
                "J. Cao",
                "W. Zhu",
                "B. Jin",
                "W. Li"
            ],
            "title": "A Deep Learning Based Dislocation Detection Method for Cylindrical Crystal Growth Process",
            "venue": "Appl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "K. Bayoudh",
                "R. Knani",
                "F. Hamdaoui",
                "A. Mtibaa"
            ],
            "title": "A survey on deep multimodal learning for computer vision: Advances, trends, applications, and datasets",
            "venue": "Vis. Comput",
            "year": 2021
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Proceedings of the Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "D. Liu",
                "X.G. Zhao",
                "Y. Zhao"
            ],
            "title": "A review of growth process modeling and control of Czochralski silicon single crystal",
            "venue": "Control Theory Appl. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "P. Zhijun",
                "J. Xiufeng",
                "L. Feng"
            ],
            "title": "Growth problems analysis of \u03a6100 mmmsilicon single crystal",
            "venue": "Semicond. Mag. 1998,",
            "year": 1998
        },
        {
            "authors": [
                "J. Dong",
                "B. Zhang",
                "J. Liu",
                "Z. Kewei",
                "L. Xiaobin",
                "Z. Caixia"
            ],
            "title": "Investigation on the growth problems in cz-si crystal",
            "venue": "Mater. Rev. 2013,",
            "year": 2013
        },
        {
            "authors": [
                "A.K. Choudhary",
                "J.A. Harding",
                "M.K. Tiwari"
            ],
            "title": "Data Mining in Manufacturing: A Review Based on the Kind of Knowledge",
            "venue": "J. Intell. Manuf",
            "year": 2009
        },
        {
            "authors": [
                "Z. Jing",
                "L. Ding",
                "Z. Yue"
            ],
            "title": "Finite Element Numerical Simulation and Control Parameter of Czochralski Silicon Monocrystal during Shoulder Growth Process",
            "venue": "J. Synth. Cryst. 2013,",
            "year": 2013
        },
        {
            "authors": [
                "J. Du"
            ],
            "title": "Research on \u201cDrop Bud",
            "venue": "Prediction Method of Monocrystalline Silicon Equal Diameter Growth Process Based on Data Mining. Master\u2019s Thesis, Zhejiang University, Hangzhou,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Huadong",
                "Z. Xiaotong",
                "T. Zengguo",
                "L. Xinge"
            ],
            "title": "Identification of Key Characteristic Parameters of Cz-Si Monocrystal during Shoulder Growth Process Based on MIC",
            "venue": "J. Synth. Cryst. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "P.K. Kankar",
                "S.C. Sharma",
                "S.P. Harsha"
            ],
            "title": "Fault diagnosis of ball bearings using machine learning methods",
            "venue": "Expert. Syst. Appl",
            "year": 2011
        },
        {
            "authors": [
                "Y. Zhang",
                "J. Ji",
                "B. Ma"
            ],
            "title": "Fault diagnosis of reciprocating compressor using a novel ensemble empirical mode decompositionconvolutional deep belief network",
            "venue": "Measurement",
            "year": 2020
        },
        {
            "authors": [
                "H. Yang",
                "Y. Cheng",
                "G. Li"
            ],
            "title": "A denoising method for ship radiated noise based on Spearman variational mode decomposition, spatial-dependence recurrence sample entropy, improved wavelet threshold denoising, and Savitzky-Golay",
            "year": 2021
        },
        {
            "authors": [
                "A. Choudhary",
                "D. Goyal",
                "S.S. Letha"
            ],
            "title": "Infrared thermography-based fault diagnosis of induction motor bearings using machine learning",
            "venue": "IEEE Sens. J. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Y. LeCun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proc. IEEE",
            "year": 1998
        },
        {
            "authors": [
                "G.E. Hinton",
                "R.R. Salakhutdinov"
            ],
            "title": "Reducing the dimensionality of data with neural networks",
            "venue": "Science",
            "year": 2006
        },
        {
            "authors": [
                "P. Vincent",
                "H. Larochelle",
                "Y. Bengio",
                "P.A. Manzagol"
            ],
            "title": "Extracting and composing robust features with denoising autoencoders",
            "venue": "In Proceedings of the 25th International Conference on Machine Learning, Helsinki, Finland,",
            "year": 2008
        },
        {
            "authors": [
                "A. Krizhevsky",
                "I. Sutskever",
                "G.E. Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Commun. ACM 2017,",
            "year": 2017
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv 2014,",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV,",
            "year": 2016
        },
        {
            "authors": [
                "R.R. Patil",
                "S. Kumar"
            ],
            "title": "Rice-Fusion: A Multimodality Data Fusion Framework for Rice Disease Diagnosis",
            "venue": "IEEE Access 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Xiang",
                "Q. Zhuo",
                "C. Zhao",
                "X. Deng",
                "T. Zhu",
                "T. Wang",
                "W. Jiang",
                "B. Lei"
            ],
            "title": "Self-supervised multi-modal fusion network for multi-modal thyroid ultrasound image diagnosis",
            "venue": "Comput. Biol. Med",
            "year": 2022
        },
        {
            "authors": [
                "E. Eslami",
                "H.B. Yun"
            ],
            "title": "Attention-based multi-scale convolutional neural network (A+ MCNN) for multi-class classification in road images",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Cao",
                "H. Yang",
                "J. Zhao",
                "S. Guo",
                "L. Li"
            ],
            "title": "Attention fusion for one-stage multispectral pedestrian detection",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ye",
                "X. Ren",
                "B. Zhu",
                "T. Tang",
                "X. Tan",
                "Y. Gui",
                "Q. Yao"
            ],
            "title": "An adaptive attention fusion mechanism convolutional network for object detection in remote sensing",
            "year": 2022
        },
        {
            "authors": [
                "J. Wang",
                "R. Huang",
                "S. Guo",
                "L. Li",
                "M. Zhu",
                "S. Yang",
                "L. Jiao"
            ],
            "title": "NAS-guided lightweight multiscale attention fusion network for hyperspectral image classification",
            "venue": "IEEE Trans. Geosci. Remote Sens",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Adam Ba"
            ],
            "title": "A method for stochastic optimization",
            "year": 2014
        }
    ],
    "sections": [
        {
            "text": "Citation: Jiang, L.; Xue, R.; Liu, D.\nNode-Loss Detection Methods for CZ\nSilicon Single Crystal Based on\nMultimodal Data Fusion. Sensors\n2023, 23, 5855. https://doi.org/\n10.3390/s23135855\nAcademic Editor: Chi-Wai Chow\nReceived: 14 May 2023\nRevised: 21 June 2023\nAccepted: 21 June 2023\nPublished: 24 June 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: CZ silicon single crystal; node-loss detection; continuous wavelet transform; convolutional neural network; attention mechanism; multimodal data fusion\n1. Introduction\nMonocrystalline silicon is an important semiconductor material that can be classified into solar-grade and IC-grade, based on its quality [1]. High-quality IC-grade monocrystalline silicon is the main material used for manufacturing integrated circuit chips. The Czochralski method is the main method used to prepare single-crystal silicon. With the development of ultra-large-scale integrated circuits, higher requirements have been placed on the quality and yield of silicon wafers, which need to increase in size while meeting industry requirements for resistivity, uniformity, crystal integrity, crystal orientation, oxygen, carbon content, purity, etc. Solar-grade monocrystalline silicon has lower quality requirements, but needs to be grown without dislocations. Dislocation defects are the core defects in the quality of monocrystalline silicon. Once dislocations occur during crystal growth, other quality parameters do not need to be considered [2]. Possible reasons for dislocation generation include impurities, fluctuations of unreasonable process parameters, etc. [3,4]. Node loss is the main manifestation of dislocations in the growth process of monocrystalline silicon and is also one of the basic judgment factors for the quality of CZ silicon ingots. Different crystal directions of single-crystal silicon have different numbers of nodes. Ingots grown in the [1 0 0] direction have four nodes. Figure 1 shows a normally\nSensors 2023, 23, 5855. https://doi.org/10.3390/s23135855 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 5855 2 of 18\ngrown ingot and two states of an ingot, one with a normal node and one with node loss. As Figure 1 demonstrates, a normally grown CZ silicon ingot has sharp nodes. However, the node of the node-loss crystal ingot disappears and the surface is very smooth.\nSensors 2023, 23, x FOR PEER REVIEW 2 of 18\nnumbers of nodes. Ingots grown in the [1 0 0] direction have four nodes. Figure 1 shows a normally grown ingot and two states of an ingot, one with a normal node and one with node loss. As Figure 1 demonstrates, a normally grown CZ silicon ingot has sharp nodes. However, the node of the node-loss crystal ingot disappears and the surface is very smooth.\nnode\nnode loss\nFigure 1. Whole normal ingot (left), normal node (middle), and node loss (right).\nDuring the growing process of a CZ silicon single crystal, the ingot needs to be regrown by re-melting and re-pulling if node loss occurs. If the loss can be promptly, quickly, and accurately identified, it not only saves costs, but also increases production efficiency. Otherwise, dislocation extension occurs and the ingot becomes polycrystalline, which results in the meaningless growth of crystals and significantly consumes production costs. Therefore, it is necessary and meaningful to research the node loss of CZ silicon single crystals. Currently, most industrial sites still rely on manual inspections and identification to identify the node loss. Due to the high randomness of node loss, it is difficult to promptly and accurately detect node loss when relying on manual inspections. Certain companies only use related single-dimension information such as image features to identify node loss during the crystal growth process to achieve a higher detection efficiency, but the accuracy of this method is not high. Based on the conventional Faster R-CNN, Zhang et al. [5] used the ResNet-50 residual neural network as the backbone to extract the image features at the meniscus, which effectively improved the accuracy of the node\u2014loss detection. However, their methods do not take into account the influence of directly related variables such as diameter, temperature, and pulling speed. Therefore, there is a large space for improvement in the node-loss detection field of CZ Silicon single crystals. In recent years, with the rapid development of convolutional neural networks and multimodal data fusion technology, excellent research and achievements have emerged. The recent popular large-scale model ChatGPT is a typical representative of those achievements. Multimodal fusion technology in deep learning refers to the process of handling different forms of data where the model completes analyses and recognition tasks [6]. Its purpose is to establish a model that can handle and correlate information from multiple modalities, enabling machine-learning algorithms to comprehensively and efficiently understand the controlled object. Finally, the complementarity of diverse heterogeneous information can be realized and the limitations of single-modal data processing can be avoided. With the rise of the attention mechanism [7], the correlation between multimodal data has been extensively mined, enabling multi-models to fuse with higher accuracy, further improving the robustness and anti-interference ability of the model. Finally, the accuracy of recognition results is improved. Thanks to the technology of multimodal\nFigure 1. Whole normal ingot (left), normal node (mid le), and node loss (right).\nDuring the growing process of si gle crystal, the ingot needs to be regro n - lti - lli if loss occurs. If t loss can be promptly, quickly, and accurately identified, it not only saves costs, but also increases production efficiency. Otherwise, dislocation extension occurs and the ingot becomes polycrystalline, which results in the meaningless growth of crystals and significantly consumes production costs. Therefore, it is necessary and meaningful to research the node loss of CZ silicon single crystals. Currently, most industrial sites still rely on manual inspections and identification to identify the node loss. Due to the high randomness of node loss, it is difficult to promptly and accurately detect node loss when relying on manual inspections. Certain companies only use related single-dimension information such as image features to identify node loss during the crystal growth process to achieve a higher detection efficiency, but the accuracy of this method is not high. Based on the conventional Faster R-CNN, Zhang et al. [5] used the ResNet-50 residual neural network as the backbone to extract the image features at the meniscus, which effectively improved the accuracy of the node\u2014loss detection. However, their methods do not take into account the influence of directly related variables such as diameter, temperature, and pulling speed. Therefore, there is a large space for improvement in the node-loss detection field of CZ Silicon single crystals. In recent years, with the rapid development of convolutional neural networks and multimodal data fusion technology, excellent research and achievements have emerged. The recent popular large-scale model ChatGPT is a typical representative of those achievements. Multimodal fusion technology in deep learning refers to the process of handling different forms of data where the model completes analyses and recognition tasks [6]. Its purpose is to establish a model that can handle and correlate information from multiple modalities, enabling machine-learning algorithms to comprehensively and efficiently understand the controlled object. Finally, the complementarity of diverse heterogeneous information can be realized and the limitations of single-modal data processing can be avoided. With the rise of the attention mechanism [7], the correlation between multimodal data has been extensively mined, enabling multi-models to fuse with higher accuracy, further improving the robustness and anti-interference ability of the model. Finally, the accuracy of recognition results is improved. Thanks to the technology of multimodal fusion, we proposed the method used in this paper on the basis of fully considering the correlation between the node-loss data of Czochralski silicon single crystal.\nSensors 2023, 23, 5855 3 of 18\nDue to the complexity of the silicon single-crystal growth process, it specifically includes the five stages of seeding, necking, shouldering, body, and tailing [8]. A large number of sensors were assembled to monitor the parameters of the silicon single-crystal growth process as well as the environmental parameters of a single-crystal furnace, thereby generating a large amount of historical data. In this paper, the ICAM-CNN method was used for the node-loss recognition of the one-dimensional diameter, temperature, and pulling speed signals directly related to node loss. The ResNet network was used for the node-loss recognition of the two-dimensional meniscus image. To further improve the accuracy, we take the advantages of one-dimensional signal feature extraction and two-dimensional image feature extraction to propose an MMFN method for multimodal data in the body process of a CZ silicon single crystal. Ultimately, the accuracy of node-loss detection could be improved and the timepoint of node loss could be identified in time, thereby saving production costs and improving production efficiency.\nThe main contributions of this paper are as follows:\n\u2022 The required data were collected and preprocessed. The diameter, temperature, pulling speed signals, and image information at the meniscus directly related to the node loss of the silicon single crystal were measured using a variety of sensors. The continuous wavelet transform was also used to preprocess the signals of the diameter, temperature, and pulling speed. \u2022 A convolutional neural network (ICAM-CNN) based on an improved channel attention mechanism was proposed. This method could be used to perform a feature fusion for the one-dimensional diameter, pulling speed, and temperature signals, finally determining the node-loss time. \u2022 A two-dimensional image classification decision-making method based on the ResNet network was adopted. For the collected two-dimensional image information of the meniscus, the ResNet network was used as the image feature extraction network to extract deep image feature information to judge whether the system demonstrated node loss. \u2022 A decision network based on multimodal data fusion\u2014a multimodal fusion network (MMFN)\u2014was proposed. MMFN first obtained the fusion features of the onedimensional diameter, temperature, and pulling speed signals through ICAM-CNN, then obtained the two-dimensional image features through the ResNet network. In the feature fusion layer, MMFN used the concatenate method to achieve multimodal data feature-level fusion. Finally, a classifier was used to identify the node-loss time. \u2022 A comparative discussion on the results of using single-modal and multimodal data fusion decisions was conducted. The results showed that using multimodal data fusion was more effective than any current single-modal data decision-making method. It could significantly improve the accuracy of CZ silicon single-crystal node-loss detection and meet the real-time and high-accuracy requirements of production sites.\n2. Related Works\nThis article focused on the application of multimodal data fusion in the detection of the node loss of CZ silicon single crystals, aiming to overcome the problem of the low diagnostic accuracy of single-dimensional data. At present, there are only a few articles researching the node loss of CZ silicon single crystals. The problem of the node loss of silicon single crystals is mainly studied and predicted from the aspects of the mechanism and the means of preventing node loss. Zhijun et al. [9] studied the problem of shoulders and broken edges in the preparation of 4-inch 111-oriented silicon single crystals and discussed the causes of dislocations. Dong et al. [10] analyzed dislocation formation from a theoretical point of view, discussing the specific reasons for node loss and bud drop. Choudhary et al. [11] studied the behavior of linear propagation dislocation along the growth direction in CZ silicon single crystals and also discussed the dislocation formation mechanism in heavily and lightly doped growth processes. Jing et al. [12] used a finite element numerical simulation to explain the\nSensors 2023, 23, 5855 4 of 18\ncause of the liquid flow line in the front part of the shoulder and proposed a method of crystal transformation for the crystal-pulling process to reduce the occurrence of node loss. Du [13] used the method of data mining to establish an online prediction model for the problem of node loss in the body stage of silicon single crystals. Zhai et al. [14] proposed a feature-selection-based prediction study on node loss at the shouldering stage, but the data used did not directly correlate with the factors causing node loss, and the prediction accuracy was low. The current approach to the problem of node-loss detection is mainly to extract features from the data, then perform classification and recognition tasks. The data feature extraction is mainly divided into one-dimensional data feature extraction and two-dimensional data feature extraction. One-dimensional signal feature extraction methods are mainly divided into statistical feature-based, model-based, transformation-based, and fractal-based methods. Kankar et al. [15] extracted features such as statistical parameters and spectral features from vibration signals collected from faulty and healthy rolling bearings as inputs for different machine-learning algorithms. The performance of the model was evaluated according to indicators such as accuracy, sensitivity, and specificity, which had a certain contribution to the fault diagnosis using machine-learning techniques. In [16], the EEMD algorithm was used to decompose a signal into multiple intrinsic mode functions; these functions were then used as the input for a convolutional deep belief network. The method recognized the characteristics of different fault states by training the convolutional deep belief network and finally realized the automatic diagnosis of reciprocating compressor faults. Yang et al. [17] introduced the principle and algorithm of wavelet threshold denoising in detail, using an improved wavelet threshold denoising algorithm to suppress the noise in the signal and, at the same time, used the Savitzky\u2013Golay filter to smooth the signal, resulting in a higher signal-to-noise ratio. Compared with other commonly used denoising methods, this method was more effective and robust. For the feature extraction of two-dimensional images and with the rapid development of image processing technology, many scholars have focused on applying automatic inspection technology based on machine vision to inspection tasks. Traditional image processing algorithms have been proposed earlier or developed maturely, such as the principal component analysis (PCA) [18]. With continuous developments in the manufacturing industry, traditional algorithms have difficulty meeting the increasing detection accuracy and realtime requirements. In recent years, deep learning has achieved continuous progress in life as well as academic research. Deep learning models have shown strong feature extraction capabilities, gradually replacing traditional algorithms and becoming a research hotspot in the field of target detection and recognition. The most representative deep learning models are convolutional neural networks (CNNs) [19], recurrent neural networks (RNNs) [20], and autoencoders (AEs) [21], which can be applied to image classification tasks. In this paper, the convolutional neural network was mainly used to extract the features of the image and finally classify the image. With the popularity of CNNs, many evolutionary networks have been proposed. Examples include AlexNet, proposed in 2012 and published in 2017 [22]; VGGNet, proposed in 2014 [23]; and ResNet, proposed in 2015 and published in 2016 [24]. They all have excellent image feature extraction abilities. For complex industrial field classification and recognition tasks, one-dimensional feature extraction and classification methods based on transformation theory and twodimensional feature extraction and classification methods based on convolutional neural networks have shortcomings and cannot fully use the features of all data, resulting in poor model generalization ability. Multimodal data fusion technology can overcome this drawback using the complementary advantages of multimodal data and combining the features of one-dimensional signals and two-dimensional images for fusion decisionmaking. This can improve the generalization ability of the model, thus enhancing the detection accuracy.\nSensors 2023, 23, 5855 5 of 18\nData fusion can be classified into three categories based on the level of information fusion, namely, data-level fusion, feature-level fusion, and decision-level fusion [25]. Feature-level fusion extracts features from signals collected from multiple sensors, then uses a network to analyze the feature information. After this, it forms a comprehensive feature set for use in the final target recognition and classification stage [26]. Compared with data-level and decision-level fusion, feature-level fusion is more flexible and can be combined with network structures, making it more versatile. In recent years, with the popularity of transformer technology [27], many attention mechanisms have been applied to fusion operations. Compared with traditional feature fusion methods, attention mechanisms can assign different weights to different parts of the input data, thereby using useful information in the data more effectively. It can automatically learn the relationship between features during the training process, thereby avoiding the tedious process of manually designing feature fusion rules. It can also adapt to different input dimensions and data types, making it highly applicable. References [28\u201331] all used attention mechanisms to fuse different features. By introducing attention mechanisms in the feature extraction process, the network can focus on important features with greater accuracy, thereby improving the effectiveness of features and the accuracy of classification and diagnosis. These studies all demonstrate that attention is an effective feature fusion method that can be used for classification tasks in various fields. To the best of our knowledge, no research has applied the attention module to node-loss detection in the growth process of single-crystal silicon.\n3. Proposed Method\nThis article conducted a detailed study on the node-loss problem of a CZ silicon single crystal and collected, and preprocessed the node-loss data, and made a new dataset. Two new deep learning-based networks, ICAM-CNN and MMFN, were then proposed. The architecture of the proposed networks is described in detail below.\n3.1. Data Collection and Preprocessing 3.1.1. Data Collection\nIn the process of crystal growth, the node loss of a Czochralski silicon single crystal is random. There are many factors that lead to node loss, such as changes in thermal stress at the meniscus, the pulling speed, and temperature. As there is no technology at present to measure the thermal stress at the meniscus, we selected three measurable and directly related variables\u2014temperature, pulling speed, and diameter\u2014as the data sources to identify node loss. The data in this article were all collected from crystal-growing equipment on site. The model of the single-crystal furnace was TDR-120CZ, as shown in Figure 2. The equipment could produce 100\u2013310 mm CZ silicon single crystals. The diameter detection range was 4\u2013350 mm. The maximum power of the equipment was 180 kw. The adjustment range of the crucible rotation speed was 0\u201315 rpm, the adjustment range of the crystal rotation speed was 0\u201320 rpm, the adjustment range of the crucible lifting speed was 0\u20130.5 mm/min, the adjustment range of the crystal lifting speed was 0\u20136 mm/min, the ultimate vacuum was 0.4 Pa, the adjustment range of the air intake was 0\u2013150 L/min, and the adjustment range of the liquid level was 0\u2013100 mm. The lifting stroke of the crystal was 2.8 m and the lifting stroke of the crucible was 600 mm. As far as the process parameters on-site were concerned, the target setting value of the diameter was 308 mm, the adjustment range of the casting speed was 0.2\u20131.1 mm/min, and the temperature adjustment was determined according to the input of the amount of silicon material. The general rule was to cool down first, then raise the temperature. The adjustment range was usually 2100\u20132500 (dimensionless).\nSensors 2023, 23, 5855 6 of 18\nSensors 2023, 23, x FOR PEER REVIEW 6 of 18 first, then raise the temperature. The adjustment range was usually 2100\u20132500 (dimensionless).\nThe crystal diameter signal used for the data fusion was acquired using a high-temperature infrared pyrometer sensor. The crystal growth temperature signal was acquired using a RAYTEK Marathon FR Infrared Thermometer sensor. The pulling speed of the crystal was obtained after conversion using Schneider XCC Series Absolute Encoders. The data of the temperature, pulling speed, and diameter were sampled every 2 s. The image data during the growth process were obtained using a Microvision MV-300UC camera. It had a resolution of 300 megapixels and used a CMOS imaging method with a frame rate of 15 fps. All acquisition devices are shown in Figure 2.\nHigh Temperature Infrared Pyrometers\nRAYTEK Marathon FR Infrared ThermometerMicrovision MV-300UC Camera\nSchneider XCC Series Absolute Encoders\nFigure 2. Capture device images.\nIn the growth process, once the temperature and pulling speed exhibit large mutations, it leads to a greater risk of node loss. There is also an immediate reaction in the diameter. As shown in Figure 3, the horizontal axis was the sampling time from the isodiametric process and the vertical axis was the diameter signal. Node loss occurred at the crystal body stage. It was not difficult to ascertain from the diameter signal that once there was node loss, the frequency of the crystal diameter change would be altered.\nFigure 2. Capture device images.\nThe crystal diameter signal used for the data fusion was acquired using a hightemperature infrared pyrometer sensor. The crystal growth temperature signal was acquired using a RAYTEK Marathon FR Infrared Thermometer sensor. The pulling speed of the crystal was obtained aft r conversion using Schn ider XCC Series Absolute Encoders. The d ta f the temperature, pulling speed, and diameter were sampled every 2 s. The image data during the growth process were obtained using a Microvision MV-300UC camera. It had a resolution of 300 megapixels and used a CMOS imaging method with a frame rate of 15 fps. All acquisition devices are shown in Figure 2. In the growth process, once the temperature and pulling speed exhibit large mutations, it leads to a greater risk of node loss. There is also an immediate reaction in the diameter. As shown in Figure 3, the horizontal axis was the sampling time from the isodiametric process and the vertical axis was the diameter signal. Node loss occurred at the crystal body stage. It was not difficult to ascertain from the diameter signal that once there was node loss, the frequency of the crystal diameter change would be altered. Sensors 2023, 23, x FOR PEER REVIEW 7 of 18\nFigure 3. Diameter difference between normal and abnormal\nFor normal on-site production, inspectors usually judge node loss according to the images. If there is node loss, the changes can be observed from the image of the meniscus captured by the camera. Figure 4 is the image data of the crystal body stage obtained using a TDR-120CZ single-crystal furnace. The image on the left is the meniscus taken during normal growth, with periodic bump information. The image on the right is the meniscus was taken when the node was lost, which was very smooth without any bump features.\n3.1.2. Data Preprocessing Through multiple crystal-pulling experiments, 1800 sets of temperature, pulling speed, and diameter near the time of node loss of different lots as well as images of the meniscus of the crystal at the corresponding time were collected. When the crystal grew normally, 1844 datasets of temperature, pulling speed, and diameter signals as well as the image data of the meniscus at the corresponding time were collected. Due to the characteristics of a long delay and a large lag in the crystal growth process, the temperature, pulling speed, and diameter signals in each set of data were represented by the data segment from the previous 7 min, that is, 210 data points. To highlight the time\u2013frequency changes of the diameter, temperature, and pulling speed signals and more accurately identify node loss, the continuous wavelet transform was used and the data preprocessing method shown in Figure 5 was proposed.\nFigure 3. Diameter differ nce b tween normal and abnormal.\nSensors 2023, 23, 5855 7 of 18\nFor normal on-site production, inspectors usually judge node loss according to the images. If there is node loss, the changes can be observed from the image of the meniscus captured by the camera. Figure 4 is the image data of the crystal body stage obtained using a TDR-120CZ single-crystal furnace. The image on the left is the meniscus taken during normal growth, with periodic bump information. The image on the right is the meniscus was taken when the node was lost, which was very smooth without any bump features.\nSensors 2023, 23, x FOR PEER REVIEW 7 of 18\nFigure 3. Diameter difference between normal and abnormal\nFor normal on-site production, inspectors usually judge node loss acc rding to the images. If there is node loss, the changes can be observed from the image of the eniscus captured by the camera. Figure 4 is the image data of the crystal body stage obtained using a TDR-120CZ single-crystal furnace. The image on the left is the meniscus taken during normal growth, with periodic bump information. The image on the right is the meniscus was taken when the node was lost, which was very smooth without any bump features.\nbump\nwithout bump\nFigure 4. The difference at the meniscus when normal (left) and with node loss (right).\n3.1.2. Data Preprocessing Through multiple crystal-pulling experiments, 1800 sets of temperature, pulling speed, and diameter near the time of node loss of different lots as well as images of the meniscus of the crystal at the corresponding time were collected. When the crystal grew normally, 1844 datasets of temperature, pulling speed, and diameter signals as well as the image data of the meniscus at the corresponding time were collected. Due to the characteristics of a long delay and a large lag in the crystal growth process, the temperature, pulling speed, and diameter signals in each set of data were represented by the data segment from the previous 7 min, that is, 210 data points. To highlight the time\u2013frequency changes of the diameter, temperature, and pulling speed signals and more accurately identify node loss, the continuous wavelet transform was used and the data preprocessing method shown in Figure 5 was proposed.\nFigure 4. The difference at the meniscus when normal (left) and with node loss (right).\n3.1.2. Data Preprocessing\nThrough multi le crystal- ulling experiments, 1800 s ts of temperat re, pulling speed, and diameter near the time f node l ss of different lots as well as images of the meniscus of the cryst l at the corresponding time w re collected. When the crystal grew normally, 1844 datasets of temperature, pulling speed, and diameter signals as well as the image data of the eniscus at the correspon ing ti e ere collected. Due to the characteristics of a long delay and a large lag in the crystal growth process, the temperature, pulling speed, and diameter signals in each set of data were represented by the data segment from the previous 7 min, that is, 210 data points. To highlight the time\u2013frequency changes of the diameter, temperature, and pulling speed signals and more accurately identify node loss, the continuous wavelet transform was used and the data preprocessing method shown in Figure 5 was proposed. Sensors 2023, 23, x FOR PEER REVIEW 8 of 18\n1 -k kf f+\nV\nT\nD\nInput data Processed Input Time-frequency Spectrum\nCWT\nFigure 5. Continuous wavelet transforms for one-dimensional signal preprocessing.\nFirst, we took the difference of each input dataset to obtain the change in temperature, diameter, and pulling speed every 2 s. Then, the continuous wavelet transform was used in the signal processing to fully extract the time\u2013frequency domain characteristic changes of the temperature, pulling speed, and diameter signals. Finally, a processed time\u2013frequency spectrum of the temperature, diameter, and pulling speed signals was obtained.\nThe formula for continuous wavelet transform is as follows:\n* , ,( ) ( ) ( )a b a bWT t f t t dt\u03d5\n+\u221e\n\u2212\u221e =   (1)\nwhere ( )f t is the original function, ( )t\u03d5 is the wavelet basis function, a is the scale parameter, and b is a time parameter. The specific form of the function is as follows:\n, 1( ) ( )a b t bt aa \u03d5 \u03d5 \u2212= (2)\nwhere a is the scale parameter and b is a time parameter. Through the transformation, the time subdivision of the signal at a high frequency and the frequency subdivision at a low frequency is finally achieved, which can adapt to the requirements of the time\u2013frequency signal analysis and focus on any detailed characteristics of the collected signals.\n3.2. Method 1: ICAM-CNN The one-dimensional signals included the diameter, temperature, and pulling speed. For one-dimensional signal fusion node-loss detection, a network structure based on ICAM-CNN was proposed, as shown in Figure 6.\nFigure 5. Continuous wavelet transforms for one-dimensional signal preprocessing.\nSensors 2023, 23, 5855 8 of 18\nFirst, we took the difference of each input dataset to obtain the change in temperature, diameter, and pulling speed every 2 s. Then, the continuous wavelet transform was used in the signal processing to fully extract the time\u2013frequency domain characteristic changes of the temperature, pulling speed, and diameter signals. Finally, a processed time\u2013frequency spectrum of the temperature, diameter, and pulling speed signals was obtained.\nThe formula for continuous wavelet transform is as follows:\nWTa,b(t) = \u222b +\u221e \u2212\u221e f (t)\u2022\u03d5a,b\u2217(t)dt (1)\nwhere f (t) is the original function, \u03d5(t) is the wavelet basis function, a is the scale parameter, and b is a time parameter. The specific form of the function is as follows:\n\u03d5a,b(t) = 1\u221a a \u03d5( t\u2212 b a ) (2)\nwhere a is the scale parameter and b is a time parameter. Through the transformation, the time subdivision of the signal at a high frequency and the frequency subdivision at a low frequency is finally achieved, which can adapt to the requirements of the time\u2013frequency signal analysis and focus on any detailed characteristics of the collected signals.\n3.2. Method 1: ICAM-CNN\nThe one-dimensional signals included the diameter, temperature, and pulling speed. For one-dimensional signal fusion node-loss detection, a network structure based on ICAMCNN was proposed, as shown in Figure 6. Sensors 2023, 23, x FOR PEER REVIEW 9 of 18\nV\nT\nD\nSoftMaxShared MLP\nInput CNN ICAM\nabnormal\nnormal\n+\n(224,224,3) (3 32 , 32 32 64,64 64)\nx\nConv(3*3)\nReLU\nMaxpooling(2*2)\nBN\nGlobalMaxPooling2D\nGlobalAvgPooling2D\n(192)\n\ud835\udc401 1 z K z k k e e=\n2\n1\nz\nK z k k e e=\n\u2026\u2026\n3\n1\nz\nK z k k e e=\nz K K z k K k e e=\nFigure 6. ICAM-CNN network structure.\nFirst, the data were preprocessed by the continuous wavelet transform. Then, the time\u2013frequency spectrogram was inputted into the CNN for training. The CNN was mainly composed of convolutional layers, pooling layers, fully connected layers, and decision layers. The features of the data were extracted by convolution and pooling operations. Below is the main calculation formula for the convolution operation.\n1( ) k k k k i i i iZ f W Z b\u2212= \u2297 + (3)\nHere, kiZ represents the feature map formed by the convolution kernel of the i th layer, kiW denotes the weight matrix, k ib represents the bias, \u2297 denotes the convolution operation, and f represents the ReLU activation function. Then, the improved channel attention mechanism, which is shown in Figure 7, was used to perform a correlation fusion analysis on the diameter, temperature, and pulling speed features extracted by the convolutional neural network. The calculation formula for the improved channel attention is as follows:\nmax\nmax\n( ( ) ( )) ( ) c avg\navg\nM f MLP Map MLP Map f V V = + = + (4)\nwhere f represents the SoftMax function, avgMap and maxMap represents the global average pooling feature and the global maximum pooling feature, 1 1cavgV R\n\u00d7 \u00d7\u2208 and 1 1\nmax cV R \u00d7 \u00d7\u2208 represents one-dimensional vectors further processed by the MLP, 1 1ccM R \u00d7 \u00d7\u2208 represents a final evaluation score for each channel. Finally, we sent the correlation fusion features processed by the ICAM-CNN network into the classifier for decision-making, and obtained the result of using the one-dimensional diameter, temperature, and pulling speed signals for the node-loss detection.\nFigure 6. ICAM-CNN network structure.\nFirst, the data were preprocessed by the continuous wavelet transform. Then, the time\u2013frequency spectrogram was inputted into the CNN for training. The CNN was mainly composed of convolutional layers, pooling layers, fully connected layers, and decision layers. The features of the data were extracted by convolution and pooling operations. Below is the main calculation formula for the convolution operation.\nk i f ( k i \u2297 Zki\u22121 + ki ) (3)\nSensors 2023, 23, 5855 9 of 18\nHere, Zki represents the feature map formed by the convolution kernel of the ith layer, Wki denotes the weight matrix, b k i represents the bias, \u2297 denotes the convolution operation, and f represents the ReLU activation function. Then, the improved channel attention mechanism, which is shown in Figure 7, was used to perform a correlation fusion analysis on the diameter, temperature, and pulling speed features extracted by the convolutional neural network. The calculation formula for the improved channel attention is as follows:\nMc = f (MLP(Mapavg) + MLP(Mapmax)) = f (Vavg + Vmax)\n(4)\nwhere f represents the SoftMax function, Mapavg and Mapmax represents the global average pooling feature and the global maximum pooling feature, Vavg \u2208 Rc\u00d71\u00d71 and Vmax \u2208 Rc\u00d71\u00d71 represents one-dimensional vectors further processed by the MLP, Mc \u2208 Rc\u00d71\u00d71 represents a final evaluation score for each channel. Sensors 2023, 23, x FOR PEER REVIEW 10 of 18\nSoftMax Shared MLP\n+ x\n(192)\n\ud835\udc401 1 z K z k k e e=\n2\n1\nz\nK z k k e e=\n\u2026\u2026\n3\n1\nz\nK z k k e e=\nz K K z k K k e e=\nGlobalMaxPooling2D GlobalAvgPooling2D\nFigure 7. ICAM structure.\n3.3. Method 2: MMFN On the basis of one-dimensional node-loss fusion detection, two-dimensional menis-\ncus image data were introduced and a fusion decision method based on multimodal data features was proposed to further improve the accuracy of node-loss detection. The specific network structure is shown in Figure 8.\n64 64 128\nabnormal\nnormal\nV\nT\nD\nSoftMaxShared MLP\nInput CNN ICAM\n+\n(224,224,3) (3 32 , 32 32 64,64 64)\nx\n(192)\n\ud835\udc74\ud835\udc841 1 z K z k k e e=\n2\n1\nz K z k k e e=\n\u2026\u2026\n3\n1\nz K z k k e e=\nz K K z k K k e e=\nFC\nG\nSoftMax\nResnet Conv BNMaxpooling(2*2) GlobalMaxPooling2DReLU GlobalAvgPooling2D\n128 256256 512512\nFigure 8. MMFN structure.\nFirst, using the ICAM-CNN method, the features for all the one-dimensional signals were extracted. Then, using the ResNet network, the features for all the two-dimensional meniscus images were extracted. After, feature concatenation technology was used to fuse the extracted features at the feature layer, achieving a feature-level fusion of the multimodal data. Finally, a fusion decision based on the features of the multimodal data was achieved through a classifier.\nFor the two-dimensional image data, the ResNet network, which is shown in Figure 8, was used as the feature extraction network. It is mainly aimed at solving the problems of gradient disappearance and model degradation in deep networks. The residual learning structure consists of a feedforward neural network and a skip connection method, as shown in Figure 9.\nFigure 7. ICAM structure.\nFinally, we sent the correlation fusion features processed by the ICAM-CNN network into the classifier for decision-making, and obtained the result of using the one-dimensional diameter, temperature, and pulling speed signals for the node-loss detection.\n3.3. Method 2: MMFN\nOn the basis of one-dimensional node-loss fusion detecti n, two-dimensional meniscus image data were intr duced and a fusion decision method based on multimodal data features was proposed to further improve the accuracy of node-loss detection. The specific network structure is shown in Figure 8. First, using the ICA -CNN method, the features for all the one-dimensional signals were extracted. Then, using the ResNet network, the features for all the two-dimensional meniscus images were extracted. After, feature concatenation technology was used to fuse the extracted features at the feature layer, achieving a feature-level fusion of the multimodal data. Finally, a fusion decision based on the features of the multimodal data was achieved through a classifier.\nSensors 2023, 23, 5855 10 of 18\nSensors 2023, 23, x FOR PEER REVIEW 10 of 18 SoftMax Shared MLP + x (192) \ud835\udc401 1 z K z k k e e= 2 1 z K z k k e e= \u2026\u2026 3 1 z K z k k e e= z K K z k K k e e= GlobalMaxPooling2D GlobalAvgPooling2D Figure 7. ICAM structure.\n3.3. Method 2: MMFN On the basis of one-dimensional node-loss fusion detection, two-dimensional meniscus image data were introduced and a fusion decision method based on multimodal data features was proposed to further improve the accuracy of node-loss detection. The specific network structure is shown in Figure 8.\n64 64 128\nabnormal\nnormal\nV\nT\nD\nSoftMaxShared MLP\nInput CNN ICAM\n+\n(224,224,3) (3 32 , 32 32 64,64 64)\nx\n(192)\n\ud835\udc74\ud835\udc841 1 z K z k k e e=\n2\n1\nz K z k k e e=\n\u2026\u2026\n3\n1\nz K z k k e e=\nz K K z k K k e e=\nFC\nG\nSoftMax\nResnet Conv BNMaxpooling(2*2) GlobalMaxPooling2DReLU GlobalAvgPooling2D\n128 256256 512512\nFigure 8. MMFN structure.\nFirst, using the ICAM-CNN method, the features for all the one-dimensional signals were extracted. Then, using the ResNet network, the features for all the two-dimensional meniscus images were extracted. After, feature concatenation technology was used to fuse the extracted features at the feature layer, achieving a feature-level fusion of the multimodal data. Finally, a fusion decision based on the features of the multimodal data was achieved through a classifier. For the two-dimensional image data, the ResNet network, which is shown in Figure 8, was used as the feature extraction network. It is mainly aimed at solving the problems of gradient disappearance and model degradation in deep networks. The residual learning structure consists of a feedforward neural network and a skip connection method, as shown in Figure 9.\nFigure 8. MMFN struct re.\nFor the two-dimensional image data, the ResNet network, which is shown in Figure 8, was used as the feature extraction network. It is mainly aimed at solving the problems of gradient disappearance and model degradation in deep networks. The residual learning structure consists of a feedforward neural network and a skip connection method, as shown in Figure 9. Sensors 2023, 23, x FOR PEER REVIEW 11 of 18\nWeight layer W1 Weight layer W2\nReLU\nF(x)\n+ ReLU\nF(x)+x\nx identity\nx\nFigure 9. Residual block structure.\n( ) ( )H x F x x= + (5) where, x represents the input, ( )F x represents the residual, and ( )H x represents the desired target. Through such a residual structure, the number of layers can be continuously superimposed to improve the accuracy of the final node-loss detection.\n4. Experimental Setup and Result 4.1. Model Training\nThe validity of the method was verified using the Czochralski silicon single-crystal node-loss dataset collected and produced by ourselves. The algorithms proposed in this paper were implemented using the open-source TensorFlow deep learning framework. The CPU used was Intel(R) Xeon(R) Gold 5318Y CPU @ 2.10 GHz (2 processors) and the GPU used Nvidia Grid V100D-8Q. A total of 3644 datasets of temperature, casting speed, diameter, and images were collected in the one-dimensional dataset, including 1800 sets of node-loss data and 1844 sets of normal growth data. Each dataset of pulling speed, temperature, diameter, and images shared the same label. The split ratio of the training set, validation set, and testing set in the data was 7:2:1 and the size of the input image was normalized to 224 \u00d7 224. The Adam, proposed by Kingma et al. [32], was used as the overall optimizer and the backpropagation algorithm to realize the optimization of the entire network model. For training, the cross-entropy loss function was selected as the loss function. After parameter optimization, the batch size of all datasets was set to 32 and the epoch was set to 20. To avoid overfitting, the dropout was set to 0.5 in the FC layer of the proposed network model and the L2 regularization coefficient of the convolution kernel was set to 0.01 in the convolution layer. The data enhancement on the collected image information of the meniscus was also performed.\n4.2. Evaluation Method This article drew a confusion matrix on the final test set and calculated the recall rate (Recall), precision (Precision), F1 score, and accuracy (Accuracy) as evaluation indicators to accurately evaluate the effectiveness of the proposed method.\nThe calculation formulas for these values are as follows. TPRecall\nTP FN = + (6)\nP Precision TP TP F = + (7)\n2( ) P RF1Score P R \u00d7= +\n(8)\n(x) = F(x) + x (5)\nhere, x repres nts he input, F(x) represents the residual, and H(x) represents the desired target. Through such a residual structure, the number of layers can be continuously superimposed to improve the accuracy of the final node-loss detection.\n4. Experimental Setup and Result 4.1. Model Training\nThe validity of the method was verified using the Czochralski silicon single-crystal node-loss dataset collected and produced by ourselves. The algorithms proposed in this paper were implemented using the open-source TensorFlow deep learning framework. The CPU used was Intel(R) Xeon(R) Gold 5318Y CPU @ 2.10 GHz (2 processors) and the GPU used Nvidia Grid V100D-8Q. A total of 3644 datasets of temperature, casting speed, diameter, and images were collected in the one-dimensional dataset, including 1800 sets of node-loss data and 1844 sets of normal growth data. Each dataset of pulling speed, temperature, diameter, and images shared the same label. The split ratio of the training set, validation set, and testing set in the data was 7:2:1 and the size of the input image was normalized to 224 \u00d7 224. The Adam, proposed by Kingma et al. [32], was used as the overall optimizer and the\nSensors 2023, 23, 5855 11 of 18\nbackpropagation algorithm to realize the optimization of the entire network model. For training, the cross-entropy loss function was selected as the loss function. After parameter optimization, the batch size of all datasets was set to 32 and the epoch was set to 20. To avoid overfitting, the dropout was set to 0.5 in the FC layer of the proposed network model and the L2 regularization coefficient of the convolution kernel was set to 0.01 in the convolution layer. The data enhancement on the collected image information of the meniscus was also performed.\n4.2. Evaluation Method\nThis article drew a confusion matrix on the final test set and calculated the recall rate (Recall), precision (Precision), F1 score, and accuracy (Accuracy) as evaluation indicators to accurately evaluate the effectiveness of the proposed method.\nThe calculation formulas for these values are as follows.\nRecall = TP\nTP + FN (6)\nPrecision = TP\nTP + FP (7)\nF1Score = P\u00d7 R\n2(P + R) (8)\nAccuracy = TP + TN\nTP + TN + FP + FN (9)\nIn the above formulas, TP represents the cases where the classifier correctly identified positive samples as positive, TN represents the cases where the classifier correctly identified negative samples as negative, FP represents the cases where the classifier incorrectly identified negative samples as positive, and FN represents the cases where the classifier incorrectly identified positive samples as negative.\n4.3. Analysis of the Training Results 4.3.1. The Result of CNN and ResNet\nAn ordinary CNN network was used to separately formulate decisions on the diameter, pulling speed, and temperature signals. We then obtained the decision result of a single signal. After, the ResNet network was used to obtain the results of the two-dimensional image data. Among the 3644 sets of data, there were 730 sets in the testing set.\nThe structure diagram of different inputs is shown in Figure 10.\nSensors 2023, 23, x FOR PEER REVIEW 12 of 18\nTP TNAccuracy TP N FP FN += + +\n(9)\nIn the above formulas, TP represents the cases where the classifier correctly identified positive samples as positive, N represents the cases where the cla sifier correctly identified negative samples a negative, FP represents the cas s w re the classifi r incorrectly identified negative s mples as positive, and FN represents t e cases where the classifier incorrectly identified positive samples as negative.\n4.3. Analysis of the Training Results 4.3.1. The Result of CNN and ResNet\nAn ordinary CNN network was used to separately formulate decisions on the diameter, pulling speed, and temperature signals. We then obtained the decision result of a single signal. After, the ResNet network was used to obtain the results of the two-dimensional image data. Among the 3644 sets of data, there were 730 sets in the testing set.\nThe structure diagram of different inputs is shown in Figure 10.\nOutput\nConv 64\nMax pool 32\nConv 32\nMax pool 64\nInput (T)\nOutput\nResnet\nInput (G)\nOutput\nConv 64\nMax pool 32\nConv 32\nMax pool 64\nInput (D)\nOutput\nConv 64\nMax pool 32\nConv 32\nMax pool 64\nInput (V)\nFigure 10. Structure diagram of different inputs.\nThe historical curves of the training set and the validation set are shown on the left of Figure 11 and the confusion matrixes of the testing sets are shown on the right of Figure 11. The calculated evaluation indicators are shown in Table 1.\n(a)\nFigure 10. Structure diagram of different inputs.\nSensors 2023, 23, 5855 12 of 18\nThe historical curves of the training set and the validation set are shown on the left of Figure 11 and the confusion matrixes of the testing sets are shown on the right of Figure 11. The calculated evaluation indicators are shown in Table 1.\nTable 1. Classification results for single-modal data without any fusion method.\nAccuracy Recall Precision F1 Score\nV Abnormal\n81.23% 72.78% 87.04% 79.27%\nNormal 89.46% 77.16% 82.85%\nT Abnormal\n74.11% 70.56% 75.37% 72.89%\nNormal 77.57% 73.03% 75.23%\nD Abnormal\n95.48% 94.44% 96.32% 95.37%\nNormal 96.49% 94.70% 95.58%\nG Abnormal\n67.12% 33.89% 98.39% 50.41%\nNormal 99.46% 60.73% 75.41%\nAs seen in Figure 11a\u2013d and Table 1, the model started to converge after 20 rounds of training. The highest accuracy rate of 95.48% was obtained by relying on the diameter signal. Combined with the F1 Score, the overall recognition rate and the classification effect were the best depending on the diameter signal. A reliance on pulling speed followed. Compared with the pulling speed signal, the diameter signal did not have a hysteresis effect on node loss; therefore, the recognition accuracy was higher, which coincided with the theory and practice. Due to the large hysteresis effect of temperature on node loss and as the crystal length continued to grow, the ability of the system to resist temperature disturbances continued to increase. Therefore, relying on a temperature classification achieved poor results, with an accuracy rate of only 74.11%. The effect of training with the ResNet network on the two-dimensional image data\nwas not ideal, with the lowest accuracy of only 67.12%. The model quickly converged, and\nthe training accuracy curve significantly fluctuated. From the Recall value, the recognition rate of the abnormal data was only 33.89% and the false detection rate was high. Compared with relying on manual inspections and identification on an industrial site, the effect was improved, but not by much. Objectively speaking, the low classification accuracy was mainly due to unclear image features.\nSensors 2023, 23, x FOR PEER REVIEW 12 of 18\nTP TNAccuracy TP TN FP FN += + + +\n(9)\nIn the above formulas, TP represents the cases where the classifier correctly identified positive samples as positive, TN represents the cases where the classifier correctly identified negative samples as negative, FP represents the cases where the classifier incorrectly identified negative samples as positive, and FN represents the cases where the classifier incorrectly identified positive samples as negative.\n4.3. Analysis of the Training Result 4.3.1. The Result of CNN and ResNet\nAn ordinary CNN network was used to separately formulate decisions on the diam-\neter, pulling speed, and temperature signals. We then obtained the decision result of a single signal. After, the ResNet network was used to obtain the results of the two-dimensional image data. Among the 3644 sets of data, there were 730 sets in the testing set.\nThe structure diagram of different inputs is shown in Figure 10.\nOutput\nConv 64\nMax pool 32\nConv 32\nMax pool 64\nInput (T)\nOutput\nR snet\nInput (G)\nOutput\nConv 64\nMax pool 32\nConv 32\nMax pool 64\nInput (D)\nOutput\nConv 64\nMax pool 32\nConv 32\nMax pool 64\nInput (V)\nFigure 10. Structure diagram of different inputs.\nThe historical curves of the training set and the valid ion et are shown on the left of Figure 11 and the confusion matrixes of the testi g sets are shown on th right of Figure 11. The calculated evaluation indicators are shown in Table 1.\nSensors 2023, 23, 5855 13 of 18 Sensors 2023, 23, x FOR PEER REVIEW 13 of 18\nTable 1. Classification results for single-modal data without any fusion method.\nAccuracy Recall Precision F1 Score\nV Abnormal 81.23% 72.78% 87.04% 79.27% Normal 89.46% 77.16% 82.85%\nT Abnormal\n74.11% 70.56% 75.37% 72.89%\nNormal 77.57% 73.03% 75.23%\nD Abnormal 95.48% 94.44% 96.32% 95.37% Normal 96.49% 94.70% 95.58%\nG Abnormal 67.12% 33.89% 98.39% 50.41% Normal 99.46% 60.73% 75.41%\n4.3.2. The Result of Method 1: ICAM CNN\nTo verify the effectiveness of the proposed ICAM-CNN method, it was compared with the concatenate method to directly splice the feature layers of the diameter, pulling speed, and temperature signals.\nSensors 2023, 23, 5855 14 of 18\nThe structure diagram of the different algorithms is shown in Figure 12.\nSensors 2023, 23, x FOR PEER REVIEW 14 of 18 As seen in Figure 11a\u2013d and Table 1, the model started to converge after 20 rounds of training. The highest accuracy rate of 95.48% was obtained by relying on the diameter signal. Combined with the F1 Score, the overall recognition rate and the classification effect were the best depending on the diameter signal. A reliance on pulling speed followed. Compared with the pulling speed signal, the diameter signal did not have a hysteresis effect on node loss; therefore, the recognition accuracy was higher, which coincided with the theory and practice. Due to the large hysteresis effect of temperature on node loss and as the crystal length continued to grow, the ability of the system to resist temperature disturbances continued to increase. Therefore, relying on a temperature classification achieved poor results, with an accuracy rate of only 74.11%.\nThe effect of training with the ResNet network on the two-dimensional image data was not ideal, with the lowest accuracy of only 67.12%. The model quickly converged, and the training accuracy curve significantly fluctuated. From the Recall value, the recognition rate of the abnormal data was only 33.89% and the false detection rate was high. Compared with relying on manual inspections and identification on an industrial site, the effect was improved, but not by much. Objectively speaking, the low classification accuracy was mainly due to unclear image features.\n4.3.2. The Result of Method 1: ICAM-CNN To verify the effectiveness of the proposed ICAM-CNN method, it was compared with the concatenate method to directly splice the feature layers of the diameter, pulling speed, and temperature signals.\nThe structure diagram of the different algorithms is shown in Figure 12.\nThe historical curves of the training set and the validation set are shown on the left of Figure 13 and the confusion matrixes on the testing set are shown on the right of Figure 13. The calculated evaluation indicators are shown in Table 2.\nff\nThe historical curves of the training set and the validation set are shown on the left of Figure 13 and the confusion matrixes on the testing set are shown on the right of Figure 13. The calculated evaluation indicators are shown in Table 2.\nSensors 2023, 23, x FOR PEER REVIEW 15 of 18\nFrom Figure 13 and Table 2, it could be observed that using the diameter, pulling speed, and temperature for multi-sensor mixed training was more effective and had a higher accuracy compared with using single diameter, temperature, and pulling speed data separately to formulate a node-loss decision. Table 2 shows that the accuracy of the direct fusion of multiple signals using the concatenate method was not as high as that of the ICAM-CNN network using ICAM for correlation fusion. The network with the attention mechanism had a fast convergence speed and could perform feature extraction and correlation fusion on different sensor data more effectively.\n4.3.3. The Result of Method 2: MMFN To further improve the accuracy of classification recognition, enhance the generali-\nzation ability of the model, and fully use the features of multiple types of data, we fused the features obtained by the ResNet and ICAM-CNN networks at the final feature layer, based on one-dimensional signal node-loss detection and two-dimensional image nodeloss detection. Then, the final multimodal data fusion results using MMFN were obtained.\nThe structure diagram of MMFN is shown in Figure 14.\nSensors 2023, 23, 5855 15 of 18\nFrom Figure 13 and Table 2, it could be observed that using the diameter, pulling speed, and temperature for multi-sensor mixed training was more effective and had a higher accuracy compared with using single diameter, temperature, and pulling speed data separately to formulate a node-loss decision. Table 2 shows that the accuracy of the direct fusion of multiple signals using the concatenate method was not as high as that of the ICAM-CNN network using ICAM for correlation fusion. The network with the attention mechanism had a fast convergence speed and could perform feature extraction and correlation fusion on different sensor data more effectively.\n4.3.3. The Result of Method 2: MMFN\nTo further improve the accuracy of classification recognition, enhance the generalization ability of the model, and fully use the features of multiple types of data, we fused the features obtained by the ResNet and ICAM-CNN networks at the final feature layer, based on one-dimensional signal node-loss detection and two-dimensional image node-loss detection. Then, the final multimodal data fusion results using MMFN were obtained.\nThe structure diagram of MMFN is shown in Figure 14.\nSensors 2023, 23, x FOR PEER REVIEW 16 of 18\nFigure 14. Structure diagram of MMFN.\nFigure 15 shows the training results using this approach. The calculated evaluation indicators are shown in Table 3.\nFigure 15. Results of the multimodal data fusion network.\nBased on the overall experimental results, an accuracy of 97.95% was achieved by using the ICAM-CNN network for single-modal fusion. By introducing image data features, an accuracy of 98.36% was achieved. Looking at the F1 Score, MMFN achieved a result of 98.31% on the testing set, which was more successful than the result obtained by ICAM-CNN and more successful than any result obtained using a single-dimensional signal.\n5. Conclusions Using machine-learning methods to identify CZ silicon single-crystal node-loss de-\ntection is a novel and challenging task. In this paper, the continuous wavelet transform was used to preprocess one-dimensional diameters, pulling speeds, and temperature signals during the crystal growth process. A one-dimensional signal fusion decision-making method ICAM-CNN was proposed, which achieved an accuracy rate of 97.75%. Compared with the accuracy rate of 95.48% obtained by the CNN for pure diameter signal\nFigure 15 shows the training results using this approach. The calculated evaluation indicators are shown in Table 3.\nSensors 2023, 23, 5855 16 of 18\nSensors 2023, 23, x FOR PEER REVIEW 16 of 18 Resnet\nInput (G)\nConcatenate Output Conv 64 Max pool 32 Conv 32 Max pool 64\nInput (D)\nConv 64 Max pool 32 Conv 32 Max pool 64\nInput (T)\nICAM Conv 64 Max pool 32 Conv 32 Max pool 64\nInput (V)\nFigure 14. Structure diagram of MMFN.\nFigure 15 shows the training results using this approach. The calculated evaluation indicators are shown in Table 3.\nFigure 15. Results of the multimodal data fusion network.\nTable 3. Evaluation indicators using a multimodal data fusion network.\nAccuracy Recall Precision F1 Score\nMMFN Abnormal 98.36% 97.22% 99.43% 98.31% Normal 99.46% 97.35% 98.40%\nBased on the overall experimental results, an accuracy of 97.95% was achieved by using the ICAM-CNN network for single-modal fusion. By introducing image data features, an accuracy of 98.36% was achieved. Looking at the F1 Score, MMFN achieved a result of 98.31% on the testing set, which was more successful than the result obtained by ICAM-CNN and more successful than any result obtained using a single-dimensional signal.\n5. Conclusions Using machine-learning methods to identify CZ silicon single-crystal node-loss detection is a novel and challenging task. In this paper, the continuous wavelet transform was used to preprocess one-dimensional diameters, pulling speeds, and temperature signals during the crystal growth process. A one-dimensional signal fusion decision-making method ICAM-CNN was proposed, which achieved an accuracy rate of 97.75%. Compared with the accuracy rate of 95.48% obtained by the CNN for pure diameter signal\nFigure 15. Results ultimodal dat fusion network.\nTable 3. Evaluatio ators using a multimodal data fusion network.\nAccuracy Recall Precision F1 Score\nMMFN Abnormal\n98.36% 97.22 99.43% 98.31%\nNormal 99.46% 97.35% 98.40%\nBased on the overall experimental results, an accuracy of 97.95% was achieved by using the ICAM-CNN network for single-modal fusion. By introducing image data features, an accuracy of 98.36% was achieved. Looking at the F1 Score, MMFN achieved a result of 98.31% on the testing set, which was more successful than the result obtained by ICAMCNN and more successful than any result obtained using a single-dimensional signal.\n5. Conclusions\nUsing machine-learning methods to identify CZ silicon single-crystal node-loss detection is a novel and challenging task. In this paper, the continuous wavelet transform was used to preprocess one-dimensional diameters, pulling speeds, and temperature signals during the crystal growth process. A one-dimensional signal fusion decision-making method ICAM-CNN was proposed, which achieved an accuracy rate of 97.75%. Compared with the accuracy rate of 95.48% obtained by the CNN for pure diameter signal recognition, the accuracy rate of 81.23% for pure pulling speed signal recognition, and the 74.11% accuracy rate for pure temperature signal recognition, this method achieved m re accurate results. On is basis, the im ge information captured by the two-dimensional image sensor was introduced and the one-dimensional signals were combined with the two-dimensional image data for mixed training to achieve multimodal data fusion decisions. On our selfmade dataset, the MMFN finally achieved an accuracy of 98.36%. Compared to the 67.12% accuracy achieved by using Resnet network recognition in pure two-dimensional meniscus images and the 97.95% accuracy achieved by using ICAM-CNN in one-dimensional signals fusion, it has improved and can verify the effectiveness of the method. Compared with the manual inspection at the production site, the method used in this paper is more accurate and has higher real-time performance, which can meet the real-time and accurate requirements in field production. The method is of great significance for improving the automation of single crystal furnaces, reducing the labor intensity of manual inspection, and preventing production accidents, and has practical industrial value. Since the data used in this paper are all collected in the industrial field, there is no need to arrange additional sensors, which have the conditions for implementation in the industrial field. In the future, based on the existing research, we plan to further enrich the node-loss dataset according to the field operation results, and constantly update the nodeloss detection method iteratively to improve the accuracy, generalization and robustness of the model. At the same time, we plan to introduce more variables related to the node-loss\nSensors 2023, 23, 5855 17 of 18\nto adapt to the crystal growth process with variable crucible rotation and even variable magnetic field strength. Finally, on the basis of the node-loss detection, we will consider the combination of data and mechanism to carry out the research on node-loss prediction.\nAuthor Contributions: Conceptualization, L.J.; Methodology, L.J. and R.X.; Software, R.X.; Validation, L.J.; Data Collection, L.J.; Writing\u2014Original Draft Preparation, L.J. and R.X.; Writing\u2014Review and Editing, L.J. and R.X.; Supervision, D.L.; Project Administration, D.L.; Funding Acquisition, D.L. All authors contributed to the article. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by the National Natural Science Foundation of China Major Scientific Research Instrument Development Project \u201cSemiconductor Silicon Single Crystal Growth Digital Twin and Quality Control System\u201d (Grant No. 62127809).\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: The data presented in this study are available on request from the corresponding author. The data are not publicly available due to legal considerations.\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Heywang, W.; Zaininger, K.H. Silicon: The semiconductor material. In Silicon: Evolution and Future of a Technology; Springer: Berlin/Heidelberg, Germany, 2004; pp. 25\u201342. 2. Zhang, G.D.; Zhai, S.Q.; Cui, H.W.; Liu, J.C. Study on Dislocation in Growing proless of Semiconductor Single Crystals. J. Synth. Cryst. 2007, 36, 301. 3. Yonenaga, I. Nitrogen effects on generation and velocity of dislocations in Czochralski-grown silicon. J. Appl. Phys. 2005, 98, 023517. [CrossRef] 4. Kajiwara, K.; Torigoec, K.; Harada, K.; Hourai, M.; Nishizawa, S.I. Oxygen concentration dependence of as-grown defect formation in nitrogen-doped Czochralski silicon single crystals. J. Cryst. Growth 2021, 570, 126236. [CrossRef] 5. Zhang, J.; Liu, H.; Cao, J.; Zhu, W.; Jin, B.; Li, W. A Deep Learning Based Dislocation Detection Method for Cylindrical Crystal Growth Process. Appl. Sci. 2020, 10, 7799. [CrossRef] 6. Bayoudh, K.; Knani, R.; Hamdaoui, F.; Mtibaa, A. A survey on deep multimodal learning for computer vision: Advances, trends, applications, and datasets. Vis. Comput. 2021, 37, 2939\u20132970. [CrossRef] 7. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, L.; Polosukhin, I. Attention is all you\nneed. In Proceedings of the Advances in Neural Information Processing Systems, Long Beach, CA, USA, 4\u20139 December 2017; pp. 6000\u20136010.\n8. Liu, D.; Zhao, X.G.; Zhao, Y. A review of growth process modeling and control of Czochralski silicon single crystal. Control Theory Appl. 2017, 34, 1\u201312. 9. Zhijun, P.; Xiufeng, J.; Feng, L. Growth problems analysis of \u03a6100 mmmsilicon single crystal. Semicond. Mag. 1998, 23, 20\u201323. 10. Dong, J.; Zhang, B.; Liu, J.; Kewei, Z.; Xiaobin, L.; Caixia, Z. Investigation on the growth problems in cz-si crystal. Mater. Rev. 2013, 27, 157\u2013159. 11. Choudhary, A.K.; Harding, J.A.; Tiwari, M.K. Data Mining in Manufacturing: A Review Based on the Kind of Knowledge. J. Intell. Manuf. 2009, 20, 501\u2013521. [CrossRef] 12. Jing, Z.; Ding, L.; Yue, Z. Finite Element Numerical Simulation and Control Parameter of Czochralski Silicon Monocrystal during Shoulder Growth Process. J. Synth. Cryst. 2013, 42, 58\u201364. 13. Du, J. Research on \u201cDrop Bud\u201d Prediction Method of Monocrystalline Silicon Equal Diameter Growth Process Based on Data Mining. Master\u2019s Thesis, Zhejiang University, Hangzhou, China, 2019. 14. Huadong, Z.; Xiaotong, Z.; Zengguo, T.; Xinge, L. Identification of Key Characteristic Parameters of Cz-Si Monocrystal during Shoulder Growth Process Based on MIC. J. Synth. Cryst. 2020, 49, 607\u2013612. 15. Kankar, P.K.; Sharma, S.C.; Harsha, S.P. Fault diagnosis of ball bearings using machine learning methods. Expert. Syst. Appl. 2011, 38, 1876\u20131886. [CrossRef] 16. Zhang, Y.; Ji, J.; Ma, B. Fault diagnosis of reciprocating compressor using a novel ensemble empirical mode decompositionconvolutional deep belief network. Measurement 2020, 156, 107619. [CrossRef] 17. Yang, H.; Cheng, Y.; Li, G. A denoising method for ship radiated noise based on Spearman variational mode decomposition,\nspatial-dependence recurrence sample entropy, improved wavelet threshold denoising, and Savitzky-Golay filter. Alex. Eng. J. 2021, 60, 3379\u20133400. [CrossRef]\n18. Choudhary, A.; Goyal, D.; Letha, S.S. Infrared thermography-based fault diagnosis of induction motor bearings using machine learning. IEEE Sens. J. 2020, 21, 1727\u20131734. [CrossRef]\nSensors 2023, 23, 5855 18 of 18\n19. LeCun, Y.; Bottou, L.; Bengio, Y.; Haffner, P. Gradient-based learning applied to document recognition. Proc. IEEE 1998, 86, 2278\u20132324. [CrossRef] 20. Hinton, G.E.; Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science 2006, 313, 504\u2013507. [CrossRef] [PubMed] 21. Vincent, P.; Larochelle, H.; Bengio, Y.; Manzagol, P.A. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th International Conference on Machine Learning, Helsinki, Finland, 5\u20139 July 2008; pp. 1096\u20131103. 22. Krizhevsky, A.; Sutskever, I.; Hinton, G.E. Imagenet classification with deep convolutional neural networks. Commun. ACM 2017, 60, 84\u201390. [CrossRef] 23. Simonyan, K.; Zisserman, A. Very deep convolutional networks for large-scale image recognition. arXiv 2014, arXiv:1409.1556. 24. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Las Vegas, NV, USA, 27\u201330 June 2016; pp. 770\u2013778. 25. Patil, R.R.; Kumar, S. Rice-Fusion: A Multimodality Data Fusion Framework for Rice Disease Diagnosis. IEEE Access 2022, 10, 5207\u20135222. [CrossRef] 26. Xiang, Z.; Zhuo, Q.; Zhao, C.; Deng, X.; Zhu, T.; Wang, T.; Jiang, W.; Lei, B. Self-supervised multi-modal fusion network for multi-modal thyroid ultrasound image diagnosis. Comput. Biol. Med. 2022, 150, 106164. [CrossRef] [PubMed] 27. Han, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; Wang, Y. Transformer in transformer. Adv. Neural Inf. Process. 2021, 34, 15908\u201315919. 28. Eslami, E.; Yun, H.B. Attention-based multi-scale convolutional neural network (A+ MCNN) for multi-class classification in road images. Sensors 2021, 21, 5137. [CrossRef] 29. Cao, Z.; Yang, H.; Zhao, J.; Guo, S.; Li, L. Attention fusion for one-stage multispectral pedestrian detection. Sensors 2021, 21, 4184. [CrossRef] [PubMed] 30. Ye, Y.; Ren, X.; Zhu, B.; Tang, T.; Tan, X.; Gui, Y.; Yao, Q. An adaptive attention fusion mechanism convolutional network for object detection in remote sensing images. Remote Sens. 2022, 14, 516. [CrossRef] 31. Wang, J.; Huang, R.; Guo, S.; Li, L.; Zhu, M.; Yang, S.; Jiao, L. NAS-guided lightweight multiscale attention fusion network for hyperspectral image classification. IEEE Trans. Geosci. Remote Sens. 2021, 59, 8754\u20138767. [CrossRef] 32. Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv 2014, arXiv:1412.6980.\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Node-Loss Detection Methods for CZ Silicon Single Crystal Based on Multimodal Data Fusion",
    "year": 2023
}