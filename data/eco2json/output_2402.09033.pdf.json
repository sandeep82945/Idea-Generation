{
    "abstractText": "Platform businesses operate on a digital core and their decision making requires high-dimensional accurate forecast streams at different levels of cross-sectional (e.g., geographical regions) and temporal aggregation (e.g., minutes to days). It also necessitates coherent forecasts across all levels of the hierarchy to ensure aligned decision making across different planning units such as pricing, product, controlling and strategy. Given that platform data streams feature complex characteristics and interdependencies, we introduce a non-linear hierarchical forecast reconciliation method that produces cross-temporal reconciled forecasts in a direct and automated way through the use of popular machine learning methods. The method is sufficiently fast to allow forecast-based high-frequency decision making that platforms require. We empirically test our framework on a unique, large-scale streaming dataset from a leading on-demand delivery platform in Europe.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jeroen Rombouts"
        },
        {
            "affiliations": [],
            "name": "Marie Ternes"
        },
        {
            "affiliations": [],
            "name": "Ines Wilms"
        }
    ],
    "id": "SP:ac83ad28fb0893920c00a873ffa4138177225254",
    "references": [
        {
            "authors": [
                "M. Abolghasemi",
                "G. Tarr",
                "C. Bergmeir"
            ],
            "title": "Machine learning applications in hierarchical time series forecasting: Investigating the impact of promotions",
            "venue": "International Journal of Forecasting, forthcoming.",
            "year": 2022
        },
        {
            "authors": [
                "M. Anderer",
                "F. Li"
            ],
            "title": "Hierarchical forecasting with a top-down alignment of independent-level forecasts",
            "venue": "International Journal of Forecasting, 38(4):1405\u20131414.",
            "year": 2022
        },
        {
            "authors": [
                "G. Athanasopoulos",
                "R.A. Ahmed",
                "R.J. Hyndman"
            ],
            "title": "Hierarchical forecasts for Australian domestic tourism",
            "venue": "International Journal of Forecasting, 25(1):146\u2013166.",
            "year": 2009
        },
        {
            "authors": [
                "G. Athanasopoulos",
                "R.J. Hyndman",
                "N. Kourentzes",
                "A. Panagiotelis"
            ],
            "title": "Forecast reconciliation: A review",
            "venue": "International Journal of Forecasting,",
            "year": 2023
        },
        {
            "authors": [
                "G. Athanasopoulos",
                "R.J. Hyndman",
                "N. Kourentzes",
                "A. Panagiotelis"
            ],
            "title": "Editorial: Innovations in hierarchical forecasting",
            "venue": "International Journal of Forecasting, forthcoming.",
            "year": 2024
        },
        {
            "authors": [
                "G. Athanasopoulos",
                "R.J. Hyndman",
                "N. Kourentzes",
                "F. Petropoulos"
            ],
            "title": "Forecasting with temporal hierarchies",
            "venue": "European Journal of Operational Research, 262(1):60\u201374.",
            "year": 2017
        },
        {
            "authors": [
                "B. Bischl",
                "M. Lang",
                "L. Kotthoff",
                "J. Schiffner",
                "J. Richter",
                "E. Studerus",
                "G. Casalicchio",
                "Z.M. Jones"
            ],
            "title": "mlr: Machine learning in R",
            "venue": "Journal of Machine Learning Research, 17(170):1\u20135.",
            "year": 2016
        },
        {
            "authors": [
                "L. Breiman"
            ],
            "title": "Random forests",
            "venue": "Machine Learning, 45:5\u201332.",
            "year": 2001
        },
        {
            "authors": [
                "M. Caporin",
                "T. Di Fonzo",
                "D. Girolimetto"
            ],
            "title": "Exploiting intraday decompositions in realized volatility forecasting: A forecast reconciliation approach",
            "venue": "arXiv preprint arXiv:2306.02952.",
            "year": 2023
        },
        {
            "authors": [
                "T. Chen",
                "P.C. Guestrin"
            ],
            "title": "XGBoost: A scalable tree boosting system",
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New York: ACM, 0(0):785\u2013794.",
            "year": 2016
        },
        {
            "authors": [
                "T. Chen",
                "T. He",
                "M. Benesty",
                "V. Khotilovich",
                "Y. Tang",
                "H. Cho",
                "K. Chen",
                "R. Mitchell",
                "I. Cano",
                "T. Zhou",
                "M. Li",
                "J. Xie",
                "M. Lin",
                "Y. Geng",
                "Y. Li",
                "J. Yuan"
            ],
            "title": "xgboost: Extreme Gradient Boosting",
            "venue": "R package version 1.7.6.1.",
            "year": 2023
        },
        {
            "authors": [
                "T. Di Fonzo",
                "D. Girolimetto"
            ],
            "title": "Cross-temporal forecast reconciliation",
            "year": 2023
        },
        {
            "authors": [
                "T. Di Fonzo",
                "D. Girolimetto"
            ],
            "title": "Spatio-temporal reconciliation of solar forecasts",
            "venue": "Solar Energy, 251:13\u201329.",
            "year": 2023
        },
        {
            "authors": [
                "B. Efron",
                "T. Hastie"
            ],
            "title": "Computer Age Statistical Inference",
            "venue": "Cambridge University Press, Cambridge.",
            "year": 2016
        },
        {
            "authors": [
                "G. Elliott",
                "A. Timmermann"
            ],
            "title": "Economic Forecasting",
            "venue": "Princeton University Press, Princeton, New Jersey.",
            "year": 2016
        },
        {
            "authors": [
                "J.H. Friedman"
            ],
            "title": "Greedy function approximation: A gradient boosting machine",
            "venue": "The Annals of Statistics, 29(5):1189\u20131232.",
            "year": 2001
        },
        {
            "authors": [
                "D. Girolimetto",
                "T. Di Fonzo"
            ],
            "title": "FoReco: Point Forecast Reconciliation",
            "venue": "R package version 2.6.",
            "year": 2023
        },
        {
            "authors": [
                "R. Hollyman",
                "F. Petropoulos",
                "M.E. Tipping"
            ],
            "title": "Understanding forecast reconciliation",
            "venue": "European Journal of Operational Research, 294(1):149\u2013160.",
            "year": 2021
        },
        {
            "authors": [
                "R. Hyndman",
                "G. Athanasopoulos",
                "C. Bergmeir",
                "G. Caceres",
                "L. Chhay",
                "M. O\u2019Hara-Wild",
                "F. Petropoulos",
                "S. Razbash",
                "E. Wang",
                "F. Yasmeen"
            ],
            "title": "forecast: Forecasting functions for time series and linear models. R package version 8.21.1",
            "year": 2023
        },
        {
            "authors": [
                "R.J. Hyndman",
                "R.A. Ahmed",
                "G. Athanasopoulos",
                "H.L. Shang"
            ],
            "title": "Optimal combination forecasts for hierarchical time series",
            "venue": "Computational Statistics & Data Analysis, 55(9):2579\u20132589.",
            "year": 2011
        },
        {
            "authors": [
                "R.J. Hyndman",
                "G. Athanasopoulos"
            ],
            "title": "Optimally reconciling forecasts in a hierarchy",
            "venue": "Foresight: The International Journal of Applied Forecasting,",
            "year": 2014
        },
        {
            "authors": [
                "R.J. Hyndman",
                "G. Athanasopoulos"
            ],
            "title": "Forecasting: principles and practice",
            "venue": "3rd. Melbourne, Australia: OTexts.",
            "year": 2021
        },
        {
            "authors": [
                "R.J. Hyndman",
                "Y. Khandakar"
            ],
            "title": "Automatic time series forecasting: The forecast package for R",
            "venue": "Journal of Statistical Software, 26(3):1\u201322.",
            "year": 2008
        },
        {
            "authors": [
                "R.J. Hyndman",
                "A.B. Koehler"
            ],
            "title": "Another look at measures of forecast accuracy",
            "venue": "International Journal of Forecasting, 22(4):679\u2013688.",
            "year": 2006
        },
        {
            "authors": [
                "R.J. Hyndman",
                "A.B. Koehler",
                "R.D. Snyder",
                "S. Grose"
            ],
            "title": "A state space framework for automatic forecasting using exponential smoothing methods",
            "venue": "International Journal of forecasting, 18(3):439\u2013454.",
            "year": 2002
        },
        {
            "authors": [
                "R.J. Hyndman",
                "A.J. Lee",
                "E. andWang"
            ],
            "title": "Fast computation of reconciled forecasts for hierarchical and grouped time series",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2016
        },
        {
            "authors": [
                "T. Januschowski",
                "Y. Wang",
                "K. Torkkola",
                "T. Erkkil\u00e4",
                "H. Hasson",
                "J. Gasthaus"
            ],
            "title": "Forecasting with trees",
            "venue": "International Journal of Forecasting, 38(4):1473\u20131481.",
            "year": 2022
        },
        {
            "authors": [
                "K. Komisarczyk",
                "P. Kozminski",
                "S. Maksymiuk",
                "P. Biecek"
            ],
            "title": "treeshap: Compute SHAP values for your tree-based models using the \u2018TreeSHAP\u2019 algorithm",
            "venue": "R package version 0.2.5.",
            "year": 2023
        },
        {
            "authors": [
                "A.J. Koning",
                "P.H. Franses",
                "M. Hibon",
                "H. Stekler"
            ],
            "title": "The M3 competition: Statistical tests of the results",
            "venue": "International Journal of Forecasting, 21(3):397\u2013409.",
            "year": 2005
        },
        {
            "authors": [
                "N. Kourentzes",
                "G. Athanasopoulos"
            ],
            "title": "Cross-temporal coherent forecasts for Aus",
            "year": 2019
        },
        {
            "authors": [
                "M. Wiener"
            ],
            "title": "Classification and regression by randomforest",
            "venue": "tralian tourism. Annals of Tourism Research,",
            "year": 2002
        },
        {
            "authors": [
                "S.M. Lundberg",
                "G.G. Erion",
                "Lee",
                "S.-I"
            ],
            "title": "Consistent individualized feature",
            "year": 2018
        },
        {
            "authors": [
                "S.M. Lundberg",
                "Lee",
                "S.-I"
            ],
            "title": "A unified approach to interpreting model predictions",
            "year": 2017
        },
        {
            "authors": [
                "C. Molnar"
            ],
            "title": "Interpretable Machine Learning",
            "venue": "2 edition.",
            "year": 2022
        },
        {
            "authors": [
                "A. Panagiotelis",
                "G. Athanasopoulos",
                "P. Gamakumara",
                "R.J. Hyndman"
            ],
            "title": "Forecast reconciliation: A geometric view with new insights on bias correction",
            "venue": "International Journal of Forecasting, 37(1):343\u2013359.",
            "year": 2021
        },
        {
            "authors": [
                "F. Petropoulos",
                "D. Apiletti",
                "V. Assimakopoulos",
                "M.Z. Babai",
                "D.K. Barrow",
                "S.B. Taieb",
                "C. Bergmeir",
                "R.J. Bessa",
                "J. Bijak",
                "Boylan",
                "J. E"
            ],
            "title": "Forecasting: theory and practice",
            "venue": "International Journal of Forecasting,",
            "year": 2022
        },
        {
            "authors": [
                "R Core Team"
            ],
            "title": "R: A Language and Environment for Statistical Computing",
            "venue": "R Foundation for Statistical Computing, Vienna, Austria.",
            "year": 2023
        },
        {
            "authors": [
                "B. Rostami-Tabar",
                "R.J. Hyndman"
            ],
            "title": "Hierarchical time series forecasting in emergency medical services",
            "venue": "Journal of Service Research, forthcoming.",
            "year": 2024
        },
        {
            "authors": [
                "Shapley",
                "L. S"
            ],
            "title": "A value for n-person games",
            "venue": "Contributions to the theory of games, 2:307\u2013317.",
            "year": 1953
        },
        {
            "authors": [
                "Y. Shi",
                "G. Ke",
                "D. Soukhavong",
                "J. Lamb",
                "Q. Meng",
                "T. Finley",
                "T. Wang",
                "W. Chen",
                "W. Ma",
                "Q. Ye",
                "Liu",
                "T.-Y.",
                "N. Titov"
            ],
            "title": "lightgbm: Light Gradient Boosting Machine",
            "venue": "R package version 3.3.5.",
            "year": 2023
        },
        {
            "authors": [
                "E. Spiliotis",
                "M. Abolghasemi",
                "R.J. Hyndman",
                "F. Petropoulos",
                "V. Assimakopoulos"
            ],
            "title": "Hierarchical forecast reconciliation with machine learning",
            "venue": "Applied Soft Computing, 112:107756.",
            "year": 2021
        },
        {
            "authors": [
                "F. Theodosiou",
                "N. Kourentzes"
            ],
            "title": "Forecasting with deep temporal hierarchies. Available at SSRN 3918315",
            "year": 2021
        },
        {
            "authors": [
                "T. Van Erven",
                "J. Cugliari"
            ],
            "title": "Game-theoretically optimal reconciliation of contemporaneous hierarchical time series forecasts",
            "venue": "Modeling and stochastic learning for forecasting in high dimensions, pages 297\u2013317. Springer.",
            "year": 2015
        },
        {
            "authors": [
                "S. Wang",
                "F. Zhou",
                "Y. Sun",
                "L. Ma",
                "J. Zhang",
                "Y. Zheng"
            ],
            "title": "End-to-end modeling of hierarchical time series using autoregressive transformer and conditional normalizing flowbased reconciliation",
            "venue": "2022 IEEE International Conference on Data Mining Workshops (ICDMW), pages 1087\u20131094. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "S.L. Wickramasuriya",
                "G. Athanasopoulos",
                "R.J. Hyndman"
            ],
            "title": "Optimal forecast reconciliation for hierarchical and grouped time series through trace minimization",
            "venue": "Journal of the American Statistical Association, 114(526):804\u2013819.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Yan"
            ],
            "title": "rBayesianOptimization: Bayesian Optimization of Hyperparameters",
            "venue": "R package version 1.2.0.",
            "year": 2021
        },
        {
            "authors": [],
            "title": "2021) and use the package mlr in R (Bischl et al., 2016",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Keywords: Hierarchical time series, Forecast reconciliation, Machine learning, Cross-temporal aggregation, Demand forecasting, Platform econometrics\n\u2217Correspondence to Ines Wilms, Maastricht University, School of Business and Economics, P.O. Box 616, 6200 MD Maastricht, The Netherlands, Email: i.wilms@maastrichtuniversity.nl. We are very grateful to Benjamin Wolter, Pablo Perez Piskunow, Roger Caminal and Afonso Rodrigues for expert advice, to Rob Hyndman, Artem Prokhorov and Roberto Reno\u0300 for comments provided on earlier versions of the paper, and to the participants of the IMS International Conference on Statistics and Data Science (ICSDS 2023) for helpful discussions. The last author was financially supported by the Dutch Research Council (NWO) under grant number VI.Vidi.211.032.\nar X\niv :2\n40 2.\n09 03\n3v 1\n[ ec\non .E"
        },
        {
            "heading": "1 Introduction",
            "text": "Time series to be forecasted are oftentimes naturally part of a hierarchical structure, where higher frequency and granular series are added together to form lower frequency aggregated series. Separate forecasts of each series rarely conserve this hierarchy, and forecast reconciliation methods are therefore required. In this paper, we consider novel forecast reconciliation methods for on-demand delivery platforms that require forecasts at different levels of cross-sectional and temporal aggregation. We introduce non-linear forecast reconciliation based on popular machine learning methods to capture the complex interdependencies of platform data.\nPlatforms such as Uber, Lyft, GrubHub, UberEats or DoorDash are nowadays omnipresent in the global economy. They operate in high frequency and on a large number of verticals (regions, product categories, etc.). Indeed, their market place is typically split up in different geographical regions, so a natural cross-sectional aggregation scheme arises from many individual delivery areas over zones towards few market places. Moreover, a temporal aggregation scheme naturally arises since, on the granular end, fast operational decisions (think in terms of minutes) are needed to ensure the platform\u2019s service couriers are at the right time and location to serve consumer demand promptly and to determine compensation schemes for couriers through dynamic pricing. On the coarser end, strategic business decisions also require long-term planning since the budget available for each delivery area is set typically using daily demand forecasts. Accurate and coherent, i.e. reconciled, demand forecasts across all levels of the cross-sectional and temporal hierarchy are therefore key to the business\u2019 success and to support aligned decision making across different planning units.\nThe literature to reconcile forecasts has been pioneered by Hyndman et al. (2011) and Athanasopoulos et al. (2009), see Hyndman and Athanasopoulos (2021) for a textbook introduction and Hyndman and Athanasopoulos (2014) for an introduction to practitioners given the strong interest from industry. Typical initial applications concern tourism where demand series are subject to a cross-sectional hierarchy across different geographical regions, though many other cases appear more recently, see for instance Caporin et al. (2023) for a financial application to realized volatility forecasting. While forecast reconciliation methods initially focused on producing coherent forecasts across cross-sectional hierarchies, the literature on forecast reconciliation in the temporal direction (Athanasopoulos et al., 2017) as well as combinations of both in cross-temporal frameworks (Kourentzes and Athanasopoulos, 2019; Di Fonzo and Girolimetto, 2023a,b) is much sparser (Petropoulos et al., 2022) though in recent years it has proliferated. We refer to Athanasopoulos et al. (2023) for an extensive review on forecast reconciliation in cross-sectional, temporal and cross-temporal hierarchies, to Hollyman et al. (2021) for the link with the forecast combination literature and Athanasopoulos et al. (2024) for recent innovations in hierarchical forecasting.\nMethodological considerations to forecast reconciliations are discussed in Hyndman et al. (2016), whereas theoretical results are first established by Wickramasuriya et al. (2019) by putting forward an optimal linear forecast reconciliation method, minimizing the sum of forecast error variances across the collection of time series; see also Van Erven and Cugliari (2015) for a game-theoretic approach to forecast reconciliation and Panagiotelis et al. (2021) for a unification of the former two works. Traditionally, forecast reconciliation is thus typically achieved by linearly combining base forecasts of all the time series in the hierarchy. Recently, however, applications of machine learning algorithms surface to reconcile forecasts in a non-linear way.\nMachine learning (ML) algorithms nowadays have become an integral component of every forecaster\u2019s toolkit, boasting a remarkable track record as demonstrated by various forecasting competitions, such as the M4 and M5 competitions (Makridakis et al., 2020, 2022b), and successful economic applications (e.g., Medeiros et al., 2021). ML methods have become standard choices for obtaining forecasts as their algorithms demand minimal user intervention\u2013 typically limited to tuning parameters \u2013and their sophisticated off-the-shelf implementations are readily available, facilitating seamless operationalization in production environments. Moreover, ML methods offer flexibility by imposing no constraints on the number of features for a given target, and they come with built-in mechanisms for feature selection and for capturing complex dependencies such as non-linearities.\nIn the forecast reconciliation literature, a cross-sectional ML-based reconciliation scheme is proposed by Spiliotis et al. (2021), on which our work is mainly built. They first collect base forecasts for all series in the cross-sectional hierarchy and then train a tree-based ML algorithm (random forest or XGBoost) with the bottom level series as response and one-step-ahead forecasts of all the time series in the hierarchy as input features. The ML fitted algorithm is hereby used to reconcile the bottom level forecasts in a non-linear way, given all base forecasts, and coherent forecasts across the complete hierarchy are obtained via bottom-up aggregation. In contrast to standard linear reconciliation approaches that reconcile forecasts by minimizing the reconciliation error between the base and reconciled forecasts, the ML-based reconciliation approach is directly geared towards obtaining the minimum forecast combination error to improve out-of-sample forecast accuracy while still adhering to coherence. This results in substantial forecast improvements as demonstrated on two applications to tourism and retail industry. Additionally, ML-based methods are more flexible than the standard ones as they allow for non-linear combinations of the base\nforecasts and selectively combine them in a direct and automated way without requiring that the information in the entire hierarchy must be used for producing reconciled forecasts.\nAnderer and Li (2022) use deep learning and LightGBM tree-based methods to propose a variant of the bottom-up method proposed by Spiliotis et al. (2021), accounting for biases that are difficult to observe at the bottom level. The latter forecasts are adjusted to obtain a higher forecasting accuracy on the upper levels of the hierarchy. Their approach works well on the M5 competition dataset consisting of over ten thousand hierarchical retail sales time series (see Makridakis et al., 2022c and Makridakis et al., 2022a for details). Abolghasemi et al. (2022) use tree-based methods and lasso regression in their dynamic top-down bottom-up method applied to fast-moving consumer goods and the M5 competition dataset. Finally, a new stream of research uses neural networks for both forecasting and reconciliation; examples of such end-to-end modeling approaches are Theodosiou and Kourentzes (2021) and Wang et al. (2022), which bring promising evidence on various datasets.\nOur paper also proposes an ML-based reconciliation approach, extending the work of Spiliotis et al. (2021) in the following ways. From a methodological point of view, we generalize their ML-based forecast reconciliation method for cross-sectional hierarchies to crosstemporal hierarchies. Additionally considering reconciliation across the temporal direction requires important design choices with respect to the ML model used for reconciliation since the inputs consist of mixed-frequency time series, namely all temporal frequencies in the temporal hierarchy. Besides, since the ML algorithm is a key backbone in our forecast reconciliation approach, we carefully investigate the effect of (i) the ML algorithm used, thereby considering not only random forest and XGBoost as in Spiliotis et al. (2021) but also LightGBM due to its successful application in the M5 competition; and (ii) tuning hyperparameters to make careful considerations on the trade-off between improvements in\nforecast accuracy and computational time. Finally, we consider an extensive set of base forecasts, ranging from academic standards to industry standards (for our platform application) as well as a combination of several base forecasts methods.\nFrom an application viewpoint, we showcase the performance of our forecast reconciliation method on platform demand data streams, thereby bringing novel insights into the performance of forecast reconciliation methods on a modern and unique dataset. We use UK demand data (September 2022-September 2023) from a last mile logistics platform where the bottom level series are 30-minute demand series for 141 delivery areas in London and in Manchester, their two largest markets. We aggregate to hourly and daily frequency in the temporal dimension, and towards zone and market level in the cross-sectional dimension. Platform data have several features that are distinct from the traditional datasets considered in the forecast reconciliation literature. First, they are high-frequency with, our top, hence lowest frequency, level (i.e. daily) consisting of the bottom level considered in earlier work such as recent M5 competition (other works even consider lower frequency settings such as monthly to yearly data). Indeed, computational speed and fast decision making at digital platforms are key to their success, thereby requiring forecasts at the intra-day level. A second distinctive feature of platform data is their streaming character which means that they face non-stationarities or shifts for example due to expansion, competition among platforms, sales of certain clients, changing customer\u2019s preferences or non-regular events such as sports events. The data on Manchester highlight the performance of the ML forecast reconciliation method in presence of data shifts. We compare our approach with state-of-the art reconciliation benchmarks from the recent literature.\nWe obtain several insights based on our application. First, our main empirical finding is that ML-based forecast reconciliation for platform data can result in substantial forecast\naccuracy improvements compared to existing linear reconciliation methods. However, MLbased reconciliation is not uniformly superior to linear methods. In fact, for the most important series, i.e. high frequency delivery area level, random forest based reconciliation typically performs best, for city level series LightGBM based reconciliation yields the smallest forecast errors. On the other hand, linear reconciliation methods outperform XGBoost based reconciliation on the bottom level series. The latter result hold for XGBooost without tuning its hyperparameters to keep computing time low. However, ignoring the huge computational cost, we also study the impact of hyperparameter tuning of the ML algorithms and find for example that XGBoost performs in this case at par with random forest in the lowest levels of the hierarchy. A second empirical finding is that ML-based reconciliation seems to work equally well irrespective of the quality of the base forecasts. Hence, exploring a large model space for obtaining base forecasts is unnecessary. Finally, data streams are prone to data shifts so we compare the forecast reconciliation methods on the Manchester market that is impacted by a major shift in our evaluation period. The results show that, while forecast performance of all methods suffer during a data shift, ML-based reconciliation is able to revert quickly back to pre-shift forecast accuracies.\nThe rest of the paper is organized as follows. Section 2 introduces the platform data we analyze in this paper. Section 3 presents our ML-based forecast reconciliation approach for cross-temporal hierarchies. Section 4 describes the forecast set-up we use to evaluate the out-of-sample forecast performance of our ML-based reconciliation approach versus state-ofthe-art benchmarks whereas Section 5 presents the results. Finally, Section 6 concludes."
        },
        {
            "heading": "2 Platform Data",
            "text": "We use a unique dataset of demand for deliveries from a leading on-demand last-mile logistics platform in Europe, Stuart, which connects businesses to a fleet of geographically independent couriers.1 We analyze UK London demand 30-minute frequency data from September 5, 2022 to September 24, 2023.2 To efficiently organize parcel deliveries, London is split into 18 zones which are constituted by the 117 bottom-level delivery areas. Planning at the delivery area level is of particular importance because it determines the remuneration paid to the couriers and and it therefore directly helps to optimize the platform\u2019s efficiency.\nDeliveries are carried out on a daily basis between 7am and 11:30pm in all areas.3 This creates a balanced dataset of 13, 090 observations for each of the 136 (=1+18+117) time series at the 30-minute frequency. For business planning purposes, the series also require temporarily aggregation to the hourly and the daily frequency, thereby resulting in temporally aggregated time series that consist of 13, 090/2 = 6, 545 and 13, 090/34 = 385 observations respectively. The forecast reconciliation therefore consists of three cross-sectional and three temporal layers implying 408 base time series.\nFigure 1 plots demand for the City of London delivery area, London Central zone, and London market at 30-minute, hourly and daily frequencies over the sample period. We observe some typical characteristics of platform data. First, intra-day area data is spiky because of peak demand periods triggered by events such as flash sales, food promotions, bad weather. These peaks are largely tempered at daily frequency. Holidays can also cause large\n1The data are provided to us in the context of ongoing research collaborations. Due to a confidentiality agreement, we are not allowed to distribute or report actual demand data. Demand data across all figures are therefore normalized between 0-100. All analyses were carried out with the original data.\n2In addition to the London market, we also analyze the case of Manchester in Section 5.4. 3Occasional overnight demand is integrated in the last 30-min of the business day.\ndrops in delivery demand, this is particularly noticeable in the end-of-year period. Second, there are strong seasonality patterns at all levels of the hierarchy. Figure A.1 in Appendix A displays these seasonality patterns. We clearly see that the intra-day seasonality (visible from both the 30-minute and hourly time series) is mainly driven by food deliveries. The bottom row of the plots traces daily seasonality, and illustrates that there can be substantial differences between weekends and the rest of the week. In general, London faces higher weekend demand while the opposite is true for the delivery area City of London. This implies that there exists heterogeneity in demand patterns at the delivery area level. In fact, unreported plots for other delivery areas reveal that very different dynamics appear.\nFigure 2 illustrates this heterogeneity across areas. In the left panel, we visualize the\naggregated demand over the sample period for each delivery area. Apart from larger activity above River Thames, there is no specific spatial pattern in demand volume, low- and highvolume delivery areas co-exist. The largest volume area is Romford, outside the city center. The City of London and Westminster area volumes respectively represent about 15% and 5% of Romford. In the right top panel of Figure 2, we plot the 30-minute average demand throughout the business day for each delivery area. The solid black median line visualizes well the lunch and dinner periods, the colored area curves stress the large variability, except for early morning demand which is overall low. The right bottom panel of Figure 2 zooms into the proportion of zeros across areas. Between 10am and 9pm, the proportion of 30- minute zero demand spells is low for most delivery areas, Stirling Road (upper green curve) however has mostly zero demand throughout the entire business day. Large diversity appears in the morning and evening hours with delivery areas that are idle and others that are active."
        },
        {
            "heading": "3 Machine Learning Based Forecast Reconciliation",
            "text": "In Section 3.1, we review the cross-temporal framework using the general notation from Athanasopoulos et al. (2023). In Section 3.2 we introduce our ML-based forecast reconciliation approach for cross-temporal hierarchies."
        },
        {
            "heading": "3.1 Cross-Temporal Hierarchies",
            "text": "We start with some general notation so that our methodology of the next section can be widely applied. We denote yb,t for b = 1, . . . , nb and t > 0 the high-frequency time series observation for bottom level b at time t. The cross-sectional hierarchy is simply obtained by summing over elements in the set of the nb bottom level time series. This yields na additional high-frequency time series resulting in a total of n = na + nb series.\nRegarding the temporal hierarchy, note that in a platform setting data is streaming but for simplicity let us consider t = 1, . . . ,m with m the number of observations to sum over when going from the highest to the lowest sampling frequency in the hierarchy. If we define k = k1, ..., kp the p temporal aggregation levels, with k1 = 1 and kp = m, then the nonoverlapping temporally aggregated series, for each series i = 1, . . . , n and k = k1, ..., kp, are given by\nx [k] i,j = jk\u2211 t=(j\u22121)k+1 yi,t j = 1, ...,m/k.\nNext, denoting \u03c4 as the observation index of the lowest frequency (top temporal level), we can stack these observations in (mk \u00d7 1) vectors x[k]i,\u03c4 = [ x [k] i,mk(\u03c4\u22121)+1, x [k] i,mk(\u03c4\u22121)+2, . . . , x [k] i,mk\u03c4 ]\u22a4 , where \u22a4 denotes the transpose of a vector and mk = m/k.\nFinally, the cross-temporal hierarchy is obtained by considering for each variable i =\n1, . . . , n of the cross-sectional hierarchy, the entire temporal hierarchy. Notation wise, we collect in a ( \u2211p l=1 mkl\u00d71) vector all temporally aggregated time series xi,\u03c4 = [ x [m] i,\u03c4 , . . . ,x [1] i,\u03c4 \u22a4]\u22a4 ,\nand then stacking all n variables into the vector x\u03c4 = [ x\u22a41,\u03c4 , . . . ,x \u22a4 n,\u03c4 ]\u22a4 .\nIn our application, regarding the cross-sectional hierarchy the bottom-level consists of nb = 117 delivery areas that are aggregated into the upper levels consisting of 18 zones (middle) and finally one market (top level), as visualized in the left panel of Figure 3. The total number of variables is thus n = 117+18+1 = 136. Regarding the temporal hierarchy, we have three frequencies p = 3, (30-min - hourly - daily), as visualized in the middle panel of Figure 3, m = 34 and k1 = 1, k2 = 2 and k3 = 34 so that x [1] i,\u03c4 (x [2] i,\u03c4 ) collects the m1 = 34 (m2 = 17) 30-min (hourly) demand observations on day \u03c4 , and x [34] i,\u03c4 corresponds to the single daily observation for variable i. Finally, the combined cross-temporal hierarchy is displayed in the right panel of Figure 3."
        },
        {
            "heading": "3.2 Forecast Reconciliation via Machine Learning",
            "text": "We are now ready to introduce our machine learning-based forecast reconciliation approach for cross-temporal hierarchies, thereby extending the work of Spiliotis et al. (2021) for cross-sectional hierarchies.\nThe general idea behind ML-based reconciliation is to model the relationship between each bottom-level time series x [1] b,\u03c4 (b = 1, . . . , nb) of the cross-temporal hierarchy and the base forecasts across the entire cross-temporal tree x\u0302 [k] i,\u03c4 (i = 1, . . . , n and k = k1, . . . , kp). We use a time series rolling-window procedure to construct these base forecasts as visualized in Figure 4. The rolling-window size of the estimation sample is denoted by Q (length of the blue bars) and the validation sample with forecast horizon by H (length of the green bars), both quantities are expressed in number of observations for the top temporal aggregation level (i.e. the daily time series in our application). Then, for each cross-sectional series at temporal aggregation level k and rolling window iteration r = 1, . . . , R (i.e. R = 4 in Figure 4 as used in our application), we estimate base forecast models on the estimation sample, consisting of mk \u00d7Q observations, and obtain mk \u00d7H-step ahead forecasts.\nNext, from the observations in the validation sample V consisting of observations V = {Q + 1, . . . , Q + R \u00d7 H}, we construct the response vector and features matrix for the ML-model which can be generally written as\nyb,V = fb(X\u0302b,V) + \u03b5b,V , (1)\nwhere fb(\u00b7) denotes the forecast function to be trained for bottom-level series b, see Section 4.2 for specifications, and \u03b5b,V represents the error term. Note that every bottom-level series in the cross-temporal hierarchy (i.e. 30-min delivery area demand in our application) is\npredicted by a separate ML model, thereby allowing the forecast reconciliation approach to adapt to different patterns in each series. The response variable yb,V in (1) is simply given by the m1 \u00d7R\u00d7H observations of the bottom-level series for variable b; hence\nyb,V = [ x [1] b,Q+1 \u22a4 , . . . ,x [1] b,Q+RH \u22a4]\u22a4 .\nAs features matrix to the ML-model, we use X\u0302b,V = x\u0302 [1] 1,Q+1 . . . x\u0302 [1] b\u22121,Q+1 x\u0302 [1] b+1,Q+1 . . . x\u0302 [1] n,Q+1 x\u0302 [1] b,Q+1 x\u0302 [k2] b,Q+1 \u2297 1k2 . . . x\u0302 [kp] b,Q+1 \u2297 1kp ... . . . ... ... . . . ... ... ... . . . ...\nx\u0302 [1] 1,Q+RH . . . x\u0302 [1] b\u22121,Q+RH x\u0302 [1] b+1,Q+RH . . . x\u0302 [1] n,Q+RH x\u0302 [1] b,Q+RH x\u0302 [k2] b,Q+RH \u2297 1k2 . . . x\u0302 [kp] b,Q+RH \u2297 1kp\n ,\nwhich contains various base forecasts on the validation sample, where \u2297 denotes the Kronecker product and 1 is a column vector of ones. More specifically, the first set of predictors, highlighted in yellow, exploits information in the cross-sectional hierarchy and contains the nb highest-frequency (i.e. 30-min in our application) base forecasts for all bottom-level variables, as well as the na highest-frequency base forecasts for the upper level series in the cross-temporal hierarchy (i.e. zone and market in our application). The second set of predictors highlighted in blue exploits information in the temporal hierarchy and contains all temporal (i.e. 30-min, hourly and daily in our application) base forecasts for the bottom-level variable b for which we are constructing our ML-model. Notice that due to the frequency mismatch, each of the lower-frequency (i.e. hourly and daily in our application) at level ka for a = 2, . . . , p are repeated respectively ka times. We hereby make the assumption that temporal gain in reconciling forecasts mainly comes from the focal variable\u2019s own temporally aggregated series to keep this second set of predictors compact. Finally, note the central role for the bottom level variable b itself (highlighted in yellow and blue) whose base forecast we aim to improve upon in this reconciliation step.\nFinally, to reconcile the actual out-of-sample demand forecasts of horizon h for forecast evaluation purposes, we use the complete sample up to N = Q+R\u00d7H to obtain the mk\u00d7H step ahead forecasts for each cross-sectional series at temporal aggregation level k. This is visualized in the bottom part of Figure 4, where the gray bar represents the training sample (of length N) on which the base forecast models are estimated and the red bar (of length H) represents the test set on which we evaluate out-of-sample forecast performance. The out-of-sample base forecasts x\u0302 [k] i,\u03c4 (i = 1, . . . , n and k = k1, . . . , kp) are used as inputs to the previously estimated ML models (one for each bottom-level series) to provide the reconciled bottom-level forecasts x\u0303 [1] b,\u03c4 (b = 1, . . . , nb) of the cross-temporal hierarchy. As a last step, these reconciled bottom-level forecasts are then aggregated as explained in Section 3.1 to obtain coherent forecasts across the complete hierarchy."
        },
        {
            "heading": "4 Design of the Forecast Study",
            "text": "We start by discussing in Section 4.1 the forecast set-up we use to evaluate out-of-sample forecast performance. In Section 4.2, we summarize the base forecast models all forecast reconciliation methods rely on, together with the machine learning methods used by our procedure to obtain the reconciled forecasts. Finally, Section 4.3 outlines the benchmark forecast methods against which we compare the performance of our procedure."
        },
        {
            "heading": "4.1 Forecast Set-up",
            "text": "To compare the forecast performance of our forecast reconciliation method against its benchmarks, we use a standard rolling-window approach using the period February 20, 2023 to September 24, 2023 (31 weeks) as test sample. In each rolling window iteration, we use\nthe most recent six months of data to estimate the base models on the estimation set (first five months), and the most recent month of data to estimate the ML model on the validation set. Hence, in line with the notation used in Figure 4, we take Q = 140 days, H = 7 days and R = 4. Note that since the demand data in our platform application consists of nonnegative integers we ensure this by rounding the base forecasts prior to using them as inputs in the ML model, i.e. x\u0302 [k]\u22c6\ni,t = max(0, \u230ax\u0302 [k] i,t\u2309), we similarly round the bottom-level reconciled\nforecasts before aggregating them.\nIn a next step, we combine the estimation and validation sample (N = 168 days) to estimate the base models and obtain final out-of-sample base forecasts as well as reconciled forecasts on the test set. As forecast horizon, we consider one week of data (i.e. horizon mk \u00d7 H for temporal aggregation level k) as this corresponds to the planning horizon for the compensation schemes of couriers through dynamic pricing which crucially relies on demand forecasts as inputs. Hence, for the 30-min series, we obtain 34\u00d7 7 = 238 forecasts; for the hourly series we obtain 17 \u00d7 7 = 119 forecasts and for the daily series we obtain H = 7 forecasts. This concludes one iteration in the outer rolling window used for forecast evaluation purposes. In the next iteration, we move one-week ahead and repeat the same steps. We roll forward until we reach the end of our sample, thereby resulting in 31 outer rolling windows.\nFigure B.1 in the Appendix B provides a visualization of iteration r and r+1 in the outer rolling window. We hereby highlight the estimation (blue bars) and validation (green bars) sets for the inner rolling window used to construct the inputs to the ML model (see Section 3.2). Due to our inner rolling-window set-up, the training set (blue bar) and forecast horizon (green bar) of the last three (inner) rolling windows in iteration r overlap with the first three (inner) rolling windows in iteration r + 1, as indicated with the shading. This offers a clear\ncomputational advantage as beyond iteration r = 1, we can re-use the base forecasts for the first three weeks from the previous iteration and need to fit the forecasting model only once to obtain the base forecasts for the fourth week to construct the validation set.\nTo evaluate the forecast performance of the various forecast methods, we use the Weighted Absolute Percentage Error (WAPE), also known as normalized version of the mean absolute error (MAE). The WAPE loss function for each temporal factor k and cross-sectional time series i = 1, . . . , n is given by\nWAPE [k] i =\n\u2211Ttest/k j=1 |A [k] i,j \u2212 F\n[k] i,j |\u2211Ttest/k\nj=1 |A [k] i,j |\n,\nwhich measures the overall deviation of the forecasted values F [k] i,j from the actual values A [k] i,j , for all time points j in the test set of size Ttest/k where Ttest denotes the number of high-frequency (i.e. 30-min) observations in the test set. We then average these WAPEs across all cross-sectional units belonging to the same cross-temporal level; for instance the overall WAPE for the bottom-level series (i.e. 30-min for the nb areas) is then given by\nWAPE[1] = (1/nb) \u2211nb b=1 WAPE [1] b .\nThe WAPE is a popular forecast metric to evaluate performance at delivery platforms (i) because of its scale-independence thereby facilitating comparisons across heterogeneous areas as well as different time/geographical aggregation levels, (ii) it is agnostic to zeros as it is very unlikely that the denominator is zero, thereby making it a suitable metric even when the demand in an area is low or intermittent, and (iii) it emphasizes accuracy for items with larger demand, thereby prioritizing accurate forecasting of demand during busy periods. Note that as robustness check we also report results for the root mean squared scaled error as well as the symmetric mean absolute percentage error."
        },
        {
            "heading": "4.2 Base Forecasts and Machine Learning Methods",
            "text": "Our procedure allows practitioners to choose their preferred base forecast models as well as their preferred ML method to perform the ML-based reconciliation. In this paper, we consider four base forecast models and three popular ML methods to this end.\nBase Forecasts. We consider one popular industry forecast model for platform data, two all-round time series models for forecasting univariate time series and a forecast combination of the former three as ensemble, base forecast model. The first base forecast model is a simple yet often-used model in the platform industry which we will label as \u201cNaive\u201d since the forecast for a specific slot (30-min, hourly or daily) of next week is simply the value observed in the same slot and day of the previous week. The following two base forecast models are oftentimes used in the hierarchical forecasting literature to obtain base forecasts, namely the class of SARIMA and exponential smoothing models. Full details on the base forecast methods are available in Appendix B.\nMachine Learning Methods. We consider three popular ML models, namely random forest, XGBoost and LightGBM. The former two are also considered in Spiliotis et al. (2021) whereas the latter has recently shown great success on the M5 competition (Makridakis et al., 2022a) consisting of an application with hierarchical retail sales time series, thereby making all suitable candidates for our platform data. More details on the ML models are available in Appendix B.\nFor all three ML methods, we report main results based on their standard implementations with default tuning parameters (see Appendix B). We do this, first to avoid excessive computing times on our streaming platform data, but second, and importantly, also because off-the-shelf implementations of tree-based methods often attain excellent performance across\na variety of settings (e.g, Januschowski et al., 2022). In Section 5.3, we investigate the improvements in forecast performance one can obtain by tuning these ML models.\nFinally, when reporting our main results, we use the sum of squared errors as loss function to train all ML models. However, our framework allows practitioners to use another loss function as they see suited for the data at hand. We also investigated the performance of the ML-based reconciliation methods when using the Tweedie loss function instead of the standard squared loss, since the former showed superior performance in the M5 competition (Januschowski et al., 2022; Makridakis et al., 2022a) on forecasting hierarchical time series as it is specifically geared towards sparse (zero-inflated) target data, which some of the considered delivery areas also display (see Figure 2). However, using a Tweedie loss instead of the regular squared loss function did not improve forecast results of our proposed ML reconciliation methods on the considered data, and are hence omitted but available from the authors upon request."
        },
        {
            "heading": "4.3 Benchmark Reconciliation Methods",
            "text": "We compare our new ML forecast reconciliation procedure against the base forecasts and five state-of-the-art linear reconciliation benchmarks for cross-temporal hierarchies, as summarized in Table 1 and implemented in the FoReco package (Girolimetto and Di Fonzo, 2023) in R (R Core Team, 2023). Apart from the Bottom Up method, the reconciliation step requires specifying how to estimate the covariance matrix of the in-sample fit errors since the benchmark methods are geared towards minimizing reconciliation errors as opposed to our procedure which is geared towards minimizing forecast combination errors. For the tcs, cst and ite benchmark methods, the temporal reconciliation covariance matrix estimator is the series variance scaling matrix (thf comb = \u201cwlsv\u201d) which is the diagonal matrix that contains\nthe estimated variances of the in-sample residuals across each level. The cross-sectional reconciliation covariance matrix estimator is a shrinkage covariance matrix of the in-sample residuals (hts comb = \u201cshr\u201d), where the off-diagonal elements are shrunk towards zero. For the oct method cross-temporal based covariance matrix estimator is simply containing the in-sample residual variance on the main diagonal (csts comb = \u201cwlsv\u201d). Full details on the cross-temporal benchmark methods are available in Di Fonzo and Girolimetto (2023a)."
        },
        {
            "heading": "5 Results",
            "text": "This section is divided in four parts. In Section 5.1, we discuss the overall forecast performance of the forecast reconciliation methods, followed by Section 5.2 where we present detailed insights on our ML-based reconciliation approach. In Section 5.3, we discuss the trade-off between computing time and forecast accuracy when incorporating hyperparameter\ntuning into our ML-based reconciliation proposal. Finally, in Section 5.4, we discuss how shifts in the demand data streams affect the performance of the reconciliation methods."
        },
        {
            "heading": "5.1 Overall Forecast Performance",
            "text": "Table 2 summarizes the forecast accuracy across the complete cross-temporal hierarchy for the different combinations of base forecasts and reconciliation methods.4 Overall, all methods find that forecasting is easier higher up in the hierarchy, i.e. lower temporal frequency and larger geographic areas, which confirms the general findings in the literature. For the set of considered base forecasts and reconciliation methods, WAPE falls from around 30% to 5% for respectively 30-minute delivery areas and the daily London hierarchy levels.\nThe ML-based forecast reconciliation results using random forest are consistently outperforming all benchmarks at the delivery area and zone levels at all frequencies. For example, using 30-minute frequency SARIMA base forecasts, we obtain a WAPE of 0.3013 for the cst method (i.e. the best linear forecast reconciliation method) compared to 0.2801 for random forest based reconciliation (i.e. the best non-linear forecast reconciliation method). However, ML-based forecast reconciliation does not systematically dominate linear methods as shown for instance by XGBoost with a WAPE of 0.3195. Furthermore, ML-based forecast reconciliation seems to be rather insensitive to the choice of the base forecasts. In fact, the Naive, ETS and SARIMA results are qualitatively the same. For example in the case of random forest, the respective WAPEs are very close with 0.2762, 0.2809 and 0.2801 respectively.\nTo assess the significance of our findings, we report in Figure 5 the results of the accuracy ranking based multiple comparisons with the best (MCB) test, a methodology popularized in the forecasting literature since Koning et al. (2005). We compute the MCB test across\n4Note that for the Naive base method, the base and bottom up forecasts are identical, and no results are reported for the linear forecast reconciliation methods since the original base forecasts are already reconciled.\nNotes: This table shows forecast accuracy measured in weighted absolute percentage error (WAPE). The forecast horizon is one week-ahead. The best forecast reconciliation results for each base forecast (Naive, ETS, SARIMA, Forecast Combination) are highlighted in gray. The linear benchmark methods are defined in Table 1.\nall levels in the cross-temporal hierarchy and report results for each of the four base forecast methods. To this end, we use the R-package tsutils (Kourentzes, 2023) with function nemenyi() to compute the MCB test. The test ranks the performance of the examined methods across the series being forecast comparing their average ranks by considering a critical difference, determined through a confidence interval. For each base forecast panel in Figure 5, the methods that do not overlap with the best ranked procedure (as highlighted by the gray zone) are significantly worse than the best. Visual inspection of the results confirms our overall finding that random forest reconciliation dominates the other procedures.\nThe important business question is whether ML-based forecast reconciliation helps im-\nproving the base forecast methods for high-frequency (i.e. 30-minute and hourly) delivery area levels, because these forecasts are direct inputs for automated decision making. The answer is yes and the forecast gains are between 10 and 15%. As an illustration for random forest reconciliation with ETS base forecasts, the WAPE is 0.2809 compared to 0.3180 for non-reconciled base forecasts. It also turns out that these high-frequency delivery area level gains become even stronger at the zone and market levels. In fact, the forecast gains mount to even 40% at the market level. For example, the London 30-minute WAPEs decrease from 0.1621 to 0.0959 with ETS base forecasts and random forest reconciliation. Here, the linear reconciliation methods only deliver marginal accuracy gains, with the best (cst) WAPE being 0.1349. However, we also note that at the London level, the overall smallest WAPEs are obtained using Naive base forecasts for 30-minute and hourly frequencies.\nFinally, in the bottom panel of Table 2, we provide forecast combination results, that is by combining with equal weights the base forecasts. We find this strategy to slightly pay off compared to the best performing base methods. For example, at the 30-min delivery area level, the random forest ML method for the base forecasts are respectively 0.2762, 0.2809 and 0.2801 while the forecast combination yields 0.2758. The gains are overall stronger for the linear reconciliation methods compared to the non-linear ones, yet the former are, overall, outperformed by the ML-based reconciliation methods. Still in practice, we generally do not know a priori which base forecasts method will perform best out-of-sample, and moreover, this likely varies across the different levels of the cross-temporal hierarchy. Then, a forecast combination base forecast approach forms an effective practice to obtain accurate and robust base forecasts, which can still be further improved upon via ML-based reconciliation to obtain coherent forecasts.\nTo sum up, random forest forecast reconciliation performs particularly well but not all\nML methods are doing better than linear methods. The strongest gains are obtained for the 30-minute and hourly frequencies. Forecast combination, i.e. averaging the base forecasts, is (marginally) beneficial. Note that these main findings are qualitatively the same when we measure forecast accuracy in terms of the symmetric mean absolute percentage error (SMAPE), a popular asymmetric loss function, or in terms of the root mean squared scaled error (RMSSE), a popular metric in the forecast reconciliation literature (see Hyndman and Koehler, 2006 for a discussion on these metrics) instead of WAPE. We refer to Tables C.1 and C.2 in Appendix C for respectively the SMAPE and RMSSE results."
        },
        {
            "heading": "5.2 ML-Based Reconciliation Insights",
            "text": "From the previous results, we find that random forest based forecast reconciliation outperforms the considered benchmarks, especially at the more granular levels in the hierarchy. To gain more insights, we investigate the link between forecast accuracy and size of the delivery areas. In addition, we investigate what drives forecast performance by computing SHAP values which are considered to be the current state-of-the-art method for interpreting ML models, see Molnar (2022) for an introduction.\nFigure 6 plots forecast performance (WAPE, vertical axis) of each delivery area (dot) based on its average demand size (horizontal axis). The left panel shows this relationship using un-reconciled base forecasts, and we find a smirk shape pattern with many WAPEs above 0.5. The Naive base forecasts are, overall, worse than ETS and SARIMA when average demand is below 20. The middle figure shows the random forest forecast reconciliation results and we see that the smirk flattens out, with hardly any noticeable difference between the base forecast methods, and few areas having WAPEs above 0.5. The right figure highlights that random forest based forecast reconciliation has a capacity to \u201ccorrect\u201d bad quality base\nforecasts as the relative WAPEs of the ML-reconciled forecasts over the base forecasts are mostly below 1.\nNext, for each random forest model with Naive base forecasts in the (outer) rollingwindow setup, we compute SHAP values (Shapley et al., 1953; Lundberg and Lee, 2017) for all observations in the test set. To this end, we use the package treeshap (Komisarczyk et al., 2023) in R, a fast implementation for tree ensemble models (Lundberg et al., 2018). Figure 7 shows the resulting relative variable importance plots by variable group for City of London as an illustration. Given the large number of features used in the ML-model, we focus on the 36 highest ranked features in terms of variable importance (which together account for more than 80% of the overall variable importance). Overall, the proportions are stable over the test sample. While City of London\u2019s own 30-min and hourly streams count for a sizable portion in the 20-30% range, the other areas 30-minute streams count together for almost 40%. Zone and market level, or own daily information contributes only marginally."
        },
        {
            "heading": "5.3 Computational Aspects and Hyperparameter Tuning",
            "text": "Our main results above are obtained by training the ML algorithms with their default hyperparameters as provided by the software packages. By doing so, we save a substantial amount of computing time which is an important consideration when deploying forecast reconciliation in a production environment. However, in this section, we investigate whether the performance of the ML-based forecast reconciliation methods can be further improved by hyperparameter tuning. To this end, we use the common practice of grid search to tune the hyperparameters of the random forest algorithm, since there are only few hyperparameters, whereas we use Bayesian optimization to tune the hyperparameters of the XGBoost and LightGBM algorithms as the latter two have a large amount of hyperparameters. Details on the tuning process are provided in Appendix B.\nTable 3 reports results for the ML-based reconciliation methods\u2013 default and tuned \u2013 with SARIMA base forecasts. The random forest algorithm with default tuning parameters,\nthese were best performing in most of our previous results, performs equally well as its tuned version. This is a great advantage when implementing the forecasting procedure in a high-dimensional streaming data environment with frequent updates. In contrast to random forest, the XGBoost algorithm greatly benefits from tuning at the high-frequency delivery area level. Here, the loss decreases from 0.3196 to 0.2790, a reduction of about 13% and XGBoost is now at the same accuracy level as random forest. For the zone and market hierarchies, the differences between default and tuning are marginal. Finally, the LightGBM algorithm is only mildly impacted, with small tuning improvements at the delivery area level.\nWhile hyperparameter tuning does result, overall, in a marginal gain in forecast accuracy, it is important to investigate the computational burden that comes along with it. Table 3 reports in the third column the approximate computing times (in seconds) for one ML fit with fixed tuning parameters as well as one round of hyperparameter tuning, and this for random forest, XGBoost and LightGBM. As well known, both the XGBoost and LightGBM algorithms are fast with default tuning parameters, requiring only about one second to train. Random forest is in comparison relatively slow with seven seconds to train. However, the\nprohibitive cost of tuning is appalling. All algorithms need large multiples of computing time to tune the hyperparameters. In particular the XGBoost and LightGBM algorithms with many tuning hyperparameters are long to optimize.5\nTo conclude, among the considered ML algorithms, the random forest algorithm is preferred because the default tuning parameters yield good accuracy levels close to the tuned ones. This is a great advantage when faced with computational constraints such as given by the platform environments we consider in this paper."
        },
        {
            "heading": "5.4 Data Shifts: The Case of Manchester",
            "text": "The streaming platform data for the London market are reasonably stable over time. In this section, we investigate how our ML-based forecast reconciliation procedure performs when facing shifts in the data streams. In fact, breaks or data shifts are regularly observed in platform data because of new competitors in the marketplace and companies moving some of their locations to other on-demand platforms.\nTo study the impact of breaks on forecast reconciliation, we focus on Manchester for which we have a cross-temporal hierarchy that consists cross-sectionally of 24 delivery area and Manchester as the entire market (there is no intermediate level consisting of zones in Manchester), and temporally of 30-minute, hourly and daily frequencies for the same sample period as London. Figure A.2 in Appendix A displays aggregated demand over the sample period in each delivery area in Manchester. Figure 8, top panel, illustrates how severe the data shift is on June 14, 2023 in all considered temporal frequencies for the Bolton Central delivery area situated North-West of Manchester.\nTable 4 summarizes the forecast accuracy for the cross-temporal hierarchy for SARIMA 5We use Bayesian optimization to tune the hyperparameters of XGBoost and LightGBM. Other methods or only subsets of parameters could be tuned. This would make the tuning faster.\nbase forecasts and all reconciliation methods; the other base forecast results are similar and available on request. We report results for the period before the data shift (i.e. prior to June 14), the period after the data shift (i.e. after July 14), and the one month period in between characterizing the data shift. There are several interesting findings for the delivery area level of the hierarchy. First, for \u201cBefore shift\u201d period we see that random forest reconciliation for delivery areas outperforms other methods with the same relative magnitudes as found in the London market where no data shifts occur. For example, random forest and cst WAPEs are 0.3226 and 0.3435, respectively. Second, we see that the volatile \u201cDuring shift\u201d period WAPEs at the delivery area level all more than double compared to the stable period before. The LightGBM ML reconciliation method performs best with a WAPE of 0.7516. Third, the \u201cAfter shift\u201d period is generally characterized by better accuracies than during the shift, but the size of the gains are much stronger for the ML-based reconciliation methods.\nTo illustrate the magnitude of this, consider cst with WAPEs of 0.7986 and 0.7023 and random forest WAPEs of 0.8004 and 0.4206, respectively for the \u201cDuring\u201d and \u201cAfter shift\u201d periods. Fourth, the un-reconciled \u201cAfter shift\u201d SARIMA base forecasts are of low quality with WAPEs still double the size compared to the \u201cBefore shift\u201d period. This can somehow be expected given that parameter estimates are based on historical data subject to the shift. However, our ML-based reconciliation method is able to transform these low quality projections into forecasts that yield WAPEs much closer to the period prior to the shift. At the market level part of the hierarchy, we observe only small differences in terms of forecasting accuracy over the three periods. This is expected since the aggregation of delivery areas not all being subject to a data shift, results in a smoother market level time series, see Figure 8, bottom panel. The LightGBM method has consistently the smallest WAPEs at the 30- minute and hourly frequency levels."
        },
        {
            "heading": "6 Conclusion",
            "text": "Forecast reconciliation based on linear methods has been shown to be very useful and successful in a wide variety of applications. Recently, ML-based forecast reconciliation methods are proposed for cross-sectional time series hierarchies. We extend the latter approach to cross-temporal hierarchies. We provide a general description of the methodology and an application to a unique platform dataset from an on-demand logistics company. In particular, the data consists of demand streams that constitute a rich hierarchy in spatial and time dimensions, with bottom level time series defined as delivery areas at 30-minute frequency.\nOur key empirical finding is that ML-based forecast reconciliation for our platform data can result in substantial forecast accuracy improvements compared to existing linear recon-\nciliation methods. However, unless one can face the huge computational cost of hyperparameter tuning, ML-based reconciliation is not uniformly superior to linear methods. Another key finding is that, as platform data streams are potentially impacted by data shifts, our approach is able to react swiftly to such instability when compared with linear approaches.\nSeveral extensions should be considered. First, applications to other datasets consisting of different cross-temporal hierarchies. Second, our ML algorithms in the platform application consist of tree-based methods only, though our approach can be used with any ML algorithm. Taking into account computing time, it would be interesting to test alternative neural network based methods using our approach. Third, we currently retrain the ML algorithms at every iteration of the rolling window, it would be interesting to investigate if online ML methods that incrementally update the forecast function as new data becomes available can lead to further computational efficiencies."
        },
        {
            "heading": "Appendix B Design of the Forecast Study",
            "text": "B.1 Rolling-Window Set-Up\niteration r\nB.2 Base Forecasts\nWe consider four base forecast models: (i) Naive, (ii) SARIMA, (iii) ETS and a (iv)\nforecast combination of the former three.\nNaive. The first base forecast model for temporal aggregation level k of a specific crosssectional time series is simply x [k] j = x [k] j\u22127m/k + \u03b5 [k] j , which we call \u201cNaive\u201d since it simply implies that the forecast of a specific time slot (30-min, hourly or daily) is the value observed in the same time slot and day of the previous week. This way of forecasting is popular in the industry since it is ultra fast, i.e. it does not require parameter estimation, and it works well in the case of strong seasonality patterns as observed in platform data.\nSARIMA. Secondly, we consider ARIMA models, as implemented in the forecast package (Hyndman et al., 2023; Hyndman and Khandakar, 2008) in R. For the daily time series, we search for each series the optimal model in the space of seasonal ARIMA models, thereby allowing for weekly seasonality (seasonal period equal to 7 for daily data) and a maximal MA order q = 3, and maximal AR order of p = 3. For the seasonal components we set the maximal MA order to Q = 1 and maximal AR order to P = 1. We restrict the maximum number of non-seasonal differences to d = 1. All other arguments are kept at their default values.\nFor the hourly and 30-min time series, we need to additionally account for the strong intra-day seasonal patterns in the platform data. Since the seasonal period is long in both cases (m1 = 34 for 30-min data and m2 = 17 for hourly data), we follow the common practice based on Fourier series to capture the seasonality. Specifically, for the 30-min time series yt, we consider the regression model\nyt = \u00b5+ S\u2211\ns=1\n[\u03b1ssin(2\u03c0st/m1) + \u03b2scos(2\u03c0st/m1)] + \u03b5t, (B.1)\nwhere \u03b5t is subsequently modeled as an ARIMA process. Analogously for the hourly data thereby taking the seasonal period m2 = 17. We select the optimal value of S via the Bayesian Information Criterion.\nETS. Thirdly, we consider exponential smoothing (ETS; Hyndman et al., 2002) as another popular model to produce the base forecasts. To this end, we use the ets function (with default arguments) as implemented in the forecast package. As above, for the 30-min and hourly series we first estimate the intra-day seasonality through the Fourier series approach and subsequently model the error term in equation (B.1) using ETS.\nForecast Combination. Our final base forecast model consists of a simple forecast combination of the previous three base forecast models. Forecast combination approaches have shown to perform well as base forecast methods in the hierarchical forecasting literature, thereby oftentimes providing an effective practice for improving forecast accuracy; see for instance Rostami-Tabar and Hyndman (2024) for an application on emergency medical services data. We opt for equal weights in the forecast combination, mainly because of its simplicity, ease in implementation and its established track-record of good performance in the forecast combination literature (e.g., Elliott and Timmermann, 2016, Chapter 14).\nB.3 Machine Learning Methods\nWe consider three machine learning methods: (i) random forest, (ii) XGBoost and (iii)\nLightGBM.\nRandom Forest. Random forests, proposed by Breiman (2001), produce forecasts by combining regression trees. A regression tree is a nonparametric method that partitions the feature space to compute local averages as forecasts, see Efron and Hastie (2016) for a textbook treatment. The tuning parameters are the number of trees that are used in the forecast combination, the number of features to randomly select when constructing each regression tree split, and the minimum number of observations in each terminal node to compute the local forecasts. We use the standard implementation of the randomForest package (Liaw and Wiener, 2002) in R with default settings for the hyperparamters (i.e. number of trees: ntree = 500, number of variables sampled at each split: mtry = # of features/3, minimum size of terminal nodes: nodesize = 5) when reporting our main results.\nIn Section 5.3, we also investigate the performance of the random forest based forecast reconciliation method when the hyperparameters are tuned. To this end, we follow Spiliotis\net al. (2021) and use the package mlr in R (Bischl et al., 2016) which implements a random grid search with cross-validation to find the optimal hyperparameters. In the cross-validation procedure, we leave each of the four subsequent weeks in the validation set once out as test sample and use the root mean squared error as cross-validation score to compute the optimal hyperparameters. Note that we tune the hyperparameters only once every four iterations (i.e. once every month) in the outer rolling window (so in 8 out of the 31 outer rolling windows) to keep the computational burden low; in between we use the optimal hyperparameters from the previous tuning round as fixed values. The lower and upper bounds for the hyperparameters are set to (2, 50) for mtry and (5, 50) for nodesize. We set the bounds for ntree between 50 and 500 on interval steps of 10.\nXGBoost. Gradient boosting, proposed by Friedman (2001), constructs forecasts by sequentially fitting small regression trees, i.e. weak learners, to the residuals by the ensemble of the previous trees. This procedure results in a one final tree, constructed as a sum of trees, used for forecasting. Extreme gradient boosting (XGBoost), introduced by Chen and Guestrin (2016), optimizes the implementation of the gradient boosting framework in terms of speed and flexibility. We use the xgboost package (Chen et al., 2023) in R with fixed choices of the tuning parameters when reporting our main results. We fix the hyperparameters to the default values as follows: 100 boosting iterations (nrounds), 6 as max tree depth (max depth), 0.3 as learning rate (eta), 1 as subsample ratio (subsample), 1 as subsample ratio of columns (colsample bytree), 1 as minimum sum of instance weight (hessian) (min child weight) and 0 as minimum loss reduction (gamma).\nSubsequently, to tune the hyperparameters, we use the same procedure as discussed for random forest but this time use Bayesian optimization to tune the hyperparameters since grid search is computationally very expensive in this case. To this end, we use the\nrBayesianOptimization (Yan, 2021) in R and consider the following intervals with lower and upperbounds for each hyperparameter, in line with Spiliotis et al. (2021): we set the values of max depth between (2, 10), the learning rate (eta) between (0.01, 0.05), subsample values between (0.3, 1), colsample bytree values between (0.3, 1), min child weight between (0, 10) and gamma between (0, 5). The values for the maximum number of boosting iterations (nrounds) are over the range of 50 and 200.\nLightGBM. LightGBM, put forward by Microsoft in 2016, is a gradient boosting framework that uses tree-based learning algorithms like XGBoost but as the name suggests has computational advantages with respect to training speed, memory usage and parallelization. The implementation of gradient-based one-side sampling and exclusive feature bundling techniques allows handling large training datasets. We use the lightgbm package (Shi et al., 2023) in R with fixed, default hyperparameters when reporting our main results: 100 boosting iterations (nrounds), 31 as maximum number of leaves (num leaves), 0.1 as learning rate (eta), 1 as subsample ratio (subsample), 1 as subsample ratio of columns (colsample bytree), 0.001 as minimum sum of instance weight (hessian) (min child weight) and 0 as \u21131-regularization (lambda l1) and no limit to the max depth (max depth) for the tree model.\nFinally, to tune the hyperparameters, we also use Bayesian optimization with the following similar prior values for the hyperparameters as with XGBoost. The maximum number of leaves (num leaves) fixed to the range of 5 and 31. The learning rate (eta) is set between (0.01, 0.05), subsample values between (0.3, 1), colsample bytree values are set between (0.3, 1), min child weight between (0, 10), max depth between (2, 10) and lambda l1 between (0, 5). The values for the maximum number of boosting iterations (nrounds) are over the range of 50 and 200."
        },
        {
            "heading": "Appendix C Platform Application: Additional Results",
            "text": "This Appendix contains the forecast accuracy results on the London data for two additional forecast error metrics, namely the symmetric mean absolute percentage error (SMAPE) and the root mean squared scaled error (RMSSE), see Hyndman and Koehler (2006) for an overview.\nThe two forecast error metrics, for temporal factor k and cross-sectional time series\ni = 1, . . . , n, are defined as\nSMAPE [k] i =\n1\nTtest/k Ttest/k\u2211 j=1 |A[k]i,j \u2212 F [k] i,j | |A[k]i,j |+ |F [k] i,j | ,\nand\nRMSSE [k] i,r = \u221a\u221a\u221a\u221a\u221a  1 mkH mk(N+H)\u2211 j=mkN+1 (A [k] i,j \u2212 F [k] i,j ) 2  /[ 1 mk(N \u2212 7) mkN\u2211 j=7mk+1 (A [k] i,j \u2212 A [k] i,j\u22127mk) 2 ] ,\nfor outer rolling window r = 1, . . . , 31, and RMSSE [k] i = 1 31 \u221131 r=1RMSSE [k] i,r given the 31 outer rolling windows in our forecast set-up. Table C.1 presents the results for the SMAPE, Table C.2 for the RMSSE."
        }
    ],
    "title": "Cross-Temporal Forecast Reconciliation at Digital Platforms with Machine Learning\u2217",
    "year": 2024
}