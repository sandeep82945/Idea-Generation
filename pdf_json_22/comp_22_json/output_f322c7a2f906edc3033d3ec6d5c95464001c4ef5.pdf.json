{
    "abstractText": "Bias mitigators can improve algorithmic fairness in machine learning models, but their effect on fairness is often not stable across data splits. A popular approach to train more stable models is ensemble learning, but unfortunately, it is unclear how to combine ensembles with mitigators to best navigate trade-offs between fairness and predictive performance. To that end, we built an open-source library enabling the modular composition of 8 mitigators, 4 ensembles, and their corresponding hyperparameters, and we empirically explored the space of configurations on 13 datasets. We distilled our insights from this exploration in the form of a guidance diagram for practitioners that we demonstrate is robust and reproducible.",
    "authors": [
        {
            "affiliations": [],
            "name": "Michael Feffer"
        },
        {
            "affiliations": [],
            "name": "Martin Hirzel"
        },
        {
            "affiliations": [],
            "name": "Samuel C. Hoffman"
        },
        {
            "affiliations": [],
            "name": "Kiran Kate"
        },
        {
            "affiliations": [],
            "name": "Parikshit Ram"
        },
        {
            "affiliations": [],
            "name": "Avraham Shinnar"
        }
    ],
    "id": "SP:374528e8b4b84ba6820723bd42e243ff304314bb",
    "references": [
        {
            "authors": [
                "A. Agarwal",
                "A. Beygelzimer",
                "M. Dudik",
                "J. Langford",
                "H. Wallach"
            ],
            "title": "A Reductions Approach to Fair Classification",
            "venue": "International Conference on Machine Learning (ICML), 60\u201369. AHRQ. 2015. Medical Expenditure Panel Sur-",
            "year": 2018
        },
        {
            "authors": [
                "N. Bantilan"
            ],
            "title": "Themis-ML: A fairness-aware machine learn-ing interface for end-to-end discrimination discovery and mitigation",
            "year": 2017
        },
        {
            "authors": [
                "G. Baudart",
                "M. Hirzel",
                "K. Kate",
                "P. Ram",
                "A. Shinnar",
                "J. Tsay"
            ],
            "title": "Pipeline Combinators for Gradual AutoML",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS).",
            "year": 2021
        },
        {
            "authors": [
                "R.K.E. Bellamy",
                "K. Dey",
                "M. Hind",
                "S.C. Hoffman",
                "S. Houde",
                "K. Kannan",
                "P. Lohia",
                "J. Martino",
                "S. Mehta",
                "A. Mojsilovic",
                "S. Nagar",
                "K.N. Ramamurthy",
                "J. Richards",
                "D. Saha",
                "P. Sattigeri",
                "M. Singh",
                "K.R. Varshney",
                "Y. Zhang"
            ],
            "title": "AI Fairness 360: An Extensible Toolkit for Detecting",
            "year": 2018
        },
        {
            "authors": [
                "J. Bergstra",
                "D. Yamins",
                "D.D. Cox"
            ],
            "title": "Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms",
            "venue": "Python in Science Conference (SCIPY), 13\u201319.",
            "year": 2013
        },
        {
            "authors": [
                "D. Bhaskaruni",
                "H. Hu",
                "C. Lan"
            ],
            "title": "Improving Prediction Fairness via Model Ensemble",
            "venue": "International Conference on Tools with Artificial Intelligence (ICTAI), 1810\u20131814.",
            "year": 2019
        },
        {
            "authors": [
                "S. Biswas",
                "H. Rajan"
            ],
            "title": "Fair Preprocessing: Towards Understanding Compositional Fairness of Data Transformers in Machine Learning Pipeline",
            "venue": "Symposium on the Foundations of Software Engineering (FSE).",
            "year": 2021
        },
        {
            "authors": [
                "L. Buitinck",
                "G. Louppe",
                "M. Blondel",
                "F. Pedregosa",
                "A. Mueller",
                "O. Grisel",
                "V. Niculae",
                "P. Prettenhofer",
                "A. Gramfort",
                "J. Grobler",
                "R. Layton",
                "J. VanderPlas",
                "A. Joly",
                "B. Holt",
                "G. Varoquaux"
            ],
            "title": "API Design for Machine Learning Software: Experiences from the scikit-learn Project",
            "year": 2013
        },
        {
            "authors": [
                "L.E. Celis",
                "L. Huang",
                "V. Keswani",
                "N.K. Vishnoi"
            ],
            "title": "Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees",
            "venue": "Conference on Fairness, Accountability, and Transparency (FAT), 319\u2013328.",
            "year": 2019
        },
        {
            "authors": [
                "T. Chen",
                "C. Guestrin"
            ],
            "title": "XGBoost: A Scalable Tree Boosting System",
            "venue": "Conference on Knowledge Discovery and Data Mining (KDD), 785\u2013794.",
            "year": 2016
        },
        {
            "authors": [
                "M. Feffer",
                "M. Hirzel",
                "S.C. Hoffman",
                "K. Kate",
                "P. Ram",
                "A. Shinnar"
            ],
            "title": "An Empirical Study of Modular Bias Mitigators and Ensembles",
            "venue": "Non-archival ICML Workshop on Benchmarking Data for Data-Centric AI (DataPerf@ICML).",
            "year": 2022
        },
        {
            "authors": [
                "M. Feldman",
                "S.A. Friedler",
                "J. Moeller",
                "C. Scheidegger",
                "S. Venkatasubramanian"
            ],
            "title": "Certifying and Removing Disparate Impact",
            "venue": "Conference on Knowledge Discovery and Data Mining (KDD), 259\u2013268.",
            "year": 2015
        },
        {
            "authors": [
                "S.A. Friedler",
                "C. Scheidegger",
                "S. Venkatasubramanian"
            ],
            "title": "The (Im)Possibility of Fairness: Different Value Systems Require Different Mechanisms for Fair Decision Making",
            "venue": "Communications of the ACM (CACM), 64(4): 136\u2013 143.",
            "year": 2021
        },
        {
            "authors": [
                "S.A. Friedler",
                "C. Scheidegger",
                "S. Venkatasubramanian",
                "S. Choudhary",
                "E.P. Hamilton",
                "D. Roth"
            ],
            "title": "A comparative study of fairness-enhancing interventions in machine learning",
            "venue": "Conference on Fairness, Accountability, and Transparency (FAT*), 329\u2013338.",
            "year": 2019
        },
        {
            "authors": [
                "N. Grgic-Hlaca",
                "M.B. Zafar",
                "K.P. Gummadi",
                "A. Weller"
            ],
            "title": "On fairness, diversity and randomness in algorithmic decision making",
            "venue": "Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML).",
            "year": 2017
        },
        {
            "authors": [
                "M. Hirzel",
                "K. Kate",
                "P. Ram"
            ],
            "title": "Engineering Fair Machine Learning Pipelines",
            "venue": "Non-archival ICLR Workshop on Responsible AI (RAI@ICLR).",
            "year": 2021
        },
        {
            "authors": [
                "K. Holstein",
                "J. Wortman Vaughan",
                "H. Daum\u00e9",
                "M. Dudik",
                "H. Wallach"
            ],
            "title": "Improving Fairness in Machine Learning Systems: What Do Industry Practitioners Need",
            "venue": "In Conference on Human Factors in Computing Systems (CHI),",
            "year": 2019
        },
        {
            "authors": [
                "F. Kamiran",
                "T. Calders"
            ],
            "title": "Data preprocessing techniques for classification without discrimination",
            "venue": "Knowledge and Information Systems, 33: 1\u201333.",
            "year": 2012
        },
        {
            "authors": [
                "F. Kamiran",
                "A. Karim",
                "X. Zhang"
            ],
            "title": "Decision Theory for Discrimination-Aware Classification",
            "venue": "International Conference on Data Mining (ICDM), 924\u2013929.",
            "year": 2012
        },
        {
            "authors": [
                "T. Kamishima",
                "S. Akaho",
                "H. Asoh",
                "J. Sakuma"
            ],
            "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer",
            "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD), 35\u201350.",
            "year": 2012
        },
        {
            "authors": [
                "M. Kearns",
                "S. Neel",
                "A. Roth",
                "Z.S. Wu"
            ],
            "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
            "venue": "International Conference on Machine Learning (ICML), 2564\u20132572.",
            "year": 2018
        },
        {
            "authors": [
                "P.J. Kenfack",
                "A.M. Khan",
                "S.A. Kazmi",
                "R. Hussain",
                "A. Oracevic",
                "A.M. Khattak"
            ],
            "title": "Impact of Model Ensemble On the Fairness of Classifiers in Machine Learning",
            "venue": "International Conference on Applied Artificial Intelligence (ICAPAI), 1\u20136.",
            "year": 2021
        },
        {
            "authors": [
                "M.S.A. Lee",
                "J. Singh"
            ],
            "title": "The Landscape and Gaps in Open Source Fairness Toolkits",
            "venue": "Conference on Human Factors in Computing Systems (CHI).",
            "year": 2021
        },
        {
            "authors": [
                "A. Mishler",
                "E. Kennedy"
            ],
            "title": "FADE: FAir Double Ensemble Learning for Observable and Counterfactual Outcomes",
            "year": 2021
        },
        {
            "authors": [
                "V. Perrone",
                "M. Donini",
                "K. Kenthapadi",
                "C. Archambeau"
            ],
            "title": "Bayesian optimization with fairness constraints",
            "venue": "ICML Workshop on Automated Machine Learning (AutoML@ICML).",
            "year": 2020
        },
        {
            "authors": [
                "G. Pleiss",
                "M. Raghavan",
                "F. Wu",
                "J. Kleinberg",
                "K.Q. Weinberger"
            ],
            "title": "On Fairness and Calibration",
            "venue": "Conference on Neural Information Processing Systems (NIPS).",
            "year": 2017
        },
        {
            "authors": [
                "M. Singh",
                "G. Ghalachyan",
                "K.R. Varshney",
                "R.E. Bryant"
            ],
            "title": "An Empirical Study of Accuracy, Fairness, Explainability, Distributional Robustness, and Adversarial Robustness. In KDD Workshop on Measures and Best Practices for Responsible AI (Responsible AI@KDD)",
            "year": 2021
        },
        {
            "authors": [
                "I. Valentim",
                "N. Louren\u00e7o",
                "N. Antunes"
            ],
            "title": "The Impact of Data Preparation on the Fairness of Software Systems",
            "venue": "International Symposium on Software Reliability Engineering (ISSRE), 391\u2013401.",
            "year": 2019
        },
        {
            "authors": [
                "J. Vanschoren",
                "J.N. van Rijn",
                "B. Bischl",
                "L. Torgo"
            ],
            "title": "OpenML: Networked Science in Machine Learning",
            "venue": "SIGKDD Explorations Newsletter,",
            "year": 2014
        },
        {
            "authors": [
                "I.H. Witten",
                "E. Frank",
                "M.A. Hall",
                "C. Pal"
            ],
            "title": "Data Mining: Practical Machine Learning Tools and Techniques",
            "venue": "Morgan Kaufmann, fourth edition.",
            "year": 2016
        },
        {
            "authors": [
                "Q. Wu",
                "C. Wang"
            ],
            "title": "Fair AutoML",
            "venue": "arXiv preprint.",
            "year": 2021
        },
        {
            "authors": [
                "K. Yang",
                "B. Huang",
                "J. Stoyanovich",
                "S. Schelter"
            ],
            "title": "Fairness-Aware Instrumentation of Preprocessing Pipelines for Machine Learning",
            "venue": "Workshop on Human-In-the-Loop Data Analytics (HILDA).",
            "year": 2020
        },
        {
            "authors": [
                "R. Zemel",
                "Y. Wu",
                "K. Swersky",
                "T. Pitassi",
                "C. Dwork"
            ],
            "title": "Learning Fair Representations",
            "venue": "International Conference on Machine Learning (ICML), 325\u2013333.",
            "year": 2013
        },
        {
            "authors": [
                "B.H. Zhang",
                "B. Lemoine",
                "M. Mitchell"
            ],
            "title": "Mitigating Unwanted Biases with Adversarial Learning",
            "venue": "Conference on AI, Ethics, and Society (AIES), 335\u2013340.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Algorithmic bias in machine learning can lead to models that discriminate against underprivileged groups in various domains, including hiring, healthcare, finance, criminal justice, education, and even child care. Of course, bias in machine learning is a socio-technical problem that cannot be solved with technical solutions alone. That said, to make tangible progress, this paper focuses on bias mitigators, which improve or replace an existing machine learning estimator (e.g., a classifier) so it makes less biased predictions (e.g., class labels) as measured by a fairness metric (e.g., disparate impact (Feldman et al. 2015)). Unfortunately, bias mitigation often suffers from high volatility, meaning the estimator is less stable with respect to group fairness metrics. In the worst case, this volatility can even cause a model to appear fair when measured on training data while being unfair on production data. Given that ensembles (e.g., bagging or boosting) can improve stability for accuracy metrics (Witten et al. 2016), we felt it was important to explore whether they also improve stability for group fairness metrics.\nUnfortunately, the sheer number of ways in which ensembles and mitigators can be combined and configured with base estimators and hyperparameters presents a dilemma. On the one hand, the diversity of the space increases the chances of it containing at least one combination with satisfactory fairness and/or predictive performance for the provided data. On the other hand, finding this combination via brute-force exploration may be untenable if resources are limited.\nTo this end, we conducted experiments that navigated this space with 8 bias mitigators from AIF360 (Bellamy et al.\n2018); bagging, boosting, voting, and stacking ensembles from the popular scikit-learn library (Buitinck et al. 2013); and 13 datasets of various sizes and baseline fairness (earlier papers used at most a handful). Specifically, we searched the Cartesian product of datasets, mitigators, ensembles, and hyperparameters both via brute-force and via Hyperopt (Bergstra, Yamins, and Cox 2013) for configurations that optimized fairness while maintaining decent predictive performance and vice-versa. Our findings confirm the intuition that ensembles often improve stability of not just accuracy but also of the group fairness metrics we explored. However, the best configuration of mitigator and ensemble depends on dataset characteristics, evaluation metric of choice, and even worldview (Friedler, Scheidegger, and Venkatasubramanian 2021). Therefore, we automatically distilled a method selection guidance diagram in accordance with the results from both brute-force search and Hyperopt exploration.\nTo support these experiments, we assembled a library of pluggable ensembles, bias mitigators, and fairness datasets. While we reused popular and well-established open-source technologies, we made several new adaptations in our library to get components to work well together. Our library is available open-source (https://github.com/IBM/lale) to encourage research and real-world adoption.\nRelated Work A few pieces of prior work used ensembles for fairness, but they used specialized ensembles and bias mitigators, in contrast to our work, which uses off-the-shelf modular components. The discrimination-aware ensemble uses a heterogeneous collection of base estimators (Kamiran, Karim, and Zhang 2012); when they all agree, it returns the consensus prediction, otherwise, it classifies instances as positive iff they belong to the unprivileged group. The random ensemble also uses a heterogeneous collection of base estimators, and picks one of them at random to make a prediction (Grgic-Hlaca et al. 2017). The paper offers a synthetic case where the ensemble is more fair and more accurate than all base estimators, but lacks experiments with real datasets. Exponentiated gradient reduction trains a sequence of base estimators using a game theoretic model where one player seeks to maximize fairness violations by the estimators so far and the other player seeks to build a fairer next estimator (Agarwal et al. 2018). In the end, for predictions, it uses weights to pick a random base ar X iv :2\n21 0.\n05 59\n4v 1\n[ cs\n.L G\n] 1\n1 O\nct 2\n02 2\nestimator. Fair AdaBoost modifies boosting to boost not for accuracy but for individual fairness (Bhaskaruni, Hu, and Lan 2019). In the end, for predictions, it gives a base estimator higher weight if it was fair on more instances from the training set. The fair voting ensemble uses a heterogeneous collection of base estimators (Kenfack et al. 2021). Each prediction votes among base estimators \u03c6t, t \u2208 1..n, with weights Wt = \u03b1 \u00b7At/(\u03a3nt=1Aj) + (1\u2212 \u03b1) \u00b7 Ft/(\u03a3nt=1Fj), where At is an accuracy metric and Ft is a fairness metric. The fair double ensemble uses stacked predictors, where the final estimator is linear, with a novel approach to train the weights of the final estimator to satisfy a system of accuracy and fairness constraints (Mishler and Kennedy 2021).\nEach of the above-listed approaches used an ensemblespecific bias mitigator, whereas we experiment with eight different off-the-shelf modular mitigators. Moreover, each of these approaches used one specific kind of ensemble, whereas we experiment with off-the-shelf modular implementations of bagging, boosting, voting, and stacking. Using off-theshelf mitigators and ensembles facilitates plug-and-play between the best available independently-developed implementations. Out of the work on fairness with ensembles discussed above, one paper had an experimental evaluation with five datasets (Agarwal et al. 2018) and the other papers used at most three datasets. In contrast, we use 13 datasets. Finally, unlike these earlier papers, our paper specifically explores fairness stability and the best ways to combine mitigators and ensembles. We auto-generate a guidance diagram from this exploration.\nOurs is not the first paper to use automated machine learning, including Bayesian optimizers, to optimize models and mitigators for fairness (Perrone et al. 2020; Wu and Wang 2021). But unlike prior work, we specifically focus on applying AutoML to ensemble learning and bias mitigation to validate our guidance diagram and other search.\nOur work takes inspiration from earlier empirical studies of fairness techniques (Biswas and Rajan 2021; Friedler et al. 2019; Holstein et al. 2019; Lee and Singh 2021; Singh et al. 2021; Valentim, Lourenc\u0327o, and Antunes 2019; Yang et al. 2020), which help practitioners and researchers better understand the state of the art. But unlike these works, we experiment with ensembles and with fairness stability.\nOur work also offers a new library of bias mitigators. While there have been excellent prior fairness toolkits such as ThemisML (Bantilan 2017), AIF360 (Bellamy et al. 2018), and FairLearn (Agarwal et al. 2018), none of them support ensembles. Ours is the first that is modular enough to investigate a large space of unexplored mitigator-ensemble combinations. We previously published some aspects of our library in a non-archival workshop with no official proceedings, but that paper did not yet discuss ensembles (Hirzel, Kate, and Ram 2021). In another non-archival workshop paper, we discussed ensembles and some of these experimental results (Feffer et al. 2022), but no Hyperopt results and only limited analysis of the guidance diagram, both of which are present in this work.\nLibrary and Datasets Aside from our experiments, one contribution of our work is implementing compatibility between mitigators from AIF360 (Bellamy et al. 2018) and ensembles from scikitlearn (Buitinck et al. 2013). To provide the glue and facilitate searching over a space of mitigator and ensemble configurations, we extended the Lale open-source library for semiautomated data science (Baudart et al. 2021).\nMetrics. This paper uses metrics from scikit-learn, including precision, recall, and F1 score. In addition, we implemented a scikit-learn compatible API for several fairness metrics from AIF360 including disparate impact (as described in Feldman et al. (2015)). We also measure time (in seconds) and memory (in MB) utilized when fitting models.\nEnsembles. Ensemble learning uses multiple weak models to form one strong model. Our experiments use four ensembles supported by scikit-learn: bagging, boosting, voting, and stacking. Following scikit-learn, we use the following terminology to characterize ensembles: A base estimator is an estimator that serves as a building block for the ensemble. An ensemble supports one of two composition types: whether the ensemble consists of identical base estimators (homogeneous, e.g. bagging and boosting) or different ones (heterogeneous, e.g. voting and stacking). For the homogeneous ensembles, we used their most common base estimator in practice: the decision-tree classifier. For the heterogeneous ensembles (voting and stacking), we used a set of typical base estimators: XGBoost (Chen and Guestrin 2016), random forest, k-nearest neighbors, and support vector machines. Finally, for stacking, we also used XGBoost as the final estimator.\nMitigators. We added support in Lale for bias mitigation from AIF360 (Bellamy et al. 2018). AIF360 distinguishes three kinds of mitigators for improving group fairness: pre-estimator mitigators, which are learned input manipulations that reduce bias in the data sent to downstream estimators (we used DisparateImpactRemover (Feldman et al. 2015), LFR (Zemel et al. 2013), and Reweighing (Kamiran and Calders 2012)); in-estimator mitigators, which are specialized estimators that directly incorporate debiasing into their training (AdversarialDebiasing (Zhang, Lemoine, and Mitchell 2018), GerryFairClassifier (Kearns et al. 2018), MetaFairClassifier (Celis et al. 2019), and PrejudiceRemover (Kamishima et al. 2012)); and post-estimator mitigators, which reduce bias in predictions made by an upstream estimator (we used CalibratedEqOddsPostprocessing (Pleiss et al. 2017)).\nFig. 1 visualizes the combinations of ensemble and mitigator kinds we explored, while also highlighting the modularity of our approach. Mitigation strategies can be applied at the level of either the base estimator or the entire ensemble, but by the nature of some ensembles and mitigators, not all combinations are feasible. First, post-estimator mitigators typically do not support predict proba functionality required for some ensemble methods and recommended for others. Calibrating probabilities from post-estimator mitigators has been shown to be tricky (Pleiss et al. 2017), so despite Lale support for other post-estimator mitigators, our\nexperiments only explored CalibratedEqOddsPostprocessing. Additionally, it is impossible to apply an in-estimator mitigator at the ensemble level, so we exclude those combinations. Finally, we decided to omit some combinations that are technically feasible but less interesting. For example, while our library supports mitigation at multiple points, say, at both the ensemble and estimator level of bagging, we elided these configuration from Fig. 1 and from our experiments.\nDatasets. We gathered the datasets for our experiments primarily from OpenML (Vanschoren et al. 2014); the exceptions come from Medical Expenditures Panel Survey (MEPS) data not hosted there (AHRQ 2015, 2016). Some have been used extensively as benchmarks elsewhere in the algorithmic fairness literature. We pulled other novel datasets from OpenML that have demographic data that could be considered protected attributes (such as race, age, or gender) and contained associated baseline levels of disparate impact. In addition, to get a sense for the predictive power of each protected attribute, we fit XGBoost models to each dataset with five different seeds and found the ranking of the average feature importance (where 1 is the most important) of the most predictive protected attribute for that dataset. In all, we used\n13 datasets, with most information summarized in Table 1 and granular feature importance information summarized in the Appendix. When running experiments, we split the datasets using stratification by not just the target labels but also the protected attributes (Hirzel, Kate, and Ram 2021), leading to moderately more homogeneous fairness results across different splits. The exact details of the preprocessing are in the open-source code for our library for reproducibility. We hope that bundling these datasets and default preprocessing with our package, in addition to AIF360 and scikit-learn compatibility, will improve dataset quality going forward.\nMethodology Given our 13 datasets, 4 types of ensembles, 8 mitigators, and all relevant hyperparameters, we wanted to gain insights about the best ways to combine ensemble learning and bias mitigation in various problem contexts and data setups. To this end, we conducted two searches over the Cartesian product of these settings and compared their results. The first was a manual grid search to determine optimal configurations for each dataset. The second also involved finding suitable configurations per dataset, but was automated via Bayesian\noptimization in Hyperopt (Bergstra, Yamins, and Cox 2013)."
        },
        {
            "heading": "Grid Search",
            "text": "We organize our grid search experiments into two steps. The first is a preliminary search that finds \u201cbest\u201d mitigators without ensembles. The second is the ensemble experiments using the mitigator configurations selected by the first.\nFirst step. It is difficult to define \u201cbest\u201d (in an empirical sense) given different dimensions of performance and datasets. To this end, we first run grid searches over each dataset, exploring mitigators and their hyperparameters with basic decision-trees where needed. We run 5 trials of 3-fold cross validation for each configuration. For each dataset, we choose a \u201cbest\u201d pre-, in-, and post-estimator mitigator: 1. Filter configurations to ones with acceptable fairness, de-\nfined as mean disparate impact between 0.8 and 1.25. 2. Filter remaining to ones with nontrivial precision. 3. Filter remaining to ones with good predictive perfor-\nmance, defined as mean F1 score (across 5 trials) greater than the average of all mean F1 scores or the median of all mean F1 scores, whichever is greater. 4. Finally, select the mitigator with maximum precision (in case of COMPAS, since true positives should be prioritized) or recall (all other datasets, since false negatives should be avoided). Tables 12 and 13 in our Appendix list the chosen preestimator and in-estimator configurations (the only postestimator configuration is CalibratedEqOddsPostprocessing).\nSecond step. Given the \u201cbest\u201d mitigator configurations, this step explores the Cartesian product of ensembles and mitigators of Fig. 1 plus ensemble hyperparameters. For bagging and boosting, the only ensemble-level hyperparameter varied between configurations was the number of base estimators: {1, 10, 100} for bagging and {1, 50, 500} for boosting. Voting and stacking use lists of heterogeneous base estimators as hyperparameters. In our experiments, these lists contained\neither 4 mitigated or 4 unmitigated base estimators. For the in-estimator mitigation case these were {PrejudiceRemover, GerryFairClassifier, MetaFairClassifier, and AdversarialDebiasing}. Lastly, stacking also has a passthrough hyperparameter controlling whether dataset features were passed to the final estimator. If passthrough is set to False, it is impossible to mitigate the final estimator due to lack of dataset features; otherwise we mitigate either the base estimators or final estimator, but not both. The second step also uses 5 trials of 3-fold cross validation for each experiment, running on a computing cluster with Intel Xeon E5-2667 processors @ 3.30GHz. Every experiment configuration run was allotted 4 cores and 12 GB memory."
        },
        {
            "heading": "Hyperopt Search",
            "text": "We used Hyperopt to perform another model configuration search, this time in a single step guided by an objective that combined predictive performance and fairness. We defined a single search space that includes all ensembles and mitigators and their hyperparameters. Then, we defined the blended scorer in Fig. 2. Finally, we ran Lale\u2019s Hyperopt wrapper, passing the blended scorer as the objective to maximize and\nNo Mit. Pre- In- Post-\nDO DV DO DV DO DV DO DV\nNo ensemble 0.42 0.18 0.73 0.38 0.87 0.44 0.53 0.24 Bagging 0.31 0.08 0.54 0.19 0.80 0.28 0.44 0.08 Boosting 0.33 0.18 0.69 0.39 0.87 0.26 0.41 0.12 Voting 0.29 0.09 0.51 0.35 0.40 0.45 0.21 0.20 Stacking 0.39 0.19 0.61 0.27 0.44 0.39 0.50 0.27\nTable 2: Standardized Disparate impact Outcome (DO) and Volatility (DV). Note that DO and DV utilize different scales.\nsetting timeouts of 10 minutes per trial and 20 hours total for each dataset, on the same cluster as for grid search."
        },
        {
            "heading": "Results",
            "text": "This section includes quantitative results of our two searches and qualitative guidance regarding future model development based on these results."
        },
        {
            "heading": "Grid Search Results",
            "text": "Result preprocessing. To facilitate cross-dataset comparisons, we applied the following procedure on a per-dataset basis for each metric of interest: (i) given all results, map all values to the same region of metric space around the point of optimal fairness (i.e. for disparate impact, we use the reciprocal of a value if it is larger than 1 for downstream calculations, and for statistical parity difference, we use the absolute value), and (ii) min-max scale the mean and standard deviation of the metric of interest, separately. After doing this for all datasets, we group remaining results by mitigator kind and ensemble type, and average the scaled values over all datasets for each group. Given a metric M , we refer to the result of this procedure using mean values as \u201cstandardized M outcome\u201d and using standard deviation as \u201cstandardized M volatility\u201d. The tables and figures that follow report values normalized as described above.\nDo ensembles help with fairness? Table 2 shows the disparate impact results. Mitigation almost always improved disparate impact outcomes, but ensemble learning generally incurred a slight penalty relative to the no-ensemble baseline. However, ensemble learning does generally reduce disparate impact volatility. In some contexts, this increased stability may be preferred over better yet more unstable predictions.\nDo ensembles help predictive performance when there is mitigation? Table 3 shows F1 results. Even with ensemble learning, mitigation decreases predictive performance, but relative to standalone mitigators, mitigated ensembles typically have better outcomes or stability, but not both. Except for a few cases, mitigated ensembles can help with predictive performance or F1 volatility."
        },
        {
            "heading": "Guidance for method selection",
            "text": "To advise future practitioners based on our results, we generated Fig. 3 from optimal configurations for particular metrics and data setups. To generate it, we do the following:\nNo Mit. Pre- In- Post-\nFO FV FO FV FO FV FO FV\nNo ensemble 0.70 0.20 0.54 0.39 0.51 0.49 0.63 0.19\nBagging 0.93 0.13 0.50 0.19 0.61 0.11 0.65 0.13 Boosting 0.84 0.28 0.49 0.25 0.52 0.28 0.63 0.13 Voting 0.77 0.09 0.40 0.36 0.45 0.50 0.58 0.19 Stacking 0.83 0.26 0.56 0.50 0.67 0.59 0.66 0.27\nTable 3: Standardized F1 outcome (FO) and volatility (FV).\n1. Organize all results by dataset. 2. Filter results for each dataset to ones that occur in the\ntop 33% of results for both standardized disparate impact outcome and standardized F1 outcome. 3. Place each result into one of four quadrants based on the dataset\u2019s baseline fairness and size. 4. Average each metric in each quadrant while grouping by model configuration. 5. Report the top 3 configurations per quadrant and metric.\nLeave-one-out evaluation. One way in which we evaluate our guidance diagram is, for each dataset, to follow the diagram generation steps while leaving out the results pertaining to that dataset, and examine differences in terms of the recommended model configurations and their performances between the new diagram and the one generated from all of the datasets. Because our guidance diagram has three recommendations per metric, the largest number of differences between a leave-one-out diagram and the full dataset diagram for a given metric is three. We also compute signed differences of metric values by subtracting the metric value of the best model recommended by the leave-one-out diagram from that of the full dataset diagram. However, if the diagram creation method generalizes well and the diagram itself has reproducible insights, these differences should be close to zero. Table 4 displays both types of these differences for all omitted datasets and the metrics disparate impact mean, disparate impact standard deviation, and F1 mean. Based on these differences, some datasets have more of an effect on the guidance diagram than others. This phenomenon will be covered in our discussion section."
        },
        {
            "heading": "Hyperopt Result Comparison",
            "text": "We purposely designed a scoring function for Hyperopt (see methodology section) that is is similar to the method we used to filter grid search results to produce the guidance diagram. Therefore, Hyperopt\u2019s solutions provide another way to evaluate the guidance diagram\u2019s suggested configurations.\nTable 5 shows the configurations returned by Hyperopt for each dataset as well as corresponding average disparate impact and F1 score with standard deviations. For comparison, it also shows the same values for the configurations recommended by the guidance diagram when average disparate impact or average F1 score is the metric of interest. A close inspection of the table reveals that while the guidance diagram rarely recommends the exact same configuration as\nthat found by Hyperopt, it can recommend one with approximately the same performance.\nDiscussion This section describes the impacts of our search results and guidance in addition to hypotheses informed by our results regarding biased data.\nGuidance diagram utility and robustness. The previous section showed that the guidance diagram and Hyperopt search recommended configurations with relatively similar performance on most of the datasets. This suggests that the guidance diagram can be useful for future practitioners to recommend starting points for model development based on their data setup and metric(s) of interest. Moreover, note that consulting the guidance diagram for a configuration that may likely produce reasonable results can be done quickly without any resource consumption, while searching may require inordinate amounts of time and compute.\nOur leave-one-out dataset experiments also suggest that our diagram generation algorithm is relatively robust to changes in the input data. This further supports the notion that\nour guidance diagram has useful recommendations. However, those experiments also showed that the presence or absence of certain datasets affected the resulting diagram more than others. For instance, the Credit-g and Bank datasets have more effects on the recommended configurations and model performance than the Adult or COMPAS datasets.\nWe attribute this phenomenon to the filtering of model results that takes place during diagram generation and properties of the datasets themselves. Note that most of the datasets in Table 1 that have large effects on the diagram have baseline disparate impact close to 0.8 (meaning they are relatively fair), and their protected attributes are not strongly predictive (based on feature importance ranking). This implies that with mitigation, it is possible to fit these datasets fairly and accurately. This in turn means model fitting results from those datasets comprise most of the results for the given quadrant after filtering to reasonable fairness and predictive performance. Therefore, when those datasets are missing, the generated diagram differs greatly from the one generated with all data. (The exceptions to this rule are TAE and Titanic. Given that those are the only two datasets in their quadrant and protected attributes are strongly predictive, it is difficult to fit either well in a fair manner. Therefore, neither contributes many fitting results after filtering, and both have tangible effects on the diagram.)\nIn light of how protected attribute feature importance of input datasets affects recommendations of the guidance diagram, one limitation of our diagram is that we do not have branches for this property of a dataset (and therefore do not provide recommendations based on this property). We counter that determining this property requires training XGBoost models, which can take time and resources, while the other properties utilized can be quickly calculated. Thus, we still argue that this is useful to future practitioners.\nWhat is \u201cgood data?\u201d As mentioned in Holstein et al. (2019), \u201cfuture research\u201d in the area of algorithmic fairness should \u201c[develop] processes and tools for fairness-focused debugging\u201d and \u201cshould also support practitioners in col-\nlecting and curating high-quality datasets in the first place\u201d. These recommendations suggest how to collect good data? and what even is \u2018good data\u2019? are questions with which the field is currently grappling.\nWe believe our results shed some light on these fronts. First, our findings suggest that converting \u201cbad data\u201d to \u201cgood data\u201d may not (just) involve making datasets larger in number of examples but (also) making them larger in number of features. Prevailing notions of algorithmic fairness may imply that the best way to fix an unfair dataset is to add examples to reduce bias. While this may work, it could be difficult to do in practice (especially given societal mechanisms behind bias), and Holstein et al. (2019) also raise that \u201cHow much data [one] would need to collect?\u201d does not typically have a clear answer. However, our results imply that collecting more data to alleviate bias may take the form of gathering more features instead of simply gathering more examples with the same features.\nThat being said, datasets like Adult and Ricci included attributes that were more predictive than protected attributes, yet they still did not strongly influence our guidance diagram. We conjecture that attributes that were more predictive were highly correlated with protected attributes, and feature importance tables included in our Appendix seem to support this notion. Therefore, if one wants to collect more features to reduce bias, they should take care to ensure that these features are not correlated with protected attributes.\nLastly, we want to highlight that regardless of the form such data collection may take, it is imperative to consider the ethics of doing so and respect wishes and privacy of the individuals whose data are utilized during the model building process and who are affected by the model predictions.\nConclusion This paper introduces a library of modular bias mitigators and ensembles and detail experiments that confirm ensembles can improve fairness stability. We also provide generalizable guidance to practitioners based on their data setup."
        },
        {
            "heading": "Additional Tables",
            "text": ""
        },
        {
            "heading": "Supplemental Material",
            "text": ""
        }
    ],
    "title": "Navigating Ensemble Configurations for Algorithmic Fairness",
    "year": 2022
}