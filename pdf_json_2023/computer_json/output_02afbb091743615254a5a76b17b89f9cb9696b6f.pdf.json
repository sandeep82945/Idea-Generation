{
    "abstractText": "Recoverable algorithms tolerate failures and recoveries of processes by using non-volatile memory. Of particular interest are self-implementations of key operations, in which a recoverable operation is implemented from its non-recoverable counterpart (in addition to reads and writes). This paper presents two self-implementations of the SWAP operation. One works in the systemwide failures model, where all processes fail and recover together, and the other in the independent failures model, where each process crashes and recovers independently of the other processes. Both algorithms are wait-free in crash-free executions, but their recovery code is blocking. We prove that this is inherent for the independent failures model. The impossibility result is proved for implementations of distinguishable operations using interfering functions, and in particular, it applies to a recoverable self-implementation of swap. 2012 ACM Subject Classification Theory of computation \u2192 Shared memory algorithms",
    "authors": [
        {
            "affiliations": [],
            "name": "Tomer Lev Lehman"
        },
        {
            "affiliations": [],
            "name": "Hagit Attiya"
        },
        {
            "affiliations": [],
            "name": "Danny Hendler"
        }
    ],
    "id": "SP:842d4a02824c393ac29ce72fa03b97c4018fcd15",
    "references": [
        {
            "authors": [
                "Marcos K Aguilera",
                "Svend Fr\u00f8lund"
            ],
            "title": "Strict linearizability and the power of aborting",
            "venue": "Technical Report HPL-2003-241,",
            "year": 2003
        },
        {
            "authors": [
                "Hagit Attiya",
                "Ohad Ben-Baruch",
                "Danny Hendler"
            ],
            "title": "Nesting-safe recoverable linearizability: Modular constructions for non-volatile memory",
            "venue": "In Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing,",
            "year": 2018
        },
        {
            "authors": [
                "Ohad Ben-Baruch",
                "Danny Hendler",
                "Matan Rusanovsky"
            ],
            "title": "Upper and lower bounds on the space complexity of detectable objects",
            "venue": "In Proceedings of the 39th Symposium on Principles of Distributed Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Naama Ben-David",
                "Guy E Blelloch",
                "Michal Friedman",
                "Yuanhao Wei"
            ],
            "title": "Delay-free concurrency on faulty persistent memory",
            "venue": "In The 31st ACM Symposium on Parallelism in Algorithms and Architectures,",
            "year": 2019
        },
        {
            "authors": [
                "Naama Ben-David",
                "Michal Friedman",
                "Yuanhao Wei"
            ],
            "title": "Survey of persistent memory correctness conditions",
            "venue": "arXiv preprint arXiv:2208.11114,",
            "year": 2022
        },
        {
            "authors": [
                "Ryan Berryhill",
                "Wojciech Golab",
                "Mahesh Tripunitara"
            ],
            "title": "Robust shared objects for nonvolatile main memory",
            "venue": "In 19th International Conference on Principles of Distributed Systems (OPODIS",
            "year": 2015
        },
        {
            "authors": [
                "Dhruva R Chakrabarti",
                "Hans-J Boehm",
                "Kumud Bhandari"
            ],
            "title": "Atlas: Leveraging locks for non-volatile memory consistency",
            "venue": "ACM SIGPLAN Notices,",
            "year": 2014
        },
        {
            "authors": [
                "David Yu Cheng Chan",
                "Philipp Woelfel"
            ],
            "title": "Recoverable mutual exclusion with constant amortized rmr complexity from standard primitives",
            "venue": "In Proceedings of the 39th Symposium on Principles of Distributed Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Kyeongmin Cho",
                "Seungmin Jeon",
                "Jeehoon Kang"
            ],
            "title": "Practical detectability for persistent lock-free data structures",
            "venue": "arXiv preprint arXiv:2203.07621,",
            "year": 2022
        },
        {
            "authors": [
                "Nachshon Cohen",
                "Rachid Guerraoui",
                "Igor Zablotchi"
            ],
            "title": "The inherent cost of remembering consistently",
            "venue": "In Proceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures,",
            "year": 2018
        },
        {
            "authors": [
                "Andreia Correia",
                "Pascal Felber",
                "Pedro Ramalhete"
            ],
            "title": "Romulus: Efficient algorithms for persistent transactional memory",
            "venue": "In Proceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures,",
            "year": 2018
        },
        {
            "authors": [
                "Andreia Correia",
                "Pascal Felber",
                "Pedro Ramalhete"
            ],
            "title": "Persistent memory and the rise of universal constructions",
            "venue": "In Proceedings of the Fifteenth European Conference on Computer Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tudor David",
                "Aleksandar Dragojevic",
                "Rachid Guerraoui",
                "Igor Zablotchi"
            ],
            "title": "Log-free concurrent data structures",
            "venue": "USENIX Annual Technical Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Panagiota Fatourou",
                "Nikolaos D Kallimanis",
                "Eleftherios Kosmas"
            ],
            "title": "The performance power of software combining in persistence",
            "venue": "In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,",
            "year": 2022
        },
        {
            "authors": [
                "Michael J Fischer",
                "Nancy A Lynch",
                "Michael S Paterson"
            ],
            "title": "Impossibility of distributed consensus with one faulty process",
            "venue": "Journal of the ACM (JACM),",
            "year": 1985
        },
        {
            "authors": [
                "Michal Friedman",
                "Naama Ben-David",
                "Yuanhao Wei",
                "Guy E Blelloch",
                "Erez Petrank"
            ],
            "title": "Nvtraverse: In nvram data structures, the destination is more important than the journey",
            "venue": "In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation,",
            "year": 2020
        },
        {
            "authors": [
                "Michal Friedman",
                "Maurice Herlihy",
                "Virendra Marathe",
                "Erez Petrank"
            ],
            "title": "A persistent lock-free queue for non-volatile memory",
            "venue": "ACM SIGPLAN Notices,",
            "year": 2018
        },
        {
            "authors": [
                "Wojciech Golab"
            ],
            "title": "Recoverable consensus in shared memory",
            "venue": "arXiv preprint arXiv:1804.10597,",
            "year": 2018
        },
        {
            "authors": [
                "Wojciech Golab"
            ],
            "title": "The recoverable consensus hierarchy",
            "venue": "In Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms and Architectures,",
            "year": 2020
        },
        {
            "authors": [
                "Wojciech Golab",
                "Danny Hendler"
            ],
            "title": "Recoverable mutual exclusion in sub-logarithmic time",
            "venue": "In Proceedings of the ACM Symposium on Principles of Distributed Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Wojciech Golab",
                "Aditya Ramaraju"
            ],
            "title": "Recoverable mutual exclusion",
            "venue": "In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,",
            "year": 2016
        },
        {
            "authors": [
                "Rachid Guerraoui",
                "Ron R Levy"
            ],
            "title": "Robust emulations of shared memory in a crash-recovery model",
            "venue": "In 24th International Conference on Distributed Computing Systems,",
            "year": 2004
        },
        {
            "authors": [
                "Maurice Herlihy"
            ],
            "title": "Wait-free synchronization",
            "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),",
            "year": 1991
        },
        {
            "authors": [
                "Maurice P Herlihy",
                "Jeannette M Wing"
            ],
            "title": "Linearizability: A correctness condition for concurrent objects",
            "venue": "ACM Transactions on Programming Languages and Systems (TOPLAS),",
            "year": 1990
        },
        {
            "authors": [
                "Morteza Hoseinzadeh",
                "Steven Swanson"
            ],
            "title": "Corundum: Statically-enforced persistent memory safety",
            "venue": "In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph Izraelevitz",
                "Terence Kelly",
                "Aasheesh Kolli"
            ],
            "title": "Failure-atomic persistent memory updates via justdo logging",
            "venue": "ACM SIGARCH Computer Architecture News,",
            "year": 2016
        },
        {
            "authors": [
                "Joseph Izraelevitz",
                "Hammurabi Mendes",
                "Michael L Scott"
            ],
            "title": "Linearizability of persistent memory objects under a full-system-crash failure model",
            "venue": "In Distributed Computing: 30th International Symposium,",
            "year": 2016
        },
        {
            "authors": [
                "Prasad Jayanti",
                "Siddhartha Jayanti",
                "Anup Joshi"
            ],
            "title": "Optimal recoverable mutual exclusion using only fasas",
            "venue": "In Networked Systems: 6th International Conference, NETYS 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Prasad Jayanti",
                "Siddhartha Jayanti",
                "Anup Joshi"
            ],
            "title": "A recoverable mutex algorithm with sub-logarithmic RMR on both cc and dsm",
            "venue": "In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing,",
            "year": 2019
        },
        {
            "authors": [
                "Prasad Jayanti",
                "Siddhartha Jayanti",
                "Anup Joshi"
            ],
            "title": "Constant rmr recoverable mutex under system-wide crashes",
            "venue": "arXiv preprint arXiv:2302.00748,",
            "year": 2023
        },
        {
            "authors": [
                "Prasad Jayanti",
                "Anup Joshi"
            ],
            "title": "Recoverable FCFS mutual exclusion with wait-free recovery",
            "venue": "In 31st International Symposium on Distributed Computing (DISC 2017). Schloss DagstuhlLeibniz-Zentrum fuer Informatik,",
            "year": 2017
        },
        {
            "authors": [
                "Prasad Jayanti",
                "Anup Joshi"
            ],
            "title": "Recoverable mutual exclusion with abortability",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Katzan",
                "Adam Morrison"
            ],
            "title": "Recoverable, abortable, and adaptive mutual exclusion with sublogarithmic rmr complexity",
            "venue": "arXiv preprint arXiv:2011.07622,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Katzan",
                "Adam Morrison"
            ],
            "title": "Recoverable, abortable, and adaptive mutual exclusion with sublogarithmic rmr complexity",
            "venue": "In 24th International Conference on Principles of Distributed Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nan Li",
                "Wojciech Golab"
            ],
            "title": "Detectable sequential specifications for recoverable shared objects",
            "venue": "In 35th International Symposium on Distributed Computing (DISC 2021). Schloss Dagstuhl-Leibniz-Zentrum fu\u0308r Informatik,",
            "year": 2021
        },
        {
            "authors": [
                "John M Mellor-Crummey",
                "Michael L Scott"
            ],
            "title": "Algorithms for scalable synchronization on shared-memory multiprocessors",
            "venue": "ACM Transactions on Computer Systems (TOCS),",
            "year": 1991
        },
        {
            "authors": [
                "Liad Nahum",
                "Hagit Attiya",
                "Ohad Ben-Baruch",
                "Danny Hendler"
            ],
            "title": "Recoverable and detectable fetch&add",
            "venue": "In 25th International Conference on Principles of Distributed Systems (OPODIS 2021). Schloss Dagstuhl-Leibniz-Zentrum fu\u0308r Informatik,",
            "year": 2022
        },
        {
            "authors": [
                "Faisal Nawab",
                "Joseph Izraelevitz",
                "Terence Kelly",
                "Charles B Morrey III",
                "Dhruva R Chakrabarti",
                "Michael L Scott"
            ],
            "title": "Dal\u00ed: A periodically persistent hash map",
            "venue": "In 31st International Symposium on Distributed Computing (DISC 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,",
            "year": 2017
        },
        {
            "authors": [
                "Pedro Ramalhete",
                "Andreia Correia",
                "Pascal Felber",
                "Nachshon Cohen"
            ],
            "title": "Onefile: A wait-free persistent transactional memory",
            "venue": "49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN),",
            "year": 2019
        },
        {
            "authors": [
                "David Schwalb",
                "Markus Dreseler",
                "Matthias Uflacker",
                "Hasso Plattner"
            ],
            "title": "NVC-hashmap: A persistent and concurrent hashmap for non-volatile memories",
            "venue": "In Proceedings of the 3rd VLDB Workshop on In-Memory Data Mangement and Analytics,",
            "year": 2015
        },
        {
            "authors": [
                "Jae-Heon Yang",
                "Jams H Anderson"
            ],
            "title": "A fast, scalable mutual exclusion algorithm",
            "venue": "Distributed Computing,",
            "year": 1995
        },
        {
            "authors": [
                "Yoav Zuriel",
                "Michal Friedman",
                "Gali Sheffi",
                "Nachshon Cohen",
                "Erez Petrank"
            ],
            "title": "Efficient lock-free durable sets",
            "venue": "Proceedings of the ACM on Programming Languages,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "This paper presents two self-implementations of the SWAP operation. One works in the systemwide failures model, where all processes fail and recover together, and the other in the independent failures model, where each process crashes and recovers independently of the other processes.\nBoth algorithms are wait-free in crash-free executions, but their recovery code is blocking. We prove that this is inherent for the independent failures model. The impossibility result is proved for implementations of distinguishable operations using interfering functions, and in particular, it applies to a recoverable self-implementation of swap.\n2012 ACM Subject Classification Theory of computation \u2192 Shared memory algorithms\nKeywords and phrases Persistent memory, non-volatile memory, recoverable objects, detectablitly\n1 Introduction\nRecent years have seen a rising interest in the failure-recovery model for concurrent computing. This model captures an unstable system, where processes may crash and recover. Two variants of the model have been considered. In the system-wide failure model (also called the globalcrash model), all processes fail simultaneously and a single process is responsible for the recovery of the whole system. In the independent failures model (also called the individualcrash model), each process can incur a crash independently of other processes and recovers independently. Recoverable algorithms, tolerating failures and recoveries, have been presented for various concurrent data structures, for both the system-wide model [9,11,14,17,25,35,37,40] and the independent-failure model [2, 4, 9, 35,37].\nThe correctness of a recoverable algorithm can be specified in several ways. Durable Linearizability [27] intuitively requires linearizability [24] of all operations that survive the crashes. Detectability [17] ensures that upon recovery, it is possible to infer whether the failed operation took effect or not and, in the former case, obtain its response. Nesting-safe Recoverable Linearizability (NRL) [2], defined for the independent failures model, ensures detectability and linearizability. It also allows the nesting of recoverable objects. By providing implementations of NRL primitive objects, a programmer can combine several of these primitives to create recoverable implementations of complex higher-level objects and algorithms. This level of abstraction can be helpful in the adoption of recoverable algorithms.\nTo facilitate high-level implementations of complex NRL objects it is helpful to introduce implementations of low-level base NRL objects. An attractive approach to designing low-level base NRL objects is through self-implementations [37], in which a recoverable operation is implemented with instances of the same primitive operation, possibly with additional reads and writes on shared variables. This approach ensures that when using the recoverable\nar X\niv :2\n30 8.\n03 48\n5v 1\n[ cs\n.D C\n] 7\nA ug\n2 02\n3\nversion of an operation, the system must only support its hardware-implemented primitive counterpart.\nNRL self-implementations already exist for various primitives, including read, write, test&set, and compare&swap [2, 4], as well as fetch&add [37]. A universal construction using NRL read, write and compare&swap objects [4] builds upon previously-introduced self-implementations of NRL objects to take any concurrent program with read write and CAS, and make it recoverable while adding only constant computational overhead.\nThis paper presents the first NRL self-implementations of swap, for both the system-wide and the independent failures models. Swap is a widely-used primitive that is employed by concurrent algorithms. Our implementations borrow ideas from the recoverable mutual exclusion (RME) [21] algorithms of [20, 29], which use a similar approach to overcome swap failures. Unlike these algorithms, however, our implementations are also challenged with the task of satisfying wait-freedom and linearizability. Both our algorithms are wait-free in crash-free executions, while the recovery code in both is blocking.\nWe also present an impossibility proof for implementing a class of distinguishable operations using a set of interfering functions [23] in a recoverable lock-free fashion in the independent failures model. In particular, this result applies to self-implementations of swap, but it also holds for, e.g., implementing swap using fetch-and-add and swap combined. Other distinguishable operations to which this proof applies are the deque of a queue object and the pop of a stack object. Our impossibility result unifies and extends specialized results for self-implementations of test&set [2] and fetch&add [37]. Another related impossibility result addresses recoverable consensus in the independent failures model [19].\nSeveral previous papers introduce general mechanisms to port existing algorithms and make them persistent, e.g., by using transactional memory [7, 11, 26, 39], universal constructions [6, 10,12], or for specific families of algorithms [4,13,16]. Most of these transformations use strong primitives such as compare&swap while their non-recoverable counterparts may use weaker primitives, in terms of their consensus number [23]. We believe future research may use our self-implementation of swap to extend general constructions such as [4] mentioned above to programs that also use swap as a primitive.\nOther papers present hand-crafted persistent implementations of specific data structures, e.g., [17,38,41,43]. In contrast to these implementations, our algorithms provide a recoverable counterpart to an atomic primitive operation, which we believe can later be used in various other implementations of recoverable algorithms. Similarly to NRL, detectable sequence specifications (DSS), introduced by Li and Golab [35], formalizes the notion of detectability. The DSS-based approach is more portable and less reliant on system assumptions in comparison to NRL, but delegates the responsibility for nesting to application code.\nOur algorithm for the independent failures model uses an RME lock such as the one presented by Golab and Ramaraju [21], which uses only reads and writes. Additionally, a long line of papers on RME exists, solving several other aspects such as abortability, FCFS, and more [8, 28,30\u201332,34].\nTo summarize, our contributions are the following: A recoverable detectable self-implementation of swap in the system-wide failures model. A recoverable detectable self-implementation of swap in the independent failures model. An impossibility proof for implementations of distinguishable operations using interfering functions in the independent failures model.\n2 Related Work"
        },
        {
            "heading": "2.1 Correctness conditions",
            "text": "Various correctness conditions and definitions exist for recoverable algorithms utilizing persistent memory. Strict linearizability [1] requires operations interrupted by a failure to take effect either before the failure or not at all. A relaxed condition called persistent atomicity [22] in the context of message passing systems, allows an operation to take effect even after a failure, before the next operation invocation of the same process. Recoverable linearizability [6] builds on the definition of persistent atomicity and restores locality \u2013 the desirable property that an execution involving multiple objects is correct if and only if its projection onto each individual object is correct [18].\nDurable linearizability [26], defined for the system-wide failures model, assumes that, upon a crash, all processes fail and do not recover, and that new processes are spawned instead. Durable linearizability requires linearizablity of all operations surviving crashes, essentially requiring linearizability of the history when pending operations and their respective crash events are removed from the history.\nDetectability [17] requires that an object provides a mechanism that can tell, for every failed operation, whether or not it was completed, and if so obtain its response. Detectability can be added as an extra requirement for various correctness properties for implementations to satisfy\nDetectable sequential specifications (DSS) [35] formalizes the notion of detectability by adding three new operations for every operation op, prep-op, exec-op, and resolve. Calling prep-op notifies the system that it should remember the outcome of the upcoming operations, exec-op executes the operation and resolve can then be called after a crash to return the outcome of the most recently prepared operation (i.e., one for which prep-op was called). DSS can be paired with any of the previously stated correctness conditions to obtain a detectable version of those properties. The DSS-based approach is more portable and less reliant on system assumptions in comparison to NRL, but delegates the responsibility for nesting to application code. In addition, the developer must implement 2 extra operations (prep-op, exec-op) for each DSS-compliant operation."
        },
        {
            "heading": "2.2 Recoverable Mutual Exclusion",
            "text": "The Recoverable Mutual Exclusion(RME) problem, formulated by [21], extends the classic mutual exclusion problem for systems in which processes may crash and then recover. In addition to standard mutual exclusion properties such as deadlock-freedom and starvationfreedom, RME may also require the Critical Section Re-entry (CSR) property, ensuring that if a process p crashes within the critical section, then no other process can enter the critical section before p recovers and re-enters it [5].\nSeveral RME implementations were presented in recent years. Golab and Ramaraju presented an n-process RME implementation using only reads and writes [21]. It is modeled after the non-recoverable ME algorithm of Yang and Anderson [42]. The algorithm is based on a binary tournament tree of height O(log n) where each node is a two-process mutex (also introduced in [21]).\nOther RME implementations include a first-come-first-served (FCFS) RME algorithm with O(log n) Remote Memory Reference (RMR) complexity [31], an abortable sub-logarithmic RMR complexity RME algorithm [33], and an abortable FCFS RME algorithm [32].\nGolab and Hendler introduce an RME algorithm for the cache-coherent(CC) model [20],\nusing fetch&store and compare&swap, which incurs O( log nlog log n ) RMRs. Their design is inspired by Mellor-Crummey and Scott\u2019s queue lock (MCS lock) [36]. The MCS lock maintains a queue based structure that determines the order of entry into the CS. Each node in the queue also holds a boolean that is used to transfer ownership of the CS from a predecessor to a successor. Jayanti et al. [29] presented an RME algorithm with the same asymptotic RMR complexity for both the distributed shared memory (DSM) and the CC models."
        },
        {
            "heading": "2.3 Recoverable implementations",
            "text": "Various hand-crafted recoverable implementations exist for specific data structures. In [17], the authors propose three different implementations of a concurrent lock-free queue, each satisfying different correctness properties, including durable linearizability and detectability. All three build on Michael and Scott\u2019s queue and consist of a linked list of nodes that hold the enqueued values, as well as head and the tail references. In addition, there exist hand-crafted recoverable implementations of concurrent hash-maps [38,41,43].\nA different approach is to introduce general mechanisms to port existing algorithms and make them persistent. Mechanisms based on transactional memory [7,11,26,39] generally utilize various log-based methods to ensure durability of existing algorithms. Berryhill et al. [6] take Herlihy\u2019s universal construction [23] and transform it to satisfy recoverable linearizability. ONLL [10] takes any deterministic object O and produces a lock-free durablylinearizable implementation of O that requires at most one persistent fence per update operation and no persistent fence for read-only operations.\nAnother approach consists of mechanisms designed for specific families of algorithms. Ben-David et al. [4] present a method that allows taking any concurrent program with reads, writes and CASs to shared memory and make it recoverable by utilizing code capsules. David et al. [13] focus on link based data structures and provide generic techniques that enable designing what they call log-free concurrent data structures. Friedman et al. [16] present a general transformation that takes a lock-free data structure from a general class of node-based tree data structures (traversal data structures) and automatically transforms it into a durably-linearizable implementation of the data structure.\n3 Model and Definitions\nWe use a simplified version of the NRL system model [2]. There are n asynchronous processes p1, . . . , pn, which communicate by applying atomic primitive read, write and read-modifywrite operations to base objects. The state of each process consists of non-volatile shared variables, which serve as base objects, as well as volatile local variables.\nWe first describe the independent failures model, in which each process can incur a crash-failure (or simply a crash) at any point during the execution independently of other processes. A crash resets all of its local variables to arbitrary values but preserves the values of all non-volatile variables.\nA process p invokes an operation Op on an object by performing an invocation step. Op completes by executing a response step, in which the response of OP is stored to a local volatile variable of p. It follows that the response value is lost if p incurs a crash before persisting it, that is, before writing it to a non-volatile variable. Operation Op is pending if it was invoked but was not yet completed; a process has at most one pending operation.\nA recoverable operation Op is associated with a recovery procedure Op.RECOVER that is responsible for completing Op upon recovery from a crash. If the object only supports a single\nrecoverable operation, then its recovery procedure is simply named RECOVER. Following a crash of process p that occurs when p has a pending recoverable operation instance, the system eventually resurrects process p by invoking the recovery procedure of the recoverable operation that was pending when p failed. This is represented by a recovery step for p.\nFormally, a history H is a sequence of steps. There are four types of steps: 1. An invocation step, denoted (INV, p, O, Op), represents the invocation by process p of\noperation Op on object O. 2. A response step s, denoted (RES, p, O, Op, ret), represents the completion by process p\nof operation Op invoked on object O by some (invocation) step s\u2032 of p, with response ret being written to a local variable of p; s is the response step that matches s\u2019. An operation Op can be completed either normally or when, following one or more crashes, the execution of Op.RECOVER is completed. 3. A crash step s, denoted (CRASH, p), represents the crash of process p. We call the recoverable operation Op of p that was pending when the crash occurred the crashed operation of s. (CRASH, p) may also occur when the recovery procedure Op.RECOVER is executed for recovering an operation of p and we say that Op is the crashed operation of s also in this case. 4. A recovery step s for process p, denoted (REC, p), is the only step by p that is allowed to follow a (CRASH, p) step s\u2032. It represents the resurrection of p by the system, in which it invokes Op.RECOVER, where Op is the crashed operation of s\u2032. We say that s is the recovery step that matches s\u2032.\nWhen a recovery procedure Op.RECOVER is invoked to recover from a crash represented by step s, we assume it receives the same arguments as those with which Op was invoked when that crash occurred.\nAs proven by [3], detectable algorithms for the NRL model must keep an auxiliary state that is provided from outside the operation, either via operation arguments or via a nonvolatile variable accessible by it. We assume that Op.RECOVER has access to a designated per-process non-volatile variable SEQp, storing the sequence number of Op. Before p invokes an operation on the object, it increments SEQp.\nAn object is a recoverable object if all its operations are recoverable. Below, we consider only histories that arise from operations on recoverable objects or atomic primitive operations.\nFix a history H. H is crash-free if it contains no crash steps (hence also no recovery steps). H|p denotes the sub-history of H consisting of all the steps by process p in H. H|O denotes the sub-history of H consisting of all the invoke and response steps on object O in H, as well as any crash step in H, by any process p, whose crashed operation is an operation on O and the corresponding recovery step by p (if it appears in H). H|<p, O> denotes the sub-history consisting of all the steps on O by p.\nA crash-free sub-history H|O is well-formed, if for all processes p, H|<p, O> is a sequence of alternating, matching invocation and response steps, starting with an invocation step. A crash-free history H is well-formed if: (1) H|O is well-formed for all objects O, and (2) each invocation event in H|p, except possibly the last one, is immediately followed by its matching response step.\nH|O is a sequential object history if it is an alternating series of invocations and the matching responses starting with an invocation; it may end with a pending invocation. The sequential specification of an object O is the set of all legal sequential histories over O. H is a sequential history if H|O is a sequential object history for all objects O.\nTwo histories H and H \u2032 are equivalent, if H|<p, O> = H \u2032|<p, O> for all processes p and objects O. Given a history H, a completion of H is a history H \u2032 constructed from H by\nselecting separately, for each object O that appears in H, a subset of the operations pending on O in H and appending matching responses to all these operations, and then removing all remaining pending operations on O (if any).\nAn operation op1 happens before an operation op2 in H, denoted op1 <H op2, if op1\u2019s response step precedes the invocation step of op2 in H.\n\u25b6 Definition 1 (Linearizability [24], rephrased). A finite crash-free history H is linearizable if it has a completion H \u2032 and a legal sequential history S such that H \u2032 is equivalent to S and <H\u2286<S (i.e., if op1 <H op2 and both op1 and op2 appear in S, then op1 <S op2).\nTo define nesting-safe recoverable linearizability, we introduce a more general notion of well-formedness that applies also to histories that contain crash/recovery steps. For a history H, we let N(H) denote the history obtained from H by removing all crash and recovery steps. A history H is recoverable well-formed if every crash step in H|p is either p\u2019s last step in H or is followed in H|p by a matching recovery step of p, and N(H) is well-formed.\n\u25b6 Definition 2 (Nesting-safe Recoverable Linearizability (NRL)). A finite history H satisfies nesting-safe recoverable linearizability (NRL) if it is recoverable well-formed and N(H) is a linearizable history. An object implementation satisfies NRL if all of its finite histories satisfy NRL.\nWe build upon the above definitions also for the system-wide failures model, but we require that if a crash step occurs, then it occurs simultaneously for all processes whose operations crash. Formally, let pi1 , . . . , pik , for k \u2264 n, be the set of processes that have a pending invocation of operation Op when a system-wide crash occurs. We represent the crash by appending the sequence (CRASH, pi1), . . . , (CRASH, pik ) to the execution. During recovery, the system executes a parameterless global recovery procedure for Op called Op.GRECOVER. We represent the recovery by appending the sequence (REC, pi1), . . . , (REC, pik ) to the execution. Once Op.GRECOVER completes, the system resurrects each of the processes pi1 , . . . , pik for performing an individual recovery procedure for Op, called Op.RECOVER. New operations on the object can be invoked only after recovery ends. If Op is the single object operation, we use the names GRECOVER and RECOVER instead of Op.GRECOVER and Op.RECOVER, respectively.\nAn algorithm is lock-free if, whenever a set of processes take a sufficient number of steps and none of them crashes, then it is guaranteed that one of them will complete its operation. An algorithm is wait-free, if any process that does not incur a crash during its execution completes it in a finite number of its steps. A swap object supports the SWAP(val) operation, which atomically swaps the object\u2019s current value cur to val and returns cur.\n4 Detectable Swap Algorithm for the System-Wide Failures Model\nA key challenge to overcome when implementing a detectable swap object from read, write, and primitive swap operations is that the return values of one or more primitive swap operations may be lost upon a system-wide failure that occurs before the operations are persisted. These non-persisted operations may have already affected the state of the swap object and, moreover, operations by other processes may have already returned the values written by these primitive operations. To ensure linearizability, the implementation must identify such operations and handle them correctly.\nThe return value of each SWAP operation must meet a few requirements. First, it should be the input of another SWAP operation (or the initial value of the swap object). Second,\nthe operand swapped in by one SWAP operation can be returned by at most a single other SWAP operation. Finally, in order to maintain linearizability, if op1 <H op2 holds, then op1 cannot return the value that was swapped in by op2.\nFigure 1 illustrates a scenario involving 6 processes, denoted p1, . . . p6, which perform 8 SWAP operations, denoted op0, . . . op7. A system-wide crash occurs when operations op0, op2, op4, op6 have already been completed (hence their return values are specified) while operations op1, op4, op5, op7 are pending. Note that operations op1, op4, op5, although not completed, have surely affected the global state of the swap object as their inputs are the return values of other operations, while op7 (pending as well) might or might not have affected the object\u2019s state.\nThere are several ways the system may recover in order to produce a correct linearizable result. In all of them, op1 must return 0. The remaining operations might return different values in the following ways: (1) op4 returns 2, op5 returns 3, and op7 returns 6. (2) op7 returns 2, op4 returning 7, and op5 returns 3. (3) op4 returns 2, op7 returns 3, and op5 returns 7. There are several possible linearizations in this example, because op7 may be linearized in several ways since its effect on the global state is unknown. Note that for correctly recovering op1, the operand of the very first operation, op0, must be recorded. This is why our algorithms retain a record of all invoked operations.\nWe represent the order of SWAP operations as a linked list of Node structures, the end of which is pointed by a tail variable manipulated with primitive swaps. The list starts with a sentinel node called headNode, which holds the object\u2019s initial value (denoted \u22a5).\nEach Node structure represents a single SWAP operation and stores a pointer prev to the node of its predecessor operation and the SWAP\u2019s operand val. The order of SWAP operations is reflected by the order of the Node structures in the list. By doing so, each Node points to the previous Node structure that represents the previous SWAP operation, hence the operation\u2019s return value will be Node.prev.val.\nA problem can occur if a process successfully swaps its Node into the list but crashes before pointing from its structure to the previous Node. This type of failure may create what is referred to as fragments in the list representing the SWAP operations. Thus, instead of a single complete list, crashes may result in several incomplete disconnected lists. In order to reconnect these fragments back to a complete list, our algorithms go over all previously-announced operations upon recovery and recreate a correctly ordered complete list of operations.\nA similar idea was used by the recoverable mutual exclusion (RME) algorithms of Golab and Hendler [20] and Jayanti et al. [29], which also have to reconnect the fragments of an MCS lock [36] linked-list based queue, caused by failures that occur just before or after primitive swap operations.\nOur algorithms need to address two challenges that do not exist in the setting of [20, 29],\nhowever. First, the SWAP operations of our algorithms are required to be wait-free whereas RME implementations are allowed to block. Second, unlike RME implementations, our algorithms are required to maintain linearizability. Specifically, the new order of list fragments, constructed during recovery, must respect the real-time order between SWAP operations.\nWe address these challenges by employing a fragment ordering scheme, which we view as the key algorithmic novelty of our algorithms. The scheme encapsulates the critical steps of each SWAP operation by two vector timestamp computations. Based on the resulting timestamps, the recovery code ensures the following invariant: if a fragment A contains a Node nA that was created after an operation associated with a Node nB on fragment B was completed, then fragment B will be ordered after fragment A in the connected list. We formally describe this fragment ordering scheme in Definition 3.\nFigure 2 presents a set of fragments that may be generated immediately after the systemwide crash ending the execution depicted in Figure 1. We describe it next and introduce a few terms that are used later in the sequel. The fragment of op7 contains a single Node; we name such 1-size fragments singleNodes. The node of op0 belongs to the single fragment that contains headNode; we name this fragment the head fragment. The nodes of op5 and op6 belong to the single fragment that contains the node pointed to by tail; we name this fragment the tail fragment. Fragments such as (op2, op1) and (op3, op4) that are neither singleNodes, nor head nor tail fragments are called middle fragments.\nWhen ordering middle fragments and singleNodes, the algorithm uses vector timestamps for maintaining linearizability. As an example, consider a linked list, reconnecting the fragments of Figure 2, in which op4.prev \u2190 op0, op1.prev \u2190 op3, op7.prev \u2190 op2 and op5.prev \u2190 op7. Although this list contains the Nodes of all operations from tail to head, it violates linearizability because op3 is ordered after op2 although it follows it in real-time order. By using the two vector timestamps, our algorithm is able to order the fragments so that linearizability is maintained.\nAnother case that may arise is a set of fragments that consists of a single complete list with one or more singleNodes, which results from a crash that occurs when none of the pending operations completed their primitive swap operations. As we prove, in this case it is safe to put all these singleNodes (in any order) at the end of the list.\nAlgorithm 1 Recoverable detectable SWAP for system-wide failures model, code for process i.\n// Text in blue is for the independent failures algorithm only. Define Node: struct {val : Value, prev : ref to Node, prevExecution : ref to Node, startVts : vector, endVts : vector, seq : int, inWork : int} Initial State: Nodes[i]\u2190 null for i \u2208 {0, . . . , n} V T S[i]\u2190 0 for i \u2208 {1, . . . , n} headNode\u2190 new(Node) headNode.val \u2190 \u22a5, headNode.seq \u2190 0, headNode.prev \u2190 null, headNode.prevExecution\u2190 null, headNode.inW ork \u2190 0. headNode.startV ts\u2190 collect(V T S), headNode.endV ts\u2190 collect(V T S) tail\u2190 headNode Nodes[0]\u2190 headNode 1: procedure SWAP(val) \u25b7 executed by process i 2: myNode\u2190 new(Node) 3: myNode.prev\u2190null, myNode.seq\u2190 SEQi, myNode.prevExecution\u2190 null, myNode.val\u2190val 4: V T S[i]\u2190 V T S[i] + 1 5: myNode.startV ts\u2190 collect(V T S) 6: prevExecution\u2190 Nodes[i] 7: myNode.prevExecution\u2190 prevExecution 8: myNode.inW ork \u2190 1 \u25b7 begin swap 9: Nodes[i]\u2190 myNode \u25b7 announce the operation 10: prev \u2190 primitiveSwap(&tail, myNode) 11: myNode.prev \u2190 prev \u25b7 persisting operation 12: myNode.endV ts\u2190 collect(V T S) 13: myNode.inW ork \u2190 0 \u25b7 finished operation 14: return myNode.prev.val"
        },
        {
            "heading": "4.1 Detailed Description of the Algorithm",
            "text": "Data structure definitions and the pseudo-code are presented by Algorithm 1. Text in blue is for the independent failures algorithm and should be disregarded for now. We first describe key data structures and shared variables.\nEach SWAP operation is represented by a single Node structure. Node.val stores the operand of the SWAP operation. Node.seq stores the sequence number of the current process\u2019s SWAP operation. Node.prev is a pointer to the Node structure representing the previous SWAP operation. Consequently, Node.prev.val stores the value that must be returned by the SWAP operation represented by Node. Each Node structure also stores two timestamp vectors of size n\u2014Node.startVts and Node.endVts. Lastly, Node.prevExecution is a pointer to the Node structure representing the previous SWAP operation by the same process (if there is one).\nNodes is an array of n + 1 pointers to Node structures. Nodes[0] points to the headNode sentinel node. For each process i \u2208 {1, . . . , n}, Nodes[i] points to the beginning of a list of Node structures, induced by prevExecution pointers, that represent the SWAP operations of process i. This array is used for recording all Node structures created throughout the execution.\nVTS is an array of length n that serves as a global vector timestamp. Entry i counts the number of operations performed by process i. tail is a pointer to a Node structure. The algorithm maintains a linked list of Node structures representing the order of SWAP operations and tail points to the last Node structure in the list.\nThe following order relation between paths is used by the global recovery procedure.\n15: procedure GRECOVER() \u25b7 Global SWAP recovery procedure 16: V \u2190 \u2205, E \u2190 \u2205 17: for i from 0 to n do 18: currNode\u2190 Nodes[i] 19: while currNode \u0338= null do 20: V \u2190 V \u222a {currNode} 21: if currNode.prev \u0338= null then 22: V \u2190 V \u222a {currNode.prev} 23: E \u2190 E \u222a {(currNode, currNode.prev)} 24: currNode\u2190 currNode.prevExecution 25: T AILNODE \u2190 new(Node) 26: \u25b7 Add graph node representing the list\u2019s tail and graph edge pointing to the tail Node 27: V \u2190 V \u222a {T AILNODE} 28: E \u2190 E \u222a {(T AILNODE, tail)} 29: Compute set P aths of maximal paths in graph G = (V, E) 30: MiddleP aths\u2190 \u2205 31: SingleNodes\u2190 \u2205 32: for path \u2208 P aths do 33: if len(path) == 1 then 34: SingleNodes\u2190 SingleNodes \u222a {start(path)} 35: Remove path from P aths 36: for path \u2208 P aths do 37: if T AILNODE \u2208 path and Nodes[0] \u2208 path then \u25b7 There is a single full path from tail to head 38: For every Node in SingleNodes re-execute SWAP from Line 10 39: return 40: else if T AILNODE \u2208 path then 41: T ailP ath\u2190 path 42: else if Nodes[0] \u2208 path then 43: HeadP ath\u2190 path 44: else 45: MiddleP aths\u2190MiddleP aths \u222a {path} 46: for curP ath \u2208 sort(MiddleP aths \u222a SingleNodes in non-increasing \u227b order) do 47: end(T ailP ath).prev \u2190 start(curP ath) 48: update T ailP ath to include added path 49: end(T ailP ath).prev \u2190 start(HeadP ath) 50: procedure RECOVER(val) \u25b7 Individual SWAP recovery procedure for process i 51: myNode\u2190 Nodes[i] 52: if myNode == null or myNode.seq < SEQi then 53: return SWAP(val) 54: else 55: return myNode.prev.val\n\u25b6 Definition 3. Given two paths A and B, we denote A \u227b B if there are nodes nA \u2208 A and nB \u2208 B such that nA.startV ts > nB .endV ts. If neither A \u227b B nor B \u227b A holds, we say that A and B are \u227b-equal.\nA SWAP operation first creates a Node structure and initializes it (Lines 2-3). It then increments its entry of the VTS, collects VTS, and writes the resulting vector timestamp to the startVTS field of its node (Lines 4-5). In Lines 6-7, the node representing the current operation is linked to the list of the previous operations executed by this process. Then, the process announces the operation by writing a pointer to its node to its entry of the\nNodes array (Line 9). Next, the procedure invokes an atomic primitiveSwap operation to read a node pointer from tail and swap it with a pointer to the node representing the current operation (Line 10). Then, the previous tail value is persisted to the prev field of the operation\u2019s Node structure (Line 11), thus adding this operation to the fragment of its predecessor operation. If a process executes Line 10 but the system crashes before it executes Line 11, a new fragment will result that cannot be reached from tail using prev pointers. (These fragments are reconnected by the global recovery procedure GRECOVER.) The operation terminates by performing a second collect of VTS, writing it to the endVts field, and returning the previous value stored at prev.val (Lines 12,14).\nThe GRECOVER procedure (Lines 15-49) of the SWAP operation is performed by the system upon recovery. As we\u2019ve mentioned before, its task is to reconnect fragments caused by a system-wide crash by creating a total order between SWAP operations that maintains linearizability. When it terminates, all the SWAP operations that were previously announced (in Line 9) are ordered in a single fragment that includes the headNode and the node pointed to by tail. As we prove, the order of operations induced by this fragment is a linearization of the execution.\nAfter the completion of GRECOVER, the system resurrects all the processes whose SWAP operations crashed, for executing the individual recovery procedure (Lines 50-55). It first checks if the sequence number of the process\u2019 last announced operation (found in the Nodes array) equals SEQi. If it does, the value of Nodes[i]\u2019s prev field is returned (Line 55). Otherwise, process i\u2019s operation crashed before it was announced and so SWAP is re-executed (Line 53).\nPending SWAP operations that updated their prev pointers before the crash can simply return the value stored in prev.val. SWAP operations that did not update their prev pointer in Line 11 before a crash are of two types: Those that executed Line 10 before the crash and those that did not. SWAP operations of the latter type are simpler to deal with since they did not change the tail pointer and can therefore be re-executed. Correctly ordering SWAP operations of the first type (i.e. those that executed Line 10 but did not execute Line 11) is more challenging, since their primitive swap operation changed tail\u2019s value but its return value was lost.\nWe proceed to describe GRECOVER in more detail. It starts by constructing a directed graph G whose nodes correspond to Node structures and whose edges correspond to prev pointers (Lines 16-28). The construction is done by traversing (non-null) prev and prevExecution fields starting from each entry of the Nodes array (Lines 16-24). After the traversal ends, a special TAILNODE node and an edge directed from it to the node pointed at by tail are added to the graph for simplifying the handling of the tail fragment (Lines 25-28). A set Paths of maximal directed paths in G is computed in Line 29. G is cycle-free, because each SWAP operation performs Line 10 at most once and, if it does, receives in response a pointer to an operation that performed Line 10 before it. Thus, the set Paths is well-defined. Each element of Paths represents a fragment.\nNext, all singleNodes (if any) are removed from Paths and inserted into a separate SingleNodes set (Lines 31-35). If Paths has a fragment that contains both TAILNODE and the headNode sentinel node then, as we prove, there are no middle fragments. In this case, each of the operations that correspond to SingleNodes is executed, in turn, starting from Line 10 and their nodes are thus appended to the end of this full path. Then GRECOVER returns (Lines 37-39). Otherwise, the fragments in Paths are categorized to a single HeadPath fragment, a single TailPath fragment, and a MiddlePaths set that contains all other paths, which are middle fragments (Lines 36-45).\nNext, all fragments other than HeadPath and TailPath are sorted in non-increasing \u227b order (see Definition 3) and are appended, one after the other, to the end of TailPath by updating appropriate prev fields (Lines 46-48). Sorting is done by comparing startVts and endVts fields as specified by Definition 3. The construction of the full order is concluded by appending the HeadPath to the end of the TailPath (Line 49).\nThe execution of GRECOVER, as well as that of the individual recovery procedure, can incur one or more crashes. In this case, the recovery process is re-executed upon recovery from each crash. As we prove, when it completes, operations are ordered correctly."
        },
        {
            "heading": "4.2 Proof of Correctness",
            "text": "We say that the list l = (a, a1 = a.prev, a2 = a1.prev..., am = am\u22121.prev, b = am.prev), for m \u2265 0, of length m + 2, is induced by prev pointers. We also say that l starts at a and l ends at b. We define the notion of a list induced by prevExecution pointers similarly.\n\u25b6 Theorem 4. Algorithm 1 implements a recoverable NRL SWAP in the system-wide failures model using only read, write and primitiveSwap operations. Its SWAP operations are wait-free.\nProof. Clearly from the code, SWAP operations are wait-free since they have no loops and so they terminate in crash-free executions. We also observe that the algorithm is linearizable in crash-free executions: An operation Op is linearized in Line 10 when it swaps a pointer to its Node to tail. The next operation to execute Line 10 after Op (if any) is guaranteed to return Op\u2019s input as its response in line 14. If Op is the first SWAP operation to perform Line 10 then, from the initialization of headNode, it returns the object\u2019s initial value \u22a5.\nNext, we consider executions that incur crashes and prove the following property: Upon completion of the GRECOVER procedure, the list L induced by prev pointers, starting from tail, contains exactly once the Node structure of every SWAP operation announced (in Line 9) in the course of the execution and ends at headNode. Moreover, the order induced by L respects operations\u2019 real-time order.\nWe first now provide several lemmas and their respective proofs, Theorem 4\u2019s proof continues following Lemma 9.\nThe following lemma ensures that in GRECOVER at Line 29, all Node structures ever announced by Line 9 are in V , and all prev pointers from Node u to Node v that were set in Line 11 are represented by an edge (u, v) \u2208 E.\n\u25b6 Lemma 5. At Line 29 {v : v was announced by Line 9} \u2286 V and {(u, v) : u.prev == v} \u2286 E\nProof. First Notice that every Node structure announced at Line 9 is inserted into the Nodes array. Also, notice that in each cell in Nodes only a single process writes.\nIn addition, every time a Node x is overwritten by a new Node y in Nodes[p] by process p, y.prevExecution == x. Therefore all of the Node structures created by p can be reached by going over the list induced by prevExecution pointers from Nodes[p] for p \u2208 {1 . . . n}. Also notice Nodes[0] is a special case of a list with constant length=1 representing the initial status.\nIn the for loop at Line 17 the recovery process goes over all process numbers from 1 to n and over 0. For each process it goes over the list induced by its prevExecution pointers, meaning currNode gets assigned every Node ever announced by Line 9 and Nodes[0], thus concluding that at Line 29 {v : v was announced by Line 9} \u2286 V . In addition by going over every announced Node and adding the TAILNODE link, we ensure that for every Node u, Node v if u.prev == v there is an edge (u, v) \u2208 E concluding that {(u, v) : u.prev == v} \u2286 E. \u25c0\nThe next lemma ensures every Node has at most one prev or tail pointer pointing at it.\n\u25b6 Lemma 6. For every Node u, either tail = u and | {v : v.prev = u} |= 0 or | {v : v.prev = u} |\u2264 1.\nProof. We prove this lemma by reviewing all lines of code that assign prev pointers. First, notice that the lemma holds in the initial state since only Nodes[0] exists and is pointed only by the tail pointer.\nWhen a prev pointer is assigned at Line 11, it is done using an atomic primitiveSwap operation, meaning that only a single SWAP operation can read the specific prev value that was previously pointed by tail.\nWhen a prev pointer is assigned during GRECOVER it is done in either Line 47 or 49. In both cases, it is assigned with a Node that starts a path in the Graph(V, E) and by lemma 5 all Nodes created, and prev pointers are represented in the graph meaning there is no other prev pointer pointing to the Node assigned because it is a start of a maximal path in the graph (V, E). \u25c0\nWe can now prove Lemma 7, showing that the set of paths Paths computed in Line 29 are node-disjoint.\n\u25b6 Lemma 7. Let P and J be two different paths that exist simultaneously in the global recovery process, and consider a Node i \u2208 P , then i /\u2208 J .\nProof. From Lemmas 6 and 5, every Node v \u2208 V has at most one incoming edge, meaning there is at most one Node u \u2208 V such that (u, v) \u2208 E. In addition, every edge (u, v) \u2208 E signifies that the node representing u has its prev pointer pointing to the node representing v. Thus, for every node u \u2208 V there is at most one v \u2208 V such that (u, v) \u2208 E. Assume towards a contradiction that there exist two maximal paths P ,J \u2208 Paths, P \u0338= J such that i \u2208 P and i \u2208 J , then either there are nodes u \u2208 P , j \u2208 J s.t u \u0338= j and (i, u), (i, j) \u2208 E, or there are nodes u \u2208 P , j \u2208 J s.t u \u0338= j and (u, i), (j, i) \u2208 E. The first option provides a contradiction because node i has two prev pointers, and the second option means node i has two prev pointers pointing at it in contradiction with Lemma 6. \u25c0\nLemma 8 shows that after a successful global recovery, the Node list starting at tail is complete and holds all announced Node structures.\n\u25b6 Lemma 8. At the end of a crash-free execution of the global GRECOVER procedure, there is a single list induced by prev pointers starting from tail and ending in Nodes[0] such that all Node structures announced at Line 9 are in it.\nProof. First, assume that the condition in Line 37 holds. In this case, there is a maximal path P \u2208 Paths that includes both TAILNODE and Nodes[0]. Assume towards a contradiction that the lemma does not hold. It follows that there is a Node i that isn\u2019t in path P . There are two sub-cases to consider. Either there is another path J in Paths, or there isn\u2019t. If the latter sub-case holds, then i must be in SingleNodes, hence SWAP will be re-executed starting from Line 10 on its behalf (in Line 38), and because the execution is crash-free, i will be inserted to the list induced by prev pointers starting at tail, and this list will end in Nodes[0] according to the code. The former sub-case is that i \u2208 J and J is a middle fragment, hence it is of length at least two. Let x be the last Node in J and let Op be the operation represented by x. Op must have executed Line 10 before the crash (otherwise no prev pointer could point at x) but did not execute Line 11 before the crash (since x is the last node in J). Immediately after Op executed Line 10, tail pointed to x. Since Op did not\nexecute Line 11, this contradicts the existence of a path in Paths starting from tail and ending in Nodes[0].\nOtherwise, the condition in Line 37 does not hold. In this case, it follows from Lines 36-49 that immediately after Line 49 is executed by Op, TailPath is a list that starts from TAILNODE, ends with Nodes[0], and contains all the Nodes in V . Thus by Lemma 5, this list contains all announced Nodes and the lemma holds. \u25c0\nLemma 9 ensures that the \u227a order can only hold in one direction for any two Paths.\n\u25b6 Lemma 9. Let A and B be two paths that exist simultaneously in the global recovery procedure. Then A \u227a B =\u21d2 B \u2280 A.\nProof. By Definition 3, A \u227a B implies that there is a Node x \u2208 A and a Node y \u2208 B such that x.endV ts < y.startV ts. From Lemma 7, both fragments are disjoint. A new fragment is created only when a process executes Line 10 but does not execute Line 11. This implies that all operations on fragment A that completed Line 10 had done so before any of the operations on fragment B performed Line 10, or vice versa. Since x.endV ts < y.startV ts and x.endV ts is only written after executing Line 10, and since y.startV ts is only set before executing Line 10 and after increasing V TS, it follows that all the operations of fragment A executed Line 10 before any the operations of fragment B have executed this line.\nIf we also have B \u227a A, then by Definition 3, there is a Node a \u2208 A, and a Node b \u2208 B, such that b.endV ts < a.startV ts. It follows that the operation represented by b executed Line 10 before the operation represented by a, which is a contradiction. \u25c0\nTo conclude the proof of Theorem 4, let N1, N2 = N1.prev, N3 = N2.prev...Nl = Nl\u22121.prev such that N1 = Tail and Nl = Nodes[0] denote the Node structures in the list induced by prev pointers beginning at tail. In the initial state, Tail = N1 = Nl = Nodes[0]. For a node x, denote by proc(x) the process that created and announced node x and let H = (SWAPproc(Nl\u22121)(Nl\u22121.val), SWAPproc(Nl\u22122)(Nl\u22122.val) ... SWAPproc(N1)(N1.val)), where N1, . . . , Nl are the nodes in the single fragment that exists immediately after the GRECOVER procedure. Let \u03b1 denote the execution that ends when GRECOVER completes. From Lemma 8, H is a sequential history that contains all the SWAP operations that were announced in \u03b1. Moreover, for any extension \u03b2 of \u03b1 in which all these SWAP operations return following the execution of their individual RECOVER procedures, they return the same values in \u03b2 and in H. Operations that weren\u2019t announced in \u03b1 but whose individual RECOVER procedures were executed in \u03b2, re-execute SWAP(val) (Line 53) and are therefore linearized when they execute Line 10.\nIt remains to show that for any two announced SWAP operations SWAP1, SWAP2, if SWAP1 terminates in \u03b1 before SWAP2 starts, then SWAP1 precedes SWAP2 in H. Let Node1, Node2 be the Nodes created by SWAP1 and SWAP2, respectively. Assume towards a contradiction that SWAP2 precedes SWAP1 in H. Then there is a path induced by prev pointers from Node1 to Node2 when GRECOVER terminates. There are two cases to consider. The first case is that all the prev pointers of the path between Node1 and Node2 were assigned by SWAP operations and not during recovery. This implies that between the time when SWAP1 executed Line 10 and the time when SWAP2 executed Line 10, no process performed Line 10 without performing Line 11, otherwise Node1 and Node2 would have been on separate fragments just before recovery. Consequently, Node1 and Node2 are on the same fragment and Node1 precedes Node2 before a crash. It follows that Node1 precedes Node2 also in the single fragment that exists when GRECOVER terminates, hence SWAP2 follows SWAP1 in H. This is a contradiction.\nThe second case is that the path induced by prev pointers from Node1 to Node2 was formed during recovery. There are two sub-cases to consider. The first is that during recovery, one of the Nodes is in singleNodes while the other is in a full path from TAILNODE to Nodes[0] (thus the condition in Line 37 is true). In this case, since operations that create Nodes in singleNodes did not complete, it must be that Node2 is in singleNodes and Node1 is on the full path. From Line 38, SWAP2 will be re-executed and so Node2 will be placed before Node1 in the list induced by prev pointers starting from tail at the end of GRECOVER, hence SWAP2 follows SWAP1 in H, a contradiction.\nThe second sub-case is when the condition of Line 37 is not satisfied. This implies that just before crashing, Node1 and Node2 were on different fragments. Let A and B respectively denote the paths representing the fragments on which Node1 and Node2 were just before the crash. Since Node1.endV ts < Node2.startV ts must hold, from Definition 3, A \u227a B holds. Consequently, from Lemma 9 B \u2280 A, therefore immediately after GRECOVER terminates there is a path from Node2 to Node1, hence SWAP2 follows SWAP1 in H, a contradiction. \u25c0\n5 Detectable Swap Algorithm for the Independent Failures Model\nIn the independent failures model, each process may crash and recover independently of other processes. A recoverable algorithm for this model must therefore allow one or more processes to execute RECOVER concurrently, while other processes may concurrently execute their SWAP operations. In order to handle this concurrency correctly, we introduce two key changes to Algorithm 2. First, the RECOVER procedure now synchronizes concurrent invocations by using a starvation-free RME lock, implemented from reads and writes only, such as that proposed by [21]. This serializes the execution of the recovery code. The goal of the second change is to allow the recovery code to wait for a concurrent SWAP operation Op to either complete or crash. Only once this happens, can the recovery code add the Node representing Op to graph G.\nThe pseudo-code of the RECOVER procedure for the independent failures model is presented by Algorithm 2. The few additions done in the pseudo-code of SWAP are presented in blue font in Algorithm 1. These consist of adding an inWork field (initialized to 0) to the Node structure, setting it (in Line 8) just before the SWAP operation is announced in Line 9, and resetting it (in Line 13) immediately after the endVTS field is updated in Line 12.\nRECOVER first checks whether the Node of the crashed operation was announced (Line 3). If it wasn\u2019t, it re-executes SWAP(val) (Line 4). Otherwise, it signals that it is performing RECOVER by writing 2 to the inWork field of its Node and then attempts to acquire mutex (Lines 5-6). Next it checks if the operation already has a value to return and if so, returns this value (Lines 7-8, 39-41).\nLines 9 - 17 construct the graph G. Unlike the system-wide failure construction, we go over all Nodes twice, thus constructing two sets of Nodes, V1 and V2. In addition, candidate paths chosen from MiddlePaths are only chosen if the start of their fragment is from V1 (Line 34). This is done because, after a single traversal that constructs V1, there might be a Node in V1 that is the start of a fragment that may be pointed by some Node x /\u2208 V1. As we prove, a second traversal ensures that the problem cannot occur for a graph constructed based on V = V1 \u222a V2. During each traversal of Nodes the algorithm waits for each Node v\u2019s inWork field, to be 0 or 2 before adding it to V (Line 7 of gatherGraph). This ensures that the operation Op that created v isn\u2019t concurrently executing its critical section of SWAP and therefore v cannot change after being added to V .\nAlgorithm 2 Recoverable detectable SWAP, for the independent failures model.\n1: procedure RECOVER(val) \u25b7 executed by process i 2: myNode\u2190 Nodes[i] 3: if myNode == null or myNode.seq < SEQi then 4: return SWAP(val) 5: myNode.inW ork \u2190 2 6: mutex.lock() 7: if myNode.prev \u0338= null then 8: GoTo Line 39 9: V1, E1 \u2190 gatherGraph() 10: tailNode\u2190 tail 11: await(tailNode.inW ork \u2208 {0, 2}) 12: V2, E2 \u2190 gatherGraph() 13: V2 \u2190 V2 \u222a {T AILNODE, tailNode} 14: E2 \u2190 E2\u222a{(T AILNODE, tailNode)} 15: V \u2190 V1 \u222a V2 16: E \u2190 E1 \u222a E2 17: Compute set P aths of maximal paths in graph G = (V, E) 18: for path \u2208 P aths do 19: if myNode \u2208 path then 20: myP ath\u2190 path 21: if len(myP ath) == 1 then 22: re-execute SWAP from Line 10 for myNode 23: mutex.release() 24: return myNode.prev.val 25: MiddleP aths\u2190 \u2205 \u25b7 May include SingleNodes 26: for path \u2208 P aths do 27: if T AILNODE \u2208 path then 28: T ailP ath\u2190 path 29: else if Nodes[0] \u2208 path then 30: HeadP ath\u2190 path 31: else 32: MiddleP aths\u2190MiddleP aths \u222a {path} 33: ordP aths\u2190 sort(MiddleP aths) in non-increasing \u227b order 34: candidate\u2190first path C \u2208 ordP aths after myP ath s.t. start(C) \u2208 V1 or null if no such C 35: if candidate \u0338= null then 36: myNode.prev \u2190 start(candidate) 37: else 38: myNode.prev \u2190 start(HeadP ath) 39: myNode.endV ts\u2190 collect(V T S) 40: mutex.release() 41: return myNode.prev.val\nThe rest of the procedure is similar to that of GRECOVER in Algorithm 1. All maximal Paths in G are calculated and classified to TailPath, HeadPath and MiddlePaths. In the end a single candidate path either from the sorted MiddlePaths or the HeadPath is selected to be linked to myNode (Lines 33-38). Note that as we ensure only for Nodes in V1 that any Node pointing at them is in V , only such Nodes are considered as candidates (Line 34). As we prove, this ensures linearizability. The traversals that construct V1 and V2 are implemented by the helper function gatherGraph.\n1: procedure gatherGraph() \u25b7 Used by Algorithm 2 2: V \u2190 \u2205 3: E \u2190 \u2205 4: for j from 0 to n do 5: currNode\u2190 Nodes[j] 6: while currNode \u0338= null do 7: await(currNode.inW ork \u2208 {0, 2}) 8: V \u2190 V \u222a {currNode} 9: if currNode.prev \u0338= null then\n10: V \u2190 V \u222a {currNode.prev} 11: E \u2190 E \u222a {(currNode, currNode.prev)} 12: currNode\u2190 currNode.prevExecution"
        },
        {
            "heading": "5.1 Correctness proof for the Swap implementation in the independent failures model",
            "text": "\u25b6 Lemma 10. Let myOp be process i\u2019s operation represented by myNode. In i\u2019s execution of RECOVER, after completing Line 9 all Nodes representing operations that completed Line 10 before myOp completed it are in V1.\nProof. The lemma holds vacuously if myOp did not execute Line 10. Let Node0 be a Node created by op0 performed by process j that completed Line 10 before myOp completed it. Because myOp completed Line 10 before crashing, and op0 completed Line 10 before myOp, upon starting i\u2019s RECOVER procedure Node0 is in the List induced by prevExecution pointers starting from Nodes[j]. When executing gatherGraph in Line 9, process i goes over all Nodes in the list induced by prevExecution pointers starting from Nodes[j], therefore, Node0 will be added to V1. \u25c0\nThe following lemma ensures that every Node Structure has at most one prev pointer or tail pointing at it.\n\u25b6 Lemma 11. For every Node u, either tail = u and | {v : v.prev = u} |= 0 or | {v : v.prev = u} |\u2264 1.\nProof. Similarly to Lemma 6, we prove this lemma by reviewing all lines of code that assign prev pointers. First, notice that the lemma holds in the initial state since only Nodes[0] exists and it is pointed only by the tail pointer.\nAs in the system-wide failures model algorithm, when a prev pointer is assigned at Line 11, it is done using an atomic primitiveSwap operation, meaning that only a single SWAP operation can read the specific prev value that was previously pointed by tail.\nThe difference from the system-wide model is that in the independent failures model algorithm a process might be in recovery while another process continues to execute SWAP. For this matter we must ensure that a recovering process does not assign a prev pointer to a Node that is about to be assigned by a process executing SWAP.\nIn RECOVER a Node v\u2019s prev pointer can be assigned in either Line 36 or 38. In both cases it is assigned by process i to a Node that is a start of a maximal path in Graph G = (V, E), Let y be the Node v.prev is assigned to. It is left to show that for any Node x either y is not assigned to x.prev at any stage or if x.prev == y then (x, y) \u2208 E during the execution of i\u2019s RECOVER procedure meaning y is not the start of a maximal path in G.\nLet process j be the process running the operation that created x. j can assign a prev pointer to x either in RECOVER or in SWAP. In the first case it will not be assigned to y\nas it will await for process i to release the mutex lock before assigning a prev pointer and if the mutex was released, then both v and y would be in V1 for j\u2019s recovery and therefore y will not be the start of a maximal path in Graph G = (V, E) for j\u2019s recovery. It is necessary to note that the Critical Section Re-entry (CSR) [21] property of the RME mutex lock guarantees that if i crashes during its recovery, i is the only process allowed to acquire the mutex lock upon its subsequent recovery. Specifically, j will wait for i to release mutex even if i crashes during its recovery.\nIn the second case, during SWAP y can be chosen to be assigned to x.prev by j\u2019s execution of Line 10, and is returned as the previous tail. y was also chosen to be assigned by i during RECOVER, this can be done in either Line 36 or 38. In the latter case it is assigned to the start of the HeadPath while v is the end of a failed fragment meaning the TailPath can not be the HeadPath because there is a failed fragment. Specifically y cannot be pointed by tail during j\u2019s execution of Line 10 as y is the start of the HeadPath.\nThe former case means y is in V1 for i\u2019s recovery as only candidate paths from MiddlePaths that start with a Node from V1 are considered by Line 34. Assume y was also chosen by j to be assigned to x.prev when j runs Line 10. This means that x is not in V1 for i\u2019s recovery because if it was then i would have waited for x.inWork to be 0 or 2 before adding it to V1 (Line 7 of gatherGraph) and when x.prev is assigned to y, x.inWork == 1. Specifically that would mean that (x, y) \u2208 E1 for i\u2019s recovery and y is not the start of a maximal path. We conclude that during i\u2019s first call to gatherGraph x is not yet announced.\nj chose y to be assigned to x during its primitveSwap of tail meaning that when y is added to V1, tail == y. It follows that when i reads tail to tailNode (Line 10), either tail = y (meaning tailNode also equals y) or x already performed its primitiveSwap. Here if tailNode == y then an edge (TAILNODE, y) is added to E2 (Line 14). Otherwise during the second call to gatherGraph (Line 12) x already performed its primitiveSwap and i will wait for x.inWork to be 0 or 2 (Line 7 of gatherGraph) therefore it will wait for x.prev to equal y and (x, y) will be in E2. In both cases E2 is then added to E (Line 16), concluding that there is an edge in E pointing to y. It follows that y is not a start of a maximal path in G for i\u2019s recovery.\n\u25c0\n\u25b6 Lemma 12. For any Node x and any Node y \u0338= x such that y is in the list induced by prev pointers starting from x, y.startV ts \u226f x.endV ts.\nProof. Let l be the list induced by prev pointers from x to y. We split the proof to 2 cases. The first is in l all prev pointers were assigned in SWAP by Line 11. The second is that there exist Nodes i, m such that i, m \u2208 l and i.prev == m and i.prev was not assigned by Line 11.\nFor the first case the proof is straightforward as that means y\u2019s operation has completed Line 10 before x\u2019s operation completed it. Therefore y.startV ts was collected (Line 5) before x.endV ts was collected by Line 12. Since VTS is incremented before collecting y.startV ts (Line 4) and VTS can only be incremented, it follows that y.startV ts \u226f x.endV ts.\nFor the second case note that i is in the list induced by prev pointers starting at x and y is in the list induced by prev pointers starting at m. When i.prev is assigned to m during recovery it is done in either Line 36 or 38. If it is assigned in Line 36 it is either done to a candidate path that is smaller or equal by \u227b order (Definition 3) meaning y.startV ts \u226f x.endV ts, or i is on the TailPath. If i is on the TailPath then x\u2019s operation performed Line 10 after any Node y that is in the list induced by prev pointers starting from m performed it also concluding y.startV ts \u226f x.endV ts.\nOtherwise it is assigned to the HeadPath. Operations on the HeadPath either succeeded and completed Line 10 before x ended meaning y.startV ts \u226f x.endV ts, or failed and were mended to fragments that were eventually mended to the HeadPath. In the latter case when those fragments were mended either x was announced or not, if it was then their fragment was smaller or equal according to \u227b order (Definition 3) than x\u2019s fragment (otherwise they would have eventually be mended to x\u2019s fragments and not the HeadPath), meaning for any Node y on their fragment y.startV ts \u226f x.endV ts. If x was not announced when those fragments were mended, then also for any Node y on their fragment y.startV ts \u226f x.endV ts because y started before x was announced and before x.endV ts was assigned and VTS can only be incremented. \u25c0\n\u25b6 Theorem 13. Algorithm 2 implements a recoverable NRL SWAP in the independent failures model using only read, write and primitiveSwap operations and satisfies NRL. Its SWAP operations are wait-free.\nProof. Proving correctness for crash-free executions, and wait-freedom of SWAP operation can be done exactly as in the proofs for the system-wide failure model. We now consider an execution \u03b1 with independent process crashes.\nLet node(Op) denote the node that represents Op. We show that the (possibly partial) order that exists between operations which is induced by prev pointers, always satisfies the following requirements. 1) Each operation Op can only return the value of the operation Op\u2032 represented by node(Op).prev and, 2) if it does, it is the only operation that returns the value of Op\u2032 and it does not precede in real-time order any Op\u2032\u2032 in the list induced by prev pointers from node(Op).\nFor any operation Op1 that returns the value of operation Op0, From Lemma 11, no other SWAP operation can return node(Op0).val.\nIt is left to show that in any execution \u03b1 if op1 ended before op2 started, then node(op2) cannot be in the list induced by prev pointers starting from node(op1) at any point during the execution. Proving this will establish that the operations can be linearized correctly according to the reversed order of the list induced by prev pointers, as in the system-wide failures model algorithm.\nAs op1 ended before op2 started, and any operation writes to its node\u2019s endV ts before completing, node(op2).startV ts > node(op1).endV ts holds. It now follows from Lemma 12 that if node(Op2) is in the list induced by prev pointers starting from node(Op1), then node(Op2).startV ts \u226f node(Op1).endV ts. This is a contradiction. \u25c0\n6 Impossibility of lock-freedom for the independent failures model\nIn this section, we prove a theorem establishing the impossibility of implementing lock-free algorithms for a wide variety of recoverable objects under the independent failures model. This generalizes previous results [2, 37] to a wider family of operations and implementations. In particular, it applies to any self-implementation of swap under the independent failures model, showing that our usage of a mutual exclusion lock in Algorithm 2 is essential.\nWe start by defining the notion of a distinguishable operation. An operation M is distinguishable, if there exists a history \u03b1base in spec and two invocations Mp and Mq of M , such that the return values of the invocations allows the system to distinguish which operation is applied right after \u03b1base. Formally:\n\u25b6 Definition 14 (Distinguishable operation). Operation M : VAL\u2192 RET is distinguishable if there exists a history \u03b1base and values x, y \u2208 VAL, z \u2208 RET , such that if M(x) and M(y)\nare applied sequentially right after \u03b1base, the first (and only the first) invocation of M to complete returns z.\nAssume a swap object with value 0 and two SWAP operations. If SWAP(1) and SWAP(2) are applied sequentially, only the first operation applied will return 0. This shows that SWAP is a distinguishable operation. Similarly, it\u2019s easy to show that pop and deque operations of the stack and queue data structures, as well as fetch&add and test&set, are also distinguishable operations.\nOur impossibility result applies to implementations of distinguishable operations that use only read, write, and a set of interfering functions, defined as follows:\n\u25b6 Definition 15 (Interfering functions [23]). Let F be a set of primitive functions indexed by an arbitrary set K. Define F to be a set of interfering functions if for all i and j in K, for any object O that supports fi and fj, and for any state S of O, (1) fj and fi commute: The application of fi to O in state S by process p followed by the\napplication of fj to O by process q leaves O (but not necessarily the local state of each process) in the same state as the application of fj to O in state S by process q followed by the application of fi to O by process p; or (2) fj overwrites fi: The application of fi to O in state S by process p followed by the application of fj to O by process q leaves O (but not necessarily the local state of each process) in the same state as the application of fj to O in state S by q alone.\nA configuration C consists of the states of all processes and the values of all shared base objects. Sometimes we use the notions of a configuration and a history interchangeably. For example, if a finite history H leads to a configuration C we may use H for representing C when H is clear from the context. Two configurations C1 and C2 are indistinguishable to a set of processes P , denoted C1\nP\u223c C2, if every process in P has the same state in C1 and C2, and every shared object holds the same value in C1 and C2.\nGiven a configuration C reached after a history \u03b1base, distinguishable operation M , and a process r \u2208 {p, q}, we say that C is r-valent if there is an execution starting from C in which the return value of M or M .RECOVER by r is z (where \u03b1base, M and z are as in Definition 14). C is bivalent if it is both p-valent and q-valent, for p \u0338= q. C is p-univalent if it is p-valent and not q-valent, and symmetrically for q-univalent. C is univalent if it is either p-univalent or q-univalent. Let C be a bivalent configuration and s be a step. If C \u25e6 s is univalent, we say that s is a critical step. We generalize the proofs of [2, 37] to prove the following theorem by using valency arguments [15,19].\n\u25b6 Theorem 16. Let M be a distinguishable operation. There is no recoverable implementation I of M from read, write and a set of K \u2265 1 of interfering primitive operations f1 . . . fK in the independent failures model, such that both M and M.RECOVER are lock-free.\nProof. Assume towards a contradiction that such a lock-free implementation exists. Assume that process p invokes M with value x and process q invokes M with value y, for x, y and z as in Definition 14.\nTo prove the theorem, we construct an execution in which each process performs an infinite number of steps and q neither crashes nor completes its operation.\nConfiguration C0, reached after execution \u03b1base, is bivalent because a solo execution of either p or q from C0 returns z. Following a standard valency argument and since we assume that M is lock-free, there is a crash-free execution starting from C0 that leads to a bivalent configuration C1, in which both p and q are about to execute a critical step. It must be\nthat one step leads to a p-univalent configuration while the other leads to a q-univalent configuration.\n\u25b7 Claim 17. The critical steps of p and q apply (possibly the same) primitives fi and fj , respectively, to the same base object.\nProof. Consider all possible steps: read, write, crash and f1 . . . fK . Assume sp and sq are critical steps by process p and q respectively, such that C1 \u25e6 sp is p-univalent while C1 \u25e6 sq is q-univalent.\nSteps sp and sq access distinct registers. In this case, these configurations are indistinguishable to p and q, that is, C \u25e6 sp \u25e6 sq\np,q\u223c C \u25e6 sq \u25e6 sp Step sq is a crash step then C \u25e6 sp \u25e6 sq\np\u223c C \u25e6 sq \u25e6 sp Steps sp and sq read the same register. Also in this case C \u25e6 sp \u25e6 sq\np,q\u223c C \u25e6 sq \u25e6 sp Step sp writes to some register r step and sq reads r. In this case, C \u25e6 sp\np\u223c C \u25e6 sq \u25e6 sp holds. Step sp applies fi 1 \u2264 i \u2264 K and step sq reads r. In this case, C \u25e6 sp\np\u223c C \u25e6 sq \u25e6 sp holds. Steps sp and sq write to the same register. In this case, C \u25e6 sp\np\u223c C \u25e6 sq \u25e6 sp holds. Step sp applies fi, 1 \u2264 i \u2264 K, step sq writes to the same register. In this case, C \u25e6 sq\nq\u223c C \u25e6 sp \u25e6 sq holds. Step sp applies fi, 1 \u2264 i \u2264 K, step sq applies fj , 1 \u2264 j \u2264 K each to a different base object O, In this case C \u25e6 sp \u25e6 sq\np,q\u223c C \u25e6 sq \u25e6 sp holds. In each of the above cases, the configurations are indistinguishable to at least one process, and therefore, must have the same valencies. Therefore, it must be that p and q apply fi and fj respectively to the same base object. \u25c0\nAssume, without loss of generality, that C1 \u25e6 p is p-univalent while C1 \u25e6 q is q-univalent. We consider two cases:\nCase 1: fi and fj commute: Consider executions C2, C3 where C2 = C1\u25e6p\u25e6q\u25e6CRASHp and C3 = C1 \u25e6 q \u25e6 p \u25e6 CRASHp. Configurations C2 and C3 are reached after p and q each take a step (in different orders) in which they apply their operations to the same base object O and then p crashes.\nA solo execution of M.RECOVER by p from both C2 and C3 must complete since I is lock-free. Furthermore, C2\np\u223c C3 holds, because p\u2019s response from the primitive fi is lost, while the value of O is the same in both configurations since fi and fj commute. Consequently, an execution of M.RECOVER by p from both C2 and C3 must return the same value. Let v denote this value.\nAssume first that v = z and thus C3 is p-valent. Configuration C1 \u25e6 q \u25e6 p is q-univalent, while C3 = C1 \u25e6 q \u25e6 p \u25e6 CRASHp is p-valent. However, C1 \u25e6 q \u25e6 p\nq\u223c C3 holds because q is unaware of p\u2019s crash. Consequently, a solo execution of q from C3 must return z, that is, C3 is also q-valent. This proves that C3 is bivalent.\nAssume then that v \u0338= z. We now show that, in this case, C2 is bivalent. Indeed, from this assumption, C2 is q-valent, because a solo execution of q after p completes (and returns v \u0338= z) must return z since, from Definition 14, exactly one of these two operations must return z. However, configuration C1 \u25e6 p \u25e6 q is p-univalent, while C2 = C1 \u25e6 p \u25e6 q \u25e6CRASHp\nq\u223c C1 \u25e6 p \u25e6 q, therefore a solo execution of q from C2 must return x s.t. x \u0338= z. Thus, C2 is bivalent.\nCase 2: fj overwrites fi: Consider executions C2, C3 where C2 = C1 \u25e6 p \u25e6 q \u25e6CRASHp and C3 = C1 \u25e6 q \u25e6 CRASHp. A solo execution of M.RECOVER by p from both C2 and C3 must complete since I is lock-free. Furthermore, C2\np\u223c C3 because p\u2019s response from the primitive fi is lost, while the value of the base object fi and fj are applied to is the same in\nboth configurations since fj overwrites fi. Therefore, an execution of M.RECOVER by p from both C2 and C3 returns the same value. Let v denote this value.\nAssume v = z and thus C3 is p-valent. C1 \u25e6 q is q-univalent, while C3 = C1 \u25e6 q \u25e6CRASHp is p-valent. C1 \u25e6 q\nq\u223c C3 holds because q is unaware of p\u2019s crash. Therefore, a solo execution of q from C3 returns z, that is, C3 is also q-valent. This proves that C3 is bivalent.\nAssume then that v \u0338= z. We show that in this case C2 is bivalent. Indeed, from our assumption, C2 is q-valent, as a solo execution of q after p completes must return z since, from Definition 14, exactly one of these two operations must return z. However configuration C1 \u25e6 p \u25e6 q is p-univalent and C2 = C1 \u25e6 p \u25e6 q \u25e6CRASHp\nq\u223c C1 \u25e6 p \u25e6 q, therefore a solo execution of q from C2 must return x \u0338= z. This establishes that C2 is bivalent.\nIn both cases, this shows that we can keep extending the execution obtaining an infinite execution in which neither p nor q complete their operations and q performs an infinite number of steps without crashing, contradicting the lock-freedom assumption. \u25c0\n7 Discussion\nWe present two NRL self-implementations of the swap object, one for the system-wide failures model and the other for the independent failures model. In both, SWAP operations are wait-free and the recovery code is blocking. In the system-wide failures model, this is a result of delegating the recovery to a single process, while in the independent failures model, it is due to coordination between the recovering process and the other processes. We also prove the impossibility of a lock-free implementation of distinguishable operations using read-write and a set of interfering functions, in the independent failures model. In particular, this shows that with independent failures, a self-implementation of swap cannot be lock-free.\nOur algorithms use O(m \u2217 n) space, where m is the number of SWAP invocations in the execution. Bounding memory consumption to O(n) is relatively easy if a recoverable swap operation by one process can wait for operations by other processes to either make progress or fail. An interesting open question is to figure out whether the space complexity of detectable swap self-implementations with wait-free operations can be reduced to o(m) or if \u2126(m) is inherently required. We leave this question for future work.\nFinally, it is also important to explore how self-implementations, in particular of swap, can be used to turn non-recoverable higher-level objects into NRL implementations of the same objects.\nReferences 1 Marcos K Aguilera and Svend Fr\u00f8lund. Strict linearizability and the power of aborting.\nTechnical Report HPL-2003-241, 2003. 2 Hagit Attiya, Ohad Ben-Baruch, and Danny Hendler. Nesting-safe recoverable linearizability:\nModular constructions for non-volatile memory. In Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing, pages 7\u201316, 2018. 3 Ohad Ben-Baruch, Danny Hendler, and Matan Rusanovsky. Upper and lower bounds on the space complexity of detectable objects. In Proceedings of the 39th Symposium on Principles of Distributed Computing, pages 11\u201320, 2020. 4 Naama Ben-David, Guy E Blelloch, Michal Friedman, and Yuanhao Wei. Delay-free concurrency on faulty persistent memory. In The 31st ACM Symposium on Parallelism in Algorithms and Architectures, pages 253\u2013264, 2019. 5 Naama Ben-David, Michal Friedman, and Yuanhao Wei. Survey of persistent memory correctness conditions. arXiv preprint arXiv:2208.11114, 2022.\n6 Ryan Berryhill, Wojciech Golab, and Mahesh Tripunitara. Robust shared objects for nonvolatile main memory. In 19th International Conference on Principles of Distributed Systems (OPODIS 2015). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016. 7 Dhruva R Chakrabarti, Hans-J Boehm, and Kumud Bhandari. Atlas: Leveraging locks for non-volatile memory consistency. ACM SIGPLAN Notices, 49(10):433\u2013452, 2014. 8 David Yu Cheng Chan and Philipp Woelfel. Recoverable mutual exclusion with constant amortized rmr complexity from standard primitives. In Proceedings of the 39th Symposium on Principles of Distributed Computing, pages 181\u2013190, 2020. 9 Kyeongmin Cho, Seungmin Jeon, and Jeehoon Kang. Practical detectability for persistent lock-free data structures. arXiv preprint arXiv:2203.07621, 2022. 10 Nachshon Cohen, Rachid Guerraoui, and Igor Zablotchi. The inherent cost of remembering consistently. In Proceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures, pages 259\u2013269, 2018. 11 Andreia Correia, Pascal Felber, and Pedro Ramalhete. Romulus: Efficient algorithms for persistent transactional memory. In Proceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures, pages 271\u2013282, 2018. 12 Andreia Correia, Pascal Felber, and Pedro Ramalhete. Persistent memory and the rise of universal constructions. In Proceedings of the Fifteenth European Conference on Computer Systems, pages 1\u201315, 2020. 13 Tudor David, Aleksandar Dragojevic, Rachid Guerraoui, and Igor Zablotchi. Log-free concurrent data structures. In 2018 USENIX Annual Technical Conference, pages 373\u2013386, 2018. 14 Panagiota Fatourou, Nikolaos D Kallimanis, and Eleftherios Kosmas. The performance power of software combining in persistence. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 337\u2013352, 2022. 15 Michael J Fischer, Nancy A Lynch, and Michael S Paterson. Impossibility of distributed consensus with one faulty process. Journal of the ACM (JACM), 32(2):374\u2013382, 1985. 16 Michal Friedman, Naama Ben-David, Yuanhao Wei, Guy E Blelloch, and Erez Petrank. Nvtraverse: In nvram data structures, the destination is more important than the journey. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 377\u2013392, 2020. 17 Michal Friedman, Maurice Herlihy, Virendra Marathe, and Erez Petrank. A persistent lock-free queue for non-volatile memory. ACM SIGPLAN Notices, 53(1):28\u201340, 2018. 18 Wojciech Golab. Recoverable consensus in shared memory. arXiv preprint arXiv:1804.10597, 2018. 19 Wojciech Golab. The recoverable consensus hierarchy. In Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms and Architectures, pages 281\u2013291, 2020. 20 Wojciech Golab and Danny Hendler. Recoverable mutual exclusion in sub-logarithmic time. In Proceedings of the ACM Symposium on Principles of Distributed Computing, pages 211\u2013220, 2017. 21 Wojciech Golab and Aditya Ramaraju. Recoverable mutual exclusion. In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing, pages 65\u201374, 2016. 22 Rachid Guerraoui and Ron R Levy. Robust emulations of shared memory in a crash-recovery model. In 24th International Conference on Distributed Computing Systems, 2004. Proceedings., pages 400\u2013407. IEEE, 2004. 23 Maurice Herlihy. Wait-free synchronization. ACM Transactions on Programming Languages and Systems (TOPLAS), 13(1):124\u2013149, 1991. 24 Maurice P Herlihy and Jeannette M Wing. Linearizability: A correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems (TOPLAS), 12(3):463\u2013492, 1990.\n25 Morteza Hoseinzadeh and Steven Swanson. Corundum: Statically-enforced persistent memory safety. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, pages 429\u2013442, 2021. 26 Joseph Izraelevitz, Terence Kelly, and Aasheesh Kolli. Failure-atomic persistent memory updates via justdo logging. ACM SIGARCH Computer Architecture News, 44(2):427\u2013442, 2016. 27 Joseph Izraelevitz, Hammurabi Mendes, and Michael L Scott. Linearizability of persistent memory objects under a full-system-crash failure model. In Distributed Computing: 30th International Symposium, DISC 2016, Paris, France, September 27-29, 2016. Proceedings 30, pages 313\u2013327. Springer, 2016. 28 Prasad Jayanti, Siddhartha Jayanti, and Anup Joshi. Optimal recoverable mutual exclusion using only fasas. In Networked Systems: 6th International Conference, NETYS 2018, Essaouira, Morocco, May 9\u201311, 2018, Revised Selected Papers, pages 191\u2013206. Springer, 2019. 29 Prasad Jayanti, Siddhartha Jayanti, and Anup Joshi. A recoverable mutex algorithm with sub-logarithmic RMR on both cc and dsm. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, pages 177\u2013186, 2019. 30 Prasad Jayanti, Siddhartha Jayanti, and Anup Joshi. Constant rmr recoverable mutex under system-wide crashes. arXiv preprint arXiv:2302.00748, 2023. 31 Prasad Jayanti and Anup Joshi. Recoverable FCFS mutual exclusion with wait-free recovery. In 31st International Symposium on Distributed Computing (DISC 2017). Schloss DagstuhlLeibniz-Zentrum fuer Informatik, 2017. 32 Prasad Jayanti and Anup Joshi. Recoverable mutual exclusion with abortability. Computing, 104(10):2225\u20132252, 2022. 33 Daniel Katzan and Adam Morrison. Recoverable, abortable, and adaptive mutual exclusion with sublogarithmic rmr complexity. arXiv preprint arXiv:2011.07622, 2020. 34 Daniel Katzan and Adam Morrison. Recoverable, abortable, and adaptive mutual exclusion with sublogarithmic rmr complexity. In 24th International Conference on Principles of Distributed Systems, 2021. 35 Nan Li and Wojciech Golab. Detectable sequential specifications for recoverable shared objects. In 35th International Symposium on Distributed Computing (DISC 2021). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2021. 36 John M Mellor-Crummey and Michael L Scott. Algorithms for scalable synchronization on shared-memory multiprocessors. ACM Transactions on Computer Systems (TOCS), 9(1):21\u201365, 1991. 37 Liad Nahum, Hagit Attiya, Ohad Ben-Baruch, and Danny Hendler. Recoverable and detectable fetch&add. In 25th International Conference on Principles of Distributed Systems (OPODIS 2021). Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr Informatik, 2022. 38 Faisal Nawab, Joseph Izraelevitz, Terence Kelly, Charles B Morrey III, Dhruva R Chakrabarti, and Michael L Scott. Dal\u00ed: A periodically persistent hash map. In 31st International Symposium on Distributed Computing (DISC 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017. 39 Pedro Ramalhete, Andreia Correia, Pascal Felber, and Nachshon Cohen. Onefile: A wait-free persistent transactional memory. In 2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), pages 151\u2013163. IEEE, 2019. 40 Matan Rusanovsky, Hagit Attiya, Ohad Ben-Baruch, Tom Gerby, Danny Hendler, and Pedro Ramalhete. Flat-combining-based persistent data structures for non-volatile memory. In Stabilization, Safety, and Security of Distributed Systems: 23rd International Symposium, SSS 2021, Virtual Event, November 17\u201320, 2021, Proceedings 23, pages 505\u2013509. Springer, 2021. 41 David Schwalb, Markus Dreseler, Matthias Uflacker, and Hasso Plattner. NVC-hashmap: A persistent and concurrent hashmap for non-volatile memories. In Proceedings of the 3rd VLDB Workshop on In-Memory Data Mangement and Analytics, pages 1\u20138, 2015.\n42 Jae-Heon Yang and Jams H Anderson. A fast, scalable mutual exclusion algorithm. Distributed Computing, 9:51\u201360, 1995. 43 Yoav Zuriel, Michal Friedman, Gali Sheffi, Nachshon Cohen, and Erez Petrank. Efficient lock-free durable sets. Proceedings of the ACM on Programming Languages, 3(OOPSLA):1\u201326, 2019."
        }
    ],
    "title": "Recoverable and Detectable  Self-Implementations of Swap",
    "year": 2023
}