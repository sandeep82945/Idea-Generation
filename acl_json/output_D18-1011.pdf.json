{
    "abstractText": "In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our model assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shaonan Wang"
        },
        {
            "affiliations": [],
            "name": "Jiajun Zhang"
        },
        {
            "affiliations": [],
            "name": "Chengqing Zong"
        }
    ],
    "id": "SP:dc31737a0b437048c12aeedfd0293f69121b181a",
    "references": [
        {
            "authors": [
                "Eneko Agirre",
                "Enrique Alfonseca",
                "Keith Hall",
                "Jana Kravalova",
                "Marius Pa\u015fca",
                "Aitor Soroa."
            ],
            "title": "A study on similarity and relatedness using distributional and wordnet-based approaches",
            "venue": "NAACL, pages 19\u201327.",
            "year": 2009
        },
        {
            "authors": [
                "Andrew J. Anderson",
                "Douwe Kiela",
                "Stephen Clark",
                "Massimo Poesio."
            ],
            "title": "Visually grounded and textual semantic models differentially decode brain activity associated with concrete and abstract nouns",
            "venue": "TACL, 5:17\u201330.",
            "year": 2017
        },
        {
            "authors": [
                "John R Anderson",
                "Gordon H Bower."
            ],
            "title": "Human associative memory",
            "venue": "Psychology press.",
            "year": 2014
        },
        {
            "authors": [
                "Mark Andrews",
                "Gabriella Vigliocco",
                "David Vinson."
            ],
            "title": "Integrating experiential and distributional data to learn semantic representations",
            "venue": "Psychological review, 116(3):463.",
            "year": 2009
        },
        {
            "authors": [
                "Lawrence W Barsalou."
            ],
            "title": "Grounded cognition",
            "venue": "Annu. Rev. Psychol., 59:617\u2013645.",
            "year": 2008
        },
        {
            "authors": [
                "Yoshua Bengio."
            ],
            "title": "Learning deep architectures for ai",
            "venue": "Foundations and trends in Machine Learning, 2(1):1\u2013127.",
            "year": 2009
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Pascal Lamblin",
                "Dan Popovici",
                "Hugo Larochelle."
            ],
            "title": "Greedy layer-wise training of deep networks",
            "venue": "Advances in neural information processing systems, pages 153\u2013160.",
            "year": 2007
        },
        {
            "authors": [
                "David M Blei",
                "Andrew Y Ng",
                "Michael I Jordan."
            ],
            "title": "Latent dirichlet allocation",
            "venue": "Journal of machine Learning research, 3(Jan):993\u20131022.",
            "year": 2003
        },
        {
            "authors": [
                "Elia Bruni",
                "Nam-Khanh Tran",
                "Marco Baroni."
            ],
            "title": "Multimodal distributional semantics",
            "venue": "J. Artif. Intell. Res.(JAIR), 49(2014):1\u201347.",
            "year": 2014
        },
        {
            "authors": [
                "Stephen Clark."
            ],
            "title": "Vector space models of lexical meaning",
            "venue": "Handbook of Contemporary Semantic Theory, The, pages 493\u2013522.",
            "year": 2015
        },
        {
            "authors": [
                "Guillem Collell",
                "Teddy Zhang",
                "Marie-Francine Moens."
            ],
            "title": "Imagined visual representations as multimodal embeddings",
            "venue": "AAAI.",
            "year": 2017
        },
        {
            "authors": [
                "Allan M. Collins",
                "Elizabeth F. Loftus."
            ],
            "title": "A spreading-activation theory of semantic processing",
            "venue": "Psychological Review, 82:407\u2013428.",
            "year": 1975
        },
        {
            "authors": [
                "Simon De Deyne",
                "Amy Perfors",
                "Daniel J Navarro."
            ],
            "title": "Predicting human similarity judgments with distributional models: The value of word associations",
            "venue": "COLING, pages 1861\u20131870.",
            "year": 2016
        },
        {
            "authors": [
                "Christiane Fellbaum."
            ],
            "title": "WordNet",
            "venue": "Wiley Online Library.",
            "year": 1998
        },
        {
            "authors": [
                "Daniela Gerz",
                "Ivan Vuli\u0107",
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simverb-3500: A largescale evaluation set of verb similarity",
            "venue": "arXiv preprint arXiv:1608.00869.",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 1026\u20131034.",
            "year": 2015
        },
        {
            "authors": [
                "Felix Hill",
                "Anna Korhonen."
            ],
            "title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what i mean",
            "venue": "EMNLP, pages 255\u2013265.",
            "year": 2014
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Multi-modal models for concrete and abstract concept meaning",
            "venue": "Transactions of the Association for Computational Linguistics, 2:285\u2013296.",
            "year": 2014
        },
        {
            "authors": [
                "Felix Hill",
                "Roi Reichart",
                "Anna Korhonen."
            ],
            "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation",
            "venue": "Computational Linguistics.",
            "year": 2016
        },
        {
            "authors": [
                "Merrill Hiscock."
            ],
            "title": "Imagery and verbal processes",
            "venue": "Psyccritiques, 19(6):487.",
            "year": 1974
        },
        {
            "authors": [
                "Douwe Kiela",
                "L\u00e9on Bottou."
            ],
            "title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics",
            "venue": "EMNLP, pages 36\u201345.",
            "year": 2014
        },
        {
            "authors": [
                "Douwe Kiela",
                "Stephen Clark."
            ],
            "title": "Multi-and cross-modal semantics beyond vision: Grounding in auditory perception",
            "venue": "EMNLP, pages 2461\u20132470.",
            "year": 2015
        },
        {
            "authors": [
                "Diederik Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Barbara Landau",
                "Linda Smith",
                "Susan Jones."
            ],
            "title": "Object perception and object naming in early development",
            "venue": "Trends in cognitive sciences, 2(1):19\u201324.",
            "year": 1998
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Nghia The Pham",
                "Marco Baroni."
            ],
            "title": "Combining language and vision with a multimodal skip-gram model",
            "venue": "ACL, pages 153\u2013163.",
            "year": 2015
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean."
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781.",
            "year": 2013
        },
        {
            "authors": [
                "Jiquan Ngiam",
                "Aditya Khosla",
                "Mingyu Kim",
                "Juhan Nam",
                "Honglak Lee",
                "Andrew Y Ng."
            ],
            "title": "Multimodal deep learning",
            "venue": "ICML, pages 689\u2013696.",
            "year": 2011
        },
        {
            "authors": [
                "Allan Paivio."
            ],
            "title": "Mental representations: A dual coding approach",
            "venue": "Oxford University Press.",
            "year": 1990
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning."
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "EMNLP, volume 14, pages 1532\u2013 1543.",
            "year": 2014
        },
        {
            "authors": [
                "Matthew A Lambon Ralph",
                "Elizabeth Jefferies",
                "Karalyn Patterson",
                "Timothy T Rogers."
            ],
            "title": "The neural and computational bases of semantic cognition",
            "venue": "Nature Reviews Neuroscience, 18(1):42.",
            "year": 2017
        },
        {
            "authors": [
                "Stephen Roller",
                "Sabine Schulte im Walde."
            ],
            "title": "A multimodal lda model integrating textual, cognitive and visual modalities",
            "venue": "EMNLP, pages 1146\u2013 1157.",
            "year": 2013
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "year": 2015
        },
        {
            "authors": [
                "Carina Silberer",
                "Vittorio Ferrari",
                "Mirella Lapata."
            ],
            "title": "Visually grounded meaning representations",
            "venue": "IEEE transactions on pattern analysis and machine intelligence.",
            "year": 2016
        },
        {
            "authors": [
                "Carina Silberer",
                "Mirella Lapata."
            ],
            "title": "Grounded models of semantic representation",
            "venue": "EMNLP, pages 1423\u20131433.",
            "year": 2012
        },
        {
            "authors": [
                "Carina Silberer",
                "Mirella Lapata."
            ],
            "title": "Learning grounded meaning representations with autoencoders",
            "venue": "ACL, pages 721\u2013732.",
            "year": 2014
        },
        {
            "authors": [
                "Richard Socher",
                "Milind Ganjoo",
                "Christopher D Manning",
                "Andrew Ng."
            ],
            "title": "Zero-shot learning through cross-modal transfer",
            "venue": "Advances in neural information processing systems, pages 935\u2013943.",
            "year": 2013
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Ruslan R Salakhutdinov."
            ],
            "title": "Multimodal learning with deep boltzmann machines",
            "venue": "Advances in neural information processing systems, pages 2222\u20132230.",
            "year": 2012
        },
        {
            "authors": [
                "Peter D Turney",
                "Patrick Pantel."
            ],
            "title": "From frequency to meaning: Vector space models of semantics",
            "venue": "Journal of artificial intelligence research, 37:141\u2013188.",
            "year": 2010
        },
        {
            "authors": [
                "Pascal Vincent",
                "Hugo Larochelle",
                "Isabelle Lajoie",
                "Yoshua Bengio",
                "Pierre-Antoine Manzagol."
            ],
            "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
            "venue": "Journal of Machine Learning",
            "year": 2010
        },
        {
            "authors": [
                "Jing Wang",
                "Julie A. Conder",
                "David N. Blitzer",
                "Svetlana. V. Shinkareva."
            ],
            "title": "Neural representation of abstract and concrete concepts: A meta-analysis of neuroimaging studies",
            "venue": "Human brain mapping, 31(10):1459\u20131468.",
            "year": 2010
        },
        {
            "authors": [
                "Shaonan Wang",
                "Jiajun Zhang",
                "Nan Lin",
                "Chengqing Zong."
            ],
            "title": "Investigating inner properties of multimodal representation and semantic compositionality with brain-based componential semantics",
            "venue": "AAAI, pages 5964\u20135972.",
            "year": 2018
        },
        {
            "authors": [
                "Shaonan Wang",
                "Jiajun Zhang",
                "Chengqing Zong."
            ],
            "title": "Learning multimodal word representation via dynamic fusion methods",
            "venue": "AAAI, pages 5973\u2013 5980.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 115\u2013124 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n115"
        },
        {
            "heading": "1 Introduction",
            "text": "Representing the meaning of a word is a prerequisite to solve many linguistic and non-linguistic problems, such as retrieving words with the same meaning, finding the most relevant images or sounds of a word and so on. In recent years we have seen a surge of interest in building computational models that represent word meanings from patterns of word co-occurrence in corpora (Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Clark, 2015; Wang et al., 2018b). However, word meaning is also tied to the physical world. Many behavioral studies suggest that human semantic representation is grounded in the external environment and sensorimotor experience (Landau et al., 1998; Barsalou, 2008). This has led to the development of multimodal representation models that utilize both textual and perceptual information (e.g., images, sounds).\nAs evidenced by a range of evaluations (Andrews et al., 2009; Bruni et al., 2014; Silberer\net al., 2016), multimodal models can learn better semantic word representations (a.k.a. embeddings) than text-based models. However, most existing models still have a number of drawbacks. First, they ignore the associations between modalities, and thus lack the ability of information transferring between modalities. Consequently they cannot handle words without perceptual information. Second, they integrate textual and perceptual representations with simple concatenation, which is insufficient to effectively fuse information from various modalities. Third, they typically treat the representations from different modalities equally. This is inconsistent with many psychological findings that information from different modalities contributes differently to the meaning of words (Paivio, 1990; Anderson et al., 2017).\nIn this work, we introduce the associative multichannel autoencoder (AMA), a novel multimodal word representation model that addresses all the above issues. Our model is built upon the stacked autoencoder (Bengio et al., 2007) to learn semantic representations by integrating textual and perceptual inputs. Inspired by the re-constructive and associative nature of human memory, we propose two associative memory modules as extensions. One is to learn associations between modalities (e.g., associations between textual and visual features), so as to reconstruct corresponding perceptual information of concepts. The other is to learn associations between related concepts, by reconstructing embeddings of both target words and their associated words. Furthermore, we propose a gating mechanism to learn the importance weights of different modalities to each word.\nTo summarize, our main contributions in this work are two-fold:\n\u2022 We present a novel associative multichannel autoencoder for multimodal word representation, which is capable of utilizing associations between different modalities and related\nconcepts, and assigning different importance weights to each modality according to different words. Results on six standard benchmarks demonstrate that our methods outperform strong unimodal baselines and state-ofthe-art multimodal models.\n\u2022 Our model successfully integrates cognitive insights of the re-constructive and associative nature of semantic memory in humans, suggesting that rich information contained in human cognitive processing can be used to enhance NLP models. Furthermore, our results shed light on the fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment."
        },
        {
            "heading": "2 Background and Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Cognitive Grounding",
            "text": "A large body of research evidences that human semantic memory is inherently re-constructive and associative (Collins and Loftus, 1975; Anderson and Bower, 2014). That is, memories are not exact static copies of reality, but are rather reconstructed from their stimuli and associated concepts each time they are retrieved. For example, when we see a dog, not only the concept itself, but also the corresponding perceptual information and associated words will be jointly activated and reconstructed. Moreover, various theories state that the different sources of information contribute differently to the semantic representation of a concept (Wang et al., 2010; Ralph et al., 2017). For instance, Dual Coding Theory (Hiscock, 1974) posits that concrete words are represented in the brain in terms of a perceptual and linguistic code, whereas abstract words are encoded only in the linguistic modality.\nIn these respects, our method employs a retrieval and representation process analogous to that of humans, in which the retrieval of perceptual information and associated words is triggered and mediated by a linguistic input. The learned cross-modality mapping and reconstruction of associated words are inspired by the human mental model of associations between different modalities and related concepts. Moreover, word meaning is tied to both linguistic and physical environment, and relies differently on each modality in-\nputs (Wang et al., 2018a). These are also captured by our multimodal representation model."
        },
        {
            "heading": "2.2 Multimodal Models",
            "text": "The existing multimodal representation models can be generally classified into two groups: 1) Jointly training models build multimodal representations with raw inputs of textual and perceptual resources. 2) Separate training models independently learn textual and perceptual representations and integrate them afterwards."
        },
        {
            "heading": "2.2.1 Jointly training models",
            "text": "A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013). Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013). For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model. Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors. Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs.\nThese methods can implicitly propagate perceptual information to word representations and at the same time learn multimodal representations. However, they utilize raw text corpus in which words having perceptual information account for a small portion. This weakens the effect of introducing perceptual information and consequently leads to the slight improvement of textual vectors."
        },
        {
            "heading": "2.2.2 Separate training models",
            "text": "The simplest approach is concatenation which fuses textual and visual vectors by concatenating them. It has been proven to be effective in learning multimodal representations (Bruni et al., 2014; Hill et al., 2014; Collell et al., 2017). Variations of this method employ transformation and dimension reduction on the concatenation result, including application of singular value decomposition (SVD) (Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014). There is also work using deep learning methods to project different modality inputs into a common\nspace, including restricted Boltzman machines (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012), autoencoders (Silberer and Lapata, 2014; Silberer et al., 2016), and recursive neural networks (Socher et al., 2013). However, the above methods can only generate multimodal vectors of those words that have perceptual information, thus reducing multimodal vocabulary drastically.\nAn empirically superior model addresses this problem by predicting missing perceptual information firstly. This includes Hill et al. (2014) who utilize the ridge regression method to learn a mapping matrix from textual modality to visual modality, and Collell et al. (2017) who employ a feedforward neural network to learn the mapping relation between textual vectors and visual vectors. Applying the mapping function on textual representations, they obtain the predicted visual vectors for all words in textual vocabulary. Then they calculate multimodal representations by concatenating textual and predicted visual vectors. However, the above methods learn separate mapping functions and fusion models, which are somewhat inelegant. In this paper we employ a neural-network mapping function to integrate these two processes into a unified multimodal models.\nAccording to this classification, our method falls into the second group. However, existing models ignore either the associative relations among modalities, associative relations among relative words, or the different contributions of each modality. This paper aims to integrate more perceptual information and the human-like associative memory into a unified multimodal model to learn better word representations."
        },
        {
            "heading": "3 Associative Multichannel Autoencoder",
            "text": "We first provide a brief description of the basic multichannel autoencoder for learning multimodal word representations (Figure 1). Then we extend the model with two associative memory modules and a gating mechanism (Figure 2) in the next sections."
        },
        {
            "heading": "3.1 Basic Mutichannel Autoencoder",
            "text": "An autoencoder is an unsupervised neural network which is trained to reconstruct a given input from its latent representation (Bengio, 2009). In this work, we propose a variant of autoencoder called multichannel autoencoder, which maps multimodal inputs into a common space.\nOur model extends the unimodal and bimodal autoencoder (Ngiam et al., 2011; Silberer and Lapata, 2014) to induce semantic representations integrating textual, visual and auditory information. As shown in Figure 1, our model first transforms input textual vector xt, visual vector xv and auditory vector xa to hidden representations:\nht = g(Wtxt + bt)\nhv = g(Wvxv + bv)\nha = g(Waxa + ba).\n(1)\nThen the hidden representations are concatenated together and mapped to a common space:\nhm = g(Wm[ht;hv;ha] + bm). (2)\nThe model is trained to reconstruct the hidden representations of the three modalities from the multimodal representation hm:\n[h\u0302t; h\u0302v; h\u0302a] = g(W \u2032mhm + bm\u0302), (3)\nand finally to reconstruct the original embeddings of textual, visual and auditory inputs:\nx\u0302t = g(W \u2032t h\u0302t + bt\u0302) x\u0302v = g(W \u2032vh\u0302v + bv\u0302) x\u0302a = g(W \u2032ah\u0302a + ba\u0302),\n(4)\nwhere x\u0302t, x\u0302v, x\u0302a are the reconstruction of input vectors xt, xv, xa, and h\u0302t, h\u0302v, h\u0302a\nare the reconstruction of hidden representations ht, hv, ha. The learning parameters {Wt,Wv,Wa,W \u2032t ,W \u2032v,W \u2032a,Wm,W \u2032m} are weight matrices, {bt, bv, ba, bt\u0302, bv\u0302, ba\u0302, bm, bm\u0302} are bias vectors. Here [\u00b7 ; \u00b7] denotes the vector concatenation, and g denotes the non-linear function which we use tanh(\u00b7).\nTraining a single-layer autoencoder corresponds to optimizing the learning parameters to minimize the overall loss between inputs and their reconstructions. Following (Vincent et al., 2010), we use squared loss:\nmin \u03b81 n\u2211 i=1 (||xit\u2212 x\u0302it||2+ ||xiv\u2212 x\u0302iv||2+ ||xia\u2212 x\u0302ia||2), (5) where i denotes the ith word, and the model parameters are \u03b81 = {Wt,Wv,Wa,Wm,W \u2032t ,W \u2032v, W \u2032a,W \u2032 m, bt, bv, ba, bm, bt\u0302, bv\u0302, ba\u0302, bm\u0302}.\nAutoencoders can be stacked to create deep networks. To enhance the quality of semantic representations, we employ a stacked multichannel autoencoder, which is composed of multiple hidden layers that are stacked together."
        },
        {
            "heading": "3.2 Integrating Modality Associations",
            "text": "In reality, the words that have corresponding images or sounds are only a small subset of the textual vocabulary. To obtain the perceptual vectors for each word, we need associations between modalities (i.e., text-to-vision and text-to-audition mapping functions), that transform the textual vectors into visual and auditory ones. Previous methods learn separate mapping functions and fusion models, which are somewhat inelegant. Here we employ a neural-network mapping function to incorporate this modality association module into multimodal models.\nTake text-to-vision mapping as an example. Suppose that T \u2208 Rmt\u00d7nt is the textual representation containing mt words, V \u2208 Rmv\u00d7nv is the visual representation containing mv ( mt) words, where nt and nv are dimensions of the textual and visual representations respectively. The textual and visual representations of the ith concept are denoted as Ti and Vi respectively. Our goal is to learn a mapping function f : g(WpT + bp) from textual to visual space such that the prediction f(Ti) is similar to the actual visual vector Vi. The set of visual representations along with their corresponding textual representations\nimage2vec\n...\nword2vec sound2vec\n...\n...Multimodal representations\ndog\n... ... ......\n.........\n...... ... ...... ...\n... ... ...\nIn case you need, we've collected the cutest small dog breeds to lif t your\nmood.\nThere's nothing that cheers you up quite as fast as a cute dog doing something peculiar.\nare used to learn the mapping function. To train the model, we employ a square loss:\nmin \u03b82 mv\u2211 i=1 ||f(Ti)\u2212 Vi||2, (6)\nwhere the training parameters are \u03b82 = {Wp, bp}. We adopt the same method to learn the text-toaudition mapping function."
        },
        {
            "heading": "3.3 Integrating Word Associations",
            "text": "Word associations are a proxy for an aspect of human semantic memory that is not sufficiently captured by the usual training objectives of multimodal models. Therefore we assume that incorporating the objective of word associations helps to learn better semantic representations. To achieve this, we propose to reconstruct the vector of associated word from the corresponding multimodal semantic representation. Specifically, in the decoding process we change the equation (3) to:\n[h\u0302t, h\u0302v, h\u0302a, h\u0302asc] = g(W \u2032mhm + bm\u0302), (7)\nand equation (4) to:\nx\u0302t = g(W \u2032t h\u0302t + bt\u0302) x\u0302v = g(W \u2032vh\u0302v + bv\u0302) x\u0302a = g(W \u2032ah\u0302a + ba\u0302)\nx\u0302asc = g(Wasch\u0302asc + basc).\n(8)\nTo train the model, we add an additional objective function, which is the mean square error\nbetween the embeddings of the associated word y and their re-constructive embeddings x\u0302asc:\nmin \u03b83 n\u2211 i=1 ||yi \u2212 x\u0302iasc||2, (9)\nwhere yi and xi are the embeddings of a pair of associated words. Here, y is the concatenation of three unimodal vectors [yt; yv; ya]. The parameters of word association module are \u03b83 = {Wt,Wv,Wa,Wm, W\u0302m,Wasc, bt, bv, ba, bm, bm\u0302, basc}. This additional criterion drives the learning towards a semantic representation capable of reconstructing its associated representation."
        },
        {
            "heading": "3.4 Integrating a Gating Mechanism",
            "text": "Considering that the meaning of each word has different dependencies on textual and perceptual information, we propose the sample-specific gate to assign different weights to each modality according to different words. The weight parameters are calculated by the following feed-forward neural networks:\ngt = g(Wgtxt + bgt)\ngv = g(Wgvxv + bgv)\nga = g(Wgaxa + bga),\n(10)\nwhere gt, gv and ga are value or vector gate of textual, visual and auditory representations respectively. For the value gate, Wgt, Wgv and Wga are vectors, and bgt, bgv and bga are value parameters. For the vector gate, the parameters Wgt, Wgv and Wga are matrices, bgt, bgv and bga are vectors. The value gate controls the importance weights of different input representations as a whole, whereas the vector gate can adjust the importance weights of each dimension of input representations.\nFinally, we compute element-wise multiplication of the textual, visual and auditory representations with their corresponding gates:\nxgt = xt gt xgv = xv gv xga = xa ga.\n(11)\nThe xgt, xgv and xga can be seen as the weighted textual, visual and auditory representations. The parameters of our gating mechanism is trained together with that of the proposed model."
        },
        {
            "heading": "3.5 Model Training",
            "text": "To train the AMA model, we use overall objective function of equation (5) + (6) + (9). In the training phase, model inputs are textual vectors, the corresponding visual vectors, auditory vectors, and association words (Figure 2). In the testing phase, we only need textual inputs to generate multimodal word representations."
        },
        {
            "heading": "4 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "Textual vectors. We use 300-dimensional GloVe vectors1 which are trained on the Common Crawl corpus consisting of 840B tokens and a vocabulary of 2.2M words2.\nVisual vectors. Our source of visual vectors are collected from ImageNet (Russakovsky et al., 2015) which covers a total of 21,841 WordNet synsets (Fellbaum, 1998) that have 14,197,122 images. For our experiments, we delete words with fewer than 50 images or words not in the Glove vectors, and sample at most 100 images for each word. To generate a visual vector for each word, we use the forward pass of a pre-trained VGGnet model3 and extract the hidden representation of the last layer as the feature vector. Then we use averaged feature vectors of the multiple images corresponding to the same word. Finally, we get 8,048 visual vectors of 128 dimensions.\nAuditory vectors. For auditory data, we gather audio files from Freesound4, in which we select words with more than 10 audio files and sample at most 50 sounds for one word. To extract auditory features, we use the VGG-net model which is pretrained on Audioset5. The final auditory vectors are averaged feature vectors of multiple audios of the same word, which contains 9,988 words of 128 dimensions6.\nWord associations. We use the word association data collected by (De Deyne et al., 2016), in which each word pair is generated by at least\n1http://nlp.stanford.edu/projects/ glove\n2We have tried skip-gram vectors and get the same conclusions.\n3http://www.vlfeat.org/matconvnet/ 4http://www.freesound.org/ 5https://research.google.com/audioset 6We build auditory vectors with the released code at: https://github.com/tensorflow/models/ tree/master/research/audioset\none subject7. This dataset includes mostly words with similar meaning (e.g., occasionally & sometimes, adored & loved, supervisor & boss) and related words (e.g., eruption & volcano, cortex & brain, umbrella & rain). We calculate the association score for each word pair (cue word + target word) as: the number of person who generated the word pair divided by the total number of people who were presented with the cue word. For training, we select pairs of associated words above a threshold of 0.15 and delete those that are not in the Glove vocabulary, which results in 7,674 word association data sets8. For the development set, we randomly sample 5,000 word association collections together with their association scores."
        },
        {
            "heading": "4.2 Model Settings",
            "text": "Our models are implemented with PyTorch (Paszke et al., 2017), optimized with Adam (Kingma and Ba, 2014). We set the initial learning rate to 0.05, and batch size to 64. We tune the number of layers over 1, 2, 3, the size of multimodal vectors over 100, 200, 300, and the size of each layer in textual channel over 300, 250, 200, 150, 100 and in visual/auditory channel over 128, 120, 90, 60. We train the model for 500 epochs and select the best parameters on the development set. All models are trained for 3 times and the average results are reported in Table 1.\nTo test the effect of each module, we separately train the following models: multichannel autoencoder with modality association (AMAM), with modality and word associations (AMAMW), with modality and word associations plus value/vector gate (AMA-MW-Gval/vec).\nFor AMA-M model, we initialize the text-tovision and text-to-audition mapping functions with pre-trained mapping matrices, which are parameters of one-layer feed-forward neural networks. The network uses input of the textual vectors, output of visual or auditory vectors, and is trained with SGD for 100 epochs. We initialize the network biases as zeros and network weights with He-initialisation (He et al., 2015). The best parameters of AMA-M model are 2 hidden layers, with textual channel size of 300, 250 and 150, visual/auditory channel size of 128,\n7The dataset can be found at: https:// simondedeyne.me/data.\n8We have done experiments with Synonyms (which are extracted from WordNet and PPDB corpora), and the results are not as good as using word associations.\n90, 60. For AMA-MW model, we use the best AMA-M model parameters as initialization, and train the model with word association data. The optimal parameter of association channel size is 300, 350, 556 (or 428 for bimodal inputs). For AMA-MW-Gval and AMA-MW-Gvec, we adopt the same training strategy as AMA-MW model. The code for training and evaluation can be found at: https://github.com/wangshaonan/ Associative-multichannel-autoencoder."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Evaluation Tasks",
            "text": "We test the baseline and proposed models on six standard evaluation benchmarks, covering two different tasks: (i) Semantic relatedness: Men-3000 (Bruni et al., 2014) and Wordrel-252 (Agirre et al., 2009); (ii) Semantic similarity: Simlex-999 (Hill et al., 2016), Semsim-7576 (Silberer and Lapata, 2014), Wordsim-203 and Simverb-3500 (Gerz et al., 2016). All test sets contain a list of word pairs along with their subject ratings.\nWe employ Spearman\u2019s correlation method to evaluate the performance of our models. This method calculates the correlation coefficients between model predictions and subject ratings, in which the model prediction is the cosine similarity between semantic representations of two words."
        },
        {
            "heading": "5.2 Baseline Multimodal Models",
            "text": "Most of existing multimodal models only utilize textual and visual modalities. For fair comparison, we re-implement several representative systems with our own textual and visual vectors. The Concatenation (CONC) model (Kiela and Bottou, 2014) is simple concatenation of normalized textual and visual vectors. The Mapping (Collell et al., 2017) and Ridge (Hill et al., 2014) models first learn a mapping matrix from textual to visual modality using feed-forward neural network and ridge regression respectively. After applying the mapping function on the textual vectors, they obtain the predicted visual vectors for all words in textual vocabulary. Then they concatenate the normalized textual and predicted visual vectors to get multimodal word representations. The SVD (Bruni et al., 2014) and CCA (Hill et al., 2014) models first concatenate normalized textual and visual vectors, and then conduct SVD or CCA transformations on the concatenated vectors.\nFor multimodal models with textual, visual and\n0.58\n0.6\n0.62\n0.64\n0.66\n100% 80% 60% 40% 20% A ve ra ge S pe ar m an 's co rr el at io ns\nPercentage of association data\nAMA-M(TV)\nAMA-M(TVA)\nauditory inputs, we implement CONC and Ridge as baseline models. The trimodal CONC model simply concatenates normalized textual, visual and auditory vectors. The trimodal Ridge model first learns text-to-vision and text-to-audition mapping matrices with ridge regression method. Then it applies the mapping functions on the textual vectors to get the predicted visual and auditory vectors. Fi ally, the normalized textual, predictedvisual and predicted-auditory vectors are concatenated to get the multimodal representations.\nAll above baseline models are implemented with Sklearn9. Same as the proposed AMA model,\n9http://scikit-learn.org/\nthe hyper-parameters of baseline models are tuned on the development set using Spearman\u2019s correlation method. In Ridge model, the optimal regularization parameter is 0.6. The Mapping model is trained with SGD for maximum 100 epochs with early stopping, and the optimal learning rate is 0.001. The output dimension of SVD and CCA models are 300."
        },
        {
            "heading": "5.3 Results and Discussion",
            "text": "As shown in Table 1, we divide all models into six groups: (1) existing multimodal models (with textual and visual inputs) in which results are reprinted from Collell et al. (2017). (2) Unimodal models with textual, (predicted) visual or (pre-\ndicted) auditory inputs. (3) Our re-implementation of baseline bimodal models with textual and visual inputs (TV). (4) Our AMA models with textual and visual inputs. (5) Our implementation of trimodal baseline models with textual, visual and auditory inputs (TVA). (6) Our AMA model with textual, visual and auditory inputs.\nOverall performance Our AMA models (in group 4 and 6) clearly outperform their baseline unimodal and multimodal models (in group 2, 3 and 5). We use Wilcoxon signed-rank test to check if significant difference exists between two models. Results show that our multimodal models perform significantly better (p < 0.05) than all baseline models.\nAs shown clearly, our bimodal and trimodal AMA models achieve better performance than baselines in both V/A (visual or auditory, the testing data that have associated visual or auditory vectors) and ZS (zero-shot, the testing data that do not have associated visual or auditory vectors) region. In other words, our models outperform baseline models on words with or without perceptual information. The good results in ZS region also indicate that our models have good generalization capacity.\nUnimodal baselines As shown in group 2, the Glove vectors are much better than CNNvisual and CNN-auditory vectors, in which CNNauditory has the worst performance on capturing concept similarities. Comparing with visual and auditory vectors, the predicted visual and auditory vectors achieve much better performance. This indicates that the predicted vectors contain richer information than purely perceptual representations and are more useful for building semantic representations.\nMultimodal baselines For bimodal models (group 3), the CONC model that combines Glove and visual vectors performs worse than Glove on four out of six datasets, suggesting that simple concatenation might be suboptimal. The Mapping and Ridge models, which combine Glove and predicted visual vectors, improve over Glove on five out of six datasets in ALL regions. This reinforces the conclusion that the predicted visual vectors are more useful in building multimodal models. The SVD model gets similar results as Ridge model. The CCA model maps different modality inputs into a common space, achieving better results on some datasets and worse results on the others.\nThe improvement on three benchmark tests shows the potential of mapping multimodal inputs into a common space.\nThe above results can also be observed in the trimodal CONC and Ridge models (group 5). Overall, the trimodal models, which utilize additional auditory inputs, get slightly worse performance than bimodal models. This is partly caused by the fusion method of concatenation. Note that our proposed AMA models are more effective with trimodal inputs as shown in group 6. Our multimodal models With either bimodal or trimodal inputs, the proposed AMA-M model outperforms all baseline models by a large margin. Specifically our AMA-M model achieves an relative improvement of 4.1% on average (4.5% with trimodal inputs) over the state-of-the-art Ridge model. This illustrates that our AMA models can productively combine textual and perceptual representations. Moreover, our AMA-MW model, which employs word associations, achieves an average improvement of 1.5% (2.7% with trimodal inputs) over the AMA-M model. That is to say, the representation ability of multimodal models can be clearly improved by learning associative relations between words. Furthermore, the AMAMW-Gval model improves the AMA-MW model by 1.3% (0.3% with trimodal inputs) on average, illustrating that the gating mechanism (especially the value gate) helps to learn better semantic representations.\nIn addition, we explore the effect of word association data size. We find that the decrease of association data has no discernible effect on model performance: when using 100%, 80%, 60%, 40%, 20% of the data, the average results are 0.6479, 0.6409, 0.6361, 0.6430, 0.6458 in bimodal model. The same trend is observed in trimodal models."
        },
        {
            "heading": "6 Conclusions and Future Work",
            "text": "We have proposed a cognitively-inspired multimodal model \u2014 associative multichannel autoencoder \u2014 which utilizes the associations between modalities and related words to learn multimodal word representations. Performance improvement on six benchmark tests shows that our models can efficiently fuse different modality inputs and build better semantic representations.\nUltimately, the present paper sheds light on the fundamental questions of how to learn word meanings, such as the plausibility of reconstructing per-\nceptual information, associating related concepts and grounding word symbols to external environment. We believe that one of the promising future directions is to learn from how humans learn and store semantic word representations to build a more effective computational model."
        },
        {
            "heading": "Acknowledgement",
            "text": "The research work descried in this paper has been supported by the National Key Research and Development Program of China under Grant No. 2017YFB1002103 and also supported by the Natural Science Foundation of China under Grant No. 61333018. The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper."
        }
    ],
    "title": "Associative Multichannel Autoencoder for Multimodal Word Representation",
    "year": 2018
}