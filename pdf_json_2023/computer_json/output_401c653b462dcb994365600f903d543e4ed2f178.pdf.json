{
    "abstractText": "In this paper, we discuss the convergence analysis of the conjugate gradient-based algorithm for the functional linear model in the reproducing kernel Hilbert space framework, utilizing early stopping results in regularization against overtting. We establish the convergence rates depending on the regularity condition of the slope function and the decay rate of the eigenvalues of the operator composition of covariance and kernel operator. Our convergence rates match the minimax rate available from the literature.",
    "authors": [
        {
            "affiliations": [],
            "name": "S. Sivananthan"
        },
        {
            "affiliations": [],
            "name": "B.K. Sriperumbudur"
        }
    ],
    "id": "SP:4981ce5691642f2de5ee9a16575e20fd8affceca",
    "references": [
        {
            "authors": [
                "N. Aronszajn"
            ],
            "title": "Theory of reproducing kernels",
            "venue": "Trans. Amer. Math. Soc. 68, 337-404",
            "year": 1950
        },
        {
            "authors": [
                "K. Balasubramanian",
                "M uller",
                "H.-G",
                "B.K. Sriperumbudur"
            ],
            "title": "Uni ed RKHS methodology and analysis for functional linear and single-index models, arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "F. Bauer",
                "S. Pereverzev",
                "L. Rosasco"
            ],
            "title": "On regularization algorithms in learning theory",
            "venue": "J. Complexity. 23 (1), 52-72",
            "year": 2007
        },
        {
            "authors": [
                "G. Blanchard",
                "N. Kr amer"
            ],
            "title": "Convergence rates of kernel conjugate gradient for random design regression",
            "venue": "Anal. Appl. Singap. 14 (6), 763-794",
            "year": 2016
        },
        {
            "authors": [
                "T.T. Cai",
                "P. Hall"
            ],
            "title": "Prediction in functional linear regression",
            "venue": "Ann. Statist. 34 (5), 2159-2179",
            "year": 2006
        },
        {
            "authors": [
                "T.T. Cai",
                "M. Yuan"
            ],
            "title": "Minimax and adaptive prediction for functional linear regression",
            "venue": "J.Amer. Statist. Assoc. 107 (499), 1201-1216",
            "year": 2012
        },
        {
            "authors": [
                "H. Cardot",
                "F. Ferraty",
                "P. Sarda"
            ],
            "title": "Spline estimators for the functional linear model",
            "venue": "Statist. Sinica. 13 (3), 571-591",
            "year": 2003
        },
        {
            "authors": [
                "X. Chen",
                "B. Tang",
                "J. Fan",
                "X. Guo"
            ],
            "title": "Online gradient descent algorithms for functional data learning",
            "venue": "J. Complexity. 70, Paper No 101635 (14)",
            "year": 2022
        },
        {
            "authors": [
                "F. Cucker",
                "S. Smale"
            ],
            "title": "On the mathematical foundations of learning",
            "venue": "Bull. Amer. Math. Soc. N.S. 39 (1), 1-49",
            "year": 2002
        },
        {
            "authors": [
                "F. Cucker",
                "Zhou",
                "D.-X."
            ],
            "title": "Learning theory: an approximation theory viewpoint, vol",
            "venue": "24. Cambridge University Press, Cambridge,",
            "year": 2007
        },
        {
            "authors": [
                "F. Ferraty",
                "P. Vieu"
            ],
            "title": "Nonparametric Functional Data Analysis",
            "venue": "Theory and practice. Springer Series in Statistics. Springer, New York",
            "year": 2006
        },
        {
            "authors": [
                "M. Forni",
                "L. Reichlin"
            ],
            "title": "Let's get real: A factor analytical approach to disaggregated business cycle dynamics",
            "venue": "The Review of Economic Studies. 65 (3), 453-473",
            "year": 1998
        },
        {
            "authors": [
                "X. Guo",
                "Guo",
                "Z.-C.",
                "L. Shi"
            ],
            "title": "Capacity dependent analysis for functional online learning algorithms",
            "venue": "Appl. Comput. Harmon. Anal. 67, Paper No 101567 (30)",
            "year": 2023
        },
        {
            "authors": [
                "P. Hall",
                "J.L. Horowitz"
            ],
            "title": "Methodology and convergence rates for functional linear regression",
            "venue": "Ann. Statist. 35 (1), 70-91",
            "year": 2007
        },
        {
            "authors": [
                "M. Hanke"
            ],
            "title": "Conjugate Gradient Type Methods for Ill-Posed Problems, vol",
            "venue": "327 of Pitman Research Notes in Mathematics Series. Longman Scienti c & Technical, Harlow,",
            "year": 1995
        },
        {
            "authors": [
                "L. Horv ath",
                "P. Kokoszka"
            ],
            "title": "Inference for functional data with applications",
            "venue": "Springer Series in Statistics. Springer, New York",
            "year": 2012
        },
        {
            "authors": [
                "Y. Li",
                "T. Hsing"
            ],
            "title": "On rates of convergence in functional linear regression",
            "venue": "J.Multivariate Anal. 98 (9), 1782-1804",
            "year": 2007
        },
        {
            "authors": [
                "J. Lin",
                "V. Cevher"
            ],
            "title": "Kernel conjugate gradient methods with random projections",
            "venue": "Appl. Comput. Harmon. Anal. 55, 223-269",
            "year": 2021
        },
        {
            "authors": [
                "M uller",
                "H.-G.",
                "U. Stadtm uller"
            ],
            "title": "Generalized functional linear models",
            "venue": "Ann. Statist. 33 (2), 774805",
            "year": 2005
        },
        {
            "authors": [
                "S. Pereverzyev"
            ],
            "title": "An introduction to arti cial intelligence based on reproducing kernel Hilbert spaces",
            "venue": "Compact Textbooks in Mathematics, Birkh auser",
            "year": 2022
        },
        {
            "authors": [
                "C. Preda",
                "G. Saporta"
            ],
            "title": "Clusterwise PLS regression on a stochastic process",
            "venue": "Comput. Statist. Data Anal. 49 (1), 99-108",
            "year": 2005
        },
        {
            "authors": [
                "J.O. Ramsay",
                "C.J. Dalzell"
            ],
            "title": "Some tools for functional data analysis",
            "venue": "J. Roy. Statist. Soc. Ser. B 53 (3), 539-572",
            "year": 1991
        },
        {
            "authors": [
                "J.O. Ramsay",
                "B.W. Silverman"
            ],
            "title": "Functional Data Analysis, second ed",
            "venue": "Springer Series in Statistics. Springer, New York",
            "year": 2005
        },
        {
            "authors": [
                "H. Tong"
            ],
            "title": "Functional linear regression with Huber loss",
            "venue": "J. Complexity 74, Paper No 101696 (14)",
            "year": 2023
        },
        {
            "authors": [
                "H. Tong",
                "M. Ng"
            ],
            "title": "Analysis of regularized least squares for functional linear regression model",
            "venue": "J. Complexity 49, 85-94",
            "year": 2018
        },
        {
            "authors": [
                "M. Yuan",
                "T.T. Cai"
            ],
            "title": "A reproducing kernel Hilbert space approach to functional linear regression",
            "venue": "Ann. Statist. 38 (6), 3412-3444",
            "year": 2010
        },
        {
            "authors": [
                "F. Zhang",
                "W. Zhang",
                "R. Li",
                "H. Lian"
            ],
            "title": "Faster convergence rate for functional linear regression in reproducing kernel Hilbert spaces",
            "venue": "Statistics. 54 (1), 167-181",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "rithm for the functional linear model in the reproducing kernel Hilbert space framework, utilizing early stopping results in regularization against over- tting. We establish the convergence rates depending on the regularity condition of the slope function and the decay rate of the eigenvalues of the operator composition of covariance and kernel operator. Our convergence rates match the minimax rate available from the literature.\n1 Introduction\nThe functional linear regression (FLR) model is one of the fundamental tools for analyzing functional data, introduced by Ramsay and Dalzell [22]. The model gained popularity due to its simplicity in dealing with high-dimensional functional data. For example, it is widely used in medicine, chemometrics, and economics [11,12,21,23]. Mathematically, the FLR model is stated as\nY = \u222b S X(t)\u03b2\u2217(t) dt+ \u03f5,\nwhere Y is a real-valued random variable, (X(t); t \u2208 S) is a continuous time process, \u03b2\u2217 is an unknown slope function and \u03f5 is a zero mean random noise, independent of X, with nite variance \u03c32. Throughout the paper, we assume that X and \u03b2\u2217 are in L2 (S), and S is a compact subset of Rd. In the context of the slope function, it is evident that\n\u03b2\u2217 := arg min \u03b2\u2208L2(S) E [Y \u2212 \u3008X,\u03b2\u3009]2 .\nThe goal is to construct an estimator \u03b2\u0302 to approximate the slope function \u03b2\u2217 using observed empirical data {(X1, Y1) , (X2, Y2) , \u00b7 \u00b7 \u00b7 , (Xn, Yn)}, whereXi's are i.i.d. copies of random functionX. The main approach in estimating the slope function \u03b2\u2217 is based on the representation of the estimator function and functional data in terms of certain basis functions. In this paper, we utilize the framework of reproducing kernel Hilbert space (RKHS) to construct an estimator function \u03b2\u0302 using the conjugate gradient method.\nIn [7], the authors used penalized B-spline basis functions to represent the estimator function and also introduced an alternate smooth version of functional principal component analysis (FPCA) to construct \u03b2\u0302. A Fourier basis approach was explored in [17] and the FPCA-based approach is\n2020 Mathematics Subject Classi cation: 62R10, 62G20, 65F22. Key words: reproducing kernel Hilbert space, conjugate gradient, covariance operator, functional linear regression. \u00a9 GuptaN., Sivananthan S., SriperumbudurB.K., 2023\n34 GuptaN., Sivananthan S., SriperumbudurB.K.\ninvestigated in [5,14,19]. One of the profound choices for the basis functions in the FPCA method is to use the eigenfunctions obtained from the covariance operator of the given data. However, Cai and Yuan [6] demonstrated that this choice may not be suitable for all cases, as shown with the example of Canadian weather data. This observation strongly motivated the researchers to explore alternative choices of basis functions.\nIt is well-known in learning theory that kernel methods represent predictor functions using data-driven kernel functions, resulting in good generalization error (see [3, 9, 10, 20]). Cai and Yuan [26] proposed utilizing the kernel method approach, wherein the estimator is expressed as a linear combination of kernel functions. The method achieves optimal rates under the assumption that the slope function \u03b2\u2217 belongs to the RKHS. Later in [6], they used the regularization technique to achieve optimal rates without the Sacks Ylvisaker condition, which was a necessary assumption in [26]. Further analysis of the FLR model within the framework of RKHS has been studied and discussed in [2, 24, 25, 27]. Since the computational complexity of these techniques is O(n3), they incur high computational costs when dealing with large datasets.\nTo address this shortcoming, Blanchard and Kr amer [4] employed the conjugate gradient method in the kernel ridge regression method, by utilizing an early stopping rule that also serves as a form of regularization. This reduces the computational complexity to O ( n2m ) for m number of\niterations [18]. Inspired by their work, in this paper, we propose an estimator \u03b2\u0302 for the FLR model by employing the conjugate gradient (CG) approach with an early stopping rule. We speci cally focus on the CG method due to its outstanding computational characteristics, setting it apart from other approaches. Since it aggressively targets the reduction of residual errors, it is commonly observed in practical applications that the CG method achieves convergence in signi cantly fewer iterations compared to other gradient descent techniques, as discussed in the context of kernel learning by [13] and [8]. We obtain a convergence rate for \u2016\u03b2\u0302 \u2212 \u03b2\u2217\u2016L2(S) and show it to align with the minimax rates of the FLR model [6, 27], thereby establishing the minimax optimality of our estimator.\nThe paper is organized as follows. In Section 2, we present the necessary background for the conjugate gradient method for the FLR model in the RKHS setting and explain some important properties of certain orthogonal polynomials. In Section 3, we discuss our assumptions and provide convergence rates of the CG method. We present the supplementary results, which will be used to prove the main theorem, in the nal section, Section 4.\n2 Preliminaries and Notations\nLet H be a Hilbert space of real-valued functions on a compact subset S of Rd. We say that H is RKHS if for every x \u2208 S, the pointwise evaluation map f 7\u2192 f(x) is continuous on H. As a consequence of the Riesz representation theorem, there is a unique kernel function k : S \u00d7 S \u2192 R, called the reproducing kernel such that k (s, \u00b7) \u2208 H for any s \u2208 S satis es the reproducing property:\nf (s) = \u3008k(s, \u00b7), f\u3009H , \u2200f \u2208 H.\nIt is easy to see that the associated kernel function k is symmetric and positive de nite. Conversely, for a given symmetric and positive de nite function k, we can construct a unique RKHS with k as the reproducing kernel. For a detailed study of RKHS, we refer the reader to [1].\nWe assume that k is continuous; then the associated RKHS H is separable and the embedding operator (inclusion operator) J : H \u2192 L2 (S), which is de ned as (Jf)(x) = \u3008k(x, \u00b7), f\u3009H is compact. The adjoint operator J\u2217 : L2 (S) \u2192 H is given by\n(J\u2217g)(x) = \u222b S k(x, t)g(t) dt.\nWe denote the integral operator, T := JJ\u2217 : L2 (S) \u2192 L2 (S) and the covariance operator C := E [X \u2297X] : L2 (S) \u2192 L2 (S), where \u2297 is the L2 (S) tensor product.\nConvergence analysis of kernel conjugate gradient for functional linear regression 35\nGiven (Xi, Yi) n i=1 i.i.d. copies of (X,Y ), our estimator is de ned as\n\u03b2\u0302 = argmin \u03b2\u2208H\n1\nn n\u2211 i=1 [Yi \u2212 \u3008\u03b2,Xi\u3009L2 ]2\n= argmin \u03b2\u2208H\n1\nn n\u2211 i=1 [Yi \u2212 \u3008J\u03b2,Xi\u3009L2 ]2\n= argmin \u03b2\u2208H\n1\nn n\u2211 i=1 [Yi \u2212 \u3008\u03b2, J\u2217Xi\u3009H]2 .\nA solution for this optimization problem can be obtained by solving\nJ\u2217C\u0302nJ\u03b2\u0302 = J \u2217R\u0302, (2.1)\nwhere\nC\u0302n := 1\nn n\u2211 i=1 Xi \u2297Xi and R\u0302 := 1 n n\u2211 i=1 YiXi.\nWe denote \u039b := T 1 2CT 1 2 and \u039bn := T 1 2 C\u0302nT 1 2 .\nThe fundamental idea behind the conjugate gradient method is to restrict the optimization problem to a set of subspaces (data dependent), known as Krylov subspaces, de ned as\nKm ( J\u2217R\u0302, J\u2217C\u0302nJ ) : = span { J\u2217R\u0302, J\u2217C\u0302nJJ \u2217R\u0302, ( J\u2217C\u0302nJ )2 J\u2217R\u0302, . . . , ( J\u2217C\u0302nJ )m\u22121 J\u2217R\u0302 } = { p ( J\u2217C\u0302nJ ) J\u2217R\u0302 : p \u2208 Pm\u22121 } ,\nwhere Pm\u22121 is a set of real polynomials of degree at most m \u2212 1. Then the CG solution after m iterations is\n\u03b2\u0302m = arg min \u03b2\u2208Km(J\u2217R\u0302,J\u2217C\u0302nJ) \u2225\u2225\u2225J\u2217R\u0302\u2212 J\u2217C\u0302nJ\u03b2\u2225\u2225\u2225 H . (2.2)\nThe iterated solution, because the problem is restricted to the Krylov subspaces, will take the form\n\u03b2\u0302m = qm ( J\u2217C\u0302nJ ) J\u2217R\u0302 with qm being a polynomial of degree at most m\u2212 1. Associated with each iterated polynomial qm, we have a residual polynomial de ned as pm (x) = 1\u2212xqm (x) \u2208 P0m, where P0m is a set of real polynomials of degree at most m and having constant term equal to 1.\nSince the construction of the estimator involves forward multiplication through the residual polynomial pm, it is essential to understand certain fundamental characteristics of these polynomials.\nSuppose (\u03ben,i, en,i)i\u2208I is an eigenvalue-eigenfunction pair for the operator \u039b\u0302n with \u03ben,i in [0, \u03ba\u039b], i \u2208 I, where {en,i : i \u2208 I} is an orthonormal system in L2 (S) and \u03ba\u039b is a constant that bounds the kernel function of the integral operator \u039b. For u > 0, denote Fu = 1[0,u) ( \u039b\u0302n ) as the orthogonal projector onto the space spanned by {en,i : i \u2208 I, \u03ben,i < u}. For each integer l \u2265 0, we will introduce measure \u00b5 (l) n which is de ned as\n\u00b5(l)n := \u2211 i\u2208I \u03beln,i \u2329 T 1 2 R\u0302, en,i \u232a2 L2 \u03b4\u03ben,i ,\nwhere \u03b4x is Dirac measure centered at x. In particular, for l = 0 we use the convention 0 0 = 1.\n36 GuptaN., Sivananthan S., SriperumbudurB.K.\nAssociated to each measure \u00b5 (l) n , l \u2265 0, we de ne the scalar product of two polynomials as\n[p, q](l) := \u222b \u03ba\u039b 0 p (t) q (t) d\u00b5(l)n (t)\n= \u2329 p ( \u039b\u0302n ) T 1 2 R\u0302, ( \u039b\u0302n )l q ( \u039b\u0302n ) T 1 2 R\u0302 \u232a L2\n= \u2211 i\u2208I p (\u03ben,i) q (\u03ben,i) (\u03ben,i) l \u2329 T 1 2 R\u0302, en,i \u232a2 L2 .\nFor l = 0, we see that\n[p, q](0) = \u2329 p ( \u039b\u0302n ) T 1 2 R\u0302, q ( \u039b\u0302n ) T 1 2 R\u0302 \u232a L2 .\nSince \u039b\u0302n is a nite rank operator, it has only a nite number of non-zero eigenvalues. Consequently, we observe that the measure \u00b5 (l) n has nite support of cardinality, independent of l. Indeed, if \u03ben,j = 0 for some j \u2208 I, then we have\nT 1 2 C\u0302nT 1 2 en,j = 0 =\u21d2 \u2329 T 1 2 C\u0302nT 1 2 en,j , en,j \u232a L2 = 0\n=\u21d2 1 n n\u2211 i=1 \u2329 T 1 2Xi, en,j \u232a2 L2 = 0\n=\u21d2 \u2329 T 1 2Xi, en,j \u232a L2 = 0, \u2200 1 \u2264 i \u2264 n.\nNow we see that \u2329 T 1 2 R\u0302, en,j \u232a L2 = 1 n n\u2211 i=1 Yi \u2329 T 1 2Xi, en,j \u232a L2 = 0.\nThis concludes that \u00b5 (l) n has nite support of cardinality (independent of l), let's say n\u03b3 \u2264 n. It is clear from (2.2) that qm minimizes\u2225\u2225\u2225J\u2217R\u0302\u2212 J\u2217C\u0302nJq (J\u2217C\u0302nJ) J\u2217R\u0302\u2225\u2225\u2225 H = \u2225\u2225\u2225T 12 R\u0302\u2212 T 12 C\u0302nT 12 q (T 12 C\u0302nT 12)T 12 R\u0302\u2225\u2225\u2225 L2\nover q \u2208 Pm\u22121. Equivalently, consider p (x) = 1\u2212 xq (x), then pm minimizes\u2225\u2225\u2225p(T 12 C\u0302nT 12)T 12 R\u0302\u2225\u2225\u2225 L2 = [p, p](0) (2.3)\nover p \u2208 P0m. In other words, we can say that pm is the orthogonal projection of origin onto the a ne subspace P0m \u2282 Pm for the scalar product [\u00b7, \u00b7](0). We take q0 = 0, p0 = 1 for m = 0. Because of the properties of projections, pm is orthogonal to P0m. Since P0m = 1 + \u03c0Pm\u22121 is parallel to \u03c0Pm\u22121, where (\u03c0(q)) (x) = xq(x) is a shift operator. So we have 0 = [pm, \u03c0q](0) = [pm, q](1) for any q \u2208 Pm\u22121 which concludes that p0, . . . , pn\u03b3\u22121 is an orthogonal sequence of polynomials with respect to [\u00b7, \u00b7](1). For m = n\u03b3 , we can see that [\u00b7, \u00b7](l) is semide nite product on Pn\u03b3 and pn\u03b3 is unique element of P0n\u03b3 satisfying [ pn\u03b3 , pn\u03b3 ] (0)\n= 0. Hence, for m = n\u03b3 , unicity of the solution holds and[ pn\u03b3 , pm ] (1) = 0 for all m \u2264 n\u03b3 . By applying the representer theorem to (2.1),\n\u03b2\u0302 \u2208 span {\u222b\nS k(\u00b7, t)Xi(t) dt : i = 1, . . . , n\n} ,\nConvergence analysis of kernel conjugate gradient for functional linear regression 37\ni,e., there exists a \u03b1 := (\u03b11, . . . , \u03b1n) \u22a4 \u2208 Rn such that \u03b2\u0302 = \u2211n i=1 \u03b1i \u222b S k (\u00b7, t)Xi (t) dt. Using this in (2.1), we can solve K\u03b1 = y to get a solution of (2.1), where\nK \u2208 Rn\u00d7n with [K]ij: = \u222b S \u222b S k(s, t)Xi(t)Xj(s) dt ds\nand y = (Y1, . . . , Yn) \u22a4 \u2208 Rn. We refer the reader to [15] for the iterative methodology to create the CG estimator of \u03b2\u0302 in (2.1).\nThe following lemma, which lists several properties of orthogonal polynomials, is proven in [4]. It will be used frequently throughout the remainder of the paper.\nLemma 2.1. (Lemma 5.2 in [4]) Let m be any integer satisfying 1 \u2264 m \u2264 n\u03b3 . 1. The polynomial pm has exactly m distinct roots belonging to (0, \u03ba\u039b], denoted by (xk,m)1\u2264k\u2264m\nin increasing order.\n2. pm is positive, decreasing and convex on the interval [0, x1,m).\n3. De ne the function \u03c6m on the interval [0, x1,m) as\n\u03c6m (x) = pm (x)\n( x1,m\nx1,m \u2212 x\n) 1 2\n.\nThen it holds\n[pm, pm] 1 2 (0) = \u2225\u2225\u2225pm (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225 L2 \u2264 \u2225\u2225\u2225Fx1,m\u03c6m (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225 L2 ,\nand furthermore, for any \u03bd \u2265 0,\nsup x\u2208[0,x1,m]\nx\u03bd\u03c62m (x) \u2264 \u03bd\u03bd \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u2212\u03bd . (2.4)\n4. Denote p (2) 0 , p (2) 1 , . . . , p (2) n\u03b3\u22121 the unique sequence of orthogonal polynomial with respect to [\u00b7, \u00b7]2\nand with constant term equal to 1. This sequence enjoys properties (1) and (2) above with( x (2) k,m ) 1\u2264k\u2264m denoting the distinct roots of p (2) m in increasing order. Then it holds that x1,m \u2264 x(2)1,m. Finally, the following holds:\n0 \u2264 p\u2032m\u22121 (0)\u2212 p \u2032 m (0) = [pm\u22121, pm\u22121](0) \u2212 [pm, pm\u22121](0)[ p (2) m\u22121, p (2) m\u22121 ] (1) \u2264 [pm\u22121, pm\u22121](0)[ p (2) m\u22121, p (2) m\u22121 ] (1) .\n3 The Main Result\nIn this section, we present the convergence rate of the conjugate gradient method in functional linear regression. Our proofs are inspired by the ideas of [4]. The analysis depends on the eigenvalue behaviour of the operator \u039b = T 1 2CT 1 2 which indicates the behaviour of eigenvalues of the kernel operator T and the covariance operator C. First, we begin with a list of assumptions that are required for our convergence rate analysis.\nAssumption 3.1. (Source Condition) There exists g \u2208 L2 such that \u03b2\u2217 = T 1 2 ( T 1 2CT 1 2 )\u03b1 g,\nwhere \u03b1 is any positive real number.\nNote that the assumption implies \u03b2\u2217 \u2208 H with additional smoothness. In [27], the authors use this source condition to derive the minimax and faster convergence rate for the Tikhonov regularization with 0 < \u03b1 \u2264 12 .\n38 GuptaN., Sivananthan S., SriperumbudurB.K.\nAssumption 3.2. (Decay Condition) For some s \u2208 (0, 1),\ni\u2212 1 s \u2272 \u03bei \u2272 i\u2212 1 s \u2200 i \u2208 I,\nwhere (\u03bei, ei)i\u2208I is the eigenvalue-eigenvector pair of operator \u039b and the symbol \u2272 means that there exist constants b,B > 0 such that bi\u2212 1 s \u2264 \u03bei \u2264 Bi\u2212 1 s for all i \u2208 I.\nThis decay of eigenvalues is related to the e ective dimensionality because under this assumption we have that N (\u03bb) := trace(\u039b (\u039b + \u03bbI)\u22121) \u2264 D2 (\u03ba\u039b\u03bb)\u2212s for all \u03bb \u2208 (0, 1] and an appropriate choice of D > 0.\nAssumption 3.3. (Fourth Moment Condition) For any f \u2208 L2 (S),\nE \u3008X, f\u30094L2 \u2264 c0 ( E \u3008X, f\u30092L2 )2 for some constant c0 > 0.\nWe de ne the early stopping rule to stop the CG method at an early stage m\u2217 n. This is mainly used for its implicit regularization property. The early stopping, de ned as m\u2217, is the rst iteration for which the residual term is less than some prede ned threshold. Now we state and prove our main theorem.\nTheorem 3.1. Let \u03b1 > 0, \u03c4 > 0 and E \u2016X\u20164 < \u221e. Suppose Assumptions 3.1 3.3 hold, and stopping rule holds with threshold\n\u2126 = (2 + \u03c4)n\u2212 \u03b1+1 1+s+2\u03b1 .\nThen for large enough n and \u03bb = c (\u03b1, \u03b4)n\u2212 1\n1+s+2\u03b1 , it holds with probability at least 1\u2212 \u03b4 that\u2225\u2225\u2225\u03b2\u0302m\u2217 \u2212 \u03b2\u2217\u2225\u2225\u2225 L2 \u2272 n\u2212 \u03b1 1+s+2\u03b1 ,\nwhere c (\u03b1, \u03b4) is a constant that depends only on \u03b1 and \u03b4. Proof. Let \u03bb \u2265 ( 4c21 n ) 1 s+1\nand de ne F\u22a5u := (I \u2212 Fu). We start by considering the error term:\u2225\u2225\u2225\u03b2\u0302m \u2212 \u03b2\u2217\u2225\u2225\u2225 L2 = \u2225\u2225\u2225J\u03b2\u0302m \u2212 \u03b2\u2217\u2225\u2225\u2225 L2\n= \u2225\u2225\u2225Jqm (J\u2217C\u0302nJ) J\u2217R\u0302\u2212 \u03b2\u2217\u2225\u2225\u2225\nL2 = \u2225\u2225\u2225T 12 qm (\u039b\u0302n)T 12 R\u0302\u2212 T 12\u039b\u03b1g\u2225\u2225\u2225\nL2 \u2264 \u2225\u2225\u2225T 12\u2225\u2225\u2225\nop \u2225\u2225\u2225qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g\u2225\u2225\u2225 L2\n\u2272 \u2225\u2225\u2225qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g\u2225\u2225\u2225\nL2 \u2264 \u2225\u2225\u2225Fu (qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g)\u2225\u2225\u2225 L2 + \u2225\u2225\u2225F\u22a5u (qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g)\u2225\u2225\u2225 L2\n\u2264 \u2225\u2225\u2225Fu (qm (\u039b\u0302n)T 12 R\u0302\u2212 qm (\u039b\u0302n)T 12 C\u0302n\u03b2\u2217)\u2225\u2225\u2225\nL2\ufe38 \ufe37\ufe37 \ufe38 Term\u22121\n+ \u2225\u2225\u2225Fu (qm (\u039b\u0302n)T 12 C\u0302n\u03b2\u2217 \u2212 \u039b\u03b1g)\u2225\u2225\u2225\nL2\ufe38 \ufe37\ufe37 \ufe38 Term\u22122\n+ \u2225\u2225\u2225F\u22a5u (qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g)\u2225\u2225\u2225\nL2\ufe38 \ufe37\ufe37 \ufe38 Term\u22123 .\nConvergence analysis of kernel conjugate gradient for functional linear regression 39\nIn the third step we have used that Jqm ( J\u2217C\u0302nJ ) J\u2217 = T 1 2 qm ( T 1 2 C\u0302nJ ) T 1 2 which can be derived easily using spectral representation. Further, we will estimate each term separately using Lemma 4.1 and Lemma 4.2.\nEstimation of Term-1:\u2225\u2225\u2225Fu (qm (\u039b\u0302n)T 12 (R\u0302\u2212 C\u0302n\u03b2\u2217))\u2225\u2225\u2225 L2\n\u2264 \u2225\u2225\u2225\u2225Fuqm (\u039b\u0302n)(\u039b\u0302n + \u03bbI) 12\u2225\u2225\u2225\u2225\nop \u2225\u2225\u2225\u2225(\u039b\u0302n + \u03bbI)\u2212 12 (\u039b + \u03bbI) 12\u2225\u2225\u2225\u2225 op\n\u00d7 \u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12 (R\u0302\u2212 C\u0302n\u03b2\u2217)\u2225\u2225\u2225\nL2 \u2272 \u221a\n\u03c32N (\u03bb) n\u03b4 \u2225\u2225\u2225\u2225Fuqm (\u039b\u0302n)(\u039b\u0302n + \u03bbI) 12\u2225\u2225\u2225\u2225 op (by Lemmas 4.1, 4.2)\n\u2264 \u221a\n\u03c32N (\u03bb) n\u03b4\n( sup\nx\u2208[0,u] x\n1 2 qm (x) + \u03bb 1 2 sup x\u2208[0,u] qm (x)\n)\n\u2264 \u221a\n\u03c32N (\u03bb) n\u03b4 ( sup x\u2208[0,u] qm (x) ) 1 2 ( sup x\u2208[0,u] xqm (x) ) 1 2 + \u03bb 1 2 \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223 \n\u2264 \u221a\n\u03c32N (\u03bb) n\u03b4 (\u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223 12 + \u03bb 12 \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223) . Following argument ensures the last inequality: If m = 0, we use pm \u2261 1 and qm \u2261 0, so the inequality follows for any u > 0. If m \u2265 1, we use pm is decreasing and convex in [0, u], qm (x) \u2264\u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223 for all x \u2208 [0, u) and xqm (x) \u2264 1 for all x \u2208 [0, u) as u \u2264 x1,m.\nEstimation of Term-2: Using Lemma 4.4, and the fact that |pm (x)| \u2264 1 for all x \u2208 [0, u), we get \u2225\u2225\u2225Fu (qm (\u039b\u0302n)T 12 C\u0302n\u03b2\u2217 \u2212 \u039b\u03b1g)\u2225\u2225\u2225\nL2 = \u2225\u2225\u2225Fu (qm (\u039b\u0302n)T 12 C\u0302nT 12\u039b\u03b1g \u2212 \u039b\u03b1g)\u2225\u2225\u2225\nL2 = \u2225\u2225\u2225Fu (qm (\u039b\u0302n) \u039b\u0302n \u2212 I)\u039b\u03b1g\u2225\u2225\u2225 L2 = \u2225\u2225\u2225Fupm (\u039b\u0302n)\u039b\u03b1g\u2225\u2225\u2225 L2\n\u22642 ( sup\nt\u2208[0,u] t\u03b1pm (t) + max {\u03b1, 1}Z\u03b1 (\u03bb) sup t\u2208[0,u] pm (t) ) \u22642 (u\u03b1 +max {\u03b1, 1}Z\u03b1 (\u03bb)) .\nEstimation of Term-3:\u2225\u2225\u2225F\u22a5u (qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g)\u2225\u2225\u2225 L2\n= \u2225\u2225\u2225\u2225F\u22a5u (\u039b\u0302n)\u22121 \u039b\u0302n (qm (\u039b\u0302n)T 12 R\u0302\u2212 \u039b\u03b1g)\u2225\u2225\u2225\u2225 L2\n\u2264 \u2225\u2225\u2225\u2225F\u22a5u (\u039b\u0302n)\u22121 (\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302+ T 12 R\u0302\u2212 \u039b\u0302n\u039b\u03b1g)\u2225\u2225\u2225\u2225\nL2\n40 GuptaN., Sivananthan S., SriperumbudurB.K. \u2264 \u2225\u2225\u2225\u2225F\u22a5u (\u039b\u0302n)\u22121 (\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302)\u2225\u2225\u2225\u2225\nL2 + \u2225\u2225\u2225\u2225F\u22a5u (\u039b\u0302n)\u22121 (T 12 R\u0302\u2212 \u039b\u0302n\u039b\u03b1g)\u2225\u2225\u2225\u2225 L2\n\u22641 u \u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 + \u2225\u2225\u2225\u2225F\u22a5u (\u039b\u0302n)\u22121 (\u039b\u0302n + \u03bbI) 12 (\u039b\u0302n + \u03bbI)\u2212 12 \u00d7 (\u039b + \u03bbI) 1 2 (\u039b + \u03bbI)\u2212 1 2 ( T 1 2 R\u0302\u2212 \u039b\u0302n\u039b\u03b1g )\u2225\u2225\u2225 L2\n\u22641 u \u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 + (u+ \u03bb) 1 2 u \u221a 2\u03c32N (\u03bb) n\u03b4 .\nThe last inequality follows from Lemmas 4.1, 4.2. So by combining all three terms, we get\u2225\u2225\u2225\u03b2\u0302m \u2212 \u03b2\u2217\u2225\u2225\u2225 L2 \u2272 \u221a \u03c32N (\u03bb) n\u03b4 (u\u0303+ \u03bb) 1 2 u\u0303 + (u\u03b1 +max {\u03b1, 1}Z\u03b1 (\u03bb))\n+ 1\nu \u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 ,\nwhere u\u0303 = min { u, \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u22121}. Now we de ne our stopping rule as m\u2217 = inf { m \u2265 0 : \u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 \u2264 \u2126 } ,\nThen \u2225\u2225\u2225\u03b2\u0302m\u2217 \u2212 \u03b2\u2217\u2225\u2225\u2225 L2 \u2272 \u221a \u03c32N (\u03bb) n\u03b4 (u\u0303+ \u03bb) 1 2 u\u0303 + (u\u03b1 +max {\u03b1, 1}Z\u03b1 (\u03bb)) + 1 u \u2126. (3.1)\nWe still have to bound \u2223\u2223\u2223p\u2032m\u2217 (0)\u2223\u2223\u2223 and for that we will use Lemma 4.5. First we will bound \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223. We assume that 0 < u < x1,m\u22121 \u2264 x(2)1,m\u22121 (see Lemma 2.1). Consider\n[pm\u22121, pm\u22121] 1 2\n(0) = \u2225\u2225\u2225pm\u22121 (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225\nL2 \u2264 \u2225\u2225\u2225p(2)m\u22121 (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225\nL2\n( As pm minimizes (2.3) over p \u2208 P0m ) \u2264 \u2225\u2225\u2225Fup(2)m\u22121 (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225 L2 + \u2225\u2225\u2225F\u22a5u p(2)m\u22121 (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225 L2\n\u2264 \u2225\u2225\u2225FuT 12 R\u0302\u2225\u2225\u2225\nL2 + u\u2212 1 2 \u2225\u2225\u2225\u2225p(2)m\u22121 (\u039b\u0302n) \u039b\u0302 12nT 12 R\u0302\u2225\u2225\u2225\u2225 L2\n\u2264 \u2225\u2225\u2225Fu (T 12 R\u0302\u2212 T 12 C\u0302n\u03b2\u2217)\u2225\u2225\u2225 L2 + \u2225\u2225\u2225FuT 12 C\u0302n\u03b2\u2217\u2225\u2225\u2225 L2 + u\u2212 1 2 \u2225\u2225\u2225\u2225p(2)m\u22121 (\u039b\u0302n) \u039b\u0302 12nT 12 R\u0302\u2225\u2225\u2225\u2225 L2\n\u2264 \u2225\u2225\u2225\u2225Fu (\u039b\u0302n + \u03bbI) 12\u2225\u2225\u2225\u2225 \u2225\u2225\u2225\u2225(\u039b\u0302n + \u03bbI)\u2212 12 (\u039b + \u03bbI) 12\u2225\u2225\u2225\u2225 \u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 (T 12 R\u0302\u2212 T 12 C\u0302n\u03b2\u2217)\u2225\u2225\u2225L2 + \u2225\u2225\u2225FuT 12 C\u0302n\u03b2\u2217\u2225\u2225\u2225\nL2 + u\u2212 1 2 \u2225\u2225\u2225\u2225p(2)m\u22121 (\u039b\u0302n) \u039b\u0302 12nT 12 R\u0302\u2225\u2225\u2225\u2225 L2 .\nFor the third last inequality, we have used the fact that \u2223\u2223\u2223p(2)m\u22121 (x)\u2223\u2223\u2223 \u2264 1 for all x \u2208 [0, x(2)1,m\u22121] as p (2) m\u22121 (0) = 1 and p (2) m\u22121 is non-increasing on [ 0, x (2) 1,m\u22121 ] .\nConvergence analysis of kernel conjugate gradient for functional linear regression 41\nFrom Lemma 4.1, Lemma 4.2 and Lemma 4.4, we get\n[pm\u22121, pm\u22121] 1 2\n(0)\n=\n\u221a 2\u03c32N (\u03bb)\nn\u03b4\n\u2225\u2225\u2225\u2225Fu (\u039b\u0302n + \u03bbI) 12\u2225\u2225\u2225\u2225 op + \u2225\u2225\u2225Fu\u039b\u0302n\u039b\u03b1g\u2225\u2225\u2225 L2 + u\u2212 1 2 \u2225\u2225\u2225\u2225p(2)m\u22121 (\u039b\u0302n) \u039b\u0302 12nT 12 R\u0302\u2225\u2225\u2225\u2225 L2\n\u2264 \u221a\n2\u03c32N (\u03bb) n\u03b4 (u+ \u03bb) 1 2 + 2c (\u03b1)u (u\u03b1 + Z\u03b1 (\u03bb)) \u2016g\u2016L2 + u \u2212 1 2 \u2225\u2225\u2225\u2225p(2)m\u22121 (\u039b\u0302n) \u039b\u0302 12nT 12 R\u0302\u2225\u2225\u2225\u2225 L2\n=\n\u221a 2\u03c32N (\u03bb)\nn\u03b4 (u+ \u03bb)\n1 2 + 2c (\u03b1)u (u\u03b1 + Z\u03b1 (\u03bb)) \u2016g\u2016L2 + u \u2212 1 2 [ p (2) m\u22121, p (2) m\u22121 ] 1 2\n(1) ,\nwhere c (\u03b1) is a constant depending on \u03b1. Here we have used that \u2223\u2223\u2223p(2)m\u22121 (x)\u2223\u2223\u2223 \u2264 1 for x \u2208 [0, x(2)1,m\u22121]. Using Assumption 3 and with the choice of \u03bb = c (\u03b1, \u03b4)n\u2212 1 1+s+2\u03b1 , we get that \u221a \u03c32N (\u03bb)\nn\u03b4 \u2272 \u03bb \u03b1+ 1 2 and\nZ\u03b1 (\u03bb) \u2264 \u03bb\u03b1. From Lemma 4.5 and the stopping rule \u2225\u2225\u2225\u039b\u0302nqm\u2217\u22121 (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 > \u2126, we get\n(2 + \u03c4)\u03bb 1 2\u03bb 1 2 +\u03b1\n<\n\u221a 2\u03c32N (\u03bb)\nn\u03b4 (\u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212 12 + \u03bb 12) + 2\n(\u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212(\u03b1+1) + c (\u03b1)Z\u03b1 (\u03bb) \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u22121) \u2016g\u2016L2 \u2264 \u221a 2\u03bb 1 2 +\u03b1\n(\u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212 12 + \u03bb 12) + 2\n(\u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212(\u03b1+1) + c (\u03b1)Z\u03b1 (\u03bb) \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u22121) \u2016g\u2016L2 . Therefore, we have\n\u03c4\u03bb 1 2\u03bb 1 2 +\u03b1 \u2264 c (\u03b1)max { \u03bb 1 2 +\u03b1 \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212 12 , \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212(\u03b1+1) \u2016g\u2016L2 ,\nZ\u03b1 (\u03bb) \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u22121 \u2016g\u2016L2} .\nFollowing the steps from [4], we see that if the rst term attains the maximum\n\u03c4\u03bb\u03b1+1 \u2264 c (\u03b1)\u03bb\u03b1+ 1 2 \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212 12 =\u21d2 \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223 \u2264 c (\u03b1, \u03c4)\u03bb\u22121, if the second term attains the maximum\n\u03c4\u03bb\u03b1+1 \u2264 c (\u03b1) \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u2212(\u03b1+1) \u2016g\u2016L2 =\u21d2 \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223 \u2264 c\u2032 (\u03b1, \u03c4)\u03bb\u22121,\nif the third term attains the maximum \u03c4\u03bb\u03b1+1 \u2264 c (\u03b1)Z\u03b1 (\u03bb) \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223\u22121 \u2016g\u2016L2 =\u21d2 \u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223 \u2264 c\u2032\u2032 (\u03b1, \u03c4)\u03bb\u22121,\nas Z\u03b1 (\u03bb) \u2272 \u03bb\u03b1.\n42 GuptaN., Sivananthan S., SriperumbudurB.K.\nFrom all three cases, we will get that\u2223\u2223\u2223p\u2032m\u2217\u22121 (0)\u2223\u2223\u2223 \u2264 c2 (\u03b1, \u03c4)\u03bb\u22121, for some constant c2 (\u03b1, \u03c4) > 0.\nIn the next step we get an bound on \u2223\u2223\u2223p\u2032m\u2217 (0)\u2223\u2223\u2223. From Lemma 2.1, we know that\n\u2223\u2223\u2223p\u2032m\u22121 (0)\u2212 p\u2032m (0)\u2223\u2223\u2223 \u2264 [pm\u22121, pm\u22121](0)[ p (2) m\u22121, p (2) m\u22121 ] (1) .\nDe ne u := a (\u03b1, \u03c4)\u03bb, where a (\u03b1, \u03c4) is a constant. Using this choice we get\u221a 2\u03c32N (\u03bb)\nn\u03b4 (u+ \u03bb)\n1 2 + 2c (\u03b1)u (u\u03b1 + Z\u03b1 (\u03bb)) \u2016g\u2016L2 \u2264 c3 (\u03b1, \u03c4)\u03bb 1 2\u03bb\u03b1+ 1 2 .\nFrom the stopping rule, we know that\u2225\u2225\u2225\u039b\u0302nqm\u2217\u22121 (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 > \u2126 = (2 + \u03c4)\u03bb 1 2\u03bb\u03b1+ 1 2 .\nSo now we can see that using all these inequalities we have that\u2223\u2223\u2223p\u2032m\u2217 (0)\u2223\u2223\u2223 \u2264 c4 (\u03b1, \u03c4)\u03bb\u22121. (3.2) Using (3.2), the choice of a (\u03b1, \u03c4) can be made accordingly such that\nu \u2264 \u2223\u2223\u2223p\u2032m\u2217 (0)\u2223\u2223\u2223\u22121 \u2264 x1,m.\nWith this inequality, we can choose u\u0303 = u. Now with this choice of u\u0303 = a (\u03b1, \u03c4)\u03bb, where a (\u03b1, \u03c4) has been taken to satisfy all the conditions on u\u0303, we will further bound (3.1). Therefore, we get that \u2225\u2225\u2225\u03b2\u0302m\u2217 \u2212 \u03b2\u2217\u2225\u2225\u2225 L2 \u2272 \u221a 2\u03c32N (\u03bb) n\u03b4 u\u0303\u22121 (\u03bb+ u\u0303) 1 2 + 2 (u\u03b1 +max {\u03b1, 1}Z\u03b1 (\u03bb))\n+ 1\nu \u2225\u2225\u2225\u039b\u0302nqm\u2217 (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 \u2264 \u03bb\u03b1.\nNow by using \u03bb = c (\u03b1, \u03b4)n\u2212 \u03b1+1\n1+s+2\u03b1 , the result follows. \u25a1 In the RKHS framework, the rst minimax convergence rate was established by Cai and Yuan [6, 26] for the Tikhonov regularization method with the source condition \u03b2\u2217 \u2208 H. Later in [27], the results were extended, and the minimax convergence rates derived with the source condition\n\u03b2\u2217 \u2208 R ( T 1 2 ( T 1 2CT 1 2 )\u03b1) for 0 < \u03b1 \u2264 12 . Our convergence rates for the conjugate gradient method match the minimax rates of [27] and also match with the convergence of rates of [13].\n4 Supplementary Results\nTechnical details in the paper depend on the estimation of the residual term as the CG method works to reduce the residual term as the number of iterations progresses. To simplify the technical part we use the fact that \u2225\u2225\u2225J\u2217C\u0302nJ\u03b2\u0302m \u2212 J\u2217R\u0302\u2225\u2225\u2225 H = \u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 . We introduce several lemmas that aid us to estimate the residual term and to prove the main theorem.\nConvergence analysis of kernel conjugate gradient for functional linear regression 43\nLemma 4.1. Under Assumptions 3.2 and 3.3 we get\u2225\u2225\u2225(\u039b\u0302n \u2212 \u039b) (\u039b + \u03bbI)\u22121\u2225\u2225\u2225 op \u2264 c1 ( n\u03bb1+s )\u2212 1 2\nfor some constant c1 > 0. We skip the proof of this lemma as it follows from the similar steps of Lemma 2 in [6].\nFrom Lemma 4.1 and for \u03bb \u2265 ( 4c21 n ) 1 s+1\n, we get\u2225\u2225\u2225(\u039b\u0302n \u2212 \u039b) (\u039b + \u03bbI)\u22121\u2225\u2225\u2225 op \u2264 1 2 .\nAs a consequence, we get\u2225\u2225\u2225\u2225(\u039b + \u03bbI)(\u039b\u0302n + \u03bbI)\u22121\u2225\u2225\u2225\u2225 op = \u2225\u2225\u2225\u2225[(\u039b\u0302n \u2212 \u039b) (\u039b + \u03bbI)\u22121 + I]\u22121\u2225\u2225\u2225\u2225 op\n\u2264 1 1\u2212 \u2225\u2225\u2225(\u039b\u0302n \u2212 \u039b) (\u039b + \u03bbI)\u22121\u2225\u2225\u2225\nop \u2264 2, \u2200\u03bb \u2265 ( 4c21 n ) 1 s+1 .\nUsing Corde's inequality, ( \u2016A\u03bdB\u03bd\u2016op \u2264 \u2016AB\u2016 \u03bd op , 0 \u2264 \u03bd \u2264 1 ) , where A and B are self-adjoint positive operators, we get\u2225\u2225\u2225\u2225(\u039b + \u03bbI)\u03bd (\u039b\u0302n + \u03bbI)\u2212\u03bd\u2225\u2225\u2225\u2225 op \u2264 2\u03bd , \u2200\u03bd \u2208 S, \u03bb \u2265 ( 4c21 n ) 1 s+1 . (4.1)\nWe will follow the similar ideas from [2,25] to prove the following lemma. Lemma 4.2. For \u03b4 > 0, with at least probability 1\u2212 \u03b4, we have that\u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12 (R\u0302\u2212 C\u0302n\u03b2\u2217)\u2225\u2225\u2225 L2 \u2264 \u221a \u03c32N (\u03bb) n\u03b4 .\nProof. De ne Zi := (\u039b + \u03bbI) \u2212 1 2 T 1 2 [YiXi \u2212 (Xi \u2297Xi)\u03b2\u2217]. Since the slope function \u03b2\u2217 satis es\nthe operator equation C\u03b2\u2217 = E [Y X], we get that the mean of random variable Zi is zero, i.e.,\nE [Zi] = (\u039b + \u03bbI)\u2212 1 2 T 1 2 [E [Y X]\u2212 C\u03b2\u2217] = 0.\nBy Markov's inequality, for any t > 0\nP (\u2225\u2225\u2225\u2225\u2225 1n n\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225 L2 \u2265 t ) \u2264 E \u2225\u2225 1 n \u2211n i=1 Zi \u2225\u22252 L2 t2 .\nNote that\nE \u2225\u2225\u2225\u2225\u2225 1n n\u2211\ni=1\nZi \u2225\u2225\u2225\u2225\u2225 2\nL2\n= 1\nn2 n\u2211 i,j=1 E \u3008Zi, Zj\u3009L2 = 1 n2 n\u2211 i \u0338=j E \u3008Zi, Zj\u3009L2 + 1 n2 n\u2211 i=1 E \u2016Zi\u20162L2 = E \u2016Z1\u20162L2 n\nand by taking t =\n\u221a E\u2225Z1\u22252L2\nn\u03b4 , with at least probability 1\u2212 \u03b4, we have\n\u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12 (R\u0302\u2212 C\u0302n\u03b2\u2217)\u2225\u2225\u2225 L2 \u2264\n\u221a\u221a\u221a\u221a\u221aE [\u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12 (Y X \u2212 (X \u2297X)\u03b2\u2217)\u2225\u2225\u22252\nL2 ] n\u03b4 . (4.2)\n44 GuptaN., Sivananthan S., SriperumbudurB.K.\nConsider\nE [\u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12 (Y X \u2212 (X \u2297X)\u03b2\u2217)\u2225\u2225\u22252\nL2 ] =E [\u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12 (Y \u2212 \u3008X,\u03b2\u2217\u3009L2)X\u2225\u2225\u22252 L2\n] =E [ (Y \u2212 \u3008X,\u03b2\u2217\u3009L2) 2 \u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 T 12X\u2225\u2225\u22252\nL2 ] =E [ \u03f52 \u2329 (\u039b + \u03bbI)\u2212 1 2 T 1 2X, (\u039b + \u03bbI)\u2212 1 2 T 1 2X \u232a L2\n] =E [ \u03f52trace ( (\u039b + \u03bbI)\u22121 T 1 2 (X \u2297X)T 1 2\n)] =E [ \u03f52 ] trace ( (\u039b + \u03bbI)\u22121 T 1 2CT 1 2\n) =\u03c32trace ( (\u039b + \u03bbI)\u22121 \u039b ) = \u03c32N (\u03bb) .\nWith this bound and (4.2), the result follows. \u25a1 Lemma 4.3. Assuming E \u2016X\u20164 < \u221e, with at least probability 1\u2212 \u03b4, we get\u2225\u2225\u2225C\u0302n \u2212 C\u2225\u2225\u2225\nHS \u2264\n\u221a E \u2016X\u20164\nn\u03b4 := \u2206.\nProof. From Chebyshev's inequality, we have that\nP (\u2225\u2225\u2225C\u0302n \u2212 C\u2225\u2225\u2225 HS > \u03be ) \u2264\nE \u2225\u2225\u2225C\u0302n \u2212 C\u2225\u2225\u22252\nHS\n\u03be2 .\nUsing Theorem 2.5 [16], we get P (\u2225\u2225\u2225C\u0302n \u2212 C\u2225\u2225\u2225 HS > \u03be ) \u2264 E \u2016X\u2016 4 n\u03be2 .\nTaking \u03be =\n\u221a E\u2225X\u22254\nn\u03b4 will conclude the result. \u25a1 The bound in the Hilbert-Schmidt is stronger than the operator norm. So the above lemma provides a stronger estimation compared to the estimation in terms of operator norm. The next lemma explains a technical bound involving operator \u039b and \u039b\u0302n which will be used repeatedly in our analysis.\nLemma 4.4. For any \u03bd > 0, and measurable \u03c6 : [0, \u03ba\u039b] \u2192 R, it holds with probability greater than 1\u2212 e\u03be that \u2225\u2225\u2225\u03c6(\u039b\u0302n)\u039b\u03bd\u2225\u2225\u2225\nop \u2272 sup t\u2208[0,\u03ba\u039b] t\u03bd\u03c6 (t) + max {\u03bd, 1}Z\u03bd (\u03bb) sup t\u2208[0,\u03ba\u039b] \u03c6 (t) ,\nwhere\nZ\u03bd (\u03bb) = \u03bb \u03bd , if \u03bd \u2264 1\n\u03ba\u03bd\u039b\u2206, if \u03bd > 1 .\nProof. Proof of this result follows the similar steps of Lemma 5.3 [4]. For \u03bd \u2264 1, we have\u2225\u2225\u2225\u03c6(\u039b\u0302n)\u039b\u03bd\u2225\u2225\u2225 op \u2264 \u2225\u2225\u2225\u03c6(\u039b\u0302n)(\u039b\u0302n + \u03bbI)\u03bd\u2225\u2225\u2225 op \u2225\u2225\u2225\u2225(\u039b\u0302n + \u03bbI)\u2212\u03bd (\u039b + \u03bbI)\u03bd\u2225\u2225\u2225\u2225 op \u2225\u2225(\u039b + \u03bbI)\u2212\u03bd \u039b\u03bd\u2225\u2225 op\n\u2272 (\nsup t\u2208[0,\u03ba\u039b] t\u03bd\u03c6 (t) + \u03bb\u03bd sup t\u2208[0,\u03ba\u039b] \u03c6 (t)\n) .\nConvergence analysis of kernel conjugate gradient for functional linear regression 45 For last inequality, we used (4.1) and the fact that \u2225\u2225(\u039b + \u03bbI)\u2212\u03bd \u039b\u03bd\u2225\u2225\nop \u2264 1. For \u03bd > 1,\u2225\u2225\u2225\u03c6(\u039b\u0302n)\u039b\u03bd\u2225\u2225\u2225 op \u2264 \u2225\u2225\u2225\u03c6(\u039b\u0302n)\u2225\u2225\u2225 op \u2225\u2225\u2225(\u039b\u03bd \u2212 \u039b\u0302\u03bdn)\u2225\u2225\u2225 op + \u2225\u2225\u2225\u03c6(\u039b\u0302n) \u039b\u0302\u03bdn\u2225\u2225\u2225 op\n\u2264 \u2225\u2225\u2225(\u039b\u03bd \u2212 \u039b\u0302\u03bdn)\u2225\u2225\u2225\nop sup t\u2208[0,\u03ba\u039b] \u03c6 (t) + sup t\u2208[0,\u03ba\u039b] t\u03bd\u03c6 (t)\n\u2272 \u2225\u2225\u2225\u039b\u2212 \u039b\u0302n\u2225\u2225\u2225\nHS sup t\u2208[0,\u03ba\u039b] \u03c6 (t) + sup t\u2208[0,\u03ba\u039b] t\u03bd\u03c6 (t) .\nHere we used that for \u03bd > 1, x\u03bd is \u03bd\u03ba\u03bd\u22121\u039b \u2212 Lipschitz over [0, \u03ba\u039b] and by Lemma 4.3 we get our result. \u25a1\nIn the following lemma, we will discuss the bound of the residual term that will be used later to bound \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223.\nLemma 4.5. Under Assumptions 3.1 3.3, E \u2016X\u20164 < \u221e and \u03bb \u2265 ( 4c21 n ) 1 s+1\n, we have that\u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225 L2 \u2264 \u221a 2\u03c32N (\u03bb) n\u03b4 (\u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u2212 12 + \u03bb 12) + 2\n(\u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u2212(\u03b1+1) + c (\u03b1)Z\u03b1 (\u03bb) \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u22121) \u2016g\u2016L2 . Proof. For bounding the residual term, we use Lemma 2.1 at the initial stage to conclude that\u2225\u2225\u2225\u039b\u0302nqm (\u039b\u0302n)T 12 R\u0302\u2212 T 12 R\u0302\u2225\u2225\u2225\nL2 = \u2225\u2225\u2225pm (\u039b\u0302n)T 12 R\u0302\u2225\u2225\u2225\nL2 \u2264 \u2225\u2225\u2225Fx1,m (\u03c6m (\u039b\u0302n)T 12 R\u0302)\u2225\u2225\u2225\nL2 = \u2225\u2225\u2225Fx1,m (\u03c6m (\u039b\u0302n)(T 12 R\u0302\u2212 T 12 C\u0302n\u03b2\u2217 + T 12 C\u0302n\u03b2\u2217))\u2225\u2225\u2225\nL2 \u2264 \u2225\u2225\u2225Fx1,m (\u03c6m (\u039b\u0302n)T 12 (R\u0302\u2212 C\u0302n\u03b2\u2217))\u2225\u2225\u2225\nL2\ufe38 \ufe37\ufe37 \ufe38 Term\u2212A\n+ \u2225\u2225\u2225Fx1,m (\u03c6m (\u039b\u0302n)T 12 C\u0302n\u03b2\u2217)\u2225\u2225\u2225\nL2\ufe38 \ufe37\ufe37 \ufe38 Term\u2212B .\nWe will take help from Lemma 4.1 and Lemma 4.2 for the estimation of both terms. Estimation of Term-A:\u2225\u2225\u2225Fx1,m (\u03c6m (\u039b\u0302n)T 12 (R\u0302\u2212 C\u0302n\u03b2\u2217))\u2225\u2225\u2225\nL2 \u2264 \u2225\u2225\u2225\u2225Fx1,m\u03c6m (\u039b\u0302n)(\u039b\u0302n + \u03bbI) 12\u2225\u2225\u2225\u2225\nop \u00d7 \u2225\u2225\u2225\u2225(\u039b\u0302n + \u03bbI)\u2212 12 (\u039b + \u03bbI) 12\u2225\u2225\u2225\u2225\nop\n\u2225\u2225\u2225(\u039b + \u03bbI)\u2212 12 (T 12 R\u0302\u2212 T 12 C\u0302n\u03b2\u2217)\u2225\u2225\u2225 L2\n(4.3)\n\u2264 \u221a\n2\u03c32N (\u03bb) n\u03b4 \u2225\u2225\u2225\u2225Fx1,m\u03c6m (\u039b\u0302n)(\u039b\u0302n + \u03bbI) 12\u2225\u2225\u2225\u2225 op\n\u2264 \u221a\n2\u03c32N (\u03bb) n\u03b4\n( sup\nx\u2208[0,x1,m] x\n1 2\u03c6m (x) + \u03bb 1 2 sup x\u2208[0,x1,m] \u03c6m (x)\n)\n\u2264 \u221a\n2\u03c32N (\u03bb) n\u03b4\n(\u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u2212 12 + \u03bb 12) .\n46 GuptaN., Sivananthan S., SriperumbudurB.K.\nFor the last inequality, we use (2.4) for \u03bd = 0, 1. Estimation of Term-B:\u2225\u2225\u2225Fx1,m (\u03c6m (\u039b\u0302n)T 12 C\u0302n\u03b2\u2217)\u2225\u2225\u2225\nL2 = \u2225\u2225\u2225Fx1,m\u03c6m (\u039b\u0302n) \u039b\u0302n\u039b\u03b1\u2225\u2225\u2225\nop \u2016g\u2016L2\n\u22642 ( sup\nt\u2208[0,x1,m] t\u03b1+1\u03c6m (t) + c (\u03b1)Z\u03b1 (\u03bb) sup t\u2208[0,x1,m] t\u03c6m (t)\n) \u2016g\u2016L2\n\u22642 (\u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u2212(\u03b1+1) + c (\u03b1)Z\u03b1 (\u03bb) \u2223\u2223\u2223p\u2032m (0)\u2223\u2223\u2223\u22121) \u2016g\u2016L2 .\n(4.4)\nwhere we used Lemma 4.4 and (2.4) in the last inequality. From (4.3) and (4.4), we obtain the result. \u25a1\nAcknowledgments\nThe authors thank the referee for his valuable comments and suggestions. S. Sivananthan acknowledges the Science and Engineering Research Board, Government of India, for the nancial support through project no. MTR/2022/000383. BKS is partially supported by the National Science Foundation DMS CAREER Award 1945396.\nReferences\n1. Aronszajn,N.: Theory of reproducing kernels. Trans. Amer. Math. Soc. 68, 337-404 (1950)\n2. Balasubramanian,K., M uller, H.-G., Sriperumbudur, B.K.: Uni ed RKHS methodology and analysis for functional linear and single-index models, arXiv preprint. (2022). arXiv: 2206.03975\n3. Bauer, F., Pereverzev, S., Rosasco, L.: On regularization algorithms in learning theory. J. Complexity. 23 (1), 52-72 (2007)\n4. Blanchard,G., Kr amer,N.: Convergence rates of kernel conjugate gradient for random design regression. Anal. Appl. Singap. 14 (6), 763-794 (2016)\n5. Cai, T.T., Hall, P.: Prediction in functional linear regression. Ann. Statist. 34 (5), 2159-2179 (2006)\n6. Cai, T.T., Yuan,M.: Minimax and adaptive prediction for functional linear regression. J.Amer. Statist. Assoc. 107 (499), 1201-1216 (2012)\n7. Cardot,H., Ferraty, F., Sarda, P.: Spline estimators for the functional linear model. Statist. Sinica. 13 (3), 571-591 (2003)\n8. Chen,X., Tang, B., Fan, J., Guo,X.: Online gradient descent algorithms for functional data learning. J. Complexity. 70, Paper No 101635 (14) (2022)\n9. Cucker, F., Smale, S.: On the mathematical foundations of learning. Bull. Amer. Math. Soc. N.S. 39 (1), 1-49 (2002)\n10. Cucker, F., Zhou,D.-X.: Learning theory: an approximation theory viewpoint, vol. 24. Cambridge University Press, Cambridge, (2007)\n11. Ferraty, F., Vieu, P.: Nonparametric Functional Data Analysis. Theory and practice. Springer Series in Statistics. Springer, New York (2006)\n12. Forni,M., Reichlin, L.: Let's get real: A factor analytical approach to disaggregated business cycle dynamics. The Review of Economic Studies. 65 (3), 453-473 (1998)\n13. Guo,X., Guo, Z.-C., Shi, L.: Capacity dependent analysis for functional online learning algorithms. Appl. Comput. Harmon. Anal. 67, Paper No 101567 (30) (2023)\nConvergence analysis of kernel conjugate gradient for functional linear regression 47\n14. Hall, P., Horowitz, J.L.: Methodology and convergence rates for functional linear regression. Ann. Statist. 35 (1), 70-91 (2007)\n15. Hanke,M.: Conjugate Gradient Type Methods for Ill-Posed Problems, vol. 327 of Pitman Research Notes in Mathematics Series. Longman Scienti c & Technical, Harlow, (1995)\n16. Horv ath, L., Kokoszka, P.: Inference for functional data with applications. Springer Series in Statistics. Springer, New York (2012)\n17. Li, Y., Hsing, T.: On rates of convergence in functional linear regression. J.Multivariate Anal. 98 (9), 1782-1804 (2007)\n18. Lin, J., Cevher, V.: Kernel conjugate gradient methods with random projections. Appl. Comput. Harmon. Anal. 55, 223-269 (2021)\n19. M uller, H.-G., Stadtm uller, U.: Generalized functional linear models. Ann. Statist. 33 (2), 774- 805 (2005)\n20. Pereverzyev, S.: An introduction to arti cial intelligence based on reproducing kernel Hilbert spaces. Compact Textbooks in Mathematics, Birkh auser (2022)\n21. Preda,C., Saporta,G.: Clusterwise PLS regression on a stochastic process. Comput. Statist. Data Anal. 49 (1), 99-108 (2005)\n22. Ramsay, J.O., Dalzell, C.J.: Some tools for functional data analysis. J. Roy. Statist. Soc. Ser. B 53 (3), 539-572 (1991)\n23. Ramsay, J.O., Silverman,B.W.: Functional Data Analysis, second ed. Springer Series in Statistics. Springer, New York (2005)\n24. Tong,H.: Functional linear regression with Huber loss. J. Complexity 74, Paper No 101696 (14) (2023)\n25. Tong,H., Ng,M.: Analysis of regularized least squares for functional linear regression model. J. Complexity 49, 85-94 (2018)\n26. Yuan,M., Cai, T.T.: A reproducing kernel Hilbert space approach to functional linear regression. Ann. Statist. 38 (6), 3412-3444 (2010)\n27. Zhang, F., Zhang,W., Li, R., Lian,H.: Faster convergence rate for functional linear regression in reproducing kernel Hilbert spaces. Statistics. 54 (1), 167-181 (2020)\nReceived 04.10.2023 Revised 22.10.2023"
        }
    ],
    "title": "Convergence analysis of kernel conjugate gradient for functional linear regression",
    "year": 2023
}