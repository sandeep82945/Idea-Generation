{
    "abstractText": "The inevitable leakage of privacy as a result of unrestrained disclosure of personal information has motivated extensive research on robust privacy-preserving mechanisms. However, existing research is mostly limited to solving the problem in a static setting with disregard for the privacy leakage over time. Unfortunately, this treatment of privacy is insufficient in practical settings where users continuously disclose their personal information over time resulting in an accumulated leakage of the users\u2019 sensitive information. In this paper, we consider privacy leakage over a finite time horizon and investigate optimal strategies to maximize the utility of the disclosed data while limiting the finite-horizon privacy leakage. We consider a simple privacy mechanism that involves compressing the user\u2019s data before each disclosure to meet the desired constraint on future privacy. We further motivate several algorithms to optimize the dynamic privacy-utility tradeoff and evaluate their performance via extensive synthetic performance tests.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chandra Sharma"
        },
        {
            "affiliations": [],
            "name": "George Amariucai"
        },
        {
            "affiliations": [],
            "name": "Shuangqing Wei"
        }
    ],
    "id": "SP:780e2f481c4fed7ad77765b618fc0d9f682134cb",
    "references": [
        {
            "authors": [
                "C. Dwork",
                "K. Kenthapadi",
                "F. McSherry",
                "I. Mironov",
                "M. Naor"
            ],
            "title": "Our data, ourselves: Privacy via distributed noise generation",
            "venue": "Annual International Conference on the Theory and Applications of Cryptographic Techniques, Springer, 2006, pp. 486\u2013503.",
            "year": 2006
        },
        {
            "authors": [
                "L. Sankar",
                "S.R. Rajagopalan",
                "H.V. Poor"
            ],
            "title": "Utilitear-privacy tradeoffs in databases: An information-theoretic approach",
            "venue": "IEEE Transactions on Information Forensics and Security, vol. 8, no. 6, pp. 838\u2013852, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "X. He",
                "X. Zhang",
                "C.-C.J. Kuo"
            ],
            "title": "A distortion-based approach to privacypreserving metering in smart grids",
            "venue": "IEEE Access, vol. 1, pp. 67\u201378, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "Q. Geng",
                "P. Viswanath"
            ],
            "title": "The optimal noise-adding mechanism in differential privacy",
            "venue": "IEEE Transactions on Information Theory, vol. 62, no. 2, pp. 925\u2013951, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "W. Wang",
                "L. Ying",
                "J. Zhang"
            ],
            "title": "On the relation between identifiability, differential privacy, and mutual-information privacy",
            "venue": "IEEE Transactions on Information Theory, vol. 62, no. 9, pp. 5018\u20135029, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "K. Kalantari",
                "L. Sankar",
                "O. Kosut"
            ],
            "title": "On information-theoretic privacy with general distortion cost functions",
            "venue": "2017 IEEE International Symposium on Information Theory (ISIT), IEEE, 2017, pp. 2865\u20132869.",
            "year": 2017
        },
        {
            "authors": [
                "J. Liao",
                "O. Kosut",
                "L. Sankar",
                "F.P. Calmon"
            ],
            "title": "Privacy under hard distortion constraints",
            "venue": "2018 IEEE Information Theory Workshop (ITW), IEEE, 2018, pp. 1\u20135.",
            "year": 2018
        },
        {
            "authors": [
                "K. Kalantari",
                "L. Sankar",
                "A.D. Sarwate"
            ],
            "title": "Robust privacy-utility tradeoffs under differential privacy and hamming distortion",
            "venue": "IEEE Transactions on Information Forensics and Security, vol. 13, no. 11, pp. 2816\u20132830, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Diamantaras",
                "S.-Y. Kung"
            ],
            "title": "Data privacy protection by kernel subspace projection and generalized eigenvalue decomposition",
            "venue": "2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), IEEE, 2016, pp. 1\u20136.",
            "year": 2016
        },
        {
            "authors": [
                "S.-Y. Kung"
            ],
            "title": "Compressive privacy: From information\\/estimation theory to machine learning [lecture notes",
            "venue": "IEEE Signal Processing Magazine, vol. 34, no. 1, pp. 94\u2013112, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.-Y. Kung",
                "T. Chanyaswad",
                "J.M. Chang",
                "P. Wu"
            ],
            "title": "Collaborative pca/dca learning methods for compressive privacy",
            "venue": "ACM Transactions on Embedded Computing Systems (TECS), vol. 16, no. 3, pp. 1\u201318, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S.-Y. Kung"
            ],
            "title": "A compressive privacy approach to generalized information bottleneck and privacy funnel problems",
            "venue": "Journal of the Franklin Institute, vol. 355, no. 4, pp. 1846\u20131872, 2018. 15",
            "year": 1846
        },
        {
            "authors": [
                "Y. Song",
                "C.X. Wang",
                "W.P. Tay"
            ],
            "title": "Compressive privacy for a linear dynamical system",
            "venue": "IEEE Transactions on Information Forensics and Security, vol. 15, pp. 895\u2013910, 2020. doi: 10.1109/TIFS.2019.2930366.",
            "year": 2020
        },
        {
            "authors": [
                "M.S. Alvim",
                "M.E. Andr\u00e9s",
                "K. Chatzikokolakis",
                "P. Degano",
                "C. Palamidessi"
            ],
            "title": "Differential privacy: On the trade-off between utility and information leakage",
            "venue": "International Workshop on Formal Aspects in Security and Trust, Springer, 2011, pp. 39\u201354.",
            "year": 2011
        },
        {
            "authors": [
                "C. Yin",
                "J. Xi",
                "R. Sun",
                "J. Wang"
            ],
            "title": "Location privacy protection based on differential privacy strategy for big data in industrial internet of things",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 14, no. 8, pp. 3628\u20133636, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yang",
                "Z. Zhang",
                "G. Miklau",
                "M. Winslett",
                "X. Xiao"
            ],
            "title": "Differential privacy in data publication and analysis",
            "venue": "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, ACM, 2012, pp. 601\u2013 606.",
            "year": 2012
        },
        {
            "authors": [
                "T. Zhu",
                "P. Xiong",
                "G. Li",
                "W. Zhou"
            ],
            "title": "Correlated differential privacy: Hiding information in non-iid data set",
            "venue": "IEEE Transactions on Information Forensics and Security, vol. 10, no. 2, pp. 229\u2013242, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "X. Wu",
                "T. Wu",
                "M. Khan",
                "Q. Ni",
                "W. Dou"
            ],
            "title": "Game theory based correlated privacy preserving analysis in big data",
            "venue": "IEEE Transactions on Big Data, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. He",
                "A. Machanavajjhala",
                "B. Ding"
            ],
            "title": "Blowfish privacy: Tuning privacy-utility tradeoffs using policies",
            "venue": "Proceedings of the 2014 ACM SIGMOD international conference on Management of data, 2014, pp. 1447\u20131458.",
            "year": 2014
        },
        {
            "authors": [
                "A. Makhdoumi",
                "N. Fawaz"
            ],
            "title": "Privacy-utility tradeoff under statistical uncertainty",
            "venue": "2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton), IEEE, 2013, pp. 1627\u20131634.",
            "year": 2013
        },
        {
            "authors": [
                "S.R. Rajagopalan",
                "L. Sankar",
                "S. Mohajer",
                "H.V. Poor"
            ],
            "title": "Smart meter privacy: A utilityprivacy framework",
            "venue": "2011 IEEE international conference on smart grid communications (SmartGridComm), IEEE, 2011, pp. 190\u2013 195.",
            "year": 2011
        },
        {
            "authors": [
                "C. Sharma",
                "B. Mandal",
                "G. Amariucai"
            ],
            "title": "A practical approach to navigating the tradeoff between privacy and precise utility",
            "venue": "ICC 2021-IEEE International Conference on Communications, IEEE, 2021, pp. 1\u20136.",
            "year": 2021
        },
        {
            "authors": [
                "C. Braun",
                "K. Chatzikokolakis",
                "C. Palamidessi"
            ],
            "title": "Quantitative notions of leakage for one-try attacks",
            "venue": "Electronic Notes in Theoretical Computer Science, vol. 249, pp. 75\u201391, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "G. Smith"
            ],
            "title": "On the foundations of quantitative information flow",
            "venue": "International Conference on Foundations of Software Science and Computational Structures, Springer, 2009, pp. 288\u2013 302.",
            "year": 2009
        },
        {
            "authors": [
                "F. Farokhi",
                "H. Sandberg"
            ],
            "title": "Fisher information as a measure of privacy: Preserving privacy of households with smart meters using batteries",
            "venue": "IEEE Transactions on Smart Grid, vol. 9, no. 5, pp. 4726\u20134734, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Optimal privacy-preserving policy using constrained additive noise to minimize the fisher information",
            "venue": "2017 IEEE 56th Annual Conference on Decision and Control (CDC), IEEE, 2017, pp. 2692\u20132697.",
            "year": 2017
        },
        {
            "authors": [
                "C.X. Wang",
                "Y. Song",
                "W.P. Tay"
            ],
            "title": "Preserving parameter privacy in sensor networks",
            "venue": "2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP), IEEE, 2018, pp. 1316\u20131320.",
            "year": 2018
        },
        {
            "authors": [
                "I. Issa",
                "S. Kamath",
                "A.B. Wagner"
            ],
            "title": "An operational measure of information leakage",
            "venue": "2016 Annual Conference on Information Science and Systems (CISS), IEEE, 2016, pp. 234\u2013 239. 16",
            "year": 2016
        },
        {
            "authors": [
                "J. Liao",
                "L. Sankar",
                "F.P. Calmon",
                "V.Y. Tan"
            ],
            "title": "Hypothesis testing under maximal leakage privacy constraints",
            "venue": "2017 IEEE International Symposium on Information Theory (ISIT), IEEE, 2017, pp. 779\u2013783.",
            "year": 2017
        },
        {
            "authors": [
                "C.T. Li",
                "A. El Gamal"
            ],
            "title": "Maximal correlation secrecy",
            "venue": "IEEE Transactions on Information Theory, vol. 64, no. 5, pp. 3916\u20133926, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Asoodeh",
                "F. Alajaji",
                "T. Linder"
            ],
            "title": "On maximal correlation, mutual information and data privacy",
            "venue": "2015 IEEE 14th Canadian Workshop on Information Theory (CWIT), IEEE, 2015, pp. 27\u201331.",
            "year": 2015
        },
        {
            "authors": [
                "T. Li",
                "N. Li"
            ],
            "title": "On the tradeoff between privacy and utility in data publishing",
            "venue": "Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2009, pp. 517\u2013526.",
            "year": 2009
        },
        {
            "authors": [
                "Y.O. Basciftci",
                "Y. Wang",
                "P. Ishwar"
            ],
            "title": "On privacy-utility tradeoffs for constrained data release mechanisms",
            "venue": "2016 Information Theory and Applications Workshop (ITA), IEEE, 2016, pp. 1\u20136.",
            "year": 2016
        },
        {
            "authors": [
                "H. Wang",
                "F.P. Calmon"
            ],
            "title": "An estimationtheoretic view of privacy",
            "venue": "2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton), IEEE, 2017, pp. 886\u2013893.",
            "year": 2017
        },
        {
            "authors": [
                "M.A. Erdogdu",
                "N. Fawaz"
            ],
            "title": "Privacy-utility trade-off under continual observation",
            "venue": "2015 IEEE International Symposium on Information Theory (ISIT), IEEE, 2015, pp. 1801\u20131805.",
            "year": 2015
        },
        {
            "authors": [
                "B. Zhou",
                "Y. Han",
                "J. Pei",
                "B. Jiang",
                "Y. Tao",
                "Y. Jia"
            ],
            "title": "Continuous privacy preserving publishing of data streams",
            "venue": "Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology, 2009, pp. 648\u2013659.",
            "year": 2009
        },
        {
            "authors": [
                "C. Dwork",
                "M. Naor",
                "T. Pitassi",
                "G.N. Rothblum"
            ],
            "title": "Differential privacy under continual observation",
            "venue": "Proceedings of the fortysecond ACM symposium on Theory of computing, 2010, pp. 715\u2013724.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Song",
                "C.X. Wang",
                "W.P. Tay"
            ],
            "title": "Privacyaware kalman filtering",
            "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 4434\u2013 4438. doi: 10.1109/ICASSP.2018.8462600.",
            "year": 2018
        },
        {
            "authors": [
                "F. Koufogiannis",
                "G.J. Pappas"
            ],
            "title": "Differential privacy for dynamical sensitive data",
            "venue": "2017 IEEE 56th Annual Conference on Decision and Control (CDC), IEEE, 2017, pp. 1118\u20131125.",
            "year": 2017
        },
        {
            "authors": [
                "S. Han",
                "G.J. Pappas"
            ],
            "title": "Privacy in control and dynamical systems",
            "venue": "Annual Review of Control, Robotics, and Autonomous Systems, vol. 1, pp. 309\u2013332, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "G. Sugiura",
                "K. Ito",
                "K. Kashima"
            ],
            "title": "Bayesian differential privacy for linear dynamical systems",
            "venue": "IEEE Control Systems Letters, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "G.F. Welch"
            ],
            "title": "Kalman filter",
            "venue": "Computer Vision: A Reference Guide, pp. 1\u20133, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W.T. Uther",
                "M.M. Veloso"
            ],
            "title": "Tree based discretization for continuous state space reinforcement learning",
            "venue": "Aaai/iaai, vol. 98, pp. 769\u2013774, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "J.P. Rust"
            ],
            "title": "A comparison of policy iteration methods for solving continuous-state, infinite-horizon markovian decision problems using random, quasi-random, and deterministic discretizations",
            "venue": "Infinite-Horizon Markovian Decision Problems Using Random, Quasirandom, and Deterministic Discretizations (April 1997), 1997.",
            "year": 1997
        },
        {
            "authors": [
                "S. Koenig",
                "R. Simmons"
            ],
            "title": "Xavier: A robot navigation architecture based on partially observable markov decision process models",
            "venue": "Artificial Intelligence Based Mobile Robotics: Case Studies of Successful Robot Systems, no. partially, pp. 91\u2013122, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "E.B. Iversen",
                "J.M. Morales",
                "H. Madsen"
            ],
            "title": "Optimal charging of an electric vehicle using a markov decision process",
            "venue": "Applied Energy, vol. 123, pp. 1\u201312, 2014. 17",
            "year": 2014
        },
        {
            "authors": [
                "Z. Feng",
                "R. Dearden",
                "N. Meuleau",
                "R. Washington"
            ],
            "title": "Dynamic programming for structured continuous markov decision problems",
            "venue": "arXiv preprint arXiv:1207.4115, 2012. 18",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "The inevitable leakage of privacy as a result of unrestrained disclosure of personal information has motivated extensive research on robust privacy-preserving mechanisms. However, existing research is mostly limited to solving the problem in a static setting with disregard for the privacy leakage over time. Unfortunately, this treatment of privacy is insufficient in practical settings where users continuously disclose their personal information over time resulting in an accumulated leakage of the users\u2019 sensitive information. In this paper, we consider privacy leakage over a finite time horizon and investigate optimal strategies to maximize the utility of the disclosed data while limiting the finite-horizon privacy leakage. We consider a simple privacy mechanism that involves compressing the user\u2019s data before each disclosure to meet the desired constraint on future privacy. We further motivate several algorithms to optimize the dynamic privacy-utility tradeoff and evaluate their performance via extensive synthetic performance tests.\nIndex terms\u2014 Dynamic privacy, Utility, Finite horizon, Kalman filter, Bellman equation.\n\u2217Chandra Sharma is a PhD student in the computer science department at Kansas State University, KS, e-mail: ch1ndra@ksu.edu.\n\u2020George Amariucai is an associate professor in the computer science department at Kansas State University, e-mail: amariucai@ksu.edu.\n\u2021Shuangqing Wei is a professor in the electrical and computer engineering department at Louisiana State University, e-mail: swei@lsu.edu."
        },
        {
            "heading": "1 Introduction",
            "text": "The unprecedented growth of big-data applications suggests that there is a growing competition in the technological world to collect and harness tremendous amounts of user information. Tech companies and other online service providers are always seeking to enhance the quality of their products and services by collecting massive amounts of information from their user base. Users often disclose their personal information either by directly engaging with the service providers, such as in the case of social media and online shopping, or indirectly, and often inadvertently, by simply possessing IoT and other smart devices, such as location trackers.\nUsers are often oblivious to the actual scale and nature of the information disclosure. The disclosed information often contains sensitive information about the users such as their current location, income, religious beliefs and sexual orientation. Further, service providers often share, and even sell, their customers\u2019 information with third parties, which makes protecting the users\u2019 private information ever so critical. In light of this, there have been increasing efforts to devise privacy-preserving mechanisms that make it difficult for external entities to infer a user\u2019s private information from the disclosed data. Such mechanisms protect the user\u2019s information often via means of distortion [1]\u2013[8] and/or compression [9]\u2013[13] before disclosure. Unfortunately, the introduction of distortion (and/or compression) entails a loss of some useful information from the disclosed data which can otherwise be utilized by the service provider to provide a customized service to the user. The challenge, therefore, is to find an optimal tradeoff between pro-\nar X\niv :2\n20 8.\n10 25\n3v 1\n[ cs\n.C R\n] 4\nJ ul\n2 02\ntecting the user\u2019s privacy and enhancing the utility of the disclosed data.\nExisting works that address the tradeoff between privacy and utility mostly treat privacy differently based on the context. In the context of data analysis, privacy is related to an adversary\u2019s ability to identify a user or the user\u2019s sensitive attributes in a database (for instance, [14]\u2013[19]), whereas in the context of information flow (over a noisy channel), privacy is related to an adversary\u2019s increase in knowledge about a user\u2019s sensitive attributes given some observable data (for instance, [2], [14], [20]\u2013[31]). While existing works try to capture different notions of privacy and derive theoretical bounds on the privacy leakage, they do so in a static setting which either assumes that a user discloses their information only once, or it treats each disclosure of the user\u2019s information independently of the previous disclosures. This model of privacy does not accurately reflect real world settings in which users continuously disclose their personal information over time (as in social media) and each disclosure is temporally correlated with the previous disclosures. Therefore, privacy models under static settings are not only incomplete but also inaccurate for many practical applications.\nIn this paper, we consider a dynamic model of privacy and capture privacy leakage over time, specifically focusing on the leakage at the end of a finite time horizon. Subject to the constraint on future privacy leakage, we investigate different strategies that yield different net utilities for the user. Notice that our privacy-utility tradeoff model is directly analogous to the investment problem from economics where a user seeks to maximize her rate of return over a finite time horizon by carefully choosing to invest a certain amount of money and spend the rest from her periodic income. Some examples of practical settings where our privacy model is useful are: 1. A person aims to run for an office n years from the present. The person seeks to cautiously regulate their social media usage in the meantime so that they can limit the amount of private information that can be inferred when they run for the office. 2. A chemical plant is designed to undergo a complete overhaul after a certain number of years. In the meantime, the plant operator hires a third-party consultant, with\nexpertise in data-based control strategies, with whom they decide to share some of the sensor data. At the same time, the operator aims to hide information about proprietary chemical processes at the time of the overhaul and therefore, looks to cautiously control the amount of shared information when the plant is in operation.\nOverall, the main contributions of the paper are as follows:\n1. We formulate a novel privacy-utility tradeoff problem capturing the dynamics of privacy leakage over a finite time horizon. Our dynamic model of privacy also captures a practical setting in which a user\u2019s perception of their own privacy may change over time.\n2. Under our privacy-utility tradeoff model, we investigate different strategies that allow a user to maximize their net utility subject to certain privacy requirements.\n3. We discuss challenges associated with finding optimal strategies for real world problems and motivate sub-optimal algorithms to solve the tradeoff problem.\n4. We extensively evaluate the performance of the sub-optimal algorithms on synthetic datasets and demonstrate that despite being sub-optimal, the proposed algorithms perform extremely well in achieving a good privacy-utility tradeoff.\n5. We formulate a simpler dynamic privacy problem that is computationally less intensive to solve but conserves the essence of the original problem.\nThe rest of the paper is organized as follows. In Section 2, we briefly review existing works closely related to this paper. In Section 3, we discuss the problem setting, explore the privacy and the utility requirements and motivate the finite-horizon privacyutility tradeoff optimization problem. In Section 4, we formulate the finite-horizon privacy-utility tradeoff problem as a Markov Decision Process problem. In Section 5, we discuss the challenges associated with\nsolving the optimization problem and motivate suboptimal algorithms. In Section 6 we discuss a simpler problem model, highlight its advantages and present a computationally less intensive algorithm to solve the simplified problem. In Section 7, we evaluate the performance of all algorithms on synthetic datasets. Finally, in Section 8, we sum up our work with closing remarks."
        },
        {
            "heading": "2 Related Work",
            "text": "The problem of optimizing the privacy-utility tradeoff in a static setting has been widely studied under different notions of privacy. Existing works mostly focus on precisely defining privacy based on the context and deriving bounds on the privacy leakage using metrics such as KL-divergence [32], Differential Privacy [14]\u2013[16], Correlated Differential Privacy [17], [18], Blowfish Privacy [19], Mutual Information [2], [6], [20], [21], [33], Changes in min-entropy [14], [23], [24], Fisher Information [25]\u2013[27], Maximal Leakage [28], [29], \u03c72-information [34] and Maximal Correlation [20], [30], [31]). However, most existing works do not model a continuous and correlated disclosure of information and therefore, do not capture the privacy leakage over time.\nThere are but a few works that model the continuous disclosure of a user\u2019s information and capture the temporal correlation between subsequent disclosures. In [35], the authors investigate the privacy leakage resulting from a continuous release of a time-series data that is correlated, both spatially and temporally, with a user\u2019s sensitive data (also considered to be time-series but non-disclosable). The privacy mechanism seeks to distort the time series data before each disclosure to impede inference attacks on the sensitive data while preserving the utility of the disclosed data. This model seeks to limit the privacy leakage at the present time using the information from the past disclosures and by carefully regulating the current disclosure (other similar models can be found in [36], [37]). In contrast, our model seeks to limit the privacy leakage in the future using the information from the past disclosures and by carefully regulating the present and all future disclosures. Due to\nthe uncertainties around the future observations, our model is more complex, but also more general, and easily simplifies to a model similar to that in [35] under a particular instantiation (namely, n = k where n represents the finite time horizon and k represents the current time step).\nRecently, researchers have started to explore the privacy issues in a dynamic setting with regard to future privacy leakage. In [38], the authors investigate the privacy issues related with the continuous disclosure of sensor measurements containing some private and some public information. They formulate the problem as a filtering problem in which they seek to find the optimal compression that maximizes the variance of the estimation error associated with the estimation of the private data while minimizing the variance of the estimation error associated with the estimation of the public data. Under their model, they investigate the privacy-utility tradeoff at the current time step and one time step into the future. The same work is further extended in [13] where the authors investigate the tradeoff multiple time steps into the future. In their formulation, they make predictions about the system\u2019s future state using the observations available up until the current time step. However, since future observations are discounted in making the prediction, the predicted state can be quite off from the actual future state. Further, the constraint is formulated in terms of the predicted future privacy leakage instead of the actual leakage. Therefore, while this model can be useful, it is not quite complete as it does not accurately reflect the actual future privacy leakage. In many practical applications, due to the inherent uncertainty about the future observations, no strategy can satisfy a nontrivial privacy constraint with certainty. A comprehensive analysis of the privacy-utility tradeoff, therefore, requires the investigation of the probability of privacy outage, i.e. the probability that the privacy constraint is not satisfied in the future."
        },
        {
            "heading": "3 Problem Description",
            "text": ""
        },
        {
            "heading": "3.1 Problem Setup",
            "text": "We consider a setting where a privacy-aware user seeks to cautiously disclose her personal information to a data analyst over a finite time horizon. The objective of the user is to maximize her instantaneous utilities, which the data analyst provides by extracting useful information from the disclosed information at each time step, while limiting the amount of leakage about her sensitive information at the end of the finite time horizon. In contrast to the static setting which models the information disclosure at a single time step, the dynamic setting under consideration models the incremental disclosure of information at every time step until the end of the finite time horizon. The solution to this dynamic privacy problem involves finding an optimal strategy that maximizes the sum of instantaneous utilities while ensuring that the privacy leakage at the end of the finite horizon remains below a pre-specified threshold with high probability.\nIn the dynamic privacy setting, we assume that each user has a set of features, represented by the random vector X, which evolves over time. We use the subscript k to denote the feature vector at time step k. At any given time step k, the feature vector consists of the user\u2019s private features represented by the random vector Xpk and the user\u2019s public features represented by the random vector Xuk such that Xk = X p k \u222aXuk . We consider a general setting where Xpk and X u k are correlated. In many real world settings, the user\u2019s observations of her own feature vectors are only available as noisy measurements (for instance, heart-rate readings from a smart watch). To model this, we assume that the true values of Xk (and consequently, X p k and X u k ) may not be directly observable; instead, there is an observable process Zk that carries information about Xk such that Zk = f(Xk). The user\u2019s instantaneous privacy and utility is directly associated with Xpk and X u k , respectively. Next, we assume that the user is willing to disclose Zk in return for some utility. However, as Zk contains information about both X p k and X u k , disclosing Zk inevitably leaks some information about\nXpk , and this leakage carries over to the future timesteps which the user seeks to avoid. To address this, we consider a privacy mechanism which perturbs Zk before disclosure. The privacy mechanism involves transforming the entire observation vector, Zk, into a lower-dimensional noisy vector, Z\u0303k. The transformation is essentially non-invertible and therefore, certain information about Zk (and consequently, Xk) is lost due to the transformation. An ideal transformation function maximizes the information loss regarding Xpk while minimizing the information loss regarding Xuk . However, due to the correlation between X p and Xu, this may not always be possible.\nA Linear Dynamical System (LDS), which is a continuous state-space generalization of a Hidden Markov Model, can be used to model the evolution of the user\u2019s feature vectors over time as well as the observation process. LDS has been widely used to model the underlying system in the context of privacy-preserving information disclosure [13], [38]\u2013 [41]. We consider a first-order LDS model in which Xk evolves according to the linear equation:\nXk = FkXk\u22121 +Wk, (1)\nwhere Fk is the state-transition matrix and Wk is the zero-mean Gaussian process noise with covariance Qk. Similarly, the observation process can be represented by the linear equation:\nZk = HkXk + Vk, (2)\nwhere Hk denotes the observation matrix and Vk denotes the zero-mean Gaussian measurement noise with covariance Rk. We consider both Fk and Hk to be full-ranked square matrices and assume that all system parameters are publicly known. For quick reference, the description of all system parameters, system vectors and other symbols used throughout this paper can be found in Table 1.\nThe privacy mechanism involves mapping the observation vector Zk to a lower-dimensional vector Z\u0303k using a compression matrix Ck such that Z\u0303k = CTk Zk. We assume that an adversary has complete knowledge about the system dynamics as well as the privacy mechanism. The goal of the data-owner\n(user) is to prudently select the compression matrices, C1, C2, \u00b7 \u00b7 \u00b7Cn, that maximize the sum of instan-\ntaneous utilities while limiting the amount of information leaked about the private features at the end of the finite time horizon, n. Note that the sequence C1, C2, \u00b7 \u00b7 \u00b7 , Cn constitutes the strategy for the dataowner. The data analyst is tasked with inferring Xuk from the disclosed sequence Z\u03031, Z\u03032, \u00b7 \u00b7 \u00b7 , Z\u0303k at each time step k while the future adversary seeks to infer Xpn from the disclosed sequence Z\u03031, Z\u03032, \u00b7 \u00b7 \u00b7 , Z\u0303n. The problem, therefore, naturally relates to an estimation problem. The dynamics of the problem are depicted in Figure 1.\nBefore discussing the formal models of privacy and utility, we first focus on the problem of estimating the latent system states, Xpk and X u k , given a series of observations. This estimation problem can be solved using the Kalman filter which is an optimal linear filter in terms of minimizing the Mean Squared Error of the\nestimates [42]. Estimation using the Kalman filter involves two steps: the prediction step in which the system states are predicted a priori and the update step in which the current measurements/observations are incorporated to update the state estimates. Formally, the Kalman filter for the LDS represented by (1) and (2) can be expressed as [42]:\nPrediction step:\nx\u0302k|k\u22121 = Fkx\u0302k\u22121|k\u22121 Pk|k\u22121 = FkPk\u22121|k\u22121F T k\nUpdate step:\nx\u0302k|k = x\u0302k|k\u22121 +Kk(Zk \u2212Hkx\u0302k|k\u22121) Pk|k = Pk|k\u22121 \u2212KkHkPk|k\u22121,\nwhere x\u0302k|k\u22121 is the a priori estimate of Xk given the observations up to time k\u2212 1, x\u0302k|k is the a posteriori estimate of Xk given the observations up to time k, Pk|k\u22121 is the a priori error covariance of the estimate x\u0302k|k\u22121, and Pk|k is the a posteriori error covariance of the estimate x\u0302k|k. The Kalman gain, Kk, is given by Kk = Pk|k\u22121H T k (HkPk|k\u22121H T k +Rk) \u22121."
        },
        {
            "heading": "3.2 Privacy and Utility Requirements",
            "text": "Let X\u0302Z,uk|k and X\u0302 Z,p k|k represent the data owner\u2019s estimate of Xuk and X p k , respectively, given the series of observations, Z1, Z2, \u00b7 \u00b7 \u00b7 , Zk. Similarly, let X\u0302Z\u0303,uk|k and X\u0302Z\u0303,pk|k represent the data analyst\u2019s estimate of Xuk and the adversary\u2019s estimate of X p k , respectively, given the series of observations, Z\u03031, Z\u03032, \u00b7 \u00b7 \u00b7 , Z\u0303k. Also, let d(X,Y ) denote some distance function that measures the distance between random vectors X and Y. An example of the distance function is the L2norm. From the utility point of view, it is desirable that d(X\u0302Z\u0303,uk|k , X\u0302 Z,u k|k ) is as small as possible for all k. A zero distance between the estimates, X\u0302Z\u0303,uk|k and X\u0302Z,uk|k , is achievable if the data owner discloses her true observations, Z1, Z2, \u00b7 \u00b7 \u00b7 , Zk, with no privacy mechanisms. Similarly, from the future privacy point of view, it is desirable that d(X\u0302Z,pn|n , X\u0302 Z\u0303,p n|n ) is\nas large as possible. However, due to the correlation between Xpk and X u k , in general, it is not feasible to both minimize the instantaneous utility losses and maximize the perceived future privacy. The problem, therefore, naturally manifests as a privacyutility trade-off optimization problem. Intuitively, the optimization problem involves finding an optimal strategy that minimizes the sum of instantaneous\nutility losses, \u2211n k=1 d(X\u0302 Z\u0303,u k|k , X\u0302 Z,u k|k ), under the constraint that the privacy leakage at the end of the finite horizon, 1\nd(X\u0302Z\u0303,p n|n ,X\u0302 Z,p n|n)\n, must not exceed a pre-specified\nthreshold 1\u03b4 . Formulating the optimization problem, however, exposes several challenges. For one,\u2211n k=1 d(X\u0302 Z\u0303,u k|k , X\u0302 Z,u k|k ) and d(X\u0302 Z\u0303,p n|n , X\u0302 Z,p n|n) are both random variables due to the uncertainties in the future observations, Zk+1, Zk+2, \u00b7 \u00b7 \u00b7 , Zn. Further, without the knowledge of the future observations, it is difficult to devise an optimal strategy that satisfies the constraint on future privacy leakage. In fact, given the Gaussian assumption for both the process noise and the measurement noise, it may not even be possible to ensure a non-trivial constraint on the future privacy leakage using any feasible sequence of actions, C1, C2, \u00b7 \u00b7 \u00b7 , Cn; it is, therefore, more appropriate to characterize strategies in terms of the probability of privacy outage, P(d(X\u0302Z\u0303,pn|n , X\u0302 Z,p n|n) < \u03b4), in addition to the total utility loss. The probability of privacy outage reflects the probability that the privacy constraint is not satisfied in the future."
        },
        {
            "heading": "4 Formulation as a Markov De-",
            "text": "cision Process Problem\nThe finite horizon privacy-utility trade-off problem fits nicely with a Markov Decision Process (MDP). In a discrete time continuous state MDP model, at every time step k, an agent observes the current state of some Markov process Sk, takes an action ak and receives a reward Rk. The state of the Markov process at time step k, in general, depends on the state and the action at time step k\u22121 and some stochastic process noise \u03c9k. The reward received by the agent at time step k depends on the current state of the\nMarkov process, the current action taken by the agent and the next state of the Markov process.\nFormally, a discrete time continuous state continuous action Markov Decision Process is a tuple (S,A, P,R, \u03b3) where S \u2208 RJ represents the state space, A \u2208 RL represents the action space, P : S \u00d7 A \u00d7 S \u2192 [0, 1] represents the state-transition function such that P (sk|ak, sk+1) gives the probability of transitioning to the next state sk+1 from the current state sk by taking an action ak. Let the random vectors Sk, Ak and Sk+1 represent the current state, the current action and the next state, respectively. As the state space is continuous, P is specified as a probability density function such that \u222b \u03a8 P (sk|ak, sk+1) dsk+1 = P(Sk+1 \u2208 \u03a8|Sk = sk, Ak = ak), where \u03a8 \u2286 S, with S the space of Sk. Similarly, R : S \u00d7 A \u00d7 S \u2192 R represents the reward function such that Rk(sk, ak, sk+1) gives the reward received by the agent at time step k by taking an action ak when the current and the next states of the Markov process are sk and sk+1, respectively. The discount factor \u03b3 \u2208 [0, 1] captures how the agent values her future rewards compared to her current reward \u2013 if \u03b3 = 1, the agent values all her future rewards equally to her current reward and if \u03b3 = 0, the agent only values her current reward and disregards all her future rewards.\nThe goal of the agent is to maximize the expected sum of her current and future discounted rewards, E[ \u2211 k \u03b3\nkRk(Sk, Ak, Sk+1)]. The agent seeks to find the optimal sequence of actions that allows her to optimize the expected sum of discounted rewards. In this regard, it is useful to define a function, called the optimal state-value function, that provides a measure of the maximum achievable sum of expected rewards from a particular state. Let V \u2217k (sk) denote the optimal state-value function at time step k given the current state sk. Then, the optimal state-value function can be written as a recursive equation using Bellman\u2019s Principle of Optimality as:\nV \u2217k (sk) = max ak \u222b S P (sk|ak, sk+1) ( Rk(sk, ak, sk+1)+\n\u03b3V \u2217k+1(sk+1) ) dsk+1.\nThe Bellman equation formulation offers a dynamic\nprogramming approach to solve the resulting optimization problem.\nThe finite-horizon privacy-utility problem can be directly translated to a finite-horizon discrete time continuous state continuous action MDP problem. Recall that in the finite-horizon privacy setting, the user seeks to find an optimal sequence of actions that allows her to maximize the sum of the instantaneous utilities while ensuring that the privacy leakage at the end of the finite horizon remains below a pre-specified threshold with high probability. This is inherently a decision problem that incorporates a meaningful notion of reward captured as the expected sum of instantaneous utilities and future privacy leakage.\nLet X\u0302Zk|k represent the data owner\u2019s estimate of X, given the series of observations, Z1, Z2, \u00b7 \u00b7 \u00b7 , Zk and PZk|k represent the error covariance associated with the estimate1. Similarly, let X\u0302Z\u0303k|k represent the data analyst\u2019s (or adversary\u2019s) estimate of X, given the series of observations, Z\u03031, Z\u03032, \u00b7 \u00b7 \u00b7 , Z\u0303k and P Z\u0303k|k represent the error covariance associated with the estimate. Now, define\ndu(k, k) , d(X\u0302Z,uk|k , X\u0302 Z\u0303,u k|k ),\ndp(j, k) , d(X\u0302Z,pj|k , X\u0302 Z\u0303,p j|k ) (n \u2265 j \u2265 k),\nSk , {Zk, X\u0302Zk\u22121|k\u22121, X\u0302 Z\u0303 k\u22121|k\u22121, P Z k\u22121|k\u22121, P Z\u0303 k\u22121|k\u22121},\nwhere Sk represents the state of the MDP (which is fundamentally different from the state of the LDS, Xk). The state variables in Sk can be recursively computed using the following sequence of Kalman fil-\n1PZ k|k is not a function of Zk. The superscript Z is used as a convention to imply that the symbol being defined directly concerns the data owner, who has observations {Zk}, rather than the adversary or data analyst, who have observations {Z\u0303k}.\nter equations:\nX\u0302Zk|k\u22121 = FkX\u0302 Z k\u22121|k\u22121,\nX\u0302Z\u0303k|k\u22121 = FkX\u0302 Z\u0303 k\u22121|k\u22121, PZk|k\u22121 = FkP Z k\u22121|k\u22121F T k +Qk,\nP Z\u0303k|k\u22121 = FkP Z\u0303 k\u22121|k\u22121F T k +Qk, KZk = P Z k|k\u22121H T k (HkP Z k|k\u22121H T k +Rk) \u22121,\nKZ\u0303k = P Z\u0303 k|k\u22121H T k Ck(C T k HkP Z\u0303 k|k\u22121H T k Ck + C T k RkCk) \u22121,\nX\u0302Zk|k = X\u0302 Z k|k\u22121 +K Z k (Zk \u2212HkX\u0302Zk|k\u22121),\nX\u0302Z\u0303k|k = X\u0302 Z\u0303 k|k\u22121 +K Z\u0303 k (C T k Zk \u2212 CTk HkX\u0302Z\u0303k|k\u22121), PZk|k = P Z k|k\u22121 \u2212K Z k HkP Z k|k\u22121,\nP Z\u0303k|k = P Z\u0303 k|k\u22121 \u2212K Z\u0303 k C T k HkP Z\u0303 k|k\u22121.\nInitially, X\u0302Z0|0 = X\u0302 Z\u0303 0|0 = E[X0] and P Z 0|0 = P Z\u0303 0|0 =\nCov(X0).\nWe now define the reward function, Rk, as\nRk(Sk, Ck, Sk+1) ={ \u03b1 ( dp(n, k + 1)\u2212 dp(n, k) ) \u2212 du(k, k) when k < n,\n\u03b1dp(n, k)\u2212 du(k, k) when k = n, (3)\nwhere \u03b1 is the privacy-utility tradeoff parameter. At any given time k, the user\u2019s goal is to chose an action Ck = f(Sk) that allows her to maximize the expected sum of the current and future rewards, E[ \u2211n t=k Rt(St, Ct, St+1)].\nLet C\u2217 = {C\u22171 , C\u22172 , \u00b7 \u00b7 \u00b7 , C\u2217n} be the set of optimal actions. Notice that at the beginning of the finite time horizon, the sum of the rewards can be expressed as\nn\u2211 k=0 Rk(Sk, Ck, Sk+1)\n= \u03b1 ( 2dp(n, n)\u2212 dp(n, 0) ) \u2212 n\u2211 k=0 du(k, k).\nSince dP (n, 0) and du(0, 0) are both zero (which follows from the assumption that X\u0302Z0|0 = X\u0302 Z\u0303 0|0 and\nPZ0|0 = P Z\u0303 0|0), substituting 2\u03b1 = \u03b2, we get\nn\u2211 k=0 Rk(Sk, Ck, Sk+1) = \u03b2d p(n, n)\u2212 n\u2211 k=1 du(k, k).\n(4)\nAs the reader may have noticed by now, the reward function is defined such that the sum of rewards captures both the privacy and the utility aspects of the problem in a single expression given in (4). The value of \u03b3 is taken to be 1 for the same reason. Note that the parameter \u03b2 in (4) directly relates to the probability of privacy outage P(d(X\u0302Z\u0303,pn|n , X\u0302 Z,p n|n) < \u03b4) and the resulting privacy-utility tradeoff; larger values of beta are expected to result in higher utility losses with lower probabilities of privacy outage while smaller values of beta are expected to result in lower utility losses with higher probabilities of privacy outage. The privacy-utility tradeoff region corresponding to different values of \u03b2 will be determined experimentally.\nWe now use the Bellman equation to formulate the finite horizon privacy-utility trade-off optimization problem. Let Vk denote the state-value function at timestep k and V \u2217k (sk) denote the optimal state-value function given the state sk = {zk, x\u0302zk\u22121|k\u22121, x\u0302 z\u0303 k\u22121|k\u22121, P z k\u22121|k\u22121, P z\u0303 k\u22121|k\u22121}. Then, using the Bellman equation of optimality, the optimization problem can be formulated as:\nV \u2217k (sk) = max Ck \u222b S\nP(sk+1|sk, Ck)\u00b7( Rk(sk, Ck, sk+1) + V \u2217 k+1(sk+1) ) dsk+1\n= max Ck \u222b Z P(zk+1|sk, Ck) \u222b \u039b\nP(x\u0302zk|k|sk, Ck)\u00b7\u222b \u2206 P(x\u0302z\u0303k|k|sk, Ck) \u222b \u03a6\nP(P zk|k|sk, Ck)\u00b7\u222b \u2126 P(P z\u0303k|k|sk, Ck) ( Rk(sk, Ck, sk+1) + V \u2217 k+1(sk+1) ) \u00b7\ndP z\u0303k|k dP z k|kdx\u0302 z\u0303 k|k dx\u0302 z k|k dzk+1,\nwhere Z,\u039b,\u2206,\u03a6 and \u2126 are the feasible spaces of zk+1, x\u0302 z k|k, x\u0302 z\u0303 k|k, P z k|k and P z\u0303 k|k, respectively.\nGiven sk and Ck, the state variables, x\u0302 z k|k, x\u0302 z\u0303 k|k, P zk|k and P z\u0303 k|k, are all deterministic (which directly follows from the application of the Kalman filter equations). Therefore,\nV \u2217k (sk) = max Ck \u222b Z\nP(zk+1|sk, Ck)\u00b7( Rk(sk, Ck, sk+1) + V \u2217 k+1(sk+1) ) dzk+1\n(5)\n= max Ck \u222b Z\nP(zk+1|zk)\u00b7( Rk(sk, Ck, sk+1) + V \u2217 k+1(sk+1) ) dzk+1.\n(6)\nIf Xk is a Gaussian process and Vk is a Gaussian white noise process, then, Zk+1|Zk \u223c N(\u00b5\u0304, \u03a3\u0304) where\n\u00b5\u0304 = E[Zk+1|Zk] = E[HXk+1 + Vk+1|Zk] = E[HXk+1|Zk] + E[Vk+1|Zk] = HE[Xk+1|Zk] = Hx\u0302zk+1|k\n= HFx\u0302zk|k\nand\n\u03a3\u0304 = Cov(Zk+1|Zk) = Cov(HXk+1 + Vk+1|Zk) = Cov[HXk+1|Zk] + Cov[Vk+1|Zk] = HCov(Xk+1|Zk)HT +R = HP zk+1|kH T +R\n= H(FP zk|kF T +Q)HT +R.\nThe optimization problem in (6) reflects the user\u2019s objective of maximizing the expected sum of the current and future rewards starting at a particular state sk. The optimizing argument C \u2217 k(sk) = arg maxCk V \u2217 k (sk) constitutes the best action taken toward the goal of maximizing the expected sum of rewards."
        },
        {
            "heading": "5 Sub-optimal Algorithms",
            "text": "The optimization problem formulated in (6) suffers from the curse of dimensionality as both the state space and the action space are continuous. Analytical methods to solve the problem are infeasible for practical problems as they do not yield closedform solutions for higher dimensional problems. Numerical algorithms, such as the value iteration algorithm and the policy iteration algorithm which advance by sweeping through all possible states at each time step, also fail as there are infinitely many states to sweep through. The problem is therefore not easily tractable without further assumptions about the state space or the action space, or both.\nA popular approach to solving similar optimization problems involves discretizing the state space (see, for instance, [43]\u2013[47]). This approach is typically suboptimal, however, it can still yield promising solutions. In what follows, we highlight different algorithms that are based on the discretization of the state space to solve the optimization problem formulated in (6)."
        },
        {
            "heading": "5.1 Value Iteration with Discretization",
            "text": "The value iteration approach to solving a finitehorizon discrete state space MDP problem involves solving the Bellman equation to find the optimal values for every possible state at every time step, starting at the end of the finite time horizon and working backwards. At time step n, the optimal value of each state is computed using the terminal reward given by Rn(Sn, Cn) = \u03b2d\np(n, n) \u2212 du(n, n). The algorithm then iteratively calculates the optimal values at previous time steps as given in Algorithm 1.\nThe major characteristic of the value iteration algorithm is that it calculates optimal values and the optimal actions associated with the states at all time steps before the actual observations are available. The calculated values are optimal for the discrete MDP, however, due to the additional discretization step (which is not intrinsic to the value iteration algorithm itself), they are typically sub-optimal for the original MDP.\nAlgorithm 1 Value Iteration Algorithm with Discretization\n1: Define the feasible state space, S. 2: Select a discretization rule, D, and discretize S\naccording to D. 3: procedure Value Iteration 4: Initialize V \u2217n (sn) for all sn \u2208 S with terminal\nrewards. 5: for k = n\u2212 1 to 1 do 6: for each sk \u2208 S do 7: V \u2217k (sk) = max\nCk \u2211 zk+1\nP(zk+1|zk)\u00b7( Rk(sk, Ck, sk+1) + V \u2217 k+1(sk+1) ) 8: C\u2217k(sk) = arg maxCk V \u2217 k (sk)\n9: end for 10: end for 11: end procedure"
        },
        {
            "heading": "5.2 Pessimistic algorithm",
            "text": "The pessimistic algorithm is a customized algorithm to solve the finite-horizon discrete state space MDP problem. The pessimistic algorithm captures an agent who always expects to transition to the worst possible state at every time step. The pessimistic algorithm seeks to optimize the value and the action associated with a state while assuming that the next transition leads to the state with the least value. The advantage of using the algorithm is that it is computationally less intensive as the state transition in the underlying model is assumed to be deterministic. The pessimistic algorithm is highlighted in Algorithm 2.\nA quick remark on the notations: min{.} represents the minimum of the set whereas arg minY {.} represents the parameter Y that corresponds to the minimum value of the set."
        },
        {
            "heading": "5.3 Optimistic algorithm",
            "text": "In contrast to the pessimistic algorithm, the optimistic algorithm captures an agent who always expects to transition to the best possible state at every time step. The optimistic algorithm seeks to optimize\nAlgorithm 2 Pessimistic Algorithm\n1: Define the feasible state space, S. 2: Select a discretization rule, D, and discretize S\naccording to D. 3: Initialize V \u2217n (sn) for all sn \u2208 S with terminal re-\nwards. 4: v#n = min{V \u2217n (sn) : sn \u2208 S} 5: s#n = arg minsn{V \u2217 n (sn) : sn \u2208 S} 6: for k = n\u2212 1 to 1 do 7: for each sk \u2208 S do 8: V \u2217k (sk) =\nmax Ck\n( Rk(sk, Ck, s # k+1) + v # k+1 ) 9: C\u2217k(sk) = arg maxCk V \u2217 k (sk)\n10: end for 11: v#k = min{V \u2217k (sk) : sk \u2208 S} 12: s#k = arg minsk{V \u2217 k (sk) : sk \u2208 S} 13: end for\nthe value and the action associated with a state while assuming that the next transition leads to the state with the highest value. The optimistic algorithm is highlighted in Algorithm 3."
        },
        {
            "heading": "6 Privacy-Utility Tradeoff Un-",
            "text": "der Estimated Privacy Leakage\nThe finite-horizon privacy-utility tradeoff problem can also be formulated with the constraint on estimated privacy leakage instead of the actual privacy leakage at the end of the finite time horizon. The resulting optimization problem is much simpler to solve, nevertheless, the dynamic privacy requirements are still captured into the problem formulation. To this end, we consider a user who seeks to maximize her instantaneous utility while limiting the estimated future leakage about her sensitive information. The resulting optimization problem is:\nAlgorithm 3 Optimistic Algorithm\n1: Define the feasible state space, S. 2: Select a discretization rule, D, and discretize S\naccording to D. 3: Initialize V \u2217n (sn) for all sn \u2208 S with terminal re-\nwards. 4: v\u2217n = max{V \u2217n (sn) : sn \u2208 S} 5: s\u2217n = arg maxsn{V \u2217 n (sn) : sn \u2208 S} 6: for k = n\u2212 1 to 1 do 7: for each sk \u2208 S do 8: V \u2217k (sk) =\nmax Ck\n( Rk(sk, Ck, s \u2217 k+1) + v \u2217 k+1 ) 9: C\u2217k(sk) = arg maxCk V \u2217 k (sk)\n10: end for 11: v\u2217k = max{V \u2217k (sk) : sk \u2208 S} 12: s\u2217k = arg maxsk{V \u2217 k (sk) : sk \u2208 S} 13: end for\nmin Ck\nd(X\u0302Z,uk|k , X\u0302 Z\u0303,u k|k ) (7)\nsubject to:\nd(X\u0302Z,pn|k , X\u0302 Z\u0303,p n|k ) \u2265 \u03b4 (8)\nIntuitively, the user seeks to disclose as much as possible (allowed by the privacy constraint) at the current time step so as to maximize her current utility with a complete disregard for her future utilities. This strategy is, therefore, referred to as a maximum disclosure strategy. In contrast, the optimization problem formulated in (6) captures a user who seeks to cautiously disclose her personal information piecewise.\nIn a dynamic setting, a user following the maximum disclosure strategy needs to solve the optimization problem at every time step as new observations are made. As the user approaches the end of the finite time horizon, the privacy constraint is more restrictive due to the accumulated leakage resulting from the past disclosures. In some cases, the actual leakage may already exceed the estimated leakage and therefore, no choice of Ck may satisfy the constraint, especially closer to the end of the finite time horizon.\nTherefore, it is more appropriate to formulate an unconstrained optimization problem that captures the semantics of the constrained problem. This leads us to the following optimization problem:\nmin Ck\nd(X\u0302Z,uk|k , X\u0302 Z\u0303,u k|k )\u2212 \u03b2 ( d(X\u0302Z,pn|k , X\u0302 Z\u0303,p n|k ) ) = max\nCk \u03b2 ( d(X\u0302Z,pn|k , X\u0302 Z\u0303,p n|k ) ) \u2212 d(X\u0302Z,uk|k , X\u0302 Z\u0303,u k|k ) (9)\nThe parameter, \u03b2, in (9) directly relates to the constraint in (8) and influences the probability of privacy outage at the end of the finite time horizon, P(d(X\u0302Z\u0303,pn|n , X\u0302 Z,p n|n) < \u03b4).\nAlthough the optimization problem formulated in (9) can easily be solved without further transformation, it is nevertheless possible to transform the optimization problem into an equivalent MDP formulation. First, define\ndu(k, k) , d(X\u0302Z,uk|k , X\u0302 Z\u0303,u k|k ),\ndp(n, k) , d(X\u0302Z,pn|k , X\u0302 Z\u0303,p n|k ),\nSk , {Zk},\nwhere Sk represents the state of the MDP. We now define the reward function, Rk, as\nRk(Sk, Ck) = \u03b2d p(n, k)\u2212 du(k, k)\nNotice that the reward function is independent of the future observations and therefore, deterministic. At any given time k, the user\u2019s goal is to chose an action Ck = f(Sk) that allows her to maximize the instantaneous reward, Rk(Sk, Ck). Since the user is oblivious to future rewards, we set \u03b3 = 0. The MDP equivalent of the optimization problem in (9) can then be expressed as:\nV \u2217k (sk) = max Ck Rk(sk, Ck) (10)\nThe optimization problem in (10) reflects the user\u2019s objective of maximizing her instantaneous reward at a particular state sk. The argument of the optimization C\u2217k(sk) = arg maxCk V \u2217 k (sk) constitutes the best\naction taken toward the goal of maximizing the instantaneous reward.\nThe main advantage of the optimization problem formulated in (10) (and equivalently, in 9) is that it does not require sweeping through all possible states at each time step (which would otherwise be required if the current reward depended on future states) and therefore, computationally much less intensive to solve. Further, the optimization problem is solved forwards as new observations become available. Algorithm 4 highlights the steps involved in solving the finite-horizon privacy-utility tradeoff optimization problem under estimated privacy leakage using the maximum disclosure strategy.\nAlgorithm 4 Maximum Disclosure Algorithm\n1: At each time step k, do 2: V \u2217k (sk) = maxCk Rk(sk, Ck) 3: C\u2217k(sk) = arg maxCk V \u2217 k (sk) 4: end"
        },
        {
            "heading": "7 Simulations",
            "text": "In this section, we evaluate the performance of the value iteration algorithm, the pessimistic algorithm, the optimistic algorithm and the maximum disclosure algorithm via synthetic simulations. For our simulations, we consider an LDS with Np = 1 and Nu = 2. We assume that Fk and Hk are time invariant such that F1 = F2 = \u00b7 \u00b7 \u00b7 = Fn = F and H1 = H2 = \u00b7 \u00b7 \u00b7 = Hn = H . Further, we assume that Xk is a zero mean Gaussian process and Wk and Vk are independent and identically distributed standard Gaussian random vectors.\nThe elements of F and H are sampled independently from a uniform distribution in the unit interval. F is further normalized such that its eigenvalues lie within a unit circle which ensures that the LDS is stable. As PZk\u22121|k\u22121 is not a function of the observations or the actions, it is computed offline. Similarly, P Z\u0303k\u22121|k\u22121 is estimated with P Z\u0303 k\u22121|0. For the value iteration, pessimistic and optimistic algorithms, a discretization function, D, is used to approximate the components of Zk, X\u0302 Z k\u22121|k\u22121 and X\u0302 Z\u0303 k\u22121|k\u22121 as binary\nvariables. As a simplest discretization strategy, we chose the function D such that:\nD(y) =\n{ E[y]\u2212 0.1 when y < E[y], E[y] + 0.1 when y \u2265 E[y].\n(11)\nThe choice of 0.1 as the distance to the quantization points from the mean is arbitrary.\nThe performances of the four algorithms are evaluated in terms of the probability of privacy outage and the average utility loss. First, each algorithm, with the exception of the maximum disclosure algorithm, is run in turn to determine the optimal actions associated with every discretized state of the Markov Decision Process. Next, 10, 000 Monte-Carlo simulations of the LDS are carried out. In each simulation of the LDS, a sequence of observations, z1, z2, \u00b7 \u00b7 \u00b7 , zn are generated. When an observation zk is generated, the Kalman filter equations are used to compute the actual state, sk. For the value iteration, the pessimistic and the optimistic algorithms, the Bellman equation (6) is solved to determine the optimal action, C\u2217k , associated with the state. For the maximum disclosure algorithm, the non-recursive equation (10) is solved to determine the optimal action, C\u2217k , associated with the state. This process is repeated until the end of the finite time horizon. At the end of the finite time horizon, any violation of the privacy constraint: d(X\u0302Z\u0303,pn|n , X\u0302 Z,p n|n) < \u03b4, is checked, which concludes one simulation. After all simulations have been completed, the probability of privacy outage and the average utility loss are calculated using\nP(outage) = number of constraint violations\ntotal number of simulations and\nAverage utility loss =\n\u2211(\u2211n k=1 d(X\u0302 Z,u k|k , X\u0302 Z\u0303,u k|k ) ) total number of simulations ,\nrespectively. The experiment is repeated multiple times for different randomly generated samples of H and F .\nFrom among multiple system models used in the simulations, three representative system models are selected that provide various insights on the performances of the four algorithms:\nModel 1:\nF = 0.06218 0.08373 0.123240.07386 0.04809 0.11332 0.13481 0.09099 0.06936  , H =\n0.30780 0.77969 0.299940.37514 0.67681 0.45616 0.98334 0.94292 0.45824  .\nModel 2:\nF = 0.02712 0.01067 0.000730.00792 0.01444 0.01576 0.01029 0.00998 0.01596  , H =\n0.02712 0.01067 0.000730.00792 0.01444 0.01576 0.01029 0.00998 0.01596  .\nModel 3:\nF = 0.12246 0.51340 0.140240.45475 0.02484 0.53664 0.35442 0.70248 0.05728  , H =\n0.75237 0.31551 0.853960.93524 0.03364 0.62274 0.01605 0.36138 0.05232  .\nFigure 2 shows the performances of the four algo-\nrithms in terms of the probability of privacy outage and the average utility loss across different values of \u03b2 for system model 1. For reference, the performance of a na\u0308\u0131ve strategy in which the user randomly selects Ck from a uniform distribution in the unit interval at each time step k is also included. In Figure 2a and Figure 2b, we observe that all four algorithms consistently outperform the random action strategy across all values of \u03b2. Also, for all four algorithms, we observe a decrease in the probability of privacy outage, and an increase in the average utility loss, as \u03b2 increases. This observation is consistent with the intuition that larger values of \u03b2 put more weight on the privacy requirement than the utility requirement and therefore, result in a decrease in the probability of privacy outage and an increase in the utility loss. For the random action strategy, however, we observe that the probability of privacy outage (Figure 2a) and the average utility loss (Figure 2b) are both virtually constant across all values of \u03b2. This is expected as the random action strategy does not account for \u03b2 in the selection of Ck.\nIn Figure 2a and Figure 2b, we also observe that the performances of the value iteration, the pessimistic and the optimistic algorithms are similar\nFigure 5: Privacy-utility tradeoff achieved by different strategies for system model 3 (\u03b4 = 0.3, n = 5 and M = 1).\nacross different values of \u03b2. We consistently observed similar performances of the three algorithms for different random samples of H and F and for different values of n. In light of this, we conclude that the average performances of the three algorithms in terms of the probability of outage and the average utility loss are similar. Consequently, it may be desirable to use the pessimistic or the optimistic algorithm over the value iteration algorithm for speed benefits. However, it should be noted that the algorithms may perform differently for a more robust discretization strategy.\nFigure 3, Figure 4 and Figure 5 highlight the privacy-utility tradeoff achieved by the maximum disclosure strategy/algorithm against the value iteration, pessimistic and optimistic algorithms for the three system models. For system model 1 (Figure 3), we observe that the maximum disclosure strategy significantly outperforms the other algorithms and achieves much better privacy-utility tradeoff. For system model 2 (Figure 4), we observe that the performance of the maximum disclosure strategy is similar to that of the value iteration, pessimistic and optimistic algorithms. Similarly, for system model 3 (Figure 5), the dynamic range for the privacy-utility tradeoff achieved by the maximum disclosure strategy is significantly higher than the other strategies\u2013 the higher dynamic range translates to more room for tuning the privacy-utility tradeoff.\nThe performance of the four algorithms, in general, depends on the system model. The relatively poor performances of the value iteration, the pessimistic and the optimistic algorithms against the maximum disclosure strategy across all representative system models can be attributed to the choice of the binary discretization strategy. For a more robust discretization strategy, we expect the three algorithms to perform better than the maximum disclosure strategy. However, increasing the quantization points in pursuit of a better discretization strategy significantly increases the computational requirements and may not be feasible for all systems. For high dimensional practical problems, maximum disclosure strategy is therefore the only computationally feasible option to solve the dynamic privacy problem.\n14"
        },
        {
            "heading": "8 Conclusion",
            "text": "The increasing privacy concerns associated with disclosing personal data have guided the state-of-the-art privacy mechanisms that afford theoretical privacy guarantees. However, existing works mostly consider the problem in a static setting which either assumes that a user discloses her information only once, or treats each disclosure of the user\u2019s information independently to the previous disclosures. In this paper, we considered a dynamic model of privacy in which a privacy-aware user cautiously discloses her personal information to a service provider over a finite time horizon. We investigated different strategies that allow a user to maximize their utility over time while limiting the privacy leakage at the end of the finite horizon. Using experimental evaluations on synthetic datasets, we showed that these strategies, although sub-optimal, can yield promising tradeoff region for the finite-horizon privacy-utility tradeoff problem. Finally, we demonstrated that there exists a simpler strategy corresponding to a simplified version of the problem, that has significant computational benefits with encouraging performance."
        }
    ],
    "title": "The Economics of Privacy and Utility: Investment Strategies",
    "year": 2022
}