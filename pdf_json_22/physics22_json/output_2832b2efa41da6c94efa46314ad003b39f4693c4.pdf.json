{
    "abstractText": "The Bayesian approach to solving inverse problems relies on the choice of a prior. This critical ingredient allows the formulation of expert knowledge or physical constraints in a probabilistic fashion and plays an important role for the success of the inference. Recently, Bayesian inverse problems were solved using generative models as highly informative priors. Generative models are a popular tool in machine learning to generate data whose properties closely resemble those of a given database. Typically, the generated distribution of data is embedded in a low-dimensional manifold. For the inverse problem, a generative model is trained on a database that reflects the properties of the sought solution, such as typical structures of the tissue in the human brain in magnetic resonance (MR) imaging. The inference is carried out in the low-dimensional manifold determined by the generative model which strongly reduces the dimensionality of the inverse problem. However, this proceeding produces a posterior that admits no Lebesgue density in the actual variables and the accuracy reached can strongly depend on the quality of the generative model. For linear Gaussian models we explore an alternative Bayesian inference based on probabilistic generative models which is carried out in the original high-dimensional space. A Laplace approximation is employed to analytically derive the required prior probability density function induced by the generative model. Properties of the resulting inference are investigated. Specifically, we show that derived Bayes estimates are consistent, in contrast to the approach employing the low-dimensional manifold of the generative model. The MNIST data set is used to construct numerical experiments which confirm our theoretical findings. It is shown that the proposed approach can be advantageous when the information contained in the data is high and a simple heuristic is considered for the detection of this case. Finally, pros and cons of both approaches are discussed. keywords: Bayesian inference, Asymptotic properties of parametric estimators, Generative models, Machine learning, Laplace approximation",
    "authors": [
        {
            "affiliations": [],
            "name": "Manuel Marschall"
        },
        {
            "affiliations": [],
            "name": "Gerd W\u00fcbbeler"
        },
        {
            "affiliations": [],
            "name": "Franko Schm\u00e4hling"
        }
    ],
    "id": "SP:30d75f8b03d787d38baecbf20477d3d9c1433a33",
    "references": [
        {
            "authors": [
                "J. Kaipio",
                "E. Somersalo"
            ],
            "title": "Statistical and computational inverse problems",
            "venue": "Springer Science & Business Media",
            "year": 2006
        },
        {
            "authors": [
                "N. Bissantz",
                "H. Holzmann"
            ],
            "title": "Statistical inference for inverse problems,",
            "venue": "Inverse Problems,",
            "year": 2008
        },
        {
            "authors": [
                "N.R. Draper",
                "H. Smith"
            ],
            "title": "Applied regression analysis",
            "venue": "John Wiley & Sons",
            "year": 1998
        },
        {
            "authors": [
                "M. Smith",
                "L. Fahrmeir"
            ],
            "title": "Spatial Bayesian variable selection with application to functional magnetic resonance imaging,",
            "venue": "Journal of the American Statistical Association,",
            "year": 2007
        },
        {
            "authors": [
                "K.-J. Lee",
                "G.L. Jones",
                "B.S. Caffo"
            ],
            "title": "and S",
            "venue": "S. Bassett, \u201cSpatial Bayesian variable selection models on functional magnetic resonance imaging time-series data,\u201d Bayesian Analysis (Online), vol. 9, no. 3, p. 699",
            "year": 2014
        },
        {
            "authors": [
                "H.W. Engl",
                "M. Hanke",
                "A. Neubauer"
            ],
            "title": "Regularization of inverse problems",
            "venue": "Springer Science & Business Media",
            "year": 1996
        },
        {
            "authors": [
                "C.P. Robert"
            ],
            "title": "The Bayesian choice: from decision-theoretic foundations to computational implementation",
            "year": 2007
        },
        {
            "authors": [
                "A. Gelman",
                "J.B. Carlin",
                "H.S. Stern",
                "D.B. Rubin"
            ],
            "title": "Bayesian data analysis",
            "venue": "Chapman and Hall/CRC",
            "year": 1995
        },
        {
            "authors": [
                "H. Rue",
                "L. Held"
            ],
            "title": "Gaussian Markov random fields: theory and applications",
            "venue": "CRC press",
            "year": 2005
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville"
            ],
            "title": "and Y",
            "venue": "Bengio, \u201cGenerative adversarial nets,\u201d Advances in neural information processing systems, vol. 27",
            "year": 2014
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "An introduction to variational autoencoders,",
            "venue": "arXiv preprint arXiv:1906.02691,",
            "year": 1906
        },
        {
            "authors": [
                "Y. Saito",
                "S. Takamichi"
            ],
            "title": "and H",
            "venue": "Saruwatari, \u201cStatistical parametric speech synthesis incorporating generative adversarial networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 1, pp. 84\u201396",
            "year": 2017
        },
        {
            "authors": [
                "H. Wang",
                "Z. Qin"
            ],
            "title": "and T",
            "venue": "Wan, \u201cText generation based on generative adversarial nets with latent variables,\u201d in Pacific-Asia conference on knowledge discovery and data mining. Springer",
            "year": 2018
        },
        {
            "authors": [
                "S.H. Hong",
                "S. Ryu",
                "J. Lim"
            ],
            "title": "and W",
            "venue": "Y. Kim, \u201cMolecular generative model based on an adversarially regularized autoencoder,\u201d Journal of chemical information and modeling, vol. 60, no. 1, pp. 29\u201336",
            "year": 2019
        },
        {
            "authors": [
                "A. Albert",
                "E. Strano",
                "J. Kaur"
            ],
            "title": "and M",
            "venue": "Gonz\u00e1lez, \u201cModeling urbanization patterns with generative adversarial networks,\u201d in IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium. IEEE",
            "year": 2018
        },
        {
            "authors": [
                "Y. Bai",
                "W. Chen",
                "J. Chen"
            ],
            "title": "and W",
            "venue": "Guo, \u201cDeep learning methods for solving linear inverse problems: Research directions and paradigms,\u201d Signal Processing, p. 107729",
            "year": 2020
        },
        {
            "authors": [
                "S. Arridge",
                "P. Maass"
            ],
            "title": "O",
            "venue": "\u00d6ktem, and C.-B. Sch\u00f6nlieb, \u201cSolving inverse problems using data-driven models,\u201d Acta Numerica, vol. 28, pp. 1\u2013174",
            "year": 2019
        },
        {
            "authors": [
                "Y.-J. Cao",
                "L.-L. Jia",
                "Y.-X. Chen",
                "N. Lin",
                "C. Yang",
                "B. Zhang"
            ],
            "title": "Z",
            "venue": "Liu, X.-X. Li, and H.-H. Dai, \u201cRecent advances of generative adversarial networks in computer vision,\u201d IEEE Access, vol. 7, pp. 14 985\u201315 006",
            "year": 2018
        },
        {
            "authors": [
                "S.-W. Park",
                "J.-S. Ko",
                "J.-H. Huh",
                "J.-C. Kim"
            ],
            "title": "Review on generative adversarial networks: Focusing on computer vision and its applications,",
            "venue": "Electronics, vol. 10,",
            "year": 2021
        },
        {
            "authors": [
                "C. Yangjie",
                "J. Lili",
                "C. Yongxia",
                "L. Nan"
            ],
            "title": "and L",
            "venue": "Xuexiang, \u201cReview of computer vision based on generative adversarial networks,\u201d Journal of Image and Graphics, vol. 23, no. 10, pp. 1433\u20131449",
            "year": 2018
        },
        {
            "authors": [
                "X. Yi",
                "E. Walia"
            ],
            "title": "and P",
            "venue": "Babyn, \u201cGenerative adversarial network in medical imaging: A review,\u201d Medical image analysis, vol. 58, p. 101552",
            "year": 2019
        },
        {
            "authors": [
                "Z. Jiang",
                "S. Zhang",
                "C. Turnadge"
            ],
            "title": "and T",
            "venue": "Xu, \u201cCombining autoencoder neural network and bayesian inversion algorithms to estimate heterogeneous fracture permeability in enhanced geothermal reservoirs,\u201d Earth and Space Science Open Archive, p. 19",
            "year": 2019
        },
        {
            "authors": [
                "N.T. M\u00fccke",
                "B. Sanderse",
                "S. Boht\u00e9"
            ],
            "title": "and C",
            "venue": "W. Oosterlee, \u201cMarkov chain generative adversarial neural networks for solving bayesian inverse problems in physics applications,\u201d arXiv preprint arXiv:2111.12408",
            "year": 2021
        },
        {
            "authors": [
                "A. Bora",
                "A. Jalal",
                "E. Price"
            ],
            "title": "and A",
            "venue": "G. Dimakis, \u201cCompressed sensing using generative models,\u201d in International Conference on Machine Learning. PMLR",
            "year": 2017
        },
        {
            "authors": [
                "M. Holden",
                "M. Pereyra",
                "K.C. Zygalakis"
            ],
            "title": "Bayesian imaging with data-driven priors encoded by neural networks: Theory",
            "venue": "methods, and algorithms,\u201d arXiv preprint arXiv:2103.10182",
            "year": 2021
        },
        {
            "authors": [
                "A. Tripp",
                "E. Daxberger"
            ],
            "title": "and J",
            "venue": "M. Hern\u00e1ndez-Lobato, \u201cSample-efficient optimization in the latent space of deep generative models via weighted retraining,\u201d Advances in Neural Information Processing Systems, vol. 33",
            "year": 2020
        },
        {
            "authors": [
                "S.A. Hussein",
                "T. Tirer"
            ],
            "title": "and R",
            "venue": "Giryes, \u201cImage-adaptive gan based reconstruction,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 04",
            "year": 2020
        },
        {
            "authors": [
                "R. Sood",
                "B. Topiwala",
                "K. Choutagunta",
                "R. Sood"
            ],
            "title": "and M",
            "venue": "Rusu, \u201cAn application of generative adversarial networks for super resolution medical imaging,\u201d in 2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA). IEEE",
            "year": 2018
        },
        {
            "authors": [
                "S. Bhadra",
                "W. Zhou"
            ],
            "title": "and M",
            "venue": "A. Anastasio, \u201cMedical image reconstruction with image-adaptive priors learned by use of generative adversarial networks,\u201d in Medical Imaging 2020: Physics of Medical Imaging, vol. 11312. International Society for Optics and Photonics",
            "year": 2020
        },
        {
            "authors": [
                "Q. Liu",
                "J. Xu",
                "R. Jiang"
            ],
            "title": "and W",
            "venue": "H. Wong, \u201cDensity estimation using deep generative neural networks,\u201d Proceedings of the National Academy of Sciences, vol. 118, no. 15",
            "year": 2021
        },
        {
            "authors": [
                "H.C. Andrews",
                "B.R. Hunt"
            ],
            "title": "Digital image restoration",
            "venue": "Prentice-Hall",
            "year": 1977
        },
        {
            "authors": [
                "L. Deng"
            ],
            "title": "The mnist database of handwritten digit images for machine learning research,",
            "venue": "IEEE Signal Processing Magazine, vol. 29,",
            "year": 2012
        },
        {
            "authors": [
                "G.M. Boynton",
                "S.A. Engel",
                "G.H. Glover"
            ],
            "title": "and D",
            "venue": "J. Heeger, \u201cLinear systems analysis of functional magnetic resonance imaging in human v1,\u201d Journal of Neuroscience, vol. 16, no. 13, pp. 4207\u20134221",
            "year": 1996
        },
        {
            "authors": [
                "M. Richard",
                "M.Y.-S. Chang"
            ],
            "title": "Fast digital image inpainting,",
            "venue": "Appeared in the Proceedings of the International Conference on Visualization, Imaging and Image Processing (VIIP",
            "year": 2001
        },
        {
            "authors": [
                "S. A"
            ],
            "title": "Carasso, \u201cLinear and nonlinear image deblurring: A documented study,",
            "venue": "SIAM journal on numerical analysis,",
            "year": 1999
        },
        {
            "authors": [
                "C. Shorten",
                "T.M. Khoshgoftaar"
            ],
            "title": "A survey on image data augmentation for deep learning,",
            "venue": "Journal of Big Data,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "Q. Pan",
                "S. Wang",
                "H. Peng",
                "T. Yang"
            ],
            "title": "and E",
            "venue": "Cambria, \u201cDisentangled variational auto-encoder for semi-supervised learning,\u201d Information Sciences, vol. 482, pp. 73\u201385",
            "year": 2019
        },
        {
            "authors": [
                "C.P. Burgess",
                "I. Higgins",
                "A. Pal",
                "L. Matthey",
                "N. Watters",
                "G. Desjardins"
            ],
            "title": "and A",
            "venue": "Lerchner, \u201cUnderstanding disentangling in \u03b2-vae,\u201d arXiv preprint arXiv:1804.03599",
            "year": 2018
        },
        {
            "authors": [
                "T. Karras",
                "S. Laine",
                "M. Aittala",
                "J. Hellsten",
                "J. Lehtinen"
            ],
            "title": "and T",
            "venue": "Aila, \u201cAnalyzing and improving the image quality of stylegan,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2020
        },
        {
            "authors": [
                "G. Casella"
            ],
            "title": "An introduction to empirical bayes data analysis,",
            "venue": "The American Statistician,",
            "year": 1985
        },
        {
            "authors": [
                "C.N. Morris"
            ],
            "title": "Parametric empirical bayes inference: theory and applications,",
            "venue": "Journal of the American statistical Association,",
            "year": 1983
        },
        {
            "authors": [
                "L. Calatroni",
                "C. Cao"
            ],
            "title": "J",
            "venue": "C. De Los Reyes, C.-B. Sch\u00f6nlieb, and T. Valkonen, \u201cBilevel approaches for learning of variational imaging models,\u201d Variational Methods: In Imaging and Geometric Control, vol. 18, no. 252, p. 2",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "keywords: Bayesian inference, Asymptotic properties of parametric estimators, Generative models, Machine learning, Laplace approximation"
        },
        {
            "heading": "1 Introduction",
            "text": "Inverse problems are ubiquitous and statistical methods for their treatment have been developed for a long time [1, 2, 3]. Frequently, inverse problems are considered in a discretized form, resulting in (large-scale) regression tasks, or they are posed as discrete inverse problems from the start, for example in functional MR imaging [4, 5]. Inverse problems are often ill-posed, or ill-conditioned in the discrete case, and reliable estimation requires some form of regularization such as Tikhonov regularization [6]. Bayesian inference [7, 8] provides an alternative approach to ill-posed inverse problems, in which the employed prior renders the estimation task well-posed. For example, a Gaussian prior can lead to a maximum a posteriori (MAP) estimate equivalent to the result of a Tikhonov regularization. Gaussian Markov random field (GMRF) priors [9] are another popular class of priors used. For example, to model a priori spatial smoothness as it is often relevant in spatial modeling or image processing.\nWhile these analytic priors have been successfully applied in many applications, they do not always adequately model the prior knowledge available in an individual problem. For example, in quantitative MR imaging of the brain employed conventional prior distributions do not truly reflect the structure of the brain [10]. Generative models from machine learning using modern architectures such as generative adversarial networks\nar X\niv :2\n20 3.\n07 75\n5v 2\n[ st\nat .M\nL ]\n7 N\nov 2\n02 2\n(GANs) [11] or variational auto-encoders (VAEs) [12], on the other hand, have proven to generate data that closely resemble the properties of data in a training set. Their efficiency, adaptability and easy accessibility in standard libraries result in a multitude of applications. From speech synthesis [13] over text generation [14] to molecule generation [15] and urbanization [16], to mention but just a few. For a comprehensive overview on the usage of deep learning methods for the solution of linear inverse problems we refer to [17], and for a review on the application of data-driven models to [18]. For an overview of generative models in computer vision see for instance [19, 20, 21], and for a review in medical imaging cf. [22].\nIn view of their capabilities, generative models have recently been considered as priors in a Bayesian treatment of inverse problems, cf., e.g., [18]. The advantage of such proceeding is that individual properties of the problem at hand, such as the structure of brain images, are adequately taken into account [10], which leads to an improved inference. Another advantage is that often the sought (discrete) function or field belongs to a low-dimensional manifold, which is exploited by current generative models such as GANs or VAEs. Then, the inference can be carried out in a low-dimensional space of latent variables, which strongly facilitates the calculation of the results in a Bayesian inference, cf. [23, 24, 25, 26].\nHowever, restricting the inference to a low-dimensional latent space has the disadvantage that it admits no Lebesgue density for the posterior in the high-dimensional space of the actual variables [27]. Furthermore, the estimation accuracy reached can strongly depend on the quality of the generative model [28]. To circumvent this drawback and somehow escape from the lower dimensional manifold, [27] takes the mean of the push-forward of the posterior in the latent space as a Bayes estimate. Some novel approaches also incorporate the inversion problem directly into the learning process [29], e.g. to achieve super resolution or to reconstruct high fidelity magnetic resonance images [30, 31]. In this regard, we like to mention [10] for their deep direct estimation procedure and the development of a conditional Wasserstein GAN discriminator, which allows sampling from the posterior.\nIn this paper, we consider a Bayesian treatment of linear Gaussian inverse problems that is carried out in the high-dimensional original variable space. The employed prior is determined by a probabilistic generative model. We propose an analytic solution to this task based on a Laplace approximation of the prior which is inspired by a treatment of [32] in the context of density estimation. The choice of the class of considered inverse problems is made in view of tasks such as inpainting, restoration of images [33] or medical imaging [31, 34]. Another motivation for our choice of problems is that it allows analytical results to be derived.\nThe properties of the proposed inference such as, for example, consistency, are explored and contrasted to those obtained for the inference which is carried out in the low-dimensional latent space. In addition, an image restoration task for blurred and noisy MNIST data [35] is taken to quantitatively compare the two approaches. The comparison is augmented by a simple heuristic for the choice of method. Pros and cons of both approaches are discussed in view of our results.\nThe paper is organized as follows. In Section 2 the considered class of inverse problems is specified and generative models are introduced. The Bayesian treatment of the inverse problems utilizing a generative model as a prior is then considered in Section 3. After recalling the inference utilizing the low-dimensional latent space and characterizing its properties, the proposed approach using a probabilistic generator is developed and explored. Subsequently, the two approaches are discussed and assessed in terms of their properties. Our treatment assumes knowledge about the variance of the observations, and finally, we briefly note on the possible generalization to the case of unknown variance. In Section 4 numerical examples are presented for a quantitative assessment. An outlook on potential future research and conclusions from our findings are given in Section 5."
        },
        {
            "heading": "2 Generative models and considered class of inverse problems",
            "text": "This section specifies the considered class of inverse problems and introduces generative models. The generative models will later be used for the construction of a prior in a Bayesian treatment of the inverse problems. Two classes of generative models are distinguished: one that uses a deterministic map applied to the latent variables and a probabilistic approach. The former will be taken to carry out a Bayesian inference in a low-dimensional latent space, the latter serves as the starting point for the proposed inference carried out directly in the\nhigh-dimensional space of the actual variables."
        },
        {
            "heading": "2.1 Linear inverse problems",
            "text": "We consider linear inverse problems of the form\ny|x \u223c N (Ax, \u03c32I), (1)\nwhere A denotes some given operator mapping from the (original) space X to the space of observables Y . We take X = Rd and Y = Rn so that A is an n by d matrix. We assume throughout that A has full rank and n \u2265 d. The goal is to infer x given the data y. The variance \u03c32 is mostly treated as a fixed given parameter and is largely suppressed in our notation, except for those cases in which it is explicitly included in the inference.\nProblems of the form (1) typically emerge from the discretization of a continuous inverse problem in which x could be a spatially distributed property of the human body or an image that has been blurred. Usually, the dimension of the (discretized) x will be high and the linearity assumption simplifies matters. The assumption made about the structure of the covariance matrix in the sampling distribution (1) essentially means that the full covariance matrix needs to be known up to a factor, a situation which, after a suitable linear transformation, yields a problem of the form in (1). We note that simple models of the form (1) are relevant in many applications, for example functional MR imaging [36], inpainting [37] or de-blurring of images [38]."
        },
        {
            "heading": "2.2 Generative models",
            "text": "Generative models such as VAEs or GANs can produce random samples by transforming a simple distribution of latent variables, e.g. a multivariate standard Gaussian distribution, through a neural network. Those networks are trained on a (large) database in such a way that the resulting distribution approximates the distribution underlying the employed database. The database could for instance consist of a training set of images with a specific characteristic for the task at hand, e.g. handwritten digits from the MNIST data set or a sequence of MR images from various patients. Often, such data sets can be modeled as belonging to a low-dimensional manifoldM\u2282 X which is exploited by the generative models through a correspondingly chosen low dimension of the latent variables.\nIn Figure 1 we depict the considered structure of the generative model. A latent vector z \u2208 Z := Rp of lower dimension p d is mapped by the generator to the original variable space X . Throughout this work,\nwe assume that such a generator or generative model is available and that it has already been trained on a data set, together with a multivariate standard Gaussian distribution \u03c0(z) = N(z|0, I) as prior for the latent variables. In literature, there are usually two types of generator outputs considered. The deterministic map, subsequently denoted by g : X \u2192M, and the probabilistic formulation. For the probabilistic formulation, we assume a Gaussian model according to\n\u03c0(x|z) = N (x|g(z),\u0393(z)), (2)\nwhere g(z) and \u0393(z) denote the outputs of a trained (deep) neural network for input z. The covariance matrix is modeled as one of the following variants.\n\u0393(z) =  \u03bb(z)\u22121I, constant with precision \u03bb(z) > 0 diag(\u03b31(z), . . . , \u03b3d(z)), diagonal with variances \u03b3i(z) > 0, i = 1, . . . , d \u0393(z), full covariance matrix in Rd,d.\n(3)\nThe motivation for the use of generative models for the data-driven construction of a prior is that these models are extremely versatile and capable to produce a distribution whose realizations closely resemble those of the database used to train them, cf. the randomly produced digits produced by a trained generative model in Figure 1 which resemble the properties of the MNIST digit database. When a (large) database is available whose members represent typical features of the solution of a considered inverse problem, a prior constructed by a generative model trained on that database can be highly informative and beneficial for a Bayesian solution to that problem. A conventional prior such as a standard Gaussian prior (Figure 1 top row) or a GMRF prior (Figure 1 middle row), used to turn an inference into a well-posed problem and exploiting the prior knowledge of smoothness, on the other hand, will generally be much less informative, for example when considering the task to infer a digit from a blurred image of it. In fact, realizations of such a prior will not even approximately resemble a digit. Supplying a large database for a particular problem often is challenging, and techniques such as data augmentation [39] or virtual experiments are used in this context. However, these issues are beyond the scope of this paper."
        },
        {
            "heading": "3 Bayesian inference using generative models",
            "text": "A Bayesian inference is considered for inverse problems of the form (1) when using a prior that is constructed from a generative model. We start by recalling an inference in latent space, followed by a push-forward through the deterministic mapping of the generator, as proposed in [27, 25]. Then an inference procedure is developed that works directly in the space X of the actual variables by using a prior constructed from a probabilistic formulation of the generator. Finally, pros and cons of both approaches are discussed in terms of the inferential properties derived for them."
        },
        {
            "heading": "3.1 Inference in latent space",
            "text": "This approach models the data from (1) by\ny|z \u223c N (Ag(z), \u03c32I), (4)\nwhere g denotes the deterministic mapping of the employed generative model and the variance \u03c32 > 0 is assumed to be known. In using the prior\n\u03c0(z) = N (z|0, I) (5)\nof the generative model, the posterior in latent space is proper and given by\n\u03c0(z|y) \u221d exp ( \u2212 12\u03c32 \u2016Ag(z)\u2212 y\u2016 2 2 ) \u03c0(z). (6)\nThen, a subsequent change of variables using the deterministic map g generates a distribution in X . By the fairly general form and usually nonlinear, non-invertible structure of g, the posterior in latent space has no closed form. A similar approach is pursuit in [23] using a deterministic Auto-Encoder and in [27] using a VAE with deterministic decoder. The latter reference correctly argues that the g push-forward of the posterior in latent space does not admit a Lebesgue density in the variable space X . We summarize these properties in the following lemma.\nLemma 1 (Degenerated push-forward). Assume that the mapping g satisfies some (weak) regularity conditions, for example continuity. Then the posterior in latent space (6) is proper and has finite p-th moment for p <\u221e. The g push-forward of \u03c0(z|y) is a distribution in X which admits no Lebesgue density.\nProof. Propriety and the existence of moments for the posterior in latent space follows directly by 1 being an upper bound for the continuous likelihood exponent and the existence of every p-th moment p <\u221e for the Gaussian prior \u03c0(z). The existence of no Lebesgue density on X follows from the fact that g maps all probability mass toM which is assumed to have lower dimensionality than X .\nEven though the g push-forward of \u03c0(z|y) does not admit a Lebesgue density, it is a well-defined distribution. To efficiently compute statistics of the g push-forward of the latent posterior, the authors in [27] employ a parallel tempered, preconditioned Crank-Nicolson MCMC to generate samples {zi}Ni from the posterior in latent space Z to subsequently approximate the posterior expectation\nE\u03c0(z|y)[g(z)|y] \u2248 1 N N\u2211 i=1 g(zi). (7)\nThis expectation does generally not belong to the image space of the generator map, but it can be expected to be close to it. Alternatively, one can numerically compute the MAP zMAP of the posterior (6) in the latent space and then use g(zMAP) as an estimate. This choice directly illuminates the limitations of the approach. By construction, g(zMAP) is limited to the image space of the generator map. This observation is a result of an inherent bias which is introduced by the statistical model (4) compared to the model (1). Consequently, arguing from a frequentist point-of-view, the latent space approach suffers from consistency issues. In particular, the posterior mean estimate in (7) viewed in dependence on the observation y takes the role of an Bayes estimator for the true value x \u2208 X under L2 loss. This estimator is usually referred to the minimum mean square error (MMSE) estimator. This interpretation allows us to formulate the following lemma as the main result regarding consistency in the latent space approach.\nLemma 2 (Inconsistency of Bayes estimator). Let g\u0302(z) = g\u0302(z)(y) = E\u03c0(z|y)[g(z)|y] denote the MMSE estimator and consider {[g\u03c3(z)}\u03c3 explicitly dependent on the data variance of the sampling distribution (4). Furthermore, assume that g is continuous and such that the model (4) is identifiable. Let x \u2208 X and assume x is not contained in the image space of the generator map, i.e., there exists a \u03b4 > 0 such that minz\u2208Z \u2016x \u2212 g(z)\u2016 \u2265 \u03b4. Then, the estimator [g\u03c3(z) is not consistent, i.e., [g\u03c3(z) does not converge to x in probability as \u03c3 \u2192 0.\nProof. By forming an identifiable model and having a posterior in latent space such as (6) implies\nA[g(z)\u03c3 p\u2212\u2192 \u03c3\u21920 Ag(z0), i.e., for all > 0, P(\u2016A[g\u03c3(z)\u2212Ag(z0)\u20162 > ) \u2212\u2192 0, (8)\nwhere z0 = arg minz\u2208Z \u2016Ax\u2212Ag(z)\u20162. Since A is full rank, it follows [g\u03c3(z) p\u2212\u2192\n\u03c3\u21920 g(z0) and with the assumption\n\u2016x\u2212 g(z0)\u2016 \u2265 \u03b4 > 0 the claim follows. In particular, for < \u03b4 it holds P(\u2016[g\u03c3(z)\u2212 x\u20162 > ) 9 0.\nRemark 3. Taking the limit with respect to \u03c3 for the considered linear inverse problem is equivalent to taking the limit with respect to infinitely many repeated observations of y. This can be seen by the fact that the mean of y in the likelihood is a sufficient statistic with variance \u03c32/k, where k is the number of repetitions.\nRemark 4. The result above generalizes to more general estimators defined by a possible different loss, which induces a different topology in X .\nHaving the asymptotic behavior of the estimator established, we are also interested in the asymptotic covariance of the g push-forward of the latent posterior.\nLemma 5 (Asymptotic covariance). Assume that g is continuous and renders the model (4) identifiable. Then, there exists z\u2217 \u2208 Z such that the latent posterior (6) converges in total variation norm to the Dirac measure centered at z\u2217. Moreover, assume that g is totally differentiable in z\u2217. Then, the asymptotic covariance of the g push-forward of the latent space posterior is given by the inverse of the Fisher information matrix at z\u2217\nC\u030c = Jz\u2217 ( \u03c3\u22122JTz\u2217A TAJz\u2217 )\u22121 JTz\u2217 , (9)\nwhere Jz\u2217 denotes the Jacobi matrix of g evaluated at z = z\u2217.\nProof. The model (4) and the latent posterior (6) fulfill the assumptions of the Bernstein-von-Mises theorem. Hence, the convergence of the posterior to a Gaussian can be assessed and C\u030c follows by simple calculus. Taking the second derivative of the log-likelihood and using that the gradient of the log-likelihood is zero at z\u2217 directly gives the asymptotic covariance of the latent posterior C1 = ( \u03c3\u22122JTz\u2217A TAJz\u2217 )\u22121\n. A subsequent linearization of g yields the claim that C\u030c = Jz\u2217C1JTz\u2217 ."
        },
        {
            "heading": "3.2 Inference in original space",
            "text": "As an alternative, we propose to perform the inference in the original space X . We introduce an efficient way of solving the inverse problem by introducing a Laplace approximation for the prior.\nThis approach uses the actual data model (1) and employs the hierarchical prior\nx|z \u223c N (g(z),\u0393(z)), z \u223c N (0, I), (10)\nwhere g and \u0393 are given by (deep) neural networks, cf. Section 2.2. Throughout this work, we assume that \u0393(z) is positive definite and its smallest eigenvalue is bounded away from zero, for every z \u2208 Z. Then, the resulting posterior is given by\n\u03c0(x|y) \u221d exp ( \u2212 12\u03c32 \u2016Ax\u2212 y\u2016 2 2 ) \u03c0(x), (11)\nand we collect some of its properties in the following lemma.\nLemma 6 (Variable space posterior is proper). The posterior (11) is proper, has finite p\u2212th moment for p <\u221e and fulfills the assumptions of the Bernstein-von-Mises theorem.\nProof. Observe that the prior\n\u03c0(x) = \u222b Z \u03c0(x|z)\u03c0(z)dz \u221d \u222b Z |\u0393(z)|\u22121/2 exp ( \u2212(x\u2212 g(z)) T\u0393(z)\u22121(x\u2212 g(z)) 2 \u2212 zT z 2 ) dz (12)\nis proper. By \u0393(z) being positive definite, the first part of the exponent is bounded. Since \u0393(z) has its smallest eigenvalue bounded away from zero, also the determinant is bounded. Then, propriety of \u03c0(x) follows by propriety of \u03c0(z). By the choice of a (bounded) Gaussian prior for \u03c0(z), also \u03c0(x) is bounded. Then, the remaining claims follow from standard theory of Bayesian inference for linear models.\nAn immediate consequence of the previous lemma is the fact that the MMSE estimator derived from the posterior (11) is consistent. Beside this obvious statistical advantage, unfortunately, the prior is computationally infeasible, since for every evaluation an integral has to be solved.\nTherefore, we make use of a linearization. A similar approach in this context has been applied for density estimation [32]. Here, a Laplace approximation is suggested to render the intractable prior \u03c0(x) feasible. The Laplace approximation applies a linearization to the generator mean by taking\ng(z) \u2248 g(z0) + Jz0(z \u2212 z0) (13)\nfor some expansion point z0, which has to be determined previously, and the Jacobian Jz0 \u2208 Rd,p of the generator mean at z0. Similarly, we have to expand the covariance matrix \u0393(z) around z0. With the argument that the variance is expected to be less volatile than the mean, it is justifiable to do a constant expansion, only. This means \u0393(z) \u2248 \u0393(z0). Higher order expansions are in general possible, since they require higher order derivatives of the mean and covariance function with respect to z which are feasible due to automatic differentiation, but the resulting posterior becomes quite unapparent.\nInserting the expansions into the prior allows for an analytic representation of \u03c0(x) and the prior PDF can be expressed as follows.\nLemma 7 (Laplace approximated prior). For z0 \u2208 Z, the prior \u03c0(x) from the hierarchical model (10) is approximated by a Gaussian distribution\n\u03c0(x) \u2248 \u03c0L(x) = N ( x|g(z0)\u2212 Jz0z0,\u0393(z0) + Jz0JTz0 ) . (14)\nThe covariance matrix \u0393(z0) + Jz0JTz0 is positive definite and thus, the prior is proper on X .\nProof. Simple calculus yields the form of \u03c0L(x) and the remaining claims follow directly from the assumption that \u0393(z0) is positive definite for every z0 \u2208 Z.\nThe prior \u03c0L(x) is a sensible choice for solving the inverse problem and computations can be carried out analytically. Moreover, the constructed approximation will preserve important properties in the fashion of Lemma 6. We formulate this claim in the following theorem.\nTheorem 8. Using the Laplace approximation \u03c0L(x) as a prior for the Bayesian inverse problem yields a Gaussian distribution as posterior on X\n\u03c0L(x|y) = N(x|x\u0302, S\u0302) (15)\nwith covariance matrix S\u0302 = ( \u03c3\u22122ATA+ ( \u0393(z0) + Jz0JTz0 )\u22121)\u22121 (16)\nand mean x\u0302 = S\u0302 [ \u03c3\u22122AT y + ( \u0393(z0) + Jz0JTz0 )\u22121 (g(z0)\u2212 Jz0z0) ] . (17)\nFor this posterior, moments of arbitrary order exist and \u03c0L converges, as \u03c3 \u2192 0, to a Gaussian centered at the maximum likelihood estimate (ATA)\u22121AT y, with covariance given by the inverse of the Fisher information matrix \u03c3\u22122ATA, for the linear model (1).\nProof. The mean and covariance are results of simple calculations for linear Gaussian models and priors. The remaining properties are clear for this Gaussian posterior. The convergence can be assessed by the Bernstein-von-Mises theorem. In particular, the posterior mean converges in probability to the maximum likelihood estimate and the covariance converges to the inverse of the Fisher information matrix of the linear model (1).\nRemark 9. It can be seen that the asymptotic covariance of the Laplace approximated posterior \u03c0L(x|y) is given by\nC\u0302 = ( \u03c3\u22122ATA )\u22121 . (18)\nBut, a quantitative comparison to the asymptotic covariance of the g push-forward latent space posterior\nC\u030c = Jz\u2217 ( \u03c3\u22122JTz\u2217A TAJz\u2217 )\u22121 JTz\u2217 (19)\nis not trivial. In general, it is not possible to conclude that C\u0302 is \u201clarger\u201d than C\u030c in some sense. In fact, numerical considerations yield that the matrix C\u0302 \u2212 C\u030c is often indefinite. However, it is unambiguous from their structure that C\u030c spans a linear space of dimension at most p. In contrast, the covariance C\u0302 spans the whole space X of dimension d.\nIn the end, we like to discuss the choice of the expansion point z0 for the Laplace approximation. A natural choice is to take the expansion point from the tuple (z0, x0) which maximizes the integrand of the prior integral, i.e., the joint prior \u03c0(x, z) = \u03c0(x|z)\u03c0(z). As long as x \u2248 x0, the approximation (11) is reasonable. If this condition is violated, both the true prior (10) and its approximation (14) are (expected to be) small, although their relative difference could be large. However, achieving this optimum in the high-dimensional space Z \u00d7X is numerically challenging and an optimizer is likely to get stuck in local optima. In our experiments, we achieved distinguished results using an efficient updating scheme which we will describe in the following.\n1. Choose as initial value in X the solution to the linear least-squares problem\nx0 = arg min x\u2208X \u03c3\u22122\u2016Ax\u2212 y\u201622 + \u2016x\u2212 g(0)\u20162\u0393(0)\u22121 (20)\nand for the latent vector take z0 = f(x0), where f denotes the encoder mean map of the employed VAE. For more general generative models, a numerical optimization for z0 can be performed which aims for the closest element in the latent space, which generates x0. Since the consideration of variants of (multi-level) latent space approaches, disentanglement or style transfer are well outside the scope of this paper, we refer to [40, 41, 42]\n2. Then, consider the log-integrand\n\u03c00(x0, z0) := logN(x0|g(z0),\u0393(z0)) + logN(z0|0, I) (21)\n\u221d \u221212(z 0)T z0 \u2212 12 log(|\u0393(z 0)|)\u2212 12(x0 \u2212 g(z 0))T\u0393(z0)\u22121(x0 \u2212 g(z0))\n= \u221212 [ log(|\u0393(z0)|) + (z0)T z0 + (x0 \u2212 g(z0))T\u0393(z0)\u22121(x0 \u2212 g(z0)) ] 3. Evaluate the new z1 by maximizing the integrand\n(I + JTz0\u0393(z 0)\u22121Jz0)z1 = JTz0\u0393(z 0)\u22121(x0 \u2212 g(z0) + Jz0z0) (22)\n4. If \u03c00(x0, z1) > \u03c00(x0, z0) take the new value z1 as expansion point candidate.\n5. repeat step 3 and 4 until no further improvement is achieved.\nNote that, the choice of x0 is taken in view of the data y. This, to some extend, renders the approach empirical Bayesian [43, 44]."
        },
        {
            "heading": "3.3 Discussion",
            "text": "Recapitulating the previous sections, we compared two Bayesian inference problems based on the original space formulation (1) and a formulation in latent space (4). In literature, it is common to consider the latent space approach which simplifies the required computations to a lower dimensional space and quantities thereof are more feasible to estimate. However, this comes at the cost of an inherent bias introduced by the employed approximate statistical model. From a statistical point-of-view, it would be beneficial to be able to establish consistent Bayes estimators which asymptotically yield correct estimates also outside of the range of the\ngenerator. Such a procedure based on the original space formulation is however numerically challenging. One possible approximative scheme is presented in the form of a Laplace approximation. This method inherits and preserves the consistency analysis of the original space approach, while being numerically feasible. In this regard, the presented approximation is expected to yield preferable solutions to the inverse problem when the information contained in the data is large."
        },
        {
            "heading": "3.4 Generalization to unknown variance",
            "text": "The natural extension of our analysis to the case of unknown variance is straightforward. We exemplary demonstrate this for the choice of an Inverse Gamma (IG) prior distribution for the variance, i.e. \u03c0(\u03c32) \u221d (\u03c32)\u22121\u2212\u03b1 exp(\u2212\u03b2/\u03c32) with shape and scale hyperparameter \u03b1, \u03b2 > 0. This often employed prior models the positive variance in a flexible manner. In fact, \u03b1 and \u03b2 can be chosen such that \u03c0(\u03c32) has no finite moment. Moreover, the explicit form allows for an analytic derivation of certain quantities of interest.\nFor the inference in latent space the hierarchical model reads\ny|z, \u03c32 \u223c N (Ag(z), \u03c32I), z \u223c \u03c0(z) = N (z|0, I),\n\u03c32|\u03b1, \u03b2 \u223c \u03c0(\u03c32) = IG(\u03c32|\u03b1, \u03b2).\nThe resulting posterior is given by\n\u03c0(z, \u03c32|y) \u221d ( \u03c32 )\u2212n/2 exp ( \u2212 12\u03c32 \u2016Ag(z)\u2212 y\u2016 2 2 \u2212 1 2\u2016z\u2016 2 2 ) \u03c0(\u03c32). (23)\nTo obtain the marginal posterior for z, the variance needs to be integrated out. This marginalization can be done analytically, which yields\n\u03c0(z|y) \u221d \u222b \u221e\n0\n( \u03c32 )\u2212n/2\u22121\u2212\u03b1 exp ( \u2212\u2016Ag(z)\u2212 y\u2016 2 2 + \u03b2\n2\u03c32\n) d\u03c32\u03c0(z) \u221d { \u2016Ag(z)\u2212 y\u201622 + \u03b2 }\u2212n+2\u03b12 \u03c0(z). (24)\nThe marginal posterior is proper, since for \u03b2 > 0 the term in the brackets is bounded away from zero. Hence, the existence of moments of arbitrary order is guaranteed by the choice of a standard Gaussian distribution for \u03c0(z). For this marginal posterior, the claims of Lemma 1 and Lemma 2 follow directly with the same arguments1. This implies that, even in the case of unknown variance, the g push-forward of the marginal posterior is inconsistent in the sense of derived Bayes estimators.\nFor the sampling distribution (1) in the variable space X and the hierarchical prior of Section 3.2, a similar marginal posterior can be derived. In particular, the hierarchical model\ny|x, \u03c32 \u223c N (Ax, \u03c32I), x|z \u223c N (g(z),\u0393(z)), z \u223c \u03c0(z) = N (z|0, I),\n\u03c32|\u03b1, \u03b2 \u223c \u03c0(\u03c32) = IG(\u03c32|\u03b1, \u03b2),\nyields the marginal posterior\n\u03c0(x|y) \u221d { \u2016Ax\u2212 y\u201622 + \u03b2 }\u2212n+2\u03b12 \u03c0(x), (25) where the prior \u03c0(x) is derived as in Section 3.2. Again, due to \u03b2 > 0, the same arguments of Lemma 6 apply, which renders Bayes estimators based on \u03c0(x|y) consistent, and \u03c0(x) can still be approximated as in Lemma 7, i.e. \u03c0L(x) \u2248 \u03c0(x). However, Theorem 8, which analytically provides the posterior as a Gaussian distribution, is not directly applicable, since it relies on the fact that the posterior is given as the product of two (non-scaled) Gaussian probability density functions (PDF). Here, the posterior is slightly different and obtaining an approximative Gaussian would rely on numerical estimation of the MAP and Hessian of \u03c0(x|y).\n1Similar to Remark 3, the information limit must be understood in the sense of repeated sampling of y.\nRemark 10. Another typical choice for the prior of the variance is the non-informative Jeffrey\u2019s prior \u03c0(\u03c32) \u221d 1/\u03c32. In this case, ensuring propriety and existence of moments for the latent space posterior requires additional assumptions on g to ensure boundedness from below of the term \u2016Ag(z) \u2212 y\u20162. A similar assumption is sufficient for the variable space."
        },
        {
            "heading": "4 Numerical examples",
            "text": "To validate our theoretical findings, we perform experiments on a well-known data set with linear inverse problems governed by a blurring operation and homoscedastic Gaussian noise. In particular the data model\ny|x \u223c N (Ax, \u03c32I) (26)\nis considered with x being an unknown vector in X = R28\u00d728 and A : X \u2192 Y = R28\u00d728 denotes a linear Gaussian blurring operator with known precision parameter \u03b7 > 0 which steers the impact of the blurring. In\nFigure 2, we show the applied blurring operation. The added noise levels in the experiments are defined for \u03c3 = 10\u2212s for s = 1, 2, 3, 4."
        },
        {
            "heading": "4.1 Generative model",
            "text": "We consider a typical example from machine learning. The data set of handwritten digits (MNIST) [35] consists of 60.000 training sample and 10.000 test samples, each sample corresponds to a grayscale image of size (28, 28). A typical representative is shown in Figure 2.\nThe generative model is chosen as an extension to the decoder of a VAE in an \u201coff-the-shelf\u201d Matlab [45] architecture. The latent space is chosen as Z = R20 and the VAE is trained by optimizing the evidence lower bound (ELBO) on the training set using Adam with a constant learning rate 10\u22123 and batch-size 512 for 20 epochs. Afterwards, to obtain a probabilistic decoder, the final deconvolution layer of the decoder is extended to incorporate a diagonal covariance and the encoder is fixed during a transfer learning step of additional five epochs. Example draws of the generative model are shown in Figure 1 (bottom row) which resulted by taking the decoder output for some z \u223c N (0, I). The results shown in this work are achieved using Python and Matlab. The Python source is available under https://gitlab1.ptb.de/marsch02/datainformed-prior."
        },
        {
            "heading": "4.2 Inference with known variance",
            "text": "For a quantitative comparison of the inversion quality of the latent space approach and the Laplace approximated variable space approach, we consider a single inference result in Figure 3 by taking one image of the test set as ground truth for the data model. Then, the blurring operator with \u03b7 = 3 is applied and Gaussian noise with \u03c3 = 10\u22122 is added. Subsequently, three approaches are applied to generate reconstruction estimates.\nFor the latent space approach of Section 3.1, a BFGS optimizer is used to numerically compute the MAP of the latent space posterior \u03c0(z|y) and application of g yields the estimate. As starting point for the optimization in latent space, the solution to the least squares problem (20) is computed and the encoder mean of the VAE yields the initial guess.\nFor the Laplace approximated approach, the choice of z0 is given in Section 3.2. With this expansion point, the mean of the Gaussian posterior \u03c0L(x|y) from theorem 8 is taken as the estimate.\nAs a reference, we additionally include a solution obtained by an L2 regularized deterministic optimization of the least-squares problem\nxL2(\u03bb) = arg min x\u2208X \u2016Ax\u2212 y\u201622 + \u03bb\u2016x\u201622. (27)\nThe regularization parameter is chosen through an oracle method by finding the value for \u03bb for which xL2(\u03bb) has the smallest difference to the ground truth in L2 norm. This reference can be seen as the best possible homoscedastic Gaussian prior for the variable space approach.\nAs a quality measure, the peak-signal-to-noise ratio (PSNR) with the ground truth is assessed,i.e.,\nPSNR(x, x\u0302) = 20 log10(L)\u2212 10 log10 \u2016x\u2212 x\u0302\u201622/d. (28)\nMotivated by the normalized images of the MNIST data set, we take L = 1, which renders the PSNR a rescaling of the mean-square error. However, a larger PSNR value indicates a better reconstruction result.\nIn Figure 3 it can be seen that, the oracle L2 regularized approach is merely able to reconstruct the shape of the original digit. In contrast, the latent space approach yields an estimate, which resembles the digit 8 albeit a 0 was used as ground truth. This behavior can be explained by the closeness of the digits 0 and 8 in the latent space representation and the highly nonlinear, non-convex optimization problem which is to solve in Z. In terms of PSNR, the best reconstruction is achieved using the Laplace approach in the variable space X . Here, the blurring of the L2 regularized solution is mostly resolved and the resulting digit is not collapsed into a 8. Additionally, due to the construction of the posterior \u03c0L(x|y) and its estimate, the covariance is a byproduct. Here, we show the square root of its diagonal in Figure 3 f). Since the posterior is a Gaussian, this corresponds to the standard deviations of the marginalized posterior for each pixel.\nFor a statistical and comprehensive comparison, we now consider 100 distinct images from the test set and perform the inference on every image. The results are collected in Figure 4. For all blurring precisions, a similar behaviour of the PSNR can be observed. With decreasing noise variance, i.e. \u03c3 \u2192 0, the variable space approach, denoted \u201cLaplace\u201d, is usually to favor over the latent space approach, denoted \u201cLatent\u201d. This verifies our theoretical findings that the variable space approach is consistent. For moderately large values of \u03c3, the latent space approach is usually to favor, since the bias introduced by the generative model is small compared to the impact of the data variance. In those cases, the latent space approach is also superior to the oracle L2 regularized approach. This can be assessed for the variable space approach only for \u03c3 \u2264 0.01."
        },
        {
            "heading": "4.3 Empirical guidance",
            "text": "We have shown that the variable space approach and the employed Laplace approximation is to favor in cases where the information contained in the data is large. However, the advice to use our approach is based on an asymptotic result and in general it is difficult to assess whether the asymptotic result is already relevant. Therefore, we propose a heuristic argument based on a simple bias estimation to give guidance on the choice of the approach.\nFor given y, both approaches yield an estimate xLaplace and xLatent. Interpreting these estimates as new ground truth allows us to perform the inversion again using virtual data yLaplace and yLatent. This subsequent inference yields xLaplaceLaplace and xLatentLatent, for which the squared error to their ground truth can be assessed. The method which yields the smaller deviation is to favor. In Figure 4 we employ this heuristic approach under the label \u201cGuide\u201d. It is to observe that, using this guide yields, on average, better results than applying only one of the two approaches. Considering squared errors in a cross-validation fashion, i.e. taking xLaplace as new observation and applying the latent space approach to obtain xLatentLaplace and vice versa, yields in our experiments a slightly worse guidance."
        },
        {
            "heading": "5 Conclusions and future research",
            "text": "We presented approaches to the solution of linear Bayesian inverse problems using generative models as a prior. Furthermore, we established convergence results for a state-of-the-art inference method in latent space and contrasted them to the asymptotic behaviour of an alternative approach in the high-dimensional variable space. Such an approach in the variable space is usually intractable due to the dimensionality and the complexity of the prior representation. Therefore, we derived a novel inference technique in the variable space based on a Laplace approximation which yields an analytic posterior distribution that inherits and preserves a Bernstein-von-Mises result. An extension to the case with unknown variance is presented and numerical examples underpin our theoretical findings. Finally, we motivate an empirical guidance to choose between the presented approaches in real scenarios.\nThis proof-of-concepts work paves the way for future research tackling various questions. Extending the numerical examples to more complex and practical data sets is an important step which raises also the need for efficient numerical treatment, e.g. to tackle the involved matrix inversions. Different and more complex models for the approximation of the prior in variable space can be pursuit, e.g. using Gaussian mixture models quadrature schemes. In this regard, efficient sampling approaches in high dimension may be required to obtain samples from the posterior. Also, connections to other recent concepts, such as bilevel optimization [46] can be of interest."
        }
    ],
    "title": "Generative models and Bayesian inversion using Laplace approximation",
    "year": 2022
}