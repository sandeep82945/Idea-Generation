{
    "abstractText": "Turbulence remains a problem that is yet to be fully understood, with experimental and numerical studies aiming to fully characterise the statistical properties of turbulent flows. Such studies require huge amount of resources to capture, simulate, store and analyse the data. In this work, we present physics-informed neural network (PINN) based methods to predict flow quantities and features of two-dimensional turbulence with the help of sparse data in a rectangular domain with periodic boundaries. While the PINN model can reproduce all the statistics at large scales, the small scale properties are not captured properly. We introduce a new PINN model that can effectively capture the energy distribution at small scales performing better than the standard PINN based approach. It relies on the training of the low and high wavenumber behaviour separately leading to a better estimate of the full turbulent flow. With 0.1% training data, we observe that the new PINN model captures the turbulent field at inertial scales leading to a general agreement of the kinetic energy spectra upto eight to nine decades as compared with the solutions from direct numerical simulation (DNS). We further apply these techniques to successfully capture the statistical behaviour of large scale modes in the turbulent flow. We believe such methods to have significant applications in enhancing the retrieval of existing turbulent data sets at even shorter time intervals.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vijay Kag"
        },
        {
            "affiliations": [],
            "name": "Kannabiran Seshasayanan"
        },
        {
            "affiliations": [],
            "name": "Venkatesh Gopinath"
        }
    ],
    "id": "SP:e6287ba943f4ef7347dc589db432146acd374001",
    "references": [
        {
            "authors": [
                "V. Dallas",
                "K. Seshasayanan",
                "S. Fauve"
            ],
            "title": "Transitions between turbulent states in a two-dimensional shear flow",
            "venue": "Physical Review Fluids,",
            "year": 2020
        },
        {
            "authors": [
                "S. Dong",
                "N. Ni"
            ],
            "title": "A method for representing periodic functions and enforcing exactly periodic boundary conditions with deep neural networks",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "K. Duraisamy",
                "G. Iaccarino",
                "H. Xiao"
            ],
            "title": "Turbulence modeling in the age of data",
            "venue": "Annual Review of Fluid Mechanics,",
            "year": 2019
        },
        {
            "authors": [
                "K. Fukami",
                "K. Fukagata",
                "K. Taira"
            ],
            "title": "Machine-learning-based spatio-temporal super resolution reconstruction of turbulent flows",
            "venue": "Journal of Fluid Mechanics,",
            "year": 2021
        },
        {
            "authors": [
                "H. Gao",
                "L. Sun",
                "J.-X. Wang"
            ],
            "title": "Super-resolution and denoising of fluid flow using physics-informed convolutional neural networks without high-resolution labels",
            "venue": "Physics of Fluids,",
            "year": 2021
        },
        {
            "authors": [
                "D.O. G\u00f3mez",
                "P.D. Mininni",
                "P. Dmitruk"
            ],
            "title": "Parallel simulations in turbulent mhd",
            "venue": "Physica Scripta,",
            "year": 2005
        },
        {
            "authors": [
                "I.J. Goodfellow",
                "Y. Bengio",
                "A. Courville"
            ],
            "title": "Deep Learning",
            "year": 2016
        },
        {
            "authors": [
                "J. Herault",
                "F. P\u00e9tr\u00e9lis",
                "S. Fauve"
            ],
            "title": "1/ f \u03b1 low frequency fluctuations in turbulent flows",
            "venue": "Journal of Statistical Physics,",
            "year": 2015
        },
        {
            "authors": [
                "Z. Hu",
                "A.D. Jagtap",
                "G.E. Karniadakis",
                "K. Kawaguchi"
            ],
            "title": "When do extended physics-informed neural networks (xpinns) improve generalization",
            "year": 2021
        },
        {
            "authors": [
                "A.D. Jagtap",
                "G.E. Karniadakis"
            ],
            "title": "Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations",
            "venue": "Communications in Computational Physics,",
            "year": 2020
        },
        {
            "authors": [
                "X. Jin",
                "S. Cai",
                "H. Li",
                "G.E. Karniadakis"
            ],
            "title": "Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "G.E. Karniadakis",
                "I.G. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics,",
            "year": 2021
        },
        {
            "authors": [
                "J. Kim",
                "C. Lee"
            ],
            "title": "Deep unsupervised learning of turbulence for inflow generation at various reynolds numbers",
            "venue": "Journal of Computational Physics,",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization, 2017",
            "year": 2017
        },
        {
            "authors": [
                "M.J. Kochenderfer",
                "T.A. Wheeler"
            ],
            "title": "Algorithms for Optimization",
            "year": 2620
        },
        {
            "authors": [
                "D. Kochkov",
                "J.A. Smith",
                "A. Alieva",
                "Q. Wang",
                "M.P. Brenner",
                "S. Hoyer"
            ],
            "title": "Machine learning\u2013accelerated computational fluid dynamics",
            "year": 2021
        },
        {
            "authors": [
                "R.H. Kraichnan"
            ],
            "title": "Inertial ranges in two-dimensional turbulence",
            "venue": "The Physics of Fluids,",
            "year": 1967
        },
        {
            "authors": [
                "R. Laubscher"
            ],
            "title": "Simulation of multi-species flow and heat transfer using physics-informed neural networks",
            "venue": "Physics of Fluids,",
            "year": 2021
        },
        {
            "authors": [
                "B. Liu",
                "J. Tang",
                "H. Huang",
                "X.-Y. Lu"
            ],
            "title": "Deep learning methods for super-resolution reconstruction of turbulent flows",
            "venue": "Physics of Fluids,",
            "year": 2020
        },
        {
            "authors": [
                "D.C. Liu",
                "J. Nocedal"
            ],
            "title": "On the limited memory bfgs method for large scale optimization",
            "venue": "Mathematical Programming,",
            "year": 1989
        },
        {
            "authors": [
                "D. Lucor",
                "A. Agrawal",
                "A. Sergent"
            ],
            "title": "Simple computational strategies for more effective physics-informed neural networks modeling of turbulent natural convection",
            "venue": "Journal of Computational Physics,",
            "year": 2022
        },
        {
            "authors": [
                "G. Narasimhan",
                "C. Meneveau",
                "T.A. Zaki"
            ],
            "title": "Large eddy simulation of transitional channel flow using a machine learning classifier to distinguish laminar and turbulent regions",
            "venue": "Phys. Rev. Fluids,",
            "year": 2021
        },
        {
            "authors": [
                "A. Ng"
            ],
            "title": "Machine Learning Yearning",
            "venue": "Online Draft,",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Machine learning of linear differential equations using gaussian processes",
            "venue": "Journal of Computational Physics,",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations, 2017b",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial differential equations, 2017c",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2019
        },
        {
            "authors": [
                "M. Raissi",
                "A. Yazdani",
                "G.E. Karniadakis"
            ],
            "title": "Hidden fluid mechanics: Learning velocity and pressure fields from flow",
            "venue": "visualizations. Science,",
            "year": 2020
        },
        {
            "authors": [
                "G.I. Sivashinsky"
            ],
            "title": "Weak turbulence in periodic flows",
            "venue": "Physica D: Nonlinear Phenomena,",
            "year": 1985
        }
    ],
    "sections": [
        {
            "text": "Keywords: Deep learning; Machine learning; PINNs; Turbulence; DNS"
        },
        {
            "heading": "1. Introduction",
            "text": "Turbulent flows occur widely in many different fields, from industrial flows to naturally occurring flows such as in geophysical and astrophysical systems where energy is distributed across a continuous range of scales. Understanding and controlling turbulence is also of paramount importance in many industrial applications such as in aerospace, automotive domains, to name a few. DNS has been a mainstay for simulating turbulent flows in such systems. We generally rely upon generating large amount of data before we can understand qualitative and quantitative features of turbulence. Nevertheless, limitation of memory and computing power persists at large Reynolds numbers and studying the statistical properties of turbulence requires long duration of simulations. Experimental studies can achieve much higher Reynolds numbers and can study turbulence for longer duration, but limitations exist on acquiring velocity and pressure fields over the whole domain.\nRecently there has been a surge of data based approaches that are used for studying turbulence (Kim and Lee 2020; Kochkov et al. 2021; Taghizadeh et al. 2021; Narasimhan et al. 2021) that is gaining importance as an alternative or as an enhancement to the existing methods. A recent work by Duraisamy et al. (2019) reviews many methods based on machine learning for turbulence modelling and remarked that there will be a surge in data-driven modelling. Standard computational fluid dynamic (CFD) models, especially direct numerical simulations (DNS) demand high computational cost as they involve resolving turbulence down to its smallest scales.\nUsually data based approaches do not take into account the underlying physical laws of the system. Many a times, these data-based approaches also suffer drawbacks of sparse training data. Even techniques like Gaussian Process Regression (GPR) can face challenges when the system is highly nonlinear as observed in Raissi et al. (2017a). One such method which alleviates these drawbacks and limitations is the physics-informed neural network (PINN) which was introduced by Raissi et al. (2017b,c, 2019); Karniadakis et al. (2021). The information of the governing equations of the system along with the boundary and initial conditions are used in the definition of the loss function which the neural network tries to minimise. The effect of adding such additional information leads to the penalizing of nonphysical solutions. It has been successfully shown to perform well in recent studies (Arzani et al. 2021; Jin et al. 2021; Wang et al. 2021). Another concept of using separate PINN for each set of governing equations is also shown to perform better than a single PINN model for the entire system (Laubscher 2021). While PINN models have successfully reproduced steady or time periodic behaviour of different systems, their performance in the chaotic or turbulent regime is still not fully explored. In fact, Karniadakis et al. (2021) mentions the limitation of PINN in studying multiscale\n\u2217 Corresponding author email address: gopinath.venkatesh2@in.bosch.com\nPreprint submitted to Physics of Fluids\nar X\niv :2\n20 3.\n02 55\n5v 3\n[ ph\nys ic\ns. fl\nudy\nn] 2\n8 M\nay 2\n02 2\n2 phenomena and they suggest that new techniques are required such as xPINNs (Jagtap and Karniadakis 2020; Hu et al. 2021) to handle such problems.\nStandard PINN based methods for turbulence therefore has shown difficulty in reproducing DNS solutions, especially when the Reynolds number is high. The main difficulty arises from the necessity of large training points and neural network architectures which demand models that can run only on multi-GPU resources (Jin et al. 2021). One possible way to overcome this difficulty is to use some training data points from existing solution from experiments or CFD simulations (Arzani et al. 2021). Recently, turbulent flows (Jin et al. 2021; Lucor et al. 2022) in reduced domains have been studied using PINN based methods with a small percentage of training data. In the work of Jin et al. (2021), such a PINN model is used for solving a sub-domain of a fully turbulent channel flow problem. They clearly show that PINN can be used in such cases to sustain turbulence over long periods of time and can also reproduce some of the turbulent features. Cai et al. (2021) also use PINN to recover time evolving flow fields based on experimental data. Alternatively, PINN has also been used to reduce the amount of training data (Gao et al. 2021; Liu et al. 2020; Fukami et al. 2021) to produce super-resolution solutions of low-resolution turbulence simulation data using deep learning methods.\nThe question still remains whether PINN based methods can capture the global behaviour of turbulence. More specifically, whether such methods can resolve and capture the fluctuations across many spatial and temporal scales in a turbulent flow is unknown. Along with existing standard CFD approaches, these methods can augment them by improving the computational efficiency. With these expectations in mind, we use the methodology of PINN for studying and understanding two-dimensional turbulence. We characterize the turbulent solutions obtained through the PINN based methods and show how such methods can be used to study large scale temporal dynamics. We are interested in developing a neural network that can reproduce the flow features by providing sparse training data which comprises data from initial conditions, boundary conditions and some spatiotemporal points from the interior domain of the DNS solution, similar to the method used in Arzani et al. (2021), Lucor et al. (2022). Apart from this approach which we call PINN-1, we also introduce another way of training the neural network using spectral decomposition of training data which we name as PINN-2. We then proceed to demonstrate solutions of these PINN based methods which can satisfactorily reproduce the statistics of two-dimensional turbulent flows. We will also discuss where the results differ from DNS solutions and how such PINN based methods can be useful to study turbulent flows."
        },
        {
            "heading": "2. Methods",
            "text": ""
        },
        {
            "heading": "2.1. Physics-informed neural networks",
            "text": "With the advent of PINN (Raissi et al. 2019), the physical laws of the fluid dynamical system, namely the Navier-Stokes equations are enforced as constraints on the neural network. This has proven to be a powerful technique to obtain reasonably accurate solutions when compared to traditional CFD solvers (Raissi et al. 2019, 2020; Jin et al. 2021). They particularly do well even when the problem setup has missing boundary conditions or parameters using training data points, from experiments or simulations (Arzani et al. 2021; Lucor et al. 2022). In this work, we use the PINN framework to study turbulence in a twodimensional domain. The boundary conditions are kept periodic in this study but it can be easily extended to other types of domains and boundary conditions. Apart from its physical significance to the problem at hand, we were also motivated to look at such a boundary condition for turbulent flow in the PINN framework based on the remarks in Jin et al. (2021).\nThe general Navier-Stokes equation describing flow physics has the following structure (Batchelor 2000), written in terms of the velocity variable u in a domain \u2126 as,\n\u2202tu = M u+N (u), (1)\nwhere N is the nonlinear operator and M is the linear operator. Here we consider the Navier-Stokes equations for the incompressible fluid, where, the pressure field satisfies a Poisson equation. The pressure term is taken to a part of N (u). u(x,0) is the initial condition and the equations satisfy a set of boundary conditions denoted by BC. Using the technology of automatic differentiation via the neural network graph structure (Kochenderfer and Wheeler 2019), the linear and nonlinear terms are calculated. For accurate neural network based approximation of the solution, a loss function has to be minimized (Raissi et al. 2019; Jin et al. 2021). Although we have a well-posed problem, we take some sparse training data from the interior of the domain \u2126 as we are trying to model flows at high Reynolds number. Such flows will involve fluctuations across different spatial and temporal scales leading to a necessity to train the neural network with large initial and boundary training data. This leads to the requirement of more computational resources, for example, codes that can run on multi-GPUs (Jin et al. 2021). Hence, we try to overcome this difficulty by providing training data from the interior of the problem domain \u2126 (Lucor et al. 2022).\n3 The equation loss Leqn is given as:\nLeqn = 1\nNeqn\nNeqn \u2211 n=1 |\u2202tu(xen, ten)\u2212M u(xen, ten)\u2212N (u(xen, ten))|2, (2)\nwhere (xe, te) denote the collocation points where equation loss is calculated and Neqn denotes the total number of these collocation points. The boundary and initial condition loss functions are given as:\nLBC = 1\nNBC\nNBC \u2211 n=1 G(u(xbn, t b n )), (3)\nLini = 1\nNini\nNini \u2211 n=1 |u(xin,0)\u2212ue(xin,0)|2, (4)\nwhere (xb, tb) are collocation points on the boundaries where the boundary loss is trained and (xi, t i) are collocation points where the initial condition loss is trained. NBC denotes the number of collocation points on the boundaries and Nini denotes the number of collocation points at initial time. ue(x,0) is the initial conditions for the set of equations in (1). The function G is a measure of the deviation of the predicted solution from the imposed boundary conditions. It takes only positive real values for any velocity field. For the current problem of two-dimensional turbulence in a periodic box, the function G can be defined as follows,\nG(u(xbn, t b n )) = |u(xbn, tbn )\u2212u(xbn +Le\u0302, tbn )|2+ |\u2207u(xbn, tbn )\u2212\u2207u(xbn +Le\u0302, tbn )|2, (5)\nwhere L represents the domain length along both x,y directions and e\u0302 represents the unit vector which is along the x or the y direction depending on the boundary point xbn. The final loss that we compute is the loss data Ldata which measures the deviation of the predicted solution from the exact solution at the interior points in the domain. It is defined as,\nLdata = 1\nNdata\nNdata\n\u2211 n=1 |u(xdn , tdn )\u2212ue(xdn , tdn )|2, (6)\nwhere (xd , td) are collocation points for training the system on a sparse set of interior points and ue denotes the exact solution which is obtained either from DNS or from experimental data. Ndata is the number of collocation points for training on data of interior points. The total loss is thus given by:\nLtot = \u03bb1Lini +\u03bb2Ldata +\u03bb3LBC +\u03bb4Leqn, (7)\nwhere \u03bb1, \u03bb2, \u03bb3 and \u03bb4 are weight coefficients for the contribution from initial, data, boundary and equation losses respectively. A schematic is shown in Fig. 1, where we show the (x,y, t) domain of the flow field where the solution is computed using the PINN model. The collocation points where the different losses are computed are shown in different symbols and colors.\nThe inputs to the neural network are the spatial and temporal coordinates (x, t). According to Goodfellow et al. (2016), each network layer output is given as:\nzl = \u03c3l (zl\u22121Wl +bl) , (8)\nwhere l is the layer number, W represents the weight, b the bias and \u03c3l is the activation function. The weights and biases are optimized so as to minimize the total loss Ltot . A gradient-based optimizer is used to perform the optimization (Liu and Nocedal 1989), namely Adam optimizer (Kingma and Ba 2017) is used for the first set of iterations and then we use L-BFGS-B (Byrd et al. 1995) for the remaining iterations until the set tolerance value is reached."
        },
        {
            "heading": "2.2. Turbulence in periodic box",
            "text": "We study forced dissipative turbulence in a doubly periodic domain of dimensions (x,y)\u2208 [0,L]\u00d7 [0,L]. The governing equation written in terms of the streamfunction \u03c8 is given by,\n\u2202t\u2206\u03c8 + J(\u2206\u03c8,\u03c8) = \u03bd\u22062\u03c8\u2212\u03b1\u2206\u03c8 +\u2206 f\u03c8 , (9)\n4\nwhere f\u03c8 is the forcing which drives the fluid motion and \u03bd is the viscosity while \u03b1 is the large scale friction. J(\u2206\u03c8,\u03c8) denotes the Jacobian given by J( f ,g) = \u2202x f \u2202yg\u2212\u2202xg\u2202y f . The forcing is chosen to be the Kolmogorov forcing f\u03c8 = f0 cos ( k f y )\nwhere k f is the forcing wavenumber. Eq. 9 with a spatial forcing, viscous dissipation and a large scale damping term is typically used to study two-dimensional turbulence (Sivashinsky 1985; Boffetta and Ecke 2012; Dallas et al. 2020; Seshasayanan et al. 2021).\nThe velocity field and the vorticity field can then be calculated from the streamfunction using u = \u2207\u00d7 (\u03c8ez), \u03c9 = \u2212\u2206\u03c8 respectively. The RMS velocity is denoted by Urms and is defined as Urms = \u221a \u3008u2 (x, t)\u3009x,t where \u3008\u00b7\u3009x,t denotes the spatial and temporal averaging. The Reynolds number is defined using the RMS velocity Urms, the length scale L and the viscosity \u03bd as Re = UrmsL/\u03bd . Similarly for the large scale friction parameter \u03b1 , we can define a large scale Reynolds number Rh = Urms/(\u03b1L). Two-dimensional turbulence shows an inverse cascade of energy and a forward cascade of enstrophy (Kraichnan 1967). Thus we need to resolve both the scales above the forcing scale and the scales below the forcing scale. We will look at both global and local quantities to study the statistical properties of turbulence. The energy E(t) = U2rms(t) and the enstrophy E (t) = \u2329 (\u2207\u00d7 (\u03c8 (x, t)ez))2 \u232a x are conserved in the absence of dissipation and forcing. The two-dimensional Fourier transform of the velocity field u(x, t) is defined as,\nu\u0302(k, t) = L\n\u2211 xn=0\nL\n\u2211 yn=0 u(xn, t)exp(\u2212ik \u00b7xn) , (10)\nwith k = (kx,ky) denoting the wavenumber vector. We then define the Energy spectra denoted by E(k, t) as,\nE(k, t) = \u2211 |k|=k |u\u0302(k, t)|2. (11)"
        },
        {
            "heading": "2.3. Pseudospectral approximation",
            "text": "The equations are solved via a pseudo-spectral method using Fourier basis functions, see Eq. (10). Time-marching is done using the four step third order Runge Kutta scheme (ARS443), (Ascher et al. 1997) and the aliasing errors are removed with the two-third dealiasing rule. The domain is discretized into Nx\u00d7Ny points with Nx along the x direction and Ny along the y direction. The pseudo-spectral solver based on Go\u0301mez et al. (2005) is written in CUDA-based python code which is run on a GPU card. The resolution of the simulation is chosen based on the Reynolds number of the flow, for the maximum Reynolds number of 4000 we have used a resolution of 5122."
        },
        {
            "heading": "3. PINN based models",
            "text": "We construct two different PINN based models for our study. In both the approaches, we use the standard PINN model and on top of that we use some training data from the interior of the domain (x,y) at certain time intervals (Lucor et al. 2022). We will refer to the first neural network as PINN-1 and it\u2019s layout is shown in Fig. 2. The output from the neural network is the predicted\n5\ntime evolving streamfunction and pressure fields which automatically satisfies the incompressibility condition. As for the second approach, we construct the neural network based on our understanding of the energy spectrum of turbulence. We know that the energy spectrum consists of a continuous distribution of energy across different length scales. We then construct neural networks for the low and high wavenumber parts where the training data which is fed to the network are from the low and high wavenumber components respectively. Previous study by Jagtap and Karniadakis (2020) and Hu et al. (2021) shows that splitting the training\n6\ndata in the physical domain can lower the value of overall loss function and is able to better predict the system. Motivated by their study, we do a decomposition in the spectral domain of the training data into low and high wavenumber components. It is known that a simple co-ordinate based selection of points leads to a good prediction of low-wavenumber behaviour while the high wavenumber modes are not captured (Tancik et al. 2020). We expect that the high wavenumber part of the flow will be better captured by this neural network architecture of separate networks for low and high wavenumber components.\nThe neural network layout for this approach is shown in Fig. 3 and we refer to it as PINN-2. As shown in Fig. 3, the inputs for the pair of low and high wavenumber neural networks is the same set of sampling points (x,y, t). The outputs from each of these neural networks are the predicted streamfunction and pressure fields corresponding to the low and high wavenumber parts. These outputs are further combined into the total streamfunction and pressure fields. These combined fields are then used for training on the equation and boundary conditions. However, initial and data losses are separately calculated for the low and high wavenumbers and added together to give the final Lini and Ldata. The data and initial fields that are fed into the two neural networks (low and high) do not have any overlap in the spectral space but the outputs from these neural networks have an overlap in the spectral space due to the nonlinearity of the neural networks. The cutoff wavenumber kc for the PINN-2 model is a parameter which separates the low and high wavenumber part of the flow field. The total loss for the model with low and high wavenumber networks will have the same expression as Eq. (7), except that the individual losses Lini and Ldata are defined for PINN-2 as Lini = (Lini)l +(Lini)h and Ldata = (Ldata)l +(Ldata)h. Here (Lini)l represents the initial loss calculated for the low-wavenumber network while (Lini)h represents the initial loss for the high-wavenumber network, their definitions remain the same as the one for Lini, see Eq. (4). Similarly (Ldata)l ,(Ldata)h represent the data loss calculated for the low and high wavenumber networks respectively. Their definitions remain the same as the one for Ldata, see Eq. (6). One can in-principle extend this idea to construct multiple cutoff wave-numbers, for example, we can construct high, mid and low-wavenumber network models. We leave such extensions for future studies.\nWe see that the definitions of the Loss function for the two PINN networks, PINN-1 and PINN-2 are different since loss initial Lini and loss data Ldata are calculated twice for PINN-2. It should be noted that the structure of the neural network in PINN-2 can support different number of neurons, layers for the low and high wave number networks. Since the number of weight parameters and biases depend on the number of neurons and layers, we can thus control the number of these parameters for low and high wavenumber networks independently."
        },
        {
            "heading": "4. Results and discussion",
            "text": "We fix the forcing amplitude f0 = 1 and the forcing wavenumber to be k f L = 8\u03c0 . Starting from an arbitrary initial condition the forced two-dimensional turbulence leads to a statistical stationary state after an initial transience. We choose a set of \u03bd ,\u03b1 that leads to a Reynolds number of Re\u2248 4.2\u00d7103 and a large scale Reynolds number of Rh\u2248 2.1 which is used as a database for the entire study. We will focus our attention on this statistical stationary state of turbulence to develop our PINN based model. The aim of the model is to capture the statistical properties of the turbulent flow over a fixed time interval for different temporal and spatial scales. We will first identify the best performing neural network and then proceed to compare it with DNS."
        },
        {
            "heading": "4.1. Results from PINNs: training & hyperparameter search",
            "text": "We define the hyperparameters, see in Table 1, over which different neural networks are trained. In the first row of the table, the number of neurons for the PINN-2 model is defined as (nlow,nhigh) where nlow is the number of neurons in the low wavenumber network and nhigh is the number of neurons in the high wavenumbers network. Losses from equation and boundary are minimized at 81 instances in the time interval t \u2208 [0,T ] with equally spaced time-steps of T/80. The data losses are minimized only from 21 equally spaced snapshots with time steps of T/20, while the initial loss is computed from t = 0. We will denote these 21 time instances where the data or initial condition is provided as training times. The rest 60 time instances are denoted as testing times since interior data is not provided to the PINN model and we plan to compare the performance of the PINN with the DNS data. For equation losses we have used Neqn = 10000 points which are randomly selected from 512\u00d7512\u00d781 points. For initial\npoints, we have chosen Nini = 1000 points out of 512\u00d7512 points at the time instant t = 0. The boundary points NBC = 2000 are selected randomly from 4\u00d7512\u00d781 points and the data points Ndata = 5000 are selected randomly from 512\u00d7512\u00d721 points. The data points Ndata correspond close to 0.1% of the total available data points. Filtering is done to separate the energy into low and high wavenumbers. The cut-off wavenumber is taken to be slightly above the forcing wavenumber at kcL = 20\u03c0 . We have also studied the results with kcL = 40\u03c0 and kcL = 60\u03c0 and found that kcL = 20\u03c0 gives the lowest error estimates, the definitions of which will be introduced in the next sub-section.\nFor all our hyperparameter sweeps, we train the model for 200000 epochs (iterations) for the Adams optimizer and then we use upto 200000 epochs using L-BFGS-B optimizer until the tolerance reaches machine precision (double precision). We use a learning rate \u03b7 = 0.001 for the Adams iterations. In this study, we use a tanh activation function for all the layers except for the last layer where we use a linear activation function (Raissi et al. 2019). The training is performed using a NVIDIA Tesla V100 card. First, we do a sweep of the list of neurons listed in Table 1 for both PINN-1 and PINN-2 models. As shown in Fig. 4a, we observe for PINN-1 model that the total loss value for 400 neurons was the lowest among others. On the other hand, for the PINN-2 model shown in Fig. 4b, the combination of (250,250) neurons for low and high wavenumber networks respectively, resulted in the least total loss value. We also remark that this PINN-2 model has 2\u00d7 ((3\u00d7250)+2\u00d7 (250\u00d7250)+(2\u00d7250)) = 252500 trainable weight parameters. In comparison, PINN-1 model with 400 neurons has (3\u00d7 400) + 2\u00d7 (400\u00d7 400) + (2\u00d7 400) = 322000 trainable weight parameters. Even though the trainable weight parameters in this case is more than 25% higher than the best PINN-2 model, going forward we will find that the PINN-2 model performs better than the PINN-1 model.\nWe now take the best performing set of neurons for the PINN-1 and PINN-2 models and perform the next hyperparameter search for the optimal number of layers as listed in Table 1. For the PINN-1 model, we show in Fig. 5a the loss functions for the sets of layers as shown in the Table 1. We see that the layer 5 network does almost as good as the layer 7 network, so we continue to use 5 layer model for further exploring PINN-1. Similarly from Fig. 5b the 5 layer model performs as well as the 7 layer model. Thus we continue with the 5 layer model for both PINN-1 and PINN-2. Next, we find the loss function for different percentages of training data. We reiterate that the training data is taken from the interior of the simulation domain from the existing DNS solution. In Figs. 6a, 6b we show the losses for different percentage of training data for both PINN-1 and PINN-2 networks. As we increase the percentage of training data we see that the total loss increases marginally because the Ldata increases. This behavior is observed generally in machine learning models (Ng 2017). We will continue with 0.1% percentage for both the PINN-1 and PINN-2 because it has comparable total loss values with the other training data sets and that it is computationally cheaper."
        },
        {
            "heading": "4.2. Comparison with DNS results",
            "text": "One of the main aim of this work is to accurately capture the modes using the proposed PINN model. We first observe the energy spectra E(k) which gives the energy content across different length scales. We define the quantity \u03b5Ek = \u2211k | log ( EPINN(k)/EDNS(k) ) | which denotes the deviation of the mean testing time energy spectra from PINN model EPINN with that of the DNS EDNS. Here E(k) denotes the mean of the energy spectra found at testing times. The spectra are averaged over 60 testing time instances. To find the best PINN model, we have tried a variety of values for \u03bb4 \u2208 (1,50,75,100,150,200). We show in Table 2 the values of \u03b5Ek for different \u03bb4 values for both PINN-1 and PINN-2 models. We find \u03bb4 = 150 for the PINN-1 model and \u03bb4 = 75 for the PINN-2 model to perform best, since they have the lowest \u03b5Ek among all the \u03bb4\u2019s examined. This is also seen from Fig. 7a which shows the energy spectra E(k) for different values of \u03bb4 for the PINN-1 model while Fig. 7b shows E(k) for different values of \u03bb4 for the PINN-2 model. Thus, for the weights in the expression Eq. (7), we have set the values to be \u03bb1 = \u03bb2 = \u03bb3 = 1,\u03bb4 = 150 for the PINN-1 model and \u03bb1 = \u03bb2 = \u03bb3 = 1,\u03bb4 = 75 for the PINN-2 model. Large value of \u03bb4 implies that a larger significance is given to the equation loss so that we penalize the unphysical solutions that deviate from the governing equations. Also, we retrain the weights once and we observe better correlation of the energy spectra and energy time series with those from the DNS. Particularly, for the chosen value of \u03bb4 = 150 for PINN-1 and \u03bb4 = 75 for PINN-2 model, retraining once reduced the \u03b5Ek to 79.91 and 47.2 respectively. Further retraining does not reduce \u03b5Ek by more than 10%, so we only retrain the weights only once.\nIn what follows, we make all the quantities dimensionless by rescaling with the typical velocity scale U , length scale L and the time scale given by L/U .\nFirst, in order to find the deviation of our PINN based solutions from DNS, we define an error measure denoted as RMSE as follows,\n\u03b5 = (\n1 nt\nnt\n\u2211 i (uiPINN\u2212uiDNS)2\n)1/2 , (12)\nwhere, nt = N2\u00d7 T = 5122\u00d7 60 is the total number of data points present at the testing times. We measure this error along with the coefficient of determination R2 for the two components of the velocity. The RMSE is calculated over the whole domain because we aim to reproduce the whole field across all length scales. We tabulate these diagnostics for both PINN-1 and PINN-2 models in Table 3. We clearly find that, for the parameters chosen to study PINN-1 and PINN-2, PINN-2 performs better with lower error measures and also has a higher R2 score as compared to PINN-1. The pressure field is also an output from the PINN models. For the PINN-2 model we find RMSE for pressure to be 0.09.\nNext, in Fig. 8a we compare the time series of the total energy E(t) of both the DNS and the PINN models. The PINN-2 model captures well the fluctuations of the total energy as a function of time as compared to the PINN-1 model. In Fig. 8b we compare the time series of the enstrophy between the DNS and the PINN models. PINN-1 model predicts a higher value of the enstrophy as compared with the DNS while PINN-2 performs better and compares well with the solution from DNS.\nNext, we look at the mean energy spectra E(k) for both PINN-1 and PINN-2 models. Fig. 9 shows the mean energy spectra comparison between the DNS and the PINN models at testing times. The spectra from the PINN-2 model shows an excellent comparison with the DNS data. The PINN-2 spectra starts deviating from the DNS spectra at values of E(k) \u223c 10\u22129. It is clear from the spectra that the large scales k . 10 agree to a very good degree at all times whereas intermediate and small scales start showing deviations around the DNS results. The energy spectra is deviating at viscous scales due to the formation of very fine\n10\nstructures at the boundaries. To capture the energy spectra well into the viscous scales, one might have to enforce the periodic boundary conditions exactly (Dong and Ni 2021).\nWe then focus on the snapshots of the velocity fields for the DNS and PINN-2 model in Fig. 10 and Fig. 11 for the testing times t = 0.45 and t = 0.98 respectively. As seen from these figures, the large scale flow structures present in the velocity fields from the PINN-2 model are identical to the actual turbulent solutions from DNS even though we have used only 0.1% of the available DNS data for training. In the third column of the Figs. 10 and 11, we show the absolute errors of the velocity fields to indicate deviations between the PINN-2 model and the DNS solutions."
        },
        {
            "heading": "4.3. Remarks on PINNs for turbulence simulations",
            "text": "The PINN-2 model compares well with the DNS results both at testing and training times. From the spectra we see that the model works best at large scales and to study this further we look at the temporal fluctuations of some of the large scale modes. We denote \u03c8\u0302(k, t) as the two-dimensional Fourier transform of the streamfunction \u03c8(x,y, t). In Fig. 12 we show the time series of the real and imaginary part of the modes \u03c8\u0302kx,ky for a few values of wavenumbers kx,ky for both the PINN-2 model and the DNS. The symbol \u03c8\u0302r denotes the real part of the mode \u03c8\u0302 while the symbol \u03c8\u0302 i denotes the imaginary part of the mode \u03c8\u0302 . As seen from the plots, the fluctuations of the large scale modes \u03c8\u030211, \u03c8\u030212, \u03c8\u030221, \u03c8\u030222, are captured very well by the PINN-2 model. For smaller scale modes \u03c8\u030233, \u03c8\u030244 we see that the time series starts to deviate from the DNS results. Since the largest modes are\n11\n12\naccurately captured, we can infer that the PINN-2 model learns to capture the long range correlations more easily than the short range correlations. While for the small scales, the statistical behaviour of these modes like the typical fluctuation amplitudes, are captured by the PINN-2 model as seen in Figs. 12.\nWe thus find that the PINN-2 model captures very well the large scale features of the two-dimensional turbulent flow even though the temporal behaviour of the small scale modes are found to deviate from the DNS results. This happens since the small scale modes only act as an effective viscosity on the large scale modes. In this particular study we have used DNS results to train our PINN model, but we can in principle also use experimental data from Particle Image Velocimetry (PIV) measurements. Such usage of PINNs has already been shown to be successful in the works of Raissi et al. (2020); Cai et al. (2021), and it reinforces our belief that the results from the PINN-2 model can be used to study even turbulent flows with larger Reynolds numbers than currently used in this study.\n13"
        },
        {
            "heading": "5. Conclusion",
            "text": "We use two PINN models to predict two-dimensional turbulence in this study. The first approach (PINN-1) involves a standard PINN model (Raissi et al. 2019) along with training of solution data from the interior of the domain. The second approach (PINN-2) is similar but involves two neural networks (low and high wavenumbers) where the initial and data losses are minimized separately for these networks. We perform a systematic hyperparameter search and we obtain an optimal set for which the total loss value is minimal. We observe that PINN-2 outperforms PINN-1 with a lower RMSE value and higher R2 score. Furthermore, we penalize the equation loss to get physically acceptable solutions, which also allows us to capture the energy spectra more accurately. The amount of training data that is used to train these models is close to 0.1% of the whole data set available at training time instances. The errors and comparisons are done at testing time instances which is three times the number of training time instances.\nWe observe that PINN-2 model predicts the velocities to reasonable accuracy as compared to the DNS results at the testing times. It provides a solution that captures the fluctuations of different quantities and their time averaged behaviour based upon the constraints from DNS data, the governing equations, boundary conditions and initial conditions. Since we use only 0.1% of the data from DNS for training the PINN based model, the results open up the possibilities of using such PINN models for studying turbulence at very fine time intervals, where standard DNS simulations face storage issues. The PINN-2 model studied here shows a very good ability to capture the fluctuations of large scale modes. Such models can possibly help in studying long time statistics of systems that show phenomena such as flow reversals and 1/ f -noise (Herault et al. 2015; Dallas et al. 2020).\nPINN-2 model utilizes a Fourier filter based domain decomposition to filter out and train the low and high wavenumber components of the quantity of interest. For future work, following the design of PINN-2 model, one could expand it further by adding additional neural networks to the existing architecture. Furthermore, one can in principle use such a network for multiscale physical problems beyond fluid dynamics. Also, one could introduce adaptive weights for the different loss functions which can in principle improve the predictive accuracy (Wang et al. 2020). Since PINN-2 model is able to capture energy spectral density across many decades, while falling into the length scales where transition between inertial and viscous range occurs, one can further explore more sophisticated models that can capture the whole viscous range accurately. Such models can also help in studying systems where DNS simulations are very expensive or difficult to be carried out."
        },
        {
            "heading": "Acknowledgements",
            "text": "The authors thank the three referees whose insightful comments and suggestions helped improve this manuscript. The authors thank the computing resources and support provided by the CI/PDP Deep Learning Platform Team at Robert Bosch GmbH, Renningen, Germany and the PARAM Shakti supercomputing facility of IIT Kharagpur established under National Supercomputing Mission (NSM), Government of India and supported by Centre for Development of Advanced Computing (CDAC), Pune. K. Seshasayanan acknowledges support from NSM Grant no. DST/NSM/R&D HPC Applications/2021/03.11, from the Institute Scheme from Innovative Research and Development (ISIRD), IIT Kharagpur, Grant No. IIT/SRIC/ISIRD/2021-2022/08 and the Start-up Research Grant No. SRG/2021/001229 from Science & Engineering Research Board (SERB), India.\nData Availability Statement\nThe data that support the findings of this study are available from the corresponding author upon reasonable request."
        },
        {
            "heading": "2020. ISSN 1991-7120.",
            "text": "X. Jin, S. Cai, H. Li, and G. E. Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations. Journal of Computational Physics, 426:109951, 2021. ISSN 0021-9991.\nG. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang, and L. Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422\u2013440, 2021.\nJ. Kim and C. Lee. Deep unsupervised learning of turbulence for inflow generation at various reynolds numbers. Journal of Computational Physics, 406:109216, 2020. ISSN 0021-9991.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017. M. J. Kochenderfer and T. A. Wheeler. Algorithms for Optimization. The MIT Press, 2019. ISBN 0262039427. D. Kochkov, J. A. Smith, A. Alieva, Q. Wang, M. P. Brenner, and S. Hoyer. Machine learning\u2013accelerated computational fluid dynamics. 118\n(21), 2021. R. H. Kraichnan. Inertial ranges in two-dimensional turbulence. The Physics of Fluids, 10(7):1417\u20131423, 1967. R. Laubscher. Simulation of multi-species flow and heat transfer using physics-informed neural networks. Physics of Fluids, 33(8):087101,\n2021. B. Liu, J. Tang, H. Huang, and X.-Y. Lu. Deep learning methods for super-resolution reconstruction of turbulent flows. Physics of Fluids, 32\n(2):025105, 2020. D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45(1):503\u2013528,\n1989. D. Lucor, A. Agrawal, and A. Sergent. Simple computational strategies for more effective physics-informed neural networks modeling of\nturbulent natural convection. Journal of Computational Physics, 456:111022, 2022. ISSN 0021-9991. G. Narasimhan, C. Meneveau, and T. A. Zaki. Large eddy simulation of transitional channel flow using a machine learning classifier to\ndistinguish laminar and turbulent regions. Phys. Rev. Fluids, 6:074608, Jul 2021. A. Ng. Machine Learning Yearning. Online Draft, 2017. M. Raissi, P. Perdikaris, and G. E. Karniadakis. Machine learning of linear differential equations using gaussian processes. Journal of\nComputational Physics, 348:683\u2013693, 2017a. ISSN 0021-9991. M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential\nequations, 2017b. M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics informed deep learning (part ii): Data-driven discovery of nonlinear partial\ndifferential equations, 2017c. M. Raissi, P. Perdikaris, and G. E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and\ninverse problems involving nonlinear partial differential equations. Journal of Computational Physics, 378:686\u2013707, 2019. M. Raissi, A. Yazdani, and G. E. Karniadakis. Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations.\nScience, 367(6481):1026\u20131030, 2020.\n15\nK. Seshasayanan, V. Dallas, and S. Fauve. Bifurcations of a plane parallel flow with kolmogorov forcing. Physical Review Fluids, 6(10):"
        },
        {
            "heading": "103902, 2021.",
            "text": "G. I. Sivashinsky. Weak turbulence in periodic flows. Physica D: Nonlinear Phenomena, 17(2):243\u2013255, 1985. S. Taghizadeh, F. D. Witherden, Y. A. Hassan, and S. S. Girimaji. Turbulence closure modeling with data-driven techniques: Investigation of\ngeneralizable deep neural networks. Physics of Fluids, 33(11):115132, 2021. M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ramamoorthi, J. T. Barron, and R. Ng. Fourier\nfeatures let networks learn high frequency functions in low dimensional domains, 2020. S. Wang, Y. Teng, and P. Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks, 2020. T. Wang, Z. Huang, Z. Sun, and G. Xi. Reconstruction of natural convection within an enclosure using deep neural network. International\nJournal of Heat and Mass Transfer, 164:120626, 2021. ISSN 0017-9310."
        }
    ],
    "title": "Physics-informed data based neural networks for two-dimensional turbulence",
    "year": 2022
}