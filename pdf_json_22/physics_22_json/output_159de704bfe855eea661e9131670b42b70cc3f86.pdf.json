{
    "abstractText": "Inversion techniques are widely used to reconstruct subsurface physical properties (e.g., velocity, conductivity) from surface-based geophysical measurements (e.g., seismic, electric/magnetic (EM) data). The problems are governed by partial differential equations (PDEs) like the wave or Maxwell\u2019s equations. Solving geophysical inversion problems is challenging due to the illposedness and high computational cost. To alleviate those issues, recent studies leverage deep neural networks to learn the inversion mappings from measurements to the property directly. In this paper, we show that such a mapping can be well modeled by a very shallow (but not wide) network with only five layers. This is achieved based on our new finding of an intriguing property: a near-linear relationship between the input and output, after applying integral transform in high dimensional space. In particular, when dealing with the inversion from seismic data to subsurface velocity governed by a wave equation, the integral results of velocity with Gaussian kernels are linearly correlated to the integral of seismic data with sine kernels. Furthermore, this property can be easily turned into a light-weight encoder-decoder network for inversion. The encoder contains the integration of seismic data and the linear transformation without need for fine-tuning. The decoder only consists of a single transformer block to reverse the integral of velocity. Experiments show that this interesting property holds for two geophysics inversion problems over four different datasets. Compared to much deeper InversionNet (Wu & Lin, 2019), our method achieves comparable accuracy, but consumes significantly fewer parameters. Earth and Environmental Sciences Division, Los Alamos National Laboratory,USA Microsoft Research, USA College of Information Sciences and Technology, The Pennsylvania State University, USA. Correspondence to: Youzuo Lin <ylin@lanl.gov>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).",
    "authors": [
        {
            "affiliations": [],
            "name": "Yinan Feng"
        },
        {
            "affiliations": [],
            "name": "Yinpeng Chen"
        },
        {
            "affiliations": [],
            "name": "Shihang Feng"
        },
        {
            "affiliations": [],
            "name": "Peng Jin"
        },
        {
            "affiliations": [],
            "name": "Zicheng Liu"
        },
        {
            "affiliations": [],
            "name": "Youzuo Lin"
        }
    ],
    "id": "SP:b375ff124bcf62d139f75e47e8c45fa7d6214325",
    "references": [
        {
            "authors": [
                "A. Adler",
                "M. Araya-Polo",
                "T. Poggio"
            ],
            "title": "Deep learning for seismic inverse problems: Toward the acceleration of geophysical analysis workflows",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2021
        },
        {
            "authors": [
                "M. Araya-Polo",
                "J. Jennings",
                "A. Adler",
                "T. Dahlke"
            ],
            "title": "Deep-learning tomography",
            "venue": "The Leading Edge,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Chen",
                "D. Huang",
                "D. Zhang",
                "J. Zeng",
                "N. Wang",
                "H. Zhang",
                "J. Yan"
            ],
            "title": "Theory-guided hard constraint projection (hcp): A knowledge-based data-driven scientific machine learning method",
            "venue": "Journal of Computational Physics,",
            "year": 2021
        },
        {
            "authors": [
                "M. Commer",
                "G.A. Newman"
            ],
            "title": "New advances in three-dimensional controlled-source electromagnetic inversion",
            "venue": "Geophysical Journal International,",
            "year": 2008
        },
        {
            "authors": [
                "S. Feng",
                "G.T. Schuster"
            ],
            "title": "Transmission+ reflection anisotropic wave-equation traveltime and waveform inversion",
            "venue": "Geophysical Prospecting,",
            "year": 2019
        },
        {
            "authors": [
                "S. Feng",
                "L. Fu",
                "Z. Feng",
                "G.T. Schuster"
            ],
            "title": "Multiscale phase inversion for vertical transverse isotropic media",
            "venue": "Geophysical Prospecting,",
            "year": 2021
        },
        {
            "authors": [
                "P. Jin",
                "X. Zhang",
                "Y. Chen",
                "S.X. Huang",
                "Z. Liu",
                "Y. Lin"
            ],
            "title": "Unsupervised learning of full-waveform inversion: Connecting CNN and partial differential equation in a loop",
            "venue": "In Proceedings of the Tenth International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "P. Jordan",
                "J. Wagoner"
            ],
            "title": "Characterizing construction of existing wells to a co2 storage target: The kimberlina site, california",
            "venue": "Technical report, National Energy Technology Laboratory (NETL),",
            "year": 2017
        },
        {
            "authors": [
                "G.E. Karniadakis",
                "I.G. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics,",
            "year": 2021
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In Sixth International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "L. Lu",
                "P. Jin",
                "G. Pang",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "Learning nonlinear operators via deeponet based on the universal approximation theorem of operators",
            "venue": "Nature Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physicsinformed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational Physics,",
            "year": 2019
        },
        {
            "authors": [
                "G.T. Schuster"
            ],
            "title": "Seismic inversion",
            "venue": "Society of Exploration Geophysicists,",
            "year": 2017
        },
        {
            "authors": [
                "J. Sun",
                "K.A. Innanen",
                "C. Huang"
            ],
            "title": "Physics-guided deep learning for seismic inversion with hybrid training and uncertainty analysis",
            "year": 2021
        },
        {
            "authors": [
                "J. Virieux",
                "S. Operto"
            ],
            "title": "An overview of full-waveform inversion in exploration geophysics",
            "venue": "WCC1\u2013WCC26,",
            "year": 2009
        },
        {
            "authors": [
                "Y. Wu",
                "Y. Lin"
            ],
            "title": "InversionNet: An efficient and accurate data-driven full waveform inversion",
            "venue": "IEEE Transactions on Computational Imaging,",
            "year": 2019
        },
        {
            "authors": [
                "F. Yang",
                "J. Ma"
            ],
            "title": "Deep-learning inversion: A nextgeneration seismic velocity model building",
            "venue": "method. Geophysics,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Zeng",
                "S. Feng",
                "B. Wohlberg",
                "Y. Lin"
            ],
            "title": "Inversionnet3d: Efficient and scalable learning for 3d full waveform inversion",
            "venue": "arXiv preprint arXiv:2103.14158,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Zhang",
                "Y. Lin"
            ],
            "title": "Data-driven seismic waveform inversion: A study on the robustness and generalization",
            "venue": "IEEE Transactions on Geoscience and Remote sensing,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "In this paper, we show that such a mapping can be well modeled by a very shallow (but not wide) network with only five layers. This is achieved based on our new finding of an intriguing property: a near-linear relationship between the input and output, after applying integral transform in high dimensional space. In particular, when dealing with the inversion from seismic data to subsurface velocity governed by a wave equation, the integral results of velocity with Gaussian kernels are linearly correlated to the integral of seismic data with sine kernels. Furthermore, this property can be easily turned into a light-weight encoder-decoder network for inversion. The encoder contains the integration of seismic data and the linear transformation without need for fine-tuning. The decoder only consists of a single transformer block to reverse the integral of velocity.\nExperiments show that this interesting property holds for two geophysics inversion problems over four different datasets. Compared to much deeper InversionNet (Wu & Lin, 2019), our method achieves comparable accuracy, but consumes significantly fewer parameters.\n1Earth and Environmental Sciences Division, Los Alamos National Laboratory,USA 2Microsoft Research, USA 3College of Information Sciences and Technology, The Pennsylvania State University, USA. Correspondence to: Youzuo Lin <ylin@lanl.gov>.\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).\n1. Introduction\n\ud835\udc7cGeophysical Measurements\n\ud835\udc62 \ud835\udc65, \ud835\udc61\nGeophysical\nproperties\n\ud835\udc66 \ud835\udc65, \ud835\udc67 \u2026 \u2026\n\ud835\udc80\n\ud835\udc48\ud835\udc5b = \u222c\ud835\udc62 \ud835\udc65, \ud835\udc61 \u03a6\ud835\udc5b \ud835\udc65, \ud835\udc61 \ud835\udc51\ud835\udc61\ud835\udc51\ud835\udc65 \ud835\udc4c\ud835\udc5a = \u222c\ud835\udc66 \ud835\udc65, \ud835\udc67 \u03a8\ud835\udc5a \ud835\udc65, \ud835\udc67 \ud835\udc51\ud835\udc65\ud835\udc51\ud835\udc67\n\ud835\udc80 \u2248 \ud835\udc34\ud835\udc7c\nt\nx\nz\nx\nFigure 1. Illustration of the near-linear relation property between geophysical measurements and properties after applying integral transform. {\u03a6n} and {\u03a8m} are two families of kernels for integral transforms (e.g., sine and Gaussian). Here, the full waveform inversion from seismic data to velocity map is used as an example.\nGeophysics inversion techniques are commonly used to characterize site geology, stratigraphy, and rock quality. These techniques uncover subsurface layering and rock geomechanical properties (such as velocity, conductivity), which are crucial in subsurface applications such as subsurface energy exploration, carbon capture and sequestration, groundwater contamination and remediation, and earthquake early warning systems. Technically, these subsurface geophysical properties can be inferred from geophysical measurements (such as seismic, electromagnetic (EM)) acquired on the surface. Some underlying partial differential equations (PDEs) between measurements and geophysical property exist, where inversion gets its name. For example, velocity is reconstructed from seismic data based on full waveform inversion (FWI) of a wave equation, while conductivity is recovered from EM measurements based on EM inversion of Maxwell\u2019s equations.\nHowever, these inversion problems can be rather challenging to solve, as they are ill-posed. Recent works study them from two perspectives: physics-driven and data-driven. The former approaches search for the optimal geophysical property (e.g., velocity) from an initial guess, such that the generated geophysical simulations based on the forward modeling of the governing equation are closed to the real measurements (Virieux & Operto, 2009; Feng & Schuster, 2019; Feng et al., 2021). These methods are computationally expensive as they require iterative optimization per sample.\nar X\niv :2\n20 4.\n13 73\n1v 2\n[ cs\n.L G\n] 1\n6 Ju\nn 20\n22\nThe latter methods (i.e., data-driven approaches) (Wu & Lin, 2019), inspired by the image-to-image translation task, employ encoder-decoder convolution neural networks (CNN) to learn the mapping between physical measurements and geophysical properties. Deep network architecture that involves multiple convolution blocks is employed as both encoder and decode, which also results in heavy reliance on data and very high computational cost in training.\nIn this paper, we found an intriguing property of geophysics inversion that can significantly simplify data-driven methods as:\nGeophysical measurements (e.g., seismic data) and geophysical property (e.g., velocity map) have nearlinear relationship in high dimensional space after integral transform.\nLet u(x, t) denote a spatio-temporal geophysical measurement along horizontal x and time t dimensions, and y(x, z) denote a 2D geophysical property along horizontal x and depth z. Since, in practice, geophysical measurement is mostly collected at the surface, and people want to invert the subsurface geophysical property, measurement u only contains spatial variable x, while property y includes (x, z). As illustrated in Figure 1, the proposed property can be mathematically represented as follows:\nU = [U1, . . . , UN ] T , Un = \u222b\u222b u(x, t)\u03a6n(x, t)dxdt,\nY = [Y1, . . . , YM ] T , Ym = \u222b\u222b y(x, z)\u03a8m(x, z)dxdz,\nY \u2248 AU , (1)\nwhere \u03a6n and \u03a8m are kernels for integral transforms. After applying integral transforms, both geophysical measurement u(x, t) and property y(x, z) are projected into high dimensional space (denoted as U and Y ), and they will have a near-linear relationship (Y \u2248 AU ). Note that the kernels ({\u03a6n}, {\u03a8m}) are not learnable, but well-known analytical kernels like sine, Fourier, or Gaussian.\nInterestingly, this intriguing property can significantly simplify the encoder-decoder architecture in data-driven methods. The encoder only contains the integral with kernel {\u03a6n} followed by a linear layer with weight matrix A in Eq. 1. The decoder just uses a single transformer (Vaswani et al., 2017) block followed by a linear projection to reverse the integral with kernels {\u03a8m}. This results in a much shallower architecture. In addition, the encoder and decoder are learnt separately. The matrix A in encoder can be directly solved by pseudo inverse and is frozen afterward. Only the transformer block and following linear layer in the decoder are learnt via SGD based optimizer.\nOur method, named InvLINT (Inversion via LINear relationship between INTegrals), achieves comparable (or even\nbetter) performance on two geophysics inversion problems (seismic full waveform inversion and electric/magnetic inversion) over four datasets (Kimberlina Leakage (Jordan & Wagoner, 2017), Marmousi (Feng et al., 2021), Salt (Yang & Ma, 2019), and Kimberlina-Reservoir (Alumbaugh et al., 2021)), but uses significantly less parameters than prior works. For instance, on Marmousi, our model only needs 1/20 parameters, compared to previous InversionNet."
        },
        {
            "heading": "2. Background",
            "text": "The governing equation of the seismic full waveform inversion is acoustic wave equation (Schuster, 2017),\n\u22072p(r, t)\u2212 1 c2(r)\n\u22022 \u2202t2 p(r, t) = s(r, t), (2)\nwhere r = (x, z) represents the spatial location in Cartesian coordinates (x is the horizontal direction and z is the depth), t denotes time, c(r) is the velocity map, p(r, t) represents the pressure wavefield, \u22072 is the Laplacian operator, and s(r, t) is the source term that specifies the location and time history of the source.\nFor the EM forward modeling, the governing equation is the Maxwell\u2019s Equations (Commer & Newman, 2008),\n\u03c3E\u2212\u2207\u00d7H = \u2212J, (3) \u2207\u00d7E + i\u03c9\u00b50H = \u2212M,\nwhere E and H are the electric and magnetic fields. J and M are the electric and magnetic sources. \u03c3 is the electrical conductivity and \u00b50 = 4\u03c0 \u00d7 10\u22127\u2126 \u00b7 s/m is the magnetic permeability of free space."
        },
        {
            "heading": "3. Methodology",
            "text": "In this section, we use seismic full waveform inversion (from seismic data to velocity) as an example to illustrate our derivation of the linear property after integral transforms. We will also show the encoder-decoder architecture based on this linear property. Empirically, our solution is also applicable to EM inversion (from EM data to conductivity)."
        },
        {
            "heading": "3.1. Near-Linear Relationship between Integral Transformations",
            "text": "In the following part, we will show the seismic data and velocity maps have the near-linear relation after integral transformation like the format of Equation 1. The seismic data p and velocity map c are governed by the wave equation (Equation 2). Note that seismic data p and velocity map c in wave equation corresponds to the input u and output y in Equation 1, respectively.\nRewriting wave equation in Fourier series: Similar to constant coefficients PDEs, we assume spatial variable\nr = (x, z) and temporal variable t are separable, i.e., p(x, z, t) = p1(x, z)p2(t), and s(x, z, t) = s1(x, z)s2(t). Thus, Equation 2 is rewritten as\nc2(x, z)(\u22072p1(x, z)p2(t)\u2212 s1(x, z)s2(t))\n= \u22022\n\u2202t2 (p1(x, z)p2(t)). (4)\nNext the temporal parts p2(t) and s2(t) are represented as Fourier series: p2(t) = \u2211N n=1Bne\nj2\u03c0nt and s2(t) =\u2211N n=1Gne j2\u03c0nt. This turns Equation 4 as:\nN\u2211 n=1 c2(x, z)(\u22072p1(x, z)Bn \u2212 s1(x, z)Gn)ej2\u03c0nt\n= N\u2211 n=1 4\u03c02n2p1(x, z)Bne j2\u03c0nt. (5)\nTo make sure both sides have the same coefficient for each n, the aggregation \u2211N n=1 and e\nj2\u03c0nt can be removed from Equation 5 as:\nc2(x, z)(\u22072p1(x, z)Bn \u2212 s1(x, z)Gn) = 4\u03c02n2p1(x, z)Bn. (6)\nBy further integrating over x, we have\n1\n4\u03c02n2\n\u222b c2(x, z) \u2223\u2223\u22072p1(x, z)Bn \u2212 s1(x, z)Gn\u2223\u2223 dx, = \u222b p1(x, z) |Bn| dx,\n=  \u222b\u222b\np1(x, z)p2(t)\ufe38 \ufe37\ufe37 \ufe38 Seismic data e\u2212j2\u03c0nt\ufe38 \ufe37\ufe37 \ufe38 Fourier kernel dtdx  , (7) where | \u00b7 | is the modulus operator of complex numbers and Bn = \u222b p2(t)e\n\u2212j2\u03c0ntdt are the Fourier coefficients. Note that sinceBn andGn are complex numbers, we take module on both sides. Here, taking the real or imaginary part, rather than modulo, does not affect our conclusions. Now, the right hand of Equation 7 is in the same format with Un in Equation 1. The kernel function \u03a6n(x, t) = e\u2212j2\u03c0nt1(x), where 1(x) = 1 for all x.\nApproximation by Integral over z: In reality the seismic data p(x, z, t) is mostly collected at the surface (z = 0). Thus, the right-hand side of Equation 7 is computable at z = 0. However, the left-hand side is hard to calculate, since \u22072p1(x, z) and s1(x, z) are unknown. Here, we hypothesize that the left-hand side at z = 0 can be approximated by leveraging velocity map at multiple depth positions as:\n1\n4\u03c02n2\n\u222b c2(x, z) \u2223\u2223\u22072p1(x, z)Bn \u2212 s1(x, z)Gn\u2223\u2223 dx\u2223\u2223\u2223\u2223 z=0\n\u2248 \u222b\u222b c2(x, z)Fn(x, z)dxdz,\n(8)\nwhere Fn(x, z) is the kernel function.\nThis hypothesis (Eq. 7\u20138) bridges integral transforms of the seismic data ( \u222b\u222b p(t, x, z)e\u2212j2\u03c0ntdtdx|z=0) and velocity\nmaps ( \u222b\u222b\nc2(x, z)Fn(x, z)dxdz) via an auxiliary function 1\n4\u03c02n2\n\u222b c2(x, z) \u2223\u2223\u22072p1(x, z)Bn \u2212 s1(x, z)Gn\u2223\u2223 dx|z=0. It has two parts: (a) the double integral of velocity maps equals the auxiliary function, and (b) the 2D kernel Fn(x, z) can be estimated by a set of basis functions, so we can further calculate the inverse problem we want to solve. The existence of Fn(x, z) to achieve the former equality can be validated by a special case Fn(x, z) =\n1 4\u03c02n2 \u2223\u2223\u22072p1(x, z)Bn \u2212 s1(x, z)Gn\u2223\u2223 \u03b4(z) where \u03b4(z) is an impulse function. The latter may weaken the former assertion of equality, but the misfit is likely small, as velocity map is continuous at most (x, z) positions and seismic data p1(x, z) and source s1(x, z) in the auxiliary function has strong correlation along x and z. Our experimental results over three datasets empirically validate this hypothesis.\nFurther simplification by a single kernel family: As discussed above, we simplify Fn(x, z) as a weighted sum of a series of basis functions:\nFn(x, z) = M\u2211 m=1 dn,m\u03a8m(x, z), (9)\nwhere dn,m is the weight and \u03a8m(x, z) is the basis function. By further plugging Equations 8 and 9 into Equation 7, we get\nM\u2211 m=1 dn,m \u222b\u222b c2(x, z)\u03a8m(x, z)dxdz\n\u2248 \u2223\u2223\u2223\u2223\u222b\u222b p(x, z, t)e\u2212j2\u03c0ntdtdx\u2223\u2223\u2223\u2223\nz=0\n. (10)\nRelation to Equation 1: Equation 10 is special case of Equation 1 We can, therefore, express Equation 10 in the form of Equation 1 by letting:\ny(x, z) = c2(x, z), u(x, t) = p(x, t),\nA = D\u2020, \u03a6n(x, t) = e \u2212j2\u03c0nt 1(x),\nwhere A is pseudo inverse of matrix D and D = [dn,m]N\u00d7M is the matrix format.\nIn particular, U = [U1, . . . , UM ]T and Y = [Y1, . . . , YN ]T are the high dimensional embeddings of the measurement\nand geophysical property. {\u03a6n} is chosen as cosine/sine or Fourier transform; while, based on the experiments, Gaussian kernel becomes our choice of the {\u03a8m} to embed the spatial information in the geophysics property. It is true that the hypothesis may seem strong, however, its validity can be supported via our extensive experimental results using multiple datasets and various PDEs."
        },
        {
            "heading": "3.2. Simplified Encoder-Decoder Architecture",
            "text": "Based on the proposed mathematical property as shown in Equation 1, we can easily design a simple network architecture, accordingly. The encoder plays exactly the same role as the right-hand side of Equation 1, while the decoder, with a neural network, approximates the inverse mapping of the integral transformation (\u03a8\u22121m (x, z)). The structure (Figure 2) of our InvLINT is described below.\nEncoder: As illustrated in Figure 2, we design the encoder exactly the same to Equation 1, where an integral transformation with kernel {\u03a6n}, n \u2208 [1, N ] is first implemented and followed by a linear layer represented by A. With such a simple linear relation, one can easily map the input measurement to the embedding of the output.\nDecoder: There are many kernel functions (like Gaussian kernel), which does not have a close form inverse transformations. Instead, we use a shallow decoder network to approximate such a pseudo-inverse. To achieve this, we first use a linear layer L1 to map Y to a more compact embedding and tile it a grid with the shape Rh\u00d7w\u00d7k. Here, h and w are the size of the velocity map with 32 times downsampling, and k is the number of channels. After that, L1(Y ) is input into a 1-layer transformer, with patch size of 1\u00d71\u00d7k. This shallow transformer is the only nonlinear part of our decoder.\nThe last parts of the model are a linear layer Lr. It upsamples each 1\u00d7 1\u00d7 k patch to a (32 + d)\u00d7 (32 + d) block1, where d is a small integer. The final predicted velocity map c\u0302 can be construed by stitching all h\u00d7w blocks together. The\n1Since the size of output may not be divided exactly by 32, the recovered shape will be slightly different for different datasets.\npurpose of this is to recover the output to the original shape with overlaps among blocks to remove the block effect."
        },
        {
            "heading": "3.3. Training",
            "text": "Because of the near-linear relation, we can easily solve the linear layer in the encoder, L1, with the least squares method. Specifically, we first compute the embedding of both encoder and decoder by integral transformations, calculate the solution of matrix A, and freeze it while training the decoder. The decoder is trained by an SGD-based optimizer. The loss function of our InvLINT is a pixel-wise MAE loss given as\nL(c\u0302, c) = `1(c\u0302, c). (11)\nPeng et al. (Jin et al., 2022) find that combining MAE, MSE, and perceptual loss together is helpful to improve the performance. However, to make a fair comparison with the previous work, we only use MAE as our loss function."
        },
        {
            "heading": "4. Experiment",
            "text": "In this section, we present experimental results of our proposed InvLINT evaluated on four datasets and compare our method with the previous works, InversionNet (Wu & Lin, 2019) and VelocityGAN Zhang et al. (2019). We also discuss different factors that affect the performance of our method."
        },
        {
            "heading": "4.1. Implementation Details",
            "text": ""
        },
        {
            "heading": "4.1.1. DATASETS",
            "text": "In experiments, we verify our method on four datasets, of which three of them are used for seismic FWI, and one of which is for an EM inversion.\nKimberlina-Leakage: The geophysical properties were developed under DOE\u2019s National Risk Assessment Program (NRAP). It contains 991 CO2 leakage scenarios, each simulated over a duration of 200 years, with 20 leakage velocity maps provided (i.e., at every ten years) for each scenario (Jordan & Wagoner, 2017). Excluding some missing velocity maps, the data are split as 807/166 scenarios for training and testing, respectively. The size of the velocity maps is 401 \u00d7 141 grid points, and the grid size is 10 meters in both directions. To synthesize the seismic data, nine sources are evenly distributed along the top of the model, with depths of 5 m. The seismic traces are recorded by 101 receivers positioned at each grid with an interval of 15 m. The source frequency is 10 Hz. Each receiver collects 1251-timestep data for 1 second.\nMarmousi: We apply the generating method in Jin et al. (2022), which follows Feng et al. (2021) and adopts the Marmousi velocity map as the style image to construct this low-\nresolution dataset. This dataset contains 30K with paired seismic data and velocity map. 24k samples are set as the training set, 3k samples are used as the validation set, and the rest are the testing set. The size of the velocity map is 70\u00d7 70, with the 10-meter grid size in both directions. The velocity ranges from 1, 500m/s to 4, 700m/s. There are S = 5 sources placed evenly with a spacing of 170 m. The source frequency is 20 Hz. The seismic data are recorded by 70 receivers with a receiver interval of 10 m. Each receiver collects 1,000-timestep data for 1 second.\nSalt: The dataset contains 140 velocity maps (Yang & Ma, 2019). We downsample it to 40 \u00d7 60 with a grid size of 10 m, and the splitting strategy 120/10/10 is applied. The velocity ranges from 1, 500m/s to 4, 500m/s. There are also S = 5 sources used, with 12-Hz source frequency and a spacing of 150 m. The seismic data are recorded by 60 receivers with an interval of 10 m, too. Each receiver collects 600-timestep data for 1 second.\nKimberlina-Reservoir: The geophysical properties were also developed under DOE\u2019s NRAP. It is based on a potential CO2 storage site in the Southern San Joaquin Basin of California (Alumbaugh et al., 2021). We use this dataset to test our method in the EM inversion problem. In this data, there are 780 EM data as the geophysical measurement, and corresponding conductivity as the geophysical property. We use 750/30 as training and testing. EM data are simulated by finite-difference method (Commer & Newman, 2008) with two sources location at x = 2.5 km, z = 3.025 km and x = 4.5 km, z = 2.5 km. There are S = 8 source frequencies from 0.1 to 8.0 Hz and recorded with its real and imaginary part. The conductivity is with the size of 351\u00d7601 (H\u00d7W ), where the grid is 10m in all dimensions."
        },
        {
            "heading": "4.1.2. TRAINING DETAILS",
            "text": "The input seismic data and EM data are normalized to the range [-1, 1]. In practice, to supply more information, it always uses multiple sources to measure, where s \u2208 [1, \u00b7 \u00b7 \u00b7S] is the index of different sources. After integration, all sources vectors will be concatenated. For the seismic data, we use \u03a6n(x, t) = sin(n\u03c0t)1(x)/(xmax \u2212 xmin) as the kernel function. However, for the EM data, since the raw data are already in the frequency domain and the input size is small, we skip the integral transformation step.\nThe Gaussian kernel can be represented as \u03a8m(x, z) = exp\n\u2212\u2016(x,z)\u2212\u00b5m\u201622 2\u03c32 . We let \u00b5m distribute evenly over the output shape. Then, the \u03c3 is set equal to the distance of adjacent \u00b5. When applying Ridge regression to solve the linear layer in the encoder, and set the regularization parameter \u03b1 = 1.\nWe employ AdamW (Loshchilov & Hutter, 2018) optimizer with momentum parameters \u03b21 = 0.5, \u03b22 = 0.999 and a weight decay of 1\u00d7 10\u22124 to update decoder parameters of the network. The initial learning rate is set to be 1\u00d7 10\u22123, and we decay the learning rate with a cosine annealing (Loshchilov & Hutter, 2016), where T0 = 5, Tmult = 2 and the minimum learning rate is set to be 1\u00d710\u22123. The size of every mini-batch is set to be 128. We implement our models in Pytorch and train them on 1 NVIDIA Tesla V100 GPU."
        },
        {
            "heading": "4.1.3. EVALUATION METRICS",
            "text": "We apply three metrics to evaluate the geophysical properties generated by our method: MAE, MSE and Structural Similarity (SSIM). In the existing literature (Wu & Lin, 2019; Zhang & Lin, 2020), both MAE and MSE have been employed to measure the pixel-wise error. SSIM is also con-\nsidered to measure the perceptual similarity (Jin et al., 2022), since both velocity maps and conductivity have highly structured information, and degradation or distortion can be easily perceived by a human. Note that when calculating MAE and MSE, we denormalize geophysical properties to their original scale while we keep them in the normalized scale [\u22121, 1] for SSIM according to the algorithm.\nMoreover, we also employ two common metrics to measure the complexity and computational cost of the model: the number of parameters (#Parameters) and Floating-point operations per second (FLOPs)."
        },
        {
            "heading": "4.2. Main Results",
            "text": "Table 1 shows the comparison results of our method with InversionNet on different datasets. Overall, our method achieves comparable or even better performance with a smaller amount of parameters and lower FLOPs. Below, we will provide in detail the comparison of all four datasets. It may be worthwhile mentioning that FWI is a quantitative inversion technique, meaning that it will yield both the shape and the quantitative values of the subsurface property.\nResults on Kimerlina-Leakage: Compared to InversionNet and VelocityGAN, our method outperforms in MAE, slightly worse in MSE and SSIM. However, our InvLINT only needs less than 1/10 parameters and FLOPs. This demonstrates the power of our model, and further validates the properties we found. The velocity maps inverted by ours and InversionNet are shown in the first two rows of Figure 3. In the second example, despite of some noise produced by our method in the background, the CO2 leakage plume (most important region as boxed out in green) has been very well imaged. Compared to ground truth, our method yields even better quantitative values than that obtained by InversionNet.\nResults on Marmousi: Marmousi is a more challenging dataset due to its more complex structure. Compared to InversionNet, our method outperforms in all three metrics with significantly less computational and memory cost (about 1/20 parameters and FLOPs). This result again demonstrates not only the power of our model but also the validity of the near-linear relationship that we found. However, in such a large and complex dataset, VelocityGAN outperform others, where the GAN structure helps generating better results. The velocity maps inverted by ours and InversionNet are illustrated in the third and fourth rows of Figure 3. Our InvLINT and InversionNet perform comparably in both the shallow and deep regions compared to the ground truth.\nResults on Salt: Compared to InversionNet, our method outperforms in MAE, and is slightly worse in MSE and SSIM with a very small gap. Moreover, our method uses 1/8 parameters and 1/5 FLOPs to those of InversionNet.\nNote that, in this challenging dataset, which only has a small number of samples, VelocityGAN cannot converge well and yields bad results. This is a side effect of its complex structure. The velocity maps inverted by ours and InversionNet are illustrated in the fifth and sixth rows of Figure 3. Consistent with quantitative results, both methods generate similar results. In the shallow region, our method output a slightly clear structure; but in a deeper region (e.g., the red region in the first example), the output of InversionNet is a little close to the ground truth. However, the overall difference can be hard to distinguish. Our method achieves comparable results with much less complexity.\nResults on Kimberlina-Reservoir: Compared to InversionNet and VelocityGAN, our method outperforms in all three metrics, with 1/2 parameters and 1/12 FLOPs to those of InversionNet. Because of the compact input, all model utilize the much smaller number of parameters. However, due to the simple architecture, InvLINT yields significantly fewer FLOPs but achieves better inversion accuracy. The conductivity results inverted by different models are shown in the last two rows of Figure 3. Contrary to previous results on the Kimberlina-Leakage dataset, our model yields clearer results. In the first example, we can see that the outputs of our model are less noisy; and in the second case, InvLINT inverts the deep region more precisely, as highlighted by the red squares. This is also consistent with the quantitative results.\nAt the same time, we find that the number of parameters of our model varies less for the same inverse problem. The number of model parameters is relatively independent of data size. In contrast, the previous methods are greatly affected by the input and output sizes. Moreover, our model not only requires fewer parameters, but also enables more efficient training and inference. When training on Marmousi dataset using 1 GPU (NVIDIA Quadro RTX 8000), our model is 9 times faster than InversionNet/VelocityGAN (1 hour vs. 9 hours). We also tested inference runtime with batch size 1 on a single thread of an Intel(R) Xeon(R) CPU Gold 6248 v3 (2.5GHz). Our model is 16 times faster than InversionNet/VelocityGAN (5 ms vs. 80ms). The small model size is suitable for memory-limited mobile devices. More visualization results are provided in the Appendix for readers who might be interested."
        },
        {
            "heading": "4.3. Ablation Tests",
            "text": "In this part, we will test how different kernel functions and network architectures will influence the performance of our method. We put our default setting at the first row of each table. For ease of illustration, we only provide results on Marmousi. Results on Kimberlina Leakage are given in the Appendix.\nDifferent Encoder Kernels\nFirst, we conduct experiments by replacing the 1D sine kernel in the encoder to different 2D sine kernels. The quantitative results are shown in the Table 2. By comparing the results in Marmousi and the results in Kinberlina-Leakage (shown in the Appendix), we can see that the optimal strategy to integrate over x axis is distinct for different datasets. In Marmousi, using kernel sin(n\u03c0t) cos(n\u03c0x) can improve the performance a lot. This kernel, however, does not perform well on other datasets (e.g., Kimberlina-Leakage)."
        },
        {
            "heading": "Dataset Encoder Kernel MAE\u2193 MSE\u2193 SSIM\u2191",
            "text": ""
        },
        {
            "heading": "Different Decoder Kernels",
            "text": "Then, we test different kernels for geophysical properties. In particular, we evaluate a series of 2D kernels: different 2D sine kernels, a sinc function kernel (sin(\u03c0\u2016r\u2212\u00b5m\u20162)/\u2016r\u2212 \u00b5m\u20162), and a Gaussian kernel with a smaller variance, noted as Gaussian\u03c3. For the sinc function, the choice of \u00b5m is the same as the Gaussian kernel, while for Gaussian\u03c3, we choose \u03c3 as 1/3 of the original. The quantitative results are shown in Table 3. As we can see, our choice of kernel outperforms the rest kernels. A smaller variance of Gaussian will yield a slightly worse result, while sinc kernel performs similarly to the Gaussian\u03c3."
        },
        {
            "heading": "Different Number of Kernels",
            "text": "We also test different numbers of kernels for both Sine and Gaussian. We evaluate performance over a 6\u00d76 grid where the dimensions of U and Y vary from 128 to 4096. The quantitative results are shown in Figure 4. Results indicate that the current selection of dimensions is appropriate. Obviously, reducing the model\u2019s size reduces its capacity, while choices of higher dimension are more prone to overfit. However, choosing a small dimension yields a smaller number of parameters and FLOPs. One can easily balance the performance and the cost based on his requirements and resources, indicating the flexibility of our model.\nDifferent Decoder Architectures\nWe aim to design an effective and efficient decoder to reverse the integral transform over a velocity map. The shifted Gaussian kernels used in integral transform split the velocity map into overlapping windows and encode the local structure within each window. To reconstruct the global structure of the velocity map from these local features, we leverage the transformer\u2019s power in modeling long-range interaction in a single layer. Options like conv/deconv require more layers to cover long range.\nTo better illustrate this, we test the performance of different decoder architecture. Results are provided in Table 4. A transformer layer followed by a linear layer is a more accurate decoder than shallow conv/deconv layers. Deeper decoders with more conv/deconv layers achieve more accurate results, but require a larger model. When using the deconv decoder of InversionNet in our method, we achieve better performance, clearly outperforming InversionNet (MAE 126.6 vs. 149.7)."
        },
        {
            "heading": "Results for a Larger Decoder",
            "text": "Here, we evaluate our method with a larger/deeper decoder. Firstly, we test it using multiple unshared linear layers, rather than a shared one Lr1, in the last part of our decoder. Furthermore, we evaluate our model with a deeper transformer. The quantitative results are shown in Table 5. The result using unshared linear layers indicates that a single linear layer is enough and the model does not benefit from more parameters. On the other hand, a deeper transformer can improve the performance. Similar to the number of kernels, the balance is based on requirements."
        },
        {
            "heading": "4.4. Singular Value Analysis",
            "text": "Another major benefit of our simplified model is the ease of analysis. Since we use only one linear layer in the encoder, we can analyze it by performing singular value decomposition. The results are shown in Figure 5. Since the singular value varies greatly in different datasets, we divide it by its maximum value to normalize it and trunk it at 150 dim. Results indicate that for all datasets, the number of essential dimensions is less than 100. In other words, a 100-dimensional latent space is sufficient to represent the data. Specifically, we can see that a ten-dimensional latent space is enough for Kimberlina-Reservoir dataset. That answers why the required number of parameters of both our InvLINT and InversionNet on Kimberlina-Reservoir datasets are much smaller than that on other datasets. All in all, with such a simple architecture, our InvLINT is able to not only help in analyzing the problem but also help us quantify the difficulty of different datasets."
        },
        {
            "heading": "4.5. Comparison to traditional FWI:",
            "text": "We performed new comparison with a widely used traditional FWI method (i.e., Multiscale FWI (Virieux & Operto, 2009)) on three seismic FWI datasets (Marmousi, Kimberlina-Leakage, Salt). Our method is consistently better on 3 datasets (MAE: 11.7 vs. 42.0 in KimberlinaLeakage, 140.7 vs. 199.5 in Marmousi, 26.1 vs. 176.6 in Salt). The traditional FWI requires a good initial guess\nand optimization per sample, resulting in slow processing (e.g., 4 hours per sample in Kimberlina-Leakage). Due to the limited rebuttal duration, we ran the comparison over 5 samples per dataset."
        },
        {
            "heading": "5. Related works",
            "text": ""
        },
        {
            "heading": "5.1. Data-driven Methods for FWI",
            "text": "Recently, based on deep learning, a new type of method has been developed. Araya-Polo et al. (2018) use a fully connected network to invert velocity maps in FWI. Wu & Lin (2019) consider the FWI as an image-to-image translation problem, and employ encoder-decoder CNN to solve. By using generative adversarial networks (GANs) and transfer learning, Zhang et al. (2019) achieved improved performance. In Zeng et al. (2021), authors present an efficient and scalable encoder-decoder network for 3D FWI. Feng et al. (2021) develop a multi-scale framework with two convolutional neural networks to reconstruct the low- and high-frequency components of velocity maps. A thorough review on deep learning for FWI can be found in Adler et al. (2021)."
        },
        {
            "heading": "5.2. Physics-informed machine learning",
            "text": "Previous pure data-driven methods can be considered as incorporating physic information in training data. On the other hand, integrating the physic knowledge into loss function or network architecture is another direction. All of them are called Physics-informed neural networks (PINN). Raissi et al. proposed utilizing nonlinear PDEs in the loss function as a soft constrain (Raissi et al., 2019). Through a hard constraint projection, Chen et al. proposed a framework to ensure model\u2019s predictions strictly conform to physical mechanisms (Chen et al., 2021). Based on the universal approximation theorem of operators, in Lu et al. (2021), authors proposed DeepONet to learn continuous operators or complex systems. Sun et al. (2021) proposed a hybrid network design, which involves deterministic, physics-based modeling and data-driven deep learning. A comprehensive review of PINN can be found in Karniadakis et al. (2021)."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we find an intriguing property of geophysics inversion: a near-linear relationship between the input and output, after applying integral transform in high dimensional space. Furthermore, this property can be easily turned into a light-weight encoder-decoder network for inversion. The encoder contains the integration of seismic data and the linear transformation without fine-tuning. The decoder consists of a single transformer block to reverse the integral of velocity with Gaussian kernels.\nExperiments show that this interesting property holds for two geophysics inversion problems over four different datasets. Compared to much deeper InversionNet, our method achieves comparable accuracy, but consumes significantly fewer parameters."
        },
        {
            "heading": "A. Appendix",
            "text": "A.1. Inversion Results of Different Datasets"
        },
        {
            "heading": "A.2. Ablation Test on Kimberlina Leakage",
            "text": "The ablation test results on Kimberlina Leakage are shown in Table 6-10"
        },
        {
            "heading": "Dataset Encoder Kernel MAE\u2193 MSE\u2193 SSIM\u2191",
            "text": ""
        },
        {
            "heading": "Dataset Decoder Kernel MAE\u2193 MSE\u2193 SSIM\u2191",
            "text": ""
        },
        {
            "heading": "Dataset #kernel MAE\u2193 MSE\u2193 SSIM\u2191",
            "text": ""
        },
        {
            "heading": "Dataset Architecture MAE\u2193 MSE\u2193 SSIM\u2191",
            "text": ""
        },
        {
            "heading": "A.3. Regression Results for the Encoder Linear Layer",
            "text": "We also show here the regression results of the linear layer in our encoder on different datasets in Table 11. As a reference, we also show the range and mean of the regression target value as ymax-ymin, |ymean|. The result demonstrate that how well the regressions are fitted."
        }
    ],
    "title": "An Intriguing Property of Geophysics Inversion",
    "year": 2022
}