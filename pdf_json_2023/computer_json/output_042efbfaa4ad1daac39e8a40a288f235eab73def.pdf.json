{
    "abstractText": "This position paper argues for the use of structured generative models (SGMs) for scene understanding. This requires the reconstruction of a 3D scene from an input image, whereby the contents of the image are causally explained in terms of models of instantiated objects, each with their own type, shape, appearance and pose, along with global variables like scene lighting and camera parameters. This approach also requires scene models which account for the cooccurrences and inter-relationships of objects in a scene. The SGM approach has the merits that it is compositional and generative, which lead to interpretability. To pursue the SGM agenda, we need models for objects and scenes, and approaches to carry out inference. We first review models for objects, which include \u201cthings\u201d (object categories that have a well defined shape), and \u201cstuff\u201d (categories which have amorphous spatial extent). We then move on to review scene models which describe the inter-relationships of objects. Perhaps the most challenging problem for SGMs is inference of the objects, lighting and camera parameters, and scene inter-relationships from input consisting of a single or multiple images. We conclude with a discussion of issues that need addressing to advance the SGM agenda.",
    "authors": [
        {
            "affiliations": [],
            "name": "Christopher K. I. Williams"
        }
    ],
    "id": "SP:ab76dec4a67ef47f347f1edd4af29aac4e988a7b",
    "references": [
        {
            "authors": [
                "S. Antol",
                "A. Agrawal",
                "J. Lu",
                "M. Mitchell",
                "D. Batra",
                "C. Zitnick",
                "D. Parikh"
            ],
            "title": "VQA: Visual Question Answering",
            "venue": "2015 IEEE International Conference on Computer Vision (ICCV), pages 2425\u20132433.",
            "year": 2015
        },
        {
            "authors": [
                "K.O. Babalola",
                "T.F. Cootes",
                "C.J. Twining",
                "V. Petrovic",
                "C. Taylor"
            ],
            "title": "3D Brain Segmentation Using Active Appearance Models and Local Regressors",
            "venue": "Metaxas, D., Axel, L., Fichtinger, G., and Sz\u00e9kely, G., editors, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2008), pages 401\u2013408. Springer. Lecture Notes in Computer Science, vol 5241.",
            "year": 2008
        },
        {
            "authors": [
                "Y. Bengio",
                "J. Louradour",
                "R. Collobert",
                "J. Weston"
            ],
            "title": "Curriculum Learning",
            "venue": "Bottou, L. and Littman, M., editors, Proceedings of the Twenty-Sixth International Conference on Machine Learning (ICML 2009), pages 41\u201348.",
            "year": 2009
        },
        {
            "authors": [
                "J.M. Bernardo",
                "A.F.M. Smith"
            ],
            "title": "Bayesian Theory",
            "venue": "Wiley, Chichester, UK.",
            "year": 1994
        },
        {
            "authors": [
                "I. Biederman"
            ],
            "title": "Recognition-by-components: A theory of human image understanding",
            "venue": "Psychological Review, 94:115\u2013147.",
            "year": 1987
        },
        {
            "authors": [
                "I. Biederman",
                "R.J. Mezzanotte",
                "J.C. Rabinowitz"
            ],
            "title": "Scene Perception: Detecting and Judging Objects Undergoing Relational Violations",
            "venue": "Cognitive Psychology, 14:143\u2013177.",
            "year": 1982
        },
        {
            "authors": [
                "V. Blanz",
                "T. Vetter"
            ],
            "title": "A Morphable Model For The Synthesis Of 3D Faces",
            "venue": "SIGGRAPH 99: Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, pages 187\u2013194. ACM Press.",
            "year": 1999
        },
        {
            "authors": [
                "G.E.P. Box",
                "N.R. Draper"
            ],
            "title": "Empirical Model-Building and Response Surfaces",
            "venue": "John Wiley & Sons.",
            "year": 1987
        },
        {
            "authors": [
                "P. Brodatz"
            ],
            "title": "Textures: A Photographic Album for Artists and Designers",
            "venue": "Dover Publications, New York.",
            "year": 1966
        },
        {
            "authors": [
                "J. Collins",
                "S. Goael",
                "K Deng"
            ],
            "title": "ABO: Dataset and Benchmarks for Real-World 3D Object Understanding",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "T.F. Cootes",
                "G.J. Edwards",
                "C.J. Taylor"
            ],
            "title": "Active Appearance Models",
            "venue": "Burkhardt, H. and Neumann, B., editors, Proceedings of the Fifth European Conference on Computer Vision, ECCV 1998, pages II 484\u2013498. Springer. Lecture Notes in Computer Science 1407.",
            "year": 1998
        },
        {
            "authors": [
                "T.F. Cootes",
                "C.J. Taylor",
                "D.H. Cooper",
                "J. Graham"
            ],
            "title": "Active Shape Models\u2014Their Training and Application",
            "venue": "Computer Visoin and Image Understanding, 61(1):38\u201359.",
            "year": 1995
        },
        {
            "authors": [
                "T.M. Cover",
                "J.A. Thomas"
            ],
            "title": "Elements of Information Theory",
            "venue": "John Wiley & Sons.",
            "year": 1991
        },
        {
            "authors": [
                "P. Dayan",
                "G.E. Hinton",
                "R.M. Neal",
                "R.S. Zemel"
            ],
            "title": "The Helmholtz Machine",
            "venue": "Neural Computation, 7(5):889\u2013904.",
            "year": 1995
        },
        {
            "authors": [
                "F. De Sousa Ribeiro",
                "K. Duarte",
                "M. Everett",
                "G. Leontidis",
                "M. Shah"
            ],
            "title": "Learning with Capsules: A Survey",
            "venue": "arXiv:2206.02664.",
            "year": 2022
        },
        {
            "authors": [
                "A.A. Efros",
                "T.K. Leung"
            ],
            "title": "Texture Synthesis by Non-parametric Sampling",
            "venue": "Proc International Conference on Computer Vision (ICCV 1999), pages 1033\u20131038.",
            "year": 1999
        },
        {
            "authors": [
                "S.M.A. Eslami",
                "N. Heess",
                "T. Weber",
                "Y. Tassa",
                "D. Szepesvari",
                "K. Kavukcuoglu",
                "G.E. Hinton"
            ],
            "title": "Attend, infer, repeat: Fast scene understanding with generative models",
            "venue": "Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems.",
            "year": 2016
        },
        {
            "authors": [
                "S.M.A. Eslami",
                "C.K.I. Williams"
            ],
            "title": "Factored Shapes and Appearances for Parts-based Object Understanding",
            "venue": "Proceedings of the British Machine Vision Conference 2011.",
            "year": 2011
        },
        {
            "authors": [
                "S.M.A. Eslami",
                "C.K.I. Williams"
            ],
            "title": "A Generative Model for Parts-based Object Segmentation",
            "venue": "Bartlett, P., Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors, Advances in Neural Information Processing Systems 25, pages 100\u2013107.",
            "year": 2012
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K.I. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The PASCAL Visual Object Classes (VOC) Challenge",
            "venue": "International Journal of Computer Vision, 88(2):303\u2013338.",
            "year": 2010
        },
        {
            "authors": [
                "P. Felzenszwalb",
                "R. Girshick",
                "D. McAllester",
                "D. Ramanan"
            ],
            "title": "Object Detection with Discriminatively Trained Part Based Models",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(9):1627\u20131645.",
            "year": 2009
        },
        {
            "authors": [
                "M.A. Fischler",
                "R.C. Bolles"
            ],
            "title": "Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Communications of the ACM, 24(6):381\u2013395.",
            "year": 1981
        },
        {
            "authors": [
                "M.A. Fischler",
                "R.A. Elschlager"
            ],
            "title": "The Representation and Matching of Pictoral Structures",
            "venue": "IEEE Transactions on Computers, 22(1):67\u201392.",
            "year": 1973
        },
        {
            "authors": [
                "D.A. Forsyth",
                "J. Ponce"
            ],
            "title": "Computer Vision: A Modern Approach",
            "venue": "Prentice Hall, second edition.",
            "year": 2003
        },
        {
            "authors": [
                "B.J. Frey",
                "N. Jojic"
            ],
            "title": "Transformation Invariant Clustering Using the EM Algorithm",
            "venue": "IEEE Trans Pattern Analysis and Machine Intelligence,",
            "year": 2003
        },
        {
            "authors": [
                "H. Fu",
                "B. Cai",
                "L. Gao",
                "Zhang",
                "L.-X.",
                "J. Wang",
                "C. Li",
                "Q. Zeng",
                "C. Sun",
                "R. Jia",
                "B. Zhao",
                "H. Zhang"
            ],
            "title": "3D-FRONT: 3D furnished rooms with layouts and semantics",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10933\u201310942.",
            "year": 2021
        },
        {
            "authors": [
                "K.S. Fu"
            ],
            "title": "Syntactic Pattern Recognition and Applications",
            "venue": "Prentice-Hall.",
            "year": 1982
        },
        {
            "authors": [
                "B. Galerne",
                "Y. Gousseau",
                "Morel",
                "J.-M."
            ],
            "title": "Random Phase Textures: Theory and Synthesis",
            "venue": "IEEE Transactions on Image Processing, 20(1):257\u2013267.",
            "year": 2011
        },
        {
            "authors": [
                "K. Greff",
                "R.L. Kaufman",
                "R. Kabra",
                "N. Watters",
                "C. Burgess",
                "D. Zoran",
                "L. Matthey",
                "M. Botvinick",
                "A. Lerchner"
            ],
            "title": "Multi-Object Representation Learning with Iterative Variational Inference",
            "venue": "ICML, volume 97 of Proceedings of Machine Learning Research, pages 2424\u20132433.",
            "year": 2019
        },
        {
            "authors": [
                "U. Grenander"
            ],
            "title": "Lectures in Pattern Theory: Volume 2 Pattern Analysis",
            "venue": "Springer-Verlag.",
            "year": 1978
        },
        {
            "authors": [
                "P.A.V. Hall"
            ],
            "title": "Equivalence Between AND/OR Graphs and Context-Free Grammars",
            "venue": "Communications of the ACM, 16(7):444\u2013445.",
            "year": 1973
        },
        {
            "authors": [
                "A.R. Hanson",
                "E.M. Riseman"
            ],
            "title": "VISIONS: A Computer System for Interpreting Scenes",
            "venue": "Hanson, A. R. and Riseman, E. M., editors, Computer Vision Systems. Academic Press.",
            "year": 1978
        },
        {
            "authors": [
                "G. Heitz",
                "D. Koller"
            ],
            "title": "Learning Spatial Context: Using Stuff to Find Things",
            "venue": "Proc ECCV 2008.",
            "year": 2008
        },
        {
            "authors": [
                "G.E. Hinton"
            ],
            "title": "Training Products of Experts by Minimizing Contrastive Divergence",
            "venue": "Neural Computation, 14:1771\u20131800.",
            "year": 2002
        },
        {
            "authors": [
                "G.E. Hinton",
                "A. Krizhevsky",
                "S.D. Wang"
            ],
            "title": "Transforming Auto-encoders",
            "venue": "Proceedings ICANN 2011.",
            "year": 2011
        },
        {
            "authors": [
                "G.E. Hinton",
                "S. Sabour",
                "N. Frosst"
            ],
            "title": "Matrix capsules with EM routing",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "M. Hueting",
                "P. Reddy",
                "V.G. Kim",
                "E. Yumer",
                "N. Carr",
                "N.J. Mitra"
            ],
            "title": "Seethrough: Finding objects in heavily occluded indoor scene images",
            "venue": "Proceedings of International Conference on 3DVision (3DV).",
            "year": 2018
        },
        {
            "authors": [
                "H. Izadinia",
                "Q. Shan",
                "S.M. Setiz"
            ],
            "title": "IM2CAD",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017.",
            "year": 2017
        },
        {
            "authors": [
                "N. Jammalamadaka",
                "A. Zisserman",
                "M. Eichner",
                "V. Ferrari",
                "C. Jawahar"
            ],
            "title": "Has My Algorithm Succeeded? An Evaluator for Human Pose Estimators",
            "venue": "Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., and Schmid, C., editors, Computer Vision\u2014ECCV 2012. Springer. Lecture Notes in Computer Science 7574.",
            "year": 2012
        },
        {
            "authors": [
                "W. Jang",
                "L. Agapito"
            ],
            "title": "CodeNeRF: Disentangled Neural Radiance Fields for Object Categories",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 12949\u201312958. Also available as arXiv:2109.01750.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Jin",
                "S. Geman"
            ],
            "title": "Context and Hierarchy in a Probabilistic Image Model",
            "venue": "In Proc CVPR",
            "year": 2006
        },
        {
            "authors": [
                "J. Johnson",
                "B. Hariharan",
                "L. van der Maaten",
                "L. Fei-Fei",
                "C.L. Zitnick",
                "R. Girshick"
            ],
            "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning",
            "year": 2017
        },
        {
            "authors": [
                "M.I. Jordan",
                "Z. Ghahramani",
                "Jaakkola",
                "L.K.T.S. Saul"
            ],
            "title": "An Introduction to Variational Methods for Graphical Models",
            "venue": "Machine Learning, 37:183\u2013233.",
            "year": 1999
        },
        {
            "authors": [
                "H. Kato",
                "D. Beker",
                "M. Morariu",
                "T. Ando",
                "T. Matsuoka",
                "W. Kehl",
                "A. Gaidon"
            ],
            "title": "Differentiable Rendering: A Survey",
            "venue": "https://arxiv.org/2006.12057.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kim",
                "C. Dyer",
                "A. Rush"
            ],
            "title": "Compound Probabilistic Context-Free Grammars for Grammar Induction",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369\u20132385.",
            "year": 2019
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "venue": "ICLR.",
            "year": 2014
        },
        {
            "authors": [
                "J.J. Kivinen",
                "C.K.I. Williams"
            ],
            "title": "Multiple Texture Boltzmann Machines",
            "venue": "Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics.",
            "year": 2012
        },
        {
            "authors": [
                "A. Kosiorek",
                "S. Sabour",
                "Y.W. Teh",
                "G.E. Hinton"
            ],
            "title": "Stacked Capsule Autoencoders",
            "venue": "Wallach, H., Larochelle, H., Beygelzimer, A., d\u2019Alch\u00e9 Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems 32.",
            "year": 2019
        },
        {
            "authors": [
                "T.D. Kulkarni",
                "W.F. Whitney",
                "P. Kohli",
                "J. Tenenbaum"
            ],
            "title": "Deep convolutional inverse graphics network",
            "venue": "Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 28.",
            "year": 2015
        },
        {
            "authors": [
                "M. Land",
                "N. Mennie",
                "J. Rusted"
            ],
            "title": "The roles of vision and eye movements in the control of activities of daily living",
            "venue": "Perception, 28:1311\u20131328.",
            "year": 1999
        },
        {
            "authors": [
                "H. Lee",
                "R. Grosse",
                "R. Ranganath",
                "A.Y. Ng"
            ],
            "title": "Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations",
            "venue": "Bottou, L. and Littman, M., editors, Proceedings of the Twenty-Sixth International Conference on Machine Learning (ICML 2009).",
            "year": 2009
        },
        {
            "authors": [
                "J. Lee",
                "Y. Lee",
                "J. Kim",
                "A.R. Kosiorek",
                "S. Choi",
                "Teh",
                "Y.-W."
            ],
            "title": "Set Transformer: A Framework for Attention-based Permutation-invariant Neural Networks",
            "venue": "Proc. of the 36th International Conference on Machine Learning, pages 3744\u20133753.",
            "year": 2019
        },
        {
            "authors": [
                "M. Li",
                "A. Gadi Patil",
                "K. Xu",
                "S. Chaudhuri",
                "O. Khan",
                "A. Shamir",
                "C. Tu",
                "B. Chen",
                "D. Cohen-Or",
                "H. Zhang"
            ],
            "title": "GRAINS: Generative Recursive Autoencoders for Indoor Scenes",
            "venue": "ACM Transactions on Graphics, 37(2):1\u201316.",
            "year": 2019
        },
        {
            "authors": [
                "N. Li",
                "C. Eastwood",
                "R. Fisher"
            ],
            "title": "Learning Object-Centric Representations of Multi-Object Scenes from Multiple Views",
            "venue": "Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems, volume 33, pages 5656\u20135666.",
            "year": 2020
        },
        {
            "authors": [
                "T. Liu",
                "S. Chaudhuri",
                "V. Kim",
                "Huang",
                "Q.-X",
                "N.J. Mitra",
                "T. Funkhouser"
            ],
            "title": "Creating Consistent Scene Graphs Using a Probabilistic Grammar",
            "venue": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia),",
            "year": 2014
        },
        {
            "authors": [
                "F. Locatello",
                "D. Weissenborn",
                "T. Unterthiner",
                "A. Mahendran",
                "G. Heigold",
                "J. Uszkoreit",
                "A. Dosovitskiy",
                "T. Kipf"
            ],
            "title": "Object-Centric Learning with Slot Attention",
            "venue": "Advances in Neural Information Processing Systems 33 (NeurIPS 2020), pages 11525\u201311538.",
            "year": 2020
        },
        {
            "authors": [
                "M. Loper",
                "N. Mahmood",
                "J. Romero",
                "G. Pons-Moll",
                "M. Black"
            ],
            "title": "SMPL: a Skinned Multi-person Linear Model",
            "venue": "ACM Transactions on Graphics, 34(6).",
            "year": 2015
        },
        {
            "authors": [
                "M.M. Loper",
                "M.J. Black"
            ],
            "title": "OpenDR: An Approximate Differentiable Renderer",
            "venue": "Computer Vision-ECCV 2014, pages 154\u2013169.",
            "year": 2014
        },
        {
            "authors": [
                "D.J.C. MacKay"
            ],
            "title": "Information Theory, Inference, and Learning Algorithms",
            "venue": "Cambridge University Press, Cambridge.",
            "year": 2003
        },
        {
            "authors": [
                "G. Marcus",
                "E. Davis",
                "S. Aaronson"
            ],
            "title": "A very preliminary analysis of DALL-E 2",
            "venue": "arXiv:2204.13807.",
            "year": 2022
        },
        {
            "authors": [
                "B. Mildenhall",
                "P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
            "venue": "Vedaldi, A., Bischof, H., Brox, T., and Frahm, J., editors, Computer Vision\u2013ECCV 2020. Springer. Lecture Notes in Computer Science 12346.",
            "year": 2020
        },
        {
            "authors": [
                "P. Moreno",
                "C.K.I. Williams",
                "C. Nash",
                "P. Kohli"
            ],
            "title": "Overcoming occlusion with inverse graphics",
            "venue": "Gang, H. and Jegou, H., editors, Computer Vision-ECCV 2016 Workshops Proceedings Part III, pages 170\u2013185. Springer. LNCS 9915.",
            "year": 2016
        },
        {
            "authors": [
                "K.P. Murphy"
            ],
            "title": "Machine Learning: a Probabilistic Perspective",
            "venue": "MIT Press.",
            "year": 2012
        },
        {
            "authors": [
                "A. Nazabal",
                "N. Tsagkas",
                "C.K.I. Williams"
            ],
            "title": "Inference and Learning for Generative Capsule Models",
            "venue": "arXiv:2209.03115.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ohta",
                "T. Kanade",
                "T. Sakai"
            ],
            "title": "An Analysis System for Scenes Containing Objects with Substructures",
            "venue": "Proc. 4th IJCPR, pages 752\u2013754.",
            "year": 1978
        },
        {
            "authors": [
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Building the gist of a scene: the role of global image features in recongition",
            "venue": "Progress in Brain Research, 155:23\u201336.",
            "year": 2006
        },
        {
            "authors": [
                "S.E. Palmer"
            ],
            "title": "Vision Science: Photons to Phenomenology",
            "venue": "MIT Press.",
            "year": 1999
        },
        {
            "authors": [
                "D. Paschalidou",
                "A. Kar",
                "M. Shugrina",
                "K. Kreis",
                "A. Geiger",
                "S. Fidler"
            ],
            "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis",
            "venue": "Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems, volume 34, pages 12013\u201312026. Curran Associates, Inc.",
            "year": 2021
        },
        {
            "authors": [
                "S.J.D. Prince"
            ],
            "title": "Computer vision: models, learning and inference",
            "venue": "Cambridge University Press.",
            "year": 2012
        },
        {
            "authors": [
                "J.A. Quinn",
                "C.K.I. Williams",
                "N. McIntosh"
            ],
            "title": "Factorial Switching Linear Dynamical Systems applied to Physiological Condition Monitoring",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, 31(9):1537\u20131551.",
            "year": 2009
        },
        {
            "authors": [
                "A. Radford",
                "L. Metz",
                "S. Chintala"
            ],
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "venue": "In International Conference on Learning Representations",
            "year": 2016
        },
        {
            "authors": [
                "A. Ramesh",
                "M. Pavlov",
                "G. Goh",
                "S. Gray",
                "C. Voss",
                "A. Radford",
                "M. Chen",
                "I. Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 8821\u20138831. PMLR.",
            "year": 2021
        },
        {
            "authors": [
                "M. Ranzato",
                "V. Mnih",
                "G.E. Hinton"
            ],
            "title": "Generating more realistic images using gated MRF\u2019s",
            "venue": "Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A., editors, Advances in Neural Information Processing Systems, volume 23.",
            "year": 2010
        },
        {
            "authors": [
                "C.E. Rasmussen",
                "C.K.I. Williams"
            ],
            "title": "Gaussian Processes for Machine Learning",
            "venue": "MIT Press, Cambridge, Massachusetts.",
            "year": 2006
        },
        {
            "authors": [
                "D.J. Rezende",
                "S. Mohamed",
                "D. Wierstra"
            ],
            "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative Models",
            "venue": "ICML.",
            "year": 2014
        },
        {
            "authors": [
                "D. Ritchie"
            ],
            "title": "Generative Models of 3D Scenes",
            "venue": "Part of the tutorial on Learning Generative Models of 3D Structures at Eurographics 2019, https://3dstructgen.github.io.",
            "year": 2019
        },
        {
            "authors": [
                "D. Ritchie",
                "K. Wang",
                "Lin",
                "Y.-A."
            ],
            "title": "Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models",
            "venue": "CVPR 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L.G. Roberts"
            ],
            "title": "Machine Perception of Three-dimensional Solids",
            "venue": "Technical Report 315, MIT Lincoln Laboratory.",
            "year": 1963
        },
        {
            "authors": [
                "L. Romaszko",
                "C.K.I. Williams",
                "J. Winn"
            ],
            "title": "Learning Direct Optimization for Scene Understanding",
            "venue": "Pattern Recognition, 105:107369.",
            "year": 2020
        },
        {
            "authors": [
                "D. Ross",
                "R. Zemel"
            ],
            "title": "Learning parts-based representations of data",
            "venue": "Journal of Machine Learning Research, 7:2369\u20132397.",
            "year": 2006
        },
        {
            "authors": [
                "S. Roth",
                "M.J. Black"
            ],
            "title": "Fields of Experts: A Framework for Learning Image Priors",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2005, pages III:860\u2013867.",
            "year": 2005
        },
        {
            "authors": [
                "J.A. Rozanov"
            ],
            "title": "Markov Random Fields and Stochastic Partial Differential Equations",
            "venue": "Math. USSR Sbornik, 32(4):515\u2013534.",
            "year": 1977
        },
        {
            "authors": [
                "M. R\u00fcnz",
                "K. Li",
                "M. Tang",
                "L. Ma",
                "C. Kong",
                "T. Schmidt",
                "I. Reid",
                "L. Agapito",
                "J. Straub",
                "S. Lovegrove",
                "R. Newcombe"
            ],
            "title": "FroDO: From Detections to 3D Objects",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2020).",
            "year": 2020
        },
        {
            "authors": [
                "S. Sabour",
                "N. Frosst",
                "G. Hinton"
            ],
            "title": "Dynamic routing between capsules",
            "venue": "Advances in Neural Information Processing Systems, pages 3856\u20133866.",
            "year": 2017
        },
        {
            "authors": [
                "T. Salimans",
                "A. Karpathy",
                "X. Chen",
                "D.P. Kingma",
                "Y. Bulatov"
            ],
            "title": "PixelCNN++: A PixelCNN Implementation with Discretized Logistic Mixture Likelihood and Other Modifications",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2017).",
            "year": 2017
        },
        {
            "authors": [
                "J.L. Sch\u00f6nberger",
                "Frahm",
                "J.-M"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "S. Seth",
                "I. Murray",
                "C.K.I. Williams"
            ],
            "title": "Model Criticism in Latent Space",
            "venue": "Bayesian Analysis , 14(3):703\u2013725.",
            "year": 2018
        },
        {
            "authors": [
                "M. Shi",
                "H. Caesar",
                "V. Ferrari"
            ],
            "title": "Weakly Supervised Object Localization Using Things and Stuff Transfer",
            "venue": "Proc. International Conference on Computer Vision (ICCV 2017).",
            "year": 2017
        },
        {
            "authors": [
                "M. Sun",
                "B. Kim",
                "P. Kohli",
                "S. Savarese"
            ],
            "title": "Relating Things and Stuff via Object Property Interactions",
            "venue": "IEEE Trans Pattern Analysis and Machine Intelligence, 36(7):1370\u20131383.",
            "year": 2014
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "arXiv:1312.6199.",
            "year": 2013
        },
        {
            "authors": [
                "R. Szeliski"
            ],
            "title": "Computer Vision: Algorithms and Applications",
            "venue": "Springer, second edition.",
            "year": 2021
        },
        {
            "authors": [
                "J.B. Tenenbaum",
                "W.T. Freeman"
            ],
            "title": "Separating style and content with bilinear models",
            "venue": "Neural Computation, 12:1247\u20131283.",
            "year": 2000
        },
        {
            "authors": [
                "A. Torralba",
                "K.P. Murphy",
                "W.T. Freeman"
            ],
            "title": "Using the Forest to See the Trees: Exploiting Context for Visual Object Detection and Localization",
            "venue": "Communications of the ACM, 53(3):107\u2013 114.",
            "year": 2010
        },
        {
            "authors": [
                "Z. Tu",
                "Zhu",
                "S.-C."
            ],
            "title": "Image Segmentation by Data-Driven Markov Chain Monte Carlo",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, 24(5):657\u2013673.",
            "year": 2002
        },
        {
            "authors": [
                "G.J.J. van den Burg",
                "C.K.I. Williams"
            ],
            "title": "On memorization in probabilistic deep generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "A. van den Oord",
                "N. Kalchbrenner",
                "L. Espeholt",
                "kavukcuoglu",
                "O. Vinyals",
                "A. Graves"
            ],
            "title": "Conditional Image Generation with PixelCNN Decoders",
            "year": 2016
        },
        {
            "authors": [
                "M.A.O. Vasilescu",
                "D. Terzopoulos"
            ],
            "title": "Multilinear Analysis of Image Ensembles: TensorFaces",
            "venue": "Heyden, A., Sparr, G., Nielsen, M., and Johansen, P., editors, Computer Vision\u2013ECCV 2002. Springer. Lecture Notes in Computer Science 2350.",
            "year": 2002
        },
        {
            "authors": [
                "A. Wang",
                "A. Kortylewski",
                "A. Yuille"
            ],
            "title": "NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2021).",
            "year": 2021
        },
        {
            "authors": [
                "C.K.I. Williams",
                "M.K. Titsias"
            ],
            "title": "Greedy Learning of Multiple Objects in Images using Robust Statistics and Factorial Learning",
            "venue": "Neural Computation, 16(5):1039\u20131062.",
            "year": 2004
        },
        {
            "authors": [
                "Y. Xia",
                "Y. Zhang",
                "F. Liu",
                "W. Shen",
                "A. Yuille"
            ],
            "title": "Synthesize then Compare: Detecting Failures and Anomalies for Semantic Segmentation",
            "venue": "Vedaldi, A., Bischof, H., Brox, T., and Frahm, J., editors, Computer Vision\u2014ECCV 2020. Springer. Lecture Notes in Computer Science 12346.",
            "year": 2020
        },
        {
            "authors": [
                "B. Yao",
                "X. Yang",
                "Zhu",
                "S.-C"
            ],
            "title": "Introduction to a Large-Scale General Purpose Ground Truth Database: Methodology, Annotation Tool and Benchmarks",
            "venue": "In EMMCVPR\u201907: Proceedings of the 6th International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition",
            "year": 2007
        },
        {
            "authors": [
                "Yeh",
                "Y.-T.",
                "L. Yang",
                "M. Watson",
                "N.D. Goodman",
                "P. Hanrahan"
            ],
            "title": "Synthesizing open worlds with constraints using locally annealed reversible jump MCMC",
            "venue": "ACM Transactions on Graphics, 31(4):1\u201311.",
            "year": 2012
        },
        {
            "authors": [
                "Yu",
                "L.-F.",
                "Yeung",
                "S.-K.",
                "Tang",
                "C.-K.",
                "D. Terzopoulos",
                "T.F. Chan",
                "S.J. Osher"
            ],
            "title": "Make it home: automatic optimization of furniture arrangement",
            "venue": "ACM Transactions on Graphics, 30(4).",
            "year": 2011
        },
        {
            "authors": [
                "A.L. Yuille",
                "C. Liu"
            ],
            "title": "Deep Nets: What have they ever done for Vision",
            "year": 2021
        },
        {
            "authors": [
                "A.R. Zamir",
                "A. Sax",
                "T. Yeo",
                "O. Kar",
                "N. Cheerla",
                "R. Suri",
                "Z. Cao",
                "J. Mailk",
                "L. Guibas"
            ],
            "title": "Robust Learning Through Cross-Task Consistency",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "P.P. Srinivasan",
                "B. Deng",
                "P. Debevec",
                "W.T. Freeman",
                "J.T. Barron"
            ],
            "title": "NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination",
            "venue": "ACM Transactions on Graphics (Proc. of SIGGRAPH Asia), 40(6).",
            "year": 2021
        },
        {
            "authors": [
                "Zhu",
                "S.-C.",
                "S. Huang"
            ],
            "title": "Computer Vision: Stochastic Grammars for Parsing Objects, Scenes, and Events",
            "venue": "Springer.",
            "year": 2021
        },
        {
            "authors": [
                "Zhu",
                "S.-C.",
                "D. Mumford"
            ],
            "title": "A Stochastic Grammar of Images",
            "venue": "Foundations and Trends in Computer Graphics and Vision, 2(4):259\u2013362.",
            "year": 2006
        },
        {
            "authors": [
                "Zhu",
                "S.-C.",
                "Y. Wu",
                "D. Mumford"
            ],
            "title": "Filters, Random Fields and Maximum Entropy (FRAME): Towards a unified theory for texture modeling",
            "venue": "Int. J. Comput. Vision, 27(2):107\u2013126.",
            "year": 1998
        }
    ],
    "sections": [
        {
            "text": "To pursue the SGM agenda, we need models for objects and scenes, and approaches to carry out inference. We first review models for objects, which include \u201cthings\u201d (object categories that have a well defined shape), and \u201cstuff\u201d (categories which have amorphous spatial extent). We then move on to review scene models which describe the inter-relationships of objects. Perhaps the most challenging problem for SGMs is inference of the objects, lighting and camera parameters, and scene inter-relationships from input consisting of a single or multiple images. We conclude with a discussion of issues that need addressing to advance the SGM agenda.\nKeywords: structured generative models, generative models, compositionality, scene understanding."
        },
        {
            "heading": "1 Introduction",
            "text": "The goal of this position paper is to promote the use of structured generative models (SGMs) for scene understanding. These models are situated in the classical framework for computer vision whereby a 3D scene is reconstructed from one or more input images. In this case the contents of the image are causally explained in terms of models of instantiated objects, each with their own type, shape, appearance and pose, along with global variables like scene lighting and camera parameters. This approach also requires scene models which account for the co-occurrences and inter-relationships of objects in a scene. Because such models can generate (or reconstruct, or explain) the scene, and because they are structured (i.e. they are composed of multiple objects and their relationships), we term them structured generative models.\nThis reconstructive framework is also known as analysis-by-synthesis (Grenander, 1978), or visionas-inverse-graphics (VIG) (Kulkarni et al., 2015; Moreno et al., 2016). It can be traced back to the early days of \u201cblocks world\u201d research in the 1960s (Roberts, 1963). Other early work in this vein\nar X\niv :2\n30 2.\n03 53\n1v 1\n[ cs\n.C V\n] 7\nF eb\n2 02\n3\nincludes the VISIONS system of Hanson and Riseman (1978), and the system of Ohta et al. (1978) for outdoor scene understanding. For example, the VISIONS system used various levels of analysis (e.g., objects, surfaces), and mappings between the image-specific parse and generic knowledge about scenes.\nAlternatives to the VIG framework are either discriminative approaches, predicting some target quantity or quantities given input image(s), or unstructured generative models. Discriminative approaches are typically applied to solve specific tasks such as object detection or semantic segmentation, which are usually specified in image space. These are usually set up as supervised learning tasks, thus requiring annotated data. Currently deep neural networks (DNNs) are the dominant method-ofchoice for such tasks. DNNs are often highly accurate, but as discriminatively-trained models they can sometimes fail badly, producing absurd mistakes,1 but with no effective indication of unreliability. One example of this is performance failures on adversarial examples (see e.g., Szegedy et al. 2013). Also as the discriminative models are trained on specific datasets, they can often perform poorly when faced with the same task but on a novel dataset with different statistics (distribution shift). The focus on the evaluation of specific tasks means that the predictions from multiple tasks are not required to create a coherent understanding of the input image in terms of the 3D world; this point is made, e.g., by Zamir et al. (2020).\nWith unstructured generative models, images are generated from a single set of latent variables, without explicit modelling of objects and their interactions. An example is the work of Radford et al. (2016) where images of bedroom scenes generated by generative adversarial networks (GANs). Here there is a single latent vector representation for the whole scene, which is not disentangled across objects. This means, for example, that it is is very hard to edit the latent representation to make specific changes in the scene (e.g., to change the colour of the bedspread), as the representation is not interpretable.\nTo be clear, we are not arguing against the use of deep neural networks in computer vision. However, for structured generative models DNNs can be used for specific modelling and inference tasks (as we will see below), rather than as one big black box. A similar point is made by Yuille and Liu (2021), who argue (their sec. 7.3) that to handle the combinatorial explosion of possible images, computer vision systems need to be compositional and generative, and that this also leads to interpretability. These points are in excellent agreement with the formulation of structured generative models.\nFig. 1 shows how an input image can be explained in terms of 3D objects, the camera pose and the illumination to produce the reconstructed image. Sometimes a full 3D version may be too onerous, and we can consider a layered \u201c2.1D\u201d model, where the objects (people) are represented by \u201csprite\u201d models of the shape and appearance, along with the background, as in Figure 2. The layers have a depth ordering, so that occlusions can be explained.\nThe goals of this paper are: to promote the SGM viewpoint; to review relevant work on object and scene modelling, and inference with SGMs; and to identify gaps/outstanding issues where further research is needed. The structure of the paper is as follows: In sec. 1.1 we discuss the rich variety of tasks associated with scene understanding. Sec. 1.2 describes the general advantages of generative models, and sec. 1.3 discusses the pros and cons of structured generative models. Sec. 2 describes modelling objects, including \u201cthings\u201d (object categories that have a well defined shape) in sec. 2.1, including parts-based models, and \u201cstuff\u201d (categories which have amorphous spatial extent) in sec. 2.2. Sec. 3 covers models of the inter-relationships of objects, focusing mostly on indoor scenes. Having defined models for objects and scenes, sec. 4 discusses how inference for the SGM may be\n1The phrase \u201cabsurd mistakes\u201d is borrowed from Daniel Kahneman\u2019s talk at the NeurIPS conference in December 2021.\ncarried out, from input consisting of a single or multiple images. Sec. 5 discusses issues that are needed to advance the SGM agenda, including datasets and benchmarks.\nNote that this paper covers a very large amount of ground, and does not aim to provide comprehensive references for each topic. Indeed, the necessary topics cover much of the content of a textbook on computer vision. Rather, it aims to use prominent examples of work to provide an illustration of the various topics."
        },
        {
            "heading": "1.1 Scene Understanding",
            "text": "Before discussing scene understanding more generally, let\u2019s first look at two example images in Fig. 3, and see what we can extract from them. These are not particularly complex scenes\u2014it would be easy to pick images with a lot more objects and relations. Consider first the outdoor scene, Fig. 3(a)\u2014we can identify that this is not taken in a dense, \u201cdowntown\u201d area, but equally not in a very rural area. We can identify objects: a small herd of 6 cattle (one is likely a calf mostly hidden behind the white-andbrown cow near the centre of the image); 5 motor vehicles (one is half occluded); a building, some lamp posts; some trees; and some road signs. This focus on objects may have distracted us from the fact that large amounts of the images are \u201cstuff\u201d categories, such as road surface and grass. The cows are on the grass, which makes sense as they can graze there, but not on the road surface.\nThe second scene, Fig. 3(b), is an indoor scene of a dining room. We notice a table, 6 chairs (of the same or similar design), 5 tablemats on the table, a clock, a light fitting. The room has an outside door, a window, and a recessed area and a large wooden cabinet (?) off to the left. There is a polished wooden floor which allows some reflections, and the walls are a yellow-greenish colour. Closer inspection would pick up smaller details like light switches, a low-level radiator, some objects on the chairs (including perhaps a child\u2019s booster chair), and part of an indoor plant at the bottom right. We can also see some outdoor railings through the window, suggesting that there is a stairway up to the area outside the door.\nSo what is scene understanding? Part of it is about identifying the objects and the stuff that are visible, but it is more than this. For each object we would like to know its category, shape and its pose relative to the scene, and the materials it is made of. We also would like to understand the camera parameters (e.g., observer viewpoint) and lighting; this will help explain occlusions and shadows. For\nexample in Fig. 3(a) one can make inferences for the direction of the sun, given the shadows of the cattle. Such a 3D representation allows counterfactual questions, such as predicting how the image would change if an object was removed or added, or if it was viewed from a new direction (novel view synthesis, NVS). It also enables interaction by an agent in the scene, e.g., by attempting to herd the cattle.\nScene understanding also includes identifying the scene type (e.g., dining room), which will give rise to expectations of what objects should (and should not) be present. And it is about spatial, functional and semantic relationships between objects.2 For example in Fig. 3(a) we might find it surprising that the cattle are not fenced off from the road to minimize collisions, but this may depend on the norms of the location where the image was taken. And in Fig. 3(b) knowledge of dining rooms means we would likely expect as many tablemats as chairs to be set on the table\u2014in fact close inspection of the image suggests that the \u201cmissing\u201d tablemat is has been placed on the seat of the chair on the right hand side.\nOne rich approach to scene understanding is via answering questions. In Visual Question Answering (VQA; see, e.g., Antol et al. 2015), one typically expects textual responses, but this might not be the best way to answer certain questions, such as \u201cwhich pixels belong to the black cow near the centre of the image?\u201d. The PASCAL VOC challenges (Everingham et al., 2010) asked three questions: For classification the question was \u201cis there an object of class X in the image?\u201d. For detection, the task was to predict the bounding box of every object of class X in the image. And for segmentation the task was to label each pixel with one of the known class labels, or background. For the last two, textual responses are not the most natural way to answer the questions.\nThe epithet \u201ca picture is worth a thousand words\u201d also suggests that text is not the most efficient way to describe a scene, particularly given the ambiguities of natural language. Instead, SGMs provide a domain-specific language for scenes. OpenAI\u2019s system text-to-image system DALL-E (Ramesh et al., 2021) can generate impressive output in response to prompts like \u201ca couple of people are sitting on a wood bench\u201d, or even a quirky prompt like \u201ca tapir made of an accordion\u201d (see Figs. 3 and 2(a) in the paper). However, it has been reported that requesting more than three objects, negation, and numbers may result in mistakes and object features may appear on the wrong object (Marcus et al., 2022). An issue here is that in the paired text and images sourced from the web, the text will likely not be sufficiently informative about the objects and their spatial relationships etc. The kinds of results produced by DALL-E and similar systems are thus unlikely to give sufficient control to graphic artists and animators, who may be broadly happy with the output of the system, but may wish to make adjustments and edits. In order to enable this, we argue that one needs object-based representations and scene models as advocated for above."
        },
        {
            "heading": "1.2 General Advantages of Generative Models",
            "text": "We first define some notation: x denotes observed data, such as an image. A generative model p\u03b8(x) defines a probability distribution over images; here \u03b8 denotes the parameters of the model. For SGMs, our model is defined in terms of latent variables z. The latent variables for a single object might be decomposed, for example into z = (zs, zt, zp), for shape, texture (including colour) and pose respectively. If there are K objects, they can each have latent variables (LVs) z1, . . . , zK ; let z0 denote the global variables (e.g., camera parameters and illumination). There can also be additional\n2See https://ps.is.mpg.de/research_fields/semantic-scene-understanding.\nlatent structure in z that models the inter-relationships of objects in the scene. We have that\np\u03b8(x) = \u222b p(z)p\u03b8(x|z) dz, (1)\nwhere p(z) is a prior over the latent variables, and p\u03b8(x|z) renders the latent description into the image.3\nThere are several advantages of generative models, as discussed below.\nPattern synthesis (unconditional generation): Generative models allow one to sample from the model, and compare the samples to the raw input data. This can be very helpful, especially to identify ways in which the model has failed to capture aspects of the input data, leading to model revision. The methods of model criticism (see e.g., Seth et al. 2018) can help detect differences; one can also train a discriminator to do this, as used in the training of GANs.\nThere are situations where such synthetic data (i.e. data sampled from the model) can be useful in its own right\u2014for example, in healthcare one may not wish to release data due to privacy concerns. But if one can create synthetic data which mimics the underlying data distribution, this can enable research to proceed much more freely. However, one has to be careful that the synthetic data has not simply \u201cmemorized\u201d some or all of the training data, as discussed in van den Burg and Williams (2021).\nImputation and restoration (conditional generation): If the input data x is split into observed data xo and missing data xm, then the task of imputation is to predict the missing data given xo. Probabilistic models also produce a probability distribution for p(xm|xo), which allows a quantification of the uncertainty. In the case that part of an image is missing, the imputation task can be called inpainting; see Fig. 4 for an example. Here a simple method might just inpaint red and black texture from the car body and tarmac, but knowledge of cars will predict a wheel in this location, and it is likely that it will match in style to the visible front wheel.\n3In eq. 1 the integral should be interpreted as a summation for latent variables that are discrete.\nIt might also happen that the observed data is a noisy or degraded version of the underlying data; in this case having models of the underlying data and the noise process allows probabilistic restoration of the data.\nAnomaly detection: It can be helpful to detect datapoints which do not conform to the learned model p(x). This task is known as anomaly detection, novelty detection or out-of-distribution (OOD) detection. For example it can be useful for an automated system to detect that the regime of operation has changed, and thus flag up that it needs attention or re-training. One way to frame this task is as a classification between p(x) and a broad (\u201ccrud-catcher\u201d) model p0(x), as used e.g. in Quinn et al. (2009). If a data point x is more likely under p0(x), it can be classified as an outlier relative to p(x).\nAnomalies can be quite subtle. In images of street scenes in North America both fire hydrants and mailboxes are common items of street furniture, but it is improbable to see a fire hydrant located on top of a mailbox\u2014this example of a contextual anomaly is from Biederman et al. (1982, Fig. 1).\nData compression: A probabilistic model p(x) can be used to compress data. Given the true data distribution p(x), Shannon\u2019s source coding theorem will assign a code of length l(x) = \u2212 log2 p(x) bits to x. Thus the expected code length is \u2212 \u222b p(x) log2 p(x) dx = H(p), the entropy of p. Such data compression can be approached in practice using, for example, using arithmetic coding, see e.g. MacKay (2003, sec. 6.2).\nIn practice we may not know the true distribution p(x), but have an alternative model q(x). In this case we have to pay a price in terms of the expected number of bits used. Let the expected code length when coding under q(x) be denoted Lq. Then\nLq = \u2212 \u222b p(x) log2 q(x) dx = \u2212 \u222b p(x) log2 ( q(x)\np(x) p(x)\n) dx = H(p)+DKL(p||q) \u2265 H(p). (2)\nHence the additional expected code length is given by the Kullback-Leibler (KL) term, which of course reduces to zero when q = p. This motivates minimizing the KL divergence to produce better codes, or equivalently to maximize the expected log likelihood \u222b p(x) log q(x) dx for a model q(x) (as the entropy term is fixed)."
        },
        {
            "heading": "1.3 Pros and Cons of Structured Generative Models",
            "text": "Below we contrast structured generative models (SGMs) compared to discriminative models, or to unstructured generative models.\n3 Structured generative models provide a coherent scene representation, rather than just output predictions for a disparate set of tasks. This representation is available for multiple tasks, including new ones not previously trained on (transfer learning).\n3 Structured generative models are compositional. This implies that when learning about a particular object type, we don\u2019t have to be concerned with other object types at the same time. This should make them more data efficient.\nIf there are inter-object relationships, these can be modelled separately from the variability of individual objects, in a hierarchical model. This allows curriculum learning (Bengio et al., 2009), where one can first focus on modelling individual object classes using class-conditional data, and then bring in within-scene inter-relationships. These advantages are not present in an unstructured generative model.\n3 The SGM representation is editable, e.g., to change the direction of the lighting, or to add/remove objects.\n3 (Structured) generative models can be trained unsupervised, or with weak supervision. Discriminative models usually require potentially expensive/difficult human labelling, although the use weaker supervision has also been explored (see, e.g., Shi et al. 2017).\n3 The SGM is interpretable/explainable. This structured approach can be contrasted with many deep generative models, which learn a rich model of the data, but with a monolithic black-box architecture which is not interpretable or easily editable. A SGM identifies certain image regions as being explained by certain objects, and can potentially provide more detailed part-level correspondences. The structured representation also enables other features such as occlusion reasoning.\n7 Discriminative models can be less susceptible to modelling limitations, as they are directly optimizing for a given task \u201cend-to-end\u201d, rather than building a general-purpose model which can be used for inference for many different tasks.\n7 The SGM framework can require expensive inference processes to infer the latent variables for the whole scene. We discuss in section 4 below how these issues can be ameliorated.\nAn example of where the SGM approach should be helpful is when an object is heavily occluded, but scene context can help with its reconstruction. Consider Fig. 5(a), where the rearmost red chair is heavily occluded.4 Knowledge that chairs grouped around a table are often of the same design in such scenes would help make strong predictions for this heavily occluded chair. This could be evaluated by outputting a 3D model of the object, or making predictions of how the scene would look from a novel viewpoint, as in Fig. 5(b). A related task is that of image inpainting, as illustrated in Fig. 4. In this case there is in effect a synthetic occluder (the mask); it is most natural to evaluate this by prediction of the masked-out region(s).\nThe SGM approach could also be used to carry out scene editing, e.g. to add or remove objects or change their properties, or alter the lighting. Here the result could be evaluated by collecting views under the relevant perturbation. Another possible task is the completion of a 3D scene given only a subset of the objects in the scene (see e.g., Li et al. 2019). This is a missing data imputation task, like image inpainting, but different as it is imputing a 3D scene, not just an image.\n4This example was inspired by Hueting et al. (2018).\nIs full inference overkill? The key advantage of the SGM approach is that it provides a unified representation, from which many different tasks or questions can be addressed. This creates a coherent understanding of the input image in terms of the 3D (or 2.1 D) world, in contrast with what might arise if different models are trained for different tasks without this underlying structure.\nIf we only care about one task, then it is certainly overkill. But acting in the real world does not require just one task. The example of tea-making in Land et al. (1999) illustrates this nicely. The goal of making tea decomposes into subgoals such as \u201cput the kettle on\u201d, \u201cmake the tea\u201d, \u201cprepare the cups\u201d. A subgoal of putting the kettle on is to \u201cfill the kettle\u201d, and this in turn requires \u201cfind the kettle\u201d, \u201clift the kettle\u201d, \u201cremove the lid\u201d, \u201ctransport to sink\u201d, \u201clocate and turn on tap\u201d, \u201cmove kettle to water stream\u201d, \u201cturn off tap when full\u201d, \u201creplace lid\u201d, and \u201ctransport to worktop\u201d. The visual tasks required include object detection; pose and shape estimation of objects so that they can be manipulated; and monitoring of the state of some variable (e.g. water level in the kettle). Carrying out the tea-making task in an unfamiliar kitchen will bring in to play knowledge about the typical layout of kitchens, and possibly about different kinds of tap mechanism. Note that subgoals such as \u201clocate and turn on tap\u201d and \u201cturn off the tap\u201d are re-usable across other tasks such as making coffee, or washing the dishes."
        },
        {
            "heading": "2 Models of Objects",
            "text": "Visual scenes can contain a lot of complexity. Components that make up the scene can be divided into \u201cthings\u201d and \u201cstuff\u201d. Things are object categories that have a well defined shape (like people or cars), while stuff corresponds to categories which have an amorphous spatial extent, such as grass and sky (see e.g., Sun et al. 2014). We first focus on approaches to model things in sec. 2.1, and then move on to model stuff in sec. 2.2."
        },
        {
            "heading": "2.1 Modelling Things: Multifactor Models",
            "text": "Here we consider modelling a class of visual objects (such as teapots or cars). These can vary in shape, and in texture (described e.g. by its colour or possibly a more complex pattern on the surface). We can also vary the position and orientation (the pose) of the camera relative to the object, and the lighting; we term these as rendering variables. Hence there are separate factors of shape, texture and rendering that combine to produce the observations. We call models with a number of separate factors multifactor models. Below we first give some examples of multifactor models, and then focus on parts-based models in sec. 2.1.1 (which are a special class of multifactor models).\nExample: Blanz and Vetter\u2019s morphable model of faces. An early example of a 3D multifactor model is due to Blanz and Vetter (1999).5 Consider n locations on the face; si records the (x, y, z) coordinates of location i, and similarly ti records the red, green and blue colour values (albedo) at the same location. These measurements were obtained with a laser scanner. These individual vectors are concatenated to produce the shape vector s = (s1, s2, . . . , sn) which has length 3n, and similarly there is an texture vector t of length 3n. Blanz and Vetter (1999) used approximately 70,000 locations on each face, and collected data from 200 subjects. Preprocessing was carried out to remove the the global 3D transformation between the faces, and an optical flow method was used to register the locations.\nGiven the shape and texture vectors for each subject, a probabilistic principal components analysis (PPCA) model can be built to capture the variation in shape and texture, with latent variables zs and\n5The description below is partly based on Chapter 17 of Prince (2012) as well as the original paper.\nzt respectively. One could alternatively use a common z for the shape and texture variation, e.g. by concatenating the s and t vectors for each example before applying PPCA. This could model the fact that a change in the shape of the mouth to produce a smile will also likely expose the teeth to view, so these changes are correlated.\nThe above description models shape and texture variation in 3D. This model is transformed geometrically into the image plane in terms of the camera intrinsic and extrinsic parameters. The colour at each pixel is determined by the Phong shading model (see e.g., Szeliski 2021, sec. 2.2.2), which accounts for diffuse and specular reflections from directed light sources, and also for ambient illumination. Denoting all of rendering variables by zr, the overall model can be fitted to a new face by optimizing zs, zt and zr so as to minimize an error measure between the observed and predicted pixels.\nThe model of Blanz and Vetter (1999) is in 3D. Such 3D models have also been used, e.g., for modelling regions in the brain (Babalola et al., 2008). Some earlier work by Cootes, Taylor and collaborators first developed 2D active shape models using a PPCA model of the shape as defined by landmarks (Cootes et al., 1995), and then developed active appearance models that also took the texture into account (Cootes et al., 1998).\nExample: CodeNeRF models disentangled Neural Radiance Fields for object categories. Blanz and Vetter\u2019s model uses a linear approach (PPCA) to model the variability due to shape and texture. Careful alignment of the data was needed in order to make this approach work. With the advent of deep learning, it is natural to ask if one can exploit more powerful nonlinear models. We first start with the Neural Radiance Field (NeRF) representation for a single object due to Mildenhall et al. (2020), and then add latent variables to model shape and texture variation, as in the CodeNeRF model of Jang and Agapito (2021).\nThe NeRF takes as input a 3D location x = (x, y, z) and a viewing direction defined by a 3D Cartesian unit vector d, and outputs a volume density \u03c3(x) and emitted colour c(x,d) = (r, g, b) at that location and direction. obtained via a neural network F\u03b8 : (x,d) \u2192 (c, \u03c3) with weights \u03b8. As Mildenhall et al. (2020, sec. 4) state, \u201cthe volume density \u03c3(x) can be interpreted as the differential probability of a ray terminating at an infinitesimal particle at location x\u201d. This means that the expected colour C(r) observed at the camera ray r(t) = o+ td, with near and far bounds tn and tf is given by6\nC(r) = \u222b tf tn T (t)\u03c3(r(t))c(r(t),d) dt, where T (t) = exp ( \u2212 \u222b t tn \u03c3(r(s)) ds ) . (3)\nHere T (t) denotes the transmittance along the ray from tn to t, i.e., the probability that the ray reaches t starting from tn without hitting any other particle. The process to obtainC(r) is known as volumetric rendering, and is differtentiable. The computation of C(r) is approximated by taking a number of samples along the ray.\nFor a single object, the NeRF representation can be obtained by minimizing the error between the observed and predicted colours at a set of ray locations for a number of different views (with known camera poses and intrinsic parameters). This is useful to allow novel view synthesis, i.e., to predict the image that would be obtained from a novel view. However, for an object class, it makes sense to have latent variables zs, zt for each object, as is done in CodeNeRF (Jang and Agapito, 2021). Here the neural network is enhanced to map F\u03b8 : (x,d, zs, zt) \u2192 (c, \u03c3). Now the optimization problem for the network weights \u03b8 is carried out over all training examples in an object class, and for each set of views of a given example the shape and texture latent variables are estimated. (A regularization penalty proportional to |zs|2+|zt|2 is also imposed on the latent variables, corresponding to a Gaussian prior.) The ability of CodeNeRF to generalize to novel shape/texture/pose combinations is illustrated in Fig. 6.\nThere have been a lot of other recent developments arising from the NeRF work. For example Zhang et al. (2021) extended NeRF to extract a surface (rather than volumetric) representation, and then solve for spatially-varying reflectance and environment lighting. This allows rendering of novel views of the object under arbitrary environment lighting, and editing of the object\u2019s material properties.\nOther multifactor models. One can sometimes use a multilinear model to handle multiple factors of variation. A multilinear model with two latent factors z1 and z2 is given by xi = \u2211 j,k wijkz 1 j z 2 k, where wijk is 3-way tensor of parameters. Tenenbaum and Freeman (2000) use a bilinear model to separate style and content, e.g. of the font (style) of different letters (content). Vasilescu and Terzopoulos (2002) use a 4-factor model to cover different facial geometries (people), expressions, head poses, and lighting conditions.\nAnother example of a multifactor model is transformation invariant clustering (Frey and Jojic, 2003). They consider the case where there is a discrete factor modelling shape/appearance variation as a mixture model, and a continuous factor arising from \u201cnuisance\u201d translations and rotations of an object in an image."
        },
        {
            "heading": "2.1.1 Parts-based Models",
            "text": "Parts-based models are old idea in computer vision. For example Fischler and Elschlager (1973) described a model termed pictoral structures, where an object is represented by a number of parts\n6Note that in this section x denotes a 3D location, not our usual meaning of an input image. Also t is overloaded to denote both the parameterization along the ray r(t), and the texture superscript.\narranged in a deformable configuration. The deformations are represented by spring-like connections between pairs of parts. Biederman (1987) has advocated for a parts-based approach in computer vision, under the name of Recognition-by-Components. More recently Felzenszwalb et al. (2009) used discriminatively-trained parts-based models to obtain state-of-the-art results (at the time) for object recognition.\nSome advantages of the parts-based approach are described by Ross and Zemel (2006), viz.\n\u2022 A partially-occluded object can be recognized if some of the parts are present;\n\u2022 A parts-based approach is a good way to model the variability in highly-articulated objects such as the human body;\n\u2022 Parts may vary less under a change of pose than the appearance of the whole object.\n\u2022 Known parts may be recombined in novel ways to model a new object class.\nParts-based models share a similar objective with perceptual organization or perceptual grouping (see, e.g., Palmer 1999, ch. 6) in that they seek to organize parts into a whole, but generally perceptual organization is seen as a generic process, e.g., for grouping edges into contours, or to group similar pixels into regions, rather than exploiting specific knowledge about certain object classes.\nBelow we give four examples of parts-based models.\nExample: Parts-based Models of Faces. One common application of parts-based models is to faces. In the UK, the \u201cPhotoFit\u201d system is used by police in the investigation of crimes to produce an image of a suspect with the help of eye witnesses. See, for example, the \u201cPhotoFit Me\u201d work of Prof. Graham Pike.7. This decomposes a face into eyes, nose, mouth, jaw and hair parts.\nRoss and Zemel (2006) proposed two models to learn such a parts-based decomposition. We describe them in relation to the modelling face images.8 The first model, Multiple Cause Vector Quantization (MCVQ), has K multinomial factors, each selecting the appearance of a given part. The \u201cmasks\u201d for each part are learned probabilistically, so that pixel j is explained by part k with multinomial probability \u03c0jk, where \u2211 k \u03c0jk = 1. The learned model is illustrated in Figure 7. The plots on the bottom left show the mask probabilities, and on the bottom right the 10 means for each VQ are shown, multiplied by the relevant mask. Notice in the bottom left panel how the masks identify regions such as the eyes, nose and chin.\nA second model is termed Multiple Cause Factor Analysis (MCFA)\u2014this is similar to MCVQ, but now the appearance each part is based on a factor analyzer instead of a discrete choice. The mask model is as for MCVQ. Nazabal et al. (2022) used a MCFA model of faces, but added a higher-level factor analysis model to correlate the factor analyzers for each part. The dataset used they used (from PhotoFit Me) is balanced by gender (female/male) and by race (Black/Asian/Caucasian), hence the high-level factor analyser can model regularities across the parts, e.g. with respect to skin tone.\nThe MCVQ and MCFA models use a simple model for the mask probabilities \u03c0jk. But suppose we are modelling a parts decomposition of side-views of cars, e.g. into wheels, body and windows. The different styles of cars will give rise to quite different mask patterns, and these can be modelled with a latent variable model. For example Eslami and Williams (2011) modelled the mask patterns with exponential family factor analysis, while Eslami and Williams (2012) used a multinomial Shape Boltzmann machine.\nExample: Parts-based Model of a Clock. Fig. 8 is reproduced from the work Zhu and Mumford (2006). The figure describes an AND-OR graph for clocks; for example there is an AND over the hands, frame and numbers components of the clock, but in each there are alternatives as encoded by OR nodes. Particular choices at the OR nodes gives rise to a parse graph.9 One of the possible parse graphs is illustrated with dark arrows, corresponding to the image at the top. Thus we observe that OR choices are made for the hands (2 not 3), the numbers (Arabic not Roman), and the shape of the frame (circular, and only the outer ring is present).\nThe AND-OR structure is similar to what we have seen for faces with the MCVQ; there are a number of parts (the AND), and for each there are a discrete set of choices (the OR). However, an AND-OR graph is generally more powerful, as it can have hierarchical structure. For example, in the clock model there are choices for all of the outer, inner, and central rings of the frame component to be present or absent.\nExample: Parts-based Models of Articulated Bodies. A classic example of an articulated object is the human body, which can be decomposed into a torso, head, arms and legs. The arms and legs can each be further decomposed; for example an arm is made up of the upper arm, lower arm and the hand, and the hand can be further decomposed into the fingers and thumb. Given the importance\n7See https://www.open.edu/openlearn/PhotoFitMe. 8The data used is aligned with respect to position and scale, so these \u201cnuisance factors\u201d do not need to be modelled\nduring learning. 9See sec. 3.3 for further discussion of grammars and AND-OR graphs.\nof human avatars in the film and gaming industries, there has been a lot of work on this topic. Here we focus on the Skinned Multi-Person Linear Model (SMPL) of Loper et al. (2015). There are two factors of variation that need to be taken into account. The first is the pose of the body, as defined by the joint angles along the kinematic tree. The second is the specific body shape of a given individual. The model for the body is defined by a mesh with some 6890 vertices in 3D. Each vertex i is assigned weights wki indicating how much part k affects vertex i.\nThe body shape is modelled with a linear basis, similar to Blanz and Vetter\u2019s model discussed in sec. 2.1. The pose of the body is determined by the axis-angle representation of the relative rotation of a part with respect to its parent in the kinematic tree. One other important part of the model is pose blend shapes, which modify the the vertex locations depending on the pose, but before the pose transformation is applied. In SMPL, the pose blend shapes are a linear function of the elements of the part rotation matrices. Pose blend shapes are needed to counter the unrealistic deformations at joints that would otherwise arise. After combining the shape and pose blend effects in the neutral pose, the final predicted location of vertex i is obtained as a weighted sum (with weights wki) of the transformation of vertex i under part k.\nExample: Capsules Another parts-based model is termed \u201ccapsules\u201d. This term was introduced in Hinton et al. (2011), with later developments including Sabour et al. (2017), Hinton et al. (2018) and Kosiorek et al. (2019). There is a recent survey paper on capsules by De Sousa Ribeiro et al. (2022). The term \u201ccapsule\u201d relates to a visual entity which outputs both the probability that the entity is present, and a set of instantiation parameters for the entity. Although capsules are usually described in an inferential manner, with the flow of information from the parts to the object, below we follow\nthe exposition of Nazabal et al. (2022) who described Generative Capsule Models. Consider an object template T which consists of N parts {pn}Nn=1. Each part pn is described by its class, pose, shape, texture. The template T has an associated latent variable vector z which affects to the pose, shape, and texture of the parts. For example for a template in 2D, part of z may define the parameters of a similarity transformation (in terms of a translation t, rotation \u03b8 and scaling s of the template). The geometric transformation between the object and the parts can be described by a linear transformation in terms of t, s cos \u03b8 and s sin \u03b8. Other parts of z can model shape and texture correlations between parts. Methods for matching observed parts to template parts are described in sec. 4.\nAlthough we have described here object-parts relationships, capsules can be formed into an hierarchical architecture, allowing e.g., the representation of object inter-relationships."
        },
        {
            "heading": "2.2 Modelling Stuff: Visual Texture",
            "text": "Forsyth and Ponce (2003, p. 164) discuss texture as follows:\nTexture is a phenomenon that is widespread, easy to recognise, and hard to define. Typically, whether an effect is referred to as texture or not depends on the scale at which it is viewed. A leaf that occupies most of an image is an object, but the foliage of a tree is a texture. Views of large numbers of small objects are often best thought of as textures. Examples include grass, foliage, brush, pebbles, and hair. Many surfaces are marked with orderly patterns that look like large numbers of small objects. Examples include the spots of animals such as leopards or cheetahs; the stripes of animals such as tigers or zebras; the patterns on bark, wood, and skin. Textures tend to show repetition: (roughly!) the same local patch appears again and again, though it may be distorted by a viewing transformation.\nTextures can be classed as regular or stochastic, although there can be gradations, such as nearregular or near-stochastic. Examples of regular textures include brickwork, tiled floor patterns, and wickerwork. Examples of stochastic textures include clouds, wood grain, and foliage. Below we will focus mainly on stochastic textures.\nWe take the goal of texture synthesis to be the generation of an arbitrarily sized region of a texture, given a (small) training sample. For regular textures it should be possible to extract the repeating element(s) and simply tile the target region appropriately, but this will not work for stochastic textures. Instead we aim to learn a generative model of the texture from the training sample. A common type of model used is an energy-based model (EBM). Let x(k) denote a patch of the image centered at location k (e.g. a square patch). The field of experts (FoE) energy is defined as\nEFoE(x) = \u2211 k \u2211 j \u03c6j(wj \u00b7 x(k)), (4)\nwhere wj is a filter the same size as the patch, and \u03c6j() is some function. A probability distribution over images is then defined by the Boltzmann distribution, i.e.,\npFoE(x) = 1\nZ(W ) exp(\u2212EFoE(x)). (5)\nHere Z(W ) is the partition function that serves to normalize pFoE(x). The term field of experts was introduced in Roth and Black (2005), as a generalization of the product of experts construction due to Hinton (2002), to handle arbitrarily-sized images.\nIn general it is not easy to draw samples directly from an energy-based model; a standard approach is to construct a Markov chain whose equilibrium distribution is the desired Boltzmann distribution. See e.g., Murphy (2012, ch. 24) for further details.\nOne simple choice would be to take the function \u03c6j to be a quadratic form in x(k). If this is positive definite, pFoE(x) will be well-defined and normalizable, and will define a Gaussian Markov random field (GMRF), see e.g., Rasmussen and Williams (2006, sec. B.5). Stationary GMRFs on a regular grid can be analyzed via Fourier analysis (Rozanov, 1977) in terms of the power spectrum. However, it is well known that image models based on simply on the power spectrum (or equivalently, on second-order statistics, via the Wiener-Khinchine theorem) are inadequate. For example, Fig. 2 in Galerne et al. (2011) shows a section of a tiled roof. Randomizing the phase of its Fourier transform, while maintaining the power spectrum, leads to a blurry image, as the phase alignment needed to create sharp edges no longer occurs.\nA Gaussian random field model can also be obtained via the maximum entropy principle (see e.g., Cover and Thomas 1991, ch. 12) on the basis of second order (covariance) constraints on the distribution. But an alternative is to consider a set of filters, and impose the constraint that the maximum entropy model matches the observed histogram for each filter. This gives rise the FRAME (Filters, Random field, and Maximum Entropy) model of Zhu et al. (1998). Roth and Black (2005) discuss the FRAME model, and note that the approach is complicated by its use of discrete filter histograms. Instead they propose the field of experts model.\nKivinen and Williams (2012) defined a different energy based model, with the energy function\nETm(x) = 1\n2\u03c32 (x\u2212 a)T (x\u2212 a)\u2212 \u2211 k,j log[1 + exp(bj + \u03c3 \u22121(wj \u00b7 x(k))]. (6)\nThis was inspired by the convolutional restricted Boltzmann machine of Lee et al. (2009), but instead of convolution uses tiled convolution as described in Ranzato et al. (2010), who argue that convolutional weight sharing creates problems due to nearby latent variables of the same filter being highly correlated. In the tiled convolutional strategy each filter tiles the image with copies of itself without overlaps (i.e. the stride equals the filter diameter). But different filters do overlap with each other, in order to avoid tiling artifacts. The energy function ETm consists of two terms. The first component corresponds to a simple spherical Gaussian with mean a. The second is obtained by integrating out the hidden units (as given in eq. 1 of Kivinen and Williams 2012) of the restricted Boltzmann machine\nanalytically. The label \u201cTm\u201d is given to this model in order to denote that it uses tiled convolution, and that the means, rather than the covariances, are modelled (by the {wj}s and a).\nKivinen and Williams (2012) showed that to model multiple textures one can keep a fixed set of filters, but adjust the biases on a per-texture basis\u2014they called this the \u201cMulti-Tm\u201d model. Fig. 9 shows results for three different textures, with two images of both the raw data and samples. The raw data is obtained as 98\u00d7 98 patches cropped from the Brodatz texture album (Brodatz, 1966).\nAn alternative to energy based models is an auto-regressive model, where the predicted pixel value xij at location (i, j) depends on a vector of context, typically to the left and above if working in raster scan order. Efros and Leung (1999) used this approach to carry out exemplar-based texture synthesis, starting with a source training sample of texture. For a target location (i, j), one identifies neighbourhoods in the source texture that are similar to the current context region, and then selects one of these regions at random (depending on the level of agreement with the context region). For filling in holes an \u201conion peeling\u201d strategy of scanning round the periphery can be used rather than raster scan order. Note that this is a non-parametric modelling approach\u2014rather than constructing a parameterized model for p(xij|context), one extracts relevant regions from the source texture.\nAuto-regressive models do not have to be exemplar-based. PixelCNN (van den Oord et al., 2016) and PixelCNN+ (Salimans et al., 2017) are prominent recent auto-regressive models which use convolutional layers for feature extraction, along with masking to ensure that only valid context regions are accessed, in order predict p(xij|context).\nAbove we have discussed the generation of flat textures on a 2D plane. Textures can be applied (mapped) to a surface; this is known as texture mapping (see, e.g., Szeliski 2021)."
        },
        {
            "heading": "3 Models of Scenes",
            "text": "A very simple model of scenes is one which randomly selects objects and puts them into a scene. This might be summarized as \u201cthe independent components of images are objects\u201d. One example of this is the 2D sprites work of Williams and Titsias (2004) illustrated in Fig. 2. Here the model has learned about the background and the two people, but a priori it would place the people at random locations in the image. A more recent example is IODINE (short for Iterative Object Decomposition Inference NEtwork) due to Greff et al. (2019). This uses an \u201cobject-centric\u201d representation, consisting of K vectors of latent variables z1, . . . , zK , one for each object. IODINE was demonstrated on 2D sprites data, and also on images of 3D scenes from the CLEVR dataset, which consists of geometric objects like spheres and cubes in random locations with random material properties. The zks for each object in IODINE were not factored into shape, texture and pose components (as discussed in sec. 2.1), but were a single vector that entangled these factors.\nHowever, in the same way that sentences are not random sequences of words,10 visual scenes are not composed of random collections of objects\u2014there are co-occurrences of objects, and relationships between them. For example, there are correlations between the scene type (e.g. kitchen. living room, urban street, rural field) and the kinds of objects observed. Also, there are stuff-stuff, things-stuff, and thing-thing interactions that occur between objects in the scene (see e.g. Heitz and Koller 2008). Examples of things-stuff interactions are that cars are (usually) found on roads, or cows on grass. An example of thing-thing interactions is that dining chairs are likely to be grouped around a dining table. As another example of thing-thing interactions, one might consider adding details to a coarse scene layout, e.g. by adding tablemats, cutlery and crockery to the dining table.11 The reader will observe\n10are sequences sentences of not words random! 11Ritchie (2019) terms this virtual \u201cset dressing\u201d.\nthat there are similarities between parts-based models described in sec. 2.1.1, and scene models. However, parts-based models are often more constrained, with a fixed number of parts, while scene models can have a variable number of objects and looser relationships. Scene relationships can also be longer-range\u2014a classic example is the relationship between a TV and a sofa for viewing it, which need to be a comfortable distance apart.\nBelow we focus particularly on models for indoor scenes, reflecting the focus in the research literature. But there are also outdoor scenes, in both rural and urban environments. In scenes of mountains or coastlines, it may be most natural to consider the carving of the landscape by erosion, e.g. by river valleys. In farmland there will be human-made field boundaries (constrained by the landscape), along with crops or livestock. In urban environments, one might use grammar-type models to generate building facades, and then \u201cdecorate\u201d the street architecture with other objects such as people, cars and street furniture.\nBelow we describe autoregressive, energy based and hierarchical models, which we cover in turn. Note that autoregressive and energy based models are not latent variable models, while hierarchical models are. In this section it is assumed that 3D data is available, e.g. 3D oriented bounding boxes (OBBs) with class labels."
        },
        {
            "heading": "3.1 Autoregressive Models",
            "text": "We take as an example the work of Ritchie et al. (2019), who describe a process where objects are added to a room layout one at a time, until a decision is made to stop. The model first extracts a top-view floor plan of the room (to define the valid region to place objects). It then feeds the floor plan to a sequence of four modules that (i) decide which object (if any) to add, (ii) specify where the object should be located, (iii) its orientation, and (iv) its physical dimensions. Once an object has been added the floor plan representation is updated to include the object, before the next calls to steps (i) to (iv). These modules are implemented with convolutional neural networks.\nLet the m ordered objects be denoted x1, x2, . . . ,xm. Each xi is comprised of an object class label, pose features (location and orientation), shape features and texture features12, so that xi = (xci ,x p i ,x s i ,x t i). Then under an autoregressive model we have that\np(x1,x2, . . . ,xm) = p(x1) m\u220f i=2 p(xi|x<i), (7)\nwhere x<i denotes the sequence x1, . . . ,xi\u22121. This is suitable for generating scenes from the model. It can also be used to compute p(x1,x2, . . . ,xm) if an ordering of the objects is given. However, when we observe an unordered set of objects X = {xi}mi=1, we should sum over all possible permutations, so that\np(X) = \u2211 \u03c0\u2208\u03a0 p(\u03c0)p(x\u03c0(1)) m\u220f i=2 p(x\u03c0(i)|x\u03c0(<i)), (8)\nwhere \u03c0 denotes a particular permutation,and \u03a0 is the set of all permutations. The prior over permutations p(\u03c0) can be taken as uniform, i.e. 1/(m!). In fact Ritchie et al. (2019) do use an ordering of objects for training the object class label module, based on a measure of the importance of the class. This depends on the average size of a category multiplied by its frequency of occurrence. This means that large objects like a bed will occur first in a bedroom scene, with other objects fitting in around it.\n12Ritchie et al. (2019) do not use texture features, but in general they could be present.\nThe model of Ritchie et al. (2019) is relatively simple, but the use of the floor plan representation with the added objects means that the chain rule of probabilities can be used readily to model the context, without simplifications such as a Markov model.13 One can ask if the autoregressive process really makes sense as a generative model of scenes? It is not unreasonable that large objects (such as the bed in a bedroom scene) should be added first. But it seems rather unlikely that people furnish bedrooms using a fixed ordering of all the object classes.\nNote also the autoregressive model does not make explicit groupings of objects that co-occur, such as a bed and nightstand, or a tv-sofa combination arranged for convenient viewing. In contrast, hierarchical models (see below) should be able to pick out such structure.\nBuilding on the work of Ritchie et al. (2019), Paschalidou et al. (2021) develop an autoregressive model using transformers (dubbed ATISS). They demonstrate some advantages of ATISS over the earlier work, e.g. in relation to scene completion, especially when objects that come early in the sequence (e.g., beds for a bedroom) are omitted. However, in the formulation of ATISS (their eq. 3), a valid sum over permutations of the sequence as in eq. 8 is replaced by a product over permutations. The motivation is to maximize the probability of generation over all possible orderings, rather than having at least one with high probability. But taking a product over sequences is not a valid probability calculation; one possible way to make sense of this is as a product-of-experts (PoE) construction (Hinton, 2002), although in this case a partition function Z should be introduced, which would complicate the learning."
        },
        {
            "heading": "3.2 Energy-based Models",
            "text": "Suppose that we have generated a number of objects to go in a room. We then need to arrange them in order to obey a number of spatial, functional and semantic relationships or constraints. The basic idea is to define an energy function that measures the fit of the configuration to these constraints, and then as per eq. 5 to define a probability distribution using the Boltzmann distribution. Yu et al. (2011) used such an approach to automatically optimize furniture arrangements, using simulated annealing to search in the configuration space.\nThe method of Yu et al. (2011) is defined for a fixed set of objects. Yeh et al. (2012) extended this idea to allow an \u201copen world\u201d, where the number of objects is variable. They used a reversible jump Markov chain Monte carlo (MCMC) method from this space.\nA problem with energy based models in general is that one generally needs to run a MCMC chain for many iterations in order to draw samples from the equilibrium distribution. This limitation applies to the above methods, meaning EBMs will not scale well to larger scenes."
        },
        {
            "heading": "3.3 Hierarchical Models",
            "text": "A natural way to obtain an hierarchical model is via a grammar-based approach. The idea of using grammars for pattern analysis is an old one, see for example the work of K. S. Fu (1982) on syntactic pattern recognition. In relation to scene understanding, perhaps the most notable work is from SongChun Zhu and collaborators. The long paper by Zhu and Mumford (2006) entitled \u201cA Stochastic Grammar of Images\u201d is a key reference. A parts-based model of clocks due to Zhu and Mumford (2006) is discussed above in sec. 2.1.1.\nA context-free grammar (CFG) is defined in terms of terminal symbols, non-terminal symbols, a start symbol, and production rules. Inspired by Liu et al. (2014), we take bedroom scenes as an\n13Although note that the floor plan representation discards the order in which objects were added.\nexample domain. Here we will have terminal symbols for observed objects such as mattress, nightstand, pillow etc. As above, the full description will involve not only the class label, but also pose, shape and texture information. Non-terminal symbols here would correspond to groupings of objects; for example we may have sleeping-area \u2192 bed nightstand-group. There may typically be one or two nightstands (usually depending on whether the bed is single or double), so that the productions nightstand-group \u2192 nightstand and nightstand-group \u2192 nightstand nightstand are both valid. An example grammar for bedroom scenes from Liu et al. (2014) is shown in Figure 10.\nA probabilistic context-free grammar (PCFG) adds a probability distribution over the productions which share the same non-terminal on the LHS of the rules. Charniak (1993, ch. 5) provides a good overview of CFGs and PCFGs. A CFG is often described in terms of a set of production rules, but it can also be described by an AND-OR graph; Hall (1973) showed the equivalence between the two. Here, the OR occurs over productions share the same non-terminal on the LHS; the AND occurs with productions with more than one symbol on the RHS, as all of these symbols must be generated. As Zhu and Mumford (2006, sec. 6.1) state, an \u201cAND-OR graph embeds the whole image grammar and contains all the valid parse graphs\u201d.\nWe have seen that the terminal symbols include class label, pose, shape and texture information in general. This means that the non-terminals which govern a set of terminals will need to include latent variables which can generate the appropriate correlations between them. For example the production sleeping-area \u2192 bed nightstand-group will need to specify the size of the bed (single or double), and this information will also need to be passed to the nightstand-group variable, so that it can determine whether to generate one or two nightstands (and their appropriate location(s)). In the CFG, pose and size information can be defined relative to higher level variables. So, for example, if the size and location of the sleeping-area has been defined, it makes sense to locate the bed relative to this. A possible alternative to encoding this information in the latent variables is to add horizontal relational structures that to encode contextual information or constraints between nodes, as proposed in Zhu and Mumford (2006, ch. 4) and Jin and Geman (2006).\nA problem with using CFGs to model visual scenes is that it can be hard to learn them from data. Even in natural language processing (NLP), most successful grammar learning uses annotated \u201ctreebank\u201d data, rather than unannotated sentences. Similarly, annotated data has been used to learn some hierarchical models for images, as in Yao et al. (2007) and Liu et al. (2014). But recently for NLP data, Kim et al. (2019) found that they could add a sentence-level continuous vector latent\nvariable in addition to the PCFG structure (to produce a \u201ccompound\u201d PCFG) yielding state-of-the-art results for parsing tasks.\nWhile grammars are one way to generate hierarchical structure, they are not the only way. Instead of discrete-valued non-terminals, one can use a continuous-valued latent vector in a node. Consider starting with a single latent vector z. The first binary split can be generated as\n[z1, z2] = f(z), (9)\nwhere z1 and z2 are the latent vectors of the two children., and f is a non-linear vector-valued function, which could be as simple as [tanh(W1z+b1), tanh(W2z+b2)] or a deeper neural network. For each generated latent vector, a binary node classifier is applied to decide whether it is a terminal (generating an object), or a non-terminal which can be further split. The advantage of the continuous state is that it is well suited to express pose, shape and texture variation. This kind of generative structure is a simplified version of the GRAINS model of Li et al. (2019) which is described below. Note that the hierarchical MCFA model for faces of Nazabal et al. (2022) (described in sec. 2.1.1) is a simple one-layer model of this type, but with K child nodes rather than just two, and without the tanh nonlinearities.\nLi et al. (2019) developed the Generative Recursive Autoencoders for INdoor Scenes (GRAINS) model. This takes as input an unstructured set of objects in 3D and computes a latent representation z, which is decoded to yield a hierarchical structure of objects which can then be rendered. In more detail, the GRAINS model decodes tbe latent vector z into five variables representing the floor and four walls of a room. Each of these is either a terminal (corresponding to an object), or a non-terminal, as determined by a node classifier neural network. There are number of non-terminals, corresponding to support, surround and co-occurrence relationships. If the model were a grammar the encoder network should sum over all parses of the data, but instead a tree structure is built heuristically bottom up, and that is used in the encoder to output z."
        },
        {
            "heading": "4 Inference",
            "text": "In this section the task is inference from a single RGB or RGBD image, or from a set of images with known viewpoints (multi-view data), assuming that the object and relational models have already been learned. Thus we are interested in p(z|x).\nWe first cover inference for objects and global variables, and then inference with scene models. Finally we discuss inference in the presence of model deficiency, where an input image cannot be reconstructed exactly.\nInference for Objects and Global Variables. The basic inference task is to (i) detect each object, and to determine its class label, shape, texture and pose information, (ii) detect and characterize regions of stuff, and (iii) determine the lighting and camera global latent variables. The inference problem for a single object is already daunting if we we consider a discretization of the values for each of the latent variables and search over these (see e.g. Yuille and Liu 2021 sec. 7.1), and there is a combinatorial explosion when considering multiple objects and the global variables. The most direct way to address the combinatorial explosion is to approximate the search. One such method is Markov chain Monte Carlo (MCMC), where the goal is construct a Markov chain whose equilibrium distribution samples from the desired posterior over the latent variables. See e.g., Murphy (2012, chapter 24) for a discussion of MCMC. Often the proposed moves in the state-space are generic and do not exploit the structure of the problem, leading to slow mixing. An example of a more\nadvanced approach is Data-driven Markov chain Monte Carlo (DDMCMC, Tu and Zhu 2002), which allows proposals such as the candidate set of object detections to be incorporated into a valid MCMC algorithm.\nAn alternative approximate approach is to use variational inference, see e.g. Jordan et al. (1999), where the goal is approximate the posterior p(z|x) with a variational distribution q(z). The variational autoencoder (Kingma and Welling, 2014; Rezende et al., 2014) uses amortized variational inference to predict a distribution over the latent variables given input data. This is achieved with an \u201cencoder\u201d network (a.k.a. a \u201crecognition model\u201d, see Dayan et al. 1995). Such amortized inference is relatively straightforward if there is one object of interest in the image, but is more complex if there are multiple objects, due e.g. to permutation symmetries. In IODINE (Greff et al., 2019) the feed-forward predictions from the image for the object latent variables are the refined iteratively to take into account effects such as explaining away. In the slot-attention model of Locatello et al. (2020), an iterative attention mechanism is used in the mapping from the inputs to the latent variables, so that they compete to explain the objects in the image. The attend, infer, repeat (AIR) model of Eslami et al. (2016) takes an alternative, sequential approach, identifying one object at a time.\nMCMC and variational inference (VI) methods can be used to express the uncertainty in the latent representation z given the data x. This may arise, e.g. due to (partial) occlusions, and can give rise to a multi-modal posterior, corresponding to different interpretations or \u201cparses\u201d of the input image. A limitation of VI methods is that they may not capture this multi-modality well if the assumed form of q(z) is unimodal. As explained in Murphy (2012, sec. 21.2.2), when the variational distribution is unimodal, then the Kullback-Leibler divergence KL(q||p) used in VI tends to pick out one mode of the posterior, and thus under-estimate the uncertainty. MCMC methods can in principle sample from a multi-modal posterior, but in practice can get trapped within one mode, unless special measures such as annealing (see e.g., Murphy 2012, sec. 24.6) are used. One way to reduce the uncertainty in q(z) is to fuse information from multiple views, as shown in Li et al. (2020).\nThe above models such as IODINE and AIR were trained unsupervised. But the development of object detectors over the last two decades means that one can train object detectors for known classes such as cars, pineapples etc. The output may be a bounding box (BBox), or possibly a region-ofinterest. Such an approach was used by Izadinia et al. (2017) in their IM2CAD system, which takes as input an image of a room. It estimates the room geometry (as a 3D box), detects objects, predicts the associated latent variables of each object, places the objects in the scene, and then optimizes their placement via rendering the scene and comparing it to the input image.\nIf the input data is a set of multi-view images, then geometric computer vision techniques can be brought to bear. Simultaneous localization and mapping (SLAM) or structure-from-motion (SfM) can be used to estimate camera poses and a sparse point cloud representation, as in COLMAP (Scho\u0308nberger and Frahm, 2016). By themselves, such techniques do not segment the data into objects, but they can be combined with object-detection bounding boxes and masks predicted from each image to produce a 3D bounding box, as in Ru\u0308nz et al. (2020, sec. 5.1). Once the objects are segmented, their shape, texture and pose latent variables can be estimated, see e.g., Ru\u0308nz et al. (2020, secs. 5.2-5.3).\nOne important recent development has been differentiable rendering, which allows optimization of the estimated latent state, in order to improve the fit between rendered scene and the input image(s). An early differentiable renderer was OpenDR due to Loper and Black (2014). Kato et al. (2020) provide a survey of various methods that have been proposed for mesh, voxel, point cloud and implicit surface representations.\nInference with Scene Models. Above we have discussed instantiating objects and the camera/lighting global variables; we now consider inference for relational structures. For example, these may be represented as a parse graph that needs to be inferred on the fly.\nOne of the major issues is to match a set of object and part detections to a scene structure like an AND-OR graph. As a simple example, consider a capsules model for an object, as discussed in sec. 2.1.1. This is comprised of a number of parts that lie in a certain geometric relationship to each other. (Similar arguments also apply to a grouping a objects in a scene, such as a computer-monitorkeyboard, but here we will use the terminology of an object and its parts.)\nNow consider that we have detected a set of parts {xm}Mm=1, and the task is to match these to a set of templates {Tk}Kk=1. Let wmnk \u2208 {0, 1} indicate whether observation xm is matched to part n of template k. The w\u2019s can be considered as a binary matrix W indexed by m and n, k. One way to frame this assignment problem is to consider valid matchings between observed and template parts, as defined by a permutation matrix. If M is not equal to the total number of model parts N = \u2211K k=1Nk, then dummy rows or columns can be added to make the problem square, e.g., in case some parts are missing. As the exact computation would require considering all possible permutations which scales exponentially with M , Nazabal et al. (2022) consider variational inference for the match variables W and the latent variables zk for each template. This implements a routing-by-agreement procedure, as discussed by Sabour et al. (2017) and Hinton et al. (2018), but derived from the variational inference equations rather than as an ad hoc objective. An alternative to routing-by-agreement is to use a random sample consensus approach (RANSAC, Fischler and Bolles 1981), where a minimal number of parts are used in order to instantiate an object template, which is then verified by finding the remaining parts in the predicted locations. Kosiorek et al. (2019) use another inference approach, where an autoencoder architecture predicts the LVs for each template, making use of a Set Transformer architecture (Lee et al., 2019) to handle the arbitrary number of observations M .\nIf the hierarchical structure has been defined in terms of a recursive neural network, as in GRAINS (Li et al., 2019), then it is possible to build an encoder to predict the scene latent variable z, which can then be decoded to produce the hierarchical structure. This was successful in GRAINS, but note that there the input is a set of segmented 3D objects, as opposed to an unsegmented image.\nAlthough it is natural to think of inferential information flows in the hierarchical model being bottom-up, from parts to objects to scenes, it does not have to happen this way. The overall scene type may be well-characterized by a global scene descriptor like the gist (Oliva and Torralba, 2006), and this will create, e.g., top-down expectations for certain object classes, and not others. For example, Torralba et al. (2010) made use of the gist to predict the scene type (such as beach scene, street scene, living room). This then made useful predictions for the vertical location in the image for objects of a certain class (e.g. cars in street scenes)14. This indicates more generally that information from the image(s) may flow in top-down as well as bottom-up fashion for inference in the scene model.\nAs Zhu and Huang (2021, sec. 1.3.4) note, vision is driven by a large number of tasks, and \u201cEach of these tasks utilizes only a small portion of the parse graph, which is constructed in real-time in a task-driven way based on a hierarchy of relevant tasks\u201d. Thus it may be useful to consider \u201clazy inference\u201d15 of the full parse graph.\nModel deficiency: Quoting George Box, we can say that \u201call models are wrong, but some are useful\u201d (Box and Draper, 1987, p. 424). There is a great richness and detail in many visual scenes,\n14In the scene types considered, the horizontal location of objects was usually not well constrained by the scene type. 15Thanks for Kevin Murphy for suggesting this term.\nand it may not be possible (or perhaps even desirable) to model this fully. However, if we cannot get zero error at the pixels, to what extent can a generative model be said to have fully explained the image? The issue here is that there can be many ways in which an aggregate measure of error (such as mean squared error, MSE) can arise: (i) the pose of an object could be slightly off, but the appearance and shape are correct, leading to a \u201chalo\u201d of errors around the boundaries of the object; (ii) the pose and shape may be correct, but the object\u2019s texture does not match the palette of known textures, and thus there is a high-frequency pattern of errors within the object\u2019s extent; (iii) an object\u2019s shape is incorrect (either due to failures in inference or modelling), but the pose and texture are correct; or (iv) there may have been a false positive or false negative detection of a small object, again leading to the same MSE.\nA natural way to tackle this problem is to compare the input and predicted images, along with the predictions for object extent etc. To my knowledge there has only been a little prior work on prediction of the quality of the outputs of a vision algorithm when ground truth is not available. Jammalamadaka et al. (2012) discuss evaluator algorithms to predict if the output of a human pose estimation algorithm is correct or not, and Xia et al. (2020) have looked at predicting failures in semantic segmentation. In the reconstructive framework the goodness-of-fit of the geometric variables (camera parameters, object pose and shape) can be measured by the intersection-over-union (IoU) of the the predicted and ground-truth object masks (as used e.g. in Romaszko et al. 2020). Thus this IoU measure could be predicted by an evaluator algorithm. Errors in these variables will have consequent effects on the pixel errors. But if the IoU is satisfactory, then object-level pixel errors will likely be due to errors in the texture or lighting variables. Assessing the significance of such errors with an evaluator algorithm will require the annotations of the severity and types of the errors made.\nIt may be thought that algorithms that provide estimates of the uncertainty in their predictions help to address the issue of the assessment of vision algorithms, and this is partially correct. However, a limitation of probabilistic model uncertainty is that it is assessed relative to a model (or a fixed set of models). But if, for example, the model\u2019s palette of textures does not match with that in an input image, then the model\u2019s posterior uncertainty measures will not characterize the true situation well. This deficiency is known as the \u201copen world\u201d (M-open) situation, in contrast to M-closed, as discussed e.g. by Bernardo and Smith (1994, \u00a76.1.2). One approach to addressing this is via model criticism, as discussed e.g., by Seth et al. (2018).\nGiven the complexity of visual scenes, it may not be possible all the detail in a scene at the pixel level. Instead, one can build a generative model of the spatial layout of image features, as computed e.g. by a DNN. Such an approach is used, for example, in the neural mesh model (NeMo) of Wang et al. (2021). While it is more interpretable to reconstruct the input image than a feature-based representation, the latter may make modelling easier. The experimental results for the NeMo generative model show that it is much more robust to OOD tasks like recognition under partial occlusion and prediction of previously unseen poses than standard feedforward DNNs."
        },
        {
            "heading": "5 Advancing the SGM agenda",
            "text": "Above I have laid out the key topics of modelling objects and scenes, and carrying out inference in these models. Below I comment on the state-of-the-art, and issues around datasets etc.\nModelling objects: The state-of-the-art seems to be at a good level. In terms of data, the Amazon Berkeley Objects (ABO) dataset of Collins et al. (2022) is a recent example of a reasonably large\ncollection of models (some 8,000 objects) with complex geometries and high-resolution, physicallybased materials that allow for photorealistic rendering.\nModelling scenes: Compared to objects, the state-of-the-art is less advanced. Much of the focus has been on indoor scenes, leaving the modelling of outdoor scenes in urban and rural environments less explored. The task of modelling scenes is also more difficult than for objects, involving variable numbers of objects and types, and spatial, functional and semantic relationships between them. There is still much to be done here, e.g., for automatic discovery of hierarchical structure such as scene-type, objects and parts from data.\nOne issue here is data. Progress in 2D image recognition has been driven by large-scale datasets. Currently there are very few large 3D datasets available, and none on the scale of, say, the ImageNet which contains over one million images. To my knowledge the 3D-FRONT dataset (Fu et al., 2021) is the largest collection of indoor scenes, with almost 19,000 rooms. (It consists of synthetic 3D indoor scenes with professionally designed layouts.) Of course the effort needed to obtain and annotate a 3D scene is much greater than simply annotating bounding boxes in images. But are there reasons to believe that we should need fewer examples for learning from 3D data? Computer graphics can, of course, provide a rich source of 3D ground truth and annotations (see e.g. the CLEVR dataset of Johnson et al. 2017), and can provide a controlled means to test compositional generalization (see their sec. 4.7). However, this requires good object and scene models to generate realistic scene layouts, and also there are issues on how well models trained on such rendered data will transfer to real scenes.\nWhile collecting 3D datasets is challenging, it is also possible to collect multi-view data, where there are multiple images of (parts of a) given scene, with known camera parameters (intrinsic and extrinsic). See, e.g., the active vision dataset from UNC16 and the Aria dataset17 from Meta. SGMs can be tested on multi\u2013view by predicting what will be observed from a novel test viewpoint.\nInference: Scene-level inference is challenging, with the need to match portions of a hierarchical scene model with image data. As we have seen above, this can involve bottom-up and top-down flows of information in the model. This could lead to complex inference processes, reminiscent of those found in earlier vision models, such as VISIONS (Hanson and Riseman, 1978) and the system of Ohta et al. (1978). It may be possible to simplify this somewhat by using the idea of \u201clazy inference\u201d, where only parts of the whole scene representation are activated in response to given task. With regard to model deficiency, to make progress it will be important to get lots of data on the kinds of deficiencies that occur most, in order to address the important issues.\nTasks and Benchmarks: Currently most tasks for computer vision are evaluated in the image plane. Partly this may be due to the fact that it is relatively easy to collect images and create annotations in this case. However, there are some 3D benchmarks such as the KITTI suite18 and nuScenes19. These include 3D object detection in road scenes (using 3D bounding boxes). Autonomous driving and bin picking tasks in cluttered scenes are examples of areas that may well push the 3D reconstructive agenda forward.\n16https://www.cs.unc.edu/\u02dcammirato/active_vision_dataset_website/. 17https://about.meta.com/realitylabs/projectaria/datasets/. 18http://www.cvlibs.net/datasets/kitti/. 19https://www.nuscenes.org.\nIn order to exploit the value of structured generative models, it will be necessary to define a set of tasks which can exploit the same underlying representation. Some examples of challenging tasks were given in sec. 1.3; these include object reconstruction under heavy occlusion, and scene editing tasks, both of which exploit the 3D and scene-level information contained in the SGM representation. The research community will need to focus attention on a tasteful choice of 3D/multi-view benchmarks and tasks in order to promote the SGM approach."
        },
        {
            "heading": "Acknowledgements",
            "text": "I thank David Hogg, Alan Yuille, Oisin Mac Aodha, Antonio Vergari, Siddharth N., Kevin Murphy, Paul Henderson, Adam Kortylewski, Hakan Bilen and Titas Anciukevicius for helpful comments and discussions.\nFor the purpose of open access, the author has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission."
        }
    ],
    "title": "Structured Generative Models for Scene Understanding",
    "year": 2023
}