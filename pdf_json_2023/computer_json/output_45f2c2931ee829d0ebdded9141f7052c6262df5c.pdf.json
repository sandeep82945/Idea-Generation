{
    "abstractText": "Large-scale foundation models, such as CLIP, have demonstrated impressive zero-shot generalization performance on downstream tasks, leveraging well-designed language prompts. However, these prompt learning techniques often struggle with domain shift, limiting their generalization capabilities. In our study, we tackle this issue by proposing STYLIP, a novel approach for Domain Generalization (DG) that enhances CLIP\u2019s classification performance across domains. Our method focuses on a domainagnostic prompt learning strategy, aiming to disentangle the visual style and content information embedded in CLIP\u2019s pre-trained vision encoder, enabling effortless adaptation to novel domains during inference. To achieve this, we introduce a set of style projectors that directly learn the domainspecific prompt tokens from the extracted multi-scale style features. These generated prompt embeddings are subsequently combined with the multi-scale visual content features learned by a content projector. The projectors are trained in a contrastive manner, utilizing CLIP\u2019s fixed vision and text backbones. Through extensive experiments conducted in five different DG settings on multiple benchmark datasets, we consistently demonstrate that STYLIP outperforms the current state-of-the-art (SOTA) methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shirsha Bose"
        },
        {
            "affiliations": [],
            "name": "Ankit Jha"
        },
        {
            "affiliations": [],
            "name": "Enrico Fini"
        },
        {
            "affiliations": [],
            "name": "Mainak Singha"
        },
        {
            "affiliations": [],
            "name": "Elisa Ricci"
        },
        {
            "affiliations": [],
            "name": "Biplab Banerjee"
        }
    ],
    "id": "SP:4335d7f3c9d0726c0714cc70e8f8c733a8a8de27",
    "references": [
        {
            "authors": [
                "Artem Babenko",
                "Anton Slesarev",
                "Alexandr Chigorin",
                "Victor Lempitsky"
            ],
            "title": "Neural codes for image retrieval",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101\u2013mining discriminative components with random forests",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Adrian Bulat",
                "Georgios Tzimiropoulos"
            ],
            "title": "Language-aware soft prompting for vision & language foundation models",
            "venue": "arXiv preprint arXiv:2210.01115,",
            "year": 2022
        },
        {
            "authors": [
                "Fabio M Carlucci",
                "Antonio D\u2019Innocente",
                "Silvia Bucci",
                "Barbara Caputo",
                "Tatiana Tommasi"
            ],
            "title": "Domain generalization by solving jigsaw puzzles",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Junbum Cha",
                "Sanghyuk Chun",
                "Kyungjae Lee",
                "Han- Cheol Cho",
                "Seunghyun Park",
                "Yunsung Lee",
                "Sungrae Park"
            ],
            "title": "Swad: Domain generalization by seeking flat minima",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Junbum Cha",
                "Kyungjae Lee",
                "Sungrae Park",
                "Sanghyuk Chun"
            ],
            "title": "Domain generalization by mutual-information regularization with pre-trained models",
            "venue": "In Computer Vision\u2013 ECCV 2022:",
            "year": 2022
        },
        {
            "authors": [
                "Shivam Chandhok",
                "Sanath Narayan",
                "Hisham Cholakkal",
                "Rao Muhammad Anwer",
                "Vineeth N Balasubramanian",
                "Fahad Shahbaz Khan",
                "Ling Shao"
            ],
            "title": "Structured latent embeddings for recognizing unseen classes in unseen domains",
            "venue": "arXiv preprint arXiv:2107.05622,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriela Csurka"
            ],
            "title": "Domain adaptation for visual applications: A comprehensive survey",
            "venue": "arXiv preprint arXiv:1702.05374,",
            "year": 2017
        },
        {
            "authors": [
                "Peng Gao",
                "Shijie Geng",
                "Renrui Zhang",
                "Teli Ma",
                "Rongyao Fang",
                "Yongfeng Zhang",
                "Hongsheng Li",
                "Yu Qiao"
            ],
            "title": "Clip-adapter: Better vision-language models with feature adapters",
            "venue": "arXiv preprint arXiv:2110.04544,",
            "year": 2021
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "In search of lost domain generalization",
            "venue": "arXiv preprint arXiv:2007.01434,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Hanting Chen",
                "Xinghao Chen",
                "Jianyuan Guo",
                "Zhenhua Liu",
                "Yehui Tang",
                "An Xiao",
                "Chunjing Xu",
                "Yixing Xu"
            ],
            "title": "A survey on vision transformer",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Olivier Henaff"
            ],
            "title": "Data-efficient image recognition with contrastive predictive coding",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Dat Huynh",
                "Ehsan Elhamifar"
            ],
            "title": "Fine-grained generalized zero-shot learning via dense attribute-based attention",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Menglin Jia",
                "Luming Tang",
                "Bor-Chun Chen",
                "Claire Cardie",
                "Serge Belongie",
                "Bharath Hariharan",
                "Ser-Nam Lim"
            ],
            "title": "Visual prompt tuning",
            "venue": "In Computer Vision\u2013ECCV 2022:",
            "year": 2022
        },
        {
            "authors": [
                "Yunpei Jia",
                "Jie Zhang",
                "Shiguang Shan",
                "Xilin Chen"
            ],
            "title": "Single-side domain generalization for face anti-spoofing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Juwon Kang",
                "Sohyun Lee",
                "Namyup Kim",
                "Suha Kwak"
            ],
            "title": "Style neophile: Constantly seeking novel styles for domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Muhammad Uzair Khattak",
                "Hanoona Rasheed",
                "Muhammad Maaz",
                "Salman Khan",
                "Fahad Shahbaz Khan"
            ],
            "title": "Maple: Multi-modal prompt learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "J Devlin M Chang K Lee",
                "K Toutanova"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Aodi Li",
                "Liansheng Zhuang",
                "Shuo Fan",
                "Shafei Wang"
            ],
            "title": "Learning common and specific visual prompts for domain 9 generalization",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Aodi Li",
                "Liansheng Zhuang",
                "Shuo Fan",
                "Shafei Wang"
            ],
            "title": "Learning common and specific visual prompts for domain generalization",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy Hospedales"
            ],
            "title": "Learning to generalize: Meta-learning for domain generalization",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy M Hospedales"
            ],
            "title": "Deeper, broader and artier domain generalization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy M Hospedales"
            ],
            "title": "Deeper, broader and artier domain generalization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Da Li",
                "Jianshu Zhang",
                "Yongxin Yang",
                "Cong Liu",
                "Yi-Zhe Song",
                "Timothy M Hospedales"
            ],
            "title": "Episodic training for domain generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Haoliang Li",
                "YuFei Wang",
                "Renjie Wan",
                "Shiqi Wang",
                "Tie- Qiang Li",
                "Alex Kot"
            ],
            "title": "Domain generalization for medical imaging classification with linear-dependency regularization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jingjing Li",
                "Erpeng Chen",
                "Zhengming Ding",
                "Lei Zhu",
                "Ke Lu",
                "Heng Tao Shen"
            ],
            "title": "Maximum density divergence for domain adaptation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Pan Li",
                "Da Li",
                "Wei Li",
                "Shaogang Gong",
                "Yanwei Fu",
                "Timothy M Hospedales"
            ],
            "title": "A simple feature augmentation for domain generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yanghao Li",
                "Naiyan Wang",
                "Jiaying Liu",
                "Xiaodi Hou"
            ],
            "title": "Demystifying neural style transfer",
            "venue": "arXiv preprint arXiv:1701.01036,",
            "year": 2017
        },
        {
            "authors": [
                "Yiying Li",
                "Yongxin Yang",
                "Wei Zhou",
                "Timothy Hospedales"
            ],
            "title": "Feature-critic networks for heterogeneous domain generalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yiying Li",
                "Yongxin Yang",
                "Wei Zhou",
                "Timothy Hospedales"
            ],
            "title": "Feature-critic networks for heterogeneous domain generalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Yuning Lu",
                "Jianzhuang Liu",
                "Yonggang Zhang",
                "Yajing Liu",
                "Xinmei Tian"
            ],
            "title": "Prompt distribution learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Mahdi Derakhshani",
                "Enrique Sanchez",
                "Adrian Bulat",
                "Cees GM Snoek",
                "Georgios Tzimiropoulos",
                "Brais Martinez"
            ],
            "title": "Variational prompt tuning improves generalization of vision-language models",
            "venue": "arXiv e-prints, pages arXiv\u20132210,",
            "year": 2022
        },
        {
            "authors": [
                "Massimiliano Mancini",
                "Zeynep Akata",
                "Elisa Ricci",
                "Barbara Caputo"
            ],
            "title": "Towards recognizing unseen categories in unseen domains",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Massimiliano Mancini",
                "Samuel Rota Bulo",
                "Barbara Caputo",
                "Elisa Ricci"
            ],
            "title": "Best sources forward: domain generalization through source-specific nets",
            "venue": "In 2018 25th IEEE international conference on image processing (ICIP),",
            "year": 2018
        },
        {
            "authors": [
                "Massimiliano Mancini",
                "Lorenzo Porzi",
                "Samuel Rota Bulo",
                "Barbara Caputo",
                "Elisa Ricci"
            ],
            "title": "Boosting domain adaptation by discovering latent domains",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Puneet Mangla",
                "Shivam Chandhok",
                "Vineeth N Balasubramanian",
                "Fahad Shahbaz Khan"
            ],
            "title": "Context-conditional adaptation for recognizing unseen classes in unseen domains",
            "venue": "arXiv preprint arXiv:2107.07497,",
            "year": 2021
        },
        {
            "authors": [
                "Hyeonseob Nam",
                "HyunJae Lee",
                "Jongchan Park",
                "Wonjun Yoon",
                "Donggeun Yoo"
            ],
            "title": "Reducing domain gap by reducing style bias",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Hongjing Niu",
                "Hanting Li",
                "Feng Zhao",
                "Bin Li"
            ],
            "title": "Domainunified prompt representations for source-free domain generalization",
            "venue": "arXiv preprint arXiv:2209.14926,",
            "year": 2022
        },
        {
            "authors": [
                "Hongjing Niu",
                "Hanting Li",
                "Feng Zhao",
                "Bin Li"
            ],
            "title": "Domainunified prompt representations for source-free domain generalization",
            "venue": "arXiv preprint arXiv:2209.14926,",
            "year": 2022
        },
        {
            "authors": [
                "Novi Patricia",
                "Barbara Caputo"
            ],
            "title": "Learning to learn, from transfer learning to domain adaptation: A unifying perspective",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Xingchao Peng",
                "Qinxun Bai",
                "Xide Xia",
                "Zijun Huang",
                "Kate Saenko",
                "Bo Wang"
            ],
            "title": "Moment matching for multi-source domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Fengchun Qiao",
                "Long Zhao",
                "Xi Peng"
            ],
            "title": "Learning to learn single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Shiv Shankar",
                "Vihari Piratla",
                "Soumen Chakrabarti",
                "Siddhartha Chaudhuri",
                "Preethi Jyothi",
                "Sunita Sarawagi"
            ],
            "title": "Generalizing across domains via cross-gradient training",
            "venue": "arXiv preprint arXiv:1804.10745,",
            "year": 2018
        },
        {
            "authors": [
                "Manli Shu",
                "Weili Nie",
                "De-An Huang",
                "Zhiding Yu",
                "Tom Goldstein",
                "Anima Anandkumar",
                "Chaowei Xiao"
            ],
            "title": "Test- 10 time prompt tuning for zero-shot generalization in visionlanguage models",
            "venue": "arXiv preprint arXiv:2209.07511,",
            "year": 2022
        },
        {
            "authors": [
                "Mainak Singha",
                "Ankit Jha",
                "Bhupendra Solanki",
                "Shirsha Bose",
                "Biplab Banerjee"
            ],
            "title": "Applenet: Visual attention parameterized prompt learning for few-shot remote sensing image generalization using clip",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Mainak Singha",
                "Harsh Pal",
                "Ankit Jha",
                "Biplab Banerjee"
            ],
            "title": "Ad-clip: Adapting domains in prompt space using clip",
            "venue": "arXiv preprint arXiv:2308.05659,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Hemanth Venkateswara",
                "Jose Eusebio",
                "Shayok Chakraborty",
                "Sethuraman Panchanathan"
            ],
            "title": "Deep hashing network for unsupervised domain adaptation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yufei Wang",
                "Haoliang Li",
                "Alex C Kot"
            ],
            "title": "Heterogeneous domain generalization via domain mixup",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Ziqi Wang",
                "Marco Loog",
                "Jan van Gemert"
            ],
            "title": "Respecting domain relations: Hypothesis invariance for domain generalization",
            "venue": "In 2020 25th International Conference on Pattern Recognition (ICPR),",
            "year": 2021
        },
        {
            "authors": [
                "Zijian Wang",
                "Yadan Luo",
                "Ruihong Qiu",
                "Zi Huang",
                "Mahsa Baktashmotlagh"
            ],
            "title": "Learning to diversify for single domain generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yongqin Xian",
                "Bernt Schiele",
                "Zeynep Akata"
            ],
            "title": "Zeroshot learning - the good, the bad and the ugly",
            "venue": "CoRR, abs/1703.04394,",
            "year": 2017
        },
        {
            "authors": [
                "Zheng Xu",
                "Wen Li",
                "Li Niu",
                "Dong Xu"
            ],
            "title": "Exploiting lowrank structure from latent domains for domain generalization",
            "venue": "In European Conference on Computer Vision,",
            "year": 2014
        },
        {
            "authors": [
                "Junkun Yuan",
                "Xu Ma",
                "Defang Chen",
                "Kun Kuang",
                "Fei Wu",
                "Lanfen Lin"
            ],
            "title": "Domain-specific bias filtering for single labeled domain generalization",
            "venue": "International Journal of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Xin Zhang",
                "Yusuke Iwasawa",
                "Yutaka Matsuo",
                "Shixiang Shane Gu"
            ],
            "title": "Amortized prompt: Lightweight finetuning for clip in domain generalization",
            "venue": "arXiv preprint arXiv:2111.12853,",
            "year": 2021
        },
        {
            "authors": [
                "Long Zhao",
                "Ting Liu",
                "Xi Peng",
                "Dimitris Metaxas"
            ],
            "title": "Maximum-entropy adversarial data augmentation for improved generalization and robustness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Liang Zheng",
                "Yali Zhao",
                "Shengjin Wang",
                "Jingdong Wang",
                "Qi Tian"
            ],
            "title": "Good practice in cnn feature transfer",
            "venue": "arXiv preprint arXiv:1604.00133,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Ziwei Liu",
                "Yu Qiao",
                "Tao Xiang",
                "Chen Change Loy"
            ],
            "title": "Domain generalization: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Conditional prompt learning for vision-language models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Learning to prompt for vision-language models",
            "venue": "International Journal of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Yongxin Yang",
                "Timothy Hospedales",
                "Tao Xiang"
            ],
            "title": "Deep domain-adversarial image generation for domain generalisation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Yongxin Yang",
                "Timothy Hospedales",
                "Tao Xiang"
            ],
            "title": "Learning to generate novel domains for domain generalization",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Yongxin Yang",
                "Timothy Hospedales",
                "Tao Xiang"
            ],
            "title": "Learning to generate novel domains for domain generalization",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Yongxin Yang",
                "Yu Qiao",
                "Tao Xiang"
            ],
            "title": "Domain generalization with mixstyle",
            "venue": "arXiv preprint arXiv:2104.02008,",
            "year": 2008
        },
        {
            "authors": [
                "Beier Zhu",
                "Yulei Niu",
                "Yucheng Han",
                "Yue Wu",
                "Hanwang Zhang"
            ],
            "title": "Prompt-aligned gradient for prompt tuning",
            "venue": "arXiv preprint arXiv:2205.14865,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Advancements in large-scale vision and language models, such as CLIP [50] and ALIGN [18], have made remarkable progress in computer vision tasks. These models employ contrastively trained vision and text encoders to capture semantically meaningful concepts in a shared embedding space. They demonstrate impressive zero-shot generalization performance using text prompts like A photo of a [CLS]. However, designing an optimal prompt is\n*equal contribution\nchallenging, and recent studies focus on data-driven prompt optimization [69]. Despite their success, prompt learning is limited to the training data distribution and is susceptible to domain shift [9]. Domain shift, common in realworld applications, poses challenges as deep learning models are sensitive to differences between training and test data distributions [17]. To tackle this, researchers explore Domain Generalization (DG) [28, 67, 71], which aims to learn a domain-agnostic representation from multiple datasets sourced from different domains for application to novel target domains. Traditional DG techniques rely on vision encoders trained exclusively on image data [70,72,73]. Recent efforts combine foundation models with prompt engineering [50, 69] to bridge the semantic gap, but their practical applicability in DG settings requires further exploration.\nIn this paper, we focus on a more challenging setting where significant visual variations exist across different domains, unlike existing prompting methods [38, 68, 69, 74] that evaluate CLIP\u2019s generalization capabilities on datasets with limited domain shift (e.g., variants of ImageNet [24]). Fig. 1 illustrates the average multi-source DG performance\nar X\niv :2\n30 2.\n09 25\n1v 3\n[ cs\n.C V\n] 2\n8 N\nov 2\non DomainNet [48], where zero-shot CLIP [50] underperforms compared to the best traditional DG model, SWAD [6], by approximately 2.5%. Using a domain-conditional prompt (A [Domain] of a [CLS]) boosts CLIP\u2019s accuracy by nearly 3%, highlighting the importance of a representative prompt for DG. However, domain-level annotations are not always available, and the static domain name may not capture the style properties characterizing the domains [42]. Existing specialized prompt-tuning techniques [64, 68, 74] improve CLIP\u2019s performance (Fig. 1), but their effectiveness for DG is uncertain as prompts refined from random vectors may not effectively encode domain knowledge [68, 74]. Zhang et al. [64] propose a domain-prompt initialization strategy based on batch statistics of visual features but overlook important lower-level style characteristics and consider domain-level supervision. Another recent approach [45] learns prompts from CLIP without utilizing visual samples from the source domains but incorporates textual domain knowledge.\nThese discussions highlight a research gap in learning prompts that account for unknown domain shifts without explicit domain identifiers. We argue that leveraging visual features in such scenarios is crucial, along with dynamically incorporating object-level variations into the prompts to aid in cross-domain generalization tasks [68]. Motivated by these considerations, our research question is whether we can utilize CLIP\u2019s vision backbone to encode image style and content information for learning domain and instanceaware prompts to address DG.\nOur proposed STYLIP: We introduce STYLIP, a novel generic prompt tuning strategy for CLIP that addresses these challenges. Our approach aims to enhance the prompts\u2019 understanding of class concepts by conditioning them on domain and content information derived from CLIP\u2019s visual space. To achieve this, we leverage the hypothesis that instance-wise feature statistics from intermediate layers of an image encoder capture the visual domain information [35]. We extract mean and standard deviation values from CLIP\u2019s intermediate feature map outputs and utilize a set of STYLE PROJECTORS to learn domainspecific tokens in the prompts. Unlike existing models such as [68,69,74] that learn prompt token embeddings from adhoc sentences, our approach benefits from using style features at different scales, which leads to improved domainaware prompts and better prompt initialization.\nAdditionally, we propose to incorporate image content information into the prompt embeddings to capture objectlevel variations and avoid overfitting to the training classes, which is particularly important in DG scenarios concerning disjoint training and test classes. While Zhou et al. [68] addresses this issue by adding high-level semantic features from CLIP\u2019s vision encoder to prompt tokens, we consider a DG setting where the distributions of training and test\nclasses differ [40]. To achieve this, we combine visual feature responses from different layers of CLIP\u2019s vision encoder and aggregate them through a CONTENT PROJECTOR. By encoding mid- to high-level image characteristics that are more generic across categories [66], we aim to enhance transferability across domains/classes. Unlike the literature [22, 68], we propose a learnable fusion network to aggregate these visual features with the final prompt embeddings obtained previously. Contributions: We highlight our major contributions as: - We introduce STYLIP, a domain-unified prompt learning strategy that leverages CLIP\u2019s frozen vision encoder to extract the domain and content information from an image and deploy them in prompt learning through light-weight learnable style and content projectors. - We acquire prompt tokens from visual style features at various scales, facilitating the consolidation of hierarchical domain knowledge, thereby assisting in generalization across different domains. Moreover, we incorporate multiscale visual content information into prompt embeddings, effectively mitigating overfitting and promoting generalization across different categories. - We showcase the performance of STYLIP for multiple datasets on five major DG tasks: i) single-source and multisource DG, ii) cross-dataset DG, iii) in-domain base to novel class DG, and iv) cross-domain base to novel class DG, a novel task we introduce in the context of prompting. Experimentally, STYLIP outperforms the competitors in all tasks at least by 0.2\u22124%. To our knowledge, ours is the first attempt to extensively study the DG problem using CLIP."
        },
        {
            "heading": "2. Related Works",
            "text": "Domain generalization. The DG problem has different variations. Single-source DG [49, 60] trains with one domain, while multi-source DG [11,21,72] considers training multiple domains simultaneously. In a closed-world setting, where the label set is shared across domains, DG approaches commonly address domain shift. Heterogeneous DG [36, 58] faces additional challenges due to different labels between the source and target domains. Previous research on DG proposed methods such as domain alignment losses [20, 32, 33, 59], self-supervised learning [5], ensemble learning [62], domain-specific networks [41], and metalearning [47]. However, these methods often require more training domains, which can influence DG performance. To overcome this, novel pseudo-domains have been generated using domain augmentation approaches [21, 34, 72, 73]. In single-source DG models [49, 60, 65], diverse styles can be synthesized by perturbing the source domain through entropy maximization, meta-learning, and adversarial learning. Conversely, methods for heterogeneous DG [31,37,72] aim to improve model generalizability for novel tasks.\nDPL [64] used CLIP [50] for multi-source DG by in-\nferring domain information from batch-wise visual features. However, DPL doesn\u2019t fully leverage CLIP\u2019s ability to extract domain-specific artifacts and can overfit with small batches due to challenges in obtaining an unbiased style estimate. Researchers have explored domain invariant prompts [26, 45] through text-based source domain knowledge or image patches for prompt input in ViT models, similar to VPT [19]. Our STYLIP approach differs from [26, 45, 64] by considering style features at different visual encoder levels to learn individual prompt tokens and exploring multi-scale visual features in prompt learning, which have been successful in various DG tasks.\nIn a recent study, Cumix [40] combines DG with the notion of zero-shot learning [61] for the recognition of new domains and classes. The following research investigated the use of structured multimodal information [8] or disentangled feature learning [43] for similar aims. Our proposed experimental setup for a base to novel class generalization is identical; however, we are interested in analyzing the performance of the prompting techniques for VLMs in this respect, contrary to the more ad-hoc models mentioned above. Prompt tuning for vision-language models (VLMs). VLMs have gained attention in language processing and computer vision [2, 3, 14, 25, 51, 54, 55]. These models utilize task-centric textual descriptions for visual data [15,16]. Earlier prompting strategies were manual but later works focused on prompt learning. CoOp [69] optimized unified and class-specific prompts through back-propagation. CoCoOp [68] addressed CoOp\u2019s generalization issue through input-conditioned prompt learning. CLIP-adapter [10] proposed fine-tuning feature adapters in visual and language branches. ProGrad [74] prevents knowledge forgetting from the foundation model. TPT [53] utilizes consistency among multiple image views for supervision. Probabilistic and variational models [38, 39] learn prompt distributions to match visual feature spreads. LASP [4] improves the learned prompt via text-to-text cross-entropy loss. MaPle [22] enhances compatibility between CLIP encoders at different levels. However, these approaches are not tailored to deal with multi-domain data. In opposition, we introduce the notion of visual content-style disentanglement for prompt learning for DG tasks using CLIP."
        },
        {
            "heading": "3. Proposed Methodology",
            "text": ""
        },
        {
            "heading": "3.1. Problem and notation",
            "text": "The DG problem involves N labelled source domains Si = {xki , yki } ni k=1 \u2248 PS i\ndata, 1 \u2264 i \u2264 N , where xi \u2208 X i, yi \u2208 Y , and PS i\ndata denote the input data, label, and the joint distribution concerning the data and the label space, respectively. Furthermore, PS i\ndata \u0338= PS j data \u2200i, j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,N}, indicating that the source domains are mutually distinct. We call the setting single-source DG if N = 1, else it is known\nas multi-source DG. The goal is to train a model f : X \u2192 Y given S = {Si}Ni=1, which is expected to generalize for a novel target domain SN+1 = {xkt , ykt } nt k=1 unseen during training with xt \u2208 X t and yt \u2208 Yt and P tdata denotes the target distribution which is different from the source distributions. Typically, we consider a closed-set setting where Y \u222a Yt = Y \u2229 Yt. Also, for the base to new class generalization setting, we consider Y \u2229 Yt = \u2205.\n3.2. The STYLIP model\nHere, we introduce STYLIP, a novel approach for DG based on CLIP [50]. STYLIP leverages CLIP\u2019s frozen vision encoder (fv) and text encoder (ft), trained on a large volume of image-text pairs (see Fig. 2). fv that transforms an input image into a feature embedding vector can be implemented with different architectures: in our experiments (see Section 4), we consider ResNet-50 (RN50) [13], and ViT-B/16 [12]. ft is built upon a Transformer [56]: it is provided with an input of a sequence of word tokens and converts them into a vectorized representation.\nAs stated, STYLIP seeks to utilize the multi-scale visual features extracted from different levels of fv to estimate the style and content primitives and further channel them in learning a generic prompt space regarding a concept. Typically, high-level representations of the deepest layer of a vision encoder tend to capture the abstract object semantics suitable for classification but suffer from a lack of description of local patterns like oriented edges or local shapes [66]. Therefore, the set of characteristics obtained from multiple levels is deemed more transferable between tasks than the high-level features alone. Similarly, the instance-wise feature statistics calculated from multiple layers of the encoder capture different levels of style, e.g., the texture in the top layers usually has larger granularity than those in the bottom layers [1].\nTo model a continuous prompt embedding space using these multilevel visual features, STYLIP (see Fig. 2) adopts a set of projector networks on top of fv and ft: a set of M style projectors {Pm}Mm=1 to encode domain characteristics into M prefix tokens {cm}Mm=1, a content projector PC to encode feature responses from all the L encoder layers of fv after reducing their dimensions using bottleneck layers {Bl}Ll=1, and a fusion projector PA. We discuss the structure of the proposed projectors in detail below. Embedding multi-level style information into prompt tokens: For calculating the style features, let us consider the vector Fl(x) = [\u00b5\u20d7l(x); \u03c3\u20d7l(x)] denoting the channel-wise mean and standard deviation of the feature map outputs from the lth layer ( 1 \u2264 l \u2264 L) of fv , also indicated as f lv(x). Here, [\u2212;\u2212] denotes the concatenation operation. Specifically, if f lv(x) is of dimensions W \u00d7H \u00d7C (height, width, and depth dimensions), the statistics corresponding to the cth feature map f lcv , (\u00b5 c l , \u03c3 c l ), are calculated as:\n\u00b5cl = 1\nWH W,H\u2211 w,h=1 f lcv (x)w,h (1)\n\u03c3cl = \u221a\u221a\u221a\u221a W,H\u2211 w,h=1 (f lcv (x)w,h \u2212 \u00b5cl )2 (2)\nIn the simplest case, when the context length M equals the number of encoder layers L, we seek to learn the mth context vector cm from Fm(x). Considering that the dimensions of Fm(x)s are inconsistent and to appropriately input Fm into the text encoder ft, we deploy the style projectors {Pm}Mm=1 and compute cm(x) = Pm(Fm(x)), i.e. the mth context vector for the text prompt. We define\nty = [c1(x)][c2(x)] \u00b7 \u00b7 \u00b7 [cM (x)][CLSy] (3)\nas the prompt for (x, y) where [CLSy] is the word embedding of label y. Finally, ft generates the embedding ft(ty).\nHowever, the context length in prompting is a hyperparameter, meaning an M different from L may be preferred for a given task. To incorporate this flexibility in our prompt learning, we consider aggregation or replication of representations from {Fl(x)}Ll=1 depending on whether M < L or M > L, respectively. Supplementing the prompt embeddings with multi-scale image content features: The paradigm of ty considers the information of the visual style of the images, but a static class embedding CLSy for all images with the label y may limit its versatility. To further generalize the prompt embeddings, we propose to supplement ty with content image information. As discussed, we extract multiple complementary visual characteristics associated with an image by aggregating the multilevel feature responses obtained from the L blocks of fv .\nOne naive way to combine this multilevel information is by flattening the feature maps of individual blocks, fol-\nlowed by concatenation. However, this leads to a very highdimensional vector representation compared to the dimensionality of ty , undermining the effects of ty in the final classification weights. As a result, the contrastive task may lead to triviality. We propose reducing the feature maps\u2019 dimensions before concatenation as a remedy. This also shrinks the size of the inputs to PC , thus controlling its number of learnable parameters and the amount of information exchanged by the two encoders.\nPrecisely, given the lth-layer feature maps f lv(x), we perform 1 \u00d7 1 convolution followed by flattening using Bl to reduce the channel depth of f lv(x) from original C to C\u0302 << C, resulting in Bl(f lv(x)) \u2208 RWHC\u0302\u00d71. Finally, we concatenate the B(f lv(x))s to obtain f\u0302v(x):\nf\u0302v(x) = [B1(f1v (x));B2(f2v (x)); \u00b7 \u00b7 \u00b7 ;BL(fLv (x))] (4) The content projector PC learns the combined image embedding PC(f\u0302v(x)) through a linear transformation. To generate the classification weights for a given (x, y), we first concatenate PC(f\u0302v(x)) with ft(ty) and transform the aggregated information through the fusion projector PA to obtain t\u0302(x, y) as follows:\nt\u0302(x, y) = PA([PC(f\u0302v(x)); ft(ty)]) (5)"
        },
        {
            "heading": "3.3. Training and inference",
            "text": "The projectors are trained using a contrastive loss Lcon between t\u0302(x, y) and the image features obtained from the final embedding layer of fv , i.e., fv(x), as follows: Lcon = argmin\n{Pm}Mm=1, {Bl}Ll=1,PC ,PA\nE (x,y)\u223cPSdata \u2212 log(p(t\u0302(x, y)\u2225x)) (6)\nwhere PSdata is the joint data distribution of S and\np(t\u0302(xk, yk)\u2225xk) = e \u03b4(t\u0302(xk,yk),fv(x k))/\u03c4\u2211 n\u2208Y e\u03b4(t\u0302(xk,n),fv(xk))/\u03c4 (7)\n\u03b4 defines the cosine similarity and \u03c4 is the temperature hyperparameter. The contrastive loss synergistically maximizes the similarity between the image and the correct class prompt embeddings while minimizing the similarity between the image and all the opposing classes.\nDuring inference, we calculate the compatibility between fv(xt) and the prompt embeddings for all classes in Yt. The class with the highest compatibility is selected as:\ny\u0302t = argmax n\u2208Yt\np(t\u0302(xt, n)\u2225xt) (8)"
        },
        {
            "heading": "4. Experimental Results",
            "text": "Datasets: We evaluate STYLIP over five benchmark datasets for multi-source and single-source domain generalization, namely Office-Home [57], PACS [29], VLCS [30], Digits-DG [72] ], and DomainNet [48]. We further analyze the performance of STYLIP for cross-dataset generalization, where STYLIP is trained on ImageNet [24] and tested on ten other different datasets [69]. Detailed descriptions of the datasets are provided in SUPPLE. Implementation, training, and evaluation protocols: We implement the projectors in (PC ,PA, {Pm}Mm=1) as single dense layers. We train the model with Adam optimizer [23] with a learning rate of 2e \u2212 2 and betas (0.9, 0.999). We consider a context length of four for all the experiments following [68, 69]. For RN50, we consider the feature map outputs from the four convolution stages to extract the style and content features, hence L = M = 4. For ViT-B/16, we obtain the embedding outputs from the L = 12 encoder layers. We further average the features for every three consecutive layers of fv in a bottom-up manner without overlap to generate four intermediate feature representations, which are subsequently used to produce four distinct domain information vectors to be passed to {Pm}4m=1. As we show in Fig. 3, the feature statistics capture the domain information in ViT-based prompt learning for STYLIP, similar to RN50. We fix the number of output channels of the bottleneck C\u0302 = 3 using cross-validation, where 10% images from each source domain are treated as the validation set. We further ablate C\u0302 in Section 4.2 to check other architecture choices. Finally, we consider a mini-batch size of 4 for DomainNet and Office-Home, while it is 8 for the other datasets, and we train the model for 10 epochs. We report the average top-1 classification performance on SN+1 over three different executions. In terms of model complexity, STYLIP is extremely light-weight and consists of 0.18% more parameters than CoOp and CoCoOp and 0.06% more parameters than MaPLe. Baselines: We consider three types of methods for comparison to check the generalizability of pre-trained CLIP features and that of the prompting strategies. Our baseline is zero-shot CLIP with the prompt as \u2018A Photo of a [CLS]\u2019. We also include domain name in the prompt as\n\u2018A [Domain] of a [CLS]\u2019. We use CLIP features to train a linear classifier, which we term Linear Probing. Furthermore, we deploy these features in conjunction with the benchmark DG technique of CROSSGRAD [52], where we put the learnable networks on top of frozen CLIP for backpropagation training. From the traditional DG literature, we report the performance of SWAD [6] for Multi-DG and SagNet [44] and DSBF [63] for Single-DG, respectively. Furthermore, we choose to compare STYLIP with existing prompt learning techniques including CoOp [69], CoCoOp [68], CLIP-Adapter [10], DPL [64], ProGrad [74], VPT [19], CSVPT [26], MaPLe [22] and, TPT [53], etc.\nFinally, we evaluate three variants of STYLIP intending to ablate the individual components of our approach: i) {Pm}Mm=1 are trained from random vectors (similar to CoOp), but we consider the multi-scale content feature learning of STYLIP through ({Bl}Ll=1,PA,PC), respectively. (STYLIP-con). This establishes the importance of including the visual style information in the prompt tokens. ii) the model without content features and ({Bl}Ll=1,PC) but with {Pm}Mm=1 (STYLIP-sty). This is to verify the importance of the multi-scale content features, and iii) the version of STYLIP where the features of the deepest layer of fv are used for the content branch together with BL only (STYLIP*). This is to assess the importance of the multiscale content features over the single-scale high-level visual content properties as used in the literature [68]."
        },
        {
            "heading": "4.1. Comparison with state-of-the-art",
            "text": "We discuss the experimental comparisons of STYLIP with the literature in the following order of the DG tasks: i) multi-source DG, ii) single-source DG, iii) cross-domain base to novel class DG, iv) in-domain base to novel class DG and, v) cross-dataset DG, respectively. We follow the leave-one-domain-out evaluation protocol for multi-source DG where all the domains except one are considered source domains while the model is to be verified on the held-out target domain. For single-source DG, we train the model on\n1Methods are trained for a large number of epochs such as VPT [19] trains for 100 epochs, whereas STYLIP is trained only with 10 epochs.\none domain and test it on the remaining domains (leave-allbut-one-domain-out). We consider the standard few-shot training dataset with 16-shots, following the CLIP literature [22, 69] for all the tasks. However, we have mentioned a detailed sensitivity analysis of STYLIP against the number of available training samples in Fig. 5. Discussions on multi-source and single-source DG: We present the mean leave-out performance of PACS, VLCS, Office-Home, Digits-DG, and DomainNet in Table 1\nfor both RN50 and ViT-B/16 backbones. Our method, STYLIP, surpasses zero-shot CLIP, Linear Probing, and domain alignment approaches by at least 3% for both backbones, achieving state-of-the-art (SOTA) results. Additionally, STYLIP outperforms competitors, including DPL, CSVPT, and other prompting methods, across all datasets and vision backbones. Notably, when using ViT-B/16, STYLIP achieves outstanding performance with scores of 86.94% on VLCS, 81.38% on Digits-DG, and 62.03% on DomainNet, surpassing others by at least 3%.\nComparatively, the performance of STYLIP-con is slightly lower than STYLIP by approximately 0.5- 1.3%, while STYLIP-sty performs marginally better than STYLIP-con but remains inferior to STYLIP. However, both STYLIP-sty and STYLIP-con exhibit comparable or better performance than other prompting methods. The limitations of these variants of STYLIP are that they only capture partial visual properties, leading to sub-optimal prompt learning. In contrast, STYLIP fully leverages both style and content information of images, reducing the gap between visual and semantic spaces. Moreover, STYLIP outperforms STYLIP* due to the multi-scale content features, which are more generalizable than deeper semantically oriented visual representations.\nIn the single-source DG setting, using PACS, VLCS, and Office-Home datasets, we report the average leaveall-but-one-domain-out in Table 2 for all domain combinations. Remarkably, STYLIP achieves a convincing improvement of approximately 1.4-2.5% over other prompting techniques, establishing a new SOTA for single-source DG. For detailed domain-wise results in both single-source and multi-source DG setups, please refer to SUPPLEMENTARY. Generalizing across novel domains and categories: In this experiment on DomainNet, we consider ClipArt as the source domain while the others denote the target domain. We divide the classes equally, and the model is trained and tested on the disjoint class sets, following [40]. In Table 3, STYLIP outperforms the other prompting techniques in nine out of ten cases by \u2248 0.3\u2212 4% while generalizing to novel classes from both the source and the target\nTable 4. Comparison with SOTA methods on base-to-new generalization. STYLIP shows better generalization performance over existing methods on 11 different recognition datasets on 16-shots and a context length of four. HM is the harmonic mean. (In %)\nAverage over 11 datasets\ndomains, respectively. For the Real domain of DomainNet, STYLIP lags [68] by a mere 0.21%. STYLIP is less prone to overfitting to the classes of the source domain due to the better transferability offered by our model through multiscale feature embedding. To validate this, we repeat this experiment using the model STYLIP*, which deals with only the deepest layer visual encodings. Confirming our hypothesis, we find that the performance of STYLIP* is consistently poorer than STYLIP for all cases (\u2248 0.2\u2212 1.2%). In-domain base to novel class generalization: In addition to the cross-domain generalization to novel categories, we show the performance of STYLIP on the 11 datasets [69] where the base and novel classes are divided for each dataset to define the source and the target domains. A context length of four and 16 samples per class are considered for training the model. We depict the average performance over all the datasets in Table 4, which shows that STYLIP beats the state-of-the-art, CoOp [69], CoCoOp [68], LASP [4], and MaPLe [22] convincingly by more than 0.8% on average H-score (H-score is the harmonic mean of the baseclass and novel-class accuracies). Specifically, STYLIP is better than CoOp and CoCoOp by \u2248 8% and 4%, respectively. We observe that STYLIP is able to beat the others both for the base as well as novel classes. This is important since the existing methods are mostly found to boost the performance of novel classes at the cost of decreasing base class performance. Refer to Supplementary for the detailed results."
        },
        {
            "heading": "4.2. Ablation analysis",
            "text": "Generalization across datasets: Following the literature [69], we perform prompt learning using 16-shots from the 1000 classes of ImageNet (source) and test on the other 10 datasets (target). On the source domain, STYLIP beats the recent [22] by almost 1.6% (Table 5). In contrast, for more specialized target datasets, such as DTD, EuroSAT and FGVAircrafts, STYLIP beats the other competitors. For the fine-grained datasets, STYLIP shows improvements up to 2%, exhibiting much stronger transferability. Analysis of multi-scale features: For the PACS and VLCS datasets with RN50, we conducted two experiments to investigate the impact of multi-scale features from fv on performance. In the first experiment, we focused on the content feature corresponding to the Lth layer of fv , varying M from 1 to 4 for the style features. In the second experiment, we fixed M = 1 for the deepest layer style features and varied L from 1 to 4 for the content features. As shown in Figure 4, increasing the number of layers for both style and content features positively influenced the performance.\nIn another experiment, we seek to show the usefulness of the multi-scale content features. In this regard, we compare STYLIP with a multi-scale version of CoCoOp [68] where we combine the multi-scale features to the input tokens instead of the deepest layer features as done in the CoCoOp paper. It can be observed from Tab. 6 that STYLIP is able to beat MS-CoCoOp on multiple datasets for the Multi-DG task. This can be attributed to the improved prompting of STYLIP using the disentangled style and content features. Context length (M): As we mention in Fig. 5, we evaluate the effects of different context lengths for multisource DG on PACS using the ViT backbone. We find that STYLIP outperforms the other techniques, including [10, 64, 68, 69, 74] for context lengths of 1, 4, 6, 12, and 16. To generate the style primitives for M = 16, we choose to replicate the feature statistics vectors for the final four en-\ncoder layers, i.e., F9\u221212(x), in addition to that of the original 12 layers and feed them to {Pm}16m=1. A context of 4 provides the optimal performance for STYLIP. We further find that a longer context length drastically deteriorates the performance of [68, 74], while STYLIP performs consistently across all context lengths. Sensitivity to the number of training samples: To assess the robustness of STYLIP versus the number of training samples for the conventional DG setting, we train the single-source DG model on PACS while varying the number of training samples per class in the range [1, 5, 10, 16, All]. As shown in Fig. 5, the DG performance of [68, 69] degrades in the low-data regime, while [10,64,74] shows comparatively better performance. Finally, STYLIP maintains its superior performance for very few training samples and shows improvements with more shots. Depth of style and content projectors: To check the sensitivity of STYLIP on the depth of {Pm}Mm=1 and PC , we consider cases of multi-source DG where the projectors are two-layers and three-layers deep with a consistent number of nodes per layer, respectively, in PACS and Office-Home (Tab. 7). We find that performance decreases marginally with increasing depth: 0.6\u22120.8% for PACS and 0.2\u22120.4%\nfor Office-Home than STYLIP with linear projectors, suggesting STYLIP is indeed lightweight. Learnable vs. non-learnable PA: Typically, PC and ft produce feature embeddings of similar dimensions; hence, one way to fuse them in PA is through element-wise feature pooling. In this regard, we use the max and average feature pooling strategies and observe in Tab. 7 that such aggregations affect the performance, reducing the multi-source DG accuracies on PACS and Office-Home by \u2248 2\u2212 3% in max pooling and \u2248 4\u2212 5% in average pooling than STYLIP. Analysis of style features: Typically, the mean and std. of the feature maps together are known to capture the visual style information. To validate the same, we study the model\u2019s performance with either mean or std. being used as input to the style projectors. In this regard, we see a decrease in the performance of 1\u2212 2% compared to STYLIP, suggesting the importance of both statistical estimates. Interestingly, we see better accuracy when only std. is used for context learning than only mean (Tab. 7). Sensitivity to the depth of the bottleneck layer C\u0302: We consider different C\u0302 values in the range 2, 3, 4, 16 to see the effects of the bottleneck dimensions in the final accuracy (Tab. 7). While C\u0302 = 3 provides the best performance, we see the numbers decreasing from C\u0302 = 4 onwards, finally producing a dip of almost 1.5% for C\u0302 = 16. Besides, we consider the scenario where 1\u00d71 convolutions are not used, and we perform global average pooling (GAP), or directly flatten the feature maps and then concatenate. Both options perform poorly compared to STYLIP by 0.5\u2212 1%."
        },
        {
            "heading": "5. Takeaways",
            "text": "In this paper, we aim to address the challenge of domain shift in DG tasks by proposing STYLIP, a domain-agnostic prompt learning strategy for CLIP. By disentangling and incorporating multi-scale visual style and content information from CLIP\u2019s frozen vision encoder into the prompt learning process, we enhance its generalizability. Extensive evaluations on various cross-domain inference tasks demonstrate the consistent state-of-the-art performance of STYLIP. Our study on task-generalizable prompt learning paves the way for new research opportunities in computer vision. Future directions could explore domain-aware prompt learning with different foundation models and extend the proposed approach to structured prediction tasks."
        }
    ],
    "title": "STYLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization",
    "year": 2023
}