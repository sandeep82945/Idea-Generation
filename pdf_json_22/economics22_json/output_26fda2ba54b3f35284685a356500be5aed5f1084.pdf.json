{
    "abstractText": "Copulas are mathematical tools for modeling joint probability distributions. In the past 60 years they have become an essential analysis tool on classical computers in various fields. The recent finding that copulas can be expressed as maximally entangled quantum states has revealed a promising approach to practical quantum advantages: performing tasks faster, requiring less memory, or, as we show, yielding better predictions. Studying the scalability of this quantum approach as both the precision and the number of modeled variables increase is crucial for its adoption in real\u2010world applications. In this paper, we successfully apply a Quantum Circuit Born Machine (QCBM) based approach to modeling 3\u2010 and 4\u2010variable copulas on trapped ion quantum computers. We study the training of QCBMs with different levels of precision and circuit design on a simulator and a state\u2010of\u2010 the\u2010art trapped ion quantum computer. We observe decreased training efficacy due to the increased complexity in parameter optimization as the models scale up. To address this challenge, we introduce an annealing\u2010inspired strategy that dramatically improves the training results. In our end\u2010to\u2010end tests, various configurations of the quantum models make a comparable or better prediction in risk aggregation tasks than the standard classical models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Daiwei Zhu"
        },
        {
            "affiliations": [],
            "name": "Weiwei Shen"
        },
        {
            "affiliations": [],
            "name": "Annarita Giani"
        },
        {
            "affiliations": [],
            "name": "Saikat Ray\u2010Majumder"
        },
        {
            "affiliations": [],
            "name": "Bogdan Neculaes"
        },
        {
            "affiliations": [],
            "name": "Sonika Johri"
        }
    ],
    "id": "SP:a107de6de994de4ccd33c0f60d60c0910dc0871b",
    "references": [
        {
            "authors": [
                "P. Embrechts",
                "A. McNeil",
                "Straumann",
                "D. Correlation",
                "dependence in risk management"
            ],
            "title": "Properties and pitfalls",
            "venue": "In Risk Management: Value at Risk and Beyond (ed. Dempster, M. A. H.) 176\u2013223",
            "year": 2002
        },
        {
            "authors": [
                "M.J. Sklar"
            ],
            "title": "Fonctions de repartition a n dimensions et leurs marges",
            "venue": "Publ. Inst. Stat. Univ. Paris",
            "year": 1959
        },
        {
            "authors": [
                "H. Joe"
            ],
            "title": "Dependence Modeling with Copulas",
            "venue": "(CRC Press,",
            "year": 2014
        },
        {
            "authors": [
                "R. Lebrun",
                "A. Dutfoy"
            ],
            "title": "An innovating analysis of the Nataf transformation from the copula viewpoint",
            "venue": "Probab. Eng. Mech. 24,",
            "year": 2009
        },
        {
            "authors": [
                "P. Lambert",
                "Vandenhende",
                "F. A copula-based model for multivariate non-normal longitudinal data"
            ],
            "title": "Analysis of a dose titration safety study on a new antidepressant",
            "venue": "Stat. Med. 21, 3197\u20133217",
            "year": 2002
        },
        {
            "authors": [
                "C. Genest",
                "Favre",
                "A.-C"
            ],
            "title": "Everything you always wanted to know about copula modeling but were afraid to ask",
            "venue": "J. Hydrol. Eng",
            "year": 2007
        },
        {
            "authors": [
                "A.J. McNeil",
                "R. Frey",
                "P. Embrechts"
            ],
            "title": "Quantitative Risk Management: Concepts, Techniques, and Tools (Princeton",
            "year": 2010
        },
        {
            "authors": [
                "C. Goodhart"
            ],
            "title": "Holistic bank regulation",
            "venue": "In Handbook of Financial Stress Testing,",
            "year": 2022
        },
        {
            "authors": [
                "J. Huang",
                "M. Lanfranconi",
                "N. Patel",
                "L. Pospisil"
            ],
            "title": "Modelling Credit Correlations: An Overview of the Moody\u2019s Analytics",
            "venue": "GCorr Model (Moody\u2019s Analytics,",
            "year": 2012
        },
        {
            "authors": [
                "Li",
                "D. On default correlation"
            ],
            "title": "A copula function approach",
            "venue": "J. Fixed Income 9, 43\u201354",
            "year": 2000
        },
        {
            "authors": [
                "K. Aas",
                "C. Czado",
                "A. Frigessi",
                "H. Bakken"
            ],
            "title": "Pair-copula constructions of multiple dependence",
            "venue": "Insur. Math. Econ",
            "year": 2009
        },
        {
            "authors": [
                "G. Mazo",
                "S. Girard",
                "F. Forbes"
            ],
            "title": "A class of multivariate copulas based on products of bivariate copulas",
            "venue": "J. Multivar. Anal. 140,",
            "year": 2015
        },
        {
            "authors": [
                "F. Durante",
                "G. Salvadori"
            ],
            "title": "On the construction of multivariate extreme value models via copulas",
            "venue": "Environmetrics 21,",
            "year": 2010
        },
        {
            "authors": [
                "G. Elidan"
            ],
            "title": "Copulas in machine learning",
            "venue": "In Copulae in Mathematical and Quantitative Finance 39\u201360 (Springer,",
            "year": 2013
        },
        {
            "authors": [
                "Zhu",
                "E.Y. et al. Generative quantum learning of joint probability distribution functions. arXiv preprint arXiv"
            ],
            "title": "2109",
            "venue": "06315",
            "year": 2021
        },
        {
            "authors": [
                "J.W.Z. Lau",
                "K.H. Lim",
                "H. Shrotriya",
                "Kwek",
                "L.C. Nisq computing"
            ],
            "title": "Where are we and where do we go",
            "venue": "AAPPS Bull. 32, 27",
            "year": 2022
        },
        {
            "authors": [
                "S. Wei",
                "Y. Chen",
                "Z. Zhou",
                "G. Long"
            ],
            "title": "A quantum convolutional neural network on nisq devices",
            "venue": "AAPPS Bull. 32,",
            "year": 2022
        },
        {
            "authors": [
                "Cont",
                "R. Empirical properties of asset returns"
            ],
            "title": "Stylized facts and statistical issues",
            "venue": "Quant. Finance 1, 223",
            "year": 2001
        },
        {
            "authors": [
                "A. Zeevi",
                "Mashal",
                "R. Beyond correlation"
            ],
            "title": "Extreme co-movements between financial assets",
            "venue": "Available at SSRN 317122",
            "year": 2002
        },
        {
            "authors": [
                "S. Sim",
                "P.D. Johnson",
                "A. Aspuru-Guzik"
            ],
            "title": "Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms",
            "venue": "Adv. Quantum Technol",
            "year": 2019
        },
        {
            "authors": [
                "M.A. Nielsen",
                "I.L. Chuang"
            ],
            "title": "Quantum Computation and Quantum Information (Cambridge",
            "year": 2000
        },
        {
            "authors": [
                "M Benedetti"
            ],
            "title": "A generative modeling approach for benchmarking and training shallow quantum circuits",
            "venue": "npj Quantum Inf",
            "year": 2019
        },
        {
            "authors": [
                "J.C. Spall"
            ],
            "title": "An overview of the simultaneous perturbation method for efficient optimization",
            "venue": "Johns Hopkins APL Tech. Dig. 19,",
            "year": 1998
        },
        {
            "authors": [
                "D Zhu"
            ],
            "title": "Training of quantum circuits on a hybrid quantum computer",
            "venue": "Sci. Adv. 5,",
            "year": 2019
        },
        {
            "authors": [
                "K Wright"
            ],
            "title": "Benchmarking an 11-qubit quantum computer",
            "venue": "Nat. Commun. 10,",
            "year": 2019
        },
        {
            "authors": [
                "P. Cap\u00e9ra\u00e0",
                "Foug\u00e8res",
                "A.-L",
                "C. Genest"
            ],
            "title": "A nonparametric estimation procedure for bivariate extreme value copulas",
            "venue": "Biometrika 84,",
            "year": 1997
        },
        {
            "authors": [
                "Y. Zhang",
                "S. Nadarajah"
            ],
            "title": "A review of backtesting for value at risk",
            "venue": "Commun. Stat. Theory Methods",
            "year": 2018
        },
        {
            "authors": [
                "J.R. McClean",
                "S. Boixo",
                "V.N. Smelyanskiy",
                "R. Babbush",
                "H. Neven"
            ],
            "title": "Barren plateaus in quantum neural network training landscapes",
            "venue": "Nat. Commun",
            "year": 2018
        },
        {
            "authors": [
                "M. Gori",
                "A. Tesi"
            ],
            "title": "On the problem of local minima in backpropagation",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
            "year": 1992
        },
        {
            "authors": [
                "X. You",
                "X. Wu"
            ],
            "title": "Exponentially many local minima in quantum neural networks",
            "venue": "In International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "K. Kawaguchi",
                "L. Kaelbling"
            ],
            "title": "Elimination of all bad local minima in deep learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "1 Vol.:(0123456789) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nwww.nature.com/scientificreports"
        },
        {
            "heading": "Copula\u2011based risk aggregation",
            "text": "with trapped ion quantum computers Daiwei Zhu 1,3*, Weiwei Shen 2,3, Annarita Giani 2, Saikat Ray\u2011Majumder 2,"
        },
        {
            "heading": "Bogdan Neculaes 2 & Sonika Johri 1",
            "text": "Copulas are mathematical tools for modeling joint probability distributions. In the past 60 years they have become an essential analysis tool on classical computers in various fields. The recent finding that copulas can be expressed as maximally entangled quantum states has revealed a promising approach to practical quantum advantages: performing tasks faster, requiring less memory, or, as we show, yielding better predictions. Studying the scalability of this quantum approach as both the precision and the number of modeled variables increase is crucial for its adoption in real\u2011world applications. In this paper, we successfully apply a Quantum Circuit Born Machine (QCBM) based approach to modeling 3\u2011 and 4\u2011variable copulas on trapped ion quantum computers. We study the training of QCBMs with different levels of precision and circuit design on a simulator and a state\u2011of\u2011 the\u2011art trapped ion quantum computer. We observe decreased training efficacy due to the increased complexity in parameter optimization as the models scale up. To address this challenge, we introduce an annealing\u2011inspired strategy that dramatically improves the training results. In our end\u2011to\u2011end tests, various configurations of the quantum models make a comparable or better prediction in risk aggregation tasks than the standard classical models.\nJoint modeling of several random variables is required in analyzing multidimensional events, from assessing climate change, to predicting economic cycles, from identifying the cause of illnesses to guarding against catastrophic events and cyberattacks. Linear correlation, because of its simplicity in calculation and its equivalence to dependence when variables follow the normal distribution, has been widely adopted in data-based analysis and decision-making for multiple variables. However, its sole measure of linear relationship between two variables sets limitations and even pitfalls, particularly when the true distribution differs considerably from the normal distribution1. Meanwhile, the concept of a copula that expresses dependence on a quantile scale offers a richer representation of dependence beyond linearity and normality2, with applications in finance, engineering and medicine3\u20136.\nIn risk management, the dependence concept is crucial because it formalizes the idea of undiversifiable risks. Financial institutions then determine risk-based capital reserve accordingly. Undiversifiable risk, also called aggregate risk or systematic risk, refers to the vulnerability to factors that impact the outcomes for various aggregate financial vehicles, such as the broad stock market. While pairwise dependence is often captured by joint distributions, it imposes a tight constraint that the marginal distribution in each dimension should be in the same family as the associated joint distribution7. As they allow for modeling the joint distribution of a random vector by estimating its marginal distributions and dependence structure separately, copulas facilitate a practically desired approach of building multivariate risk models, where the marginal behavior of individual risk factors are often known better than their dependence structure8. In practice, as financial institutions are exposed to multiple types of risk, risk management and aggregation via dependence structure on the corporate level are required by both daily operation and regulation9. In credit risk modeling, risk factors are selected and aggregated by Gaussian copulas in the GCorr risk model by Moody\u2019s Analytics10. For derivatives pricing, the risk profile and pricing of collateralized debt obligations (CDOs) are mainly based on Gaussian copulas11. In addition, the recently developed vine copulas enable the flexible modelling of the dependence structure for portfolios in high dimensions12.\nWhile various bivariate copulas exist, how to construct copulas in higher dimensions is less clear. The three main classes of copulas, namely, Archimedean, vine and elliptical copulas, have their inherent shortcomings. Archimedean copulas lack flexibility in high dimensions due to their limited parameters. Vine copulas that\nOPEN\n1IonQ Inc., 4505 Campus Drive, College Park, MD, USA. 2GE Research, One Research Circle, Niskayuna, NY, USA. 3These authors contributed equally: Daiwei Zhu and Weiwei Shen. *email: daiwei@terpmail.umd.edu\n2 Vol:.(1234567890) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\noffer greater flexibility by increasing complexity in the modeling process by decomposing the density into conditional bivariate densities are prone to overfitting in applications. For example, in the modeling process for a ten dimensional canonical vine copulas, over one million decompositions are needed13. Elliptical copulas assume undesired dependence structure such as similar tail symmetry among all pairs of variables14. As pointed out by the survey paper15, limited algorithmic innovation in the copula community is borrowed from machine learning for automatically inferring the model structure from observed data. With the imposed mathematical formulations, the flexibility of the model is restricted by the assumed parameterization. In contrast, we present a data driven quantum method in this paper which does not make assumptions about the parametric forms of the dependence structure, and thus has a higher degree of modeling flexibility.\nFollowing the machine learning approaches to copulas, it has been demonstrated that a generative learning algorithm on trapped ion quantum computers for up to 8 qubits outperformed equivalent classical generative learning models with the same number of parameters in terms of the Kolmogorov\u2013Smirnov (KS) test16. In that work, a Quantum Generative Adversarial Network (QGAN) and a Quantum Circuit Born Machine (QCBM) were trained to generate samples from joint distributions of historical returns of two individual stocks from the technology sector. While this work outlined a general quantum implementation of copulas, numerical results from a quantum simulator and from quantum hardware were restricted to two variables. The scalability of the technique, which is at the core of the potential quantum advantage offered by this approach, is yet to be comprehensively analyzed. An end-to-end evaluation of how such an approach would perform in actual applications is also not developed. Explorations in these directions are pivotal for unlocking the practical potential of the aforementioned QCBM, as well as other flavors of variational quantum algorithms, when applied to noisy intermediate-scale quantum (NISQ) devices17,18. In this work, we increase the number of variables to three and four. We perform training on the latest generation quantum computer from IonQ (IonQ Aria) with up to 8 qubits, and evaluate the trained model with up to 16 qubits. Our quantum approach outperforms the classical methods in some of the end-to-end tests. We also observe a drop in the training efficacy as the model becomes larger or uses more qubits to support higher precision. To address this issue, we develop specialized techniques to train the parametric quantum circuits in higher dimensions that can be applied to hybrid quantum algorithms beyond the application domains in this paper. Finally, in the supplementary materials, we present a workflow to perform an end-to-end test that evaluates the efficacy of the model in real-world risk aggregation tests."
        },
        {
            "heading": "Problem description",
            "text": "In our study, we model the returns of four representative stock market indices\u2014DJI, VIX, N225 and RUT\u2014by copulas.\n1. Dow Jones Industrial Average (DJI) is the average of stock prices of 30 selected large and influential U.S. companies. 2. Market Volatility Index (VIX) created by the Chicago Board Options Exchange measures the stock market volatility through options on the S &P 500 Index. 3. Japan Nikkei Market Index (N225) is a stock market index including Japan\u2019s top 225 companies that represent Japanese economy after the World War II. 4. Russell 2000 Index (RUT) is a stock market index that tracks the performance of 2000 small-cap companies in the U.S.\nThey are easily accessible and have long periods of time from 01/04/2001 to 12/30/2020. These years reflect the vicissitudes of the market environment, such as multiple financial crashes and booms. On the one hand, DJI and RUT highlight the long-term performance of the U.S. market with their limited selection bias. They are highly positively correlated. On the other hand, N225 represents a foreign index with mild dependence with the U.S. market and VIX, as a market fear gauge, commonly negatively correlates with market indices. Thus, through empirical evaluations on those data, we can thoroughly understand the performance of different approaches in various domains in the dependence spectrum. After the data cleaning step each index has 4729 daily log returns. The data with computed statistics are shown in Table\u00a01. In our study, we assume returns of each index are independent and identically distributed in time and standardize the four indices. Figure\u00a01 illustrates the diverse dependence relations between indices as discussed. This study models the joint distribution of the returns of\n3 Vol.:(0123456789) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nthe four assets via copulas and then tests the corresponding risk estimates for an equally-weighted portfolio of those four assets, where both classical and quantum methods are employed in the modeling step for comparison."
        },
        {
            "heading": "Classical approach",
            "text": "In this section, we first recapitulate the copula framework used in this paper. Then we describe the steps of generating new samples from the given datasets by the classical approach. Illustration over a wide range of applications and theory of copulas may be found in the monograph3.\nSuppose that a random variable X has a continuous distribution. Then the random variable UX = FX(X) follows a standard uniform distribution, where FX(\u00b7) is its cumulative distribution function (CDF). This process is called the probability integral transform. It underlies the copula approach which is the transformation of a joint distribution into a set of marginal distributions used with a dependence function called a copula C(\u00b7) . The copula C is a multivariate distribution function with marginals following standard uniform distributions.\nSklar\u2019s theorem states that if F(x1, . . . , xn) is a joint distribution function with marginal distributions F1(x1), . . . , Fn(xn) , then there exists a copula function C : [0, 1]n \u2192 [0, 1] such that\nor\nwhere we call (x1, . . . , xn) \u2208 Rn as the variable in the original space and (u1, . . . , un) \u2208 [0, 1]n as the variable in the copula space. (1) offers the steps of building a copula model from input (x1, . . . , xn) in the original space, while (2) shows how we use a built copula model from the transformed input (u1, . . . , un) in the copula space. We provide a constructive algorithm combining the two steps starting from data inputs. Note that the sample in the copula space will be called the pseudo-sample. Denote by X \u2208 RM\u00d7n the input data matrix, Y \u2208 RN\u00d7n the\n(1)F(x1, . . . , xn) = C(F1(x1), . . . , Fn(xn)),\n(2)C(u1, . . . , un) = F(F\u221211 (u1), . . . , F \u22121 n (un)),\nFigure\u00a01. Scatterplot of the standardized returns of four indices in Table\u00a01. The diagonal subplots are the histograms for the four assets. Among those pairwise subplots, DJI and RUT are highly positively correlated. DJI and RUT are both negatively correlated with VIX. N225 has mild dependence with the U.S. market and VIX.\nAlgorithm\u00a01. Classical Copula Modeling\n4 Vol:.(1234567890) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nrandom sample matrix, and UX \u2208 RN\u00d7n the random pseudo-sample matrix after applying probability integral transform into each column of the input data matrix X , where M is the number of n-dimensional input data and N is the desired number of simulated samples. Denote by xi\u00b7 the i-th row of the data matrix X , x\u00b7j the j-th column of input data matrix, and xij the element in the i-th row and j-th column of the data matrix. Denote by ui\u00b7 the i-th row of the random pseudo-sample matrix, and u\u00b7j the j-th column of random pseudo-sample matrix. Denote by yi\u00b7 the i-th row of the random sample matrix, and y\u00b7j the j-th column of random sample matrix. Algorithm\u00a01 summarizes the steps of simulating N n-dimensional random sample points Y from M n-dimensional input data points X.\nWe apply the above algorithm to model the returns of the four indices. In line with known stylized results in equity returns, Student\u2019s t-distribution is used for the marginal distributions and t-copula is chosen for the dependence structure19,20. Figure\u00a02 illustrates the pseudo-samples after the probability integral transform. The pseudo-samples in the copula space are used as the input data for the quantum approach, and after the quantum approach generates new data in the copula space, the estimated inverse transformation F\u0302j\n\u22121 , j = 1, . . . , n is applied for study in the original return space."
        },
        {
            "heading": "Quantum formulation",
            "text": "A general quantum state on k qubits can be defined as follows:\nwhere |i\ufffd ( i \u2208 Z ) are known as \u201ccomputational basis states\u201d, and ci are complex numbers with the condition that \u2211\ni |ci| 2 = 1 . In general, a quantum state can be prepared by the application of a quantum circuit to a set of qubits which are all initialized to the 0 state. Measurement on the qubits after the state is prepared is equivalent to sampling from a random number generator, where the probability of obtaining the random number i is |ci|2 . A parameterized quantum circuit can be optimized to learn the joint distribution of the variables in a given dataset. After training, the quantum circuit can be executed the desired number of times to produce samples for downstream applications.\nThe previous work16 proposed a parametric quantum circuit ansatz that prepares a quantum state corresponding to a discretized copula distribution. This ansatz was used to model the correlation between two variables by optimizing the circuit parameters based on the dataset consisting of the returns of two individual stocks. In this study we train a generalized version of the ansatz which can handle an arbitrary number of variables. The ansatz is shown in Fig.\u00a03. To model n-variable copulas discretized to precision of m bits per variable, we need m\u00d7 n qubits. These qubits are divided evenly among n registers, where each register corresponds to one of the variables. Maximally entangled states called Greenberger\u2013Horne\u2013Zeilinger (GHZ) states are then formed which consist of one qubit from each register. At this point, the reduced density matrix of each register is an identity matrix, representing a standard uniform marginal. We then perform unitary transformations, denoted by U1, . . . ,Un in Fig.\u00a03, on each of the registers. The unitary transformations Ui are implemented via parameterized quantum\n(3)| \ufffd = 2k\u22121 \u2211\ni=0\nci|i\ufffd,\nFigure\u00a02. Pseudo-samples in the copula space of the four indices in Table\u00a01. The diagonal subplots are the histograms of the pseudo-samples of the four indices. Given the finite number of data points, the histograms are not perfect standard uniform distributions. Among those pairwise subplots, DJI and RUT are highly positively correlated. DJI and RUT are both negatively correlated with VIX. N225 has mild dependence with the U.S. market and VIX. Compared with Fig.\u00a01, the dependence structure has been more clearly revealed.\n5 Vol.:(0123456789) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\ncircuits. In principle, there are infinitely many designs of circuits that can realize any specific Ui , as stated in21. In practice, the choices are often made to leverage native controls, as well as known symmetries of the target dataset. As a rule of thumb, implementations with deeper circuits will have more expressibility, which quantifies the capability of a parameterized quantum circuit to reach different points in the Hilbert space21. In our work, Ui contains layers of the parametric circuit unit shown in Fig.\u00a03b. Each of the gates has an individual parameter controlling the angle of the single- or two-qubit rotation operation. The parametric circuit units are optimized for hardware implementation. In particular, the arbitrary single-qubit rotations layer is implemented as a sequential application of a single-qubit rotation along the z\u2212axis, Rz(\u03b8) = exp ( \u2212 i 12\u03b8\u03c3\u0302z ) , and a single-qubit rotation along the x\u2212axis, Rx(\u03c6) = exp ( \u2212 i 12\u03c6\u03c3\u0302x ) , where \u03c3\u0302i stands for the Pauli matrices, and the three parameters \u03b8 ,\n\u03c6 and \u03c8 correspond to the rotation angles of the gates. According to the Euler decomposition, any arbitrary single-qubit rotation can be decomposed into a sequential application of Rz(\u03b8) , Rx(\u03c6) and Rz(\u03c8) . We can commute the last Rz(\u03c8) through the entangling gate Rzz(\u03b8) = exp(\u2212i\u03b8\u03c3\u0302z \u2297 \u03c3\u0302z) and merge into the first Rz gate of the next layer. Here \u2297 is the tensor product, which indicates that the two Pauli matrices are applied on two different qubits. The Rzz gate of arbitrary angle is decomposed into a controlled-not gate (CNOT), a Rz gate, and another CNOT gate, for hardware implementation22. The above construction of Ui not only reduces the number of free parameters in optimization, but also maximizes the use of Rz gates, which are implemented virtually, thereby being noise-free. Similar to most of the popular universal quantum circuit ansatzes, with enough layers this structure is capable of representing any unitary transformation.\nWe employ a hybrid circuit optimization framework for training. Within this framework, the parametric circuit ansatz creates a quantum state from which samples are generated according to the measurements on the qubits. The probability of getting a specific readout result is directly related to the amplitude of such state in the superposition. Because this is known as Born\u2019s rule, such a procedure is generally known as the Quantum Circuit Born Machine (QCBM)23. Within each iteration of the hybrid optimization loop, we evaluate the parametric ansatz for a specified number of repetitions to estimate the distribution of the generated samples. Then we compare the generated distribution against the distribution of the target data. The difference, quantified by a cost function, is then used to drive a optimizer to modify the variational parameters. For this work, we use the simultaneous perturbation stochastic approximation (SPSA) algorithm24 as the optimizer. A brief explanation of the SPSA algorithm is given in the supplementary materials.\nTo comprehensively appraise the training results of different implementations of copula modeling, we randomly split the data set by 80/20 into the training and testing set. After the model is trained, we can evaluate\nFigure\u00a03. The hybrid training loop for the Quantum Circuit Born Machine (QCBM) framework: (a) The parametric ansatz used to model an n-variable copula. (b) The structure of each layer used to construct Ui . Each Ui constructed with such layers is used to address their corresponding variable. (c) In each iteration of the optimization loop, the ansatz with a given set of parameters is executed on the quantum computer repeatedly to collect a statistical estimation of the output states, which is represented here as a histogram. It is then compared with a target histogram. The difference, quantified by the KL-divergence is used by a classical optimizer to drive the optimization. The optimization loop repeats until convergence.\n6 Vol:.(1234567890) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nit with both an in-sample test (with the training set) and out-of-sample test (with the test set). These two tests benchmark not only how well the model trained, but also the utility of the trained model.\nTo model the real-world data with quantum circuits, we apply the following conversion between real-valued pseudo-samples in the copula space and the binary strings represented by the qubits. First, depending on how many qubits are allocated to each variable, we digitize the real-valued pseudo-samples into binary strings. The binary strings of all variables are then concatenated into a single binary string. To elaborate, assume we use a n-variable ansatz, with m qubits per variable. A sample in the copula space can be written as (d0, . . . , dn\u22121) with di \u2208 [0, 1) . The copula-space sample is then converted into binary-represented samples (b0, . . . , bn\u22121) , where bi is the largest m-digit binary number bi = bi,0...bi,m\u22121 that 12m \u2211m\u22121 j=0 bi,j2\nj \u2264 di . Here bi,j stands for the j-th digit of bi . Then the digitized binary representation is combined into a single binary number B = \u2211n\u22121 i=0 bi2\nm\u00d7i . Now this value can be exactly represented by measurement on the qubits. For example, assume we use two qubits for each variable. A pseudo-sample of two variables in the copula space (d1, d2) = (0.735, 0.222) is first converted into (b1, b2) = (10, 00) , and then combined into 1000. We use the distribution of binary strings converted from the training set as the target to train the quantum model.\nThe conversion of binary-valued measurement results of qubits to pseudo-samples is similar. We split each measurement on the qubits into n m-digit binary numbers. We convert each binary number bi = bi,0...bi,m\u22121 back into a real-valued number 2\u2212m\n\u2211m\u22121 j=0 bi,j2 j = di \u2032 , and then pad them with randomly generated numbers\n\u03b4 \u2208 [0, 12m ) . In the prior example, a qubit readout 1000 is first split into (b1, b2) = (10, 00) . Then the pair is converted back to real number as (d1\u2032, d2\u2032) = (0.5+ \u03b41, 0+ \u03b42) , where \u03b41 and \u03b42 are independently drawn from [0,\u00a00.25). Note the conversion into qubit representation is lossy due to the limited digits offered by the quantum model, while the conversion backwards is lossless.\nTo train the quantum model, as a common choice of cost function for a QCBM, we consider the Kullback\u2013Leibler divergence (KL-Divergence) to capture the difference of two distributions25:\nThe KL-Divergence is asymmetric with respect to P and Q. We set P as the distribution generated by the QCBM and set Q as the target distribution in our hybrid training. To avoid numerical singularity, we use the clipped version of the KL-Divergence as25:\nThe value of \u03b5 should be small enough so that it keeps the behavior of the KL-Divergence intact, yet large enough to prohibit numerical singularity. We heuristically set \u03b5 as 10\u22128."
        },
        {
            "heading": "Results",
            "text": "We present results from training the quantum models both on a simulator as well as trapped ion quantum hardware. The experimental demonstration was performed on the newest generation IonQ quantum processing unit (QPU). This system, as in previous IonQ QPUs26, utilizes trapped Ytterbium ions where two states in the ground hyperfine manifold are used as qubit states. These states are manipulated by illuminating individual ions with pulses of 355\u00a0nm light that drive Raman transitions between the ground states defining the qubit. By configuring these pulses, arbitrary single qubit gates and Molmer\u2013Sorenson type two-qubit gates can both be realized. Compared to its predecessors, this QPU features not only an order of magnitude better peak performance but also considerably better robustness in terms of gate fidelities. This allows for deep circuits with many shots to be run over a reasonable period of time. This increased data collection rate has made it possible to run hybrid optimization such as the one in this paper.\nFigure\u00a04a,b show two examples of training with hybrid quantum-classical optimizations involving 3 (DJI, VIX and N225) and 4 (DJI, VIX, N225 and RUT) variables, each with 2 qubits per variable. In both cases, the training on both the simulator and hardware converges, indicating that the training is practically scalable to higher than 2 dimensions studied in16. In Fig.\u00a04b, due to noise in the hardware, the experiment is unable to converge to as low of a minimum as the simulator. This effect is expected to be mitigated on future generations of hardware as the noise level becomes lower.\nWe first discuss in-sample testing results. For each variable, we compute the basic statistics, including the mean, standard deviation, skewness, kurtosis, the 5-th percentile, the 25-th percentile, the 50-th percentile, the 75-th percentile and the 95-th percentile of the simulated samples, for both classical and quantum implementation. The results for the basic statistics from the classical and quantum methods all produce less than 0.5% relative difference against the training data. Since all copulas have the standard uniform marginal, these tests have verified that our quantum ansatz in Fig.\u00a03 has captured the standard uniform marginal for each variable, irregardless of the training processes. We omit concrete values for simplicity.\nTo examine the ability of our quantum model to learn the correlation between variables, we then report in-sample tests based on Pearson linear correlations \u03c1 and upper tail dependence coefficients . In particular, Pearson correlation mainly captures the linear relation of two random variables around the body of the associated distributions, whereas the upper tail dependence coefficient measures the co-movements of two random variables in the tails of the distribution27. In financial applications, given a portfolio of multiple assets, the aggregated risk is mainly driven by the co-movements of the associated assets as the individual risks have been diversified away. When the market is volatile but remains far from capitulation, various aggregated risk measures can be chiefly\n(4)DKL(P,Q) = \u2211\nx\u2208\u03c7\nP(x) log\n(\nP(x)\nQ(x)\n)\n.\n(5)DKL(P,Q) = \u2211\nx\u2208\u03c7\nP(x) log\n(\nP(x)\nmax(Q(x), \u03b5)\n)\n.\n7 Vol.:(0123456789) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\ncaptured by Pearson correlation. When the market crashes, as big losses are in the tails of the return distributions, tail dependence coefficients are representative measures. Specifically, that the tail dependence coefficient is nonzero indicates the tail of the distribution is heavier than normal distributions.\nFigure\u00a05 illustrates that correlations estimated from different configurations gradually approach the ground truth as we enhance the expressibility of the variational ansatz by including more qubits and more layers. The configuration with 3-qubits per variable, 3-layers of variational ansatz per variable yields the best performance. More complex models with 4-qubits per variable show inferior performance due to the degradation in the optimization processes. We believe that the close correlation results from both correlations by the quantum and the classical method underscore that the quantum method has well captured correlated movement of random variables.\nIn the last column of Fig.\u00a05c,d, the inferior performance of the models suggests that the optimization has failed completely. In this case, the number of basis states of the qubits are 24\u00d74 > 65,000 , which is the same as the number of different samples the same number of classical bits can generate. But the number of data points we supply to the training is only 4729, which is not enough to train a model with such a precision. In contrast, the next smallest models have 24\u00d73 = 4096 basis states, by which the model are trained successfully. Consequently, we suggest that to train a QCBM, the number of qubits per variable should be chosen such that the number of basis states is less than the number of data points, so that the information included in the data is sufficient to train at such precision."
        },
        {
            "heading": "End\u2011to\u2011end evaluation",
            "text": "We perform out-of-sample validation for both the copula and the original return space for an end-to-end evaluation of the utility of the quantum framework. The latter are included in supplementary materials.\nAfter training, we check the accuracy of the estimated 95% value at risk (VaR) and expected shortfall (ES) on the testing set for an equally-weighted portfolio8,28. Financial institutions commonly calculate a spectrum of risk measures to quantify their risk exposure of their positions on a regular basis, and then reserve capitals accordingly for possible risk events due to internal needs or regulatory requirement9. Among them, VaR is probably the most widely used risk measure. Given some confidence level \u03b1 , the VaR of a portfolio with loss L at the confidence level \u03b1 is given the smallest number l such that the probability that the loss L is no larger than 1\u2212 \u03b1 . Formally, denoted by FL the cumulative distribution function of the loss, we have\nVaR is simply a quantile of the loss distribution. ES is closely related to VaR and often reported with VaR as a coherent risk measure. Mathematically,\n(6)VaR\u03b1 = inf {l \u2208 R : FL(l) \u2265 \u03b1}.\n(7)ES\u03b1 = 1\n1\u2212 \u03b1\n\u222b 1\n\u03b1\nVaRu(L)du.\nFigure\u00a04. Optimization on a simulator vs. real quantum computer (experiment). (a) Hybrid optimization of a three-variable parametric ansatz. Each register of the ansatz includes two qubits. In both experiment and simulation, we use 500 SPSA optimization steps to optimize the ansatz. Each step involves executing two circuits to probe the gradient stochastically. The optimization in experiment and in simulation matches remarkably well. (b) Optimization of a four-variable parametric ansatz. Each register of the ansatz includes two qubits. In both experiment and simulation, we use 600 SPSA optimization steps to optimize the ansatz. Due to larger number of gates, the circuit execution suffers from experimental noise non-trivially which leads to a higher value of the final cost function. However, the optimization evolution in experiment and simulation qualitatively agree with each other.\n8 Vol:.(1234567890) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nES averages VaR over all levels u \u2265 \u03b1 further into the tail of the loss distribution, and ES\u03b1 \u2265 VaR\u03b1 . Both are focused on the tail of the loss distribution as adverse co-movements of assets commonly lead to large loss on the tail. Hence, VaR and ES represent the further scrutiny of the modelled dependence structure on a portfolio besides Pearson correlations and tail dependence coefficients in Fig.\u00a05. As we only consider \u03b1 = 0.95 , the subscript is omitted for simplicity.\nWe report the ratio between the number of observed and expected failures for VaR, and report the ratio between the observed and expected severity for ES, where the failure is defined as the number of losses that are higher than the estimated VaR and severity is defined as the ratio between ES and VaR. Intuitively, the number of failures should be close to (1\u2212 \u03b1) of the tested samples for an accurate model, and the severity shows the extent to which ES captures tail risk that is not gauged by VaR. For ease of presentation, we call the former as the ratio of failures and the latter as the ratio of severity. By using ratios instead of raw values, we can more easily quantify the quality of the models in both copula space and return space in that observing near-one ratios in out-of-sample testing indicates the estimated VaR and ES are accurate. After training, before testing the decision rules of VaR and ES, VaR and ES are estimated on the data generated by the classical and the quantum approach, respectively. The accuracy of the estimated VaR and ES is evaluated by comparing the observed ratios with the expected ones on the testing data. In addition, all values are armed with the 95% confidence interval characterizing the estimation error by bootstrapping.\nFigure\u00a06 illustrates the two ratios estimated by different methods. The ratio of failures demonstrate higher variance as VaR is inherently more challenging to estimate than ES, while the ratio of severity is relatively stable as it takes average over losses on the tail in its calculation of ES. Both the classical and quantum methods are inclined to a conservative estimate for risk as the ratios are all not higher than one. Results from the simulator outperform the classical method in various configurations. Specifically, the optimal performance is roughly achieved around 3-qubits per variable and 1-layer structure for each Ui . The unimproved results for 4-qubits per variable, which is consistent with our observation in the correlator tests, is caused by the optimizer performance degradation. In the next section, we present a method to improve optimizer performance."
        },
        {
            "heading": "Annealing training strategy",
            "text": "Generally, optimization is a critical part of any hybrid quantum approach, including the QCBM. However, it is known that the optimization of quantum ansatzes can critically suffer from two issues: vanishing gradient and local minima of low utility. The vanishing gradient problem is also known as barren plateau29. As the expressibility of a hybrid quantum ansatz grows, the gradient corresponding to the variational parameters in general decreases. Such a trend will eventually render the estimation of gradients impossible due to limited precision caused by the finite number of measurements possible. High-cost local minima are another common issue for both classical and quantum machine-learning30\u201332. The training or optimization procedure cannot guarantee the retrieval of global minimum. Instead, local minima are almost always obtained. Hence, even in the absence of vanishing gradient, effective strategies to converge to a useful local minimum via optimization is of great significance.\nFigure\u00a05. The two-variable correlators estimated using samples generated from experiment and samples generated from simulation, with different hyperparameters: number of qubits used for each variable; and number of layers (as shown in Fig.\u00a03) used to address each variable. Within each column indexed i-qubits per variable (abbreviated as \u201cq.p.v\u201d in the figure), results corresponding to 1\u20134 layers are ordered from left to right. The colored lines correspond to the ground-truth of the correlators shown with the same color. We use spheres to represent simulation results and triangle to represent results from the QPU. \u201ce.\u201d in the label stands for experiments. \u201cs.\u201d in the label stands for simulation. (a) Estimation of linear correlations \u03c1 using 3-variable copula of different configurations. (b) Estimation of upper tail dependence coefficients using 3-variable copula of different configurations. (c) Estimation of linear correlations \u03c1 using 4-variable copula of different configurations. (d) Estimation of upper tail dependence coefficients using 4-variable copula of different configurations. Note that \u03c1ij and ij represent the linear correlation and the upper tail dependence coefficient between the i-th and j-th variable, respectively. The error bars are omitted since they are of the size of the markers. The ground truths are based on the training data set, thereby representing an in-sample test.\n9 Vol.:(0123456789) Scientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nWe present an approach inspired by the adiabatic annealing process to address these two issues33. We call our approach \u201cannealing training\u201d. The intuition is that by using an adaptive target (equivalently, cost function), we can first perform a relatively easy training, and then gradually increase the difficulty of the training. Assume P0 = p0(x) is the target distribution for our training. We define P(\u03b7) = p(x, \u03b7) as a \u201cfuzzy\u201d target such that p(x, \u03b7) = \u03b7u(x)+ (1\u2212 \u03b7)p0(x) , where u(x) is the uniform distribution of all the possible outputs x of the qubits. We start the training by randomly initializing the parameters of the model and perform the desired number of optimization steps with P(\u03b70) as the target. In principle, the annealing process should start with \u03b7 = 1 for the best performance. In practice, to save time in the annealing steps, \u03b7 can be initialized with a smaller value. This corresponds to starting annealing at a relatively low temperature, which may compromise the final training results depending on the specific cases. In our study, we heuristically start with \u03b70 = 0.8 . We then repeat the \u201cramping down\u201d steps, gradually decreasing \u03b7 down to 0, at which point the target becomes the original target P0 . Within each ramping down cycle, we first decrease \u03b7 by the chosen step size \u03b4\u03b7 , then initialize the parameters of ansatz with the final parameters obtained from the last ramping down cycle. We finish a cycle by repeating the optimization steps for the desired number of iterations.\nAnalogous to the thermal annealing process, as long as the ramping down step size \u03b4\u03b7 is sufficiently small, there should be a high chance that the high-utility minima obtained from the last ramping down cycle are close to those of the current target.\nWe compare the results from annealing training against those from standard training in Fig.\u00a07. For each training methods, we perform the training with 800 and 200 iterations. With the same training method, more training iterations always generate better training results. But we observe that results obtained from annealing training outperform results obtained from standard training, even with 4-fold reduction in training iterations. We also observe that the standard deviation of training results, from stochastically choosing initial parameters, is smaller in annealing training. This is expected because through the annealing process, as the level of the induced uniform noise decreases, the minima will shift with the transformation of the cost-function landscape. But as long as such transformation is slow, the classical optimizer should drive the parameters to follow the shift of the minima. This also explains why annealing training can reach better results with fewer iterations. Because the parameters always start near local minima at each annealing step. Admittedly, repeating the annealing steps induces nontrivial overhead compared against the standard training. However, we expect such overheads to stay constant as the complexity of the model grows. Moreover, the overhead is typically acceptable if it leads to difference between failure and success of the training."
        },
        {
            "heading": "Conclusion and outlook",
            "text": "With quantum entanglement, we can generate correlations among different qubits that have no classical correspondence. There is indication that these correlations are more efficient at modeling structure in the tail of data distributions, and this provides a route to modeling copulas that can better estimate quantities relevant\nFigure\u00a06. Out of sample test results for different QCBM models in the copula space, with different hyperparameters: number of qubits used for each variable; and number of layers (as shown in Fig.\u00a03) used to address each variable.. To perform the out-of-sample test, we split our data into a training set and a testing set, each contains 80% and 20% of the total samples exclusively. We use the training set to train our QCBM to generate samples to benchmark against the test data data set using two utility based metrics. The two metrics are based on an equally-weighted portfolio of the three and four indices, respectively. All values are armed a 95% confidence interval. \u201cq.p.v.\u201d along the horizontal axes stands for \u201cqubits per variable\u201d. Within each column indexed i-qubits per variable (abbreviated as \u201cq.p.v\u201d in the figure), results corresponding to 1\u20133 layers are ordered from left to right. (a) Ratio between observed and expected failures of the portfolio, aggregated with the 3-variable quantum model. (b) Ratio between observed and expected failures of the portfolio, aggregated with the 4-variable quantum model. (c) Ratio between observed and expected severity of the portfolio, aggregated with the 3-variable quantum model. (d) Ratio between observed and expected severity of the portfolio, aggregated with the 4-variable quantum model. The yellow line corresponds to the ratio obtained by the classical copula model. The lighter-yellow-colored region corresponds to the estimation error of the classical model results. Note that both the classical and the simulation approach estimate VaR and ES by 100K generated trials, while the experimental approach estimates VaR and ES by 5K generated trials.\n10\nVol:.(1234567890)\nScientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nto risk management. To gain critical insight into the practicality of the quantum approach as the problem size scales up, we numerically and experimentally demonstrated modelling of 3- and 4-variable copulas of a variety of configurations, on real-world index data for stock markets. We showed effective training in both simulation and the latest generation quantum computer from IonQ. We observed that the effectiveness of conventional optimization methods decreases as the complexity of the parametric quantum models grows. This complexity, mirroring the expressibility or raw power of the parametric quantum model, usually has to grow to accommodate larger problem sizes. To address this challenge, we introduced a novel optimization technique inspired by annealing, which greatly enhances the efficiency of training. This method provides an opportunity to extend variational quantum algorithms into the regime where the problem size would previously limit the efficacy of conventional optimizers. For future studies, it is of practical interest to characterize the annealing training more comprehensively by applying it to different types of problems.\nWe have performed in-sample and out-of-sample tests to evaluate multiple aspects of our trained models. Different from the prior work16, we presented an end-to-end test using twenty years of data for four representative stock market indices with diverse dependence structures tested on various metrics in risk aggregation applications including the value at risk (VaR) and the expected shortfall (ES). The use of four indices with different underlying dependence structures and various risk metrics has allowed us to fairly appraise the proposed quantum framework from both in- and out-of-sample tests. It has also enabled us to pinpoint the numerical challenges in scaling to high-dimensional problems with more complex dependence structures.\nTo conclude, about 60 years after the introduction of copulas, quantum computing is opening up new opportunities towards modelling and leveraging of dependence concepts. In particular, quantum copulas provide an additional tool for institutional investors in multi-asset return modeling for better risk assessment. This study takes quantum modelling of copulas several steps closer towards practical deployment and real-world impact. In future work, it is desirable to further characterize quantum advantage by extending our study to various sets of real-world data with a variety of interdependencies."
        },
        {
            "heading": "Data availibility",
            "text": "The stock market data are accessible using Python package finance. (at https:// pypi. org/ proje ct/ yfina nce/). All data generated or analyzed during this study are included in this published article and its supplementary\nFigure\u00a07. During annealing training, we start the training with a linear combination of the target distribution and with uniform distribution. Here, the initial target is a 20\u201380% mixture, which we refer to as uniform noise level \u03b7 = 0.8 . We repeat \u201cramping down\u201d steps, gradually decreasing the uniform noise level down to 0, at which point the target become the original target histogram. Within each ramping down cycle, we first decrease the noise level by the step size, then initialize the parameters of ansatz with the optimal parameters obtained from last ramping down cycle. Finally, we finish a cycle by repeating the optimization loops for a specified times. (a) Histogram of 20% target distribution mixed with 80% of uniform distribution. ( \u03b7 = 0.8 ). (b) Histogram of 60% target distribution mixed with 40% of uniform distribution. ( \u03b7 = 0.4 ). (c) Histogram of the original target distribution. ( \u03b7 = 0 ). (d) Optimization results (as KL-divergence) obtained with and without annealing training. We use a step size of 0.02 ( \u03b4\u03b7 = 0.02 ) for the ramping down. For annealing training, the optimization loops of each ramping down step is initialized to the final parameters from the last ramping down step. The results after each ramping down step is compared to the training with random initialization, but aimed at the same mixture of original target distribution and the uniform distribution. We explore two different number, 200 and 800, of optimization loops within each ramping down cycle. We observe that the annealing training, even with 4 times less iterations in each cycle, reaches a better final cost function.\n11\nVol.:(0123456789)\nScientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\ninformation files. The datasets used and/or analyzed during the current study are also available from the corresponding author upon reasonable request.\nReceived: 16 March 2023; Accepted: 4 October 2023"
        },
        {
            "heading": "Acknowledgements",
            "text": "A.G., W.S., S.R.M. and B.N. would like to thank Dave Vernooy for supporting quantum research\u2014computing, networking and sensing\u2014at GE Research."
        },
        {
            "heading": "Author contributions",
            "text": "D.Z., W.S., A.G., S.R.M, B.N. and S.J. designed research. W.S., A.G., S.R.M, and B.N. acquired and prepared the stock market data for the study. W.S., A.G., S.R.M, and B.N. implemented the classical model. D.Z. and S.j. formulated the quantum model. D.Z. and W.S. performed numerical simulation of the quantum model. D.Z. and S.J. performed experiments on the quantum computer. D.Z. and W.S. analyzed the experimental data. All authors contributed to the preparation of the manuscript."
        },
        {
            "heading": "Competing interests",
            "text": "The authors declare no competing interests."
        },
        {
            "heading": "Additional information",
            "text": "Supplementary Information The online version contains supplementary material available at https:// doi. org/ 10. 1038/ s41598- 023- 44151-1.\n12\nVol:.(1234567890)\nScientific Reports | (2023) 13:18511 | https://doi.org/10.1038/s41598-023-44151-1\nCorrespondence and requests for materials should be addressed to D.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or\nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n\u00a9 The Author(s) 2023"
        }
    ],
    "title": "Copula\u2010based risk aggregation with trapped ion quantum computers",
    "year": 2023
}