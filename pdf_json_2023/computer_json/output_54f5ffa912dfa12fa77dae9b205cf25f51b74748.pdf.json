{
    "abstractText": "Masked autoencoding has shown excellent performance on self-supervised video representation learning. Temporal redundancy has led to a high masking ratio and customized masking strategy in VideoMAE. In this paper, we aim to further improve the performance of video masked autoencoding by introducing a motion guided masking strategy. Our key insight is that motion is a general and unique prior in video, which should be taken into account during masked pretraining. Our motion guided masking explicitly incorporates motion information to build temporal consistent masking volume. Based on this masking volume, we can track the unmasked tokens in time and sample a set of temporal consistent cubes from videos. These temporal aligned unmasked tokens will further relieve the information leakage issue in time and encourage the MGMAE to learn more useful structure information. We implement our MGMAE with an online efficient optical flow estimator and backward masking map warping strategy. We perform experiments on the datasets of Something-Something V2 and Kinetics-400, demonstrating the superior performance of our MGMAE to the original VideoMAE. In addition, we provide the visualization analysis to illustrate that our MGMAE can sample temporal consistent cubes in a motion-adaptive manner for more effective video pre-training.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bingkun Huang"
        },
        {
            "affiliations": [],
            "name": "Zhiyu Zhao"
        },
        {
            "affiliations": [],
            "name": "Guozhen Zhang"
        },
        {
            "affiliations": [],
            "name": "Yu Qiao"
        },
        {
            "affiliations": [],
            "name": "Limin Wang"
        }
    ],
    "id": "SP:5e701ec3926643fcfc889393a043d43b2342157d",
    "references": [
        {
            "authors": [
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Georg Heigold",
                "Chen Sun",
                "Mario Lu\u010di\u0107",
                "Cordelia Schmid"
            ],
            "title": "Vivit: A video vision transformer",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Wele Gedara Chaminda Bandara",
                "Naman Patel",
                "Ali Gholami",
                "Mehdi Nikkhah",
                "Motilal Agrawal",
                "Vishal M Patel"
            ],
            "title": "Adamae: Adaptive masking for efficient spatiotemporal learning with masked autoencoders",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Songhao Piao",
                "Furu Wei"
            ],
            "title": "BEit: BERT pre-training of image transformers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Wenbo Bao",
                "Wei-Sheng Lai",
                "Chao Ma",
                "Xiaoyun Zhang",
                "Zhiyong Gao",
                "Ming-Hsuan Yang"
            ],
            "title": "Depth-aware video frame interpolation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Gedas Bertasius",
                "Heng Wang",
                "Lorenzo Torresani"
            ],
            "title": "Is space-time attention all you need for video understanding",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kelvin CK Chan",
                "Shangchen Zhou",
                "Xiangyu Xu",
                "Chen Change Loy"
            ],
            "title": "Basicvsr++: Improving video superresolution with enhanced propagation and alignment",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Chen",
                "Alec Radford",
                "Rewon Child",
                "Jeffrey Wu",
                "Heewoo Jun",
                "David Luan",
                "Ilya Sutskever"
            ],
            "title": "Generative pretraining from pixels",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yutao Cui",
                "Cheng Jiang",
                "Limin Wang",
                "Gangshan Wu"
            ],
            "title": "Mixformer: End-to-end tracking with iterative mixed attention",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Navneet Dalal",
                "Bill Triggs"
            ],
            "title": "Histograms of oriented gradients for human detection",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "year": 2005
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In North American Chapter of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Haoqi Fan",
                "Bo Xiong",
                "Karttikeya Mangalam",
                "Yanghao Li",
                "Zhicheng Yan",
                "Jitendra Malik",
                "Christoph Feichtenhofer"
            ],
            "title": "Multiscale vision transformers",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Haoqi Fan",
                "Yanghao Li",
                "Kaiming He"
            ],
            "title": "Masked autoencoders as spatiotemporal learners",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Feichtenhofer",
                "Haoqi Fan",
                "Jitendra Malik",
                "Kaiming He"
            ],
            "title": "Slowfast networks for video recognition",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Raghav Goyal",
                "Samira Ebrahimi Kahou",
                "Vincent Michalski",
                "Joanna Materzynska",
                "Susanne Westphal",
                "Heuna Kim",
                "Valentin Haenel",
                "Ingo Fr\u00fcnd",
                "Peter Yianilos",
                "Moritz Mueller- Freitag",
                "Florian Hoppe",
                "Christian Thurau",
                "Ingo Bax",
                "Roland Memisevic"
            ],
            "title": "The \"something something\" video database for learning and evaluating visual common sense",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhewei Huang",
                "Tianyuan Zhang",
                "Wen Heng",
                "Boxin Shi",
                "Shuchang Zhou"
            ],
            "title": "Rife: Real-time intermediate flow estimation for video frame interpolation",
            "venue": "arXiv preprint arXiv:2011.06294,",
            "year": 2020
        },
        {
            "authors": [
                "Sunil Hwang",
                "Jaehong Yoon",
                "Youngwan Lee",
                "Sung Ju Hwang"
            ],
            "title": "Efficient video representation learning via masked video modeling with motion-centric token selection",
            "venue": "arXiv preprint arXiv:2211.10636,",
            "year": 2022
        },
        {
            "authors": [
                "Will Kay",
                "Joao Carreira",
                "Karen Simonyan",
                "Brian Zhang",
                "Chloe Hillier",
                "Sudheendra Vijayanarasimhan",
                "Fabio Viola",
                "Tim Green",
                "Trevor Back",
                "Paul Natsev",
                "Mustafa Suleyman",
                "Andrew Zisserman"
            ],
            "title": "The kinetics human action video dataset",
            "venue": "arXiv preprint arXiv:1705.06950,",
            "year": 2017
        },
        {
            "authors": [
                "Heeseung Kwon",
                "Manjin Kim",
                "Suha Kwak",
                "Minsu Cho"
            ],
            "title": "Motionsqueeze: Neural motion feature learning for video understanding",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Yan Li",
                "Bin Ji",
                "Xintian Shi",
                "Jianguo Zhang",
                "Bin Kang",
                "Limin Wang"
            ],
            "title": "TEA: temporal excitation and aggregation for action recognition",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhaoyang Liu",
                "Donghao Luo",
                "Yabiao Wang",
                "Limin Wang",
                "Ying Tai",
                "Chengjie Wang",
                "Jilin Li",
                "Feiyue Huang",
                "Tong Lu"
            ],
            "title": "TEINet: Towards an efficient architecture for video recognition",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ze Liu",
                "Jia Ning",
                "Yue Cao",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Han Hu"
            ],
            "title": "Video swin transformer",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyang Liu",
                "Limin Wang",
                "Wayne Wu",
                "Chen Qian",
                "Tong Lu"
            ],
            "title": "Tam: Temporal adaptive module for video recognition",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Deepak Pathak",
                "Philipp Kr\u00e4henb\u00fchl",
                "Jeff Donahue",
                "Trevor Darrell",
                "Alexei A. Efros"
            ],
            "title": "Context encoders: Feature learning by inpainting",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Mandela Patrick",
                "Dylan Campbell",
                "Yuki M. Asano",
                "Ishan Misra",
                "Florian Metze",
                "Christoph Feichtenhofer",
                "Andrea Vedaldi",
                "Jo\u00e3o F. Henriques"
            ],
            "title": "Keeping your eye on the ball: Trajectory attention in video transformers",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiwu Qing",
                "Shiwei Zhang",
                "Ziyuan Huang",
                "Xiang Wang",
                "Yuehuan Wang",
                "Yiliang Lv",
                "Changxin Gao",
                "Nong Sang"
            ],
            "title": "Mar: Masked autoencoders for efficient action recognition",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Two-stream convolutional networks for action recognition in videos",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Hao Tan",
                "Jie Lei",
                "Thomas Wolf",
                "Mohit Bansal"
            ],
            "title": "Vimpac: Video pre-training via masked token prediction and contrastive learning",
            "venue": "arXiv preprint arXiv:2106.11250,",
            "year": 2021
        },
        {
            "authors": [
                "Jing Tan",
                "Jiaqi Tang",
                "Limin Wang",
                "Gangshan Wu"
            ],
            "title": "Relaxed transformer decoders for direct action proposal generation",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zachary Teed",
                "Jia Deng"
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Zhan Tong",
                "Yibing Song",
                "Jue Wang",
                "Limin Wang"
            ],
            "title": "Video- MAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Du Tran",
                "Heng Wang",
                "Matt Feiszli",
                "Lorenzo Torresani"
            ],
            "title": "Video classification with channel-separated convolutional networks",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pascal Vincent",
                "Hugo Larochelle",
                "Yoshua Bengio",
                "Pierre- Antoine Manzagol"
            ],
            "title": "Extracting and composing robust features with denoising autoencoders",
            "venue": "In International Conference on Machine Learning,",
            "year": 2008
        },
        {
            "authors": [
                "Pascal Vincent",
                "Hugo Larochelle",
                "Isabelle Lajoie",
                "Yoshua Bengio",
                "Pierre-Antoine Manzagol"
            ],
            "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Limin Wang",
                "Bingkun Huang",
                "Zhiyu Zhao",
                "Zhan Tong",
                "Yinan He",
                "Yi Wang",
                "Yali Wang",
                "Yu Qiao"
            ],
            "title": "Videomae v2: Scaling video masked autoencoders with dual masking",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Limin Wang",
                "Yu Qiao",
                "Xiaoou Tang"
            ],
            "title": "Action recognition with trajectory-pooled deep-convolutional descriptors",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Limin Wang",
                "Zhan Tong",
                "Bin Ji",
                "Gangshan Wu"
            ],
            "title": "TDN: Temporal difference networks for efficient action recognition",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Limin Wang",
                "Yuanjun Xiong",
                "Zhe Wang",
                "Yu Qiao",
                "Dahua Lin",
                "Xiaoou Tang",
                "Luc Van Gool"
            ],
            "title": "Temporal segment networks for action recognition in videos",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Rui Wang",
                "Dongdong Chen",
                "Zuxuan Wu",
                "Yinpeng Chen",
                "Xiyang Dai",
                "Mengchen Liu",
                "Yu-Gang Jiang",
                "Luowei Zhou",
                "Lu Yuan"
            ],
            "title": "Bevt: Bert pretraining of video transformers",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaolong Wang",
                "Ross Girshick",
                "Abhinav Gupta",
                "Kaiming He"
            ],
            "title": "Non-local neural networks",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Chen Wei",
                "Haoqi Fan",
                "Saining Xie",
                "Chao-Yuan Wu",
                "Alan Yuille",
                "Christoph Feichtenhofer"
            ],
            "title": "Masked feature prediction for self-supervised visual pre-training",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zheng Zhang",
                "Yue Cao",
                "Yutong Lin",
                "Jianmin Bao",
                "Zhuliang Yao",
                "Qi Dai",
                "Han Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xitong Yang",
                "Xiaodong Yang",
                "Sifei Liu",
                "Deqing Sun",
                "Larry Davis",
                "Jan Kautz"
            ],
            "title": "Hierarchical contrastive motion learning for video action recognition",
            "venue": "arXiv preprint arXiv:2007.10321,",
            "year": 2020
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Francis EH Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokensto-token vit: Training vision transformers from scratch on imagenet",
            "venue": "In IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Zach",
                "Thomas Pock",
                "Horst Bischof"
            ],
            "title": "A duality based approach for realtime tv-l 1 optical flow",
            "venue": "In Pattern Recognition: 29th DAGM Symposium, Heidelberg,",
            "year": 2007
        },
        {
            "authors": [
                "Chen-Lin Zhang",
                "Jianxin Wu",
                "Yin Li"
            ],
            "title": "Actionformer: Localizing moments of actions with transformers",
            "year": 2022
        },
        {
            "authors": [
                "Guozhen Zhang",
                "Yuhan Zhu",
                "Haonan Wang",
                "Youxin Chen",
                "Gangshan Wu",
                "Limin Wang"
            ],
            "title": "Extracting motion and appearance via inter-frame attention for efficient video frame interpolation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Kaidong Zhang",
                "Jingjing Fu",
                "Dong Liu"
            ],
            "title": "Flow-guided transformer for video inpainting",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhao",
                "Yuanjun Xiong",
                "Dahua Lin"
            ],
            "title": "Trajectory convolution for action recognition",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Yuan Zhi",
                "Zhan Tong",
                "Limin Wang",
                "Gangshan Wu"
            ],
            "title": "Mgsampler: An explainable sampling strategy for video action recognition",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Daquan Zhou",
                "Bingyi Kang",
                "Xiaojie Jin",
                "Linjie Yang",
                "Xiaochen Lian",
                "Qibin Hou",
                "Jiashi Feng"
            ],
            "title": "Deepvit: Towards deeper vision transformer",
            "venue": "arXiv preprint arXiv:2103.11886,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Attention-based Transformer [38] has witnessed great success in computer vision since the introduction of Vision Transformer (ViT) [12]. It has been applied for a variety of vision tasks and obtains state-of-the-art performance, such as image classification [36, 52, 59], object detection [23, 46], semantic segmentation [49], and object tracking [8]. Thanks to this high performance, ViT models have been also applied to the video domain for action recognition [1, 5] and detection [33, 54]. However, the high capacity of Transformer often demands pre-training on a large-scale\nB: Corresponding author (lmwang@nju.edu.cn).\ndataset to reduce the over-fitting risk of subsequent finetuning. Therefore, an effective pretraining strategy of ViT is particularly important for obtaining excellent performance in the video domain due to the smaller video dataset.\nThe early video transformers [1, 5] often rely on the pretraining of image-based transformer derived from the largescale image dataset [10]. This pre-training scheme makes the learnt video model to be naturally biased by image-based ViTs. Recently, masked autoencoding (MAE) [14, 35, 41] has been explored for pre-training video transformer on the video dataset due to its simplicity and promising result in image domain [17]. However, unlike the image, video data is equipped with an extra time dimension and exhibits the unique property of temporal redundancy and correlation. This property requires some customized designs on video masked autoencoder compared with image-based MAE. For example, VideoMAE and MAE-ST both propose to use an extremely high masking ratio in video masked autoencoder pre-training to improve its performance. In addition, VideoMAE devises a tube masking strategy of dropping tokens\nar X\niv :2\n30 8.\n10 79\n4v 1\n[ cs\n.C V\n] 2\n1 A\nug 2\n02 3\nat the same position across frames to further relieve the information leakage in time. This tube masking approach, though straightforward, makes the assumption of no or small motion occurring between adjacent frames. Such an assumption might be not true for some scenarios with high-speed motion.\nBased on the above analysis, in this paper, we aim to propose a new masking strategy for improving video masked autoencoder pre-training, by explicitly using motion information to reduce information leakage in time. Specifically, we devise the Motion Guided Masking in the video masked encoder processing and the resulted masked autoencoder is termed as MGMAE. Motion is general prior information contained by video. The optical flow representation explicitly encodes the movement of each pixel from the current frame to the next one. We propose to use this optical flow to align masking maps between adjacent frames to build consistent masking volumes across time. The consistent masking volumes enable to build a more challenging reconstruction task by enforcing only a small set of cube tracks visible to the encoder. Hopefully, this motion guided masking can further relieve the risk of information leakage in time and encourage learning more meaningful visual representations.\nMore specifically, we use an online and lightweight optical flow estimator (RAFT [34]) to capture motion information, which could be seamlessly integrated into the existing VideoMAE framework. To build the temporally consistent masking volume, we first randomly generate an initial masking map at the base frame. Then, we use the estimated optical flow to warp the initial masking map to adjacent frames. With multiple warping operations, we build the temporal consistent masking volume for all frames in the video. Finally, based on this masking volume, we sample a set of visible tokens to MAE encoders with top-k selection based on a frame-wise manner. The same autoencoding process with the original VideoMAE is applied to these sampled tokens for video pretraining. With this simple motion guided masking, we are able to further increase the difficulty of video pre-training task and thus lead to a better pre-trained model for subsequent fine-tuning.\nWe mainly verify the effectiveness of the proposed MGMAE on the datasets of Something-Something V2 [16] and Kinetics-400 [20] by comparing them with the original tube masking in VideoMAE. The results demonstrate that MGMAE pre-training can result in more powerful video foundation models with higher fine-tuning accuracy on the downstream tasks. In particular, on the motion-centric benchmark of Something-Something, the improvement of MGMAE is more evident, implying that our motion guided masking is adaptive to motion variations and can better capture temporal structure information for pre-training. We hope our findings can inspire some specific and unique designs in video masked autoencoding with respect to image counter-\nparts."
        },
        {
            "heading": "2. Related Work",
            "text": "Masked Visual Modeling. Masked autoencoder is a longstanding unsupervised learning framework in computer vision. The early work presented general form of denoising autoencoder [39, 40] for learning representation by reconstructing the clean signal from the noisy inputs. The other work [27] also treated masking modeling as inpainting missing regions from the surrounding context by using convolutions. Inspired by the great success of masked language modeling [11], some works also attempted to apply this pre-training paradigm to the vision domain for selfsupervised pre-training. For example, iGPT [7] followed the GPT work [30] in NLP and processed a sequence of pixels for casual prediction of the next pixels. The original ViT [12] used the masked token prediction as a self-supervised training step on large-scale image datasets but failed to obtain impressive results. Recently, several interesting works have obtained a great breakthrough in self-supervised image pretraining by using masked image modeling, such as BEiT [3], SimMIM [50], and MAE [17]. BEiT [3] directly followed the BERT framework and proposed to predict the discrete token label for masked patches, by requiring an explicit tokenizer to build the token dictionary. SimMIM [50] and MAE [17] shared the same design of directly predicting the pixels of masked patches without any tokenizer design. Furthermore, MAE [17] devised an asymmetric encoder-decoder architecture to speed up the masked image pre-training.\nSince the great success in masked image modeling, some works have tried to extend this new pre-training paradigm to the video domain for self-supervised video pre-training. BEVT [45] and VIMPAC [32] proposed to learn video representation by predicting discrete visual tokens in a similar way to BEiT. However, their performance improvement in video action recognition is limited. MaskFeat [48] used the HOG features [9] as the reconstructed targets of masked patches and achieved excellent performance on the video recognition with a multi-scale vision transformer. VideoMAE [35] and MAE-ST [14] extended the image MAE to the video domain for representation learning with vanilla vision transformer. They both proposed to use an extremely high masking ratio to deal with video data redundancy. Meanwhile, VideoMAE [35] used the tube masking to further increase the difficulty of reconstruction. Several works building upon VideoMAE have emerged. For instance, MAR [29] reduced both training and inference costs by introducing running cell masking. Meanwhile, VideoMAE V2 [41] proposes a dual masking strategy to decrease pre-training overhead, and by expanding both the model size and dataset, it further explores the scalability of VideoMAE. Our proposed motion guided masking aims to improve the performance of VideoMAE by building a more challenging masking and reconstruction\ntask. In contrast to the original VideoMAE, our MGMAE explicitly use the optical flow to align the masking maps across frames and generate the temporal consistent masking volume to sample a set of visible tokens.\nMotion Guided Modeling. Motion information, such as optical flow, is a general prior information in videos and represents the unique characteristics distinct from images. Optical flow has been widely introduced to provide a strong prior in both low-level and high-level vision tasks on video. For low-level video tasks, the motion is often used to align the information of auxiliary frames to the corresponding region of the target frame. In the case of video super-resolution, BasicVSR++ [6] uses optical flow to enhance the appearance of low-resolution frames by transferring features from neighbor frames. For video inpainting, Zhang et al. [56] exploits the motion difference extracted by optical flows to instruct the attention retrieval in transformer for high-fidelity video inpainting. As for video frame interpolation, mainstream methods leverage optical flow directly on the image to synthesize the intermediate frame, such as DAIN [4] and RIFE [18], while Zhang et al. [55] introduces a unified operation utilizing inter-frame attention to concurrently extract motion and appearance information, and blends a hybrid CNN and Transformer design for efficiency and fine-grained detail preservation. For high-level video tasks, the optical flow is directly used as a data modality as network input for action recognition [31,44]. TDD [42] utilized motion trajectories to pool deep convolutional features for action recognition. Trajectory Convolution [57] incorporated the motion information into temporal convolutional kernel design. MSNet [21] proposes a pluggable MotionSqueeze module to generate motion information across frames. VideoMS [19] generates mask maps by calculating the feature difference after patch embedding, making an attempt at dynamically adjusting mask positions. AdaMAE [2] introduces an end-to-end trainable adaptive masking strategy for MAEs, leveraging an auxiliary sampling network to prioritize tokens from high spatiotemporal information regions. Yang et al. [51] leverage hierarchical motion information to improve the extracted video features. MotionFormer [28] employed the trajectory for attention computation in the video transformer. TEA [22] and TDN [43] used RGB difference to approximate the motion information and incorporate this information into the video CNN backbone design. MGSampler [58] explored the motion information to select a subset of representative frames for efficient video action recognition. Our MGMAE shares the same spirit with these motion guided modeling works. We focus on employing motion information as a cue to generate masking maps for masked video pre-training."
        },
        {
            "heading": "3. Method",
            "text": "In this section, we first revisit the pre-training paradigm of VideoMAE to well introduce our MGMAE in Sec. 3.1. Then we present the details of motion guided masking map generation in Sec. 3.2. Finally, we describe the MGMAE pretraining under temporal consistent masking maps in Sec. 3.3."
        },
        {
            "heading": "3.1. VideoMAE revisited",
            "text": "VideoMAE is a simple masked video autoencoder with an asymmetric encoder-decoder architecture with an extra cube embedding to handle the input sampled frames. Next, we briefly revisit its implementation detail.\nCube Embedding. VideoMAE divides the input video clip I of size T \u00d7 3 \u00d7 H \u00d7 W into non-overlapping cubes C = { Ci | Ci \u2208 R2\u00d716\u00d716\u00d73 }N i=1\n, where N = T 2 \u00d7 H 16 \u00d7 W 16 is the number of cubes. Then apply cube\nembedding on the cubes to produce the video tokens T ={ Ti | Ti \u2208 RD }N i=1\n, where Ti represents the cube embedding with positional encoding, and D is the channel.\nMasking Strategy. VideoMAE uses the tube masking strategy with an extremely high masking ratio \u03c1 (i.e. 90%), which samples the same spatial positions across all frames of the input video clip. Specifically, VideoMAE first generate a H 16 \u00d7 W 16 binary mask map M\n\u2032 where 0 represents unmasked and 1 represents masked. Then it replicates it in temporal dimension and then flattens it to produce the token-level mask map M whose size is N for the input video clip. We denote M as the masking maps.\nEncoder. The encoder is a vanilla ViT with joint spacetime attention [5]. For computation efficiency, only the unmasked visible tokens Tv = {Ti}i/\u2208M added with the fixed positional embedding are fed into the encoder to obtain the latent features Z of size Nv\u00d7D, where Nv = \u230a(1\u2212\u03c1)N\u230b is the total number of the unmasked visible tokens.\nDecoder. The decoder is a narrower and shallower ViT than the encoder. It takes the concatenated token sequences as input, which is formed by the concatenation between the latent features Z and the learnable [MASK] tokens with the fixed position embedding added, to reconstruct the normal-\nized video cubes C\u0302 = { C\u0302i | C\u0302i \u2208 R2\u00d716\u00d716\u00d73 }N i=1 .\nLoss. The pre-training object is to minimize the Mean Square Error Loss between the normalized C and C\u0302 on the\nmasked positions, i.e. 1\u03c1N \u2211 i\u2208M \u2223\u2223\u2223C\u0302i \u2212 norm(Ci)\u2223\u2223\u22232.\nAfter the pre-training, the encoder will be used as the backbone network to fine-tune on the downstream tasks to obtain a specialized model."
        },
        {
            "heading": "3.2. Motion Guided Masking Map",
            "text": "Time is a unique characteristic of video and has different properties with the space dimension. When devising masked video autoencoder, we need to carefully take this extra time dimension into account and come up with a customized design. Information leakage in time is an important issue in masked video pre-training. When information leakage occurs, the model can easily reconstruct the masked cubes based on the visible tokens of adjacent frames. In this case, it will greatly reduce the difficulty of the reconstruction task and lead to a pre-trained model with poor fine-tuning performance. A trivial solution to information leakage is to increase the masking ratio. VideoMAE [35] and MAE-ST [14] increase the masking ratio to 90% to greatly increase the difficulty of reconstruction. In addition, VideoMAE makes the small motion assumption and adapts the tube masking strategy, which masks the same spatial position in all frames. However, this small motion prior is not always true for motion-dominated videos. A more reasonable approach is to keep each object in the video clip visible or invisible at all times. To achieve this goal, we propose the motion guided masking strategy to replace the tube masking\nstrategy in VideoMAE. The strategy has two procedures: we first use the optical flow as guidance to generate temporally consistent masking volumes of the input video clip and then sample the unmasked visible tokens based on the temporal consistent masking volume. We will detail these two procedures in Sec. 3.2 and Sec. 3.3.\nIn general, the procedure for generating the temporal consistent masking volumes has four steps as follows.\n\u2022 Step 1: Determine the base frame Ib, where b is the index of the base frame.\n\u2022 Step 2: Randomly generate a pixel-level initial mask map Mb with size H \u00d7W as the mask map of Ib.\n\u2022 Step 3: Extract the dense flows F bidirectionally from the base frame Ib in the input video clip I.\n\u2022 Step 4: Warp the initial masking map Mb under the guidance of dense flows F and progressively build the temporal consistent masking volume M of size T \u00d7 H \u00d7W .\nDetermine the base frame. By default, we choose the middle frame as the base frame. In motion guided masking, we need to ensure all objects in the base frame remain consistently visible or invisible in all frames of the input clip.\nNote that objects may (dis)appear over time due to object or camera movement, and warping the masking map under optical flow can result in some holes due to pixels mapped out of bounds. So the choice of the base frame may have an impact on the suppression of information leakage. We ablate the choice of the base frame in Sec. 4.2 and the middle frame is the optimal choice.\nGenerate the initial mask map. We initialize a pixellevel masking map for the base frame with the distribution of the Gaussian Mixture Model (GMM). We use the masking map to indicate the visible or invisible state of the cubes in the base frame. Previous masking strategies usually adapt a token-level binary initialization, i.e. either all 0 or all 1 within each token of size 2\u00d7 16\u00d7 16, which actually breaks the continuity of the object surface texture.\nSpecifically, we first randomly pick N\u0302v = \u230a(1 \u2212 \u03c1) \u00d7 H 16 \u00d7 W 16 \u230b tokens whose centers are denoted with c =\n{c\u20d7i : (ci1, ci2)}N\u0302vi . Then we generate 2D Gaussian distributions Ni(ci, \u03c32) centered on the midpoint of each token, where \u03c3 is the standard deviation and taken as the cube size (16, 16). Thus we will obtain the mixed Gaussian distribu-\ntion P(c, \u03c32) = \u2211N\u0302v\ni Ni(c\u20d7i, \u03c32) corresponding to the base frame. And the probability density function of the mixed Gaussian distribution is used to indicate the probability that the cube (token) is visible in the base frame.\nExtract optical flows. We use both online and offline alternative methods to extract optical flow. Online method adapts RAFT [34] (the small version) to estimate the flows of the input video clip. Offline method applies the traditional TVL1 [53] algorithm to extract the dense flows between all adjacent frames in advance. We perform the consistent crop-resize-rescale operations when reading flows. Online and offline methods achieve the similar results. More details see in Sec. 4.2.\nIn practice, we only extract the flows F forward and backward from the base frame Ib, i.e.\nF = {\u03c5i\u2192i+1}b\u22121i=1 \u222a {\u03c5i\u2192i\u22121} T i=b+1 , (1)\nwhere the flow \u03c5i\u2192j denotes the flow from Ii to Ij .\nWarp masking maps with flows. We utilize the method of backward warping to generate the temporal consistent masking map of the video clip in a progressive manner. Forward warping \u03d5F and backward warping \u03d5B are two opposite patterns of warping. Both can be effectively employed to construct the masking volume from the initial masking map. Regrettably, forward warping suffers from hole or occlusion problems, that is, no flow vectors may pass to a certain pixel, or there may be multiple flow vectors passing to the same pixel. In contrast, backward warping maps the pixels of a\ngiven map one by one to individual locations. It\u2019s noteworthy that while backward warping doesn\u2019t escape from the issue of holes caused by mapping out of bounds, these holes are usually fewer than in forward warping and tend to occur at the boundaries of the map, thus causing less damage to the information distribution. For the holes caused by backward warping, we fill them with the values of Mb at the same position to simulate the tube masking strategy.\nFormally, given the flows F and the base frame masking volume Mb, the mask map Mi of Ii can be constructed as\nMi =  \u03d5B(Mi+1, \u03c5i\u2192i+1), 1 \u2264 i < b Mb, i = b\n\u03d5B(Mi\u22121, \u03c5i\u2192i\u22121), b < i \u2264 T . (2)\nSubsequently, the entire masking volume M = {Mi}Ti=1 of the video clip I can be constructed by backward warping the flows F bidirectionally, originating from the base frame mask map Mb."
        },
        {
            "heading": "3.3. Motion Guided MAE",
            "text": "We build our MGMAE based on the above motion guided masking map. The temporal consistent masking volume indicates the probability that the corresponding position in the adjacent frame is visible under optical flow tracking. In order to suppress information leakage as much as possible, we sample the video tokens with the highest visible probability along the temporal dimension. Specifically, we first perform average pooling with kernel size 2\u00d7 16\u00d7 16 on the masking volume M to obtain the token-level masking volume M\u2032 of size T2 \u00d7 H 16\u00d7 W 16 . Then we pick the top-N\u0302v locations of each mask map of size H16 \u00d7 W 16 along the temporal dimension and thus sample Nv corresponding video tokens as the unmasked visible tokens.\nThese sampled tokens according to our temporal consistent masking volume are fed into the asymmetric encoderdecoder for autoencoding based pre-training. The resulted pre-training framework is called Motion Guided Masked Autoencoder (MGMAE). The pre-trained model by our MGMAE is applied in the same way with the original VideoMAE for fine-tuning the downstream tasks.\nDiscussion. Previous works [14, 35] extended MAE to the video domain. They opted for the random (agnostic) masking and tube (space-only) masking strategies, respectively. Random masking introduces no explicit inductive bias about the video\u2019s space and time dimension. It aims to present a unified feature representation learning framework with minimal domain knowledge. We argue that although this idea is simple, time is inherently a distinctive dimension from space. By recognizing this, we could better leverage this prior information for enhanced video masked autoencoding. Tube masking assumes that a large area of the frame contains\nno or small motion, and thus masking the same position across frames could greatly reduce the information leakage risk. However, for motion-dominated video datasets such as Something-Something, this assumption will no longer hold true. Our proposed motion guided masking offers a more general and conceptually simple solution to take temporal correlation into account. It could be viewed as an adaptive video masking strategy and create more challenging yet meaningful tasks in video pre-training."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Dataset",
            "text": "Following the original VideoMAE, we evaluate our MGMAE on Kinetics-400 (K400) [20] and SomethingSomething V2 (SSV2) [16]. K400 contains about 240k training videos and 20k validation videos from YouTube and the actions in K400 are usually coupled with specific objects or scenes, such as brushing teeth and playing piano. While SSV2 contains about 169k training videos and 25k validation videos, and the categories in SSV2 only care about specific motion patterns (e.g. push, pull). We first pre-train the video transformer with our MGMAE on the corresponding dataset for self-supervised representation learning. Then, we report the fine-tuning performance of pre-trained models on the target datasets for action recognition. In our MGMAE pretraining, we generally follow the setting and implementation of the original VideoMAE [35]. We use the RAFT [34] to extract optical flow due to its efficiency and accuracy in our MGMAE pre-training."
        },
        {
            "heading": "4.2. Ablation Studies",
            "text": "In this subsection, we conduct in-depth ablation experiments on the choice in each step of our motion guided masking strategy. We pre-train the ViT-base model 800 epochs on the SSV2 dataset with 16 80G-A100 GPUs, and then finetune the encoder on the SSV2 dataset for action recognition. All models share the same training schedule and report the 2 clips \u00d7 3 crops accuracy.\nChoice of the base frame. In this study, we investigate the influence of base frame selection for initial masking generation process. We compare the middle frame as the based frame with either the first or a random frame, and the result is shown in Tab. 1a. It implies the middle frame is the best.\nWarping method with optical flow. We compare two kinds of warping methods to align masking maps across frames as explained in Sec. 3.2. As previously mentioned above, the forward warping often leads to more severe occlusion and hole problems in the masking warping process. On the contrary, backward warping can effectively relieve\nthis issue and ensure a more smooth masking warping. The result in Tab. 1b demonstrates that back warping contributes to better performance.\nSampling strategy of top-k visible tokens. We examine and compare the two sampling strategies to select the visible tokens based on our temporal consistent masking volume. Frame-level strategy samples the top-k locations for each frame independently, while clip-level strategy samples the top-k locations for the entire video jointly. As in Tab. 1c, the frame-level top-k sampling strategy achieves slightly better performance.\nMasking initialization at the base frame. We ablate the choice of generating the initial masking at the base frame as shown in Tab. 1d. The token-level initialization method divides the mask map into H16 \u00d7 W 16 tokens of size 16\u00d7 16, and randomly sets 90% tokens to 0 (representing masked) and 10% tokens to 1 (representing unmasked). The pixellevel initialization method randomly sets 90% pixels to 0 and 10% pixels to 1. The initialization process of the mixed Gaussian method has been detailed in Sec. 3.2. The result demonstrates that the mixed Gaussian initialization method works the best.\nHole filling method. We investigate the various methods to handle the holes problem brought by the mapping out of bound in backward warping in Tab. 1e. To determine the real holes caused by warping, we set the 0 in the initial mask map to value 1e \u2212 8, and then the locations equal to 0 in new mask maps are treated as the holes. We experiment with 5 methods to fill the holes: Invisible method fills all holes with 0, while Visible method fills all holes with 1. Random method randomly fills the holes to 0 with probability of masking ratio \u03c1 and to 1 with probability of 1\u2212 \u03c1. Previous map method fills holes using the values from the same spatial positions as the last generated mask map. Conversely, Tube method fills the holes with the value from the corresponding positions of the initial mask map, aligning with tube masking principles. We see that the tube method performs the best among all the methods.\nMethod of optical flow estimation. We evaluate the effect of different methods of optical flow estimation as shown in Tab. 1f. For the offline method, we use the TVL1 algorithm to extract optical flows in advance and it achieve a comparable accuracy to the online RAFT optical flow. Although VideoMAE is 1.3 times faster to train than MGMAE with RAFT-small (set to 6 testing iterations) to estimate flows, MGMAE has a clear advantage in terms of performance and reducing the risk of overfitting. We find that the offline method is not much faster than the online method because\ncase Acc@1 Acc@5 first frame 70.5 92.7\nrandom frame 70.6 92.9 middle frame 71.0 93.1\n(a) Base frame selection. We perform ablation study to select the base frame as the first, random or the middle frame.\ncase Acc@1 Acc@5 forward 70.5 92.9\nbackward 71.0 93.1\n(b) Warping method. We choose forward or backward warping method for aligning masking maps.\ncase Acc@1 Acc@5 clip-level 70.7 92.9\nframe-level 71.0 93.1\n(c) Sampling strategy. We perform a study to choose the visible token generation strategy based on motion guided masking volume.\ncase Acc@1 Acc@5 token rand. 70.9 93.0 pixel rand. 70.8 93.0\nmixed Gauss 71.0 93.1\n(d) Masking initialization. We compare three methods to generate the masking map in the base frame. Two binary random methods and one mixed Gaussian method.\ncase Acc@1 Acc@5 random 70.8 92.9 invisible 70.7 92.8 visible 70.8 93.1\nprevious map 70.6 92.8 tube 71.0 93.1\n(e) Hole filling. We choose some baseline choices to fill the value in the hole place caused by warping. We also use the tube filling consistent with the tube masking.\nmethod time Acc@1 None (VideoMAE) 32 h 69.6\nTVL1 (offline) 41 h 71.2 RAFT with 6 iters 43 h 71.3\nRAFT with 12 iters 56 h 71.0\n(f) Method of optical flow estimation. We perform the ablation study to investigate the influence of different methods of optical flow estimation.\nTable 1: Parts of the ablation experiments on the Something-Something V2 dataset. Our MGMAE pre-training is implemented with the 16-frame vanilla ViT-B backbone. All models are pre-trained for 800 epochs and the masking ratio is \u03c1 = 90%. The inference protocol is to report the fine-tuning action recognition accuracy with 2 clips \u00d7 3 crops. The default choice for our\nmodel is colored in gray . Although the default setting is not optimal in terms of the method of optical flow estimation, we believe that this does not affect the conclusions of the ablation experiments.\nIO (reading optical flow from disk) will be a bottleneck to increase the training speed. Note that our default setting is not optimal, but the conclusions drawn in other ablation experiments should not be impacted.\nMasking ratio. The performance of MGMAE highlights the importance of improving masking strategies even at high masking rates (e.g. 90%). However, after applying MGMAE, it remains questionable whether such high masking rates are still necessary. Indeed, as pointed out by [14, 35], blindly increasing the masking rate could potentially degrade the model performance. Our ablation study presented in Fig. 3 shows that sustaining a extremely high masking rate of over 80% is also crucial even with MGMAE. We think the video background and large objects mainly drive the need for a high masking ratio. Video backgrounds are often wide and simple. If the mask ratio is not high enough, the model can still rebuild pixels from other background parts, even if nearby frames mask similar sections. For large objects, a lower ratio might let the model use the texture from a different part of the object when another section is masked. It can also be observed that MGMAE performs optimally with 85% masking ratio, but 90% still seems to be a decent choice when considering the trade-off between training efficiency and performance.\nExposure of masked objects. Another proposition worth considering is whether occasional exposure of masked ob-\njects help with masked video modeling pre-train. We completed a complementary experiment. Specifically, after building the masking volume, we add Gaussian noise on the mask map of one randomly selected frame. This modification may provide a chance for masked objects to be exposed. The results showed an accuracy of 71.2%, slightly higher than the default setting of 71.0%."
        },
        {
            "heading": "4.3. Main Results and Visualization Analysis",
            "text": "After the detailed ablation study on the design of MGMAE, we further perform a deeper analysis by comparing it with the original VideoMAE. We also provide some inter-\nmediate visualization results to illustrate the motion guided sampling process.\nPre-train loss implies a more challenging task. The core design of MGMAE is to dynamically sample the positions of masked token under the guidance of optical flows and aims at increasing the difficulty of the reconstruction task. As can be seen in Tab. 2, the pre-train loss of MGMAE is always larger than that of VideoMAE by more than 0.05. This loss gap implies motion guided masking further suppresses information leakage and indeed constructs a more challenging mask-reconstruct pretext task for masked video modeling. This more difficult task would like to encourage learning more effective representations.\nDetailed breakdown of comparison between MGMAE and VideoMAE. To understand the distinct impacts of the MGMAE and VideoMAE masking strategies on video model pre-training, we delved deeper into the per-class accuracy\nvariations between the two. Fig. 4 showcases the 29 categories with the most pronounced differences in classification accuracy between MGMAE and VideoMAE.\nMGMAE: a more effective video representation learner. MGMAE benefits greatly from the harder task constructed by our motion guided masking strategy. On the one hand, the model has to encode the relationship between visible and invisible tokens harder, which can better guide the model training. On the other hand, the suppression of information leakage may well reduce the overfitting risk of the pretraining, and thus the model can be pre-trained much longer. As shown in Tab. 3, MGMAE consistently maintains an obvious fine-tuning performance gap with VideoMAE on the motion-centric SSV2 dataset (1.4% at 800 epochs and 1.5% at 2400 epochs) and also has some improvement on the scene-centric Kinetics-400 dataset (1.2% at 800 epochs and 0.3% at 1600 epochs).\nVisualization. We randomly seletec a video clip in SSV2 validation set and show its reconstruction example in Fig. 5. We can see that the mask map changes with object movements, which makes it more difficult for the model to reconstruct the original video."
        },
        {
            "heading": "4.4. Comparison with the state-of-the-art methods",
            "text": "We compare our approach with the previous state-of-theart methods on the Kinetics-400 and Something-Something V2 datasets. The results are shown in Table 5 and Table 4.\nFor a fair comparison, we mainly list the results with similar computational cost. On the Something-Something V2 dataset, our MGMAE with ViT-B backbone achieves a performance of 72.3% when trained for 2400 epochs, which outperforms the original VideoMAE by 1.5%. On the Kinetics400 dataset, our MGMAE obtains slightly better performance than the original VideoMAE. The small performance improvement might be ascribed to the fact that Kinetics-400 is a scene-centric action recognition benchmark and motion information is less important compared with the SomethingSomething dataset."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we have proposed the Motion Guided Masked Autoencoders (MGMAE), which adapts the motion guided masking strategy to dynamically sample the\nunmasked visible tokens under the guidance of flows, thus suppressing information leakage to build a more challenging task for masked video pre-training. Experiments have shown that MGMAE has good performance and maintains a highperformance advantage over the previous methods under fair comparison. In addition, our strategy also reduces the risk of pre-training overfitting, which allows the model to benefit from longer pre-training.\nAcknowledgements. This work is supported by the National Key R&D Program of China (No. 2022ZD0160900, No.2022ZD0160100), the National Natural Science Foundation of China (No. 62076119, No. 61921006), Shanghai Committee of Science and Technology (Grant No. 21DZ1100100), and Collaborative Innovation Center of Novel Software Technology and Industrialization."
        }
    ],
    "title": "MGMAE: Motion Guided Masking for Video Masked Autoencoding",
    "year": 2023
}