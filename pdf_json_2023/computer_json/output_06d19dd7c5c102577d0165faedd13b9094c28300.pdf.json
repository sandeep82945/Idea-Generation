{
    "abstractText": "In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Henri De Plaen"
        },
        {
            "affiliations": [],
            "name": "Johan A. K. Suykens"
        }
    ],
    "id": "SP:7726b1cd9347abb466956f408f7d1eff3587c2c4",
    "references": [
        {
            "authors": [
                "C.M. Ala\u0131\u0301z",
                "M. Fanuel",
                "J.A.K. Suykens"
            ],
            "title": "Convex formulation for kernel PCA and its use in semisupervised learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2018
        },
        {
            "authors": [
                "D.J. Bartholomew",
                "M. Knott",
                "I. Moustaki"
            ],
            "title": "Latent variable models and factor analysis: A unified approach, volume 904",
            "year": 2011
        },
        {
            "authors": [
                "A.T. Basilevsky"
            ],
            "title": "Statistical factor analysis and related methods: theory and applications",
            "year": 2009
        },
        {
            "authors": [
                "S. Mika",
                "B. Sch\u00f6lkopf",
                "A. Smola",
                "M\u00fcller",
                "K.-R",
                "M. Scholz",
                "G. R\u00e4tsch"
            ],
            "title": "Kernel PCA and de-noising in feature spaces",
            "venue": "Advances in neural information processing systems,",
            "year": 1998
        },
        {
            "authors": [
                "A. Pandey",
                "J. Schreurs",
                "J.A.K. Suykens"
            ],
            "title": "Robust generative restricted kernel machines using weighted conjugate feature duality. In Machine Learning, Optimization, and Data Science: 6th International Conference, LOD 2020",
            "venue": "Revised Selected Papers,",
            "year": 2020
        },
        {
            "authors": [
                "A. Pandey",
                "J. Schreurs",
                "J.A.K. Suykens"
            ],
            "title": "Generative restricted kernel machines: A framework for multi-view generation and disentangled feature learning",
            "venue": "Neural Networks,",
            "year": 2021
        },
        {
            "authors": [
                "A. Pandey",
                "H. De Meulemeester",
                "H. De Plaen",
                "B. De Moor",
                "J.A.K. Suykens"
            ],
            "title": "Recurrent restricted kernel machines for time-series forecasting",
            "venue": "Proceedings of ESANN 2022,",
            "year": 2022
        },
        {
            "authors": [
                "A. Pandey",
                "M. Fanuel",
                "J. Schreurs",
                "J.A.K. Suykens"
            ],
            "title": "Disentangled representation learning and generation with manifold optimization",
            "venue": "Neural Computation,",
            "year": 2022
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "A.J. Smola"
            ],
            "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
            "year": 2001
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "A. Smola",
                "M\u00fcller",
                "K.-R"
            ],
            "title": "Nonlinear component analysis as a kernel eigenvalue problem",
            "venue": "Neural computation,",
            "year": 1998
        },
        {
            "authors": [
                "J. Schreurs",
                "J.A.K. Suykens"
            ],
            "title": "Generative kernel PCA",
            "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN),",
            "year": 2018
        },
        {
            "authors": [
                "J.A.K. Suykens"
            ],
            "title": "Deep Restricted Kernel Machines Using Conjugate Feature Duality",
            "venue": "Neural Computation,",
            "year": 2017
        },
        {
            "authors": [
                "J.A.K. Suykens",
                "T. Van Gestel",
                "J. Vandewalle",
                "B. De Moor"
            ],
            "title": "A support vector machine formulation to PCA analysis and its kernel version",
            "venue": "IEEE Transactions on Neural Networks,",
            "year": 2003
        },
        {
            "authors": [
                "M.E. Tipping",
                "C.M. Bishop"
            ],
            "title": "Probabilistic principal component analysis",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
            "year": 1999
        },
        {
            "authors": [
                "D. Winant",
                "J. Schreurs",
                "J.A.K. Suykens"
            ],
            "title": "Latent space exploration using generative kernel PCA. In Artificial Intelligence and Machine Learning: 31st Benelux AI Conference, BNAIC 2019, and 28th Belgian-Dutch Machine Learning",
            "venue": "BENELEARN",
            "year": 2019
        },
        {
            "authors": [
                "Z. Zhang",
                "G. Wang",
                "Yeung",
                "D.-Y",
                "J.T. Kwok"
            ],
            "title": "Probabilistic kernel principal component analysis",
            "venue": "Department of Compputer Science,",
            "year": 2004
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Classical datasets often consist of many features, making dimensionality reduction methods particularly appealing. Principal Component Analysis (PCA) is one of the most straightforward frameworks to that goal and it is hard to find a domain in machine learning or statistics where it has not proven to be useful. PCA considers new decorrelated features by computing the eigendecomposition of the covariance matrix.\nProbabilistic models on another side participate to the building of a stronger foundation for machine learning models. By considering models as probability distributions, we are able to natively access notions such as variance or sampling, i.e. generation. A probabilistic approach to PCA, known as Probabilistic Principal Component Analysis (Prob. PCA), has been formulated by (Tipping & Bishop, 1999). Its principles can be visualized in the primal part of Table 1.\nEven endowed with a probabilistic interpretation, PCA remains restricted to linear relations between the different features. Kernel Principal Component Analysis (KPCA) (Mika et al., 1998; Scho\u0308lkopf et al., 1998) was an attempt to give a non-linear extension to (non-probabilistic) PCA by decomposing a kernel matrix instead of the covariance matrix. An earlier attempt to give a probabilistic formulation of KPCA has been done by (Zhang et al., 2004). As developed further,\n1KU Leuven, Department of Electrical Engineering (ESAT), STADIUS Center for Dynamical Systems, Signal Processing and Data Analytics, Kasteelpark Arenberg 10, 3001 Leuven, Belgium. Correspondence to: Henri De Plaen <henri.deplaen@esat.kuleuven.be>.\nICML 2023 Workshop on Duality for Modern Machine Learning, Honolulu, Hawaii, USA. Copyright 2023 by the author(s).\nthe latter model does not consist in a kernel equivalent of the Prob. PCA, but rather in another model based on similar principles.\nMore recently, Restricted Kernel Machines (Suykens, 2017) opened a new door for a probabilistic version of PCA both in primal and dual. They essentially use the Fenchel-Young inequality on a variational formulation of KPCA (Suykens et al., 2003; Ala\u0131\u0301z et al., 2018) to obtain an energy function, closely resembling to Restricted Boltzmann Machines. The framework has been further extended to generation (Schreurs & Suykens, 2018; Winant et al., 2020), incorporating robustness (Pandey et al., 2020), multi-view models (Pandey et al., 2021), deep explicit feature maps (Pandey et al., 2022b) or times-series (Pandey et al., 2022a)."
        },
        {
            "heading": "1.1. Contributions",
            "text": "1. We characterize the Prob. PCA framework in Hilbert spaces and give a dual interpretation to the model.\n2. We develop a new extension of KPCA incorporating a noise assumption on the explicit feature map.\n3. We give a probabilistic interpretation of the generation in KPCA.\n4. We illustrate how the dual model works on a toy and a real dataset and show its connections to KPCA2."
        },
        {
            "heading": "2. Primal and Dual Spaces",
            "text": "The key idea behind the duality in PCA is that outer and inner products share the same eigenvalues. The consequence is that instead of decomposing the covariance matrix of any given feature map, we can decompose the associated Gram matrix, i.e. the kernel matrix. The former is considered as the primal formulation and the latter as the dual formulation and they are both equivalent. Extending Prob. PCA to a dual formulation is however not straightforward: if all feature maps have an associated kernel, the converse is trickier. Some kernels correspond to feature maps in infinite dimensional spaces, where probability distributions cannot be properly defined. We therefore need to choose well defined finite subspaces to work in and consider linear operators instead of matrices. All formal definitions, propositions and proofs are provided in Appendix A.\n2Resources: https://hdeplaen.github.io/kppca.\nar X\niv :2\n30 7.\n10 07\n8v 1\n[ cs\n.L G\n] 1\n9 Ju\nl 2 02\n3"
        },
        {
            "heading": "2.1. Primal Spaces",
            "text": "Feature Space H. Given an input space X , we first consider any feature map \u03c6 : X \u2192 H. Following (Ala\u0131\u0301z et al., 2018), we will consider a separable, possibly infinite dimensional, Hilbert space (H, \u27e8\u00b7, \u00b7\u27e9H). By \u03c6, we denote an element of H and its adjoint by \u03c6\u2217 = \u27e8\u03c6, \u00b7\u27e9 \u2208 H\u2217, with H\u2217 \u223c H its Fre\u0301chet-Riesz dual space. Essentially, it corresponds to the transpose \u03c6\u22a4 in real, finite dimensional spaces as \u03c6\u22a41 \u03c62 = \u27e8\u03c61,\u03c62\u27e9H, but generalizes it for the possibly infinite dimensional spaces that will be necessary for the introduction of kernels. Furthermore, we assume our space to be defined over the reals such that \u27e8\u00b7, \u00b7\u27e9H : H\u00d7H \u2192 R and its inner product is symmetric \u27e8\u03c61,\u03c62\u27e9H = \u27e8\u03c62,\u03c61\u27e9H. If H is of finite dimension d, we can therefore identify its canonical basis u1, . . . ,ud with the canonical basis of Rd.\nFinite Feature Space HE . Considering a set of N observations {xi \u2208 X}Ni=1, the idea is to work directly in H by considering instead the feature map of the datapoints \u03c6i = \u03c6 (xi). We can however not define a normal distribution onto the full H yet as it is possibly infinite dimensional. We therefore have to consider a finite subspace HE \u2282 H. A natural choice would be HE = span {\u03c61, . . . ,\u03c6N}. We now first have to find an orthonormal basis for HE ."
        },
        {
            "heading": "2.2. Dual Spaces",
            "text": "Kernels. For each feature map, there is an induced positive semi-definite kernel k : X \u00d7 X \u2192 R : k (x,y) = \u27e8\u03c6(x), \u03c6(y)\u27e9H = \u03c6(x)\u2217\u03c6(y). Inversely, to each positive semi-definite kernel corresponds a, possibly infinite dimensional, feature map, even if not explicitly defined. This follows from the theory of Reproducing Kernel Hilbert Spaces. We refer to (Scho\u0308lkopf & Smola, 2001) for further info.\nKernel Space E . We now consider a finite dimensional Hilbert space (E , \u27e8\u00b7, \u00b7\u27e9E) of dimension N , the number of observations. It is defined similarly as above, with orthonormal basis e1, . . . , eN . The basis also defines the identity over E as IE = \u2211N i=1 eie \u2217 i . The goal for E is to represent the space of the kernel representations. We therefore define the linear operator \u03a6 : E \u2192 H : \u2211 i=1 \u03c6ie \u2217 i and\nits adjoint \u03a6\u2217 : H \u2192 E : \u2211N\ni=1 ei\u03c6 \u2217 i . Essentially, \u03a6 \u2217\nreturns the kernel value with each datapoint: \u03a6\u2217\u03c6(x) =\u2211N i=1 ei (\u03c6 \u2217 i\u03c6(x)) = \u2211N i=1 eik (xi,x) for any x \u2208 X . Similarly, \u03a6 projects this value back as a linear combination of the different \u03c6i\u2019s, thus mapping back to HE \u2282 H. For this reason, the covariance \u03a6 \u25e6\u03a6\u2217 = \u2211N i=1 \u03c6i\u03c6 \u2217 i acts as a projector from H \u2192 HE . Its eigenvectors therefore form an orthonormal basis of the finite feature space HE ,\nwhich acts as the primal equivalent of the kernel space E .\nCentered Kernels. In most applications however, we prefer to work with the centered feature map, which we define as \u03c6c(\u00b7) = \u03c6(\u00b7)\u2212\u03c6c with \u03c6c = 1N \u2211N i=1 \u03c6i. We denote the associated kernel associated centered kernel kc : X \u00d7 X \u2192 R : kc(x1,x2) = \u03c6c(x1)\u2217\u03c6c(x2). This leads to the definition of a new centered operator \u03a6c = \u2211 i=1(\u03c6i\u2212\u03c6c)e\u2217i =\n\u03a6 ( IE \u2212 1N 1E\u00d7E ) , with 1E\u00d7E = \u2211N i,j=1 eie \u2217 j . As always, we also consider its adjoint \u03a6\u2217c . Considering the dual operator, we have \u03a6\u2217c \u25e6 \u03a6c = \u2211N i=1(\u03c6i \u2212 \u03c6c)\u2217(\u03c6i \u2212 \u03c6c)eie \u2217 j = \u2211N i=1 kc(xi,xj)eie \u2217 j . We notice now that HE = span{\u03c61, . . . ,\u03c6N} = span{\u03c61 \u2212 \u03c6c, . . . ,\u03c6N \u2212 \u03c6c} because \u03c6c is a linear combination of the elements of the basis. Therefore, the primal operator \u03a6c \u25e6 \u03a6\u2217c =\u2211N\ni=1(\u03c6i \u2212 \u03c6c)(\u03c6i \u2212 \u03c6c)\u2217 also acts as a projector from H \u2192 HE and we can choose its eigenvectors instead as an orthonormal basis of HE .\nCovariance and Kernels. We now consider the key idea behind the duality in PCA: the operators \u03a6c \u25e6 \u03a6\u2217c and \u03a6\u2217c \u25e6\u03a6c are self-adjoint, positive semi-definite and share the same non-zero eigenvalues. We have \u03a6c \u25e6 \u03a6\u2217c =\u2211N\ni=1 \u03bbiviv \u2217 i and HE = span{v1, . . . ,vN}. Similarly, we have \u03a6\u2217c \u25e6\u03a6c = \u2211N i=1 \u03bbi\u03f5i\u03f5 \u2217 i and E = span{\u03f51, . . . , \u03f5N}. The identity over the (primal) finite feature space HE can now be defined as IHE = \u2211N i=1 viv \u2217 i and the identity over\nthe (dual) kernel space E as IE = \u2211N i=1 \u03f5i\u03f5 \u2217 i . This is synthetized in the two first columns of Table 2. The identity over H reads IH = IHE +PH\u22a5E , with PH\u22a5E a projector over the null space of \u03a6c \u25e6 \u03a6\u2217c . It most be noted that it may happen that these basis may contain too much basis vectors if the two operators \u03a6\u2217c \u25e6 \u03a6c and \u03a6c \u25e6 \u03a6\u2217c are not of full rank. In particular, this is the case when dim(H) = d is finite and d < N . In this particular case, we would also have dim(HE) = dim(E) = d. Without loss of generality, we will assume that this is not the case. Similarly, we will neglect the case N > d as we could just neglect the null space of \u03a6\u2217c \u25e6\u03a6c.\nNotations. We can now define our probabilistic model over HE . We will therefore use the notation \u03d5 instead of \u03c6 to consider the feature map in our finite dimensional subspace HE . More formally, we have \u03d5 : X \u2192 HE : IHE \u25e6 \u03c6 and following from that \u03d5c : X \u2192 HE : IHE \u25e6 \u03c6c. In particular, we have the observations \u03d5i = \u03d5(xi) = \u03c6i and \u03d5c = \u03c6c, as those are linear combinations of the basis. For the sake of readability, we will write \u03d5 = \u03d5(x), the image of a random variable x \u2208 X and refer to it as a feature observation or representation. Given any Hilbert space, a an element of it and a linear operator \u03a3 from and to that space, we consider the multivariate normal distribution a \u223c N ( b,\u03a3 ) as the\ndistribution with density 1Z exp ( \u2212 12 (a\u2212 b) \u2217\u03a3\u22121(a\u2212 b) ) . It is well defined if Z is non-zero and finite."
        },
        {
            "heading": "3. Primal Model",
            "text": "We will now essentially follow the work of (Tipping & Bishop, 1999) and redefine the model distributions. This section corresponds to the primal formulation and we only consider the feature representations. It does not yet introduce the kernel representations, which will appear in the dual formulation (Section 4)."
        },
        {
            "heading": "3.1. Model and Latent Space",
            "text": "Factor Analysis. The starting point is to consider a factor analysis relationship (Bartholomew et al., 2011; Basilevsky, 2009) between the feature observations \u03d5 and the latent variables h. In particular, we consider\n\u03d5 = Wh+ \u00b5+ \u03b6. (1)\nThe observations \u03d5 live in the primal space HE of dimension N . We consider an isotropic normal noise \u03b6 \u223c N ( 0, \u03c32IHE ) of variance \u03c32 \u2208 R>0 and a mean \u00b5 \u2208 HE .\nLatent Space L. The latent variables h on the other hand live in a latent dual space L \u2282 E of dimension q \u2264 N . They are related by a primal interconnection linear operator W . As it was the case before with \u03a6, the interconnection operator does not project to the full space HE because of its reduced dimensionality. It therefore projects to yet another feature space HL \u2282 HE , which acts as the primal equivalent of the latent space L. The equality of these two spaces only holds if q = N . We will therefore consider the mappings W \u2217 : HE \u2192 L and W : L \u2192 HL. The identity over L can be written as IL = \u2211q p=1 rpr\n\u2217 p , over HL as IHL =\u2211q\np=1 \u03f1p\u03f1 \u2217 p and finally the identity over HE rewritten as IHE = IHL + PH\u22a5L , with PH\u22a5L as a projector over the null space of W \u2217 \u25e6W . This is summarized in the last column of Table 2."
        },
        {
            "heading": "3.2. Feature Distributions",
            "text": "Latent-Based Generation. The relation between the feature observations and the latent variables being set up\n(Eq. (1)), we can derive the conditional probability of the feature observations given a latent variable:\n\u03d5|h \u223c N ( Wh\u2212 \u00b5, \u03c32IHE ) . (2)\nAs discussed earlier, we see that the latent variables do not participate to the full scope of the observations in HE , but only to their component in HL. The rest is only constituted from the isotropic normal noisy mean. This distribution can be interpreted as a generative one: given a latent variable, we can sample a variable in feature space. Absolute Generation. Considering the latent prior h \u223c N ( 0, IL ) , we can derive the marginal distribution of the observations in feature space:\n\u03d5 \u223c N ( \u00b5,W \u25e6W \u2217 + \u03c32IHE ) . (3)\nIt can be considered as the data distribution of the model. Sampling from it also means generating feature representations in a more absolute way, i.e., without considering any latent variable, or more precisely considering a random latent variable according to its prior. As a consequence of Eq. (2) and the isotropic aspect of the latent prior, we see that the observations are only non-isotropically distributed in HL. Again, the rest is only the isotropically normally noisy mean. In other words, this means that the model parameter W only influences \u03d5 for its components in HL."
        },
        {
            "heading": "3.3. Training the Model",
            "text": "Maximum Likelihood. As we now have the marginal distribution of the model (Eq. (3)), the goal is to find the optimal hyperparameters W and \u00b5 to match the set of observations {\u03d5i}Ni=1. One way to determine them is by maximizing the likelihood of our observations. The Maximum Likelihood (ML) estimator for the hyperparameters is given by:\n\u00b5ML = \u03d5c, (4)\nWML = q\u2211 p=1 \u221a \u03bbp/N \u2212 \u03c32vpr\u2217p, (5)\nwith {(\u03bbp,vp)}qp=1 the q dominant eigenpairs of \u03a6c \u25e6\u03a6 \u2217 c (\u03bb1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03bbq \u2265 \u00b7 \u00b7 \u00b7\u03bbN ), and {rp}qp=1 and arbitrary orthonormal basis of the latent space L. The choice for the latter basis is arbitrary and makes the model rotational invariant in latent space. An additional condition is that \u03c32 \u2264 \u03bbq/N . It is not surprising to see that the optimal mean \u00b5ML corresponds to the mean of the observations \u03d5c. We observe that WML corresponds to the eigendecomposition of the centered covariance, at the exception that the noise assumption is substracted from its spectrum. By looking back at Eq. (1), it makes sense to avoid the noise in WML as it is still going to be added by the term \u03b6.\nNoise Variance. Maximizing the likelihood as a function of \u03c32 leads to\n\u03c32ML = 1\nN(N \u2212 q) N\u2211 p=q+1 \u03bbp. (6)\nThe eigenvalue \u03bbp corresponds to the variance for each component vp of the covariance \u03a6c \u25e6 \u03a6\u2217c . The total variance of the data, noise included, is equal to 1N \u2211N p=1 \u03bbp and the variance learned by the model through the primal interconnection operator to 1N \u2211q p=1 \u03bbp. Hence, the maximum likelihood estimator for the noise variance \u03c32ML can be interpreted as the mean of the variance that is discarded by the model. It also verifies the earlier condition that \u03c32 \u2264 \u03bbq/N , as the eigenvalues are taken in descending order. It can be interpreted as the normalized mean variance of the left over eigendirections, i.e. the orthogonal space of the latent space: L\u22a5 = E\\L. By consequence, we may decide to choose the latent dimension q = dim(L) and deduct \u03c32ML. In the opposite, we may also decide to set an arbitrary \u03c32 and deduct the latent dimension q instead. We therefore can consider either \u03c32 or q as an additional hyperparameter. We must however keep in mind that this is strongly going to be influenced by the distribution of the eigenvalues and that the latent dimension q for the same \u03c32 may heavily vary from application to application.\nUncentered Features. We may also consider not to consider the mean as an optimizable hyperparameter and set it arbitrarily to \u00b5 = 0. In this case, Eq. (5) would be the same at the difference that the WML would be constructed from the dominant eigenpairs of the uncentered covariance \u03a6 \u25e6\u03a6\u2217 instead of its centered counterpart \u03a6c \u25e6\u03a6\u2217c ."
        },
        {
            "heading": "3.4. Dimensionality Reduction in Feature Space",
            "text": "Latent Projection. Up to now, we only considered the distribution of the feature variables \u03d5. We can also calculate the posterior distribution of the latent variable h given the primal feature variable \u03d5:\nh|\u03d5 \u223c N ( \u03a3\u22121h|\u03d5 \u25e6W \u2217(\u03d5\u2212 \u00b5), \u03c32\u03a3\u22121h|\u03d5 ) , (7)\nwith \u03a3h|\u03d5 = ( W \u2217 \u25e6W + \u03c32IL )\u22121 . The mean of the distribution can be considered as a pseudo-inverse of the observation \u03d5, but regularized by \u03c32. This regularization ensures to avoid the noise. If the prior of the latent variables was isotropic, this is not the case anymore for the posterior. If we consider the maximum likelihood estimator for the primal interconnection operator WML, the variance becomes \u03c32\u03a3\u22121h|\u03d5 = N\u03c3 2 \u2211q p=1 \u03bb \u22121 p rpr \u2217 p . It can be interpreted as the uncertainty for each component of the latent variable h (w.r.t. the eigendirection rp), due to the noise assumption. By consequence, the greater the explained variance \u03bbp for the eigendirection vp of the covariance \u03a6c \u25e6\u03a6\u2217c , the smaller\nthe corresponding uncertainty on the component rp of the latent vairable h. For each observation in feature space \u03d5, this returns a distribution for the latent variable \u03d5 and can therefore be considered as a sort of probabilistic projection in latent space L.\nMaximum A Posteriori. Up to now, we were only considering distributions. The only way to go from a feature representation to a latent variable or the opposite was probabilistic. In order to have a deterministic approach, we need proper mappings. One way is to consider the Maximum A Posteriori (MAP) of h given \u03d5. It maps the feature observation \u03d5 \u2208 HE to latent variable hMAP \u2208 L, hence reducing the dimensionality of any input to that of the latent space. To allow it to work for any input \u03c6 \u2208 H, we may again consider the projection \u03d5 = IHE\u03c6. As W \u2217 ML \u25e6 IHE = W \u2217ML:\nhMAP = ( W \u2217ML \u25e6WML + \u03c32IL )\u22121 \u25e6W \u2217ML (\u03c6\u2212\u03c6c) .\n(8)\nTo map back to the feature space HL, we may consider the maximum a posteriori of \u03d5 given h (Eq. (3)). This gives\n\u03d5MAP = WMAPh+ \u03d5c. (9)\nThe final projection reads \u03d5MAP =WML \u25e6 ( W \u2217ML \u25e6WML + \u03c32IL )\u22121 \u25e6W \u2217ML (\u03c6\u2212\u03c6c) + \u03d5c.\n(10)\nNo Noise. We may also decide not to consider \u03c32 as a parameter to optimize and set it to an arbitrary value. The latent dimensions q could also be set an arbitrary value, without it to be related to the latent dimension q according to Eq. (6). We notice that in the limit of \u03c32 \u2192 0, we recover the classical Principal Component Analysis reconstruction scheme. Indeed the conditional probability distributions become exact relations. We also notice that the condition \u03c32 \u2264 \u03bbq/N (Prop. 3) is then always satisfied. Furthermore, when q = dim(HE), the reconstruction is perfect in HE and in particular for our original observations {\u03c6i}Ni=1 and \u03c6c (as we have \u03d5i = \u03c6i). Indeed, we would have\nhMAP = W + ML (\u03c6\u2212\u03c6c) , (11)\nwith W+ML the Moore-Penrose pseudo-inverse of WML. . We note here the symmetry with Eq. (9). If the maximum likelihood estimator for \u03c32 is to be respected (Eq. (6)), this would mean that all components are kept (L = E) and the model reconstructs the full feature variance. In this case, the primal interconnection operator would become WML =\u2211N\np=1\n\u221a \u03bbp/Nvpr \u2217 p and be invertible. Its Moore-Penrose\npseudo-inverse would become an exact inverse. Eqs. (9) and (11) would become exact opposites and there would be no loss due to the dimensionality reduction as there would be no noise to discard. By consequence, the reduction would become an identity over HE : \u03d5MAP\u2212\u03d5c = IHL (\u03c6\u2212\u03c6c)."
        },
        {
            "heading": "4. Dual Model",
            "text": "Kernels without Dual. In (Zhang et al., 2004), the authors made the kernel matrix appear by considering the new observations {\u2211d i=1 uiu \u2217 j\u03d5(xi) }N j=1\n. In other words, each new datapoint consists in one particular feature of the feature map, for each original datapoint. If the original datapoints were organized as a matrix in RN\u00d7d, this would correspond to taking its transpose as new datapoints. The outer product of the covariance matrix is transformed to the inner product of the kernel matrix. If indeed this formulation makes the kernel appear, it is not a dual formulation of the original problem, but another problem. In this section, we show how the spaces defined hereabove help us build an equivalent dual formulation of the problem.\nDual Formulation. While keeping an equivalence with the primal model, we will now see that we can directly work in dual spaces E and L without considering the feature spaces at all, i.e. resorting to the primal space H and its subsets. As we did for the primal feature variable \u03d5, we will consider kc = \u03a6 \u2217 c(\u03d5 \u2212 \u03d5c) = \u2211N i=1 kc(x,xi)ei to represent the image in E , of a random variable x \u2208 X . We will refer to it as a dual feature variable."
        },
        {
            "heading": "4.1. Representation",
            "text": "Considering the dual spaces, we can always express the interconnection operator W in the (non-orthonormal) basis {\u03d51 \u2212 \u03d5c, . . . ,\u03d5N \u2212 \u03d5c}. As a consequence, we can always write\nW = \u03a6c \u25e6A, (12)\nwith A : L \u2192 L, the dual interconnection operator. Given the maximum likelihood estimator for the primal interconnection operator WML, we can directly deduce the dual one:\nAML = q\u2211 p=1 \u221a 1/N \u2212 \u03c32\u03bb\u22121p \u03f5pr\u2217p, (13)\nwith {(\u03bbp, \u03f5p)}qp=1 the q dominant eigenpairs of \u03a6 \u2217 c \u25e6\u03a6c and {rp}qp=1 an arbitrary orthonormal basis of the latent space L. The rotational invariance of the dual interconnection operator AML is inherited from its primal counterpart WML. Again, if we consider an optimized mean \u00b5 = 0, we would have the relation WML = \u03a6 \u25e6AML with AML then based on the eigenpairs of the non-centered \u03a6\u2217 \u25e6 \u03a6 instead. Using the same structure for AML, the optimal (primal) interconnection operator WML could be expressed in the (non-ortonormal) basis {\u03d51, . . . ,\u03d5N}."
        },
        {
            "heading": "4.2. Kernel Distributions",
            "text": "Projection and Generation. We can also consider the dual counterparts of the distributions of the primal model (Eqs. (2) and (7)). For the sake of simplicity and to avoid\nheavier equations with non-centered kernels, we will only consider here the equations of the trained model, in particular with \u00b5ML = \u03d5c leading to centered kernels:\nkc|h \u223c N ( (\u03a6\u2217c \u25e6\u03a6c) \u25e6AMLh, \u03c32\u03a6\u2217c \u25e6\u03a6c ) , (14)\nh|kc \u223c N ( \u03a3\u22121h|kc \u25e6AMLkc,\u03a3 \u22121 h|kc ) , (15)\nwith \u03a3h|kc = ( A\u2217ML \u25e6 ( \u03a6\u2217c \u25e6\u03a6c ) \u25e6AML + \u03c32IL )\u22121 ."
        },
        {
            "heading": "4.3. Dimensionality Reduction in Kernel Space",
            "text": "Maximum A Posteriori. This now allows us to consider the dimensionality reduction in kernel space in a similar fashion as in Section 3.4. Again we consider the MAP of the latent variable h given the kernel representation kc:\nhMAP = ( A\u2217ML \u25e6 ( \u03a6\u2217c \u25e6\u03a6c ) \u25e6AML + \u03c32IL )\u22121 \u25e6AMLkc, (16)\nand similarly with the MAP of the kernel representation kc given the latent variable h:\n(kc)MAP = (\u03a6 \u2217 c \u25e6\u03a6c) \u25e6AMLh. (17)\nAs for the primal model, the dimensionality reduction in dual is computed as (kc)MAP = (\u03a6 \u2217 c \u25e6\u03a6c) \u25e6AMLhMAP.\nNo Noise. Again, considering \u03c32 \u2192 0 makes both dual conditional distributions become exact relations. In a ML context for \u03c32 (Eq. (6)), this would imply that q = dim(E) and we would recover an identity (kc)MAP = kc, i.e. no reduction. Without considering a ML context for \u03c32 \u2192 0 and choosing an arbitrary q \u2264 dim(E), the reduction become exactly the reconstruction done in KPCA."
        },
        {
            "heading": "4.4. Kernel Sampling",
            "text": "Probabilistic Sampling. The dual counterpart of Eq. (3) after training is given by\nkc \u223c N ( 0,A\u2217ML \u25e6AML + \u03c32 (\u03a6\u2217c \u25e6\u03a6c) \u22121 ) . (18)\nThe covariance A\u2217ML \u25e6 AML + \u03c32 (\u03a6\u2217c \u25e6\u03a6c) \u22121 can be decomposed as B \u25e6 B\u2217, with B : E \u2192 E : N\u22121/2 \u2211q p=1 \u03bbp\u03f5p\u03b5 \u2217 p + \u2211N p=q+1 \u03c3\u03bb 1/2 p \u03f5p\u03b5 \u2217 p and {\u03b5i} N i=1 any arbitrary orthonormal basis of the latent space E . This decomposition allows us to sample kc on the trained model with kc = B\u03be with \u03be \u223c N (0, IE). We see that B is rotational invariant, which is not surprising as this is also the case for the distribution from which \u03be is sampled. In practice and for simplicity, we may decide too choose the canonical basis for {\u03b5i}Ni=1 as any choice would be identified to the same covariance and to the same sampling of kc. We will therefore assume that \u03b5i = ei for all i = 1, . . . , N . In that particular case, B is self-adjoint\nand by consequence corresponds to the matrix square root of A\u2217ML \u25e6AML + \u03c32 (\u03a6\u2217c \u25e6\u03a6c) \u22121.\nKPCA Sampling The classical sampling done by KPCA (Schreurs & Suykens, 2018) corresponds to the limit of \u03c32 \u2192 0 for an arbitrary latent dimension q. Unless the latent dimension is chosen as q = dim(E), the sampling in that case can never cover E fully, but rather L, as B is not a bijection. The second term of B ( \u2211N p=q+1 \u03c3\u03bb 1/2 p \u03f5p\u03b5 \u2217 p) allows B to be a bijection no matter what is the choice of the latent dimension q, as long as \u03c32 > 0. We thus always sample in the full E . This can be observed at Fig. 2."
        },
        {
            "heading": "5. Experiments",
            "text": "Hilbert Spaces to Matrices. Working in Hilbert spaces is helpful to treat possibly infinite dimensional feature maps, but not very useful for practical applications. Matrix representations are possible in primal if d is finite and in dual if N is finite. It suffices to consider the different canonical basis. For the latent space L, this enforces a unique representation for WML and AML, but we must keep in mind that they are rotational invariant. All the operators and elements described before are then represented in matrix or vector format (Table 3). We will use the tilde to denote these matrices and use software-like notation by denoting with (\u00b7)i1:i2,j1:j2 the matrix truncated to its i1 to i2 rows and j1 to j2 columns.\nPreimage. Given a dual representation, we will also consider the kernel smoother preimage method, as suggested by (Schreurs & Suykens, 2018):\nx\u0302 = \u2211N i=1(k\u0303)ixi\u2211N i=1(k\u0303)i . (19)\nIn practice, as we work with centered feature maps and kernels, it may be that the kernel smoother may be unstable due to its normalization term. We therefore may consider to add a stabilization term.\n-2 0 2 4\n-1\n0\n1\n2\n3 4 Generated Dataset KPCA Prob. PCA\n(a) With q = 1 component, the explained variance is 31.23% and \u03c32 = 1.40%.\n-2 0 2 4 -3\n-2\n-1\n0\n1\n2\n3 Generated Dataset KPCA Prob. PCA\n(b) With q = 3 components, the explained variance is 54.03% and \u03c32 = 0.98%.\n-2 0 2 4 6\n-2\n-1\n0\n1\n2\n3 Generated Dataset KPCA Prob. PCA\n(c) With q = 10 components, the explained variance is 91.93% and \u03c32 = 0.20%.\nFigure 3. Visualisation of the Probabilistic PCA reconstruction (in blue) the classical KPCA (in red). Samples generated by are also given (in grey). The dataset contains N = 20 points (in black).\n(a) Sample of original datapoints.\n(b) Datapoints of Fig. 4a after dimensionality reduction.\n(c) Generated datapoints. The sample u\u0303 is uniform on [\u22121, 1] for the two first components and zero for the others. The horizontal axis varies in the first component and the vertical one in the second component.\nFigure 4. The Probabilistic PCA dual formulation on the MNIST dataset restricted to 0\u2019s and 1\u2019s, with N = 500 datapoints, with q = 2 components. The explained variance is 27.97% and \u03c32 = 0.14%."
        },
        {
            "heading": "5.1. Model",
            "text": "The direct application of the theoretical discussions of the previous sections leads to the decompositions K\u0303c = E\u0303\u039b\u0303E\u0303\u22a4, C\u0303c = V\u0303 \u039b\u0303V\u0303 \u22a4, \u03a6\u0303c = V\u0303 \u039b\u03031/2E\u0303\u22a4. The value of the operators after training are given in Table 4. Once the model is trained, we can verify that W\u0303 = \u03a6\u0303cA\u0303.We can also have a look at the hidden variables. A way to do it is to consider the MAP of h given \u03d5 or k. We have\nhMAP = N\u039b\u0303 \u22121 1:q,1:qA\u0303 \u22a4k\u0303c (if rank(K\u0303c) \u2265 q), (20) = N\u039b\u0303\u221211:q,1:qW\u0303 \u22a4(\u03d5\u0303\u2212 \u03d5\u0303c) (if H is finite), (21)\nand( kc ) MAP = K\u0303cA\u0303h\u0303 (if rank(K\u0303c) \u2265 q), (22)\n\u03d5MAP = W\u0303 h\u0303+ \u03d5\u0303c (if H is finite). (23)\nAs developed in Section 4, we can easily generate samples in both feature and kernel representations. For the latter and in canonical basis, it becomes\nk\u0303c = B\u0303u\u0303, with u\u0303 \u223c N (0, IN ). (24)"
        },
        {
            "heading": "5.2. Examples",
            "text": "As the primal case is already treated by (Tipping & Bishop, 1999), we consider here the model in its dual formulation. A toy example can by found in Fig. 3. We use an RBF kernel k(x,y) = exp ( \u2212\u2225x\u2212 y\u222522/(2\u03b32) ) with bandwidth \u03b3 = 2. As the number of components increases, the mean variance of the N \u2212 q unused components \u03c32 becomes smaller and the model tends to the classical KPCA model. Another way the reduce \u03c32 is to increase the number of components q, with \u03c32 \u2192 0 when q \u2192 N . This can be observed in Fig. 3c: the Probabilistic PCA model resembles closely the KPCA model, whereas more variance is left over, i.e. not projected back, in Fig.s 3a and 3b. The results of the generation is Gaussian, which is a consequence of the linearity of the preimage method chosen (Eq. (19)). Here again, as the number of components increases and \u03c32 decreases, the model is allowed to project back more variance and the distribution becomes wider. Another example on the MNIST dataset (LeCun & Cortes, 2010) with the RBF kernel with \u03b3 = 4 is given at Fig. 4."
        },
        {
            "heading": "6. Conclusion",
            "text": "Probabilistic Interpretation. By reformulating the Prob. PCA model in Hilbert space, we were able to define a formulation of it. Likewise Prob. PCA in primal was englobing classical PCA (with \u03c32 \u2192 0), Prob. PCA in dual is also englobing KPCA in the same limit. Furthermore, we are now able to sample in dual space, enhancing the understanding of the generation done with KPCA.\nLimitations. As most kernel methods, the model is still limited by the need of a preimage method to go back to the input space once a sample is projected or generated. Furthermore, training the model in dual required to find the q first eigenvalues of the kernel matrix, which may become expensive as the number of datapoints N increases. Generating renders the problem even worse as it requires the computation of all eigenvalues. The model also requires to determine a \u03c32 or alternatively a latent dimension q."
        },
        {
            "heading": "Acknowledgements",
            "text": "EU: The research leading to these results has received funding from the European Research Council under the European Union\u2019s Horizon 2020 research and innovation program / ERC Advanced Grant E-DUALITY (787960). This paper reflects only the authors\u2019 views and the Union is not liable for any use that may be made of the contained information. Research Council KUL: Tensor Tools for Taming the Curse iBOF/23/064, Optimization frameworks for deep kernel machines C14/18/068. Flemish Government: FWO projects: GOA4917N (Deep Restricted Kernel Machines: Methods and Foundations), PhD/Postdoc grant. This research received funding from the Flemish Government (AI Research Program). Henri De Plaen and Johan A. K. Suykens are also affiliated to Leuven.AI \u2013 KU Leuven institute for AI, B-3000, Leuven, Belgium."
        },
        {
            "heading": "A. Theoretical Development",
            "text": "For brevity of notations, we will define the expectation value as \u2225a\u22252\u03a3 = a\u2217\u03a3a \u2208 R\u22650, with a an element of a Hilbert space and \u03a3 a linear operator from and to that space. The norm is a particular case with the identity as operator \u03a3 = I . The density function of the multivariate normal distribution a \u223c N (b,\u03a3) can be rewritten as 1Z exp ( \u2212 12\u2225a\u2212 b\u2225 2 \u03a3\u22121 ) .\nA.1. Primal and Dual Spaces\nLemma 1. The operators \u03a6c \u25e6 \u03a6\u2217c and \u03a6\u2217c \u25e6 \u03a6c are self-adjoint, positive semi-definite and share the same non-zero eigenvalues. In particular, we have the eigenvector relations vi = (\u03bbi)\u22121/2\u03a6c\u03f5i and \u03f5i = (\u03bbi)\u22121/2\u03a6\u2217cvi.\nProof. The result stated here is inspired from (Ala\u0131\u0301z et al., 2018). Self-adjointness is a consequence of the definition of the inner product, which also guarantees the positive semi-definiteness. (=\u21d2) Let us suppose that (\u03a6\u2217c \u25e6\u03a6c)ei = \u03bbi\u03f5i with \u03bbi \u0338= 0. We then have \u03a6c \u25e6 (\u03a6\u2217c \u25e6\u03a6c)\u03f5i = (\u03a6c \u25e6\u03a6\u2217c) \u25e6\u03a6c\u03f5i = \u03bbi\u03a6c\u03f5i. Hence, we have that \u03a6c\u03f5i is eigenvector, but not necessarily normalized. In fact, its norm is given by \u03f5\u2217i\u03a6 \u2217 c \u25e6\u03a6c\u03f5i = \u03bbi. We thus have the relation ui = (\u03bbi)\u22121/2\u03a6\u03f5i. (\u21d0=) We suppose now that (\u03a6c \u25e6\u03a6\u2217c)ui, which leads to \u03a6\u2217c \u25e6 (\u03a6c \u25e6\u03a6\u2217c)ui = (\u03a6 \u25e6\u03a6\u2217) \u25e6\u03a6\u2217cui = \u03bbiu\u2217. Again, we have the relation \u03f5i = (\u03bbi)\u22121/2\u03a6\u2217cui.\nA.2. Primal Model\nA.2.1. FEATURE DISTRIBUTION\nDefinition 1 (Conditional Feature Distribution). Considering \u03d5,\u00b5 \u2208 HE , h \u2208 L, a linear operator W : L \u2192 HL and its adjoint W \u2217 : HE \u2192 L. We define the conditional probability distribution of the primal feature variable \u03d5 with variance \u03c32 \u2208 R>0 as\np (\u03d5 |h ) = 1( \u03c3 \u221a 2\u03c0 )N exp( \u221212\u03c32 \u2225\u03d5\u2212Wh\u2212 \u00b5\u22252IHE ) . (25)\nThe following proposition verifies that the conditional distribution of \u03d5 given h (Def. 1) is well defined. To ease the readability, we first consider a Lemma.\nLemma 2. Given values a \u2208 R>0 and b, c \u2208 R, we have the following integral\u222b R exp ( \u22121 2 ax2 + bx+ c ) dx = \u221a 2\u03c0 a exp ( b2 2a + c ) . (26)\nProof. We first find the primitive \u222b exp ( \u2212 12ax 2 + bx + c ) dx = \u221a \u03c0 2a exp ( b2 2a + c ) erf ( ax\u2212b\u221a\n2a\n) , with the error function\ndefined as erf(x) = 2\u221a \u03c0 \u222b x 0 e\u2212t 2 dt. Indeed, we have ddxerf(x) = 2\u221a \u03c0 e\u2212x 2\n. It suffices then to derivate the primitive to verify them. To conclude the proof, it suffices to notice that the error function is symmetric and that limx\u2192+\u221e erf(x) = 1.\nFrom now on, we will consider the singular value decomposition of the primal interconnection linear operator W =\u2211q p=1 sp\u03f1pr \u2217 p , with \u03f1p \u2208 HE and r\u2217p \u2208 L\u2217 two sets of orthonormal variables and sp \u2208 R>0 the singular values.\nProposition 1. Def. 1 is a well-defined distribution. More specifically, its measure is normalized:\u222b L p (\u03d5 |h ) d\u03d5 = 1. (27)\nProof. By considering the singular value decomposition of W , the term inside the exponential becomes\n\u2212 1 2\u03c32 N\u2211 i=1 { (v\u2217i \u03d5) 2 \u2212 (v\u2217i \u03d5) ( 2 q\u2211 p=1 ( ( r\u2217ph ) ( \u03f1\u2217pvi ) sp ) + (v\u2217i \u00b5) )} + C, (28)\nwith C = \u2212 12\u03c32 (Wh+ \u00b5) \u2217 (Wh+ \u00b5) = \u2212 12\u03c32 \u2211N i=1 \u2211q p=1 ( (r\u2217ph)(\u03f1 \u2217 pvi)sp + (v \u2217 i \u00b5 )2\n. Integrating over HE means integrating over span {vi, . . . ,vN}, thus for all (v\u2217i \u03d5) \u2208 R. Using Lemma 2 and by Fubini\u2019s theorem, we have\u222b\nF p (\u03d5 |h,\u03f1 ) d\u03d5 =\n( \u03c3 \u221a 2\u03c0 )\u2212N exp(\u2212C) \u222b RN exp ( N\u2211 i=1 ( \u2212 1 2\u03c32 x2i + bixi )) dx, (29)\n= ( \u03c3 \u221a 2\u03c0 )\u2212N exp(\u2212C) N\u220f i=1 \u222b R exp ( \u2212 1 2\u03c32 x2i + bixi ) dxi, (30)\n= ( \u03c3 \u221a 2\u03c0 )\u2212N exp(\u2212C) N\u220f i=1 \u03c3 \u221a 2\u03c0 exp ( 1 2 \u03c32b2i ) , (31)\n= exp\n( 1\n2 \u03c32 N\u2211 i=1 b2i \u2212 C\n) , (32)\nwith bi = 1\u03c32 ( (r\u2217ph)(\u03f1 \u2217 pvi)sp + (v \u2217 i \u00b5 ) . The proof is concluded by observing that C = 12\u03c3 2 \u2211N i=1 b 2 i .\nWe can now consider the marginal distribution of the feature representation \u03d5.\nLemma 3. Both linear operators W \u25e6W \u2217 + \u03c32IHE and W \u2217 \u25e6W + \u03c32IL are positive definite, of full rank and invertible. The linear operator W \u25e6W \u2217 + \u03c32IHE shares the q non-zero eigenvalues of W \u2217 \u25e6W + \u03c32IL, with the remaining N \u2212 q eigenvalues being equal to \u03c32. In particular, we have\neig ( W \u2217 \u25e6W + \u03c32IL ) = { s2p + \u03c3 2 }q p=1 , (33)\neig ( W \u25e6W \u2217 + \u03c32IHE ) = { s2p + \u03c3 2 }q p=1 \u222a { \u03c32 }N\u2212q . (34)\nProof. We consider the eigenvalues of W \u2217 \u25e6W + IL = \u2211q p=1(s 2 p + \u03c3 2)\u03f1p\u03f1 \u2217 p. Its eigenvalues are thus given by s 2 p + \u03c3 2 for p = 1, . . . , q and we directly conclude that they are all strictly positive. In a similar fashion, the eigenvalues of W \u25e6W \u2217 + \u03c32IHE = \u2211q p=1(s 2 p + \u03c3 2)rpr \u2217 p + \u03c3\n2PHE are given by s2p + \u03c32 for p = 1, . . . , q and \u03c32 for k = q + 1, . . . , N . All the eigenvalues are strictly positive. Hence are the operators positive definite, full rank and invertible.\nProposition 2 (Marginal Feature Distribution). Assuming the conditional distribution of \u03d5 given h (Def. 1), assumed normally distributed, i.e., p(h) = (2\u03c0)\u2212q/2 exp ( \u2212 12h \u2217h ) , the posterior distribution of the primal feature vector \u03d5 is given by\np(\u03d5) = 1\nZ\u03d5 exp ( \u22121 2 \u2225\u03d5\u2212 \u00b5\u22252\u03a3\u22121 \u03d5|W ) , (35)\nwith Z\u03d5 = (2\u03c0)N/2 ( (\u03c32)N\u2212q \u220fq p=1(s 2 p + \u03c3 2) )1/2 and \u03a3\u03d5|W = W \u25e6W \u2217 + \u03c32IHE\nProof. The joint probability distribution is given by Bayes\u2019 theorem as p(\u03d5,h) = p(\u03d5|h)p(h). The integration proceeds in a very similar fashion as the proof of Prop. 1. Similarly, the term inside the exponential becomes\n\u2212 1 2\u03c32 q\u2211 p=1 {( r\u2217ph )2 ( s2p + \u03c3 2 ) \u2212 ( r\u2217ph ) ( 2sp ( \u03f1\u2217p(\u03d5\u2212 \u00b5) ))} +D, (36)\nwith\nD = \u2212 1 2\u03c32 (\u03d5\u2212 \u00b5)\u2217 (\u03d5\u2212 \u00b5) , (37)\n= \u2212 1 2\u03c32 q\u2211 p=1 (\u03f1\u2217p(\u03d5\u2212 \u00b5))2 \u2212 1 2\u03c32 (\u03d5\u2212 \u00b5)\u2217 PHE (\u03d5\u2212 \u00b5) . (38)\nHere again, integrating on the whole latent space L means integrating for all ( r\u2217ph ) \u2208 R. Again, we use Lemma 2 and Fubini\u2019s theorem. After some simplification inside the exponential, we can use the development made in Lemma 3 to find the following:\n\u22121 2\n{ q\u2211\np=1\n(\u03f1\u2217p(\u03d5\u2212 \u00b5))2\ns2p + \u03c3 2\n+ 1 \u03c32 (\u03d5\u2212 \u00b5)\u2217 PHE (\u03d5\u2212 \u00b5)\n} = \u22121\n2 (\u03d5\u2212 \u00b5)\u2217\n( W \u25e6W \u2217 + \u03c32IHE )\u22121 (\u03d5\u2212 \u00b5) . (39)\nThe normalization term follows similarly to Proposition 1 and we can verify the consistency of the obtained distribution by looking at the spectrum using Lemma 3.\nA.2.2. MAXIMUM LIKELIHOOD\nTraining the model in primal corresponds to maximizing the likelihood of the observations in the finite feature space HE . Proposition 3 (Primal Maximum Likelihood). Provided \u03c32 \u2264 \u03bbq/N , where \u03bbq is the qth greatest eigenvalue of \u03a6c \u25e6\u03a6\u2217c , the Maximum Likelihood (ML) of W and \u00b5 given the observations {\u03d5i}Ni=1 is given by\n\u00b5ML = \u03d5c, (40)\nWML = q\u2211 p=1 \u221a \u03bbp/N \u2212 \u03c32vpr\u2217p, (41)\nwith {(\u03bbp,vp)}qp=1 the greatest eigenpairs of \u03a6c \u25e6\u03a6 \u2217 c (w.r.t. the eigenvalues). Proof. The maximum likelihood of the observations {\u03d5i}Ni=1 is computed as argmaxW ,\u00b5 log (\u220fN i=1 p(\u03d5i|W ,\u00b5) ) = argmaxW ,\u00b5L\u03d5, with L\u03d5 the likelihood function, which can be written as\nL\u03d5 = \u2212 N\n2\n{ N log(2\u03c0) + (N \u2212 q) log ( \u03c32 ) +\nq\u2211 p=1 log ( s2p + \u03c3 2 ) + 1 N\u03c32 N\u2211 i=1 (\u03d5i \u2212 \u00b5)\u2217PHE (\u03d5i \u2212 \u00b5)\n+ 1\nN q\u2211 p=1\n1\ns2p + \u03c3 2 ( N\u2211 i=1 ( \u03f1\u2217p(\u03d5i \u2212 \u00b5) ))2} .\n(42)\nThe optimization of the mean \u00b5 is trivial and we have \u00b5ML = \u03d5c. The optimization of W = \u2211q p=1 sp\u03f1pr \u2217 p is less trivial. We first note that optimizing for {\u03f1p}qp=1, {s2p} q p=1 and {rp} q p=1 is not identifiable: indeed, two singular values may be equal. Furthermore, the likelihood function L\u03d5 is independent of the basis {rp}qp=1 and for any solution of \u03f1p, we must also admit its opposite \u2212\u03f1p too. In addition to that, computing the saddle points of the likelihood is not straightforward as we cannot consider the vectors {\u03f1p}qp=1 to be independent variables as they must remain orthonormal: the variation of one basis vector \u03f1p must keep it normalized and has an influence on the other basis vectors. For sp however, the variations may happen with the only constraint of strict positivity. We may thus consider\n\u2202L\u03d5 \u2202sp = 0 \u21d0\u21d2 s2p + \u03c32 = 1 N\n( \u03f1\u2217p ( N\u2211 n=1 \u03d5n \u2212 \u00b5 ))2 . (43)\nWe now consider the fact that PHE = IHE \u2212 \u2211q p=1 \u03f1p\u03f1 \u2217 p. The likelihood function restricted to the terms dependent on \u03f1p reduces to 1\n2\n{ q\u2211\np=1\n( 1\ns2p + \u03c3 2 \u2212 1 \u03c32\n) \u03a6\u2217c \u25e6 ( \u03f1p\u03f1 \u2217 p ) \u25e6\u03a6c } . (44)\nFrom there, we can deduce that the likelihood is maximized when {\u03f1p}qp=1 forms a basis of span(v1, . . . ,vp), with {vp} q p=1 the q greatest eigenvectors of \u03a6c \u25e6 \u03a6\u2217c (w.r.t. the eigenvalues \u03bbp\u2019s). We may thus identify both basis: \u03f1p = vp and by consequence s2p + \u03c3 2 = \u03bbp/N (from Eq. (43)), for all p = 1, . . . , q.\nWhat about the other choices of basis? At the end, it would not change anything as it would be identified to the same solution. Indeed, as the \u03f1p\u2019s could always be written as a linear combination of {vp}qp=1, we could always write the interconnection operator as WML = \u2211q p=1(\u03bbp/N \u2212 \u03c32)vpr\u2217p as the choice of the orthonormal basis {rp} q p=1 is arbitrary.\nA.2.3. DIMENSIONALITY REDUCTION IN FEATURE SPACE\nProposition 4 (Primal Conditional Latent Distribution). The conditional distribution of h given \u03d5 is given by\np (h |\u03d5 ) = 1 Zh|\u03d5 exp ( \u2212 1 2\u03c32 \u2225h\u2212m\u22252\u03a3\u22121 h|\u03d5 ) , (45)\nwith \u03a3h|\u03d5 = ( W \u2217 \u25e6W + \u03c32IL )\u22121 , m = (\u03a3h|\u03d5)\u22121W \u2217(\u03d5\u2212 \u00b5) and Zh|\u03d5 = (\u03c3 \u221a 2\u03c0)q (\u220fq p=1(s 2 p + \u03c3 2) )\u22121/2 .\nProof. The methodology is analogous to Props. 1 and 2.\nA.3. Dual Model\nA.3.1. REPRESENTATION\nProposition 5 (Dual Representation). Given any interconnection operator W : L \u2192 HL, we have the following representation:\nW = \u03a6c \u25e6A, (46)\nwith A : L \u2192 L, the dual interconnection operator.\nProof. Following our definitions, we know that \u03a6c \u25e6 \u03a6\u2217c \u2265 q. By consequence, the linear operator \u03a6c : H \u2192 E has at least q non-zero singular values. As we have dim(L) = dim(HL), the proof is concluded by recalling that the primal interconnection operator W : L \u2192 HL is also linear.\nProposition 6 (Dual Maximum Likelihood). Given the operator WML (Prop. 3) and provided \u03c32 \u2264 \u03bbq/N , where \u03bbq is the qth greatest eigenvalue of \u03a6\u2217c \u25e6\u03a6c, the dual interconnection operator AML : L \u2192 L is given by:\nAML = q\u2211 p=1 \u221a 1/N \u2212 \u03c32\u03bb\u22121p \u03f5pr\u2217p, (47)\nwith {(\u03bbp, \u03f5p)}qp=1 the greatest eigenpairs of \u03a6 \u2217 c \u25e6\u03a6c (w.r.t. the eigenvalues) and {rp} q p=1 an arbitrary orthonormal basis of the latent space L.\nProof. Using the relation vp = (\u03bbp)\u22121/2\u03a6c\u03f5p, we have\nWML = q\u2211 p=1 \u221a \u03bbp/N \u2212 \u03c32vpr\u2217p, (48)\n= q\u2211 p=1 (\u03bbp) \u22121/2 \u221a \u03bbp/N \u2212 \u03c32\u03a6c\u03f5pr\u2217p, (49)\n= \u03a6c \u25e6\n( q\u2211\np=1\n\u221a 1/N \u2212 \u03c32\u03bb\u22121p \u03f5pr\u2217p ) . (50)\nThe proof is concluded by insuring that 1/N \u2212 \u03c32\u03bb\u22121p is never negative.\nA.3.2. DIMENSIONALITY REDUCTION IN KERNEL SPACE\nProposition 7 (Conditional Kernel Distribution). Under the same assumption as Prop. 6 and provided that \u03a6\u2217c \u25e6\u03a6c is of full rank, the posterior distribution of the dual feature variable kc given the latent variable h is given by\np (kc|h) = 1\nZkc|h exp ( \u2212 1 2\u03c32 \u2225kc \u2212 (\u03a6\u2217c \u25e6\u03a6c) \u25e6AMLh\u2225 2 \u03a3\u22121 kc|h ) , (51)\nwith Zkc|h = ( \u03c3 \u221a 2\u03c0 )N \u220fN i=1 \u03bbi and \u03a3kc|h = \u03a6 \u2217 c \u25e6\u03a6c.\nProof. We consider the optimal primal interconnection operator WML = \u03a6c \u25e6 AML together with the optimal mean \u00b5ML = \u03d5c and fill it in Def. 1. The covariance is deducted by considering \u03d5\u2212\u03d5c = ( \u03a6\u2217c )\u22121\u25e6\u03a6\u2217c(\u03d5\u2212\u03d5c) = (\u03a6\u2217c)\u22121kc.\nProposition 8 (Dual Conditional Latent Distribution). Under the same assumption as Prop. 6, the posterior distribution of the latent variable h given the dual feature variable kc is given by\np(h|kc) = 1\nZh|kc exp ( \u2212 1 2\u03c32 \u2225h\u2212m\u2032\u22252\u03a3\u22121 h|kc ) , (52)\nwith \u03a3h|kc = ( A\u2217ML \u25e6 ( \u03a6\u2217c \u25e6\u03a6c ) \u25e6AML + \u03c32IL )\u22121 , Zh|kc = Zh|\u03d5 (Prop. 4) and m \u2032 = ( \u03a3\u22121h|kc \u25e6A \u2217 ML ) kc.\nProof. It suffices to consider the optimal primal interconnection operator WML = \u03a6c \u25e6AML, as well as the optimal mean \u00b5ML = \u03d5c and fill both in Prop. 4.\nA.3.3. KERNEL SAMPLING\nProposition 9 (Marginal Kernel Distribution). Provided that \u03a6\u2217c \u25e6\u03a6c is of full rank and given the assumptions of Prop. 6, the posterior distribution of the the dual feature kc after optimization is given by\np(kc) = 1\nZkc exp ( \u22121 2 \u2225kc\u22252\u03a3\u22121kc ) , (53)\nwith \u03a3kc = A \u2217 ML \u25e6AML + \u03c32 (\u03a6\u2217c \u25e6\u03a6c) \u22121 and Zkc = (2\u03c0) N/2N\u2212q/2 \u220fq p=1 \u03bbp \u220fN p=q+1(\u03c3 2\u03bbp) 1/2.\nProof. We start from Prop. 2 and develop the covariance:\nWMLW \u2217 ML + \u03c3 2IHE = \u03a6c \u25e6 ( A\u2217 \u25e6A+ \u03c32 (\u03a6\u2217c \u25e6\u03a6c) \u22121 ) \u25e6\u03a6\u2217c , (54)\n= \u03a6c \u25e6\n( 1\nN q\u2211 p=1 \u03f5p\u03f5 \u2217 p + \u03c3 2 N\u2211 p=q+1 \u03bb\u22121p \u03f5p\u03f5 \u2217 p ) \u25e6\u03a6\u2217c . (55)\nWe set \u00b5ML = \u03d5c and observe that\n(\u03d5\u2212 \u03d5c)\u2217 ( WMLW \u2217 ML + \u03c3 2IHE )\u22121 (\u03d5\u2212 \u03d5c) , = k\u2217c ( \u03a6\u2217c \u25e6 ( WMLW \u2217 ML + \u03c3 2IHE ) \u25e6\u03a6c )\u22121 kc, (56)\n= k\u2217c\n( 1\nN q\u2211 p=1 \u03bb2p\u03f5p\u03f5 \u2217 p + \u03c3 2 N\u2211 p=q+1 \u03bbp\u03f5p\u03f5 \u2217 p\n) kc. (57)"
        }
    ],
    "title": "A Dual Formulation for Probabilistic Principal Component Analysis",
    "year": 2023
}