{
    "abstractText": "Multimodal Sentiment Analysis (MSA) has been a popular topic in natural language processing nowadays, at both sentence and aspect level. However, the existing approaches almost require large-size labeled datasets, which bring about large consumption of time and resources. Therefore, it is practical to explore the method for few-shot sentiment analysis in crossmodalities. Previous works generally execute on textual modality, using the prompt-based methods, mainly two types: handcrafted prompts and learnable prompts. The existing approach in few-shot multi-modality sentiment analysis task has utilized both methods, separately. We further design a hybrid pattern that can combine one or more fixed hand-crafted prompts and learnable prompts and utilize the attention mechanisms to optimize the prompt encoder. The experiments on both sentencelevel and aspect-level datasets prove that we get a significant outperformance. Then we conduct an ablation study and a series of further analysis",
    "authors": [
        {
            "affiliations": [],
            "name": "Zikai Zhou"
        },
        {
            "affiliations": [],
            "name": "Haisong Feng"
        },
        {
            "affiliations": [],
            "name": "Baiyou Qiao"
        },
        {
            "affiliations": [],
            "name": "Gang Wu"
        },
        {
            "affiliations": [],
            "name": "Donghong Han"
        }
    ],
    "id": "SP:f4b59c4af963c0e946d4f70e421f93e4a4b5ad10",
    "references": [
        {
            "authors": [
                "Dong Zhang",
                "Shoushan Li",
                "Qiaoming Zhu",
                "Guodong Zhou"
            ],
            "title": "Effective sentiment relevant word selection for multimodal sentiment analysis in spoken language",
            "venue": "ACM MM, 2019, pp. 148\u2013156.",
            "year": 2019
        },
        {
            "authors": [
                "Xincheng Ju",
                "Dong Zhang",
                "Rong Xiao",
                "Junhui Li",
                "Shoushan Li",
                "Min Zhang",
                "Guodong Zhou"
            ],
            "title": "Joint multi-modal aspect sentiment analysis with auxiliary cross-modal relation detection",
            "venue": "EMNLP, 2021, pp. 4395\u20134405.",
            "year": 2021
        },
        {
            "authors": [
                "Quoc-Tuan Truong",
                "Hady W. Lauw"
            ],
            "title": "Vistanet: A Visual Aspect Attention Network for Multimodal Sentiment Analysis",
            "venue": "AAAI, 2019, pp. 305\u2013312.",
            "year": 2019
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "ACL/IJCNLP, 2021, pp. 3816\u20133830",
            "year": 2021
        },
        {
            "authors": [
                "Timo Schick",
                "Hinrich Sch\u00fctze"
            ],
            "title": "It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
            "venue": "In NAACL,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Yu",
                "Dong Zhang"
            ],
            "title": "Few-shot multi-modal sentiment analysis with prompt-based vision-aware language modeling",
            "venue": "ICME, 2022, pp. 1\u20136.",
            "year": 2022
        },
        {
            "authors": [
                "B. Liu"
            ],
            "title": "Sentiment analysis: mining opinions, sentiments, and emotions",
            "year": 2015
        },
        {
            "authors": [
                "B. Liu"
            ],
            "title": "Sentiment analysis and opinion mining (introduction and survey)",
            "year": 2012
        },
        {
            "authors": [
                "Y. Ding",
                "J. Yu",
                "J. Jiang"
            ],
            "title": "Recurrent neural networks with auxiliary labels for cross-domain opinion target extraction",
            "venue": "in AAAI,",
            "year": 2017
        },
        {
            "authors": [
                "H. Xu",
                "B. Liu",
                "L. Shu",
                "P.S. Yu"
            ],
            "title": "Double embeddings and CNN-based sequence labeling for aspect extraction",
            "venue": "in ACL,",
            "year": 2018
        },
        {
            "authors": [
                "W. Xue",
                "T. Li"
            ],
            "title": "Aspect-based sentiment analysis with gated convolutional networks",
            "venue": "in ACL,",
            "year": 2018
        },
        {
            "authors": [
                "J. Devlin",
                "M. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT pre-training of deep bidirectional transformers for language understanding, in NAACL-HLT",
            "year": 2019
        },
        {
            "authors": [
                "K. Li",
                "C. Chen",
                "X. Quan",
                "Q. Ling",
                "Y. Song"
            ],
            "title": "Conditional Augmentation for Aspect Term Extension via Masked Sequence-to-Sequence Generation",
            "venue": "in ACL,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "T. Qian"
            ],
            "title": "Enhancing aspect term extraction with soft prototypes",
            "venue": "in EMNLP,",
            "year": 2020
        },
        {
            "authors": [
                "H. Wan",
                "Y. Yang",
                "J. Du",
                "Y. Liu",
                "K. Qi",
                "J.Z. Pan"
            ],
            "title": "Target-aspect-sentiment joint detection for aspect-based sentiment analysis",
            "venue": "in AAAI,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Wang",
                "M. Huang",
                "X. Zhu",
                "L. Zhao"
            ],
            "title": "Attention-Based LSTM for Aspect-Level Sentiment Classification, in EMNLP",
            "year": 2016
        },
        {
            "authors": [
                "Y. Tian",
                "G. Chen",
                "Y. Song"
            ],
            "title": "Aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble",
            "venue": "NAACL-HLT,",
            "year": 2021
        },
        {
            "authors": [
                "C. Zhang",
                "Q. Li",
                "D. Song"
            ],
            "title": "Syntax-aware aspect-level sentiment classification with proximity weighted convolution network",
            "venue": "SIGIR, 2019, pp. 1145\u20131148",
            "year": 2019
        },
        {
            "authors": [
                "B. Huang",
                "K. Carley"
            ],
            "title": "Syntax-aware aspect-level sentiment classification with graph attention networks",
            "venue": "EMNLP-IJCNLP, 2019, pp. 5469\u201354",
            "year": 2019
        },
        {
            "authors": [
                "Ruifan Li",
                "Hao Chen",
                "Fangxiang Feng",
                "Zhanyu Ma",
                "Xiaojie Wang",
                "Eduard Hovy"
            ],
            "title": "Dual-Graph Convolutional Networks for Aspect-Based Sentiment Analysis, ACL-IJNLP",
            "year": 2021
        },
        {
            "authors": [
                "Zheng Zhang",
                "Zili Zhou",
                "Yanna Wang"
            ],
            "title": "SSEGCN: Syntactic and Semantic Enhanced Graph Convolutional Network for Aspect-Based Sentiment Analysis",
            "year": 2022
        },
        {
            "authors": [
                "Xiaocui Yang",
                "Shi Feng",
                "Daling Wang",
                "Yifei Zhang"
            ],
            "title": "Image-text multimodal emotion classification via a multiview attentional network",
            "venue": "IEEE Transactions on Multimedia, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Jiajia Tang",
                "Kang Li",
                "Xuanyu Jin",
                "Andrzej Cichocki",
                "Qibin Zhao",
                "Wanzeng Kong"
            ],
            "title": "CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network ACL-IJCNLP, pages 5301\u20135311",
            "year": 2021
        },
        {
            "authors": [
                "Yang Wu",
                "Yanyan Zhao",
                "Hao Yang",
                "Song Chen",
                "Bing Qin",
                "Xiaohuan Cao",
                "Wenting Zhao"
            ],
            "title": "Sentiment Word-Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors",
            "year": 2022
        },
        {
            "authors": [
                "Nan Xu",
                "Wenji Mao",
                "Guandan Chen"
            ],
            "title": "Multi-interactive memory network for aspect-based multimodal sentiment analysis",
            "venue": "In Proceedings of AAAI",
            "year": 2019
        },
        {
            "authors": [
                "Jianfei Yu",
                "Jing Jiang",
                "Li Yang",
                "Rui Xia"
            ],
            "title": "Improving multimodal named entity recognition via entity span detection with a unified multimodal transformer",
            "venue": "In Proceedings of ACL",
            "year": 2020
        },
        {
            "authors": [
                "Xincheng Ju",
                "Dong Zhang",
                "Rong Xiao",
                "Junhui Li",
                "Shoushan Li",
                "Min Zhang",
                "Guodong Zhou"
            ],
            "title": "Joint Multimodal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection [EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Hosseini-Asl",
                "Ehsan"
            ],
            "title": "A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis.\" NAACL-HLT (2022)",
            "year": 2022
        },
        {
            "authors": [
                "P. Zhang",
                "T. Chai",
                "Y. Xu"
            ],
            "title": "Adaptive prompt learning-based few-shot sentiment analysis, ArXiv",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Chen",
                "Ningyu Zhang",
                "Xin Xie",
                "Shumin Deng",
                "Yunzhi Yao",
                "Chuanqi Tan",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen"
            ],
            "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction, arXiv e-prints, page arXiv:2104.07650",
            "year": 2021
        },
        {
            "authors": [
                "Yi Sun",
                "Yu Zheng",
                "Chao Hao",
                "Hangping Qiu"
            ],
            "title": "NSP-BERT: A Prompt-Based Zero-Shot Learner Through an Original Pre-Training Task: Next Sentence Prediction",
            "venue": "arXiv e-prints,",
            "year": 2021
        },
        {
            "authors": [
                "Taylor Shin",
                "Yasaman Razeghi",
                "Robert L. Logan IV",
                "Eric Wallace",
                "Sameer Singh"
            ],
            "title": "Autoprompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
            "venue": "EMNLP, 2020, pp. 4222\u20134235.",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "ACL/IJCNLP, 2021, pp. 3816\u20133830.",
            "year": 2021
        },
        {
            "authors": [
                "Chengyu Wang",
                "Jianing Wang",
                "Minghui Qiu",
                "Jun Huang",
                "Ming Gao"
            ],
            "title": "TransPrompt: Towards an Automatic Transferable Prompting Framework for Few-Shot Text Classification in EMNLP",
            "year": 2021
        },
        {
            "authors": [
                "Teng Niu",
                "Shiai Zhu",
                "Lei Pang",
                "Abdulmotaleb El-Saddik"
            ],
            "title": "Sentiment analysis on multi-view social data",
            "venue": "MMM, 2016, pp. 15\u201327.",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT, 2019, pp. 4171\u20134186",
            "year": 2019
        },
        {
            "authors": [
                "Jie Zhou",
                "Jiabao Zhao",
                "Jimmy Xiangji Huang",
                "Qinmin Vivian Hu",
                "Liang He"
            ],
            "title": "MASAD: A large-scale dataset for multimodal aspect-based sentiment analysis",
            "venue": "Neurocomputing, vol.455, pp. 47\u201358."
        }
    ],
    "sections": [
        {
            "text": "Multimodal Sentiment Analysis (MSA) has been a popular topic in natural language processing nowadays, at both sentence and aspect level. However, the existing approaches almost require large-size labeled datasets, which bring about large consumption of time and resources. Therefore, it is practical to explore the method for few-shot sentiment analysis in crossmodalities. Previous works generally execute on textual modality, using the prompt-based methods, mainly two types: handcrafted prompts and learnable prompts. The existing approach in few-shot multi-modality sentiment analysis task has utilized both methods, separately. We further design a hybrid pattern that can combine one or more fixed hand-crafted prompts and learnable prompts and utilize the attention mechanisms to optimize the prompt encoder. The experiments on both sentencelevel and aspect-level datasets prove that we get a significant outperformance. Then we conduct\nan ablation study and a series of further analysis\nKeywords:Multimodal sentiemnt analysis ; prompt learning ; aspect-based multimodal sentiment analysis; Few-shot multimdal sentiment analysis"
        },
        {
            "heading": "1. Introduction",
            "text": "Multi-modal sentiment analysis (MSA) has gained popularity among researchers in recent years because of its wide practicality in e-commerce [1], social media [2], and human-computer interaction [3]. Previous studies commonly cast MSA into two kinds of sub-tasks: sentence-level sentiment detection with given representations of modalities and aspect-based sentiment analysis (MABSA) with given aspect terms and accompanying multi-modal pairs. In this paper, we mainly adopt textual and visual modalities to execute the tasks above mentioned and evaluate our work.\nHowever, most existing approaches ignore the fact that labeled multi-modal datasets are large and expensive. Hence, the most practical solution should be to investigate dew-shot learning strategies that have the ability to perform relatively well with a low-resource training set, such as LM_BFF [4] in text-based tasks, etc. Prompt methods have been popular for NLP tasks since PET [5]'s proposal, for they make pre-trained models execute new tasks with low resources or even no data for training. Besides, the recent study PVLM [6] in MSA has demonstrated that prompt tuning fits not only textual but multimodal scenarios. PVLM separately tests two main types of prompt methods: fixed and unlearnable prompts and learnable prompts for few-shot multimodal sentiment analysis (FMSA) and few-shot multimodal aspect-based sentiment analysis (FMABSA). However, for the fixed prompt, the best choice of PVLM in most cases, it's well known that the fixed prompt has one disadvantage: it's hard to craft the optimal prompt by humans. As shown in the result of PVLM, each hard prompt gained the best performance in the partial dataset. Moreover, we find that the learnable prompt also achieves and even outperforms the hard prompt on a special dataset.\nSo we have reason to believe that an effective fusion of two prompt-based methods mainly used to enable prLM (pre-\ntrained language model) to perform better for FMSA and its design is exchanging simultaneously. To solve the above challenge, we propose the hybrid syntax-enhanced prompt model for sentiment reorganization of text and vision double-modalities, relying on a few trainings. Our method incorporates fixed and learnable prompts to improve the multimodal few-shot effect and transform FMSA and FMABSA tasks into a language modeling problem with masks that return the probability of being designated sentiment words (verbalizers) based on given hybrid prompts. Specifically, we first conduct a hybrid prompt pattern to merge one or more hard prompts with learnable prompt tokens appropriately, which do not simply merge two single hard prompts or hard and soft prompts but contain hard prompts and learnable (soft) tokens together. We find that the prompt encoder of PVLM performs unsatisfactorily. So, with PLVM's as the backbone, we further optimize it into an interactable and syntax-aware reference where biaffine attention is introduced to referencing the syntax knowledge and SDPA attention to enriching the semantic information through interactively transferring representation among learnable tokens. Our main contributions can be summarized as follows:\n\u2022 We propose a novel multimodal hybrid syntax-aware prompt model for both FMSA and FMABSA tasks. \u2022 We designed the hybrid pattern to merge fixed prompts with learnable prompts to make it more robust. \u2022 We propose a syntax-aware inference unit wrapped with the biaffine and scaled dot product attention (SDPA) mechanisms\nto encode a learnable prompt for FMSA.\n\u2022 We execute experiments on four multimodal sentiment datasets and outperform the existing method significantly."
        },
        {
            "heading": "2. Related Work",
            "text": "2.1. Sentiment Analysis and Multi-modal Sentiment Analysis\nThe core task of SA is to predict the sentiment polarity of given sentences. It had been a popular direction for research and business in NLP (neutral language processing) as early as 2010. There are a number of review books and papers covering a wide range of early methods and applications. [7,8,9]. Aspect-based Sentiment Analysis (ABSA) as a subtask of SA, proposed to predict the sentiment polarity for aspects, has received high attention. The research relatively includes aspectopinion co-extraction [10\u201314], aspect-opinion pair extraction [15\u201316], and aspect-opinion-sentiment triple extraction [17- 20]. Recently, the approaches for the ABSA task mainly utilized tree and graph structures based on language syntax knowledge [20\u201324].\nSimilarly, in a multi-modal scenario, at the sentence level, multi-modal sentiment analysis (MSA), as another subtask of SA, on text-image pairs aims to recognize the sentiment according to the given text and corresponding image. [25] publish a cross-modal emotion detection dataset on textual and visual modality and design a multi-view network with attention to the features of visual objects, visual scenes from images, and textual representation. [26] proposes a Coupled-Translation Fusion Network, achieving multi-modal information interaction through coupled learning to ensure robustness when modalities miss. [27] find that automatic speech recognition may cause errors and hurt the prediction task, based on which they propose to refine the erroneous sentiment words by predicting them from other modalities. At the aspect level, different from text-based tasks, it's acknowledged as challenging to fuse textual and image information. As the pioneer in MSA, [28] collected a benchmark dataset in Chinese from a digital product feedback online platform and designed a memory network gathering multi-interaction information to fuse representations from both textual and visual modalities. More recently, [29] contributed two datasets with open-ended aspects from Twitter for MABSA and effectively combined both modalities by leveraging BERT as the backbone. In the same period, Yu proposed a fusion network with target-aware attention, solving target-based sentiment reorganization in both textual and multi-modal modalities. Then [30] first utilizes cross-modal triplet extraction to assist in the multi-modal aspect-level sentiment classification task.\nHowever, all the above approaches depend on large, multi-modally labeled data, which has been difficult to collect and\nannotate. Different from them, our proposed method, which solves the few-shot multi-modal sentiment analysis and aspectbased sentiment analysis tasks, avoids the necessity of a large training set by simply relying on a few training sets.\n2.2. Few-shot learning and prompt-based method\nFor the reason mentioned above, so that the recent research starts to pay attention to the few-shot learning for SA, we list the alternative works in relevant sentiment analysis tasks we find: [6] conducts the Trans-Modal Information Fusion Model PVLM based on a few samples in multimodal sentiment analysis; [31] transforms sentiment classification into a text generation task and tests on a few shots and the full shot, achieving significant improvement. In AP [32], based on the additive attention mechanism, an adaptive encoder is constructed, which is trained by large-scale data sets, and transform learning is applied to a few-shot task. Within the best scope of our finding, only PVLM can be referred to and followed for the multimodal sentiment task, which utilizes the prompt-based method for FMSA for the first time and proves that prompt tuning replacing fine-tuning is significantly beneficial.\nPrompt tuning, originating from prompt learning (prompt-based learning), Since the early period, prompt learning was just integrating the constructed template into the original text, but different manual discrete templates have proven to have a great impact on the results. Later, the continuous prompt method was proposed to avoid manual construction, which achieved similar performance or even better than the manual template. Essentially, prompt learning is an efficient way to make processing closer to natural language, converting downstream tasks into corresponding PrLM self-supervised learning tasks. Recently, Chen et al. (2021) [33] solved the relationship extraction task with the Mask Language Model (MLM)\u2014BERT [39]. Sun et al. (2021) [34] applied prompts to the downstream task, the next sentence prediction (NSP). Moreover, as the pre-training model fits successfully on NLP tasks, the prompt-based pre-training model can perform well on few-shot tasks, too. On many text-based tasks, [35] utilizes auto-constructed prompts for text categorization. [36] introduce prompts, significantly improve the result of text regression on few-shot datasets, and construct prompts to assist text generation tasks. [37] use prompts to improve the generalization performance of the pre-training language model on different one-shot tasks.\nBased on these approaches, we propose a hybrid prompt that combines both a hand-crafted template (the hard prompt) and a learnable template (the soft prompt) to sufficiently use prompt-based methods. Besides, we employ the attention mechanisms for gathering syntax information and semantic representation to optimize the prompt encoder of PLVM and further explore the prompt tuning's potential for FMSA."
        },
        {
            "heading": "3. Proposed Methods",
            "text": "the input from original modalities and prompts, then the Inference unit encodes and inference for pseudo-tokens(colored in brown) and utilizes the output to replace the original tokens(colored in green), the encoder and embedding depend on the pre-trained model, the [MASK]'s are shown as MASK[1] and [MASK]2, the prediction scores of them are different. In this case, right prediction(MASK[1]) and wrong (MASK[2]) are utilized by the Fusion strategy, merging directly or choosing. Here the result after merging is \"positive\" with the highest prediction scores, too.\nWe first introduce how our proposed framework catches and processes the representation from two modalities with the\ndesigned hybrid patterns in cross-modality tasks.\nFor the vision modality, following the PLVM, we leverage the NF-RestNet to extract and eject the neural vision\nrepresentation into the textual embeddings and format the process as follows:\n\ud835\udc49 = \ud835\udc4a\ud835\udc56\ud835\udc43\ud835\udc5c\ud835\udc5c\ud835\udc59(\ud835\udc45\ud835\udc52\ud835\udc60\ud835\udc41\ud835\udc52\ud835\udc61(\ud835\udc4b)) + \ud835\udc4f\ud835\udc56\n\ud835\udc49\u02dc = \ud835\udc5f\ud835\udc52\ud835\udc60\u210e\ud835\udc4e\ud835\udc5d\ud835\udc52(\ud835\udc49 ) = [\ud835\udc631, \u2026 , \ud835\udc63\ud835\udc57 , \u2026 , \ud835\udc63\ud835\udc41\ud835\udc56], \ud835\udc63\ud835\udc57 \u2208 \ud835\udc45\ud835\udc51\ud835\udc61\nwhere V \u2208\ud835\udc45\ud835\udc51\ud835\udc5b\ud835\udc61 , \ud835\udc4a\ud835\udc56 \u2208 \ud835\udc45 \ud835\udc51\ud835\udc61\u00d7\ud835\udc51\ud835\udc5b\ud835\udc61 , \ud835\udc4f\ud835\udc56 \u2208 \ud835\udc45 \ud835\udc51\ud835\udc5b\ud835\udc61 , nt = \ud835\udc51\ud835\udc61 \u00d7 \ud835\udc41\ud835\udc56, \ud835\udc41\ud835\udc56 that is defined to limiting the number of tokens representing original vision features. \ud835\udc4b denotes the input of image modality and \ud835\udc51\ud835\udc61 refers to the dimension of textual embedding for the PrLM.\nFor textual modality, we optimize the prompt encoder of p-tuning by attention mechanisms. In detail, we utilize the SDPA to enrich the semantic information and biaffine attention to understand the syntax information helpful to reference (We explore the combination strategy of two attention mechanisms in further analysis and employ the best framework) and not simply gather the representation from both modalities alternatively, but also supplement the hand-crafted hard prompts.\nThe figure \u201c?\u201d shows the framework of our proposed method. In this figure, we take an instance with two [MASK] tokens\nin hybrid pattern for aspect-level tasks."
        },
        {
            "heading": "Task definition",
            "text": "In this paper, we assume that it is possible to obtain a pre-trained language model M, such as BERT, and that we aim to tune it on the multi-modal sentiment analysis (FMSA) task and the multi-modal sentiment analysis (FMABSA) task with just a few training sets.\nFor sentence-level MSA (FMSA), given a multi-modal sample \ud835\udc46 and emotion labels L, it contains an image and a corresponding text \ud835\udc3c = \ud835\udc561,.., \ud835\udc56\ud835\udc5b, in which n denotes the length of the tokenized sentence. Our purpose is to recognize that the\nclassification of the sentiment expressed by both the text and image corresponds to \"positive\", \"neutral\" or \"negative\" as the space of L for 3-classes or \"positive\", \"negative,\" for 2-classes.\nFor aspect-level MSA, given a text-image sample \ud835\udc46, \ud835\udc3c, image, \ud835\udc3f, similar to MSA, with a phrase or word as aspect \ud835\udc34 = \ud835\udc4e1,\u2026,\ud835\udc4e\ud835\udc5a especially, in which m denotes the number Our goal is to predict which space it will most probably fall into among the spaces of the \ud835\udc3f with respect to \ud835\udc34."
        },
        {
            "heading": "Multimodal-aware prompt-based tuning",
            "text": "Multimodal models proposed by related approaches are prone to being infectious because of overfitting. An effective strategy for solving this problem is prompt tuning instead of fine-tuning. We conduct the prompt-based method on text modality and embed a visual model, which represents the original input from the image feature space. Given a pre-trained language model (PrLM) L, prompt tuning directly tasks the L with the natural language template or pseudo-tokens for adaptation learning.\nHybrid pattern for prompt-based tuning\nFor instance, on a text sample, we can design the pattern, transforming the classification task of three polarities (i.e., positive, neutral, and negative) into a cloze and mask-filled task. It can be formatted as follows: For \ud835\udc3c (e.g., \"a delicious food\"), \ud835\udc3c\u210e\ud835\udc4e\ud835\udc5f\ud835\udc51 = \ud835\udc47\uff08\ud835\udc3c\uff09:\n\ud835\udc3c\u210e\ud835\udc4e\ud835\udc5f\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\ud835\udc3c \u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. [\ud835\udc46\ud835\udc38\ud835\udc43]\n\ud835\udc3c\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\ud835\udc3c [\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc46\ud835\udc38\ud835\udc43]\nLet \ud835\udc3f conjecture whether it is more possible to fill in \"delicious\" (positive) or \"bad\" (negative) for \"mask.\" In our hybrid pattern, they are combined as follows:\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. \ud835\udc3c[\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc46\ud835\udc38\ud835\udc43]\nwhen hard templates merge, as follows:\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. \ud835\udc3c\ud835\udc61 \ud835\udc56\ud835\udc60 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. [\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51]\nFor aspect-level (single text modality), it is similar to the previous patterns except for the separate joining of aspects. \ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = \ud835\udc47\uff08\ud835\udc3c, \ud835\udc34\uff09, Formally,\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. \ud835\udc39\ud835\udc5c\ud835\udc5f \ud835\udc34, \ud835\udc56\ud835\udc61 \ud835\udc56\ud835\udc60 [\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc46\ud835\udc38\ud835\udc43]\nIn a multi-modal task, we merge the pseudo-visual tokens \ud835\udc49 = \ud835\udc631,,, \ud835\udc63\ud835\udc57, which work by generating the feature space from the visual modality into the above patterns. By doing this, we combine the text and vision. So that the prompt method working for PrLM is successfully applied to both modes in the FMSA task. For sentence-level MSA:\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\ud835\udc49[\ud835\udc46\ud835\udc38\ud835\udc43]\u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. \ud835\udc53\ud835\udc5c\ud835\udc5f \ud835\udc34 [\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51]\ud835\udc3c[\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51] [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e] [[\ud835\udc46\ud835\udc38\ud835\udc43]\nwhere \ud835\udc57 denotes the length of \ud835\udc49. Similarly, for aspect-level, aspect terms \ud835\udc34, original text \ud835\udc3c, and vision tokens \ud835\udc49 compose into \ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = \ud835\udc47\uff08\ud835\udc3c, \ud835\udc49, \ud835\udc34\uff09.\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51]\ud835\udc49\ud835\udc57 [\ud835\udc46\ud835\udc38\ud835\udc43]\u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. \ud835\udc53\ud835\udc5c\ud835\udc5f \ud835\udc34, [\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51]\ud835\udc3c[\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51] [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e] [\ud835\udc46\ud835\udc38\ud835\udc43]\nIt is worth noting that there are some considerations due to the specific design. We give the form for reference, in which \ud835\udc56 , \ud835\udc57, \ud835\udc58, \ud835\udc5a, \ud835\udc5b, \ud835\udc66, and \ud835\udc67 all represent the length of the given prompts \ud835\udc5d .\ud835\udc3b\ud835\udc5d\ud835\udc5f\ud835\udc5c\ud835\udc5a\ud835\udc5d\ud835\udc61 = [for \"\" the sentence is [MASK]] , where the characters \ud835\udc3c and \ud835\udc49 mean the input of text and image modality.At the sentence level (double fixed templates)\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51][\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51]\ud835\udc49\ud835\udc57 [\ud835\udc46\ud835\udc38\ud835\udc43]\u210e\ud835\udc5c\ud835\udc64 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]. \ud835\udc53\ud835\udc5c\ud835\udc5f \ud835\udc34, [\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51]\ud835\udc3c[\ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc51] [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e] [\ud835\udc46\ud835\udc38\ud835\udc43]\nEnsure that \ud835\udc67 is not 0 to ensure the functionality of the visual modality. At the aspect level (single fixed template, for instance; double fixed templates similarly)\n\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51 = [\ud835\udc36\ud835\udc3f\ud835\udc46]\ud835\udc5d\ud835\udc56 \u210e\ud835\udc5c\ud835\udc64 \ud835\udc5d\ud835\udc57[\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e] \ud835\udc5d\ud835\udc58 \ud835\udc61\u210e\ud835\udc52 \ud835\udc60\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc52 \ud835\udc5d\ud835\udc5a \ud835\udc3c \ud835\udc5d\ud835\udc5b \ud835\udc56\ud835\udc60 [\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e]\ud835\udc5d\ud835\udc66 [\ud835\udc46\ud835\udc38\ud835\udc43] \ud835\udc5d\ud835\udc67 \ud835\udc49 [\ud835\udc46\ud835\udc38\ud835\udc43]\nEnsure that \ud835\udc67 is not 0 and \ud835\udc57 is 0, avoiding destroying the aspect terms.\nWe show examples in table $$ for a detailed description with red color for Inappropriate setting.\nSyntax-aware Inference module based on multi-attention\nSyntactic dependency\nWe utilize examples to display the dependency in textual mode and how it inflects the SA and ABSA tasks.\nIn sentential affective analysis, e.g., Figure 2, the sentence is parsed as a tree by dependency. As shown, the word \"like\" repressing the emotion is parsed as the \"root\" of a tree and has types of relationships (detailly referring to http://www.universaldependencies.org) with \"i\",\"do\", and \"apple\" acting as the child of \"eating\" in edge \"eating->apple\" because \"apple\" depends on \"eating.\"\nFor aspect-level, take Figure 3 as an instance: for edges that contain words (\"tasty\" \"pizza\"), \"tasty\" and \"pizza\" are part of the aspect term \"tasty pizza\". Similarly, for the edge \"nice->pizza,\" \"pizza\" acts as an opinion target of \"nice\" and is endowed with positive emotion. Based on this, it is understandable that the finding of dependency between \"tasty\" and \"pizza\" is helpful to enhance the aspect term and recognize the sentiment for \"pizza\". Likewise, the discovery of the edge between nice and pizza rather than understanding \"pizza\" with \"slow\" is necessary and effective. In short, the discovery of dependency and the learning of task-dependent word representations for SA and ABSA both have positive effects on assistant performance."
        },
        {
            "heading": "Biaffine Attention Mechanism",
            "text": "The biaffine attention mechanism, proposed for neural dependency parsing, has been proven effective in syntactic dependency parsing. Besides, it is widely used in text-based explicit dependency trees and GCN-based models. It is understandable that learning the syntax information makes the model's understanding of the representation of natural language more similar to that us human.\nIn previous approaches, they labeled each word of a sentence with tags and computed the cross-entropy loss. Unfortunately, the pseudo-tags embedded in the learnable sequential prompt method mean that they cannot carry explicit tags. However, it is well known that cue-based learning makes input contain more natural representations or learn more natural representations adaptively. Based on this, they have improved on few-shot and even zero-shot, so we think that the introduction of the biaffine attention mechanism to learn grammar representation is helpful in the few-shot task.\nBefore being combined with the original text, Biaffine learns the syntax dependencies that pseudo-tag locations should learn adaptively. Specially, we feed the representation information integrated by BI-LSTM layers and self-attention as double-channel inputs into the Biaffine Attention Module, which extracts the syntax representation and enriches the representation of textual and image modalities. In the biaffine attention module, a multi-layer perceptron (MLP) is utilized for the input of channels to reduce the dimensions of inputs, followed by two deep bilinear attentions. The process formats as follows:\n\u210e\ud835\udc56 \ud835\udc53 = \ud835\udc40\ud835\udc3f\ud835\udc43\ud835\udc53(\u210e\ud835\udc56)\n\u210e\ud835\udc57 \ud835\udc60 = \ud835\udc40\ud835\udc3f\ud835\udc43\ud835\udc60(\u210e\ud835\udc57)\n\ud835\udc54\ud835\udc56,\ud835\udc57 = \u210e\ud835\udc56 \ud835\udc53 \ud835\udc47 \ud835\udc481\u210e\ud835\udc57 \ud835\udc60 +\ud835\udc482(\u210e\ud835\udc56 \ud835\udc53 \u2295 \u210e\ud835\udc57 \ud835\udc60 ) + \ud835\udc4f\n\ud835\udc5f\ud835\udc56,\ud835\udc57,\ud835\udc58 = exp(\ud835\udc54\ud835\udc56,\ud835\udc57,\ud835\udc58)\n\u2211 exp(\ud835\udc54\ud835\udc56,\ud835\udc57,\ud835\udc59) \ud835\udc5a \ud835\udc59=1\n\ud835\udc45 = \ud835\udc35\ud835\udc56\ud835\udc4e\ud835\udc53\ud835\udc53\ud835\udc56\ud835\udc5b\ud835\udc52(\ud835\udc40\ud835\udc3f\ud835\udc43\ud835\udc53(\ud835\udc3b), \ud835\udc40\ud835\udc3f\ud835\udc43\ud835\udc60(\ud835\udc3b))\nThe score vector \ud835\udc5f\ud835\udc56,\ud835\udc57 \u2208 \ud835\udc45 1\u00d7\ud835\udc5a formats the relations between \ud835\udc64\ud835\udc56 and \ud835\udc64\ud835\udc57; \ud835\udc5a means the types of relations, and \ud835\udc5f\ud835\udc56,\ud835\udc57,\ud835\udc58denotes the score of the \ud835\udc58-th relation type for pair (\ud835\udc64\ud835\udc56 , \ud835\udc64\ud835\udc57). The adjacency vector \ud835\udc45 \u2208 \ud835\udc45 \ud835\udc5b\u00d7\ud835\udc5b\u00d7\ud835\udc5a (because the input of channels has the same dim \ud835\udc5b shown sentence length) models the relation between words. \ud835\udc481, \ud835\udc482, and \ud835\udc4f are learnable weights and biases. \u2295 denotes concatenation. Eq. (16) collects process of Eqs. (14) to (16)"
        },
        {
            "heading": "Interaction-based self-attention (SDPA)",
            "text": "To integrate with fixed manual templates and fit the few-shot task of multi-modal sentiment analysis (FMSA), we introduce the SDPA mechanism from Transformer. With the assistance of scaled dot-product attention, the inference unit can better capture the semantic information of an input sequence by learning the interaction information in the input sequence based on the similarity among them.\nMoreover, when integrating SDPA into the inference unit, the trainable prompt template encoder remembers and extracts\nmore semantic information about the hard template. It is suitable and helpful for a mixed prompt strategy."
        },
        {
            "heading": "Loss computation",
            "text": "In our approach, we define mappings \ud835\udf11: \ud835\udc4c \u2192 \ud835\udc49, where \ud835\udc4c represents a label space of labels in a data set, and \ud835\udc49 represents a collection of separate vocabularies in the MLM model. For each triplet of visual input \ud835\udc49, text \ud835\udc42, and aspect \ud835\udc34, \ud835\udc3c denote the original input for each sample in the pattern, which contains pseudo-tags, human-built prompt prompts, and one or more [MASK] tokens. We can use the sentiment classification task to mask cluster problems in MLM and compute the probability of predicting class \ud835\udc66 \u2208 \ud835\udc4c as:\n\ud835\udc5d(\ud835\udc66|(\ud835\udc49, \ud835\udc42, \ud835\udc34)) = (\ud835\udc5d([\ud835\udc40\ud835\udc34\ud835\udc46\ud835\udc3e] = \ud835\udf11(\ud835\udc66))|\ud835\udc3c\u210e\ud835\udc66\ud835\udc4f\ud835\udc5f\ud835\udc56\ud835\udc51) = exp(\ud835\udf14\ud835\udf11(\ud835\udc66). \u210e\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58)\n\u2211 exp(\ud835\udf14\ud835\udf11(\ud835\udc59). \u210e\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58)\ud835\udc59\u2208\ud835\udc4c\nWhere \u210e\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58 indicates the hidden layer representation of [mask], \ud835\udc64\ud835\udc40 is the final Layer representations corresponding to \ud835\udc49, and \ud835\udc40 can be fine-tuned to minimize the loss in the sample (\ud835\udc49, \ud835\udc42, \ud835\udc34, \ud835\udc66) following the rule of cross-entropy. Here are some details different from PVLM\n1): Our hybrid pattern contains one or more masks for different templates. For the prediction and metric, we choose only one [MASK]\u2019s result from the probability scores generated for one or more [MASK] tokens through a fusion strategy. The alternative strategy mainly contains: choose which perform best when evaluated to follow and add the probability scores of different [MASK] tokens together directly. 2): Compared with PVLM, when implemented, we filter out the final state for only [MASK] tokens rather than all, which releases the requirement for computational power."
        },
        {
            "heading": "4. Experiments",
            "text": "To evaluate the performance of our proposed model on sentence-level MSA and aspect-level MSA, we conducted experiments and evaluations on four datasets (two for sentence-level and two for aspect-level) with a few training sets, comparing our method with the results coming from relevant approaches to multi-modal sentiment analysis.\nDatasets\n2) sentence-level (MSA): Both MVSA-S and MVSA-D come from [38]. Specifically, MVSA-D is a larger dataset, and it\nis collected from Twitter and labeled as having three polarities, too.\nFirstly, we construct the training set, test set, and validation set from each data set as 8:1:1. To satisfy the requirement of the few-shot task, we gather a few training sets from the original training set by about 1% random sampling. Otherwise, because it's too few for MVSA-S training sets, we build a 2 percent sampling to guarantee the credibility of our approach."
        },
        {
            "heading": "Setting",
            "text": "For text modality: consistent with PVLM, we utilize Bert as the pre-raining model to compare. However, Bert is just the backbone of the Bert-series pre-training models, which can employ the prompt-based method and perform better than Bert usually.\nFor visual modality, we build the presentation vectors from images using NF-ResNet. For super-parameters, we fine-tune the learning rate from 1e-5 to 1e-4, the embedding length of vision representation from 1 to 5, and the dropout of the Biaffine attention mechanism. See the table for details. Besides, the layers and hidden size of LSTM which are utilized in attention mechanisms and prompt encoder can be changed, too. More importantly, the hybrid pattern has various choices for the set of pseudo-tokens. It changes with the setting of pattern. We set the alternated length for every pseudo-tokens from 1 to 3 and keep the same length of all pseudo-tokens when conduct experiments.\nAs it has been proved that fine-tuning can lead to instability on different small sets of data, we measure average performance across different training sets gathered by random sampling using the average as a criterion for comparison, and we argue we obtain more stable and robust evaluation results through this strategy."
        },
        {
            "heading": "Baselines",
            "text": "For a sufficient comparison, we compare two groups of baselines with our proposed approaches. The first group is the\nprevious tuning, and the main prompt tuning approaches are only based on text modality:\n1) BERT [39], as the most popular and competitive pre-train model for sentiment analysis in text-based tasks, where the Masked Language Model is utilized to pre-train the bidirectional transformer to generate deep bidirectional language representation,\n2) BERT+BL [29], a variant of BERT, utilizes another transformer encoder layer at the top.\n3) Prompt Tuning (PT) [6] only uses single textual prompts, with manual templates as [\ud835\udc60]\ud835\udc47 < \ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc511 < \ud835\udc62\ud835\udc5b\ud835\udc60\ud835\udc52\ud835\udc511 > [\ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58]. [/\ud835\udc60]or pseudo templates such as \ud835\udc4e\ud835\udc60 [\ud835\udc60] \ud835\udc47 \ud835\udc3c\ud835\udc61 \ud835\udc64\ud835\udc4e\ud835\udc60 \ud835\udc4e \ud835\udc5a\ud835\udc4e\ud835\udc60\ud835\udc58]. [/\ud835\udc60]\n4) LM_BFF [4], which prompts tuning with demonstrations like GPT-3 but performs well on the few-shot GLUE\nbenchmark.\nThe second group is composed of excellent approaches to multi-modal analysis:\n1) TOMBERT, baseline on aspect-level MSA, which is based on BERT, and utilizes stack self-attention to capture the\ninteraction information from modalities [29].\n2) MMAP [40], the state-of-the-art (SOTA) approach for MABSA, which decreases the gap between image space\nrepresentation and text space representation.\n3)MVAN [25], the SOTA for sentence-level MSA, which adopts an interactive learning mechanism that uses the memory\nnetwork to model the cross-view dependencies.\n4) PVLM, the most competitive and latest method, utilizes prompt learning on text representation and converts the visual\nrepresentation into an embedding vector with text. It is worth noting that it just trains on a limited training set."
        },
        {
            "heading": "5. Main result",
            "text": ""
        },
        {
            "heading": "Comparison experiment",
            "text": "1) The prompt-based models, PT and LM-BFF, outperform the fine-tuned-based models BERT and BERT-BL in most datasets with little training. This is consistent with previous studies: in NSP-BERT, the use of prompts in few-shot learning significantly improved the results; even with zero sets for training, it performed best on almost NLP tasks. This shows that the use of prompt-based methods for FSA in text mode is reasonable. We speculate that this is mainly because prompting is a natural language way to learn and understand the original input, which makes the prompt-based method less dependent on the training sample.\n2) In multimodal, MVAN and MMAP apparently underperform PVLM at the aspect-level and sentence level. Different hard templates make PVLM work best on multiple data sets, but not always. PVLM first introduces prompts into text and image bimodality and proves that prompt-based methods are more effective than SOTA at both sentence and aspect levels separately. This further demonstrates that our prompt-based strategy is suited to few-shot multi-modal sentiment analysis (FMSA). Unfortunately, previous research has shown that in NLP, it is difficult to build a fixed template that performs best in all datasets. As we can see from the results, PVLM does not work best in Twitter 17 when using fixed template 1. This proves that the problem of the fixed template itself still exists under the premise of two-mode, which suggests that a single fixed prompt is not the best solution. This is the basis of our proposed method based on combining the prompts, replacing the single hand-crafted prompt.\n3) Our proposed model gets an overall outperformance on the datasets for evaluation. Compared with the best performance of PVLM for accuracy in the Twitter-17, our model still outperforms PLVM by 1.5%. In the rest of the experiments, the best improvement was not more than 2\u20133%. For the macro-f1, the apparent improvements, from 4% to 8%, perform in the majority of cases. When compared with the single learnable prompt of PLVM, it's mainly due to the fact that our HPM method merges the artificially crafted prompt template with the learnable prompt template, as our method has gained not only the pseudo token but also the fixed prompt template since initialization, which means that both the learnable sequence of prompts and the PrLM model can learn explicit prompt representation from the fixed prompt template to further enhance the performance when the training set is small. Otherwise, different from the p-tuning used by PVLM, we introduce SDPA and biaffine attention to learn interactive information and the underlying dependency syntax representation among learnable prompts. Compared with the hard template method, the primary reason for apparent outperformance is conjectured to be the learnability and adaptability of learnable prompts. Wrapped with our multi-attention-based prompt encoder as an inference unit, the proposed model gathers the syntax and semantic information clearly useful for the reorganization of emotions.\n4) We have an interesting discovery that accuracy seems to be unable to be used to measure the effectiveness of the model individually. Sometimes accuracy reaches a relatively high level, even higher than our report, even though macro-f1 is apparently low. Throughout the evaluation process, we pursue the balance of both metrics. Additionally, it can be seen that our model improved more significantly on macrof1 than on accuracy because, in most cases of our experiment, macrof1 still improved significantly with a relatively constant accuracy result."
        },
        {
            "heading": "Ablation study",
            "text": "We design ablation experiments among different datasets for both aspect-level tasks and sentence-level tasks to demonstrate every module's necessity. And list the main results in Table $$. Firstly, we remove the biaffine attention (w/o BA), then we further remove the SDPA (w/o SDPA), which means that our reference encoder is just like p-tuning' s, and finally, to evaluate the helpfulness of hybrid hard templates and learnable prompts, we remove the learnable prompt (w/o LP) and another hard template (without DP), respectively. When implementing, we set the pseudo-tokens to be empty and set the length of the pseudo-tokens 0 for w/o LP. The removal of the different modules makes the model's performance ineffective, which indicates that these parts are necessary for our model.\nFor BA and SDPA ,the difference between two methods demonstrate the effectiveness of this module ,for DP and LP, which are separately removed ,the necessity of them should be shown between w/o SDPA ,which means remove all attention mechanisms.\nFrom the table we find that: On the one hand, biaffine attention and SDPA have a positive effect on our model, which shows that two attention mechanisms are effective and indispensable for reference encoders to optimize p-tuning in multimodal small sample tasks (FMSA and FMABSA). On the other hand, the simple combined double templates contribute little improvement but a little adverse reduction on macro-f1, which means that a simple mix of two hard templates without learnable prompt optimization may be inapposite.\nAn overall improvement in the performance using the proposed model can be apparently observed as it successfully uses a combination of continuous template and discrete prompt and we not only enhance the interactive information among modalities by Self-attention and learnable tokens but gather the syntax knowledge by Biaffine attention. In the framework of our model, one or more discrete prompts and learnable prompts are all utilized in a hybrid pattern, whose outputs are fused into natural inputs in each batch for enhancing the syntactic representation of texts. Thus, our proposed framework makes the input of the PLM model richer than the original text. It contains not only discrete prompts used in the PET model but also learnable pseudo-tags used in the p-tuning series."
        },
        {
            "heading": "6. Further analysis",
            "text": "Compare with PVLM in MVSA-single for splits.\nConsidering the size of the MVSA-single dataset is too small, to ensure the credibility of the experimental results, a 2% shot was used to compare with PVLM and our method. Since the MVSA-S dataset is seriously imbalanced, we utilize the weighted-f1 together as metrics. As the table demonstrates, we achieve an obvious improvement over all metrics and settings.\nDoes image embedding have positive effects?\nAlthough we achieve an overall improvement, there are still some details to be considered. First of all, we need to verify that the embedding of image modes is necessary and effective. Although prompt tuning has been used previously to represent text modes, it is necessary to test for redundancy in embedding images for our model."
        },
        {
            "heading": "In which modality do learnable prompts work?",
            "text": "To further explore the effect of learnable prompts on the text and visual modalities, we conducted patterns working on the two modalities separately. The test results from Figure $$$ show that the experiment on single mode underperforms that on both, but the performance is still better than that using a simple hard template. It's mainly because our reasoner learns useful cross-modal representation information and narrows the gap between the modalities rather than just depending on the output from a single modality.\nHow to fuse the attentions with BI-LSTM efficiently?\nTo further explore the appropriate fusion method for the attention mechanisms, we organized experiments on sentence level and aspect-level, respectively. The results from figure $$ show that the best choice is \"Biaffine-SDPA\", which means using the Biaffine attention to process the linear combination of SDPA output and the original hidden states of BI-LSTM (followed by MLP merging the result of Biaffine attention with the linear combination above mentioned together). \"Biaffine +SDPA\" denotes the method of directly feeding the hidden state of BI-LSTM into SDPA and Biaffine attention alone. It's mainly because, compared with BI-LSTM's hidden states, the output of SDPA contains extra semantic representation and interaction information, which is helpful to biaffine attention.\nIn summary, in this chapter, we analyze how our method works between modalities and how to fuse the attention mechanisms better. Compared with the experimental results in single mode, the proposed method is necessary and effective. Through the comparison, we find that the attention mechanism is helpful in integrating semantic information and learning syntax information. A combination of these two attention mechanisms is a better choice than individually using them."
        },
        {
            "heading": "7. Conclusion",
            "text": "In this paper, we first propose an inference unit that utilizes fixed templates and learnable prompts together through an\ninference unit optimized by multi-attention mechanisms. The model applies biaffine attention and SDPA attention in the inference unit to gather syntax and semantic information among learnable tokens, which not only enhances the material for referencing but also achieves sufficient interaction among tokens for two modalities. Moreover, to sufficiently utilize the prompt methods, we conduct patterns for the combination of hard templates and learnable prompt tokens, so our proposed modal gathers not only learnable representation but also natural fixed templates, which are helpful for understanding. Then comparison experiments are conducted on two tasks, followed by the ablation study, the result of which verifies the efficacy of our method and the necessity of the attention mechanisms and learnable prompt method for the hybrid prompt strategy. We further explore and discover how our method benefits from cross-modal interaction instead of a separate modality. We believe our work can inspire creativity in prompt learning and few-shot multimodal sentiment analysis in the future."
        },
        {
            "heading": "8. Acknowledgement",
            "text": "The study was funded by the National Natural Science Foundation of China (Grant Nos. 61672144). Zikai Zhou and\nHaisong Feng contributed equally to this work. Correspondence should be addressed to Baiyou Qiao."
        },
        {
            "heading": "9. Reference",
            "text": "[1] Dong Zhang, Shoushan Li, Qiaoming Zhu, and Guodong Zhou, \"Effective sentiment relevant word selection for multimodal sentiment analysis in spoken language,\" in ACM MM, 2019, pp. 148\u2013156. [2] Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li, Shoushan Li, Min Zhang, and Guodong Zhou, \"Joint multi-modal\naspect sentiment analysis with auxiliary cross-modal relation detection,\" in EMNLP, 2021, pp. 4395\u20134405.\n[3] Quoc-Tuan Truong and Hady W. Lauw, \"Vistanet: A Visual Aspect Attention Network for Multimodal Sentiment\nAnalysis,\" in AAAI, 2019, pp. 305\u2013312.\n[4] Tianyu Gao, Adam Fisch, and Danqi Chen, \"Making pre-trained language models better few-shot learners,\" in\nACL/IJCNLP, 2021, pp. 3816\u20133830\n[5] Timo Schick and Hinrich Sch\u00fctze 2021. \"It\u2019s Not Just Size That Matters: Small Language Models Are Also Few-Shot\nLearners\". In NAACL, 2021, pp. 2339\u20132352,\n[6] Yang Yu and Dong Zhang, \"Few-shot multi-modal sentiment analysis with prompt-based vision-aware language\nmodeling,\" in ICME, 2022, pp. 1\u20136.\n[7] Liu B. Sentiment analysis: mining opinions, sentiments, and emotions Cambridge University Press, 2015. [8] Liu B. Sentiment analysis and opinion mining (introduction and survey), Morgan & Claypool, May 2012. [9] Pang B. and Lee L., Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2008,\n2(1\u20132): pp. 1\u2013135\n[10] K. Schouten and F. Frasincar, Survey on Aspect-Level Sentiment Analysis, IEEE Trans. Knowl. Data Eng., vol. 28,\nno. 3, pp. 813\u2013830, 2016.\n[11] Y. Ding, J. Yu, and J. Jiang, Recurrent neural networks with auxiliary labels for cross-domain opinion target\nextraction, in AAAI, 2017, pp. 3436\u20133442.\n[12] H. Xu, B. Liu, L. Shu, and P. S. Yu, Double embeddings and CNN-based sequence labeling for aspect extraction, in\nACL, 2018, pp. 592\u2013598.\n[13] W. Xue and T. Li, Aspect-based sentiment analysis with gated convolutional networks, in ACL, 2018, pp. 2514\u2013\n2523.\n[14] J. Devlin, M. Chang, K. Lee, and K. Toutanova, BERT pre-training of deep bidirectional transformers for language\nunderstanding, in NAACL-HLT, 2019, pp. 4171\u20134186.\n[15] K. Li, C. Chen, X. Quan, Q. Ling, and Y. Song, Conditional Augmentation for Aspect Term Extension via Masked\nSequence-to-Sequence Generation, in ACL, 2020, pp. 7056\u20137066.\n[16] Z. Chen and T. Qian, Enhancing aspect term extraction with soft prototypes, in EMNLP, 2020, pp. 2107\u20132117. [17] S. Tulkens and A. van Cranenburgh, Embarrassingly simple unsupervised aspect extraction, in ACL, 2020, pp. 3182\u2013\n3187.\n[18] H. Wan, Y. Yang, J. Du, Y. Liu, K. Qi, and J. Z. Pan, Target-aspect-sentiment joint detection for aspect-based\nsentiment analysis, in AAAI, 2020, pp. 9122\u20139129\n[19] Y. Wang, M. Huang, X. Zhu, and L. Zhao, Attention-Based LSTM for Aspect-Level Sentiment Classification, in\nEMNLP, 2016, pp. 606\u2013615.\n[20] Y. Tian, G. Chen, and Y. Song, Aspect-based sentiment analysis with type-aware graph convolutional networks and\nlayer ensemble, in NAACL-HLT, 2021, pp. 2910\u20132922.\n[21] C. Zhang, Q. Li, and D. Song, \"Syntax-aware aspect-level sentiment classification with proximity weighted\nconvolution network,\" in SIGIR, 2019, pp. 1145\u20131148\n[22] B. Huang and K. Carley, \"Syntax-aware aspect-level sentiment classification with graph attention networks,\" in\nEMNLP-IJCNLP, 2019, pp. 5469\u201354\n[23] Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xiaojie Wang, and Eduard Hovy, 2021, Dual-Graph\nConvolutional Networks for Aspect-Based Sentiment Analysis, ACL-IJNLP, pages 6319\u20136329\n[24] Zheng Zhang, Zili Zhou, and Yanna Wang. 2022. SSEGCN: Syntactic and Semantic Enhanced Graph Convolutional\nNetwork for Aspect-Based Sentiment Analysis. NAACL, pages 4916\u20134925\n[25] Xiaocui Yang, Shi Feng, Daling Wang, and Yifei Zhang, \"Image-text multimodal emotion classification via a multi-\nview attentional network,\" IEEE Transactions on Multimedia, 2020\n[26] Jiajia Tang, Kang Li, Xuanyu Jin, Andrzej Cichocki, Qibin Zhao, and Wanzeng Kong, 2021 CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network ACL-IJCNLP, pages 5301\u20135311.\n[27] Yang Wu, Yanyan Zhao, Hao Yang, Song Chen, Bing Qin, Xiaohuan Cao, and Wenting Zhao, 2022, Sentiment\nWord-Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors, ACL 2022, pages 1397\u20131406.\n[28] Nan Xu, Wenji Mao, and Guandan Chen. 2019. Multi-interactive memory network for aspect-based multimodal\nsentiment analysis In Proceedings of AAAI 2019, pages 371\u2013378.\n[29] Jianfei Yu, Jing Jiang, Li Yang, and Rui Xia. 2020b. Improving multimodal named entity recognition via entity span\ndetection with a unified multimodal transformer In Proceedings of ACL 2020, pages 3342\u20133352\n[30] Xincheng Ju, Dong Zhang, Rong Xiao, Junhui Li, Shoushan Li, Min Zhang, and Guodong Zhou (2021) Joint Multi-\nmodal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection [EMNLP, pages 4395\u20134405]\n[31] Hosseini-Asl, Ehsan et al., \"A Generative Language Model for Few-shot Aspect-Based Sentiment Analysis.\"\nNAACL-HLT (2022).\n[32] P. Zhang, T. Chai, and Y. Xu, Adaptive prompt learning-based few-shot sentiment analysis, ArXiv abs/2205.07220 (2022). [33] Xiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng, Yunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen 2021 KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction, arXiv e-prints, page arXiv:2104.07650.\n[34] Yi Sun, Yu Zheng, Chao Hao, and Hangping Qiu. 2021. NSP-BERT: A Prompt-Based Zero-Shot Learner Through\nan Original Pre-Training Task: Next Sentence Prediction. arXiv e-prints, page arXiv:2109.03564\n[35] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh, \"Autoprompt: Eliciting\nKnowledge from Language Models with Automatically Generated Prompts,\" in EMNLP, 2020, pp. 4222\u20134235.\n[36] Tianyu Gao, Adam Fisch, and Danqi Chen, \"Making pre-trained language models better few-shot learners,\" in\nACL/IJCNLP, 2021, pp. 3816\u20133830.\n[37] Chengyu Wang, Jianing Wang, Minghui Qiu, Jun Huang, and Ming Gao, 2021 TransPrompt: Towards an Automatic\nTransferable Prompting Framework for Few-Shot Text Classification in EMNLP, pages 2792\u20132802,\n[38] Teng Niu, Shiai Zhu, Lei Pang, and Abdulmotaleb El-Saddik, \"Sentiment analysis on multi-view social data,\" in\nMMM, 2016, pp. 15\u201327.\n[39] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \"BERT: pre-training of deep bidirectional\ntransformers for language understanding,\" in NAACL-HLT, 2019, pp. 4171\u20134186\n[40] Jie Zhou, Jiabao Zhao, Jimmy Xiangji Huang, Qinmin Vivian Hu, and Liang He, \"MASAD: A large-scale dataset for\nmultimodal aspect-based sentiment analysis,\" Neurocomputing, vol.455, pp. 47\u201358."
        }
    ],
    "title": "Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment analysis",
    "year": 2023
}