{
    "abstractText": "Asset allocation is an investment strategy that aims to balance risk and reward by constantly redistributing the portfolio\u2019s assets according to certain goals, risk tolerance, and investment horizon. Unfortunately, there is no simple formula that can find the right allocation for every individual. As a result, investors may use different asset allocations\u2019 strategy to try to fulfil their financial objectives. In this work, we conduct an extensive benchmark study to determine the efficacy and reliability of a number of optimization techniques. In particular, we focus on traditional approaches based on Modern Portfolio Theory, and on machine-learning approaches based on deep reinforcement learning. We assess the model\u2019s performance under different market tendency, i.e., both bullish and bearish markets. For reproducibility, we provide the code implementation code in this repository.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ricard Durall"
        }
    ],
    "id": "SP:2a33cb16cf9481a53425ba3e8f602f5f5e9cec5a",
    "references": [
        {
            "authors": [
                "Zhidong Bai",
                "Huixia Liu",
                "Wing-Keung Wong"
            ],
            "title": "Enhancement of the applicability of markowitz\u2019s portfolio optimization by utilizing random matrix theory",
            "venue": "Mathematical Finance: An International Journal of Mathematics, Statistics and Financial Economics,",
            "year": 2009
        },
        {
            "authors": [
                "Eric Benhamou",
                "David Saltiel",
                "Jean-Jacques Ohana",
                "Jamal Atif"
            ],
            "title": "Detecting and adapting to crisis pattern with context based deep reinforcement learning",
            "venue": "In 2020 25th International Conference on Pattern Recognition (ICPR),",
            "year": 2021
        },
        {
            "authors": [
                "Fischer Black",
                "Robert Litterman"
            ],
            "title": "Global portfolio optimization",
            "venue": "Financial analysts journal,",
            "year": 1992
        },
        {
            "authors": [
                "Hans Buehler",
                "Lukas Gonon",
                "Josef Teichmann",
                "Ben Wood"
            ],
            "title": "Deep hedging",
            "venue": "Quantitative Finance,",
            "year": 2019
        },
        {
            "authors": [
                "Gaurav Chakravorty",
                "Ankit Awasthi",
                "Brandon Da Silva"
            ],
            "title": "Deep learning for global tactical asset allocation",
            "venue": "Available at SSRN",
            "year": 2018
        },
        {
            "authors": [
                "Chung-Cheng Chiu",
                "Tara N Sainath",
                "Yonghui Wu",
                "Rohit Prabhavalkar",
                "Patrick Nguyen",
                "Zhifeng Chen",
                "Anjuli Kannan",
                "Ron J Weiss",
                "Kanishka Rao",
                "Ekaterina Gonina"
            ],
            "title": "State-of-the-art speech recognition with sequence-to-sequence models",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Vijay K Chopra",
                "William T Ziemba"
            ],
            "title": "The effect of errors in means, variances, and covariances on optimal portfolio choice. In Handbook of the fundamentals of financial decision making: Part I, pages 365\u2013373",
            "venue": "World Scientific,",
            "year": 2013
        },
        {
            "authors": [
                "Yves Choueifaty",
                "Yves Coignard"
            ],
            "title": "Toward maximum diversification",
            "venue": "The Journal of Portfolio Management,",
            "year": 2008
        },
        {
            "authors": [
                "Yves Choueifaty",
                "Tristan Froidure",
                "Julien Reynier"
            ],
            "title": "Properties of the most diversified portfolio",
            "venue": "Journal of investment strategies,",
            "year": 2013
        },
        {
            "authors": [
                "Peter Christoffersen",
                "Vihang Errunza",
                "Kris Jacobs",
                "Hugues Langlois"
            ],
            "title": "Is the potential for international diversification disappearing? a dynamic copula approach",
            "venue": "The Review of financial studies,",
            "year": 2012
        },
        {
            "authors": [
                "Li Deng",
                "Geoffrey Hinton",
                "Brian Kingsbury"
            ],
            "title": "New types of deep neural network learning for speech recognition and related applications: An overview",
            "venue": "IEEE international conference on acoustics, speech and signal processing,",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Fabio D Freitas",
                "Alberto F De Souza",
                "Ailson R De Almeida"
            ],
            "title": "Prediction-based portfolio optimization model using neural networks",
            "year": 2009
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actor-critic methods",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems,",
            "year": 2014
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Robert A Haugen",
                "Nardin L Baker"
            ],
            "title": "The efficient market inefficiency of capitalization\u2013weighted stock portfolios",
            "venue": "The journal of portfolio management,",
            "year": 1991
        },
        {
            "authors": [
                "James B Heaton",
                "Nick G Polson",
                "Jan Hendrik Witte"
            ],
            "title": "Deep learning for finance: deep portfolios",
            "venue": "Applied Stochastic Models in Business and Industry,",
            "year": 2017
        },
        {
            "authors": [
                "Miquel Noguer i Alonso",
                "Sonam Srivastava"
            ],
            "title": "Deep reinforcement learning for asset allocation in us equities",
            "venue": "Technical report,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengyao Jiang",
                "Jinjun Liang"
            ],
            "title": "Cryptocurrency portfolio management with deep reinforcement learning",
            "venue": "Intelligent Systems Conference (IntelliSys),",
            "year": 2017
        },
        {
            "authors": [
                "Zhengyao Jiang",
                "Dixing Xu",
                "Jinjun Liang"
            ],
            "title": "A deep reinforcement learning framework for the financial portfolio management problem",
            "venue": "arXiv preprint arXiv:1706.10059,",
            "year": 2017
        },
        {
            "authors": [
                "Petter N Kolm",
                "Gordon Ritter"
            ],
            "title": "Modern perspectives on reinforcement learning in finance",
            "venue": "Modern Perspectives on Reinforcement Learning in Finance (September",
            "year": 2019
        },
        {
            "authors": [
                "Vijay Konda",
                "John Tsitsiklis"
            ],
            "title": "Actor-critic algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2012
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Zhipeng Liang",
                "Hao Chen",
                "Junhao Zhu",
                "Kangkang Jiang",
                "Yanran Li"
            ],
            "title": "Adversarial deep reinforcement learning in portfolio management",
            "venue": "arXiv preprint arXiv:1808.09940,",
            "year": 2018
        },
        {
            "authors": [
                "Timothy P Lillicrap",
                "Jonathan J Hunt",
                "Alexander Pritzel",
                "Nicolas Heess",
                "Tom Erez",
                "Yuval Tassa",
                "David Silver",
                "Daan Wierstra"
            ],
            "title": "Continuous control with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1509.02971,",
            "year": 2015
        },
        {
            "authors": [
                "Chi-Ming Lin",
                "Jih-Jeng Huang",
                "Mitsuo Gen",
                "Gwo-Hshiung Tzeng"
            ],
            "title": "Recurrent neural network for dynamic portfolio selection",
            "venue": "Applied Mathematics and Computation,",
            "year": 2006
        },
        {
            "authors": [
                "John Lintner"
            ],
            "title": "The valuation of risk assets and the selection of risky investments in stock portfolios and capital budgets. In Stochastic optimization models in finance, pages 131\u2013155",
            "year": 1975
        },
        {
            "authors": [
                "Rand Kwong Yew Low",
                "Robert Faff",
                "Kjersti Aas"
            ],
            "title": "Enhancing mean\u2013variance portfolio selection by modeling distributional asymmetries",
            "venue": "Journal of Economics and Business,",
            "year": 2016
        },
        {
            "authors": [
                "S\u00e9bastien Maillard",
                "Thierry Roncalli",
                "J\u00e9r\u00f4me"
            ],
            "title": "T\u00eb\u0131letche. The properties of equally weighted risk contribution portfolios",
            "venue": "The Journal of Portfolio Management,",
            "year": 2010
        },
        {
            "authors": [
                "Harry M Markowitz"
            ],
            "title": "Portfolio selection",
            "venue": "In Portfolio selection. Yale university press,",
            "year": 1968
        },
        {
            "authors": [
                "Robert C Merton"
            ],
            "title": "On estimating the expected return on the market: An exploratory investigation",
            "venue": "Journal of financial economics,",
            "year": 1980
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing atari with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1312.5602,",
            "year": 2013
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Adria Puigdomenech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Thien Hai Nguyen",
                "Kiyoaki Shirai",
                "Julien Velcin"
            ],
            "title": "Sentiment analysis on social media for stock movement prediction",
            "venue": "Expert Systems with Applications,",
            "year": 2015
        },
        {
            "authors": [
                "Seyed Taghi Akhavan Niaki",
                "Saeid Hoseinzade"
            ],
            "title": "Forecasting s&p 500 index using artificial neural networks and design of experiments",
            "venue": "Journal of Industrial Engineering International,",
            "year": 2013
        },
        {
            "authors": [
                "Samer Obeidat",
                "Daniel Shapiro",
                "Mathieu Lemay",
                "Mary Kate MacPherson",
                "Miodrag Bolic"
            ],
            "title": "Adaptive portfolio asset allocation optimization with deep learning",
            "venue": "International Journal on Advances in Intelligent Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Thierry Roncalli",
                "Guillaume Weisang"
            ],
            "title": "Risk parity portfolios with risk factors",
            "venue": "Quantitative Finance,",
            "year": 2016
        },
        {
            "authors": [
                "David E Rumelhart",
                "Geoffrey E Hinton",
                "Ronald J Williams"
            ],
            "title": "Learning internal representations by error propagation",
            "venue": "Technical report, California Univ San Diego La Jolla Inst for Cognitive Science,",
            "year": 1985
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber",
                "Sepp Hochreiter"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput,",
            "year": 1997
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "William F Sharpe"
            ],
            "title": "Capital asset prices: A theory of market equilibrium under conditions of risk",
            "venue": "The journal of finance,",
            "year": 1964
        },
        {
            "authors": [
                "William F Sharpe"
            ],
            "title": "The sharpe ratio",
            "venue": "Streetwise\u2013the Best of the Journal of Portfolio Management,",
            "year": 1998
        },
        {
            "authors": [
                "David Silver",
                "Guy Lever",
                "Nicolas Heess",
                "Thomas Degris",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Hongyang Yang",
                "Xiao-Yang Liu",
                "Shan Zhong",
                "Anwar Walid"
            ],
            "title": "Deep reinforcement learning for automated stock trading: An ensemble strategy",
            "venue": "In Proceedings of the First ACM International Conference on AI in Finance,",
            "year": 2020
        },
        {
            "authors": [
                "Yunan Ye",
                "Hengzhi Pei",
                "Boxin Wang",
                "Pin-Yu Chen",
                "Yada Zhu",
                "Ju Xiao",
                "Bo Li"
            ],
            "title": "Reinforcementlearning based portfolio management with augmented asset movement prediction states",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Asset allocation is an investment strategy that aims to balance risk and reward by constantly redistributing the portfolio\u2019s\nassets according to certain goals, risk tolerance, and investment horizon. Unfortunately, there is no simple formula that can\nfind the right allocation for every individual. As a result, investors may use different asset allocations\u2019 strategy to try to fulfil\ntheir financial objectives. In this work, we conduct an extensive benchmark study to determine the efficacy and reliability of\na number of optimization techniques. In particular, we focus on traditional approaches based on Modern Portfolio Theory,\nand on machine-learning approaches based on deep reinforcement learning. We assess the model\u2019s performance under\ndifferent market tendency, i.e., both bullish and bearish markets. For reproducibility, we provide the code implementation\ncode in this repository.\nKeywords: Asset Allocation, Portfolio Selection, Markowitz Portfolio, Deep Reinforcement Learning, Benchmarking"
        },
        {
            "heading": "1 Introduction",
            "text": "Asset allocation is an investment strategy that aims to balance risk and reward by apportioning a portfolio\u2019s assets according to an individual\u2019s financial situation, risk tolerance, investment horizon, and goals. Fixed asset allocation refers to the portfolio that remains the same until the investor, or the portfolio\u2019s manager on behalf of the investor, decides to change the portfolio. The \u201c60/40 portfolio\u201d is a well-known fixed allocation strategy that has been employed as a trusty guidepost for moderate risk investors. It essentially consists of allocating 60% of the portfolio to equities, and the other 40% to bonds and other fixed-income instruments. Another popular fixed asset allocation is the equal weight method. This strategy gives the same importance to each asset in the portfolio, independent of external events. Although fixed allocation approaches have been revered for their simplicity and reliability, the field of asset allocation still has a lot of room for improvement. The mean-variance optimization model, proposed by Markowitz (Markowitz, 1968) serves as the keystone to Modern Portfolio Theory (MPT). It is a practical method for selecting investments to maximize their overall returns within an acceptable level of risk; the Sharpe ratio (Sharpe, 1998) is one of its most employed add-ons to measure the risk-adjusted return with respect to the risk-free asset. However, Markowitz model applies historical return and volatility as a proxy for future expectations in the allocation model. Consequently, the model can make inaccurate assumptions since the returns and variances will likely not be the same in the future (Merton, 1980). (Black and Litterman, 1992) solved this issue by applying an equilibrium return, based partially on Capital Asset Pricing Model (CAPM) (Sharpe, 1964; Lintner, 1975), as a baseline\nar X\niv :2\n20 8.\n07 15\n8v 1\n[ q-\nfi n.\nPM ]\n1 4\nJu l 2\n02 2\nfor defining the expected return vector. Further alternatives are the equal volatility portfolio, that uses the same amount of volatility in every asset; minimum variance portfolio, (Haugen and Baker, 1991; Chopra and Ziemba, 2013), that provides the lowest variance among all possible portfolios of risky assets; maximum diversification portfolio (Choueifaty and Coignard, 2008; Choueifaty et al., 2013), that maximizes the ratio of weighted-average asset volatilities to portfolio volatility; maximum decorrelation portfolio (Christoffersen et al., 2012), that maximizes the diversification ratio based on the correlation matrix; and risk parity (Maillard et al., 2010; Roncalli and Weisang, 2016), that uses the concept of the Security Market Line (SML) as part of its approach; being SML a graphical representation of the CAPM. Besides the debatable assumption about the stationarity in time of the market, another drawback from Markowitz\u2019s approach appears when the amount of different assets is large enough. MPT uses the covariance to determine which assets are included in the portfolio. This statistical measure has quadratic growth with the number of asset and thus, those portfolios with rich diversity of assets will inevitably suffer from computational problems. To circumvent this issue, (Bai et al., 2009) proposed to employ the theory of the large-dimensional random matrix. Finally, another reason for poor performance of the mean-variance portfolio might be caused by the symmetry of asset returns. (Low et al., 2016) showed that it is possible to enhance Markowitz\u2019s portfolio selection by allowing distributional asymmetries.\nWith the increasing use of artificial intelligence, deep-learning approaches have achieved remarkable breakthroughs leading to state-of-the-art results in various domains such as computer vision (Krizhevsky et al., 2012; Goodfellow et al., 2014), natural language processing (Vaswani et al., 2017; Devlin et al., 2018), and speech recognition (Deng et al., 2013; Chiu et al., 2018). Such remarkable success has also sparked the interest of the finance research community. As a result, in the past years, the number of deep-learning applications for portfolio asset allocation has dramatically increased. (Lin et al., 2006) designed a dynamic portfolio selection model by incorporating the Recurrent Neural Network (RNN) (Rumelhart et al., 1985). (Freitas et al., 2009; Niaki and Hoseinzade, 2013; Nguyen et al., 2015; Heaton et al., 2017) relied on deep neural networks to model the market\u2019s behaviour so that the final solution found the optimal asset allocation. A similar approach by (Chakravorty et al., 2018) dealt with macroeconomic data in conjunction with price-volume data in a walk-forward setting. (Obeidat et al., 2018) proposed to predict future portfolio\u2019s returns using Long Short-Term Memory (LSTM) (Schmidhuber et al., 1997) neural networks, as they are a more suitable than RNNs for processing time-series (sequential data). Nonetheless, all the previous models lack interaction with the market. In other words, they cannot adapt and consequently, they underperform in non-stationary scenarios. To address this limitation, reinforcement-learning-based systems are suitable candidates. (Almahdi and Yang, 2017) introduced a recurrent-reinforcement-learning method, with a coherent risk-adjusted performance objective function, named the Calmar ratio, to obtain both buy and sell signals that updated the asset allocation weights. (Jiang and Liang, 2017) presented a Convolutional Neural Network (CNN) (LeCun et al., 1998) to dynamically optimize cryptocurrency portfolios. A follow-up work by (Jiang et al., 2017; i Alonso and Srivastava, 2020) assessed the impact of different types of layers, including CNN, LSTM and RNN. Similarly, (Liang et al., 2018) applied deep-reinforcement-learning algorithms with continuous action space to asset allocation. In particular, they investigated the model-free Deep Deterministic Policy Gradient (DDPG) (Silver et al., 2014) as well as the Proximal Policy Optimization (PPO) (Schulman et al., 2017). (Buehler et al., 2019) presented a framework to hedge a portfolio of derivatives, where the system did not depend on specific market dynamics, such as transaction costs, market impact, liquidity constraints or risk limits. (Kolm and Ritter, 2020) investigated the link between portfolio allocation and reinforcement learning, namely, showing how the latter could be used to solve intertemporal financial problems. (Yang et al., 2020) introduced an ensemble strategy that employed\nvarious deep-reinforcement schemes to learn a unified strategy that maximized the investment return. (Ye et al., 2020) proposed a framework, coined state augmented reinforcement learning, that aimed to leverage additional diverse information from alternative sources other than classical structured financial data like asset prices. Finally, driven by the Covid-19 financial crisis, (Benhamou et al., 2021) focused on models that detected extreme negative patterns, and consequently dis-investing the assets."
        },
        {
            "heading": "2 Methodology",
            "text": "First, we present the traditional approaches, where we discuss different strategies based on the Markowitz model. Then, we describe deep reinforcement learning and its paradigm for asset allocation."
        },
        {
            "heading": "2.1 Markowitz Mean-Variance Portfolio Theory",
            "text": "Modern Portfolio Theory (Markowitz, 1968) introduces a financial method, for risk-averse investors, to construct diversified portfolios that optimize their returns. Namely, the mean-variance optimization approach. This technique aims at assembling a portfolio given some pre-defined constraints. Risk and return trade-off is at the heart of such a method, and its main components are the standard deviation and expected return of the assets. While variance (derived from the standard deviation) expresses the degree of spread in the data set, by showing how spread out the returns of a specific asset are, the expected return represents the probability of the estimated return of the investments. MPT makes several key assumptions that the practitioners should be aware of before using the mean-variance optimization. The main ones are the following:\n\u2022 The risk of the portfolio is based on its volatility of returns, i.e., price fluctuations.\n\u2022 The analysis is conducted on a single-period model of investment.\n\u2022 Investors are rational, averse to risk and eager to increase consumption. As a result, the utility function is concave and increasing.\n\u2022 Investors seek either to maximize their portfolio return for a given level of risk or to minimize their risk for a given return.\nFrom a mathematical perspective, given a portfolio p with n assets, we can calculate the standard deviation as: \u03c3p = \u221a \u03c32p, (1)\nthe variance as:\n\u03c32p = n\u2211 i=1 n\u2211 j=1 \u03c9i\u03c9iConv(ri, rj) = \u03c9 T\u2126\u03c9, (2)\nand the expected return as:\nE(rp) = n\u2211 i=1 \u03c9iE(ri). (3)\nThe variable \u03c9 denotes the weights of the individual assets, and the variable rp the return of the portfolio.\nAs we mentioned before, the trade-off between the risk and the expected return will be critical when building a portfolio. Using the Markowitz model for analysing portfolios helps to discover the efficient\nfrontier, which is the combination of assets that offers the highest expected return for a defined level of risk, or the lowest risk for a given level of expected return (see Figure 1). Portfolios that lie below the efficient frontier are suboptimal, as they do not provide enough return for the level of risk. Portfolios that cluster to the right of the efficient frontier are also suboptimal because they have a higher level of risk for the defined rate of return. In this work, we are interested in two important points along the efficient frontier: minimum variance portfolio and tangency portfolio."
        },
        {
            "heading": "2.1.1 The Minimum Risk Mean-Variance Portfolio",
            "text": "The point where the hyperbola (efficient frontier) changes from convex to concave is where the minimum variance portfolio lies. This portfolio allocation has a unique solution that can be found by solving a simple quadratic optimization problem via standard Lagrange multiplier methods. The optimization problem can be formulated as:\nmin \u03c9\n1 2 \u03c9T\u2126\u03c9\nsubject to \u03c9Tr = E(rp) and \u03c9T1 = 1.\n(4)\nThe vector \u03c9 denotes the individual investments (weights of the assets) subject to the condition that the available capital is fully invested, i.e., \u03c9T1 = 1. The lower bound on the target return E(rp) is expressed by the condition \u03c9Tr = E(rp), where the vector r estimates the expected mean of the assets (E(ri))."
        },
        {
            "heading": "2.1.2 The Tangency Portfolio",
            "text": "The tangency portfolio is the asset allocation that maximizes the Sharpe ratio (Sharpe, 1998). This ratio measures the excess return earned over the risk-free rate per unit of volatility or total risk, which helps investors to better understand the return of their investment. It can be formulated as:\nSharpe ratio = E(rp) \u2212 rf\n\u03c3p (5)\nwhere rf stands for risk-free rate, i.e., the theoretical rate of return of an investment with zero risk like U.S. treasury rate. The tangency portfolio optimization can be formulated as:\nmax \u03c9\n\u03c9Tr \u2212 rf \u03c9T\u2126\u03c9\nsubject to \u03c9T1 = 1.\n(6)\nGraphically, it is the point where a straight line through the rf is tangent to the efficient frontier, in the Markowitz model space."
        },
        {
            "heading": "2.1.3 The Risk Parity Portfolio",
            "text": "Risk parity (Maillard et al., 2010; Roncalli and Weisang, 2016) is an alternative approach to the Markowitz model that focuses on the allocation of the risk instead of the capital. This method asserts that when asset allocations are adjusted to the same risk level, the portfolio can achieve a higher Sharpe ratio and thus, it can be more resistant to market downturns. To achieve that, the risk parity portfolio tries to constrain each asset to contribute equally to the portfolio overall volatility. The optimization problem can be formulated as:\nmin \u03c9\n1 2 \u03c9T\u2126\u03c9 \u2212 1 n ln(\u03c9)\nsubject to \u03c9T1 = 1. (7)"
        },
        {
            "heading": "2.2 Deep Reinforcement Learning",
            "text": "Machine learning is a branch of artificial intelligence that allows machines to learn from data, identify patterns and make decisions without being explicitly programmed for it. Reinforcement learning is an area of machine learning that focuses on training an algorithm following the cut-and-try approach. More specifically, the algorithm needs to learn to take actions that maximize the final reward in a particular situation. To that end, this algorithm (agent) evaluates a current situation (state), takes an action, and receives feedback (reward) from the environment. Positive feedback is given when the action is correct, and negative feedback otherwise. Similar to other mathematical methods, machine-learning algorithms have different flavours, each of them with its own advantages and disadvantages. Commonly, these algorithms can be divided into supervised, unsupervised and reinforcement learning. Supervised learning works with labelled data and mainly deals with regression and classification tasks. The unsupervised learning, however, employs unlabelled data and tries to determine patterns and associations within the\ndata. This technique tackles clustering and associative rule mining problems. Finally, reinforcement learning uses a learning agent to interact with the environment based on an action-reward system through a trade-off between exploitation and exploration. The main goal of this type of learning is to find the best sequence of decisions that maximizes the long-term reward. Due to the absence of training data, reinforcement-learning algorithms are bound to learn from their experience. In particular, they learn how to act best through many attempts and failures."
        },
        {
            "heading": "2.2.1 Actor Critic Optimization",
            "text": "Actor-Critic (AC) (Konda and Tsitsiklis, 1999) is a temporal difference method that has two separated memory structures to explicitly represent the policy and the value function. On the one hand, the policy structure is known as the actor because it decides which action should be taken. It essentially controls how the agent behaves by learning the optimal policy \u03c0 (policy-based). On the other hand, the estimated value function is known as the critic. It evaluates the action made by the actor by computing the value function (value-based), which can be the action-value q\u03c0 or state-value v\u03c0. These two structures participate in an optimization game, where they both get better in their own role as the time passes. The outcome is that the overall method (system) achieves superior results than systems based on solely one structure.\nThe Advantage Actor-Critic (A2C) (Mnih et al., 2016) algorithm is a variation of AC, where the algorithm specifically uses estimates of the advantage function for its bootstrapping, i.e., to update a value based on some estimates and not on some exact values. The function of the advantage function is to determine how good an action compared to average action for a specific state is. By doing so, the variance between the old and the new policies is reduced, and consequently the stability of the reinforcement-learning algorithm improves. The advantage function can be defined as:\na\u03c0(s, a) = q\u03c0(s, a) \u2212 v\u03c0(s). (8)\nDeep Deterministic Policy Gradient (DDPG)(Lillicrap et al., 2015) is another AC method. It combines ideas from Deterministic Policy Gradient (DPG) (Silver et al., 2014) and Deep Q-Network (DQN) (Mnih et al., 2013). Namely, DDPG uses a critic that learns from a temporal loss and an actor that learns using policy gradient. However, DDPG is an off-policy method. This means that it can sample batches from large experience buffers, making this approach more sample-efficient, at least at training.\nAlthough DDPG can provide excellent results, it is frequently brittle with respect to hyperparameters and other kinds of tiresome fine-tuning dependencies. Furthermore, a common failure of DDPG is that this algorithm continuously overestimates the q\u03c0 values of the critic network, and it can eventually lead to the agent falling into a local optimum or to a catastrophic forgetting. Twin Delayed DDPG (TD3) (Fujimoto et al., 2018) is an algorithm that addresses this issue by introducing three novel critical tricks: (1) employing two critic networks, (2) delaying the updates of the actor and (3) adding noise to regularize the target action.\nSoft Actor-Critic (SAC) (Haarnoja et al., 2018) is an algorithm that optimizes a stochastic policy in an off-policy fashion, forming a bridge between stochastic policy optimization and DDPG-based approaches. The biggest feature of SAC is its modified objective function, where instead of only seeking to maximize the lifetime rewards, the algorithm also tries to maximize the entropy of the policy. We can think of entropy as how unpredictable a random variable is. For instance, if a random variable always takes a single value, then it has zero entropy. As a rule of thumb, we want a high entropy in our policy to\nexplicitly encourage exploration, by assigning equal probabilities to actions that have same or nearly equal q\u03c0 values, and to ensure that it does not collapse.\nAnother alternative to modify policy gradient methods using experiences from old versions of the policy is through the importance sampling technique. This technique weights samples based on the difference between the action probability distributions, given by the current and old policies. Trust region policy optimization (TRPO) (Schulman et al., 2015) is one policy gradient method that guarantees that the new update\u2019s policy is not far away from the old policy, or at least, that the new policy is within the trust region of the old policy. However, in practice, TRPO is a relatively complicated algorithm to implement and not always a suitable candidate. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is a follow-up work that simplifies the algorithm. More specifically, PPO is a first-order optimization that defines the probability ratio between the new and old policies. Instead of adding complex constraints, e.g., Kullback-Leibler, PPO imposes a policy ratio to stay within a small interval around 1. It can be viewed as a combination of A2C (having multiple workers) and TRPO (using trust region to improve the actor)."
        },
        {
            "heading": "2.2.2 Deep Reinforcement Learning for Asset Allocation",
            "text": "Portfolio management can be modelled as a deep-reinforcement-learning (DRL) problem, where deep neural networks are used to optimize the portfolio risk-adjusted returns for a given set of assets. To guarantee convergence, these neural networks need to understand the market\u2019s behaviour when reallocating the assets. The DRL framework for portfolio management can be built as follows:\nAgent: A deep neural network is proposed as the agent, whose goal is to find an optimal function (either policy- or value-based) that learns actions that maximize the reward.\nActions: They are the outputs of the network for a given period, and they represent the final portfolio weights, i.e., the percentage of each asset within the portfolio.\nState: It is the input used to feed the network and describes the current situation of the stock market via financial indicators. These indicators are made of a tensor of features, which includes the current balance of our portfolio, the stock prices, and the owned assets.\nEnvironment: The stock market acts as the environment. It receives the actions taken by the agent and sends back the reward to the agent.\nReward : The difference between the previous and the current portfolio values is our reward (differential returns).\nIn a DRL approach, usually the agent uses the stochastic gradient descent, based on a market snapshot (state) and on the reward (cost-adjusted returns), to learn the action (weights) that leads to the optimal portfolio allocation for a given point in time. In Figure 2, we can see the connection among the different elements of a DRL framework. Finally, we need to make a few assumptions for a correct interpretability of our results:\n\u2022 It is possible to trade at the market any time.\n\u2022 Our transactions do not to affect the market price of the assets."
        },
        {
            "heading": "3 Experiments",
            "text": "In this work, we address the task of asset allocation. Given a portfolio with a set of n assets, we aim to benchmark several allocation optimization methods, including traditional and DRL ones. To that end, we conduct a comprehensive evaluation, where we investigate the robustness of such approaches for different market\u2019s conditions as well as different time frequencies of reallocation. In particular, we study bull and bear market scenarios, on a day-trading setup."
        },
        {
            "heading": "3.1 Experimental Settings",
            "text": "When the exchanges close, the last trading price of the stock is recorded as the closing price of the share. Nonetheless, it is not always reliable since it might not provide an accurate picture of the true value. Therefore, the closing price is adjusted considering factors such as dividends, stock splits, and new stock issues, originating the adjusted close price. In all our experiments, we use adjusted close price as input.\nFor each market scenario, we select a set of 8 stocks, from well-known companies, that follow the market tendency that we target to study. We use data from a period of seven years, where 80% of the dataset goes into the training set, and the remaining 20% of the dataset goes into the testing set. Notice that for the traditional methods, where no learning is involved, we do not have such a split. Furthermore, for these methods, we set a windows size of 50 and no transaction costs are involved. On the contrary, for the DRL-based counterpart, we use a windows size of 1 and their costs are set to 0.1% for each trade total value.\nWe evaluate the performance of 9 different algorithms: tangency portfolio, minimum variance portfolio, risk parity, equal weight, A2C, PPO, DDPG, SAC and TD3. While the first four approaches are deterministic, the last five approaches, based on the Actor-Critic algorithm, are not due to the (stochastic) weight initialization process. Thus, we report results from 10 independent runs to obtain uncertainty boundaries."
        },
        {
            "heading": "3.2 Bull Market Scenario",
            "text": "Bull market occurs when investment prices rise for a sustained period of time. Propelled by the thriving economies and low unemployment rate, investors are eager to buy or hold onto securities. The result is a buyer\u2019s market. Bull markets tend to last for months or even years. Famous bull markets were the 1970s economic recovery as well as the pre-global financial crisis bull market.\nOur first case of study focuses on 8 stocks that follow a bullish trend. These securities belong to the following companies: Apple (APP), General Electric (GE), JPMorgan Chase (JPM), Microsoft (MSFT), Vodafone Group (VOD), Nike (NKE), Nvidia (NVDA) and 3M (MMM). We use adjusted close prices from the 1st of January 2010 to the 1st of January 2017 (see Figure 3).\nFigure 4 plots the cumulative returns of the different asset allocation methodologies proposed in the current work. While top Figure 4 shows the run that achieves the best performances, bottom shows the run that obtains the worst. Furthermore, Table 1 displays other metrics that help to dissect the methods, to better understand their outcomes. From all these results, we can derive two main observations: (1) DRL-based models outperform all the traditional approaches, except for PPO, and (2) DRL-based models seem to be unstable, i.e., weight initialization-dependent, and therefore, not as reliable as traditional approaches. As a result, a hybrid combination between traditional and DRL-based could be a suitable solution since it could find an optimal trade-off: high returns plus stability in the long run."
        },
        {
            "heading": "3.3 Bear Market Scenario",
            "text": "Bear market occurs when stock prices fall 20% or more for a sustained period of time. Triggered by periods of economic slowdown and higher unemployment rate, investors are reluctant to buy, often fleeing for the safety of cash or fixed-income securities. The result is a seller\u2019s market. Bear markets can last from a few weeks to several years. The first and most famous bear market was the great depression. The dot com bubble in 2000 and the housing crisis of 2007\u20132008 are other examples.\nOur second case of study focuses on 8 stocks that follow a bearish trend. These securities belong to the following companies: PepsiCo (PEP), AAR Corp (AIR), British Petroleum (BP), BASF (BAS), Bayer AG (BAYN), Lufthansa (LHA), The Walt Disney Company (DIS) and The Coca-Cola Company (KO). We use adjusted close prices from the 1st of January 2003 to the 1st of January 2010 (see Figure 5).\nFigure 6 plots the cumulative returns of the different asset allocation methodologies proposed in the current work. While top 6 shows the run that achieves the best performances, bottom shows the run that obtains the worst. Furthermore, Table 2 displays other metrics that help to dissect the methods, to better understand their outcomes. In Figure 6, we can observe that although all methods, except for minimum volatility, end up with positive returns, only tangent portfolio is able to deliver positive returns during (almost) the whole testing period. Moreover, in this bear setup, the gap between traditional and DRL-based approaches is much reduced, raising concerns of the utility of DRL in declining markets, with the exception of the PPO algorithm."
        },
        {
            "heading": "3.4 Conclusions",
            "text": "In this work, we explore the potential of using optimization algorithms for the asset allocation task. To that end, we conduct an extensive benchmark study on 9 different algorithms: tangency portfolio, minimum variance portfolio, risk parity, equal weight, A2C, PPO, DDPG, SAC and TD3. We evaluate their efficacy and reliability on different market conditions, i.e., bullish and bearish tendencies.\nTraditional approaches, based on Markowitz portfolio, do not require any fitting optimization process (training) since they do not employ learnable parameters. In our experiments, these models show stable results, achieving competitive performance on both market scenarios. Among them, tangency portfolio stands out as this method almost always provides the highest annual and cumulative returns as well as the best Sharpe and Calmar ratios. Risk parity obtains slightly inferior results, except for a few specific cases, where it outperforms the rest. As for minimum variance portfolio, it excels at keeping a low annual volatility (fulfilling this extra requirement), but in return the other financial metrics are negatively affected. Finally, equal weight offers surprisingly decent outcomes, taking into consideration that no optimization is involved. Although all these traditional proposals can successfully deal with stable market environment, they all are sensitive to outliers and abrupt market\u2019s changes. Therefore, in high volatile markets, traditional approaches are not well suited. A second drawback arises from their specificity. These algorithms were conceived for specific financial scenarios, and thus, they are rigid tools based solely on asset returns. In case we wanted to consider other relevant technical indicators such as moving average convergence/divergence, these methods would not be our best candidate.\nOn the other hand, DRL results are more difficult to interpret. While it is true that deep-learning approaches tend to have runs (random seed settings) that surpass their traditional counterparts, they also provide runs with weaker performance. For example, PPO and SAC achieve overall the best results in both bullish and bearish markets, respectively, nonetheless, none of them can even beat the equal weight strategy when having poor runs. The reason for such fluctuations is the training process of these models. In other words, at training time when the algorithms optimize their agents to learn to make a sequence of decisions through the \u201ctrial and error\u201d process, there are involved stochastic events. As a result, independent runs might eventually lead to different optimal or suboptimal solutions. To cope with this flaw, one could use more data that would indirectly help with convergence and stabilize the training process. Another complementary solution would be to feed the models with more technical indicators or larger input-data windows, leveraging in this manner the intrinsic flexibility of these neural network architectures.\nWe expect in the upcoming years to see a lot of exciting new research connecting even more to the fields of finance (asset allocation) and deep reinforcement learning. Namely, we believe that novel self-attention implementations, such as transformers, could further boost the model\u2019s performance. We would also expect that solutions, trained on synthetic data as a proxy, could help learning better features to lead to more reliable and stable results. Finally, future work that includes ethics and social aspects, like sustainability, needs to be developed. While it is true that some assets are already following certain ethical codes, to the best of our knowledge, there is no DRL-based approach that incorporates such behaviour into its internal running."
        },
        {
            "heading": "4 Appendix",
            "text": "In this subsection, we visualize the in-depth evolution of the weight allocation for each of the previous scenarios. This helps us to gain insight of the different running. Additionally, we plot the statistics (mean and standard deviation) of the DRL-based models for 10 independent runs.\nBy analysing the evolution of the different weight allocations, we can draw several conclusions. First, in general, all the DRL-based models avoid dramatic reallocation, changing the portfolio\u2019s configuration in a stepwise fashion. In other words, there are no big differences in the assets\u2019 allocation between two consecutive portfolios. This is, however, an expected behaviour since these models are trained with a transaction cost that penalizes reallocation. On the other hand, traditional approaches, except for the equal weight, display much dynamic reallocation as no transaction costs are included in their algorithms. Second, in the bull market, all DRL-based proposals have substantial variations among different runs, which only some of them result into optimal performance. Nonetheless, we notice that this is not the case for the bear scenario, since the results show a smaller variation, and thus, a more reliable and stable running. Finally, it is interesting to observe that often the worst runs are associated with \u201cmono\u201d asset portfolios (see Figure 8, Figure 11 and Figure 12), i.e., allocation with only one asset."
        }
    ],
    "title": "Asset Allocation: From Markowitz to Deep Reinforcement Learning",
    "year": 2022
}