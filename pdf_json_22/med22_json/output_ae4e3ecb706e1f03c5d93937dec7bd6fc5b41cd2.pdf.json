{
    "abstractText": "GABRIELA AUGUSTINOV ( \uf0e0 gabriela.augustinov@uni-luebeck.de ) Institute of Medical Informatics, University of L\u00fcbeck, Germany https://orcid.org/0000-0001-94437825 MUHAMMAD ADEEL NISAR FR\u00c9D\u00c9RIC LI Institute of Medical Informatics, University of L\u00fcbeck, Germany AMIR TABATABAEI MARCIN GRZEGORZEK Institute of Medical Informatics, University of L\u00fcbeck, Germany KEYWAN SOHRABI Sebastian Fudickar Institute of Medical Informatics, University of L\u00fcbeck, Germany https://orcid.org/0000-0002-35535131",
    "authors": [
        {
            "affiliations": [],
            "name": "GABRIELA AUGUSTINOV"
        },
        {
            "affiliations": [],
            "name": "ADEEL NISAR"
        },
        {
            "affiliations": [],
            "name": "FR\u00c9D\u00c9RIC LI"
        },
        {
            "affiliations": [],
            "name": "AMIR TABATABAEI"
        },
        {
            "affiliations": [],
            "name": "MARCIN GRZEGORZEK"
        },
        {
            "affiliations": [],
            "name": "KEYWAN SOHRABI"
        }
    ],
    "id": "SP:e1a777b78a9b0115cf842dc9125d172655f8c2f2",
    "references": [
        {
            "authors": [
                "Saedeh Abbaspour",
                "Faranak Fotouhi",
                "Ali Sedaghatbaf",
                "Hossein Fotouhi",
                "Maryam Vahabi",
                "Maria Linden"
            ],
            "title": "A Comparative Analysis of Hybrid Deep Learning Models for Human Activity Recognition",
            "venue": "Sensors 20,",
            "year": 2020
        },
        {
            "authors": [
                "Dzmitry Bahdanau",
                "Kyunghyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Neural Machine Translation by Jointly Learning to Align and Translate",
            "venue": "CoRR abs/1409.0473",
            "year": 2015
        },
        {
            "authors": [
                "Antonio Bevilacqua",
                "Kyle MacDonald",
                "Aamina Rangarej",
                "Venessa Widjaya",
                "Brian Caulfield",
                "Tahar Kechadi"
            ],
            "title": "Human Activity Recognition with Convolutional Neural Networks. In Machine Learning and Knowledge Discovery in Databases. Springer International Publishing, 541\u015b552",
            "year": 2019
        },
        {
            "authors": [
                "Diane Cook",
                "Kyle D. Feuz",
                "Narayanan C. Krishnan"
            ],
            "title": "Transfer learning for activity recognition: a survey",
            "venue": "Knowledge and Information Systems 36,",
            "year": 2013
        },
        {
            "authors": [
                "L. Minh Dang",
                "Kyungbok Min",
                "Hanxiang Wang",
                "Md. Jalil Piran",
                "Cheol Hee Lee",
                "Hyeonjoon Moon"
            ],
            "title": "Sensor-based and vision-based human activity recognition: A comprehensive survey",
            "venue": "Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "In International Conference on Learning Representations. https://openreview.net/forum?id=YicbFdNTTy",
            "year": 2021
        },
        {
            "authors": [
                "Alex Graves",
                "Abdel-rahman Mohamed",
                "Geoffrey Hinton"
            ],
            "title": "Speech Recognition with Deep Recurrent Neural Networks. https://doi.org/10",
            "year": 2013
        },
        {
            "authors": [
                "Athanasios Lentzas",
                "Dimitris Vrakas"
            ],
            "title": "Non-intrusive human activity recognition and abnormal behavior detection on elderly people: a review",
            "venue": "Artificial Intelligence Review 53,",
            "year": 2019
        },
        {
            "authors": [
                "Fr\u00e9d\u00e9ric Li",
                "Kimiaki Shirahama",
                "Muhammad Adeel Nisar",
                "Xinyu Huang",
                "Marcin Grzegorzek"
            ],
            "title": "Deep Transfer Learning for Time Series Data Based on Sensor Modality Classification",
            "venue": "Sensors 20,",
            "year": 2020
        },
        {
            "authors": [
                "Thang Luong",
                "Hieu Pham",
                "Christopher D. Manning"
            ],
            "title": "Effective Approaches to Attention-based Neural Machine Translation",
            "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics",
            "year": 2015
        },
        {
            "authors": [
                "Iveta Dirgov\u00e1 Lupt\u00e1kov\u00e1",
                "Martin Kubov\u010d\u00edk",
                "Ji\u0159\u00ed"
            ],
            "title": "Wearable Sensor-Based Human Activity Recognition with Transformer Model",
            "venue": "Pospi\u0301chal",
            "year": 2022
        },
        {
            "authors": [
                "SaifMahmud",
                "Tanjid HasanM. Tonmoy",
                "Kishor K. Bhaumik",
                "Mahbubur A.K. Rahman",
                "AshrafulM. Amin",
                "Mohammad Shoyaib",
                "MuhammadA.H. Khan",
                "Amin A. Ali"
            ],
            "title": "Human Activity Recognition from Wearable Sensor Data Using Self-Attention. https://doi.org/10.48550/ARXIV.2003.09018 Manuscript submitted to ACM 14 Augustinov, Nisar",
            "year": 2020
        },
        {
            "authors": [
                "Vittorio Mazzia",
                "Simone Angarano",
                "Francesco Salvetti",
                "Federico Angelini",
                "Marcello Chiaberge"
            ],
            "title": "Action Transformer: A self-attention model for short-time pose-based human action recognition",
            "venue": "Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Michelle E. Mlinac",
                "Michelle C. Feng"
            ],
            "title": "Assessment of Activities of Daily Living, Self-Care, and Independence",
            "venue": "Archives of Clinical Neuropsychology 31,",
            "year": 2016
        },
        {
            "authors": [
                "Pradeep M. Muragundi",
                "Ashwin M. Tumkur",
                "Akash N. Naik",
                "Rohit K. Shetty"
            ],
            "title": "Health-related Quality of Life Measurement",
            "venue": "Journal of Young Pharmacists",
            "year": 2012
        },
        {
            "authors": [
                "Muhammad Adeel Nisar",
                "Kimiaki Shirahama",
                "Fr\u00e9d\u00e9ric Li",
                "Xinyu Huang",
                "Marcin Grzegorzek"
            ],
            "title": "Rank Pooling Approach for Wearable Sensor-Based ADLs Recognition",
            "venue": "Sensors 20,",
            "year": 2020
        },
        {
            "authors": [
                "Theodoros Ntakouris"
            ],
            "title": "Timeseries classification with a Transformer model",
            "venue": "Retrieved April",
            "year": 2021
        },
        {
            "authors": [
                "Marcel Post"
            ],
            "title": "Definitions of Quality of Life: What Has Happened and How to Move On",
            "venue": "Topics in Spinal Cord Injury Rehabilitation 20,",
            "year": 2014
        },
        {
            "authors": [
                "Md Zia Uddin",
                "Ahmet Soylu"
            ],
            "title": "Human activity recognition using wearable sensors, discriminant analysis, and long short-term memory-based neural structured learning",
            "venue": "Scientific Reports 11,",
            "year": 2021
        },
        {
            "authors": [
                "Franz A. Van-Horenbeke",
                "Angelika Peer"
            ],
            "title": "Activity, Plan, and Goal Recognition: A Review",
            "venue": "Frontiers in Robotics and AI",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is All You Need",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long",
            "year": 2017
        },
        {
            "authors": [
                "Michalis Vrigkas",
                "Christophoros Nikou",
                "Ioannis A. Kakadiaris"
            ],
            "title": "A Review of Human Activity Recognition Methods. Frontiers in Robotics and AI 2 (Nov",
            "year": 2015
        },
        {
            "authors": [
                "Joshua M. Wiener",
                "Raymond J. Hanley",
                "Robert Clark",
                "Joan F. Van Nostrand"
            ],
            "title": "Measuring the Activities of Daily Living: Comparisons Across National Surveys",
            "venue": "Journal of Gerontology 45,",
            "year": 1990
        },
        {
            "authors": [
                "Ming Zeng",
                "Haoxiang Gao",
                "Tong Yu",
                "Ole J. Mengshoel",
                "Helge Langseth",
                "Ian Lane",
                "Xiaobing Liu"
            ],
            "title": "Understanding and improving recurrent networks for human activity recognition by continuous attention",
            "venue": "In Proceedings of the 2018 ACM International Symposium on Wearable Computers",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Transformer-Based Recognition of Activities of Daily Living from Wearable Sensor Data GABRIELA AUGUSTINOV\u00a0 (  gabriela.augustinov@uni-luebeck.de )\nInstitute of Medical Informatics, University of L\u00fcbeck, Germany https://orcid.org/0000-0001-94437825 MUHAMMAD ADEEL NISAR\u00a0 FR\u00c9D\u00c9RIC LI\u00a0\nInstitute of Medical Informatics, University of L\u00fcbeck, Germany AMIR TABATABAEI\u00a0 MARCIN GRZEGORZEK\u00a0\nInstitute of Medical Informatics, University of L\u00fcbeck, Germany KEYWAN SOHRABI\u00a0 Sebastian Fudickar\u00a0\nInstitute of Medical Informatics, University of L\u00fcbeck, Germany https://orcid.org/0000-0002-35535131\nResearch Article\nKeywords: transformer, human activity recognition, atomic activities, activities of daily living, attention, lstm, neural networks\nPosted Date: September 2nd, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-2015249/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License. \u00a0 Read Full License\nTransformer-Based Recognition of Activities of Daily Living from Wearable\nSensor Data\nGABRIELA AUGUSTINOV, Institute of Medical Informatics, University of L\u00fcbeck, Germany\nMUHAMMAD ADEEL NISAR, Department of IT, University of the Punjab, Pakistan\nFR\u00c9D\u00c9RIC LI, Institute of Medical Informatics, University of L\u00fcbeck, Germany\nAMIR TABATABAEI, Institute of Medical Informatics, Justus-Liebig-University Giessen, Germany\nMARCIN GRZEGORZEK, Institute of Medical Informatics, University of L\u00fcbeck, Germany and Department of\nKnowledge Engineering, University of Economics, Poland\nKEYWAN SOHRABI, Faculty of Health Sciences, THM University of Applied Sciences Giessen, Germany\nSEBASTIAN FUDICKAR, Institute of Medical Informatics, University of L\u00fcbeck, Germany\nSmart support systems for the recognition of Activities of Daily Living (ADLs) can help elderly people live independently for longer improving their standard of living. Many machine learning approaches have been proposed lately for Human Activity Recognition (HAR), including elaborated networks that contain convolutional, recurrent, and attentive layers. The ubiquity of wearable devices has provided an increasing amount of time-series data that can be used for such applications in an unobtrusive manner. But there are not many studies on the performance of the attention-based Transformer model in HAR, especially not for complex activities such as ADLs. This work implements and evaluates the novel self-attention Transformer model for the classification of ADLs and compares it to the already well-established approach of recurrent Long-Short Term Memory (LSTM) networks. The proposed method is a two-level hierarchical model, in which atomic activities are initially recognized in the first step and their probability scores are extracted and utilized for the Transformer-based classification of seven more complex ADLs in the second step. The Transformer is used at the second step to classify seven ADLs. Our results show that the Transformer model reaches the same performance and even outperforms LSTM networks cleary in the subject-dependent configuration (73.36 % and 69.09 %), while relying only on attention-mechanism to depict global dependencies between input and output without the need to use any recurrence. The proposed model was tested using two different segment lengths, indicating its effectiveness in learning long-range dependencies of shorter actions in complex activities.\nCCS Concepts: \u2022 Computing methodologies\u2192 Supervised learning by classification; Neural networks.\nAdditional Key Words and Phrases: transformer, human activity recognition, atomic activities, activities of daily living, attention, lstm, neural networks\nAuthors\u2019 addresses: Gabriela Augustinov, Institute of Medical Informatics, University of L\u00fcbeck, Ratzeburger Allee 160, L\u00fcbeck, Germany, 23562, gabriela.augustinov@uni-luebeck.de; Muhammad Adeel Nisar, Department of IT, University of the Punjab, Lahore, Pakistan, adeel.nisar@pucit.edu.pk; Fr\u00e9d\u00e9ric Li, Institute of Medical Informatics, University of L\u00fcbeck, L\u00fcbeck, Germany, 23562, fr.li@uni-luebeck.de; Amir Tabatabaei, Institute of Medical Informatics, Justus-Liebig-University Giessen, Rudolf-Buchheim-Str. 6, Giessen, Germany, 35392, seyed.a.tabatabaei@informatik.med.uni-giessen.de; Marcin Grzegorzek, Institute of Medical Informatics, University of L\u00fcbeck, Ratzeburger Allee 160, L\u00fcbeck, Germany, 35392 and Department of Knowledge Engineering, University of Economics, Katowice, Poland, 40-287, marcin.grzegorzek@uni-luebeck.de; Keywan Sohrabi, Faculty of Health Sciences, THM University of Applied Sciences Giessen, Wiesenstrasse 14, Giessen, Germany, 35390, keywan.sohrabi@ges.thm.de; Sebastian Fudickar, Institute of Medical Informatics, University of L\u00fcbeck, Ratzeburger Allee 160, L\u00fcbeck, Germany, 35392, sebastian.fudickar@uni-luebeck.de.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2022 Association for Computing Machinery. Manuscript submitted to ACM\nManuscript submitted to ACM 1\nACM Reference Format: Gabriela Augustinov, Muhammad Adeel Nisar, Fr\u00e9d\u00e9ric Li, Amir Tabatabaei, Marcin Grzegorzek, Keywan Sohrabi, and Sebastian Fudickar. 2022. Transformer-Based Recognition of Activities of Daily Living from Wearable Sensor Data. 1, 1 (August 2022), 14 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Globally, we have witnessed a rapid increase in life expectancy of 6.6 years, while unfortunately, the Healthy Life Expectancy (HALE) has not increased at the same swift pace (5.4 years) and the witnessed increase can be rather attributed to the overall lower mortality than to a lower number of years lived with illnesses [18]. Due to this fact, the improvement of the healthy quality of life and standard of living in the aging population has become a central objective in healthcare. The quality of life is defined by the \u017aability to perform everyday activities\u017e as well as \u0142patient satisfaction with levels of functioning\u017e [19]. In particular, the concept of Health-Related Quality of Life (HRQoL) [15] was developed to describe the evaluation of health status and well-being in regard to specific diseases or life as a whole. HRQoL is influenced by three domains: physical, mental, and social well-being. Various instruments have been developed to evaluate each domain, usually involving the assessment of difficulty while engaging in activities of daily living [15].\nActivities of daily living (ADL) [25] is a healthcare term for people\u2019s daily self-care basic tasks of everyday life such as washing, feeding, and dressing oneself. They are used by medical doctors as a measurement of physical or mental disability. Over the years, healthcare professionals have come up with many ways of measuring ADLs to assess the health status of a person [25]. Identifying a person\u2019s ability or inability to perform these activities can provide an assessment of the person\u2019s quality of life with potential applications to improve it [14].\nADL dependance is correlated with poorer quality of life [14], thus monitoring and intervening in case of behavioral changes in patients or elderly people is crucial in helping them maintain an independent life. Smart systems that can continuously observe and recognize ADLs based on monitoring by using sensors are efficient, considering that monitoring by humans is costly and problematic because of the increasing percent of aging population [8]. Such systems can also warn them and their caregivers about forthcoming unprecedented incidents such as falls or other health risks [21].\nThe automatic identification of physical activities of human agents in real life settings is called Human Activity Recognition (HAR). HAR describes the concept of machines examining input data from sensors in order to understand various human activities from that input [5]. The fast advancement in the field of deep learning has been of great benefit in HAR because it can learn high-level significant features by training an end-to-end neural network and it can eliminate most of the effort in terms of time and computing resources involved in designing features [1].\nThis study implements and evaluates the novel attention-based Transformer model architecture [23] and compares it to the already well-established approach of recurrent LSTM networks to classify ADLs in time-series data using atomic activities. Atomic activities are short-term, simple actions which are performed in a sequence in order to complete composite activities. Composite activities are complex, long-term activities such as ADLs.\nThis paper is organized as follows: Section 2 presents an overview of relevant existing work regarding the most common used machine learning methods for HAR. Section 3 describes the methodology of extracting atomic scores and designing the classification with the LSTM and Transformer models. Section 4 presents the experimental results followed by a discussion and finally the conclusion of this work.\nManuscript submitted to ACM"
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "In this section we discuss the most common machine learning methods using non-intrusive sensor based HAR in existing work.\nConvolutional Neural Networks (CNN) are powerful in image processing tasks and machine vision but have also been used extensively for HAR, either by using real images as input or by using the raw time-series signals as input and then applying 1-D instead of 2-D convolution [3]. Other approaches have been also proposed, such as using multi-channel CNNs, that leverage more sensor signals to classify human activities [9]. CNNs have been used to recognize activities such as walking, cycling, sitting, or standing, yielding good performance in recognizing activities that belong to the class of simple physical movements or postures. However, simple CNNs struggle with recognizing more complex activities, such as walking up or down stairs, running, watching TV or housekeeping activities and need a high amount of training data to be able to learn complicated features [12]. Recurrent Neural Networks (RNNs), especially Long-Short Term Memory (LSTM) networks have soon been used instead of CNNs for human activity recognition, achieving improved results [21]. RNNs used alongside CNNs (hybrid models) have achieved better performance with substantial margin over simple CNN models in recognizing more complex activities [1].\nA RNN is a neural network with \u0142a memory\u017e because it has an internal state which is updated in an iterative loop, making the network able to process input sequences with varying lengths. With every iteration, the internal hidden state, is evolving to be dependent on every previous time step, providing - after processing the whole input sequence - a representation of it [7]. RNNs are incapable of securing context from all possible feature combinations, but an attention mechanism applied on recurrent layers has helped achieve this. The authors in [26] used a combination of attention mechanisms and LSTM layers and showed an improvement over baseline LSTM models with no attention on multiple HAR datasets. However RNNs are rather complex due to their inherently sequential nature and because very long sequences cannot be used in batches. To deal with the computational complexity associated with the recurrence, self-attention-based models such as the Transformer have been proposed [23].\nThe term \u0142attention mechanism\u017e was introduced in [2] in the context of machine translation, where the authors proposed an extended encoder-decoder model which aims to improve the performance in longer sentences by encoding the input sentence into a sequence of vectors, from which a subset is used for the decoding part. For the attention mechanism either a weighted sum or the dot product can be used to compute the context vectors [10].\nThe architecture of the Transformer was proposed in [23] as a new and promising approach for machine translation tasks and it relies exclusively on attention mechanisms, and thus, overcomes necessity of convolution or recurrence. Recently, the Vision Transformer (ViT) has been proposed in [6] and it was confirmed that CNNs can be replaced by a pure transformer applied directly to sequences of image patches. Based on the architecture of the ViT, [13] proposed a Transformer for short-time action recognition (one second) using skeletal pose estimations from RGB images as input data. [11] used normalized three seconds time-series data from the smartphone motion sensors (accelerometer and gyroscope) directly as input to the Transformer to classify short-time activities mainly related to the pose or physical movement of the subject.\nArchitectures based only on attention mechanisms have recently outperformed some of the previous mentioned approaches in HAR, while being computationally superior at the same time. [13] tested four different versions of the Transformer model in terms of complexity (micro, small, medium, and large number of trainable parameters) and even the smallest Transformer model outperformed CNNs while having a significantly reduced number of trainable parameters (228k vs. 4062k). The model proposed in [11] achieved 99.2 % accuracy and was a great improvement\nManuscript submitted to ACM\nin comparison to the 89 % achieved with a Random Forest classifier. The drawback of this study is the fact, that no comparison with other more established state of the art models such as RNNs has been done. The previous mentioned Transformer-based works used only short, simple activities, mainly related to the pose of the subjects, while in our work we aim to classify longer and complex ADLs using the Transformer architecture.\nMore recently, systems that recognize the plan and the goal beyond the activity in a holistic way, have been shown to be useful for the recognition of complex activities especially in the robotics field. Goal recognition refers to the problem of deducing the intention of humans by observing a set of actions carried out. Plan recognition is the problem of understanding the set of actions that have been or will be executed by the agent in order to reach a goal, given a set of observed actions performed by the person [22]. In our work we focus on activity recognition only and do not take into consideration goal or plan recognition.\nRelated work usually differentiates human activities based on the target task or application. Simple atomic actions, poses and motions are usually easy to identify accurately with various machine learning models [1, 9, 11, 13], whereas composite activities, such as ADLs require either more intricate models or a more complex feature engineering [16]. It is a usual approach to decompose complex ADLs into simpler activities, which are generally easier to recognize [24]. For ADLs, longer segments ranging from 30 seconds to 5 minutes have been considered in [16] with a hierarchical model, that uses atomic scores as features for the recognition of ADLs using a Rank Pooling approach. Similarly, we used a two-layer hierarchical model where atomic activities are identified in the first step and their probability scores are used for the recognition of composite activities at higher level in the second step using the Transformer model."
        },
        {
            "heading": "3 METHODOLOGY DESCRIPTION",
            "text": "In order to evaluate the attention-based Transformer model architecture in comparison to LSTM for the recognition of ADLs, we firstly extract probabilistic scores of atomic activities. Secondly, the probability scores are utilized for the Transformer-based classification of seven composite activities in the second step."
        },
        {
            "heading": "3.1 Dataset description",
            "text": "An important distinction that needs to be addressed when looking at HAR is the type of activity. There are multiple ways wherein the activities are defined in related work, but usually a main differentiation needs to be made between simple actions and complex activities. In this work, we focus similarly on two main types of activities: atomic and composite activities. Atomic activities are short-term, simple actions which are performed in a sequence in order to complete composite activities, such as opening or closing a door. Composite activities are complex, long-term activities such as ADLs.\nThe applied datasets contain 61 classes of four seconds long atomic activities from eight subjects and seven classes of composite activities with various lengths ranging from 30 seconds to 5 minutes from five subjects [16]. The classes of atomic activities are separated into two categories: six state activities, characterizing the pose and 55 behavioral activities, characterizing the actions of the subjects. The atomic activities are shown in Table 1. The classes of composite activities are: brushing teeth, cleaning room, handling medication, preparing food, styling hair, using telephone and washing hands.\nThe HAR approach in this work is a wearable-based recognition approach involving multiple sensors from three devices: a smartphone placed in the front left pocket of the jeans, a smartwatch placed on the left wrist and a pair of smart glasses. The data recorded with the smartphone device comes from four sensors, with a sampling frequency of 200 Hz: accelerometer, gyroscope, gravity and linear accelerometer. Two different smartwatches were used for recording Manuscript submitted to ACM\nthe atomic activities and the composite activities dataset, which are equipped with an accelerometer and a gyroscope, recording data with a sampling frequency of 67 Hz (atomic dataset) and 100 Hz (composite dataset). The smart glasses are also equipped with an accelerometer and a gyroscope, recording the data with a 20 Hz sampling frequency."
        },
        {
            "heading": "3.2 Extracting the atomic scores",
            "text": "The atomic activity recognition for the purpose of extracting atomic scores was done using the deep transfer learning approach proposed by [9]. The aim of transfer learning is to improve performance on a task in a target domain by transferring knowledge acquired on one or more related source domains. This implies the basic assumption that there exists some relationship between the source and the target problems, even when the source task and target task or the source data and the target data differ, that make it possible to transfer knowledge successfully between them [4].\nThe deep transfer learning approach proposed by [9] involves training a source domain neural network, in which general characteristics of time-series data are learned by classifying sensor modalities using raw single-channel input. The learned parameters are then transferred to the target domain, designed to classify four seconds long atomic activities. The target neural network is a multi-channel Deep Neural Network (mDNN) trained on data acquired from the three wearable devices. All channels of each sensor modality have been used for the training. Two individual classification tasks were defined for 6 state activities and 55 behavioral activities, because of their potential overlap, such as being in a sitting or a standing position while writing or drinking. One mDNN was trained for the classification of the six state activities and another one for the classification of the 55 behavioral activities. Thus, the scores contain at the same time information on the state, as well as information on the behavior of the subject in each four second frame.\nThe process of extracting atomic scores from raw sensor data of composite activities is shown in Figure 1. It followed\nthe next steps:\n(1) Segmentation of composite activities: Take data corresponding to the first 20 seconds/60 seconds from the\noriginal signal. Apply padding at the end of the sequence for the activities that are shorter than 20 seconds/60 seconds respectively. (2) Segmentation into windows of four seconds: Segment data into five/15 non-overlapping windows of four seconds.\nManuscript submitted to ACM\n(3) Feeding data into the mDNNs: Feed the windows of data to the state and behavioral mDNN respectively. Extract\nthe six class probabilities from the prediction values of the last dense layer of the state mDNN and the 55 class probabilities from the prediction values of the last dense layer of the behavioral mDNN.\n(4) Concatenate the probabilities to form 61 atomic scores. (5) Form sequence of atomic scores: Concatenate the five/15 vectors of atomic scores.\nA sequence of atomic scores extracted as described above was used as input to the Transformer model and to the LSTM network, which architectures are described in Section 3.2 and 3.3 respectively. The input of the Transformer model is a flat vector of the 61 atomic scores concatenated to the length of the input sequence (e.g. 15 vectors for a 60 second sequence times 61 atomic scores = 915x1 input values). For the LSTM model we used both the flattened input as described above for the Transformer, as well as a non-flattened input (15x61).\nTwo configurations in terms of how the subjects\u2019 data was divided for the training and testing set were used: a subject dependent and a subject independent configuration. One session of data from all subjects for training, one for testing was used for the subject dependent configuration. The the subject independent configuration is more similar to real-life setting with new subjects data as test set. Leave-one-subject-out Cross-Validation (LOSO-CV) was applied in this case, where the number of folds represents the number of subject and for each fold, the data is trained on n-1 subjects and tested on the remaining subject."
        },
        {
            "heading": "3.3 Long-Short Term Memory network architecture",
            "text": "The LSTM network used as benchmark is a simple recurrent neural network with one LSTM layer and two fully connected layers. Recurrent models with more stacked LSTM layers have also been tested but adding even one more LSTM hidden layer led to overfitting of the model. The exact architecture of the LSTM model is shown in Figure 2 and the training parameters are shown in Table 2."
        },
        {
            "heading": "3.4 Transformer architecture",
            "text": "The Transformer was proposed in [23] as a new and simple architecture for translation and other sequence to sequence models. This novel architecture relies only on attention mechanisms to depict global dependencies between input and output. The initially proposed Transformer uses attention in three ways: a typical encoder-decoder attention mechanism, Manuscript submitted to ACM\nself-attention layers in the encoder and masked self-attention in the decoder. Self-attention is the mechanism which allows the Transformer to avoid using any recurrence. Self-attention is also called intra-attention, because it relates different positions of a single sequence in order to compute a representation of the sequence [23].\nManuscript submitted to ACM\nGenerally, an attention function means that a query and a set of key-value pairs, where the query, keys and values are all vectors, which are processed to an output, that is the representation of the dependencies in the input sequences. Manuscript submitted to ACM\nTo compute the output, first, a weight for each value is calculated by applying a compatibility function of the query with the corresponding key. Using these weights, the output is computed as the weighted sum of the values.\nInstead of using only one attention function, the Transformer performs the attention function in parallel on linear projections of the queries, keys, and values h (number of heads) times with different, learned linear projections of them. This way the model can jointly attend to information from different representation sub-spaces at different positions [23].\nThe Transformer architecture used is a Transformer model using the Keras MultiHeadAttention Layer [20], which is an implementation of multi-headed attention as described in [23]. The same paper was the inspiration for the Transformer model for Timeseries classification as found in the Keras demonstration codes [17] and as used in this work. The architecture used is shown in the model plot of the Transformer model in Figure 3. The parameters in Transformer architecture used are shown in Table 2. Increasing the number of attention heads and number of transformer blocks increased the complexity of the model without leading to improved results. A more complex model was not needed, since the input vector of atomic scores is very small in dimension."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "The performance metrics used to evaluate the models are the average accuracy (macro), averaged a one-vs-all F1 scores (AF1) and the Mean Average Precision (MAP). The Area Under the Curve (AUC) has been also calculated by an one-vs-all approach to evaluate the classification of composite activities.\nThe results of the LSTM model are shown in Table 3 and for the Transformer model in Table 4 for following configurations: Both, subject-dependent, and subject-independent using the atomic-scores of segments of the lengths of 20 and 60 seconds as input.\nTo validate the results, Leave-One-Subject-Out Cross-Validation (LOSO-CV) has been performed. This is a special case of cross-validation where the number of folds equals the number if subjects in the dataset. This means, that the\nManuscript submitted to ACM\nlearning algorithm is applied once for each subject (five times in the actual case), using all data from the other subjects as training set and the data of the selected subject as test set.\nWe observed a pattern of overfitting in the Transformer and in the LSTM. The accuracy of the training and validation sets over the number of training epochs is plotted in Figure 4 and 5 for the Transformer and LSTM respectively. In case of the Transformer, the training accuracy reaches 100 % while the validation accuracy stagnates around the 70 % line. Nevertheless, the validation accuracy of the Transformer is higher than that of the LSTM network in the case of subject dependent training and testing. The ROC curves (one-vs-rest) of the seven ADLs classified with the Transformer are shown in Figure 6.\nThe confusion matrices (absolute) of the 60 seconds subject dependent classification are shown in Figure 7 for the\nLSTM and in Figure 8 for the Transformer respectively. Manuscript submitted to ACM\nManuscript submitted to ACM"
        },
        {
            "heading": "5 DISCUSSION",
            "text": "We observed that in terms of Accuracy: Both models need to be trained for a significant amount of epochs (300) to be able to recognize composite activities. The Transformer network is partially outperforming the LSTMmodels, while also having the advantage of using no recurrence. This is an important insight for future work, where it is intended to use raw data as input, instead of the extracted atomic scores as features, and where the complexity because of recurrence becomes a problem.\n\u2022 The Transformer performs as well as the state-of-the-art LSTM model and it even outperforms it while using\nshorter as well as longer segments and, in the subject-dependent configuration. In the subject-independent configuration, the LSTM seems to be able to reach a slightly higher accuracy than the Transformer model. \u2022 Both LSTM and Transformer perform better when using long segments (e.g., 60 seconds) rather than short ones\n(e.g., 20 seconds): \u015b 60 seconds, subject dependent configuration: the LSTM accuracy for non-flattened input = 69.09 % and the\nTransformer accuracy = 73.36 %\n\u015b 20 seconds: the LSTM accuracy = 46.59 % and the Transformer accuracy = 53.64 %\nBoth the Transformer and the LSTM models seem to struggle with overfitting when trained on the datasets used in this work, as shown in Figure 4 and Figure 5. This might be due to the fact that the number of subjects is very low, and the models are able to learn the specific patterns of one subject, but there is not enough diversity in the data to be able to perform well with new data. This fact is further highlighted by the differences in performance when comparing the subject-dependent with the subject-independent configurations. The subject-dependent configuration performs better under all circumstances, as Table 3 and 4 indicate.\nManuscript submitted to ACM"
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In order to investigate the practicability to apply solely attention-based architecture for the recognition of ADLs, we have applied both, Transformer, and LSTM models regarding their ability to classify seven ADLs. The results indicate that the Transformer model partially outperforms the LSTM model in our experiments, thus pointing towards the superiority of Transformer models regarding the classification of composite long-term activities.\nRaw data might be cluttered with noise or actions that are meaningless for a specific HAR task, especially for recognizing ADLs. For example, while cooking, irrelevant atomic activities for the cooking task might be present during the time frame. Furthermore, since each repetition of a complex activity might differ in regard to the ordering of atomic actions even when performed by the same person, it is important to extract elaborated features from raw data when aiming to recognize composite activities, that take into account the different atomic actions performed in a different order. The Transformer model seems to be more robust in this regard, and outperformed LSTM considering the above mentioned challenges in the recognition of ADLs.\nUsing a hierarchical model was confirmingly a proper way to deal with these complexities resulting from the use of raw data as input to the Transformer model. Further investigation of raw data with transformers are essential, since the main quality of transformers, is their ability to learn long-time dependencies more efficiently than other state-of-the art networks for time-series data."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The study is funded by the German Federal Ministry of Education and Research (Project No. 01ZZ2007)."
        }
    ],
    "title": "Transformer-Based Recognition of Activities of Daily Living from Wearable Sensor Data",
    "year": 2022
}