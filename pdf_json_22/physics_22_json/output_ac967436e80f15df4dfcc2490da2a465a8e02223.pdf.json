{
    "abstractText": "An early example of the ability of deep networks to improve the statistical power of data collected in particle physics experiments was the demonstration that such networks operating on lists of particle momenta (four-vectors) could outperform shallow networks using features engineered with domain knowledge. A benchmark case is described, with extensions to parameterized networks. A discussion of data handling and architecture is presented, as well as a description of how to incorporate physics knowledge into the network architecture.",
    "authors": [
        {
            "affiliations": [],
            "name": "Pierre Baldi"
        },
        {
            "affiliations": [],
            "name": "Peter Sadowski"
        },
        {
            "affiliations": [],
            "name": "Daniel Whiteson"
        }
    ],
    "id": "SP:e1f0e1254ac7ee15d8fc0e87bb2d92c7c35ac676",
    "references": [
        {
            "authors": [
                "J. Alwall",
                "M. Herquet",
                "F. Maltoni",
                "O. Mattelaer",
                "T. Stelzer"
            ],
            "title": "MadGraph 5 : Going Beyond",
            "venue": "JHEP 06 ",
            "year": 2011
        },
        {
            "authors": [
                "GEANT",
                "S. Agostinelli"
            ],
            "title": "GEANT4\u2013a simulation toolkit",
            "venue": "Nucl. Instrum. Meth. A",
            "year": 2003
        },
        {
            "authors": [
                "H. Abramowicz",
                "A. Caldwell",
                "R. Sinkus"
            ],
            "title": "Neural network based electron identification in the ZEUS calorimeter",
            "venue": "Nucl. Instrum. Meth. A 365 ",
            "year": 1995
        },
        {
            "authors": [
                "DELPHI Collaboration"
            ],
            "title": "Classification of the hadronic decays of the Z into b and c quark pairs using a neural network",
            "venue": "Phys. Lett. B 295 ",
            "year": 1992
        },
        {
            "authors": [
                "R. Vazquez",
                "F. Halzen",
                "E. Zas"
            ],
            "title": "Improving the \u010cerenkov imaging technique with neural networks",
            "venue": "Physical Review D 45 ",
            "year": 1992
        },
        {
            "authors": [
                "P. Baldi"
            ],
            "title": "Deep Learning in Science: Theory",
            "venue": "Algorithms, and Applications. Cambridge University Press, Cambridge, UK",
            "year": 2021
        },
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe"
            ],
            "title": "and H",
            "venue": "White, Multilayer feedforward networks are universal approximators., Neural networks 2 ",
            "year": 1989
        },
        {
            "authors": [
                "P. Baldi",
                "P. Sadowski",
                "D. Whiteson"
            ],
            "title": "Searching for exotic particles in high-energy physics with deep learning",
            "venue": "Nature Commun. 5 ",
            "year": 2014
        },
        {
            "authors": [
                "P. Baldi",
                "P. Sadowski",
                "D. Whiteson"
            ],
            "title": "Enhanced Higgs Boson to \u03c4+\u03c4\u2212 Search with Deep Learning",
            "venue": "Phys. Rev. Lett. 114 ",
            "year": 2015
        },
        {
            "authors": [
                "ATLAS Collaboration"
            ],
            "title": "Search for a multi-Higgs-boson cascade in W+W\u2212bb\u0304 events with the ATLAS detector in pp collisions at \u221a s = 8 TeV",
            "venue": "Phys. Rev. D 89 ",
            "year": 2014
        },
        {
            "authors": [
                "ATLAS Collaboration"
            ],
            "title": "Observation of a new particle in the search for the Standard Model Higgs boson with the ATLAS detector at the LHC",
            "venue": "Phys. Lett. B 716 ",
            "year": 2012
        },
        {
            "authors": [
                "CMS Collaboration"
            ],
            "title": "Observation of a new boson at a mass of 125 GeV with the CMS experiment at the LHC",
            "venue": "Phys. Lett. B 716 ",
            "year": 2012
        },
        {
            "authors": [
                "R. Santos",
                "M. Nguyen",
                "J. Webster",
                "S. Ryu",
                "J. Adelman",
                "S. Chekanov",
                "J. Zhou"
            ],
            "title": "Machine learning techniques in searches for tt\u0304h in the h\u2192 bb\u0304 decay channel",
            "venue": "JINST 12 ",
            "year": 2017
        },
        {
            "authors": [
                "O.J. Dunn"
            ],
            "title": "Multiple Comparisons among Means",
            "venue": "Journal of the American Statistical Association 56 ",
            "year": 1961
        },
        {
            "authors": [
                "P. Baldi",
                "K. Cranmer",
                "T. Faucett",
                "P. Sadowski",
                "D. Whiteson"
            ],
            "title": "March 8",
            "venue": "2022 2:4 ws-rv9x6 Artificial Intelligence for High Energy Physics fourv page 21 Deep Learning From Four Vectors 21 Parameterized neural networks for high-energy physics, The European Physical Journal C 76 ",
            "year": 2016
        },
        {
            "authors": [
                "CMS Collaboration"
            ],
            "title": "Search for resonant and nonresonant higgs boson pair production",
            "venue": "JHEP 2018 ",
            "year": 2018
        },
        {
            "authors": [
                "ATLAS Collaboration"
            ],
            "title": "Performance of mass-decorrelated jet substructure observables for hadronic two-body decay tagging in ATLAS",
            "venue": "ATL-PHYS-PUB-2018-014",
            "year": 2018
        },
        {
            "authors": [
                "CMS Collaboration"
            ],
            "title": "Search for a charged Higgs boson decaying into top and bottom quarks in events with electrons or muons in proton-proton collisions at \u221a s = 13 TeV",
            "venue": "JHEP 01 ",
            "year": 2020
        },
        {
            "authors": [
                "P. Baldi"
            ],
            "title": "The inner and outer approaches to the design of recursive neural architectures",
            "venue": "Data mining and knowledge discovery 32 ",
            "year": 2018
        },
        {
            "authors": [
                "C.R. Qi",
                "H. Su",
                "K. Mo",
                "L.J. Guibas"
            ],
            "title": "PointNet: Deep learning on point sets for 3D classification and segmentation",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2017
        },
        {
            "authors": [
                "R. Beck",
                "P. Sadowski",
                "Y. Glaser"
            ],
            "title": "Szapudi, Refined redshift regression in cosmology with graph convolution networks, in NeurIPS Machine Learning and the Physical Sciences",
            "year": 2019
        },
        {
            "authors": [
                "A. Butter",
                "G. Kasieczka",
                "T. Plehn",
                "M. Russell"
            ],
            "title": "Deep-learned Top Tagging with a Lorentz Layer",
            "venue": "SciPost Phys. 5 ",
            "year": 2018
        },
        {
            "authors": [
                "P. Baldi",
                "K. Bauer",
                "C. Eng",
                "P. Sadowski",
                "D. Whiteson"
            ],
            "title": "Jet substructure classification in high-energy physics with deep neural networks",
            "venue": "Phys. Rev. D 93 ",
            "year": 2016
        },
        {
            "authors": [
                "D. Guest",
                "J. Collado",
                "P. Baldi",
                "S.-C. Hsu",
                "G. Urban",
                "D. Whiteson"
            ],
            "title": "Jet Flavor Classification in High-Energy Physics with Deep Neural Networks",
            "venue": "Phys. Rev. D 94 ",
            "year": 2016
        },
        {
            "authors": [
                "G. Urban",
                "K.J. Geras",
                "S.E. Kahou",
                "O. Aslan",
                "S. Wang",
                "R. Caruana",
                "A. Mohamed",
                "M. Philipose",
                "M. Richardson"
            ],
            "title": "Do deep convolutional nets really need to be deep and convolutional",
            "venue": "arXiv:1603.05691 [stat.ML] ",
            "year": 2016
        },
        {
            "authors": [
                "T. Cohen",
                "M. Welling"
            ],
            "title": "Group equivariant convolutional networks",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning. PMLR, New York, New York, USA, 20\u201322 Jun",
            "year": 2016
        },
        {
            "authors": [
                "T. Cohen",
                "M. Weiler",
                "B. Kicanaoglu",
                "M. Welling"
            ],
            "title": "Gauge equivariant convolutional networks and the icosahedral CNN",
            "venue": "Proceedings of the 36th International Conference on Machine Learning. PMLR, Long Beach, California, USA, 09\u201315 Jun",
            "year": 2019
        },
        {
            "authors": [
                "G. Louppe",
                "K. Cho",
                "C. Becot",
                "K. Cranmer"
            ],
            "title": "QCD-Aware recursive March 8",
            "venue": "2022 2:4 ws-rv9x6 Artificial Intelligence for High Energy Physics fourv page 22 22 Daniel Whiteson neural networks for jet physics, JHEP 01 ",
            "year": 2019
        },
        {
            "authors": [
                "T. Cheng"
            ],
            "title": "Recursive Neural Networks in Quark/Gluon Tagging",
            "venue": "Comput. Softw. Big Sci. 2 ",
            "year": 2018
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation, in International Conference on Medical image computing and computer-assisted intervention",
            "year": 2015
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long Short-Term Memory",
            "venue": "Neural computation 9 ",
            "year": 1997
        },
        {
            "authors": [
                "K. Cho",
                "B. van Merri\u00ebnboer",
                "C. Gulcehre",
                "D. Bahdanau",
                "F. Bougares",
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
            "year": 2014
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "ICLR ",
            "year": 2014
        },
        {
            "authors": [
                "T. Luong",
                "H. Pham",
                "C.D. Manning"
            ],
            "title": "Effective approaches to attention-based neural machine translation",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
            "year": 2015
        },
        {
            "authors": [
                "M. Cacciari",
                "G.P. Salam",
                "G. Soyez"
            ],
            "title": "The anti-kt jet clustering algorithm",
            "venue": "JHEP 04 ",
            "year": 2008
        },
        {
            "authors": [
                "C. Goller",
                "A. Kuchler"
            ],
            "title": "Learning task-dependent distributed representations by backpropagation through structure",
            "venue": "Neural Networks",
            "year": 1996
        },
        {
            "authors": [
                "R. Socher",
                "C.C. Lin",
                "C.D. Manning",
                "A.Y. Ng"
            ],
            "title": "Parsing natural scenes and natural language with recursive neural networks, in Proceedings of the 28th international conference on machine learning (ICML-11)",
            "year": 2011
        },
        {
            "authors": [
                "M. Erdmann",
                "E. Geiser",
                "Y. Rath",
                "M. Rieger"
            ],
            "title": "Lorentz boost networks: autonomous physics-inspired feature engineering",
            "venue": "JINST 14 ",
            "year": 2019
        },
        {
            "authors": [
                "M. Fenton",
                "A. Shmakov",
                "T. Ho",
                "S. Hsu",
                "D. Whiteson",
                "P. Baldi"
            ],
            "title": "Permutationless many-jet event reconstruction with symmetry preserving attention networks",
            "venue": "",
            "year": 2020
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805 ",
            "year": 2018
        },
        {
            "authors": [
                "M. Reichstein",
                "G. Camps-Valls",
                "B. Stevens",
                "M. Jung",
                "J. Denzler",
                "N. Carvalhais"
            ],
            "title": "and Prabhat",
            "venue": "Deep learning and process understanding for data-driven Earth system science, Nature 566 ",
            "year": 2019
        },
        {
            "authors": [
                "K.J. Bergen",
                "P.A. Johnson"
            ],
            "title": "M",
            "venue": "V. de Hoop, and G. C. Beroza, Machine learning for data-driven discovery in solid earth geoscience, Science 363 ",
            "year": 2019
        },
        {
            "authors": [
                "A. Karpatne",
                "G. Atluri",
                "J.H. Faghmous",
                "M. Steinbach",
                "A. Banerjee",
                "A. Ganguly",
                "S. Shekhar",
                "N. Samatova",
                "V. Kumar"
            ],
            "title": "Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data",
            "venue": "IEEE Transactions on Knowledge and Data Engineering 29 ",
            "year": 2017
        },
        {
            "authors": [
                "J. Willard",
                "X. Jia",
                "S. Xu",
                "M. Steinbach",
                "V. Kumar"
            ],
            "title": "Integrating Physics-Based Modeling with Machine Learning: A Survey, arXiv:2003.04919",
            "year": 2003
        },
        {
            "authors": [
                "P. M\u00e1rquez-Neila",
                "M. Salzmann",
                "P. Fua"
            ],
            "title": "Imposing Hard Constraints on Deep Networks: Promises and Limitations, arXiv:1706.02025",
            "year": 2025
        },
        {
            "authors": [
                "Y. Bar-Sinai",
                "S. Hoyer",
                "J. Hickey",
                "M.P. Brenner"
            ],
            "title": "Learning data-driven discretizations for partial differential equations",
            "venue": "Proceedings of the National Academy of Sciences 116 ",
            "year": 2019
        },
        {
            "authors": [
                "E. de Bezenac",
                "A. Pajot",
                "P. Gallinari"
            ],
            "title": "Deep learning for physical processes: Incorporating prior scientific knowledge",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment",
            "year": 2019
        },
        {
            "authors": [
                "J. Ling",
                "A. Kurzawski",
                "J. Templeton"
            ],
            "title": "Reynolds averaged turbulence modelling using deep neural networks with embedded invariance",
            "venue": "Journal of Fluid Mechanics 807 ",
            "year": 2016
        },
        {
            "authors": [
                "J.L. Wu",
                "H. Xiao",
                "E. Paterson"
            ],
            "title": "Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework",
            "venue": "Physical Review Fluids 3 ",
            "year": 2018
        },
        {
            "authors": [
                "T. Bolton",
                "L. Zanna"
            ],
            "title": "Applications of Deep Learning to Ocean Data Inference and Subgrid Parameterization",
            "venue": "Journal of Advances in Modeling Earth Systems 11 ",
            "year": 2019
        },
        {
            "authors": [
                "X. Jia",
                "J. Willard",
                "A. Karpatne",
                "J. Read",
                "J. Zwart",
                "M. Steinbach",
                "V. Kumar"
            ],
            "title": "Physics guided RNNs for modeling dynamical systems: A case study in simulating lake temperature profiles, in SIAM International Conference on Data",
            "year": 2019
        },
        {
            "authors": [
                "M. Raissi",
                "A. Yazdani",
                "G.E. Karniadakis"
            ],
            "title": "Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations",
            "venue": "Science 367 ",
            "year": 2020
        },
        {
            "authors": [
                "T. Beucler",
                "M. Pritchard",
                "S. Rasp",
                "J. Ott",
                "P. Baldi",
                "P. Gentine"
            ],
            "title": "Enforcing Analytic Constraints in Neural-Networks Emulating Physical Systems",
            "venue": "Physics Review Letters ",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Chapter 1"
        },
        {
            "heading": "Deep Learning From Four Vectors",
            "text": "Pierre Baldi"
        },
        {
            "heading": "Department of Computer Science, UC Irvine, Irvine CA, USA 92627",
            "text": "Peter Sadowski"
        },
        {
            "heading": "Department of Computer Science, University of Hawaii, XYZ",
            "text": "Daniel Whiteson"
        },
        {
            "heading": "Department of Physics & Astronomy, UC Irvine, Irvine CA, USA 92627",
            "text": "An early example of the ability of deep networks to improve the statistical power of data collected in particle physics experiments was the demonstration that such networks operating on lists of particle momenta (four-vectors) could outperform shallow networks using features engineered with domain knowledge. A benchmark case is described, with extensions to parameterized networks. A discussion of data handling and architecture is presented, as well as a description of how to incorporate physics knowledge into the network architecture."
        },
        {
            "heading": "Contents",
            "text": "1. Introduction: Pre-Deep Learning State-of-the-Art . . . . . . . . . . . . . . . . 2 2. Application of Deep Learning to Four Vectors . . . . . . . . . . . . . . . . . . 3 3. Parameterized Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n3.1. Parameterized Network Structure & Training . . . . . . . . . . . . . . . 8 3.2. Physical Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4. Handling Sets of Four Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4.1. Data-Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.2. Canonicalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.3. Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5. Physics-aware networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5.1. Physics-Informed Architecture Design . . . . . . . . . . . . . . . . . . . . 16\n5.2. Incorporating Physics Constraints . . . . . . . . . . . . . . . . . . . . . . 18 6. Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n1\nar X\niv :2\n20 3.\n03 06\n7v 1\n[ he\npex\n] 6\nM ar\n2 02\n2"
        },
        {
            "heading": "1. Introduction: Pre-Deep Learning State-of-the-Art",
            "text": "The goal of the statistical analysis of particle physics data is to infer bounds on parameters of physical theories, such as the masses of particles or their rate of production in specific interactions. These statistical tasks, which involve classification, hypothesis testing, regression, and goodness-of-fit testing, require a statistical likelihood model p(x|\u03b8) which describes the probability of observing experimental data x for specific values of the parameters \u03b8 of a physical theory.\nUnfortunately, the statistical likelihood model can almost never be expressed analytically, due to the complex nature of the relationship between the theoretical parameters \u03b8 and the high-dimensional (102 \u2212 108)) data x. Instead, statistical models are typically estimated from samples generated using Monte Carlo methods [1, 2], whose computational expense limits the dimensionality of the feature space to O(100). In this context, it becomes vital to reduce the dimensionality of the data, and many initial applications [3\u20135] of machine learning to particle physics focused on development of classifiers, which offered powerful ways to perform this dimensional reduction and produce a single feature which summarizes much of the available information relevant to the statistical question.\nEarly applications of machine learning in physics [6, 7] were largely limited to shallow machine learning methods including boosted decision trees and artificial neural networks with a single hidden layer. This was primarily for historical reasons, including broad unawareness of the power of deep neural networks, misguided thinking and publications about local minima or vanishing gradients, as well as concerns over the problem of interpreting their output. In addition, it was well known that neural networks with a single hidden layer have universal approximation properties [8], although highly non-linear functions may require an intractable number of hidden nodes. The combination of these circumstances led physicists to focus on shallow rather than deep machine learning methods, and shallow classifiers rapidly proliferated in physics (see [6] for an early application of neural networks). The approach successfully boosted the performance of many statistical analyses by allowing physicists to employ multiple observables, and was commonly referred to as \u201cmulti-variate analysis\u201d. Such applications reduced the dimensionality of the feature space to one or two, allowing for estimation of the statistical models needed for inference tasks.\nWhile dimensional reduction nearly always involves some loss of information, it does not necessarily reduce relevant information, as the statis-\ntical task usually only requires a subset of the information. The NeymanPearson lemma shows that the optimal decision boundary for a hypothesis test between two statistical models (in any dimension feature space) can be determined by knowledge of their ratios; the full models are not needed. However, it was long suspected that shallow networks fell short of capturing all of the relevant information contained in the feature space. While it is not possible to directly estimate the absolute optimal performance without building an optimal classifier, it is possible to demonstrate that a given network is not optimal by finding a more powerful example.\nA common experience in pre-deep-learning particle physics was to perform exhaustive feature engineering to simplify the task for the shallow classifier. A shallow network on four vectorsa, for example, would be compared to a shallow network with features built using domain knowledge. Such domain-specific expert features are generally non-linear functions of four vectors that capture physical insights about the data. Almost invariably, the expert features would boost the performance, despite adding no unique information, demonstrating that the shallow classifiers had failed to discover these non-linear strategies on their own.\nThis feature-search approach is labor-intensive and not necessarily optimal; a robust machine learning method would obviate the need for this additional step and capture all of the available classification power directly from the raw data. Thus, the stage was set for deep learning."
        },
        {
            "heading": "2. Application of Deep Learning to Four Vectors",
            "text": "In this section we describe a benchmark classification task [9, 10] that exemplifies a common use case for machine learning in particle physics: discrimination between signal and background processes. This example demonstrates a common failure mode of shallow networks on four vectors, which exhibit reduced performance compared to networks that use features engineered with domain knowledge."
        },
        {
            "heading": "Benchmark Case for Higgs Bosons",
            "text": "A typical classification task in particle physics distinguishes between a signal process, where new particles are produced, and one or more background processes, which mimic the nature and number of particles observed, but aFour vectors in momentum space are a generalization of three-dimensional momentum, including the total energy E, as (E, p\u0304) and are typically used in particle physics to specify a particle\u2019s momentum and total energy.\ncan be distinguished by their kinematics. An example [9] examined by experiments at the LHC is the production of a heavy electrically-neutral Higgs boson (gg \u2192 H0), which decays to a heavy electrically-charged Higgs boson (H\u00b1) and a W boson [11, 12]. The H\u00b1 boson subsequently decays to a second W boson and the light Higgs boson, h0 observed by the ATLAS [13] and CMS [14] experiments. The light Higgs boson decays predominantly to a pair of bottom quarks, giving the process:\ngg \u2192 H0 \u2192W\u2213H\u00b1 \u2192W\u2213W\u00b1h0 \u2192W\u2213W\u00b1bb\u0304, (1)\nwhich leads to W\u2213W\u00b1bb\u0304 shown in Fig. 1. For the benchmark case here, mH0 = 425 GeV and mH\u00b1 = 325 GeV have been assumed.\nThe background process mimics this signal but without the Higgs boson intermediate state. It produces a pair of top quarks, each of which decay to Wb, also giving W\u2213W\u00b1bb\u0304.\nBoth processes yield the same set of observed particles: one charged lepton, four jets (two of which have b-tags) and missing transverse momentum. Together, the twenty-one individual momentum components of these particles comprise our low-level feature set.\nThe low-level features show some differences between the signal and background processes \u2014 Fig. 2 shows the marginal distributions for two of these kinematic features. However, we see even larger differences in higher-level features constructed using domain knowledge of the different intermediate states. As the difference in the two hypotheses lies mostly in the existence of new intermediate Higgs boson states, it is possible to\ndistinguish between the two hypotheses by attempting to identify whether the intermediate state existed by reconstructing its characteristic invariant mass. In the signal hypothesis we expect peaks in m`\u03bd , mjj , mbb\u0304, mWbb\u0304, mWWbb\u0304, while the background should peak in mj`\u03bd and mjjj . Figure 3 shows the difference in distributions for two of these high level variables."
        },
        {
            "heading": "Performance",
            "text": "Deep neural networks (DNN) were compared to shallow neural networks (NN) and boosted decision trees (BDT) on three different subsets of input features: the low-level features only, the high-level features only, and both. Performance on the test set was measured in terms of Area Under the ROC curve (AUC) and discovery significance (Table 1), as well as signal efficiency and background rejection; see Fig. 4.\nThe shallow neural networks and BDTs trained with the high-level features perform significantly better than those trained on only the low-level features, demonstrating the importance of feature engineering in shallow machine learning models. However, training all three methods with only the high-level features leads to lower performance than training with the complete set of features, indicating that the low-level features contain additional information that is not being captured by these engineered features. Only the deep learning approach shows nearly equal performance using the low-level features and the complete features. This suggests that it is automatically discovering high-level abstractions similar to those captured\nby the hand-engineered features, obviating the need for laborious feature engineering."
        },
        {
            "heading": "Discussion",
            "text": "It is widely accepted in experimental high-energy physics that machine learning is a powerful approach boosting statistical power in exotic particle searches. Until the advent of deep learning, physicists reluctantly accepted the limitations of the shallow machine learning classifiers \u2014 laboriously constructing non-linear feature combinations to help guide shallow networks and BDTs. This benchmark study shows that advances in deep learning can lift these limitations by automatically discovering powerful feature combinations directly from low-level features. Similar conclusions were reached in other benchmark cases, such as in searches for tt\u0304h production [15]."
        },
        {
            "heading": "3. Parameterized Networks",
            "text": "The deep learning approach described above solves a simple signal versus background classification task for a hypothesized particle with a particular mass. But the mass of a hypothesized particle is generally unknown. In\npractice, a hypothesized particle will have a range of possible masses, each of which would produce a different type of signal in the data. The classification tasks at different masses are closely related, but distinct. Physicists need a way to evaluate this class of signal hypotheses against the null (background) hypothesis.\nA naive way to address this problem is to perform a finite number of individual comparisons. For each of K possible mass values, a hypothesized particle with that mass is simulated using Monte Carlo to produce a data set, and a machine learning model is trained to discriminate between it and the background. Methods from statistics can be used to account for the problem of multiple-hypothesis testing (e.g. the Bonferroni correction [16]). However, this naive approach is too conservative when the data distributions from different mass values are related. In practice, the distribution is expected to vary smoothly with the continuous parameter of the particle \u2014 that is, one expects similar mass values to result in similar distributions of observation data. This suggests the use of machine learning to model the relationship between the mass parameter and the data distribution.\nA machine learning solution to this problem is to train a single classifier to perform particle searches for an entire range of possible mass values [17, 18]. This is done by extending the list of input features to include one or more additional parameters that describe the larger scope of the problem such as a new particle\u2019s mass. The approach can be applied to any classification model; however, neural networks provide a smooth interpolation in this new parameter space, while tree-based classifiers may not. A single parameterized network can replace a set of individual networks trained for specific cases, as well as smoothly interpolate to cases where it has not been trained. In the case of a search for a hypothetical new particle, this greatly simplifies the task \u2013 by requiring only one network \u2013 as well as making the results more powerful \u2013 by allowing them to be interpolated between specific values. In addition, they may outperform isolated networks by generalizing from the full parameter-dependent dataset. In this section we describe the use of parameterized neural networks and provide a realistic example. For a real application, see Refs. [19\u201322].\n3.1. Parameterized Network Structure & Training\nA standard neural network takes as input a vector of features, x\u0304, and computes a function of these features, f(x\u0304). Parameterized networks address the case where the task is part of a larger context, described by one or more\nparameters, \u03b8\u0304, by computing a function of both inputs: f(x\u0304, \u03b8\u0304). Thus, a parameterized neural network makes different predictions for input x\u0304 for different contexts \u03b8\u0304; see Fig. 5. Unlike other networks, a parameterized network requires a value of \u03b8\u0304 to perform inference on input x\u0304, as the network output is a function of both.\nParameterized neural networks require some additional considerations during training. Each training example for such a parameterized network has the form (x\u0304, \u03b8\u0304, y)i, where y is the target output. However, in classification problems \u03b8\u0304 may not be meaningful for a particular target class. For example, the mass of a new particle is not meaningful for the background training examples. To avoid divulging information about y, one must randomly assign values [18] to \u03b8\u0304 according to the same distribution used for the signal class.\nAnother issue is that the distribution of \u03b8\u0304 in the training set represents a prior distribution that influences the final model, and should be specified carefully. Traditionally, \u03b8\u0304 is determined by the hypothesis and fixed during detector simulations, producing samples from a conditional distribution p(x\u0304|\u03b8\u0304, y); in parameterized neural networks, the training data would typically be generated by first sampling \u03b8\u0304 from a class-specific prior p(\u03b8\u0304|y) then x\u0304 from p(x\u0304|\u03b8\u0304, y). The robustness of the resulting parameterized classifier to the distribution of \u03b8\u0304 in the training sample will depend on the physics encoded in the distributions p(x\u0304|\u03b8\u0304, y) and how much they change with \u03b8\u0304. The prior p(\u03b8\u0304|y) should be chosen carefully and should be considered when interpreting results, just as one would carefully consider a fixed \u03b8\u0304 when building and evaluating a traditional classifier. In the studies presented below, the training data consists of equal sized samples for a few discrete values of \u03b8\u0304 \u2014 the conditional distribution p(x\u0304|\u03b8\u0304, y) varies smoothly enough in \u03b8\u0304 that this reasonably approximates a uniform prior over \u03b8\u0304.\n3.2. Physical Example\nParameterized networks address a common problem in searches for new particles of unknown mass, and we provide an illustrative example from Ref. [18]. Consider the search for a new particle X which decays to tt\u0304, examining the most powerful decay mode in which tt\u0304\u2192W+bW\u2212b\u0304\u2192 qq\u2032b`\u03bdb\u0304. The dominant background is standard model tt\u0304 production, which is identical in final state but distinct in kinematics due to the lack of an intermediate resonance. Figure 6 shows diagrams for the signal and background processes.\nThe set of event-level features include 21 low-level kinematic features resulting from reconstruction algorithms and 5 high-level features which incorporate physics domain knowledge. The distributions of four highlevel features are shown in Fig. 7 to illustrate the differences between the signal distribution at different values of the particle mass, mX , and the\nbackground distribution.\nIn order to test how well a parameterized neural network generalizes to new parameter values, an experiment compared the performance of a fixed neural network architecture trained on three different training data sets with different distributions of mX , and tested on a data with mX = 1000 GeV. The three different training data sets contained signal samples with different mass distributions: (1) mX = 1000 GeV only; (2) mX = 500, 750, 1000, 1250, 1500 GeV; and (3) mX = 500, 750, 1250, 1500 GeV (no mX = 1000 GeV). In each case, the training set contains 7M examples, the test set contains 1M, and approximately the same number of training and testing examples are used per mass point. On each data set, the same neural network architecture was trained, containing five 500-dimensional ReLU layers followed by a logistic output unit for binary classification. Parameters were initialized from a Gaussian distribution with mean zero and width 0.1, and updated using stochastic gradient descent with mini-\nbatches of size 100 and 0.5 momentum. The learning rate was initialized to 0.1 and decayed by a factor of 0.89 every epoch. Training was stopped after 200 epochs.\nThe results show that the parameterized network not only matches the performance of a network trained on a single mass value, but is able to generalize to mass values it has never seen before. Figure 8 shows that the parameterized network trained on mX = 500, 750, 1000, 1250, 1500 GeV matches the performance of the fixed network trained on mX = 1000 only. In the third data set, mX = 1000 samples are removed from the training set so that the network must interpolate its solution, but the performance is unchanged, demonstrating that the parameterized network is able to generalize even in this high-dimensional example.\nWe note, however, that while the ability of the parameterized network was demonstrated in this case, and we expect this ability to generalize due to networks excellent performance in interpolation tasks, one cannot claim to predict similar quality of interpolation for an arbitrary task. Performance in a specific task would require a dedicated study.\nThe high dimensionality of this problem makes it difficult to visually explore the dependence of the neural network output on the parameter mX . However, Fig. 9 compares the performance of the parameterized network to a single network trained at mX = 1000 GeV when applied across the mass range of interest, a common use case. This demonstrates the loss of performance incurred by some traditional approaches and recovered in this approach. Similarly, we see that a single network trained an unlabeled mixture of signal samples from all masses has reduced performance at each mass value tested."
        },
        {
            "heading": "4. Handling Sets of Four Vectors",
            "text": "The deep neural network architectures discussed so far have consisted entirely of sequential layers, where each layer is fully-connected to the layer below. However a key advantage of artificial neural network models is the ability to design neural network architectures that reflect properties of the data. These architecture design choices enable us to constrain the class of functions to be considered, or more generally, incorporate an implicit bias for some functions over others. Examples include convolutional neural networks, Siamese neural networks, and various forms of recursive neural networks [23]. These architecture designs have been critical to the success of deep learning in computer vision, natural language processing, and bioin-\nformatics. In this section we examine the neural network design choices that can be used to handle sets of four vectors in physics.\nNeural networks can be designed to have two key properties that are relevant to sets: invariance and equivariance. First, a function implemented by a neural network is invariant with respect to an operation if applying that operation to the input does not affect the output. A common example from deep learning is object detection with a convolutional neural network that is invariant to translations of the input image \u2014 the function output could be a single value corresponding to whether an object is present in the image, regardless of whether a translation operation is applied to the image. Second, a function implemented by a neural network is said to be equivariant with respect to an operation if applying that operation to the input results in a predictable change in the output. In a convolutional neural network, each convolutional layer is equivariant to translations because\ntranslating the input leads to a deterministic translation in the output representation.\nFor machine learning models that take sets as inputs, it is often desirable to have a model that is invariant or equivariant with respect to permutation of the set elements. For example, 3D vision models perform object detection from a set of 3D points on the surface of an object [24], and astronomy models predict the redshift of galaxies from a set of nearby galaxies [25, 26]. Similarly, exotic particle searches in physics involve classifying collision events based on sets of resulting four vectors. The function to be learned should be invariant to the ordering of the set elements. There are at least three ways to try to create supervised neural network models that are invariant to an operation: (1) data-augmentation, (2) canonicalization, and (3) architecture design. We discuss each in turn.\n4.1. Data-Augmentation\nIn data-augmentation, the training data is expanded by applying an operation to all training examples. In practice, it is usually more efficient to apply a random operation to the input data at training time. Either way, the network is forced to learn how to be invariant to that operation. For example, in object-detection models it is common to augment the data during training with random translations, rotations, and mirroring operations. On four vectors, ordering and boosting operations [27, 28] can augment the data. The disadvantage of this approach is that the model must learn that the output should be invariant \u2014 this requirement is not enforced by the model. Because this can make the learning problem much more difficult, it is typically used as a last resort when the other methods are not available.\n4.2. Canonicalization\nThe second way to achieve invariance is through canonicalization of the input. In this method, the input is always mapped to some canonical element of the group defined by the operation, which enforces invariance without any other constraints on the model. An example is to enforce translational invariance in computer vision by \u201ccentering\u201d an image at some deterministically-chosen point, or enforcing rotational invariance by rotating the image around that point until it is oriented along some canonical axis (as in Ref. [29] for jet substructure classification). For sets of four vectors, permutation invariance can be achieved by sorting the particles based on pT, as is done in Ref. [30]. A potential disadvantage of this approach, besides having to come up with a good canonicalization scheme, is that the canonicalization procedure can introduce discontinuities in the function to be learned \u2014 a canonicalization that is sensitive to small changes in the inputs is undesirable, and could be worse than no canonicalization at all.\n4.3. Architecture Design\nThe third way to achieve equivariance and invariance in machine learning is through architecture design, where the hypothesis space is constrained to functions that satisfy the condition. For example in convolutional neural network architectures, the convolution layers are equivariant to translations, and together with pooling layers they can be made invariant to translations. These architectures have been critical to the success of deep learning in computer vision [31]. Invariance to other input transforma-\ntions can also be enforced through combinations of weight-sharing and pooling operations. Ideas from Lie group theory can be applied to this problem [32, 33]. One can trivially define neural network architectures that guarantee invariance to any transformation by defining an ensemble model that applies identical subnetworks to every possible transformation of the input, then pooling the result. Clearly this becomes intractable \u2014 or at least inefficient \u2014 for applications where the number of elements in the group is large or infinite, but there are often simpler approaches.\nFor input sets, permutation invariance can be achieved by: (1) applying an identical subnetwork to each set element, using shared weights; then (2) pooling the output. The shared weights result in equivariance to permutations of the inputs, since the new outputs will be equivalent to the permutation of the original outputs. The second step achieves invariance, e.g. with max or mean pooling of the possibly-multidimensional outputs. Designing neural network architectures that account for data symmetries like permutation invariance is one example of incorporating physics knowledge into the machine learning model, which is discussed more in the next section."
        },
        {
            "heading": "5. Physics-aware networks",
            "text": "In applying machine learning to physics problems, one is often presented with the challenge of bringing physics knowledge to bear on the machine learning models [34, 35]. This situation can present itself in different forms: choosing of the relevant input and output variables, adding priors or regularization terms in the loss function, or imposing constraints on the neural architectures. Each of these contributes to explicit or implicit model bias, which can greatly affect the resulting performance. Often it is difficult to predict how these choices will affect performance, so they are treated as hyperparameters and optimized by trying different variations. Here we consider two different situations corresponding to physics-informed architecture design and incorporation of physics constraints.\n5.1. Physics-Informed Architecture Design\nThe permutation-invariant models described above are one example of incorporating domain-knowledge into a neural network architecture. We can design neural network architectures that account for additional physics knowledge by taking advantage of other architecture design motifs. These\ninclude the local connectivity, weight sharing, and pooling of convolutional neural networks, but also skip connections [36], gating [37, 38], and attention [39\u201341]. We briefly discuss two other physics-informed neural network architectures applicable to four vectors.\nOne example of a physics-informed neural network architecture is Ref. [34]. Decaying particles in the detector typically result in decay products that are hierarchically clustered in space and time (jet substructures). Thus, sets of four vectors often have additional structure that can be exploited. When the clustering hierarchy of each event can be reconstructed, for example using a sequential recombination jet algorithm [42], this additional information can be incorporated into the network. Recursive neural network architectures can be constructed to match the topology of the jet clustering algorithms, analogous to models from Natural Language Processing that take advantage of sentence parse trees [43, 44]. The recursive physics neural network architecture is constructed on a per-event basis to reflect the tree structure of that event. In addition to the properties of permutation invariance (assuming each node is permutation invariant) and scalability to an arbitrary number of set elements, this model has the additional property of local connectivity among related elements in the set, which can lead to better generalization.\nAnother example is the Lorentz-Boosted Neural Networks in Ref. [45], in which the first hidden layer of the network is interpreted as \u201ccomposite particles\u201d and corresponding \u201crest frames,\u201d and represented as linear combinations of the input four vectors. Each learned composite particle is then boosted into its corresponding rest frame using the non-linear Lorentz transformation. The resulting feature representations are then fed into a neural network, and the entire system is trained using back-propagation. The major advantage of this architecture is that it constrains the representation of the data into a form that is readily interpreted by physicists (i.e. Lorentz-transformed four vectors) and for which physically meaningful features can be extracted such as invariant masses, pseudorapidities, and so forth.\nYet another example is the approach described in [46], which uses recursive neural networks, of the form of transformer architectures [41, 47] used in language processing and tensor attention mechanisms, applied to manyjet event reconstruction in a manner that is invariant to any permutation of the four vectors in the variable-size input set.\n5.2. Incorporating Physics Constraints\nHere we consider the situation where there are physical laws, in the form of exact equations, relating the values of some of the relevant variables. In addition to physics, many fields of science and engineering (e.g., fluid dynamics, hydrology, solid mechanics, chemistry kinetics) have exact, often analytic, closed-form constraints, i.e. constraints that can be explicitly written using analytic functions of the system\u2019s variables. Examples include translational or rotational invariance, conservation laws, or equations of state. While physically-consistent models should enforce constraints to within machine precision, data-driven algorithms often fail to satisfy wellknown constraints that are not explicitly enforced. In particular, while neural networks may provide powerful classification and regression tools for nonlinear systems, they may optimize overall performance while violating these constraints on individual samples.\nDespite the need for physically-informed neural networks for complex physical systems [48\u201351], enforcing constraints [52] has been limited mostly to physical systems governed by specific equations, such as advection equations [53\u201355], Reynolds-averaged Navier-Stokes equations [56, 57], or quasigeostrophic equations [58]. Thus it is necessary to have methods that can enforce analytic constraints in more general settings. Here we describe two general ways for enforcing constraints, first in a soft way, and then in a hard way.\nIn general, let us assume that there is a constraint of the form C(x, y, z) = 0 that must be satisfied by the input variables x, the output variables y, and possibly some auxiliary variables z. If E is the error function of the neural network trained on the pairs (x, y), we can enforce the constraints in a soft way by adding a penalty term to the loss function, e.g. using a new loss function of the form E \u2032 = E + \u03bbC2 where \u03bb is an additional hyperparameter controlling the strength of the corresponding regularization (or equivalently log prior) terms. This approach has been used for instance in climate modeling [59\u201361]While this approach can be effective, there is no guarantee that the constraints may not be violated.\nA general way for enforcing constraints in a hard way is described in [62]. There are several possible implementation of this idea, but the gist of it is to augment the basic neural architecture with an additional neural network to enforce the constraints. For this, we can first decompose y nonuniquely as y = (y1, y2) . Then we introduce a first neural network with adaptive weights that produces an output y\u20321, trying to predict y1 from x.\nThis is followed by a second network which computes y\u20322 from x, y \u2032 1 and z, enforcing the constraint C to machine precision. The weights of the second network are fixed and determined by the knowledge of C. For instance, the second network can be linear if the constraint C is linear. We can then combine the two networks into a single overall architecture whose final output is the vector (y\u20321, y \u2032 2). This output always satisfies the constraint C by construction. Furthermore, it can be compared to the target (y1, y2) and the resulting errors can be backpropagated through the combined network, through both the fixed and adjustable weights. As a result of this approach, the constraint C is satisfied at all times, both during and after learning."
        },
        {
            "heading": "6. Conclusions",
            "text": "We have reviewed the advent of deep learning in high-energy physics, first used in classification tasks operating on four-vector features before being applied to tracks, images, graphs, and low-level detector data. Even in the case of four-vectors where the number of features is relatively small, deep learning can be used to improve classification performance and incorporate domain knowledge in various forms. In particular, neural networks can be designed to model a set of related functions using parameterized networks, capture permutation invariance in sets with weight-sharing and pooling, and incorporate additional physics constraints through architecture design or augmented loss functions."
        }
    ],
    "title": "Deep Learning From Four Vectors",
    "year": 2022
}