{
    "abstractText": "Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-toimage models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard training convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on Stable Diffusion for 48 different concepts and three personalization methods demonstrate the competitive performance of our approach, which makes adaptation up to 8 times faster with no significant drops in quality.",
    "authors": [
        {
            "affiliations": [],
            "name": "Anton Voronov"
        },
        {
            "affiliations": [],
            "name": "MIPT"
        },
        {
            "affiliations": [],
            "name": "Yandex"
        },
        {
            "affiliations": [],
            "name": "Mikhail Khoroshikh"
        }
    ],
    "id": "SP:30ed66cb421e0b507c6e062e7abf74fdd7cd8746",
    "references": [
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H. Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Nupur Kumari",
                "Bingliang Zhang",
                "Richard Zhang",
                "Eli Shechtman",
                "Jun-Yan Zhu"
            ],
            "title": "Multiconcept customization of text-to-image diffusion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Clipscore: A reference-free evaluation metric for image captioning",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho"
            ],
            "title": "Classifier-free diffusion",
            "venue": "guidance. ArXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Hila Chefer",
                "Yuval Alaluf",
                "Yael Vinker",
                "Lior Wolf",
                "Daniel Cohen-Or"
            ],
            "title": "Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2023
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro"
            ],
            "title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers",
            "venue": "arXiv preprint arXiv:2211.01324,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Patrick von Platen",
                "Suraj Patil",
                "Anton Lozhkov",
                "Pedro Cuenca",
                "Nathan Lambert",
                "Kashif Rasul",
                "Mishig Davaadorj",
                "Thomas Wolf"
            ],
            "title": "Diffusers: State-of-the-art diffusion models",
            "venue": "https: //github.com/huggingface/diffusers,",
            "year": 2022
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Andrey Voynov",
                "Qinghao Chu",
                "Daniel Cohen-Or",
                "Kfir Aberman"
            ],
            "title": "p+: Extended textual conditioning in text-to-image generation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ligong Han",
                "Yinxiao Li",
                "Han Zhang",
                "Peyman Milanfar",
                "Dimitris Metaxas",
                "Feng Yang"
            ],
            "title": "Svdiff: Compact parameter space for diffusion fine-tuning",
            "year": 2023
        },
        {
            "authors": [
                "Bahjat Kawar",
                "Shiran Zada",
                "Oran Lang",
                "Omer Tov",
                "Huiwen Chang",
                "Tali Dekel",
                "Inbar Mosseri",
                "Michal Irani"
            ],
            "title": "Imagic: Text-based real image editing with diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Prompt-to-prompt image editing with cross attention control",
            "venue": "arXiv preprint arXiv:2208.01626,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Hao Liew",
                "Hanshu Yan",
                "Daquan Zhou",
                "Jiashi Feng"
            ],
            "title": "Magicmix: Semantic mixing with diffusion models",
            "venue": "arXiv preprint arXiv:2210.16056,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Null-text inversion for editing real images using guided diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 Jegou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large text-to-image synthesis models have recently attracted the attention of the research community due to their ability to generate high-quality and diverse images that correspond to the user\u2019s prompt in natural language [1, 2, 3, 4, 5]. The success of these models has driven the development of new problem settings that leverage their ability to draw objects in novel environments. One particularly interesting task is personalization (or adaptation) of text-to-image models to a small dataset of images provided by the user. The goal of this task is to learn the precise details of a specific object or visual style captured in these images: after personalization, the model should be able to generate novel renditions of this object in different contexts or imitate the style that was provided as an input.\nSeveral recent methods, such as Textual Inversion [6], DreamBooth [7], and Custom Diffusion [8], offer easy and parameter-efficient personalization of text-to-image models. Still, a major obstacle on the path to broader use of such methods is their computational inefficiency. Ideally, models should adapt to user\u2019s images in real or close to real time. However, as noted by the authors of the first two papers, the training time of these methods can be prohibitively long, taking up to two hours for a single concept. The reported training time for Custom Diffusion is 12 minutes per concept on a single GPU, which is much faster but still outside the limits of many practical applications.\n\u2217Equal contribution. Correspondence to mryabinin0@gmail.com\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 2.\n04 84\n1v 3\n[ cs\n.C V\n] 1\nIn this work, we seek answers to the following questions: are text-to-image customization techniques inherently time-consuming and can we decrease their runtime without major quality tradeoffs? Focusing on the training dynamics, we observe that the CLIP [9] image similarity score (often used to assess image quality in such tasks [10]) grows sharply only in the early stages of Textual Inversion and hardly improves after that. However, as shown in Figure 1, neither the training loss nor the gradient norm indicate the convergence of the concept embedding, which prevents us from stopping the adaptation process earlier. While it is possible to score samples from the model with CLIP during training, generating them can take quite a long time.\nWith that in mind, we study the training objective itself and attempt to understand the reasons of its non-informative dynamics. As we demonstrate, the primary cause lies in several sources of stochasticity (such as diffusion time steps or the diffusion noise) introducing noise to the loss function. If we sample random variables only once and fix them across training steps, the loss for these inputs reflects convergence better even when training with the original (fully stochastic) objective.\nMotivated by this finding, we propose Deterministic VARiance Evaluation (DVAR), an early stopping criterion for text-to-image customization methods that generates a single fixed batch of inputs at the beginning of training and evaluates the model on this batch after each optimization step. This criterion is straightforward to implement, has two interpretable hyperparameters, and its stopping time corresponds to convergence in terms of the CLIP image score. We validate DVAR by comparing it with a range of baselines on three popular adaptation methods, showing that it is possible to run methods like Textual Inversion and DreamBooth much faster with up to 8x speedups and little to no decline in quality. For Custom Diffusion, our method recovers the optimal number of training iterations, which makes it useful to avoid empirical tuning of the step count for every specific dataset.\nIn summary, our contributions are as follows:\n\u2022 We investigate the difficulties in detecting the convergence of text-to-image adaptation methods from training-time metrics. As we demonstrate, the objective becomes much more interpretable across iterations if computed on the same batch of inputs without resampling any random variables.\n\u2022 We propose DVAR, a simple early stopping criterion for text-to-image personalization techniques. This criterion is easy to compute and use, does not affect the training process and correlates with convergence in terms of visual fidelity.\n\u2022 We compare this criterion with multiple baselines on Stable Diffusion v1.5, a popular text-to-image model, across three methods: Textual Inversion, DreamBooth, and Custom Diffusion.1 DVAR offers a significant decrease in runtime while having quality comparable to both original methods and other early stopping baselines.\n1The code of our experiments is available at github.com/yandex-research/DVAR."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Denoising Diffusion Probabilistic Models",
            "text": "Diffusion models [11] are a class of generative models that has become popular in recent years due to their ability to generate both diverse and high-fidelity samples [12]. They approximate the data distribution through iterative denoising of a variable zt sampled from Gaussian distribution. In simple words, the model \u03f5\u03b8 is trained to predict the noise \u03f5, following the objective below:\nmin \u03b8\nEz0, \u03f5\u223cN(0,I), t\u223cU [1,T ]||\u03f5\u2212 \u03f5\u03b8(zt, c, t)||22. (1)\nHere, zt corresponds to a Markov chain forward process zt(z0, \u03f5) = \u221a \u03b1tz0 + \u221a 1\u2212 \u03b1t\u03f5 that starts from a sample z0 of the data distribution. For example, in the image domain, z0 corresponds to the target image, and zT is its version with the highest degree of corruption. In general, c can represent the condition embedding from any other modality. The inference (reverse) process occurs with a fixed time step t and starts from zt equal to a sample from the Gaussian distribution."
        },
        {
            "heading": "2.2 Text-to-image generation",
            "text": "The most natural and well-studied source of guidance for generative models is the natural language [13, 14, 15] because of its convenience for the user, ease of collecting training data, and significant improvements in text representations over the past several years [5]. The condition vector c is often obtained from Transformer [16] models like BERT [17] applied to the input text.\nState-of-the-art text-to-image models perform forward and reverse processes in the latent space of an autoencoder model z0 = E(x), x = D(z0), where x is the original image, and E ,D are the encoder and the decoder, respectively. We experiment with Stable Diffusion v1.5 [3], which uses a variational autoencoder [18] for latent representations. Importantly, this class of autoencoder models is not deterministic, which makes inference even more stochastic but leads to a higher diversity of samples. In total, given image and caption distributions X,Y , the training loss can be formulated as:\nLtrain = Ey,x,\u03f5||\u03f5\u2212 \u03f5\u03b8(zt(E(x), \u03f5), c(y), t)||22, (2) y \u223c Y, x \u223c X, \u03f5 \u223c N (0, I), t \u223c U [0, T ] (3)"
        },
        {
            "heading": "2.3 Adaptation of text-to-image models",
            "text": "While text-to-image generation models are flexible because of the natural language input, it is often difficult to design a prompt that corresponds to an exact depiction of an object of choice. Hence, several methods for adapting such models to a given collection of images were developed. In this work, we focus on the most well-known methods, discussing others in Appendix A.\nMost approaches inject the target concept into the text-to-image model by learning the representation of a new token v\u0302 inserted into the language encoder vocabulary. The representation is learned by optimizing the reconstruction loss from Equation 2 for a few (typically 3 \u2013 5) reference images I with respect to the embedding of v\u0302, while other parts of the model are frozen. The main advantage of these methods is the ability to flexibly operate with the \u201cpseudo-word\u201d v\u0302 in natural language, for example, by placing the concept corresponding to that word into different visual environments.\nTextual Inversion [6] is the simplest method for adaptation of text-to-image models that updates only the token embedding for v\u0302. While this method is parameter-efficient, its authors report that reaching acceptable inversion quality requires 6000 training iterations, which equals \u22482 GPU hours on most machines. This number is a conservative estimate of the number of steps sufficient for all concepts: authors of the method propose selecting the earliest checkpoint that exhibits sufficient train image reconstruction quality2.\nWhile Textual Inversion only learns the embedding of the target token, DreamBooth [7] does the same with a fully unfrozen U-Net component, and Custom Diffusion [8] updates the projection matrices of cross-attention layers that correspond to keys and values. These methods use a significantly smaller number of iterations, 1000 and 500 respectively, which is also not adaptive and can lead to overfitting.\n2github.com/rinongal/textual_inversion/issues/34#issuecomment-1230831673"
        },
        {
            "heading": "3 Understanding adaptation dynamics",
            "text": "As we explained previously, the goal of our study is to find ways of speeding up text-to-image without significantly degrading the quality of learned concept representations. To accomplish this goal, we analyze the optimization process of running Textual Inversion on a given dataset. As we demonstrate in Section 4 and in Appendix B, the same findings hold for DreamBooth and Custom Diffusion, which makes our analysis broadly applicable to different methods for customized text-to-image generation.\nWe apply Textual Inversion to concepts released by Gal et al. [6], using Stable Diffusion v1.53 as the base model. We monitor several metrics during training:\n1. First, one would hope to observe that optimizing the actual training objective would lead to its convergence, and thus we track the value of Ltrain.\n2. Second, we monitor the gradient norm, which is often used for analyzing convergence in non-convex optimization. As the model converges to a local optimum, the norm of its gradient should also decrease to zero or stabilize at a point determined by the gradient noise.\n3. Lastly, every 50 training iterations, we generate 8 samples from the model using the current concept embedding and score them with the CLIP image similarity score using the training set as references. In the original Textual Inversion paper, this metric is named the reconstruction score and is used for quantitative evaluation.\nNote that we do not rely on the CLIP text-image score for captions: in our preliminary experiments, we observed no identifiable dynamics for this metric when using the entire set of CLIP caption templates. Writing more specific captions and choosing the most appropriate ones for each concept takes substantial manual effort; hence, we leave caption alignment out of the scope for this study."
        },
        {
            "heading": "3.1 Initial observations",
            "text": "First, we would like to view the training dynamics in terms of extrinsic evaluation: by measuring how the CLIP image score changes throughout training, we can at least estimate how fast the samples begin to resemble the training set. For this, we perform inversion of all concepts released by [6, 7, 8] (a total of 48 concepts): an example of such an experiment for one concept is available in Figure 2.\nFrom these experiments, we observe that the CLIP image score exhibits sharper growth at an early point of training (often within the first 1000 iterations) and stabilizes later. This finding agrees with the results of our own visual inspection: the generated samples for most concepts undergo the most drastic changes at the beginning and do not improve afterwards. Practically, this observation means that we can interrupt the inversion process much earlier without major drawbacks if we had a criterion for detecting its convergence. What indicators can we use to create such a criterion?\nThe most straightforward idea is to consider the training loss Ltrain. Unfortunately, it is not informative by default: as we demonstrate in Figure 2, the loss exhibits too much noise and has no signs of convergence. The gradient norm of the concept embedding is also hardly informative: in the same experiment, we can see that it actually increases during training instead of decreasing. As we show in Appendix C, these findings generally hold even for much larger training batches, which means that the direct way of making the loss less stochastic is not practical. Still, as reported in the original work and shown by samples and their CLIP scores, the model successfully learns the input concepts. Curiously, we see no reflection of that in the dynamics of the training objective.\nAnother approach to early stopping is to leverage our observations about the CLIP image score and measure it during training, terminating when the score fails to improve for a specific number of iterations. However, there are two downsides to this approach. First, frequently sampling images during training significantly increases the runtime of the method. Second, this criterion can be viewed as directly maximizing the CLIP score, which is known to produce adversarial examples for CLIP instead of actually improving the image quality [2].\n3We chose this version because it was given as a default in the Diffusers [19] example for Textual Inversion."
        },
        {
            "heading": "3.2 Investigating the sources of randomness",
            "text": "We hypothesize that the cause of excessive noise in Ltrain is several factors of randomness injected at each training step, as we mentioned previously in Section 2.2. Thus, we aim to estimate the influence of the following factors on the dynamics of the inversion objective:\n1. Input images x 2. Captions corresponding to images y 3. VAE Latent representations for images E(x) 4. Diffusion time steps t 5. Gaussian diffusion noise \u03f5\nNow, our goal is to identify the factors of stochasticity that affect the training loss. Importantly, we do not change the training batches, as it alters the objective of Textual Inversion and might affect its outcome. Thus, we train the model in the original setting (with batches of entirely random data) but evaluate it on batches with some sources of randomness fixed across all iterations. Note that E(x) depends on x: if we resample the input images, we also need to resample their latent representations.\nFirst, we try the most direct approach of making everything deterministic: in other words, we compute Ldet, which is the same as Ltrain, but instead of the expectation over random data and noise, we compute it on the same inputs after each training step. Formally, we can define it as\nLdet = ||\u03f5\u2212 \u03f5\u03b8(zt(E(x), \u03f5), c(y), t)||22, (4) with x, y, E(x), t, and \u03f5 sampled only once in the beginning of inversion. Essentially, this means that the only argument of this function that changes across training iterations is c(y) that depends on the trained concept embedding.\nAs we show in Figure 2, this version of the objective becomes informative, indicating convergence across a broad range of concepts. Moreover, it displays approximately the same behavior as the CLIP score and is much less expensive to compute, which makes Ldet particularly useful as a metric for the stopping criterion.\nFor the next step, we aim to find if any of the above sources of stochasticity have negligible impact on the noise in Ltrain or can be compensated with larger batches. We evaluate them separately and provide results in Section 4.5. Our key findings are that (1) resampling captions and VAE encoder noise still preserves the convergence trend, (2) using random images or resampling diffusion noise reveals the training dynamics only for large batches, and (3) sampling different diffusion timesteps leads to a non-informative training loss regardless of the batch size. Still, for the sake of simplicity and efficiency, we compute Ldet on a batch of 8 inputs sampled only once at the beginning of training for the rest of our experiments."
        },
        {
            "heading": "3.3 Deterministic Variance Evaluation",
            "text": "The results above show that fixing all random components of the textual inversion loss makes its dynamics more interpretable. To achieve our final goal of decreasing the inversion runtime, we need to design an early stopping criterion that leverages Ldet to indicate convergence. We propose Deterministic Variance Evaluation (DVAR), a simple variance-based early stopping criterion. It maintains a rolling variance estimate of Ldet over the last N steps, and once this rolling variance becomes less than \u03b1 of the global variance estimate (0 < \u03b1 < 1), we stop training. A pseudocode implementation of DVAR is shown in Figure 3.\nThis criterion is easy to implement and has two hyperparameters that are easy to tune: the window size for local variance estimation N and the threshold \u03b1. In our experiments, we found {N = 310, \u03b1 = 0.15} for Textual Inversion, {N = 440, \u03b1 = 0.4} for DreamBooth, and {N = 180, \u03b1 = 0.15} for Custom Diffusion to work relatively well across all concepts we evaluated.\nImportantly, we use this criterion while training in the regular fully stochastic setting: our goal is not to modify the objective, and using fixed random variables and data can affect the model\u2019s generalization capabilities. As we demonstrate in Section 4, our approach demonstrates significant improvements when compared to baselines, even when all sources of randomness are fixed.\nAlong with DVAR, we tested other early stopping strategies that use Ldet and are based on different notions of loss value stabilization, such as estimating the trend coefficient or tracking changes in the mean instead of variance. As we show in Appendix E, most of them result in less reliable convergence indicators and have hyperparameters that do not transfer as well between different image collections."
        },
        {
            "heading": "4 Experiments",
            "text": "In this section, we compare DVAR with several baselines in terms of sample fidelity for the learned concept, output alignment with novel prompts, and training time. Our goal is to verify that this early stopping criterion is broadly applicable and has little impact on the outcome of training."
        },
        {
            "heading": "4.1 Setup and data",
            "text": "We evaluate approaches to early stopping on three popular text-to-image personalization methods: Textual Inversion, DreamBooth with LoRA [20], and Custom Diffusion, all applied to Stable Diffusion v1.54. For Textual Inversion, we use the implementation from the Diffusers library5 [19] and take the hyperparameters from the official repository6. We use the official code and hyperparameters7 for Custom Diffusion, and we use the implementation of DreamBooth-LoRA from the Diffusers library8.\nFor evaluation, we combine the datasets published by authors of the three techniques above that were available as of March 2023, which results in a total of 48 concepts. Our main results are obtained by applying each method to all of these concepts, which in total took around 80 GPU hours. Each experiment used a single NVIDIA A100 80GB GPU. Although the methods we study can be used with several concepts at a time, we train on the images of each concept separately. Following Equation 2, the training batches are generated from a small set of original images with randomly sampled augmentations (central/horizontal crop), captions, timesteps and diffusion noise.\n4huggingface.co/runwayml/stable-diffusion-v1-5 5github.com/huggingface/diffusers/tree/main/examples/textual_inversion 6github.com/rinongal/textual_inversion 7github.com/adobe-research/custom-diffusion 8github.com/huggingface/diffusers/tree/main/examples/dreambooth-lora\nTo automatically assess the quality of generated images, we employ CLIP image-to-image similarity for images generated from prompts used during training (\u201cTrain CLIP img\u201d), which allows us to monitor the degree to which the target concept is learned. However, relying solely on this metric can be risky, as it does not capture overfitting. To measure generalization, we utilize CLIP text-to-image similarity, evaluating the model\u2019s ability to generate the concept in new contexts with prompts that are not seen at training time (\u201cVal CLIP txt\u201d). For each method, we report both the number of iterations and the average adaptation runtime, since the average duration of one iteration may differ due to intermediate image sampling or additional loss computation. In addition, we evaluate the identity preservation and diversity of all methods in Appendix F."
        },
        {
            "heading": "4.2 Baselines",
            "text": "We compare DVAR with several baselines: the original training setup of each adaptation method (named as \u201cBaseline\u201d), early stopping based on the CLIP similarity of intermediate samples to the training set (\u201cCLIP-s\u201d), as well as the original setup with the reduced number of iterations and no intermediate sampling (\u201cFew iters\u201d).\nSpecifically, the original setup runs for a predetermined number of steps (6100, 1000, and 500 for Textual Inversion, Dreambooth-LoRA, and Custom Diffusion accordingly), sampling 8 images every 500/100/100 iterations and computing their CLIP similarity to the training set images. For each customization method, we tune the hyperparameters of CLIP-s: sampling frequency (number of training iterations between generating samples), as well as the threshold for minimal improvement and the number of consecutive measurements without improvement before stopping. These hyperparameters are chosen on a held-out set of 4 concepts to achieve 90% of the maximum possible train CLIP image score for the least number of iterations. The final checkpoint is selected from the iteration with the best train CLIP image score.\nAfter running CLIP-s on all concepts, we determine the average and maximum number of steps before early stopping across concepts. Then, we run the baseline method with a reduced number of iterations and no intermediate sampling. This baseline can serve as an estimate of the minimum time required for the adaptation to converge, as it has the most efficient iterations among all methods. However, in real-world scenarios and for new customization approaches, the exact number of iterations is unknown in advance, which makes this method less applicable in practice, as it would involve rerunning such large-scale experiments for every new approach."
        },
        {
            "heading": "4.3 Results",
            "text": "The results of our experiments are shown in Table 1. As we can see, DVAR is more efficient than the baseline and CLIP-s in terms of the number of iterations and overall runtime, approaching the performance of \u201cFew iters\u201d (which can be considered an oracle method) while being fully inputadaptive and not relying on costly intermediate sampling. In particular, the fixed step count of \u201cFew Iters\u201d results in higher standard deviations for almost all metrics: from a practical standpoint, it means that this number of iterations might be either sufficient or excessive when adapting a model to a given set of images. We additionally discuss the need for the adaptive number of iterations in Appendix G.\nFurthermore, although CLIP-s and the original setup optimize the train CLIP image score by design (in case of the baseline, this metric is used for choosing the best final checkpoint), DVAR is able to achieve nearly the same final results. Finaly, with a sufficiently long training time, the increase in the train CLIP image score leads to a decline in the validation CLIP text score, which indicates overfitting on input images: this phenomenon is illustrated in more detail in Figure 4 and Appendix H. As we can see from the CLIP text scores of DVAR, early stopping helps mitigate this issue."
        },
        {
            "heading": "4.4 Human evaluation",
            "text": "To verify the validity of our findings, we compare the quality of images obtained with DVAR with that of the original methods. To do that, we used a crowdsourcing platform, asking its users to compare samples of the baseline approach and samples from the model finetuned for the number of iterations determined by DVAR.\nWe conducted two surveys: the first survey compared samples derived from prompts seen by the model during training, measuring the reconstruction ability of adaptation. Participants were instructed\nto select the image that resembled the reference object the most. The second survey compared samples generated using novel prompts, measuring the final model capability for customization. Participants were asked to determine which image corresponded better to the given prompt. Each pair of images was assessed by multiple participants, ensuring an overlap of 10 responses. For each set of 4 responses, we paid 0.1$, and the annotation instructions can be viewed in Appendix I.\nOur findings obtained from these evaluations are shown in Table 2. As we can see, DVAR allows for early stopping without sacrificing the reconstruction quality for two out of three evaluated customization methods. Hence, our approach enables efficient training and reduces the computational cost of finetuning on new concepts. While applying DVAR to Textual Inversion slightly reduces the reconstruction quality, this can be attributed to the overfitting of the original method: other methods, which use significantly fewer iterations, are able to avoid this issue."
        },
        {
            "heading": "4.5 Analysis and ablation",
            "text": "Having demonstrated the advantages of DVAR, we now perform a more detailed analysis of the effect of the changes we introduce to the procedure of text-to-image adaptation. We aim to answer a series of research questions around our proposed criterion and the behavior of the training objective with all factors of variation fixed across iterations.\nIs it possible to observe convergence without determinism? To answer this question, we conduct a series of experiments where we \u201cunfix\u201d each component responsible for the stochasticity of the training procedure one by one. For each component, we aim to find the smallest size of the batch that preserves the indicativity of the evaluation loss. For large batch sizes, we accumulate losses from several forward passes.\nFigure 5 shows the results of these experiments. While for some parameters increasing the batch size leads to a more evident trend in loss, the stochasticity brought by varying the timesteps t on each iteration cannot be eliminated even with batches as large as 512. This can be explained by the scale of the reconstruction loss being highly dependent on the timestep of the reverse diffusion process. Thus, aggregating it over multiple different points makes it completely uninformative, as the scale of the trend is several orders of magnitude less than the average values of the loss.\nIs it possible to use smaller batches for evaluation and still detect convergence? To address this question, we ran experiments with deterministic batches of size 1, 2 and 4 for multiple concepts and generally observed the same behavior (as illustrated in Figure 8 of Appendix C). However, the dynamics of the objective become more dependent on the timesteps that are sampled: as a result, the early stopping criterion becomes more brittle. Hence, we use a batch size of 8 in our main experiments, as it corresponds to a reasonable tradeoff between stability and computational cost.\nDoes the selection of diffusion timesteps influence the behavior of the deterministic objective? In our approach, timesteps are uniformly sampled from a range of 0 to 1000 once at the beginning of the training process. However, according to the analysis presented in prior work [15], timesteps from\ndifferent ranges have varying effects on the results of text-to-image generation. This raises a question of whether it is possible to sample timesteps only from a specific range, as it might lead to a higher correlation between the loss and the target metrics. Our findings in Appendix J indicate that there is no significant difference in correlations when we change the sampling interval."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we analyze the training process of text-to-image adaptation and the impact of different sources of stochasticity on the dynamics of its objective. We show that removing all these origins of noise makes the training loss much more informative during training. This finding motivates the creation of DVAR, a simple early stopping criterion that monitors the stabilization of the deterministic loss based on its running variance. Through extensive experiments, we verify that DVAR reduces the training time by 2 \u2013 8 times while still achieving image quality similar to baselines.\nOur work offers a robust and efficient approach for monitoring the training dynamics without requiring computationally expensive procedures. Moreover, it has adaptive runtime depending on the input concept, which naturally mitigates overfitting. These advantages make DVAR a useful tool for textto-image researchers and practitioners, as it provides a more efficient and controlled training process, ultimately leading to improved results while avoiding the unnecessary computational overhead."
        },
        {
            "heading": "A Additional related work",
            "text": "Most existing methods for text-to-image personalization achieve the goal by finetuning diffusion models. In addition to the approaches evaluated in our work, there exist many other techniques that were recently proposed. For example, in P+ [21], the concept is inverted into a sequence of per-layer tokens, and SVDiff [22] focuses on learning the singular values of the SVD decomposition of weight matrices. Imagic [23] divides the adaptation process into two phases. In the first phase, this method optimizes the embedding in the space of texts instead of tokens, and in the second one, it trains the model separately to reconstruct the target image from the embedding. The optimized and target embeddings can then be used for image manipulation by interpolating between them. Furthermore, adaptation can also be performed with gradient-free methods using evolutionary algorithms [24].\nText-to-image model customization can also be broadly viewed as an instance of image editing: given the caption and the image generated from it, we want to change the picture according to the new prompt. There are several methods in this field that use diffusion models and have similar problem statements. For example, Prompt-to-Prompt [25] solves the task by injecting the new prompt into cross-attention, and MagicMix [26] replaces the guidance vector after several steps during inference. The intrinsic disadvantage of such methods is the need for an additional inversion step for editing existing images, like DDIM [27] or Null-text Inversion [28].\nB Indicators of convergence for other customization methods\nWe provide examples of the behavior of standard model convergence indicators for other techniques for text-to-image personalization in Figures 6 and 7. While the reconstruction loss and the gradient norm are still hardly informative, both the CLIP image score and Ldet exhibit a trend that corresponds to improvements in the visual quality of samples."
        },
        {
            "heading": "C The impact of the batch size on convergence dynamics",
            "text": "As we increase the batch size for our evaluation objective, we sample a more diverse range of timesteps, making the loss less dependent on the random seed and more indicative of convergence. Figure 8 provides an illustration of this phenomenon.\nMoreover, we provide the results of training in a regular Textual Inversion setup with much larger batches. As we demonstrate in Figure 9a, the loss dynamics remain the same even if we increase the batch size to 128, which is 64 times larger than the default one and thus requires 64x more time for a single optimization step.\nAlthough the gradient norm of training runs with extremely large batches begins to display convergence (see Figure 9b), using the batches of this size is less practical than using an early stopping\ncriterion. Specifically, when training with these batch sizes, the quality of samples reaches its peak at approximately 250 training steps. This corresponds to \u2248 16,000 forward and backward passes with the batch size of 4 (or 8,000 with the batch size of 8 \u2014 the largest size that fits into an 80GB GPU for Stable Diffusion) and thus gives no meaningful speedup to the inversion procedure."
        },
        {
            "heading": "D Example pseudocode of using DVAR for training",
            "text": "In Figure 10, we provide an example PyTorch training loop that uses an implementation of DVAR defined in Figure 3. This code samples the evaluation batch once before training and computes Ldet on it after each training iteration. A full implementation of training with DVAR is given at github.com/yandex-research/DVAR.\ndef training_loop(model, train_dataloader, eval_losses, eval_batch, optimizer, window_size, threshold):"
        },
        {
            "heading": "E Evaluation of other stopping criteria",
            "text": "Besides the ratio of the rolling variance to the global variance used in DVAR, we also experimented with other metrics for early stopping that consider the behavior of Ldet. All of these metrics have a hyperparameter n that denotes the number of last objective values to use for computing aggregates.\nTo obtain the EMA percentile, we calculate the exponential moving average (with \u03b1 = 0.1) at the moment t and n steps back, then apply the following formula:\nMEMA = EMA(t)\u2212 EMA(t\u2212 n)\nEMA(t\u2212 n) .\nThe Hall criterion is simply the difference between the rolling maximum and the rolling minimum divided by the rolling average over n steps:\nMHall = max(Lndet)\u2212min(Lndet)\nmean(Lndet) ,\nwhere Lndet denotes the last n values of Ldet. The Trend metric is the slope of a linear regression trained on loss values in a window of size n, computed at each step using the closed-form solution for linear regression. The main issue of this criterion is its longer evaluation time compared to others.\nThe dynamics of all these metrics are shown in Figure 11. The main challenge we face with these metrics is their unstable behavior, which prevents the transfer of hyperparameters between concepts.\n0 250 500 750\n0.25\n0.50\n0.75\n1.00\nVariance ratio\n0 250 500 750\n\u22120.02\n\u22120.01\n0.00\nEMA percentile\n0 250 500 750\n0.010\n0.015\n0.020\n0.025 Hall\n0 250 500 750\n\u22122\n0\n2\n4\n\u00d710\u22126 Trend\n0 250 500 750 0.00\n0.25\n0.50\n0.75\n1.00\n0 250 500 750\n\u22120.003\n\u22120.002\n\u22120.001\n0.000\n0 250 500 750\n0.001\n0.002\n0.003\n0 250 500 750\n\u22125\n0\n5\n\u00d710\u22126\n0 250 500 750 Step\n0.00\n0.25\n0.50\n0.75\n1.00\n0 250 500 750 Step\n\u22120.003\n\u22120.002\n\u22120.001\n0.000\n0 250 500 750 Step\n0.001\n0.002\n0.003\n0 250 500 750 Step\n\u22125\n0\n5\n\u00d710\u22126"
        },
        {
            "heading": "F Quantitative comparison with additional metrics",
            "text": "We conduct an additional quantitative evaluation of different early stopping approaches with two metrics introduced in [7] named DINO and DIV.\nThe DINO metric represents the average cosine similarity between ViT-S/16 DINO [29] embeddings of generated and real images. DINO is more sensitive to differences between subjects of the same class than CLIP image score, which is more suitable for measuring the degree of unique feature preservation. By contrast, DIV was proposed to evaluate the diversity of generated images. It is computed as the average LPIPS [30] cosine similarity between samples generated with the same prompt. Low values of DIV signify the model overfitting the training images.\nTable 3 shows that DVAR increases the DIV metric for two personalization techniques, which indicates reduced overfitting, whereas having only minor losses in subject fidelity as measured by DINO. Overall, we conclude that our approach does not cause any significant quality degradations while decreasing customization time in an adaptive manner.\nG Variance across concepts\nTo additionally justify the need for an adaptive early stopping technique, we conduct two experiments. First, we analyze the variability of Ldet across concepts. Second, we compare the distribution of stop iterations selected by DVAR with those selected by non-adaptive early stopping methods.\nG.1 Ldet variation over concepts\nIn Figure 12, we depict the behavior of normalized Ldet for four concepts across all three personalization methods. Normalization is necessary, because the scale of loss varies highly across concepts.\nFrom this plot, we can conclude that the dynamics of Ldet moderately differ from one concept to another. For example, the objective function exhibits earlier saturation on some concepts and methods and demonstrates a more unstable behavior on others.\nG.2 Final steps distribution\nWe compare the distribution of the final steps selected by DVAR to the non-adaptive methods. As we can see from Figure 13, simply throwing away some of the iterations might be sub-optimal for some concepts, indicating the need for an adaptive early stopping technique."
        },
        {
            "heading": "H Qualitative comparison",
            "text": "In Figure 14, we show a side-by-side comparison of DVAR and Baseline methods. Validation images are obtained from the final training checkpoints using the validation prompt. This comparison confirms that our method achieves similar results in terms of concept reconstruction and prevents overfitting for all three adaptation approaches.\nMoreover, this comparison demonstrates that focusing only on the quality of samples during training can be detrimental to the model\u2019s ability to transfer concepts to unseen contexts. In such cases, the model tends to generate outputs that resemble the training images regardless of the input prompt."
        },
        {
            "heading": "I Human evaluation instructions",
            "text": "In Figure 15, we provide the instructions for the quality assessment tasks that were shown to the annotators. We aimed to make the instructions as unambiguous and detailed as possible, minimizing the potential influence of different interpretations among individuals. All annotators were required to complete a training task specifically designed to ensure a clear understanding of the decision-making algorithm. Furthermore, we incorporated control tasks with obvious correct answers to filter out bots or low-quality annotators who might vote randomly.\nTask description: Two neural networks tried to learn the object depicted by the Reference image and to generate it in another environment (e.g. another background, view angle, style or composition) described in text prompt on the top of the task. Please help us understand which neural network does the job better in terms of matching the text prompt by answering one question for a given pair of generated images. How to answer the question:\n1. Carefully look at the reference image. First of all, try to coarsely describe the class of the given image (e.g. table, dog, toy, backpack, etc.). Then pay attention to details, try to figure out what distinguishes this specific object from another from its class: its shape, texture, coloring, additional elements or specific details, etc.\n2. If the reference object is present only on one of the pictures select the picture where it is present. If the reference object is not present on any of the pictures select Equal. Else: compare two generated images by their resemblance to the reference. We suggest going from the most general traits to the more specific details:\n3. Which of the generated images better captures the class of the original object? If both images equally reflect the coarse class of the object, proceed to step 4. Otherwise, please select a corresponding image.\n4. Which of the generated images better reflects the shape of the object being learned? If you think that both images equally reflect the shape of the object, proceed to step 5. Otherwise, please select a corresponding image.\n5. Which of the generated images better reflects the colouring of the object being learned? If you think that both images reflect the object\u2019s color equally, proceed to step 6. Otherwise, please select a corresponding image.\n6. Are there any details that one generated image reflected better than another? If yes, please select a corresponding image. Otherwise, please select \"Equally similar\".\nReference:\n<REFERENCE IMAGE>\nGenerated image A:\n<GENERATED IMAGE>\nGenerated image B:\n<GENERATED IMAGE>\nWhich of the generated images resembles the reference more?\n(a) Instructions for Reconstruction survey.\nTask description: Two neural networks tried to learn the object depicted by the Reference image and to generate it in another environment (e.g. another background, view angle, style or composition) described in text prompt on the top of the task. Please help us understand which neural network does the job better in terms of matching the text prompt by answering one question for a given pair of generated images. How to answer the question: Each prompt (text description that was used to generate image) contains a word or phrase in brackets like <cat-statue> (this is a unique identifier of the object being learned by the network) and the description of a scene in which we want to see this object depicted. Descriptions may include specifications of different places, circumstances, change of reference object color, texture, shape, etc."
        },
        {
            "heading": "J Correlations of timesteps sampling intervals and metrics",
            "text": "To investigate the impact of varied timesteps, we divide the interval from 0 to 1000 into three equal parts (beginning, middle, and end) and evaluate the Ldet with timesteps sampled from each subinterval. Next, we run the training process over 1500 steps for 9 concepts, with sampling images every 50 steps and calculating four CLIP-based quality metrics: train image similarity, train text similarity, validation image similarity, and validation text similarity.\nInspired by previous research [15], we hypothesize that different timestep intervals might exhibit similarities with different metrics. However, as indicated in Table 4, our results do not reveal such behavior. The values within the rows demonstrate remarkable similarity to each other across all subintervals, even though there are different trends for particular metrics."
        }
    ],
    "title": "Is This Loss Informative? Faster Text-to-Image Customization by Tracking Objective Dynamics",
    "year": 2023
}