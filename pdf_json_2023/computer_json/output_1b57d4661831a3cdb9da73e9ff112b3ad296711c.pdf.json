{
    "abstractText": "Finetuning pretrained language models (LMs) have enabled appealing performance on a diverse array of tasks. The intriguing taskagnostic property has driven a shifted focus from task-specific to task-agnostic distillation of LMs. While task-agnostic, computeefficient, performance-preserved LMs can be yielded by task-agnostic distillation, previous studies mainly sit in distillation of either encoder-only LMs (e.g., BERT) or decoderonly ones (e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g., T5) can posit very distinguished behaviors. Frustratingly, we discover that existing taskagnostic distillation methods can fail to handle the distillation of encoder-decoder LMs. To the demand, we explore a few paths and uncover a path named as MINIEND that successfully tackles the distillation of encoderdecoder LMs in a task-agnostic fashion. We examine MINIEND on language understanding and abstractive summarization. The results showcase that MINIEND is generally effective and is competitive compared to other alternatives. We further scale MINIEND up to distillation of 3B encoder-decoder language models with interpolated distillation. The results imply the opportunities and challenges in distilling large language models (e.g., LLaMA).",
    "authors": [
        {
            "affiliations": [],
            "name": "Chen Zhang"
        },
        {
            "affiliations": [],
            "name": "Yang Yang"
        },
        {
            "affiliations": [],
            "name": "Jingang Wang"
        },
        {
            "affiliations": [],
            "name": "Dawei Song"
        }
    ],
    "id": "SP:2844c488c807e6c926d2da6fbe0d9336885ec858",
    "references": [
        {
            "authors": [
                "Luisa Bentivogli",
                "Peter Clark",
                "Ido Dagan",
                "Danilo Giampiccolo."
            ],
            "title": "The seventh PASCAL recognizing textual entailment challenge",
            "venue": "Proceedings of the Fourth Text Analysis Conference, TAC 2011, Gaithersburg, Maryland, USA, November 14-",
            "year": 2011
        },
        {
            "authors": [
                "Daniel M. Cer",
                "Mona T. Diab",
                "Eneko Agirre",
                "I\u00f1igo Lopez-Gazpio",
                "Lucia Specia."
            ],
            "title": "Semeval2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation",
            "venue": "Proceedings of the 11th International Workshop on Se-",
            "year": 2017
        },
        {
            "authors": [
                "ran Narang",
                "Gaurav Mishra",
                "Adams Yu",
                "Vincent Y. Zhao",
                "Yanping Huang",
                "Andrew M. Dai",
                "Hongkun Yu",
                "Slav Petrov",
                "Ed H. Chi",
                "Jeff Dean",
                "Jacob Devlin",
                "Adam Roberts",
                "Denny Zhou",
                "Quoc V. Le",
                "Jason Wei"
            ],
            "title": "Scaling instruction-finetuned language",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "William B. Dolan",
                "Chris Brockett."
            ],
            "title": "Automatically constructing a corpus of sentential paraphrases",
            "venue": "Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, Jeju Island, Korea, October 2005, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "CoRR, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "Tinybert: Distilling BERT for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, On-",
            "year": 2020
        },
        {
            "authors": [
                "Hector J. Levesque",
                "Ernest Davis",
                "Leora Morgenstern."
            ],
            "title": "The winograd schema challenge",
            "venue": "Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International",
            "year": 2012
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Li",
                "Zijian Wang",
                "Ming Tan",
                "Ramesh Nallapati",
                "Parminder Bhatia",
                "Andrew O. Arnold",
                "Bing Xiang",
                "Dan Roth."
            ],
            "title": "DQ-BART: efficient sequenceto-sequence model via joint distillation and quantization",
            "venue": "Proceedings of the 60th Annual Meeting of",
            "year": 2022
        },
        {
            "authors": [
                "Chen Liang",
                "Haoming Jiang",
                "Zheng Li",
                "Xianfeng Tang",
                "Bin Yin",
                "Tuo Zhao."
            ],
            "title": "Homodistil: Homotopic task-agnostic distillation of pre-trained transformers",
            "venue": "CoRR, abs/2302.09632.",
            "year": 2023
        },
        {
            "authors": [
                "Zhenghao Lin",
                "Yeyun Gong",
                "Xiao Liu",
                "Hang Zhang",
                "Chen Lin",
                "Anlei Dong",
                "Jian Jiao",
                "Jingwen Lu",
                "Daxin Jiang",
                "Rangan Majumder",
                "Nan Duan."
            ],
            "title": "PROD: progressive distillation for dense retrieval",
            "venue": "Proceedings of the ACM Web Conference 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Frederick Liu",
                "Siamak Shakeri",
                "Hongkun Yu",
                "Jing Li."
            ],
            "title": "Enct5: Fine-tuning T5 encoder for nonautoregressive tasks",
            "venue": "CoRR, abs/2110.08426.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized BERT pretraining approach",
            "venue": "CoRR, abs/1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Seyed-Iman Mirzadeh",
                "Mehrdad Farajtabar",
                "Ang Li",
                "Nir Levine",
                "Akihiro Matsukawa",
                "Hassan Ghasemzadeh."
            ],
            "title": "Improved knowledge distillation via teacher assistant",
            "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI",
            "year": 2020
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B. Cohen",
                "Mirella Lapata."
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Lan-",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jian Zhang",
                "Konstantin Lopyrev",
                "Percy Liang."
            ],
            "title": "Squad: 100, 000+ questions for machine comprehension of text",
            "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin,",
            "year": 2016
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Abigail See",
                "Peter J. Liu",
                "Christopher D. Manning."
            ],
            "title": "Get to the point: Summarization with pointergenerator networks",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 -",
            "year": 2017
        },
        {
            "authors": [
                "Sam Shleifer",
                "Alexander M. Rush."
            ],
            "title": "Pre-trained summarization distillation",
            "venue": "CoRR, abs/2010.13002.",
            "year": 2020
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Y. Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference",
            "year": 2013
        },
        {
            "authors": [
                "Siqi Sun",
                "Yu Cheng",
                "Zhe Gan",
                "Jingjing Liu."
            ],
            "title": "Patient knowledge distillation for BERT model compression",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat-",
            "year": 2019
        },
        {
            "authors": [
                "Chaofan Tao",
                "Lu Hou",
                "Wei Zhang",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Ping Luo",
                "Ngai Wong."
            ],
            "title": "Compression of generative pre-trained language models via quantization",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q. Tran",
                "Xavier Garcia",
                "Dara Bahri",
                "Tal Schuster",
                "Huaixiu Steven Zheng",
                "Neil Houlsby",
                "Donald Metzler."
            ],
            "title": "Unifying language learning paradigms",
            "venue": "CoRR, abs/2205.05131.",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural",
            "year": 2017
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "7th International Conference on Learning Representa-",
            "year": 2019
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Shaohan Huang",
                "Li Dong",
                "Furu Wei."
            ],
            "title": "Minilmv2: Multi-head selfattention relation distillation for compressing pretrained transformers",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP",
            "year": 2021
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou."
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Con-",
            "year": 2020
        },
        {
            "authors": [
                "Alex Warstadt",
                "Amanpreet Singh",
                "Samuel R. Bowman."
            ],
            "title": "Neural network acceptability judgments",
            "venue": "Transactions on Association for Computational Linguistics, 7:625\u2013641.",
            "year": 2019
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R. Bowman."
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
            "year": 2018
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Zexuan Zhong",
                "Danqi Chen."
            ],
            "title": "Structured pruning learns compact and accurate models",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Yi Yang",
                "Chen Zhang",
                "Dawei Song."
            ],
            "title": "Sparse teachers can be dense with knowledge",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Zhang",
                "Yang Yang",
                "Qifan Wang",
                "Jiahao Liu",
                "Jingang Wang",
                "Yunsen Xian",
                "Wei Wu",
                "Dawei Song."
            ],
            "title": "Minidisc: Minimal distillation schedule for language model compression",
            "venue": "CoRR, abs/2205.14570.",
            "year": 2022
        },
        {
            "authors": [
                "Shengqiang Zhang",
                "Xingxing Zhang",
                "Hangbo Bao",
                "Furu Wei."
            ],
            "title": "Attention temperature matters in abstractive summarization distillation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Pretrained language models (LMs) powered by finetuning have achieved remarkable performance on a wide range of downstream tasks (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019). Driven by the pursued task-agnostic property, distillation of LMs has witnessed a paradigm shift from taskspecific to task-agnostic distillation (Sanh et al., 2019). Under a teacher-student regime, taskagnostic distillation distils pretrained LMs into ones of small compute on pretraining data so that\n\u2217Dawei Song is the corresponding author.\nthese small LMs can be applied to tasks by finetuning (Jiao et al., 2020; Wang et al., 2020; Liang et al., 2023). In contrast, task-specific distillation distils finetuned LMs on finetuning data and consumed resource can be even huge when the number of tasks explode (Hinton et al., 2015; Sun et al., 2019; Xia et al., 2022; Yang et al., 2022). Additionally, it is acknowledged that task-agnostic distillation typically brings performance gain over task-specific distillation does (Zhang et al., 2022a).\nDespite so many merits, prior studies mostly lie in distillation of either encoder-only LMs (e.g., BERT, Devlin et al., 2019) or decoder-only LMs (e.g., GPT, Radford et al., 2019) and largely ignore the signifance of task-agnostic distillation of encoder-decoder LMs (e.g., T5, Raffel et al., 2020) given recent advances in task-specific distillation of encoder-decoder LMs though (Shleifer and Rush, 2020; Zhang et al., 2022b; Li et al., 2022;\nar X\niv :2\n30 5.\n12 33\n0v 1\n[ cs\n.C L\n] 2\n1 M\nay 2\n02 3\nTao et al., 2022). Frustratingly, we find that existing distillation methods may fail to handle taskagnostic distillation of encoder-decoder LMs since encoder-decoder LMs can behave very differently in comparison with encoder-only and decoder-only LMs (e.g., the use of cross-attention, Vaswani et al., 2017). The failures of prior methods are showcased in Figure 1.\nTo the end, we investigate to, in a task-agnostic style, save the distillation of encoder-decoder LMs from the awkward position. Specifically, we reveal that the key to unlocking the expressiveness of distillation is the interplay between the encoder and the decoder. Therefore, we offer a path named as MINIEND that successfully tackles the distillation of encoder-decoder LMs by alternatively distilling the cross-attention to explicitly fall to both the encoder and the decoder.\nWe check MINIEND on language understanding and abstractive summarization in sense that encoder-decoder LMs are more capable of sequence-to-sequence tasks. For evaluation on language understanding, we take GLUE (Wang et al., 2019) to benchmark the performance. For evaluation on abstractive summarization, we adopt CNN/DailyMail (See et al., 2017) and XSum (Narayan et al., 2018) as two testbeds. The results of both distilling T5 and BART indicate that MINIEND is effective and competitive to other compression options such as quantization. We further scale our method up to the distillation of 3B T5xlarge with the aid of progressive distillation. The results suggest that distilling large language models (e.g., LLaMA, Touvron et al., 2023) should be promising but can be challenging."
        },
        {
            "heading": "2 Encoder-Decoder Interplay",
            "text": ""
        },
        {
            "heading": "2.1 Architecture Perspective",
            "text": "Typically, an encoder-decoder LM is composed of an encoder and a decoder, each of which is essentially a stack of transformer layers (Vaswani et al., 2017). Concretely, a transformer layer in the encoder contains a multihead self-attention (MSA) module and a feedforward network (FFN) module. Similarly, a transformer layer in the decoder comprises an MSA module, an FFN module, and additionally a multihead cross-attention (MCA) module that is inserted between the MSA and the FFN modules and accounts for absorption of encoded information from the encoder. Around each of these modules is attached necessarily a layer normaliza-\ntion and a residual connection.\nMSA and FFN Mathematically, the procedure that a transformer encoder layer consumes an intermediate encoder input X \u2208 Rn\u00d7d containing a n-length sequence of d-dimension vectors from last layer and gives an output to next layer can be depicted as a composition of MSA and FFN:\nMSA(X;WQ,WK,WV)\n= A\u2211 i SelfAttn(X;WQi ,W K i )XW V i W O i ,\nSelfAttn(X;WQi ,W K i )\n= Softmax(XWQi W K> i X >/dA),\nFFN(X;WI,WO) = I\u2211 j g(XWIj)W O j ,\nwhere potential details (e.g., linear bias and layer normalization) are omitted. i is used to indicate i-th head parameterized by WQi , W K i , W V i \u2208 Rd\u00d7d A , WOi \u2208 Rd A\u00d7d among A heads, and j is used to indicate j-th intermediate neuron parameterized by WIj \u2208 Rd\u00d71 and WOj \u2208 R1\u00d7d among I neurons. g is an activation function (e.g., GELU).\nMCA Likewise, the procedure that a transformer decoder layer processes an intermediate decoder input Z \u2208 Rm\u00d7d based on the final encoder output E \u2208 Rn\u00d7d can be incrementally described as an insertion of MCA:\nMCA(Z,E;WQ \u2032 ,WK \u2032 ,WV \u2032 )\n= A\u2211 i CrossAttn(Z,E;WQ \u2032 i ,W K\u2032 i )EW V\u2032 i W O\u2032 i ,\nCrossAttn(Z,E;WQ \u2032 i ,W K\u2032 i )\n= Softmax(ZWQ \u2032 i W K\u2032> i E >/dA),\nHere, each cross-attention head is parameterized by another set of parameters WQ \u2032\ni , W K\u2032 i , W V\u2032 i \u2208\nRd\u00d7dA , WO\u2032i \u2208 Rd A\u00d7d.\nInterplay through Architecture In this architectural sense, the decoder is tightly connected to the encoder through MCA modules. In spite that state-of-the-art methods mainly manipulate the decoder during distillation (e.g., logits, Zhang et al., 2022b), the encoder could be learned anyway through the connections offered by MCA modules. However, it is still not clear to what extent\nthe encoder-decoder interplay is significant in the distillation and whether the implicit connections mentioned above are enough for alignment of the interplay."
        },
        {
            "heading": "2.2 Gradient Perspective",
            "text": "More thoroughly, we take a closer look at the connections between the encoder and the decoder through the lens of gradients.\nWe examine the gradient norms of last layer hidden states of both the encoder and the decoder under two distinguished distillation objectives when distilling from BART (Lewis et al., 2020). The intuition lies in that, in contrast to implicit consideration of the encoder-decoder interplay, a distillation objective explicitly involving the encoder-decoder interplay alignment could behave much differently in terms of gradients if the interplay is central to the distillation of encoder-decoder LMs. And naturally, if suboptimal cases are identified in the implicit objective, we can further highlight that the implicit objective suffers from the limited interplay alignment and the explicit objective can provide a more effective one.\nImplicit versus Explicit Objective We instantiate the implicit objective as aligning logits and last decoder layer self-attention distributions, and the explicit objective as aligning logits, last decoder layer self-attention distributions, and last decoder layer cross-attention distributions. The core idea of last layer attention distribution alignment is borrowed from MiniLM (Wang et al., 2021). Any alignment can be abstracted as L(S; T ,D\u2217), where D\u2217 denotes, with slight abuse of notation, the distribution of the input. As a crucial part, the alignment of self-attention is like the following:\nLSelfAttn(S; T ,DZ) = EZ\u223cDZ R\u2211\nk=1\nKL(Reln(Z; TWQk ),Reln(Z; SWQk ))\n+ KL(Reln(Z; TWKk ),Reln(Z; SWKk )) + KL(Reln(Z; TWVk ),Reln(Z; SWVk )),\nReln(Z;TWQk )\n= Softmax(ZTWQk TWQ>k Z >/dR),\nwhere S and T are the teacher and the student, and KL stands for kullback-leibler divergence. Particularly, attention heads are first merged from the\noriginal A attention heads and then split to R heads for alignment of the number of attention heads. T /SWQk is the redistributed query parameter of the k-th head within totally R heads from the last decoder layer, likewise T /SWKk and\nT /SWVk are the key and value parameters.\nThe alignment of cross-attention is similar but sort of different in that the keys and the values are aligned in fact from the encoder side, as the following:\nLCrossAttn(S; T ,DZ,DE) = EZ\u223cDZ,E\u223cDE R\u2211\nk=1\nKL(Reln(Z; TWQ \u2032 k ),Reln(Z; SWQ \u2032 k ))\n+ KL(Reln(E; TWK \u2032 k ),Reln(E; SWK \u2032 k )) + KL(Reln(E; TWV \u2032\nk ),Reln(E; SWV \u2032 k )),\nHere, the notations should be self-contained by referring to previously mentioned ones.\nInterplay through Gradient To recap, the implicit objective is:\nLLogit(S; T ,DZ) + LSelfAttn(S; T ,DZ),\nContrarily, the explicit objective is derived by adding a cross-attention term as:\nLLogit(S; T ,DZ) + LSelfAttn(S; T ,DZ) + LCrossAttn(S; T ,DZ,DE),\nPreliminary results are shown in Figure 2, from which we can see that 1) the implicit objective and the explicit objective lead to distinct gradient variations, and 2) the implicit objective exhibits gradient spikes, compared with smooth gradient transitions from the explicit objective, that may result in instability for a nice convergence (Zeng et al., 2022). Thereby, from the gradient perspective, we safely conclude that the encoder-decoder interplay is of importance in the distillation of encoder-decoder LMs and an explicit correspondence to the interplay is superior to an implicit one."
        },
        {
            "heading": "3 MINIEND",
            "text": "With aforementioned justifications in mind, we propose a path dubbed as MINIEND that tackles the distillation of encoder-decoder LMs under the guidance of the encoder-decoder interplay alignment. The path can be built in two directions. An overview of these two directions is given in Figure 3.\nDecoder Cross-Attention The first is the one used in our pilot study. That said, we should always plus a fraction towards the alignment of output logits and the overall distillation objective is therefore depicted as:\nL(S; T ,DZ,DE) = LLogit(S; T ,DZ)+ LSelfAttn(S; T ,DZ) + LCrossAttn(S; T ,DZ,DE),\nThe alignment of logits can be further detailed as:\nLLogit(S; T ,DZ) = EZ\u223cDZ CE(ZSWE,ZTWE),\nwhere CE stands for soft cross entropy and T /SWE denotes output embedding.\nEncoder Self-Attention The second is an alternative to the first one where the interplay part is accounted by the last encoder self-attention distributions instead as:\nL(S; T ,DZ,DX) = LLogit(S; T ,DZ)+ LSelfAttn(S; T ,DZ) + LEncSelfAttn(S; T ,DX),\nThe rationale of introducing the encoder selfattention alignment abides in that this term together with the decoder self-attention alignment can sufficiently replace the cross-attention term and align the encoder-decoder interplay by aligning both the encoder and the decoder."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Data and Metrics",
            "text": "Following the pretraining of T5 and BART, we use C4 (Raffel et al., 2020) as the corpus for task-agnostic distillation of T5 and OpenWebText (Gokaslan et al., 2019) for that of BART. They are separately processed to follow the pretraining styles of T5 and BART. That is, C4 is converted to the masked language modeling style and OpenWebText is converted to the denoising style.\nFor evaluation of MINIEND, we mainly take GLUE (Wang et al., 2019) for language understanding. The GLUE benchmark consists of two sequence classification tasks, SST-2 (Socher et al., 2013), i.e., CoLA (Warstadt et al., 2019), and seven sequence-pair classification tasks, i.e., MRPC (Dolan and Brockett, 2005), STS-B (Cer et al., 2017), QQP, MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Bentivogli et al., 2011), WNLI (Levesque et al., 2012). We exclude WNLI and CoLA due to the evaluation\ninconsistency (in other words, MiniLMs get dramatically worse results while LMs get much better ones as found out in Xia et al., 2022) and use the left tasks. Following BERT (Devlin et al., 2019), we report Accuracy (Acc) on SST-2, MNLI, QNLI, RTE, Spearman Correlation scores (SpCorr) on STS-B, and F1 on MRPC, QQP, CoNLL. Average score over tasks from GLUE (GLUE Score) is additionally computed. Regarding that one of the most promising properties of encoder-decoder LMs is sequence-to-sequence modeling, we additionally adopt CNN/DailyMail (See et al., 2017) and XSum (Narayan et al., 2018) for abstractive summarization. We report Rouge-{1,2,L} (Rg-{1,2,L}) on both of them. Results are reported on development sets. GFLOPs are also attached as theoretical speedup references.\nThe detailed data statistics, maximum sequence lengths, and metrics for datasets we use are shown in Table 1, where the corpora used for distillation is also attached."
        },
        {
            "heading": "4.2 Implementation",
            "text": "The distillation is carried out on 16 Nvidia A100s. The number of relation heads is set to 32. After the distillation, the finetuning is carried out on one Nvidia A100. For language understanding tasks, T5 is finetuned with simplicity and performance guarantee following EncT5 (Liu et al., 2021) which uses the very first token (i.e., [BOS]) representation from the decoder, while BART is finetued following its original paper which uses the very last token (i.e., [EOS]) representation from the decoder. As for abstractive summarization tasks, both T5 and BART are finetuned in a sequence-to-\nsequence manner. For fast development, we use greedy search for T5 and beam search for BART only. The beam search setting strictly follows the original paper. In order to achieve higher training efficiency, we utilize fully-sharded data parallel (Zhao et al., 2023) to shard both the teacher and the student across GPUs during the distillation. For all cases, students are always randomly initialized before the distillation following MiniLM (Wang et al., 2020).\nThe details of hyperparameters for distillation and finetuning are shown in Table 2. We will be releasing our code and scripts in the final version for exact reproducibility."
        },
        {
            "heading": "4.3 Baselines",
            "text": "We name two variants of MINIEND as MINIENDD and MINIEND-E respectively, where MINIENDD uses decoder cross-attention for interplay alignment and MINIEND-E uses encoder self-attention instead. As there are no existing work in taskagnostic distillation of encoder-decoder LMs, we mainly compare MINIEND to task-agnostic baselines that are heavily adapted to encoder-decoder LMs and task-specific baselines that may be not super fair for comparison.\nWe compare MINIEND-D and MINIEND-E distilled from T5 to task-agnostic baselines on GLUE, CNN/DailyMail, and XSum: MlmKD (Hinton et al., 2015) that directly distils masked language modeling logits; MiniLM (Wang et al., 2021) that distils last decoder layer attention distributions; MlmKD+MiniLM that is essentially a combination of preceding two. We also compare MINIENDD and MINIEND-E distilled from T5 to a task-\nspecific baseline that is as far as we know the most comparable one on GLUE: MiniDisc (Zhang et al., 2022a) that exploits a teacher assistant for large compression.\nOn the other hand, we compare MINIEND-D distilled from BART to two recent task-specific baselines on CNN/DailyMail and XSum: LogitKD and DQ-BART (Li et al., 2022) that jointly quantizes and distils from the teacher.\nFor MINIEND above baselines, student structures are denoted either with *L;*H for number of layers and dimension of hidden states in random initialization, or with *% for preserved portion of parameters in pruning initialization."
        },
        {
            "heading": "4.4 Main Results",
            "text": "Baselines fail, yet MINIEND triumphs. From results in Table 3 and Table 4, we can tell that baselines fail to handle the distillation of encoder-decoder LMs since they either underperform the baseline pretrained from scratch or out-\nperform it by only a small margin. For example, MlmKD+MiniLM achieves 84.5 versus 84.6 from T5 in GLUE Score, and 35.8 versus 35.7 from T5 in CNN/DailyMail Rg-1.\nContrarily, MiniEnD can safely escape from performance degradation and bring further performance increment. For example, MINIEND-D reaches 0.1 absolute improvement in GLUE Score, and 0.9 absolute improvement in XSum Rg-1. The improvement in GLUE Score seems to be not very significant, but can be boosted according to the ablation. That is, MINIEND-E w/o LLogit goes up to 85.0, which is notably better than 84.6 from T5 in the average sense. All count, and interplay forms the key. On another note, removing LLogit will consistently produce performance deterioration on CNN/DailyMail and XSum. We conjecture there is a tradeoff of using between using LLogit or not. Namely, the use of LLogit will offer better generative ability but worse discriminative ability, and the removal of it will\nwork reversely. Anyway, either LCrossAttn in MINIEND-D or LSelfEncAttn in MINIEND-E shall be a crucial ingredient as the interplay alignment term is the only difference between MINIEND and MlmKD+MiniLM but results in a considerable performance gap.\nAnd it may be suspected that whether LSelfAttn is still important given that MiniLM is not an ideal choice for the distillation of encoder-decoder LMs. We suggest the use of it in two aspects: 1) MlmKD+MiniLM is better than MlmKD alone; 2) the interplay alignment will witeness a subtle performance drop after the removal of LSelfAttn, say MINIEND-D will decrease from 84.7 to 83.0 in GLUE Score. Quantization has two sides. MINIEND surpasses most of them except DQ-BART. However, we should emphasize that quantized LMs usually perform better but run much slower than distilled LMs do when compression is the same. In our case, DQBART uses 8 bit precision and gives rise to a 4\u00d7 model size reduction which is the same as that of MINIEND. In addition to that, MINIEND is orthogonal to quantization and thus can be enhanced with other quantization schemes."
        },
        {
            "heading": "4.5 Analyses",
            "text": "Data Scaling Some would wonder whether the huge amounts of GPU hours due to the large pre-\ntraining corpus is necessary. So we inspect the performance variation of MINIEND-D by varying data scale, which is shown in Figure 4.\nThe results generally hint that using a portion of data could hardly approximate the full data performance, though half data can achieve acceptable performance. Therefore, we suggest the use of full data in the distillation.\nModel Scaling Inspired by pioneering work finding a curse that larger teachers induces worse students, we double check the existence of the curse and offer a trial solution to the curse so that we can scale the teacher up to 3B T5xlarge.\nFrom the results in Table 5, we observe that the curse of capacity gap still exists in our case. With the increase of teacher scale, the student performance decreases. We attempt to apply common solutions the circumvent the curse. The first is to make the student learn from a teacher assistant distilled from the teacher (Mirzadeh et al., 2020). The second is to make the student to learn from a smaller teacher and then from the teacher (Lin et al., 2023). Both two solutions inherit the idea of inserting an additional distillation step thus progressive distillation. We reveal that teacher assistantbased distillation is somewhat useful but not as excepted since T5xlarge\u21d2T512L;384H\u21d2T56L;384H still does not imrpove over T5xlarge\u21d2T56L;384H\nin some cases. Nonetheless, we unearth that progressive distillation is more promising in terms of consistent performance gains when comparing T5xlarge\u21d2{T5large\u21d2T512L;384H} to T5xlarge\u21d2T512L;384H. We claim that distilling large language models like LLaMA can therefore be appealing but challenging."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we aim to provide a path that successfully tackles the distillation of encoder-decoder LMs, which fails most previous methods in the area. We find through a pilot study that the encoderdecoder interplay is a key component that should be aligned in the distillation so that the distilled encoder-decoder LMs are promising. Based on the idea, we propose two directions that the encoderdecoder interplay alignment can be incorporated\nand verify their effectiveness on a language understanding benchmark and two abstractive summarization datasets. We further scale the distillation of encoder-decoder LMs to a 3B teacher that requires additional distillation steps. In this sense, we recommend future research to devote more efforts to exploring how large language models can be distilled.\nLimitations\nThis paper lacks a validation study on more recently advanced encoder-decoder LMs such as Flan (Chung et al., 2022) and UL2 (Tay et al., 2022) as well as their instruction-tuned version."
        }
    ],
    "title": "Task-agnostic Distillation of Encoder-Decoder Language Models",
    "year": 2023
}