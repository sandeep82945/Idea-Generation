Existing language and vision models achieve impressive performance in image-text understanding. Yet, it is an open question to what extent they can be used for language understanding in 3D environments and whether they implicitly acquire 3D object knowledge, e.g. about different views of an object. In this paper, we investigate whether a state-of-the-art language and vision model, CLIP, is able to ground perspective descriptions of a 3D object and identify canonical views of common objects based on text queries. We present an evaluation framework that uses a circling camera around a 3D object to generate images from different viewpoints and evaluate them in terms of their similarity to natural language descriptions. We find that a pre-trained CLIP model performs poorly on most canonical views and that fine-tuning using hard negative sampling and random contrasting yields good results even under conditions with little available training data. 1 conclusion We developed a new framework to assess the capabilities of L&V models to ground viewpoint descriptions. Through our research, we discovered that a standard CLIP model struggles to distinguish between different viewpoints. To address this, we explored a combination of different loss objectives on synthetic data to make it easier to retrieve viewpoints from language descriptions. Our experiments revealed that incorporating random contrasting leads to a more accurate and seamless scoring function, as compared to using only text and human-centric images. Our framework thus offers a promising approach to scale L&V models trained on large-scale image-text datasets for applications that involve interaction in the 3D world. Limitations We deliberately opted for a simple controllable setup in order to gain a precise understanding of viewpoint representation in CLIP. Our experiments are restricted to canonical views and canned descriptions since they are easy to generate and evaluate automatically. In conclusion, we believe that our framework has the potential to provide a more comprehensive understanding of reporting biases in image-text data used for pretraining L&V models. By conducting a 360-degree analysis of the scoring function, our framework allows for a more thorough examination of these biases, as everything is visible and nothing can be hidden from the investigator, unlike when evaluating against a set of gold-standard viewpoints. Ethics Statement
3D models from the ShapeNet dataset are available for research and non-commercial purposes as well as the LAION-5B data set. We did not collect any personal information from any annotators. We clearly state the intended use of our models, which is to support human-centric interaction with AI models in the 3D world. acknowledgments We thank the Michael Stifel Center Jena for funding this work, which is part of the Carl Zeiss
Foundation-funded project ’A Virtual Workshop for Digitization in the Sciences’ (062017-02). a experiment details This section provides additional details on our experimental setup. Section A.1 contains further visualizations of the experiments discussed in section 5. Section A.2 provides details about the implementation of the search algorithms used in our benchmark. A.1 Scoring Function Analysis The following plots illustrate the score distributions obtained with the different model ablations CLIPPRE-TR, CLIP-FT, and CLIP-RC-HNS. Scoring Function PRETR. Figure 6a shows the score distribution of the PRE-TRained CLIP model over 3D objects from the test set of the ShapeNet dataset. Scoring Function FT. Figure 6b depicts the scoring distribution of the CLIP-FT model over 3D objects from the test set of the ShapeNet dataset. Scoring Function RC-HNS. Figure 7a illustrates the score distribution of the CLIP-RC-HNS model over 3D objects from the test set of the ShapeNet dataset. Comparison of Score Distributions for Object Only Queries. To understand which viewpoints CLIP scores best on an object-only query such as a picture of a car, we compare these objectonly queries for all object categories tested on respective 3D objects from the test set. This tells us which viewpoints CLIP associates most with a given object category. Figure 8a indicates that a PRE-TRained CLIP model is not able to distinguish specific viewpoint queries from pure object queries. Comparison of Optimal Viewpoints. Figure 8b shows the viewpoint images obtained from the optima of the scoring distributions generated by a CLIP model and a CLIP-RC-HNS model. The images illustrate that descriptions of viewpoints are indeed a bias in CLIP. Figure 7b illustrates the viewpoints resulting from the global optima of the scoring functions obtained from the CLIP-RC-HNS model. A.2 Search Algorithm Analysis In our work, we are particularly interested in the impact of the shape of the scoring function on the performance of various search algorithms. Section A.2.1 provides details on the implementation
of greedy search. Section A.2.3 illustrates how the search algorithms listed above perform their task on a sphere. A.2.1 Greedy Search Implementation Details We implement a greedy search algorithm as a representative for gradient-based approaches. The greedy search starts with a grid-based approach on the Goldberg polyhedron and always follows the region with the highest score. It tries to find the optimum by greedily selecting the highest scoring regions at each iteration and searching in their neighboring regions at the next iteration. The search is initialized with k randomly selected starting points (here k = 6) from the Goldberg polyhedron. In addition, a cutoff value c must be chosen to determine how many grid points will be considered in the next iteration of the search. The cutoff value can be described as a relative percentage or as an absolute cutoff value. After evaluating all viewpoints with respect to the given query, the next iteration is started by selecting the locations with the highest scores considering the selected cutoff. All obtained scores and their neighboring sample points from the Goldberg polyhedron are added to the list of investigated viewpoints. After that, the next iteration is started. The neighborhood range n, which specifies the number of neighborhood grid points to be examined, can be adjusted. The search can be terminated after i iterations or when no new items have been added to the list of investigated viewpoints. In summary, the greedy search is parameterized by: (k, c, n, i). We chose greedy search as a test algorithm for our benchmark to see how much gradient-based methods as candidate algorithms for the text-viewpoint retrieval task in a 3D environment depend on a smooth structure of the scoring function in their performance. We use a greedy nearest-neighbour heuristic, since the function is only defined at a fixed number of points due to the discretization of the search space. A.2.2 Bayesian Search Implementation Details
Bayesian optimization (Mockus, 1994) is used to estimate the optimum of a black-box function that is costly to evaluate. The algorithm updates its Bayesian prior based on the stepwise function values obtained, increasing the certainty that the regions are likely to be optima and therefore more likely to be explored than other regions of the black box function. Then, the number of samples from
the regions of interest is increased accordingly. We construct the search problem as a Bayesian optimization as follows: The input of the search algorithm is a vector of size five describing the camera position on the hypersphere around the target object: r, θ, ϕ, x, y. In this parameterization, θ and ϕ are spherical coordinates, r is the distance to the center of the 3D object, and x and y are the orientations of the camera along the horizontal and vertical axes. The location of the optimum of the scoring function with respect to a query q depends on the rotation of the 3D object, which we only know is centered around (0, 0, 0). Therefore, Bayesian search tries to find the optimum of the scoring function with respect to the properties of the 3D object at hand given the search query q. For our benchmarks, we use the implementation of the Bayesian optimization algorithm in Head et al. (2021). A.2.3 Search Algorithm Behavior on Sphere The experiments in Section 5 have shown that a smooth scoring function is advantageous for search algorithms in text-viewpoint retrieval. This section visually analyzes why this is the case by examining how the algorithms perform on a sphere around a target object. Figure 8c illustrates how the different algorithms approach the regions with higher scores differently. The greedy search with a low cutoff spreads across the sphere in waves, starting from the initial points. Once it touches a high point, it remains attached to it. In this respect, a good initialization is impor-
tant, e.g., through a high number of random starting points. Bayesian search also starts from randomly initialized starting points around the hypersphere. Compared to greedy search, it reaches the optimum much faster and more purposefully, since sampling is not bound to any local constraints, such as neighboring regions. Another advantage over greedy search is that random starting points have much lower cost than in greedy search, since they do not cause additional computations in the following iteration. The figure shows that the focus of sampling from random starting points across the sphere leads to small, concentrated regions with high scores. In terms of success rate, Bayesian search is less prone to confounding optima, since a certain number of samples are drawn randomly from different regions anyway. Therefore, the approach is more robust to cases with multiple optima, as is the case with the CLIP-FT model. Despite these obstacles, a solution is reached relatively quickly. However, if the scoring function has a ragged structure like the CLIP-PRETR model, even a sampling-based approach has difficulty identifying the optimal regions due to the raggedness and non-uniformity of the function. A.3 Retrieval Metrics Analysis Table 4 shows the precision and recall metrics on synthetic data broken down by object category. Table 5 shows the precision and recall metrics on real data obtained from the LAION-5B data set. (a) Scoring Function Distribution of CLIP PRE-TR model on cars, motorbikes, airplanes, benches, and mugs for the six canonical viewpoint queries. (b) Scoring Function Distribution of the CLIP-FT model on cars, motorbikes, airplanes, benches, and mugs for the six canonical viewpoint queries. Figure 6: Scoring Function Distributions on CLIP PRE-TR and CLIP-FT.
(a) Scoring Function Distribution of the CLIP-RC-HNS model on cars, motorbikes, airplanes, benches, and mugs for the six canonical viewpoint queries.