{
    "abstractText": "There is a strong demand for virtual reality (VR) to bring quality healthcare to underserved populations. This paper addresses this need with the design and prototype of SURVIVRS: Surround VideoBased Virtual Reality for Surgery Guidance. SURVIVRS allows a remote specialist to guide a local surgery team through a virtual reality (VR) telepresence interface. SURVIVRS is motivated by a need for medical expertise in remote and hard-to-reach areas, such as low-to-middle-income countries (LMICs). The remote surgeon interface allows the live observation of a procedure and combines 3D user interface annotation and communication tools on streams of the surgical site and the patient vitals monitor. SURVIVRS also supports debriefing and educational experiences by offering the ability for users to watch recorded surgeries from the point of view of the remote expert. The main contributions of this work are: the feasibility demonstration of the SURVIVRS system through a rigorous 3D user interface design process; the implementation of a prototype application that realizes the proposed design; and a usability evaluation of SURVIVRS showing that the tool was highly favored by users from the general population. The paper discusses the next steps in this line of research aimed at more equitable and diverse access to healthcare.",
    "authors": [
        {
            "affiliations": [],
            "name": "Amani Taweel"
        },
        {
            "affiliations": [],
            "name": "Joaquim Jorge"
        },
        {
            "affiliations": [],
            "name": "Anderson Maciel"
        },
        {
            "affiliations": [],
            "name": "Jo\u00e3o Ricardo Nickenig Vissoci"
        },
        {
            "affiliations": [],
            "name": "Regis Kopper"
        }
    ],
    "id": "SP:249e9676989e394fbdc88294ad0df1b5587f7155",
    "references": [
        {
            "authors": [
                "L. Argyriou",
                "D. Economou",
                "V. Bouki",
                "I. Doumanis"
            ],
            "title": "Engaging immersive video consumers: Challenges regarding 360-degree gamified video applications",
            "venue": "2016 15th international conference on ubiquitous computing and communications and 2016 international symposium on cyberspace and security (IUCC-CSS), pp. 145\u2013152. IEEE",
            "year": 2016
        },
        {
            "authors": [
                "H.A. Aziz"
            ],
            "title": "Virtual reality programs applications in healthcare",
            "venue": "Journal of Health & Medical Informatics, 9(1):305",
            "year": 2018
        },
        {
            "authors": [
                "J. Brooke"
            ],
            "title": "Sus-a quick and dirty usability scale",
            "venue": "Usability evaluation in industry,",
            "year": 1996
        },
        {
            "authors": [
                "A. Fuller",
                "T. Tran",
                "M. Muhumuza",
                "M.M. Haglund"
            ],
            "title": "Building neurosurgical capacity in low and middle income countries",
            "venue": "Eneurologicalsci, 3:1\u20136",
            "year": 2016
        },
        {
            "authors": [
                "W. Huang",
                "L. Alem",
                "F. Tecchia"
            ],
            "title": "Handsin3d: supporting remote guidance with immersive virtual environments",
            "venue": "IFIP Conference on Human-Computer Interaction, pp. 70\u201377. Springer",
            "year": 2013
        },
        {
            "authors": [
                "S.V. Kotsis",
                "K.C. Chung"
            ],
            "title": "Application of see one",
            "venue": "do one, teach one concept in surgical training. Plastic and reconstructive surgery, 131(5):1194",
            "year": 2013
        },
        {
            "authors": [
                "R. McCloy",
                "R. Stone"
            ],
            "title": "Virtual reality in surgery",
            "venue": "Bmj, 323(7318):912\u2013 915",
            "year": 2001
        },
        {
            "authors": [
                "A. Rizzo",
                "S.T. Koenig"
            ],
            "title": "et al",
            "venue": "Is clinical virtual reality ready for primetime? Neuropsychology, 31(8):877",
            "year": 2017
        },
        {
            "authors": [
                "M. Usoh",
                "E. Catena",
                "S. Arman",
                "M. Slater"
            ],
            "title": "Using presence questionnaires in reality",
            "venue": "Presence, 9(5):497\u2013503",
            "year": 2000
        }
    ],
    "sections": [
        {
            "text": "Index Terms: Human-centered computing\u2014Human computer interaction (HCI)\u2014Interaction paradigms\u2014Virtual reality\n*e-mail: ataweel@uncg.edu \u2020e-mail: jorgej@tecnico.ulisboa.pt \u2021e-mail: anderson.maciel@tecnico.ulisboa.pt \u00a7e-mail: jnv4@duke.edu \u00b6e-mail: kopper@uncg.edu"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Many developing countries worldwide show an unmet demand for medical professionals with specific specialties. As an example, in 2007 there were approximately only 27 neurosurgeons available in East Africa, across seven countries with a total population of 270 million, placing the ratio of neurosurgeons to residents at 1:10 million [4].1 Virtual reality (VR) technology has the potential to reduce such gaps in healthcare access.\nThe major problem this research proposes to address is the lack of specialized surgeons in hard-to-reach areas at an accessible cost. Specifically, this project offers a means to enable collaboration between surgeons who are geographically apart. This collaboration happens through live communication via immersive surround video streams in virtual reality. This allows remotely located surgeons to guide and give instructions to the surgery room on short notice without the need to travel or incur additional costs.\nOur proposed solution is SURVIVRS: Surround Video-Based Virtual Reality for Surgery Guidance, an immersive virtual reality telepresence tool for remote surgeons to guide medical procedures in real time. Figure 1 shows the main components of SURVIVRS.\nSURVIVRS contains three main display components: a live 360\u00b0 stream of the surgery room, from the vantage point of a locally mounted 360\u00b0 camera (henceforth referred to as 360\u00b0 view), a live high-resolution stream of a spot camera aimed at the local site of the surgery (henceforth referred to as site view), and a live visualization of the patient vitals monitor (henceforth referred to as vitals view). We posit that, for the remote expert surgeon to offer fully informed guidance to a less experienced surgery team, it is important that they have a contextual view of the surgery theater. As long as the remote surgeon is connected to SURVIVRS, they maintain their spatial context and orientation through the 360\u00b0 view\n1As a means of comparison, the ratio of neurosurgeons to residents in the United States was approximately 1:65,580 in 2007 [4].\nar X\niv :2\n30 2.\n03 95\n3v 1\n[ cs\nthat provides a first-person immersive video stream rendered to a VR head-mounted display (HMD). The site view and the vitals view are always displayed near the upper corners of the remote surgeon\u2019s view and can be zoommed in on demand.\nIn addition to live surgery guidance, SURVIVRS also serves an educational benefit. Traditionally, surgery had been taught through the \u201cSee One, Do One, Teach One\u201d method, where trainees perform procedures after observing similar ones. Although this method has been criticized as unsafe to patients, its core process remains unchanged [6]. SURVIVRS can potentially improve the \u201cSee One\u201d stage of surgical training by playing back surgeries to trainees. The replay includes all tools available to the remote surgeon during the live procedure, including the 360\u00b0 view, the site view, and the vitals view. This way, students can review a surgery, even adding annotations on any necessary parts of the stream.\nThis project used several tools for its development. It uses the Unity game engine to provide a VR application that offers immersive telepresence to a remote user (the expert surgeon) with tools that allow the guidance of a procedure. For this application to succeed, it needs to ensure a friendly, intuitive, and usable immersive virtual environment on the remote surgeon\u2019s side. To that end, an annotation system with varied input options was developed to allow the surgery team to visualize guidance cues during the surgery precisely. Another goal of this research was to create a live streaming link between the VR application (surgeon\u2019s side) and a web app (simulating the surgery room side) with high resolution, low cost, and a clear audio connection. The final goal was to make a web application that emulates the surgery room side easy to use with options to change camera sources, resolutions, and tools to record the call.\nIn summary, the main contribution of this work is the design of SURVIVRS, a VR interface that connects a specialist surgeon to a surgery room, allowing them to guide a surgery team through the course of a procedure. This research is motivated mainly by the prospect of SURVIVRS enabling efficient communication between surgical teams in LMICs and other hard-to-reach areas and experienced specialists in developed countries to ultimately improve access to critical care and outcomes in remote locations."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 Virtual Reality in Healthcare",
            "text": "Virtual reality is a way to connect humans to computers by using HMDs to simulate virtual and interactive environments. VR provides an immensely realistic and controllable environment for users to interact across many application areas. These VR features have been used in experiences and clinical settings for over two decades [8]. One of the first real-world applications of VR was in 1993 to treat mental health disorders. Then VR applications were developed to handle specific phobias [2].\nMcCloy and Stone [7] argue that VR simulators are suited for general surgery, while Augmented Reality (AR) is suited to guide neurosurgery. SURVIVRS offers a remote AR-like interface by giving the expert surgeon a real-time view of the live surgery room.\nOur research aims to employ VR technology to allow communication between remote medical specialists and non-specialist surgeons in a surgery room with the goal of providing instructions for a more successful surgical outcome. Using a VR application, we propose a prototype tool that, in the future, could work over a connection between a doctor from a developed country and a surgery room in an LMIC. Also, we offer the ability for a remotely guided procedure to be recorded, raising the opportunity for medical students to replay the VR view of the remote surgeon to learn and review the surgery."
        },
        {
            "heading": "2.2 Interaction with Immersive 360\u00b0 Videos",
            "text": "Technology of 360\u00b0 video uses omnidirectional camera systems and allows viewing with a first-person perspective. These experiences\nhave been described as highly immersive and potentially engaging environments surrounded by a total sense of existence [1]."
        },
        {
            "heading": "2.3 Remote Collaboration through Virtual Reality",
            "text": "Connecting the world and sharing real-time experiences between people who may be geographically distributed is now an urgent need. Specifically, there is a demand for remote guidance, where a remotely situated expert assists their team on the other end of some procedure, such as a repair, maintenance, surgery, or communication task. This type of collaboration is required in many fields, such as education and healthcare.\nHuang et al. [5] developed a 3D system called HandsIn3D based on 3D virtual immersed space and 3D hand gestures. This project connects a worker and a helper through 3D stereoscopic cameras that offer a visualization of the work site by the helper and augments the worksite by rendering representations of the helper\u2019s hands.\nComparing the HandsIn3D project with our own research, they are based on common principles of remote live video-based guidance, but the implementation and goals are entirely different. While HandsIn3D uses a 3D camera to get the other side, SURVIVRS uses a 360\u00b0 camera to film the surgery room entirely. Moreover, instead of creating virtual hands on the work side to help people, our work offers an annotation system to allow a remote expert to live annotate and take screenshots. Finally, the main goal of the HandsIn3D project was to remotely help distant workers to complete manual tasks, whereas our project targets live remote guidance to a surgical procedure."
        },
        {
            "heading": "3 DESIGN AND IMPLEMENTATION",
            "text": "We used an iterative design process to propose and refine the design of SURVIVRS. Early on, we used low-fidelity prototyping to experiment with a variety of interface ideas. This allowed the research team to openly discuss ideas without loss of code-based work.\nAfter finishing the iterative design process, we conducted several steps to ensure the final usability of the tool. Those steps involved developing the user interfaces by implementing code to the prototypes, testing the design by running and experiencing it, and finally evaluating the usability of the tool by study volunteers. Figure 1 shows screenshots of the SURVIVRS prototype remote surgeon interface used in the usability evaluation.\nAs the focus of this project is the implementation of the remote surgeon\u2019s guidance user interface, we simulated the surgery room through a web application (Figure 2) that streams the view of the remote surgeon with virtual cameras and screen-capture software."
        },
        {
            "heading": "3.1 Software Tools",
            "text": "We implemented the SURVIVRS prototype in Unity2 (version 2020.3.33f1) with OpenXR3. The VR application uses the Unity Video player as a broadcaster for the 360\u00b0 video which is rendered as a texture onto the scene\u2019s Skybox. Unity Render Streaming (URS) was used to live stream the VR view of the remote surgeon to the simulated surgery room web application. Finally, OBS Studio was used as a virtual camera to stream the contents of the live 360\u00b0 video footage captured by the 360\u00b0 camera."
        },
        {
            "heading": "3.2 Hardware",
            "text": "We used the Oculus Meta Quest 2 with two tracked controllers as the VR platform for the remote surgeon. Live streaming of the 360\u00b0 video was achieved by an Insta360 Pro camera4. For this prototype, we used two spot cameras to capture the image of the local surgery site, and a view of the patient vitals monitor. For future implementations, we expect to render a digital vitals monitor using\n2https://www.unity.com 3https://www.khronos.org/openxr 4https://www.insta360.com/product/insta360-pro\nthe patient\u2019s data, rather than a video feed, which is lower quality and unnecessarily takes up bandwidth.\nThe surgery room simulation is run in a server PC with WiFi to communicate with the VR application. All three cameras and a monitor are connected to this PC, which displays the surgery room simulation web application."
        },
        {
            "heading": "3.3 System Design",
            "text": "The primary purpose of this project is to offer a first step towards enabling a remote surgeon to guide a surgeons\u2019 team in a remote area such as sub-Saharan Africa. Figure 3 shows a conceptual image of a doctor wearing the guiding a remote surgery (a) and their corresponding VR view (b).The surgeon can turn in any direction to see everything in the surgery room. As the user changes their view direction, the 2D heads-up UI follows their head movement in all directions after a 0.5-sec delay through a smooth animation.\nFigure 4 shows the information flowchart of the entire communication between the VR application and the simulated surgery room\nweb application. The flow starts from any side by the connection ID to start streaming videos over a WebSocket after finishing the signaling between them. The surgery room web app streams the 360\u00b0 video, the local surgery site video, and tge patient data monitor video to the VR application on the remote surgeon side. On the VR application side, the specialist can interact with these videos by annotating them in the live stream. All these annotations as well as the three streams are shown on the surgery room screen to be viewed by all surgeons (Figure 2). A live two-way audio connection is also established. After closing the stream, a copy of the video call is saved on the server to be used in the \u201cview-recorded video\u201d mode in the VR application by medical students or other doctors in our idealized scenario."
        },
        {
            "heading": "3.3.1 Annotation Interface",
            "text": "One of the most essential features of SURVIVRS is to give the remote surgeon the ability to guide the surgery by pointing to details and convey them to the surgery room through an annotation interface. The annotation system appears on the 2D streams when the surgeon points the controller to one of the 2D videos in the main interface and clicks the trigger buttin to zoom it in. After the video becomes larger, a menu of drawing tools is displayed in front of the surgeon in the VR application, as shown in Figure 5. The menu has four annotation supporting tools: undo and redo; shapes (pencil\u2013free drawing, oval, rectangle, arrow, and eraser); and a play/pause button which, in addition to playing and pausing the video, also takes a screenshot of the annotated 2D video (Figure 5 shows previously taken screenshots in the left-bottom corner of the UI. Whenever an annotation starts, the 2D video is automatically paused, so that image movement does not disturb the annotation target."
        },
        {
            "heading": "4 EVALUATION",
            "text": "We performed a usability evaluation of the SURVIVRS prototype to understand how users from the general population react to the tool before we test it with less available medical doctors. Institutional Review Board approval was obtained prior to data collection. The evaluation was performed with the \u201cview recorded videos\u201d mode to allow the experience of a real surgery video and avoid technical issues such as latency that are not part of the user interface."
        },
        {
            "heading": "4.1 Participants",
            "text": "For this evaluation, six volunteers participated in the experiment, four had no vision issues, and two had strabismus. There were three females and three males, ages ranging from 18 to 44. Most of the participants (66.6%) rated themselves as slightly familiar with VR, while the rest were either not familiar with VR at all (16.6%) or moderately familiar with VR (16.6%)."
        },
        {
            "heading": "4.2 Measures",
            "text": "Overall perceived usability was measured with the System Usability Scale [3]. Presence was measured with the Slater-Usoh-Steed presence questionnaire [9]. We also developed an overall perceptions questionnaire with specific questions about the design and functionality of SURVIVRS user interface."
        },
        {
            "heading": "4.3 Procedure",
            "text": "After the initial greeting and signing the consent form, participants were given general instructions about the experiment. Then, participants wore the HMD and controllers to begin the experiment. At all times, during data collection, the experimenter observed through casting from the HMD to a PC.\nAfter ensuring that the participant got familiar with the VR HMD (Meta Quest 2), the experimenter asked the participant to complete a series of ten tasks that were meant to allow the exploration of all application features. No help or instruction on how to complete a task was given to the participants.\nAfter finishing all tasks, participants were asked to complete the post-study questionnaires"
        },
        {
            "heading": "5 RESULTS",
            "text": ""
        },
        {
            "heading": "5.1 System Usability",
            "text": "Figure 6 shows the SUS Usability scores for each participant. We see that, for 5 out of the 6 participants, their scores are considerably higher than the 68-point threshold for average usability 5.\n5https://www.usability.gov/how-to-and-tools/methods/system-usabilityscale.html"
        },
        {
            "heading": "5.2 Presence",
            "text": "Summary results for each question of the presence questionnaire are shown in Table 1."
        },
        {
            "heading": "5.3 Overall Perceptions",
            "text": "On average all questions in the Overall Perceptions questionnaire responses were above 3.5 in a scale of 1 to 5 where 5 is best."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "The results of the usability study indicate that the SURVIVRS prototype is a usable tool that elicits a high sense of presence and is perceived as effective and sufficient by non-medical users.\nSeveral points were noted during the experiment. For example, five participants had difficulty drawing in their first attempt, but drawing became easier the second time. Also, no participant understood that the play/pause button also took screenshots. Furthermore, some participants wondered why zooming out screenshots has a different way than zooming out the videos. This makes a bad user experience (UX) impression. Also, four out of seven participants, while trying to click on the patient monitor video to zoom it in, by mistake, clicked on the home button and got out of the application. Other comments included slight dizziness (1), need for help (2), slanted 360\u00b0 video (3), feeling small (1), and realistic hospital feeling, but lacking hospital odors (1)."
        },
        {
            "heading": "7 CONCLUSION AND FUTURE WORK",
            "text": "We proposed SURVIVRS, a surgery guidance tool that allows remote expert surgeons to guide medical teams without high expertise. Our prototype tool showed positive results, and is a promising first step towards realizing the full vision of SURVIVRS.\nThere are several next steps to advance our project. After feedback-based refinements are made, we plan on testing the user interface with surgeons to gain domain-specific feedback. We also need to test the tool with a live stream of a (simulated) surgery and address the many technical challenges associated with it. At the medium term, we will develop a local surgeon interface using monitors in the surgery room or smart glasses.\nIn the long term, we aim to incorporate augmented reality tools in the surgery room to close the telepresence loop and integrate a representation of the remote surgeon and spatial in-context annotations."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is being partially supported by Fundac\u0327a\u0303o para a Cie\u0302ncia e a Tecnologia (Portuguese Foundation for Science and Technology) through grants 2022.09212.PTDC (XAVIER), UIDB/50021/2020 and Carnegie Mellon Portugal grant SFRH/BD/151465/2021 under the auspices of the UNESCO Chair on AI&XR."
        }
    ],
    "title": "SURVIVRS: Surround Video-Based Virtual Reality for Surgery Guidance",
    "year": 2023
}