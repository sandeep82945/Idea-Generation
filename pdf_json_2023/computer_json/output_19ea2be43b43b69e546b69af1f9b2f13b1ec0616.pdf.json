{
    "abstractText": "This work introduces a flexible architecture for realtime occupancy forecasting. In contrast to existing, more computationally expensive architectures, the proposed model exploits recursive latent state estimation, using learned transformer-based prediction and update modules. This allows for highly efficient real-time inference on an embedded system (profiled on an Nvidia Xavier AGX), and the inclusion of a broad set of information from a diverse set of sensors. The architecture is able to process sparse and occluded observations of agent positions and scene context as this is made available, and does not require motion tracklet inputs. MotionPerceiver accomplishes this by encoding the scene into a latent state that evolves in time with self-attention and is updated with contextual information such as traffic signals, road topology or agent detections using cross-attention. Occupancy predictions are made by sparsely querying positions of interest as opposed to generating a fixed size raster image, which allows for variable resolution occupancy prediction or local querying by downstream trajectory optimisation algorithms, saving computational effort.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bryce Ferenczi"
        },
        {
            "affiliations": [],
            "name": "Michael Burke"
        },
        {
            "affiliations": [],
            "name": "Tom Drummond"
        }
    ],
    "id": "SP:413b9cae99b04b3c9f7c9f11d12d5995c834fcbb",
    "references": [
        {
            "authors": [
                "Mayank Bansal",
                "Alex Krizhevsky",
                "Abhijit Ogale"
            ],
            "title": "Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst",
            "year": 2018
        },
        {
            "authors": [
                "Yuning Chai",
                "Benjamin Sapp",
                "Mayank Bansal",
                "Dragomir Anguelov"
            ],
            "title": "Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction",
            "year": 2019
        },
        {
            "authors": [
                "Scott Ettinger",
                "Shuyang Cheng",
                "Benjamin Caine",
                "Chenxi Liu",
                "Hang Zhao",
                "Sabeek Pradhan",
                "Yuning Chai",
                "Ben Sapp",
                "Charles R. Qi",
                "Yin Zhou",
                "Zoey Yang",
                "Aur\u2019elien Chouard",
                "Pei Sun",
                "Jiquan Ngiam",
                "Vijay Vasudevan",
                "Alexander McCauley",
                "Jonathon Shlens",
                "Dragomir Anguelov"
            ],
            "title": "Large scale interactive motion forecasting for autonomous driving: The waymo open motion",
            "year": 2021
        },
        {
            "authors": [
                "Jiyang Gao",
                "Chen Sun",
                "Hang Zhao",
                "Yi Shen",
                "Dragomir Anguelov",
                "Congcong Li",
                "Cordelia Schmid"
            ],
            "title": "Vectornet: Encoding hd maps and agent dynamics from vectorized representation",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Junru Gu",
                "Chen Sun",
                "Hang Zhao"
            ],
            "title": "Densetnt: Endto-end trajectory prediction from dense goal sets",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yihan Hu",
                "Wenxin Shao",
                "Bo Jiang",
                "Jiajie Chen",
                "Siqi Chai",
                "Zhening Yang",
                "Jingyu Qian",
                "Helong Zhou",
                "Qiang Liu"
            ],
            "title": "Hope: Hierarchical spatial-temporal network for occupancy flow prediction. 2022",
            "venue": "doi: 10.48550/ARXIV",
            "year": 2022
        },
        {
            "authors": [
                "Xin Huang",
                "Xiaoyu Tian",
                "Junru Gu",
                "Qiao Sun",
                "Hang Zhao"
            ],
            "title": "Vectorflow: Combining images and vectors for traffic occupancy and flow prediction. 2022",
            "venue": "doi: 10.48550/ARXIV.2208.04530. URL https://arxiv.org/abs/",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Jaegle",
                "Sebastian Borgeaud",
                "Jean-Baptiste Alayrac",
                "Carl Doersch",
                "Catalin Ionescu",
                "David Ding",
                "Skanda Koppula",
                "Daniel Zoran",
                "Andrew Brock",
                "Evan Shelhamer",
                "Olivier J. Henaff",
                "Matthew M. Botvinick",
                "Andrew Zisserman",
                "Oriol Vinyals",
                "Joao Carreira"
            ],
            "title": "Perceiver IO: A general architecture for structured inputs and outputs",
            "venue": "CoRR, abs/2107.14795,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaosong Jia",
                "Li Chen",
                "Penghao Wu",
                "Jia Zeng",
                "Junchi Yan",
                "Hongyang Li",
                "Yu Qiao"
            ],
            "title": "Towards capturing the temporal dynamics for trajectory prediction: a coarseto-fine approach",
            "venue": "In 6th Annual Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaosong Jia",
                "Penghao Wu",
                "Li Chen",
                "Hongyang Li",
                "Yu Liu",
                "Junchi Yan"
            ],
            "title": "Hdgt: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding",
            "venue": "arXiv preprint arXiv:2205.09753,",
            "year": 2022
        },
        {
            "authors": [
                "Jinkyu Kim",
                "Reza Mahjourian",
                "Scott Ettinger",
                "Mayank Bansal",
                "Brandyn White",
                "Ben Sapp",
                "Dragomir Anguelov"
            ],
            "title": "Stopnet: Scalable trajectory and occupancy prediction for urban autonomous driving",
            "year": 2022
        },
        {
            "authors": [
                "Stepan Konev",
                "Kirill Brodt",
                "Artsiom Sanakoyeu"
            ],
            "title": "Motioncnn: A strong baseline for motion prediction in autonomous driving. 2022",
            "venue": "doi: 10.48550/ARXIV.2206",
            "year": 2022
        },
        {
            "authors": [
                "Alex H. Lang",
                "Sourabh Vora",
                "Holger Caesar",
                "Lubing Zhou",
                "Jiong Yang",
                "Oscar Beijbom"
            ],
            "title": "Pointpillars: Fast encoders for object detection from point clouds",
            "year": 2019
        },
        {
            "authors": [
                "Ming Liang",
                "Bin Yang",
                "Rui Hu",
                "Yun Chen",
                "Renjie Liao",
                "Song Feng",
                "Raquel Urtasun"
            ],
            "title": "Learning lane graph representations for motion forecasting",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Liu",
                "Zhiyu Huang",
                "Chen Lv"
            ],
            "title": "Strajnet: Multi-modal hierarchical transformer for occupancy flow field prediction in autonomous driving. 2022",
            "venue": "doi: 10.48550/ARXIV.2208.00394. URL https://arxiv.org/abs/",
            "year": 2022
        },
        {
            "authors": [
                "Reza Mahjourian",
                "Jinkyu Kim",
                "Yuning Chai",
                "Mingxing Tan",
                "Ben Sapp",
                "Dragomir Anguelov"
            ],
            "title": "Occupancy flow fields for motion forecasting in autonomous driving",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Jiquan Ngiam",
                "Benjamin Caine",
                "Vijay Vasudevan",
                "Zhengdong Zhang",
                "Hao-Tien Lewis Chiang",
                "Jeffrey Ling",
                "Rebecca Roelofs",
                "Alex Bewley",
                "Chenxi Liu",
                "Ashish Venugopal",
                "David Weiss",
                "Ben Sapp",
                "Zhifeng Chen",
                "Jonathon Shlens"
            ],
            "title": "Scene transformer: A unified architecture for predicting multiple agent",
            "venue": "trajectories. arXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "venue": "NIPS-W,",
            "year": 2017
        },
        {
            "authors": [
                "Dmytro Poplavskiy"
            ],
            "title": "Waymo open dataset occupancy and flow prediction challenge solution: Look around, 2022",
            "venue": "URL https://storage.googleapis.com/waymo-uploads/ files/research/OccupancyFlow/Dmytro1.pdf",
            "year": 2022
        },
        {
            "authors": [
                "Amir Sadeghian",
                "Vineet Kosaraju",
                "Ali Sadeghian",
                "Noriaki Hirose",
                "Hamid Rezatofighi",
                "Silvio Savarese"
            ],
            "title": "Sophie: An attentive gan for predicting paths compliant to social and physical constraints",
            "year": 2019
        },
        {
            "authors": [
                "Tim Salzmann",
                "Boris Ivanovic",
                "Punarjay Chakravarty",
                "Marco Pavone"
            ],
            "title": "Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2020
        },
        {
            "authors": [
                "Balakrishnan Varadarajan",
                "Ahmed Hefny",
                "Avikalp Srivastava",
                "Khaled S. Refaat",
                "Nigamaa Nayakanti",
                "Andre Cornman",
                "Kan Chen",
                "Bertrand Douillard",
                "Chi Pang Lam",
                "Dragomir Anguelov",
                "Benjamin Sapp"
            ],
            "title": "Multipath++: Efficient information fusion and trajectory aggregation for behavior prediction",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Mingkun Wang",
                "Xinge Zhu",
                "Changqian Yu",
                "Wei Li",
                "Yuexin Ma",
                "Ruochun Jin",
                "Xiaoguang Ren",
                "Dongchun Ren",
                "Mingxu Wang",
                "Wenjing Yang"
            ],
            "title": "Ganet: Goal area network for motion forecasting. 2022",
            "venue": "doi: 10.48550/ARXIV.2209.09723. URL https://arxiv.org/abs/",
            "year": 2022
        },
        {
            "authors": [
                "Benjamin Wilson",
                "William Qi",
                "Tanmay Agarwal",
                "John Lambert",
                "Jagjeet Singh",
                "Siddhesh Khandelwal",
                "Bowen Pan",
                "Ratnesh Kumar",
                "Andrew Hartnett",
                "Jhony Kaesemodel Pontes",
                "Deva Ramanan",
                "Peter Carr",
                "James Hays"
            ],
            "title": "Argoverse 2: Next generation datasets for selfdriving perception and forecasting",
            "venue": "In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Ye Yuan",
                "Xinshuo Weng",
                "Yanglan Ou",
                "Kris Kitani"
            ],
            "title": "Agentformer: Agent-aware transformers for sociotemporal multi-agent forecasting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nMotion forecasting is a critical task for autonomous vehicles that need to plan their trajectory in a dynamic environment alongside other agents. This task is challenging, as the path of an agent depends on a variety of aspects, including environmental context, agent goals, navigable area and potential interactions with other agents. Moreover, the vast sensor suite available on autonomous vehicles produces data in a variety of structures with different properties. This information could be time sensitive, for example the position of other agents and the state of signalling equipment such as traffic lights; or static, such as the road topology which includes lane markings or pedestrian crossings. This creates challenges around representation, as data is sometimes sparse and succinct, and easily represented as a single point in the environment; or can be more complex, for example, lane markings which are sets of directional splines. Finally, the amount of data within a scene can vary as most features are instance based, some scenes may have more lane markings or agents than others.\nMotion forecasting models designed for use in autonomous systems also introduce a number of auxiliary requirements. They need to be faster than real-time, capable of being efficiently queried by trajectory optimisation or planning algorithms, and should encompass or capture some notion of uncertainty, enabling risk-based motion planning.\nThis paper introduces MotionPerceiver, a real-time motion forecasting model explicitly designed to handle these representation challenges, and allow for integration of information from a broad range of sensors. MotionPerceiver models a scene as a latent state that is updated using latent\nrepresentations of potentially diverse sensor information, as they are made available. Importantly, this representation takes design cues from Perceiver-IO [8] to enable deterministic memory and computational cost through a fixed latent state dimension, regardless of the number of instances or agents of interest. MotionPerceiver\u2019s latent state is queried to estimate the probability that a position in the scene is occupied. This can be done with an array of positions to generate a rasterized birds-eye-view image of occupancy probability of the scene. A learned time evolution function predicts how the latent state will evolve into the future. This is accomplished with multi-head self-attention to capture the dynamical interactions between the features embedded in the state such as the positions of vehicles and pedestrians. The predicted latent state can be updated with new or extra information, such as new observations of agent positions, with cross-attention. The predicted latent state queries the new measurement to find relevant information to update its state. This state-based update scheme can be considered analogous to a filtering approach that updates the latent representation of the scene as new information is made available (see Fig. 1).\nMotion forecasting needs to be completed at an above\nar X\niv :2\n30 6.\n08 87\n9v 1\n[ cs\n.R O\n] 1\n5 Ju\nn 20\n23\nreal-time rate to incorporate the new sensor measurements that provide crucial observations of the scene. To process a causal signal efficiently, it is preferable to only process new information that has arrived, rather than reprocessing old data with the new. Models that use a sequence for input are inefficient as they begin inference from scratch and reprocess old information at each new measurement. A more efficient strategy would be able to update the motion forecast with new information. It is also critical that a motion prediction solution scales to large numbers of agents to cope with large crowds. Solutions that scale super-linearly or are not parallelisable are practically unusable in real deployments, even with improvements in hardware. MotionPerceiver\u2019s latent state predictionupdate scheme is designed with a data streaming paradigm in mind. Work done building the latent state representation isn\u2019t discarded when a new measurement is available, unlike most other architectures that perform fixed-sequence based inference ([1, 5, 6, 7, 11, 12, 15, 16, 17, 20, 21, 22, 23, 27]).\nFinally, robustness to noisy or incomplete data is essential for real-world applications where occlusions and blind spots are common. Sequence based models commonly use tracklets of agents in the environment as input. These tracklets are easily corrupted when the agent is occluded or there is a fault in the algorithm that results in an id-switch. MotionPerceiver\u2019s latent state update does not require explicit data association, and is immune to the aforementioned problems.\nIn summary, the core contributions of this work are: \u2022 Introducing a transformer based architecture that per-\nforms recursive state estimation to forecast dense occupancy probability in a scene with sparse agent detections and trivially integrate additional scene information. \u2022 Encoding a dynamic scene and measurements into a latent state that can be propagated forward in time via a learned time-evolution function, enabling highly efficient future\ntrajectory prediction with a data-streaming methodology. \u2022 Demonstrating that implicit information such as navigable\nroad can be uncovered from social dynamics, and then factored into the model forecasting."
        },
        {
            "heading": "II. MOTIONPERCEIVER",
            "text": "MotionPerceiver, shown in Fig. 2, models agents navigating a social environment by applying a learnt function that predicts how the latent state of the scene, S (shown in the blue path), evolves in time. This latent state can be updated or corrected by measurements of the scene. This functional approach enables trivial extension to incorporate additional information to the latent state. For example, if topographical road features are available, this context can be encoded and applied to the state at each time-step with cross-attention, as depicted in the purple Key-Value path of Fig. 3. The time evolution function itself comprises a series of self-attention modules as shown in the orange Key-Value path of Fig. 3. These self- and crossattention based functions allow the trained model to capture relevant social dynamics between the agents and context when evolving the latent state forward. MotionPerceiver is inspired by Perceiver-IO [8], an architecture that functions by processing a latent array with self-attention and is designed to handle arbitrary input and output data using cross-attention."
        },
        {
            "heading": "A. Recursive state estimation using transformers",
            "text": "MotionPerceiver\u2019s higher-order functions are obtained using transformer [24] multi-head attention operations with an additional forward pass through a multi-layer perceptron (MLP) (1). These residual operations apply an additive update to latent state S to produce S\u2032, where intermediate hidden state S\u0302 is defined for clarity and dk is the keys dimension,\nS\u0302 = softmax( QKT\u221a\ndk )V + S, S\u2032 = mlp(S\u0302) + S\u0302. (1)\nLatent variables, Q,K,V, are derived from linear projections of the input(s) to the higher-order function. An MLP is used for the linear projection operations q, k, v on the inputs that create Q,K,V.\nThe time evolution process (2) is modeled as a learnt function F , that only depends on the previous state St\u22121|t\u22121, to predict the next state St|t\u22121, akin to a Markov chain transition,\nSt|t\u22121 = F(St\u22121|t\u22121). (2)\nHence the latent state St\u22121|t\u22121 is the input which is used to generate the query-key-value tuple for the self attention layers,\nQ = q(St\u22121|t\u22121), K = k(St\u22121|t\u22121), V = v(St\u22121|t\u22121). (3)\nThis process is depicted visually in the orange and cyan path(s) of Fig. 3 where St\u22121 is the \u201cLatent State\u201d input. This self-attention process is repeated several times as a block to create the overall time evolution function.\nExternal information It, such as a measurement of agent positions at time t, can be used to update the latent state. This is applied by a learned function U , after the state has been propagated to the time frame when the information was captured,\nSt|t = U(F(St\u22121|t\u22121), It) = U(St|t\u22121, It). (4)\nTo update the latent state with information It, crossattention is applied. Here, the latent variable q(St) queries transformed key-value pairs generated from the measurement k(It), v(It), shown,\nQ = q(St|t\u22121), K = k(It), V = v(It). (5)\nThis process is also depicted visually in Fig. 3 where we follow the orange path for \u201cObservation\u201d It, and the cyan path for \u201cLatent State\u201d St|t\u22121. The update cross-attention is followed by a self-attention layer with latent state St|t as input, as previously defined in (3).\nHowever, the forward propagation before update paradigm does not apply for the very first measurement update, which initialises the latent state. Rather, a set of learned embeddings query the first agent measurement with cross-attention. This standalone initialisation module is used once, as opposed to all other learned functions, which are reapplied multiple times to the latent state during the forecasting process.\nAt any point in time, the latent state can be queried with position P to determine the occupancy o\u0302t, given the current state of the scene St|\u00b7 with learned function O,\no\u0302t = O(P,St|\u00b7) = Pr(P|St|\u00b7). (6)\nOnce again, this is performed using a cross-attention operation with query-key-value tuple (7), where P is the sinusoidal positional encoding of P. However, it must be noted that this\noperation does not include the residual addition of St|\u00b7 used in the multi-head attention component of (1).\nQ = q(P), K = k(St|\u00b7), V = v(St|\u00b7) (7)"
        },
        {
            "heading": "B. Transforming Scene Information",
            "text": "A key advantage of the MotionPerceiver architecture is its flexibility to ingest data from a wide variety of information sources. Fundamentally, we just need to find a reasonable strategy to encode that data in a feature vector schema that can be queried by latent state. In general, information from the scene has some locality associated with it. We follow a common transformer paradigm, and choose to represent positional information as a sinusoidal feature[24]. In this section, we outline methods of processing raw data from the scene into a feature vector for MotionPerceiver\u2019s latent state to attend to.\nDetections of agents are tokenised by concatenating measured properties into a single feature vector. The x, y, \u03b8 (yaw) pose of an agent detection in an arbitrary, static global frame, is represented as a sinusoidal feature vector. The class of an agent is represented as a one hot encoding. Additional features such as directional velocity, yaw rate and bounding box extents are included as scalar values. Distance properties are normalised, while velocity and other units are not.\nEncoding contextual features which can be represented as a single point such as traffic lights is relatively simple. The position is encoded as a sinusoidal feature and the signal type is concatenated as a one-hot embedding. At time-steps where signal information is available, this is used to update the latent state with cross-attention after the time evolution step (as shown in t = 2 of Fig. 2). Conceptually this enforces the causal notion that if signals were to change at this exact time-step, none of the agents will have reacted to this change yet. As a result, this new signal will only affect the future states1.\nRoad-graph information on the other hand, is a more complex feature to represent. There are a few candidate methods\n1In this paper, we do not consider connected autonomous vehicles, i.e. vehicles are not directly streaming traffic signalling data and are unaware when signals will change in the future. Therefore we only consider causal traffic signalling, and signalling updates are not considered during future forecasting.\nfor integration into the model. Describing a lane marking as a vector of points is infeasible due to our sinusoidal position encoding methodology, which would make this become unreasonably inefficient. Therefore, we need to summarise each of these features into a form with a single positional embedding. VectorNet [4] proposes a hierarchical graph neural network to encode a road-graph (as well as agent dynamics for motion forecasting) by first locally processing the individual splines into features of fixed size, then modeling the global interactions between these features. This allows each spline to be concisely represented as a single feature of fixed size. The locality of this feature can be then defined by a single point, the mean or median of the spline.\nAlternatively, the road-graph can be rasterized into an image and then patchified to yield a set of tokens that represent different parts of the map. The contents of this patch is encoded with an MLP to create a latent feature representation of that patch, which is concatenated with a sinusoidal position embedding. This has the added benefit of granular control over the resolution of the context map, and not have significant run-time dependency on the content density of the map. For simplicity, the rasterized topology method is used in this paper.\nTo integrate topology context into the model, cross-attention is applied as the final stage of each latent state prediction (shown in Fig. 2). The rationale here is that this will enable the model to make final adjustments to the predicted position of vehicles given the topology context. For example, predicted occupancy should be adjusted to prefer the lane centers of the road topography as this is what vehicles will be following the vast majority of the time, rather than drifting off the road (unless of course the target is measured as veering off the road). Since the road topology is static, it only needs to be encoded once, and can be repeatedly used in future time-steps. Furthermore, the positional component of the features can be updated if the coordinate frame is to move."
        },
        {
            "heading": "C. Loss Function",
            "text": "MotionPerceiver is trained with a focal loss due to class imbalance between occupied and unoccupied pixels in the ground truth. The focal loss Lf is defined for occupancy prediction logits o\u0302 and ground truth binary occupancy o where \u03c3 is the sigmoid activation function and \u03b3 is the focal weighting factor,\nF = \u03c3(o\u0302)o+ (1\u2212 o)(1\u2212 \u03c3(o\u0302)), (8) Lf = \u2212 log(o\u0302)(1\u2212F)\u03b3 (9)\nAn additional weighting factor \u03b1 is used to weight positive samples (10),\n\u03b1f = \u03b1o\u0302+ (1\u2212 \u03b1)(1\u2212 o) (10)\nand is uniformly averaged over all time-steps t and rasterized occupancy pixels p for final loss L,\nL = 1 tp \u2211 t \u2211 p \u03b1fLf . (11)"
        },
        {
            "heading": "D. Online Inference",
            "text": "A key design feature of MotionPerceiver is the ability to update a latent state at a point in time with a new measurement and predict from there. To deploy this in an online application, the latent state that corresponds to the next measurement time can be saved so an update step can be applied to it, and the next forecast roll-out can begin from there. Hence, only one latent state needs to be saved, which has the memory requirement of the number of latent tokens multiplied by their dimensionality and the data type (i.e. floating point 32 or integer 8). As mentioned previously, this is a substantially more efficient approach when compared to others which require repeated inference over a sequence, most of which contains historical data that has already been seen and processed. To the best of our knowledge, there is no other architecture that natively handles streaming input data this way.\nFurthermore, we are implicitly enforcing temporal correlation among timesteps, as the latent state requires information from the previous timestep. This is not the case for sequential architectures which predict trajectory directly with an MLP[10], as there is no direct relation between output neurons of an MLP decoder [9]."
        },
        {
            "heading": "III. EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "A. Training Environment",
            "text": "All models2 were trained with PyTorch [19] from scratch with ADAMW using a learning rate of 1e\u22123 and a polynomial schedule with power 0.9 and epoch target of 50. For occupancy focal loss, we use \u03b1 = 0.75 and \u03b3 = 2. A batch size of 128 was used with varying mini-batch size depending on GPU Memory available 3. The main contributor to training time is back-propagation, rather than the forward pass. To conserve GPU memory, time points where the loss is applied are sparsely sampled. A combination of consistent and randomly selected time points (between 0-4sec) are used on each mini-batch. Random time point selection prevents spurious prediction artefacts that can occur if only consistent samples are used, as the network is not penalised for this. Scene data is sampled at a 2Hz interval during the past phase, i.e. MotionPerceiver receives agent and signal inputs at frame 0, 5 and 10.\nModels are tested on two datasets (Interaction, Waymo Open Motion Dataset), but predominately trained and evaluated on the larger and more comprehensive Waymo Open Motion Dataset [3] (WOMD) which consists of 104k segments of 9s sequences captured at 10Hz with a total of 7.64M motion tracks. Each sequence contains data such as agent position and velocity, state of traffic lights and road features. These sequences were captured by Waymo\u2019s self-driving car platform. This unfortunately has the consequence of severe and frequent occlusions in the tracked agents. Discussion of issues\n2Code and model weights will be released on publication. 3Either an array of 8x GTX 1080Ti 11GB, 4-8x RTX3090 24GB or 4x\nRTX A6000 48GB in Distributed Data Parallel Training\nwhich arise from this and training remedies can be found in the appendix.\nFor our ablations, we center the occupancy map and our coordinate system to the median position of all agents over all time and set the coverage of the occupancy map to [- 100m,100m] in the x and y direction relative to this position4. This coverage was chosen by analysing the dataset and finding the maximum difference between valid agents across an entire sequence. In other words, our occupancy region of interest (ROI) is large enough to track all observable agents over time in the sequence. The same configuration is used when evaluating on the Interaction Dataset. This differs from the methodology more commonly used with WOMD, which transforms the coordinate system into the self driving vehicles frame and introduces an extra offset that places the vehicle 1/4 below the map center and has a coverage of [-40m,40m]. We retrain our model in the WOMD evaluation frame for the purpose comparing with others. However, it is worth noting that this evaluation protocol considers vehicles outside of the ROI in the past and present frames as observable, and expects models to predict when these vehicles will enter the ROI of the occupancy map in future frames. This is problematic for our model as it uses normalised positions for sinusoidal encoding, and is thus unable to correctly process positions outside of the ROI. We introduce an additional evaluation method in the WOMD frame, that does not consider vehicles outside the ROI observable, and remove them from evaluation."
        },
        {
            "heading": "B. Performance metrics",
            "text": "Occupancy forecasting performance is measured with two statistics, Area Under Curve (AUC) and Soft Intersectionover-Union (Soft IoU). AUC, the area underneath the receiver operating characteristic curve, is generated by measuring the true and false positive rates at positive sample thresholds between 0 and 1. Soft IoU is computed using Eqn. 12 where O is ground truth, O\u0302 the prediction and p the pixels in the occupancy image,\nSoftIoU = \u2211 p OO\u0302\u2211\np(O + O\u0302 \u2212 OO\u0302) . (12)"
        },
        {
            "heading": "C. Uncertainty calibration",
            "text": "In general, we found MotionPerceiver quite conservative when it came to occupancy probability prediction over longer horizons. We remedy this by applying a simple scaling function to the predicted occupancy logit O\u0302 that increases the decay rate of predicted occupancy by a factor \u03b2,\nO\u0302 = { \u03b2O\u0302 if O\u0302 < 0 O\u0302 else .\n(13)\nFor all experiments \u03b2 is fixed as 8, which resulted in the WOMD Soft IOU performance improvement shown in table I.\n4This is larger and arguably more challenging than the region of interest typically considered by existing motion forecasting models, as it results in more agents and more interesting, longer horizon, dynamic relationships."
        },
        {
            "heading": "D. General Insights",
            "text": "The learned update function integrates information from new measurements of agents from the scene into the latent state. This has the effect of reducing uncertainty and error in vehicle position within the latent state. Furthermore, this adds any previously unobserved agents to the latent state. Both behaviours are demonstrated in Fig. 1. Here, MotionPerceiver does not immediately remove agents which aren\u2019t measured by an update to the latent state, and the occupancy probability associated with the agent continues to dilate until it fades away entirely. If the agent is in motion, this dilation comes with some form of a directed smear resulting from learned uncertainty over the agent\u2019s future state.\nWe first investigate the capacity of the network to perform motion forecasting without road and signalling context. Interestingly, MotionPerceiver demonstrates evidence of social reasoning by predicting typical traffic behaviour such as waiting for traffic on the main road to be clear before merging (Fig. 4 and 5). We include further examples of this behaviour in the appendix and supplementary material.\nAs expected, accuracy declines the further out the prediction is made from the last measurement as shown in Fig. 6. MotionPerceiver performs exceedingly well in AUC as truepositive-rates increase significantly before the false-positiverate. There is practically full coverage of the target vehicles\nas the false-positive-rate begins to increase, leading to an AUC \u2248 1 within our chosen reference frame."
        },
        {
            "heading": "E. Model Capacity",
            "text": "We briefly investigate model scaling with regard to the latent state\u2019s size, with results shown in Table II. Quadrupling the latent state capacity, i.e. doubling the number of latent variables and the number of channels, results in a modest performance increase in IoU. An increase in latent variable channels also results in a 3.6x increase in the number of learnable architecture parameters. However, the number of learnable parameters of the model is still an order of magnitude lower than similar occupancy forecasting models [6, 7, 15, 20] which typically have 15M or more parameters."
        },
        {
            "heading": "F. Adding Context",
            "text": "To demonstrate that forecasting performance is improved by updating the latent state with additional information, we run an ablation study to determine the contributions of each contextual update. Table III shows that MotionPerceiver is\nalready a strong baseline model with no context. The model seems to accurately predict future occupancy, based solely on vehicle position and learned interactions between agents. Adding context has the greatest positive impact to forecasting performance further into the future. Mean IoU and AUC over all time is only increased by 7% and 2% respectively, however at the 6s way-point, IoU performance increases by 13%. We note that in the individual ablations, road topology context has a slight advantage in mean Soft IoU, whereas signal context is more valuable in AUC. However, it should be noted that the inclusion of both sources of information gives the best performance."
        },
        {
            "heading": "G. Comparison with Existing Models",
            "text": "We explore the performance of the best model (with all contextual information included) in more detail below. We first investigate the impact of the different evaluation methods in Table IV to aid benchmarking, and then compare our performance with previous works on the withheld testing set of WOMD using their online evaluation server (Table V).\nTable. IV shows that MotionPerceiver performs approximately the same in both our and the WOMD evaluation reference frames on a subset of the validation data. We note that the ROI filter is not needed in our reference frame as it encompasses the entire scene. As expected, performance decreases when using the WOMD test and validation protocol where ground truth includes agents that were not present within the ROI in previous frames and are therefore not tracked by our model.\nTable V shows a quantitative performance comparison between models on WOMD witheld testing set, evaluated on Waymo\u2019s servers.\nH. Interaction Dataset\nTable VI shows MotionPerceiver performance on the Interaction Dataset [28]. This dataset is substantially smaller and less diverse than WOMD, consisting of data collected\nfrom 11 locations comprising 59k Sequences with 40k Motion Tracks. Furthermore, Interaction is missing traffic signalling, so this is not used for these experiments. For the Interaction roadgraph we use virtual lines defined within corresponding OpenStreetMap[18] files to create a rasterized image of the roadgraph for context.\nIt should be noted that, due to the data hungry nature of transformers, MotionPerceiver could not be trained from scratch on a dataset as small as Interaction, however it can be fine-tuned and or evaluated on such datasets. However we do not elect to fine-tune, and evaluate directly with a WOMD trained model. We benchmark against [16], the only other model that has been evaluated on Interaction and provides occupancy performance."
        },
        {
            "heading": "I. Runtime",
            "text": "It is not yet common practice to report on the applicability of motion forecasting architectures for online embedded use, but this is a core benefit of the proposed architecture, so we benchmark our system on a common embedded platform,\nthe Nvidia Jetson AGX. We export the individual high-level functions in MotionPerceiver by exporting each as an ONNX model. These models are then benchmarked with trtexec 5. Trtexec parses and optimizes the ONNX model for inference on Nvidia devices. During this process, it also benchmarks the inference time, which is reported in Table VII. From this, we can calculate a theoretical inference time for a forecast 8s into the future by repeated applications of TimePropagation and RoadCtxUpdate: 80 \u00b7 (0.905ms + 0.305ms) = 96.8ms. This approximately matches the periodicity of the sensor sample time of WOMD. When each new sample comes in, we can resume from the corresponding latent state and apply an agent and signal update (0.379ms+0.460ms), then predict another 8s into the future.\nA simple method to decrease total forecast time would be to increase the duration each time propagation function moves into the future, thus requiring fewer iterations for longer horizon prediction. This should be changed appropriately for downstream tasks to match the time resolution required. Since WOMD is evaluated at 1s intervals, a learned forecasting function that predicts 1s into the future would result in a MotionPerceiver inference time of 9.68ms for an 8s forecast at a 1s time resolution."
        },
        {
            "heading": "IV. RELATED WORK",
            "text": "Motion forecasting is the task of predicting the future position of agents in a scene. There are two paradigms to achieve this, per-agent and full-scene. Per-agent methods [12] transform the scene into the target\u2019s frame to predict how the target will evolve over time. These architectures become prohibitively expensive as scene transformation and forecasting is required for each agent, resulting in a linear, but parallelisable, scaling of inference cost. Scene-based methods holistically encode an entire scene and state to jointly predict motion of all agents, resulting in efficient inference. This is the dominant architecture in the literature, and the approach\n5https://github.com/NVIDIA/TensorRT/tree/main/samples/trtexec\nfollowed by MotionPerceiver. A variety of encoding architectures have been explored in this category including pointpillars [11, 13, 16], fully-convolutional [12, 20], transformer [15], graph [7] and hybrid methods [6].\nSequence based architectures consume measurements that have been accumulated over time for inference, for example by concatenating the trajectory history of an agent into a featurevector [4, 7, 27] or accumulating timestamped positions of agents in a point-cloud [11, 16]. This is the dominant inference paradigm for state-of-the-art methods due to the simplicity of capturing spatio-temporal features as a single input. However this has the disadvantage that inference is repeated from scratch for each forecast when a new measurement of a scene is captured. Furthermore, accurate tracking methods are required to create the trajectory history of targets for the input to these models. Motion forecasting performance could potentially be degraded if these tracklets are malformed by tracking algorithm errors. MotionPerceiver avoids this repetition of inference, and does not require tracklet inputs.\nTrajectory forecasting is a prediction task where trajectory splines are predicted for individual agents. This can be in the form of a probabilistic set, which represents the multi-modal decision distribution of the target [5, 12, 22, 23]. There are a variety of higher-level architectural designs which have been explored to accomplish this task. State-of-the-art trajectory planners commonly follow a paradigm of finding a set of likely goals for each agent, to then generate feasible trajectories towards these goals [5, 25]. Others use anchor based methods as a basis for multi-modal prediction forecasting. These anchors can be derived statically from the dataset before training [2] or learned during training [23]. Other methods forecast future motion without anchors or goals in mind [12, 17, 22, 27]. An important formalisation is to predict control inputs for the motion model of the agent to prevent infeasible motion predictions [22, 23]. However, the weakness of per-agent trajectory forecasting is the high complexity this formulation can elicit in larger, crowded scenes, which downstream algorithms have to process. A path-planning algorithm utilising these models has to parse, optimize and validate the multi-modal distribution of trajectory splines for each agent, adding a large number of constraints and complexity.\nOccupancy Prediction on the other-hand, is a motion forecasting formulation that predicts the probability that a point in a region is occupied by an agent. This paradigm is amenable to path planning algorithms as they can query whether a position in the navigable area is occupied at a time-point with a single read from an array. This formulation in isolation erases the identities of agents, however methods which also predict flow of occupancy over time [6, 7, 11, 15, 16, 20] enable retracing from where occupancy has originated, and thus the instance id. Occupancy prediction implicitly permits multimodal trajectory forecasting or prediction with uncertainty as the occupancy from a particular agent is able to spread beyond the real size of the agent as its position becomes more unknown.\nIncluding context via the integration of a mix of static\nand dynamic features is essential for forecasting performance. Static features can include topological features such as lane markings, potholes or static signage such as stop signs. Dynamic features are time dependent and include traffic signalling and information about other agents in the environment. Integrating this context into the model to capture environmental influences can be achieved in a variety of ways. Motion forecasting models within an autonomous driving settings predominantly utilise road topology features such as lane markings as context [5, 7, 14, 15, 25]. A smaller subset of models include traffic signals [6, 11, 16, 17], but this is likely attributed to some popular datasets [26, 28] not containing traffic signal information. MotionPerceiver is explicitly designed to be extendable and integrate a variety of features that may influence agent dynamics, as additional information becomes more widely available in driving datasets."
        },
        {
            "heading": "V. LIMITATIONS AND FURTHER WORK",
            "text": "Motion prediction models such as [5, 25] explicitly predict the goal position of an agent to then predict the motion of the agent towards that goal. This is a powerful construct as a key challenge of motion forecasting is modeling the future actions agents will decide to take to achieve these goals. In its current formulation, MotionPerceiver does not explicitly try to predict the end goal of agents. We hypothesise that including this knowledge into the architecture would improve performance even further.\nMoreover, MotionPerceiver does not preserve instance identity in occupancy predictions. Although this is not needed for path planning, future work could explore adding instance tracking by attributing output occupancy predictions to agent observations by analysing attention of the transformer functions to uncover this information.\nImportantly, for MotionPerceiver to be used for pathplanning, the model needs to be updated by conditioning on ego-agent actions. This could be potentially achieved by adding positional updates of the ego vehicle\u2019s planned trajectory during the future forecasting. MotionPerceiver is lightweight enough that several trajectories can be proposed in parallel from an initial state. Future work should focus on integrating this with trajectory optimisation strategies that fully leverage the efficient inference speed of MotionPerceiver.\nFinally, we qualitatively observe reduced performance on smaller agents (pedestrians, cyclists) when compared to vehicles. We run a brief study on class-aware model performance, i.e. the model must also distinguish occupancy from different classes. We measure poor performance on these underrepresented, smaller classes. Further detail can be found in the appendix. Improving small object forecasting performance is of upmost importance as avoiding collisions with pedestrians and cyclists is critical, and clearly in need of significant attention by the broader motion forecasting research community."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "This paper introduces MotionPerceiver, a motion forecasting architecture designed explicitly for fast and online use. The\nproposed architecture encodes a scene into a latent state that is evolved forward in time with a learned time evolution function and can be updated with new measurements and context. This learned recursive state estimation approach is significantly more computationally efficient than existing sequence processing architectures that repeatedly re-process past information. The learned time evolution function used by MotionPerceiver comprises a series of self-attention modules that capture social dynamics between the agents in the latent state. New measurements of the scene update the latent state with cross attention that refines the latent state and reduces error and uncertainty in agent positioning.\nMotionPerceiver predicts probability of occupancy at a given position by querying the latent space with a corresponding position embedding. A grid of positional embeddings can be used to create a rasterized image, or probabilistic occupancy grid. Alternatively, the latent state can be queried with sparse, local position embeddings as required by downstream algorithms, saving computational effort. For example, path planning algorithms do not need to parse the entire occupancy image, but can sample targeted points for trajectory optimisation.\nResults show that MotionPerceiver captures complex multiagent interactions from agent position measurements alone, but is trivially extensible with additional information, which increases forecast accuracy even further. Rasterized lane topology context provides a greater benefit than the inclusion of traffic information signalling, but the inclusion of all available contextual information is most effective."
        },
        {
            "heading": "VII. APPENDIX",
            "text": ""
        },
        {
            "heading": "A. Occlusion in Waymo Open Motion Dataset",
            "text": "The Waymo Open Motion Dataset (WOMD) is captured on Waymo\u2019s self driving car platform. This results significant occlusions in tracked agent trajectories, and the occasional ID switch. This is particularly problematic towards the end of the sequence where approximately half of the agents observed in the past and current frames are unobserveable in the future frames, shown in Fig. 7.\nThis results in an overestimation of the false positive rate due to poor ground truth data. This is shown in Fig. 8 where the model correctly predicts an occluded vehicle\u2019s position in future frames, but it is missing from the ground truth. Datasets such as Interaction [28] are collected from a fixed position at a point of interest, resulting in consistent observability of the map over time."
        },
        {
            "heading": "B. Speculative Prediction",
            "text": "The future frames of Waymo Open Motion includes detections of agents that have not been observed in past or present frames, which the dataset categorizes as occluded vehicles. We found that care needs to be taken to filter out these unobserved agents. The speculative prediction that arises from the inclusion of occluded agents in the ground truth loss hinders statistical performance, Table VIII. An example of speculative prediction where no ground truth occluded agents appear is shown in Fig. 9. Furthermore, there are areas where\na homogeneous cloud of occupancy grows (Fig. 10). This speculative prediction is removed by only training models to predict occupancy for observable agents, as shown in Table VIII."
        },
        {
            "heading": "C. Class Aware Occupancy",
            "text": "We briefly evaluate class-aware occupancy by using a separate MLP decoder for each class when querying occupancy (refer to eq. 7 and 1). We remind the reader that the input to the model already includes a one-hot encoding of the agent class. Cyclists are not reported by other methods, but also performs worse than pedestrians for our method (SoftIoU < 0.02), indicating that this is an under-represented class requiring substantially more investigation in future research."
        },
        {
            "heading": "D. Learnt Social Interactions",
            "text": "We include interesting social dynamics learnt by MotionPerceiver (Fig. 11, 12, 13), these scenarios are also included in the supplementary video. We show in the top right the time frame index of the sequence, relative to the present frame i.e. \u201cfuture: +80\u201d is 8sec into the future."
        },
        {
            "heading": "E. Model Architecture",
            "text": "The parameters used for the model with all context available are shown in Table X. We note that we add a dummy agent and signal feature because if there is no measured agents/signals and the entire input is masked, PyTorch\u2019s Transformer output is NaN. This is a constant dummy token of all zeros."
        }
    ],
    "title": "MotionPerceiver: Real-Time Occupancy Forecasting for Embedded Systems",
    "year": 2023
}