{
    "abstractText": "We experience various sensory stimuli every day. How does this integration occur? What are the inherent mechanisms in this integration? The \u201cunity assumption\u201d proposes a perceiver\u2019s belief of unity in individual unisensory information to modulate the degree of multisensory integration. However, this has yet to be verified or quantified in the context of semantic emotion integration. In the present study, we investigate the ability of subjects to judge the intensities and degrees of similarity in faces and voices of two emotions (angry and happy). We found more similar stimulus intensities to be associated with stronger likelihoods of the face and voice being integrated. More interestingly, multisensory integration in emotion perception was observed to follow a Gaussian distribution as a function of the emotion intensity difference between the face and voice\u2014the optimal cut-off at about 2.50 points difference on a 7-point Likert scale. This provides a quantitative estimation of the multisensory integration function in audio-visual semantic emotion perception with regards to stimulus intensity. Moreover, to investigate the variation of multisensory integration across the population, we examined the effects of personality and autistic traits of participants. Here, we found no correlation of autistic traits with unisensory processing in a nonclinical population. Our findings shed light on the current understanding of multisensory integration mechanisms.",
    "authors": [],
    "id": "SP:d8e8ef13c66ced7773b23f9e43d3f0b794937f22",
    "references": [
        {
            "authors": [
                "D. Alais",
                "D. Burr"
            ],
            "title": "The ventriloquist effect results from near-optimal",
            "year": 2004
        },
        {
            "authors": [
                "D. Angelaki",
                "Y. Gu",
                "G. DeAngelis"
            ],
            "title": "Multisensory integration",
            "venue": "bimodal integration. Curr. Biol",
            "year": 2009
        },
        {
            "authors": [],
            "title": "Configural processing in autism and its relationship to face",
            "year": 2006
        },
        {
            "authors": [
                "G. Bird",
                "R. Cook"
            ],
            "title": "Mixed emotions: the contribution of alexithymia",
            "year": 2013
        },
        {
            "authors": [
                "J. 1038/tp.2013.61 Bizley",
                "G. Jones",
                "S. Town"
            ],
            "title": "Where are multisensory signals combined",
            "year": 2016
        },
        {
            "authors": [
                "P. j.conb.2016.06.003 Boersma"
            ],
            "title": "Praat, a system for doing phonetics by computer",
            "venue": "Glot Int",
            "year": 2001
        },
        {
            "authors": [
                "D. 341\u2013345. Brainard"
            ],
            "title": "The psychophysics toolbox",
            "venue": "Spat. Vis",
            "year": 1997
        },
        {
            "authors": [
                "J. Vroomen"
            ],
            "title": "Intersensory binding across space and time",
            "year": 2013
        },
        {
            "authors": [
                "Y.C. Chen",
                "C. Spence"
            ],
            "title": "When hearing the bark helps to identify the dog",
            "year": 2010
        },
        {
            "authors": [
                "M. Chita-Tegmark"
            ],
            "title": "Social attention in ASD: a review and meta-analysis",
            "year": 2016
        },
        {
            "authors": [
                "I. 016-0077-x Choi",
                "J.Y. Lee",
                "S.H. Lee"
            ],
            "title": "Bottom-up and top-down modulation",
            "year": 2018
        },
        {
            "authors": [
                "R. Cook",
                "R. Brewer",
                "P. Shah",
                "G. Bird"
            ],
            "title": "Alexithymia, not autism",
            "year": 2013
        },
        {
            "authors": [
                "B. de Gelder",
                "J. Vroomen"
            ],
            "title": "Recognizing emotions by ear and by eye",
            "year": 2000
        },
        {
            "authors": [
                "T. Noesselt"
            ],
            "title": "Multisensory interplay reveals crossmodal",
            "venue": "Cogn. Neurosci. Emotion",
            "year": 2008
        },
        {
            "authors": [
                "M. 1038/415429a Greiner",
                "D. Pfeiffer",
                "R. Smith"
            ],
            "title": "Principles and practical application",
            "year": 2000
        },
        {
            "authors": [
                "F. 1717948115 Happ\u00e9",
                "U. Frith"
            ],
            "title": "The weak coherence account: detail-focused",
            "year": 2006
        },
        {
            "authors": [],
            "title": "Causal inference in multisensory perception",
            "venue": "PLoS One 2:e943",
            "year": 2007
        },
        {
            "authors": [
                "P. Liu",
                "S. Rigoulot",
                "M.D. Pell"
            ],
            "title": "Culture modulates",
            "year": 2015
        },
        {
            "authors": [
                "S.R. 034 Livingstone",
                "F.A. Russo"
            ],
            "title": "The ryerson audio-visual database",
            "year": 2018
        },
        {
            "authors": [
                "P.F. Lovibond"
            ],
            "title": "Manual for the Depression Anxiety",
            "year": 1995
        },
        {
            "authors": [
                "D. Foundation. Lundqvist",
                "A. Flykt",
                "A. \u00d6hman"
            ],
            "title": "Stress Scales, 2nd Edn. Sydney",
            "year": 1998
        },
        {
            "authors": [
                "C. Luo",
                "E. Burns",
                "H. Xu"
            ],
            "title": "Association between autistic traits",
            "venue": "Clinical Neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "L. Ruta",
                "L. Reale"
            ],
            "title": "Psychiatric comorbidities in asperger",
            "year": 2012
        },
        {
            "authors": [
                "H. McGurk",
                "J. MacDonald"
            ],
            "title": "Hearing lips and seeing voices",
            "venue": "Nature",
            "year": 1976
        },
        {
            "authors": [
                "B. Stein"
            ],
            "title": "Interactions among converging sensory inputs",
            "year": 1983
        },
        {
            "authors": [
                "A. Mihalik",
                "U. Noppeney"
            ],
            "title": "Causal inference in audiovisual perception",
            "year": 2020
        },
        {
            "authors": [],
            "title": "The VideoToolbox software for visual psychophysics",
            "year": 1997
        },
        {
            "authors": [
                "E. 156856897X00366 Pellicano",
                "D. Burr"
            ],
            "title": "When the world becomes \u201ctoo real\u201d",
            "year": 2012
        },
        {
            "authors": [
                "V.M. Sloutsky"
            ],
            "title": "Auditory dominance and its change",
            "year": 2004
        },
        {
            "authors": [
                "T. 2004.00747.x Rohe",
                "U. Noppeney"
            ],
            "title": "Distinct computational principles govern",
            "year": 2016
        },
        {
            "authors": [
                "L. Shams",
                "U.R. Beierholm"
            ],
            "title": "Causal inference in perception",
            "year": 2010
        },
        {
            "authors": [
                "L. Shams",
                "Y. Kamitani",
                "S. Shimojo"
            ],
            "title": "What you see is what you hear",
            "venue": "Cogn. Sci",
            "year": 2010
        },
        {
            "authors": [
                "R.A. 200101220-00009 Stevenson",
                "T.W. James"
            ],
            "title": "Audiovisual integration in human",
            "year": 2009
        },
        {
            "authors": [
                "K. Uno",
                "K. Yokosawa"
            ],
            "title": "Unity assumption between face and voice",
            "venue": "emotion. Psychol. Sci",
            "year": 2021
        },
        {
            "authors": [
                "M.M. Wanrooij"
            ],
            "title": "The principle of inverse effectiveness in audiovisual",
            "year": 2019
        },
        {
            "authors": [
                "A. 00335 Vatakis",
                "A. Ghazanfar",
                "C. Spence"
            ],
            "title": "Facilitation of multisensory",
            "year": 2018
        },
        {
            "authors": [
                "J. Vroomen",
                "M. Keetels"
            ],
            "title": "Perception of intersensory synchrony",
            "year": 2010
        },
        {
            "authors": [
                "J.A. Schirillo"
            ],
            "title": "Unifying multisensory signals across time and space",
            "year": 2004
        },
        {
            "authors": [
                "D.R. Wozny",
                "L. Shams"
            ],
            "title": "Recalibration of auditory space following",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "ORIGINAL RESEARCH published: 04 March 2022 doi: 10.3389/fnins.2022.782318\nEdited by: Julian Keil,\nUniversity of Kiel, Germany\nReviewed by: Patrick Bruns,\nUniversity of Hamburg, Germany James Kenneth Moran, Charit\u00e9 \u2013 Universit\u00e4tsmedizin Berlin, Germany\n*Correspondence: Hong Xu xuhong@ntu.edu.sg\n\u2020These authors have contributed equally to this work\nSpecialty section: This article was submitted to\nPerception Science, a section of the journal Frontiers in Neuroscience\nReceived: 24 September 2021 Accepted: 09 February 2022\nPublished: 04 March 2022\nCitation: Sou KL, Say A and Xu H (2022) Unity Assumption in Audiovisual\nEmotion Perception. Front. Neurosci. 16:782318. doi: 10.3389/fnins.2022.782318\nUnity Assumption in Audiovisual Emotion Perception Ka Lon Sou1,2\u2020, Ashley Say1\u2020 and Hong Xu1*\n1 Psychology, School of Social Sciences, Nanyang Technological University, Singapore, Singapore, 2 Humanities, Arts and Social Sciences, Singapore University of Technology and Design, Singapore, Singapore\nWe experience various sensory stimuli every day. How does this integration occur? What are the inherent mechanisms in this integration? The \u201cunity assumption\u201d proposes a perceiver\u2019s belief of unity in individual unisensory information to modulate the degree of multisensory integration. However, this has yet to be verified or quantified in the context of semantic emotion integration. In the present study, we investigate the ability of subjects to judge the intensities and degrees of similarity in faces and voices of two emotions (angry and happy). We found more similar stimulus intensities to be associated with stronger likelihoods of the face and voice being integrated. More interestingly, multisensory integration in emotion perception was observed to follow a Gaussian distribution as a function of the emotion intensity difference between the face and voice\u2014the optimal cut-off at about 2.50 points difference on a 7-point Likert scale. This provides a quantitative estimation of the multisensory integration function in audio-visual semantic emotion perception with regards to stimulus intensity. Moreover, to investigate the variation of multisensory integration across the population, we examined the effects of personality and autistic traits of participants. Here, we found no correlation of autistic traits with unisensory processing in a nonclinical population. Our findings shed light on the current understanding of multisensory integration mechanisms.\nKeywords: multisensory integration, semantic emotion perception, unity assumption, autistic traits, Weak Central Coherence Theory\nHIGHLIGHTS\n- Multisensory integration of emotion intensity follows a Gaussian distribution. - Angry requires more similar audiovisual intensity to form \u201cUnity\u201d judgment than happy. - Angry emotion relies more on auditory input, but happy relies more on visual input.\nINTRODUCTION\nIn daily life, we constantly experience stimuli from various sensory modalities in our environment. It has been postulated that these stimuli are processed in modality-specific unisensory systems before integration in the multi-modal process for coherent perception, known as multisensory integration (Meredith and Stein, 1983; Choi et al., 2018). However, increasing evidence has recently\nFrontiers in Neuroscience | www.frontiersin.org 1 March 2022 | Volume 16 | Article 782318\nsuggested this integration to occur even earlier for auditory, visual, and somatosensory input at various subcortical layers and the sensory periphery (Driver and Noesselt, 2008; Bizley et al., 2016; Wang et al., 2017; Gruters et al., 2018). Yet, the extent at which multisensory integration occurs in each area remains highly debated (Rohe and Noppeney, 2016). The strength of such integration also varies according to cue congruency. When stimuli are congruent, the multisensory effect is typically strong. Particularly in spatial integration, percepts tend to be biased toward either auditory or visual information according to their relative reliabilities (Alais and Burr, 2004; Mihalik and Noppeney, 2020). In more complex perception, two mismatched stimuli may even be integrated to create an illusion such as the McGurk effect in which a presented face with the mouth displaying \u201cga\u201d accompanied with an auditory sound \u201cba\u201d generates the illusory perception of \u201cda\u201d in more than 75% of adults. However, only about half of children experience such illusory effects, with the other half experiencing auditory dominance in such audiovisual conflicts (McGurk and MacDonald, 1976; Robinson and Sloutsky, 2004). When such unimodal stimuli are weak in intensity or presented with poor signal-to-noise ratio, a strong audiovisual benefit\u2014the principle of \u201cinverse effectiveness\u201d enhances perception of the weak signals through multisensory integration (Meredith and Stein, 1986; Angelaki et al., 2009; Stevenson and James, 2009; van de Rijt Luuk et al., 2019). Taken together, this raises the question: How weak of an intensity is needed to trigger inverse effectiveness in multisensory integration?\nThe \u201cunity assumption\u201d has proposed a perceiver\u2019s belief of individual unisensory information belonging together to modulate the degree of multisensory integration, particularly in higher-level factors such as speech signals (Chen and Spence, 2017; Vatakis et al., 2018; Uno and Yokosawa, 2021). The closer two pieces of sensory information are spatially, temporally, and semantically, the stronger the unity assumption belief would be, making multisensory integration more likely to occur (Wallace et al., 2004; Wozny and Shams, 2011). This is heavily reliant on causal inference, the implicit perceptual process whereby perceivers determine if signals come from a common, or separate cause (Shams and Beierholm, 2010; Mihalik and Noppeney, 2020). While spatial and temporal limits of the unity assumption have been widely examined (for reviews, see Vroomen and Keetels, 2010; Chen and Vroomen, 2013), the semantical limit has rarely been investigated. In this study, we aimed to understand how semantically similar in emotion intensity for the face and voice to meet unity assumption, and how meeting this assumption would affect multisensory integration strategies.\nWhile low-level audiovisual perception may rely only on spatial and temporal congruence, an additional feature\u2014 semantic congruence\u2014plays an increasingly important role as audiovisual stimuli get more complex, e.g., in speech, animal videos or emotion expression perception (de Gelder and Vroomen, 2000; Chen and Spence, 2017). Typically, it has been found that when semantic categories of the visual and auditory stimuli are congruent, the categorization accuracy and reaction time would be facilitated (de Gelder and Vroomen, 2000; Collignon et al., 2008; Koppen et al., 2008; Chen and\nSpence, 2010). However, unlike spatial and temporal congruence, semantic congruence is usually categorical (i.e., Congruent vs. Incongruent), and is rarely quantified in the literature. The lack of quantification results in the inability to examine the exact criteria for semantic aspects of the unity assumption. In fact, under its strict definition, it can be argued that the semantic congruence effect is irrelevant to the unity assumption as it is rare to directly ask if subjects perceive audiovisual stimuli to correspond to the same object/event (Chen and Spence, 2017). This raises the question of the role semantic congruence plays in the integration of audiovisual stimuli.\nThis study, therefore, attempts to address the above questions by quantifying semantic congruence in the context of audiovisual emotion perception. Here, semantic congruence was measured as the degree of emotional intensity similarity in audiovisual stimuli (a face and voice), and their respective sources\u2014whether from the same person or not, as perceived by subjects. This enabled us to then identify a critical face-voice emotional intensity difference for meeting the unity assumption. Concurrently, we also explored any difference in the strategies used in multisensory emotion perception when the unity assumption was met, as compared to when it was not. As previous studies have demonstrated multisensory emotion perception to be visually dominant when the intensities of audiovisual emotions were incongruent (Collignon et al., 2008), we hypothesize that facial information contributes more toward multisensory emotion perception than vocal information when the unity assumption is not met, but expect equal contributions when it is.\nThe ability for multisensory integration is known to vary across the population (McGurk and MacDonald, 1976; Robinson and Sloutsky, 2004; Laurienti et al., 2006; Charbonneau et al., 2013). As predicted by the Weak Central Coherence Theory (Happ\u00e9 and Frith, 2006), Autism Spectrum Disorder (ASD) and autistic traits have been associated with disrupted holistic perception (Behrmann et al., 2006; Nakahachi et al., 2008; Luo et al., 2017). In multisensory perception of emotion, it was found that reaction time facilitation of autistic individuals was less than that of typically developed individuals (Charbonneau et al., 2013). This atypical integration of multisensory input might explain the impaired social cognitive abilities in ASD (Kawakami et al., 2020a). Though results have previously shown autistic individuals to be unable to successfully integrate the two separate sensory stimuli, the underlying mechanisms for this impairment in multisensory emotion perception still remain unclear. One recent theory attempting to address this is the Bayesian model of autistic perception (Pellicano and Burr, 2012; Sevgi et al., 2020). The theory postulates that the social/nonsocial perceptual deficits in autism result from a deviation in the construction of top-down influences, i.e., priors, and/or the reduction in use of such priors. One of these priors in multisensory integration is the unity assumption, where the observer perceives two different sensory signals to be coming from the same source (K\u00f6rding et al., 2007; Shams and Beierholm, 2010; Chen and Spence, 2017). In support of this, one recent study showed that individuals with higher autistic traits tended to have a narrower temporal window for audiovisual integration than those with lower autistic traits during low-level\nFrontiers in Neuroscience | www.frontiersin.org 2 March 2022 | Volume 16 | Article 782318\nmultisensory perception (Kawakami et al., 2020b). In a similar thread, could autistic individuals also have stricter criterion for the semantic aspect of the unity assumption? In this case, the reduced multisensory benefit for emotion perception in autistic individuals may be explained as a reduced ability to perceive emotionally charged facial and auditory cues to be from the same person. However, the opposite has also been reported (Poole et al., 2017; Kawakami et al., 2020a), suggesting that this may not be a universal phenomenon.\nAnother prediction of the Bayesian model of autistic perception is that autistic individuals tend not to use the prior during multisensory perception (Pellicano and Burr, 2012; Sevgi et al., 2020). Accordingly, autistic traits should therefore attenuate the strategic change in audiovisual integration when the unity assumption is either met, or not. The autistic deviation in audiovisual emotion perception may be expected to occur only when the unity assumption is met. To test this hypothesis, we may lend theoretical understanding toward the autistic influence in multisensory emotion perception: If the change occurs when the \u201cunity assumption\u201d is met, it indicates autistic influences in multisensory emotion perception to be partially due to a deviation in top-down influences; though if it occurs regardless of whether the unity assumption is met, it might instead point toward the autistic influence occurring at the stage independent from top-down influences, i.e., either before integration or at a unimodal stage.\nTherefore, we aimed to address two questions in this study: (1) How similar would the emotional intensities of the face and voice be to meet the \u201cunity assumption,\u201d and how would this then affect an individual\u2019s audiovisual integration strategy? This question was addressed directly by asking if subjects perceived audiovisual stimuli to be similar in emotional intensity such that the face and voice were possibly coming from the same person. By comparing the differences in emotional intensities of the face and voice of subjects\u2019 judgments, we identified the point at which the unity assumption came into effect. Next, by modeling subjects\u2019 responses, we could obtain their reliance on individual visual or auditory information for comparison across conditions. (2) How do autistic traits affect the criteria of the \u201cunity assumption\u201d and audiovisual integration strategy? To answer this question, we included measures of autistic traits, alexithymia, depression, anxiety, and stress to examine their effects and control for the influence of such common comorbidities of autism (Mazzone et al., 2012; Bird and Cook, 2013; Poqu\u00e9russe et al., 2018)."
        },
        {
            "heading": "MATERIALS AND METHODS",
            "text": ""
        },
        {
            "heading": "Participants",
            "text": "Forty-eight subjects (mean age = 21.69, SD = 1.85; 36 females) consented to and participated in the experiment at the lab. All participants met the inclusion criteria of being University students and having normal, or corrected-to-normal vision, with no diagnosed clinical or neurological impairments. Sample size was determined based on previous studies (Brandwein et al., 2015; West et al., 2018). This study was approved by the Institutional Review Board (IRB) at Nanyang Technological\nUniversity, Singapore, by the Code of Ethics of the World Medical Association (Declaration of Helsinki) for experiments involving human subjects."
        },
        {
            "heading": "Stimulus",
            "text": "Facial stimuli were obtained from the Karolinska Directed Emotional Faces\u2014Dynamic (KDEF-dyn) database (Lundqvist et al., 1998; Calvo et al., 2018). The dynamic faces were 30-frame videos, each a frame-by-frame morph first from a neutral (frame 1) to emotional (frame 30) face of the same actor. Videos of angry and happy faces of 4 actors (2 females; face ID F02, F09, M06, and M11) were selected based on high recognition rates (> 90%) as observed in a previous validation study (Calvo et al., 2018). From the original videos, we further derived five video types, each representing a different condition: (1) Angry Strong: 30-frame video from neutral to 100% angry (30th frame of original angry video); (2) Angry Weak: 15-frame video from neutral to 50% angry (15th frame of original angry video); (3) Happy Strong: 30- frame video from neutral to 100% happy (30th frame of original happy video); (4) Happy Weak: 15-frame video from neutral to 50% happy (15th frame of original happy video); (5) Neutral: 1st frame of the videos (0% angry/happy). In the 30- and 15- frame videos, each frame lasted for 40 and 79 ms, respectively, with the last frame presented for an additional 500 ms to mitigate the effects of an abrupt end to the video. Together, the duration of each video was 1.69 s. Faces in the neutral condition were also presented for 1.69 s. All frames were converted to grayscale and matched in luminance using MATLAB R2018b (Mathworks, MA, United States) using the toolbox SHINE (Willenbockel et al., 2010). Faces were consistently presented at 6.5 inches \u00d7 5 inches. In total, there were 20 facial stimuli videos (5 emotional conditions\u00d7 2 genders\u00d7 2 actors per gender).\nVoice stimuli were obtained from the dataset Ryerson AudioVisual Database of Emotional Speech and Song (RAVDESS) (Livingstone and Russo, 2018). Strong Angry voice, Weak Angry voice, Strong Happy voice, Weak Happy voice, and Neutral voice of 4 actors (2 females; voice IDs are F08, F14, M03, and M15) were similarly selected based on the high recognition rates observed in a previous validation study (\u2265 80%, except one weak happy voice clip with 40% accuracy and one strong happy voice clip with 70% accuracy) (Livingstone and Russo, 2018). Each clip depicted an actor speaking an emotionally neutral sentence (\u201cKids are talking by the door,\u201d \u201cDogs are sitting by the door\u201d) either in an emotional or neutral voice. Voice volume intensities were standardized using Praat 6.1.12 (Boersma, 2001). The average duration of voices was 1.69 s (SD = 0.31 s), to match the duration of face stimuli. In total, there were 20 voice stimuli clips (5 emotional conditions \u00d7 2 genders \u00d7 2 actors per gender). The voices were presented from the computer (iMac) speaker, the volume adjusted such that the voices could be clearly heard.\nThe multisensory stimuli were then generated by combining the face and voice stimuli. Only stimuli of the same gender and emotional category (except neutral stimuli which were constructed of both angry and happy) were combined. As a result, there were 144 multisensory stimuli (2 emotions (Angry and Happy) \u00d7 9 intensity combinations (32 intensities: Neutral, Weak, and Strong) \u00d7 4 actor combinations (22 actors) \u00d7 2\nFrontiers in Neuroscience | www.frontiersin.org 3 March 2022 | Volume 16 | Article 782318\ngenders). The duration of each frame of face stimulus was calculated as: (Duration of the Voice Stimulus\u20140.5 s)/Total Number of Frames in the Face Stimulus. For example, if a 30- frame face stimulus was combined with a 1.2 s voice stimulus, the duration of each frame would be (1.2\u20130.5 s)/30 frames = 23 ms. Again, the last frame was presented for an additional 500 ms to mitigate the effects of an abrupt end to the video. For Neutral face stimuli, the face was presented for the same duration as the voice stimulus."
        },
        {
            "heading": "Apparatus",
            "text": "All visual and auditory stimuli were presented on a 27-inch iMac with a refresh rate of 60 Hz and spatial resolution of 1,920\u00d7 1,080 pixels. The behavioral tasks were conducted in MATLAB R2018b (Mathworks, MA, United States) with the Psychophysics Toolbox extensions (Brainard, 1997; Pelli, 1997). The questionnaires were presented in Google forms. The experiment was conducted in a dim-lit room of a quiet psychophysics lab."
        },
        {
            "heading": "Procedure",
            "text": "Upon reading the study information and indicating participatory consent, subjects then proceeded to first complete either the questionnaires or behavioral tasks in an order randomized across subjects. Three questionnaires\u2014the Autism Spectrum Quotient (AQ) (Baron-Cohen et al., 2001), Toronto Alexithymia Scale (TAS-20) (Bagby et al., 1994) and Depression, Anxiety, Stress Scale (DASS-21) (Lovibond and Lovibond, 1995) were completed in a randomized, consecutive order.\nIn the behavioral task session, subjects first familiarized themselves with the stimulus validation task through a practice block. This block also served as time for volume calibration to\nensure clear presentation of the voice stimuli. After, subjects then completed the stimulus validation block in which they identified the emotions of the face stimuli or the voice stimuli from the choices \u201cAngry,\u201d \u201cHappy,\u201d \u201cNeutral,\u201d and \u201cNone of the above.\u201d\nNext, there was a second practice block for subjects to then familiarize themselves with the Unisensory and Multisensory tasks. The Unisensory and Multisensory blocks were completed in a randomized order. In the Unisensory block (Figure 1A), either a face or voice stimulus was presented on screen, or from the audio speaker only for an average of 1.69 s. After each presentation, subjects rated the emotional intensity of the stimulus on a 7-point Likert scale. For the Angry or Happy stimuli, each was presented twice throughout the block, subjects being asked to rate the intensity of anger or happiness, respectively. For Neutral stimuli, each was presented four times throughout the block, with subjects asked to rate the intensity of anger in half of the trials, and the intensity of happiness in the other half. As a result, there were a total of 96 trials in the Unisensory block, of 2 modalities (Face and Voice) \u00d7 2 emotions (Angry and Happy)\u00d7 3 intensities (Neutral, Weak, and Strong)\u00d7 2 actors\u00d7 2 genders\u00d7 2 repetitions.\nIn the Multisensory block (Figure 1B), a multisensory stimulus was also presented on screen and from the speaker for an average of 1.69 s. After presentation, subjects were asked to rate the emotional intensity of the stimulus on a 7-point Likert scale. Each of the Angry and Happy stimuli was presented once throughout the block. Subjects were asked only to rate the intensity of anger for Angry stimuli, and the intensity of happiness in Happy stimuli. For Neutral stimuli, each stimulus was shown twice throughout the block, subjects asked to rate the intensity of anger in half, and the intensity of happiness\nFrontiers in Neuroscience | www.frontiersin.org 4 March 2022 | Volume 16 | Article 782318\nin the other half of the trials. After the intensity ratings, the subjects were also asked to judge: \u201cAre the emotional intensities of the face and the voice similar to each other?\u201d Subjects pressed \u201cO\u201d on a keyboard to indicate if intensities were Similar and \u201cP\u201d if they were Not Similar. \u201cEmotional intensities are similar\u201d was defined as \u201cit is possible that the face and voice are coming from the same person\u201d at the beginning of the block. In total, there were 144 trials in the Multisensory block: 2 emotions (Angry and Happy) \u00d7 9 intensity combinations (32 intensities: Neutral, Weak, and Strong) \u00d7 4 actor combinations (22 actors)\u00d7 2 genders."
        },
        {
            "heading": "Statistical Analysis",
            "text": "In the Multisensory block, trials were categorized by two factors: (1) Similarity\u2014Similar vs. Not Similar trials in which the facevoice pair was judged as \u201csimilar\u201d or \u201cnot similar\u201d in emotional intensity; and (2) Emotion\u2014Angry vs. Happy trials in which the subjects were asked to rate the intensity of anger and happiness, respectively. One of our aims was to identify how emotionally different the face and voice had to be for perceivers to believe the two were from/not from the same person (i.e., meeting/not meeting the \u201cunity assumption\u201d). To achieve this, we conducted the Receiver Operating Characteristic (ROC) analysis (Greiner et al., 2000) on audiovisual emotional intensity differences in order to classify the two states for each subject. Audiovisual emotional intensity differences were calculated by subtracting the average intensity rating for voices in all emotions from the average intensity rating of faces in all emotion conditions. These were calculated based on ratings in the validation (unimodal) trials only. The optimal cut-off was identified by maximizing the Youden index (J = Sensitivity + Specificity \u2212 1) (Hilden, 1991). If more than one cut-off showed the same Youden index, we chose the one with minimum differences in sensitivity and specificity. A one-way (Emotion) within-subject Analysis of Covariance (ANCOVA) was conducted to investigate if the cut-off value differed across emotions. Normalized AQ, TAS20, Depression, Stress and Anxiety scores were included in the analysis as covariates.\nAnother aim was to identify the individual contributions of the face and voice during multisensory emotion perception and to examine if these contributions differed across conditions. To achieve this aim, the intensity rating responses of each subject were fitted into the following linear regression model:"
        },
        {
            "heading": "IntensityM = \u03b2V IntensityV + \u03b2AIntensityA + c",
            "text": "Where IntensityM , IntensityV , and IntensityA represent the emotional intensity ratings of the multisensory stimulus (from the Multisensory block), the face (from the visual trials of the Unisensory block), and the voice (from the auditory trials of the Unisensory block) respectively. Coefficients \u03b2V and \u03b2A represent weights of the emotional intensities of the face and voice, respectively, in predicting the emotional intensity of the multisensory stimulus, and coefficient c representing the remaining factors not captured by face or voice.\nThe sensory reliance score was calculated by subtracting \u03b2A from \u03b2V such that the more positive the sensory reliance was, the more a subject was relying on visual input during\nmultisensory perception, whereas the more negative the sensory reliance score was, the heavier the subject\u2019s reliance on auditory input was. When the sensory reliance score was equal to zero, the visual input and the auditory input were equally weighted in multisensory perception. After, a 2 (Similarity) \u00d7 2 (Emotion) within-subjects ANCOVA was conducted to compare the sensory reliance scores across conditions. The unisensory degree score was calculated by taking the absolute value of the sensory reliance score. The higher the unisensory degree score was, the heavier the subject\u2019s reliance on unisensory input during multisensory perception, whereas the lower the unisensory degree score, the heavier the subject\u2019s reliance on both sensory input during multisensory perception. Furthermore, 2 (Similarity) \u00d7 2 (Emotion) within-subjects ANCOVA was conducted to compare the unisensory degree scores across conditions. In both ANCOVAs, covariates of normalized AQ, TAS-20, Depression, Anxiety, and Stress scores were included.\nStatistical analysis was conducted using SPSS Statistics 25 (IBM, NY, United States) and Matlab R2018b (Mathworks, MA, United States)."
        },
        {
            "heading": "RESULTS",
            "text": ""
        },
        {
            "heading": "Behavioral Response Accuracy and Unisensory Emotional Intensity Rating",
            "text": "Unisensory stimuli presented in the validation block generally achieved high recognition rates (> 74%; Figure 2A), with the exception of Weak Angry faces (M = 67.7%, SD = 30.5%), as well as Weak Angry (M = 66.7%, SD = 33.6%), Strong Happy (M = 64.1%, SD = 26.8%), and Weak Happy (M = 53.1%, SD = 28.5%) voices. Emotions in facial stimuli appeared to be better recognized than voice stimuli [t(47) = 3.96, p < 0.001], with high average recognition rates of both faces (M = 84.4%, SD = 11.7%) and voices (M = 74.6%, SD = 13.3%).\nIn facial stimuli, stronger happy (M = 97.9%, SD = 7%) intensities were better recognized compared to weaker happy intensities [M = 84.4%, SD = 24.5%; t(47) = 4.42, p < 0.001]. Similarly in conditions for the angry emotion, stronger angry (M = 79.2%, SD = 22.1%) intensities were better recognized than weaker angry displays [M = 67.7%, SD = 30.5%; t(47) = 3.02, p < 0.001]. Interestingly, neutral faces (M = 92.7%, SD = 15.4%) were better recognized than both Strong and Weak Angry faces [t(47) = 3.44, p = 0.001, and t(47) = 4.53, p < 0.001, respectively], but significantly less accurately than Strong Happy faces [t(47) = 2.48, p = 0.02].\nIn contrast, auditory stimuli of stronger intensities were better recognized than weak intensity conditions for both angry [t(47) = 5.92, p < 0.001, with strong angry voices: M = 93.8%, SD = 16.7% and weak angry voices: M = 66.7%, SD = 33.6%] and happy emotions [t(47) = 2.17, p = 0.035, with strong happy voices: M = 64.1%, SD = 26.8%, and weak happy voices: M = 53.1%, SD = 28.5%]. Different from faces, the neutral voice (M = 95.3%, SD = 13.3%) was better recognized than weak angry voices [t(47) = 6.51, p< 0.001] but not strong angry voices [t(47) = 0.77, p = 0.44], as well as better than both strong and weak happy voices [t(47) = 7.25, p< 0.001, and t(47) = 8.94, p< 0.001, respectively].\nFrontiers in Neuroscience | www.frontiersin.org 5 March 2022 | Volume 16 | Article 782318\nIn the intensity rating task (Figure 2B), as expected, strong emotion conditions were rated as of higher intensity than weak [t(47) = 22.5, p < 0.001, with Strong: M = 5.99, SD = 0.62, and Weak: M = 4.43, SD = 0.79] and neutral conditions [t(47) = 37.1, p < 0.001, with Neutral: M = 1.78, SD = 0.63]. Interestingly, angry voices were rated as of higher intensity than angry faces (t(47) = 4.46, p < 0.001, with Face: M = 3.94, SD = 0.70, and Voice: M = 4.24, SD = 0.60). However, this difference between face and voice was not observed in the happy emotion [t(47) = 1.15, p = 0.26, with Face: M = 3.99, SD = 0.54, and Voice: M = 4.10, SD = 0.81].\nFrontiers in Neuroscience | www.frontiersin.org 6 March 2022 | Volume 16 | Article 782318\nDifference in Emotional Intensities Between Face and Voice to Meet the \u201cUnity Assumption\u201d To understand when subjects judged the face and voice to be of similar emotion intensity, we further analyzed the audiovisual emotional intensity differences and their responses of \u201cSimilar\u201d judgment (Figure 3A). Subjects were most likely to judge stimuli to be of \u201cSimilar\u201d intensities when the intensity difference was +0.5 (Face stronger; M = 85.9%, SD = 12.4%), 0 (M = 85.1%, SD = 16.5%), and\u22120.5 (Voice stronger; M = 83.6%, SD = 18.2%). When the audiovisual emotional intensity difference increased to approximately\u00b1 2, the likelihood of making \u201cSimilar\u201d judgments dropped below 50%. Fitting the subjects\u2019 responses into a Gaussian curve using the Curve Fitting toolbox in Matlab R2018b (Mathworks, MA, United States), we found that the center of the curve was significantly biased toward the face compared to the voice [b = +0.21, 95% CI = (0.10, 0.32)]. This suggests that when faces were slightly emotionally stronger than the voice, that subjects would still judge them to be of \u201cSimilar\u201d emotion intensities.\nWe then conducted the ROC analysis to investigate subjects\u2019 criteria for judging audiovisual stimuli to be of a similar emotion. The average cut-off intensity for differentiating \u201cSimilar\u201d trials from \u201cNot Similar\u201d trials was 2.50 (SD = 0.71). The result suggesting that when the emotional intensity difference between the face and voice was below 2.50 (on a 7-point Likert scale), subjects judged them to be of similar emotion intensity and perceived the face and voice to be from the same person, as defined at the beginning of the experimental block. Thus, the unity assumption would be met when the intensity differences between face and voice was below 2.50 on the 7-point Likert scale.\nA one-way analysis of covariance (ANCOVA) on intensity ratings with the main effect of Emotion and covariates AQ, TAS, Depression, Anxiety and Stress revealed a main effect of Emotion [F(1, 42) = 5.73, p = 0.021, partial \u03b72 = 0.12], but no effects of any covariates (p\u2019s\u2265 0.25). Further analysis revealed that the optimal cut-off difference (in absolute value) for face-voice emotional intensity to differentiate Similar trials from Not Similar trials was 2.32 (SD = 0.83), and 2.68 (SD = 0.92) points on the 7- point Likert scale for Angry and Happy emotions, respectively (Figure 3B). It thus suggests that the Angry emotion requires a higher similar audiovisual intensity than the Happy emotion to form a \u201cUnity\u201d judgment."
        },
        {
            "heading": "Reliance on Face or Voice in",
            "text": "Multisensory Integration Do participants rely on the face or voice for their judgment? We calculated the sensory reliance score as the difference between auditory and visual coefficients in the linear regression model by \u03b2V \u2212 \u03b2A. The 2 (Similarity) \u00d7 2 (Emotion) within-subjects ANCOVA (AQ, TAS-20, Depression, Anxiety and Stress as the covariates) on the sensory reliance score showed a significant main effect of Emotion [F(1, 42) = 46.13, p < 0.001, partial \u03b72 = 0.52]. The sensory reliance score of the Angry condition (M =\u22120.14, SD = 0.31) was significantly different from that of the Happy condition [M = 0.08, SD = 0.26; t(47) = 7.04, p < 0.001; Figure 4A]. This suggests that the perception of angry emotion tends to rely more on auditory information, whereas happy emotion relies more on visual information. A significant main effect of Similarity was also observed [F(1, 42) = 10.17, p = 0.003, partial \u03b72 = 0.19]. Sensory reliance scores in the Similar condition (M = 0.05, SD = 0.22) were significantly higher than that of the Non Similar condition [M = \u22120.10, SD = 0.32; t(47) = 3.18,\nFIGURE 3 | Audiovisual emotion intensity differences in multisensory integration. (A) Effect of audiovisual emotion intensity difference on judgment that audiovisual stimuli were similar in emotional intensity. Positive audiovisual emotional intensity differences indicate the emotional intensity of faces to be stronger than voice; negative differences indicate voices to be stronger than faces. The estimated parameters for the Gaussian distribution: a = 0.86, 95% CI = [0.73, 1.0]; b = 0.21, 95% CI = [0.1, 0.23]; c = 0.88, 95% CI = [0.72, 1.04]. (B) Face-voice emotional intensity absolute difference cut-off for differentiating Similar trials and Not Similar trials by emotion conditions. Optimal cut-offs were obtained from each participant using ROC analysis by maximizing the Youden index (J = Sensitivity + Specificity \u2212 1). The error bars indicate standard errors of mean (SEM) in (A,B).\nFrontiers in Neuroscience | www.frontiersin.org 7 March 2022 | Volume 16 | Article 782318\nFIGURE 4 | Sensory reliance in multisensory integration. (A) Sensory reliance score by emotion conditions. Positive values in sensory reliance score indicate a higher reliance on visual input, negative values indicating a higher reliance on auditory input during multisensory emotion perception. \u2217\u2217\u2217p < 0.001. The error bars indicate standard errors of mean (SEM). (B) Relationship between AQ and sensory reliance score. Solid triangles and circles indicate the responses from the subjects with AQ \u2265 25.\np < 0.01]. Furthermore, we also found a significant interaction effect of Emotion \u00d7 Similarity \u00d7 Anxiety [F(1, 42) = 6.85, p = 0.01, partial \u03b72 = 0.14). No other effects related to AQ, TAS20, Depression, Anxiety, and Stress were found to be significant (p\u2019s \u2265 0.09)."
        },
        {
            "heading": "Autistic Traits Do Not Correlate With Sensory Reliance",
            "text": "To investigate whether autistic traits (AQ) were associated with sensory reliance, we correlated AQ with sensory reliance scores using Pearson\u2019s correlations (Figure 4B). Surprisingly, we found no significant correlations of AQ scores with sensory reliance in Happy or Angry conditions (Figure 4B). Overall, the sensory reliance scores of Happy condition are more positive than those of Angry condition. As positive reliance scores indicate reliance on visual signals, this may suggest that the Happy emotion condition elicited heavier reliance on visual signals while the Angry emotion relied more on auditory signals, consistent with findings from the above ANCOVA."
        },
        {
            "heading": "Testing the Reliability-Weighted",
            "text": "Multisensory Integration Model in Similar"
        },
        {
            "heading": "Trials",
            "text": "When the emotion intensities of both sensory modalities were similar (Similar condition), which sensory modality did the subjects rely on to make their judgments? The reliabilityweighted model (Ernst and Banks, 2002; Alais and Burr, 2004) may provide an answer to this. The reliability-weighted model predicts that the weights of sensory information during multisensory integration depend on the relative reliability of sensory information. Therefore, if vision is more situationally reliable than auditory information, visual information will be weighted heavier during multisensory integration; otherwise, auditory information will be weighted heavier.\nTo test if subjects\u2019 responses in the Similar trials fitted with the reliability-weighted model, we correlated the subjects\u2019 sensory reliance scores with their audiovisual difference in emotion\nrecognition accuracy (recognition accuracy in face\u2014recognition accuracy in voice). Surprisingly, results showed that the two were not significantly correlated with each other (r = 0.13, p = 0.41). However, to further explore whether there was an effect of emotion in the relationship, we separated the analyses by emotions such that sensory reliance scores of the Angry condition or Happy condition were correlated with the recognition accuracy difference of face and voice. We found a significant correlation between sensory reliance and accuracy difference favoring the same modality in the Angry condition (Angry: r = 0.41, p = 0.004; Figure 5A), but not in the Happy condition (Happy: r = \u22120.23, p = 0.12; Figure 5B). This observation may lend support to the reliability-weighted model in Angry perception when the unity assumption is met. The reason this correlation was not observed in Happy emotion might be due to visual dominance in the Happy emotion as generally, happy faces were recognized more accurately than happy voices (Figure 5B). Comparing Figures 5A,B, it thus suggests an emotional effect in multisensory integration."
        },
        {
            "heading": "DISCUSSION",
            "text": "We investigated the criteria for multisensory integration through judgments of emotion from faces and voices at different intensities. We found that the more similar the audiovisual stimuli intensity, the more likely the unity assumption was, demonstrating the effects of cue disparity in a semantic context. There is an emotional effect in multisensory integration, with the Angry emotions observed to require more similar audiovisual intensities to form a \u201cUnity\u201d judgment as compared to the Happy emotions. The Angry emotion relied more on auditory stimuli whereas the Happy emotion relied more on visual stimuli. Lastly, we observed autistic traits to be unrelated to levels of unisensory reliance on one sensory modality for emotion perception in the present non-clinical population.\nThe major question we attempted to address in this study was the criterion of the semantic aspect of the unity assumption in\nFrontiers in Neuroscience | www.frontiersin.org 8 March 2022 | Volume 16 | Article 782318\nFIGURE 5 | Reliability-weighted multisensory integration model. Relationship between the difference in the audiovisual difference in emotion recognition accuracy and the sensory reliance scores in the Similar trials with (A) angry stimuli; and (B) happy stimuli. Positive values in sensory reliance score indicate a higher reliance on visual input, and negative values indicate a higher reliance on auditory input during multisensory emotion perception.\nemotion perception. In line with the maximum-likelihood model of Bayesian integration, we found that multisensory integration in semantic emotion perception follows a Gaussian distribution, with the presently identified criterion at around 2.5 points on a 7- point Likert scale in audiovisual emotional intensity difference. This is so as to directly reveal the multisensory integratory function, and to quantify the intensity differences in multisensory integration for semantic emotion perception. Interestingly, we also found the semantic criterion of the unity assumption to be more lenient for stronger faces than voices of the same emotion. Similar findings have been reported in studies investigating the temporal criterion of the unity assumption (Slutsky and Recanzone, 2001; Lewald and Guski, 2003; Chen and Vroomen, 2013). The sensory bias was explained as light typically reaching the perceiver earlier than sound since it travels faster, our brain therefore adjusting the time window for audiovisual integration to account for this difference. Our findings may suggest that facial emotion is accompanied with a weaker or less expressive voice emotion, or that facial emotion is given more weights on intensity rating than voice emotion. These possibilities may be further investigated in future studies.\nMoreover, our findings also support that the unity assumption can modulate the audiovisual integration strategy used during audiovisual emotion perception. One tends to rely more on the information from visual inputs rather than auditory input during emotion perception when the unity assumption is met compared to when it was not. A similar finding was reported by Wallace et al. (2004), who showed that when audiovisual stimuli were judged as unified spatially, the spatial ventriloquist effect was completely biased toward visual information; whereas when they were judged as not unified, no bias, or even a negative bias was induced. One possible explanation is that once the unity assumption is met, the perceiver no longer cognitively calculates the average emotional intensity of the face-voice pair but instead perceives the face-voice pair as a whole, and this holistic percept is biased toward the dominating sensory inputs following the principles of multisensory integration, such as the reliability-weighted model (Ernst and Banks, 2002; Alais and Burr, 2004; Collignon et al., 2008; Chandrasekaran, 2017). In\nthis study, visual input dominated when the unity assumption was met. This might have been due to the faces being more accurately recognized, and thus more reliable, than voices in the emotion identification task. Our findings in the angry perception condition support the reliability-weighted model. We found that those who could recognize angry faces more accurately than angry voices also tended to rely more on the faces than voices during audiovisual anger perception. However, this was not observed in the happy emotion, where only two of our subjects were more accurate in recognizing happy voices than faces. The lack of performance range in the happy emotion renders us unable to comment on the relationship between recognition accuracy and sensory reliance for happy perception at present.\nInterestingly, our finding that angry emotion perception relies more on auditory stimuli while happy emotion perception relies more on visual stimuli suggests an emotion specific modality dominance effect. In previous studies examining the temporal and spatial criteria of the unity assumption, it was found that the audiovisual stimulus would be judged as coming from the same location, or at the same time if their temporal and spatial discrepancies were within \u2212100 ms (sound first) to +300 ms (visual first) and at less than 15\u25e6, respectively, as indicated by the ventriloquism effect (Slutsky and Recanzone, 2001; Lewald and Guski, 2003; Chen and Vroomen, 2013). As above, sensory bias in the temporal criterion has been explained as being due to light generally reaching the perceiver earlier than sound, hence elongating time windows for adaptation in the brain (Slutsky and Recanzone, 2001). On the other hand, it was also suggested that our auditory sensory system dominates in the temporal domain and visual system dominates in the space domain (Choi et al., 2018). Therefore, the finding that angry emotion relies more on auditory stimuli may indicate a temporal dominance in the perception of anger.\nA previous study by Collignon et al. (2008) reported audiovisual emotion perception to be generally visual dominant. Visual dominance, however, was not observed for both happy and angry emotions in our study. This may be due to differences in experimental design. For example, while the emotions of disgust and fear were used in Collignon et al.\u2019s (2008) study, here we\nFrontiers in Neuroscience | www.frontiersin.org 9 March 2022 | Volume 16 | Article 782318\nused anger and happiness. Further, the task implemented was different as well. While Collignon et al. (2008) asked subjects to identify the emotional category of audiovisual stimuli, we asked our subjects to rate the emotional intensity instead. Therefore, our instructions may have encouraged subjects to weigh the face and voice more equally during the task. With reference to the causal inference model, participants would have attributed both stimuli to a single, common cause, or C = 1 in this study, which should then produce an optimal estimate of stimulus intensity (Shams et al., 2010). Though an alternative thread to the causal inference model considers when a perceiver might perceive the two stimuli to be of separate causes, or C = 2, in which case intensity estimations are made based on separate percepts. Moreover, Collignon et al. (2008) conducted their study in Canada while the present study was conducted in Singapore, potentially suggesting a cultural effect on sensory reliance during multisensory emotion perception (Tanaka et al., 2010; Liu et al., 2015). Such cultural effects have been noted previously, with Japanese subjects observed to weight auditory emotional information more than the Dutch in audiovisual emotion perception (Tanaka et al., 2010). Similar cultural findings were replicated when contrasting Chinese and English speakers\u2019 neural responses\u2014the N400 signal more strongly interfering when judging emotionally incongruent faces than voices in English speakers compared to Chinese speakers (Liu et al., 2015). These cultural differences were suggested to result from differences in societal norms where East Asians may tend to mask expressions for indirectness in communication, in contrast to Western cultures (for more in-depth discussion, see Liu et al., 2015). Therefore, the type of emotion, judgment tasks and cultural difference may influence interpretations of findings across multisensory integration studies.\nIn this study, we also explored the influence of autistic traits on multisensory emotion perception in a non-clinical population. Surprisingly, we observed that autistic traits were not significantly correlated with unisensory processing ability when a single source was assumed. This appears contrary to findings of the Weak Central Coherence Theory (Happ\u00e9 and Frith, 2006). The Bayesian model of autistic perception postulates that the social/non-social perceptual deficits in autism result from deviation in the construction of top-down influences, i.e., priors, and/or the reductions in the use of such priors (Pellicano and Burr, 2012; Sevgi et al., 2020). Aligning with this model, a recent study suggested that autistic traits are associated with stricter temporal criteria for the unity assumption, one of the priors in multisensory perception (K\u00f6rding et al., 2007; Shams and Beierholm, 2010; Chen and Spence, 2017), suggesting a deviated construction of the prior (Kawakami et al., 2020b). However, this association is not consistently observed in the literature (Kawakami et al., 2020a). Poole et al. (2017), for example, found no differences between adults with autism spectrum conditions and neurotypical individuals in temporal acuity. Similarly, de Boer-Schellekens et al. (2013) found no autistic influence in low-level audiovisual multisensory integration.\nOne potential explanation for this finding may be that the autistic influence occurs at a stage independent from top-down influences, specifically in the semantic aspect of audiovisual\nemotion perception of angry and happy faces (Chen and Spence, 2017). This impairment may be related to the social attention deficit in autism, which has been commonly reported in the literature. By reviewing results from these studies, Chita-Tegmark (2016) concluded that the autistic deficits in social attention is likely the result of the difficulty faced in monitoring large number of social stimuli, e.g., humans, or social interactions. This would lead us to expect autistic individuals to be unable to successfully attend to social information from both visual and auditory channels simultaneously. The observed results may potentially be attributed to the nature of study stimuli, involving recognition of emotional faces and voices. The exact relationship between autistic traits and emotional recognition in faces remains inconclusive. For example, in a clinical sample, Cook et al. (2013) found autism and autistic spectrum conditions to be unrelated with facial emotion perception ability, irrespective of gender, IQ, and age. Further, Ola and Gullon-Scott (2020) also noted impaired facial emotion recognition to be associated with alexithymia, but not with autistic traits. Interestingly, despite the well-reported impairments of autistic traits in multisensory processing, this was not observed in the instance of audiovisual emotion integration. Especially as the literature remains sparse in this regard, further understanding may elucidate the complex link between facial and auditory emotion perception in multisensory integration for autistic individuals and the general population.\nThe current study presents a few limitations. First, here we referred to the unity assumption as a dichotomous process, though recent research suggested a continuous nature of processing (Chen and Spence, 2017). Next, as mentioned above, we did not presently account for the segregated percept (C = 2) branch, and modeling both branches may gain a full spectrum of intensity percepts. We expect these to be addressed in future studies, which may also investigate the exact mechanisms of multisensory integration for autistic individuals (by disrupted holistic perception, attenuated priors, reduced weights, social attention deficits, or other possibilities) in a semantic emotion context."
        },
        {
            "heading": "DATA AVAILABILITY STATEMENT",
            "text": "The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "The studies involving human participants were reviewed and approved by Institutional Review Board (IRB) at Nanyang Technological University, Singapore. The participants provided their written informed consent to participate in this study."
        },
        {
            "heading": "AUTHOR CONTRIBUTIONS",
            "text": "KS and HX designed the experiment. KS created the stimuli and collected and analyzed the data in the first draft of the manuscript.\nFrontiers in Neuroscience | www.frontiersin.org 10 March 2022 | Volume 16 | Article 782318\nAS collected and analyzed the data in the following drafts. KS wrote the first draft of the manuscript with the other authors responsible for manuscript revisions and comments. All authors contributed to the article and approved the submitted version."
        },
        {
            "heading": "FUNDING",
            "text": "This research was supported by the Nanyang Technological University Research Scholarship (KS), SGUnited Traineeship Programme (AS), School of Social Sciences, College of\nHumanities, Arts and Social Sciences, Nanyang Technological University (HX), and the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-RP-2020-019) to HX."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank Dr. Fun Lau for her valuable input toward the conception of the study, and our participants for their contribution to this work."
        }
    ],
    "title": "Unity Assumption in Audiovisual Emotion Perception",
    "year": 2022
}