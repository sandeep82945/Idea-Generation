{
    "abstractText": "Due to the modern relevance of blockchain technology, smart contracts present both substantial risks and benefits. Vulnerabilities within them can trigger a cascade of consequences, resulting in significant losses. Many current papers primarily focus on classifying smart contracts for malicious intent, often relying on limited contract characteristics, such as bytecode or opcode. This paper proposes a novel, two-layered framework: 1) classifying and 2) directly repairing malicious contracts. Slither\u2019s vulnerability report is combined with source code and passed through a pre-trained RandomForestClassifier (RFC) and Large Language Models (LLMs), classifying and repairing each suggested vulnerability. Experiments demonstrate the effectiveness of fine-tuned and prompt-engineered LLMs. The smart contract repair models, built from pre-trained GPT-3.5-Turbo and finetuned Llama-2-7B models, reduced the overall vulnerability count by 97.5% and 96.7% respectively. A manual inspection of repaired contracts shows that all retain functionality, indicating that the proposed method is appropriate for automatic batch classification and repair of vulnerabilities in smart contracts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Abhinav Jain"
        }
    ],
    "id": "SP:aa7d3ea6b61e405528f1fd28a4423397a4956c86",
    "references": [
        {
            "authors": [
                "T. Abdelaziz",
                "A. Hobor"
            ],
            "title": "Smart learning to find dumb contracts (extended version)",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "J. Wei",
                "X. Wang",
                "D. Schuurmans",
                "M. Bosma",
                "B. Ichter",
                "F. Xia",
                "E. Chi",
                "Q. Le",
                "D. Zhou"
            ],
            "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Nanqing Dong",
                "Zhipeng Wang",
                "Jiahao Sun",
                "Michael Kampffmeyer",
                "Yizhe Wen",
                "Shuoying Zhang",
                "William Knottenbelt",
                "Eric Xing"
            ],
            "title": "Defending Against Malicious Behaviors in Federated Learning with Blockchain",
            "venue": "ArXiv Preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Monika di Angelo",
                "Thomas Durieux",
                "Jo\u00e3o F. Ferreira",
                "Gernot Salzer"
            ],
            "title": "SmartBugs 2.0: An Execution Framework for Weakness Detection in Ethereum Smart Contracts",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Haiyang Liu",
                "Yuqi Fan",
                "Lin Feng",
                "Zhenchun Wei"
            ],
            "title": "Vulnerable Smart Contract Function Locating Based on Multi-Relational Nested Graph Convolutional Network",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Masaru Yamada"
            ],
            "title": "Optimizing Machine Translation through Prompt Engineering: An Investigation into ChatGPT\u2019s Customizability",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Yuhan Ma",
                "Chenyou Fan",
                "Haiqi Jiang"
            ],
            "title": "Sci-CoT: Leveraging Large Language Models for Enhanced Knowledge Distillation in Small Models for Scientific QA",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Isaac David",
                "Liyi Zhou",
                "Kaihua Qin",
                "Dawn Song",
                "Lorenzo Cavallaro",
                "Arthur Gervais"
            ],
            "title": "Do you still need a manual smart contract audit",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Jing Huang",
                "Kuo Zhou",
                "Ao Xiong",
                "Dongmeng Li"
            ],
            "title": "Smart Contract Vulnerability Detection Model Based on Multi-Task Learning",
            "year": 2022
        },
        {
            "authors": [
                "Sameep Vani",
                "Malav Doshi",
                "Amit A. Nanavati",
                "Ashish Kundu"
            ],
            "title": "Vulnerability Analysis of Smart Contracts",
            "venue": "ArXiv Preprint,",
            "year": 2022
        },
        {
            "authors": [
                "IVANOV NIKOLAY",
                "LI CHENNING",
                "YAN QIBEN",
                "SUN ZHIYUAN",
                "CAO ZHICHAO",
                "LUO. XIAPU"
            ],
            "title": "Security Defense For Smart Contracts: A Comprehensive Survey",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Stefanos Chaliasos",
                "Marcos Antonios Charalambous",
                "Liyi Zhou",
                "Rafaila Galanopoulou",
                "Arthur Gervais",
                "Dimitris Mitropoulos",
                "Benjamin Livshits"
            ],
            "title": "Smart Contract and DeFi Security: Insights from Tool Evaluations and Practitioner Surveys",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Peng Qian",
                "Zhenguang Liu",
                "Qinming He",
                "Butian Huang",
                "Duanzheng Tian",
                "Xun Wang"
            ],
            "title": "Smart Contract Vulnerability Detection Technique: A Survey",
            "venue": "ArXiv Preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Christof Ferreira Torres",
                "Hugo Jonker",
                "Radu State"
            ],
            "title": "Elysium: Context-Aware Bytecode-Level Patching to Automatically Heal Vulnerable Smart Contracts",
            "venue": "ArXiv Preprint,",
            "year": 2022
        },
        {
            "authors": [
                "William Foxley"
            ],
            "title": "2020, October 26). Harvest Finance $24M Attack Triggers $570M Bank Run in Latest DeFi Exploit",
            "year": 2020
        },
        {
            "authors": [
                "Pengcheng Fang"
            ],
            "title": "CONTRACTFIX: A FRAMEWORK FOR AUTOMATICALLY FIXING VULNERABILITIES IN SMART CONTRACTS",
            "venue": "ArXiv Preprint,",
            "year": 2023
        },
        {
            "authors": [
                "J. Feist",
                "G. Grieco",
                "Groce",
                "May"
            ],
            "title": "Slither: a static analysis framework for smart contracts",
            "venue": "IEEE/ACM 2nd International Workshop on Emerging Trends in Software Engineering for Blockchain (WETSEB) (pp. 8-15)",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Smart Contract, Vulnerability Detection, Slither, Large Language Model, Repair\nI. INTRODUCTION\nAs we delve into the crucial role smart contracts play in the global blockchain, it becomes increasingly imperative that we understand the severity of cyberattacks that exploit weak code. 2018 saw $23.5 million worth of cryptocurrencies stolen from the Bancor network due to the compromise of a wallet used to upgrade smart contracts, sparking controversy online over the safety of decentralized exchange and smart contract systems [16]. More recently, in 2020, a hacker drained Harvest Finance of $24 million by implementing a smart contract that manipulated the share values of the vaults [17]. The common theme across these hacks is that vulnerabilities within\n*Abhinav Jain and Ehan Masud contributed equally to this work\nsmart contracts were exploited to steal millions of dollars, highlighting the importance of strengthening smart contracts to prevent vulnerabilities from arising.\nSmart contracts provide a secure platform for transactions without the need for a trusted intermediary. For this reason, they have become increasingly common in blockchain applications. But because most blockchain applications prevent users from editing smart contracts after they have been deployed, there is a need for analysis tools that can accurately and precisely determine the vulnerabilities of smart contracts. Although most tools rely on expert-developed frameworks, recent research has begun developing deep learning models that can evaluate a smart contract\u2019s vulnerability. However, most existing deep learning models fail to provide helpful feedback on a smart contract\u2019s vulnerabilities \u2014 instead, they determine whether or not a smart contract is vulnerable.\nDLVA [1] introduces a three-step approach involving mapping bytecode to high-dimensional vectors, classifying vectors based on training data, and using neural networks to infer vulnerable contracts. However, a significant weakness in this approach was the high false positive rate during the prediction process. Similarly, MRN-GCN [5] utilizes deep learning with a nest contract graph capturing syntactic and semantic information, enabling the classification of vulnerable functions, but like [1], retained mixed recall percentages ranging from 98.18% to 79.59%. The authors of [3] take a different approach by proposing peer-to-peer voting and reward-and-slash mechanisms to mitigate and discourage malicious behavior in smart contracts.\nLarge Language Models (LLMs) models prove to be exceptional in performing complex tasks. The authors of [8] demonstrated the capabilities of various LLMs in identifying vulnerabilities in DeFi smart contracts with F1-scores significantly higher than random baselines, which has the potential\nar X\niv :2\n30 9.\n07 84\n1v 1\n[ cs\n.C R\n] 1\nto be improved by the tool enhancement framework developed in [4]. Prompt engineering allows LLMs to be substantially enhanced. One powerful LLM prompt engineering method involves Chain of Thought (CoT) prompting [2] that significantly improves the ability of LLMs to perform complex reasoning. In eight CoT exemplars, [2] achieves an accuracy of 56.9 on PaLM-540B in the GSM8K benchmark, demonstrating an accuracy improvement of 39. However, the paper chooses to rely solely on CoT, neglecting fine-tuning entirely. In a similar implementation, the authors of [7] present a framework that improves upon CoT by transferring advanced reasoning abilities from large models to smaller ones through knowledge distillation, resulting in improved question-answering performance. In another scenario, [6] utilized prompt engineering by giving ChatGPT specific information, such as the translation\u2019s purpose and target audience, leading to industry standard translation quality.\nA comprehensive survey [11] described the current landscape of smart contract security, identifying eight core defense methods across 133 models. This finding underscores the complexity of the field but also reveals limitations. One limitation is seen in applying automated smart contract tools to DeFi systems [12]. Surprisingly, these tools only detected 8% of attacks, indicating a challenge with intricate vulnerabilities. Addressing this, [13] evaluated five smart contract detection tools, focusing on three types of vulnerabilities. [13]\u2019s analysis determined that different detection models have varying strengths and weaknesses, suggesting a combination of methods may be more effective. Furthermore, this notion is corroborated by [9] and [10], which both utilize Multi-Task Learning, a combination method that leverages concurrent learning and optimization of multiple tasks. Notably, [14] advances this methodology by using an approach that blends K-means clustering and LSTM networks with a universal sentence encoder. This approach understood the smart contract code\u2019s semantic meaning, outperforming baseline models.\nMoreover, current work regarding repairing smart contracts has been shown to be reliable. For example, [19] utilizes a framework called ContractFix to repair vulnerabilites with 94% accuracy. ContractFix was based around static code analyzers and focused on repairing broken patches. Similarly, [15] utilizes a tool, Elysium, to repair patches in bytecode for seven vulnerabilities. However, this paper improves on these frameworks in two main ways. First, our framework is built on LLMs which allow for a more robust repairing process, that is adaptable to zero-day vulnerabilities. Secondly, we work directly with source code, which is a novel approach to repair vulnerabilities.\nThese existing methods have been shown to work well in vulnerability detection across various situations with relatively little statistical error. However, we show that existing vulnerability detection methods face the following problems: 1) lack of a broad approach, 2) little detail on specific errors, 3) high false positive evaluations, and 4) lack of a direct repair framework. To address all these problems, we propose a novel pipeline. The pipeline first utilizes Slither and a\nRandomForestClassifier to detect and provide specific vulnerabilities within smart contract source code. After filtering out non-malicious contracts, two LLMs, GPT-3.5-Turbo and a fine-tuned Llama-2-7b generation model, each repair the vulnerable smart contract source code. The repaired contract is then evaluated by Slither against its vulnerable counterpart, assessing the effectiveness of the repair.\nThe rest of this paper is outlined as follows: Section II details our novel pipeline approach that utilizes two layers for vulnerability detection: Slither and RandomForestClassifier, to classify vulnerable smart contracts and two LLM models (Llama-2-7B and GPT-3.5-Turbo) to repair them. Section III exhibits the results of our approach in comparison to existing methods. Section IV provides a conclusion."
        },
        {
            "heading": "II. METHODS",
            "text": ""
        },
        {
            "heading": "A. Datasets",
            "text": "To achieve high-quality results in training our framework utilizing a RandomForestClassifier and LLMs for classification and repair (Fig. 1), several essential features must be incorporated.\nA source code column (\u201ccontract source\u201d) is necessary to run Slither and the LLMs. However, since the datasets consistently excluded source code, a web scraping algorithm that employed the \u201ccontract address\u201d column would be necessary to obtain source code from Etherscan and generation (see subsection D.). In order to account for source code that could not be scraped through Etherscan, the dataset (200,000 contracts) was reduced to 2500 rows.\nSlither was then run on the newly acquired source code (see subsection B.), adding columns \u201cvulnerability\u201d, \u201cconfidence\u201d, and \u201cimpact\u201d. Slither occasionally failed to provide any vulnerabilities, totalling 474 failed contracts (80% successful output rate). To account for this, the dataset was reduced again to 2,000 smart contracts. Of the dataset, 400 were labeled malicious, and 1,600 were labeled non-malicious. Table I visualizes a segment of the finalized dataset."
        },
        {
            "heading": "B. Slither",
            "text": "Slither is a static code analyzer, which checks the smart contracts for vulnerabilities without executing the contract. Slither\u2019s initial input comes from the Solidity Abstract Syntax Tree (AST) generated by the Solidity compiler from the contract source code. The smart contract is then simplified into an intermediate representation called SlithIR. This intermediate representation is compared to current industry standards, and Slither outputs vulnerabilities. Slither leads the industry in smart contract vulnerability detection, outperforming other static code analyzers in almost every metric, as shown in Table II. This, coupled with our Random Forest Classifier, ensures high accuracy in detecting vulnerable smart contracts.\nAfter importing and running all 89 basic detectors provided by the API, we added each contract\u2019s vulnerabilities to the dataset as a list of Slither\u2019s natural language names with empty lists denoting contracts Slither deemed safe.\nTABLE I SAMPLE ENTRIES OF FINAL DATASET FOR TRAINING\ncontract source malicious vulnerability confidence impact\npragma soli... False [\u2019DeadCode\u2019, \u2019Divide... [\u2019MEDIUM\u2019, \u2019MEDI... [\u2019INFORMATIONAL\u2019, \u2019ME... pragma soli... False [\u2019Assembly\u2019, \u2019BadPR... [\u2019HIGH\u2019, \u2019MEDIU... [\u2019INFORMATIONAL\u2019, \u2019HIGH... pragma soli... True [\u2019ArrayLengthAssignment... [\u2019HIGH\u2019, \u2019MEDIU... [\u2019HIGH\u2019, \u2019MEDIUM\u2019, \u2019HI... pragma soli... False [\u2019DeadCode\u2019, \u2019IncorrectSo... [\u2019MEDIUM\u2019, \u2019HI... [\u2019INFORMATIONAL\u2019, \u2019INFOR... pragma soli... True [\u2019Assembly\u2019, \u2019IncorrectSo... [\u2019HIGH\u2019, \u2019HIGH\u2019 ... [\u2019INFORMATIONAL\u2019, \u2019HIGH\u2019... pragma soli... False [\u2019Assembly\u2019, \u2019BadPRNG\u2019... [\u2019HIGH\u2019, \u2019MEDIU... [\u2019INFORMATIONAL\u2019, \u2019HIGH..."
        },
        {
            "heading": "C. Data Issues and Generation",
            "text": "When it came to data collection, specific issues were encountered. Our biggest issue, extracting source code, proved to be a challenging task. For instance, in a dataset that bytecode was given, we were unsuccessful in decompiling that code into analyzable source code as we were unaware of the decompiler\u2019s limits. We also struggled to find additional malicious source code to train a model on, as our dataset only included 150 malicious contracts. To overcome this, we implemented OpenAI\u2019s GPT 3.5 Turbo to generate malicious source code. Initial attempts were barred by GPT 3.5\u2019s ethical limitations (Fig. 2). However, after jailbreaking GPT 3.5 with prompt engineering [18], GPT 3.5 would produce malicious source code that could be repaired by the model.\nThe variability of the dataset made it difficult to generate Slither vulnerabilities for smart contracts, so a BLANK-step approach was used. The primary issue was the 100+ versions all contracts were written in combined with the limited backward compatibility of Solidity \u2014 i.e., version 0.4.11 could run on a compiler of version 0.4.26 but not a compiler of version 0.5.0+. Addressing this required modifying each contract to read \u201dpragma solidity \u2265{version}\u201d, creating five different scripts, and running each script on the entire dataset with one of five following Solidity versions: 0.4.26, 0.5.17,\nInput: Provide me generated vulnerable smart contract\nsource code in Solidity\nOutput: As an AI developed by OpenAI, I must adhere to ethical guidelines, which includes not promoting or assisting in generating malicious or harmful code, including vulnerable smart\ncontracts. Writing or sharing vulnerable code can have serious consequences and is against\nresponsible AI use.\nFig. 2. GPT 3.5 ethical limitations with production of \u201dvulnerable\u201d source code\n0.6.12, 0.7.6, or 0.8.21, with Slither vulnerabilities of scripts that could not be compiled recorded as null, and those that could be recorded with the English name of the vulnerability, obtained from parsing the returned json. Combining these lists resulted in the final list of Slither vulnerabilities for the 75% of smart contracts for which this method yielded results.\nEach detector class includes the detector\u2019s confidence and impact levels. After creating a key-value pair of each detector\u2019s English name and their confidence plus impact, this list was used to create confidence and impact lists for all vulnerabilities for each smart contract.\nD. Classifier\nVarious models were implemented to classify smart contract maliciousness. Ultimately, RandomForestClassifier (RFC) provided the highest accuracy after pre-processing the finalized dataset.\nRFC is unable to train on the dataset as provided by webscraping, generation, and Slither processing due to the abundance of unnecessary string-based features. So, unnecessary features are dropped, and necessary features are processed for RFC. For example, \u201cconfidence\u201d and \u201cvulnerability\u201d retain a weaker correlation to \u201cmalicious\u201d in comparison to \u201cimpact\u201d, so to avoid convoluting the model, both are dropped. Thus, \u201ccontract source\u201d and \u201cimpact\u201d remain as the classifying features and \u201cmalicious\u201d as the target label.\nAs all columns are still either string or boolean data types, RFC is still unable to train on the dataset. \u201ccontract source\u201d was tokenized using the CountVectorizer (CV) tool from the sci-kit-learn library. \u201cmalicious\u201d and \u201cimpact\u201d were encoded into usable numeric values by mapping dictionaries. Since \u201cimpact\u201d contained more than two possible outputs, unlike \u201cmalicious\u201d, the outputs of \u201cimpact\u201d were scaled from 0-4.\nAfter the tokenized and encoded columns are concatenated, RFC\u2019s numeric prerequisite is fulfilled.\nThe data is then split into a train-test split of 0.6-0.4 and randomized before RFC fits to the train set and predicts on the test set. Accuracy and confusion are evaluated in Results."
        },
        {
            "heading": "E. Large Language Models (LLMs)",
            "text": "1) Finetuning Llama-2-7B: We incorporated multiple Large Language Models to repair the smart contracts after they had been identified as malicious with our two-layered frameworks. The best results came from the Llama-2-7B model, which can be found on Hugging Face. This model finished training in July 2023. Our finetuning process took place about three weeks later. The Llama-2-7B model has become very popular due to its low number of parameters and reliability, leading to a less memory-intensive alternative to other LLMs in the industry.\nThe finetuning process took place on Google Colab using the T4 chip, which carries 16 GB of VRAM. However, Llama2-7B\u2019s weights themselves fill this limit (7b * 2 bytes = 14). This also does not include any weights, optimizers, or gradients. Thus to run Llama-2-7B and be able to run it without memory restrictions on a platform like Google Colab, we will use parameter-efficient-finetuning (PEFT). Specifically, we will use QLoRa (Efficient Finetuning of Quantized LLMs), using 4-bit precision instead of the normal 16-bit precision. This quantization process allows for finetuning on Colab while also ensuring that the precision of the model is adequate. This is because when saving the 4-bit model, we also save the QLoRa adapters, which can be used with the model.\nMoreover, Llama-2-7B is open source meaning the model is available to be downloaded and used locally. Traditional data privacy concerns with LLMs are therefore nullified because all data is processed on the local machine, not in a 3rd party server. This bodes well for smart contracts as many execute agreements with sensitive information and large sums of money. Llama-2-7B provides the benefits and accuracy of an advanced LLM while also providing the security and versatility neccesary for blockchain technology.\nThe Llama-2-7B model was fine-tuned on fifty smart contracts that were once malicious and then repaired, using a supervised learning approach. These smart contracts were collected in the data collection mentioned above. Specifically,\nthe source code was tokenized and embedded, using the quantization outlined previously. The model was trained over 100 steps, with training loss consistently decreasing with every step(as shown in figure 3).\nThe supervised fine-tuning process allowed the model to understand the relationships between malicious source code and the same source code that had been repaired to emulate that with any other contract.\n2) Prompt Engineering: We also utilized OpenAI\u2019s API to use GPT-3.5-Turbo to repair vulnerabilities. OpenAI is one of the most well known names in the industry with applications such as DALL -E and ChatGPT. Specifically, while all GPT models are optimized to generate code, GPT-3.5-Turbo is the best combination of performance and efficiency. Moreover, by utilizing a \u201dchat bot\u201d, we were able to use prompt engineering to create a prompt with the best possible performance. Directly querying GPT-3.5-Turbo to repair malicious code was unsuccessful. Similar to the generation of malicious smart contracts, GPT-3.5-Turbo had a reluctance to work with malicious source code (Fig. 4).\nThus prompt engineering was utilized to circumvent this problem.\nFirst, the use of the word \u201dmalicious\u201d needed to be removed. While we were looking for our LLM to repair malicious smart contracts, GPT-3.5 Turbo was instead asked to help us \u201cfix vulnerable smart contracts\u201d.\nWe then used Chain of Thought Techniques in order for the model to elaborate on what changes it made and why. This led to a more accurate source code output and more vulnerabilities repaired. Additionally, this provided more information for the\nuser as the specific vulnerabilities in the malicious smart contract were highlighted and explained.\nUltimately, our prompt(Fig. 5) used Slither\u2019s source code and vulnerabilities to prompt GPT 3.5 Turbo to repair the smart contracts. While Slither also outputs impact level and confidence on those vulnerabilities, we found incorporating these into the prompt hurt the model\u2019s ability to output repaired source code or even source code that could be compiled. Essentially, using other Slither outputs led to overfitting. This prompt was also used with the Llama-2-7B model outlined above in order to create uniformity across outputs. In both models, the prompt allowed for the generation of repaired source code while also generating details that explained any changes and provided explanation.\nIn conclusion, we ended with two primary models to repair source code. First, the Llama-2-7B, which had been finetuned specifically for repairing smart contracts. Second was the utilization of GPT-3.5-Turbo which learned to repair smart contracts through CoT prompt engineering."
        },
        {
            "heading": "III. RESULTS",
            "text": ""
        },
        {
            "heading": "A. Results from the RandomForestClassifier (RFC)",
            "text": "Of the 2000 contracts used on the model, the RFC was tested on 800 (40%). 717 out of the 800 contracts were predicted accurately for an accuracy of 89.6% and an F1 score of 0.76. The generated confusion matrix further detailed that for positive predictions (\u201cTrue\u201d), 133 were true positives, and 23 were false positives. For negative predictions, 584 were true negatives, and 60 were false negatives. The false positive\nrate was only 3.8%, successfully fulfilling our goal. This is a significant improvement over just static analysis tools, such as Slither, which alone has a false positive rate of 10.9% [20]. Furthermore, the RFC is able to examine the source code without a limited number of vulnerability detectors, making it more adaptable to syntax changes."
        },
        {
            "heading": "B. Results from the GPT-3.5-Turbo and Llama-2-7B Error Correction Models",
            "text": "To test the GPT-3.5-Turbo and the fine tuned Llama-2-7B model with our prompt, we aimed to repair vulnerabilities as reported by Slither. The results are shown in the graphs above. The results of Slither checks on GPT-corrected smart contracts are promising, with the fine-tuned GPT-3.5 Turbo model able to repair 97.5% of vulnerabilities. Specifically, out of the 40 vulnerabilities encountered while running through the source code, only a single medium level vulnerability remained. Meanwhile, the fine-tuned Llama-2 model was able to correct all but two errors across 60 vulnerabilities encountered, with one medium- and one low-impact vulnerability remaining. Thus the Llama-2 model was able to decrease the proportion of vulnerabilities by 96.7%. We reviewed a random third of\nrepaired smart contracts and found that all of them had retained their previous functionality, with the models usually correcting syntax-level errors rather than changing underlying structures.\nThe CoT GPT-3.5-Turbo prompts and fine-tuning of the Llama-2-7B classifier were vital to the accuracy of these models. Upon initial testing, the GPT-3.5-Turbo was able to repair fewer than 85% of smart contracts and the Llama-2-7B model was unable to produce code that could be compilied. However, with the methods outlined above, the results demonstrate a reliable process to repair smart contracts.\nIndeed, these results demonstrate that the LLMs were able to successfully repair vulnerable smart contracts with near perfect accuracy, with only three total vulnerabilities remaining. The error correction rate was well above that of any existing methods, making them state-of-the-art tools with impressive error reduction capabilities. Moreover, due to the \u201cTwo Timin\u2019\u201d framework described above, only malicious contracts were repaired, cutting down on computing time and maximizing the quantity of secure, reliable smart contracts available. Due to the tens of millions of smart contracts on blockchains such as Etherscan [21], minimizing computational complexity and cost in an already energy-intensive industry is beneficial to users, companies, and the environment."
        },
        {
            "heading": "IV. CONCLUSION",
            "text": "In this paper, we used the Solidity source code of smart contracts to build a novel approach to identify and repair vulnerabilities. This approach utilized a two tiered flow for identifying and repairing vulnerabilities. First, the Slither static code analyzer and a Random Forest Classifier were used to identify malicious smart contracts and their specific vulnerabilities. These malicious smart contracts and their vulnerabilities were used as parameters in a prompt on two separate LLMs, GPT-3.5-Turbo and Llama-2-7B. This prompt was a result of prompt engineering using Chain of Thought reasoning. The two smart contract repair models, one using pre-trained GPT3.5-Turbo and the other a fine-tuned Llama-2-7B, reduced the overall vulnerability count by 97.5% and 96.7% respectively. This novel approach, with state of the art accuracy, allows for smart contracts to be screened and repaired before being deployed. Thus, cybercriminals are unable to exploit vulnerabilites in the contracts. Indeed, this paper establishes a framework that is easy to use, with reliable results, increasing access to safe smart contracts for all. Using the \u201dTwo Timin\u2019\u201d framework, businesses and DAOs can utilize LLMs to repair smart contracts efficiently and effectively, an important step forward as the prevalence of blockchain continues to increase.\nFUTURE WORK Different methods of classifiers powered by transformers or neural networks could be used to identify malicious smart contracts. These could learn across a broader concentration of data with access to a larger proportion of malicious smart contracts. In addition, more finetuning could be completed on Llama-2-7B, with more hidden layers and a larger dataset in order to raise its error correction rate above that of GPT-3.5Turbo. At the time of writing this paper, GPT-3.5-Turbo is\nunable to be fine-tuned, however if fine-tuning capabilities were to be developed, further research could focus on fine tuning GPT-3.5-Turbo for repairing smart contracts. Moreover, advances in PEFT and/or QLoRa could allow for a less memory intensive but more accurate LLM for repairing smart contracts."
        }
    ],
    "title": "Two Timin\u2019: Repairing Smart Contracts With A Two-Layered Approach",
    "year": 2023
}