{
    "abstractText": "The short text matching task employs a model to determine whether two short texts have the same semantic meaning or intent. Existing short text matching models usually rely on the content of short texts which are lack information or missing some key clues. Therefore, the short texts need external knowledge to complete their semantic meaning. To address this issue, we propose a new short text matching framework for introducing external knowledge to enhance the short text contextual representation. In detail, we apply a self-attention mechanism to enrich short text representation with external contexts. Experiments on two Chinese datasets and one English dataset demonstrate that our framework outperforms the state-of-the-art short text matching models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Mao Yan Chen"
        },
        {
            "affiliations": [],
            "name": "Haiyun Jiang"
        },
        {
            "affiliations": [],
            "name": "Yujiu Yang"
        }
    ],
    "id": "SP:fb2f2b43200082f1a0bace9a5fd93be63c81bfb5",
    "references": [
        {
            "authors": [
                "Adam Berger",
                "Rich Caruana",
                "David Cohn",
                "Dayne Freitag",
                "Vibhu Mittal."
            ],
            "title": "Bridging the lexical chasm: statistical approaches to answer-finding",
            "venue": "Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in",
            "year": 2000
        },
        {
            "authors": [
                "Jing Chen",
                "Qingcai Chen",
                "Xin Liu",
                "Haijun Yang",
                "Daohe Lu",
                "Buzhou Tang"
            ],
            "title": "The bq corpus: A large-scale domain-specific chinese corpus for sentence semantic equivalence identification",
            "year": 2018
        },
        {
            "authors": [
                "Lu Chen",
                "Yanbin Zhao",
                "Boer Lyu",
                "Lesheng Jin",
                "Zhi Chen",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "Neural graph matching networks for chinese short text matching",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Revisiting pretrained models for Chinese natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing:",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Cui",
                "Wanxiang Che",
                "Ting Liu",
                "Bing Qin",
                "Ziqing Yang",
                "Shijin Wang",
                "Guoping Hu."
            ],
            "title": "Pre-training with whole word masking for chinese bert",
            "venue": "arXiv preprint arXiv:1906.08101.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "Po-Sen Huang",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng",
                "Alex Acero",
                "Larry Heck."
            ],
            "title": "Learning deep structured semantic models for web search using clickthrough data",
            "venue": "Proceedings of the 22nd ACM international conference on Information & Knowl-",
            "year": 2013
        },
        {
            "authors": [
                "Yuxuan Lai",
                "Yansong Feng",
                "Xiaohan Yu",
                "Zheng Wang",
                "Kun Xu",
                "Dongyan Zhao."
            ],
            "title": "Lattice cnns for matching based chinese question answering",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 6634\u20136641.",
            "year": 2019
        },
        {
            "authors": [
                "Xin Liu",
                "Qingcai Chen",
                "Chong Deng",
                "Huajun Zeng",
                "Jing Chen",
                "Dongfang Li",
                "Buzhou Tang."
            ],
            "title": "Lcqmc: A large-scale chinese question matching corpus",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages",
            "year": 2018
        },
        {
            "authors": [
                "Boer Lyu",
                "Lu Chen",
                "Su Zhu",
                "Kai Yu."
            ],
            "title": "Let: Linguistic knowledge enhanced graph transformer for chinese short text matching",
            "venue": "arXiv preprint arXiv:2102.12671.",
            "year": 2021
        },
        {
            "authors": [
                "Yuxian Meng",
                "Wei Wu",
                "Fei Wang",
                "Xiaoya Li",
                "Ping Nie",
                "Fan Yin",
                "Muyu Li",
                "Qinghong Han",
                "Xiaofei Sun",
                "Jiwei Li."
            ],
            "title": "Glyce: Glyph-vectors for chinese character representations",
            "venue": "arXiv preprint arXiv:1901.10125.",
            "year": 2019
        },
        {
            "authors": [
                "Yelong Shen",
                "Xiaodong He",
                "Jianfeng Gao",
                "Li Deng",
                "Gr\u00e9goire Mesnil."
            ],
            "title": "Learning semantic representations using convolutional neural networks for web search",
            "venue": "Proceedings of the 23rd international conference on world wide web, pages 373\u2013374.",
            "year": 2014
        },
        {
            "authors": [
                "Richard Socher",
                "Eric H Huang",
                "Jeffrey Pennin",
                "Christopher D Manning",
                "Andrew Y Ng."
            ],
            "title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
            "venue": "Advances in neural information processing systems, pages 801\u2013809.",
            "year": 2011
        },
        {
            "authors": [
                "Yan Song",
                "Tong Zhang",
                "Yonggang Wang",
                "Kai-Fu Lee"
            ],
            "title": "Zen 2.0: Continue training and adaption for n-gram enhanced text encoders",
            "venue": "arXiv preprint arXiv:2105.01279",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, pages 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "Shuohang Wang",
                "Jing Jiang."
            ],
            "title": "Learning natural language inference with lstm",
            "venue": "arXiv preprint arXiv:1512.08849.",
            "year": 2015
        },
        {
            "authors": [
                "Zhiguo Wang",
                "Wael Hamza",
                "Radu Florian."
            ],
            "title": "Bilateral multi-perspective matching for natural language sentences",
            "venue": "arXiv preprint arXiv:1702.03814.",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Short text matching is an essential task that has been applied in question answering (Berger et al., 2000), paraphrase identification (Socher et al., 2011) and information retrieval (Huang et al., 2013). In recent years, deep neural networks achieve surprising performance in this field. We can roughly classify deep text matching models into two types: 1) representation-based text matching (Huang et al., 2013; Shen et al., 2014) and 2) interaction-based text matching (Wang and Jiang, 2015; Wang et al., 2017; Devlin et al., 2018). The interactive-based framework is usually performs better than the representation-based framework. Though interactive-based framework achieved very promising results, their performance still suffers from the lack of enough contextual information since words or expressions in a short text usually have ambiguous meanings.\nEspecially in Chinese scenarios, both characterlevel and word-level tokenization introduce serious\n\u2217Corresponding authors(yang.yujiu@sz.tsinghua.edu.cn, haiyunjiang@tencent.com).\nsemantic information errors or missing. Recent studies show that encoding multi-granularity information (Lai et al., 2019; Chen et al., 2020) and word sense information (Lyu et al., 2021) into sentences can mitigate this problem. They further improve the performance, while this word-level information reinforces is still helpless in many cases that need relevant sentence-level contextual information supplement.\nAs seen in Figure 1, sentences 1 and 2 refer to the same question but the word-level semantic information is not enough to connect them. Therefore, we take the original sentences as queries to search related contexts by search engines. The retrieved contexts usually contain enough contextual information to relate the two original short texts. In this case, both short texts refer to \"interest of bank loan\", where the matching model could easily classify them to be matched.\nFrom this insight, we propose a context-aware BERT matching model (CBM) for short text matching, which enrich the semantic representation of a short text by external semantic-related sentences,\nar X\niv :2\n20 3.\n01 84\n9v 1\n[ cs\n.C L\n] 3\nM ar\n2 02\n2\ninstead of word-levle knowledge. As seen in Figure 1, both sentences have multiple related contextual sentences1. CBM selects the needed contexts and updates the short text representation according to the context-enhanced attention mechanism. Our experiments on two Chinese datasets and one English dataset show that our model achieves new state-of-the-art performance. Our contributions are three folds:\n\u2022 We are the first to propose a framework that enhances short text representation by external sentence-level knowledge.\n\u2022 We crawled a huge amount of contextual sentences for the three commonly used benchmark datasets, which benefits future research.\n\u2022 We design a simple but efficient model to utilize the sentences for short text representation reinforcement. Experiments show that our model achieves new SoTA performance."
        },
        {
            "heading": "2 Framework",
            "text": "Given two sentences, Sa = {s1a, s2a, ..., sia, ..., sna} and Sb = {s1b , s2b , ..., s j b, ..., s m b }, we aim to decide whether two sentences have the same semantic meaning. sia and s j b denotes the i-th and j-th token in sentence a and b, respectively. Different from existing methods, we not only use the sentences in datasets, but also utilize the external sentences crawled from search engines to enhance the context. Each sentence Si has a set of contexts: Ci = {c1i , c2i , ..., c j i , ..., c n i }, where c j i represents the j-th context for sentence Si. Our framework has three modules: 1) Contexts Crawler, 2) Context Selector, and 3) Contextenhanced Text Matcher."
        },
        {
            "heading": "2.1 Context Crawler",
            "text": "For each sentence Si, we obtain the set of contexts Ci\u2032 corresponding to Si by crawling the search engine results. The retrieved contexts Ci\u2032 are noisy and dirty, so we first remove the noise by preprocessing and perform a regular cleaning. Also, all contents related to personal information is removed. Finally, we will have a clean context set Ci for each sentence Si."
        },
        {
            "heading": "2.2 Context Selector",
            "text": "First, we use BERT baseline model to perform semantic similarity task for each pair of sentence and\n1We denote a contextual sentence as a context for short.\ncontext, Sa with c j b or Sb with c j a. Aftrer that, each pair of a sentence and a context has a similarity score of dji , higher means higher semantic similarity, lower means lower semantic similarity. For instance, dja is the similarity score for the pair of Sa and c j b. For all positive samples (Sa and Sb are semantically matched), we use the hyperparameter da to classify the context and sentence pairs into similar or dissimilar. All dji > da is similar and others are dissimilar. Otherwise, for all negative samples (Sja and S j b are not semantically matched), we use the hyperparameter db to classify the context and sentence pairs into similar and dissimilar, with all dji > db being similar and the rest being dissimilar.\nFor all positive samples, S+a and S + b , we want the context of S+a to have similar semantic information as S+b . Also, we want the context of S + b to have similar semantic information as S+a . On the contrary, for all negative samples, S\u2212a and S \u2212 b , we expect the contexts of S\u2212a to be semantically dissimilar to S\u2212b . It is also expected that the contexts of S\u2212b are not semantically similar to S \u2212 a .\nHowever, we do not have ground truth labels for test set to make the context selection by labels. Therefore, we construct a context selector to determine whether we want to use the context based on the semantic information of the two sentences and the context.\nWhen we construct contexts for the above positive and negative samples, using similar semantic contexts for positive samples and dissimilar semantic contexts for negative samples. We first put this data as pseudo labels into a BERT classifier for training. The inputs are Sa, Sb, c j i , where i \u2208 [a, b], and the model output will be [0, 1], indicating whether this context will be used or not. Finally, the context selector integrates all relevant contexts into one context set, C\u0302."
        },
        {
            "heading": "2.3 Context-enhanced Text Matcher",
            "text": "First, we encode Sa and Sb by sentence BERT to obtain their embedding, ha and hb. Then, we use context BERT model to encode c\u0302a, c\u0302b to obtain the embeddings of the contexts, hca and h c b, respectively. Afterward, we concatenate ha, hb, hca and h c b together and input them into a 3-layer Transformer model. Finally, we obtain the representation ha, hb, which are the final context-enhanced text representation of Sa and Sb."
        },
        {
            "heading": "2.4 Matching Classifier",
            "text": "Our model predict predict the text similarity of two context-enhanced text representations.\nhfinal = [ha;hb; |ha \u2212 hb|] (1)\npi = FFN(hfinal) (2)\nwhere FFN(\u00b7) is a feed forward network with two hidden layers and a output layer, a relu activation after each hidden layer.\nFor each training sample {Sa, Sb, y}, we aim to minimize the BCE loss:\nL = \u2212 N\u2211 i=1 (ylog(pi) + (1\u2212 y)log(1\u2212 p)) (3)\nwhere y \u2208 {0, 1} is the label of the i-th training sample and p \u2208 {0, 1} is the prediction of our model taking the sentence pair as input."
        },
        {
            "heading": "2.5 Result Selector",
            "text": "Since not every pair of short texts need context enhancement, for those pairs have high confidence with BERT baseline, we will keep the results and logits. We set the output logits of BERT baseline and our model to be y\u0302i and y\u0304i, respectively. Then, the final result will be as follow:\nyi = y\u0302i + y\u0304i \u2212 1 (4)\nwhere yi \u2208 {0, 1} is the final predicted label of i-th sample, and yi equal to 1 if yi is larger than or equal to 0.5. Otherwise, yi will be set to 0."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Dataset",
            "text": "We conduct our experiments on Bank Question (BQ) (Chen et al., 2018), large-scale Chinese question matching corpus (LCQMC) (Liu et al., 2018) and the Quora Question Paraphrasing corpus (QQP) datasets for semantic textual similarity task. BQ is a large-scale domain-specific Chinese corpus for sentence semantic matching. It is collected from customer service logs by a Chinese bank. LCQMC is a large-scale chinese question matching corpus. It focuses on intent matching rather than paraphrase."
        },
        {
            "heading": "3.2 Experiments",
            "text": "\u2022 BERT-Baseline: A chinese pretrained BERT,\ncalled Chinese-BERT-wwm, provided by (Cui et al., 2019).\n\u2022 ERNIE 2.0: A continual pre-training framework named ERNIE 2.0 which incrementally builds pre-training tasks and then learn pretrained models on these constructed tasks via\ncontinual multi-task learning. (Sun et al., 2021)\n\u2022 LET-BERT(Lyu et al., 2021): A Linguistic knowledge Enhanced graph Transformer (LET) to deal with word ambiguity using HowNet.\n\u2022 ZEN 2.0 Base(Song et al., 2021): An updated n-gram enhanced pre-trained encoder on Chinese and Arabic.\n\u2022 GMN-BERT: A neural graph matching method (GMN) for Chinese short text matching.\n\u2022 Glyce+BERT(Meng et al., 2019): Glyce provide glyph-vectors for logographic language representations.\n\u2022 RoBERTa-wwm-ext-large: A chinese pretrained RoBERTa model which is also provided by (Cui et al., 2019).\nIn Table 1, both Bert Baseline and our model are the results of our tuning of the hyperparameters to the best. All other experimental results are using the best results on the corresponding paper. In comparison, our model results outperform all baselines on the BQ dataset and outperform the previous best model by nearly 2% in F1 values. On the LCQMC dataset, we also achieve the state of the art results."
        },
        {
            "heading": "3.3 Details",
            "text": "The BERT models used in our experiments are the BERT-wwm-ext version provided by HIT University. Learning rate, epoch size, sequence size and batch size and context size are 3e-5, 3, 512, 64 and 3, respectively. All experiments are run on V100 graphics cards."
        },
        {
            "heading": "3.4 Ablation Studies",
            "text": "Ours+RoBERTa: Replacing BERT model in our framework with RoBERTa model.\nOurs-share: Removing share parameter mechanism between context encoder and sentence encoder. Ours-cs+random: Removing context selector module and randomly choosing k contexts. Ours-cs+topk: Removing context selector module and choosing top k relevant contexts. Ours-rs: Removing result selector module. As seen in Table 2, removing the context selector will hurt the performance significantly because the unfiltered contexts contain serious noise. If we randomly select k contexts, then the model could not take advantage of the context information since the contexts may be irrelevant. However, when we select K most relevant contexts, due to the properities of the search engine, contexts will be exactly the same or similar to short texts. Therefore, the contexts will turn out to be useless for the model. As a result, the ablation studies proves that the context selector module can effectively filter the noisy contexts and provide high-quality contexts for each short text. Our model shares the parameters in context encoder and short text encoder, then they could encode the contexts and short texts into the same semantic space. Finally, the ablation studies demostrate that sharing parameters boosts the performance and efficiency of the model."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this work, we proposed a novel external knowledge enhanced BERT for Chinese short text matching. Our model takes two sentences and some related contexts as input and integrates the external information to moderate word ambiguity. The proposed method is evaluated on two Chinese benchmark datasets and obtains the best performance. Theablation studies also demonstrate that both semantic information and multi-granularity information are important for text matching modeling."
        }
    ],
    "title": "Context Enhanced Short Text Matching using Clickthrough Data",
    "year": 2022
}