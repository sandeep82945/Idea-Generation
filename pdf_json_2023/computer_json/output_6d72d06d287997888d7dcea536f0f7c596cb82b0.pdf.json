{
    "abstractText": "With the rapid growth of social media networks and internet accessibility, most businesses are becoming vulnerable to a wide range of threats and attacks. Thus, intrusion detection systems (IDSs) are considered one of the most essential components for securing organizational networks. They are the first line of defense against online threats and are responsible for quickly identifying potential network intrusions. Mainly, IDSs analyze the network traffic to detect any malicious ac\u2010 tivities in the network. Today, networks are expanding tremendously as the demand for network services is expanding. This expansion leads to diverse data types and complexities in the network, which may limit the applicability of the developed algorithms. Moreover, viruses and malicious attacks are changing in their quantity and quality. Therefore, recently, several security researchers have developed IDSs using several innovative techniques, including artificial intelligence methods. This work aims to propose a support vector machine (SVM)\u2010based deep learning system that will classify the data extracted from servers to determine the intrusion incidents on social media. To implement deep learning\u2010based IDSs for multiclass classification, the CSE\u2010CIC\u2010IDS 2018 dataset has been used for system evaluation. The CSE\u2010CIC\u2010IDS 2018 dataset was subjected to several preprocess\u2010 ing techniques to prepare it for the training phase. The proposed model has been implemented in 100,000 instances of a sample dataset. This study demonstrated that the accuracy, true\u2010positive re\u2010 call, precision, specificity, false\u2010positive recall, and F\u2010score of the proposed model were 100%, 100%, 100%, 100%, 0%, and 100%, respectively.",
    "authors": [
        {
            "affiliations": [],
            "name": "Khadija M. Abuali"
        }
    ],
    "id": "SP:96d1bddfb10f43eb9fb9c4be7c5a04fe50d75c4b",
    "references": [
        {
            "authors": [
                "K.M. Abuali",
                "L. Nissirat",
                "A. Al\u2010Samawi"
            ],
            "title": "Intrusion Detection Techniques in Social Media Cloud: Review and Future Directions",
            "venue": "Wirel. Commun. Mob. Comput",
            "year": 2023
        },
        {
            "authors": [
                "S.A.P. Kumar",
                "A. Kumar",
                "S. Srinivasan"
            ],
            "title": "Statistical based intrusion detection framework using six sigma technique",
            "venue": "IJCSNS Int. J. Comput. Sci. Netw. Secur. 2007,",
            "year": 2007
        },
        {
            "authors": [
                "H. Om",
                "T. Hazra"
            ],
            "title": "Statistical techniques in anomaly intrusion detection system",
            "venue": "Int. J. Adv. Eng. Technol. 2012,",
            "year": 2012
        },
        {
            "authors": [
                "C. Azad",
                "V.K. Jha"
            ],
            "title": "Data mining\u2010based hybrid intrusion detection system",
            "venue": "Indian J. Sci. Technol",
            "year": 2014
        },
        {
            "authors": [
                "J. Jha",
                "L. Ragha"
            ],
            "title": "Intrusion detection system using support vector machine",
            "venue": "Int. J. Appl. Inf. Syst. IJAIS 2013,",
            "year": 2013
        },
        {
            "authors": [
                "J. Li",
                "Z. Zhao",
                "R. Li",
                "H. Zhang"
            ],
            "title": "Ai\u2010based two\u2010stage intrusion detection for software defined iot networks",
            "venue": "IEEE Internet Things J. 2018,",
            "year": 2102
        },
        {
            "authors": [
                "R. Patgiri",
                "U. Varshney",
                "T. Akutota",
                "R. Kunde"
            ],
            "title": "An investigation on intrusion detection system using machine learning",
            "venue": "In Proceedings of the 2018 IEEE Symposium Series on Computational Intelligence (SSCI), Bangalore, India,",
            "year": 2018
        },
        {
            "authors": [
                "U. Ravale",
                "N. Marathe",
                "P. Padiya"
            ],
            "title": "Feature selection based hybrid anomaly intrusion detection system using K means and RBF kernel function",
            "venue": "Procedia Comput. Sci",
            "year": 2015
        },
        {
            "authors": [
                "V.P.K. Sistla",
                "V.K.K. Kolli",
                "L.K. Voggu",
                "R. Bhavanam",
                "S. Vallabhasoyula"
            ],
            "title": "Predictive Model for Network Intrusion Detection System",
            "venue": "Using Deep Learning. Rev. D\u2019intelligence Artif",
            "year": 2020
        },
        {
            "authors": [
                "J. Kim",
                "N. Shin",
                "S.Y. Jo",
                "S.H. Kim"
            ],
            "title": "Method of intrusion detection using deep neural network",
            "venue": "In Proceedings of the 2017 IEEE International Conference on Big Data and Smart Computing (BigComp), Jeju, Republic of Korea,",
            "year": 2017
        },
        {
            "authors": [
                "S.N. Nguyen",
                "V.Q. Nguyen",
                "J. Choi",
                "K. Kim"
            ],
            "title": "Design and implementation of intrusion detection system using convolutional neural network for DoS detection",
            "venue": "In Proceedings of the 2nd International Conference onMachine Learning and Soft Computing,",
            "year": 2018
        },
        {
            "authors": [
                "H. Wang",
                "Z. Cao",
                "B. Hong"
            ],
            "title": "A network intrusion detection system based on convolutional neural network",
            "venue": "J. Intell. Fuzzy Syst",
            "year": 2020
        },
        {
            "authors": [
                "P. Toupas",
                "D. Chamou",
                "K.M. Giannoutakis",
                "A. Drosou",
                "D. Tzovaras"
            ],
            "title": "An intrusion detection system for multi\u2010class classifica\u2010 tion based on deep neural networks",
            "venue": "In Proceedings of the 2019 18th IEEE International Conference on Machine Learning and Applications (ICMLA), Boca Raton, FL, USA,",
            "year": 2019
        },
        {
            "authors": [
                "P. Liu"
            ],
            "title": "An intrusion detection system based on convolutional neural network",
            "venue": "In Proceedings of the 2019 11th International Conference on Computer and Automation Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "L. Chen",
                "X. Kuang",
                "A. Xu",
                "S. Suo",
                "Y. Yang"
            ],
            "title": "A novel network intrusion detection system based on CNN",
            "venue": "In Proceedings of the 2020 Eighth International Conference on Advanced Cloud and Big Data (CBD), Taiyuan, China,",
            "year": 2020
        },
        {
            "authors": [
                "L. Mohammadpour",
                "T.C. Ling",
                "C.S. Liew",
                "C.Y. Chong"
            ],
            "title": "A convolutional neural network for network intrusion detection system",
            "venue": "Proc. Asia\u2010Pac. Adv. Netw. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "J. Kim",
                "Y. Shin",
                "E. Choi"
            ],
            "title": "An intrusion detection model based on a convolutional neural network",
            "venue": "J. Multimed. Inf. Syst. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Z.K. Maseer",
                "R. Yusof",
                "N. Bahaman",
                "S.A. Mostafa",
                "C.F.M. Foozy"
            ],
            "title": "Benchmarking of machine learning for anomaly based intrusion detection systems in the CICIDS2017 dataset",
            "venue": "IEEE Access 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S. Ho",
                "S. Al Jufout",
                "K. Dajani",
                "M. Mozumdar"
            ],
            "title": "A novel intrusion detection model for detecting known and innovative cyberat\u2010 tacks using convolutional neural network",
            "venue": "IEEE Open J. Comput. Soc. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "D. Kilichev",
                "W. Kim"
            ],
            "title": "Hyperparameter Optimization for 1D\u2010CNN\u2010Based Network Intrusion Detection Using GA and PSO.Math\u2010 ematics 2023",
            "year": 2023
        },
        {
            "authors": [
                "U.K. Lilhore",
                "P. Manoharan",
                "S. Simaiya",
                "R. Alroobaea",
                "M. Alsafyani",
                "A.M. Baqasah",
                "S. Dalal",
                "A. Sharma",
                "K. Raahemifar"
            ],
            "title": "HIDM: Hybrid Intrusion Detection Model for Industry 4.0 Networks Using an Optimized CNN\u2010LSTM with Transfer Learning",
            "venue": "Sensors 2023,",
            "year": 2023
        },
        {
            "authors": [
                "D. Ore\u0161ki",
                "D. Andro\u010dec"
            ],
            "title": "Genetic algorithm and artificial neural network for network forensic analytics",
            "venue": "In Proceedings of the 2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO), Opatija, Croatia,",
            "year": 2020
        },
        {
            "authors": [
                "R.I. Farhan",
                "A.T. Maolood",
                "N.F. Hassan"
            ],
            "title": "Optimized deep learningwith binary PSO for intrusion detection on CSE\u2010CIC\u2010IDS2018 dataset",
            "venue": "J. Al\u2010Qadisiyah Comput. Sci. Math. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A. Fatima",
                "N. Nazir",
                "M.G. Khan"
            ],
            "title": "Data cleaning in data warehouse: A survey of data pre\u2010processing techniques and tools",
            "venue": "Int. J. Inf. Technol. Comput. Sci. 2017,",
            "year": 2017
        },
        {
            "authors": [
                "C.O.S. Sorzano",
                "J. Vargas",
                "A.P. Montano"
            ],
            "title": "A survey of dimensionality reduction techniques",
            "venue": "arXiv 2014,",
            "year": 2014
        },
        {
            "authors": [
                "V.R. Joseph"
            ],
            "title": "Optimal ratio for data splitting",
            "venue": "Stat. Anal. Data Min. ASA Data Sci. J. 2022,",
            "year": 2022
        },
        {
            "authors": [
                "R. Ayachi",
                "M. Afif",
                "Y. Said",
                "M. Atri"
            ],
            "title": "Strided convolution instead of max pooling for memory efficiency of convolutional neural networks",
            "venue": "In Proceedings of the 8th International Conference on Sciences of Electronics, Technologies of Information and Telecommunica\u2010 tions (SETIT\u201918); Springer International Publishing: Berlin/Heidelberg, Germany,",
            "year": 2020
        },
        {
            "authors": [
                "Z. Wang",
                "M. Li",
                "H. Wang",
                "H. Jiang",
                "Y. Yao",
                "H. Zhang",
                "J. Xin"
            ],
            "title": "Breast cancer detection using extreme learning machine based on feature fusion with CNN deep features",
            "venue": "IEEE Access 2019,",
            "year": 2019
        },
        {
            "authors": [
                "H. Shih\u2010Cheng",
                "P. Anuj",
                "S. Saeed",
                "B. Imon",
                "M.P. Lungren"
            ],
            "title": "Fusion of medical imaging and electronic health records using deep learning: A systematic review and implementation guidelines",
            "venue": "NPJ Digit. Med. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "M. Sabzekar",
                "M. GhasemiGol",
                "M. Naghibzadeh",
                "H.S. Yazdi"
            ],
            "title": "Improved DAG SVM: A New Method for Multi\u2010Class SVM Clas\u2010 sification",
            "venue": "In Proceedings of the International Conference on Artificial Intelligence IC\u2010AI, Las Vegas, NV, USA,",
            "year": 2009
        },
        {
            "authors": [
                "L. Liu",
                "P. Wang",
                "J. Lin"
            ],
            "title": "Intrusion detection of imbalanced network traffic based on machine learning and deep learning",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "S. Rizvi",
                "M. Scanlon",
                "J. McGibney",
                "J. Sheppard"
            ],
            "title": "Deep learning based network intrusion detection system for resource\u2010 constrained environments",
            "venue": "In Proceedings of the International Conference on Digital Forensics and Cyber Crime,",
            "year": 2022
        },
        {
            "authors": [
                "A.A. Hagar",
                "B.W. Gawali"
            ],
            "title": "Apache Spark and Deep Learning Models for High\u2010Performance Network Intrusion Detection Using CSE\u2010CIC\u2010IDS2018",
            "venue": "Comput. Intell. Neurosci",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Citation: Abuali, K.M.; Nissirat, L.;\nAl\u2011Samawi, A. Advancing Network\nSecurity with AI: SVM\u2011Based Deep\nLearning for Intrusion Detection.\nSensors 2023, 23, 8959. https://\ndoi.org/10.3390/s23218959\nAcademic Editor: Naveen\nChilamkurti\nReceived: 8 October 2023\nRevised: 30 October 2023\nAccepted: 31 October 2023\nPublished: 3 November 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: intrusion detection system; deep learning; CIC\u2011IDS2018; support vector machines; multiclass classification"
        },
        {
            "heading": "1. Introduction",
            "text": "The contemporary proliferation of technical breakthroughs has led to a heightened reliance on networks and applications in individuals\u2019 everyday endeavors. Various sec\u2011 tors, such as business, education, healthcare, banking, and e\u2011governments, exemplify the significant dependence on the Internet. Furthermore, the exponential expansion of social media platforms is engendering novel forms of hazards that extend beyond the virtual realm and manifest in tangible reality. Due to the increasing dependence on network in\u2011 frastructure and the anticipated escalation in both the number and sophistication of attacks, traditional security techniques are considered insufficient in ensuring the necessary levels of security [1]. In contemporary society, online social networks have assumed a crucial role, being widely regarded as essential means of communication for individuals to con\u2011 nect with their loved ones. These platforms enable individuals to maintain contact and foster relationships, regardless of geographical barriers. Every day, substantial volumes of data, including photographs, videos, and concise messages, are sent via widely used social networking platforms such as Twitter, Facebook, Instagram, and YouTube, among others. The significance of social networks is growing due to the enormous volume of data\nSensors 2023, 23, 8959. https://doi.org/10.3390/s23218959 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 8959 2 of 19\nthey possess. Consequently, they are widely regarded as highly sought\u2011after resources by hackers seeking to obtain individuals\u2019 informationwithout their explicit authorization. As a result, to counter attacks from hackers, who are also constantly evolving their attack de\u2011 tection techniques, there is a need for sophisticated and more effective techniques than before. The traditional systems used for identifying abnormal activity often use detection techniques based on event analysis and pre\u2011set rules. They have drawbacks, including a lack of information on actual attacks, the cost of falsely detecting attacks in the field of se\u2011 curity, and the inability to handle ever\u2011updating attack techniques. Therefore, there is an urgent need for an automated system that learns from abnormal network traffic to detect new upcoming attacks. This paper aims to propose a support vectormachine (SVM)\u2011based deep learning system that will classify the data extracted from servers to determine the in\u2011 trusion incidents in social media networks. The main reason that motivates us to conduct this study is that previous studies provided evidence that using deep learning in intru\u2011 sion detection systems is deemed effective in detecting abnormal activity in large\u2011scalable networks. Thus, this study proposed a deep learning\u2011based intrusion detection system."
        },
        {
            "heading": "2. Related Works",
            "text": "At an early stage, intrusion detection systems were implemented using traditional techniques. Many studies have been conducted using statistical techniques to detect net\u2011 work intrusions. One of the earliest studies [2] built a statistical\u2011based model using Six Sigma to estimate crucial network parameter thresholds. The suggestedmethodology pro\u2011 vides a predetermined threshold to distinguish between normal, uncertain, and abnormal values for critical network variables and performs a comprehensive vulnerability assess\u2011 ment using these values. The statistical model had a prediction rate of 97%. In this study, the researchers conducted a comparative analysis of three statistical host\u2011based intrusion detection system approaches: principal component analysis (PCA), Chi\u2011square distribu\u2011 tion, andGaussianmixed distribution [3]. The outcome reveals that the PCA andGaussian mixed distributions each have a detection rate of 97.5%, while the Chi\u2011square distribution has a detection rate of 90%. The authors of [4] proposed a data mining\u2011based intrusion de\u2011 tection system. The framework is based on the DTNB, which is a hybrid classifier (DTNB) that combines the naive Bayes (NB) and decision table (DT) methods. The accuracy result of the DTNB reached 97%. However, the main drawback of using intrusion detection\u2011 based statistical techniques is that they cannot handle large scalable networks. The necessity to construct a proficient intrusion detection system employing contem\u2011 porary technology arose as a result of the swift advancement of attackers\u2019 methodologies in breaching computer networks. In the study conducted by the authors of [5], a model was developed to discern benign traffic frommalicious communication using support vec\u2011 tor machines (SVMs). The framework of the proposed model consists of numerous stages, includingNSL\u2011KDDdataset preprocessing, utilizing the information gain ratio to rank fea\u2011 tures (IGR), using a K\u2011means classifier to create the optimal feature subset, and construct\u2011 ing an intrusion detection system based on support vector machines (SVMs). According to the study\u2019s findings, the accuracy for 23 NSL\u2011KDD features was 99.32%, while the ac\u2011 curacy for 30 features was 99.37%. In [6], the study employed an enhanced Bat algorithm using the K\u2011means technique for feature extraction. Subsequently, the random forest al\u2011 gorithmwas employed for categorizing, resulting in the generation of the ultimate output. The KDD Cup 1999 dataset was employed to assess the suggested model. Based on the findings of the study, the obtained accuracy was determined to be 96.42%, while the false\u2011 positive rate was observed to be 0.98%. Furthermore, the aforementioned study suggests the utilization of intrusion detection systems that employ random forest (RF) and support vector machine (SVM) algorithms [7]. The implementation of data preparation techniques in the NSL\u2011KDD dataset was initially conducted. The study\u2019s findings indicate that when all characteristics were utilized, SVM classifiers accurately categorized 90% of instances of DoS attacks, 89% of instances of Probe attacks, 79% of instances of R2L attacks, and 100% of instances of U2R attacks, after selecting the relevant features. In terms of accuracy, the\nSensors 2023, 23, 8959 3 of 19\nRF classifier demonstrated varying performance across different attack types. Specifically, the accuracy rates for DoS, Probe, R2L, and U2R were 85%, 88%, 78%, and 100%, respec\u2011 tively, when considering all characteristics. The authors [8] developed a hybrid model that incorporates data mining approaches such as the K\u2011means clustering method and the RBF kernel function, both of which are commonly employed in support vector machines (SVMs) for classification tasks. The KDD CUP 99 dataset was employed for training and evaluating the suggested model. The results of the study demonstrate that the suggested KMSVM model achieves an accuracy of 92.86% while utilizing all attributes. In compari\u2011 son, KM achieves an accuracy of 86.67% and SVM achieves an accuracy of 40%. Sistla, V. [9] built a network intrusion detection system using an SVM and a deep convolution neural network (DCNN) in which the model\u2019s performance was evaluated using the NSL\u2011KDD dataset. The experiment\u2019s findings indicate that the accuracy was 96%. Similarly, Kim, J. and Shin, N. [10] used the KDD Cup 99 dataset to develop a deep neural network (DNN)\u2011based intrusion detection system. The results demonstrated that the accuracy was 99%. Also, Nguyen, S. [11] proposed a deep learning\u2011based intrusion detection system. The result demonstrates that the CNN model produced 99.87% detec\u2011 tion accuracy. Similarly, Wang and Hong [12] presented a novel convolutional neural network\u2011basedmethod for detecting intrusions in the network. Where the accuracy of this model reached 97.7%. In addition, by using deep learning convolutional neural networks, Toupas and Chamou [13] and Liu, P. [14] produced accuracy higher than 99%. Similarly, in [15\u201317], the accuracy reached more than 99% when deep learning was used to detect intrusion in the network traffic. Furthermore, Maseer and Yusof [18] used support vec\u2011 tor machines (SVMs) in the convolutional neural network (CNN) model. The proposed CNNmodel consists of an input layer that receives the data, convolution layers that create a features map from the data, and pooling layers that select the maximum values of the features map in the convolution layer; this is followed by another convolutional layer, an\u2011 other max pooling layer, a fully connected layer with a Relu activation function that learns and classifies outputs, and an output layer that represents the connection line as either normal or an attack. The models are tested on the CICIDS2017 dataset as part of the SVM classifier and are represented according to the factors involved in building the training models where the SVMmodel acts as a kernel function. The model produced an accuracy of 99.27%. In addition, Ref. [19] proposed an intrusion detection system (IDS) based on the convolutional neural network on CICIDS2017. The CNN model received an 8 \u00d7 8 matrix as the input. The topology consists of a convolution layer kernel size of 3 \u00d7 3, a max pool\u2011 ing layer with a kernel size of 2 \u00d7 2, a convolution layer kernel size of 3 \u00d7 3, and a max pooling layer with a kernel size of 2\u00d7 2. Then, the pooled feature maps are converted into a 1D array in row order, which is the format expected by the fully connected layer. This generates an array of 288 elements for each input matrix. The study demonstrated that the accuracy of the proposed model reached 99.78%. In addition, the optimization of hyper\u2011 parameters in one\u2011dimensional (1D) convolutional neural networks (CNNs) for network intrusion detection with a genetic algorithm (GA) and particle swarm optimization (PSO) is examined in [20]. The model has been tested on the NSW\u2011NB15 dataset. According to the study results, the PSO and GA produced accuracy of 99.28% and 99.31%, respectively. Also, in [21], the study provides a hybrid intrusion detection method that makes use of an optimized CNN by applying enhanced CNN parameters via the grey wolf optimizer (GWO) method, which improves the model\u2019s prediction accuracy by fine\u2011tuning the CNN parameters. By testing the model on the UNW\u2011NB15 dataset, the accuracy reached 94.25%. Moreover, Ref. [22] developed an intrusion detection system using an artificial neural net\u2011 work and genetic algorithm. The experiments have been conducted on Bot\u2011IoT, which consists of network traffic from different types of attacks. The results demonstrated that using a neural network with a genetic algorithm is effective in classifying network traffic and detecting abnormal traffic. Due to the rise of internet\u2011based services, traditional tools have become inadequate for processing large volumes of network traffic. Therefore, there is an urgent need for an\nSensors 2023, 23, 8959 4 of 19\nefficient and fast intrusion detection system that can process large amounts of complex net\u2011 work traffic. However, the previouswork provides evidence that using deep learning in in\u2011 trusion detection systems is deemed effective in detecting abnormal activity in large\u2011scale networks. Using deep learning to analyze large\u2011size network traffic can increase the de\u2011 tection accuracy and reduce false detection. In addition, support vector machines (SVMs) are the most used method for classifying network traffic. Thus, this research proposed a deep learning\u2011based intrusion detection system using a support vector machine method for classifying network traffic on social media. This research is intended to fill the gap in research and investigate classification and intrusion detection in social media data. The rest of this paper is organized as follows. Section 2 demonstrates related work on network intrusion detection using deep learning andmachine learningmethods. Section 3 describes the preprocessing procedure and data analysis. In Section 4, we describe the proposed architecture of the deep neural network. In Section 5, we present the results of our work, comparing it with related works. Finally, Section 6 gives a conclusion to this paper and presents future work."
        },
        {
            "heading": "3. Proposed Model",
            "text": ""
        },
        {
            "heading": "3.1. Data Preprocessing",
            "text": "This section outlines the preprocessing techniques that were implemented on the dataset before training the proposed model. The CSE\u2011CIC\u2011IDS 2018 dataset undergoes several preprocessing techniques, including the removal of columns with zero values, di\u2011 mensionality reduction, and down\u2011sampling using principal component analysis (PCA), data normalization, and the enumeration of class tags. In some studies, such as [23], remov\u2011 ingmissing values, data normalization, and the enumeration of class tags are the only data preprocessing techniques that are applied to the dataset. However, the applied data pre\u2011 processing does not overfit the model during the training because after testing the model with new data, we see that the model is able to produce high detection accuracy. This provides evidence that the model is not overfitted because when the model is overfitted, it will not be able to produce high prediction results with the new data.\n3.1.1. CSE\u2011CIC\u2011IDS 2018 Dataset This work relies on a public intrusion detection dataset, namely CSE\u2011CIC\u2011IDS 2018, which was created by The Canadian Institute for Cybersecurity (CIC) and the Communi\u2011 cations Security Establishment (CSE) [24]. This dataset is synthesized to generate similar traffic to the real network traffic. It was generated by using the AWS computing platform to simulate the topology of a common LAN network. It contains 14 distinct attacks. These 14 classes and their distribution are shown in Figure 1. The CSE\u2011CIC\u2011IDS2018 traffic is generated by the CICFlowMeter. The CICFlowMeter is a Java\u2011written network traffic flow generator that provides greater flexibility in selecting and adding new features and better control over the length of the flow timeout. The sta\u2011 tistical features, such as duration, number of packets, number of bytes, length of packets, etc., are also calculated separately in the forward and reverse direction because it gener\u2011 ates bidirectional flows (Biflow), where the first packet determines the forward (source to destination) and backward (destination to source) directions. For feature extraction, the CICFlowMeter\u2011V3 extracted 80 traffic features from the raw data and exported them as a CSV file. However, in this dataset, there is a set of columns that has zero values in all the entries. Also, the number of instances for each class is not balanced. In addition, the data are in different scales, with a high correlation between instances. For this, the study applied data preprocessing on the dataset before feeding it to the model.\nSensors 2023, 23, 8959 5 of 19Sensors 2023, 23, x FOR PEER REVIEW f 20\nFigure 1. Dataset Classes Distribution.\nThe CSE-CIC-IDS2018 traffic is generated by the CICFlowMeter. The CICFlowMeter is a Java-written network traffic flow generator that provides greater flexibility in selecting and adding new features and better control over the length of the flow timeout. The statistical features, such as duration, number of packets, number of bytes, length of packets, etc., are also calculated separately in the forward and reverse direction because it generates bidirectional flows (Biflow), where the first packet determines the forward (source to destination) and backward (destination to source) directions. For feature extraction, the CICFlowMeter-V3 extracted 80 traffic features from the raw data and exported them as a CSV file. However, in this dataset, there is a set of columns that has zero values in all the entries. Also, the number of instances for each class is not balanced. In addition, the data are in different scales, with a high correlation between instances. For this, the study applied data preprocessing on the dataset before feeding it to the model.\n3.1.2. Data Preprocessing Deep learning algorithms have a direct correlation with the data they operate on, and to provide precise outcomes, it is imperative to preprocess the data. Data cleaning procedures are employed to eliminate correlation, eliminate redundancy, eliminate missing data, and balance classes. Data consistency is important in the field of deep learning for effectively processing and understanding data. Data that lack consistency are a significant challenge in training deep learning models [25] Missing data: the CSE-CIC-IDS2018 dataset consists of 80 columns. Based on conducting a visual investigation, it was seen that a specific set of columns contains a value of zero for all entries. The columns with zero values have been removed. The columns that have been removed from the dataset are: bw_psh_flag, bw_urg_flag, fw_byt_blk_avg, fw_pkt_blk_avg, fw_blk_rate_avg, bw_byt_blk_avg, bw_pkt_blk_avg, and bw_blk_rate_avg. These columns have been removed from the dataset using the MATLAB function before feeding the data into the CNN model. Dimensionality reduction: data normalization involves scaling down the data to be in the same range as the firing function, between 0 and 1 [26]. In the CSE-CIC-IDS2018 dataset, features are distinctive in their statistical properties, such as mean and standard deviation. Consequently, dimensionality reduction is implemented to bring all features into the same statistical range. The dimensionality reduction has been applied using the principal component analysis (PCA). In addition, PCA is also applied to eliminate the correlation and map the data into a low-dimensional space, where each feature is mapped into a PCA space, providing the most important information. Initially, we trained the\n87 230 611 1730 10990 41508 139890 161934 187589 193360 286191\n461912 686012\n6112151\n0 1000000 2000000 3000000 4000000 5000000 6000000 7000000\nSQL Injection Brute Force -XSS\nBrute Force -Web DDOS attack-LOIC-UDP\nDoS attacks-Slowloris DoS attacks-GoldenEye\nDoS attacks-SlowHTTPTest Infilteration\nSSH-Bruteforce FTP-BruteForce\nBot DoS attacks-Hulk\nDDOS attack-HOIC Benign\nFigure 1. Dataset Classes Distribution.\n3.1.2. Data Preprocessing Deep learning alg rithms have a direct c rrel tion with the data they operate on, and to provi e precise outcomes, it is imperative to preprocess the data. Data cleaning pro\u2011 cedures are employed to eliminate correlation, eliminate redundancy, eliminate missing data, nd balance classes. Data consis ency is important in the field of de p learning fo eff ctively processing and understanding data. Data that lack co sis ency are a signifi ant challe ge i training deep learning models [25]. Missing data: the CSE\u2011CIC\u2011IDS2018 dataset consists of 80 columns. Based on con\u2011 ducting a visual investigation, it was se n that a specific set of columns contains a value of zero for all entries. The colum s with zero values have bee remove . The columns that have b en removed from the dataset are: bw_psh_flag, bw_urg_flag, fw_byt_blk_avg, fw_pkt_blk_avg, fw_blk_rate_avg, bw_byt_blk_avg, bw_pk _blk_avg, and bw_blk_rate_ avg. These columns have been removed from the dataset using the MATLAB function before feeding the data into the CNN model. Dimensionality reduction: data normalization involves scaling down the data to be in the same range as the firing function, between 0 and 1 [26]. In the CSE\u2011CIC\u2011IDS2018 dataset, features are distinctive in their statistical properties, such as mean and standard deviation. Consequently, dimensionality reduction is implemented to bring all features into the same statistical range. The dimensionality reduction has been applied using the principal component analysis (PCA). In addition, PCA is also applied to eliminate the cor\u2011 relation andmap the data into a low\u2011dimensional space, where each feature ismapped into a PCA space, providing the most important information. Initially, we trained the model before applying dimensionality reduction, but the training accuracy could not exceed 70%. After a deep investigation of the dataset, we applied dimensionality reduction using prin\u2011 cipal component analysis. After applying PCA, the dataset size was reduced dramatically. The dataset had only 32 retained columns and 45 highly correlated features, which are con\u2011 sidered redundant and have aminor contribution to the classification process. The original data in 2D, before applying PCA, are shown in Figure 2A, whereas the correlated data af\u2011 ter applying PCA in 2D are shown in Figure 2B. PCA was applied to the dataset using the build\u2011in layer in MATLAB, which defined the correlation of the data by calculating the eigenvector and eigenvalue of the covariance matrix. Then, the function was finding the major component by sorting these components by their eigenvalue in decreasing order.\nSensors 2023, 23, 8959 6 of 19\nSensors 2023, 23, x FOR PEER REVIEW 6 of 20\nmodel before applying dimensionality reduction, but the training accuracy could not exceed 70%. After a deep investigation of the dataset, we applied dimensionality reduction using principal component analysis. After applying PCA, the dataset size was reduced dramatically. The dataset had only 32 retained columns and 45 highly correlated features, which are considered redundant and have a minor contribution to the classification process. The original data in 2D, before applying PCA, are shown in Figure 2A, whereas the correlated data after applying PCA in 2D are shown in Figure 2B. PCA was applied to the dataset using the build-in layer in MATLAB, which defined the correlation of the data by calculating the eigenvector and eigenvalue of the covariance matrix. Then, the function was finding the major component by sorting these components by their eigenvalue in decreasing order.\n(A) (B)\nFigure 2. Correlation Matrix, (A) Raw Data 2D, (B) Row Data 3.\nDown-sampling: in CSE-CIC-IDS2018, there are four classes with few instances, as shown in Figure 1. These four classes have less than 2000 instances, whereas the rest have more than 10,000 instances. To keep the balance between these classes, all classes with less than 10,000 instances have been removed. The four classes that were removed are SQL Injection, Brute Force-XSS, Brute Force-Web, and DDOS attack-LOIC-UDP. Therefore, this work considers ten classes, including nine attacks and benign traffic. Initially, the dataset had 17% attack traffic and 83% benign traffic. The down-sampling technique is applied to minimize the chance of overfitting and high bias on the network. For this, only 10,000 instances from each class are randomly chosen. Thus, the total number of instances for all classes will be 100,000, where the classes are completely balanced. These ten classes are: Benign, DDOS attack-HOIC, DoS attack-HULK, Bot, FTP-BruteForce, SSH-BruteForce, Infiltration, DoS attack-SlowHTTPTest, DoS attack-GoldenEye and DoS attack-Slowlorie. The down-sampling was implemented manually in the Excel file with the help of the filter feature. Enumeration of class tags: as we are implementing multilevel classification, the \u2018Label\u2019 column in the CSE-CIC-IDS2018 dataset was converted into numerical values for computational efficiency purposes. The class tags are shown in Table 1. Class tags were implemented with the help of the for-loop function in MATLAB, where each class in the forloop is assigned to a specific class tag.\nFigure 2. Correlation Matrix, (A) Raw Data 2D, (B) Row Data 3.\nDown\u2011sa pling: in CSE\u2011CIC\u2011IDS2018, there are four classes with few instances, as shown in Figure 1. These four classes have less than 2000 instances, whereas the rest have more than 10,000 instances. To keep the balance between these classes, all classes with less than 10,000 instances have been removed. The four classes that were removed are SQL Injection, Brute Force\u2011XSS, Brute Force\u2011Web, and DDOS attack\u2011LOIC\u2011UDP. There\u2011 fore, this work considers te classes, including ni e attacks and benign traffic. Initially, the dataset had 17% attack traffic and 83% benign traffic. The down\u2011sampling technique is applied to minimiz the chance of overfitt ng and high bias on the network. F r this, only 10,000 instances from each class are randomly chosen. Thus, the total number of instances for all classes will be 100,000, where th cl sses are completely balanced. These ten class s are: Benign, DDOS attack\u2011HOIC, DoS attack\u2011HULK, Bot, FTP\u2011BruteForce, SSH\u2011BruteForce, Infil ration, DoS ttack\u2011SlowHTTPTest, DoS attack\u2011GoldenEye and DoS attack\u2011Slowlorie. The down\u2011sampling was implemented manually in the Excel file with the help of the filter feature. Enumeration of cla s tags: as we are implementing multilevel cla sification, the \u2018La\u2011 bel\u2019 column in the CSE\u2011CIC\u2011IDS2018 dataset was converted into numerical values for com\u2011 putational efficiency purposes. The class tags are shown in Table 1. Class tags were imple\u2011 mented it t e el of the for\u2011lo p function inMATLAB, where each class in the for\u2011l op is assigned to a specific class tag.\nData splitting: data splitting is frequently used in deep learning to separate data into train, test, and validation sets. The training dataset is a collection of instances that are used to fit the model\u2019s parameters. The validation dataset is part of the dataset used to fit the model\u2019s hyperparameters. The testing dataset is a part of the dataset used to evaluate the proposed model and detect the biasing on the network. The dataset splitting ratio used in this study is 50% for training, 20% for validation, and 30% for testing; this splitting ratio is one of the common splitting used in the literature [27]. The data splitting has been imple\u2011 mented in the MATLAB code where 50% of the dataset is taken as the training set, 20% is\nSensors 2023, 23, 8959 7 of 19\ntaken as the validation set and 30% is taken as the testing set. According to this splitting, each set of the dataset is used in different phases in training, validation and testing."
        },
        {
            "heading": "3.2. Deep Neural Network Architecture",
            "text": "The proposed architecture for the deep learning system comprises an input layer, a se\u2011 ries of hidden layers, and an output layer. The architectural structure is divided into three distinct blocks, namely the input block, which consists of the input layer and the general convolutional layer; the second block, which consists of three parallel branches that em\u2011 ploy stacked convolutional layers followed by a fully connected layer to extract features; and the third block, which has two completely connected layers, namely the SoftMax layer and the classification layers, which are responsible for decision\u2011making. In each block, a series of layers are present, whereby the convolution layers are responsible for feature ex\u2011 traction. The primary purpose of the pooling layer is to extract salient characteristics from the input data and effectively reduce its dimensionality. Nevertheless, the pooling layer may lead to the loss of important information within the data. For this, we employed a convolutional layer with a stride value of one to decrease the dimensionality of the feature instead of using the pooling layer [28]. In the process of feature extraction, the rectified lin\u2011 ear unit (ReLU) activation function is utilized to activate every neuron inside the output of the network. The rectified linear unit (ReLU) is widely employed as an activation function to introduce nonlinearity into the system. The activation function layer serves as a crucial component within convolutional neural networks (CNNs), responsible for converting the input into a significant and interpretable representation of the data. However, to mitigate computational complexity and introduce parallelism to the data, the suggested architec\u2011 ture is partitioned into three parallel branches. Furthermore, the combination of several convolutional neural networks (CNNs) results in improved classification accuracy com\u2011 pared to individual CNNs [29]. This is because deep learning fusion has several benefits over traditionalmachine learning techniques. First, it can combinemultiple data sources to create a more accurate and comprehensive understanding of data. This allows for more ac\u2011 curate predictions and better classification. Additionally, deep learning fusion can process large amounts of data quickly and efficiently, making it ideal for applications that require real\u2011time analysis [30]. However, the three blocks of the proposedmodel will be discussed in detail in the following subsections. The proposed topology is shown in Figure 3. Sensors 2023, 23, x FOR PEER REVIEW 8 of 20 model will be discussed in d tail in the following subsecti ns. The proposed topology is shown in Figure 3.\nFigure 3. The Proposed Deep Learning System.\nA. Input Block: Within this section, a data input layer has been implemented to facilitate the insertion of network traffic into the convolutional neural network (CNN) model. Subsequently, a single two-dimensional convolutional layer was employed for feature extraction, consisting of ten kernels with dimensions of 5 \u00d7 1. The 2D convolutional layer is accompanied afterward by a batch normalization layer, which performs normalization on a mini-batch of data, independently across all observations for each channel. In this batch normalization layer, the topology is divided into three branches, and it is connected with the first convolution layer in the first, second, and third branches of the second block. The input block is shown in Figure 3. B. Branching Block: The second block consists of three branches. These three branches are identical, including the same number of layers and kernels. Each branch starts from the batch normalization layer in the input block and ends with the depth concatenation layer in the output block. Each branch of the model utilizes three convolution layers, where each layer employs five kernels of size 5 \u00d7 1, and each convolution layer is followed by a batch normalization layer. At the end of each block, a fully connected layer with seven neurons is employed. The branching block is shown in Figure 3. C. Output Block: This block has a depth concatenation layer, which takes inputs that have the same height and width and concatenates them along the channel dimension. This concatenation layer has three inputs to connect each fully connected layer at the end of each branch in the branching block with the output block. Subsequently, the architecture includes two fully connected layers. The first fully connected layer consists of 15 neurons, while the subsequent fully connected layer is composed of 10 neurons. These layers are utilized to classify 10 distinct classes. Following this, a SoftMax layer and a classification layer are employed to classify network traffic. The output block is depicted in Figure 3.\nDAG SVM Algorithm This work aims to integrate an SVM with deep learning to develop IDS for social media. In SVM implementation, the SoftMax layer is replaced by the SVM layer. However, since the proposed architecture has been trained initially with the SoftMax layer, the SVM has been applied with the optimal activations (weight and bias) from the training. The reason for taking the activation instead of training the SVM again on the dataset is that the complexity will be higher in training the SVM again within the data. However, the kernel used for applying the SVM is a polynomial kernel [31]. The kernel function that has been applied in this work is:\nFigure 3. The Proposed Deep Learning System.\nA. Input Block: Within this section, a data input layer has been implemented to facil\u2011 itate the insertion of network traffic into the convolutional neural network (CNN) model. Subsequently, a single two\u2011dimensional convolutional layer was employed for feature ex\u2011 traction, consisting of ten kernels with dimensions of 5 \u00d7 1. The 2D convolutional layer is accompanied afterward by a batch normalization layer, which performs normalization on a mini\u2011batch of data, independently across all observations for each channel. In this batch\nSensors 2023, 23, 8959 8 of 19\nnormalization layer, the topology is divided into three branches, and it is connected with the first convolution layer in the first, second, and third branches of the second block. The input block is shown in Figure 3. B. Branching Block: The second block consists of three branches. These three branches are identical, including the same number of layers and kernels. Each branch starts from the batch normalization layer in the input block and ends with the depth concatenation layer in the output block. Each branch of themodel utilizes three convolution layers, where each layer employs five kernels of size 5 \u00d7 1, and each convolution layer is followed by a batch normalization layer. At the end of each block, a fully connected layer with seven neurons is employed. The branching block is shown in Figure 3. C. Output Block: This block has a depth concatenation layer, which takes inputs that have the same height andwidth and concatenates them along the channel dimension. This concatenation layer has three inputs to connect each fully connected layer at the end of each branch in the branching block with the output block. Subsequently, the architecture includes two fully connected layers. The first fully connected layer consists of 15 neurons, while the subsequent fully connected layer is composed of 10 neurons. These layers are utilized to classify 10 distinct classes. Following this, a SoftMax layer and a classification layer are employed to classify network traffic. The output block is depicted in Figure 3.\nDAG SVM Algorithm This work aims to integrate an SVM with deep learning to develop IDS for social media. In SVM implementation, the SoftMax layer is replaced by the SVM layer. However, since the proposed architecture has been trained initially with the SoftMax layer, the SVM has been applied with the optimal activations (weight and bias) from the training. The reason for taking the activation instead of training the SVM again on the dataset is that the complexity will be higher in training the SVM again within the data. However, the kernel used for applying the SVM is a polynomial kernel [31]. The kernel function that has been applied in this work is:\nf(x) =\u3016(x \u2212 p)\u3017^q, (1)\nwhere x is the data, and p and q are the constants that have been estimated during training. phase. The SVM topology is shown in Figure 4.\nSensors 2023, 23, x FOR PEER REVIEW 9 of 20\nf(x)=\u3016(x \u2212 ) , (1)\nr is t t , are t e c st ts t t sti t ri tr i i . hase. The SV topology is sho n in Fig re 4.\nFigure 4. SVM Implementation.\n4. Experimental Setup Once the preprocessing approaches have been applied, the dataset is subsequently fed into the proposed model for training. Table 2 presents comprehensive information for the 15 training experiments that were carried out. The first round of experiments focused on 30 features extracted using PCA, which were treated as an independent feature called PCA space. This round consisted of three experiments. In the subsequent rounds, the number of features was expanded to 40, 50, 60, and finally 68. For each one of these feature numbers, experiments were conducted three times. Subsequently, the CNN model was enhanced by adding a support vector machine (SVM) layer, and subsequently, five experiments were conducted on 100,000 instances. Similarly, for model classification, there were five experiments conducted with different features ranging from 30 to 68 features, with 10 feature increments for each experiment. These experiments are carried out on a personal computer using the hardware and software characteristics listed in Table 3. The practical implementation was executed using MATLAB version R2022b, where the performance of the proposed deep learning model is assessed through the utilization of a confusion matrix. The training parameters utilized for optimizing weights and biases are associated with the Adam optimizer. The training parameters are presented in Table 4. However, developing and training the proposed model has some challenges. The first challenge is that training the model requires high computational resources. The second challenge is that applying the preprocessing technique and obtaining a high training accuracy takes a lot of trial and time. This challenge results in limitations, where the system is only tested on the CSE-CIC-IDS2018 dataset and has not been tested on another dataset. This limitation will be considered, and the system will be tested on another dataset in future work. The training and validation phase is shown in Figure 5, and part of the code with the training details is shown in Figure 6. Part of the preprocessing work is shown in Figure 7.\nFigure 4. SV I ple entation."
        },
        {
            "heading": "4. Experimental Setup",
            "text": "Once the preprocessing approaches have been applied, the dataset is subsequently fed into the proposed model for training. Table 2 presents comprehensive information for the 15 training experiments that were carried out. The first round of experiments focused on 30 features extracted using PCA, whichwere treated as an independent feature called PCA space. This round consisted of three experiments. In the subsequent rounds, the num\u2011 ber of features was expanded to 40, 50, 60, and finally 68. For each one of these feature\nSensors 2023, 23, 8959 9 of 19\nnumbers, experiments were conducted three times. Subsequently, the CNN model was enhanced by adding a support vector machine (SVM) layer, and subsequently, five experi\u2011 ments were conducted on 100,000 instances. Similarly, for model classification, there were five experiments conducted with different features ranging from 30 to 68 features, with 10 feature increments for each experiment. These experiments are carried out on a personal computer using the hardware and software characteristics listed in Table 3. The practical implementation was executed using MATLAB version R2022b, where the performance of the proposed deep learningmodel is assessed through the utilization of a confusionmatrix. The training parameters utilized for optimizing weights and biases are associated with the Adam optimizer. The training parameters are presented in Table 4. However, developing and training the proposed model has some challenges. The first challenge is that training the model requires high computational resources. The second challenge is that applying the preprocessing technique and obtaining a high training accuracy takes a lot of trial and time. This challenge results in limitations, where the system is only tested on the CSE\u2011 CIC\u2011IDS2018 dataset and has not been tested on another dataset. This limitation will be considered, and the system will be tested on another dataset in future work. The training and validation phase is shown in Figure 5, and part of the code with the training details is shown in Figure 6. Part of the preprocessing work is shown in Figure 7.\nSensors 2023, 23, 8959 10 of 19\nSensors 2023, 23, x FOR PEER REVIEW 10 of 20 Table 2. Number of Experiments. Number of Experiments Number of Features Experiment 1 Experiment 2 Experiment 3 Experiment 4 Experiment 5 30 3 40 3 50 3 60 3 68 3 Total number of Experiments = 15 Table 3. Hardware Specifications. Items Specification Manufacturer<break/>City<break/>Country MAC Foxconn <break/>Designed by California, assembled in China<break/>U.S Model MacBook Pro 2021 Processor Type Apple M1 Pro chip Installed Memory Apple M1 Pro Chip, 16GB unified memory Processor Speed 10-core CPU with 6 performance cores and 2 efficiency cores, 14-core GPU, 16-core Neural Engine, 200 GB/s memory bandwidth Number of Cores 10-core CPU, 14-core GPU Number of Threads Single Thread Machine Learning Tool MATLAB (License Number: 40921045) version R2021b Table 4. Experiment Parameters.\nParameters Value Algorithm Adam solver\nMax Epoach 50 Mini Batch Size 1000\nValidationFrequency 5 ValidartionPatient 10\nExecutionEnvironment Parallel Pooling\nSensors 2023, 23, x FOR PEER REVIEW 11 of 20\n5. Experimental Result This section presents the result of the proposed architecture in terms of accuracy, true-positive recall, precision, specificity, false-positive recall, and F-score for each one of the 10 different classes that our model can detect. The evaluation of the model is based on a confusion matrix. The results will be discussed in detail in the following subsection. The convergence of training accuracy is demonstrated in Figure 8A, which displays the average of three experiments conducted on the same dataset with 100,000 records and several features used for training. The accuracy of training surpasses 95% after 35 epochs, and the stability of training is generally excellent, regardless of the number of features employed. However, when all the features are chosen for training, there is a slight improvement in the convergence rate. Figure 8B exhibits the convergence of training loss during the training epoch, with each curve representing the average of three experiments run on the same data setting (100,000 records and a fixed number of features used for training). The training loss drops to less than 0.5% after 45 epochs, and the stability of training loss is generally good, regardless of the number of features used. However, there is a slight decrease in the loss convergence rate when 60 or all features are selected for training. Similarly, the convergence of validation accuracy during the training convergence is described in Figure 9A. Each curve represents the average of three experiments conducted using the identical data setting (100,000 records and the number of features used in training). After 30 epochs, the validation accuracy reaches more than 95%, regardless of the number of features employed. However, validation demonstrates a high oscillation at the beginning of the training until epoch 30. When 60 features are chosen for training, the rate\nSensors 2023, 23, x FOR PEER REVIEW 11 of 20\n5. Experimental Result This section presents the result of the proposed architecture in terms of accuracy, true-positive recall, precision, specificity, false-positive recall, and F-score for each one of the 10 different classes that our model can detect. The evaluation of the model is based on a confusion matrix. The results will be discussed in detail in the following subsection. The convergence of training accuracy is demonstrated in Figure 8A, which displays the average of three experiments conducted on the same dataset with 100,000 records and several features used for training. The accuracy of training surpasses 95% after 35 epochs, and the stability of training is generally excellent, regardless of the number of features employed. However, when all the features are chosen for training, there is a slight improvement in the convergence rate. Figure 8B exhibits the convergence of training loss during the training epoch, with each curve representing the average of three experiments run on the same data setting (100,000 records and a fixed number of features used for training). The training loss drops to less than 0.5% after 45 epochs, and the stability of training loss is generally good, regardless of the number of features used. However, there is a slight decrease in the loss convergence rate when 60 or all features are selected for training. Similarly, the convergence of validation accuracy during the training convergence is described in Figure 9A. Each curve represents the average of three experiments conducted using the identical data setting (100,000 records and the number of features used in training). After 30 epochs, the validation accuracy reaches more than 95%, regardless of the number of features employed. However, validation demonstrates a high oscillation at the beginning of the training until epoch 30. When 60 features are chosen for training, the rate"
        },
        {
            "heading": "5. Experimental Result",
            "text": "This section presents result of the proposed architecture in terms of accuracy, true\u2011 positive recall, pr cision, specificity, false\u2011positive recall, and F\u2011s ore for each one of the 10 different classes that our model can detect. The evaluation of the model is based on a confusion matrix. The results will be iscussed in detail i the following subsection.\nSensors 2023, 23, 8959 11 of 19\nA. Training and Validation Accuracy\nThe convergence of training accuracy is demonstrated in Figure 8A, which displays the average of three experiments conducted on the same dataset with 100,000 records and several features used for training. The accuracy of training surpasses 95% after 35 epochs, and the stability of training is generally excellent, regardless of the number of features em\u2011 ployed. However, when all the features are chosen for training, there is a slight improve\u2011 ment in the convergence rate. Figure 8B exhibits the convergence of training loss during the training epoch, with each curve representing the average of three experiments run on the same data setting (100,000 records and a fixed number of features used for training). The training loss drops to less than 0.5% after 45 epochs, and the stability of training loss is generally good, regardless of the number of features used. However, there is a slight decrease in the loss convergence rate when 60 or all features are selected for training.\nSensors 2023, 23, x FOR PEER REVIEW 12 of 20\nbeginning of the training until epoch 30. When 60 features are chosen for training the ate of converg nce is slightly imp oved. When 60 features are selected, the convergence is greater than 95% after 10 epochs. On the other hand, the convergence of validati n loss during the training convergence is described in Figure 9B. Each curve represents the average of three experiments co ducted using the identical data setting (100,000 records and the number of features used in training). After 45 epochs, the validation loss reaches less than 0.5%, and regardless of the number of features employed, validation loss demonstrates very high stability overall. When 60 or all features are chosen for training, the rate of validation loss is slightly reduced.\nSimilarly, the convergence of validation accuracy during the training convergence is described in Figure 9A. Each curve represents the average of three experiments conducted using the identical data setting (100,000 records and the number of features used in train\u2011 ing). After 30 epochs, the validation accuracy reaches more than 95%, regardless of the number of features employed. However, validation demonstrates a high oscillation at the beginning of the training until epoch 30. When 60 features are chosen for training, the rate of convergence is slightly improved. When 60 features are selected, the convergence is greater than 95% after 10 epochs. On the other hand, the convergence of validation loss during the training convergence is described in Figure 9B. Each curve represents the\nSensors 2023, 23, 8959 12 of 19\naverage of three experiments conducted using the identical data setting (100,000 records and the number of features used in training). After 45 epochs, the validation loss reaches less than 0.5%, and regardless of the number of features employed, validation loss demon\u2011 strates very high stability overall. When 60 or all features are chosen for training, the rate of validation loss is slightly reduced. Sensors 2023, 23, x FOR PEER REVIEW 13 of 20\n(A)\n(B)\nFigure 9. Validation: (A) Validation Accuracy. (B) Validation Loss.\nThe accuracy of the proposed model when applied to 100,000 records using 30, 40, 50, 60, and 68 features is the same. Therefore, we have only presented the confusion matrix for the 68 features in Figure 10. Table 5 provides a detailed analysis of the accuracy of the confusion matrix for 100,000 records with 68 features. This study found that the proposed model performed well when applied to the CSE-CIC-IDS2018 dataset using 68 features and 100,000 records in all ten classes, including benign, bot, DDoS attack-HOIC, DoS Attack-GoldenEye, DoS Attack-Hulk, DoS Attack-SlowHTTPTest, DoS Attack-Slowlories, FTP-BruteForce, infiltration, and SSH-BruteForce. The accuracy, true-positive recall, precision, specificity, false-positive recall, and F-score for all classes were 100%, 100%, 100%, 100%, 0%, and 100%, respectively. Mainly, obtained 100% accuracy after applying the data preprocessing technique to the chosen synthesis dataset. The data preprocessing techniques had a major effect on our accuracy, where the accuracy before applying data preprocessing did not exceed 70% and the accuracy after applying data preprocessing reached 100%. This clarifies the reason that some studies produced varied results by applying the same model on the same synthesis dataset. This is because each researcher applies different data preprocessing techniques based on the researcher\u2019s investigation of the chosen dataset. In other words, in this study, the accuracy did not exceed 70% before applying dimensionality reduction, whereas the accuracy reached 100% after removing the correlation and the duplication of the dataset using principal component analysis (PCA). In addition, the preprocessing techniques of the synthesis dataset depend on the\nFigure 9. Validation: (A) Validation Accuracy. (B) Validation Loss.\nB. Classification Accuracy\nThe accuracy of the proposed model when applied to 100,000 records using 30, 40, 50, 60, and 68 features is the same. Therefore, we have only presented the confusion matrix for the 68 features in Figure 10. Table 5 provides a detailed analysis of the accuracy of the confusion matrix for 100,000 records with 68 features. This study found that the proposed model performedwell when applied to the CSE\u2011CIC\u2011IDS2018 dataset using 68 features and 100,000 records in all ten classes, including benign, bot, DDoS attack\u2011HOIC, DoS Attack\u2011 GoldenEye, DoS Attack\u2011Hulk, DoS Attack\u2011SlowHTTPTest, DoS Attack\u2011Slowlories, FTP\u2011 BruteForce, infiltration, and SSH\u2011BruteForce. The accuracy, true\u2011positive recall, precision, specificity, false\u2011positive recall, and F\u2011score for all classes were 100%, 100%, 100%, 100%, 0%, and 100%, respectively. Mainly, obtained 100% accuracy after applying the data pre\u2011 processing technique to the chosen synthesis dataset. The data preprocessing techniques had a major effect on our accuracy, where the accuracy before applying data preprocess\u2011 ing did not exceed 70% and the accuracy after applying data preprocessing reached 100%. This clarifies the reason that some studies produced varied results by applying the same\nSensors 2023, 23, 8959 13 of 19\nmodel on the same synthesis dataset. This is because each researcher applies different data preprocessing techniques based on the researcher\u2019s investigation of the chosen dataset. In other words, in this study, the accuracy did not exceed 70% before applying dimensional\u2011 ity reduction, whereas the accuracy reached 100% after removing the correlation and the duplication of the dataset using principal component analysis (PCA). In addition, the pre\u2011 processing techniques of the synthesis dataset depend on the researched investigation of the dataset. In other words, the preprocessing techniques applied to the synthesis dataset affect the accuracy result, and this preprocessing technique depends on the researcher\u2019s in\u2011 vestigation of the dataset itself. For example, in [32], we can see that the accuracy of using the same model on the same dataset after applying different preprocessing techniques is different. The reason is that the preprocessing techniques applied to the dataset affect the result even if the same model is applied to the same dataset. In addition, the main concept of overfitting is that the model is unable to produce good prediction results on the new data, whereas the proposed model was able to produce 100% accuracy on the new dataset. This provides evidence that the proposedmodel is not overfitted because it has high predic\u2011 tion capability on the new data. The consistent accuracy across different feature sets and the achievement of 100% accuracy after data preprocessing highlight the robustness of our model. While this performance pertains to the CSE\u2011CIC\u2011IDS2018 dataset, it is important to recognize that results may vary when applying the model to different datasets; this is a consideration we intend to address in our future work, further enhancing the model\u2019s real\u2011 world applicability and generalizability. Additionally, the 0% false\u2011positive recall signifies a low risk of false alarms, an essential factor for intrusion detection system reliability.\nSensors 2023, 23, x FOR PEER REVIEW 14 of 20\n(PCA). In addition, the preprocessing techniques of the synthesis dataset depend on the researched investigation of the dataset. In other words, the preprocessing techniques applied to the synthesis dataset affect the accuracy result, and this preprocessing technique depends on the researcher\u2019s investigation of the dataset itself. For example, in [32], we can see that the accuracy of using the same model on the same dataset after applying different preprocessing techniques is different. The reason is that the preprocessing techniques applied to the dataset affect the result even if the same model is a plied to the same dataset. In addition, the mai concept of overfitting is that the model is unable to produce good prediction results on the new dat , wh reas proposed model was able to produce 100% accuracy on the new dataset. This provid s evidence that the proposed model is ot overfitted because i has high prediction cap ili y on the new data. The consist t accur cy acro s differ nt featur sets and the achievement f 100% accuracy after data preprocessing highlight the robustness of our model. While this performance per ains to th CSECIC-IDS2018 dataset, it is important to ecogniz that results may vary when applying the model to different datasets; this is a considera ion we intend to address in our future work, further e hancing the model\u2019s real- orld licability and generalizability. Additionally, the 0% false-positive recall signifie a l w risk of false alarms, an essential factor for int usion detection system reliability.\nFigure 10. Confusion Matrix for 68 Features.\nTable 5. Confusion Accuracy Analysis for 100,000 Records of 68 Features.\nClasses Labels Accuracy True-Positive\nRecall Sensitivity\nPredicted Positive Precision Actual Negative Specificity\nFalse-Positive Recall\nSensitivity F-Score\nBenign 100 100 100 100 0 100 Bot 100 100 100 100 0 100\nDDoS attacksHOIC\n100 100 100 100 0 100\nDoS AttacksGoldenEye 100 100 100 100 0 100\nDoS AttacksHulk 100 100 100 100 0 100\nDoS AttacksSlowHTTPTest\n100 100 100 100 0 100\nDoS AttacksSlowlories\n100 100 100 100 0 100\nFTP-BrutForce 100 100 100 100 0 100 Infiltration 100 100 100 100 0 100\nFigure 10. Confusion Matrix for 68 Features.\nTable 5. Confusion Accuracy Analysis for 100,000 Records of 68 Features.\nClasses Labels Accuracy True\u2011Positive\nRecall Sensitivity\nPredicted Positive Precision\nActual Negative Specificity\nFalse\u2011Positive Recall\nSensitivity F\u2011Score\nBenign 100 100 100 100 0 100 Bot 100 100 100 100 0 100\nDDoS attacks\u2011HOIC 100 100 100 100 0 100 DoS Attacks\u2011GoldenEye 100 100 100 100 0 100\nDoS Attacks\u2011Hulk 100 100 100 100 0 100 DoS Attacks\u2011SlowHTTPTest 100 100 100 100 0 100 DoS Attacks\u2011Slowlories 100 100 100 100 0 100\nFTP\u2011BrutForce 100 100 100 100 0 100 Infiltration 100 100 100 100 0 100 SSH\u2011BruteForrce 100 100 100 100 0 100\nSensors 2023, 23, 8959 14 of 19\nC. Support Vector Machine (SVM)\nAlthough SoftMax achieved an accuracy of 100%, SoftMaxwas replacedwith an SVM for the following reasons. Firstly, an SVM is more versatile than SoftMax in terms of its generalization capability. Secondly, an SVM provides better interpretability than deep learning methods like SoftMax. This is because an SVM takes into account the distribution of the data and probabilities, and also shows how the decision boundary divides the re\u2011 gions, making it easier to interpret the results. The SVM has been applied on 100,000 data sizes with 30 and 60 features. In terms of applying SVM on 100,000 rows with 30 features, Figure 11A shows a scat\u2011 tered matrix where the diagonal represents the probability distribution of each class in different projections on the dimensionality. The table includes 100 images that illustrate the scattered data and their projection in different dimensions. The columns and rows of the table represent the dimensionality of the data, ranging from 1 to 10. The 100 im\u2011 ages show the scattered data and their projection for each combination of columns and rows. The probability distribution function of the data shows that there is at least one class that is well\u2011defined and has its highest value in one of the dimensions. The scatter matrix that represents the data is symmetric, with the upper triangle being a mirror image of the lower triangle. As a result, all of the images in the table are mirror images of each other. The legend in the table provides the class tags that correspond to the data points in the scatter matrix. Figure 11B shows the projection of the data on Feature 1 and Feature 2 in dimension\u2011 ality. It shows the regions created by SVM projection in two dimensions. These regions are divided into contours to classify the classes. The contours in the figure distribute the probabilities of the maximum posterior, which range from 0.2 to 0.8, as shown on the right side. The colors in the figure represent the probability range and divide the regions into different probabilities to determine the various classes. Additionally, the figure illustrates that there is interference between the classes because it represents ten dimensions in a two\u2011 dimensional projection. However, combining the contour image in Figure 11B with the 100 scattered images in Figure 11A reveals that projecting data in different dimensions can help identify one or two classes. Figure 11A contains 100 images, each representing the pro\u2011 jection of different features for each face of the 10 dimensions. Figure 11C demonstrates the projection of the data onto Feature 3 and Feature 4 in two dimensions.\nSensors 2023, 23, x FOR PEER REVIEW 15 of 20\nInfiltration 100 100 100 100 0 100 SSHBruteForrce 100 100 100 100 0 100\nA. Support Vector Machine (SVM) Although SoftMax achieved an accuracy of 100%, SoftMax was replaced with an SVM\nfor the following reasons. Firstly, an SVM is more versatile than SoftMax in terms of its generalization capability. Secondly, an SVM provi es better int rpretability than deep learning m thods lik SoftMax. This is because an SVM takes into acc unt th distribution of the data and probabilities, and also shows how the decision boundary divides the regions, making it easier to interpret the results. The SVM has been applied on 100,000 data sizes with 30 and 60 features.\nIn terms of applying SVM on 100,000 rows with 30 features, Figure 11A shows a scattered mat ix wh r the diagonal repr ents the prob bility distribu ion of each class in different projections on the dimensionality. The table includes 100 images that illustrate the scattered data and their projection in different dimensions. The columns and rows of the table represent the dimensionality of the data, ranging from 1 to 10. The 100 images show the scattered data and their projection for each combination of columns and rows. The probability distribution function of the data shows that there is at least one class that is well-defined and has its highest valu in one of the dimensions. The scatter matrix that represents the data is symmetric, with the upper triangle being a mirror image of the lower triangle. As a result, all of the images in the table are mirror images of each other. The legend in the table provides the class tags that correspond to the data points in the scatter matrix.\nSensors 2023, 23, 8959 15 of 19\nSensors 2023, 23, x FOR PEER REVIEW 16 of 20\nFigure 11B shows the projection of the data on Feature 1 and Feature 2 in dimensionality. It shows the regions created by SVM projection in two dimensions. These regions are divided into contours to classify the classes. The contours in the figure distribute the probabilities of the maximum posterior, which range from 0.2 to 0.8, as shown on the right side. The colors in the figure represent the probability range and divide the regions into different probabilities to determine the various classes. Additionally, the figure illustrates that there is interference between the classes because it represents ten dimensions in a two-dimensional projection. However, combining the contour image in Figure 11B with the 100 scattered images in Figure 11A reveals that projecting data in different dimensions can help identify one or two classes. Figure 11A contains 100 images, each representing the projection of different features for each face of the 10 dimensions. Figure 11C demonstrates the projection of the data onto Feature 3 and Feature 4 in two dimensions.\nIn terms of applying the SVM on 100,000 rows with 60 features, Figure 12A shows a scattered matrix where the diagonal represents the probability distribution of each class in different projections on the dimensionality. The table includes 100 images that illustrate the scattered data and their projection in different dimensions. The columns and rows of the table represent the dimensionality of the data, ranging from 1 to 10. The 100 images show the scattered data and their projection for each combination of column and row. The probability distribution function of the data shows that there is at least one class that is well-defined and has its highest value in one of the dimensions. The scatter matrix that\nIn terms of applying the SVM on 100,000 rows with 60 features, Figure 12A shows a scattered matrix where the diagonal represents the probability distribution of each class in different projections on the dimensionality. The table includes 100 images that illustrate the scattered data and their projection in different dimensions. The columns and rows of the table represent the dimen ionality of the d ta, ranging from 1 to 10. The 100 im\u2011 ages sh w the scattered data and their projecti n for each combination of column and row. The probability distribution function of the data shows that there is at least one class that is well\u2011defined and has its highest value in one of the dimensions. The scatter matrix that represents the data is symmetric, with the upper triangle being a mirror image of the lower triangle. As a result, all of the images in the table are mirror images of each other. The legend in the table provides the class tags that correspond to the data points in the scatter matr x.\nSensors 2023, 23, 8959 16 of 19 Sensors 2023, 23, x FOR PEER REVIEW 17 of 20\nSensors 2023, 23, 8959 17 of 19\nFigure 12B shows the projection of the data on Feature 1 and Feature 2 in dimension\u2011 ality. It shows the regions created by SVM projection in two dimensions. These regions are divided into contours to classify the classes. The contours in the figure distribute the probabilities of the maximum posterior, which range from 0.2 to 0.8, as shown on the right side. The colors in the figure represent the probability range and divide the regions into different probabilities to determine the various classes. Additionally, the figure illustrates that there is interference between the classes because it represents ten dimensions in a two\u2011 dimensional projection. However, combining the contour image in Figure 12B with the 100 scattered images in Figure 12A reveals that projecting data in different dimensions can help identify one or two classes. Figure 12A contains 100 images, each representing the pro\u2011 jection of different features for each face of the 10 dimensions. Figure 12C demonstrates the projection of the data onto Feature 3 and Feature 4 in two dimensions. The Table 6 is comparing the accuracy of the proposed model with the state of art approaches."
        },
        {
            "heading": "6. Conclusions",
            "text": "A deep learning\u2011based intrusion detection system using a support vector machine (SVM) was implemented for social media. The proposed model is capable of detecting abnormal behavior in the network and classifying the type of traffic between 10 differ\u2011 ent cases. During the data analysis and preprocessing of the dataset, the number of in\u2011 put features to the model was significantly reduced without compromising performance. The proposed model achieves a classification accuracy of 100% in multi\u2011class classification problems. In future work, the model will be tested on another dataset to test the validity of the proposedmodel. Moreover, the model complexity will be further reduced by reducing the number of convolutional layers while maintaining good performance, which will also reduce training time for future experiments.\nAuthor Contributions: Conceptualization, L.N. andK.M.A.; methodology; L.N. andK.M.A. for anal\u2011 ysis, L.N. and K.M.A.; writing original draft preparation, K.M.A. and A.A.\u2011S.; writing\u2014review and editing, L.N. and A.A.\u2011S.; supervision, L.N. All authors have read and agreed to the published ver\u2011 sion of the manuscript.\nFunding: The authors acknowledge the Deanship of Scientific Research, Vice Presidency for Gradu\u2011 ate Studies and Scientific Research, King Faisal University, Saudi Arabia (GRANT 4346). The authors extend their appreciation for the financial support that has made this study possible.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not Applicable.\nData Availability Statement: No new data were created or analyzed in this study. Data sharing is not applicable to this article.\nConflicts of Interest: The authors declare no conflict of interest.\nSensors 2023, 23, 8959 18 of 19\nReferences 1. Abuali, K.M.; Nissirat, L.; Al\u2011Samawi, A. Intrusion Detection Techniques in Social Media Cloud: Review and Future Directions. Wirel. Commun. Mob. Comput. 2023, 2023, 6687023. [CrossRef] 2. Kumar, S.A.P.; Kumar, A.; Srinivasan, S. Statistical based intrusion detection framework using six sigma technique. IJCSNS Int. J. Comput. Sci. Netw. Secur. 2007, 7, 333. 3. Om, H.; Hazra, T. Statistical techniques in anomaly intrusion detection system. Int. J. Adv. Eng. Technol. 2012, 5, 387. 4. Azad, C.; Jha, V.K. Data mining\u2011based hybrid intrusion detection system. Indian J. Sci. Technol. 2014, 7, 781. [CrossRef] 5. Jha, J.; Ragha, L. Intrusion detection system using support vector machine. Int. J. Appl. Inf. Syst. IJAIS 2013, 3, 25\u201330. 6. Li, J.; Zhao, Z.; Li, R.; Zhang, H. Ai\u2011based two\u2011stage intrusion detection for software defined iot networks. IEEE Internet Things J. 2018, 6, 2093\u20132102. [CrossRef] 7. Patgiri, R.; Varshney, U.; Akutota, T.; Kunde, R. An investigation on intrusion detection system using machine learning. In\nProceedings of the 2018 IEEE Symposium Series on Computational Intelligence (SSCI), Bangalore, India, 18\u201321 November 2018; pp. 1684\u20131691.\n8. Ravale, U.; Marathe, N.; Padiya, P. Feature selection based hybrid anomaly intrusion detection system using K means and RBF kernel function. Procedia Comput. Sci. 2015, 45, 428\u2013435. [CrossRef] 9. Sistla, V.P.K.; Kolli, V.K.K.; Voggu, L.K.; Bhavanam, R.; Vallabhasoyula, S. Predictive Model for Network Intrusion Detection System Using Deep Learning. Rev. D\u2019intelligence Artif. 2020, 34, 323\u2013330. [CrossRef] 10. Kim, J.; Shin, N.; Jo, S.Y.; Kim, S.H. Method of intrusion detection using deep neural network. In Proceedings of the 2017 IEEE International Conference on Big Data and Smart Computing (BigComp), Jeju, Republic of Korea, 13\u201316 February 2017; pp. 313\u2013316. 11. Nguyen, S.N.; Nguyen, V.Q.; Choi, J.; Kim, K. Design and implementation of intrusion detection system using convolutional neural network for DoS detection. In Proceedings of the 2nd International Conference onMachine Learning and Soft Computing, New York, NY, USA, 2\u20134 February 2018; pp. 34\u201338. 12. Wang, H.; Cao, Z.; Hong, B. A network intrusion detection system based on convolutional neural network. J. Intell. Fuzzy Syst. 2020, 38, 7623\u20137637. [CrossRef] 13. Toupas, P.; Chamou, D.; Giannoutakis, K.M.; Drosou, A.; Tzovaras, D. An intrusion detection system for multi\u2011class classifica\u2011 tion based on deep neural networks. In Proceedings of the 2019 18th IEEE International Conference on Machine Learning and Applications (ICMLA), Boca Raton, FL, USA, 16\u201319 December 2019; pp. 1253\u20131258. 14. Liu, P. An intrusion detection system based on convolutional neural network. In Proceedings of the 2019 11th International Conference on Computer and Automation Engineering, New York, NY, USA, 23\u201325 February 2019; pp. 62\u201367. 15. Chen, L.; Kuang, X.; Xu, A.; Suo, S.; Yang, Y. A novel network intrusion detection system based on CNN. In Proceedings of the 2020 Eighth International Conference on Advanced Cloud and Big Data (CBD), Taiyuan, China, 5\u20136 December 2020; pp. 243\u2013247. 16. Mohammadpour, L.; Ling, T.C.; Liew, C.S.; Chong, C.Y. A convolutional neural network for network intrusion detection system. Proc. Asia\u2011Pac. Adv. Netw. 2018, 46, 50\u201355. 17. Kim, J.; Shin, Y.; Choi, E. An intrusion detection model based on a convolutional neural network. J. Multimed. Inf. Syst. 2019, 6, 165\u2013172. [CrossRef] 18. Maseer, Z.K.; Yusof, R.; Bahaman, N.; Mostafa, S.A.; Foozy, C.F.M. Benchmarking of machine learning for anomaly based intrusion detection systems in the CICIDS2017 dataset. IEEE Access 2021, 9, 22351\u201322370. [CrossRef] 19. Ho, S.; Al Jufout, S.; Dajani, K.; Mozumdar, M. A novel intrusion detection model for detecting known and innovative cyberat\u2011 tacks using convolutional neural network. IEEE Open J. Comput. Soc. 2021, 2, 14\u201325. [CrossRef] 20. Kilichev, D.; Kim,W. Hyperparameter Optimization for 1D\u2011CNN\u2011Based Network Intrusion Detection Using GA and PSO.Math\u2011 ematics 2023, 11, 3724. [CrossRef] 21. Lilhore, U.K.; Manoharan, P.; Simaiya, S.; Alroobaea, R.; Alsafyani, M.; Baqasah, A.M.; Dalal, S.; Sharma, A.; Raahemifar, K. HIDM: Hybrid Intrusion Detection Model for Industry 4.0 Networks Using an Optimized CNN\u2011LSTM with Transfer Learning. Sensors 2023, 23, 7856. [CrossRef] [PubMed] 22. Ore\u0161ki, D.; Andro\u010dec, D. Genetic algorithm and artificial neural network for network forensic analytics. In Proceedings of the 2020 43rd International Convention on Information, Communication and Electronic Technology (MIPRO), Opatija, Croatia, 28 September\u20132 October 2020; pp. 1200\u20131205. 23. Farhan, R.I.; Maolood, A.T.; Hassan, N.F. Optimized deep learningwith binary PSO for intrusion detection on CSE\u2011CIC\u2011IDS2018 dataset. J. Al\u2011Qadisiyah Comput. Sci. Math. 2020, 12, 16. [CrossRef] 24. Leevy, J.L.; Khoshgoftaar, T.M. A survey and analysis of intrusion detection models based on cse\u2011cic\u2011ids2018 big data. J. Big Data 2020, 7, 104. [CrossRef] 25. Fatima, A.; Nazir, N.; Khan, M.G. Data cleaning in data warehouse: A survey of data pre\u2011processing techniques and tools. Int. J. Inf. Technol. Comput. Sci. 2017, 9, 50\u201361. [CrossRef] 26. Sorzano, C.O.S.; Vargas, J.; Montano, A.P. A survey of dimensionality reduction techniques. arXiv 2014, arXiv:1403.2877. 27. Joseph, V.R. Optimal ratio for data splitting. Stat. Anal. Data Min. ASA Data Sci. J. 2022, 15, 531\u2013538. [CrossRef] 28. Ayachi, R.; Afif, M.; Said, Y.; Atri, M. Strided convolution instead of max pooling for memory efficiency of convolutional neural\nnetworks. In Proceedings of the 8th International Conference on Sciences of Electronics, Technologies of Information and Telecommunica\u2011 tions (SETIT\u201918); Springer International Publishing: Berlin/Heidelberg, Germany, 2020; Volume 1, pp. 234\u2013243.\nSensors 2023, 23, 8959 19 of 19\n29. Wang, Z.; Li, M.; Wang, H.; Jiang, H.; Yao, Y.; Zhang, H.; Xin, J. Breast cancer detection using extreme learning machine based on feature fusion with CNN deep features. IEEE Access 2019, 7, 105146\u2013105158. [CrossRef] 30. Shih\u2011Cheng, H.; Anuj, P.; Saeed, S.; Imon, B.; Lungren, M.P. Fusion of medical imaging and electronic health records using deep learning: A systematic review and implementation guidelines. NPJ Digit. Med. 2020, 3, 136. 31. Sabzekar, M.; GhasemiGol, M.; Naghibzadeh, M.; Yazdi, H.S. Improved DAG SVM: A New Method for Multi\u2011Class SVM Clas\u2011 sification. In Proceedings of the International Conference on Artificial Intelligence IC\u2011AI, Las Vegas, NV, USA, 12\u201315 July 2009; pp. 548\u2013553. 32. Liu, L.; Wang, P.; Lin, J.; Liu, L. Intrusion detection of imbalanced network traffic based on machine learning and deep learning. IEEE Access 2020, 9, 7550\u20137563. [CrossRef] 33. Rizvi, S.; Scanlon, M.; McGibney, J.; Sheppard, J. Deep learning based network intrusion detection system for resource\u2011 constrained environments. In Proceedings of the International Conference on Digital Forensics and Cyber Crime, Boston, MA, USA, 6\u201318 November 2022; Springer Nature: Cham, Switzerland; pp. 355\u2013367. 34. Hagar, A.A.; Gawali, B.W. Apache Spark and Deep Learning Models for High\u2011Performance Network Intrusion Detection Using CSE\u2011CIC\u2011IDS2018. Comput. Intell. Neurosci. 2022, 2022, 3131153. [CrossRef] [PubMed]\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual au\u2011 thor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Advancing Network Security with AI: SVM\u2010Based Deep Learning for Intrusion Detection",
    "year": 2023
}