{
    "abstractText": "We investigate a multi-agent decision-making problem where a large population of agents is responsible for carrying out a set of assigned tasks. The amount of jobs in each task varies over time governed by a dynamical system model. Each agent needs to select one of the available strategies to take on one or more tasks. Since each strategy allows an agent to perform multiple tasks at a time, possibly at distinct rates, the strategy selection of the agents needs to be coordinated. We formulate the problem using the population game formalism and refer to it as the task allocation game. We discuss the design of a decision-making model that incentivizes the agents to coordinate in the strategy selection process. As key contributions, we propose a method to find a payoffdriven decision-making model, and discuss how the model allows the strategy selection of the agents to be responsive to the amount of remaining jobs in each task while asymptotically attaining the optimal strategies. Leveraging analytical tools from feedback control theory, we derive technical conditions that the model needs to satisfy, which are used to construct a numerical approach to compute the model. We validate our solution through simulations to highlight how the proposed approach coordinates the agents in task allocation games.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shinkyu Park"
        },
        {
            "affiliations": [],
            "name": "Julian Barreiro-Gomez"
        }
    ],
    "id": "SP:43aaa45978fac6c6a0061f5955638b820ea7c1e9",
    "references": [
        {
            "authors": [
                "S. Park",
                "Y.D. Zhong",
                "N.E. Leonard"
            ],
            "title": "Multi-robot task allocation games in dynamically changing environments",
            "venue": "2021 IEEE International Conference on Robotics and Automation (ICRA), 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Parker"
            ],
            "title": "Alliance: an architecture for fault tolerant multirobot cooperation",
            "venue": "IEEE Transactions on Robotics and Automation, vol. 14, no. 2, pp. 220\u2013240, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "C. Yang",
                "L. Kaplan",
                "E. Blasch",
                "M. Bakich"
            ],
            "title": "Optimal placement of heterogeneous sensors for targets with Gaussian priors",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems, vol. 49, no. 3, pp. 1637\u20131653, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "L. Parker",
                "F. Tang"
            ],
            "title": "Building multirobot coalitions through automated task solution synthesis",
            "venue": "Proceedings of the IEEE, vol. 94, no. 7, pp. 1289\u20131305, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "I. Jang",
                "H.-S. Shin",
                "A. Tsourdos"
            ],
            "title": "Anonymous hedonic game for task allocation in a large-scale multiple agent system",
            "venue": "IEEE Transactions on Robotics, vol. 34, no. 6, pp. 1534\u20131548, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Saad",
                "Z. Han",
                "T. Basar",
                "M. Debbah",
                "A. Hjorungnes"
            ],
            "title": "Hedonic coalition formation for distributed task allocation among wireless agents",
            "venue": "IEEE Transactions on Mobile Computing, vol. 10, no. 9, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "H.-L. Choi",
                "L. Brunet",
                "J.P. How"
            ],
            "title": "Consensus-based decentralized auctions for robust task allocation",
            "venue": "IEEE Transactions on Robotics, vol. 25, no. 4, pp. 912\u2013926, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "S. Amaya",
                "A. Mateus"
            ],
            "title": "Tasks allocation for rescue robotics: A replicator dynamics approach",
            "venue": "Artificial Intelligence and Soft Computing, L. Rutkowski, R. Scherer, M. Korytkowski, W. Pedrycz, R. Tadeusiewicz, and J. M. Zurada, Eds. Cham: Springer International Publishing, 2019, pp. 609\u2013621.",
            "year": 2019
        },
        {
            "authors": [
                "S. Berman",
                "A. Halasz",
                "M.A. Hsieh",
                "V. Kumar"
            ],
            "title": "Optimized stochastic policies for task allocation in swarms of robots",
            "venue": "IEEE Transactions on Robotics, vol. 25, no. 4, pp. 927\u2013937, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "A. Pashaie",
                "L. Pavel",
                "C.J. Damaren"
            ],
            "title": "A population game approach for dynamic resource allocation problems",
            "venue": "International Journal of Control, vol. 90, no. 9, pp. 1957\u20131972, 2017.",
            "year": 1957
        },
        {
            "authors": [
                "J.R. Marden"
            ],
            "title": "State based potential games",
            "venue": "Automatica, vol. 48, no. 12, pp. 3075\u20133088, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "N. Li",
                "J.R. Marden"
            ],
            "title": "Designing games for distributed optimization",
            "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 2, pp. 230\u2013242, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "W.H. Sandholm"
            ],
            "title": "Population Games and Evolutionary Dynamics",
            "year": 2011
        },
        {
            "authors": [
                "E. Ramirez-Llanos",
                "N. Quijano"
            ],
            "title": "A population dynamics approach for the water distribution problem",
            "venue": "International Journal of Control, vol. 83, pp. 1947\u20131964, 2010.",
            "year": 1947
        },
        {
            "authors": [
                "M.J. Fox",
                "J.S. Shamma"
            ],
            "title": "Population games, stable games, and passivity",
            "venue": "Games, vol. 4, pp. 561\u2013583, Oct. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J. Barreiro-Gomez",
                "C. Ocampo-Martinez",
                "N. Quijano",
                "J.M. Maestre"
            ],
            "title": "Non-centralized control for flow-based distribution networks: A game-theoretical insight",
            "venue": "Journal of The Franklin Institute, vol. 354, pp. 5771\u20135796, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Park",
                "N.C. Martins",
                "J.S. Shamma"
            ],
            "title": "From population games to payoff dynamics models: A passivity-based approach",
            "venue": "2019 IEEE 58th Conference on Decision and Control (CDC), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Park",
                "J.S. Shamma",
                "N.C. Martins"
            ],
            "title": "Passivity and evolutionary game dynamics",
            "venue": "2018 IEEE Conference on Decision and Control (CDC), 2018, pp. 3553\u20133560.",
            "year": 2018
        },
        {
            "authors": [
                "M. Arcak",
                "N.C. Martins"
            ],
            "title": "Dissipativity tools for convergence to Nash equilibria in population games",
            "venue": "IEEE Transactions on Control of Network Systems, vol. 8, no. 1, pp. 39\u201350, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Quijano",
                "C. Ocampo-Martinez",
                "J. Barreiro-Gomez",
                "G. Obando",
                "A. Pantoja",
                "E. Mojica-Nava"
            ],
            "title": "The role of population games and evolutionary dynamics in distributed control systems: The advantages of evolutionary game theory",
            "venue": "IEEE Control Systems Magazine, vol. 37, no. 1, pp. 70\u201397, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Park",
                "N.E. Leonard"
            ],
            "title": "KL divergence regularized learning model for multi-agent decision making",
            "venue": "2021 American Control Conference (ACC), 2021, pp. 4509\u20134514.",
            "year": 2021
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Learning with delayed payoffs in population games using Kullback-Leibler divergence regularization",
            "venue": "arXiv.org, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "H.K. Khalil"
            ],
            "title": "Nonlinear Systems; 3rd ed",
            "venue": "Upper Saddle River, NJ: Prentice-Hall,",
            "year": 2002
        }
    ],
    "sections": [
        {
            "text": "As key contributions, we propose a method to find a payoffdriven decision-making model, and discuss how the model allows the strategy selection of the agents to be responsive to the amount of remaining jobs in each task while asymptotically attaining the optimal strategies. Leveraging analytical tools from feedback control theory, we derive technical conditions that the model needs to satisfy, which are used to construct a numerical approach to compute the model. We validate our solution through simulations to highlight how the proposed approach coordinates the agents in task allocation games.\nI. INTRODUCTION\nWe investigate task allocation games to study coordination in repeated strategic interactions in a large population of agents. Consider that there is a finite number of tasks to be carried out by the agents. We quantify the amount of jobs remaining in each task with a positive variable, and every agent can select one of the available strategies at a time to take on one or more tasks. The main objective is to design a decentralized decision-making model that allows the agents to coordinate and minimize remaining jobs in all tasks.\nTask allocation games are relevant to engineering applications. For instance, in multi-robot resource retrieval [1], [2], a team of multiple robots is tasked with searching and collecting target resources across partitioned areas in a given environment. Each task can be defined as collecting resources from an area and the strategy selection refers to taking one of the tasks. In target tracking applications [3], a group of mobile units with heterogeneous sensing capabilities are deployed to collect data about the states of multiple targets of our interests. Based on the type of equipped sensors, each\nPark\u2019s work was supported by funding from King Abdullah University of Science and Technology (KAUST). Barreiro-Gomez\u2019s work was supported by the Center on Stability, Instability, and Turbulence (SITE) and Tamkeen under the NYU Abu Dhabi Research Institute grant CG002.\nPark is with the Electrical and Computer Engineering, King Abdullah University of Science and Technology (KAUST), Thuwal 23955, Saudi Arabia. shinkyu.park@kaust.edu.sa\nBarreiro-Gomez is with NYUAD Research Institute, New York University Abu Dhabi, PO Box 129188, Abu Dhabi, United Arab Emirates. jbarreiro@nyu.edu\nmobile unit can collect different sets of data on the targets\u2019 states. A task is defined as collecting data on a portion of the target states and a strategy specifies which pair of available sensors a mobile unit can equip. In both scenarios, the amount of resources to collect and the data to gather vary depending on past strategy selection of the agents and also on environmental changes and target dynamics.\nTo design a model for the agent strategy selection in such engineering applications, we investigate task allocation in dynamically changing environments. Multi-agent task allocation problems have been widely studied across various research communities [4]\u2013[10]. A game-theoretic approach to the problem using replicator dynamics is investigated in [8]. The authors of [5], [6] use the hedonic game to study the coordination of multiple agents in task allocation. Applications of population game approaches to address task allocation in swarm robotics [9] and the control of a water distribution system [10] are discussed. Also, relevant to the task allocation game that we investigate in this work, whose formalism is defined in a state space, the state-based potential game has been studied in [11], and the design of state-based game to solve distributed optimization is proposed in [12].\nA majority of existing works assume that the environment underlying the game is static and aim to find the optimal task allocation. In contrast, we study the design of a decisionmaking model under which the agents can repeatedly switch among multiple tasks to minimize remaining jobs in the tasks. We adopt the population game formalism [13] to state the problem and to study the decision-making model design. The model prescribes how the agents take on a given set of tasks and how the agents should switch among the tasks by revising their strategy selection to asymptotically attain optimality. We consider that each agent in the population is given a set of n strategies to carry out assigned tasks where we denote the agents\u2019 strategy profile \u2013 the distribution of the agents\u2019 strategy selection \u2013 by a non-negative vector x = (x1, \u00b7 \u00b7 \u00b7 , xn). Remaining jobs associated with m tasks are denoted by a non-negative vector q = (q1, \u00b7 \u00b7 \u00b7 , qm) for which a dynamic model describes how q changes \u2013 both growth by environmental changes and reduction by the agents \u2013 based on the agents\u2019 strategy selection x.\nBased on the evolutionary dynamics framework [13], we specify a decentralized decision-making model that allows individual agents to revise their strategy selection based on a payoff vector p = (p1, \u00b7 \u00b7 \u00b7 , pn), where each pi is the payoff an agent receives when it selects the i-th strategy. As the main contribution, we design a payoff mechanism to define how p should depend on q to encourage the\nar X\niv :2\n30 6.\n02 27\n8v 3\n[ ee\nss .S\nY ]\n1 8\nSe p\n20 23\nagents to select the tasks with more jobs to perform and asymptotically attain the minimum of a given cost c(q). Applying convergence analysis tools [14]\u2013[19] that are based on passivity theory in the population games literature, we establish conditions under which the agents\u2019 strategy profile converges and asymptotically attain the optimal profile. We use the conditions to compute the payoff mechanism.\nThe paper is organized as follows. In Section II, we explain the task allocation game formulation and the main problem we address in this paper. In Section III, we present the main result on the payoff mechanism design and analysis on convergence of the agent strategy revision process to the optimal strategy profile. In Section IV, we present simulation results to illustrate our main contribution. We conclude the paper with a summary and future plans in Section V."
        },
        {
            "heading": "II. PROBLEM DESCRIPTION",
            "text": "Consider a large population of agents that are assigned with m tasks and are given n strategies to carry out the tasks.1 We associate each task j \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} with a variable qj \u2265 0 which quantifies the amount of jobs remaining in the task. Let xi \u2265 0 denote the portion of the agents selecting strategy i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} and a fixed positive number M to be the mass of the agent population satisfying M = \u2211n i=1 xi. 2 Each agent selects one of the strategies at a time based on payoff vector p = (p1, \u00b7 \u00b7 \u00b7 , pn).\nLet Rn+ be the set of all n-dimensional vectors with nonnegative entries, and let XM be the space of all feasible states x = (x1, \u00b7 \u00b7 \u00b7 , xn) of the population defined as XM ={ x \u2208 Rn+ \u2223\u2223 \u2211n i=1 xi = M } . Given a matrix G \u2208 Rn\u00d7m, we represent G using its column and row vectors as follows:\nG = ( Gcol1 \u00b7 \u00b7 \u00b7 Gcolm ) = G row 1 ...\nGrown  . (1)"
        },
        {
            "heading": "A. Task Allocation Games",
            "text": "To investigate the task allocation problem, we formalize the problem as a large population game in which the agents select strategies to perform jobs in the assigned tasks quantified by q = (q1, \u00b7 \u00b7 \u00b7 , qm). The vector q varies over time based on the agents\u2019 strategy selection and changes in the environment. Hence, each agent needs to evaluate and adaptively select a strategy based on q.\nGiven x(t) and q(t), at each time t, the following ordinary differential equation describes the rate of change of q(t).\nq\u0307(t) = \u2212F(q(t), x(t))\ufe38 \ufe37\ufe37 \ufe38 reduction rate + w\ufe38\ufe37\ufe37\ufe38 growth rate , q(0) = q0 \u2208 Rm+ , (2)\nwhere F : Rm+ \u00d7Rn+ \u2192 Rm+ is a continuously differentiable mapping3 that defines the reduction rate, which quantifies how fast the agents adopting strategy profile x reduce q, and\n1The number of tasks is not necessarily the same as that of available strategies, i.e., m \u0338= n.\n2Considering the population state x as control input to (2), the population mass M can be interpreted as a limit on the control input.\n3To have the reduction rate mapping defined for any population mass M , we define the domain of F as Rm+ \u00d7 Rn+.\nthe constant vector w = (w1, \u00b7 \u00b7 \u00b7 , wm) \u2208 Rm+ represents the growth rate for q due to environmental changes. To ensure that the positive orthant Rm+ is forward-invariant for q(t) under (2), each Fi of F = (F1, \u00b7 \u00b7 \u00b7 ,Fm) satisfies Fi(q, x) \u2264 wi if qi = 0. For notational convenience, let us define O as the set of stationary points of (2), i.e.,\nO = {(q, x) \u2208 Rm+ \u00d7 XM | F(q, x) = w}. (3) We make the following assumption on the mapping F . Assumption 1: The reduction rate Fi for each task i depends only on its associated variable qi(t) and the agent strategy selection x(t), and increases as does qi(t). For instance, in the resource retrieval application discussed in Section I, when there is a larger volume of resources spread out across the areas, the robots would need to travel a shorter distance on average to locate and retrieve the resources and hence given a fixed strategy profile x, the variable qi(t) decreases at a faster rate. We formalize such assumptions as \u2202Fj\u2202qi (q, x) = 0 if i \u0338= j and \u2202Fi \u2202qi\n(q, x) > 0. \u25a1 According to Assumption 1, we represent the reduction rate as F(q, x) = (F1(q1, x), \u00b7 \u00b7 \u00b7 ,Fm(qm, x)), where for fixed x, each Fi is an increasing function of qi.\nRemark 1: Suppose that given x in XM , there is q in Rm+ satisfying F(q, x) = w. By Assumption 1, q is unique. \u25a1\nThe following examples illustrate how the dynamic game model (2) can be adopted in control systems applications.\nExample 1 (Multi-Robot Resource Collection [1]): Let m = n and F = (F1, \u00b7 \u00b7 \u00b7 ,Fn) be defined as\nFi(qi, xi) = Ri exp(\u03b1iqi)\u2212 1 exp(\u03b1iqi) + 1 x\u03b2ii , (4)\nwhere Ri, \u03b1i, and \u03b2i are positive constants. The parameter Ri represents the maximum reduction rate associated with strategy i, and \u03b1i and \u03b2i are coefficients specifying how the reduction rate Fi depends on qi and xi, respectively. Note that each function Fi satisfies Fi(0, x) = 0 and Assumption 1. Here, m = n and only the agent selecting strategy i can reduce qi associated with task i.\nExample 2 (Heterogeneous Sensor Scheduling [3]): We adopt the model (2) as an abstract description of how mobile units\u2019 sensor scheduling affects the uncertainty reduction in estimating states of multiple targets. Let m < n and F = (F1, \u00b7 \u00b7 \u00b7 ,Fm) be defined as\nFi(qi, x) = \u2211 j\u2208Ni Ri exp(\u03b1iqi)\u2212 1 exp(\u03b1iqi) + 1 x\u03b2ij , (5)\nwhere Ni denotes the set of the strategies (available sensor configurations of a mobile unit) that can collect data on the state of the i-th target. The parameters Ri, \u03b1i, \u03b2i have the same interpretation as in Example 1. Unlike the previous example, the strategies are defined to allow the agents to reduce multiple task-associated variables of q.\nExample 3 (Water Distribution Control [14], [16], [20]): There are m reservoirs each of which is assigned with a respective maximum water level l\u0304i for i in {1, \u00b7 \u00b7 \u00b7 ,m}. Denote by (l1(t), \u00b7 \u00b7 \u00b7 , lm(t)) water levels of the reservoirs at each time t. Let w be a constant\noutflow specifying water demands by consumers and (x1(t), \u00b7 \u00b7 \u00b7 , xn(t)) be controllable inflows, where n does not necessarily coincide with the number m of reservoirs. The simplified dynamics for the water levels can be defined as l\u0307i(t) = Fi(l\u0304i \u2212 li(t), x(t))\u2013w satisfying Fi(0, x) = 0 to ensure that each reservoir cannot hold water above its maximum level: for instance, Fi(l\u0304i \u2212 li, x) = (l\u0304i\u2212li)l\u0304i xi. By defining qi(t) = l\u0304i\u2013li(t) as remaining space in each reservoir i, we can derive the dynamic model as\nq\u0307i(t) = \u2212 1\nl\u0304i qi(t)xi(t) + w."
        },
        {
            "heading": "B. Agent Strategy Revision Model",
            "text": "Our model is based on the evolutionary dynamics framework [13] in which the strategy revision protocol \u03f1\u03b8i : Rn \u2192 R+ determines an agent\u2019s strategy revision based on the payoff vector p \u2208 Rn, where \u03b8 = (\u03b81, \u00b7 \u00b7 \u00b7 , \u03b8n) \u2208 XM is a parameter of the protocol. We adopt the Kullback-Leibler Divergence Regularized Learning (KLD-RL) protocol [21], [22] to define \u03f1\u03b8i (p) as\n\u03f1\u03b8i (p) = \u03b8i exp(\u03b7 \u22121pi)\u2211n l=1 \u03b8l exp(\u03b7 \u22121pl) , (6)\nwhere \u03b7 > 0. The protocol \u03f1\u03b8i (p) describes the probability of an agent switching to strategy i given p and \u03b8. Note that the smaller the value of \u03b7, the more the strategy revision depends on the value of p.\nEach agent is given an opportunity to revise its strategy selection at each jump time of an independent and identically distributed Poisson process, and uses the protocol to select a new strategy or keep its current strategy selection. Since the strategy revision of individual agents only depends on the payoff vector and takes place independently of each other, their decision-making is decentralized and the coordination among them occurs implicitly through their decision-making model. Based on discussions in [13, Chapter 4], as the number of agents in the population tends to infinity, the following ordinary differential equation describes how each component of x(t) = (x1(t), \u00b7 \u00b7 \u00b7 , xn(t)) evolves over time.\nx\u0307i(t) = V\u03b8i (p(t), x(t)) = \u2211n j=1 xj(t)\u03f1 \u03b8 i (p(t))\u2212 xi(t) \u2211n j=1 \u03f1 \u03b8 j (p(t)). (7)\nWe refer to (7) as the Evolutionary Dynamics Model (EDM). Note that at an equilibrium state (p\u2217, x\u2217) of the EDM (7) under the KLD-RL protocol (6), if \u03b8 = x\u2217, the following implication holds:\nx\u2217i > 0 =\u21d2 p\u2217i = max 1\u2264j\u2264n p\u2217j . (8)\nEq. (8) means that every agent receives the highest payoff at (p\u2217, x\u2217) if the parameter \u03b8 of (6) is the same as x\u2217.\nGiven the protocol \u03f1\u03b8i as in (6), we aim to design a payoff mechanism for the agents to asymptotically adopt the optimal strategy profile that minimizes a given cost c(q). For instance, in Example 1, if we design the payoff mechanism as p = q, the robots would select strategy i to take on task i and asymptotically minimize limt\u2192\u221e max1\u2264i\u2264m qi(t), as\ndiscussed in [1]. However, in many applications, such oneto-one correspondence between tasks and available strategies may not exist, and depending on the cost we want to minimize, such a simple payoff mechanism would not be the best design choice as we illustrate in Figure 1(a).\nIn addition, since the payoff mechanism depends on the vector q(t), the mechanism would incentivize the agents to take on the tasks with larger qi(t). Hence, compared to other models that directly control the population state x(t) to the optimal state x\u2217 (for instance, the model proposed in [9]), our strategy revision model is more responsive to changes of q(t) and hence reduces the task-associated variables q(t) at a faster rate as we depict in Figure 1(b).\nTwo examples of the cost function we consider are \u2022 (square of) the 2-norm of q: c(q) = \u2211m i=1 q 2 i , and\n\u2022 the \u221e-norm of q: c(q) = max1\u2264i\u2264m qi. For the payoff mechanism design, we consider a linear model defined by a matrix G \u2208 Rn\u00d7m as follows:\np = Gq. (9)\nOur main problem investigates finding the matrix G that allows the agents to asymptotically minimize the cost c(q(t)). We formally state the problem as follows.\nProblem 1: Given the dynamic model (2) of the task allocation game and the EDM (7), compute the payoff matrix G under which the cost c(q(t)) is asymptotically minimized."
        },
        {
            "heading": "III. PAYOFF MATRIX DESIGN",
            "text": "By interconnecting the dynamic model of the game (2), the payoff mechanism (9), and the EDM (7) with (6) as its revision protocol, as illustrated in Figure 2, we can write the\nstate equation of the resulting closed-loop model as follows:\n{ q\u0307(t) = \u2212F(q(t), x(t)) + w p(t) = Gq(t)\n(10a)\nx\u0307i(t) = M \u03b8i exp(\u03b7 \u22121pi(t))\u2211n l=1 \u03b8l exp(\u03b7 \u22121pl(t)) \u2212 xi(t). (10b)\nGiven an initial condition (q(0),x(0))\u2208Rm+\u00d7XM , we assume the closed-loop model (10) has a unique solution. Let S be the set of equilibrium states of (10). The proper design of G should ensure that the following two conditions hold. (R1) The state (q(t), x(t)) converges to the stationary points\nof (10a), i.e., it holds that limt\u2192\u221e inf(r,z)\u2208O(\u2225q(t) \u2212 r\u22252 + \u2225x(t)\u2212 z\u22252) = 0.\n(R2) When the closed-loop model (10) reaches an equilibrium state (q\u2217, x\u2217), it attains the minimum cost, i.e., c(q\u2217) = inf(q,x)\u2208O c(q).\nWe adopt passivity tools [15], [17] to find technical conditions under which (R1) and (R2) are attained and use the conditions to design the payoff matrix G. The critical step in the convergence analysis (R1) is in establishing passivity for both (10a) and (10b) by finding a so-called \u03b4-storage function for (10a) and \u03b4-antistorage function for (10b).4 Then, by constructing a Lyapunov function using the two storage functions, we establish convergence results for (10).\nTo proceed, by [22, Lemma 3], (10b) is \u03b4-passive and has the \u03b4-storage function S\u03b8 : Rn \u00d7 XM \u2192 R+ given by\nS\u03b8(p, x)= max z\u2208XM (pT z\u2212\u03b7D(z||\u03b8))\u2212(pTx\u2212\u03b7D(x||\u03b8)), (11)\nwhere D(\u00b7\u2225\u00b7) is the KL divergence. Note that S\u03b8 satisfies S\u03b8(p, x)=0 \u21d4 V\u03b8(p, x)=0 \u21d4 \u2207TxS\u03b8(p, x)V\u03b8(p, x)=0\n(12a)\nS\u03b8(p(t), x(t))\u2212 S\u03b8 (p(t0), x(t0)) \u2264 \u222b t t0 p\u0307T (\u03c4)x\u0307(\u03c4) d\u03c4, \u2200t \u2265 t0 \u2265 0 (12b) for any payoff vector trajectory p(t), t \u2265 0. The mapping V\u03b8 = (V\u03b81 , \u00b7 \u00b7 \u00b7 ,V\u03b8n) is the vector field of the EDM (7).\nThe dynamic game model (2) is qualified as \u03b4-antipassive [17] if there is a \u03b4-antistorage function L : Rm+ \u00d7XM \u2192 R+ satisfying the following two conditions:\nL(q, x) = 0 \u21d4 F(q, x) = w \u21d4 \u2207Tq L(q, x)(F(q, x)\u2212 w) = 0 (13a)\nL(q(t), x(t))\u2212 L (q(t0), x(t0)) \u2264 \u2212 \u222b t t0 q\u0307T (\u03c4)GT x\u0307(\u03c4) d\u03c4, \u2200t \u2265 t0 \u2265 0, (13b)\nwhere (13b) needs to hold for any given population state trajectory x(t), t \u2265 0. According to (13a), the function L(q, x) can be used to measure how far the state (q, x) is\n4We refer to [17, Definition 10] and [17, Definition 12], respectively, for the formal definitions of passivity for (10a) and (10b).\nfrom the equilibrium of (10a). By their respective definitions [17], both S\u03b8 and L need to be continuously differentiable.\nRecall O given as in (3). For (q\u2217, x\u2217) \u2208 O satisfying x\u2217i > 0 =\u21d2 p\u2217i = max\n1\u2264j\u2264n p\u2217j , \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 , n} (14)\nwith p\u2217 = Gq\u2217, let us assign \u03b8 = x\u2217 for (10b). We can establish the following lemma.\nLemma 1: If the dynamic game model (10a) is \u03b4antipassive, then given that q(t), t \u2265 0 is bounded, the state (q(t), x(t)) of the closed-loop model (10) converges to S. Also (q\u2217, x\u2217) is the equilibrium state of (10) for all \u03b7 > 0.\nThe proof of the lemma is given in Appendix. Resorting to Lemma 1, to meet the requirements (R1) and (R2), we need to construct the payoff matrix G such a way that (10a) becomes \u03b4-antipassive and (q\u2217, x\u2217) \u2208 O minimizing c(q) is an equilibrium state of (10). The following theorem states the technical conditions on G that ensure (R1) and (R2). To state the theorem, we define a continuously differentiable mapping g : Rm+ \u2192 Rn+ that maps any q \u2208 Rm+ to y = g(q) satisfying F(q, y) = w.5 The statement of the theorem holds if such g exists.\nTheorem 1: Let us define\nhi(q, x) = (Fi(qi, x)\u2212 wi) (x\u2212 g(q)), i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} and let (q\u2217, x\u2217) be the stationary point of (10a) attaining the minimum cost inf(q,x)\u2208O c(q). Suppose the matrix G satisfies\nG\u2207xF(q, x) = \u2207TxF(q, x)GT , \u2200(q, x) \u2208 Rm+ \u00d7 Rn+ (15a) hTi (q, x)G col i > 0, \u2200(q, x) /\u2208 O, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m} (15b) (Growi \u2212Growj )x\u2217i q\u2217 \u2265 0, \u2200i, j \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, (15c)\nwhere Gcoli and G row i are the column and row vectors of G defined as in (1), respectively. The dynamic game model (10a) is \u03b4-antipassive and (q\u2217, x\u2217) is an equilibrium state of (10) with \u03b8 = x\u2217 for any \u03b7 > 0.\nThe proof of the theorem is given in Appendix. Under the condition (15b), whenever qi(t) is increasing, i.e., q\u0307i(t) = \u2212Fi(qi(t), x(t)) + wi > 0, the matrix G incentivizes the agents to revise their strategies toward g(q), which is the strategy profile required to make the rate q\u0307(t) to zero. In other words, G is designed to encourage the agents to select strategies that reduce the rate q\u0307(t).\nProposition 1: Let (q\u2217, x\u2217) be the stationary point of (10a) attaining the minimum cost inf(q,x)\u2208O c(q). Consider the closed-loop model (10) for which \u03b8 = x\u2217 and the payoff matrix G satisfies (15). As the parameter \u03b7 of (10b) increases, (q\u2217, x\u2217) becomes the unique equilibrium state of (10). In other words, it holds that lim\u03b7\u2192\u221e sup(q\u0304,x\u0304)\u2208S D(x\u0304 \u2225x\u2217) = 0, where S is the set of equilibrium states of (10).\nThe proof of the proposition is provided in Appendix. In conjunction with Lemma 1 and Theorem 1, Proposition 1 implies that as \u03b7 becomes sufficiently large, the state trajectory (q(t), x(t)), t \u2265 0 converges to near the optimal state\n5We remark that g(q) does not necessarily belong to XM . We interpret g(q) has the strategy profile that attains the equilibrium state for a given q when there is no limit on the population mass M .\n(q\u2217, x\u2217). According to (6), we note that smaller \u03b7 is desired to make the agent strategy revision responsive to changes in p(t) and also in q(t). Hence, a good practice is to use smaller \u03b7 at the beginning of the task allocation game, and if needed, as q\u0307(t) goes to zero, the agents can gradually increase the value of \u03b7 to ensure that x(t) converges to x\u2217."
        },
        {
            "heading": "IV. SIMULATIONS",
            "text": "We use Examples 1 and 2 to illustrate our main results and discuss how the cost function and parameters of the dynamic model (2) affect the payoff matrix design. In both examples, we select the following fixed parameters M = 1, Ri = 3.5, \u03b1i = 0.05, and \u03b2i = 1 for (4) and (5), and \u03b7 = 0.001 for (10b).6 We use two different cost functions c(q) for Example 1 and two distinct growth rates w for Example 2."
        },
        {
            "heading": "A. Computation of G",
            "text": "We explain the steps to compute G. First, note that (15a) is satisfied if G has the following structures:\n1) For Example 1, Gij = 0 if i \u0338= j. 2) For Example 2, Gij = 0 if i /\u2208 Nj and Gij = Gj\notherwise, where Gj is a real number. Then, we find (q\u2217, x\u2217) \u2208 O that minimizes the cost\nfunction c(q) using the following optimization.\nmin (q,x)\u2208Rm+ \u00d7XM\nc(q) subject to F(q, x) = w. (16)\nNote that since F is a nonlinear mapping, the optimization can be non-convex and the solution we find is locally optimal.\nOnce we find (q\u2217, x\u2217), we compute the matrix G satisfying (15) for which we first need to find the mapping g. Instead of explicitly finding g, we draw random samples {(qs, xs)}Ss=1 \u2282 Rm+ \u00d7XM and find ys \u2208 Rn+ that minimizes \u2225F(qs, ys)\u2212w\u222522 for each sample (qs, xs). Note that assuming \u2207xF(q, x) has full rank at (qs, ys), which is the case in both examples, the minimizer ys satisfies F(qs, ys) = w.\nAs the last step, the design of G can be formulated as the following linear programming:\nmin G\u2208Rn\u00d7m 1 (17) subject to (Fi(qs,i, xs)\u2212 wi) (xs \u2212 ys)TGcoli > 0, \u2200i \u2208 {1, \u00b7 \u00b7 \u00b7 ,m}, \u2200s \u2208 {1, \u00b7 \u00b7 \u00b7 , S}\n(Growi \u2212Growj )x\u2217i q\u2217 \u2265 0, \u2200i, j \u2208 {1, \u00b7 \u00b7 \u00b7 , n}, where qs,i is the i-th element of qs = (qs,1, \u00b7 \u00b7 \u00b7 , qs,m). Since we evaluate the condition (15b) using a finite number of sampled points {(qs, xs)}Ss=1, we would obtain an approximate solution satisfying (15) only at the sampled points. However, as the sample size S tends to infinity, the solution G is more likely to satisfy (15) over the entire state space Rm+ \u00d7 XM ."
        },
        {
            "heading": "B. Simulation results for Example 1 (m = 4, n = 4)",
            "text": "Using the methods explained in Section IV-A, we compute the optimal state (q\u2217, x\u2217) minimizing i) c(q) = \u2211m i=1 q 2 i and\n6We select \u03b7 = 0.001 as all population state trajectories in the simulations converge to the optimal x\u2217 with the small positive \u03b7.\nii) c(q) = max1\u2264i\u2264m qi, where we use the fixed growth rate w = (0.05, 0.25, 1.00, 2.00) for both cases. Then, we design the payoff matrix G using (17) as follows.\ni) For c(q) = \u2211m\ni=1 q 2 i ,\nG = 1.00 0.00 0.00 0.000.00 0.66 0.00 0.00 0.00 0.00 0.48 0.00 0.00 0.00 0.00 0.40  . ii) For c(q) = max1\u2264i\u2264m qi,\nG = 1.00 0.00 0.00 0.000.00 1.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00  . Note that when we use the \u221e-norm to define the cost c(q), the optimal design of G equally incentivizes the agents proportional to the remaining jobs q. On the other hand, when the 2-norm is used, given that the values of q1(t), \u00b7 \u00b7 \u00b7 , q4(t) are equal, the payoff matrix G assigns the highest payoff to strategy 1 and the lowest payoff to strategy 4. Recall that under the pre-selected growth rate w, task 1 has the lowest growth rate and task 4 has the highest, and hence maintaining lower q1(t) is easier \u2013 it needs a less number of agents \u2013 than q4(t). Hence, under the 2-norm cost function, the agents prioritize to carry out the tasks with lower growth rates.\nFigures 3 and 4 depict the resulting trajectories for x(t) and q(t). Notice that the population states at the equilibrium in the two cases are similar; however, the trajectories for q(t) are different and, hence, so do the costs evaluated along the trajectories as we discussed in Figure 1(a). We observe that there is a large variation in the agent strategy revision at the beginning of the simulations as the agents repeatedly switch among the strategies to reduce qi(t) with a larger value."
        },
        {
            "heading": "C. Simulation results for Example 2 (m = 4, n = 6)",
            "text": "We consider that there are 4 target states and 4 types of sensors each of which can measure a single state of the target. Each mobile unit can be equipped with two types of sensors and we define a strategy based on a pair of sensors employed on a mobile unit. According to the strategy definition, we can define the set Ni in (5) as the strategies that can measure the i-th target state: N1 = {1, 2, 3}, N2 = {1, 4, 5}, N3 = {2, 4, 6}, and N4 = {3, 5, 6}. We use the square of the 2- norm to define the cost function, i.e., c(q) = \u2211m i=1 q 2 i .\nWe design G with two distinct growth rates as follows. i) For w = (0.5, 1.0, 1.5, 2.0),\nG =  1.00 0.81 0.00 0.00 1.00 0.00 0.72 0.00 1.00 0.00 0.00 0.67 0.00 0.81 0.72 0.00 0.00 0.81 0.00 0.67 0.00 0.00 0.72 0.67  . ii) For w = (0.1, 0.5, 1.0, 2.0),\nG =  1.68 0.99 0.00 0.00 1.68 0.00 0.80 0.00 1.68 0.00 0.00 0.67 0.00 0.99 0.80 0.00 0.00 0.99 0.00 0.67 0.00 0.00 0.80 0.67  . By comparing the above two payoff matrices, we can infer that the optimal G assigns higher payoffs to the strategies as their respective growth rates become smaller. Figures 5 and 6 depict the resulting trajectories for x(t) and q(t). Notably, as the growth rate of the 4-th target state becomes relatively higher than those of other target states, more agents adopt strategies 3, 5, and 6, which can be used to measure the 4-th state. Similar to the simulation results in Section IV-B, we\ncan observe a large variation in the agent strategy revision at the beginning of the simulations as the agents responsively revise their strategies based on the value of qi(t)."
        },
        {
            "heading": "V. CONCLUSIONS",
            "text": "We investigated the design of the payoff mechanism in the task allocation games. The mechanism determines payoffs p given the vector q that quantifies the amount of jobs in the assigned tasks to the agents, and the payoffs incentivize the agents to repeatedly revise their strategy selection. We discussed how to design the payoff matrix G using the passivity tools to ensure that the agents asymptotically attain the optimal strategy profile. Using the numerical examples, we demonstrated how our results can be used to design G and how the parameters of the dynamic game model affect the optimal design of G. For future directions, we plan to consider the design of nonlinear payoff mechanisms p = G(q), and to explore the idea of learning the dynamic model and computing G alongside the model learning.\nAPPENDIX"
        },
        {
            "heading": "A. Proof of Lemma 1",
            "text": "From (12b) and (13b), we can derive\nd\ndt\n( S\u03b8(p(t), x(t)) + L(q(t), x(t)) ) = \u2207TxS\u03b8(p(t), x(t))V\u03b8(p(t), x(t))\n+\u2207Tq L(q(t), x(t))(\u2212F(q(t), x(t)) + w) \u2264 0, (18)\nwhere by (12a) and (13a), the equality holds if and only if S\u03b8(p(t), x(t)) = L(q(t), x(t)) = 0. Therefore, by LaSalle\u2019s invariance principle [23], if q(t) is bounded, then the trajectory (q(t), x(t)), t \u2265 0 converges to the equilibrium states of (10).\nBy the definition of the KLD-RL protocol [21], [22], with \u03b8 = x\u2217, (q\u2217, x\u2217) is an equilibrium state of (10) for any \u03b7 > 0 if it holds that x\u2217 \u2208 argmaxz\u2208XM (zTGq\u2217), which can be validated by (14). \u25a0"
        },
        {
            "heading": "B. Proof of Theorem 1",
            "text": "We define a \u03b4-antistorage function L : Rm+ \u00d7 XM \u2192 R+ using a line integral along a curve s from g(q) to x:\nL(q, x) = \u222b x g(q) G(F(q, s)\u2212 w) \u2022 ds. (19)\nRecall that g(q) \u2208 Rn+ is such that F(q, g(q)) = w. By evaluating the line integral along the curve given by s(\u03c4) = \u03c4x+ (1\u2212 \u03c4)g(q), \u03c4 \u2208 [0, 1], we can derive\nL(q, x) = (x\u2212 g(q))TG \u222b 1 0 (F(q, s(\u03c4))\u2212 w) d\u03c4. (20)\nNote that for every \u03c4 in (0, 1), it holds that\n(x\u2212 g(q))TG (F(q, s(\u03c4))\u2212 w)\n= 1\n\u03c4 m\u2211 i=1 (s(\u03c4)\u2212 g(q))TGcoli (Fi(qi, s(\u03c4))\u2212 wi) . (21)\nHence, under (15b), we can verify that L(q, x) \u2265 0 where the equality holds if and only if F(q, x) = w. In what follows, we show that L satisfies (13a) and (13b). To begin with, using (15a), we can establish\n\u2207xL(q, x) = G \u222b 1 0 (F(q, s(\u03c4))\u2212 w) d\u03c4\n+ \u222b 1 0 \u2207Tz F(q, z) \u2223\u2223\u2223 z=s(\u03c4) \u03c4 d\u03c4 GT (x\u2212 g(q))\n= G \u222b 1 0 d d\u03c4 \u03c4F(q, s(\u03c4)) d\u03c4 \u2212Gw = G (F(q, x)\u2212 w) . (22)\nHence, we can derive\n\u2207TxL(q, x)x\u0307 = (F(q, x)\u2212 w)TGT x\u0307 = \u2212p\u0307T x\u0307. (23)\nSimilarly, the partial derivative of L(q, x) with respect to q can be computed as\n\u2207qL(q, x) = \u2212\u2207Tq g(q) \u222b 1 0 G (F(q, s(\u03c4))\u2212 w) d\u03c4\n+ \u222b 1 0 ( \u2207Tq GF(q, s(\u03c4))\n+\u2207Tq g(q)\u2207Tz GF(q, z) \u2223\u2223\u2223 z=s(\u03c4) (1\u2212 \u03c4) ) d\u03c4 (x\u2212 g(q))\n= \u222b 1 0 \u2207Tq F(q, s(\u03c4)) d\u03c4 GT (x\u2212 g(q))\n+\u2207Tq g(q) \u222b 1 0 G ( d d\u03c4 F(q, s(\u03c4))(1\u2212 \u03c4) + w ) d\u03c4\ufe38 \ufe37\ufe37 \ufe38\n=0\n. (24)\nTherefore, we can derive\n\u2207Tq L(q, x)q\u0307 = (x\u2212 g(q))T \u222b 1 0 \u2207qGF(q, s(\u03c4)) d\u03c4 (\u2212F(q, x) + w).\nThen, the time derivative of L becomes d\ndt L(q, x) = (x\u2212 g(q))T \u00d7 \u222b 1 0 \u2207qGF(q, s(\u03c4)) d\u03c4 (\u2212F(q, x) + w)\u2212 p\u0307T x\u0307 (25)\nHence, for L to satisfy (13b), it suffices to show that the following inequality holds.\n(x\u2212g(q))T \u222b 1 0 \u2207qGF(q, s(\u03c4))d\u03c4(\u2212F(q, x)+w)\u22640. (26)\nBy Assumption 1, we can rewrite the above equation as (x\u2212 g(q))T \u222b 1 0 \u2207qGF(q, s(\u03c4)) d\u03c4 (\u2212F(q, x) + w)\n= m\u2211 i=1 \u222b 1 0 \u2202Fi \u2202qi (qi, s(\u03c4))d\u03c4(x\u2212g(q))TGcoli (\u2212Fi(qi, x)+wi),\nwhere Gcoli is the i-th column vector of G defined as in (1). Consequently, by Assumption 1, the condition (15b) ensures (26), where the equality holds if and only if (q, x) \u2208 O. Hence, in conjunction with the fact that L(q, x) = 0 \u21d4 F(q, x) = w, we can validate that (13a) holds.\nRecall that, according to (8), for (q\u2217, x\u2217) minimizing the cost function c(q) to be the equilibrium state of the closedloop model (10), it suffices to show\nx\u2217i > 0 =\u21d2 p\u2217i = max 1\u2264j\u2264n p\u2217j\n=\u21d2 Growi q\u2217 = max 1\u2264j\u2264n Growj q \u2217\n=\u21d2 (Growi \u2212Growj )q\u2217 \u2265 0 (27) holds for all i, j in {1, \u00b7 \u00b7 \u00b7 , n}, where Growi is the i-th row vector of G defined as in (1). Hence, the condition (15c) ensures that (q\u2217, x\u2217) is the equilibrium state of (10). This completes the proof. \u25a0"
        },
        {
            "heading": "C. Proof of Proposition 1",
            "text": "First of all, according to Lemma 1 and (15c), (q\u2217, x\u2217) is an equilibrium point of (10). By the definition of the KLD-RL protocol [21], [22], with \u03b8 = x\u2217, for any other equilibrium state (q\u0304, x\u0304) of (10), it holds that\n(x\u0304\u2212 x\u2217)TGq\u0304 \u2265 \u03b7D(x\u0304 \u2225x\u2217). (28) We prove the statement of the proposition by showing that for any fixed positive constant \u03f5, when \u03b7 is sufficiently large, there is no equilibrium state (q\u0304, x\u0304) of (10) satisfying D(x\u0304 \u2225x\u2217) \u2265 \u03f5.\nBy contradiction, for each \u03b7 > 0, suppose there is an equilibrium state (q\u0304, x\u0304) for which D(x\u0304 \u2225x\u2217) \u2265 \u03f5 holds. When \u03b7 is sufficiently large, for (q\u0304, x\u0304) to be an equilibrium state of (10), (x\u0304 \u2212 x\u2217)TGq\u0304 needs to be large enough to satisfy (28). Note that (x\u0304\u2212x\u2217)TGq\u0304 = \u2211mi=1(x\u0304\u2212x\u2217)TGcoli q\u0304i. Let i be an index for which (x\u0304 \u2212 x\u2217)TGcoli q\u0304i becomes arbitrarily large and so does q\u0304i as \u03b7 increases. According to (15b), when q = q\u0304 and x = x\u2217, it holds that g(q\u0304) = x\u0304 and hence we have\n(Fi(q\u0304i, x\u2217)\u2212 wi)(x\u2217 \u2212 x\u0304)TGcoli > 0. (29) By Assumption 1 and by the fact that (q\u2217, x\u2217) is an equilibrium state of (10), as q\u0304i becomes arbitrarily large, Fi(q\u0304i, x\u2217) > wi holds in which case by (29), it holds that (x\u0304\u2212x\u2217)TGcoli < 0. However, this contradicts the requirement that (x\u0304 \u2212 x\u2217)TGcoli q\u0304i takes a large positive value. This completes the proof. \u25a0"
        }
    ],
    "title": "Payoff Mechanism Design for Coordination in Multi-Agent Task Allocation Games",
    "year": 2023
}