{
    "abstractText": "When visualizing a high-dimensional dataset, dimension reduction techniques are commonly employed which provide a single 2 dimensional view of the data. We describe ENS-t-SNE: an algorithm for Embedding Neighborhoods Simultaneously that generalizes the t-Stochastic Neighborhood Embedding approach. By using different viewpoints in ENS-t-SNE\u2019s 3D embedding, one can visualize different types of clusters within the same high-dimensional dataset. This enables the viewer to see and keep track of the different types of clusters, which is harder to do when providing multiple 2D embeddings, where corresponding points cannot be easily identified. We illustrate the utility of ENS-t-SNE with real-world applications and provide an extensive quantitative evaluation with datasets of different types and sizes.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jacob Miller"
        },
        {
            "affiliations": [],
            "name": "Vahan Huroyan"
        },
        {
            "affiliations": [],
            "name": "Raymundo Navarrete"
        },
        {
            "affiliations": [],
            "name": "Md Iqbal Hossain"
        },
        {
            "affiliations": [],
            "name": "Stephen Kobourov"
        }
    ],
    "id": "SP:acfe0a5e6b9efa4daf718ce44b00f73a591bc4b2",
    "references": [
        {
            "authors": [
                "R. Agrawal",
                "J. Gehrke",
                "D. Gunopulos",
                "P. Raghavan"
            ],
            "title": "Automatic subspace clustering of high dimensional data for data mining applications",
            "venue": "L. M. Haas and A. Tiwary, eds., SIGMOD 1998, Proceedings ACM SIGMOD International Conference on Management of Data, June 2-4, 1998, Seattle, Washington, USA, pp. 94\u2013105. ACM Press,",
            "year": 1998
        },
        {
            "authors": [
                "D. Archambault",
                "H. Purchase",
                "B. Pinaud"
            ],
            "title": "Animation, small multiples, and the effect of mental map preservation in dynamic graphs",
            "venue": "IEEE TVCG, 17(4):539\u2013552,",
            "year": 2010
        },
        {
            "authors": [
                "S. Bai",
                "X. Bai",
                "L.J. Latecki",
                "Q. Tian"
            ],
            "title": "Multidimensional scaling on multiple input distance matrices",
            "venue": "Thirty-First AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "C. Baumgartner",
                "C. Plant",
                "K. Kailing",
                "H. Kriegel",
                "P. Kr\u00f6ger"
            ],
            "title": "Subspace selection for clustering high-dimensional data",
            "venue": "Proceedings of the 4th IEEE International Conference on Data Mining (ICDM 2004), 1-4 November 2004, Brighton, UK. IEEE Computer Society,",
            "year": 2004
        },
        {
            "authors": [
                "M. Belkin",
                "P. Niyogi"
            ],
            "title": "Laplacian eigenmaps and spectral techniques for embedding and clustering",
            "venue": "Advances in Neural Information Processing Systems 14, pp. 585\u2013591. MIT Press,",
            "year": 2001
        },
        {
            "authors": [
                "A.C. Belkina",
                "C.O. Ciccolella",
                "R. Anno",
                "R. Halpert",
                "J. Spidlen",
                "J.E. Snyder-Cappione"
            ],
            "title": "Automated optimized parameters for t-distributed stochastic neighbor embedding improve visualization and analysis of large datasets",
            "venue": "Nature communications, 10(1):1\u201312,",
            "year": 2019
        },
        {
            "authors": [
                "M. Brehmer",
                "T. Munzner"
            ],
            "title": "A multi-level typology of abstract visualization tasks",
            "venue": "IEEE TVCG, 19(12):2376\u20132385,",
            "year": 2013
        },
        {
            "authors": [
                "Y. Cao",
                "L. Wang"
            ],
            "title": "Automatic selection of t-sne perplexity",
            "venue": "CoRR, abs/1708.03229,",
            "year": 2017
        },
        {
            "authors": [
                "P. Eades",
                "W. Lai",
                "K. Misue",
                "K. Sugiyama"
            ],
            "title": "Preserving the mental map of a diagram",
            "venue": "Technical report, Technical Report IIAS-RR-91-16E, Fujitsu Laboratories,",
            "year": 1991
        },
        {
            "authors": [
                "M. Espadoto",
                "R.M. Martins",
                "A. Kerren",
                "N.S.T. Hirata",
                "A.C. Telea"
            ],
            "title": "Toward a quantitative survey of dimension reduction techniques",
            "venue": "IEEE Trans. Vis. Comput. Graph.,",
            "year": 2021
        },
        {
            "authors": [
                "E.B. Fowlkes",
                "C.L. Mallows"
            ],
            "title": "A method for comparing two hierarchical clusterings",
            "venue": "Journal of the American statistical association, 78(383):553\u2013569,",
            "year": 1983
        },
        {
            "authors": [
                "T. Fujiwara",
                "Y. Kuo",
                "A. Ynnerman",
                "K. Ma"
            ],
            "title": "Feature learning for dimensionality reduction toward maximal extraction of hidden patterns",
            "venue": "CoRR, abs/2206.13891,",
            "year": 2022
        },
        {
            "authors": [
                "B. Ghojogh",
                "A. Ghodsi",
                "F. Karray",
                "M. Crowley"
            ],
            "title": "Stochastic neighbor embedding with gaussian and student-t distributions: Tutorial and survey",
            "venue": "CoRR, abs/2009.10301,",
            "year": 2020
        },
        {
            "authors": [
                "M. Gleicher",
                "D. Albers",
                "R. Walker",
                "I. Jusufi",
                "C.D. Hansen",
                "J.C. Roberts"
            ],
            "title": "Visual comparison for information visualization",
            "venue": "Inf. Vis., 10(4):289\u2013309,",
            "year": 2011
        },
        {
            "authors": [
                "J. Han",
                "J. Pei",
                "M. Kamber"
            ],
            "title": "Data mining: concepts and techniques",
            "venue": "Elsevier,",
            "year": 2011
        },
        {
            "authors": [
                "A.M. Horst",
                "A.P. Hill",
                "K.B. Gorman"
            ],
            "title": "palmerpenguins: Palmer Archipelago (Antarctica) penguin data, 2020",
            "venue": "R package version 0.1.0",
            "year": 2020
        },
        {
            "authors": [
                "M.I. Hossain",
                "V. Huroyan",
                "S.G. Kobourov",
                "R. Navarrete"
            ],
            "title": "Multiperspective, simultaneous embedding",
            "venue": "IEEE Trans. Vis. Comput. Graph., 27(2):1569\u20131579,",
            "year": 2021
        },
        {
            "authors": [
                "D. J\u00e4ckle",
                "M. Hund",
                "M. Behrisch",
                "D.A. Keim",
                "T. Schreck"
            ],
            "title": "Pattern trails: visual analysis of pattern transitions in subspaces",
            "venue": "2017 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 1\u201312. IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Jansen",
                "P. Dragicevic",
                "J. Fekete"
            ],
            "title": "Evaluating the efficiency of physical visualizations",
            "venue": "W. E. Mackay, S. A. Brewster, and S. B\u00f8dker, eds., 2013 ACM SIGCHI Conference on Human Factors in Computing Systems, CHI \u201913, Paris, France, April 27 - May 2, 2013, pp. 2593\u20132602. ACM,",
            "year": 2013
        },
        {
            "authors": [
                "Y. Jansen",
                "P. Dragicevic",
                "P. Isenberg",
                "J. Alexander",
                "A. Karnik",
                "J. Kildal",
                "S. Subramanian",
                "K. Hornb\u00e6k"
            ],
            "title": "Opportunities and challenges for data physicalization",
            "venue": "B. Begole, J. Kim, K. Inkpen, and W. Woo, eds., Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, CHI 2015, Seoul, Republic of Korea, April 18-23, 2015, pp. 3227\u20133236. ACM,",
            "year": 2015
        },
        {
            "authors": [
                "I.T. Jolliffe"
            ],
            "title": "Principal Component Analysis",
            "venue": "Springer Series in Statistics. Springer,",
            "year": 1986
        },
        {
            "authors": [
                "S. Kanaan Izquierdo",
                "A. Ziyatdinov",
                "M.A. Burgue\u00f1o",
                "A. Perera Lluna"
            ],
            "title": "Multiview: a software package for multiview pattern recognition methods",
            "venue": "Bioinformatics, (bty1039):1\u20133,",
            "year": 2018
        },
        {
            "authors": [
                "D. Kobak",
                "G.C. Linderman"
            ],
            "title": "Initialization is critical for preserving global data structure in both t-sne and umap",
            "venue": "Nature biotechnology, 39(2):156\u2013157,",
            "year": 2021
        },
        {
            "authors": [
                "S. Liu",
                "D. Maljovec",
                "B. Wang",
                "P. Bremer",
                "V. Pascucci"
            ],
            "title": "Visualizing high-dimensional data: Advances in the past decade",
            "venue": "IEEE Trans. Vis. Comput. Graph., 23(3):1249\u20131268,",
            "year": 2017
        },
        {
            "authors": [
                "L. v. d. Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "L. McInnes",
                "J. Healy",
                "N. Saul",
                "L. Gro\u00dfberger"
            ],
            "title": "UMAP: uniform manifold approximation and projection",
            "venue": "J. Open Source Softw., 3(29):861,",
            "year": 2018
        },
        {
            "authors": [
                "S. Mitra",
                "S. Saha",
                "M. Hasanuzzaman"
            ],
            "title": "Multi-view clustering for multi-omics data using unified embedding",
            "venue": "Scientific reports,",
            "year": 2020
        },
        {
            "authors": [
                "T. Rodosthenous",
                "V. Shahrezaei",
                "M. Evangelou"
            ],
            "title": "Multi-view data visualisation via manifold learning",
            "venue": "arXiv preprint:2101.06763,",
            "year": 2021
        },
        {
            "authors": [
                "R.N. Shepard"
            ],
            "title": "The analysis of proximities: multidimensional scaling with an unknown distance function",
            "venue": "Psychometrika, 27(2):125\u2013140,",
            "year": 1962
        },
        {
            "authors": [
                "J. Tang",
                "J. Liu",
                "M. Zhang",
                "Q. Mei"
            ],
            "title": "Visualizing large-scale and highdimensional data",
            "venue": "Proceedings of the 25th international conference on world wide web, pp. 287\u2013297,",
            "year": 2016
        },
        {
            "authors": [
                "A. Tatu",
                "F. Maass",
                "I. F\u00e4rber",
                "E. Bertini",
                "T. Schreck",
                "T. Seidl",
                "D.A. Keim"
            ],
            "title": "Subspace search and visualization to make sense of alternative clusterings in high-dimensional data",
            "venue": "7th IEEE Conference on Visual Analytics Science and Technology, IEEE VAST 2012, Seattle, WA, USA, October 14-19, 2012, pp. 63\u201372. IEEE Computer Society,",
            "year": 2012
        },
        {
            "authors": [
                "W.S. Torgerson"
            ],
            "title": "Multidimensional scaling: I",
            "venue": "theory and method. Psychometrika, 17:401\u2013419,",
            "year": 1952
        },
        {
            "authors": [
                "L. van der Maaten"
            ],
            "title": "Accelerating t-sne using tree-based algorithms",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "M. Wattenberg",
                "F. Vi\u00e9gas",
                "I. Johnson"
            ],
            "title": "How to use t-sne effectively",
            "venue": "Distill, 1(10):e2,",
            "year": 2016
        },
        {
            "authors": [
                "T. Xia",
                "D. Tao",
                "T. Mei",
                "Y. Zhang"
            ],
            "title": "Multiview spectral embedding",
            "venue": "IEEE Trans. Syst. Man Cybern. Part B, 40(6):1438\u20131446,",
            "year": 2010
        },
        {
            "authors": [
                "B. Xie",
                "Y. Mu",
                "D. Tao"
            ],
            "title": "m-SNE: Multiview stochastic neighbor embedding",
            "venue": "K. W. Wong, B. S. U. Mendis, and A. Bouzerdoum, eds., Neural Information Processing. Theory and Algorithms, pp. 338\u2013346. Springer Berlin Heidelberg, Berlin, Heidelberg,",
            "year": 2010
        },
        {
            "authors": [
                "B. Xie",
                "Y. Mu",
                "D. Tao",
                "K. Huang"
            ],
            "title": "m-SNE: Multiview stochastic neighbor embedding",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 41(4):1088\u20131096,",
            "year": 2011
        },
        {
            "authors": [
                "X. Yuan",
                "D. Ren",
                "Z. Wang",
                "C. Guo"
            ],
            "title": "Dimension projection matrix/tree: Interactive subspace visual exploration and analysis of high dimensional data",
            "venue": "IEEE Trans. Vis. Comput. Graph., 19(12):2625\u20132633,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Dimension Reduction, Joint Optimization, Simultaneous Embedding, t-SNE"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Dimension reduction has become one of the primary techniques to visualize high dimensional data. These techniques place data points from a high dimensional space into the (typically) two-dimensional plane of the computer screen while maintaining some characteristics of the dataset. Well known algorithms include PCA [23], t-SNE [28] and UMAP [29]. These algorithms assume either a high dimensional dataset as an input or a distance matrix between its instances. However, many such datasets contain complex phenomena which cannot be captured in a single, two dimensional, static view. Often there are several subspaces of interest to compare, so small multiple plots are employed [27, 34]. Even with coordinated views, it is difficult to track where groups of points go from one projection to the next, in effect offloading the mental effort of comparison to the user [16]. We present a technique to capture multiple subspaces of interest in a single 3D embedding. We also utilize the 2D linear projections from the computed 3D embedding, to visualize specific cluster relationships within the corresponding subspaces. Finally, we provide a seamless transition between the subspace views, which reduces the movement of the points from one view to the next, as the views are obtained by simple rotation of the 3D embedding.\n\u2022 Authors are at the University of Arizona, Department of Computer Science \u2022 Jacob Miller is the corresponding author. Email: jacobmiller1@arizona.edu\nManuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication xx xxx. 201x; date of current version xx xxx. 201x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx"
        },
        {
            "heading": "1.1 Motivation",
            "text": "Our proposed algorithm, named ENS-t-SNE, facilitates a challenging task within datasets: understanding the differences among individual data points, groups of points, or the dataset as a whole. In the typology of Brehmer and Munzner [8], this is a \u201ccompare\" task. There are two visualization comparison designs applicable to subspaces, as described by Gleicher et al. [16]: tjuxtaposition and superposition. Juxtaposition places points separately, as in a small multiple plot, but requires scanning to detect differences and similarities. Superpostion uses the same space to show two or more subspaces, resulting in more complex stimulus, which is closer to a direct comparison. ENS-t-SNE allows us access to both these comparison designs. The 3D embedding produced by ENS-t-SNE can be seen as a superpostion, by encoding each subspace from a different view of the object. ENS-t-SNE can provide juxtapostion with small multiple plots corresponding to projections for each subspace; see Fig. 5.\nReal-world multi-perspective embeddings can be found in art, e.g., the \u201c1, 2, 3\u201d sculpture by J. Hopkins which looks like the different numbers depending on the viewpoint1 and \u201cSquaring the circle\u201d by Troika2. Motivated by such 3D physicalizations, as well as by work on Multi-Perspective Simultaneous Embedding [19], our proposed algorithm enables viewers to virtually \u201cwalk around\u201d an ENS-t-SNE embedding and see different aspects of the same dataset. The concept of data physicalization (data exists in a physical space) has been shown to be effective for information retrieval when the visualization can be realized [21, 22].\nWe present a technique to capture multiple subspaces of interest in a\n1https://www.jameshopkinsworks.com/commissions4.html 2https://trendland.com/troika-squaring-the-circle/\nar X\niv :2\n20 5.\n11 72\n0v 2\n[ cs\n.L G\n] 2\nsingle 3D embedding. We also utilize the 2D linear projections from the computed 3D embedding, to visualize specific cluster relationships within the corresponding subspaces. Finally, we provide a seamless transition between the subspace views, which reduces the movement of the points from one view to the next, as the views are obtained by simple rotation of the 3D embedding."
        },
        {
            "heading": "1.2 Our Contributions",
            "text": "We propose, describe and provide an implementation for ENS-t-SNE: a technique to perform dimension reduction on a high-dimensional dataset that captures multiple subspaces of interest in a single 3D embedding. Unlike the only prior work in this domain, which optimized global distance preservation (distances between all pairs of points) [19], we focus on preserving local relationships (clusters). We also visualize specific cluster relationships within the corresponding subspaces, with the help of 2D linear projections from the computed 3D embedding. Finally, we provide a seamless transition between the subspace views, which reduces the movement of the points from one view to the next, as the views are obtained by simple rotation of the 3D embedding; see Fig. 2. This is accomplished by generalizing the t-SNE algorithm [28] to simultaneously optimize multiple distance matrices, while also simultaneously optimizing projection views. In addition to the implementation, we provide a video (also in supplemental material)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "The related work section is organized as follows. First, we review several dimensionality reduction algorithms that are widely used in visualization. Second, we delve into the fundamentals of t-SNE, to provide the needed background information needed for its generalization, ENS-t-SNE. Third, we review algorithms for subspace clustering, a domain that shares a common goal with ENS-t-SNE: finding multiple embeddings, each capturing a distinct aspect of the data. Fourth, we consider related prior approaches to simultaneous or multi-view embeddings. Finally, we review Multi-Perspective Simultaneous Embedding (MPSE), an algorithm that preserves global distances. Dimension Reduction. A wide variety of dimension reduction techniques abound: Principal Component Analysis (PCA) [23], Multi-Dimensional Scaling (MDS) [32], Laplacian Eigenmaps [6], t-Distributed Stochastic Neighbor Embedding (t-SNE) [28], Uniform Manifold Approximation and Projection (UMAP) [29]. These techniques attempt to capture the variance in the data (PCA), the global distances in the data (MDS), the local distances in the data (t-SNE), manifolds in the data (Laplacian Eigenmaps, UMAP). For a survey of other single projection dimension reduction methods, see [12]. Note that none of these allows the viewer the opportunity to compare and contrast different aspects (subspaces). T-Distributed Stochastic Neighbor Embedding (t-SNE) [28, 37]. creates a low dimensional embedding from a high dimensional dataset, based on the short distances between points in the data. Unlike stress based methods such as MDS, t-SNE converts these distances into a probability distribution which tells us the likelihood that two data are \u2018neighbors\u2019 and should appear near each other. The mathematical formulation of the problem is the following: Given an N \u00d7N distance matrix D, and a perplexity parameter, \u03c3 , t-SNE seeks to minimize the following cost function:\nC(Y ) = \u2211 i, j pi j log pi j qi j\n(1)\nHere, P = [pi j] is determined by D and the perplexity parameter, where pi j = p j|i+pi| j 2N and\np j|i = e\u2212(Di j) 2/2\u03c3 2i\n\u2211k \u0338=i e\u2212(Dik) 2/2\u03c3 2i\n. (2)\nFurthermore, Q = [qi j] is a function of Y and is defined as\nqi j =\n( 1+\u2225(yi\u2212y j)\u22252 )\u22121 \u2211k \u0338=l ( 1+\u2225(yk \u2212yl)\u22252 )\u22121 . (3)\nThe computational complexity of t-SNE is high and speed improvements have been proposed [36]. Although the original paper proposes default values and ranges for the t-SNE hyperparameters (perplexity, learning rate, etc.), automatically selecting these parameters is also a topic of interest [7, 9]. A recent paper reviews t-SNE and applications thereof [15]. In our ENS-t-SNE algorithm we optimize a generalization of the cost function of t-SNE (eq. 1) per projection, as detailed in section 3.\nSubspace Clustering. A single projection or perspective may not be sufficient to understand diverse patterns in high-dimensional data. The goal of subspace clustering is to find multiple embeddings, each capturing a different aspect of the data [27]. Indeed, two different sets of dimensions may hold different, or even conflicting, patterns. The Pattern Trails tool orders axis-aligned subspaces, plotting them as a series of 2D embeddings and overlaying parallel coordinates to track the overall pattern of data position changes [20]. Tatu et al. [34] use the SURFING [5] algorithm to prune away uninteresting subspaces, while the interesting subspaces are embedded with MDS and incorporated into a visual analytics tool for further filtering and exploration. Fujiwara et al. provide a feature learning method and visualization tool to explore non-axis aligned subspaces using a series of UMAP projections to embed the data [14]. For more detail, see the subspace clustering section from the recent survey by Liu et al. [27].\nCombining subspace clustering techniques with ENS-t-SNE seems like a promising idea. First, subspace clustering algorithms are the natural way to select interesting subspaces as input into ENS-t-SNE, especially for truly large and high-dimensional datasets where domain knowledge and expertise might not be enough. Second, ENS-t-SNE offers a powerful tool to perform comparison tasks on interesting subspaces, something that is typically done with small multiple plots (which do not support the full range of comparison tasks) [16]. We illustrate this by example in Section 5.2, with a dataset used in several subspace clustering papers [34, 41].\nSimultaneous Embedding. Some recent algorithms for simultaneous embedding/multiview embedding include Multiview Stochastic Neighbor Embedding (m-SNE) [39, 40], based on a probabilistic framework that integrates heterogeneous features of the dataset into one combined embedding and Multiview Spectral Embedding (MSE) [38], which encodes features in such a way to achieve a physically meaningful embedding. Multi-view Data Visualization via Manifold Learning [31], proposes extensions of t-SNE, LLE and ISOMAP, for dimensionality reduction and visualization of multiview data by computing and summing together the gradient descent for each data-view. Multi-view clustering for multi-omics data using unified embedding [30] uses the sum of the Kullback-Leibler divergence over the datapoints, which leads to a simple gradient adjusting the position of the samples in the embedded space.\nThe approaches above seek to achieve one of two objectives: either generating an embedding that encompasses multiple subspaces [38\u201340], thereby potentially blending the information and lacking a guarantee of preserving individual subspaces of interest, or solely creating 2D projections where each corresponds to a distinct subspace [30, 31]. In the latter case, however, the challenge lies in establishing clear correspondences between these views. The advantage of ENS-t-SNE is that it first creates a 3D embedding, where all the information is encoded and specific projections contain more fine-grained information based on subspaces of interest. Thus, there are smooth transitions between the subspace views; consequently, ENS-t-SNE ensures coherence across all 2D perspectives, fostering a more nuanced and comprehensive representation of the underlying data.\nMulti-Perspective Simultaneous Embedding (MPSE). Motivated by multi-view MDS, introduced in [4, 24], MPSE [19] computes a 3D embedding and set of projections from the 3D embedding much like ENS-t-SNE. The MPSE algorithm can be seen as a generalization of MDS that is able to visualize multiple distance matrices simultaneously, by producing a three-dimensional embedding, so that the different distance matrices are preserved after projecting the 3D coordinates to 2D ones using specified projections. Given a set of M distance\nmatrices of size N \u00d7N: D1,D2, . . . ,DM , MPSE aims to find 3D coordinates x1,x2, . . . ,xN for the samples i = 1, . . . ,N and a set of 3D to 2D projection matrices \u03a01,\u03a02, . . . ,\u03a0M so that\n\u03c32M(X,\u03a0) = M\n\u2211 m=1 \u03c32(\u03a0m X) (4)\nwhere \u03c32 is the MDS stress function. MPSE aims to preserve global distances: all pairwise distances in the input high-dimensional space must match all pairwise distances in low dimensional space. In contrast, ENS-t-SNE aims to preserve in low dimensional space only distances between nearby points in high dimensional space."
        },
        {
            "heading": "3 EMBEDDING NEIGHBORHOODS SIMULTANEOUSLY",
            "text": "Our proposed ENS-t-SNE algorithm is a generalization of the standard t-SNE algorithm. For ENS-t-SNE, we assume a set of distance matrices for the same set of objects. Similar to Multi-Perspective Simultaneous Embedding [19] with MDS, we generalize t-SNE. For this purpose we generalize the objective function of t-SNE onto the one that would take multiple distance matrices and have one projection for each on which the desired distances would locally be preserved. We generalize the objective function of t-SNE as follows: Assume we are given M distance matrices between n objects. We define\nC\u0303 = M\n\u2211 m=1 \u2211 i \u2211 j pmi j log pmi j qmi j\n(5)\nwhere pmj|i and q m i j for 1 \u2264 i, j \u2264 N and 1 \u2264 m \u2264 M are defined as\npmj|i = e\u2212(D m i j) 2/2(\u03c3 mi )2\n\u2211k \u0338=i e\u2212(D m ik)\n2/2(\u03c3 mi )2 , and pmi j =\npmj|i + p m i| j\n2N , (6)\nand\nqmi j =\n( 1+\u2225\u03a0m(yi\u2212y j)\u22252 )\u22121 \u2211k \u0338=l ( 1+\u2225\u03a0m(yk \u2212yl)\u22252\n)\u22121 . (7) Here Dmi j for 1 \u2264 i, j \u2264 n corresponds to the m-th distance matrix between objects i and j, \u03a0m corresponds to the m-th projection, which depending on the problem might be given or not, and y1, . . . ,yN correspond to the desired embedding. The objective function C\u0303 is a function of the embedding y1, . . . ,yN and projections \u03a01, . . . ,\u03a0M . The goal is to find y1, . . . ,yN and \u03a01, . . . ,\u03a0M that would minimize C\u0303. Here, the C\u0303 is a non-convex function. The only parameter that we have not discussed yet are the \u03c3mi . These parameters are application specific and can vary depending on the dataset. Depending on the density of the dataset around each high dimensional point, the values of \u03c3mi can vary. The denser the dataset is, the smaller \u03c3mi can be chosen to be.\nIn order to pick appropriate values for \u03c3mi for all 1 \u2264 i \u2264 N and 1 \u2264 m \u2264 M, we follow the steps of [28] and use the notion of perplexity. Perplexity can be interpreted as roughly the effective number of neighbors we want to capture around each point. Each value of \u03c3mi creates a new distribution Pmi over the datapoints. Similar to t-SNE we perform a binary search to compute the value of \u03c3mi that produces a distribution Pmi with a fixed perplexity that is specified by the user. The perplexity is defined as\nPerp(Pmi ) = 2 H(Pmi ), (8)\nwhere H(Pmi ) is the Shannon entropy function, defined as\nH(Pmi ) =\u2212\u2211 j pmj|i log2(p m j|i). (9)\nIn order to optimize the objective function of ENS-T-SNE (5) we use stochastic gradient descent discussed in Sec. 3.1. We would like to mention that one can compute the exact gradients of C\u0303.\nThere are two natural variants of this problem. The first version assumes that we are given a set of projections \u03a01, . . . ,\u03a0M and a set of distance matrices D1, . . . ,DM \u2208 RN\u00d7N between N objects, where\neach projection matrix corresponds to one distance matrix. The goal is to find an embedding of the dataset in 3D Y = {y1, . . . ,yN}such that on each of these projections the corresponding distances are locally preserved. The second version assumes only a set of distance matrices D1, . . . ,DM \u2208 RN\u00d7N and finds the best embedding Y = {y1, . . . ,yN} as well as the projections \u03a01, . . . ,\u03a0M onto which the local distances would be preserved."
        },
        {
            "heading": "3.1 ENS-T-SNE Algorithm",
            "text": "In this section we summarize the ENS-T-SNE algorithm and provide a practical implementation. Given a list of N \u00d7N distance matrices D1,D2, . . . ,DM and a perplexity parameter Perp, ENS-T-SNE algorithm aims to find a three-dimensional embedding y1,y2, . . . ,yN \u2208 R3 and a set of projection matrices \u03a01,\u03a02, . . . ,\u03a0M \u2208 R2\u00d73 so that for each perspective m = 1,2, . . . ,M, the corresponding projected dataset \u03a0m y1,\u03a0m y2, . . . ,\u03a0m yM minimizes the t-SNE cost function C(Ym) on that particular 2D Euclidean space as much as possible.\nWe write Y = [y1,y2, . . . ,yN ] and \u03a0 = [\u03a01,\u03a02, . . . ,\u03a0M ]. Similar to t-SNE, we accomplish this using a gradient descent type algorithm. The ENS-T-SNE cost function (5) is defined as the sum of the t-SNE cost function evaluated at each of the 2D projections \u03a01 Y,\u03a02 Y, . . . ,\u03a0M Y.\nC\u0303(Y,\u03a0) = M\n\u2211 m=1\nC(\u03a0m Y) = M\n\u2211 m=1 \u2211 i< j pmi j log pmi j qmi j , (10)\nwhere C is the t-sne cost function. The gradients of C\u0303 with respect to Y and \u03a01,\u03a02, . . . ,\u03a0M are then\n\u2207YC\u0303(Y,\u03a01,\u03a02, . . . ,\u03a0M) = M\n\u2211 m=1 (\u03a0m)T \u2207C(\u03a0m Y) (11)\nand \u2207\u03a0mC\u0303(Y,\u03a01,\u03a02, . . . ,\u03a0M) = \u2207C(\u03a0m Y)YT . (12)\nAs derived in [28], the gradient \u2207C = [\n\u2202C \u2202 y1 , \u2202C\u2202 y2 , . . . , \u2202C\u2202 yN\n] is given by\n\u2202C \u2202 yi = 4\u2211 j (pi j \u2212qi j)(yi\u2212y j).\nIn its simplest form (full gradient and projected gradient descent), the update rules are\nY 7\u2192 Y+\u00b5\u2207YC\u0303(Y,\u03a01,\u03a02, . . . ,\u03a0M)\nand \u03a0m 7\u2192 Q ( \u03bd\u2207\u03a0mC\u0303(Y,\u03a01,\u03a02, . . . ,\u03a0M) ) ,\nwhere \u00b5,\u03bd > 0 are learning rates and Q maps a 2\u00d7 3 matrix to its nearest orthogonal matrix.\nIn practice, we found that a combination of adaptive learning rate and stochastic gradient descent works the best in consistently avoiding local minima. To avoid flat solutions, we first optimize for the embedding Y while keeping the projections fixed (which are randomly chosen among the set of 2x3 orthogonal matrices). This algorithm is described in supplemental material.\nInitialization: Similar to t-SNE, the objective function of ENSt-SNE in (5) is non-convex. Thus, gradient-based methods are not guaranteed to find global optima; e.g., it is known that t-SNE is vulnerable to being caught in local minima when randomly initialized [25]. Further, different ENS-t-SNE runs with random initialization, produce different embeddings. A good initialization for ENS-t-SNE can help reach better solutions and provide deterministic results. In the case of standard t-SNE, a \u2018smart initilization\u2019 based on a 2D PCA projection tends to be better than random initialization. This addresses both the poor local minima problem and the non-determinism.\nSince we are given multiple pairwise dissimilarity matrices in ENSt-SNE, we cannot simply use a single PCA projection for initialization. Therefore, we devise a \u2018smart initialization\u2019 strategy by first taking\nthe average over all pairwise dissimilarity matrices and then applying dimensionality reduction to 3D via classical (Torgerson) MDS [35]. When using this initialization scheme we also deterministically compute a set of 2x3 orthogonal matrices, ensuring that the ENS-t-SNE optimization is deterministic."
        },
        {
            "heading": "4 EXPERIMENTS ON SYNTHETIC DATA",
            "text": "We first verify the ENS-t-SNE algorithm on constructed synthetic data. Construction of Synthetic Data: In order to create a dataset with multiple perspectives, each containing multiple clusters, we propose the following procedure. Fix the number of points and the number of projections; for each projection fix the number of clusters, and the number of points corresponding to each cluster. For each perspective, randomly split the points between all the groups and define distances by assigning smaller within-cluster distances and larger between-cluster distances. The procedure is formally defined below.\nLet the number of points be N and the number of perspectives be M. For each perspective set the number of clusters to NCm, for 1 \u2264 m \u2264 M, and the number of points corresponding to each cluster to Nc,m, where 1 \u2264 m \u2264 M and 1 \u2264 c \u2264 NCm. Note, that the total number N of points per perspective is a fixed constant and these points are in correspondence with each other. That is, N = \u2211NCmc=1 Nc,m for all 1 \u2264 m \u2264 M.\nFor each perspective 1 \u2264 m \u2264 M, the sample points with labels 1,2, . . . ,N are randomly assigned to one of NCm clusters. That is, the sample point 1 \u2264 i \u2264 N will have labels l1i , l2i , . . . , lMi . Hence, two samples l1 and l2 may share the same label in some of the M perspectives, but are unlikely to share the same labels in all of them. Next, we create the distance matrix between the N datapoints as follows: The observed distance between points i, j for 1 \u2264 i, j,\u2264 N in each perspective m,\nwhere 1 \u2264 m \u2264 M is given by\nDmi j = { dmin + \u03b5 if l m i = l m j\ndmout + \u03b5 otherwise (13)\nwhere \u03b5 \u223c N(0,\u03c32) is a normal random variable with mean 0 and standard deviation \u03c32, dmin corresponds to within cluster distance for the m-th perspective and dmout corresponds to the outside cluster distances. Experiments: Since the goal of ENS-t-SNE is to preserve local structures, this dataset suits its purpose and serves as a good example to show experimentally how the proposed algorithm works. The aim is to show that ENS-t-SNE preserves the local structures of the dataset, that is, each of the perspectives recovers the corresponding original clusters.\nWe run ENS-t-SNE for datasets generated according to Section 4. For the first experiment, we fix the number of perspectives to be M = 3, the number of datapoints N = 400, the number of clusters per perspective NCm = 2 for 1 \u2264 m \u2264 3. We create the distance matrices for each perspective by formula (13) described in Section 4 with din = 1 and dout = 2 and \u03c32 = 0.1. We run the ENS-t-SNE algorithm, summarized in Section 3.1, for the distance matrices and a fixed perplexity value of 40. The result are demonstrated in Figure 2.\nWe use separate visual channels to encode the different types of clusters. Specifically, to show the original clusters for the first perspective, we use colors (blue and orange), for the second perspective, we use the shape (circles and squares), and for the third perspective, we use texture, filled and not filled; see Figure 2. We observe that ENS-t-SNE did a good job finding an embedding in 3D with 3 perspectives, such that for the first perspective the blue and orange datapoints are separated, for the second perspective, datapoints shown in circles and squares are separated, and for the third perspective, filled and not filled datapoints are separated. As expected, ENS-t-SNE split the data into multiple small clusters in 3D, but in such a way that for each 2D perspective,\nthe ones with similar features in 3D overlap and create bigger clusters. For the second experiment, we fix the number of perspectives to be M = 3, the number of datapoints N = 400, the number of clusters per perspective NC1 = 2, NC2 = 3, and NC3 = 4. We create the distance matrices for each perspective by formula (13) described in Section 4 with din = 1 and dout = 2 and \u03c32 = 0.1. We run the ENS-t-SNE algorithm for these distance matrices with perplexity value 40. The results are demonstrated in Figure 3.\nWe again use separate visual channels to encode the different types of cluster. For the first type of clustering with NC1 = 2 we use texture: either filled or not filled. The second type of clustering has NC2 = 3 separate clusters, and we use shape: square, triangle, or circle. The third clustering has NC3 = 4 and we use the color channel: blue, orange, green, and red. Note how the computed 3D embedding allows us to see in each view 2, 3 and 4 clusters, and that they are well defined and separated in the corresponding projections.\nCompare this to how MPSE performs; see Fig. 4. Clusters are more mixed and not clearly separable. For example, the projections that is supposed to separate the data by color, mixes up the blue and red clusters, and both of them are too close to the green. If the cluster identities were not given as part of the input, we would not get as clear an idea about the structure of the data from the MPSE embedding and projections as with ENS-t-SNE."
        },
        {
            "heading": "5 EXPERIMENTS ON REAL-WORLD DATA",
            "text": "In this section we demonstrate the application of ENS-t-SNE algorithm on real-world datasets. The quality of ENS-t-SNE embeddings is affected by choice of subspaces, so determining how to select them is important. We describe two approaches for subspace selection.\nThere might be some clear semantic grouping of features, or some grouping of features that is of interest. If this is the case, ENS-t-SNE can be readily applied and we show an example of such as dataset with the Palmer\u2019s Penguins in section 5.1.\nHowever, feature grouping is not always clear. The data might have hundreds of features or come from where the meaning of features is\nunclear. Subspace clustering algorithms can efficiently find subspaces of interest. The USDA food composition dataset is frequently analyzed in subspace clustering literature; we use two interesting subspaces identified by Tatu et al. [34] and show how ENS-t-SNE can provide insights about the different groups in the data. Finally, we apply the CLIQUE subspace clustering algorithm [2] to the auto-mpg dataset to identify interesting subspaces. The resulting ENS-t-SNE embedding shows patterns that are missing from the standard t-SNE embedding. Further real-world data examples are in supplemental materials."
        },
        {
            "heading": "5.1 Palmer\u2019s Penguins",
            "text": "The Palmer\u2019s Penguins dataset [18] is a collection of 344 penguins with documented bill length, bill depth, body mass, flipper length, and sex. The dataset contains 3 different species of penguins.\nApplying standard t-SNE on this dataset produces six distinct clusters, one for each sex-species pair; see Fig. 7(a). However, we can obtain a more fine-grained information by utilizing ENS-t-SNE and two subspaces: the 4-dimensional physical attributes and the 1-dimensional sex attribute. Specifically, we create two pairwise distance matrices, the first one based on the 4D physical attributes which have numerical values. The second one is based on penguin\u2019s sex, where the distance between same sex penguins is 0 and the distance between different sex penguins is 1. Running ENS-t-SNE with these distance matrices produces the embedding in Fig. 5(a).\nThe first view (middle) aims to capture the physical attributes and the second view (right) aims to capture the sex information. While ENS-t-SNE also clusters the data into 6 distinct clusters in 3D, the positions of these clusters are more meaningful. In the first view (Fig. 5b) the algorithm has captured physical attributes which are largely correlated with the 3 penguin species colored blue for Adelie, orange for Gentoo, and green for Chinstrap. Meanwhile, the second view (Fig. 5c) shows two large clusters based on sex: circles for female penguins and rectangles for male penguins.\nWhile one could run t-SNE on each subdimension separately and plot them as small multiples, such a visualization does not capture the\ncorrespondence between datapoints in the two independent embeddings. In the ENS-t-SNE embedding, each point belongs to two clusters; one for its species and one for its sex. In an interactive environment, one can follow a datapoint from one projection to the other. In other words, there is a transition between the two views in three dimensions that is missing when using small multiples. Comparing the t-SNE and ENS-t-SNE embeddings of the dataset we can see that unlike in tSNE, the positions of the clusters in ENS-t-SNE are meaningful. For example, the two orange clusters in t-SNE embedding in Figure 7 are far from each other, while they are close in the ENS-t-SNE embedding in Figure 5. Thus both datapoint positions and cluster positions are more meaningful in the ENS-t-SNE embedding."
        },
        {
            "heading": "5.2 Food Composition Dataset",
            "text": "The USDA Food Composition Dataset [1] is a collection foods specified by nutrient components (e.g., calories, proteins, fats, iron, vitamins). The full dataset contains over 7000 entries with 46 dimensions; after removing entries with missing values we have 948 unique foods, which we use for our experiments. We walk through how one might use ENS-t-SNE for exploratory data analysis. We first utilize a standard t-SNE projection to get a sense of the data; see Fig. 7(c). There are roughly three large clusters (indicated by color), and we confirm by running a k-means clustering on the high dimensional data and see that it matches the clusters in the t-SNE projection.\nManually examining the clusters for human-interpretable meaning shows that the first cluster (red) contains almost entirely meats, while the second and third clusters (blue and orange) appear to have a lot in common, which is unexpected given the k-means results and the t-SNE plot. While the orange cluster contains many grains and vegetables and the blue cluster contains many fruits and beverages, both clusters contain many dairy products such as milks, cheeses, and yogurts.\nWe suspect though, that there are more interesting insights to be gained by looking at subspaces of the data since there is seemingly an overlap in the blue and yellow clusters. In fact, this dataset has been used in several subspace clustering papers [34, 41]. We apply the method from [34] and obtain two subspaces that we then pass on to ENS-t-SNE to visually investigate further.\nWe call the first subspace \u2018waters+lipids\u2019, as it contains the following features: water, vitamin E, sodium, total lipids, and calories. We call the second subspace \u2018proteins+vitamins\u2019 as it contains protein, vitamin B6, vitamin B12, and vitamin D. We computed the corresponding pairwise distance matrices and applied ENS-t-SNE to these pairwise distance matrices; see Fig. 8.\nFrom the 3D embedding obtained by ENS-t-SNE (Figure 8a) we can make the following observations: In 3D there are strong orange, red, and blue clusters, though there is a portion of the embedding where the colors are mixed, mostly between orange and blue. In the first projection (Figure 8b), corresponding to \u2018water+lipids\u2019, each cluster has been separated though there are many points that fall between clusters; notably between blue and orange. Our implementation provides a hover popout when mousing over datapoints with labels, so we use this to\nconfirm that these are blue points that are particularly \u2018watery\u2019 such as lettuce, cucumber, baby-food.\nIn the second view (Fig. 8c) which corresponds to the \u2018proteins+vitamins\u2019 subspace, we see a much different picture. The meats have been strongly clustered, with the red cluster in the top left. The blue and orange clusters, that were distinct in the previous projection, have been mixed, indicating that the blue and orange clusters have largely similar protein and vitamin components. Notably, there are two blue/orange clusters. One sits closer to the meats cluster and contains dairy products like milk, infant formula, and cheese. The second cluster contains other meatless foods from both the orange and blue clusters. Note again that ENS-t-SNE provides a more meaningful embedding of both individual datapoints and clusters. In particular, the similarity between the blue and orange clusters is missing from the standard t-SNE view, as the distance between clusters in t-SNE is often arbitrary."
        },
        {
            "heading": "5.3 Auto-MPG Dataset",
            "text": "The auto-mpg dataset from the UCI machine learning repository [10], provides data for 398 cars, each with the following 8 attributes: mpg, cylinders, displacement, horsepower, weight, acceleration, model year, origin. The CLIQUE subspace clustering algorithm to the data. We select two \u201cinteresting\" subspaces, measured with the Fowlkes-Mallows score [13]: the first includes (mpg, cylinders, displacement), and the second includes (horsepower, weight, acceleration).\nWe apply ENS-t-SNE algorithm for the two subspaces using perplexity value 30. The corresponding 3D embedding by ENS-t-SNE is demonstrated in Figure 9. In order to show the clusters in the obtained embedding, we use colors (red, blue, and orange) and shapes (diamond, triangle, square, and crosses).\nTo show the clusters in embedded dataset, we use the cylinders and the weights. We partition the total data into three groups based on the number of cylinders as follows: In the first group we place all the cars with 4 or fewer cylinders, in the second group are cars with 5 or 6 cylinders, and in the third group we put the cars with more than 6 cylinders. In Figure 9 the first group is colored in red, the second group is colored in blue and the third group is colored in orange. We further partition the dataset into four groups based on the weights according to 25, 50 and 75 quantiles. In Figure 9 the datapoints corresponding to the first group are shown in diamond shapes, the second group in crosses, the third group in circles and the fourth group in triangles.\nFigure 9 shows that ENS-t-SNE was able to find an embedding of the dataset in 3D separating the data into several clusters. The first perspective groups together datapoints with the same colors, i.e., cars with similar numbers of cylinders are grouped together; see the second subfigure of Figure 9. The second perspective groups together datapoints with the same shapes, i.e., cars with similar weights are grouped together; see the third subfigure of Figure 9.\nIn Figure 9 we observe that although in the two perspectives cars are clustered according to corresponding dimensions (number of cylinders and weight), there are some exceptions. For example, the blue outliers in the second (and also third) subfigure correspond to two exceptional\ncars which have low weights but higher number of cylinders (5 or 6). Consider, for comparison, the standard t-SNE visualization of the same dataset in 2D; see Figure 7(b). The dominant factor for the embedding is the number of cylinders, resulting in three well-separated clusters in the embedding. Note, however, that the t-SNE embedding completely missed the weight information, as there is no pattern between the shapes. Contrast this with the ENS-t-SNE embedding, where both relationships (number of cylinders and weight) can be seen from the corresponding directions; see Fig. 9.\nApplying ENS-t-SNE to similar datasets (with multiple interpretations) makes it possible to find a visualization that respects all interpretations. Furthermore, datapoints on the periphery of the clusters and outliers can be interpreted as datapoints that are very similar in one interpretation but completely different in others."
        },
        {
            "heading": "6 QUANTITATIVE EVALUATION",
            "text": "The real-world and synthetic examples above show that ENS-t-SNE can provide meaningful 3D datapoint positions and cluster positions. Here we provide some quantitative data.\nWhile MDS, t-SNE and UMap can produce 3D embeddings, they cannot optimize per-projection views, as MPSE and ENS-t-SNE can. We could use single projection techniques to obtain subspace embed-\ndings but the resulting plots will be largely unrelated. Thus the only available technique that can be directly compared to ENS-t-SNE is MPSE. With this in mind, we quantitatively evaluate MPSE and ENS-tSNE, using the metrics trustworthiness, continuity, neighborhood hit and stress, as recommended in a recent survey [12].\nFor each metric, we compute the 3D values with respect to all distances, as well as the per-perspective values with respect to each subspace. For the following definitions, let N be the size of a dataset (number of points), and K be a parameter for the size of a neighborhood. For trustworthiness, continuity, and neighborhood hit we use K = 7 for all evaluations, as in [12].\nTrustworthiness measures how well neighbors in the embedding match the neighbors in high dimensional space, with large errors penalized heavily. As its name implies, a high trustworthiness is a good indication that one can trust the local patterns in the embedding. This is a measure of precision with respect to clusters:\n1\u2212 2NK(2N\u22123K\u22121) \u2211 N i=1 \u2211 j\u2208UKi r(i, j)\u2212K (14)\nwhere UKi is the set of points among the K nearest neighbors of point i in the embedded space but not in the high dimensional space and r(i, j) is the rank of the embedded point j with respect to the embedded\nnearest neighobors of point i. Continuity is related to trustworthiness, but measures how many neighors are missing in the embedding but are present in high dimensional space. A high continuity score means that most of the neighborhood around a given point is nearby (rather than far away). This is a measure of recall with respect to clusters:\n1\u2212 2NK(2N\u22123K\u22121) \u2211 N i=1 \u2211 j\u2208V Ki r\u0302(i, j)\u2212K (15)\nwhere V Ki is the set of points among the K nearest neighbors of point i in the high dimensional space but not in the embedding, and r\u0302(i, j) is the rank of point j with respect to i in the high dimensional space.\nNeighborhood Hit (NH) measures the proportion of nearest neighbors of a point in the embedding which have the same label, similar to a k-nearest neighbor classifier. In order to use this measure, we need labels which we have (or generate) for each of our datasets. Since our datasets have two or more sets of different labels, when we compute NH on the three dimensional embedding we take the Cartesian product of the two labelings. For instance, if a penguin is both an Adelie penguin and female, then we assign it a label of (Adelie, female). Formally, NH is defined as follows:\n\u2211Ni=1 1 KN | j \u2208 Ne K i : l j = li| (16)\nwhere NeKi is the K nearest neighborhood of point i in the embedding, and li is the label of point i.\nWe note that the food composition dataset does not have ground truth labels, but there is good evidence that our clustering is accurate. The clustering has a high silhouette score and is visually validated in Fig 7 and Fig. 8.\nStress is the sum of squared differences in the distances between the embedding and high dimensional space:\n\u2211i< j(||Xi \u2212X j||\u2212di, j)2 (17)\nwhere Xi is the embedded position of point i and di, j is the distance between points i, j in the high dimensional space. Low stress indicates good distance preservation. Note that MPSE directly optimizes stress for each view, so we expect it to often outperform ENS-t-SNE.\nTable 1 shows the results. Note how ENS-t-SNE tends to outperform MPSE on the continuity, trustworthiness, and NH scores for the 2d views indicated that as expected ENS-t-SNE is more reliably capturing local structures in the projected views. Low stress is not the goal of this algorithm, so having high stress compared to MPSE is acceptable.\nStability. Although we cannot directly quantitatively compare to t-SNE (or other single perspective dimension reduction embeddings), we can measure how similar a set of projections are. It is known that for many comparative tasks it is desirable to have as little change as possible while still being faithful to the data. This notion is often referred to as stability or preservation of the mental map [3, 11].\nWe adapt the notion of stability to a pair of embeddings, stability(E1,E2) computed by first aligning E2 to E1 as close as possible using affine transformations and then computing the average\ndistance between corresponding points in the two embeddings. For a series of embeddings E1, . . .En we compute the average pairwise stability, i.e., \u2211i< j stability(Ei,E j).\nIn all datasets under consideration, ENS-t-SNE produces better stability between small multiple projections. The results for an average of 30 trials are shown in Fig. 10. Note that ENS-t-SNE has consistently better stability than t-SNE, which is not unexpected, as t-SNE has no correspondence between small multiple projections."
        },
        {
            "heading": "6.1 Scalability",
            "text": "In this section we consider the scalability of ENS-t-SNE with respect to the number of perspectives, the number of clusters per perspective, and the number of datapoints. In particular, the goal is to evaluate how the accuracy or the speed of ENS-t-SNE is affected as these parameters increase in value. The results indicate that the runtime of ENS-t-SNE scales reasonably well as these parameters increase. The accuracy decreases when the number of perspectives and clusters grows. However, if the number of perspectives is two the accuracy does not decrease as the number of clusters increase.\nIn order to measure the accuracy of the embedding for a dataset containing several clusters, we define the separation error as follows: For a 2D image containing two labels, the best linear classifier is found and the proportion of errors in this classification is returned. For a 2D image containing more than two labels, a linear classifier for every possible combination of two labels is computed, and the average proportion of errors between all combinations is returned. For a 3D embedding with multiple perspectives, with each image having two or more labels, the separation error in each image is computed and the average is returned.\nTo check the scalability of the proposed ENS-t-SNE algorithm we create datasets as described in Section 4. We consider N = 400 datapoints and vary the number of perspectives M = 2,3, . . . ,10, making\nsure that each perspective has NCm = 2 identifiable clusters in it. We then apply ENS-t-SNE for each of these datasets using perplexity values 40,80,160,240 and report the results in Fig. 11(a). The x-axis of Figure 11(a) shows the number of perspectives and the y-axis shows the separation error. The results indicate that for a small number of perspectives (2,3) the separation error is small and as the number of perspective increases the error grows.\nNext, we test the scalability of the algorithm as the number of clusters per perspectives increases. We create the datasets in a similar fashion setting N = 400, M = 2,3, and varying the number of clusters per perspective NCm = 2,3, . . . ,10. We run ENS-t-SNE with perplexity values 40,80,160,240 and report the separation error in Fig. 11(b): as the number of clusters increase for 3 perspectives, the separation error grows, while for 2 perspectives the separation error is stable.\nWe continue by analyzing the influence of the number of datapoints\non the running time of the algorithm. For this purpose, we create datasets containing clusters according to Section 4. We set the number of perspectives M = 2,3 and the number of clusters per perspective NCm = 2, while varying the total number of datapoints N from 200 to 1800 in increments of 200. We run ENS-t-SNE for these datasets with perplexity value 0.2\u2217N and report the running time as a function of N in Figure 11(c). The results indicate a steady increase in running time as the number of datapoints and the number of perspectives grow.\nThe final experiment tests the effect of the number of perspectives on the accuracy of the algorithm. We generate data as follows: We uniformly distribute 1000 points in a solid 3D ball, and randomly select several perspectives. We label each point in each perspective according to on which side of the perspective the points fall into. The perplexity is fixed to 600 and the number of perspectives vary from 2 to 10. We report the separation error vs the number of perspectives in Fig. 11(d). The results indicate that when there are no forced clusters, the algorithm has more freedom to separate the data into parts that respect the original label assignment, providing more stable separation error."
        },
        {
            "heading": "7 LIMITATIONS",
            "text": "Naturally, there are many limitations to ENS-t-SNE. Here we consider only a partial list, starting with scalability. It is known that t-SNE is computationally expensive and in this prototype we have not yet considered applying ideas for speeding it up, such as those in [33, 36].\nWhile MPSE [19] focuses on simultaneously capturing global distances between objects and ENS-t-SNE aims to capture local neighborhoods, other approaches for dimension reduction, such as UMAP [29], optimize both at the same time. It would be worthwhile to quantitatively verify the extend to which these goals can be realized by the different approaches.\nThe utility of ENS-t-SNE depends on finding interesting subspaces/subdimensions and combinations thereof. We have not yet considered automating the process by using approaches such as those for subspace clustering as part of the ENS-t-SNE pipeline. Setting up the distance matrices that ENS-t-SNE needs can be done in different ways, as illustrated in the different examples in the paper. Evaluating different approaches and automating the process remains to be done.\nWhile we expect that 3D ENS-t-SNE embeddings might be easier to interpret and work with, compared to small multiple type visualizations (with or without linked views), we are not aware of human-subject studies to validate this intuition."
        },
        {
            "heading": "8 CONCLUSIONS AND FUTURE WORK",
            "text": "We described ENS-t-SNE, a generalization of t-SNE, which computes a 3D embedding of a dataset along with a set of 2D projections that optimize subspace clustering information. We note that while our paper describes ENS-t-SNE in 3D, the technique can be applied to higher dimensions (lower than the number of input dimensions).\nAs the main part of the paper describes the proposed ENS-t-SNE algorithm (from the idea to the implementation), the quantitative and qualitative evaluation is just sketched out here. Nevertheless, several different types of experiments, on synthetic and real-world datasets, indicate that ENS-t-SNE can indeed simultaneously capture multiple different types of relationships defined on the same set of highdimensional objects. All source code, experimental data, and analysis described in this paper are available on github (along with a video explanation) at https://github.com/enggiqbal/MPSE-TSNE.\nAn interesting direction that we began to explore is to extend the objective function such that each perspective shows the t-SNE embedding for different values of perplexities; see the supplemental material.\nAnother possible application is using ENS-t-SNE to visualize image datasets, based on different parts of the input images. We include some preliminary results for the MNIST dataset in the supplemental material.\nENS-t-SNE generalizes t-SNE to multiple perspectives. Generalizing other dimensionality reduction techniques, such as UMAP [29] might be of interest. Combining local and global perspective at the same time, for example by combining ENS-t-SNE and MPSE [19], might provide embeddings that allow us to balance local and global distance preservation."
        },
        {
            "heading": "A ENS-T-SNE PSEUDOCODE",
            "text": "Algorithm 1 Practical implementation of ENS-T-SNE\nRequire: N \u00d7N pairwise distance matrices: D1,D2, . . . ,DM , perplexity parameter Perp. for perspectives m = 1,2, . . . ,M do\nCompute Pm = f (Dm,Perp) end for Assign initial values to Y , \u03a0 = [\u03a01,\u03a02, . . . ,\u03a0M ], \u00b5,\u03bd . for batch size = n/4, n/2 do\nfor t = 1,2,T do Y = Y+\u00b5\u2207ssYC\u0303(Y,\u03a0) \u00b5 = \u00b5 +1\nend for end for for batch size = n/4, n/2 do\nfor t = 1, . . . ,T do Y = Y+\u00b5\u2207ssYC\u0303(Y,\u03a0) \u03a0 = \u03a0+Q ( \u03bd\u2207ss\u03a0C\u0303(Y,\u03a0) ) \u00b5 = \u00b5 +1 \u03bd = \u03bd +1\nend for end for\nEnsure: Y, \u03a01,\u03a02, . . . ,\u03a0M .\nB VISUALIZING MNIST DATASET BY ENS-T-SNE In this section we apply ENS-t-SNE to visualize the MNIST handwritten digit database [26]. The dataset contains 70,000 handwritten digits in greyscale, each of sizes 28\u00d728; see examples from the dataset in Figure 12. The standard way of applying machine learning algorithms to the MNIST dataset is to vectorize the matrices corresponding to the greyscale pixel values of each digit and obtain a vector of size 28\u00d728 = 784. Thus, each instance of the dataset can be viewed as a single datapoint in R784, with the idea being that points corresponding to the same digit should be close to each other. Dimensionality reduction techniques that aim to capture global distances (e.g., Principal Component Analysis and Multi-Dimensional Scaling) are known to perform poorly when embedding such data into 2D or 3D. Non-linear dimensionality reduction techniques that focus on local neighborhoods, such as t-SNE and UMAP, perform much better.\nWe apply ENS-t-SNE to embed the MNIST dataset (and similar datasets which require local neighborhood preservation) in 3D to capture the clusters corresponding to each digit. The idea is to define multiple distance/similarity measures between pairs of datapoints that would capture different properties/characteristics. As a simple initial example, each image can be divided into two parts: top and bottom; see Figure 13. We then compute the Frobenius norm, \u2225A\u2225F between the matrices corresponding to the greyscale pixel values of each image. For a given matrix A, \u2225A\u2225F is defined as the square root of the sum of the squares of its elements and can be viewed as the vector L2 norm of the vector of all elements of the matrix. Formally, for a matrix A = (ai, j)) m,n i=1, j=1, its Frobenius norm is defined as:\n\u2225A\u2225F = \u221a\u221a\u221a\u221a m\u2211 i=1 n \u2211 j=1 a2i j.\nWe now have two different sets of distance matrices: one corresponds to the top part of each image and the other corresponds to the bottom part of each image.\nThe idea behind this experiment is to apply ENS-t-SNE to a dataset with different perspectives, where some points are close to each other (in the same cluster) in one perspective, but are far from each other (in different clusters) in the other perspective. The ENS-t-SNE algorithm should place the points in 3D so that the desired properties are satisfied in the corresponding perspectives.\nThe distance matrices for the top and bottom parts of all datapoints are the inputs to the ENS-t-SNE algorithm. The results are different than those from the standard t-SNE applied on the original datapoints, as illustrated in Figures 15 and 14. The bottom view (the third subfigure of Fig. 14) shows that digits 1, 7 and 4 (sometimes 9, depending on the handwriting) are close to each other. This is expected since for most of these digits the bottom parts are nearly straight line segments oriented roughly the same way. However, since there is a significant difference in the top\nparts, the clusters are separated in 3D. Similarly, the pair of digits 3 and 5 are close in the bottom view and the digits 8 and 9 are close in the top view.\nComparing the standard t-SNE embedding in Figure 15, we can see that the ENS-t-SNE embedding managed to avoid some \u201cerrors\". For example in Figure 15, there are some 0s that appear within the cluster of 6s while that is not the case for ENS-t-SNE; see the left subfigure of Fig. 14.\nIn addition to the visual comparison between the MNIST dataset embedding obtained from ENS-t-SNE and the standard t-SNE, we also evaluate them using cluster accuracy [17, Section 10.6.3]. Specifically, given an embedding (from ENS-t-SNE or t-SNE) we apply k-means clustering with 10 clusters and compare to the ground truth 10 clusters. For each embedding, finding the correct cluster labels is based on considering all re-orderings of the k-means cluster labels and selecting the one that best matches the ground truth. After that, count the number of points that are correctly clustered and normalize it by the total number of points. The results suggest that ENE-t-SNE does as well or better than standard t-SNE: 0.539 for ENS-t-SNE in Fig. 14 vs. 0.478 for standard t-SNE in Fig. 15 (with higher scores corresponding to higher accuracy). The experiments were run for randomly chosen 1000 instances of MNIST dataset and averaged over 5 runs. In general, the accuracy is comparable, with a slight advantage to ENS-t-SNE.\nC VISUALIZING THE EFFECT OF PERPLEXITY The choice of perplexity parameter in t-SNE greatly affects the quality of achieved embedding; see [37]. Usually, smaller perplexity parameters produce visualizations that better reflect local distances between samples, however, when the perplexity is very small the algorithm fails to find a sufficiently good solution. Thus, finding an appropriate value of perplexity for which t-SNE would find the best possible embedding has been of research interest [9].\nENS-t-SNE can be used to visualize the differences between various perplexity parameters on the same given distance matrix. In practice, ENS-t-SNE seems to overcome the issue of not producing good results for smaller/larger-than-ideal perplexity value, as long as one of the perplexity values passed into ENS-t-SNE is sufficiently good. Specifically, we apply ENS-t-SNE to the same set of distance matrices but with different perplexity values. The goal is to find an embedding of the dataset in 3D so that on the different projections it solves the t-SNE optimization with different perplexity values (but with the same distance matrix).\nThe problem formulation is as follows. Let D be an N \u00d7N distance matrix and perp1,perp2, . . . ,perpM > 0 be a list of perplexity parameters of interest. We wish to minimize the following cost function\nC\u0303(X ,\u03a0;perp1,perp2, . . . ,perpM) = M\n\u2211 m=1 C(\u03a0mX ;perpm) (18)\nwhere Y 7\u2192C(Y ;perpm) is the t-SNE cost function with perplexity parameter perpm. Minimizing (18) is achieved using a momentum based batch or stochastic gradient descent.\nIn Figure 16, we show an application of ENS-t-SNE for a dataset that contains 2 clusters, constructed according to the model described in Section 4, with M = 1 and with N = 400 datapoints, using perplexity parameters equal to 3 and 100. The 3D embedding for the corresponding computed distance matrix, shown in the first row of Figure 16, and for the given two values of the perplexity parameter is the solution to problem (18). The two figures in the second row of Figure 16 show the projections of the 3D embedding that best represent the perspectives of this data set with perplexities 3 and 100.\nAs a way of comparison, we also compute the corresponding standard t-SNE 2D embeddings for the same distance matrix D with perplexities 3 and 100 (the last row of Figure 16). It is easy to see that standard t-SNE found the clusters when the perplexity was high and failed to find them when the perplexity was low, while ENS-t-SNE captured the clusters for both perplexity values. We note that whereas the images produced by ENS-t-SNE are projections of the same 3D embedding, the images produced by t-SNE are obtained independently."
        }
    ],
    "title": "ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE",
    "year": 2023
}