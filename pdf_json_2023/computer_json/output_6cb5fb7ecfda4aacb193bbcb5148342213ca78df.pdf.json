{
    "abstractText": "We propose a unified framework for low resource automatic speech recognition tasks named meta audio concatenation (MAC). It is easy to implement and can be carried out in extremely low resource environments. Mathematically, we give a clear description of MAC framework from the perspective of bayesian sampling. In this framework, we leverage a novel concatenative synthesis text-to-speech system to boost the low resource ASR task. By the concatenative synthesis text-to-speech system, we can integrate language pronunciation rules and adjust the TTS process. Furthermore, we propose a broad notion of meta audio set to meet the modeling needs of different languages and different scenes when using the system. Extensive experiments have demonstrated the great effectiveness of MAC on low resource ASR tasks. For CTC greedy search, CTC prefix, attention, and attention rescoring decode mode in Cantonese ASR task, Taiwanese ASR task, and Japanese ASR task the MAC method can reduce the CER by more than 15%. Furthermore, in the ASR task, MAC beats wav2vec2 (with fine-tuning) on common voice datasets of Cantonese and gets really competitive results on common voice datasets of Taiwanese and Japanese. Among them, it is worth mentioning that we achieve a 10.9% character error rate (CER) on the common voice Cantonese ASR task, bringing about 30% relative improvement compared to the wav2vec2 (with fine-tuning).",
    "authors": [
        {
            "affiliations": [],
            "name": "Zeping Min"
        },
        {
            "affiliations": [],
            "name": "Qian Ge"
        },
        {
            "affiliations": [],
            "name": "Zhong Li"
        }
    ],
    "id": "SP:20f310ece2f67c818955555d346507411d752f2d",
    "references": [
        {
            "authors": [
                "Mohammad Ahadi",
                "Hamid Sheikhzadeh",
                "Robert Brennan",
                "George Freeman"
            ],
            "title": "An energy normalization scheme for improved robustness in speech recognition",
            "venue": "In Eighth International Conference on Spoken Language Processing,",
            "year": 2004
        },
        {
            "authors": [
                "Alexei Baevski",
                "Steffen Schneider",
                "Michael Auli"
            ],
            "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
            "venue": "arXiv preprint arXiv:1910.05453,",
            "year": 2019
        },
        {
            "authors": [
                "Alexei Baevski",
                "Yuhao Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "William Chan",
                "Navdeep Jaitly",
                "Quoc V Le",
                "Oriol Vinyals"
            ],
            "title": "Listen, attend and spell",
            "venue": "arXiv preprint arXiv:1508.01211,",
            "year": 2015
        },
        {
            "authors": [
                "Alexis Conneau",
                "Alexei Baevski",
                "Ronan Collobert",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "title": "Unsupervised cross-lingual representation learning for speech recognition",
            "venue": "arXiv preprint arXiv:2006.13979,",
            "year": 2020
        },
        {
            "authors": [
                "Linhao Dong",
                "Shuang Xu",
                "Bo Xu"
            ],
            "title": "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu"
            ],
            "title": "Conformer: Convolution-augmented transformer for speech recognition",
            "year": 2005
        },
        {
            "authors": [
                "Fran\u00e7ois Hernandez",
                "Vincent Nguyen",
                "Sahar Ghannay",
                "Natalia Tomashenko",
                "Yannick Esteve"
            ],
            "title": "Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation",
            "venue": "In International conference on speech and computer,",
            "year": 2018
        },
        {
            "authors": [
                "Yosuke Higuchi",
                "Niko Moritz",
                "Jonathan Le Roux",
                "Takaaki Hori"
            ],
            "title": "Momentum pseudo-labeling: Semisupervised asr with continuously improving pseudo-labels",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Rubeena A Khan",
                "Janardan Shrawan Chitode"
            ],
            "title": "Concatenative speech synthesis: A review",
            "venue": "International Journal of Computer Applications,",
            "year": 2016
        },
        {
            "authors": [
                "Jaeyoung Kim",
                "Han Lu",
                "Anshuman Tripathi",
                "Qian Zhang",
                "Hasim Sak"
            ],
            "title": "Reducing streaming asr model delay with self alignment",
            "venue": "arXiv preprint arXiv:2105.05005,",
            "year": 2021
        },
        {
            "authors": [
                "Aleksandr Laptev",
                "Roman Korostik",
                "Aleksey Svischev",
                "Andrei Andrusenko",
                "Ivan Medennikov",
                "Sergey Rybin"
            ],
            "title": "You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation",
            "venue": "In 2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI),",
            "year": 2020
        },
        {
            "authors": [
                "Jason Li",
                "Ravi Gadde",
                "Boris Ginsburg",
                "Vitaly Lavrukhin"
            ],
            "title": "Training neural speech recognition systems with synthetic speech augmentation",
            "venue": "arXiv preprint arXiv:1811.00707,",
            "year": 2018
        },
        {
            "authors": [
                "Qi Li",
                "Jinsong Zheng",
                "Augustine Tsai",
                "Qiru Zhou"
            ],
            "title": "Robust endpoint detection and energy normalization for real-time speech and speaker recognition",
            "venue": "IEEE Transactions on Speech and Audio Processing,",
            "year": 2002
        },
        {
            "authors": [
                "Fernando L\u00f3pez",
                "Jordi Luque"
            ],
            "title": "Iterative pseudo-forced alignment by acoustic ctc loss for self-supervised asr domain adaptation",
            "venue": "arXiv preprint arXiv:2210.15226,",
            "year": 2022
        },
        {
            "authors": [
                "Zeping Min",
                "Cheng Tai"
            ],
            "title": "Why pseudo label based algorithm is effective?\u2013from the perspective of pseudo labeled data",
            "venue": "arXiv preprint arXiv:2211.10039,",
            "year": 2022
        },
        {
            "authors": [
                "Zeping Min",
                "Qian Ge",
                "Zhong Li"
            ],
            "title": "10 hours data is all you need",
            "venue": "arXiv preprint arXiv:2210.13067,",
            "year": 2022
        },
        {
            "authors": [
                "Vassil Panayotov",
                "Guoguo Chen",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Librispeech: an asr corpus based on public domain audio books",
            "venue": "In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2015
        },
        {
            "authors": [
                "Daniel Povey",
                "Arnab Ghoshal",
                "Gilles Boulianne",
                "Lukas Burget",
                "Ondrej Glembek",
                "Nagendra Goel",
                "Mirko Hannemann",
                "Petr Motlicek",
                "Yanmin Qian",
                "Petr Schwarz"
            ],
            "title": "The kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding, number CONF",
            "venue": "IEEE Signal Processing Society,",
            "year": 2011
        },
        {
            "authors": [
                "Elena Rodr\u00edguez",
                "Bel\u00e9n Ru\u00edz",
                "\u00c1ngel Garc\u00eda-Crespo",
                "Fernando Garc\u00eda"
            ],
            "title": "Speech/speaker recognition using a hmm/gmm hybrid model",
            "venue": "In International Conference on Audio-and Video-Based Biometric Person Authentication,",
            "year": 1997
        },
        {
            "authors": [
                "Andrew Rosenberg",
                "Yu Zhang",
                "Bhuvana Ramabhadran",
                "Ye Jia",
                "Pedro Moreno",
                "Yonghui Wu",
                "Zelin Wu"
            ],
            "title": "Speech recognition with augmented synthesized speech",
            "year": 2019
        },
        {
            "authors": [
                "Nick Rossenbach",
                "Albert Zeyer",
                "Ralf Schl\u00fcter",
                "Hermann Ney"
            ],
            "title": "Generating synthetic audio data for attention-based speech recognition systems",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Steffen Schneider",
                "Alexei Baevski",
                "Ronan Collobert",
                "Michael Auli"
            ],
            "title": "wav2vec: Unsupervised pre-training for speech recognition",
            "venue": "arXiv preprint arXiv:1904.05862,",
            "year": 2019
        },
        {
            "authors": [
                "Guangzhi Sun",
                "Yu Zhang",
                "Ron J Weiss",
                "Yuan Cao",
                "Heiga Zen",
                "Andrew Rosenberg",
                "Bhuvana Ramabhadran",
                "Yonghui Wu"
            ],
            "title": "Generating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Youcef Tabet",
                "Mohamed Boughazi"
            ],
            "title": "Speech synthesis techniques. a survey",
            "venue": "In International Workshop on Systems, Signal Processing and their Applications,",
            "year": 2011
        },
        {
            "authors": [
                "Andros Tjandra",
                "Sakriani Sakti",
                "Satoshi Nakamura"
            ],
            "title": "Listening while speaking: Speech chain by deep learning",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2017
        },
        {
            "authors": [
                "Sei Ueno",
                "Masato Mimura",
                "Shinsuke Sakai",
                "Tatsuya Kawahara"
            ],
            "title": "Data augmentation for asr using tts via a discrete representation",
            "venue": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),",
            "year": 2021
        },
        {
            "authors": [
                "Shinji Watanabe",
                "Takaaki Hori",
                "Suyoun Kim",
                "John R Hershey",
                "Tomoki Hayashi"
            ],
            "title": "Hybrid ctc/attention architecture for end-to-end speech recognition",
            "venue": "IEEE Journal of Selected Topics in Signal Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Shinji Watanabe",
                "Takaaki Hori",
                "Shigeki Karita",
                "Tomoki Hayashi",
                "Jiro Nishitoba",
                "Yuya Unno",
                "Nelson Enrique Yalta Soplin",
                "Jahn Heymann",
                "Matthew Wiesner",
                "Nanxin Chen"
            ],
            "title": "Espnet: End-to-end speech processing toolkit",
            "venue": "arXiv preprint arXiv:1804.00015,",
            "year": 2018
        },
        {
            "authors": [
                "Colin Wei",
                "Kendrick Shen",
                "Yining Chen",
                "Tengyu Ma"
            ],
            "title": "Theoretical analysis of self-training with deep networks on unlabeled data",
            "venue": "arXiv preprint arXiv:2010.03622,",
            "year": 2020
        },
        {
            "authors": [
                "Qiantong Xu",
                "Tatiana Likhomanenko",
                "Jacob Kahn",
                "Awni Hannun",
                "Gabriel Synnaeve",
                "Ronan Collobert"
            ],
            "title": "Iterative pseudo-labeling for speech recognition",
            "venue": "arXiv preprint arXiv:2005.09267,",
            "year": 2020
        },
        {
            "authors": [
                "Zhuoyuan Yao",
                "Di Wu",
                "Xiong Wang",
                "Binbin Zhang",
                "Fan Yu",
                "Chao Yang",
                "Zhendong Peng",
                "Xiaoyu Chen",
                "Lei Xie",
                "Xin Lei"
            ],
            "title": "Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit",
            "venue": "arXiv preprint arXiv:2102.01547,",
            "year": 2021
        },
        {
            "authors": [
                "Cheng Yi",
                "Jianzong Wang",
                "Ning Cheng",
                "Shiyu Zhou",
                "Bo Xu"
            ],
            "title": "Transfer ability of monolingual wav2vec2. 0 for low-resource speech recognition",
            "venue": "In 2021 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "We propose a unified framework for low resource automatic speech recognition tasks named meta audio concatenation (MAC). It is easy to implement and can be carried out in extremely low resource environments. Mathematically, we give a clear description of MAC framework from the perspective of bayesian sampling. In this framework, we leverage a novel concatenative synthesis text-to-speech system to boost the low resource ASR task. By the concatenative synthesis text-to-speech system, we can integrate language pronunciation rules and adjust the TTS process. Furthermore, we propose a broad notion of meta audio set to meet the modeling needs of different languages and different scenes when using the system. Extensive experiments have demonstrated the great effectiveness of MAC on low resource ASR tasks. For CTC greedy search, CTC prefix, attention, and attention rescoring decode mode in Cantonese ASR task, Taiwanese ASR task, and Japanese ASR task the MAC method can reduce the CER by more than 15%. Furthermore, in the ASR task, MAC beats wav2vec2 (with fine-tuning) on common voice datasets of Cantonese and gets really competitive results on common voice datasets of Taiwanese and Japanese. Among them, it is worth mentioning that we achieve a 10.9% character error rate (CER) on the common voice Cantonese ASR task, bringing about 30% relative improvement compared to the wav2vec2 (with fine-tuning)."
        },
        {
            "heading": "1 Introduction",
            "text": "Automatic speech recognition (ASR) is a traditional task with wide application. Before deep learning became popular, HMM-GMM (Rodr\u00edguez et al., 1997) models, which have an elegant mathematical form, were widely used for ASR tasks. It performed well in some simpler speech recognition scenarios. And the most famous HMM-GMM speech recognition toolkit was Kaldi (Povey et al., 2011). As the rise of deep learning, a large number of end-to-end speech recognition models have emerged, such as Speech-transformer (Dong et al., 2018), Conformer (Gulati et al., 2020), Espnet (Watanabe et al., 2018), Wenet (Yao et al., 2021), LAS (Chan et al., 2015), etc. Furthermore, there are also many pretrained large models, such as vq-wav2vec (Baevski et al., 2019), wav2vec (Schneider et al., 2019) and wav2vec2 (Baevski et al., 2020) (Conneau et al., 2020).\nar X\niv :2\n30 2.\n03 49\n8v 2\n[ cs\n.C L\nHowever, whether for a HMM-GMM (Rodr\u00edguez et al., 1997) model or advanced end-to-end models such as wenet (Yao et al., 2021), obtaining a practical model of speech recognition often requires a large amount of data for the learning (hundreds of hours, if not tens of thousands of hours). While in many scenarios it is often difficult and expensive to get enough audio training data, such as dialects. A straightforward solution is to use TTS for data augmentation. There has been a lot of research on TTS data augmentation methods for ASR tasks, such as (Laptev et al., 2020; Rossenbach et al., 2020; Sun et al., 2020). In this work, we propose a unified framework for boosting low resource ASR tasks named MAC. The key in the MAC framework is leveraging a novel concatenative synthesis TTS system. It is worth mentioning that compared with former TTS for ASR tasks architectures, our MAC framework has the following advantages:\n\u2022 MAC leverages a novel concatenative synthesis text-to-speech system to boost the ASR task, which can integrate language pronunciation rules as the prior knowledge. Furthermore, the process of generating audio has strong interpretability and is easy to be adjusted in the generation process;\n\u2022 Under the MAC framework, we propose a broad notion of meta audio set, and its dimensions can be flexibly determined according to prior knowledge, model complexity budget, audio data size, etc. So we can easily adjust the complexity and performance of the concatenative synthesis text-to-speech system.\n\u2022 There is no need to train additional TTS neural networks, and the generation process is just simple splicing, which is easy to implement and saves computing resources;\n\u2022 Most importantly, MAC framework can be carried out in extremely low resource environments (e.g., training data less than 10h) without the help of additional labeled data. And MAC has a promising potential to model any low resource language as long as there is prior knowledge of the pronunciation rules of the language.\nLots of experiments have demonstrated the great effectiveness of our method in low resource ASR tasks. For the Cantonese ASR task, Taiwanese ASR task, and Japanese ASR task, the MAC framework can reduce the CER by more than 15%. Furthermore, MAC outperformed fine-tuning wav2vec2 on the Cantonese common speech dataset and obtained very competitive results on the Taiwanese and Japanese common speech datasets. Among them, we achieved a 10.9% character error rate (CER) on the common spoken Cantonese ASR task, resulting in a significant relative improvement of about 30% compared to fine-tuning wav2vec2 model.\nAlso, semi-supervised learning and transfer learning are currently two main ways to deal with the (labeled) data limit problem. However, both of the approaches will possibly face certain difficulties in some low resource scenarios. We will briefly discuss these below."
        },
        {
            "heading": "1.1 Semi-supervised learning",
            "text": "For semi-supervised learning, pseudo-label algorithms are widely used. There are many variants of pseudo label algorithms. For example, iterative pseudo label algorithm (Xu et al., 2020). But their basic ideas are using the pseudo labels to help to train the model.\nTable 1 reported in Higuchi et al. (2022) demonstrated the effectiveness of iterative pseudo-label semisupervised algorithms. The experiments are conducted on the LibriSpeech dataset Panayotov et al. (2015) and TED3 dataset Hernandez et al. (2018). LL-10h, LS-100h, LS-360h and LS-860h in Table 1 represent the different splits of the LibriSpeech dataset Panayotov et al. (2015) and the results are evaluated on dev-other split and test-other split of LibriSpeech dataset.\nBesides the fact that the performance can be indeed improved greatly, we also notice that the quality of the initial model performance (the first row in Table 1) has a remarkable impact on the final performance. There are also some theoretical results to explain these phenomena (see e.g., (Wei et al., 2020) and (Min & Tai, 2022)). Actually, in real low resource scenarios, it can be rather difficult to obtain a suitable initial model (pseudo-label generator) due to the limited labeled data. These difficulties seriously affect the performance of pseudo label based semi-supervised learning when applied to extremely low resource ASR tasks."
        },
        {
            "heading": "1.2 Transfer learning",
            "text": "For fine-tuning pretrained large models, wav2vec2 (Baevski et al., 2020; Conneau et al., 2020) is one of the most representative pretrained models. Wav2vec2 has demonstrated strong transfer learning capabilities on ASR tasks. Using wav2vec2, the CER can be significantly reduced (Yi et al., 2021). Although transfer learning is generally quite effective, the effectiveness of transfer learning can be significantly constrained, if there is a large gap between the target speech domain and the pretrained speech domain. For example, the wav2vec2 Baevski et al. (2020) model is pretrained on English audio data in the English speech domain, which gives a 4.8% CER by using 10 minutes of English audio labeled data to fine-tune. However, it only achieves a 28.32% CER even using 27k training utts in the Japanese domain (Yi et al., 2021)."
        },
        {
            "heading": "1.3 Our contributions",
            "text": "The contributions can be summarized as follows:\n\u2022 We propose MAC framework, which leverages a novel concatenative synthesis text-to-speech system to boost the ASR task. And it can integrate language pronunciation rules as prior knowledge.\n\u2022 We innovatively proposed a broad notion of meta audio set, which helps MAC to integrate different prior knowledge and flexibly apply it to different scenarios.\n\u2022 Mathematically, we give a clear description of MAC from the perspective of bayesian sampling.\n\u2022 Our experiments show that MAC brings more than 15% reduction in CER in Cantonese ASR task, Taiwanese ASR task, and Japanese ASR task."
        },
        {
            "heading": "2 Relate work",
            "text": "There have been many attempts to use TTS for data augmentation to benefit ASR e.g. (Laptev et al., 2020; Rossenbach et al., 2020; Sun et al., 2020; Li et al., 2018; Ueno et al., 2021; Rosenberg et al., 2019; Tjandra et al., 2017). Among them, (Ueno et al., 2021) keep their eyes on the perspective of the representation. Results in Rosenberg et al. (2019) indicate the effectiveness of TTS data enhancement, although it may not be as good as the model trained on real speech data. (Tjandra et al., 2017) takes advantage of the close connection between TTS and ASR models.\nHowever, current methods often suffer from severe difficulties in real speech recognition tasks in extremely low resource environments. For example, requiring additional data to train the TTS system or extremely low resource environments (around 10h) failing to meet the required audio of the models. In addition, the current NN based TTS systems often lack interpretability and are difficult to perform the necessary manual adjustment.\nRecently, (Min et al., 2022) proposed a new interesting discovery. Experiments in Min et al. (2022) show that competitive performance on Mandarin ASR tasks can be achieved with only 10h of Mandarin audio data with the following steps:\n\u2022 Train models on one Mandarin audio dataset and perform force alignment to get Mandarin characteraudio pairs.\n\u2022 Map the character to pinyin using character-pinyin dictionary to build pinyin-audio database.\n\u2022 For transcription in another Mandarin audio dataset, query the audio clips corresponding to each Mandarin character and concatenate them.\n\u2022 Perform energy normalization.\nAlthough the Min et al. (2022) is focused on Mandarin audio ASR tasks, it does have many properties that are well-suited for low resource ASR tasks. For example, it does not require other labeled audio data to work, which is valuable in the low resource environment. Besides the synthesis process is efficient, interpretable, and convenient for human intervention.\nInspired by the (Min et al., 2022), we propose the MAC, leveraging a novel concatenative synthesis text-tospeech system to boost the ASR task performance. In (Min et al., 2022), the meta audio refers specifically to pinyin since only focuses on Mandarin ASR tasks. In this work, we propose a much more general concept of meta audio than (Min et al., 2022). By specific meta audio, we can utilize the prior knowledge of pronunciation rules in a large number of language scenes. These advantages make MAC ideal for extremely low resource language ASR tasks. Furthermore, the work in (Min et al., 2022), focusing on Mandarin, can be regarded as a special case under the MAC framework. Extensive experiments prove the great effectiveness of MAC, particularly on low resource ASR tasks."
        },
        {
            "heading": "3 Method",
            "text": "Our basic idea is to generate the audio data by concatenating the meta audios corresponding to given transcription texts and then train the ASR neural network using the generated audio data. In our method, we propose a much more general concept of meta audio set. Based on the meta audio set conception , our MAC can be expressed mathematically rigorously from the perspective of bayesian sampling."
        },
        {
            "heading": "3.1 General audio datasets",
            "text": "The mathematical formulation is as follows. Denote the audio wave space by X , and the transcription text space by Y, i.e., X = {x :all the audio waves}, Y = {y : all the transcription texts}.\nIn general, for ASR tasks, the labeled audio dataset consists of audio-transcription pairs {(xi, yi)}Ni=1 sampled from a certain underlying distribution P , and {xi}Ni=1 \u223c Px, {yi}Ni=1 \u223c Py with Px, Py as the marginal distribution of P .\nObtaining a practical speech recognition model often requires hundreds even thousands of hours of audiotranscription pairs for training. Unfortunately, getting audio-transcription pairs is usually expensive. And in many scenarios such as dialects, one can only access around ten hours of audio-transcription pairs. However, the audio-only data x \u223c Px and text-only data y \u223c Py are respectively easier to access. Generally, we have a paired dataset D = {(xi, yi)}Ni=1, an audio-only dataset Daudio = {xi} N1 i=1 and a text-only dataset Dtext = {yi}N2i=1 with N2 > N1 > N . This is the typical setting of low recourse ASR tasks. Now our goal is to sample paired audio-transcription data (x, y) from the underlying distribution P .\nBasically, we have P (x, y) = Py(y)P (x | y), (1)\nwhere Py(y) corresponds to the distribution of transcriptions, and P (x | y) denotes the distribution of audios given a certain transcription y. Therefore, the sampling of new data (audio-transcription pair) (x, y) can\nbe divided into two stages. First, the transcription text y is sampled from Py(y), and then the audio x corresponding to the previous transcription text y is sampled from P (x | y). The first stage, i.e., sampling of transcription text y, is relatively easy since usually there is sufficient text-only data in Dtext = {yi}N2i=1, and we can model/approximate Py by P\u0303y(y) = 1N2 \u2211 yi\u2208Dtext \u03b4(yi) (i.e., substitute the probability by sample frequency). Therefore, the key is to analyze and maximize P (x | y), which is in fact a TTS task."
        },
        {
            "heading": "3.2 Meta audio sequence space A",
            "text": "In order to perform further \"decoupling\" analysis on the conditional probability distribution P (x | y), we first introduce the meta audio set. Here the meta audios refer to the basic modeling units of specific language pronunciations. For example, there are about 50 phonemes\nin English. If we want to use these phonemes as modeling units to characterize English pronunciation, these phonemes form a nature meta audio set of English. In this way, it is similar to monophonic modeling in HMM-GMM Rodr\u00edguez et al. (1997) and Kaldi Povey et al. (2011). However, in our framework, the meta audio set selections are flexible. For instance, a fusing phoneme (treating several phonemes as the same phoneme) set can be seen as a meta audio set in English. For a specific language, we can reasonably determine the meta audio according to its pronunciation rules e.g. pinyin in Mandarin and kana in Japanese.\nThe meta audio sequence space A represents sequences set of meta audios. Certainly, different meta audio sets may imply different meta audio sequence space A.\nMapping function t : Y \u2192 A. To reflect language-specific pronunciation rules, we also need a function t : Y \u2192 A to map a transcription text to its corresponding meta audio sequence. The construction of t requires prior knowledge of pronunciation rules. Obviously, different languages may have different mappings t and even different meta audio sequences A in the same language may have different mappings t. Figure 1 shows examples of t for English and Japanese."
        },
        {
            "heading": "3.3 Decoupling analysis",
            "text": "In this section, we perform a fine-grained decoupling analysis on the probability P (x | y), the conditional distribution of audios given transcriptions. On the one hand, when an audio x \u2208 X is given, we have a\nconditional distribution P (y |x). This is in fact the goal of ASR tasks: predict corresponding texts given audios by estimating P (y |x). On the other hand, when a transcription y \u2208 Y is given, we have another conditional distribution P (x | y). We can decouple P (x | y) via the meta audio sequence space A and the mapping function t:\nP (x | y) = \u2211 a\u2208A P (x, a | y)\n= \u2211 a\u2208A P (x | a, y)P (a | y)\n=P (x | a = t(y), y)\n(2)\nHere, we applied the fact that P (a | y) is a degenerate distribution with the probability 1 at a = t(y). Furthermore, the meta audio sequence contains all the pronunciation information of the transcription text, hence\nP (x | a = t(y), y) = P (x | a = t(y)), (3)\nwhich gives P (x | y) = P (x | a = t(y)). (4)\nFigure 2 illustrates this decoupling analysis."
        },
        {
            "heading": "3.4 Bayesian inference on P (x | a)",
            "text": "Based on Section 3.3 and Eq. (4), instead of analyzing P (x | y) directly, we can turn to study P (x | a). In general, this is much easier, since a \u2208 A usually has a much lower dimension than y \u2208 Y. The dimension here refers to the number of element classes per position of a \u2208 A or y \u2208 Y. For example, in English, the dimension of a \u2208 A is about 50 (here we naturally select the phoneme as meta audio for simple interpretation), while the dimension of y \u2208 Y may be much higher since there is a large number of words.\nNotice that P (x | a) \u221d Px(x)P (a |x), (5)\nThe goal is now converted to maximize Px(x) and P (a |x) in order to maximize P (x | a). Recall that Px(x) reflects the prior probability of audios x \u2208 X , we will discuss it later (in Section 3.6). For P (a = (a(1), a(2), ..., a(n)) |x), we have\nP (a |x) = \u2211\ns n\u220f i=1 P ( a(i) |x(i) = [xsi , xsi+1) ) (6)\nHere, s = (s1, s2, ..., sn+1) represents the time slice of x \u2208 X . Eq. (6) holds because: 1) the audio wave x can be also properly divided (maybe not unique) to obtain the clip x(i) = [xsi , xsi+1) corresponding to each a(i). For instance, we can segment an English audio wave of one sentence and get the audio wave segmentation corresponding to the sentence\u2019s meta audio sequence (here meta audio sequence is phoneme sequence since here we naturally select the phoneme as meta audio for simple interpretation); 2) the audio wave clip x(i) in x is monotonous with respect to a(i) in a. That is, the timestamps of audio waves and meta audios must match with each other, i.e., x(i) and only x(i) corresponds to a(i); 3) For simplicity, we treat this correspondence as independent.\nUnfortunately, it is quite expensive to consider all the time slices s = (s1, s2, ..., sn+1) in Eq. (6). However, for each fixed time slice s0 = (s01, s02, ..., s0n+1), we can obtain a lower bound of P (a |x):\nP (a |x) = \u2211\ns n\u220f i=1 P ( a(i) |x(i) = [xsi , xsi+1) ) \u2265\nn\u220f i=1 P ( a(i) |x(i) = [xs0 i , xs0 i+1 ) ) .\n(7)\n3.5 Maximization of P (a |x)\nAccording to Eq. (7) in Section 3.4, we can approximately maximize P (a = (a(1), a(2), ..., a(n)) |x) by maximizing a lower bound determined by some fixed partition s0 = (s01, s02, ..., s0n+1). The maximization of right hand side of Eq. (7) can be considered in another direction: for each element a(i) in a, we are required to find an audio wave clip x(i) that makes P ( a(i) |x(i) ) as large as possible. Fortunately, this can be achieved by performing force alignment (Kim et al., 2021; L\u00f3pez & Luque, 2022). Specifically, we first map the transcription text y \u2208 Y in the labeled audio dataset D = {(xi, yi)}Ni=1 into a \u2208 A to get a corresponding dataset {(xi, ai)}Ni=1. Then, we train the ASR model and perform force alignment on {(xi, ai)}Ni=1 to get the audio wave clip corresponding to the meta audio element for each ai (with high probability). For further efficiency, we can store the forced alignment results and build a database B. Therefore, when we aim to maximize \u220fn i=1 P ( a(i) |x(i) = [xs0 i , xs0 i+1 ) ) for some fixed time slice s, we just query the audio clip x(i) corresponding to each meta audio element a(i). Here, the time slice s is considered implicitly, since when we concatenate these audio clips, it automatically forms a complete time of the audio wave. The procedure of building the database B is illustrated in Figure 3.\nRemark 1 (Database size) For each element a(i), we may get different corresponding audio clips (with high probability) by conducting force alignment on {(xi, ai)}Ni=1. We just store all of them in the database B to enrich the selections and increase the diversity of synthesized audios.\n3.6 Maximization of Px(x)\nIn this section, we discuss about maximizing Px(x). In Section 3.5, for each element a(i) in a, we need to find an audio clip x(i) to enlarge P ( a(i) |x(i) ) as much as possible. However, the audio wave ob-\ntained by combining these audio clips may give a higher probability \u220fn i=1 P ( a(i) |x(i) ) , but negatively affect Px(x = (x(1), x(2), ..., x(n))). For example, the volume of generated (x(1), x(2), ..., x(n)) may change rapidly and frequently, leading to a low Px(x = (x(1), x(2), ..., x(n))). The reason can be mathematically understood that the support set of Px is likely to be a very small (proper) subset in the whole audio space X . Hence, simply merging these audio clips may cause the synthesized audio wave x = (x(1), x(2), ..., x(n)) corresponding to a = (a(1), a(2), ..., a(n)) to be severely distorted.\nTo solve these problems, we need some regularization techniques to improve the modeling of Px(x = (x(1), x(2), ..., x(n))). A simple but effective regularization is the energy normalization (Li et al., 2002; Ahadi et al., 2004; Min et al., 2022), and here we just imitate the operation as in (Min et al., 2022). That is, averaging the energy of sampled audio clips (x(1), x(2), ..., x(n)) corresponding to the meta audio sequence a = (a(1), a(2), ..., a(n)) \u2208 A. Mathematically, we have\nE = \u2211n\ni=1\u2016x(i)\u2016 n , (8)\n{ x(i) }n i=1 \u2192 { x(i) \u2016x(i)\u2016 \u2217 E }n i=1 . (9)\nSince the energy normalization in Eq. (9) only linear scaling x(i), i = 1, 2, ..., n, we further have\nP ( a(i) |x(i) ) \u2248 P ( a(i) | x (i) \u2016x(i)\u2016 \u2217 E ) . (10)\nCertainly, other (more complex) regularization operations are also widely used in concatenative synthesisbased text-to-speech (TTS) systems (Tabet & Boughazi, 2011; Khan & Chitode, 2016). In our experiments, the energy normalization is easy to implement and works well. More importantly, our ultimate goal is to do ASR tasks, so we do not pay too much attention to the fine-grained quality of the audio generated by\nTTS which may lead to overly complex models. However, improving the quality of the synthesized audio may give better results and we will further discuss the applicability of other (more complex) regularization operations in the future work."
        },
        {
            "heading": "3.7 Workflow of MAC",
            "text": "According to previous sections, the MAC framework steps can be summarized as Fig 4."
        },
        {
            "heading": "4 Experiments",
            "text": "We verify the effectiveness of MAC on three real low resource Cantonese, Taiwanese, and Japanese ASR tasks on the corresponding dataset of them in common voice dataset 1. The experimental results show that MAC has defeated the results achieved by fine-tuning large-scale wav2vec2 pretrained models, and achieved very competitive results on these tasks. More importantly, MAC brings a remarkable performance boost on some tasks. For instance, we have achieved 10.9% CER on the common voice dataset of Cantonese for ASR tasks, leading an around 30% relative improvement compared to fine-tuning wav2vec2 model."
        },
        {
            "heading": "4.1 Datasets",
            "text": "Common voice is currently one of the most widely used multilingual audio datasets, which is a public speech dataset contributed by volunteers around the world. For each language, we use the training split of the corresponding dataset in common voice dataset as the origin labeled data to conduct MAC. The proportion of origin labeled data and synthetic data by MAC are shown in Fig. 5. For comparison, we also train directly on origin train split labeled data as the baseline results. We use the test split of the corresponding dataset in common voice dataset to evaluate.\n1We use the zh-HK dataset in the common voice dataset for the Cantonese ASR task."
        },
        {
            "heading": "4.2 Models",
            "text": "We use the advanced hybrid CTC/Attention architecture Watanabe et al. (2017) with the conformer Gulati et al. (2020) encoder as Wenet 2 (Yao et al., 2021). It is worth mentioning that in the evaluation phase, the final output is jointly determined by both CTC-decoder and attention-decoder for better performance when we use the attention rescoring decode mode. That is, the attention decoder rescores the top candidates given by the CTC-decoder. We conduct experiments on 2 \u00d7 RTX 3090GPUs (24GB) and 4 \u00d7 P100GPUs (16GB)."
        },
        {
            "heading": "4.3 Details and results",
            "text": "Below we present the experimental details and results\nWe can see that our MAC method substantially exceeds the fine-tuning wav2vec2 model result on the Cantonese recognition task and is comparable to the wav2vec2 model results on other tasks.\n2We refer to some codes from https://github.com/wenet-e2e/wenet"
        },
        {
            "heading": "4.3.1 Cantonese ASR",
            "text": "For Cantonese, the natural choice of the meta-audio set is Cantonese pinyin at first glance. But taking advantage of the broad notion of the meta audio set, we make some adjustments on the Cantonese pinyin to simplify our modeling process, including but not limited to not distinguishing between different tones in Cantonese pinyin in the meta audio set. e.g. regarding \"haa4\" and \"haa6\" as one meta audio in the meta audio set. The function t is to map a Cantonese transcription text y to the meta audio sequence a \u2208 A. For a Cantonese transcription text y = (y1, y2, ..., ym), we generally have the approximation\nt(y1, y2, ..., ym) \u2248 (t(y1), t(y2), ..., t(ym)). (11)\nHence, we can reverse the order of Step 3 and Step 4 in Section 3.7 to simplify the overall process. That is, we first train the model on original labeled dataset (x, y) and perform forced alignment. And then pass the results through the mapping function t to build the database B for Cantonese. In this way, we can use the trained baseline Hybrid CTC/attention model to do force alignment and do not need to train another model for force alignment. We use transcriptions in the validation split of the zh-hK dataset in common voice dataset and remove transcriptions appearing in the test split as the text-only dataset Dtext for Step 5. For Step 7, we use the energy normalization method described in Eq. (8) and (9)."
        },
        {
            "heading": "4.3.2 Taiwanese ASR",
            "text": "For Taiwanese, the natural choice of the meta-audio set is pinyin at first glance. But taking advantage of the broad notion of the meta audio set, we make some adjustments on the Cantonese pinyin to simplify our modeling process, including but not limited to simplification of some polyphonic characters. The function t is to map a Taiwanese transcription text y \u2208 Y to a meta audio sequence a \u2208 A. The pinyin mapping function t also has approximation Eq. (11) and hence we can also reverse the order of Step 3 and Step 4 to simplify the overall process. That is as Cantonese task above, we first train the model on original labeled dataset (x, y) and perform forced alignment, and then pass the results through the mapping function t to build the database B for Taiwanese. In this way, we can use the trained baseline Hybrid CTC/attention model to do force alignment and do not need to train another model for force alignment. We use transcriptions in the validation split of the taiwan dataset in common voice dataset and remove transcriptions appearing in the test split as the text-only dataset Dtext for Step 5. For Step 7, we use the energy normalization method described in Eq. (8) and (9)."
        },
        {
            "heading": "4.3.3 Japanese ASR",
            "text": "For Japanese, the natural choice of the meta-audio set is kana at first glance. But taking advantage of the broad notion of the meta audio set, we make some adjustments on the Cantonese pinyin to simplify our modeling process, including but not limited to simplification of some special kanas. The function t is to map a Japanese transcription text y \u2208 Y to a kana sequence a \u2208 A. Here, we do not reverse Step 3 and Step 4. We use transcriptions in the validation split of the japan dataset in common voice dataset and remove transcriptions appearing in the test split as the text-only dataset Dtext for Step 5. For Step 7, we use the energy normalization method described in Eq. (8) and (9)."
        },
        {
            "heading": "4.3.4 Performance",
            "text": "All results are shown in Table 2. We just refer to fine-tuning wav2vec2 results from urls 3 so we did not apply the decode modes (CTC greedy search, CTC prefix beam search, attention, attention rescoring) to them. Firstly, one can observe a significant performance improvement by MAC compared to the advanced hybrid CTC/attention baseline model. For all CTC greedy search, CTC prefix, attention, and attention rescoring decode mode in all three Cantonese ASR task, Taiwanese ASR task, and Japanese ASR task, the MAC method can reduce the CER by more than 15%.\n3we refer the fine-tuning wav2vec2 results on urls. Cantonese: https://huggingface.co/ctl/ wav2vec2-large-xlsr-cantonese and Japanese: https://huggingface.co/qqhann/w2v_hf_jsut_xlsr53 and Taiwanese: https://huggingface.co/voidful/wav2vec2-large-xlsr-53-tw-gpt and we round to one decimal place.\nMore importantly, MAC beats wav2vec2 (with fine-tuning) and achieves new SOTA on common voice datasets of Cantonese and Japanese when applied to ASR tasks. * in Table 2 represents that the 24.9 CER result is achieved by using extra data than the Japanese audio dataset in the common voice dataset during finetuning wav2wev2. We can see that MAC relatively improves the performance by about 30% on the Cantonese ASR task and even beats wav2vec2 fine-tuned with extra data on the Japanese ASR task. In addition, for the Taiwanese ASR task, MAC also achieves results comparable to the fine-tuned wav2vec2 model."
        },
        {
            "heading": "5 Conclusion",
            "text": "MAC proposed in this work is a unified framework for low resource automatic speech recognition tasks. We propose a broad notion of meta audio set to help MAC to be available as long as we have the knowledge of pronunciation rules to construct a suitable meta audio set. Hence it can meet the modeling needs of numerous languages and numerous scenes. Besides, we give a clear mathematical description of MAC framework from the perspective of bayesian sampling. Our experiments have demonstrated the great effectiveness of MAC on low resource speech recognition tasks, with remarkable improvements in accuracy even without tuning hyperparameters carefully. We hope that the MAC method can contribute to the development of low resource audio recognition tasks."
        }
    ],
    "title": "MAC: A unified framework boosting low resource automatic speech recognition",
    "year": 2023
}