{
    "authors": [
        {
            "affiliations": [],
            "name": "Euibeom Shin"
        }
    ],
    "id": "SP:335a16ee4519941355dbbd03dec82814f90e6f57",
    "references": [
        {
            "authors": [
                "HC Kimko",
                "SB Duffull"
            ],
            "title": "Simulation for designing clinical trials: a pharmacokineticpharmacodynamic modeling perspective",
            "year": 2003
        },
        {
            "authors": [
                "HC Kimko",
                "CC Peck"
            ],
            "title": "American Association of Pharmaceutical Scientists",
            "venue": "New York: AAPS Press : Springer; xvi,",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "Page 2/13\nPurpose To systematically assess the ChatGPT large language model on diverse tasks relevant to pharmacokinetic data analysis.\nMethods ChatGPT was evaluated with prototypical tasks related to report writing, code generation, noncompartmental analysis, and pharmacokinetic word problems. The writing task consisted of writing an introduction for this paper from a draft title. The coding tasks consisted of generating R code for semilogarithmic graphing of concentration-time pro les and calculating area under the curve and area under the moment curve from time zero to in nity. Pharmacokinetics word problems on single intravenous, extravascular bolus, and multiple dosing were taken from a pharmacokinetics textbook. Chain-of-thought and problem separation were assessed as prompt engineering strategies when errors occurred.\nResults ChatGPT showed satisfactory performance on the report writing, code generation tasks and provided accurate information on the principles and methods underlying pharmacokinetic data analysis. However, ChatGPT had high error rates in numerical calculations involving exponential functions. The outputs generated by ChatGPT were not reproducible: the precise content of the output was variable albeit not necessarily erroneous for different instances of the same prompt. Incorporation of prompt engineering strategies reduced but did not eliminate errors in numerical calculations.\nConclusions ChatGPT has the potential to become a powerful productivity tool for writing, knowledge encapsulation, and coding tasks in pharmacokinetic data analysis. The poor accuracy of ChatGPT in numerical calculations require resolution before it can be reliably used for PK and pharmacometrics data analysis."
        },
        {
            "heading": "INTRODUCTION",
            "text": "Large language models (LLM) such as ChatGPT from OpenAI 1, Bard from Google 2, and others are an emerging arti cial intelligence technology that has engendered great public interest. LLM are deep learning neural networks trained on a large body of text and other information that are capable of twoway interactions with users in a manner that approximates the complexity and nuance of human conversations.\nPage 3/13\nPharmacokinetics (PK) and pharmacodynamics (PK-PD) modeling is well established as an effective tool in the pre-clinical setting for understanding time courses of drug concentrations and effects, for interspecies scaling, and dose determination. Population modeling with non-linear mixed effects and Bayesian methods extends PK-PD modeling. It is particularly useful for interpreting sparse clinical data and for clinical trial simulations to de ne the scope of drug concentration and effect variability 3, 4. Every new innovator drug application submitted to the United States Food and Drug Administration contains PK-PD and population modeling data.\nPK-PD analyses requires individuals with specialized multi-disciplinary training and utilizes complex software tools with steep learning curves 5, 6. The model development processes require high levels of human intervention, and the interpretation of results requires expertise and experience. There is a shortage of quali ed manpower for the PK-PD analyst workforce needs of industry, regulatory agencies, and academia 5, 6.\nWe reasoned that powerful capabilities of LLM might be potentially harnessed in PK-PD data analyses settings in several ways, e.g., to facilitate self-learning of PK analysis concepts, for identifying the range of available software tools, to provide templates for coding, to conduct uncomplicated PK analyses, and for report preparation. While LLM are reasonably facile at interpreting lay language inputs with a conversational or informal tone, their effectiveness and utility can be further enhanced using structured prompts and exemplars. This has led to research into \u201cprompt engineering\u201d strategies that that can guide the underlying LLM into yielding results that more effective for speci c classes of tasks. Prompt engineering strategies such as chain-of-thought prompting, which provides the LLM a limited number of examples containing an input, the chain of thought, and the correct output, improves the performance of LLM for solving diverse arithmetic word problems and symbolic reasoning problems 7.\nThe main goals of this study are to obtain proof-of-concept evidence for utilizing LLM for PK data analysis and to serve as a primer that might motivate pharmaceutical scientists to further explore LLM for more advanced modeling and simulation tasks."
        },
        {
            "heading": "METHODS",
            "text": "ChatGPT Methods The default version of ChatGPT 4.0 2 was run at the chat.openai.com on a MacBook Air computer running macOS Ventura 13.5.1. Screenshots from individual ChatGPT runs were saved.\nAll ChatGPT experiments were replicated \u2265 3 times. For experiments involving numerical calculations, the accuracy rate was computed as the number of correct answers based on 10 replicate experiments. Numerical answers from ChatGPT were considered correct if they were within \u00b1 0.5% of the true value; this corresponds to the maximum percent error with an answer expressed to three signi cant gures. ChatGPT history was cleared between experiments.\nPage 4/13\nAccuracy rates obtained with and without prompt engineering directives were compared. Speci c prompt engineering directives were introduced for queries that elicited high error rates. These included the inclusion of phrases such as \u201cReason it out before action\u201d, \u201cYou are a good calculator\u201d, \u201cProvide the most detailed responses\u201d, breaking down multi-part problems into separate questions (i.e., asking one question at a time), and breaking down multi-part problems into separate questions combined with the directive \u201cprovide steps\u201d. Case Study 1. Scienti c Report Generation In the rst experiment, ChatGPT was tasked to generate an outline for a four-paragraph introduction section of a research paper with a title \u201cEvaluation of Prompt Engineering Strategies for PKPD Analysis with ChatGPT\u201d, a draft title that was among those considered for this paper. In the second experiment, ChatGPT was prompted to write a four-paragraph introduction section with references for the research paper with appropriate references. Case Study 2. Software Code Generation Both experiments in this Case Study used concentration-time data shown in Fig.\u00a02A from Patient ID-1 in the vignette in the Ubiquity software package 8.\nWe requested code from ChatGPT for creating a semi-logarithmic graph and the methods for computing the area under the curve (AUC) and area under the moment curve (AUMC) from zero to in nity. No additional prompt engineering was done, and chat history was cleared between experiments.\nAUC and AUMC calculations used the prompt \u201cProvide R code to calculate the AUC and AUMC from time 0 to in nity for a non-compartment analysis for the following concentration(ng/ml) vs. time(hour) pro le. [TIME_HR]-(1,4,8,24,72,168,336,504,671.9999) [C_ng_ml]-(9953.8813, 9704.5133, 9383.7171, 8223.5475, 5685.306, 3118.7764, 1673.885, 1215.2236, 964.6353)\u201d\nThe code generated by ChatGPT was run using the R statistical program 9 in the RStudio environment on a MacBook Pro computer running macOS Catalina (10.15.7). Case Study 3. Pharmacokinetics Calculations Word Problems The word problems were selected from the textbook Clinical Pharmacokinetics: Concepts and Applications, 3rd Edition, by Rowland and Tozer 10. The answers from ChatGPT were compared to the answer key in the textbook. We assessed the capabilities of ChatGPT at solving word problems involving PK calculations related to intravenous (Chap.\u00a03, Problem 7) and extravascular bolus Chap.\u00a06, Problem 8) dosing, and multiple oral dosing.\nPage 5/13"
        },
        {
            "heading": "RESULTS",
            "text": "Case Study 1. Scienti c Report Generation As an LLM, ChatGPT is viewed as a potentially powerful tool to facilitate text generation and writing. In this Case Study, ChatGPT was prompted to provide an outline for the Introduction section of a research paper based on a title and the Introduction section with references. Figure 1 shows screenshots of the ChatGPT output from these two experiments, which were conducted no prompt engineering strategies other than clearly state the task.\nThe outline produced by ChatGPT is shown in Fig. 1A. The theme and purpose for each paragraph was clearly stated for each of the four paragraphs in the ChatGPT-generated outline. A bulleted list with the main points to be made in each paragraph followed. These results with the outline were considered promising given that limited information, i.e., title only with the PKPD abbreviation, was provided to the LLM.\nThe Introduction section is shown in Fig.\u00a01B. The generated Introduction section had four paragraphs and hewed to the outline in Fig.\u00a01A. The ChatGPT-generated Introduction section was viewed generally concordant with accepted writing standards in pharmaceutical science journals and contained two in-text citations and a reference section.\nWe passed the ChatGPT-generated Introduction section to the Grammarly typing assistance program, which contains a plagiarism check feature. The ChatGPT output was reported as plagiarism free.\nThe main weaknesses of the ChatGPT-generated Introduction section were its short length and the limited number of references. The short length can be attributed to the prompt statement, which speci cally requested only four paragraphs. The Introduction also lacked speci city regarding the PK-PD problems that might bene t from the use of ChatGPT and detail regarding prompt engineering strategies that have been proposed.\nNonetheless, ChatGPT\u2019s pro ciency in generating a plausible Introduction for a research manuscript with limited title information and no prompt engineering suggests its potential value as a productivity tool in the writing process.\nCase Study 2. Software Code Generation ChatGPT can generate code for programming tasks in several languages including Python and R, which are increasingly used in PK-PD data analysis.\nFigure 2A summarizes the prompt and data entered for ChatGPT and the results for the semi-log graphing. ChatGPT recommended the ggplot2 package 11 and generated R code for creating the graph. The R code ran without errors and generated a line graph (Fig.\u00a02B) titled \u201cSemi-log plot of Concentration\nPage 6/13\nover Time,\u201d with the x-axis and y-axis labeled Time (HR) and Concentration(ng/ml). The graph esthetics were simple as the minimal theme was recommended in the code. We did not seek to obtain the best t regression line to an exponential equation for the data.\nAUC and AUMC calculations are invariably the rst steps in non-compartmental analysis (NCA) of PK data. Some PK domain expertise is required for calculating the AUC and AUMC from time 0 to in nity since parametric interpolation is used to obtain the area of the region between the last observation time point and in nity.\nAs a rst step in our experiment, we investigated whether ChatGPT could provide a detailed description of the methodology needed for calculating AUC and AUMC. The Results in Fig.\u00a02C demonstrate that ChatGPT description is appropriate and complete \u2013 the integration and the extrapolation procedures are correctly described using equations where necessary.\nSeveral issues were encountered when conducting AUC and AUMC analyses with concentration-time pro le data from Patient ID-1. The ChatGPT-generated R code included useful comments on the procedural steps and ran without syntax errors, but the output was highly variable between occasions even with the same prompt. Despite these variability issues, ChatGPT used appropriate methodology, e.g., integration with the trapezoidal rule, extrapolation from the last observed time point to in nity in every case, which indicates that ChatGPT had generally accessed and identi ed the correct sources of information for NCA analyses. The accuracy of AUC calculations was 5/10 whereas accuracy of AUMC calculations was 1/10. The elimination rate constant was wrongly calculated in 5/10 experiments, and this was propagated to errors in AUC and AUMC values. In 3 of 10 experiments, ChatGPT assumed that the last two observations were on the terminal phase, in 5 of 10 experiments ChatGPT assumed the last three observations were on the terminal phase, and in one experiment, ChatGPT t an exponential function to all the data. In every experiment, the remainder of the R code for AUC calculations was correct in terms of trapezoidal rule and extrapolation from the last observed time point to in nity. The corresponding R code for AUMC calculations was correct in 3/10 experiments with most of the errors occurring in the term that extrapolated AUMC from the last time point to in nity.\nThe results indicate that ChatGPT can generate R code for basic PK analysis tasks such as creating graphs. In calculating metrics such as AUC and AUMC, ChatGPT is capable of correctly using the trapezoidal rule but is susceptible to range of errors.\nCase Study 3. Pharmacokinetics Calculations Word Problems Case Study 3A. ChatGPT was posed a ve-part single intravenous (IV) bolus dosing PK problem requiring calculation of a) volume of distribution, b) elimination half-life, c) total AUC, d) total clearance, e) plasma concentration at 70 minutes after dose administration.\nPage 7/13\nAn 88% accuracy rate was achieved was achieved without prompt engineering (Table 1); 83% of the errors occurred in part (e), which required exponential arithmetic and had an error rate of 50%. Interestingly, ChatGPT set up the exponential equation required for calculation correctly even for incorrect answers. The time in minutes was correctly converted to hours to match the units of the elimination rate constant of the exponential equation.\nPrompt engineering efforts were directed exclusively at part (e). The directive \"Reason it out before action\" did not improve accuracy whereas, \u201cYou are an accurate calculator\u201d, increased accuracy by 6%. When the question in part (e) was posed separately, ChatGPT accuracy increased from 50\u201370%. When further prompted to detail each calculation step, ChatGPT achieved 100% accuracy. However, when the entire question set was presented with the directive to detail each calculation step, accuracy decreased by 6%. This suggests that ChatGPT accuracy improves if calculation problems are compartmentalized.\nCase Study 3B. ChatGPT was posed a seven-part PK problem requiring bioavailability calculations after a single subcutaneous or oral dose. Data on AUC, observed half-life and fraction excreted unchanged in urine following intravenous, subcutaneous, and oral dosing were provided (see Table\u00a01).\nWithout any prompt engineering, the accuracy of 94.3% (4 errors in 70 experiments) was achieved (Table\u00a01). With the prompt engineering directive \"Reason it out before action\", the accuracy improved to 98.6% (1 error in 70 experiments). When each question in Case 3B was posed separately or with the directive \u201cYou are an accurate calculator\u201d, there were no errors (100% accuracy).\nCase Study 3C. This was a 3-part PK problem on multiple oral dosing given the concentration-time pro le for a single oral dose.\nChatGPT yielded an accuracy of 80% (2 errors in each of 10 experiments) on parts a and b-1 without any prompt engineering. The accuracy increased to 100% with the directive \u201cProvide each step of the calculation\u201d.\nChatGPT did not correctly solve part b-2, which required application of the superposition principle to correctly calculate the multiple dosing concentration pro les from the single oral dosing pro le, in any of the 10 experiments. ChatGPT used the equations for a model with linear absorption and onecompartment elimination and made errors in selecting the equation for steady-state trough concentrations. The directive used in parts a and b-1 did not yield accuracy improvements. However, the directive \u201cUse the superposition principle. The multiple oral dosing pro le is the linear superposition of the single oral dose pro le at each dose and adjust concentrations based on the ratio of the multiple dose pro le to the single dose pro le. Assume a steady state approximated happens after 4 doses\u201d yielded an accuracy of 2/10. With an exemplar based on a correct response, the accuracy only improved to 4/10.\nDISCUSSION\nPage 8/13\nWe investigated the strengths and weaknesses of the ChatGPT large language model in PK/PD data analysis and pharmacometrics. In our experimental design, we intentionally included diverse problems representative of the breadth of conceptual learning, quantitative analyses, visualization, coding, and report writing tasks required in the domain. We found that the scienti c information and analysis processes recommended by ChatGPT to be appropriate and accurate. There were challenges related to lack of determinism in the algorithm and arithmetic errors in the numerical calculations.\nCalculation word problems frequently require arithmetic, commonsense, and symbolic reasoning. Evidence suggests that LLM \u201cstruggle\u201d with calculation word problems, which is somewhat unexpected given the satisfactory performance on tasks such as writing and code generation 12, 13. Interestingly, experiments have shown that the accuracy of ChatGPT in calculations involving exponentiation involving decimals, logarithmic and trigonometric function are only ~ 50% 12, 13. The lack of numerical accuracy with exponentiation and logarithmic operations represents a signi cant limitation for PK and pharmacometric analyses. We encountered these issues in solving the AUMC and textbook PK calculation problems where we obtained an accuracy of ~ 50% for the calculations involving exponentials. Interestingly, in nearly every experiment with the wrong answers, ChatGPT had the correct mathematical expression but the wrong numerical calculations. We expect that these issues might fully resolve once LLM incorporate a calculator in the algorithm.\nThe outputs from ChatGPT can vary even when the same prompt is used, which makes it di cult to replicate any particular outcome in exact detail. In stochastic modeling, the lack of determinism is usually addressed by seeding the random number generator. It is not clear that an analogous strategy for resolving this problem is available in this setting because LLM and other generative AI algorithms produce random variates from a complex high dimensional joint distribution 14, 15. This limitation is problematic in the regulatory setting where reproducibility of results is critical.\nPrompt engineering is an approach to transfer knowledge from the user to the LLM 7 and has been shown to improve the performance on arithmetic word problems 16: e.g., prompting ChatGPT to show its work 17, or adding the chain-of-thought phrase \u201cLet\u2019s think step-by-step\u201d improved the accuracy of GPT-3 on arithmetic problems 18. We attempted chain-of-thought and problem separation prompt engineering techniques in the cases that we encountered high error rates. While ChatGPT accuracy improved with chain-of-thought prompting on arithmetic problems, we were not able to eliminate numerical calculations errors. This is concordant with the ndings of Chen et al. 18 who did not nd performance improvement for ChatGPT on arithmetic reasoning problems but noted utility in other problem areas. This was attributed to incorporation of chain-of-thought features into ChatGPT 18.\nGiven that ChatGPT is an emerging AI tool, there have not been many papers that have speci cally investigated its utility in PK analyses and pharmacometrics. One exception is the research by Cloesmeijer et al. 19 who investigated usefulness of ChatGPT for code generation; the R code generated was satisfactory but the NONMEM code contained errors. We investigated R code generation in Case Study 2\nPage 9/13\nbut did not investigate NONMEM code here. We also did not conduct calculations involving population PK modeling given the high error rates in the AUC and AUMC calculations. While we found that the R code generated ran without issues in RStudio, thorough reviewing and debugging for conceptual errors was required. It might be useful to evaluate ChatGPT code with a test bank of questions with known answers.\nOur results suggest that ChatGPT could be a useful productivity aid for writing, knowledge encapsulation, and programming tasks. However, the high rate of errors in arithmetic calculations could limit its utility for more complex tasks and diverse data analysis scenarios in PK data analysis and pharmacometrics. If the limitations are overcome, ChatGPT could become a valuable tool for automating all aspects of PK and pharmacometric data analysis. Prompt protocol research and rigorous performance evaluation, and validation studies on benchmark data sets will be required to build the con dence and certainty expected in the pharmaceutical regulatory environment.\nDeclarations FUNDING INFORMATION\nThis is unfunded research. Support from Grant MS190096 from the Department of Defense Multiple Sclerosis Research Program for the O ce of the Congressionally Directed Medical Research Programs (CDMRP) to the Ramanathan laboratory is gratefully acknowledged."
        },
        {
            "heading": "CONFLICT OF INTEREST DISCLOSURE",
            "text": "Euibeom Shin has no con icts.\u00a0\nDr. Murali Ramanathan received research funding from the National Multiple Sclerosis Society, Department of Defense, National Science Foundation, and National Institute of Neurological Diseases and Stroke. He receives royalty from a self-published textbook."
        },
        {
            "heading": "AUTHOR CONTRIBUTIONS",
            "text": "Euibeom Shin \u2013 Data analysis, manuscript preparation.\nMurali Ramanathan \u2013 Study concept and design, data analysis, manuscript preparation.\nFinancial Con icts:\u00a0See disclosure statement.\u00a0\nCon dentiality: Use of the information in this manuscript for commercial, non-commercial, research or purposes other than peer review not permitted prior to publication without expressed written permission of the author.\u00a0\nReferences 1. OpenAI (2023) ChatGPT (June 26 version) Large language model.\nPage 10/13\n2. Google AI (2023) Bard Large language model.\n3. Kimko HC, Duffull SB (2003) Simulation for designing clinical trials: a pharmacokineticpharmacodynamic modeling perspective. New York: Marcel Dekker; xviii, 396 p. p\n4. Kimko HC, Peck CC, American Association of Pharmaceutical Scientists (2011). Clinical trial simulations: applications and trends. New York: AAPS Press : Springer; xvi, 538 p. p\n5. Bonate PL, Barrett JS, Ait-Oudhia S, Brundage R, Corrigan B, Duffull S, Gastonguay M, Karlsson MO, Kijima S, Krause A, Lovern M, Neely M, Ouellet D, Plan EL, Rao GG, Standing J, Wilkins J, Zhu H (2023) Training the next generation of pharmacometric modelers: a multisector perspective. J Pharmacokinet Pharmacodyn. Epub 2023/08/13. https://www.ncbi.nlm.nih.gov/pubmed/37573528\n. Michelet R, Aulin LBS, Borghardt JM, Costa TD, Denti P, Ibarra M, Ma G, Meibohm B, Pillai GC, Schmidt S, Hennig S, Kloft C (2023) Barriers to global pharmacometrics: educational challenges and opportunities across the globe. CPT Pharmacometrics Syst Pharmacol 12(6):743\u2013747 Epub 2023/03/25. https://www.ncbi.nlm.nih.gov/pubmed/36960632\n7. White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert H, Elnashar A, Spencer-Smith J, Schmidt DC A prompt pattern catalog to enhance prompt engineering with ChatGPT. arxiv 2023;cs SE 2302:11382. https://arxiv.org/abs/2302.11382\n. Harrold JM, Abraham AK (2014) Ubiquity: a framework for physiological/mechanism-based pharmacokinetic/pharmacodynamic model development and deployment. J Pharmacokinet Pharmacodyn 41(2):141\u2013151 Epub 2014/03/13. https://www.ncbi.nlm.nih.gov/pubmed/24619141\n9. R Core Team (2017) R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria\n10. Rowland M, Tozer TN (1995) Clinical pharmacokinetics: concepts and applications. 3rd ed. Baltimore: Williams & Wilkins; xiv, 601 p. p\n11. Wickham H (2009) ggplot2: Elegant Graphics for Data Analysis. Use R. :1-212. ://WOS:000269437100014.\n12. Frieder S, Pinchetti L, Chevalier A, Gri ths R-R, Salvatori T, Lukasiewicz T, Petersen PC, Berner J Mathematical capabilities of ChatGPT. arXiv. 2023:arXiv:2301.13867v2. https://arxiv.org/abs/2301.13867\n13. Yuan Z, Yuan H, Tan C, Wang W, Huang S (2023) How well do large language models perform in arithmetic tasks? arXiv. :arXiv:2304.02015. https://arxiv.org/abs/2304.02015\n14. Nair R, Mohan DD, Frank S, Setlur S, Govindaraju V, Ramanathan M (2023) Generative adversarial networks for modelling clinical biomarker pro les with race/ethnicity. Br J Clin Pharmacol 89(5):1588\u20131600 Epub 2022/12/03. https://www.ncbi.nlm.nih.gov/pubmed/36460305\n15. Nair R, Mohan DD, Setlur S, Govindaraju V, Ramanathan M (2023) Generative models for age, race/ethnicity, and disease state dependence of physiological determinants of drug dosing. J Pharmacokinet Pharmacodyn 50(2):111\u2013122 Epub 2022/12/25. https://www.ncbi.nlm.nih.gov/pubmed/36565395\nPage 11/13\n1 . Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, Chi EH, Le QV, Zhou D (eds) (2022) Chain-ofthought prompting elicits reasoning in large language models. 36th Conference on Neural Information Processing Systems (NeurIPS 2022); ; New Orleans, LA: NeuroIPS Foundation\n17. Shakarian P, Koyyalamudi A, Ngu N, Mareedu L An independent evaluation of ChatGPT on mathematical word problems (MWP). arXiv. 2023:arXiv:2302.13814v2. https://arxiv.org/abs/2302.13814\n1 . Chen J, Chen L, Huang H, Zhou T (2023) When do you need chain-of-thought prompting for ChatGPT? arXiv. : arXiv:2304.03262v2. https://arxiv.org/abs/2301.13867\n19. Cloesmeijer M, Janssen A, Koopman S, Cnossen M, Mathot R (2023) ChatGPT in pharmacometrics? Potential opportunities and limitations. Authorea.\nTable\nTable 1 is available in the Supplementary Files section.\nFigures\nFigure 1\nPage 12/13\nResults from Case Study 1 evaluating the text generation capabilities of ChatGPT. Figure 1A summarizes the ChatGPT prompt and output when tasked to provide an outline for the Introduction section for a research manuscript with a draft version of the title for this paper. Figure 1B summarizes the corresponding ChatGPT output when it was tasked to write a complete Introduction section with references for a research manuscript with the same title as this paper. The white-on-black screenshots from ChatGPT were recolored to improve contrast.\nResults from Case Study 2 evaluating the code generation capabilities of ChatGPT and to obtain information regarding area under the curve (AUC) and area under the moment curve (AUMC) calculations.\nPage 13/13\nFigure 2A summarizes the ChatGPT prompt and output when tasked to provide R code for a semilogarithmic graph for a concentration-time data set. Figure 2B shows the graph generated when the code in Figure 2A was executed. Figure 2C summarizes the ChatGPT output when it was tasked to provide an outline of the methods used to compute AUC and AUMC from time zero to in nity. The white-on-black screenshots from ChatGPT were recolored to improve contrast.\nSupplementary Files\nThis is a list of supplementary les associated with this preprint. Click to download.\nTABLE1.docx"
        }
    ],
    "title": "Evaluation of Prompt Engineering Strategies for Pharmacokinetic Data Analysis with the ChatGPT Large Language Model",
    "year": 2023
}