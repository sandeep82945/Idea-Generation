{
    "abstractText": "In this study, an analytical solution of elliptical Kepler's equation, which gives the position of a celestial body moving in orbit as a function of time, is designed by using artificial intelligence techniques. For the eccentric anomaly, Kepler\u2019s equation is a transcendental equation with no precise analytical solution. In this paper, a high precision approximate analytical solution is presented to determine eccentric anomaly. The proposed method is based on machine learning where a non-iterative accurate solution is learned from training data. The solution to Kepler\u2019s solution is created using an artificial neural network based on the universal approximation theorem. Simulation results show that this solution is computationally efficient and has a constant complexity.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maozhang Zheng"
        },
        {
            "affiliations": [],
            "name": "Jianjun Luo"
        },
        {
            "affiliations": [],
            "name": "Zhaohui Dang"
        }
    ],
    "id": "SP:305ea8a2815e017086cbaf85b09d4a4d2c4832e9",
    "references": [
        {
            "authors": [
                "P. Colwell"
            ],
            "title": "Solving Kepler\u2019s Equation over Three Centuries",
            "year": 1993
        },
        {
            "authors": [
                "R.H. Battin"
            ],
            "title": "An introduction to the mathematics and methods of astrodynamics",
            "venue": "AIAA, (1999).",
            "year": 1999
        },
        {
            "authors": [
                "V. Raposo-Pulido",
                "J. Pelaez"
            ],
            "title": "An efficient code to solve the Kepler\u2019s equation hyperbolic case",
            "venue": "Astronomy & Astrophysics, 619, A129(2018).",
            "year": 2018
        },
        {
            "authors": [
                "W. Gander"
            ],
            "title": "On Halley\u2019s iteration method",
            "venue": "The American Mathematical Monthly, 92(2), 131-134(1985).",
            "year": 1985
        },
        {
            "authors": [
                "J.M.A. Danby",
                "T.M. Burkardt"
            ],
            "title": "The solution of Kepler\u2019s equation",
            "venue": "I. Celestial Mechanics, 31(2), 95-107(1983).",
            "year": 1983
        },
        {
            "authors": [
                "N.M. Swerdlow"
            ],
            "title": "Kepler\u2019s iterative solution to Kepler\u2019s equation",
            "venue": "Journal for the History of Astronomy, 31(4), 339- 341(2000).",
            "year": 2000
        },
        {
            "authors": [
                "B.A. Conway"
            ],
            "title": "An improved algorithm due to Laguerre for the solution of Kepler\u2019s equation",
            "venue": "Celestial mechanics, 39(2), 199-211(1986).",
            "year": 1986
        },
        {
            "authors": [
                "D. Mortari",
                "A. Elipe"
            ],
            "title": "Solving Kepler\u2019s equation using implicit functions",
            "venue": "Celestial Mechanics and Dynamical Astronomy, 118(1), 1-11(2014).",
            "year": 2014
        },
        {
            "authors": [
                "J.J. Davis",
                "C. Bruccoleri",
                "D. Mortari"
            ],
            "title": "Quasi constant-time orbit propagation without solving Kepler\u2019s equation",
            "venue": "Proc. of the 2008 Space Flight Mechanics Meeting Conf., (2008).",
            "year": 2008
        },
        {
            "authors": [
                "D. Mortari",
                "J.J. Davis",
                "C. Bruccoleri"
            ],
            "title": "Fast orbit propagation without solving Kepler\u2019s equation",
            "venue": "Proc. of the 2009 Space Flight Mechanics Meeting Conf., (2009).",
            "year": 2009
        },
        {
            "authors": [
                "J.J. Davis",
                "D. Mortari",
                "C. Bruccoleri"
            ],
            "title": "Sequential solution to Kepler\u2019s equation",
            "venue": "Celestial Mechanics and Dynamical Astronomy, 108(1), 59-72(2010).",
            "year": 2010
        },
        {
            "authors": [
                "A. Alshaery",
                "A. Ebaid"
            ],
            "title": "Accurate analytical periodic solution of the elliptical Kepler equation using the Adomian decomposition method",
            "venue": "Acta Astronautica, 140, 27-33(2017).",
            "year": 2017
        },
        {
            "authors": [
                "A.F. Aljohani",
                "R. Rach",
                "E. El-Zahar",
                "A.M. Wazwaz",
                "A. Ebaid"
            ],
            "title": "Solution of the hyperbolic Kepler equation by Adomian\u2019s asymptotic decomposition method",
            "venue": "Rom. Rep. Phys. 70(2), 112(2018).",
            "year": 2018
        },
        {
            "authors": [
                "D. Mortari",
                "A. Clocchiatti"
            ],
            "title": "Solving Kepler\u2019s equation using B\u00e9zier curves",
            "venue": "Celestial Mechanics and Dynamical Astronomy, 99(1), 45-57(2007).",
            "year": 2007
        },
        {
            "authors": [
                "A.R. Pimienta-Penalver"
            ],
            "title": "Accurate Kepler Equation Solver without Transcendental Function Evaluations], State University of New York",
            "year": 2013
        },
        {
            "authors": [
                "T. Fukushima"
            ],
            "title": "A method solving Kepler\u2019s equation without transcendental function evaluations",
            "venue": "Celestial Mechanics and Dynamical Astronomy, 66(3), 309-319(1996).",
            "year": 1996
        },
        {
            "authors": [
                "S.A. Feinstein",
                "C.A. McLaughlin"
            ],
            "title": "Dynamic discretization method for solving Kepler\u2019s equation",
            "venue": "Celestial Mechanics and Dynamical Astronomy, 96(1), 49-62(2006).",
            "year": 2006
        },
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe",
                "H. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks, 2(5), 359-366(1989). Proc. of SPIE Vol. 12506 125066B-7 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use",
            "year": 1989
        }
    ],
    "sections": [
        {
            "text": "orbit as a function of time, is designed by using artificial intelligence techniques. For the eccentric anomaly, Kepler\u2019s equation is a transcendental equation with no precise analytical solution. In this paper, a high precision approximate analytical solution is presented to determine eccentric anomaly. The proposed method is based on machine learning where a non-iterative accurate solution is learned from training data. The solution to Kepler\u2019s solution is created using an artificial neural network based on the universal approximation theorem. Simulation results show that this solution is computationally efficient and has a constant complexity.\nKeywords: Kepler\u2019s equation, machine learning, neural network"
        },
        {
            "heading": "1. INTRODUCTION",
            "text": "Kepler\u2019s equation (KE), which describes how a body moves under the influence of gravity, is derived in orbital mechanics. In his book Astronomia Nova [1], Johannes Kepler first discovered it. The elliptical KE is\nsinM E e E= \u2212 (1)\nwhere M , E and e designate mean anomaly, eccentric anomaly and eccentricity, respectively. Both M , E are\nfundamental parameters for determining the position of a moving celestial body in an elliptical orbit. KE is a transcendental equation because it involves a sine function. The exact analytical solution is unknown. It\u2019s simple to calculate M for a given value of E . However, because there is no closed-form solution, the inverse issue, which involves finding E while M and e is known, can be far more difficult. Usually, E needs to be estimated by series\nexpansions or numerical methods. Kepler himself approximate his equation by simple iteration in 1621 in his book Epitome of Copernican Astronomy [1]. KE is one of the core equations and has a lot of applications in orbital mechanics, therefore even though many academics have developed several ways to solve it, this subject continues to draw attention. For KE, finding a simple, accurate, and analytical solution is still of practical importance.\nIn this paper, the inverse problem is transformed to a machine learning (ML) problem, more exactly a supervised learning problem. By solving the supervised learning problem, a new solution is learned from the pre-calculated data. The proposed method, called ML-based method, takes the advantage of the great flexibility of neural networks (NNs). The approach is appropriate for extensive and quick orbit propagation since the complexity of the suggested algorithm is constant and independent of the eccentricity and transition time.\nThe rest of this paper is structured as follows. Existing approaches to resolving KE are carefully compiled and reviewed in Section 2. The primary idea behind the suggested machine learning-based solution is described in Section 3. And in Section 4, the effectiveness of the suggested strategy is verified using numerical simulation. Finally, Section 5 provides a summary of the result."
        },
        {
            "heading": "2. METHODS FOR SOLVING KEPLER\u2019S EQUATION",
            "text": "Many academics have looked into the inverse problem of KE. Finding approaches to arrive at the solution with high accuracy and minimal computational expense is the aim. Figure 1 illustrates how the various approaches to solving KE can be categorized. In this section, some very efficient and classical methods are introduced firstly as shown in Table 1. In Section 4, several of these techniques will be contrasted with our new technique in terms of their effectiveness and precision.\n# zhengmaozhang@mail.nwpu.edu.cn\n* zhaohuidang@nwpu.edu.cn\nThird International Conference on Computer Science and Communication Technology (ICCSCT 2022) edited by Yingfa Lu, Changbo Cheng, Proc. of SPIE Vol. 12506, 125066B\n\u00a9 2022 SPIE \u00b7 0277-786X \u00b7 doi: 10.1117/12.2661776\nProc. of SPIE Vol. 12506 125066B-1 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nTable 1. Ways to solve Kepler\u2019s equation.\nCategories Methods Advantages Disadvantages\nIterative method\nKepler\u2019s method [6] The algorithm is simple Low convergence speed; initial value required\nNewton\u2019s Method and it\u2019s variants\nNewton\u2019s method [2] Quadratic convergence Convergence depends on initial value\nHalley\u2019s method [4] Third degree convergence Second-order derivative is needed; Convergence depends on initial value\nDanby\u2019s method [5] Fourth degree convergence Third-order derivative is needed; Convergence depends on initial value\nConway\u2019s method [7] Convergence is guaranteed Convergence is slow; runtime depends on e and\nM\nMortari\u2019s method [8]\nHigh computational efficiency; convergence is guaranteed Runtime depends on e and M\nSequential method Davis et al. [9-11]\nTaking the information of the neighborhood point. Only suitable for the orbit propagation performed at constant time step\nExpansion method\nLagrange expansion [2] More efficient than Fourier-\nBessel expansion\nDiverge for some value of M when e < 0.662743419\nFourier-Bessel expansion [2] Convergence for all\neccentricity values\nNeed to compute Bessel functions of the first kind of order n; many terms are needed for large e and make it very computational expensive\nAD Method [12, 13] Convergence is faster than\nFourier-Bessel expansion\nnth-order derivative should be calculated; many terms are needed for large e; and make it very computational expensive\nTwo-steps method\nBezier curve method [14] Complexity of the algorithm\nis constant\nCubic algebraic equations should be solved and select the solution satisfying the condition\nP-C method [15] Complexity of the algorithm\nis constant The expression of the solution is very complex\nData-based method\nFukushima [16] and\nFeinstein [17]\nHigh computational efficiency Need pre-computed data and iterative process\nML-based method\nThe method proposed in this paper\nHigh efficiency; constant complexity; non-iterative; concise expression\nProc. of SPIE Vol. 12506 125066B-2 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nKepler himself created the first method for resolving his equation, and Newton\u2019s method came next. The idea of Newton\u2019s method [2, 3] (or Newton-Raphson\u2019s Method) is to approximate the KE by the first two terms in a Taylor series expansion. By extending the series to the n-term, a generalized Newton\u2019s method is obtained. When 3n = , the\ngenerated solution has an equivalent form to Halley\u2019s method [4]. When 4n = , the generated solution is same to Danby\u2019s method [5]. Moreover, truncating the series to the first-order leads to a method which is identical to Kepler\u2019s method [6]. Thus, Kepler\u2019s method can also be seen as a variant of Newton\u2019s method. In order to finding E while M and e is known, the initial value of E should be searched first. Then the repetition is continued until accuracy is\nsatisfied. On the other hand, Newton\u2019s technique and its variations may diverge if the original guess is not sufficiently close to the solution. According to Danby [5], as the degree of convergence increases, so do the initial value\u2019s sensitivity and the risk of divergence. Although Newton\u2019s approach and its variations perform well close to the solution, they lack a feature that would allow them to converge worldwide. The convergence of Conway\u2019s method [7] and Mortari\u2019s method [8] is guaranteed. However, these methods, like Newton\u2019s method and its variants, has different iteration steps for different M and e which means the runtime is dependent on M and e . Except the classification result as shown in\nTable 1, the methods can also be divided into two categories: single-point method and sequential method. KE is solved using the single-point method, but this method does not benefit from the fact that KE has been computed at the previous neighborhood point. While sequential method [9-11] calculates the present value using the value from the previous moment, making full use of the previous calculation. However, this kind of methods are only applicable to orbit propagation problems with fixed time steps. In addition, the propagation of the initial error causes such algorithms to be sensitive to the error of initial step. Expansion method, including Lagrange expansion [2], Fourier-Bessel expansion [2] and Adomian Decomposition (AD) Method [12, 13], expands the solution of KE into a polynomial consists of N terms. This kind of algorithms are more suitable for the case of small eccentricity, because when the eccentricity is large, more terms need to be reserved in order to improve the accuracy, which leads to low calculation efficiency. The two-step method is a non-iterative method which divides the problem of solving KE into two steps. First, calculate an initial estimate by using a technology, such as Bezier Curve [14] and series expansion [15]. Then correct the initial value to a high accuracy using a generalized Newton\u2019s method which is applied only once, rather than in a loop. Data-based method [16, 17] determines a start value according to the pre-computed data and then using iterative method to correct it.\nFor a number of issues in Celestial Mechanics, KE must be solved numerous times. Due to this, it is crucial for these applications to efficiently compute mean anomaly. For instance, in trajectory planning problems of the relative motion, the true anomaly should be calculated repeatedly which involves solving KE. The accuracy and speed of trajectory planning are significantly impacted by the effectiveness and stability of the eccentric anomaly calculation solution. Therefore, in this paper a method based on machine learning with low computation cost and constant complexity is presented."
        },
        {
            "heading": "3. MACHINE LEARNING-BASED SOLUTION",
            "text": "This section presents an innovative method for solving KE. E is a function of M and e , defined as ( , )E h M e= . The\nfunction\u2019s precise formulation cannot be expressed in closed form. Since 2 radians are swept off per period T , the eccentric anomaly may be calculated for any M in an infinite interval [0, )+ by\n([ ], ) [ ] 2 2\nM M E h e M\n  = + \u2212 (2)\nwhere [ ] denotes the remainder and 0 [ / 2 ] 2M    . In addition, the function ( , )E h M e= is centrosymmetric\nabout E M = = , namely\n(2 , ) 2 ( , ) , 0 , 2 2h M e h M e M M     \u2212 = \u2212    \u2212  (3)\nTherefore, the eccentric anomaly in the whole interval may be derived as long as the function ( , )E h M e= is computed\nin the interval 0 M   .\nIn supervised learning, a machine learning job, an input is mapped to an output using examples of input-output pairings. With each training example including an input item and the desired output value, or \"labelled training examples,\" a\nProc. of SPIE Vol. 12506 125066B-3 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nfunction is created. A supervised learning algorithm produces an inferred function by looking at the training data. The inferred function can be used to approximate the unknown function ( , )E h M e= in the inverse problem."
        },
        {
            "heading": "3.1 Feedforward neural network",
            "text": "Artificial NNs, or NNs for short, are a mathematical algorithmic model for distributed parallel information processing that replicates the behavioral properties of animal neural networks. Such networks rely on the system's complexity to process information by altering the interaction between a large number of internally linked nodes.\nUniversal approximation theory (UAT) [18] pointed out that any bounded and regular function from one finitedimension space to another can be approximated by an ordinary multilayer feedforward NN with any required accuracy. The network must have a sufficient number of neurons with a linear output layer and at least one hidden layer.\nAs shown in Figure 2, a feedforward NN may be used to estimate an unknown function ( , )h M e and produce an\nanalytical solution with high precision. For the approximation problem of the function ( , )E h M e= , there are two\ninputs and one output. One may think of the network as a composite function ( , )g M e . Thus, the estimate of eccentric\nanomaly is designed as\n\u02c6 ( , )E g M e= (4)\nIn hidden layers, one of the squashing functions, the hyperbolic tangent sigmoid transfer function, is used in this research as the active function\n2\n2 ( ) 1, 1,..., 1\n1 l x f x l L e\u2212 = \u2212 = \u2212 +\n(5)\nand in the output layer, the activation function is\n( )Lf x x= (6)"
        },
        {
            "heading": "3.2 Generation of training data",
            "text": "Due to the lack of an accurate solution to the eccentric anomaly, we are unable to get the exact value E when given M and e . Calculating M when given E and e is, fortunately, simple. As a result, Algorithm 1 is made to acquire the\ntraining data.\nAlgorithm 1: Data generation for training\nInput: The interval of true anomaly, d uE E E  , and the interval of eccentricity, d ue e e  ;\nOutput: Training data composed by N training examples ( ),, ..; 1,.k kkM E k Ne = ;\nProc. of SPIE Vol. 12506 125066B-4 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\n1 Discrete the interval of E into 1EN \u2212 equal parts;\n2 Discrete the interval of e into 1eN \u2212 equal parts;\n3 1k =\n4 for 1; ;Ei i N i=  ++\n5 for 1; ;ej j N j=  ++\n6 , sini j i j iM E e E \u2212\n7 ,( , ; ) ([ , ], )k k k i j j ie E eM M E\n8 k + +\n9 end\n10 end\n11 Return ( , ; ), 1,... ,k k k E ee E k N N N NM = = "
        },
        {
            "heading": "3.3 Final adjustment",
            "text": "UAT shows that given enough neurons, we can obtain solutions to KE with arbitrary accuracy. Too many neurons can make it difficult to train the network. Therefore, in order to reduce the training difficulty of the network, a small network\nis chosen as the learning model in this paper. In this case, although an estimate E\u0302 very close to the true value E can be obtained, the accuracy may not be high enough. To improve the accuracy, a single-step adjustment algorithm is applied\n\u02c6 \u02c6E E  = + (7)\nThe single-step adjustment algorithm designed by Halley's method [4] is as follows\n2\n\u02c6 \u02c6 \u02c62( sin )(1 cos )\n\u02c6 \u02c6 \u02c6 \u02c62(1 cos ) ( sin ) sin\nE e E M e E\ne E E e E M e E \n\u2212 \u2212 \u2212 = \u2212\n\u2212 \u2212 \u2212 \u2212 (8)\nTo increase the solution\u2019s accuracy, a single-step adjustment is performed. The analytical solution\u2019s complexity does not change because there is only one iteration."
        },
        {
            "heading": "4. NUMERICAL SIMULATION",
            "text": "As of October 2020, one hundred and ninety-nine elliptical orbit objects with eccentricity greater than 2 were discovered in the publicly accessible data (www.space-track.org). Excentricity is 0.898411 at its highest level. Therefore, the greatest limit of eccentricity, according to our assumptions, is 0.9. Decompose [0 , ]E  and [0 , 0.9]e 2 into\nsmaller intervals of equal size using eighty points respectively. 6400 training examples are obtained using Algorithm 1.\nIn this paper, we verify the performance of the proposed algorithm using a three-layer neural network that contains two hidden layers, each with nine neurons. Using Matlab toolbox, train the network with 6400 training examples and the residuals of the network at the examples are shown in Figure 3.\nIn practical engineering application, the inverse problems are solved with given eccentricities for different mean anomalies. Therefore, we take the max error in computed E under a given eccentricity, as shown in Figure 4, as index of accuracy of the algorithm. As shown in Figure 4 the algorithm proposed in this paper has high accuracy with final adjustment. When 0.89e = , the worst accuracy of 116.4 10\u2212 is obtained. For 0.7e  the ML-based method has a precision of 1510\u2212 .\nProc. of SPIE Vol. 12506 125066B-5 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use\nThe series methods and the ML-based method can be classified into one category based on the fact that these two kinds of methods both use known models to approximate unknown functions. Figures 5 and 6 show that for small eccentricity the Lagrange method and Fourier-Bessel method can obtain same precision with ML-based method even with a few terms. For large eccentricity the ML-based method has a higher precision. Additionally, the Lagrange series diverges when it surpasses 0.662743419, which suggests that adding additional terms produces poorer outcomes. For large eccentricity the number of items of Lagrange method and Fourier-Bessel method is greatly increased. This makes Lagrange method and Fourier-Bessel method very computational expensive as shown in Table 2. Table 2 shows that ML-based method is more efficient than the Lagrange method and Fourier-Bessel method. With increasing of the number of terms used for improving solutions\u2019 accuracy, the mean runtime of ML-based method is nearly constant and very small. However, the mean runtime of Lagrange method and Fourier-Bessel method increased dramatically.\nthe ML-based approach.\nTable 2. Mean runtime of Lagrange method, FB method and ML-based method.\nMethod Mean runtime, ms\nML-based 0.98\nLagrange method 38.9( 5N = ) 608.8( 25N = ) 2319.2( 50N = )\nFourier-Bessel method 264.2( 5N = ) 1819.2( 25N = ) 4744.7( 50N = )\nProc. of SPIE Vol. 12506 125066B-6 Downloaded From: https://www.spiedigitallibrary.org/conference-proceedings-of-spie on 18 Jan 2024 Terms of Use: https://www.spiedigitallibrary.org/terms-of-use"
        },
        {
            "heading": "5. CONCLUSION",
            "text": "In this paper a non-iterative method based on machine learning for solving KE is presented. Based on neural networks\u2019 tremendous flexibility, this technique can offer a high precision solution to KE. It can be seen as an analytical solving tool because the new method does not require any iterative computation or numerical integration. It also avoids the shortcoming of sensitivity of initial guess in some classical methods for solving KE. Numerical accuracy tests show that, the new algorithm behaves better than the most existed methods both in computational efficiency and accuracy. The approach outlined in this work, aside from the elliptical instance, may also be used to parabolic case and hyperbolic case."
        }
    ],
    "title": "Machine learning-based solution of Kepler\u2019s equation",
    "year": 2024
}