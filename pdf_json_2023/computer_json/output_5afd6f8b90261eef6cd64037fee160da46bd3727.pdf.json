{
    "abstractText": "We introduce efficient (1 + \u03b5)-approximation algorithms for the binary matrix factorization (BMF) problem, where the inputs are a matrix A \u2208 {0, 1}n\u00d7d, a rank parameter k > 0, as well as an accuracy parameter \u03b5 > 0, and the goal is to approximate A as a product of low-rank factors U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d. Equivalently, we want to find U and V that minimize the Frobenius loss \u2225UV\u2212A\u2225F . Before this work, the state-of-the-art for this problem was the approximation algorithm of Kumar et al. [ICML 2019], which achieves a C-approximation for some constant C \u2265 576. We give the first (1 + \u03b5)-approximation algorithm using running time singly exponential in k, where k is typically a small integer. Our techniques generalize to other common variants of the BMF problem, admitting bicriteria (1+\u03b5)-approximation algorithms for Lp loss functions and the setting where matrix operations are performed in F2. Our approach can be implemented in standard big data models, such as the streaming or distributed models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ameya Velingker"
        },
        {
            "affiliations": [],
            "name": "Maximilian V\u00f6tsch"
        },
        {
            "affiliations": [],
            "name": "David P. Woodruff"
        },
        {
            "affiliations": [],
            "name": "Samson Zhou"
        }
    ],
    "id": "SP:e7ce7f06bca3caf5eb27b0b00146b3265f4eff90",
    "references": [
        {
            "authors": [
                "Frank Ban",
                "Vijay Bhattiprolu",
                "Karl Bringmann",
                "Pavel Kolev",
                "Euiwoong Lee",
                "David P. Woodruff"
            ],
            "title": "A PTAS for lp-low rank approximation",
            "venue": "In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2019
        },
        {
            "authors": [
                "J Paul Brooks",
                "Jos\u00e9 H Dul\u00e1",
                "Edward L Boone"
            ],
            "title": "A pure l1-norm principal component analysis",
            "venue": "Computational statistics & data analysis,",
            "year": 2013
        },
        {
            "authors": [
                "Vladimir Braverman",
                "Dan Feldman",
                "Harry Lang",
                "Adiel Statman",
                "Samson Zhou"
            ],
            "title": "Efficient coreset constructions via sensitivity sampling",
            "venue": "In Asian Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Karl Bringmann",
                "Pavel Kolev",
                "David P. Woodruff"
            ],
            "title": "Approximation algorithms for l0-low rank approximation",
            "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Olivier Bachem",
                "Mario Lucic",
                "Andreas Krause"
            ],
            "title": "Practical coreset constructions for machine learning",
            "venue": "arXiv preprint arXiv:1703.06476,",
            "year": 2017
        },
        {
            "authors": [
                "Olivier Bachem",
                "Mario Lucic",
                "Andreas Krause"
            ],
            "title": "Scalable k-means clustering via lightweight coresets",
            "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2018
        },
        {
            "authors": [
                "Radim Belohl\u00e1vek",
                "Vil\u00e9m Vychodil"
            ],
            "title": "Discovery of optimal factors in binary data via a novel method of matrix decomposition",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2010
        },
        {
            "authors": [
                "Frank Ban",
                "David P. Woodruff",
                "Qiuyi (Richard) Zhang"
            ],
            "title": "Regularized weighted low rank approximation",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Parinya Chalermsook",
                "Sandy Heydrich",
                "Eugenia Holm",
                "Andreas Karrenbauer"
            ],
            "title": "Nearly tight approximability results for minimum biclique cover and partition",
            "venue": "In Algorithms - ESA 2014 - 22th Annual European Symposium, Proceedings,",
            "year": 2014
        },
        {
            "authors": [
                "L. Sunil Chandran",
                "Davis Issac",
                "Andreas Karrenbauer"
            ],
            "title": "On the parameterized complexity of biclique cover and partition",
            "venue": "In 11th International Symposium on Parameterized and Exact Computation,",
            "year": 2016
        },
        {
            "authors": [
                "Vincent Cohen-Addad",
                "Karthik C. S"
            ],
            "title": "Inapproximability of clustering in lp metrics",
            "venue": "In 60th IEEE Annual Symposium on Foundations of Computer Science,",
            "year": 2019
        },
        {
            "authors": [
                "Xue Chen",
                "Eric Price"
            ],
            "title": "Active regression via linear-sample sparsification",
            "venue": "In Conference on Learning Theory, COLT,",
            "year": 2019
        },
        {
            "authors": [
                "Vincent Cohen-Addad",
                "David Saulpic",
                "Chris Schwiegelshohn"
            ],
            "title": "A new coreset framework for clustering",
            "venue": "Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Sitan Chen",
                "Zhao Song",
                "Runzhou Tao",
                "Ruizhe Zhang"
            ],
            "title": "Symmetric sparse boolean matrix factorization and applications",
            "venue": "In 13th Innovations in Theoretical Computer Science Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Kenneth L. Clarkson",
                "David P. Woodruff"
            ],
            "title": "Low rank approximation and regression in input sparsity time",
            "venue": "In Symposium on Theory of Computing Conference,",
            "year": 2013
        },
        {
            "authors": [
                "Chen Dan",
                "Kristoffer Arnsfelt Hansen",
                "He Jiang",
                "Liwei Wang",
                "Yuchen Zhou"
            ],
            "title": "Low rank approximation of binary matrices: Column subset selection and generalizations",
            "venue": "In 43rd International Symposium on Mathematical Foundations of Computer Science,",
            "year": 2018
        },
        {
            "authors": [
                "Petros Drineas",
                "Michael W. Mahoney",
                "S. Muthukrishnan"
            ],
            "title": "Subspace sampling and relative-error matrix approximation: Column-row-based methods",
            "venue": "In Algorithms ESA",
            "year": 2006
        },
        {
            "authors": [
                "Fedor V. Fomin",
                "Petr A. Golovach",
                "Daniel Lokshtanov",
                "Fahad Panolan",
                "Saket Saurabh"
            ],
            "title": "Approximation schemes for low-rank binary matrix approximation problems",
            "venue": "ACM Trans. Algorithms,",
            "year": 2020
        },
        {
            "authors": [
                "Yinghua Fu",
                "Nianping Jiang",
                "Hong Sun"
            ],
            "title": "Binary matrix factorization and consensus algorithms",
            "venue": "In 2010 International Conference on Electrical and Control Engineering,",
            "year": 2010
        },
        {
            "authors": [
                "Herbert Fleischner",
                "Egbert Mujuni",
                "Dani\u00ebl Paulusma",
                "Stefan Szeider"
            ],
            "title": "Covering graphs with few complete bipartite subgraphs",
            "venue": "Theor. Comput. Sci.,",
            "year": 2009
        },
        {
            "authors": [
                "Dan Feldman",
                "Melanie Schmidt",
                "Christian Sohler"
            ],
            "title": "Turning big data into tiny data: Constant-size coresets for k-means, pca, and projective clustering",
            "venue": "SIAM J. Comput.,",
            "year": 2020
        },
        {
            "authors": [
                "Harold W. Gutch",
                "Peter Gruber",
                "Arie Yeredor",
                "Fabian J. Theis"
            ],
            "title": "ICA over finite fields - separability and algorithms",
            "venue": "Signal Process.,",
            "year": 2012
        },
        {
            "authors": [
                "Nicolas Gillis",
                "Stephen A. Vavasis"
            ],
            "title": "On the complexity of robust PCA and l1-norm low-rank matrix approximation",
            "venue": "Math. Oper. Res.,",
            "year": 2018
        },
        {
            "authors": [
                "Peng Jiang",
                "Jiming Peng",
                "Michael Heath",
                "Rui Yang"
            ],
            "title": "A clustering approach to constrained binary matrix factorization. In Data Mining and Knowledge Discovery for Big Data, pages 281\u2013303",
            "year": 2014
        },
        {
            "authors": [
                "Mehmet Koyut\u00fcrk",
                "Ananth Grama"
            ],
            "title": "PROXIMUS: a framework for analyzing very high dimensional discrete-attributed datasets",
            "venue": "In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2003
        },
        {
            "authors": [
                "Qifa Ke",
                "Takeo Kanade"
            ],
            "title": "Robust subspace computation using",
            "venue": "l1 norm,",
            "year": 2003
        },
        {
            "authors": [
                "Qifa Ke",
                "Takeo Kanade"
            ],
            "title": "Robust l1 norm factorization in the presence of outliers and missing data by alternative convex programming",
            "venue": "In IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2005
        },
        {
            "authors": [
                "Tapas Kanungo",
                "David M. Mount",
                "Nathan S. Netanyahu",
                "Christine D. Piatko",
                "Ruth Silverman",
                "Angela Y. Wu"
            ],
            "title": "A local search approximation algorithm for k-means clustering",
            "venue": "Comput. Geom.,",
            "year": 2004
        },
        {
            "authors": [
                "Ravi Kumar",
                "Rina Panigrahy",
                "Ali Rahimi",
                "David P. Woodruff"
            ],
            "title": "Faster algorithms for binary matrix factorization",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Ravi Kannan",
                "Santosh S. Vempala"
            ],
            "title": "Spectral algorithms. Found",
            "venue": "Trends Theor. Comput. Sci.,",
            "year": 2009
        },
        {
            "authors": [
                "Nojun Kwak"
            ],
            "title": "Principal component analysis based on l1-norm maximization",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2008
        },
        {
            "authors": [
                "Euiwoong Lee",
                "Melanie Schmidt",
                "John Wright"
            ],
            "title": "Improved and simplified inapproximability for k-means",
            "venue": "Inf. Process. Lett.,",
            "year": 2017
        },
        {
            "authors": [
                "Haibing Lu",
                "Jaideep Vaidya",
                "Vijayalakshmi Atluri",
                "Yuan Hong"
            ],
            "title": "Constraint-aware role mining via extended boolean matrix decomposition",
            "venue": "IEEE Trans. Dependable Secur. Comput.,",
            "year": 2012
        },
        {
            "authors": [
                "Raphael A. Meyer",
                "Cameron Musco",
                "Christopher Musco",
                "David P. Woodruff",
                "Samson Zhou"
            ],
            "title": "Near-linear sample complexity for lp polynomial regression",
            "venue": "In Proceedings of the 2023 ACM-SIAM Symposium on Discrete Algorithms,",
            "year": 2023
        },
        {
            "authors": [
                "Arvind V. Mahankali",
                "David P. Woodruff"
            ],
            "title": "Optimal l1 column subset selection and a fast PTAS for low rank approximation",
            "venue": "In Proceedings of the 2021 ACM-SIAM",
            "year": 2021
        },
        {
            "authors": [
                "James Orlin"
            ],
            "title": "Contentment in graph theory: covering graphs with cliques",
            "venue": "Indagationes Mathematicae (Proceedings),",
            "year": 1977
        },
        {
            "authors": [
                "Young Woong Park",
                "Diego Klabjan"
            ],
            "title": "Three iteratively reweighted least squares algorithms for l1-norm principal component analysis",
            "venue": "Knowledge and Information Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Aditya Parulekar",
                "Advait Parulekar",
                "Eric Price"
            ],
            "title": "L1 regression with lewis weights subsampling. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM",
            "year": 2021
        },
        {
            "authors": [
                "Amichai Painsky",
                "Saharon Rosset",
                "Meir Feder"
            ],
            "title": "Generalized independent component analysis over finite alphabets",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Amichai Painsky",
                "Saharon Rosset",
                "Meir Feder"
            ],
            "title": "Linear independent component analysis over finite fields: Algorithms and bounds",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2018
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "Siamak Ravanbakhsh",
                "Barnab\u00e1s P\u00f3czos",
                "Russell Greiner"
            ],
            "title": "Boolean matrix factorization and noisy completion via message passing",
            "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ilya P. Razenshteyn",
                "Zhao Song",
                "David P. Woodruff"
            ],
            "title": "Weighted low rank approximations with provable guarantees",
            "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2016
        },
        {
            "authors": [
                "Jouni K. Sepp\u00e4nen",
                "Ella Bingham",
                "Heikki Mannila"
            ],
            "title": "A simple algorithm for topic identification in 0-1 data",
            "venue": "In Knowledge Discovery in Databases: PKDD 2003, 7th European Conference on Principles and Practice of Knowledge Discovery in Databases,",
            "year": 2003
        },
        {
            "authors": [
                "Tom\u00e1s Singliar",
                "Milos Hauskrecht"
            ],
            "title": "Noisy-or component analysis and its application to link analysis",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2006
        },
        {
            "authors": [
                "Bao-Hong Shen",
                "Shuiwang Ji",
                "Jieping Ye"
            ],
            "title": "Mining discrete patterns via binary matrix factorization",
            "venue": "In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2009
        },
        {
            "authors": [
                "Zhao Song",
                "David P. Woodruff",
                "Peilin Zhong"
            ],
            "title": "Low rank approximation with entrywise l1-norm error",
            "venue": "Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2017
        },
        {
            "authors": [
                "Jaideep Vaidya",
                "Vijayalakshmi Atluri",
                "Qi Guo"
            ],
            "title": "The role mining problem: finding a minimal descriptive set of roles",
            "venue": "In Proceedings of the 12th ACM symposium on Access control models and technologies,",
            "year": 2007
        },
        {
            "authors": [
                "David P. Woodruff"
            ],
            "title": "Sketching as a tool for numerical linear algebra",
            "venue": "Found. Trends Theor. Comput. Sci.,",
            "year": 2014
        },
        {
            "authors": [
                "Arie Yeredor"
            ],
            "title": "Independent component analysis over galois fields of prime order",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 2011
        },
        {
            "authors": [
                "Zhongyuan Zhang",
                "Tao Li",
                "Chris Ding",
                "Xiangsun Zhang"
            ],
            "title": "Binary matrix factorization with applications",
            "venue": "In Seventh IEEE international conference on data mining (ICDM",
            "year": 2007
        },
        {
            "authors": [
                "Yinqiang Zheng",
                "Guangcan Liu",
                "Shigeki Sugimoto",
                "Shuicheng Yan",
                "Masatoshi Okutomi"
            ],
            "title": "Practical low-rank matrix approximation under robust l1-norm",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Marinka Zitnik",
                "Blaz Zupan"
            ],
            "title": "Nimfa: A python library for nonnegative matrix factorization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2012
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Low-rank approximation is a fundamental tool for factor analysis. The goal is to decompose several observed variables stored in the matrix A \u2208 Rn\u00d7d into a combination of k unobserved and uncorrelated variables called factors, represented by the matrices U \u2208 Rn\u00d7k and V \u2208 Rk\u00d7d. In particular, we want to solve the problem\nmin U\u2208Rn\u00d7k,V\u2208Rk\u00d7d\n\u2225UV \u2212A\u2225,\nfor some predetermined norm \u2225\u00b7\u2225. Identifying the factors can often decrease the number of relevant features in an observation and thus significantly improve interpretability. Another benefit is that low-rank matrices allow us to approximate the matrixA with its factorsU andV using only (n+d)k\n\u2217Google Research. E-mail: ameyav@google.com. \u2020Faculty of Computer Science, Univie Doctoral School Computer Science DoCS, University of Vienna. Email:\nmaximilian.voetsch@univie.ac.at. \u2021Carnegie Mellon University. E-mail: dwoodruf@andrew.cmu.edu. Work done while at Google Research. \u00a7UC Berkeley and Rice University. E-mail: samsonzhou@gmail.com.\nM. Vo\u0308tsch: This project has received funding from the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (Grant agreement No. 101019564 \u201cThe Design of Modern Fully Dynamic Data Structures (MoDynStruct)\u201d.\nar X\niv :2\n30 6.\n01 86\n9v 1\n[ cs\n.D S]\n2 J\nun 2\nparameters rather than the nd parameters needed to represent A. Moreover, for a vector x \u2208 Rd, we can approximate the matrix-vector multiplication Ax \u2248 UVx in time (n+d)k, while computing Ax requires nd time. These benefits make low-rank approximation one of the most widely used tools in machine learning, recommender systems, data science, statistics, computer vision, and natural language processing. In many of these applications, discrete or categorical datasets are typical. In this case, restricting the underlying factors to a discrete domain for interpretability often makes sense. For example, [KPRW19] observed that nearly half of the data sets in the UCI repository [DG17] are categorical and thus can be represented as binary matrices, possibly using multiple binary variables to represent each category.\nIn the binary matrix factorization (BMF) problem, the input matrix A \u2208 {0, 1}n\u00d7d is binary. Additionally, we are given an integer range parameter k, with 0 < k \u226a n, d. The goal is to approximate A by the factors U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d such that A \u2248 UV. The BMF problem restricts the general low-rank approximation problem to a discrete space, making finding good factors more challenging (see Section 1.3)."
        },
        {
            "heading": "1.1 Our Contributions",
            "text": "We present (1+ \u03b5)-approximation algorithms for the binary low-rank matrix factorization problem for several standard loss functions used in the general low-rank approximation problem. Table 1 summarizes our results.\nBinary matrix factorization. We first consider the minimization of the Frobenius norm, defined by \u2225A\u2212UV\u22252F = \u2211 i\u2208[n] \u2211 j\u2208d |Ai,j\u2212 (UV)i,j |2, where [n] := {1, . . . , n} and Ai,j denotes the entry in the i-th row and the j-th column of A. Intuitively, we can view this as finding a least-squares approximation of A.\nWe introduce the first (1+\u03b5)-approximation algorithm for the BMF problem that runs in singly exponential time. That is, we present an algorithm that, for any \u03b5 > 0, returns U\u2032 \u2208 {0, 1}n\u00d7k,V\u2032 \u2208 {0, 1}k\u00d7d with\n\u2225A\u2212U\u2032V\u2032\u22252F \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225A\u2212UV\u22252F .\nFor \u03b5 \u2208 (0, 1), our algorithm uses 2O\u0303(k2/\u03b54) poly(n, d) runtime and for \u03b5 \u2265 1, our algorithm uses 2O\u0303(k\n2) poly(n, d) runtime, where poly(n, d) denotes a polynomial in n and d. By comparison, [KPRW19] gave a C-approximation algorithm for the BMF problem also using runtime 2O\u0303(k 2) poly(n, d), but for some constant C \u2265 576. Though they did not attempt to optimize for C, their proofs employ multiple triangle inequalities that present a constant lower bound of at least 2 on C. See Section 1.2 for a more thorough discussion of the limitations of their approach. [FGL+20] introduced a (1+ \u03b5)-approximation algorithm for the BMF problem with rank-k factors. However, their algorithm uses time doubly exponential in k, specifically 2 2O(k) \u03b52 log2 1 \u03b5 poly(n, d), which [BBB+19] later improved to doubly exponential runtime 2 2O(k) \u03b52 log 1 \u03b5 poly(n, d), while also showing that time 2k \u2126(1)\nis necessary even for constant-factor approximation, under the Small Set Expansion Hypothesis and the Exponential Time Hypothesis.\nBMF with Lp loss. We also consider the more general problem of minimizing for Lp loss for a given p, defined as the optimization problem of minimizing \u2225A \u2212UV\u2225pp = \u2211 i\u2208[n] \u2211 j\u2208d |Ai,j \u2212 (UV)i,j |p. Of particular interest is the case p = 1, which corresponds to robust principal component analysis, and which has been proposed as an alternative to Frobenius norm low-rank approximation that is more robust to outliers, i.e., values that are far away from the majority of the data points [KK03, KK05, Kwa08, ZLS+12, BDB13, MKP14, SWZ17, PK18, BBB+19, MW21]. On the other hand, for p > 2, low-rank approximation with Lp error increasingly places higher priority on outliers, i.e., the larger entries of UV.\nWe present the first (1 + \u03b5)-approximation algorithm for the BMF problem that runs in singly exponential time, albeit at the cost of incurring logarithmic increases in the rank k, making it a bicriteria algorithm. Specifically, for any \u03b5 > 0, our algorithm returns U\u2032 \u2208 {0, 1}n\u00d7k\u2032 ,V\u2032 \u2208 {0, 1}k\u2032\u00d7d with\n\u2225A\u2212U\u2032V\u2032\u2225pp \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225A\u2212UV\u2225pp,\nwhere k\u2032 = O ( k log2 n\n\u03b52\n) . For \u03b5 \u2208 (0, 1), our algorithm uses 2poly(k/\u03b5) poly(n, d) runtime and for\n\u03b5 \u2265 1, our algorithm uses 2poly(k) poly(n, d) runtime. Previous work [KPRW19] gave a C-approxmiation algorithm for this problem, using singly exponential runtime 2poly(k) poly(n, d), without incurring a bicriteria loss in the rank k. However, their constant C \u2265 1222p\u22122 + 2p\u22121 is large and depends on p. Again, their use of multiple triangle inequalities in their argument bars this approach from being able to achieve a (1+\u03b5)-approximation. To our knowledge, no prior works achieved (1 + \u03b5)-approximation to BMF with Lp loss in singly exponential time.\nBMF on binary fields. Finally, we consider the case where all arithmetic operations are performed modulo two, i.e., in the finite field F2. Specifically, the (i, j)-th entry of UV is the inner product \u27e8Ui,V(j)\u27e9 of the i-th row of U and the j-th column of V, taken over F2. This model has been frequently used for dimensionality reduction for high-dimensional data with binary attributes [KG03, SJY09, JPHY14, DHJ+18] and independent component analysis, especially in the context of signal processing [Yer11, GGYT12, PRF15, PRF18]. This problem is also known as bipartite clique cover, the discrete basis problem, or minimal noise role mining and\nhas been well-studied in applications to association rule mining, database tiling, and topic modeling [SBM03, SH06, VAG07, MMG+08, BV10, LVAH12, CIK16, CSTZ22].\nWe introduce the first bicriteria (1 + \u03b5)-approximation algorithm for the BMF problem on binary fields that runs in singly exponential time. Specifically, for any \u03b5 > 0, our algorithm returns U\u2032 \u2208 {0, 1}n\u00d7k\u2032 ,V\u2032 \u2208 {0, 1}k\u2032\u00d7d with\n\u2225A\u2212U\u2032V\u2032\u2225pp \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225A\u2212UV\u2225pp,\nwhere k\u2032 = O ( k logn\n\u03b5\n) and all arithmetic operations are performed in F2. For \u03b5 \u2208 (0, 1), our\nalgorithm has running time 2poly(k/\u03b5) poly(n, d) and for \u03b5 \u2265 1, our algorithm has running time 2poly(k) poly(n, d).\nBy comparison, [KPRW19] gave a bicriteria C-approximation algorithm for the BMF problem on binary fields with running time 2poly(k) poly(n, d), for some constant C \u2265 40001. Even though their algorithm also gives a bicriteria guarantee, their approach, once again, inherently cannot achieve (1+\u03b5)-approximation. On the other hand, [FGL+20] achieved a (1+\u03b5)-approximation without a bicriteria guarantee, but their algorithm uses doubly exponential running time 2 2O(k) \u03b52 log2 1 \u03b5 poly(n, d), which [BBB+19] later improved to doubly exponential running time 2 2O(k) \u03b52 log 1\n\u03b5 poly(n, d), while also showing that running time doubly exponential in k is necessary for (1 + \u03b5)-approximation on F2.\nApplications to big data models. We remark that our algorithms are conducive to big data models. Specifically, our algorithmic ideas facilitate a two-pass algorithm in the streaming model, where either the rows or the columns of the input matrix arrive sequentially, and the goal is to perform binary low-rank approximation while using space sublinear in the size of the input matrix. Similarly, our approach can be used to achieve a two-round protocol in the distributed model, where either the rows or the columns of the input matrix are partitioned among several players, and the goal is to perform binary low-rank approximation while using total communication sublinear in the size of the input matrix. See Section 5 for a formal description of the problem settings and additional details."
        },
        {
            "heading": "1.2 Overview of Our Techniques",
            "text": "This section briefly overviews our approaches to achieving (1 + \u03b5)-approximation to the BMF problem. Alongside our techniques, we discuss why prior approaches for BMF fail to achieve (1 + \u03b5)-approximation.\nThe BMF problem under the Frobenius norm is stated as follows: Let U\u2217 \u2208 {0, 1}n\u00d7k and V\u2217 \u2208 {0, 1}k\u00d7d be optimal low-rank factors, so that\n\u2225U\u2217V\u2217 \u2212A\u22252F = min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F . (1)\nOur approach relies on the sketch-and-solve paradigm, and we ask of our sketch matrix S that it is an affine embedding, that is, given U\u2217 and A, for all V \u2208 {0, 1}k\u00d7d,\n(1\u2212 \u03b5)\u2225U\u2217V \u2212A\u22252F \u2264 \u2225SU\u2217V \u2212 SA\u22252F \u2264 (1 + \u03b5)\u2225U\u2217V \u2212A\u22252F .\nObserve that if S is an affine embedding, then we obtain a (1+ \u03b5)-approximation by solving for the minimizer V\u2217 in the sketched space. That is, given S and U\u2217, instead of solving Equation 1 for V\u2217, it suffices to solve\nargmin V\u2208{0,1}k\u00d7d\n\u2225SU\u2217V \u2212 SA\u22252F .\nGuessing the sketch matrix S. A general approach taken by [RSW16, KPRW19, BWZ19] for various low-rank approximation problems is first to choose S in a way so that there are not too many possibilities for the matrices SU\u2217 and SA and then find the minimizer V\u2217 for all guesses of SU\u2217 and SA. Note that this approach is delicate because it depends on the choice of the sketch matrix S. For example, if we chose S to be a dense matrix with random Gaussian entries, then since there are 2nk possibilities for the matrix U\u2217 \u2208 {0, 1}n\u00d7k, we cannot enumerate the possible matrices SU\u2217. Prior work [RSW16, KPRW19, BWZ19] made the key observation that if A (and thus U\u2217) has a small number of unique rows, then a matrix S that samples a small number of rows of A has only a small number of possibilities for SA.\nTo ensure that A has a small number of unique rows for the BMF problem, [KPRW19] first find a 2k-means clustering solution A\u0303 for the rows of A. Instead of solving the problem on A, they then solve BMF on the matrix A\u0303, where each row is replaced by the center the point is assigned to, yielding at most 2k unique rows. Finally, they note that \u2225U\u2217V\u2217\u2212A\u22252F is at least the 2k-means cost, as U\u2217V\u2217 has at most 2k unique rows. Now that A\u0303 has 2k unique rows, they can make all possible guesses for both SU\u2217 and SA\u0303 in time 2O\u0303(k 2). By using an algorithm of [KMN+04] that achieves roughly a 9-approximation to k-means clustering, [KPRW19] ultimately obtain a C-approximation to the BMF problem, for some C \u2265 576.\nShortcomings of previous work for (1 + \u03b5)-approximation. While [KPRW19] do not optimize for C, their approach fundamentally cannot achieve (1 + \u03b5)-approximation for BMF for the following reasons. First, they use a k-means clustering subroutine [KMN+04], (achieving roughly a 9-approximation) which due to hardness-of-approximation results [CK19, LSW17] can never achieve (1+\u03b5)-approximation, as there cannot exist a 1.07-approximation algorithm for k-means clustering unless P=NP. Moreover, even if a (1 + \u03b5)-approximate k-means clustering could be found, there is no guarantee that the cluster centers obtained by this algorithm are binary. That is, while UV has a specific form induced by the requirement that each factor must be binary, a solution to k-means clustering offers no such guarantee and may return Steiner points. Finally, [KPRW19] achieves a matrix S that roughly preserves SU\u2217 and SA. By generalizations of the triangle inequality, one can show that \u2225SU\u2217V\u2217 \u2212 SA\u22252F preserves a constant factor approximation to \u2225U\u2217V\u2217 \u2212A\u22252F , but not necessarily a (1 + \u03b5)-approximation.\nAnother related work, [FGL+20], reduces instances of BMF to constrained k-means clustering instances, where the constraints demand that the selected centers are linear combinations of binary vectors. The core part of their work is to design a sampling-based algorithm for solving binaryconstrained clustering instances, and the result on BMF is a corollary. Constrained clustering is a harder problem than BMF with Frobenius loss, so it is unclear how one might improve the doubly exponential running time using this approach.\nOur approach: computing a strong coreset. We first reduce the number of unique rows in A by computing a strong coreset A\u0303 for A. The strong coreset has the property that for any choices\nof U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d, there exists X \u2208 {0, 1}n\u00d7k such that\n(1\u2212 \u03b5)\u2225UV \u2212A\u22252F \u2264 \u2225XV \u2212 A\u0303\u22252F \u2264 (1 + \u03b5)\u2225UV \u2212A\u22252F .\nTherefore, we instead first solve the low-rank approximation problem on A\u0303 first. Crucially, we choose A\u0303 to have 2poly(k/\u03b5) unique rows so then for a matrix S that samples poly(k/\u03b5) rows, there are 2poly(k/\u03b5) possibilities for SA\u0303, so we can make all possible guesses for both SU\u2217 and SA\u0303. Unfortunately, we still have the problem that \u2225SU\u2217V\u2217 \u2212 SA\u0303\u22252F does not even necessarily give a (1 + \u03b5)-approximation to \u2225U\u2217V\u2217 \u2212 A\u0303\u22252F .\nBinary matrix factorization. To that end, we show that when S is a leverage score sampling matrix, then S also satisfies an approximate matrix multiplication property. Therefore S can effectively be used for an affine embedding. That is, the minimizer to \u2225SU\u2217V\u2217 \u2212 SA\u0303\u22252F produces an (1+ \u03b5)-approximation to the cost of the optimal factors \u2225U\u2217V\u2217\u2212 A\u0303\u22252F . Thus, we can then solve\nV\u2032 = argmin V\u2208{0,1}k\u00d7d \u2225SU\u2217V \u2212 SA\u0303\u22252F U\u2032 = argmin U\u2208{0,1}n\u00d7k \u2225UV\u2032 \u2212A\u22252F ,\nwhere the latter optimization problem can be solved by iteratively optimizing over each row so that the total computation time is O ( 2kn ) rather than 2kn.\nBMF on binary fields. We again form the matrix A\u0303 by taking a strong coreset ofA, constructed using an algorithm that gives integer weight wi to each point, and then duplicating the rows to form A\u0303. That is, if the i-th row Ai of A is sampled with weight wi in the coreset, then A\u0303 will contain wi repetitions of the row Ai. We want to use the same approach for binary fields to make guesses for SU\u2217 and SA\u0303. However, it is no longer true that S will provide an affine embedding over F2, in part because the subspace embedding property of S computes leverage scores of each row of U\u2217 and A with respect to general integers. Hence, we require a different approach for matrix operations over F2.\nInstead, we group the rows of A\u0303 by their number of repetitions, so that group Gj consists of the rows of A\u0303 that are repeated [(1 + \u03b5)j , (1 + \u03b5)j+1) times. That is, if Ai appears wi times in A\u0303, then it appears a single time in group Gj for j = \u230alogwi\u230b. We then perform entrywise L0 low-rank approximation over F2 for each of the groups Gj , which gives low-rank factors U(j) and V(j). We then compute U\u0303(j) by duplicating rows appropriately so that if Ai is in Gj , then we place the row of U(j) corresponding to Ai into the i-th row of U\u0303(j), for all i \u2208 [n]. Otherwise if Ai is not in Gj , then we set i-th row of U\u0303(j) to be the all zeros row. We compute V(j) by padding accordingly and then collect\nU\u0303 = [ U\u0303(0)| . . . |U\u0303(\u2113) ] , V\u0303\u2190 V\u0303(0) \u25e6 . . . \u25e6 V\u0303(i),\nwhere [ U\u0303(0)| . . . |U\u0303(\u2113) ] denotes horizontal concatenation of matrices and V\u0303(0) \u25e6 . . . \u25e6 V\u0303(i) denotes\nvertical concatenation (stacking) of matrices, to achieve bicriteria low-rank approximations U\u0303 and V\u0303 to A\u0303. Finally, to achieve bicriteria factors U\u2032 and V\u2032 to A, we ensure that U\u2032 achieves the same block structure as U\u0303.\nBMF with Lp loss. We would again like to use the same approach as our (1+ \u03b5)-approximation algorithm for BMF with Frobenius loss. To that end, we observe that a coreset construction for clustering under Lp metrics rather than Euclidean distance is known, which we can use to construct A\u0303. However, the challenge is that no known sampling matrix S guarantees an affine embedding. One might hope that recent results on active Lp regression [CP19, PPP21, MMWY22, MMM\n+22, MMM+23] can provide such a tool. Unfortunately, adapting these techniques would still require taking a union bound over a number of columns, which would result in the sampling matrix having too many rows for our desired runtime.\nInstead, we invoke the coreset construction on the rows and the columns so that A\u0303 has a small number of distinct rows and columns. We again partition the rows of A\u0303 into groups based on their frequency, but now we further partition the groups based on the frequency of the columns. Thus, it remains to solve BMF with Lp loss on the partition, each part of which has a small number of rows and columns. Since the contribution of each row toward the overall loss is small (because there is a small number of columns), we show that there exists a matrix that samples poly(k/\u03b5) rows of each partition that finally achieves the desired affine embedding. Therefore, we can solve the problem on each partition, pad the factors accordingly, and build the bicriteria factors as in the binary field case."
        },
        {
            "heading": "1.3 Motivation and Related Work",
            "text": "Low-rank approximation is one of the fundamental problems of machine learning and data science. Therefore, it has received extensive attention, e.g., see the surveys [KV09, Mah11, Woo14]. When the underlying loss function is the Frobenius norm, the low-rank approximation problem can be optimally solved via singular value decomposition (SVD). However, when we restrict both the observed input A and the factors U,V to binary matrices, the SVD no longer guarantees optimal factors. In fact, many restricted variants of low-rank approximation are NP-hard [RSW16, SWZ17, KPRW19, BBB+19, BWZ19, FGL+20, MW21].\nMotivation and background for BMF. The BMF problem has applications to graph partitioning [CIK16], low-density parity-check codes [RPG16], and optimizing passive organic LED (OLED) displays [KPRW19]. Observe that we can use A to encode the incidence matrix of the bipartite graph with n vertices on the left side of the bipartition and d vertices on the right side so that Ai,j = 1 if and only if there exists an edge connecting the i-th vertex on the left side with the j-th vertex on the right side. Then UV can be written as the sum of k rank-1 matrices, each encoding a different bipartite clique of the graph, i.e., a subset of vertices on the left and a subset of vertices on the right such that there exists an edge between every vertex on the left and every vertex on the right. It then follows that the BMF problem solves the bipartite clique partition problem [Orl77, FMPS09, CHHK14, Neu18], in which the goal is to find the smallest integer k such that the graph can be represented as a union of k bipartite cliques.\n[KPRW19] also present the following motivation for the BMF problem to improve the performance of passive OLED displays, which rapidly and sequentially illuminate rows of lights to render an image in a manner so that the human eye integrates this sequence of lights into a complete image. However, [KPRW19] observed that passive OLED displays could illuminate many rows simultaneously, provided the image being shown is a rank-1 matrix and that the apparent brightness of an image is inversely proportional to the rank of the decomposition. Thus [KPRW19] notes that BMF can be used to not only find a low-rank decomposition that illuminates pixels in a way that\nseems brighter to the viewer but also achieves binary restrictions on the decomposition in order to use simple and inexpensive voltage drivers on the rows and columns, rather than a more expensive bank of video-rate digital to analog-to-digital converters.\nBMF with Frobenius loss. [KPRW19] first gave a constant factor approximation algorithm for the BMF problem using runtime 2O\u0303(k 2) poly(n, d), i.e., singly exponential time. [FGL+20] introduced a (1 + \u03b5)-approximation to the BMF problem with rank-k factors, but their algorithm uses doubly exponential time, specifically runtime 2 2O(k) \u03b52 log2 1 \u03b5 poly(n, d), which was later improved to doubly exponential runtime 2 2O(k) \u03b52 log 1 \u03b5 poly(n, d) by [BBB+19], who also showed that 2k \u2126(1)\nruntime is necessary even for constant-factor approximation, under the Small Set Expansion Hypothesis and the Exponential Time Hypothesis. By introducing sparsity constraints on the rows of U and V, [CSTZ22] provide an alternate parametrization of the runtime, though, at the cost of runtime quasipolynomial in n and d.\nBMF on binary fields. Binary matrix factorization is particularly suited for datasets involving binary data. Thus, the problem is well-motivated for binary fields when performing dimensionality reduction on high-dimension datasets [KG03]. To this end, many heuristics have been developed for this problem [KG03, SJY09, FJS10, JPHY14], due to its NP-hardness [GV18, DHJ+18].\nFor the special case of k = 1, [SJY09] first gave a 2-approximation algorithm that uses polynomial time through a relaxation of integer linear programming. Subsequently, [JPHY14] produced a simpler approach, and [BKW17] introduced a sublinear time algorithm. For general k, [KPRW19] gave a constant factor approximation algorithm using runtime 2poly(k) poly(n, d), i.e., singly exponential time, at the expense of a bicriteria solution, i.e., factors with rank k\u2032 = O (k log n). [FGL+20] introduced a (1 + \u03b5)-approximation to the BMF problem with rank-k factors, but their algorithm uses doubly exponential time, specifically runtime 2 2O(k) \u03b52 log2 1 \u03b5 poly(n, d), which was later improved to doubly exponential runtime 2 2O(k) \u03b52 log 1\n\u03b5 poly(n, d) by [BBB+19], who also showed that doubly exponential runtime is necessary for (1 + \u03b5)-approximation without bicriteria relaxation under the Exponential Time Hypothesis.\nBMF with Lp loss. Using more general Lp loss functions can result in drastically different behaviors of the optimal low-rank factors for the BMF problem. For example, the low-rank factors for p > 2 are penalized more when the corresponding entries of UV are large, and thus may choose to prioritize a larger number of small entries that do not match A rather than a single large entry. On the other hand, p = 1 corresponds to robust principal component analysis, which yields factors that are more robust to outliers in the data [KK03, KK05, Kwa08, ZLS+12, BDB13, MKP14, SWZ17, PK18, BBB+19, MW21]. The first approximation algorithm with provable guarantees for L1 low-rank approximation on the reals was given by [SWZ17]. They achieved poly(k) \u00b7 log dapproximation in roughly O (nd) time. For constant k, [SWZ17] further achieved constant-factor approximation in polynomial time.\nWhen we restrict the inputs and factors to be binary, [KPRW19] observed that p = 1 corresponds to minimizing the number of edges in the symmetric difference between an unweighted bipartite graph G and its approximation H, which is the multiset union of k bicliques. Here we represent the graph G with n and d vertices on the bipartition\u2019s left- and right-hand side, respectively, through\nits edge incidence matrix A. Similarly, we have Ui,j = 1 if and only if the i-th vertex on the left bipartition is in the j-th biclique and Vi,j = 1 if and only if the j-th vertex on the right bipartition is in the i-th biclique. Then we have \u2225UV \u2212A\u22251 = |E(G)\u25b3E(H)|. [CIK16] showed how to solve the exact version of the problem, i.e., to recover U,V under the promise that A = UV, using 2O(k 2) poly(n, d) time. [KPRW19] recently gave the first constant-factor approximation algorithm for this problem, achieving a C-approximation using 2poly(k) poly(n, d) time, for some constant C \u2265 1222p\u22122 + 2p\u22121."
        },
        {
            "heading": "1.4 Preliminaries",
            "text": "For an integer n > 0, we use [n] to denote the set {1, 2, . . . , n}. We use poly(n) to represent a fixed polynomial in n and more generally, poly(n1, . . . , nk) to represent a fixed multivariate polynomial in n1, . . . , nk. For a function f(n1, . . . , nk), we use O\u0303 (f(n1, . . . , nk)) to denote f(n1, . . . , nk) \u00b7 poly(log f(n1, . . . , nk)).\nWe generally use bold-font variables to denote matrices. For a matrix A \u2208 Rn\u00d7d, we use Ai to denote the i-th row of A and A(j) to denote the j-th column of A. We use Ai,j to denote the entry in the i-th row and j-th column of A. For p \u2265 1, we write the entrywise Lp norm of A as\n\u2225A\u2225p = \u2211 i\u2208[n] \u2211 j\u2208[d] Api,j 1/p . The Frobenius norm of A, denoted \u2225A\u2225F is simply the entrywise L2 norm of A:\n\u2225A\u2225F = \u2211 i\u2208[n] \u2211 j\u2208[d] A2i,j 1/2 . The entrywise L0 norm of A is\n\u2225A\u22250 = |{(i, j) | i \u2208 [n], j \u2208 [d] : Ai,j \u0338= 0}| .\nWe use \u25e6 to denote vertical stacking of matrices, so that\nA(1) \u25e6 . . . \u25e6A(m) = A (1) ...\nA(m)  . For a set X of n points in Rd weighted by a function w, the k-means clustering cost of X with\nrespect to a set S of k centers is defined as Cost(X,S,w) := \u2211 x\u2208X w(x) \u00b7min s\u2208S \u2225x\u2212 s\u222522.\nWhen the weights w are uniformly unit across all points in X, we simply write Cost(X,S) = Cost(X,S,w).\nOne of the core ingredients for avoiding the triangle inequality and achieving (1+\u03b5)-approximation is our use of coresets for k-means clustering:\nDefinition 1.1 (Strong coreset). Given an accuracy parameter \u03b5 > 0 and a set X of n points in Rd, we say that a subset C of X with weights w is a strong \u03b5-coreset of X for the k-means clustering problem if for any set S of k points in Rd, we have\n(1\u2212 \u03b5)Cost(X,S) \u2264 Cost(C, S,w) \u2264 (1 + \u03b5)Cost(X,S).\nMany coreset construction exist in the literature, and the goal is to minimize |C|, the size of the coreset, while preserving (1 \u00b1 \u03b5)-approximate cost for all sets of k centers. If the points lie in Rd, we can find coresets of size O\u0303 ( poly(k, d, \u03f5\u22121) ) , i.e., the size is independent of n.\nLeverage scores. Finally, we recall the notion of a leverage score sampling matrix. For a matrix A \u2208 Rn\u00d7d, the leverage score of row ai with i \u2208 [n] is defined as ai(A\u22a4A)\u22121a\u22a4i . We can use the leverage scores to generate a random leverage score sampling matrix as follows:\nTheorem 1.2 (Leverage score sampling matrix). [DMM06a, DMM06b, Mag10, Woo14] Let C > 1 be a universal constant and \u03b1 > 1 be a parameter. Given a matrix A \u2208 Rn\u00d7d, let \u2113i be the leverage score of the i-th row of A. Suppose pi \u2208 [ min ( 1, C\u2113i log k\n\u03b52\n) ,min ( 1, C\u03b1\u2113i log k\n\u03b52\n)] for all i \u2208 [n].\nFor m := O ( \u03b1 \u03b52 d log d ) , let S \u2208 Rm\u00d7n be generated so that each row of S randomly selects row j \u2208 [n] with probability proportional to pj and rescales the row by 1\u221ampj . Then with probability at least 0.99, we have that simultaneously for all vectors x \u2208 Rd,\n(1\u2212 \u03b5)\u2225Ax\u22252 \u2264 \u2225SAx\u22252 \u2264 (1 + \u03b5)\u2225Ax\u22252.\nThe main point of Theorem 1.2 is that given constant-factor approximations pi to the leverage scores \u2113i, it suffices to sample O (d log d) rows of A to achieve a constant-factor subspace embedding of A, and similar bounds can be achieved for (1 + \u03b5)-approximate subspace embeddings. Finally, we remark that S can be decomposed as the product of matrices DT, where T \u2208 Rm\u00d7n is a sparse matrix with a single one per row, denoting the selection of a row for the purposes of leverage score sampling and D is the diagonal matrix with the corresponding scaling factor, i.e., the i-th diagonal entry of D is set to 1\u221ampj if the j-th row of A is selected for the i-th sample."
        },
        {
            "heading": "2 Binary Low-Rank Approximation",
            "text": "In this section, we present a (1 + \u03b5)-approximation algorithm for binary low-rank approximation with Frobenius norm loss, where to goal is to find matrices U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d to minimize \u2225UV \u2212A\u22252F . Suppose optimal low-rank factors are U\u2217 \u2208 {0, 1}n\u00d7k and V\u2217 \u2208 {0, 1}k\u00d7d, so that\n\u2225U\u2217V\u2217 \u2212A\u22252F = min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F .\nObserve that if we knew matrices SU\u2217 and SA so that for all V \u2208 {0, 1}k\u00d7d,\n(1\u2212 \u03b5)\u2225U\u2217V \u2212A\u22252F \u2264 \u2225SU\u2217V \u2212 SA\u22252F \u2264 (1 + \u03b5)\u2225U\u2217V \u2212A\u22252F ,\nthen we could find a (1 + \u03b5)-approximate solution for V\u2217 by solving the problem\nargmin V\u2208{0,1}k\u00d7d\n\u2225SU\u2217V \u2212 SA\u22252F\ninstead. We would like to make guesses for the matrices SU\u2217 and SA, but first we must ensure there are not too many possibilities for these matrices. For example, if we chose S to be a dense matrix with random gaussian entries, then SU\u2217 could have too many possibilities because without additional information, there are 2nk possibilities for the matrix U\u2217 \u2208 {0, 1}n\u00d7k. We can instead choose S to be a leverage score sampling matrix, which samples rows from U\u2217 and A. Since each row of U\u2217 has dimension k, then there are at most 2k distinct possibilities for each of the rows of U\u2217. On the other hand, A \u2208 {0, 1}n\u00d7d, so there may be 2d distinct possibilities for the rows of A, which is too many to guess.\nThus we first reduce the number of unique rows in A by computing a strong coreset A\u0303 for A. The strong coreset has the property that for any choices of U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d, there exists X \u2208 {0, 1}n\u00d7k such that\n(1\u2212 \u03b5)\u2225UV \u2212A\u22252F \u2264 \u2225XV \u2212 A\u0303\u22252F \u2264 (1 + \u03b5)\u2225UV \u2212A\u22252F .\nTherefore, we instead first solve the low-rank approximation problem on A\u0303 first. Crucially, A\u0303 has 2poly(k/\u03b5) unique rows so then for a matrix S that samples poly(k/\u03b5) rows, there are ( 2poly(k/\u03b5)\npoly(k/\u03b5)\n) =\n2poly(k/\u03b5) possible choices of SA\u0303, so we can enumerate all of them for both SU\u2217 and SA\u0303. We can then solve\nV\u2032 = argmin V\u2208{0,1}k\u00d7d \u2225SU\u2217V \u2212 SA\u0303\u22252F\nand U\u2032 = argmin\nU\u2208{0,1}n\u00d7k \u2225UV\u2032 \u2212A\u22252F ,\nwhere the latter optimization problem can be solved by iteratively optimizing over each row, so that the total computation time is O ( 2kn ) rather than 2kn. We give the full algorithm in Algorithm 4\nand the subroutine for optimizing with respect to A\u0303 in Algorithm 3. We give the subroutines for solving for V\u2032 and U\u2032 in Algorithm 1 and Algorithm 2, respectively.\nAlgorithm 1 Algorithm for computing optimal V given U\nInput: A\u0303 \u2208 {0, 1}N\u00d7d, U \u2208 {0, 1}N\u00d7k Output: V\u2032 = argminV\u2208{0,1}k\u00d7d \u2225UV \u2212 A\u0303\u2225F 1: for i = 1 to i = d do \u25b7Optimize for each column individually 2: Set V\u2032(i) = argminV(i)\u2208{0,1}k\u00d71 \u2225UV(i) \u2212 A\u0303(i)\u22252 \u25b7Enumerate over all 2k possible binary\nvectors 3: return V\u2032 = [ V\u2032(1)| . . . |V\u2032(d) ] First, we recall that leverage score sampling matrices preserve approximate matrix multiplica-\ntion.\nLemma 2.1 (Lemma 32 in [CW13]). Let U \u2208 RN\u00d7k have orthonormal columns, A\u0303 \u2208 {0, 1}N\u00d7d, and S \u2208 Rm\u00d7N be a leverage score sampling matrix for U with m = O ( 1 \u03b52 ) rows. Then,\nPr [ \u2225U\u22a4S\u22a4SA\u0303\u2212U\u22a4A\u0303\u22252F < \u03b52\u2225U\u22252F \u2225A\u0303\u22252F ] \u2265 0.99.\nNext, we recall that leverage score sampling matrices give subspace embeddings.\nAlgorithm 2 Algorithm for computing optimal U given V\nInput: A\u0303 \u2208 {0, 1}N\u00d7d, V \u2208 {0, 1}k\u00d7d Output: U\u2032 = argminU\u2208{0,1}N\u00d7k \u2225UV \u2212 A\u0303\u2225F 1: for i = 1 to i = N do \u25b7Optimize for each row individually 2: Set U\u2032i = argminUi\u2208{0,1}1\u00d7k \u2225UiV \u2212 A\u0303i\u22252 \u25b7Enumerate over all 2 k possible binary vectors\n3: return U\u2032 = U\u20321 \u25e6 . . . \u25e6U\u2032N\nAlgorithm 3 Low-rank approximation for matrix A\u0303 with t distinct rows\nInput: A\u0303 \u2208 {0, 1}N\u00d7d with at most t distinct rows, rank parameter k, accuracy parameter \u03b5 > 0 Output: U\u2032 \u2208 {0, 1}n\u00d7k,V\u2032 \u2208 {0, 1}k\u00d7d satisfying the property that \u2225U\u2032V\u2032 \u2212 A\u22252F \u2264 (1 +\n\u03b5)minU\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212 A\u0303\u22252F 1: V \u2190 \u2205 2: for each guess of SU\u2217 and SA\u0303, where S is a leverage score sampling matrix withm = O ( k log k \u03b52 ) rows with weights that are powers of two up to poly(N) do 3: V \u2190 V \u222a argminV\u2208{0,1}k\u00d7d \u2225SU\u2217V \u2212 SA\u0303\u22252F \u25b7Algorithm 1 4: for each V \u2208 V do 5: Let UV = argminU\u2208{0,1}N\u00d7k \u2225UV \u2212 A\u0303\u22252F \u25b7Algorithm 2 6: V\u2032 \u2190 argminV\u2208{0,1}k\u00d7d \u2225SUVV \u2212 SA\u0303\u22252F 7: U\u2032 \u2190 UV\u2032 8: return (U\u2032,V\u2032)\nTheorem 2.2 (Theorem 42 in [CW13]). For U \u2208 RN\u00d7k, let S \u2208 Rm\u00d7N be a leverage score sampling matrix for U \u2208 {0, 1}N\u00d7k with m = O ( k log k \u03b52 ) rows. Then with probability at least 0.99, we have for all V \u2208 Rk\u00d7d, (1\u2212 \u03b5)\u2225UV\u22252F \u2264 \u2225SUV\u22252F \u2264 (1 + \u03b5)\u2225UV\u22252F .\nFinally, we recall that approximate matrix multiplication and leverage score sampling suffices to achieve an affine embedding.\nTheorem 2.3 (Theorem 39 in [CW13]). Let U \u2208 RN\u00d7k have orthonormal columns. Let S be a sampling matrix that satisfies Lemma 2.1 with error parameter \u03b5\u221a\nk and also let S be a subspace\nembedding for U with error parameter \u03b5. Let V\u2217 = argminV \u2225UV \u2212 A\u0303\u2225F and X = UV\u2217 \u2212 A\u0303. Then for all V \u2208 Rk\u00d7d,\n(1\u2212 2\u03b5)\u2225UV \u2212 A\u0303\u22252F \u2212 \u2225X\u22252F \u2264 \u2225SUV \u2212 SA\u0303\u22252F \u2212 \u2225SX\u22252F \u2264 (1 + 2\u03b5)\u2225UV \u2212 A\u0303\u22252F \u2212 \u2225X\u22252F .\nWe first show that Algorithm 3 achieves a good approximation to the optimal low-rank factors for the coreset A\u0303.\nLemma 2.4. Suppose \u03b5 < 110 . Then with probability at least 0.97, the output of Algorithm 3 satisfies\n\u2225U\u2032V\u2032 \u2212 A\u0303\u22252F \u2264 (1 + 6\u03b5)\u2225U\u2217V\u2217 \u2212 A\u0303\u22252F .\nProof. Let V\u2032\u2032 = argminV\u2208{0,1}k\u00d7d \u2225SU\u2217V \u2212 A\u0303\u22252F and let U\u2032\u2032 = argminU\u2208{0,1}N\u00d7k \u2225SUV\u2032\u2032 \u2212 A\u0303\u22252F Since the algorithm chooses U\u2032 and V\u2032 over U\u2032\u2032 and V\u2032\u2032, then\n\u2225U\u2032V\u2032 \u2212 A\u0303\u22252F \u2264 \u2225U\u2032\u2032V\u2032\u2032 \u2212 A\u0303\u22252F .\nDue to the optimality of U\u2032\u2032,\n\u2225U\u2032\u2032V\u2032\u2032 \u2212 A\u0303\u22252F \u2264 \u2225U\u2217V\u2032\u2032 \u2212 A\u0303\u22252F .\nLet X = U\u2217V\u2217\u2212A\u0303. Note that since U\u2217 has orthonormal columns, then by Lemma 2.1, the leverage score sampling matrix S achieves approximate matrix multiplication with probability at least 0.99. By Theorem 2.2, the matrix S also is a subspace embedding for U. Thus, S meets the criteria for applying Theorem 2.3. Then for the correct guess DT of matrix S corresponding to U\u2217 and conditioning on the correctness of S in Theorem 2.3,\n\u2225U\u2217V\u2032\u2032 \u2212 A\u0303\u22252F \u2264 1 1\u2212 2\u03b5 [\u2225SU\u2217V\u2032\u2032 \u2212 SA\u0303\u22252F \u2212 \u2225SX\u22252F + \u2225X\u22252F .]\nDue to the optimality of V\u2032\u2032,\n1\n1\u2212 2\u03b5 [\u2225SU\u2217V\u2032\u2032 \u2212 SA\u0303\u22252F \u2212 \u2225SX\u22252F + \u2225X\u22252F ] \u2264\n1\n1\u2212 2\u03b5 [\u2225SU\u2217V\u2217 \u2212 SA\u0303\u22252F \u2212 \u2225SX\u22252F + \u2225X\u22252F ].\nThen again conditioning on the correctness of S,\n1\n1\u2212 2\u03b5 [\u2225SU\u2217V\u2217 \u2212 SA\u0303\u22252F \u2212 \u2225SX\u22252F + \u2225X\u22252F ]\n\u2264 1 1\u2212 2\u03b5 [(1 + 2\u03b5)\u2225U\u2217V\u2217 \u2212 A\u0303\u22252F + \u2225SX\u22252F \u2212 \u2225X\u22252F \u2212 \u2225SX\u22252F + \u2225X\u22252F ] \u2264 (1 + 6\u03b5)\u2225U\u2217V\u2217 \u2212 A\u0303\u22252F ,\nfor sufficiently small \u03b5, e.g., \u03b5 < 110 . Thus, putting things together, we have that conditioned on the correctness of S in Theorem 2.3,\n\u2225U\u2032V\u2032 \u2212 A\u0303\u22252F \u2264 (1 + 6\u03b5)\u2225U\u2217V\u2217 \u2212 A\u0303\u22252F .\nSince the approximate matrix multiplication property of Lemma 2.1, the subspace embedding property of Theorem 2.2, and the affine embedding property of Theorem 2.3 all fail with probability at most 0.01, then by a union bound, S succeeds with probability at least 0.97.\nWe now analyze the runtime of the subroutine Algorithm 3.\nLemma 2.5. Algorithm 3 uses 2O(m 2+m log t) poly(N, d) runtime for m = O ( k log k \u03b52 ) .\nProof. We analyze the number of possible guesses D and T corresponding to guesses of SA\u0303 (see the remark after Theorem 1.2). There are at most ( t m ) = 2O(m log t) distinct subsets of m = O ( k log k \u03b52 ) rows of A\u0303. Thus there are 2O(m log t) possible matrices T that selects m rows of A\u0303, for the purposes of leverage score sampling. Assuming the leverage score sampling matrix does not sample any rows with leverage score less than 1poly(N) , then there are O (logN) m = 2O(m log logN) total guesses for\nthe matrix D. Note that log n \u2264 2m implies that 2O(m log logN) \u2264 2O(m2) while logN > 2m implies that 2O(m log logN) \u2264 2O(log\n2 logN) \u2264 N . Therefore, there are at most 2O(m2+m log t)N total guesses for all combinations of T and D, corresponding to all guesses of SA\u0303.\nFor each guess of S and SA\u0303, we also need to guess SU\u2217. Since U\u2217 \u2208 {0, 1}N\u00d7k is binary and T samples m rows before weighting each row with one of O (logN) possible weights, the number of total guesses for SU\u2217 is (2 \u00b7 O (logN))mk.\nGiven guesses for SA and SU\u2217, we can then compute argminV\u2208{0,1}k\u00d7d \u2225SU\u2217V \u2212 SA\u22252F using O ( 2kd ) time through the subroutine Algorithm 1, which enumerates through all possible 2k binary vectors for each column. For a fixed V, we can then compute UV = argminU\u2208{0,1}N\u00d7k \u2225UV\u2212A\u22252F using O ( 2kN ) time through the subroutine Algorithm 2, which enumerates through all possible 2k binary vectors for each row of UV. Therefore, the total runtime of Algorithm 3 is 2O(m 2+m log t) poly(N, d).\nWe recall the following construction for a strong \u03b5-coreset for k-means clustering.\nTheorem 2.6 (Theorem 36 in [FSS20]). Let X \u2282 Rd be a subset of n points, \u03b5 \u2208 (0, 1) be an accuracy parameter, and let t = O\n( k3 log2 k\n\u03b54\n) . There exists an algorithm that uses O ( nd2 + n2d+ nkd\n\u03b52 + nk\n2\n\u03b52 ) time and outputs a set of t weighted points that is a strong \u03b5-coreset for k-means clustering with probability at least 0.99. Moreover, each point has an integer weight that is at most poly(n).\nAlgorithm 4 Low-rank approximation for matrix A\nInput: A \u2208 {0, 1}n\u00d7d, rank parameter k, accuracy parameter \u03b5 > 0 Output: U\u2032 \u2208 {0, 1}n\u00d7k,V\u2032 \u2208 {0, 1}k\u00d7d satisfying the property that \u2225U\u2032V\u2032 \u2212 A\u22252F \u2264 (1 +\n\u03b5)minU\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F 1: t\u2190 O ( 23kk2\n\u03b54\n) \u25b7Theorem 2.6 for 2k-means clustering\n2: Compute a strong coreset C for 2k-means clustering of A, with size t and total weight N = poly(n) 3: Let A\u0303 \u2208 {0, 1}N\u00d7d be the matrix representation of C, where weighted points are duplicated appropriately 4: Let (U\u0303, V\u0303) be the output of Algorithm 3 on input A\u0303 5: U\u2032 \u2190 argminU\u2208{0,1}n\u00d7k \u2225UV\u0303 \u2212A\u22252F , V\u2032 \u2190 V\u0303 \u25b7Algorithm 2 6: return (U\u2032,V\u2032)\nWe now justify the correctness of Algorithm 4.\nLemma 2.7. With probability at least 0.95, Algorithm 4 returns U\u2032,V\u2032 such that\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F .\nProof. Let M\u0303 be the indicator matrix that selects a row of U\u0303V\u0303 = U\u0303V\u2032 to match to each row of A, so that by the optimality of U\u2032,\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 \u2225M\u0303U\u0303V\u0303 \u2212A\u22252F .\nNote that any V is a set of k points in {0, 1}d and so each row Ui of U induces one of at most 2k possible points UiV \u2208 {0, 1}d. Hence \u2225UV\u2212A\u22252F is the objective value of a constrained 2k-means clustering problem. Thus by the choice of t in Theorem 2.6, we have that A\u0303 is a strong coreset, so that\n\u2225M\u0303U\u0303V\u0303 \u2212A\u22252F \u2264 (1 + \u03b5)\u2225U\u0303V\u0303 \u2212 A\u0303\u22252F .\nLet U\u2217 \u2208 {0, 1}n\u00d7k and V\u2217 \u2208 {0, 1}k\u00d7d such that\n\u2225U\u2217V\u2217 \u2212A\u22252F = min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F .\nLet M\u2217 be the indicator matrix that selects a row of U\u2217V\u2217 to match to each row of A\u0303, so that by Lemma 2.4,\n(1 + \u03b5)\u2225U\u0303V\u0303 \u2212 A\u0303\u22252F \u2264 (1 + \u03b5)2\u2225M\u2217U\u2217V\u2217 \u2212 A\u0303\u22252F .\nThen by the choice of t in Theorem 2.6, we have that\n(1 + \u03b5)2\u2225M\u2217U\u2217V\u2217 \u2212 A\u0303\u22252F \u2264 (1 + \u03b5)3\u2225U\u2217V\u2217 \u2212A\u22252F .\nThe desired claim then follows from rescaling \u03b5.\nWe now analyze the runtime of Algorithm 4.\nLemma 2.8. Algorithm 4 uses 2O\u0303(k 2/\u03b54) poly(n, d) runtime.\nProof. By Theorem 2.6, it follows that Algorithm 4 uses O ( nd2 + n2d+ nkd\n\u03b52 + nk\n2\n\u03b52\n) time to com-\npute A\u0303 \u2208 {0, 1}N\u00d7d with N = poly(n). By Lemma 2.5, it follows that Algorithm 3 on input A\u0303 thus uses runtime 2O(m 2+m log t) poly(N, d) for m = O ( k log k \u03b52 ) and t = O ( 23kk2 \u03b54 ) . Finally, com-\nputing U\u2032 via Algorithm 2 takes O ( 2kn ) time after enumerating through all possible 2k binary\nvectors for each row of U\u2032. Therefore, the total runtime of Algorithm 4 is 2 O\u0303 ( k2 log2 k \u03b54 ) poly(n, d) = 2O\u0303(k 2/\u03b54) poly(n, d).\nCombining Lemma 2.7 and Lemma 2.8, we have:\nTheorem 2.9. There exists an algorithm that uses 2O\u0303(k 2/\u03b54) poly(n, d) runtime and with probability at least 23 , outputs U \u2032 \u2208 {0, 1}n\u00d7k and V\u2032 \u2208 {0, 1}k\u00d7d such that\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F ."
        },
        {
            "heading": "3 F2 Low-Rank Approximation",
            "text": "In this section, we present a (1+\u03b5)-approximation algorithm for binary low-rank approximation on F2, where to goal is to find matrices U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d to minimize the Frobenius norm loss \u2225UV\u2212A\u22252F , but now all operations are performed in F2. We would like to use the same approach as in Section 2, i.e., to make guesses for the matrices SU\u2217 and SA while ensuring there are not too many possibilities for these matrices. To do so for matrix operations over general integers, we chose S to be a leverage score sampling matrix that samples rows from U\u2217 and A. We then\nused the approximate matrix multiplication property in Lemma 2.1 and the subspace embedding property in Theorem 2.2 to show that S provides an affine embedding in Theorem 2.3 over general integers. However, it no longer necessarily seems true that S will provide an affine embedding over F2, in part because the subspace embedding property of S computes leverage scores of each row of U\u2217 and A with respect to general integers. Thus we require an alternate approach for matrix operations over F2.\nInstead, we form the matrix A\u0303 by taking a strong coreset of A and then duplicating the rows according to their weight wi to form A\u0303. That is, if the i-th row Ai of A is sampled with weight wi in the coreset, then A\u0303 will contain wi repetitions of the row Ai, where we note that wi is an integer. We then group the rows of A\u0303 by their repetitions, so that group Gj consists of the rows of A\u0303 that are repeated [(1 + \u03b5)j , (1+ \u03b5)j+1) times. Thus if Ai appears wi times in A\u0303, then it appears a single time in group Gj for j = \u230alogwi\u230b.\nWe perform entrywise L0 low-rank approximation over F2 for each of the groups Gj , which gives low-rank factors U(j) and V(j). We then compute U\u0303(j) \u2208 Rn\u00d7d from U(j) by following procedure. If Ai is in Gj , then we place the row of U\n(j) corresponding to Ai into the i-th row of U\u0303(j), for all i \u2208 [n]. Note that the row of U(j) corresponding to Ai may not be the i-th row of U(j), e.g., since Ai will appear only once in Gj even though it appears wi \u2208 [(1 + \u03b5)j , (1 + \u03b5)j+1) times in A. Otherwise if Ai is not in Gj , then we set i-th row of U\u0303(j) to be the all zeros row. We then achieve V(j) by padding accordingly. Finally, we collect\nU\u0303 = [ U\u0303(0)| . . . |U\u0303(\u2113) ] , V\u0303\u2190 V\u0303(0) \u25e6 . . . \u25e6 V\u0303(i)\nto achieve bicriteria low-rank approximations U\u0303 and V\u0303 to A\u0303. Finally, to achieve bicriteria lowrank approximations U\u2032 and V\u2032 to A, we require that U\u2032 achieves the same block structure as U\u0303. We describe this subroutine in Algorithm 5 and we give the full low-rank approximation bicriteria algorithm in Algorithm 6.\nWe first recall the following subroutine to achieve entrywise L0 low-rank approximation over F2. Note that for matrix operations over F2, we have that the entrywise L0 norm is the same as the entrywise Lp norm for all p.\nLemma 3.1 (Theorem 3 in [BBB+19]). For \u03b5 \u2208 (0, 1), there exists a (1 + \u03b5)-approximation algorithm to entrywise L0 rank-k approximation over F2 running in d \u00b7 npoly(k/\u03b5) time.\nAlgorithm 5 Algorithm for computing optimal U given V(1), . . . ,V(\u2113)\nInput: A\u0303 \u2208 {0, 1}N\u00d7d, V(1), . . . ,V(\u2113) \u2208 {0, 1}k\u00d7d Output: U\u2032 = argminU\u2208{0,1}N\u00d7\u2113k \u2225UV \u2212 A\u0303\u2225F , where U is restricted to one nonzero block of k\ncoordinates 1: for i = 1 to i = N do 2: Set (U\u2032i, j \u2032) = argminUi\u2208{0,1}1\u00d7k,j\u2208[\u2113] \u2225UiV (j) \u2212 A\u0303i\u22252 \u25b7Enumerate over all 2k possible\nbinary vectors, all \u2113 indices 3: Pad U\u2032i with length \u2113k, as the j \u2032-th block of k coordinates 4: return U\u2032 = U\u20321 \u25e6 . . . \u25e6U\u2032N\nWe first justify the correctness of Algorithm 6.\nAlgorithm 6 Bicriteria low-rank approximation on F2 for matrix A Input: A \u2208 {0, 1}n\u00d7d, rank parameter k, accuracy parameter \u03b5 > 0 Output: U\u2032 \u2208 {0, 1}n\u00d7k,V\u2032 \u2208 {0, 1}k\u00d7d satisfying the property that \u2225U\u2032V\u2032 \u2212 A\u22252F \u2264 (1 +\n\u03b5)minU\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F , where all matrix operations are performed in F2 1: \u2113\u2190 O ( logn \u03b5 ) , t\u2190 O ( (2k\u2113)3k2 \u03b54 ) , k\u2032 \u2190 \u2113k \u25b7Theorem 2.6 for 2k-means clustering 2: Compute a strong coreset C for 2k-means clustering of A, with size t and total weight N = poly(n) 3: Let A\u0303 \u2208 {0, 1}N\u00d7d be the matrix representation of C, where weighted points are duplicated appropriately 4: For i \u2208 [\u2113], let G(i) be the group of rows (removing multiplicity) of A\u0303 with frequency [(1 + \u03b5)i, (1 + \u03b5)i+1) 5: Let (U\u0303(i), V\u0303(i)) be the output of Lemma 3.1 on input G(i), padded to Rn\u00d7k and Rk\u00d7d, respectively 6: V\u0303\u2190 V\u0303(0) \u25e6 . . . \u25e6 V\u0303(\u2113) 7: Use Algorithm 5 with V\u0303(0), . . . , V\u0303(\u2113) and A to find U\u2032 8: return (U\u2032,V\u2032) with V\u2032 = V\u0303\nLemma 3.2. With probability at least 0.95, Algorithm 6 returns U\u2032,V\u2032 such that\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F ,\nwhere all matrix operations are performed in F2. Proof. Let U\u0303\u2190 [ U\u0303(0)| . . . |U\u0303(\u2113) ] in Algorithm 6. Let M\u0303 be the indicator matrix that selects a row of U\u0303V\u0303 = U\u0303V\u2032 to match to each row of A, so that by the optimality of U\u2032,\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 \u2225M\u0303U\u0303V\u0303 \u2212A\u22252F .\nSince V is a set of k points in {0, 1}d and each row Ui of U induces one of at most 2k possible points UiV \u2208 {0, 1}d, then \u2225UV\u2212A\u22252F is the objective value of a constrained 2k-means clustering problem, even when all operations performed are on F2. Similarly, V(j) is a set of k points in {0, 1}d for each j \u2208 [\u2113]. Each row Ui of U induces one of at most 2k possible points UiV(j) \u2208 {0, 1}d for a fixed j \u2208 [\u2113], so that \u2225UV\u2032 \u2212A\u22252F is the objective value of a constrained 2k\u2113-means clustering problem, even when all operations performed are on F2.\nHence by the choice of t in Theorem 2.6, it follows that A\u0303 is a strong coreset, and so\n\u2225M\u0303U\u0303V\u0303 \u2212A\u22252F \u2264 (1 + \u03b5)\u2225U\u0303V\u0303 \u2212 A\u0303\u22252F .\nWe decompose the rows of A\u0303 into G(0), . . . ,G(\u2113) for \u2113 = O ( logn \u03b5 ) . Let Gi be the corresponding indices in [n] so that j \u2208 Gi if and only if A\u0303j is nonzero in Gi. Then we have\n\u2225U\u0303V\u0303 \u2212 A\u0303\u22252F = \u2211 i\u2208[\u2113] \u2211 j\u2208Gi \u2225U\u2032jV\u2032 \u2212 A\u0303j\u22252F .\nSince each row in Gi is repeated a number of times in [(1 + \u03b5) i, (1 + \u03b5)i+1), then\u2211\nj\u2208Gi\n\u2225U\u2032jV\u2032 \u2212 A\u0303j\u22252F \u2264 (1 + \u03b5)2 min U(i)\u2208{0,1}n\u00d7k,V(i)\u2208{0,1}\u00d7k\u00d7d \u2225U(i)V(i) \u2212G(i)\u22252F ,\nwhere the first factor of (1 + \u03b5) is from the (1 + \u03b5)-approximation guarantee of U(i) and V(i) by Lemma 3.1 and the second factor of (1 + \u03b5) is from the number of each row in G(i) varying by at most a (1 + \u03b5) factor. Therefore,\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 (1 + \u03b5)3 \u2211 i\u2208[\u2113] min U(i)\u2208{0,1}n\u00d7k,V(i)\u2208{0,1}k\u00d7d \u2225U(i)V(i) \u2212G(i)\u22252F\n\u2264 (1 + \u03b5)3 min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212 A\u0303\u22252F .\nLet U\u2217 \u2208 {0, 1}n\u00d7k and V\u2217 \u2208 {0, 1}k\u00d7d such that\n\u2225U\u2217V\u2217 \u2212A\u22252F = min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F ,\nwhere all operations are performed in F2. Let M\u2217 be the indicator matrix that selects a row of U\u2217V\u2217 to match to each row of A\u0303, so that by Lemma 2.4,\nmin U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d\n\u2225UV \u2212 A\u0303\u22252F \u2264 (1 + \u03b5)\u2225M\u2217U\u2217V\u2217 \u2212 A\u0303\u22252F .\nThen by the choice of t in Theorem 2.6 so that A\u0303 is a strong coreset of A,\n\u2225M\u2217U\u2217V\u2217 \u2212 A\u0303\u22252F \u2264 (1 + \u03b5)\u2225U\u2217V\u2217 \u2212A\u22252F .\nTherefore, we have \u2225U\u2032V\u2032 \u2212A\u22252F \u2264 (1 + \u03b5)5\u2225U\u2217V\u2217 \u2212A\u22252F\nand the desired claim then follows from rescaling \u03b5.\nIt remains to analyze the runtime of Algorithm 6.\nLemma 3.3. Algorithm 6 uses 2poly(k/\u03b5) poly(n, d) runtime. Proof. By Theorem 2.6, we have that Algorithm 6 usesO ( nd2 + n2d+ nkd\n\u03b52 + nk\n2\n\u03b52\n) time to compute\nA\u0303 \u2208 {0, 1}N\u00d7d with N = poly(n). By Lemma 3.1, it takes d \u00b7 (2k)poly(k/eps) time to compute U\u0303(i), V\u0303(i) for each i \u2208 [\u2113] for \u2113 = O ( logn \u03b5 ) . Hence, it takes 2poly(k/eps) poly(n, d) runtime to compute\nU\u0303 and V\u0303. Finally, computing U\u2032 via Algorithm 5 takes O ( 2k \u2032 n ) time after enumerating through all possible 2k\u2113 binary vectors for each row of U\u2032. Therefore, the total runtime of Algorithm 4 is 2poly(k/\u03b5) poly(n, d).\nBy Lemma 3.2 and Lemma 3.3, we thus have:\nTheorem 3.4. There exists an algorithm that uses 2poly(k/\u03b5) poly(n, d) runtime and with probability at least 23 , outputs U \u2032 \u2208 {0, 1}n\u00d7k\u2032 and V\u2032 \u2208 {0, 1}k\u2032\u00d7d such that\n\u2225U\u2032V\u2032 \u2212A\u22252F \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u22252F ,\nwhere k\u2032 = O ( k log k\n\u03b5\n) ."
        },
        {
            "heading": "4 Lp Low-Rank Approximation",
            "text": "In this section, we present a (1 + \u03b5)-approximation algorithm for binary low-rank approximation with Lp loss, where to goal is to find matrices U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d to minimize \u2225UV \u2212A\u2225pp. We would like to use the same approach as in Section 2, where we first compute a weighted matrix A\u0303 from a strong coreset for A, and then we make guesses for the matrices SU\u2217 and SA and solve for minV\u2208{0,1}k\u00d7d \u2225SU\u2217V\u2212SA\u22252F while ensuring there are not too many possibilities for the matrices SU\u2217 and SA. Thus to adapt this approach to Lp loss, we first require the following strong coreset construction for discrete metrics:\nTheorem 4.1 (Theorem 1 in [CSS21]). Let X \u2282 Rd be a subset of n points, \u03b5 \u2208 (0, 1) be an accuracy parameter, p \u2265 1 be a constant, and let\nt = O ( min(\u03b5\u22122 + \u03b5\u2212p, k\u03b5\u22122) \u00b7 k log n ) .\nThere exists an algorithm that uses poly(n, d, k) runtime and outputs a set of t weighted points that is a strong \u03b5-coreset for k-clustering on discrete Lp metrics with probability at least 0.99. Moreover, each point has an integer weight that is at most poly(n).\nFor Frobenius error, we crucially require the affine embedding property that\n(1\u2212 \u03b5)\u2225U\u2217V \u2212A\u22252F \u2264 \u2225SU\u2217V \u2212 SA\u22252F \u2264 (1 + \u03b5)\u2225U\u2217V \u2212A\u22252F ,\nfor all V \u2208 {0, 1}k\u00d7d. Unfortunately, it is not known whether there exists an efficient samplingbased affine embedding for Lp loss.\nWe instead invoke the coreset construction of Theorem 4.1 on the rows and the columns so that A\u0303 has a small number of distinct rows and columns. We again use the idea from Section 3 to partition the rows of A\u0303 into groups based on their frequency, but now we further partition the groups based on the frequency of the columns. It then remains to solve BMF with Lp loss on the partition, each part of which has a small number of rows and columns. Because the contribution of each row toward the overall loss is small (because there is a small number of columns), it turns out that there exists a matrix that samples poly(k/\u03b5) rows of each partition that finally achieves the desired affine embedding. Thus, we can solve the problem on each partition, pad the factors accordingly, and build the bicriteria factors as in the binary field case. The algorithm appears in full in Algorithm 9, with subroutines appearing in Algorithm 7 and Algorithm 8.\nAlgorithm 7 Algorithm for computing optimal U given V(1), . . . ,V(\u2113)\nInput: A\u0303 \u2208 {0, 1}N\u00d7d, V(1), . . . ,V(\u2113) \u2208 {0, 1}k\u00d7d Output: U\u2032 = argminU\u2208{0,1}N\u00d7\u2113k \u2225UV \u2212 A\u0303\u2225 p p, where U is restricted to one nonzero block of k\ncoordinates 1: for i = 1 to i = N do 2: Set (U\u2032i, j \u2032) = argminUi\u2208{0,1}1\u00d7k,j\u2208[\u2113] \u2225(UiV (j) \u2212 A\u0303i\u2225pp \u25b7Enumerate over all 2k possible\nbinary vectors, all \u2113 indices 3: Pad U\u2032i with length \u2113k, as the j \u2032-th block of k coordinates 4: return U\u2032 = U\u20321 \u25e6 . . . \u25e6U\u2032N\nWe first justify the correctness of Algorithm 8 by showing the existence of an L0 sampling matrix S that achieves a subspace embedding for binary inputs.\nAlgorithm 8 Low-rank approximation for matrix A\u0303 with t distinct rows and t\u2032 distinct columns\nInput: A\u0303 \u2208 {0, 1}N\u00d7D with at most t distinct rows and r distinct columns Output: U\u2032,V\u2032 with \u2225UV \u2212 A\u0303\u2225p \u2264 (1 + \u03b5)minU\u2208{0,1}N\u00d7k,V\u2208{0,1}k\u00d7D \u2225UV \u2212 A\u0303\u2225p 1: V \u2190 \u2205 2: for each guess of SU\u2217 and SA, where S is a L0 sampling matrix with m = O ( kp+1\n\u03b52 log r\n) rows\nwith weights that are powers of two up to poly(N) do 3: V \u2190 V \u222a argminV\u2208{0,1}k\u00d7D \u2225SU\u2217V \u2212 SA\u2225 p p \u25b7Algorithm 1 with Lp loss 4: for each V \u2208 V do 5: Let UV = argminU\u2208{0,1}N\u00d7k \u2225UV \u2212A\u2225 p p \u25b7Algorithm 2 with Lp loss 6: V\u2032 \u2190 argminV\u2208{0,1}k\u00d7d \u2225SUVV \u2212 SA\u2225 p p 7: U\u2032 \u2190 UV\u2032 8: return (U\u2032,V\u2032)\nAlgorithm 9 Bicriteria low-rank approximation with Lp loss for matrix A Input: A \u2208 {0, 1}n\u00d7d, rank parameter k, accuracy parameter \u03b5 > 0 Output: U\u2032 \u2208 {0, 1}n\u00d7k,V\u2032 \u2208 {0, 1}k\u00d7d satisfying the property that \u2225U\u2032V\u2032 \u2212 A\u2225pp \u2264 (1 +\n\u03b5)minU\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u2225 p p 1: t\u2190 O ( min(\u03b5\u22122 + \u03b5\u2212p, k\u03b5\u22122) \u00b7 k log n ) \u25b7Theorem 4.1\n2: \u2113\u2190 O ( logn \u03b5 ) , k\u2032 \u2190 \u2113k 3: Compute a strong coreset C for 2k-means clustering ofA, with t rows, with weightsN = poly(n) 4: Compute a strong coreset C \u2032 for 2k-means clustering of C, with t rows and columns, with\nweights N,D = poly(n) 5: Let A\u0303 \u2208 {0, 1}N\u00d7D be the matrix representation of C, where weighted points are duplicated\nappropriately 6: For i \u2208 [\u2113], let G(i) be the group of rows (removing multiplicity) of A\u0303 with frequency [(1 +\n\u03b5)i, (1 + \u03b5)i+1) 7: For i, j \u2208 [\u2113], let G(i,j) be the group of columns (removing multiplicity) of G(i,j) with frequency\n[(1 + \u03b5)j , (1 + \u03b5)j+1)\n8: Compute the low-rank minimizers (U\u0303(i,j), V\u0303(i,j)) on input G(i,j) using Algorithm 8, padded to Rn\u00d7k and Rk\u00d7D, respectively\n9: U\u0303\u2190 [ U\u0303(0,0)|U\u0303(1,0)| . . . |U\u0303(\u2113,\u2113) ] , V\u0303\u2190 V\u0303(0,0) \u25e6 V\u0303(1,0) . . . \u25e6 V\u0303(\u2113,\u2113)\n10: Use Algorithm 7 with U\u0303(0,0), U\u0303(1,0) . . . , U\u0303(\u2113,\u2113) and C to find V\u2032 11: Use V\u2032 and A to find U\u2032, i.e., Algorithm 2 with dimension k\u2032 and Lp loss 12: return (U\u2032,V\u2032)\nLemma 4.2. Given matrices A \u2208 {0, 1}n\u00d7k and B \u2208 {0, 1}n\u00d7r, there exists a matrix S \u2208 Rm\u00d7n with m = O ( kp+1\n\u03b52 log r\n) such that with probability at least 0.99, we have that simultaneously for all\nX \u2208 {0, 1}k\u00d7r, (1\u2212 \u03b5)\u2225AX\u2212B\u2225pp \u2264 \u2225SAX\u2212 SB\u2225pp \u2264 (1 + \u03b5)\u2225AX\u2212B\u2225pp.\nProof. Let M \u2208 {0, 1, . . . , k}n\u00d71 be an arbitrary matrix and let S be a set that contains the nonzero\nrows of M and has cardinality that is a power of two. That is, |S| = 2i for some integer i \u2265 0. Let z be a random element of S, i.e., a random non-zero row of M, so that we have\nE [ 2i \u00b7 \u2225z\u2225pp ] = \u2225M\u2225pp.\nSimilarly, we have Var(2i \u00b7 \u2225z\u2225pp) \u2264 2ikp \u2264 2kp\u2225M\u2225pp.\nHence if we repeat take the mean of O ( kp\n\u03b52\n) estimators, we have that with probability at least 0.99,\n(1\u2212 \u03b5)\u2225M\u2225pp \u2264 \u2225SM\u2225pp \u2264 (1 + \u03b5)\u2225M\u2225pp.\nWe can further improve the probability of success to 1 \u2212 \u03b4 for \u03b4 \u2208 (0, 1) by repeating O ( log 1\u03b4 ) times. By setting M = Ax \u2212 B(i) for fixed A \u2208 {0, 1}n\u00d7k, x \u2208 {0, 1}k, and B \u2208 {0, 1}n\u00d7r with i \u2208 [r], we have that the sketch matrix gives a (1 + \u03b5)-approximation to \u2225Ax\u2212B(i)\u2225pp. The result then follows from setting \u03b4 = 1\n2kr , taking a union bound over all x \u2208 {0, 1}k, and then a union\nbound over all i \u2208 [r].\nWe then justify the correctness of Algorithm 9.\nLemma 4.3. With probability at least 0.95, Algorithm 9 returns U\u2032,V\u2032 such that\n\u2225U\u2032V\u2032 \u2212A\u2225pp \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u2225pp.\nProof. Let M1 and M2 be the sampling and rescaling matrices used to acquire A\u0303 \u2208 RN\u00d7D, so that by the optimality of U\u2032,\n\u2225U\u2032V\u2032 \u2212A\u2225pp \u2264 \u2225M1U\u0303V\u0303M2 \u2212A\u2225pp.\nObserve that V is a set of k points in {0, 1}d. Thus, each row Ui of U induces one of at most 2k possible points UiV \u2208 {0, 1}d. Hence, \u2225UV \u2212A\u2225pp is the objective value of a constrained 2k-clustering problem under the Lp metric. Similarly, since V\n(j) is a set of k points in {0, 1}d for each j \u2208 [\u2113], then each row Ui of U induces one of at most 2k possible points UiV(j) \u2208 {0, 1}d for a fixed j \u2208 [\u2113]. Therefore, \u2225UV\u2032\u2212A\u2225pp is the objective value of a constrained 2k\u2113-clustering problem under the Lp metric.\nBy the choice of t in Theorem 4.1, A\u0303 is a strong coreset, and so\n\u2225M1U\u0303V\u0303M2 \u2212A\u22252F \u2264 (1 + \u03b5)\u2225U\u0303V\u0303 \u2212 A\u0303\u22252F .\nWe decompose the rows of A\u0303 into groups G(0), . . . ,G(\u2113) for \u2113 = O ( logn \u03b5 ) . For each group G(i),\nwe decompose the columns of G(i) into groups G(i,0), . . . ,G(i,\u2113) for \u2113 = O ( logn \u03b5 ) . Let Gi be the indices in [n] corresponding to the rows in G(i) and let Gi,j be the indices in [n] corresponding to the columns in G(i,j). Then\n\u2225U\u0303V\u0303 \u2212 A\u0303\u2225pp = \u2211 i\u2208[\u2113] \u2211 a\u2208Gi \u2211 j\u2208[\u2113] \u2211 b\u2208Gi,j \u2223\u2223\u2223(U\u2032V\u2032)a,b \u2212 A\u0303a,b\u2223\u2223\u2223p .\nSince each row in Gi is repeated a number of times in [(1+ \u03b5) i, (1+ \u03b5)i+1) and each column in Gi,j is repeated a number of times in [(1 + \u03b5)i, (1 + \u03b5)i+1), then\u2211 a\u2208Gi \u2211 b\u2208Gi,j \u2223\u2223\u2223(U\u2032V\u2032)a,b \u2212 A\u0303a,b\u2223\u2223\u2223p \u2264 (1 + \u03b5)3 min U\u2208{0,1}n\u00d7k,V\u2208{0,1}\u00d7k\u00d7d \u2211 a\u2208Gi \u2211 b\u2208Gi,j\n\u2223\u2223\u2223(UV)a,b \u2212 A\u0303a,b\u2223\u2223\u2223p , where the first factor of (1 + \u03b5) is from the (1 + \u03b5)-approximation guarantee of U(i) and V(i) by Lemma 3.1 and the second and third factors of (1 + \u03b5) is from the number of each row and each column in G(i,j) varying by at most (1 + \u03b5) factor. Therefore,\n\u2225U\u2032V\u2032 \u2212A\u2225pp \u2264 (1 + \u03b5) \u2211 i\u2208[\u2113] \u2211 a\u2208Gi \u2211 j\u2208[\u2113] \u2211 b\u2208Gi,j \u2223\u2223\u2223(U\u2032V\u2032)a,b \u2212 A\u0303a,b\u2223\u2223\u2223p \u2264 (1 + \u03b5)4 min\nU\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212 A\u0303\u2225pp\nLet U\u2217 \u2208 {0, 1}n\u00d7k and V\u2217 \u2208 {0, 1}k\u00d7d be minimizers to the binary Lp low-rank approximation problem, so that\n\u2225U\u2217V\u2217 \u2212A\u2225pp = min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u2225pp.\nLet M3 and M4 be the indicator matrices that select rows and columns of U \u2217V\u2217 to match to each row of A\u0303, so that by Lemma 2.4,\nmin U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d\n\u2225UV \u2212 A\u0303\u2225pp \u2264 (1 + \u03b5)\u2225M3U\u2217V\u2217M4 \u2212 A\u0303\u2225pp.\nThen by the choice of t in Theorem 4.1 so that A\u0303 is a strong coreset of A,\n\u2225M3U\u2217V\u2217M4 \u2212 A\u0303\u2225pp \u2264 (1 + \u03b5)\u2225U\u2217V\u2217 \u2212A\u2225pp.\nTherefore, \u2225U\u2032V\u2032 \u2212A\u2225pp \u2264 (1 + \u03b5)6\u2225U\u2217V\u2217 \u2212A\u2225pp\nand the desired claim then follows from rescaling \u03b5.\nWe now analyze the runtime of Algorithm 9.\nLemma 4.4. For any constant p \u2265 1, Algorithm 9 uses 2poly(k/\u03b5) poly(n, d) runtime.\nProof. By Theorem 4.1, we have that Algorithm 9 uses 2O(k) \u00b7 poly(n, d) time to compute A\u0303 \u2208 {0, 1}N\u00d7D with N,D = poly(n). We now consider the time to compute U\u0303(i,j), V\u0303(i,j) for each i, j \u2208 [\u2113] for \u2113 = O ( logn \u03b5 ) . For each i, j, we make guesses for SU\u2217 and SA in Since SU\u2217 and SA\nhave m = O ( kp+1 log r\n\u03b52\n) rows, then there are ( t m ) possible choices for SU\u2217 and ( t m ) choices for SA,\nwhere t = 2 k logn \u03b5p . Hence, there are 2 poly(k/\u03b5) poly(n, d) possible guesses for SU\u2217 and SA.\nFor each guess of SU\u2217 and SA, Algorithm 8 iterates through the columns of V\u0303(i,j), which uses 2O(k) \u00b7 poly(n, d) time. Similarly, the computation of U\u0303(i,j), U\u2032, and V\u2032 all take 2O(k) \u00b7 poly(n, d) time. Therefore, the total runtime of Algorithm 9 is 2poly(k/\u03b5) poly(n, d).\nBy Lemma 4.3 and Lemma 4.4, we thus have:\nTheorem 4.5. For any constant p \u2265 1, there exists an algorithm that uses 2poly(k/\u03b5) poly(n, d) runtime and with probability at least 23 , outputs U \u2032 \u2208 {0, 1}n\u00d7k\u2032 and V\u2032 \u2208 {0, 1}k\u2032\u00d7d such that\n\u2225U\u2032V\u2032 \u2212A\u2225pp \u2264 (1 + \u03b5) min U\u2208{0,1}n\u00d7k,V\u2208{0,1}k\u00d7d \u2225UV \u2212A\u2225pp,\nwhere k\u2032 = O ( k log2 k\n\u03b52\n) .\nWe note here that the poly(k/\u03b5) term in the exponent hides a kp factor, as we assume p to be a (small) constant."
        },
        {
            "heading": "5 Applications to Big Data Models",
            "text": "This section describes how we can generalize our techniques to big data models such as streaming or distributed models.\nAlgorithmic modularization. To adapt our algorithm to the streaming model or the distributed model, we first present a high-level modularization of our algorithm across all applications, i.e., Frobenius binary low-rank approximation, binary low-rank approximation over F2, and binary low-rank approximation with Lp loss. We are given the input matrix A \u2208 {0, 1}n\u00d7d in each of these settings. We first construct a weighted coreset A\u0303 for A. We then perform a number of operations on A\u0303 to obtain low-rank factors U\u0303 and V\u0303 for A\u0303. Setting V\u2032 = V\u0303, our algorithms finally use A and V\u2032 to construct the optimal factor U\u2032 to match V\u2032."
        },
        {
            "heading": "5.1 Streaming Model",
            "text": "We can adapt our approach to the streaming model, where either the rows or columns of the input matrix arrive sequentially. For brevity, we shall only discuss the setting where the rows of the input matrix arrive sequentially; the setting where the columns of the input matrix arrive sequentially is symmetric.\nFormal streaming model definition. We consider the two-pass row-arrival variant of the streaming model. In this setting, the rank parameter k and the accuracy parameter \u03b5 > 0 are given to the algorithm before the data stream. The input matrix A \u2208 {0, 1}n\u00d7d is then defined through the sequence of row-arrivals, A1, . . . ,An \u2208 {0, 1}d, so that the i-th row that arrives in the data stream is Ai. The algorithm passes over the data twice so that in the first pass, it can store some sketch S that uses space sublinear in the input size, i.e., using o(nd) space. After the first pass, the algorithm can perform some post-processing on S and then must output factors U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d after being given another pass over the data, i.e., the rows A1, . . . ,An \u2208 {0, 1}d.\nTwo-pass streaming algorithm. To adapt our algorithm to the two-pass streaming model, recall the high-level modularization of our algorithm described at the beginning of Section 5. The first step is constructing a coreset A\u0303 of A. Whereas our previous coreset constructions were offline, we now require a streaming algorithm to produce the coreset A\u0303. To that end, we use the following\nwell-known merge-and-reduce paradigm for converting an offline coreset construction to a coreset construction in the streaming model.\nTheorem 5.1. Suppose there exists an algorithm that, with probability 1 \u2212 1poly(n) , produces an offline coreset construction that uses f(n, \u03b5) space, suppressing dependencies on other input parameters, such as k and p. Then there exists a one-pass streaming algorithm that, with probability 1\u2212 1poly(n) , produces a coreset that uses f(n, \u03b5 \u2032) \u00b7 O (log n) space, where \u03b5\u2032 = \u03b5logn .\nIn the first pass of the stream, we can use Theorem 5.1 to construct a strong coreset C of A with accuracy O (\u03b5). However, C will have 2poly(k) \u00b7 poly ( 1 \u03b5 , log n ) rows, and thus, we cannot immediately duplicate the rows of C to form A\u0303 because we cannot have log n dependencies in the number of rows of A\u0303.\nAfter the first pass of the stream, we further apply the respective offline coreset construction, i.e., Theorem 2.6 or Theorem 4.1 to C to obtain a coreset C \u2032 with accuracy \u03b5 and a number of rows independent of log n. We then use C \u2032 to form A\u0303 and perform a number of operations on A\u0303 to obtain low-rank factors U\u0303 and V\u0303 for A\u0303. Setting V\u2032 = V\u0303, we can finally use the second pass of the data stream over A, along with V\u2032, to construct the optimal factor U\u2032 to match V\u2032. Thus the two-pass streaming algorithm uses 2poly(k) \u00b7 d \u00b7 poly ( 1 \u03b5 , log n ) total space in the row-arrival model.\nFor the column-arrival model, the two-pass streaming algorithm uses 2poly(k) \u00b7n \u00b7poly ( 1 \u03b5 , log d ) total space."
        },
        {
            "heading": "5.2 Two-round distributed algorithm.",
            "text": "Our approach can also be adapted to the distributed model, where the rows or columns of the input matrix are partitioned across multiple users. For brevity, we again discuss the setting where the rows of the input matrix are partitioned; the setting where the columns of the input matrix are partitioned is symmetric.\nFormal distributed model definition. We consider the two-round distributed model, where the rank parameter k and the accuracy parameter \u03b5 > 0 are known in advance to all users. The input matrix A \u2208 {0, 1}n\u00d7d is then defined arbitrarily through the union of rows, A1, . . . ,An \u2208 {0, 1}d, where each row Ai may be given to any of \u03b3 users. An additional central coordinator sends and receives messages from the users. The protocol is then permitted to use two rounds of communication so that in the first round, the protocol can send o(nd) bits of communication. The coordinator can process the communication to form some sketch S, perform some post-processing on S, and then request additional information from each user, possibly using o(nd) communication to specify the information demanded from each user. After the users again use o(nd) bits of communication in the second round of the protocol, the central coordinator must output factors U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d.\nTwo-round distributed algorithm. To adapt our algorithm to the two-round distributed model, again recall the high-level modularization of our algorithm described at the beginning of Section 5. The first step is constructing a coreset A\u0303 of A. Whereas our previous coreset constructions were offline, we now require a distributed algorithm to produce the coreset A\u0303. To that end, we request that each of the t users send a coreset with accuracy O (\u03b5) of their respective rows. Note that each user can construct the coreset locally without requiring any communication since\nthe coreset is only a summary of the rows held by the user. Thus the total communication in the first round is just the offline coreset size times the number of players, i.e., \u03b3 \u00b72poly(k) \u00b7poly ( 1 \u03b5 , log n ) rows.\nGiven the union C of the coresets sent by all users, the central coordinator then constructs a coreset C \u2032 of A with accuracy \u03b5, again using an offline coreset construction. The coordinator then uses C \u2032 to form A\u0303 and performs the required operations on A\u0303 to obtain low-rank factors U\u0303 and V\u0303 for A\u0303.\nThe coordinator can then send V\u2032 to all players, using V\u2032 and their local subset rows of A to construct U\u2032 collectively. The users then send the rows of U\u2032 corresponding to the rows of A local to the user back to the central coordinator, who can then construct U\u2032. Thus the second round of the protocol uses O\u0303 (nk + kd) \u00b7 poly ( 1 \u03b5 ) bits of communication. Hence, the total com-\nmunication of the protocol is d\u03b3 \u00b7 2poly(k) \u00b7 poly ( 1 \u03b5 , log n ) + O\u0303 (nk + kd) \u00b7 poly ( 1 \u03b5 ) in the two-round row-partitioned distributed model. For the two-round column-partitioned distributed model, the total communication of the protocol is n\u03b3 \u00b7 2poly(k) \u00b7 poly ( 1 \u03b5 , log d ) + O\u0303 (nk + kd) \u00b7 poly ( 1 \u03b5 ) ."
        },
        {
            "heading": "6 Experiments",
            "text": "In this section, we aim to evaluate the feasibility of the algorithmic ideas of our paper against existing algorithms for binary matrix factorization from previous literature. The running time of our full algorithms for BMF is prohibitively expensive, even for small k, so our algorithm will be based on the idea of [KPRW19], who only run their algorithms in part, obtaining weaker theoretical guarantees. Indeed, by simply performing k-means clustering, they obtained a simple algorithm that outperformed more sophisticated heuristics in practice.\nWe perform two main types of experiments, first comparing the algorithm presented in the next section against existing baselines and then showing the feasibility of using coresets in the BMF setting.\nBaseline and algorithm. We compare several algorithms for binary matrix factorization that have implementations available online, namely the algorithm by Zhang et al. [ZLDZ07], which has been implemented in the NIMFA library [ZZ12], the message passing algorithm of Ravanbakhsh et al. [RPG16], as well as our implementation of the algorithm used in the experiments of [KPRW19]. We refer to these algorithms as Zh, MP, and kBMF, respectively. We choose the default parameters provided by the implementations. We chose the maximum number of rounds for the iterative methods so that the runtime does not exceed 20 seconds, as all methods besides [KPRW19] are iterative. However, in our experiments, the algorithms usually converged to a solution below the maximum number of rounds. We let every algorithm use the matrix operations over the preferred semiring, i.e. boolean, integer, or and-or matrix multiplication, in order to achieve the best approximation. We additionally found a binary matrix factorization algorithm for sparse matrices based on subgradient descent and random sampling1 that is not covered in the literature. This algorithm was excluded from our experiments as it did not produce binary factors in our experiments. Specifically, we found that it produces real-valued U and V, and requires binarizing the product UV after multiplication, therefore not guaranteeing that the binary matrix is of rank k.\n1https://github.com/david-cortes/binmf\nMotivated by the idea of partially executing a more complicated algorithm with strong theoretical guarantees, we build upon the idea of finding a k-means clustering solution as a first approximation and mapping the Steiner points to their closest neighbors in A, giving us a matrix V of k binary points, and a matrix U of assignments of the points of A to their nearest neighbors. This solution restricts U to have a single non-zero entry per row. Instead of outputting this U as [KPRW19] did, we solve the minimization problem minU\u2208{0,1}n\u00d7k \u2225UV\u2212A\u22252F exactly at a cost of 2k per row, which is affordable for small k. For a qualitative example of how this step improves the solution quality, see Figure 1. We call this algorithm kBMF+.\nUsing k-means as the first step in a binary matrix factorization algorithm is well-motivated by the theoretical and experimental results of [KPRW19], but does not guarantee a (1 + \u03b5)approximation. However, as we do not run the full algorithm, we are not guaranteed a (1 + \u03b5)approximation either way, as unfortunately, guessing the optimal matrix V is very time-consuming. We would first have to solve the sketched problem \u2225SA\u0303\u2212 SUV\u22252F for all guesses of SA and SU.\nWe implement our algorithm and the one of [KPRW19] in Python 3.10 and numpy. For solving k-means, we use the implementation of Lloyd\u2019s algorithm with k-means++ seeding provided by the scikit-learn library [PVG+11]. All experiments were performed on a Linux notebook with a 3.9 GHz 12th generation Intel Core i7 six-core processor with 32 gigabytes of RAM.\nDatasets. We use both real and synthetic data for our experiments. We choose two datasets from the UCI Machine Learning Repository [DG17], namely the voting record of the 98th Congress, consisting of 435 rows of 16 binary features representing each congressperson\u2019s vote on one of 16 bills, and the Thyroid dataset2, of 9371 patient data comprising 31 features. We restricted ourselves to only binary features, leaving us with 21 columns. Finally, we use the ORL dataset of faces, which we binarize using a threshold of 0.33, as in [KPRW19].\nFor our synthetic data, we generate random matrices, where each entry is set to be 1 independently with probability p, at two different sparsity levels of p \u2208 {0.1, 0.5}. Additionally, we generate low-rank matrices by generating U \u2208 {0, 1}n\u00d7k and V \u2208 {0, 1}k\u00d7d and multiplying them together in F2. We generate U and V at different sparsity levels of 0.5 and 0.1, for k \u2208 {5, 10, 15}. Finally, we also use these matrices with added noise, where after multiplying, each bit is flipped with probability pe \u2208 {0.01, 0.001}.\nWe generate 25 matrices of size 250 \u00d7 50 for each configuration. These classes are named, in order of introduction: full, lr, and noisy.\nLimitations. We opted to use only binary datasets, thus limiting the available datasets for our experiments. Because of this, our largest dataset\u2019s size is less than 10000. Our algorithms are practical for these sizes and the parameters k we have chosen. Investigating the feasibility of algorithms for binary matrix factorization for large datasets may be an interesting direction for future research."
        },
        {
            "heading": "6.1 Comparing Algorithms for BMF",
            "text": "Synthetic data. For each algorithm, Table 2 shows the mean Frobenius norm error (i.e. errA(U,V) = \u2225UV\u2212A\u2225F ) across 10 runs of each algorithm and the mean runtime in milliseconds for the synthetic datasets described above. For our choices of parameters, we find that all algorithms terminate in under a second, with Zhang\u2019s algorithm and BMF being the fastest and the message-passing algorithm generally being the slowest. This is, of course, also influenced by the fact that the algorithms\u2019 implementations use different technologies, which limits the conclusions we can draw from the data. We find that the kBMF+ algorithm slows down by a factor of 1.5 for small k \u2208 {2, 3, 5}, and 15 when k = 15, compared to the kBMF algorithm.\nThis is offset by the improved error, where our algorithm kBMF+ generally achieves the best approximation for dense matrices, being able to sometimes find a perfect factorization, for example, in the case of a rank 5 matrix, when using k \u2208 {10, 15}. Even when the perfect factorization is not found, we see that the Frobenius norm error is 2-10 times lower. On spare matrices, we find that Zhang\u2019s and the message-passing algorithms outperform kBMF+, yielding solutions that are about 2 times better in the worst case (matrix of rank 5, with sparsity 0.1 and k = 5). The kBMF algorithm generally performs the worst across datasets, which is surprising considering the results of [KPRW19]. Another point of note is that Zhang\u2019s algorithm is tuned for sparse matrices, sometimes converging to factors that yield real-valued matrices. If so, we attempted to round the matrix as best we could.\nReal data. As before, Table 3 shows the algorithms\u2019 average Frobenius norm error and average running time. We observe, that all algorithms are fairly close in Frobenius norm error, with the\n2https://www.kaggle.com/datasets/emmanuelfwerr/thyroid-disease-data\nbest and worst factorizations\u2019 error differing by about up to a factor of 3 across parameters and datasets. Zhang\u2019s algorithm performs best on the Congress dataset, while the message-passing algorithm performs best on the ORL and Thyroid datasets. The kBMF algorithm generally does worst, but the additional processing we do in kBMF+ can improve the solution considerably, putting it on par with the other heuristics. On the Congress dataset, kBMF+ is about 1.1-2 times worse than Zhang\u2019s, while on the ORL dataset, it is about 10-30% worse than the message-passing algorithm. Finally, the Thyroid dataset\u2019s error is about 10-20% worse than competing heuristics.\nWe note that on the Thyroid datasets, which has almost 10000 rows, Zhang\u2019s algorithm slows considerably, about 10 times slower than kBMF and even slower than kBMF+ for k = 15. This suggests that for large matrices and small to moderate k, the kBMF+ algorithm may actually run faster than other heuristics while providing comparable results. The message-passing algorithm slows tremendously, being almost three orders of magnitude slower than kBMF, but we believe this could be improved with another implementation.\nDiscussion. In our experiments, we found that on dense synthetic data, the algorithm kBMF+ outperforms other algorithms for the BMF problem. Additionally, we found that is competitive for sparse synthetic data and real datasets. One inherent benefit of the kBMF and kBMF+ algorithms is that they are very easily adapted to different norms and matrix products, as the clustering step, nearest neighbor search, and enumeration steps are all easily adapted to the setting we want. A benefit is that the factors are guaranteed to be either 0 or 1, which is not true for Zhang\u2019s heuristic, which does not always converge. None of the existing heuristics consider minimization of Lp norms, so we omitted experimental data for this setting, but we note here that the results are qualitatively similar, with our algorithm performing best on dense matrices, and the heuristics performing well on sparse data."
        },
        {
            "heading": "6.2 Using Coresets with our Algorithm",
            "text": "Motivated by our theoretical use of strong coresets for k-means clustering, we perform experiments to evaluate the increase in error using them. To this end, we run the BMF+ algorithm on either the entire dataset, a coreset constructed via importance sampling [BLK17, BFL+21], or a lightweight\ncoreset [BLK18]. Both of these algorithms were implemented in Python. The datasets in this experiment are a synthetic low-rank dataset with additional noise (size 5000 \u00d7 50, rank 5 and 0.0005 probability of flipping a bit), the congress, and thyroid datasets.\nWe construct coresets of size rn for each r \u2208 {0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, . . . , 0.9}. We sample 10 coresets at every size and use them when finding V in our BMF+ algorithm. Theory suggests that the quality of the coreset depends only on k and the dimension of the points d, which is why in Figure 2, we observe a worse approximation for a given size of coreset for larger k. We find that the BMF+ algorithm performs just as well on lightweight coresets as the one utilizing the sensitivity sampling framework. This is expected in the binary setting, as the additive error in the weaker guarantee provided by lightweight coresets depends on the dataset\u2019s diameter. Thus, the faster, lightweight coreset construction appears superior in this setting.\nWe observe that using coreset increases the Frobenius norm error we observe by about 35%, but curiously, on the low-rank dataset, the average error decreased after using coresets. This may be due to coreset constructions not sampling the noisy outliers that are not in the low-dimensional subspace spanned by the non-noisy low-rank matrix, letting the algorithm better reconstruct the original factors instead.\nOur datasets are comparatively small, none exceeding 1000 points, which is why, in combination with the fact that the coreset constructions are not optimized, we observe no speedup compared to the algorithm without coresets. However, even though constructing the coreset takes additional time, the running time between variants remained comparable. We expect to observe significant speedups for large datasets using an optimized implementation of the coreset algorithms. Using off the shelf coresets provides a large advantage to this algorithm\u2019s feasibility compared to the iterative methods when handling large datasets."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this paper, we introduced the first (1 + \u03b5)-approximation algorithms for binary matrix factorization with a singly exponential dependence on the low-rank factor k, which is often a small parameter. We consider optimization with respect to the Frobenius loss, finite fields, and Lp loss. Our algorithms extend naturally to big data models and perform well in practice. Indeed, we conduct empirical evaluations demonstrating the practical effectiveness of our algorithms. For future research, we leave open the question for (1 + \u03b5)-approximation algorithms for Lp loss without bicriteria requirements."
        }
    ],
    "title": "Fast (1 + \u03b5)-Approximation Algorithms for Binary Matrix Factorization",
    "year": 2023
}