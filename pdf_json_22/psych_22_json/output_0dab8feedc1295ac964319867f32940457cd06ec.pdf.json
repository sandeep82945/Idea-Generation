{
    "abstractText": "A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations. Inequalities are reflected in the data collected for scientific purposes. When not properly accounted for, machine learning (ML) models leart from data can reinforce these structural inequalities or biases. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. We also show that mitigation techniques, both standard and our own post-hoc method, can be effective in reducing the level of unfair bias. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about the impact of debiasing interventions. Finally, we provide practical recommendations to develop bias-aware ML models for depression risk prediction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Vien Ngoc Dang"
        },
        {
            "affiliations": [],
            "name": "Anna Cascarano"
        },
        {
            "affiliations": [],
            "name": "Rosa H. Mulder"
        },
        {
            "affiliations": [],
            "name": "Charlotte Cecil"
        },
        {
            "affiliations": [],
            "name": "Maria A. Zuluaga"
        },
        {
            "affiliations": [],
            "name": "Jer\u00f3nimo Hern\u00e1ndez-Gonz\u00e1lez"
        },
        {
            "affiliations": [],
            "name": "Karim Lekadir"
        }
    ],
    "id": "SP:19e158ef0be63c9f67136e7630c1d691bda19ab2",
    "references": [
        {
            "authors": [
                "M.J. Friedrich"
            ],
            "title": "Depression is the leading cause of disability around the world",
            "venue": "JAMA 317,",
            "year": 2017
        },
        {
            "authors": [
                "S. Bachmann"
            ],
            "title": "Epidemiology of suicide and the psychiatric perspective",
            "venue": "Int. J. Environ. Res. Public Heal. 15,",
            "year": 2018
        },
        {
            "authors": [
                "Bueno-Notivol",
                "J. et al. Prevalence of depression during the covid-19 outbreak"
            ],
            "title": "A meta-analysis of community-based studies",
            "venue": "Int. J. Clin. Heal. Psychol. 21, 100196",
            "year": 2021
        },
        {
            "authors": [
                "V Anttila"
            ],
            "title": "Analysis of shared heritability in common disorders of the brain",
            "venue": "Science 360,",
            "year": 2018
        },
        {
            "authors": [
                "D.H. Geschwind",
                "J. Flint"
            ],
            "title": "Genetics and genomics of psychiatric disease",
            "venue": "Science 349,",
            "year": 2015
        },
        {
            "authors": [
                "Harald",
                "R. et al. An exposome perspective"
            ],
            "title": "early-life events and immune development in a changing world",
            "venue": "The J. Allergy Clin. Immunol. 140, 24\u201340",
            "year": 2017
        },
        {
            "authors": [
                "J Olesen"
            ],
            "title": "The economic cost of brain disorders in europe",
            "venue": "Eur. J. Neurol",
            "year": 2012
        },
        {
            "authors": [
                "I.Y. Chen",
                "P. Szolovits",
                "M. Ghassemi"
            ],
            "title": "Can ai help reduce disparities in general medical and mental health care",
            "venue": "AMA journal ethics 21,",
            "year": 2019
        },
        {
            "authors": [
                "Y Park"
            ],
            "title": "Comparison of methods to reduce bias from clinical prediction models of postpartum depression",
            "venue": "JAMA Netw. Open 4,",
            "year": 2021
        },
        {
            "authors": [
                "M. Nemesure",
                "M. Heinz",
                "R Huang"
            ],
            "title": "Predictive modeling of depression and anxiety using electronic health records and a novel machine learning approach with artificial intelligence",
            "venue": "Sci. Reports",
            "year": 1980
        },
        {
            "authors": [
                "P.W. Corrigan",
                "B.G. Druss",
                "D.A. Perlick"
            ],
            "title": "The impact of mental illness stigma on seeking and participating in mental health care",
            "venue": "Psychol. Sci. Public Interest 15,",
            "year": 2014
        },
        {
            "authors": [
                "E.C. Wong",
                "R.L. Collins",
                "J. Cerully",
                "R. Seelam",
                "B. Roth"
            ],
            "title": "Differences in mental illness stigma and discrimination among californians experiencing mental health challenges",
            "venue": "RAND Corporation",
            "year": 2017
        },
        {
            "authors": [
                "R.P. Albert"
            ],
            "title": "Why is depression more prevalent in women",
            "venue": "J. Psychiatry Neurosci",
            "year": 2015
        },
        {
            "authors": [
                "F. Lubin",
                "A. Lusky",
                "A. Chetrit",
                "R. Dankner"
            ],
            "title": "Lifestyle and ethnicity play a role in all-cause mortality",
            "venue": "The J. Nutr",
            "year": 2003
        },
        {
            "authors": [
                "D Runyan"
            ],
            "title": "Longitudinal studies on child abuse and neglect (longscan) ages 0-18, version 1.4 dataset",
            "venue": "National Data Archive on Child Abuse and Neglect",
            "year": 2014
        },
        {
            "authors": [
                "A Tran"
            ],
            "title": "Health assessment of french university students and risk factors associated with mental health disorders",
            "venue": "PLOS ONE 12,",
            "year": 2017
        },
        {
            "authors": [
                "Sudlow",
                "C. et al. Uk biobank"
            ],
            "title": "an open access resource for identifying the causes of a wide range of complex diseases of middle and old age",
            "venue": "PLoS medicine 12, e1001779",
            "year": 2015
        },
        {
            "authors": [
                "Q. Feng",
                "Q. Zhang",
                "Y. Du",
                "Y. Ye",
                "Q. He"
            ],
            "title": "Associations of physical activity, screen time with depression, anxiety and sleep quality among chinese college freshmen",
            "venue": "PloS One 9,",
            "year": 2014
        },
        {
            "authors": [
                "N. Bayram",
                "N. Bilgel"
            ],
            "title": "The prevalence and socio-demographic correlations of depression, anxiety and stress among a group of university students",
            "venue": "Soc Psychiat Epidemiol",
            "year": 2008
        },
        {
            "authors": [
                "E. Ovuga",
                "J. Boardman",
                "D. Wasserman"
            ],
            "title": "Undergraduate student mental health at makerere university, uganda",
            "venue": "World Psychiatry",
            "year": 2006
        },
        {
            "authors": [
                "K. Kroenke",
                "R.L. Spitzer",
                "Williams",
                "J.B.W. The phq-9"
            ],
            "title": "validity of a brief depression severity measure",
            "venue": "J. Gen. Intern. Medicine 16, 606\u2013613",
            "year": 2001
        },
        {
            "authors": [
                "H. Yu",
                "F. Huang",
                "C. Lin"
            ],
            "title": "Dual coordinate descent methods for logistic regression and maximum entropy models",
            "venue": "Mach Learn",
            "year": 2011
        },
        {
            "authors": [
                "T. Chen",
                "Guestrin",
                "C. Xgboost"
            ],
            "title": "A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785\u2013794",
            "year": 2016
        },
        {
            "authors": [
                "J.W. Tukey"
            ],
            "title": "Comparing individual means in the analysis of variance",
            "venue": "Biometrics 5,",
            "year": 1949
        },
        {
            "authors": [
                "J Xu"
            ],
            "title": "Algorithmic fairness in computational medicine",
            "venue": "EBioMedicine 84,",
            "year": 2022
        },
        {
            "authors": [
                "T. Calders",
                "F. Kamiran",
                "M. Pechenizkiy"
            ],
            "title": "Building classifiers with independency constraints",
            "venue": "In IEEE International Conference on Data Mining Workshops,",
            "year": 2009
        },
        {
            "authors": [
                "P. Mosteiro",
                "J. Kuiper",
                "J. Masthoff",
                "F. Scheepers",
                "M. Spruit"
            ],
            "title": "Bias discovery in machine learning models for mental health",
            "venue": "Information 13,",
            "year": 2022
        },
        {
            "authors": [
                "S. Verma",
                "J. Rubin"
            ],
            "title": "Fairness definitions explained",
            "venue": "In Proceedings of the International Workshop on Software Fairness,",
            "year": 2018
        },
        {
            "authors": [
                "M. Feldman",
                "S.A. Friedler",
                "J. Moeller",
                "C. Scheidegger",
                "S. Venkatasubramanian"
            ],
            "title": "Certifying and removing disparate impact",
            "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2015
        },
        {
            "authors": [
                "G. Pleiss",
                "M. Raghavan",
                "F. Wu",
                "J. Kleinberg",
                "K.Q. Weinberger"
            ],
            "title": "On fairness and calibration",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Rodolfa",
                "K.T. et al. Case study"
            ],
            "title": "predictive fairness to reduce misdemeanor recidivism through social service interventions",
            "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency, 142\u2013153",
            "year": 2020
        },
        {
            "authors": [
                "T. Jang",
                "P. Shi",
                "X. Wang"
            ],
            "title": "Group-aware threshold adaptation for fair classification",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Bellamy",
                "R.K. E"
            ],
            "title": "Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias (2018)",
            "year": 2018
        },
        {
            "authors": [
                "I.Y. Chen",
                "F.D. Johansson",
                "D. Sontag"
            ],
            "title": "Why is my classifier discriminatory",
            "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "T. Calders",
                "F. Kamiran",
                "M. Pechenizkiy"
            ],
            "title": "Building classifiers with independency constraints",
            "venue": "In IEEE International Conference on Data Mining Workshops,",
            "year": 2009
        },
        {
            "authors": [
                "A.K. Menon",
                "R.C. Williamson"
            ],
            "title": "The cost of fairness in binary classification",
            "venue": "In Proceedings of the 1st Conference on Fairness, Accountability and Transparency,",
            "year": 2018
        },
        {
            "authors": [
                "A. de Hond",
                "A. Leeuwenberg",
                "Hooft",
                "L. et al. Guidelines",
                "quality criteria for artificial intelligence-based prediction models in healthcare"
            ],
            "title": "a scoping review",
            "venue": "npj Digit. Medicine 5, 2",
            "year": 2022
        },
        {
            "authors": [
                "Friedler",
                "S. A"
            ],
            "title": "A comparative study of fairness-enhancing interventions in machine learning",
            "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,",
            "year": 2019
        },
        {
            "authors": [
                "M.B. Zafar",
                "I. Valera",
                "M.G. Rodriguez",
                "Gummadi",
                "K.P. Fairness constraints"
            ],
            "title": "mechanisms for fair classification",
            "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, 962\u2013970",
            "year": 2017
        },
        {
            "authors": [
                "K.T. Rodolfa",
                "H. Lamba",
                "R. Ghani"
            ],
            "title": "Empirical observation of negligible fairness\u2013accuracy trade-offs in machine learning for public policy",
            "venue": "Nat. Mach. Intell",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "A significant level of stigma and inequality exists in mental healthcare, especially in under-served populations. Inequalities are reflected in the data collected for scientific purposes. When not properly accounted for, machine learning (ML) models",
            "text": "leart from data can reinforce these structural inequalities or biases. Here, we present a systematic study of bias in ML models designed to predict depression in four different case studies covering different countries and populations. We find that standard ML approaches show regularly biased behaviors. We also show that mitigation techniques, both standard and our own post-hoc method, can be effective in reducing the level of unfair bias. No single best ML model for depression prediction provides equality of outcomes. This emphasizes the importance of analyzing fairness during model selection and transparent reporting about the impact of debiasing interventions. Finally, we provide practical recommendations to develop bias-aware ML models for depression risk prediction."
        },
        {
            "heading": "Introduction",
            "text": "Depression is a leading cause of disability worldwide, a major risk factor for the global burden of disease, and can even lead to suicide [1, 2]. Taking into account that the global prevalence of depression increased by 25% during the COVID-19 outbreak [3], being able to identify those individuals at risk would be of great value to enable the application of personalized preventive measures. To this end, it is necessary to characterize the factors leading up to the development of depression. Research to date points to the importance of both genetic and environmental factors (as well as their interaction) in the etiology of depression [4, 5]. Furthermore, environmental factors have been shown to co-occur, exerting cumulative effects on depression risk. The totality of these environmental influences is often referred to as the exposome and includes environmental and lifestyle factors, as well as traumatic life events [6]. Exposome data does not only provide an alternative picture, it is also relatively inexpensive and easy to acquire, typically through questionnaires [7]. Motivated by the successful application of machine learning (ML) in different contexts of the medical domain, there is a spike in the use of ML for the detection, diagnosis, and treatment of depression [8\u201310]. Specifically, supervised ML methods are commonly used to learn predictive models from historical data, which are then applied to predict possible illness development in new cases and patients.\nRecently, concerns have been raised about algorithmic bias [8] and the undesirable ability of ML models of amplifying unfair behaviors masked in past practice, that is, in the data used for model learning. The term \u201calgorithmic bias\u201d refers to differences in the predictive power of models when applied to different subgroups of the population. These differences are particularly worrying if they are found when the subgroups are determined according to some protected attribute such as ethnicity, sex, or age. The subgroup that is adversely impacted by the bias of the ML model is known as the unprivileged group, and the subgroup that is unfairly benefited is known as the privileged group. Although this undesirable behavior of ML models is nowadays well known, assessing the bias of ML models (and trying to mitigate it) is not a common practice in healthcare applications. Chen et al. [8] examine an ML algorithm on psychiatric notes to predict 30-day psychiatric readmission regarding sex, ethnicity, and insurance type without addressing algorithmic bias. Park et al. [9] reduce bias for clinical prediction models of postpartum depression only associated with one protected attribute - binarized ethnicity (Black individuals and White\nar X\niv :2\n21 1.\n05 32\n1v 3\n[ cs\n.L G\n] 2\n6 O\nindividuals). In mental health, several unintentional discriminative behaviors have been detected, which could potentially be reproduced by ML models if they are reflected in the training data. Specifically, a lack of representation of the patient subgroups has been reported; for example, some ethnic groups do not use mental health services as much as others due to cultural stigma surrounding mental illness [11, 12]. Prior research shows that the prevalence or incidence of depression differs across sex subgroups; women are about twice as likely as men to develop depression during their lifetime [13]. In addition, manifest discrepancies in relevant factors such as lifestyle or nutritional habits between subgroups have also been reported [14]. These, and possibly other factors, can be rooted in the data which is used to learn ML models for depression prediction.\nIn this paper, we investigate how ML algorithms could perpetuate or reduce structural inequality and unfair bias learned from the data. We present a systematic analysis of algorithmic bias in ML models designed to predict the presence or absence of depression from environmental and lifestyle data, using four public datasets: LONGSCAN [15], FUUS [16], NHANES [17], and the UK Biobank (UKB) [18]. We study unfair bias on protected attributes including demographic factors (sex, ethnicity, nationality), socioeconomic status (age, income, academic qualifications), and co-morbidities (cardiovascular disease (CVD), diabetes), and evaluate the interplay of model accuracy and fairness. For this study, protected attributes were agreed based on standard choices in the related literature. We analyze the ability of standard bias mitigation techniques to reduce the discrimination level of the models learned for our four case studies. The mitigation effect is measured as the performance difference, in terms of fairness and standard ML metrics, before and after performing bias mitigation. We have found unfair biases in the behavior of the models learned with standard ML techniques regarding several protected attributes in all the case studies. We also found, however, that mitigation techniques are effective in reducing discrimination levels. Our results suggest that bias monitoring is pertinent in the evaluation of ML-based predictive models in mental health and current mitigation techniques provide a powerful toolset to mitigate unfair algorithmic bias."
        },
        {
            "heading": "Methods",
            "text": "In this section, we introduce and perform an initial descriptive analysis of the available data of our case studies (Sec. Datasets). In the following, we describe the predictive models tested and the five strategies we considered to mitigate bias (Sec. Bias mitigation approaches). We also explain both standard ML and fairness evaluation metrics used in our experiments (Sec. Evaluation metrics)."
        },
        {
            "heading": "Datasets",
            "text": "This study uses four public datasets: LONGSCAN, FUUS, NHANES, and UK Biobank. We select these datasets to cover a spectrum of sizes, from small to large, and to showcase diverse methodologies for diagnosing depression. We aim to assess potential biases across different scales and diagnostic methods. These differences allow us to investigate how the size of the dataset (both in terms of the number of samples and input variables) influences bias in both plain ML models and those combined with debiasing techniques. All the participants and/or their carers provided written informed consent in LONGSCAN, NHANES, and UKB studies. This was not required in the case of FUUS according to the laws that regulate \u201cnon-interventional clinical research\u201d in France [16]. While LONGSCAN and FUUS are datasets of late adolescents, NHANES and UKB have most subjects between 40 and 80 years of age. Supplementary Table 1 describes the protected attributes considered for each dataset, as they differ between datasets. The LONGSCAN, FUUS, NHANES, and UKB datasets have relatively equal proportions of male and female subjects. A higher prevalence of depression in women versus men is evident in the LONGSCAN, NHANES, and UKB datasets. No sex effect is found among college freshmen in the FUUS dataset, which is consistent with previous studies [19\u201321]. Other protected attributes have a highly skewed distribution.\nParticipants and features. Participants in LONGSCAN were enrolled from five United States\u2019 sites (South, East, Midwest, Northwest, and Southwest), with different selection criteria, representing varying levels of risk or exposure to maltreatment during the period spanning from 1991 to 2012. The LONGSCAN interview and questionnaire data were collected when target children were at ages 4, 6, 8, 12, 14, 16, and 18. Out of 1354 total participants, we kept the 67.3% of children who completed an interview at age 18 years including depression outcomes, leading to 911 samples available for our study. Among these 911 individuals, there were 363 cases with depression at the age of 18, and 548 controls. The study design is depicted in Supplementary Figure 1: data from three different stages (early childhood, late childhood, teen) were collected to predict depression at age of 18. Up to 23 descriptive variables grouped as demographic variables, lifestyles variables, and adverse exposures variables, both time-invariant and repeatedly measured along these stages, were considered (see Supplementary\nTable 2). Data is available under request; its use for this study was approved by the National Data Archive on Child Abuse and Neglect (NDACAN). Participants in FUUS were undergraduate students who underwent a compulsory medical visit at the university medical service in Nice (France) between September 2012 and June 2013. Among 4184 total participants, there are 528 cases with depression and 3656 controls. A total of 62 biomedical and demographic features were used including binary, ordinal and continuous variables (see Supplementary Table 3). Participants in NHANES provided data between 2005 and 2018 and were selected by random sampling of the American population. Among 36259 total participants, there were 3168 cases\n2/11\nwith depression and 33091 controls. A total of 86 features were used in our study, including demographic data, socioeconomic status, medical history, lifestyle characteristics, and prescription medications. Factors that might involve data leakage, such as depression-specific medications, were excluded (see Supplementary Table 4). FUUS and NHANES datasets are publicly available. Participants in UKB were enrolled from 22 United Kingdom\u2019s centers from 2006 to 2010. Among the 461,033 participants initially without depression, 18112 cases (3.93%) developed depression. Up to 143 descriptive variables were considered, including demographic data, socioeconomic status, medical history, lifestyle characteristics, early life factors, and traumatic events (see Supplementary Table 5). Data is available under request; its use for this study was approved by UKB, under the project title \u201cAssociation between Early-Life-Stress and Psycho-Cardio-Metabolic Multi-Morbidity: The EarlyCause H2020 Project\u201d (application number 65769).\nBuilding the ground truth - depression outcome. We acquired ground truth label information \u2013whether a participant has depression or not\u2013 using dataset-specific information. In LONGSCAN, depression was assessed using a self-reported questionnaire at age 18, which includes a specific question regarding having depression. In NHANES, the Patient Health Questionnaire-9 (PHQ-9) was used. This screening questionnaire consists of 9 items (scored 0-3) and has a specificity and sensitivity of 88% for major depressive disorder (MDD) at a threshold score of 10 or more [22]. Therefore, we chose the threshold at PHQ-9 score 10. Additionally, we carefully excluded participants\u2019 feelings and expressions, as well as lifestyle characteristics (physical activity, diet, sleep habits) from the set of descriptive features, which could cause label leakage. In FUUS, the depression outcome was evaluated in a two-stage process. If the result of an initial 4-item screening questionnaire indicated possible presence of MDD (at least two of the four symptoms present), the participants were assessed by a medical provider for the full Diagnostic and Statistical Manual of Mental Disorders Fourth Edition (DSM IV) criteria [10]. In UKB, the depression outcome was defined as an occurrence of a depressive episode (ICD10 code F32 and F33) after the date of assessment, which was drawn from hospital inpatient diagnoses or conditions self-reported. Note that this label information might be noisy due to the use of questionnaires. Addressing biased labels is a complex ML problem that falls beyond the scope of our study.\nTwo popular types of ML models were learned from this data: (i) logistic regression (LR) [23], a linear classifier, and (ii) extreme gradient boosting (XGB) [24], a boosting ensemble method. In this paper, we do not focus on the algorithmic aspects of the ML methods considered, but rather on their clinical application and the fairness assessment of their predictions. We use k-fold cross validation (k = 5 for UKB, k = 10 for the rest) for performance evaluation and nested cross-validation for hyper-parameters tuning (see Supplementary Table 6). Tukey\u2019s range test [25] identifies the 95%-significance for true-positive rates for each group."
        },
        {
            "heading": "Bias mitigation approaches",
            "text": "Many techniques have been proposed over the last few years to address algorithmic fairness [26]. However, there is a significant shortfall in addressing fairness and bias concerns when ML has been applied to the field of psychiatry. Only a handful of studies have adopted methods to counteract bias. For instance, reweighing (RW) bias-mitigation technique [27] was used to minimize bias when forecasting future benzodiazepine administrations [28]. Likewise, others applied Suppression (SUP) [29] and RW approaches to reduce bias in the prediction of postpartum depression [9]. In healthcare and other fields, the Disparate Impact Remover (DIR) method [30] has shown its ability to effectively mitigate bias while maintaining a satisfactory level of predictive performance. Furthermore, the Calibrated Equalized Odds Post-processing (CPP) method [31] was proposed in the context of healthcare to predict whether an individual has a heart condition. Among all the available bias-mitigation techniques, we consider four standard methods in this study: SUP, RW, DIR, and CPP. Moreover, we propose a novel post-hoc disparity mitigation named Population Sensitivity-Guided Threshold Adjustment (PSTA).\nSuppression (SUP). Protected attributes are directly removed from the training dataset under the assumption that access to this information is the main cause of bias. First, the protected attribute is removed from the dataset. Then, a new new ML model is learned from this new version of the dataset.\nReweighing (RW). This method weights the samples in each (group, label) combination differently to make the protected attribute and outcome statistically independent of each other before model learning. The weight of a sample is directly proportional to the frequency of its label in the whole population and inversely proportional to the frequency of its label in its subgroup. The model is learned from this new training dataset with weighted samples.\nDisparate impact remover (DIR). Given a dataset D = (X ,Y,C), with protected attribute X , remaining attributes Y , and class variable or outcome C, this repair process attempts to remove bias in the remaining features Y . New values are assigned to all the cases and variables in Y . The new values ensure that all the groups follow the same distribution over every variable, making adjustments based on percentiles and quantile functions. The predictive model is learned from the new training dataset. In the three previous bias-mitigation techniques, we act on the training data. Once a newly prepared version of the training dataset is obtained, an ML model is learned from it, with the effectiveness of bias mitigation evaluated based on this model\u2019s outputs. The following two techniques perform differently. The model is learned from the original data. Then, the outputs of\n3/11\nthe model are modified under some criteria to reduce disparities. The success of the mitigation approach is then evaluated using these modified outputs.\nCalibrated equalized odds postprocessing (CPP). The method calibrates the predicted probability so that the false-positive rate or the false-negative rate of privileged and unprivileged groups are on average equal. It modifies the score outputs of the model for the different subgroups so that the output labels meet an equalized odds objective. In our clinical application, we focus on identifying individuals at risk of depression and desire equitable outcomes; therefore, we consider recall more important than precision, which leads us to set a cost constraint objective: the equal false-negative rates between the subgroups.\nPopulation Sensitivity-Guided Threshold Adjustment (PSTA). This approach, our own bias mitigation technique, is inspired by the observation that when there are prevalence rate discrepancies between groups in the training dataset, ML models usually associate the positive class mainly with the characteristics of the subgroup of highest prevalence. Supplementary Figures 2a and 2b illustrate the predicted probability distributions generated by a ML model with data from UKB with different prevalences for females and males suffering from depression. The distributions differ significantly. The difference arises from the prevalence rate discrepancies in the training dataset, leading the model to inherently assign lower probabilities of suffering from depression to individuals from the male subgroup, which has a lower prevalence rate. Applying the default single decision threshold at 0.5 for both groups results in a lower TPR and a higher FPR for the male group, exacerbating disparities in the outcomes. This example model would benefit from a tailored threshold adaptation that considers the disparities in the distribution of probabilities between different subgroups, ultimately improving both fairness and accuracy. The use of threshold adaptation to address bias and unfairness in ML models is not entirely new, as evidenced by previous works such as [32, 33]. However, our proposal introduces a novel procedure for obtaining the optimal threshold, which differentiates it from existing approaches. Our method aims to improve fairness while maintaining model performance. It is guided by the fairness metric Equal Opportunity, which is applicable to categorical protected features, i.e., it is not limited to binary features only. The main objective is to enhance sensitivity for unprivileged groups while maintaining an acceptable FPR, which refers to a level that is not significantly higher than the overall population\u2019s FPR, ensuring a balance between minimizing incorrect positive predictions and effectively detecting true positive cases. It determines subgroup-specific thresholds for unprivileged groups to ensure that their sensitivity aligns with that of the overall population on training data, encompassing the sensitivity of all subgroups. For the remaining groups, the standard threshold at 0.5 is employed. This conventional 0.5 threshold can be adjusted as a hyperparameter to better suit specific context requirements. The whole procedure is thoroughly detailed in Algorithm 1. As depicted in Supplementary Figure 2c , by utilizing the optimal threshold generated by PSTA on the training set for the unprivileged group (male in this case), the FPR of the unprivileged group reaches an acceptable value in the test set. This example is illustrative of the type of correction that PSTA enforces, leading to more accurate and fair predictions.\nIn this study, we use three different pre-processing mitigation techniques (SUP, RW, and DIR), which operate over training data, and two different post-processing mitigation techniques (CPP and PSTA), which operate over the model\u2019s predictions. See a graphical description of how they operate in Supplementary Figure 3. Generally, pre-processing and post-processing techniques are model-agnostic. There exists a third type of bias mitigation technique, called in-processing methods, which operate during classifier construction, leading to a significant reliance on the specific type of model in use. Given our study\u2019s focus on broadly applicable and adaptable mitigation techniques, we concentrated on pre- and post-processing methods, as their adaptability aligns better with our research objective. An in-depth comparison of bias mitigation techniques is beyond the scope of this study, which aims to demonstrate that with a diverse toolkit, ML practitioners can expect to build a model with increased fairness without a significant loss of predictive performance."
        },
        {
            "heading": "Evaluation metrics",
            "text": "Performance metrics. To assess predictive performance, we consider two standard ML performance metrics, namely, the area under the receiver operating characteristic curve (AUC-ROC), that is an average of false positive rates versus true positive rates across all the possible thresholds, and the balanced accuracy (BAcc), which is the arithmetic mean of sensitivity and specificity, using the presence and absence of depression as the positive and negative class, respectively. This weighting is especially beneficial for extremely imbalanced datasets, ensuring equitable representation in performance assessment. When discussing the fairness-accuracy trade-off, we report on empirical accuracy measured by the latter metric because, in practice, classification will be performed at a fixed threshold [9], making BAcc a more pertinent measure in such scenarios. We also report AUC-ROC performance after applying the most effective bias mitigation algorithms in Supplementary Table 7. Fairness metrics. There are three kinds of fairness metrics depending on the fairness concept that they account for: metrics that account for individual fairness, for group fairness, and for both [34]. In this study, we focus exclusively on group fairness metrics for binary classification tasks, which means that subgroups defined by a protected attribute should receive similar model outcomes measured by some statistical metrics. In this clinical application, identifying all individuals at risk of depression is desirable; therefore, we consider sensitivity over specificity which leads us to compare true-positive rates across subgroups. Specifically, we choose the equal opportunity difference (EOD) metric, which states that a binary classifier is fair if its\n4/11\nAlgorithm 1 Population Sensitivity-Guided Threshold Adjustment (PSTA) Input: A training set D = {(yi,xi,ci)}ni=1, where yi are the values of the descriptive variables, xi is the value of the protected\nattribute and ci the class for individual i, and P = {pi}ni=1 where pi is the vector of probabilities predicted for individual i and each class label.\nResult: A vector of threshold values \u03b8 , one per group \u03b8 = (\u03b8x)x\u2208\u2126X 1: Initialize threshold vector to \u03b8x = 0.5 for all the groups, x \u2208 \u2126X 2: sensitivityoverall := Calculate sensitivity in the overall training set with current \u03b8 , i.e., all decision thresholds at 0.5 3: for each unprivileged group, u in \u2126X do 4: Initialize minimum difference minDi f f = \u221e 5: Initialize optimal threshold for group \u03b8\u0302u = 0.5 6: for each possible threshold value \u03b8 \u2032 \u2208 [0,1] do 7: Calculate the sensitivity of the unprivileged group u, sensitivityu 8: di f f := abs(sensitivityu \u2212 sensitivityoverall) 9: if di f f < minDi f f then 10: minDi f f = di f f 11: \u03b8\u0302u = \u03b8 \u2032 12: end if 13: end for 14: Adopt threshold \u03b8u = \u03b8\u0302u 15: end for 16: Use the vector of thresholds \u03b8 to predict the class of new data instances\ntrue-positive rates (TPR) are equal across groups (i.e., a value of 0 indicates complete fairness). Let D = (X ,Y,C) be the dataset, with the protected variable X , regular descriptive variables Y , and the class variable C. Predictions provided by an ML model are denoted C\u0302. Let us define \u2126X as the set of possible values of variable X . For example, for the protected attribute \u201csex\u201d, \u2126X = {male, female}. A subgroup of the population is formally defined as all the samples in dataset D with the same value x \u2208 \u2126X assigned to the protected attribute X . We define the TPR of a specific subgroup x \u2208 \u2126X as\nT PRx(C\u0302) := EY [C\u0302|C = 1,X = x]\nExact equality, T PRx0(C\u0302) = T PRx1(C\u0302), for x0,x1 \u2208 \u2126X , is often hard to verify or enforce in practice. More generally, we use differences among the TPRs of the different subgroups x \u2208 \u2126X to measure the level of discrimination \u0393 [35]:\n\u0393TPR(C\u0302) := |TPRx0(C\u0302)\u2212TPRx1(C\u0302)|\nSpecifically, EOD measures the difference of TPR between the unprivileged and privileged groups:\nEOD = TPRx=unprivileged \u2212TPRx=privileged\nThis study is contextualized in a project focused on developing efficient screening and risk prediction tools for depression in primary care settings. At this point, underdiagnosis has more severe implications than misdiagnosis. When a misdiagnosis occurs, patients still receive clinical care, and clinicians can draw upon additional symptoms and data sources to correct the error. On the other hand, underdiagnosis may lead to individuals not receiving the necessary treatment and support, exacerbating their mental health condition. This reasoning motivates our decision for the group fairness EOD metric, in line with previous applications in the mental health domain [9, 28]. EOD is an attainable and practical fairness criterion which mandates equal TPRs across the demographic subgroups. This decision allows us to focus on minimizing disparities in TPRs across different groups, ensuring that all individuals at risk of depression are identified and provided with the necessary care and support they require."
        },
        {
            "heading": "Results",
            "text": "In this section, we study the behavior of the different ML models before and after bias mitigation techniques are applied. Firstly, ML models are learned from data and their predictive performance and unfair bias are quantified (Sec. Initial model performance and bias assessment). Secondly, bias mitigation techniques are applied to these models and to assess their efficiency, we evaluate the interplay between predictive performance and fairness in the adapted models (Sec. Model performance after bias mitigation). We perform this analysis in four datasets for the prediction of depression: LONGSCAN, FUUS, NHANES, and UKB.\n5/11\nInitial model performance and bias assessment For the sake of simplicity, we only report results with LR models in this manuscript. Results with XGB models, qualitatively similar to those of LR, are available in the Supplement. Regarding predictive performance, LR models achieve lower prediction accuracy with LONGSCAN and FUUS datasets, with BAcc of 0.621 (95% CI: 0.577-0.664) and 0.615 (95% CI: 0.598-0.632), respectively. The predictive performance of LR models is considerably better with NHANES and UKB datasets, with BAcc of 0.719 (95% CI: 0.711-0.727) and 0.729 (95% CI: 0.725-0.734), respectively. For a comprehensive performance report including AUC-ROC metrics, see Supplementary Table 8. These results support that the predictive ability of ML models increases with the amount of training data, as NHANES and UKB are larger. Figure 1 displays the fairness metric of the base and debiased LR models by dataset, protected attribute and subgroup, providing a comprehensive comparison of fairness performance before and after the application of bias mitigation techniques. In each plot, the horizontal shift or difference in performance for different subgroups reveals the bias. In each frame, the upper plot shows results before applying a bias mitigation technique, and the lower plot shows results after mitigation. Let us analyze the bias assessment by (type of) protected attribute: demographic factors, socioeconomic status, and co-morbidities.\nProtected attributes: demographic factors (sex, ethnicity, nationality). We show consistent sex differences across datasets in the top row of Figure 1, with higher TPRs for female subjects. The differences in TPRs between sexes are only statistically significant at the 95% confidence level for the LONGSCAN, NHANES, and UKB datasets. Note that there are no sex differences in rates of depression among subjects in the FUUS dataset (see Supplementary Table 1). Interestingly, the mean difference between sexes in the UKB and NHANES datasets (0.1121, 0.167, p < 0.001) is less than in the LONGSCAN dataset (0.4103, p < 0.001). Sex, the top importance feature in LR and XGB for the LONGSCAN dataset, has a much lower rank in the feature importance ranking for the NHANES and UKB datasets (see the feature importance rankings in the Supplement), which means depression outcome is less sensitive to sex bias in the NHANES and UKB datasets with a large sample size and features compared to the LONGSCAN dataset. We highlight the benefit obtained from considering a larger number of risk factors in the predictive model and adequate sample size to reduce bias. This evidence is in line with [35], where enhanced data collection is pointed out as a means to lessen discrimination without sacrificing accuracy. The second row of Figure 1 shows differences in TPRs between racial groups, which were not statistically significant, with black subjects and \u201cother/multiracial\u201d subjects having the lowest true-positive rates for LONGSCAN and NHANES datasets, respectively; except for the case between black subjects and white subjects in the LONGSCAN dataset, their TPRs have non-overlapping confidence intervals, indicating a significant difference. Interestingly, the UKB subjects in the \u201cdo not know/prefer not to answer\u201d (\u2018Missing\u2019) group have the lowest TPR, compared with others. As shown in Figure 1, TPRs for nationality have non-overlapping confidence intervals. Specifically, foreigner subjects have a higher rate than French subjects. We note that foreigner subjects have a higher observed depression rate in the training set.\nProtected attributes: socioeconomic status (age, income, academic qualifications). We find that the TPRs do not differ much across age groups with many overlapping intervals. However, subjects under 20 years old have the lowest TPR. This may be partially due to the fact that small subset sizes (see Supplementary Table 1) may not reflect accurate depression rates amongst the adolescents and young adults subpopulation, whose symptoms of depression and other mental illnesses have increased significantly over the last decades [36]. Differences in TPRs in the NHANES dataset are also observed between qualification groups. As seen in Figure 1, subjects in the \u201cLevel 5\u201d group have the lowest TPR, compared with others in both the NHANES and UKB datasets. Subjects in the \u201cLevel 0\u201d (Refused/Don\u2019t know/Missing) group in the NHANES dataset are also on the unprivileged side. As shown in Figure 1, TPRs for income have non-overlapping confidence intervals. Specifically, low-income subjects have a much higher rate than high-income subjects. We note that low-income subjects have a higher baseline observed event rate.\nProtected attributes: co-morbidities (CVD, diabetes). As shown in Figure 1, TPRs for CVD and diabetes all have non-overlapping confidence intervals. Specifically, individuals experiencing CVD/diabetes have a much higher rate than subjects without CVD/diabetes. These inequitable outcomes support that CVD and diabetes should be considered as important comorbidity of depression.\nModel performance after bias mitigation To assess bias mitigation, we analyze changes in fairness metrics between the previously discussed base models and the new classifiers obtained after bias mitigation. Moreover, given the clinical application, most people would not find it fair to reduce discriminatory outcomes if it identifies fewer actual positives overall. There exists an open discussion in the related literature [37, 38] regarding the actual existence of the so-called fairness-accuracy trade-off when bias mitigation is implemented, meaning that predictive performance is reduced if one tries to make the model fairer. A trade-off of accuracy for fairness is often undesirable in healthcare. Thus, we have two objectives: increasing accuracy and non-discrimination. We report the results using 2D points combining two metrics: (i) a fairness metric, measured by EOD and (ii) a standard ML performance metric, measured by BAcc, on the test set, to determine whether our models for depression prediction are experiencing a\n6/11\nfairness-accuracy trade-off. A model can be considered as fair if EOD is between -0.1 and 0.1, its ideal value is 0 and for BAcc, the larger the better. Figure 2 presents, for each dataset and protected attribute, the 2D points in the fairness-accuracy space achieved by each bias mitigation technique. The base model, without bias mitigation, is also shown (gray point and cross). The implemented bias methods help to improve fairness, for all protected attribute perspectives. This is a desirable mitigation result. Note that Figure 1 showcases group-specific TPRs for LR classifiers after applying the best-performing bias-mitigation techniques for each protected attribute. This allows for a detailed evaluation of their effectiveness across\n7/11\ndifferent study populations. It is noteworthy that in general debiasing through RW, DIR, and PSTA substantially improves fairness without compromising model accuracy. SUP only partially achieves fairness between groups. This method removes the protected attribute from the training dataset, as they are considered biased features. This result suggests that bias is not only contained in those features but elsewhere. DIR not only excludes these attributes but also adjusts non-protected features that are highly correlated to them. In any case, this repair tool is a good baseline to investigate whether this bias comes from non-protected features. On the other hand, the CPP technique preserves precision when calibrating recall, which results in BAcc reduction, according to most of the protected attributes, except for the diabetes attribute. Furthermore, it is important to highlight that, when applying bias-mitigation methods to the LONGSCAN dataset with the protected attribute sex, a significant decrease in BAcc is observed across all methods. This observation serves as evidence that the model may struggle to effectively learn the underlying structure of the data due to the relatively small dataset. Consequently, this emphasizes the necessity of having sufficient samples and relevant predictors when utilizing ML algorithms for risk prediction tasks in order to develop both fair and accurate models. Note that TPRs for the best mitigation technique (RW) regarding the \u201cincome\u201d protected attribute on models learned from the NHANES dataset still have non-overlapping confidence intervals, although the mean difference between groups was considerably reduced from 0.2485 (p < 0.001) to 0.0929 (p < 0.001)."
        },
        {
            "heading": "Discussion",
            "text": "ML algorithms have achieved state-of-the-art performance in many clinical tasks. However, to deploy them in these life-ordeath-stakes applications, it must be understood that they can induce biases against unprivileged subgroups and precautionary actions need to be taken in different deployment stages [39]. Here, we leverage our empirical study on four datasets to analyze the need of bias mitigation techniques, as well as their effectiveness, when using ML models to predict mental health issues such as depression.\nBias found when following the standard ML approach. Our results indicate that models learned following the standard ML approach show regularly unfair biased behaviors (see Figure 1). We find that, for the classification problem, unequal distribution\n8/11\nof classes between groups in the training dataset can lead the predictive model to learn that one group has a higher probability of being part of one class or another. (In Supplementary Table 1, evidence of this behavior can be observed). Therefore, ML models trained on the unbalanced dataset of a trial population, even if the sample in clinical trials is representative of the patient population, provide potential inequitable outcomes if deployed without fairness analysis. This evidence encourages ML practitioners in healthcare not only to report the model performance on the overall population regardless of the subject membership to subpopulations but also to audit and address algorithmic bias.\nBias can be mitigated. Our results show that the bias mitigation techniques improve fairness compared to the no-intervention base models. All the techniques considered enhance results in terms of the difference in TPRs, in different proportions. However, the techniques exhibit differences regarding the effect on the accuracy of the classifiers. Those learned in combination with the RW, DIR, and PSTA mitigation techniques tend to preserve predictive performance, whereas other techniques (SUP, CPP) usually compromise the level of accuracy (see Figure 2). Our findings point out that the RW technique combined with the use of a larger number of risk factors diminishes the impact of the protected attributes (and other possible proxy attributes) on the outcome of the model, which leads to reduced algorithmic bias in NHANES and UKB datasets. This solution is appropriate in our case study as it performs well when it is integrated into predictive model learning while preserving the distribution and values of the original training data, unlike DIR and SUP. In addition, we find that our proposed post-hoc disparity mitigation method (PSTA) tends to mitigate bias while preserving predictive performance. In this method, the distinct treatment of subpopulations aims to address subgroup-specific disparities. Conventional one-size-fits-all methods might inadvertently accentuate biases, especially for underrepresented groups. Therefore, our approach ensures fairness by recognizing and addressing these unique subgroup challenges. However, as other post-doc technical solutions for imposing fairness, it might be difficult to obtain the optimal threshold for subgroups with small populations underrepresented in the training set. The choice of the method must depend on the specific domain of application and desired outcomes. In the larger effort to create a fair system overall, methods like RW and DIR not only increase the TPR for the unprivileged groups but also reduce the privileged group\u2019s FPR, resulting in fewer false positives. In contrast, PSTA increases the TPR of the unprivileged groups, accepting a higher FPR to reduce the TPR gap without intervening in the privileged groups. In primary care settings, where early detection and prevention are key, PSTA may be more appropriate, as it ensures that more people are identified for further evaluation and potential intervention, maximizing the overall benefit. Conversely, in secondary care settings focused on diagnosing and treating specific conditions, techniques such as RW and DIR may be more suitable, as they balance TPR and FPR across demographic groups, ensuring effective and fair resource allocation. We also highlight the importance of prioritizing adequate data collection before employing any debiasing technique to improve fairness in predictive models. By doing so, we aim to encourage communities to open health data, further contributing to the development of more equitable ML solutions.\nFairness-accuracy, a real trade-off? In fairness literature, the existence of a trade-off between fairness and accuracy is a common assumption, that is, that fairness cannot be improved without sacrificing predictive performance [27]. A few studies also have pointed out that this trade-off may necessitate the application of more complex methods [40, 41]. Based on our empirical observations, we find that the fairness\u2013accuracy trade-off for the datasets examined in this paper can be bent if a set of bias mitigation techniques is considered, and not just one. Standard techniques RW, DIR, and CPP, or our proposed method PSTA can reduce bias while preserving predictive performance for specific (dataset, protected attribute) pairs. This was particularly evident for the modest-sized to large datasets (FUUS, NHANES, UKB), though the dynamics shifted a bit for the smaller dataset (LONGSCAN). Thus, this trade-off is not consistently observed in our case studies in depression prediction without requiring complex ML methods. In line with [42], this evidence encourages the ML community to intentionally propose frameworks that maximize both predictive performance enhancement and bias reduction, aiming to bend the trade-off. There is probably no golden bullet, as there is no single best ML model for all prediction problems providing equality of outcomes naturally. ML practitioners need to figure out which combination of type of classifier and bias mitigation algorithm is appropriate for the use case at hand so that it produces the best results in terms of both accuracy and fairness.\nIn conclusion, we conduct an empirical study on four exposome datasets to show the ability of bias mitigation techniques to increase fairness of machine learning models obtained to predict depression from environmental and lifestyle data. In addition, our main effort in this work has been directed toward providing empirical evidence to encourage clinical decision-makers to carefully evaluate a proposed framework in terms of both its accuracy and fairness prior to deployment. Experimental results support the idea that it is possible to improve algorithmic fairness without sacrificing their predictive performance. We consider that our promising results could enable a wider use of ML techniques in mental healthcare. This should inevitably go hand-in-hand with the assessment of possible biases in the models and the appropriate mitigation techniques if required. In the future, we expect to examine simultaneously the effects of having multiple protected attributes such as ethnicity, sex, socio-economic status, geographical location, or co-morbidities (type 1 and type 2 diabetes, or cardiovascular disease), as well as extending to use protective factors data, such as intelligence, temperament, cognitive appraisal, support from a significant person, which may counteract the negative effects of risk factors for depression, along with environmental and lifestyle data. In\n9/11\naddition, we aim to integrate genetic and biological data, which are robust risk factors for depression, in our research."
        },
        {
            "heading": "Data Availability",
            "text": "Code for data processing and analysis is available at https://github.com/ngoc-vien-dang/FairML-Depression. Detailed dataset sources and accessibility conditions are also provided in the repository\u2019s README."
        },
        {
            "heading": "Acknowledgements",
            "text": "VND, RHM, CC, JHG, and KL have received funding from the European Union\u2019s Horizon 2020 research and innovation programme under Grant Agreement No 848158, EarlyCause."
        },
        {
            "heading": "Author contributions statement",
            "text": "Vien Ngoc Dang: Methodology, Investigation, Data curation, Software, Visualization, Writing - original draft, review & editing. Anna Cascarano, Rosa H. Mulder, Charlotte Cecil, and Maria A. Zuluaga: Writing - review & editing. Jer\u00f3nimo Hern\u00e1ndezGonz\u00e1lez: Conceptualization, Writing - original draft, review & editing, Co-supervision. Karim Lekadir: Conceptualization, Resources, Writing - review, Supervision."
        },
        {
            "heading": "Additional information",
            "text": "Competing interests The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n11/11"
        }
    ],
    "title": "Fairness and bias correction in machine learning for depression prediction: results from four study populations",
    "year": 2023
}