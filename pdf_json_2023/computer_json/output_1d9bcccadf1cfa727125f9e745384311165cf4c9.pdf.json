{
    "abstractText": "Narrative is a ubiquitous component of human communication. Understanding its structure plays a critical role in a wide variety of applications, ranging from simple comparative analyses to enhanced narrative retrieval, comprehension, or reasoning capabilities. Prior research in narratology has highlighted the importance of studying the links between cognitive and linguistic aspects of narratives for effective comprehension. This interdependence is related to the textual semantics and mental language in narratives, referring to characters\u2019 motivations, feelings or emotions, and beliefs. However, this interdependence is hardly explored for modeling narratives. In this work, we propose the task of automatically detecting prominent elements of the narrative structure by analyzing the role of characters\u2019 inferred mental state along with linguistic information at the syntactic and semantic levels. We introduce a STORIES dataset of short personal narratives containing manual annotations of key elements of narrative structure, specifically climax and resolution. To this end, we implement a computational model that leverages the protagonist\u2019s mental state information obtained from a pre-trained model trained on social commonsense knowledge and integrates their representations with contextual semantic embed-dings using a multi-feature fusion approach. Evaluating against prior zero-shot and supervised baselines, we find that our model is able to achieve significant improvements in the task of identifying climax and resolution.",
    "authors": [
        {
            "affiliations": [],
            "name": "Prashanth Vijayaraghavan"
        },
        {
            "affiliations": [],
            "name": "Deb Roy"
        }
    ],
    "id": "SP:f5453e8aa216a4fee4a1823f665c72fc387324f9",
    "references": [
        {
            "authors": [
                "M.H. Abrams",
                "G. Harpham"
            ],
            "title": "A glossary of literary terms",
            "venue": "Nelson Education.",
            "year": 2014
        },
        {
            "authors": [
                "C.O. Alm",
                "R. Sproat"
            ],
            "title": "Emotional sequencing and development in fairy tales",
            "venue": "International Conference on Affective Computing and Intelligent Interaction, 668\u2013674. Springer.",
            "year": 2005
        },
        {
            "authors": [
                "J. Beck"
            ],
            "title": "Life\u2019s stories",
            "venue": "The Atlantic, 1.",
            "year": 2015
        },
        {
            "authors": [
                "R.A. Berman",
                "D.I. Slobin"
            ],
            "title": "Relating events in narrative: A crosslinguistic developmental study",
            "venue": "Psychology Press.",
            "year": 2013
        },
        {
            "authors": [
                "R.L. Boyd",
                "K.G. Blackburn",
                "J.W. Pennebaker"
            ],
            "title": "The narrative arc: Revealing core narrative structures through text analysis",
            "venue": "Science advances, 6(32): eaba2196.",
            "year": 2020
        },
        {
            "authors": [
                "J. Bruner"
            ],
            "title": "The narrative construction of reality",
            "venue": "Critical inquiry, 18(1): 1\u201321.",
            "year": 1991
        },
        {
            "authors": [
                "J.S. Bruner"
            ],
            "title": "Actual minds, possible worlds",
            "venue": "Harvard university press.",
            "year": 2009
        },
        {
            "authors": [
                "B. Ceran",
                "R. Karad",
                "A. Mandvekar",
                "S.R. Corman",
                "H. Davulcu"
            ],
            "title": "A semantic triplet based story classifier",
            "venue": "2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, 573\u2013580. IEEE.",
            "year": 2012
        },
        {
            "authors": [
                "J. Chen",
                "J. Chen",
                "Z. Yu"
            ],
            "title": "Incorporating structured commonsense knowledge in story completion",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, 6244\u20136251.",
            "year": 2019
        },
        {
            "authors": [
                "Y.-A. Chung",
                "H.-Y. Lee",
                "J. Glass"
            ],
            "title": "Supervised and unsupervised transfer learning for question answering",
            "venue": "arXiv preprint arXiv:1711.05345.",
            "year": 2017
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805.",
            "year": 2018
        },
        {
            "authors": [
                "A. Dirkson",
                "S. Verberne",
                "W. Kraaij"
            ],
            "title": "Narrative Detection in Online Patient Communities",
            "venue": "Text2Story@ ECIR, 21\u201328.",
            "year": 2019
        },
        {
            "authors": [
                "R.A. Dore",
                "S.J. Amendum",
                "R.M. Golinkoff",
                "K. HirshPasek"
            ],
            "title": "Theory of mind: A hidden factor in reading comprehension",
            "venue": "Educational Psychology Review,",
            "year": 2018
        },
        {
            "authors": [
                "S. Edunov",
                "M. Ott",
                "M. Auli",
                "D. Grangier"
            ],
            "title": "Understanding back-translation at scale",
            "venue": "arXiv preprint arXiv:1808.09381.",
            "year": 2018
        },
        {
            "authors": [
                "J. Eisenberg",
                "M. Finlayson"
            ],
            "title": "A simpler and more generalizable story detector using verb and character features",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, 2708\u20132715.",
            "year": 2017
        },
        {
            "authors": [
                "D.K. Elson"
            ],
            "title": "Detecting story analogies from annotations of time, action and agency",
            "venue": "Proceedings of the LREC 2012 Workshop on Computational Models of Narrative, Istanbul, Turkey, 91\u201399.",
            "year": 2012
        },
        {
            "authors": [
                "J. Ely",
                "A. Frankel",
                "E. Kamenica"
            ],
            "title": "Suspense and surprise",
            "venue": "Journal of Political Economy, 123(1): 215\u2013260.",
            "year": 2015
        },
        {
            "authors": [
                "M.A. Finlayson",
                "P.H. Winston"
            ],
            "title": "Analogical retrieval via intermediate features: The Goldilocks hypothesis",
            "year": 2006
        },
        {
            "authors": [
                "J.L. Fleiss",
                "J. Cohen"
            ],
            "title": "The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability",
            "venue": "Educational and psychological measurement, 33(3): 613\u2013619.",
            "year": 1973
        },
        {
            "authors": [
                "G. Freytag"
            ],
            "title": "Die technik des dramas",
            "venue": "S. Hirzel.",
            "year": 1894
        },
        {
            "authors": [
                "T. Goodwin",
                "B. Rink",
                "K. Roberts",
                "S. Harabagiu"
            ],
            "title": "UTDHLT: COPACETIC system for choosing plausible alternatives",
            "venue": "* SEM 2012: The First Joint Conference on Lexical and Computational Semantics\u2013Volume 1: Proceedings of the main conference and the shared task, and Volume",
            "year": 2012
        },
        {
            "authors": [
                "A. Gordon",
                "C. Bejan",
                "K. Sagae"
            ],
            "title": "Commonsense causal reasoning using millions of personal stories",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 25.",
            "year": 2011
        },
        {
            "authors": [
                "A. Gordon",
                "R. Swanson"
            ],
            "title": "Identifying personal stories in millions of weblog entries",
            "venue": "Third international conference on weblogs and social media, data challenge workshop, San Jose, CA, volume 46, 16\u201323.",
            "year": 2009
        },
        {
            "authors": [
                "A.S. Gordon",
                "L. Huangfu",
                "K. Sagae",
                "W. Mao",
                "W. Chen"
            ],
            "title": "Identifying Personal Narratives in Chinese Weblog Posts",
            "year": 2013
        },
        {
            "authors": [
                "M.A. Hearst"
            ],
            "title": "Text Tiling: Segmenting text into multiparagraph subtopic passages",
            "venue": "Computational linguistics, 23(1): 33\u201364.",
            "year": 1997
        },
        {
            "authors": [
                "C. Hutto",
                "E. Gilbert"
            ],
            "title": "Vader: A parsimonious rulebased model for sentiment analysis of social media text",
            "venue": "Proceedings of the International AAAI Conference on Web and Social Media, volume 8.",
            "year": 2014
        },
        {
            "authors": [
                "A. Jhala"
            ],
            "title": "Exploiting structure and conventions of movie scripts for information retrieval and text mining",
            "venue": "Joint International Conference on Interactive Digital Storytelling, 210\u2013213. Springer.",
            "year": 2008
        },
        {
            "authors": [
                "S.B. Johnson",
                "S. Bakken",
                "D. Dine",
                "S. Hyun",
                "E. Mendon\u00e7a",
                "F. Morrison",
                "T. Bright",
                "T. Van Vleck",
                "J. Wrenn",
                "P. Stetson"
            ],
            "title": "An electronic health record based on structured narrative",
            "venue": "Journal of the American Medical Informatics Association, 15(1): 54\u201364.",
            "year": 2008
        },
        {
            "authors": [
                "A. Jorge",
                "R. Campos",
                "A. Jatowt",
                "S. Nunes"
            ],
            "title": "First international workshop on narrative extraction from texts (Text2Story 2018)",
            "venue": "ECIR, 833\u2013834.",
            "year": 2018
        },
        {
            "authors": [
                "A.M. Jorge",
                "R. Campos",
                "A. Jatowt",
                "S. Bhatia"
            ],
            "title": "The 2 nd International Workshop on Narrative Extraction from Text: Text2Story 2019",
            "venue": "European Conference on Information Retrieval, 389\u2013393. Springer.",
            "year": 2019
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "T. Ko\u010disk\u1ef3",
                "J. Schwarz",
                "P. Blunsom",
                "C. Dyer",
                "K.M. Hermann",
                "G. Melis",
                "E. Grefenstette"
            ],
            "title": "The narrativeqa reading comprehension challenge",
            "venue": "Transactions of the Association for Computational Linguistics, 6: 317\u2013328.",
            "year": 2018
        },
        {
            "authors": [
                "B. Koopman",
                "L. Cripwell",
                "G. Zuccon"
            ],
            "title": "Generating clinical queries from patient narratives: a comparison between machines and humans",
            "venue": "Proceedings of the 40th international ACM SIGIR conference on Research and development in information retrieval, 853\u2013856.",
            "year": 2017
        },
        {
            "authors": [
                "W. Labov"
            ],
            "title": "Narrative pre-construction",
            "venue": "Narrative inquiry, 16(1): 37\u201345.",
            "year": 2006
        },
        {
            "authors": [
                "W. Labov",
                "J. Waletzky"
            ],
            "title": "Narrative analysis: Oral versions of personal experience",
            "year": 1997
        },
        {
            "authors": [
                "W.G. Lehnert"
            ],
            "title": "Plot units and narrative summarization",
            "venue": "Cognitive science, 5(4): 293\u2013331.",
            "year": 1981
        },
        {
            "authors": [
                "E. Levi",
                "G. Mor",
                "S. Shenhav",
                "T. Sheafer"
            ],
            "title": "CompRes: A Dataset for Narrative Structure in News",
            "venue": "arXiv preprint arXiv:2007.04874.",
            "year": 2020
        },
        {
            "authors": [
                "B. Li",
                "B. Cardier",
                "T. Wang",
                "F. Metze"
            ],
            "title": "Annotating high-level structures of short stories and personal anecdotes",
            "venue": "arXiv preprint arXiv:1710.06917.",
            "year": 2017
        },
        {
            "authors": [
                "B. Li",
                "S. Lee-Urban",
                "G. Johnston",
                "M. Riedl"
            ],
            "title": "Story generation with crowdsourced plot graphs",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 27.",
            "year": 2013
        },
        {
            "authors": [
                "G. Marchionini",
                "P. Liebscher",
                "X. Lin"
            ],
            "title": "Authoring hyperdocuments: Designing for interaction",
            "venue": "Interfaces for Information Retrieval and Online Systems. Greenwood Press, New York, NY, 119\u2013131.",
            "year": 1991
        },
        {
            "authors": [
                "A. McCabe",
                "M. Allyssa",
                "C. Peterson"
            ],
            "title": "Developing narrative structure",
            "venue": "Psychology Press.",
            "year": 1991
        },
        {
            "authors": [
                "S. Mohammad"
            ],
            "title": "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales",
            "venue": "arXiv preprint arXiv:1309.5909.",
            "year": 2013
        },
        {
            "authors": [
                "N. Mostafazadeh",
                "M. Roth",
                "A. Louis",
                "N. Chambers",
                "J. Allen"
            ],
            "title": "Lsdsem 2017 shared task: The story cloze test",
            "venue": "Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, 46\u201351.",
            "year": 2017
        },
        {
            "authors": [
                "M. Murray"
            ],
            "title": "Narrative psychology and narrative analysis",
            "year": 2003
        },
        {
            "authors": [
                "J. Ouyang",
                "K. McKeown"
            ],
            "title": "Towards Automatic Detection of Narrative Structure",
            "venue": "LREC, 4624\u20134631.",
            "year": 2014
        },
        {
            "authors": [
                "J. Ouyang",
                "K. McKeown"
            ],
            "title": "Modeling reportable events as turning points in narrative",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2149\u20132158.",
            "year": 2015
        },
        {
            "authors": [
                "A. Palmer"
            ],
            "title": "The construction of fictional minds",
            "venue": "Narrative, 10(1): 28\u201346.",
            "year": 2002
        },
        {
            "authors": [
                "P. Papalampidi",
                "F. Keller",
                "L. Frermann",
                "M. Lapata"
            ],
            "title": "Screenplay Summarization Using Latent Narrative Structure",
            "venue": "arXiv preprint arXiv:2004.12727.",
            "year": 2020
        },
        {
            "authors": [
                "P. Papalampidi",
                "F. Keller",
                "M. Lapata"
            ],
            "title": "Movie Plot Analysis via Turning Point Identification",
            "venue": "arXiv preprint arXiv:1908.10328.",
            "year": 2019
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "In Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 1532\u20131543.",
            "year": 2014
        },
        {
            "authors": [
                "D.E. Polkinghorne"
            ],
            "title": "Narrative knowing and the human sciences",
            "venue": "Suny Press.",
            "year": 1988
        },
        {
            "authors": [
                "G. Prince"
            ],
            "title": "A grammar of stories: An introduction, volume 13",
            "venue": "Walter de Gruyter.",
            "year": 2012
        },
        {
            "authors": [
                "E. Rahimtoroghi",
                "T. Corcoran",
                "R. Swanson",
                "M.A. Walker",
                "K. Sagae",
                "A.S. Gordon"
            ],
            "title": "Minimal narrative annotation schemes and their applications",
            "venue": "7th Workshop on Intelligent Narrative Technologies.",
            "year": 2014
        },
        {
            "authors": [
                "A.S. Rao",
                "M. P Georgeff"
            ],
            "title": "BDI agents: from theory to practice",
            "venue": "In Icmas,",
            "year": 1995
        },
        {
            "authors": [
                "H. Rashkin",
                "A. Bosselut",
                "M. Sap",
                "K. Knight",
                "Y. Choi"
            ],
            "title": "Modeling naive psychology of characters in simple commonsense stories",
            "venue": "arXiv preprint arXiv:1805.06533.",
            "year": 2018
        },
        {
            "authors": [
                "M.O. Riedl",
                "R.M. Young"
            ],
            "title": "Narrative planning: Balancing plot and character",
            "venue": "Journal of Artificial Intelligence Research, 39: 217\u2013268.",
            "year": 2010
        },
        {
            "authors": [
                "L. Rokach",
                "O. Maimon",
                "M. Averbuch"
            ],
            "title": "Information retrieval system for medical narrative reports",
            "venue": "International Conference on Flexible Query Answering Systems, 217\u2013228. Springer.",
            "year": 2004
        },
        {
            "authors": [
                "Ryan",
                "M.-L."
            ],
            "title": "Embedded narratives and tellability",
            "venue": "Style, 319\u2013340.",
            "year": 1986
        },
        {
            "authors": [
                "M. Sap",
                "R. Le Bras",
                "E. Allaway",
                "C. Bhagavatula",
                "N. Lourie",
                "H. Rashkin",
                "B. Roof",
                "N.A. Smith",
                "Y. Choi"
            ],
            "title": "Atomic: An atlas of machine commonsense for if-then reasoning",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, 3027\u20133035.",
            "year": 2019
        },
        {
            "authors": [
                "S.B. Schafer"
            ],
            "title": "Exploring the Collective Unconscious in the Age of Digital Media",
            "venue": "IGI Global.",
            "year": 2016
        },
        {
            "authors": [
                "R. Swanson",
                "E. Rahimtoroghi",
                "T. Corcoran",
                "M. Walker"
            ],
            "title": "Identifying narrative clause types in personal stories",
            "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 171\u2013180.",
            "year": 2014
        },
        {
            "authors": [
                "A. Urooj",
                "A. Mazaheri",
                "M Shah"
            ],
            "title": "MMFTBERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings,",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 5998\u20136008.",
            "year": 2017
        },
        {
            "authors": [
                "P. Vijayaraghavan",
                "D. Roy"
            ],
            "title": "Modeling human motives and emotions from personal narratives using external knowledge and entity tracking",
            "venue": "Proceedings of the Web Conference 2021, 529\u2013540.",
            "year": 2021
        },
        {
            "authors": [
                "D. Wilmot",
                "F. Keller"
            ],
            "title": "Suspense in short stories is predicted by uncertainty reduction over neural story representation",
            "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics, 1763\u20131788.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Yang",
                "D. Yang",
                "C. Dyer",
                "X. He",
                "A. Smola",
                "E. Hovy"
            ],
            "title": "Hierarchical attention networks for document classification",
            "venue": "Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies, 1480\u2013",
            "year": 2016
        },
        {
            "authors": [
                "X. Yuan",
                "T. Wang",
                "C. Gulcehre",
                "A. Sordoni",
                "P. Bachman",
                "S. Subramanian",
                "S. Zhang",
                "A. Trischler"
            ],
            "title": "Machine comprehension by text-to-text neural question generation",
            "venue": "arXiv preprint arXiv:1705.02012",
            "year": 2017
        },
        {
            "authors": [
                "berg",
                "Finlayson"
            ],
            "title": "A work by Piper (Piper 2018) specifically used linguistic aspect of text to measure fictionality, i.e., distinguish works of fiction from non-fiction. In our work, we use a pretrained BERT model for our classification",
            "year": 2018
        },
        {
            "authors": [
                "Edunov"
            ],
            "title": "2018) based on pretrained English\u2194German",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "Narratives are the fundamental means by which people organize, understand, and explain their experiences in the world around them. Researchers in the field of psychology maintain that the default mode of human cognition is a narrative mode (Beck 2015). Humans share their personal experiences by picking specific events or facts and weaving them together to make meaning. These are referred to as personal narratives, a form of autobiographical storytelling that gives shape to experiences. Polkinghorne (1988) suggested that personal narratives, like other stories, follow broad characteristics involving: (a) typically a beginning, middle, and end, (b) specific plots with different characters and settings, or events. Often, characters learn something or change as a result of the situation or a conflict and resolution, but\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nnot always. Some of these characteristics provide the basis for the organizational framework of a story, commonly referred to as the narrative structure or the storyline. The growing amount of personal narrative text information in the form of social media posts, comments, life stories, or blog posts presents new challenges in keeping track of the storyline or events that form the defining moments of the narrative. Several recent works (Dore et al. 2018; Yuan et al. 2017; Chung, Lee, and Glass 2017; Koc\u030cisky\u0300 et al. 2018; Mostafazadeh et al. 2017) have made efforts to advance the research in narrative comprehension. However, the development of computational models that automatically detect and interpret different structural elements of a narrative remains an open problem. Discovery of structural elements of a narrative has many applications in: (a) retrieval of narratives based on similar dramatic events or concepts instead of keywords (McCabe, Allyssa, and Peterson 1991; Finlayson and Winston 2006; Marchionini, Liebscher, and Lin 1991), (b) linking related stories that form a narrative thread towards theme generation (Berman and Slobin 2013), (c) summarization of stories (Lehnert 1981; Papalampidi et al. 2020) and (d) story ending prediction or generation (Chen, Chen, and Yu 2019; Li et al. 2013; Mostafazadeh et al. 2017), (e) commonsense reasoning (Goodwin et al. 2012; Gordon, Bejan, and Sagae 2011), to list a few.\nSeveral narrative theories have been proposed such as Freytag (Freytag 1894), Prince (Prince 2012), Bruner (Bruner 1991, 2009), Labov & Waletzky (Labov and Waletzky 1997), to name a few. These theories explain different elements of a narrative structure containing typical orderings between them. Certain elements of the narrative structure are correlated across different narrative theories. For example, Bruner\u2019s \u2018breach in canonicity\u2019 (Bruner 1991) could correspond to (a) Freytag\u2019s \u2018climax\u2019 \u2013 referring to the\nar X\niv :2\n30 2.\n09 41\n8v 1\n[ cs\n.C L\n] 1\n8 Fe\nb 20\n23\n\u2018turning point\u2019 of the fortunes of the protagonist (Abrams and Harpham 2014) or (b) Labov\u2019s \u2018most reportable event\u2019 (MRE) \u2013 describing the event that has the greatest effect upon the goals, motivations and emotions of the characters (participants) in the narrative (Labov and Waletzky 1997; Labov 2006). Shorter narratives tend to consist mostly of complicating actions that culminate in the MRE or climax and instances of events that reach a \u2018resolution\u2019 stage indicated by a swift drop in dramatic tension, while the other structural elements are more likely to occur in longer narratives. Figure 1 (Left) shows Freytag\u2019s pyramid containing the key elements of the narrative structure and Figure 1 (right) contains highlights of climax and resolution for a sample personal narrative. Thus, our work aims to leverage computational approaches at the intersection of information retrieval, NLP, and aspects of psychology, and model the key elements of narrative structure \u2013 MRE and resolution. As an operating definition, we consider an MRE to be contained in a sentence(s) based on the following criteria \u2013 it is an explicit event that can be reported as the summary of the story and occurs at the highest tension point of the story. Similarly, an event qualifies as \u2018resolution\u2019 if it usually occurs after the MRE and resolves the dramatic tension in the narrative.\nRecently, Papalampidi et al. (Papalampidi, Keller, and Lapata 2019) introduced a dataset consisting of movie screenplays and plot synopsis annotated with turning points. Few attempts have been made at annotating elements of highlevel narrative structures (Li et al. 2017) and automatically extracting them from free text. Ouyang et al. (Ouyang and McKeown 2015)\u2019s study on predicting MRE in narratives is the closest work to the problem considered in this paper. While most of these methods rely on syntactic, semantic, surface-level affect, or narrative features obtained using hand-engineering or pre-trained semantic embedding methods to model narrative structure, we investigate the role of a protagonist\u2019s psychological states in capturing the pivotal events in the narrative and their relative importance in identifying the elements of narrative structure \u2013 Climax and Resolution. We find a basis for this study in prior theoretical frameworks (Murray 2003; Ryan 1986; Ouyang and McKeown 2014; Lehnert 1981; Schafer 2016) that emphasize (a) how narrative structure organizes the use of psychological concepts (e.g. intentions, desires and emotions) and mediates all the human interactions and their social behavior, and (b) how protagonist\u2019s mental states (both implicit and explicit inferences, also imputed by readers) and psychological trajectory correlate with the classic dramatic arc of stories. Thus, to obtain the protagonist\u2019s mental states, we refer to a recent work (Vijayaraghavan and Roy 2021; Sap et al. 2019; Rashkin et al. 2018) that learns to embed characters\u2019 mental states using an external memory module. Our contributions are summarized below: \u2022 A STORIES1 corpus containing a collection of Red-\ndit Personal Narratives with fine-grained annotations of prominent structural elements of a narrative \u2013 climax and resolution.\n\u2022 An end-to-end neural network for modeling narrative 1Short for STructures Of Reddit PEsonal Stories\nstructure, referred to as M-SENSE2, that allows for integration of protagonist\u2019s mental state representations with linguistic information via multi-feature fusion.\n\u2022 Experiments that analyze the impact of our modeling choices for short personal narratives. Specifically, we gauge the influence of incorporating mental state embeddings and report an improvement in F1 scores of \u223c 11% and \u223c 13% over the base model for predicting climax and resolution respectively."
        },
        {
            "heading": "Related Work",
            "text": "There is a large body of prior work that focuses on different aspects of narrative comprehension. Computational analysis of narratives operates at the level of characters and plot events. Examples include plot-related studies \u2013 story plot generation, plot summarization, detecting complex plot units, modeling event schemas and narrative chains and movie question-answering; character-based studies \u2013 inferring character personas or archetypes, analyzing interpersonal relationships and emotion trajectories, identifying enemies, allies, heroes; story-level analysis \u2013 story representation, predicting story endings, modeling story suspense, and creative or artistic storytelling, to list a few.\nSeveral studies have analyzed the literature in narratology and formulated different goals and annotation labels associated with narratives for modeling their structure. Elson\u2019s (Elson 2012) Story Intention Graph (SIG) provided an annotation schema to capture timelines as well as beliefs, intentions, and plans of story characters. The annotations in this approach are similar to story generation methods described in Belief-Desire-Intention agents (Rao, Georgeff et al. 1995) and intention-based story planning (Riedl and Young 2010). Previous studies like (Gordon, Bejan, and Sagae 2011) have analyzed personal weblog stories containing everyday situations. Rahimtoroghi et al. (Rahimtoroghi et al. 2014) and Swanson et al. (Swanson et al. 2014) used a subset of Labov\u2019s categories, including orientation, action, and evaluation in such personal weblog narratives. Black and Wilensky (1979) evaluate the functionality of story grammar in story understanding, As explained earlier, (Papalampidi, Keller, and Lapata 2019)\u2019s dataset for analyzing turning points is a valuable addition in this area of work. Moreover, there have been consistent efforts (Jorge et al. 2018, 2019) that study the link between Information Retrieval (IR) and narrative representations from a given text. These include works that exploit narrative structure in movies for IR (Jhala 2008), detect and retrieve narratives in health domain (patient communities & medical reports) (Johnson et al. 2008; Rokach, Maimon, and Averbuch 2004; Dirkson, Verberne, and Kraaij 2019; Koopman, Cripwell, and Zuccon 2017), identify narrative structures in news stories (Boyd, Blackburn, and Pennebaker 2020; Levi et al. 2020) or generate summaries from screenplays or novels (Papalampidi et al. 2020), to name a few. Given this wide spectrum of work, we leverage mental state representation models that are pre-trained using social commonsense knowledge aggregated using IR and text mining tech-\n2Short for Mental State Enriched Narrative Structure modEl\nniques. We employ the ensuing mental state embeddings in tandem with contextual semantic embeddings towards our primary objective of identifying elements of high-level narrative structure \u2013 climax and resolution. We also conduct a detailed analysis of the outcome and the contribution of the protagonist\u2019s psychological state trajectory to our task."
        },
        {
            "heading": "Dataset Collection",
            "text": "Figure 2 presents our data collection pipeline. First, we collect Reddit posts from two communities: /r/offmychest and /r/confession using the PushShift API 3. Next, we filter the collected data to retain only those posts that do not contain tags like \u201c[Deleted]\u201d, \u201cNSFW\u201d 4 or \u201cover 18\u201d. Finally, we further narrow down the aggregated posts using a BERTbased story classifier. The pipeline is described in the Appendix."
        },
        {
            "heading": "Annotation",
            "text": "Here, we explain the annotation process involved in the construction of our STORIES dataset. Table 1 shows the descriptive statistics of our dataset.\nSetup We created a user interface for MTurk workers to make the annotation procedure convenient for capturing key elements of the narrative structure \u2013 climax and resolution. The user interface allows the workers to highlight parts of the text that qualify as climax and resolution using red and green colors respectively. Three annotators were mainly involved in the annotation process. Each worker is presented a sampled text from the Reddit personal narrative corpus. Additionally, the workers are provided with an option of selecting checkboxes: \u201cNo Climax\u201d or \u201cNo Resolution\u201d. This caters to those personal stories that don\u2019t contain a climax or resolution.\nAgreements Once the data is collected using our annotation setup, we measure the inter-annotator agreement (IAA) at the sentence-level. For sentence-level agreement, we use the following metrics: (i) Fleiss\u2019s kappa (\u03ba) (Fleiss and Cohen 1973), (ii) mean annotation distance (D), i.e., the distance between two annotations for each category, normalized by story length (Papalampidi, Keller, and Lapata 2019).\n3https://pushshift.io/ 4NSFW \u2013 not safe for work\nAnalysis We study the appearance of climax and resolution sentences by estimating their mean position normalized by the story length. We present the distribution of the position of both the structural elements in Figure 3. While the average position for climax (0.61) coincides with the peak, we observe that the resolution contents occur later in the story. Table 2 shows the sentence-level IAA measures for each narrative element. We observe that substantial agreement is achieved for both the climax and resolution. Clearly, we obtain higher agreement values for resolution than the climax. Figure 4a displays sample annotations (e.g. multisentence or non-contiguous highlights; no resolution) from our STORIES dataset."
        },
        {
            "heading": "M-SENSE: Modeling Narrative Structure",
            "text": "In this paper, we explore different modeling and analysis methods for understanding narratives and automatically extracting text segments that act as key elements of narrative structure, particularly climax and resolution. The models are provided a narrative text T with L sentences, T = [S1, S2, ..., SL], as input. Here, each sentence Si contains Ni words {wi1, wi2, .., wiNi} from vocabulary V . Towards automatic detection of structural elements, we formulate it as a sentence labeling task where the goal is to predict a label y\u0302i \u2208 {None,Climax,Resolution} for each sentence Si, based on the story context. Beyond linguistic features extracted from narratives, we focus on a dominant aspect in which a narrative is formed or presented, that is an account of characters\u2019 mental states \u2013 motives and emotions. Thus, we leverage transfer learning from pretrained models trained to infer characters\u2019 mental states from a narrative. We implement a multi-feature fusion based learning model, M-SENSE, that potentially encapsulates syntactic, semantic, characters\u2019 mental state features towards our overall goal of predicting climax and resolution in short personal narratives. Our M-SENSE model consists of the following components: Ensemble Sentence Encoders, which computes persentence linguistic & mental state embeddings.\nFusion layer, which integrates the protagonist\u2019s mental state information with the extracted linguistic features. Story Encoder, which maps the fused encodings into a sequence of bidirectionally contextualized embeddings. Interaction layer, which estimates state transition across sequential context windows to identify the boundaries. Classification layer, which involves linear layers to eventually calculate the label probabilities."
        },
        {
            "heading": "Ensemble Sentence Encoders",
            "text": "In this work, we aim to exploit both linguistic and mental state features for an enhanced model for narratives.\nExtracting Linguistic Representations Pretrained general purpose sentence encoders usually capture a hierarchy of linguistic information such as low-level surface features, syntactic features and high-level semantic features. Given a narrative text with L sentences T = [S1, S2, ..., SL], this component outputs hidden representations for sentences Hsents = [h\n1, h2, ..., hL] using different encoding methods.In our MSENSE model, we use a token-level BERTbased sentence encoder (more in the Appendix). Here, each sentence is prepended with a special [CLS] token at the beginning of each sentence and appended with a [SEP ] token at the end of each sentence in the narrative. We apply both position and segment embeddings and feed to the pre-trained BERT model as:\nH = [h1[CLS], .., h 1 N1 , h1[SEP ], .., h i [CLS], .., h i Ni , .., hL[SEP ]]\n= BERT(T ) (1)\nThe hidden representation of the ith [CLS] token from the top BERT layer is extracted as the semantic embedding of the ith sentence. However, we drop the subscript [CLS] from hi[CLS] and denote the output semantic embeddings as: HxSemsents = [h 1, h2, ..., hL]."
        },
        {
            "heading": "Incorporating Protagonist\u2019s Mental Representation",
            "text": "Prior studies have established how the progression of a story is as much a reflection of a sequence of a protagonist\u2019s motivation and emotional states as it is the workings of an abstract grammar (Palmer 2002; Mohammad 2013; Alm and Sproat 2005). We follow a recent work (Vijayaraghavan and Roy 2021) that implements a NEMO model, a variant of a Transformer-based encoder-decoder architecture to embed and explain characters\u2019 (or entities\u2019) mental states. We extract the embeddings of intents and emotional reactions of the protagonist for a given sentence in the narrative conditioning on the prior story context. Figure 4b contains the overview of the NEMO architecture. The computation of mental state embeddings are facilitated by a knowledge enrichment module that consolidates commonsense knowledge about social interactions and an external memory module that tracks entities\u2019 mental states. Using prior context (S<i), entity (ej) and mental state attribute information (m \u2208 {xIntent, xReact} representing intent and emotional reaction respectively), we use the encoder, STORYENTENC(\u00b7), in this trained model to obtain entityaware mental state representation of the current sentence\nSi. The encoding process in the NEMO model is given by:\n(H\u0302ixIntent, H\u0303 i xReact) = STORYENTENC(Si, S<i, ej ,m);\n\u2200m \u2208 {xIntent, xReact} (2)\nwhere ej \u2208 E is the entity, (H\u0302ixIntent, H\u0303ixReact) is the resulting entity-aware intent and emotion representation of the ith-sentence given the story context. In this work, we use the narrator (\u201cI\u201d or \u201cself\u201d in the personal narratives) as the protagonist. We only utilize the hidden representations of the [CLS] token from both (H\u0302ixIntent, H\u0303 i xReact) for subsequent processing steps. We denote these intent and emotion representation as: Hxintentsents = [h\u0302\n1, .., h\u0302L] and HxReactsents = [h\u03031, .., h\u0303L] respectively."
        },
        {
            "heading": "Transformer-based Fusion Layer",
            "text": "Given multiple sentence-level embeddings, we apply a fusion strategy to derive a unified sentence embedding for our classification task. Let hik;\u2200k \u2208 {1, ...,K} denote different per-sentence latent vectors. In our case, K = 3 and hi1 = hi;hi2 = h\u0302i;hi3 = h\u0303i are embeddings related to semantics (xSem), intents (xIntent) and reactions (xReact) of the ith sentence respectively. Drawing ideas from the literature of multimodal analysis (Urooj et al. 2020), we treat the multiple latent vectors as a sequence of features by first concatenating them together. We introduce a special token [FUSE] 5 that accumulates the latent features from different sentence encodings. The final hidden representation of [FUSE] token obtained after feeding them to a Transformer layer is the fused output sentence representation: hifuse = TF(\u2016Kk=0 hik), where TF refers to the transformer encoder layer and hi0 (i.e. when k = 0) is set to the trainable [FUSE] vector."
        },
        {
            "heading": "Story Encoder",
            "text": "We apply Transformer layers on the top of the sentence representations to extract narrative-level features. We refer it as Inter-sentence Transformer. Intuitively, the transformer layer focuses on possibly different sentences in the narrative, and produces context-aware sentence embeddings. This is given as:\nH\u0302 l = LayerNorm(C\u0302l\u22121 + MHA(C\u0302l\u22121) C\u0302l = LayerNorm(H\u0302 l + FFL(H\u0302 l)) Csents = [c 1, c2, ..., cL] = C\u0302nL (3)\nwhere C\u03020 = PE(Hsents), PE refers to the positional encoding, LayerNorm refers to layer normalization operation, MHA is the multi-head attention operation and FFL is a feed-forward layer (Vaswani et al. 2017). The superscript l indicates the depth of the stacked Transformer layers. The output from the topmost layer, l = nL, is our contextualized sentence embeddings Csents."
        },
        {
            "heading": "Interaction layer",
            "text": "In this layer, we compute the transition of state across sentences by measuring similarity metrics in the embedding\n5[FUSE] is similar to the commonly used [CLS] token.\nspace between sequential context windows and concatenating them with contextualized embeddings. By choosing windows of size s, we compute the left (cileft) and right (c i right) context information for the ith sentence by computing the mean sentence embedding within that window. Finally, we get the interaction-feature enhanced context-aware embeddings: Esents = [e1, e2, ..., eL]."
        },
        {
            "heading": "Classification layer",
            "text": "The resulting embeddings are mapped to a C-dimensional output using a softmax-based classification layer. Here,C = 3 is the number of labels. This step is given as: y\u0302i = softmax(fs(e i))."
        },
        {
            "heading": "Experiments",
            "text": "We conduct experiments to study the following research questions:\nRQ1: How does our model compare with other baselines for identifying climax and resolution in short narratives?\nRQ2: How do various model components contribute to the overall performance? To what extent do mental state representations play a role for our classification task?"
        },
        {
            "heading": "Overall Predictive Performance (RQ1)",
            "text": "Baselines We compare our model with a set of carefully selected zero-shot (see the Appendix section) & supervised baselines, shown as follows.\nRandom baseline, which assigns labels (Climax, Resolution or None) to sentences randomly.\nDistribution baseline, which picks sentences that lie on the peaks of the empirical distributions for climax and resolution in our training set as explained earlier.\nHeuristic baseline, which labels the sentences as climax or resolution based on heuristics. While we use the sentence\nthat is the closest semantic neighbour of the post title as climax, the last sentence in the narrative is labeled as the resolution (as explained in the Appendix section.). A recent work (Wilmot and Keller 2020) has explored surprise a measure of suspense in narratives. (Ely, Frankel, and Kamenica 2015)\u2019s surprise is defined as the amount of change from previous sentence to the current sentence in the narrative (see the Appendix). We encode the sentences in the story using the following approaches and eventually compute suspense measures for our classification task. GloVeSim (Pennington, Socher, and Manning 2014), BERT (Devlin et al. 2018), USE (Cer et al. 2018), which computes semantic embeddings using average word vectors (using GloVe) or Transformer-based models. STORYENC (Papalampidi, Keller, and Lapata 2019), which uses the hierarchical RNN based language model to encode sentences in the story. STORYENTENC (Vijayaraghavan and Roy 2021), which encodes the sentences in the story from the protagonist\u2019s perspective. Here, we denote intent and emotion embeddings as (Eint = H xIntent sents ) and (Eemo = H xReact sents ) respectively. CAM and TAM (Papalampidi, Keller, and Lapata 2019) consist of bidirectional LSTM model with the latter model having an additional interaction layer to compute boundaries between the topics in each story. M-SENSE\u2013FUSION, which is a variant of our M-SENSE model without mental state embeddings. M-SENSE, which is our complete model incorporating protagonist\u2019s mental representation.\nResults Table 3 outlines the results of our evaluation. We report the performance of simple baselines, of which the distribution baseline turns out to be the strongest. The heuristic baseline performs slightly better than the random baseline. This suggests that the Reddit post title contains relevant signal to predict the climax while the last sentence heuristic for resolution is only as good as a random classifier.\nModels F1 \u2191 D \u2193 C R C R\nRandom 0.196 0.143 29.05 30.57 Distribution 0.274 0.315 15.79 14.42 Heuristic 0.217 0.147 23.74 26.82\nGloVeSim 0.312 0.344 12.06 11.65 BERTtok 0.408 0.441 9.37 8.09 BERTsent 0.352 0.366 10.88 9.73 USEsent 0.379 0.391 10.42 9.58 STORYENC 0.410 0.438 8.81 7.46 Eint 0.437 0.462 8.19 6.94 Eemo 0.429 0.475 8.43 6.67\nTAM 0.565\u00b10.022 0.609\u00b10.008 5.90\u00b13.18 5.02\u00b12.82 CAM 0.578\u00b10.019 0.604\u00b10.0032 6.58 \u00b14.02 5.44\u00b13.05 M-SENSE 0.694*\u00b10.0027 0.743*\u00b10.0015 4.15\u00b11.84 3.20\u00b11.06\nTable 3: Evaluation Results of different models to detect Climax and Resolution. We report F1 score per class & percent (D) for these models. We use \u2191, \u2193 to indicate if higher/ lower values mean better performance respectively. * refers to significance (p < 0.05) over TAM using a paired T-Test.\nApplying suspense-based approaches with different sentence embedding methods yields relative improvement over the simple baselines in terms of both the evaluation metrics. As expected, sentence-level BERT/USE performs worse than its token-level counterpart. We attribute this variation in performance to the lack of any story context information for computing latent embedding, thereby affecting the assessment of state changes in the narrative. However, sentencelevel USE\u2019s ability to produce better similarity estimates gives it a slight advantage over sentence-level BERT. Notably, sentence representations obtained from models trained on stories (STORYENC, STORYENTENC) recorded comparable to improved results over other sentence embedding methods. Strikingly, computing surprise using protagonist mental state embeddings exhibit an overall enhanced classification capability. We find that the intent embedding (Eint) helps achieve the best zero-shot performance for detecting climax. A competitive outcome for resolution is obtained using protagonist\u2019s emotion representation (Eemo). We compare our complete M-SENSE model with the best performing prior models such as CAM, TAM (Papalampidi, Keller, and Lapata 2019) applied for similar tasks. As we can see, supervised fine-tuning approaches easily beat the earlier results obtained using zero-shot methods. Finally, our M-SENSE model achieves an absolute improvement of \u223c 20.07% and \u223c 22% for climax and resolution prediction respectively."
        },
        {
            "heading": "Ablation Study (RQ2)",
            "text": "To evaluate the contributions of each component in our M-SENSE model, we conduct an ablation study using the validation set. For this study, we compare our best performing M-SENSE model with alternative modeling choices for each of the components. Table 4 shows the results of our study. We modify one component at a time and report their performance using F1 metric. This involves either replacing a component (denoted by \u201cw/\u201d) or removing a compo-\nModel Variants F1 \u2191 C R\nM-SENSE 0.688 0.738\nSentence Encoder Variants w/ Sentence-level BERT 0.665 0.709 w/ Sentence-level USE 0.677 0.726\nStory Encoder Variant w/o Story Encoder 0.620 0.653 w/ Inter-Sentence RNN 0.659 0.705\nInteraction Layer Variant w/o Interaction Layer 0.654 0.716\nFusion Layer Variants \u2013w/o Fusion Layer 0.614 0.640 \u2013w/o Eint 0.638 0.703 \u2013w/o Eemo 0.652 0.687\nTable 4: We report F1 score per class with non-default modeling choices for each component of our model.\nnent (denoted by \u201cw/o\u201d to refer without the component). For eg. \u201cw/ Sentence-level BERT\u201d refers to replacing token-level BERT in our M-SENSE model with sentence-level BERT as our sentence encoder; \u201cw/o Eemo\u201d indicates the removal of protagonist\u2019s emotion state embedding from the fusion layer.\nInfluence of Mental State Embeddings: In this study, we examine the necessity of a fusion layer and probe the influence of protagonist\u2019s mental state embeddings towards our classification task. Notably, the results in Table 4 validate the benefits of introducing the fusion layer and demonstrate the relative performance gains obtained with intent and emotion embeddings. In the absence of a fusion layer, we observe that the performance drop is \u223c 11% and \u223c 13% for predicting climax and resolution respectively. The loss of the protagonist\u2019s intent information impacts the climax prediction more. This is analogous to the effect emotion information has on resolution prediction."
        },
        {
            "heading": "Analysis and Discussion",
            "text": "Effect of Story Length: Here, we compare the performance of different sentence encoders with and without fusion layer for detecting climax in narratives with varying length. Figure 5 shows the results of this analysis. We observe that the tokenlevel BERT outperforms sentence-level BERT and USE encoders for narratives containing up to 13\u221214 sentences, but the performance gradually degrades beyond 14 sentences. Sentence-level USE encoder produces stable and relatively better outcomes for longer narratives (story length > 14). With the introduction of mental state representation through fusion layer, the F1 score improved significantly irrespective of the sentence encoder used.\nError Analysis: In order to estimate why our model augmented with mental state representation performs better, we conduct error analysis between our full M-SENSE model and the model without mental representation fusion (M-SENSE\u2212 Fusion). For those narratives where the latter model fails to predict correctly, we gauge the patterns emerging out of the\nfollowing analysis: (a) Using VADER6 (Hutto and Gilbert 2014) a normalized, weighted composite sentiment score is computed for each sentence in the narrative, and (b) Using state classification (Rashkin et al. 2018; Vijayaraghavan and Roy 2021), we assess Maslow\u2019s motivation or intent categories associated with sentences predicted as climax or resolution in the narrative and analyze for any pattern related to ground truth climax/resolution sentences. For predicting resolution, the M-SENSE \u2212 Fusion makes \u223c 28% more mistakes than M-SENSE model for narratives with homogeneous endings (i.e. narratives having same sentiment sentences in the neighbourhood of resolution closer to the end of the story). M-SENSE \u2212 Fusion model is unable to discern clearly and predicts a different sentence as resolution. Based on our analysis (b), there is a clear pattern that M-SENSE gains significantly over the M-SENSE \u2212 Fusion when the ground-truth climax sentences belong to \u201cEsteem\u201d and \u201cLove/Belonging\u201d categories. Our attention analysis results are shown in the Appendix section."
        },
        {
            "heading": "Task: Modeling Movie Turning Points",
            "text": "Given that our work is primarily focused on modeling narrative structure in personal narratives, we analyze how we can apply such a model towards identifying climax and resolution in movie plot synopsis. (Papalampidi, Keller, and Lapata 2019) introduced a TRIPOD dataset containing a corpus of movie synopses annotated with turning points (TPs). By testing our model on this dataset, we evaluate our model\u2019s performance on an out-of-domain dataset. The dataset identified five major turning points in the movie synopses and screenplay, referring to them as critical events that prevent the narrative from drifting away. By their definitions for each of these categories (Papalampidi, Keller, and Lapata 2019), TP4 and TP5 align clearly with our usage of climax and resolution from prior narrative theories. Due to this alignment, it is relevant to use our model to predict these two categories in the TRIPOD dataset. However, we focus on the movie plot synopses in this work and use the cast information collected from IMDb as a part of this dataset.\nWe first apply our M-SENSE trained on our STORIES corpus directly and evaluate its zero-shot performance (referred as ZS). We assume the protagonist in the movie to be the top character from the IMDb cast information. Though this may not always be true, it measures how our model fares\n6https://github.com/cjhutto/vaderSentiment\non this dataset for predicting TP4 and TP5. Further, we use sentence-level USE-based sentence encoder as some of the wiki plot synopses are longer than what can be accommodated by our token-level BERT model. Additionally, we also fine-tune our model with the training set of the TRIPOD dataset. This is denoted by M-SENSE(FT )."
        },
        {
            "heading": "Results",
            "text": "We display our model\u2019s performance compared to the best performing TAM reported in the original work (Papalampidi, Keller, and Lapata 2019). TAM with TP views implemented separate encoders for each of the categories and computed different representations for the same sentences acting as different views related to each TP. Similarly, TAM+Entities enriched the model with entity information by applying coreference resolution and obtain entity-specific representations. Table 5 shows our model\u2019s results compared to the prior proposed approaches for modeling turning points in plot synopses. We find that our model in zero-shot settings outperforms a supervised TAM+TP views model, though it falls slightly behind the best supervised model.Also, we restrict our model for predicting only two of the five major turning point labels. Finally, our fine-tuned model outperforms the best performing model, significantly reducing the mean annotation distance by an average of \u223c 20% on both the turning point labels. Thus, we are able to achieve remarkable improvement on an out-of-domain dataset even with assumptions on protagonist information. Therefore, we demonstrate that our MSENSE model can predict climax and resolution in stories beyond just personal narratives, albeit limited by story length at this point."
        },
        {
            "heading": "Conclusion",
            "text": "Towards modeling high-level narrative structure, we construct a dataset of personal narratives from Reddit containing annotations of climax and resolution sentences. Next, we introduce a deep neural model, referred to as M-SENSE, that learns to effectively integrate protagonist\u2019s psychological state features with linguistic information towards improved modeling of narrative structure. We experimentally confirm that our model outperforms several zero-shot and supervised baselines and benefits significantly from incorporating protagonist\u2019s mental state embeddings. Our model is able to achieve \u223c 20% higher success in prediction task than the previous methods.We believe that our work will advance the research in understanding the larger dynamics of narrative communication and aid future efforts towards developing AI tools that can interact with users though stories."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Dataset Collection",
            "text": "The first stage in this pipeline is dedicated to ingesting posts from Reddit. To collect natural first-person stories, we rely on Reddit communities comprising user-generated textual accounts of happy events, long-standing baggage, recent trauma, life experiences, adventurous encounters or guilt, and redemption episodes. To this end, we aggregate posts from two communities: /r/offmychest and /r/confession using the PushShift API 7. The Pushshift API provides access to a database of all Reddit posts made since Reddit\u2019s launch as a social platform. We obtain \u223c 440, 000 posts from this step.\nNext, we filter the collected data to retain only those posts that do not contain tags like \u201c[Deleted]\u201d, \u201cNSFW\u201d 8 or \u201cover 18\u201d. Relying on Prince\u2019s definition (Prince 2012) of a minimal story to comprise a starting state, a statechanging event, and an ending state, we eliminate posts containing less than three sentences. The subsequent stages in the pipeline are explained in the sections below."
        },
        {
            "heading": "Story Classifier",
            "text": "The aggregated data consists of a wide variety of contents some of which do not qualify as personal narratives. In order to separate such non-narrative content from the collected data, we develop a story classifier that takes textual content as input and predicts the likelihood of the input text being a story.\nStory vs. Non-Story Dataset We gather a diverse collection of first-person blog text drawn randomly from the Spinn3r Blog Dataset containing everyday situations (Gordon and Swanson 2009). Consistent with our filtering approach for Reddit posts, we follow a similar length criterion and sample \u223c 1, 500 blog posts. Further, we randomly selected \u223c 1, 500 texts from our Reddit posts corpus. Together, we obtain a total of \u223c 3, 000 posts to be annotated by MTurk workers.\nFor each post, annotators were instructed to read the textual content and choose one among the three labels: Story, Non-Story or Unsure (Gordon et al. 2013). We define these categories as follows \u2013 (a) Story: Non-fictional narratives that people share with each other about their own life experiences. They contain a sequence of causally or temporally related events with the narrator being a participant; (b) Non-Story: Texts that are not primarily personal stories or don\u2019t give account of past events. They may or may not contain texts from first-person point of view but include opinion pieces, excerpts from news articles, recipes, technical explanations, facts, questions or some random discussion, personal advice, to list a few. When the annotators are uncertain about the right label, they are allowed to select the \u201cUnsure\u201d option. The three workers reached unanimous agreement on 76% of the cases. We use the majority vote when such an agreement is not reached. Of the 3,000 posts, 1,197 posts\n7https://pushshift.io/ 8NSFW \u2013 not safe for work\nwere tagged as \u201cStory\u201d, 1,173 as \u201cNon-Story\u201d and remaining as \u201cUnsure\u201d.\nModel We introduce a story classifier that separates nonfictional narratives from non-narrative textual content. Prior work have used feature engineering to extract features like Tf-Idf, Semantic Triplets, VerbNet & coreference resolution chain based character features (Ceran et al. 2012; Eisenberg and Finlayson 2017) for this task. A work by Piper (Piper 2018) specifically used linguistic aspect of text to measure fictionality, i.e., distinguish works of fiction from non-fiction. In our work, we use a pretrained BERT model for our classification task. Given an input text, the goal is to predict if the text qualifies as a story or not. We formulate the input text as T = {S1, S2, ..., Sn}, where Si is the ith sentence of the text. Following (Devlin et al. 2018), we tokenize the input text and concatenate all tokens as a new sequence, {[CLS], S1, [SEP ], S2, [SEP ], ..., Sn\u22121, [SEP ], Sn, [SEP ]}, where [CLS] is a special token used for classification and [SEP ] is a delimiter. Each token is initialized with a vector by summing the corresponding token and position embedding from pretrained BERT, and then encoded into a hidden state. Finally, we get [H[CLS], HS1 , HS2 , ..,HSn , H[SEP ]] as an encoding output. We concatenate the [CLS]-token representation from the last four layers of the model for our classification task. We apply two linear layers on top of the concatenated output representation with a sigmoid activation function at the final linear layer. We optimize the binary-cross entropy loss and choose the model with least loss on the validation set as our final story classifier. We evaluate this model on the held-out test set. In Table 6a, we report the F1 score, and compare our approach to other baselines. The best performing model achieves an F1score of 0.79. Finally, we feed the Reddit posts to the trained story classifier and obtain a probability score, p, that indicates the likelihood of the post being a story. Furthermore, to increase the reliability of our data, we discard all those posts with probability score lesser that a chosen threshold \u03b4, i.e. p < \u03b4. In our work, we set \u03b4 to 0.75. This procedure yields a total of 63,258 stories, referred to as Reddit Personal Narratives dataset.\nAgreement We analyze the discrepancies in the annotated data to gain insights about the potential challenges in the annotation process.For climax, we note that the annotators get confused with sentences that involve events contributing to rising action (or Labov\u2019s complicating action) which eventually culminate in a climax (or Labov\u2019s MRE). Further, we observe that the differences are finer in many instances and\nhence harder to reliably detect. Though we achieve higher agreement on resolution category, the annotation gets less accurate with ambiguities in resolution and aftermath/endings, especially when narratives don\u2019t have a clear resolution. Interestingly, the annotators are able to discern between the two interest categories despite the high cognitive load and complexity involved in detecting them from unstructured user generated content."
        },
        {
            "heading": "M-SENSE Model",
            "text": "Token-BERT Token-level BERT: Since BERT produces output vectors that are grounded to tokens instead of sentences, we undertake an input processing step that involves insertion of a special [CLS] token at the beginning of each sentence and a [SEP ] token at the end of each sentence in the input sequence. we feed the entire narrative text T to the input processing step to get the following sequence: {[CLS], w11, w12, .., w1N1 , [SEP ], [CLS], w21, w 2 2, .., w 2 N2 , [SEP ], .., [SEP ], [CLS], wL1 , w L 2 , .., wLNL , [SEP ]}, where w i j is the j\nth in ith sentence in the narrative. The multiple [CLS] symbols will aggregate the features for sentences taking the context into consideration. Next, we apply alternating segment embeddings indicative of different sentences in our input textual narrative. Given a narrative with four sentences, [S1, S2, S3, S4], we assign the segment embeddings as [EA, EB , EA, EB ]."
        },
        {
            "heading": "Zero-shot Approaches",
            "text": "In addition to our M-SENSE model and its variants, we experiment with zero-shot methods that utilize either simple heuristics or suspense-based approaches to model narrative structure.\nHeuristic-based Approaches In addition to different modeling approaches, we experiment with simple heuristics for automatically labeling the sentences in story. This method assumes that the title of the Reddit post provides the summary of the post and hence, could refer to the MRE/climax of the narrative. We use a pretrained sentence embedding model and compute the semantic similarity between the each sentence in the narrative and the original title of the Reddit post. We compute USE embeddings of sentences to identify the nearest neighbor of the post title and label it as the climax. Next, we assign the last sentence as the resolution because it is more like for the cognitive tension to drop and reach a resolution at the near end of the narrative.\nSuspense-based Approaches A recent work (Wilmot and Keller 2020) has explored surprise and uncertainty reduction as a measure of suspense in narratives by considering sentences as the primary unit of processing. In our work, we mainly focus on surprise values based on consequential state change in narratives. Intuitively, a large change in any particular state indicates increased surprise at that point in the narrative. (Ely, Frankel, and Kamenica 2015)\u2019s surprise is defined as the amount of change from previous sentence to the current sentence in the narrative. Peaks in such measures could reflect potential events where protagonist faces key obstacles and these may act as the defining moments\nof narrative structure. Thus, we examine different sentence embedding techniques to compute state changes including change in protagonist\u2019s mental state representations in our experiments to recognize suspenseful states. This will act as a relevant baseline to determine the effectiveness of semantic and mental state features."
        },
        {
            "heading": "Training & Hyperparameters",
            "text": "Though we approach narrative structure model as a sentence classification task, we divide the collected data based on number of narratives into train, validation and test sets at 70-10-20 split. In our sentence labeling task, each sentence will be accompanied with the entire narrative context. On the training set, we perform data augmentation by replacing sentences in narrative context with their paraphrases. The paraphrases are generated using a back-translation approach (Edunov et al. 2018) based on pretrained English\u2194German translation model. We limit the number of such modified sentences to 20% of the story length. We tune the hyperparameters using grid search and the best configuration is obtained based on validation set performance. Our best configuration consists of a two-layer story encoder (nL = 2) and a single transformer-based fusion layer with 12 heads. For the window size in the interaction layer, it is set to two sentences, i.e., s = 2. We also added a dropout with rate 0.2 in order to prevent overfitting. We optimize using Adam (Kingma and Ba 2014) at a learning rate of \u03b1 = 0.0001 and a batch size of 32 with PyTorch (Paszke et al. 2019) being our model implementation framework. We report the score over an average of 3 runs. All our experiments were conducted on multiple NVIDIA Tesla P100 GPUs each with 12GB memory."
        },
        {
            "heading": "Results",
            "text": "We conducted ablation study in Section . Table 4 shows the results of our study. We describe some of the aspects below.\nImportance of Contextualization: Next, we evaluate the contribution of a story encoder to our classification task. We observe that the performance drops significantly (by \u223c 10% and \u223c 12% for climax and resolution prediction respectively) without a story encoder. The importance of contextualizing the story sentences is established as we see a marked improvement of \u223c 8% in the average overall performance with the introduction of an inter-sentence RNN story encoding layer. Still, this performance lags behind our default M-SENSE setting with Transformer-based story encoding layer. We find the story encoder being relevant even if the inter-sentence dependencies are captured using the tokenlevel BERT model. We attribute this to the task-specific intersentence relationships being unearthed as we fine-tune our model.\nChoice of Sentence Encoder: From the results, it is clear that token-level BERT generally performs better the sentence-level BERT variant. This is unsurprising as the sentence-level approach produces embeddings without story context. Such an approach results in the loss of fine-grained inter-sentence token dependencies that the token-level BERT can extract. Experiments also suggest that sentence-level\nUSE model trained on textual similarity tasks results in better F1 scores than the BERT counterpart.\nImpact of Interaction Layer: The addition of an interaction layer yields an average \u223c 4% gain in performance for identifying climax and resolution. The advantage of introducing an interaction layer has been studied in prior studies (Hearst 1997; Papalampidi, Keller, and Lapata 2019) and we find the performance improvement to be congruous with these studies."
        },
        {
            "heading": "Attention Analysis",
            "text": "We conduct attention analysis on those narratives containing sentences belonging to \u201cEsteem\u201d and \u201cLove/ Belonging\u201d categories. Specifically, we study the functioning of Transformer-based Fusion layer for aggregating multiple latent embeddings \u2013 Semantic (Sem), Intent (Int), and Emotion (Emo). We then visualize the attention heatmaps from the fusion layer corresponding to these predicted climax and resolution sentences by computing the average attention score map over all heads. Figure 6 displays the visualized attention map for sample stories belonging to the above mentioned categories. Since, [FUSE] is the aggregated output over all the three latent embeddings. We note that the attention map has high attention scores between intent (Int) and [FUSE] vectors for stories related to \u201cEsteem\u201d motivation category, while more weight is assigned for emotion (Emo) in samples associated with \u201cLove/Belonging\u201d category."
        }
    ],
    "title": "M-SENSE: Modeling Narrative Structure in Short Personal Narratives Using Protagonist\u2019s Mental Representations",
    "year": 2023
}