{
    "abstractText": "Uncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of \u201cuncertainty\u201d for a specific task (classification or regression) under a specific loss (binary loss, crossentropy loss, squared loss, etc.); (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of empirical risk minimization or optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function through a partial differential equation and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properness of existing uncertainty measures (including entropy uncertainty, least confidence uncertainty, margin-based uncertainty, etc.) from two aspects: surrogate property and loss convexity. It can also be used to develop new uncertainty measures. Furthermore, we propose a new notion for designing uncertainty measures called loss as uncertainty. The idea is to use the conditional expected loss given the features as the uncertainty measure. Such an uncertainty measure has nice analytical properties and, more importantly, a generality to cover both classification and regression problems (in contrast to the existing case-by-case design of uncertainty measures). These developments enable us to provide the first generalization bound for uncertainty sampling algorithms under both stream-based and pool-based settings, in the full generality of the underlying model and problem. Lastly, we establish some connection between certain variants of the uncertainty sampling algorithms with risk-sensitive objectives and distributional robustness, which can partly explain the advantage of uncertainty sampling algorithms when the sample size is small.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shang Liu"
        },
        {
            "affiliations": [],
            "name": "Xiaocheng Li"
        }
    ],
    "id": "SP:441d1cad6c6f3eb68473dee9e21e887468eb0d49",
    "references": [
        {
            "authors": [
                "Angluin",
                "Dana."
            ],
            "title": "Queries and concept learning",
            "venue": "Machine learning 2 319\u2013342.",
            "year": 1988
        },
        {
            "authors": [
                "Arora",
                "Sanjeev",
                "L\u00e1szl\u00f3 Babai",
                "Jacques Stern",
                "Z Sweedyk."
            ],
            "title": "The hardness of approximate optima in lattices, codes, and systems of linear equations",
            "venue": "Journal of Computer and System Sciences 54(2) 317\u2013331.",
            "year": 1997
        },
        {
            "authors": [
                "Atlas",
                "Les",
                "David Cohn",
                "Richard Ladner."
            ],
            "title": "Training connectionist networks with queries and selective sampling",
            "venue": "Advances in neural information processing systems 2.",
            "year": 1989
        },
        {
            "authors": [
                "Balcan",
                "Maria-Florina",
                "Alina Beygelzimer",
                "John Langford."
            ],
            "title": "Agnostic active learning",
            "venue": "Proceedings of the 23rd international conference on Machine learning. 65\u201372.",
            "year": 2006
        },
        {
            "authors": [
                "Balcan",
                "Maria-Florina",
                "Andrei Broder",
                "Tong Zhang."
            ],
            "title": "Margin based active learning",
            "venue": "Learning Theory: 20th Annual Conference on Learning Theory, COLT 2007, San Diego, CA, USA; June 13-15, 2007. Proceedings 20 . Springer, 35\u201350.",
            "year": 2007
        },
        {
            "authors": [
                "Bartlett",
                "Peter L",
                "Olivier Bousquet",
                "Shahar Mendelson."
            ],
            "title": "Localized rademacher complexities",
            "venue": "International Conference on Computational Learning Theory. Springer, 44\u201358.",
            "year": 2002
        },
        {
            "authors": [
                "Bartlett",
                "Peter L",
                "Michael I Jordan",
                "Jon D McAuliffe."
            ],
            "title": "Convexity, classification, and risk bounds",
            "venue": "Journal of the American Statistical Association 101(473) 138\u2013156.",
            "year": 2006
        },
        {
            "authors": [
                "Baum",
                "Eric B",
                "Kenneth Lang."
            ],
            "title": "Query learning can work poorly when a human oracle is used",
            "venue": "International joint conference on neural networks. IEEE Press, 335\u2013340.",
            "year": 1992
        },
        {
            "authors": [
                "Bott",
                "Raoul",
                "Loring W Tu"
            ],
            "title": "Differential forms in algebraic topology, vol",
            "year": 1982
        },
        {
            "authors": [
                "Bousquet",
                "Olivier",
                "St\u00e9phane Boucheron",
                "G\u00e1bor Lugosi."
            ],
            "title": "Introduction to statistical learning theory",
            "venue": "Summer school on machine learning. Springer, 169\u2013207.",
            "year": 2003
        },
        {
            "authors": [
                "Cohn",
                "David",
                "Les Atlas",
                "Richard Ladner."
            ],
            "title": "Improving generalization with active learning",
            "venue": "Machine learning 15 201\u2013221.",
            "year": 1994
        },
        {
            "authors": [
                "Culotta",
                "Aron",
                "Andrew McCallum."
            ],
            "title": "Reducing labeling effort for structured prediction tasks",
            "venue": "AAAI , vol. 5. 746\u2013751.",
            "year": 2005
        },
        {
            "authors": [
                "Dagan",
                "Ido",
                "Sean P Engelson"
            ],
            "title": "Committee-based sampling for training probabilistic classifiers",
            "venue": "Machine Learning Proceedings 1995 . Elsevier,",
            "year": 1995
        },
        {
            "authors": [
                "Dasgupta",
                "Sanjoy",
                "Adam Tauman Kalai",
                "Claire Monteleoni."
            ],
            "title": "Analysis of perceptron-based active learning",
            "venue": "Learning Theory: 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005. Proceedings 18 . Springer, 249\u2013263.",
            "year": 2005
        },
        {
            "authors": [
                "Duchi",
                "John C",
                "Peter W Glynn",
                "Hongseok Namkoong."
            ],
            "title": "Statistics of robust optimization: A generalized empirical likelihood approach",
            "venue": "Mathematics of Operations Research 46(3) 946\u2013969.",
            "year": 2021
        },
        {
            "authors": [
                "Foygel Barber",
                "Rina",
                "Emmanuel J Candes",
                "Aaditya Ramdas",
                "Ryan J Tibshirani."
            ],
            "title": "The limits of distribution-free conditional predictive inference",
            "venue": "Information and Inference: A Journal of the IMA 10(2) 455\u2013482.",
            "year": 2021
        },
        {
            "authors": [
                "Hanneke",
                "Steve"
            ],
            "title": "Theory of disagreement-based active learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning",
            "year": 2014
        },
        {
            "authors": [
                "H\u00fcllermeier",
                "Eyke",
                "Willem Waegeman."
            ],
            "title": "Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods",
            "venue": "Machine Learning 110 457\u2013506.",
            "year": 2021
        },
        {
            "authors": [
                "Kelly",
                "Markelle",
                "Rachel Longjohn",
                "Kolby Nottingham."
            ],
            "title": "UCI machine learning repository",
            "venue": "URL https://archive.ics.uci.edu.",
            "year": 2021
        },
        {
            "authors": [
                "Kuleshov",
                "Volodymyr",
                "Nathan Fenner",
                "Stefano Ermon."
            ],
            "title": "Accurate uncertainties for deep learning using calibrated regression",
            "venue": "International conference on machine learning. PMLR, 2796\u20132804.",
            "year": 2018
        },
        {
            "authors": [
                "Kumar",
                "Ananya",
                "Percy S Liang",
                "Tengyu Ma."
            ],
            "title": "Verified uncertainty calibration",
            "venue": "Advances in Neural Information Processing Systems 32.",
            "year": 2019
        },
        {
            "authors": [
                "Ledoux",
                "Michel",
                "Michel Talagrand."
            ],
            "title": "Probability in Banach Spaces: isoperimetry and processes, vol",
            "venue": "23. Springer Science & Business Media.",
            "year": 1991
        },
        {
            "authors": [
                "Lewis",
                "David D."
            ],
            "title": "A sequential algorithm for training text classifiers: Corrigendum and additional data",
            "venue": "Acm Sigir Forum, vol. 29.2. ACM New York, NY, USA, 13\u201319.",
            "year": 1995
        },
        {
            "authors": [
                "Lin",
                "Yi."
            ],
            "title": "A note on margin-based loss functions in classification",
            "venue": "Statistics & probability letters 68(1) 73\u201382.",
            "year": 2004
        },
        {
            "authors": [
                "Liu",
                "Shang",
                "Zhongze Cai",
                "Xiaocheng Li."
            ],
            "title": "Distribution-free model-agnostic regression calibration via nonparametric methods",
            "venue": "arXiv preprint arXiv:2305.12283 .",
            "year": 2023
        },
        {
            "authors": [
                "Maurer",
                "Andreas",
                "Massimiliano Pontil."
            ],
            "title": "Empirical bernstein bounds and sample variance penalization",
            "venue": "arXiv preprint arXiv:0907.3740 .",
            "year": 2009
        },
        {
            "authors": [
                "Mussmann",
                "Stephen",
                "Percy Liang."
            ],
            "title": "On the relationship between data efficiency and error for uncertainty sampling",
            "venue": "International Conference on Machine Learning. PMLR, 3674\u20133682.",
            "year": 2018
        },
        {
            "authors": [
                "Mussmann",
                "Stephen",
                "Percy S Liang."
            ],
            "title": "Uncertainty sampling is preconditioned stochastic gradient descent on zero-one loss",
            "venue": "Advances in Neural Information Processing Systems 31.",
            "year": 2018
        },
        {
            "authors": [
                "Namkoong",
                "Hongseok",
                "John C Duchi."
            ],
            "title": "Variance-based regularization with convex objectives",
            "venue": "Advances in neural information processing systems 30.",
            "year": 2017
        },
        {
            "authors": [
                "Orabona",
                "Francesco",
                "Nicolo Cesa-Bianchi"
            ],
            "title": "Better algorithms for selective sampling",
            "venue": "Proceedings of the 28th international conference on machine learning: Bellevue, Washington, USA, june 28",
            "year": 2011
        },
        {
            "authors": [
                "Raj",
                "Anant",
                "Francis Bach"
            ],
            "title": "Convergence of uncertainty sampling for active learning",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas",
                "Andrew McCallum"
            ],
            "title": "Toward optimal active learning through monte carlo estimation",
            "venue": "Conference on Machine Learning. PMLR,",
            "year": 2001
        },
        {
            "authors": [
                "Department of Computer Sciences. Settles",
                "Burr",
                "Mark Craven",
                "Soumya Ray."
            ],
            "title": "Multiple-instance active learning",
            "venue": "Advances in neural",
            "year": 2007
        },
        {
            "authors": [
                "Shamir",
                "Ohad",
                "Tong Zhang"
            ],
            "title": "Stochastic gradient descent for non-smooth optimization: Convergence",
            "venue": "fifth annual workshop on Computational learning theory",
            "year": 2013
        },
        {
            "authors": [
                "Wang",
                "Ran",
                "Chi-Yin Chow",
                "Sam Kwong"
            ],
            "title": "Ambiguity-based multiclass active learning",
            "year": 2015
        },
        {
            "authors": [
                "Bartlett"
            ],
            "title": "1\u2212\u03bc to meet the value",
            "year": 2006
        },
        {
            "authors": [
                "Cp(\u0176"
            ],
            "title": "Note that a binary classification loss l is said to be classification-calibrated (Bartlett et al., 2006) (or Fisher consistent (Lin, 2004)) if H\u2212(p) > H(p) for any p",
            "venue": "Bartlett et al",
            "year": 2006
        },
        {
            "authors": [
                "Y E [l(g(X"
            ],
            "title": "\u03c8(\u03b6) + \u01eb. Equipped with such powerful tools, all we need to do is to find the surrogate link functions of those active learning models. But before we proceed to the particular calculation, we notice that the analysis in Bartlett et al. (2006) is designed for the margin-based models, while our Example 1 is not based on the margin",
            "year": 2006
        },
        {
            "authors": [
                "Bartlett"
            ],
            "title": "39 and positive uncertainty function U leads to the same minimizer of the expected equivalent loss as the expected original margin loss, while the latter by the arguments",
            "year": 2006
        },
        {
            "authors": [
                "Bousquet"
            ],
            "title": "2003)) is that for any \u03b4 > 0, the following holds with probability at least 1\u2212 \u03b4",
            "year": 2003
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 7.\n02 71\n9v 3\n[ cs\n.L G\n] 2\n0 Ju\nl 2 02\n3\nUncertainty sampling is a prevalent active learning algorithm that queries sequentially the annotations of data samples which the current prediction model is uncertain about. However, the usage of uncertainty sampling has been largely heuristic: (i) There is no consensus on the proper definition of \u201cuncertainty\u201d for a specific task (classification or regression) under a specific loss (binary loss, crossentropy loss, squared loss, etc.); (ii) There is no theoretical guarantee that prescribes a standard protocol to implement the algorithm, for example, how to handle the sequentially arrived annotated data under the framework of empirical risk minimization or optimization algorithms such as stochastic gradient descent. In this work, we systematically examine uncertainty sampling algorithms under both stream-based and pool-based active learning. We propose a notion of equivalent loss which depends on the used uncertainty measure and the original loss function through a partial differential equation and establish that an uncertainty sampling algorithm essentially optimizes against such an equivalent loss. The perspective verifies the properness of existing uncertainty measures (including entropy uncertainty, least confidence uncertainty, margin-based uncertainty, etc.) from two aspects: surrogate property and loss convexity. It can also be used to develop new uncertainty measures. Furthermore, we propose a new notion for designing uncertainty measures called loss as uncertainty. The idea is to use the conditional expected loss given the features as the uncertainty measure. Such an uncertainty measure has nice analytical properties and, more importantly, a generality to cover both classification and regression problems (in contrast to the existing case-by-case design of uncertainty measures). These developments enable us to provide the first generalization bound for uncertainty sampling algorithms under both stream-based and pool-based settings, in the full generality of the underlying model and problem. Lastly, we establish some connection between certain variants of the uncertainty sampling algorithms with risk-sensitive objectives and distributional robustness, which can partly explain the advantage of uncertainty sampling algorithms when the sample size is small."
        },
        {
            "heading": "1 Introduction",
            "text": "Active learning is a machine learning paradigm where the learning algorithm interactively queries humans (or some other information source) to annotate new data points. Different from supervised learning, an active learning algorithm begins with all the data samples unlabeled and adaptively decides which samples to query for labels. The study of active learning is motivated by the great availability of unlabeled data and the prohibitive cost of getting all the data labeled. Its goal is to improve data efficiency and reduce the labeling cost by querying only a small proportion of the data but still getting a satisfying performance.\nThe study of active learning algorithms can be categorized according to two standards: scenarios and querying strategies (Settles, 2009). The scenarios of active learning are determined by how the data is generated and observed. The query synthesis scenario allows the learner to generate de novo examples rather than samples from a distribution (Angluin, 1988). While query synthesis is practical for many problems, labeling arbitrarily generated instances could be awkward for human experts (Baum and Lang, 1992). Comparatively, if the data is generated from a fixed unknown distribution, then we call it either\nstream-based sampling or pool-based sampling, depending on the way that unlabeled samples arrive. If the samples arrive in a sequence, the learner queries the labels from a stream (Atlas et al., 1989; Cohn et al., 1994). Otherwise, the learner can observe the pool of unlabeled samples (Lewis, 1995). In this paper, we focus on stream-based and pool-based scenarios.\nThe second criterion to categorize the active learning algorithms is the querying strategy, among which uncertainty sampling is \u201cperhaps the simplest and most commonly used query framework\u201d (Settles, 2009). Roughly speaking, the uncertainty sampling strategy is to query the samples that the model is uncertain about (Lewis, 1995). Other strategies include query-by-committee (Seung et al., 1992), expected model change (Settles et al., 2007), expected error reduction (Roy and McCallum, 2001), and expected variance reduction (Wang et al., 2015). Although rigorous theoretical results have been obtained for some of the other querying strategies (Balcan et al., 2006; Hanneke et al., 2014), theoretical understanding of the uncertainty sampling strategy is still lacking. Some initial yet intriguing results have been established for various kinds of uncertainty measurements. Mussmann and Liang (2018b) show that the threshold-based uncertainty sampling (i.e., to query only the samples of which the uncertainty is above a threshold) can be interpreted as performing a preconditioned stochastic gradient step on a smoothed version of the population zero-one loss that converges to the population zero-one loss. The non-convexity of the zero-one loss implies that the threshold-based uncertainty sampling could be trapped in local minima, suggesting the necessity of a warm start. Tifrea et al. (2022) consider a similar threshold-based uncertainty, where the threshold is chosen implicitly via querying the least confident several samples under the Bayes optimal hypothesis. For a handcrafted linearly separable distribution, Tifrea et al. (2022) prove a finite-sample lower bound on the logistic regression in the high-dimensional case for the empirical risk minimization algorithm as Lewis (1995), and claim the less efficiency of uncertainty sampling against passive learning both theoretically and empirically. Apart from the pool-based setting and the threshold-based uncertainty, Raj and Bach (2022) design their algorithm in the stream-based setting with a margin-based uncertainty. They prove that the stream-based algorithm will converge with an O(1/T ) error rate under a strictly linearly separable data distribution.\nDespite all those efforts, there has been no systematic theoretical understanding of data efficiency or even the convergence of uncertainty sampling. Besides, existing theoretical works are restricted to particular forms of uncertainty sampling algorithms. In addition, all existing theoretical results are made for linear classifiers. And there is little theoretical understanding of the probabilistic-based uncertainty measurements (Dagan and Engelson, 1995; Culotta and McCallum, 2005) or the regression problem. In this paper, we propose a general framework to analyze uncertainty sampling algorithms and introduce a notion of equivalent loss. We establish that the uncertainty sampling algorithms essentially optimize against such an equivalent loss objective. By inspecting the surrogate and the optimization properties of the equivalent loss, we not only recover existing theoretical results but also generalize to uncertainty sampling algorithms under other contexts such as multi-class classification and regression. Our contribution can be summarized as follows:\n\u2022 We introduce the equivalent loss as a loss function specified through a partial differential equation\nin terms of the used uncertainty and the original loss function. Then we establish that uncertainty sampling algorithms essentially optimize against this equivalent loss.\n\u2022 For binary classification, we examine the existing uncertainty measures and theoretical results\nthrough the lens of equivalent loss. Specifically, we show that the error rate of the margin-based uncertainty in Raj and Bach (2022) will converge to zero regardless of the underlying data distribution, compared to their assumption that the data needs to be strictly separable. We recover the non-convexity observations of the threshold-based models (Mussmann and Liang, 2018b; Tifrea et al., 2022). We also analyze the probabilistic uncertainty models, showing their Fisher\nconsistency.\n\u2022 We generalize this notion to the multi-classification and the regression problems with our loss-as-\nuncertainty principle. Equipped with such an uncertainty measure, the convergence can be proved for any convex and non-negative loss functions for binary classification, multi-class classification, and regression.\n\u2022 We also study several other variants of uncertainty sampling algorithms and draw connections\nwith risk-sensitive loss and distributional robustness. Specifically, we show the exponential-lossas-uncertainty will be minimizing the softmax of the loss, the top-k-max uncertainty sampling essentially minimizes the conditional value at risk (CVaR), and the mixture of uniform and uncertainty sampling recovers a distributionally robust optimization formulation."
        },
        {
            "heading": "2 Problem Setup",
            "text": "Consider the problem of predicting the label Y from the feature X , where (X,Y ) is independently drawn from an unknown distribution P . We denote the marginal distribution of X to be PX and the conditional distribution of Y on X is PY |X . Let X and Y denote the support of X and Y respectively. Suppose X \u2208 Rd is a bounded set with an upper bound of MX with respect to the Euclidean norm. For a binary classification problem, Y = {\u22121,+1}. For a K-nary classification problem, Y = [K] = {1, . . . ,K}. For a regression problem, we assume Y = [\u2212MY ,MY ] is a bounded set with an upper bound of MY .\nFor the canonical setting of supervised learning, a full dataset of both features and labels is completely revealed to the learner at the beginning. For active learning, the learner starts with only observations of the features X \u2019s and needs to decide which of the labels Y \u2019s to query or whether to query the labels Y \u2019s. In this paper, we consider two mainstream settings for active learning.\n\u2022 Stream-based setting. The dataset DXT consists of T i.i.d. features {Xt}Tt=1 from PX . The samples arrive sequentially. At each time t, upon the arrival of Xt, the learner decides whether\nto query the sample: if so, Yt is revealed to the learner; otherwise, it moves on to the next time period. The feature and the label (if queried) of the t-th time period will be discarded (but not cached) after the time period. Without loss of generality, we still assume the presence of the label Yt sampled from PY |X=Xt ; it may just not be revealed to the learner depending on the querying decision.\n\u2022 Pool-based setting. The dataset DXn consists of n i.i.d. features {Xi}ni=1 from PX . The whole dataset DXn is revealed all at once to the learner at the beginning. The learner queries samples from the dataset sequentially. Unlike the stream-based setting, the information from past queries\nwill be retained and can be repeatedly utilized by the learner.\nThroughout the paper, we consider a parameterized family of hypotheses denoted by F = {f\u03b8(\u00b7) : \u03b8 \u2208 \u0398, f\u03b8(\u00b7) : X \u2192 Y}. We assume the parameter set \u0398 has an upper bound of M\u0398 under the Euclidean norm. We denote the loss function l : Y \u00d7 Y \u2192 R, i.e., l(Y\u0302 , Y ) measures the loss of predicting Y with Y\u0302 . With a slight overload of the notation, we denote l(\u03b8; (X,Y )) = l(f\u03b8(X), Y ) as the prediction loss of the model f\u03b8(\u00b7) on the sample (X,Y ). For uncertainty sampling algorithms, a key component is an uncertainty function/measure U(\u03b8;X) : X \u2192 [0,\u221e). The uncertainty function quantifies the uncertainty about a sample X given the model parameter \u03b8. The specification of the uncertainty function usually depends on both the underlying hypothesis class F and the loss function l (Dagan and Engelson, 1995; Culotta and McCallum, 2005; Dasgupta et al., 2005; Balcan et al., 2007; Mussmann and Liang, 2018b; Raj and Bach, 2022; Tifrea et al.,\n2022). The general idea is to spend more querying efforts on those samples that the current model is uncertain about, in the hope to maximize the improvement of the model learning."
        },
        {
            "heading": "3 Uncertainty Sampling for Binary Classification",
            "text": ""
        },
        {
            "heading": "3.1 Generic algorithm under stream-based setting",
            "text": "We begin our discussion with the binary classification problem. In the following, we present a generic algorithm of uncertainty sampling under the stream-based setting. Specifically, Algorithm 1 queries the data samples based on the model uncertainty and updates the model parameter according to a gradient descent procedure. It takes the uncertainty function U(\u03b8;X) as an input. At each time t, the algorithm observes only the feature Xt and calculates the uncertainty U(\u03b8t;Xt). Here, without loss of generality, we assume the uncertainty is between [0, 1]. Then, with probability U(\u03b8t;Xt), the algorithm queries the label of the sample and performs a gradient descent update; with probability 1\u2212U(\u03b8t;Xt), the algorithm does not make a query and hence not update the parameters. In this way, a larger value of uncertainty will encourage the querying of a sample.\nAlgorithm 1 Uncertainty sampling with gradient descent update (stream-based version) Input: Dataset DT = {(Xt, Yt)}Tt=1, step sizes {\u03b7t}Tt=1 > 0, uncertainty function U(\u03b8;X) : X \u2192 [0, 1] 1: Initialize \u03b81; \u03b8\u03041 \u2190 \u03b81 2: for t = 1, ..., T do 3: Observe Xt and calculate U(\u03b8t;Xt) 4: Generate \u03bet \u223c Unif[0, 1] 5: if \u03bet \u2264 U(\u03b8t;Xt) then 6: Query the label Yt and update\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7t \u00b7 \u2202l(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n7: else 8: Do not query the label Yt and let \u03b8t+1 \u2190 \u03b8t\n9: end if\n10: \u03b8\u0304t+1 \u2190 (1\u2212 1t+1 )\u03b8\u0304t + 1t+1\u03b8t+1 11: end for Output: \u03b8\u0304T+1\nThe core idea of the algorithm is to query only the samples that the model is uncertain about, and the uncertainty function quantifies such uncertainty. In the following, we review three examples of the uncertainty function used in the literature as special cases of the generic algorithm.\nExample 1 (Probabilistic model (Dagan and Engelson, 1995; Culotta and McCallum, 2005)). A probabilistic model outputs q(\u03b8;X) : X \u2192 [0, 1] to estimate the true conditional probability P(Y = +1|X). The entropy uncertainty (Dagan and Engelson, 1995) considers the entropy of q(\u03b8;X):\nU(\u03b8;X) := \u2212 [q(\u03b8;X) log(q(\u03b8;X)) + (1\u2212 q(\u03b8;X)) log(1 \u2212 q(\u03b8;X))] ,\nwhere q = q(X ; \u03b8) \u2208 (0, 1). The least confidence uncertainty (Culotta and McCallum, 2005) considers\nU(\u03b8;X) := 1\u2212max{q(X ; \u03b8), 1\u2212 q(X ; \u03b8)} = min{q(X ; \u03b8), 1\u2212 q(X ; \u03b8)}.\nThese two uncertainties are often accompanied by the following cross-entropy loss that trains the probabilistic model\nl(\u03b8; (X,Y )) = \u2212 [1{Y = +1} log q(\u03b8;X) + 1{Y = \u22121} log(1 \u2212 q(\u03b8;X))]\nwhere 1{\u00b7} is the indicator function. Equivalently, we can also represent the loss function by l(Y\u0302 , Y ) = \u2212 log (\n1+Y\u0302 \u00b7Y 2\n)\nwhere Y\u0302 = 2q(\u03b8;X)\u2212 1 \u2208 [\u22121, 1] is the predicted expectation.\nFor a probabilistic model, q(\u03b8;X) reflects the confidence of the prediction. When q(\u03b8;X) is close to 1, the model is confident that Y = +1, while q(\u03b8;X) is close to 0, it is confident that Y = \u22121. For both ends, the uncertainty is small for both the entropy uncertainty and the least confidence uncertainty. When the model is less confident about the prediction and outputs q(\u03b8;X) close to 12 , the uncertainty becomes larger.\nExample 2 (Margin-based model (Raj and Bach, 2022)). Another class of classification model is marginbased, such as support vector machines (SVMs). Consider a linear SVM model that predicts Y with the sign of \u03b8\u22a4X . The margin-based uncertainty function is defined by\nU\u00b5(\u03b8;X) := 1\n1 + \u00b5|\u03b8\u22a4X |\nwhere \u00b5 > 0 is a hyper-parameter. The associated loss function for learning such margin-based models is squared margin loss\nl(\u03b8; (X,Y )) = (max{0, 1\u2212 Y \u00b7 \u03b8\u22a4X})2.\nEquivalently, the loss function can be written in the form of\nl(Y\u0302 , Y ) = (max{0, 1\u2212 Y \u00b7 Y\u0302 })2,\nwhere Y\u0302 = \u03b8\u22a4X .\nFor linear classifiers, |\u03b8\u22a4x| is proportional to the distance from a sample to the classification hyperplane. The margin-based uncertainty captures the intuition that the closer a sample is to the classification hyperplane, the more uncertain the learner is about the sample.\nExample 3 (Threshold-based uncertainty (Orabona et al., 2011; Mussmann and Liang, 2018b; Tifrea et al., 2022)). The pool-based version of Example 2 works with a fixed set of samples and results in a thresholdbased uncertainty function. At each time step, the algorithm will query the most uncertain sample in the given dataset with index it = argmini\u2208Ut |\u03b8\u22a4Xi| where the set Ut contains the indices of unqueried samples at time t. Such a procedure can be captured by the following uncertainty function\nU(\u03b8;X) := 1{|\u03b8\u22a4X | \u2264 \u03b3}.\nwhere \u03b3 > 0 is a hyper-parameter that may change over time. Tifrea et al. (2022) analyze this uncertainty function and derive some negative theoretical results on its performance. Specifically, they consider the following loss for a logistic regression model\nl(\u03b8; (X,Y )) = log(1 + exp(\u2212Y \u00b7 \u03b8\u22a4X)).\nEquivalently, the loss can be written as\nl(Y\u0302 , Y ) = log(1 + exp(\u2212Y \u00b7 Y\u0302 )),\nwhere Y\u0302 = \u03b8\u22a4X .\nAs in the margin-based model, the quantity |\u03b8\u22a4x| reflects the confidence of the prediction, and thus it is inversely proportional to the uncertainty. The threshold-based uncertainty queries only those samples where the confidence is smaller than the threshold \u03b3."
        },
        {
            "heading": "3.2 Equivalent loss",
            "text": "Now we show a general property of Algorithm 1 that, with this selective querying procedure, the algorithm essentially optimizes against an alternative loss function which we name as the equivalent loss; and the alternative loss is jointly determined by the uncertainty function U and the original loss function l. Specifically, if we combine the two cases of query and not query for the update step in Algorithm 1, we obtain the following\nE\u03bet [\u03b8t+1|\u03b8t, Xt, Yt] = \u03b8t \u2212 \u03b7t \u00b7 U(\u03b8t;Xt) \u00b7 \u2202l(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\nwhere the expectation is taken with respect to \u03bet which is the sampling random variable that determines whether to query the sample.\nSuppose (for the moment) there exists a loss function l\u0303 such that\n\u2202l\u0303(\u03b8; (x, y)) \u2202\u03b8 = U(\u03b8;X) \u00b7 \u2202l(\u03b8; (x, y)) \u2202\u03b8 (1)\nholds for all \u03b8 \u2208 \u0398 and (x, y) \u2208 X \u00d7 Y (we will discuss the existence of l\u0303 in the following subsection). Then the parameter update can be written as\nE\u03bet [\u03b8t+1|\u03b8t, Xt, Yt] = \u03b8t \u2212 \u03b7t \u00b7 \u2202l\u0303(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n.\nProposition 1. Suppose there exists l\u0303 satisfying (1). Then Algorithm 1 essentially performs stochastic gradient descent (SGD) with respect to the loss function l\u0303.\nDefinition 1. We say l\u0303 is the equivalent loss for the uncertainty function U and the original loss function l, if it satisfies (1).\nThe equivalent loss l\u0303 can be viewed as a surrogate loss of the original loss l twisted by the uncertainty function U . If l\u0303 exists, it provides a convenient handle to understand and analyze the algorithm. In the following, we derive the equivalent loss l\u0303 for the previous examples.\nExample 1 (Continued). Example 1 considers a probabilistic model q(\u03b8;X) that estimates the true conditional probability P(Y = +1|X), and the loss function is the cross-entropy loss.\n\u2022 For the entropy uncertainty, the equivalent loss\nl\u0303(\u03b8; (X,Y )) = q log(q) + (1\u2212 q) log(1\u2212 q)\u2212 1{Y = +1} \u00b7 Li2(q)\u2212 1{Y = \u22121} \u00b7 Li2(1\u2212 q) + Li2(1),\nwhere q = q(\u03b8;X) stands for the prediction model and the function Li2(z) = \u2212 \u222b z 0 log(1\u2212u) u du is the Spence\u2019s function.\n\u2022 For the least confidence uncertainty, the equivalent loss\nl\u0303(\u03b8; (X,Y )) =\n \n \u22121{Y = \u22121} \u00b7 log(2(1 \u2212 q))\u2212 q + log(2), if q < 12 ; \u22121{Y = +1} \u00b7 log(2q)\u2212 (1\u2212 q) + log(2), if q \u2265 12 .\nwhere q = q(\u03b8;X) stands for the prediction model.\nExample 2 (Continued). For the margin-based model, the equivalent loss for the margin-based uncertainty function (defined in Example 2) and the squared margin loss is\nl\u0303\u00b5(\u03b8; (X,Y )) =\n   \n   \u2212 2\u00b5 ( 1\u00b5 \u2212 1) log(1\u2212 \u00b5Y \u00b7 Y\u0302 )\u2212 2\u00b5Y \u00b7 Y\u0302 + C, if Y \u00b7 Y\u0302 \u2264 0; \u2212 2\u00b5 ( 1\u00b5 + 1) log(1 + \u00b5Y \u00b7 Y\u0302 ) + 2\u00b5Y \u00b7 Y\u0302 + C, if Y \u00b7 Y\u0302 \u2208 (0, 1); 0, if Y \u00b7 Y\u0302 \u2265 1,\nwhere the prediction Y\u0302 = \u03b8\u22a4X , the constant C = 2\u00b5 ( 1 \u00b5 + 1) log(1 + \u00b5) \u2212 2\u00b5 , and the hyper-parameter \u00b5 > 0 is the same one that defines the margin-based uncertainty function.\nExample 3 (Continued). The equivalent loss for the threshold-based uncertainty and the logistic loss function is given by the following:\nl\u0303\u03b3(\u03b8; (X,Y )) =\n    \n    log(1 + exp(\u03b3)), if Y \u00b7 Y\u0302 \u2264 \u2212\u03b3, log(1 + exp(\u2212Y \u00b7 Y\u0302 )), if Y \u00b7 Y\u0302 \u2208 (\u2212\u03b3, \u03b3), log(1 + exp(\u2212\u03b3)), if Y \u00b7 Y\u0302 \u2265 \u03b3\nwhere the prediction Y\u0302 = \u03b8\u22a4X and the hyper-parameter \u03b3 is the same one that specifies the thresholdbased uncertainty function.\nFor these three examples, the derivation of the equivalent loss is standard and it is by solving the partial differential equation (PDE) (1), and we defer the details to Appendix A.1. We remark that these equivalent loss functions specify the objective function that Algorithm 1 optimizes, and they are jointly determined by the pair of the uncertainty function and the original loss function."
        },
        {
            "heading": "3.3 Surrogate property of the equivalent loss",
            "text": "The derivation of equivalent loss makes it clear the objective function of the uncertainty sampling procedure. Then a natural question is whether the equivalent loss is a \u201csuitable\u201d loss for the binary classification problem. Recall that the practical goal of training a binary classifier is commonly to achieve a high classification accuracy, i.e., to optimize the binary loss l01(Y\u0302 , Y ) := 1(Y\u0302 6= Y ). While the binary loss is in general computationally intractable (Arora et al., 1997), the margin loss, the logistic loss, and the crossentropy loss can all be viewed as a surrogate loss of the binary loss that enjoys better computational structure such as convexity. In this light, the equivalent loss derived from uncertainty sampling can also be viewed as a surrogate of the binary loss. Following the principles of Bartlett et al. (2006), we can examine the suitability of an equivalent loss and hence certify the properness of the uncertainty function.\nDefinition 2 (Surrogate loss (Bartlett et al., 2006)). A loss function l(\u00b7, \u00b7) is said to be a surrogate of the binary loss if there exists a continuous, non-negative, and non-decreasing function \u03c8 such that for any measurable function f : X \u2192 Y and any probability distribution P on X \u00d7 Y = X \u00d7 {\u22121,+1},\n\u03c8\n(\nL01(f)\u2212 inf g\u2208G L01(g)\n)\n\u2264 E [l(f(X), Y )]\u2212 inf g\u2208G E [l(g(X), Y )] , (2)\nwhere G is the set of all measurable functions, and L01(f) := E [l01(f(X), Y )] denotes the expected binary loss. All the expectations are taken with respect to the distribution P .\nThe definition establishes a connection between the oracle generation bound under the loss l and that under the binary loss. It can thus verify the properness of a loss l by whether training a model with l can also lead to a performance guarantee for the binary loss. An important property of the link function \u03c8 is that if z \u2192 0 as \u03c8(z)\u2192 0, then the loss function is classification-calibrated (Bartlett et al., 2006). This ensures that the minimizer of the loss l among all the measurable functions will be the Bayes optimal classifier; the property is also known as the Fisher consistency.\nTheorem 1 (Theorem 3 in Bartlett et al. (2006)). For any loss function l that can be expressed as l(Y\u0302 , Y ), one can construct a link function \u03c8. Furthermore, the constructed link function is mini-max optimal in the sense that for any non-negative loss l, any |X | \u2265 2, any risk level \u03b6 \u2208 [0, 1], and any precision \u01eb > 0, there exists a probability distribution on X\u00d7{\u22121,+1} such that L01(f)\u2212infg\u2208G L01(g) = \u03b6 and\n\u03c8(\u03b6) \u2264 E [l(f(X), Y )]\u2212 inf g\u2208G E [l(g(X), Y )] \u2264 \u03c8(\u03b6) + \u01eb.\nThe loss l is classification-calibrated (Fisher consistent) if and only if for any z \u2208 (0, 1], \u03c8(z) > 0.\nBartlett et al. (2006) provide a way to derive the link function \u03c8 (See our Appendix A.2 for more details). They further prove that this surrogate property\u2019s link function is mini-max optimal by the existence of a probability distribution to make the surrogate upper bound arbitrarily tight. They also establish some equivalence between the link function and the Fisher consistency. While such a conclusion is only stated for margin-based models where l(Y\u0302 , Y ) = l(Y\u0302 \u00b7 Y ) in (Bartlett et al., 2006), their analysis in Theorem 1 indeed applies to more general loss functions such as the cross entropy written as loss l(Y\u0302 , Y ) = 1{Y = +1} \u00b7 l(Y\u0302 ,+1) + 1{Y = \u22121} \u00b7 l(Y\u0302 ,\u22121). In the following proposition, we re-examine the previous examples and calculate the corresponding link functions against binary loss.\nProposition 2. All the equivalent losses in Example 1, Example 2, and Example 3 are surrogate losses for binary loss. Specifically,\n\u2022 Example 1 \u2013 entropy uncertainty (see Figure 1). The link function\n\u03c8(z) = 1 + z\n2 \u00b7 Li2\n(\n1 + z\n2\n)\n+ 1\u2212 z 2 \u00b7 Li2\n(\n1\u2212 z 2\n) \u2212 Li2 ( 1\n2\n)\n\u2212 1 + z 2 \u00b7 log(1 + z)\u2212 1\u2212 z 2 \u00b7 log(1\u2212 z)\n= log(2) \u00b7 z2 + o(z2) as z \u2192 0.\nwhere Li2(z) is the Spence\u2019s function as defined earlier.\n\u2022 Example 1 \u2013 least confidence uncertainty (see Figure 2)\n\u03c8(z) = 1 + z 2 log(1 + z)\u2212 z 2 = z2 2 + o(z2) as z \u2192 0.\n\u2022 Example 2 \u2013 margin-based uncertainty (see Figure 3)\n\u03c8\u00b5(z) = 2 \u00b52 (1 + \u00b5z) log(1 + \u00b5z)\u2212 2 \u00b5 z = 2z2 + o(z2) as z \u2192 0.\n\u2022 Example 3 \u2013 threshold-based uncertainty (see Figure 4)\n\u03c8\u03b3(z) =\n \n 1 2 [(1 + z) log(1 + z) + (1\u2212 z) log(1\u2212 z)] , if z \u2264 z0; 1 2 [(1 + z) log(1 + z0) + (1\u2212 z) log(1\u2212 z0)] , if z \u2265 z0,\nwhere z0 is a constant determined by the threshold \u03b3\nz0 := 2\n(\nexp\n(\n\u03b3\n1 + exp(\u03b3)\n)\n+ exp ( \u2212\u03b3 1 + exp(\u2212\u03b3) ))\u22121 \u2212 1.\nAs z \u2192 0, \u03c8\u03b3(z) = z2 + o(z2).\nAs noted earlier, the link function helps to transfer the excessive risk bound under the equivalent loss to that under the binary loss. In the next subsection, we pursue such a roadmap by first establishing the convergence rate under the equivalent loss and then transferring it to a performance guarantee under the binary loss."
        },
        {
            "heading": "3.4 Convergence analysis for convex loss",
            "text": "From the perspective of equivalent loss, the stream-based uncertainty sampling of Algorithm 1 can be viewed as a stochastic gradient descent algorithm to minimize the objective function E[l\u0303(\u03b8; (X,Y ))]. Now we establish the convergence rate against such an objective.\nDefinition 3 (Loss convexity). A loss function l(\u03b8; (X,Y )) is said to be a convex loss if it is convex with respect to \u03b8 for any X \u2208 X and Y \u2208 Y.\nWhen the equivalent loss is convex, we let\n\u03b8\u0303\u2217 := argmin \u03b8\u2208\u0398 E\n[ l\u0303 (\u03b8\u2217, (X,Y )) ]\nand have the following convergence bound.\nProposition 3. Suppose that (i) for the original loss, \u2225 \u2225\n\u2225 \u2202l(\u03b8;(X,Y )) \u2202\u03b8\n\u2225 \u2225 \u2225\n2 \u2264 G for all \u03b8 \u2208 \u0398 almost surely\nfor (X,Y ) \u223c P; (ii) for the initial point, \u2016\u03b81 \u2212 \u03b8\u0303\u2217\u20162 \u2264 D; (iii) the equivalent loss is a convex loss. Then\nwith the step size \u03b7t = D G \u221a T+1 , Algorithm 1 yields the following bound\nE\n[\nl\u0303 ( \u03b8\u0304T+1, (X,Y ) )\n] \u2264 1 T\nT \u2211\nt=1\nE\n[ l\u0303 (\u03b8t, (X,Y )) ] \u2264 E [ l\u0303 ( \u03b8\u0303\u2217, (X,Y ) )] + GD\u221a T + 1 .\nWe would like to draw a comparison between the bound in Proposition 3 and the bound obtained by a standard SGD algorithm against the equivalent loss objective. Note that Algorithm 1 queries only part of the samples, but it achieves the same order of 1\u221a T as the standard SGD which naively queries all the samples. The sacrifice here is the larger variance which is reflected by the constant G in the bound; comparatively, the corresponding gradient variance will be smaller for the standard SGD against the equivalent loss objective.\nThe analysis of Proposition 3 follows the standard analysis of stochastic gradient descent, and it states in the expectation sense. For high probability bounds, a typical concentration argument will yield a similar bound with an additional log(T ) factor. We note that the rate of O(1/ \u221a T ) can be further improved to O(1/T ) for strongly convex functions. Furthermore, if we only consider the last iteration \u03b8T+1 rather than the average \u03b8\u0304T+1, Shamir and Zhang (2013) give an expectation bound of O(log(T )/ \u221a T ) (or O(log(T )/T )) for non-smooth convex (or strongly convex) functions.\nTheorem 2. Suppose the equivalent loss l\u0303 induced by Algorithm 1 is a surrogate loss for the binary loss with link function \u03c8. Also, the parameter space \u0398 satisfies the conditions in Proposition 3, and the step size \u03b7t = D\nG \u221a T+1 . Then we have\nE\n[ L01(f\u03b8\u0304T+1) ]\n\u2212 inf g\u2208G\nL01(g) \u2264 \u03c8\u22121 ( GD\u221a T + 1 + ( E [ l\u0303(f\u03b8\u0303\u2217(X), Y ) ] \u2212 inf g\u2208G E [ l\u0303(g(X), Y ) ] )) ,\nwhere L01(f) = E [l01(f(X), Y )] denotes the expected binary loss as earlier, and the expectation is with respect to the training data and the algorithm\u2019s randomness. Here G is the set of all measurable functions.\nTheorem 2 exemplifies how the performance guarantee under the equivalent loss l\u0303 (Proposition 3) can induce an excessive risk bound under the binary loss through the link function \u03c8. There are two terms on the right-hand side which correspond to estimation error and approximation error, respectively. The first term comes from the SGD learning procedure, and it captures the estimation suboptimality of \u03b8\u0304T+1 against the best parameter \u03b8\u0303 \u2217. While such an error bound on the estimation suboptimality will generally involve the complexity of the hypothesis class, the online nature of the stream-based setting enables a neat analysis alike other online convex optimization algorithms. The second term captures the approximation suboptimality between the best parameter \u03b8\u0303\u2217 in the prescribed hypothesis class and the best one in the class of all measurable functions. The term will shrink as we enlarge the hypothesis class. We note that this approximation term is not pertaining to the uncertainty sampling algorithm or the equivalent loss, but it also appears in the standard supervised learning setting when transforming the excessive risk bound under margin/cross-entropy loss to that under binary loss.\nWe make the following two remarks based on Theorem 2:\n\u2022 Convergence rate: We note that the link function plays a key role in transforming the excessive risk\nbound: it determines the convergence rate under the binary loss. For all the examples calculated so far (See Proposition 2), the link function \u03c8(z) = \u0398(z2) as z \u2192 0, which implies that \u03c8\u22121(z) \u223c \u0398(z\u2212 1 2 ). Thus it will lead to a convergence rate of O(T\u2212 1 4 ) under the binary loss. This does not mean a performance deterioration of uncertainty sampling. For comparison, under the supervised learning regime, the margin loss corresponds to a link function \u03c8(z) = \u0398(z), while the crossentropy loss and the logistic loss, among others, all correspond to a link function \u03c8(z) = \u0398(z2).\nMore importantly, we emphasize that T in the bound represents the number of arrived samples in Algorithm 1 but not the number of queried samples. That is, the uncertainty sampling algorithm achieves the same rate of theoretical convergence for the cross-entropy loss but uses potentially much fewer queried samples. For the margin loss, we provide a short discussion in the next section arguing why it is not compatible with the existing uncertainty sampling algorithms.\n\u2022 Convexity: An important condition in obtaining the bound is the convexity of the loss function with\nrespect to the underlying parameter. While the non-convexity induced by the neural networks is commonly acknowledged as a benign non-convexity, the non-convexity induced by the loss function such as the binary loss or the truncated loss which may cause bad local minima is the type of nonconvexity we try to avoid. This gives a new perspective to understanding the existing uncertainty functions:\n\u2013 The equivalent loss for either the entropy uncertainty or the least confidence uncertainty is\nconvex with respect to the predicted probability q = q(\u03b8;X) for Example 1.\n\u2013 The equivalent loss is convex for the squared margin loss in Example 2. The convexity can\nthus explain why Raj and Bach (2022) develop the algorithm based on the squared margin loss rather than the vanilla margin loss: any margin-based uncertainty U(\u03b8;X |) = h(|\u03b8\u22a4X |) for some non-decreasing function h(\u00b7) will induce a non-convex equivalent loss when the original loss is the margin loss (see Proposition 4).\n\u2013 The equivalent loss is non-convex for the truncated loss in Example 3 (see Figure 4), which\nprovides an explanation for the bad performance of uncertainty sampling (Tifrea et al., 2022).\nThis discussion underlines that in addition to the surrogate property, we desire the equivalent loss induced by the uncertainty function also has a convexity structure."
        },
        {
            "heading": "3.5 Two more examples",
            "text": "We conclude our discussion of the binary classification problem with two more examples.\nExample 4 (Margin loss with margin-based uncertainty induces non-convexity). As noted earlier, all the link functions calculated so far for the equivalent losses have that \u03c8(z) is of order z2 as z \u2192 0. For the standard supervised learning problem, the margin loss (also known as the Hinge loss) has \u03c8(z) = z. In fact, we can calculate the link function for the equivalent loss associated with the margin loss and the margin-based uncertainty as follows. The margin loss is\nl(\u03b8; (X,Y )) = max{0, 1\u2212 Y \u00b7 \u03b8\u22a4X}\nand the margin-based uncertainty is\nU\u00b5(\u03b8;X) = 1\n1 + \u00b5|\u03b8\u22a4X | .\nThen the equivalent loss is\nl\u0303(\u03b8; (X,Y )) =\n    \n    1 \u00b5 log(1\u2212 \u00b5 \u00b7 Y \u00b7 \u03b8\u22a4X) + 1\u00b5 log(1 + \u00b5), if Y \u00b7 \u03b8\u22a4X \u2264 0; \u2212 1\u00b5 log(1 + \u00b5 \u00b7 Y \u00b7 \u03b8\u22a4X) + 1\u00b5 log(1 + \u00b5), if Y \u00b7 \u03b8\u22a4X \u2208 (0, 1); 0, if Y \u00b7 \u03b8\u22a4X \u2265 1.\nAnd its link function is\n\u03c8\u00b5(z) = log(1 + \u00b5)\n\u00b5 \u00b7 z,\nwhich is of the desirable linear order. However, as plotted in Figure 5, the equivalent loss is non-convex with respect to the margin \u03b8\u22a4X. Thus Proposition 3 no longer applies, and practically, the loss may induce bad local minima. This also justifies the choice of the squared margin loss in (Raj and Bach, 2022). In the following proposition, it establishes that for the margin loss, if the induced equivalent loss is convex, then any differentiable margin-based uncertainty function must be constant.\nProposition 4. Consider the margin loss and an uncertainty function that can be expressed by U(\u03b8;X) = h(|\u03b8\u22a4X |) where h(\u00b7) is a non-increasing, non-negative, and piece-wise differentiable function. Then h(\u00b7) must be a constant function,\nh(\u00b7) \u2261 C\nfor some C > 0 if the equivalent loss is continuous and convex.\nThe non-decreasing requirement is natural for that we want to assign a larger uncertainty value to a sample with a smaller margin. The proposition gives a negative result on designing uncertainty functions for the margin loss in that there does not exist a non-trivial uncertainty function that retains the convexity structure for the equivalent loss. While Proposition 3 and Theorem 2 provide positive results on establishing the convergence rate of the uncertainty sampling algorithm, Proposition 4 and Example 3 give negative results on the non-convexity issue associated with some uncertainty functions.\nGoing beyond analyzing the existing uncertainty functions, we can apply the machinery to derive\nnew uncertainty functions such as the following example.\nExample 5 (Exponential loss with exponential uncertainty). The loss function and the uncertainty function are defined by\nl(\u03b8; (X,Y )) = exp(\u2212Y \u00b7 \u03b8\u22a4X).\nU\u00b5(\u03b8;X) = exp(\u2212\u00b5|\u03b8\u22a4X |).\nThe equivalent loss takes a similar shape as the exponential loss:\nl\u0303\u00b5(\u03b8; (X,Y )) =\n \n\n1 1+\u00b5 \u00b7 exp(\u2212(1 + \u00b5)Y \u00b7 \u03b8\u22a4X) + \u00b5 1+\u00b5 , if Y \u00b7 \u03b8\u22a4X \u2265 0;\n1 1\u2212\u00b5 \u00b7 exp(\u2212(1\u2212 \u00b5)Y \u00b7 \u03b8\u22a4X)\u2212 \u00b5 1\u2212\u00b5 , if Y \u00b7 \u03b8\u22a4X < 0.\nThe link function for the surrogate property is\n\u03c8\u00b5(z) = 1 1\u2212 \u00b52 ( 1\u2212 \u00b5z \u2212 (1\u2212 z) 1+\u00b52 (1 + z) 1\u2212\u00b52 ) = z2 + o(z2) as z \u2192 0.\nSee Figure 6 for a visualization of these functions.\nWe note that this equivalent property not only maintains the convexity of the exponential loss but also exhibits a strong convexity when both \u0398 and X are bounded. This is a property that does not hold for equivalent losses derived upon margin-based loss but can be helpful in accelerating the convergence rate of gradient-based algorithms."
        },
        {
            "heading": "3.6 Numerical illustration",
            "text": "After previous theoretical discussions, we utilize a numerical example to demonstrate the equivalence between the uncertainty sampling and the equivalent loss and the convexity conditions. We adopt the synthetic data generation from Mussmann and Liang (2018b), where the feature points follow a mixture of two-dimensional Gaussian distributions. All the Gaussians. All Gaussians are of (0.5, 0.5) standard deviance, where the centers are located at 4 distinct positions: (\u22122, 0), (2, 0), (0,\u22122), (0, 2). The percentages of the four Gaussians are 20%, 30%, 40%, 10%, where the former two are aligned with positive labels while the latter two are negative. For each example, we start from random initialization, apply both the original loss minimization and the equivalent loss minimization algorithms on the synthetic data, and plot their final decision boundaries. As for the uncertainty sampling, we also choose the random initial points, set the step size to be small enough (10\u22124), and run sufficiently many iterations (107). The final decision boundaries obtained by the uncertainty sampling are compared with the two empirical risk minimization boundaries.\nFigure 7, 8, 9, 10, and 11 show the final decision boundaries obtained by different algorithms. We can observe that the uncertainty sampling algorithm achieves almost the same decision boundary as the equivalent loss minimization rather than the original loss. Besides, Figure 9 and 10 imply that their corresponding equivalent losses are non-convex and of local minimum, which coincides with our theoretical computation. A noteworthy fact is that although we show the non-convexity of the logistic regression model under the cross entropy loss and the probabilistic uncertainties, Figure 7a and 7b show that they might be of no local minimum or be able to avoid from being trapped into them."
        },
        {
            "heading": "4 Loss as Uncertainty: Multi-Class Classification and Regression",
            "text": "In the previous section, we discuss the problem of binary classification and propose the notion of equivalent loss to verify the properness of an uncertainty function. However, the discussion, along with the uncertainty functions, has been quite specialized to the problem of binary classification and therefore can be hardly applied to the more general multi-class classification and regression problems. In particular, for binary classification, the uncertainty function and the loss function can be expressed by a single-variable function of either the predicted probability q or the margin \u03b8\u22a4X . While this usually ensures the existence of the equivalent loss l\u0303, the structure no longer holds for multi-class classification and regression problems. In this section, we develop a general principle for designing uncertainty functions \u2013 \u201closs as uncertainty\u201d, which umbrellas binary classification, multi-class classification, and regression problems as special cases. The idea is, rather than handcrafting uncertainty functions case-by-case, we propose using conditional expected loss as the uncertainty function. Such an uncertainty function endows nice analytical properties for the learning problem, and it provides a guideline for the uncertainty quantification/calibration of a prediction model."
        },
        {
            "heading": "4.1 Loss as uncertainty",
            "text": "We first define the conditional loss which marginalizes Y given the feature X .\nDefinition 4 (Conditional loss). Define the conditional (expected) loss as\nL(\u03b8;X) := E [l(\u03b8; (X,Y ))|X ]\nwhere the expectation is taken with respect to the conditional distribution of Y |X with (X,Y ) \u223c P .\nNote that the conditional loss is a function of the parameter \u03b8 and the feature X . Suppose we let the uncertainty function simply be the conditional loss. Then we have the equivalent loss being exactly the square of the original loss.\nProposition 5. Suppose the uncertainty function U(\u03b8;X) = L(\u03b8;X). Then Algorithm 1 essentially performs stochastic gradient descent with respect to the loss function E[L\u0303(\u03b8;X)] where the expectation is with respect to X \u223c PX and the equivalent loss\nL\u0303(\u03b8;X) := 1\n2 (L(\u03b8;X))2.\nCompared to Proposition 1, the loss-as-uncertainty design performs SGD against a loss that marginalizes out the label Y . It results in a small twist in the proof, but it is not essential. Importantly, the result holds for all differentiable conditional loss L(\u03b8;X), and saves us from finding the solution to PDE (1) case-by-case. In other words, the result applies generally to the problem of binary classification, multi-class classification, and regression. It reduces the design of the uncertainty function to a calibration problem of estimating the conditional loss L(\u03b8;X). In terms of uncertainty sampling for regression problems, a similar uncertainty that measures conditional variance has already been proposed (Settles, 2009). Settles (2009) justifies such a variance uncertainty by showing the equivalence between variance and entropy under the Gaussian distribution assumption, while for more general distributions, the equivalence does not hold. We provide a different but more general explanation that the conditional variance is the conditional loss (when the estimation is the true conditional mean) regardless of the underlying distribution.\nWe provide the following two motivations for \u201closs as uncertainty\u201d: Convexity: The design retains the convexity of the original loss. Suppose that the original loss l is non-negative and convex. Then it leads to the non-negativity and convexity of the conditional loss L. Consequently,\n\u22022L\u0303\n\u2202\u03b82 = \u2202L \u2202\u03b8 \u00b7 ( \u2202L \u2202\u03b8 )\u22a4 + \u22022L \u2202\u03b82 0.\nMore generally, it is easy to verify that the convexity is still retained for L\u0303 if the uncertainty function\nU(\u03b8;X) = h(L(\u03b8;X)) for some non-decreasing and non-negative scalar function h(\u00b7). Existence of solution to (1): The PDE (1) becomes a multi-variate one for multi-class classification for that there will be one predicted probability for each class. And multi-variable functions generally do not have an indefinite integral, whereas the single-variable case is guaranteed by the fundamental theorem of calculus. If we aim to find a well-defined equivalent loss that always produces the same gradient as the uncertainty sampling in expectation, a necessary condition is that the path integral of its derivatives \u2211d\nj=1 U \u00b7 \u2202l\u2202\u03b8j d\u03b8j should depend not on the chosen path but only on the starting and the ending points. Assume that both U and l are smooth functions of \u03b8. From the basics of differential forms and algebraic topology (Bott et al., 1982), such a requirement is equivalent to finding some U such that the exchangeability holds, \u2202U\n\u2202\u03b8i \u00b7 \u2202l \u2202\u03b8j = \u2202U \u2202\u03b8j \u00b7 \u2202l \u2202\u03b8i , \u2200i 6= j,\nwhere a natural choice is U = h(l) such that h(\u00b7) has an anti-derivative. Consequently, this leads to the choice of h(\u00b7) as a non-decreasing and non-negative function with the special case of the identity function. We defer more discussions to Appendix B.5."
        },
        {
            "heading": "4.2 Oracle case",
            "text": "Now we analyze Algorithm 1 with the choice of U(\u03b8;X) = L(\u03b8;X). Here we assume the algorithm has an oracle access to L(\u03b8;X). Note that this entails the knowledge of the conditional distribution PY |X . In the next subsection, we analyze the case where such oracle is not available and one needs to calibrate the conditional loss to obtain an estimate of L(\u03b8;X).\nWith slight overload of notation, we write\nL(f) = E[l(f(X), X)|X ], L\u0303(f) = 1 2 (E[l(f(X), X)|X ])2 = 1 2 L2(f)\nfor some hypothesis f .\nAlso, without loss of generality, we assume the loss is non-negative. Then for any two hypotheses f\nand g, we have\nL\u0303(f)\u2212 L\u0303(g) = 1 2 L2(f)\u2212 1 2 L2(g) = 1 2 (L(f) + L(g)) (L(f)\u2212 L(g)) \u2265 1 2 (L(f)\u2212 L(g))2 (3)\nwhere the last inequality comes from the non-negativeness of L.\nProposition 6 (Loss as uncertainty). For any measurable hypothesis f , we have the following bound for U(\u03b8;X) = L(\u03b8;X),\nE[L(f)]\u2212 inf g\u2208G\nE[L(g)] \u2264 \u221a 2 (\nE[L\u0303(f)]\u2212 inf g\u2208G E[L\u0303(g)]\n)\nwhere G denotes the class of all measurable functions as before, and the expectation is taken with respect to X \u223c PX .\nProposition 6 presents the link function between L\u0303 and L, and this gives a handle of transforming a performance guarantee with respect to the squared conditional loss L\u0303 to that with respect to an original loss L. Then one can derive similar results as Proposition 3 and Theorem 2.\nFurthermore, a careful examination of the derivation in (3) leads to an improved link function, and\nconsequently a faster convergence rate. Let\ng\u2217 := argmin g\u2208G E[L(g)]\ndenotes the best measurable hypothesis, and\n\u01eb\u2217 := argmin x\u2208X E[l(g\u2217(X), X)|X = x]\nbe the pointwise minimum conditional risk. Then the following proposition expresses the link function with \u01eb\u2217.\nProposition 7 (Improved link function and convergence rate). Under the same setup as Proposition 6, we have\nE[L(f)]\u2212 inf g\u2208G E[L(g)] \u2264 2 \u01eb\u2217 \u00b7 ( E[L\u0303(f)]\u2212 inf g\u2208G E[L\u0303(g)] ) .\nThe error bound in Proposition 7 becomes smaller when \u01eb\u2217 grows larger, i.e., the data become more noisy and inseparable. This seems to contradict the results of (Mussmann and Liang, 2018a; Tifrea et al., 2022) that the data efficiency of uncertainty sampling algorithms is in strong negative correlation with the error rate of the final classifier. However, we should note that Proposition 7 is stated with respect to the excessive risks\u2019 relationships of any hypothesis f rather than the excessive risk itself, while the latter term is dealt by the SGD\u2019s convergence analysis as in Proposition 3. The convergence analysis is made with respect to the number of periods/observed features T rather than the number of queried samples. If the data become more separable from the decision boundary, the expected loss as the querying probability will decrease, leading to a smaller number of queries and higher data efficiency; thus it reconciles Mussmann and Liang (2018a)\u2019s observation.\nResults such as Proposition 6 and Proposition 7 are not restricted to the binary classification problem but are generally applicable to the multi-class classification problem and the regression problem. While the existing development of uncertainty sampling algorithms has mainly focused on the classification problem, few uncertainty measurements have been proposed for the regression problem. Our result here gives a pointer for such development; for example, one can use the estimated mean-squared error itself as the uncertainty measure for the regression problem."
        },
        {
            "heading": "4.3 Estimated loss and loss calibration",
            "text": "The analysis of the oracle case in previous can also be adapted to a setting where one uses the estimated conditional loss as uncertainty. Specifically, consider\nU(\u03b8;X) = L\u0302(\u03b8;X)\nwhere L\u0302(\u03b8;X) is an estimate of L(\u03b8;X). Then although the equivalent loss relation does not hold exactly, one can still analyze the estimation error of Algorithm 1.\nSuppose the estimates satisfy\nE\n[\n\u2223 \u2223L\u0302(\u03b8t;X)\u2212 L(\u03b8t;X) \u2223 \u2223\n]\n\u2264 \u03b4t (4)\nwhere the expectation is taken with respect to both \u03b8t and X \u223c PX that is independent of \u03b8t.\nTheorem 3 (Convergence rate under estimated conditional loss). Let U(\u03b8;X) = L\u0302(\u03b8;X) \u2208 [0, 1] such that (4) holds. Under the same condition as Proposition 3, Algorithm 1 yields the following bound\nE\n[ L\u0303(\u03b8\u0304T+1;X) ]\n\u2264 min \u03b8\u2208\u0398 E\n[ L\u0303(\u03b8;X) ] + GD\u221a T + D T\nT \u2211\nt=1\n\u03b4t,\nwhere the expectation is taken with respect to both X and \u03b8\u0304T+1.\nThe estimate L\u0302(\u03b8t;X) can be obtained from a separate validation dataset by adapting uncertainty quantification methods (Kuleshov et al., 2018; Kumar et al., 2019; H\u00fcllermeier and Waegeman, 2021; Foygel Barber et al., 2021). We note that compared to the model calibration literature, the condition (4) aims for an individual calibration objective in that it measures the calibration/estimation error for each X , and then takes expectation, rather than a population/average calibration or group calibration objective."
        },
        {
            "heading": "5 Pool-Based Setting",
            "text": "In this section, we analyze the uncertainty sampling algorithm under the pool-based setting and continue to adopt the conditional loss as the uncertainty function. Different from the stream-based setting, the features for all the samples are given at the beginning. To distinguish between the number of samples and the number of steps for the gradient descent algorithm, we use i = 1, . . . , n to index the samples and t = 1, . . . T to index the gradient descent time steps.\nAlgorithm 2 presents the pool-based uncertainty sampling algorithm. At each time step, the algorithm calculates the uncertainty for each sample in the data pool Dn given the current model parameter \u03b8t. Then the algorithm samples an index according to the probability distribution proportional to the uncertainty and queries the label of the sampled index. Based on this new label, the algorithm updates the model parameter via gradient descent."
        },
        {
            "heading": "5.1 Repeated-query v.s. single-query",
            "text": "For the pool-based setting in our paper, we consider a repeated-query setting where the learner may query the same sample Xi multiple times. Practically, this captures the situation where different human experts may provide different labels for the same sample feature X .\nProposition 8. With the uncertainty function U(\u03b8;X) = L(\u03b8;X) and a proper choice of the step size \u03b7t, Algorithm 2 essentially performs stochastic gradient descent to minimize\nEP\u0302n X\n[ L\u0303(\u03b8;X) ] := 1\nn\nn \u2211\ni=1\n(E [l(\u03b8; (X,Y ))|X = Xi])2 (5)\nwhere the subscript P\u0302nX denotes the empirical distribution of DXn .\nAlgorithm 2 Uncertainty sampling with gradient descent update (pool-based version) Input: Unlabeled dataset DXn = {Xi}ni=1, step size {\u03b7t}Tt=1 > 0, uncertainty function U(\u03b8;X) 1: Initialize \u03b81; \u03b8\u03041 \u2190 \u03b81 2: for t = 1, ..., T do 3: Calculate the uncertainty U(\u03b8t;Xi) for each Xi \u2208 Dn 4: Sample it \u2208 [n] according to the probability distribution \u221d U(\u03b8t;Xi) 5: Query a label Yt of Xit 6: Update the parameter via gradient descent\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7t \u00b7 \u2202l(\u03b8; (Xit , Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n7: \u03b8\u0304t+1 \u2190 (1\u2212 1t+1 )\u03b8\u0304t + 1t+1\u03b8t+1 8: end for\nOutput: \u03b8\u0304T+1\nWe remark that the choice of the step size involves an adjustment based on the normalizer of the\nprobability distribution,\nSt =\nn \u2211\ni=1\nU(\u03b8t;Xi).\nThis ensures the length of the step size does not scale with the uncertainty level. We defer more details to the proof in Appendix B.9.\nThe repeated-query setting is entailed by the objective (5), which optimizes the empirical conditional loss that marginalizes out Y. In theory, the analysis still goes through for the single-query setting, and accordingly, Algorithm 2 performs SGD to minimize\n1 n\nn \u2211\ni=1\n(l(\u03b8; (X,Y ))) 2 .\nBut this will require U(\u03b8;Xi) = l(\u03b8; (Xi, Yi)) for i \u2208 [n]. This uncertainty function is not as practical for that it depends on the realized label Yi, and thus it will be generally hard to estimate this quantity without observing Yi."
        },
        {
            "heading": "5.2 Theoretical analysis",
            "text": "We describe a general challenge in analyzing pool-based uncertainty sampling. The algorithm dynamic works as follows:\n\u03b8t \u2192 (Xt, Yt)\u2192 \u03b8t+1.\nAt each time t, we observe a new sample and use the sample to update the model parameter. If (Xt, Yt) is sampled uniformly from the data pool or from the distribution P , it can be viewed as an exogenous randomness. Such an exogeneity provides great convenience in analyzing the convergence behavior of \u03b8t under online algorithms. However, for the uncertainty sampling algorithm, the parameter \u03b8t determines the uncertainty value and consequently the sampling distribution of (Xt, Yt); and this makes the update dynamics more complicated. In this light, our perspective of equivalent loss and the notion of loss as uncertainty becomes helpful. Specifically, while the sampling distribution of (Xt, Yt) bears dependence on the parameter \u03b8t, one can absorb the sampling distribution into the gradient and make the sample (Xt, Yt) exogenous again, but against an alternative objective of l\u0303 or L\u0303.\nTherefore, the error bound of \u03b8\u0304T+1 in Algorithm 2 can be derived in a few standard steps:\n\u2022 Establish a convergence result like Proposition 3 for Algorithm 2. Note that the objective here is\nthe empirical conditional loss but no longer the expected loss, but this will not change the nature of the analysis.\n\u2022 Develop a generalization argument to connect the empirical condition loss with the expected loss.\n\u2022 Use the link function argument to transform the excessive risk bound under L\u0303 to the original loss\nL or binary loss L01."
        },
        {
            "heading": "5.3 Numerical experiments",
            "text": "To show that our loss as uncertainty principle can be a practical option for the multi-class classification and the regression problems, we test our pool-based algorithm Algorithm 2 (denoted by active) on 5 UCI datasets (Kelly et al., 2021) in comparison with the uniform sampling algorithm (marked as passive). Our implementation of Algorithm 2 drops out the adjusting term St to simplify the step sizes to be constant. The source code and data can be found on https://github.com/liushangnoname/Uncertainty-Sampling.\nEstimation of loss: In order to get an estimation of the conditional expected loss, we carry out the non-parametric estimator in Liu et al. (2023) with a little adaptation to the active learning setting. Liu et al. (2023) is focused on supervised learning, where they split out an independent validation set to calibrate the error. Their argument is that the independence of the validation set is crucial to avoid an underestimated error, while in our active learning setting, we can still apply the \u201closs as uncertainty\u201d principle even if the error estimation is not calibrated as long as the estimation reflects the relative quantitative relationships. On the contrary, the preciousness of the labels encourages us to utilize every label for gradient descent training. We henceforth do not split the labels into validation and training in our active learning implementation.\nMulti-class classification: We test two types of classifiers: logistic regression with cross-entropy loss and support vector machine with margin loss. We choose 3 datasets where the linear classifiers get acceptable performance on prediction accuracy, named Dry Bean, Waveform Version 1, and Covertype. For the Covertype dataset, we randomly pick 10000 samples from the whole set. We run 30 trials, where the dataset is randomly split according to an 80-20 proportion for training and testing each time. For each trial, the uncertainty sampling and the uniform sampling share the same Gaussian initialization and the same constant step sizes. The averaged accuracy v.s. step numbers result is shown in Figure 12, 13, and 14. For the Dry Bean and the Covertype datasets, uncertainty sampling with the \u201closs as uncertainty\u201d principle outperforms uniform sampling, while for the Waveform dataset, the performances are similar.\nRegression: As for the regression problem, we test the kernelized linear regression model, where the kernel is chosen among linear, polynomial, and radial basis functions. Two datasets named Forest Fires and QSAR Aquatic Toxicity are examined, where the datasets are chosen so that the kernelized linear regression is of acceptable performance and computational cost. The results are shown in Figure 15. Although for the QSAR dataset, our uncertainty sampling does not achieve dominant performance, it still reaches the same level as the uniform sampling. For the Forest Fires, our algorithm shows its superiority to passive learning."
        },
        {
            "heading": "6 Other Variants of Uncertainty Sampling",
            "text": ""
        },
        {
            "heading": "6.1 Exponential loss as uncertainty",
            "text": "Now we explore an alternative choice for the uncertainty function for Algorithm 2 where\nU(\u03b8;X) = exp (L(\u03b8;X)) .\nTo generate some intuitions, we first make some derivations under the oracle case where we have direct access to EP\u0302n [l(\u03b8; (X,Y ))|X = Xi] = l(\u03b8; (Xi, Yi)). To simplify the notation, we abbreviate the gradient we take at time step t to gt:\ngt := \u2202l(\u03b8; (Xit , Yat))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n.\nIf we define the uncertainty as the exponential of the conditional expected loss and utilize the structure\nof the softmax distribution, then the conditional expectation of the gradient gt is\nE [gt|Ft] = n \u2211\ni=1\nexp ( l(\u03b8t; (Xi, Yi)) ) \u00b7 \u2207\u03b8l(\u03b8t; (Xi, Yi)) \u2211n\nj=1 exp ( l(\u03b8t; (Xj , Yj)) ) = \u2207\u03b8\n(\nlog (\nn \u2211\ni=1\nexp ( l(\u03b8t; (Xi, Yi)) )\n)\n)\n.\nBy viewing the overall equivalent loss as the log-sum-exp (softmax) function\nL\u0303 := log (\nn \u2211\ni=1\nexp ( l(\u03b8; (Xi, Yi)) )\n)\n,\nwe have\nE [gt|Ft] = \u2207\u03b8L\u0303(\u03b8t).\nProposition 9. With the uncertainty function U(\u03b8;X) = exp(L(\u03b8;X)), Algorithm 2 essentially performs\nstochastic gradient descent to minimize\nlog (\nn \u2211\ni=1\nexp ( L(\u03b8;X) )\n)\n.\nWe note that the objective in Proposition 9 is risk-sensitive rather than risk-neutral such as expectation. In the following section, we continue to study two more variants of uncertainty sampling that relate to the risk profile and robustness of the underlying loss.\n6.2 Top-k-max uncertainty sampling\nSome variant of the uncertainty sampling algorithm queries the most uncertain samples, in replacement of the sampling step in Algorithm 2. Algorithm 3 describes such a variant: at each time step, the algorithm randomly picks one of the m most uncertain samples and queries the sample. Then the algorithm performs a gradient descent step based on the queried sample.\nAlgorithm 3 Top-k-max uncertainty sampling (pool-based setting) Input: Unlabeled dataset Dn = {Xi}ni=1, step size {\u03b7t}Tt=1 > 0, uncertainty function U(\u03b8;X), m \u2208 N 1: Initialize \u03b81 2: for t = 1, ..., T do 3: Calculate the uncertainty U(\u03b8t;Xi) for each Xi \u2208 Dn 4: Let {it1, ...., itn} be a permutation of {1, 2, ..., n} such that\nU(\u03b8t;Xit1) \u2265 U(\u03b8t;Xit2) \u2265 \u00b7 \u00b7 \u00b7 \u2265 U(\u03b8t;Xitn)\n5: Randomly sample it from {it1, ...., itm} \u2013 the m largest uncertainty indices 6: Query a label Yt of Xit 7: Update the parameter via gradient descent\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7t \u00b7 \u2202l(\u03b8; (Xit , Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n8: end for\nOutput: \u03b8T+1\nProposition 10. With the uncertainty function U(\u03b8;X) = L(\u03b8;X), Algorithm 3 essentially performs stochastic gradient descent to minimize\nCVaR\u03b1 P\u0302n\nX\n(E [l(\u03b8; (X,Y ))|X ]) (6)\nwhere the subscript P\u0302nX denotes the empirical distribution of DXn and the risk level \u03b1 = mn .\nProposition 10 gives the objective of Algorithm 3 when using the conditional loss as the uncertainty\nfunction. Here the conditional value-at-risk is defined by\nCVaR\u03b1Q(\u03be) := E [ \u03be \u2223 \u2223\u03be \u2265 Q\u22121\u03b1 (\u03be) ]\nwhere the underlying random variable \u03be follows the distribution Q, and Q\u22121\u03b1 (\u03be) denotes the \u03b1-quantile of \u03be. Note that Algorithm 3 uses loss as uncertainty, and by querying the most uncertain samples, it focuses on the samples with the largest conditional loss, which naturally leads to the CVaR objective. We remark that the CVaR is a risk-sensitive objective rather than a risk-neutral one such as expectation/average.\nWhile the result is presented for the oracle case of U(\u03b8;X) = L(\u03b8;X), we may expect similar risksensitive behavior for the uncertainty sampling algorithm when the used U(\u03b8;X) is strongly correlated with L(\u03b8;X). Also, the risk level \u03b1 = mn partly explains why the arg-max strategy (where m = 1) may have volatile behavior: it may focus on the very tail part of the loss."
        },
        {
            "heading": "6.3 Distributionally robust optimization as a variant of uncertainty sampling",
            "text": "In this section, we establish some equivalence between distributionally robust optimization under \u03c72divergence and a variant of uncertainty sampling. Algorithm 4 implements a mixture of uniform sampling and uncertainty sampling (Algorithm 3). At each time step, the algorithm queries a sample uniformly randomly with probability 1\u2212\u03b3, and follows the top-k-max uncertainty sampling with probability \u03b3. It is a natural algorithm in that it softly combines uncertainty sampling with the standard learning procedure of uniform sampling.\nAlgorithm 4 Mixture of uniform and uncertainty sampling (pool-based setting) Input: Unlabeled dataset Dn = {Xi}ni=1, step size {\u03b7t}Tt=1 > 0, uncertainty function U(\u03b8;X), threshold \u03b3 \u2208 (0, 1), m \u2208 N\n1: Initialize \u03b81 2: for t = 1, ..., T do 3: Generate \u03bet \u223c Unif[0, 1] 4: if \u03bet \u2264 1\u2212 \u03b3 then 5: %% with probability 1\u2212 \u03b3, do uniform sampling 6: Randomly pick an index it from {1, 2, ..., n} 7: else 8: %% with probability \u03b3, do uncertainty sampling 9: Calculate the uncertainty U(\u03b8t;Xi) for each Xi \u2208 Dn\n10: Let {it1, ...., itn} be a permutation of {1, 2, ..., n} such that\nU(\u03b8t;Xit1) \u2265 U(\u03b8t;Xit2) \u2265 \u00b7 \u00b7 \u00b7 \u2265 U(\u03b8t;Xitn)\n11: Randomly sample it from {it1, ...., itm} \u2013 the m largest uncertainty indices 12: end if 13: Query a label Yt of Xit 14: Update the parameter via gradient descent\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b7t \u00b7 \u2202l(\u03b8; (Xit , Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n15: end for Output: \u03b8T+1\nProposition 11. With the uncertainty function U(\u03b8;X) = L(\u03b8;X), Algorithm 4 essentially optimize the following distributionally robust objective\nmax p\u2208U(P)\nn \u2211\ni=1\npi \u00b7 E[l(\u03b8; (X,Y ))|X = Xi], (7)\nwhere U(P) is the ambiguity set for the probability vector p = (p1, ..., pn). It is defined by\nU(P) := { p\n\u2223 \u2223 \u2223 \u2223 \u2223 n \u2211\ni=1\npi = 1, 0 \u2264 pi \u2264 m+ (n\u2212m)\u03b3\nmn ,D\u03c6\n(\np\n\u2225 \u2225 \u2225 ( 1\nn , . . . ,\n1 n )\u22a4) \u2264 \u03b3 2n(n\u2212m) 2nm\n}\n,\nwhere D\u03c6(p\u2016q) = \u2211n i=1 qi\u03c6 ( pi qi ) is the \u03c6-divergence with \u03c6(z) = 12 (z \u2212 1)2.\nProposition 11 gives the objective of the mixture of uncertainty sampling and uniform sampling. Note that when we employ loss as uncertainty, the samples with larger losses will be more frequently sampled and optimized over. This intuition is aligned with the design of the above distributionally robust optimization formulation which assigns larger weights to samples with larger losses. There is a small difference between these two in that uncertainty sampling uses the conditional expected loss whereas the robust objective uses the empirical loss, yet the difference is not essential. The distributionally robust objective (7) bears certain equivalence to the variance regularized objective (Namkoong and Duchi, 2017; Duchi et al., 2021)\n1 n\nn \u2211\ni=1\nL(\u03b8;Xi) +\n\u221a\n\u03b32(n\u2212m) m \u00b7 VarP\u0302n X (L(\u03b8;X)),\nwhere the latter objective (called variance regularized empirical risk) can act as a high probability upper bound for the population risk (Bartlett et al., 2002; Maurer and Pontil, 2009). To avoid a vain upper bound, the theory of distributionally robust optimization suggests a choice of \u03b3 = \u221a\nCm n(n\u2212m) so that\n\u03b32(n\u2212m) m = C n , and this will render Algorithm 4 a strong tendency to the uniform sampling."
        },
        {
            "heading": "A Derivation of equivalent losses and surrogate link function",
            "text": "This section will present the detailed calculations of the equivalent losses and the surrogate link functions of all the listed examples in previous sections. The subscript of \u00b5 or \u03b3 will sometimes be omitted for simplicity when the text is clear."
        },
        {
            "heading": "A.1 Equivalent loss in Section 3.2",
            "text": "Example 1 (Equivalent loss of (Dagan and Engelson, 1995; Culotta and McCallum, 2005)). Both the loss and the uncertainty function can be expressed as a function of predicted probability q(X ; \u03b8). By the chain rule, \u2202l\u0303\n\u2202\u03b8 = \u2202l\u0303 \u2202q \u00b7 \u2202q \u2202\u03b8 ,\nU \u00b7 \u2202l \u2202\u03b8 = U \u00b7 \u2202l \u2202q \u00b7 \u2202q \u2202\u03b8 .\nHence if we can find some l\u0303 such that \u2202l\u0303\n\u2202q = U \u00b7 \u2202l \u2202q ,\nthen we have accomplished the task. The indicator function 1{Y = +1} where Y \u2208 {\u22121,+1} can be transformed into Y+12 which we denote as p by a slightly abuse of notations. Then the derivative of the original cross-entropy loss can be presented as \u2202l\n\u2202q = \u2212p q + 1\u2212 p 1\u2212 q = q \u2212 p q(1\u2212 q) .\nWe start with the entropy uncertainty U = \u2212q log(q)\u2212 (1\u2212 q) log(1\u2212 q) in Dagan and Engelson (1995).\nU(q) \u00b7 \u2202l \u2202q = p log(q)\u2212 (1\u2212 p) log(1\u2212 q)\u2212 (1\u2212 p) \u00b7 q log(q) 1\u2212 q + p \u00b7 (1\u2212 q) log(1\u2212 q) q\n= p log(q)\u2212 (1\u2212 p) log(1\u2212 q)\u2212 (1\u2212 p) \u00b7 (q \u2212 1) log(q) + log(q) 1\u2212 q + p \u00b7 \u2212q log(1\u2212 q) + log(1\u2212 q) q = p log(q)\u2212 (1\u2212 p) log(1\u2212 q) + (1\u2212 p) log(q) \u2212 (1\u2212 p) \u00b7 log(q) 1\u2212 q \u2212 p log(1\u2212 q) + p \u00b7 log(1\u2212 q) q = log(q)\u2212 log(1\u2212 q)\u2212 (1 \u2212 p) \u00b7 log(q) 1\u2212 q + p \u00b7 log(1\u2212 q) q .\nThen by calculating its indefinite integral, we have\n\u222b\nU(q) \u00b7 \u2202l \u2202q dq = q log(q) + (1\u2212 q) log(1\u2212 q)\u2212 p \u00b7 Li2(q)\u2212 (1\u2212 p) \u00b7 Li2(1 \u2212 q) + C,\nwhere Li2(z) is the Spence\u2019s function,\nLi2(z) = \u2212 \u222b z\n0\nlog(1\u2212 z) z dz.\nSince we are interested in the excessive risk (which is the expected difference between those hypotheses and the optimal measurable function), the selection of C does not matter. We simply select C = Li2(1) = \u03c02\n6 to make the equivalent loss vanish at p = q = 0 and p = q = 1, which yields the equivalent loss l\u0303\npresented in Section 3.2.\nWe continue with the least confident uncertainty U = min{q, 1\u2212q} in Culotta and McCallum (2005).\nFor q \u2208 [0, 12 ], we have\nU(q) \u00b7 \u2202l \u2202q = q \u2212 p 1\u2212 q\n= q \u2212 1 + 1\u2212 p\n1\u2212 q\n= \u22121 + 1\u2212 p 1\u2212 q .\nIts indefinite integral is simple:\n\u222b\nU(q) \u00b7 \u2202l \u2202q dq = \u2212q \u2212 (1\u2212 p) log(1 \u2212 q) + C, \u2200q \u2208 [0, 0.5].\nSimilarly, we can compute the indefinite integral for q \u2208 [0.5, 1]: \u222b\nU(q) \u00b7 \u2202l \u2202q dq = q \u2212 p log(q) + C, \u2200q \u2208 [0.5, 1].\nThe equivalent loss function is piece-wise continuous. We select the constants properly to avoid the jump discontinuity at q = 12 . To let the values at q = 1 2 match each other, we select the constants so that\nl\u0303 =\n \n \u2212(1\u2212 p) \u00b7 log(2(1\u2212 q))\u2212 q + log(2), if q < 0.5; \u2212p \u00b7 log(2q)\u2212 (1\u2212 q) + log(2), if q \u2265 0.5.\nAgain, we don\u2019t choose the overall constant deliberately. The log(2) term is simply to make the equivalent loss vanish at p = q = 0 and p = q = 1.\nExample 2 (Equivalent loss of (Raj and Bach, 2022)). For the SVM-based methods, both the loss and the uncertainty function can be expressed as a function of Y \u00b7 Y\u0302 , where Y\u0302 = \u03b8\u22a4X . By the similar chain rule arguments in Example 1, we can find the equivalent loss with respect to \u03b8 as long as we can find that with respect to Y \u00b7 Y\u0302 . To simplify the notations, we denote Y \u00b7 Y\u0302 = Y \u03b8\u22a4X by s. As a reminder, we again state the squared Hinge loss\nl(s) =\n \n (1\u2212 s)2, if s \u2264 1; 0, if s \u2265 1,\nand the uncertainty function\nU\u00b5(s) =\n \n (1 \u2212 \u00b5s)\u22121, if s \u2264 0; (1 + \u00b5s)\u22121, if s \u2265 0.\nWe compute the amount U \u00b7 \u2202l\u2202s and its indefinite integral in three parts. For s \u2265 1, the result is straightforward: the equivalent loss must be a constant. We select the constant to be zero for some notation convenience. For s \u2208 [0, 1],\nU\u00b5(s) \u00b7 \u2202l \u2202s = \u22122(1\u2212 s) \u00b7 1 1 + \u00b5s\n= \u22122 \u2212 1\u00b5 (\u00b5s+ 1) + 1\u00b5 + 1\n1 + \u00b5s\n= 2 \u00b5 \u2212 2( 1 \u00b5 + 1) \u00b7 1 1 + \u00b5s .\nIts indefinite integral is\n\u222b\nU\u00b5(s) \u00b7 \u2202l\n\u2202s ds =\n2 \u00b5 \u00b7 s\u2212 2 \u00b5 ( 1 \u00b5 + 1) \u00b7 log(1 + \u00b5s) + C,\nwhere we select C = 2\u00b5 \u2212 2\u00b5 ( 1\u00b5 + 1) \u00b7 log(1 + \u00b5) so that the values at s = 1 coincide. For s \u2264 0, we can complete the calculation similarly:\nU\u00b5(s) \u00b7 \u2202l \u2202s = \u22122(1\u2212 s) \u00b7 1 1\u2212 \u00b5s\n= \u22122 \u2212 1\u00b5 (1\u2212 \u00b5s)\u2212 1\u00b5 + 1\n1\u2212 \u00b5s\n= \u2212 2 \u00b5 + 2( 1 \u00b5 \u2212 1) \u00b7 1 1\u2212 \u00b5s .\nThe indefinite integral is\n\u222b\nU\u00b5(s) \u00b7 \u2202l \u2202s ds = \u2212 2 \u00b5 \u00b7 s\u2212 2 \u00b5 ( 1 \u00b5 \u2212 1) \u00b7 log(1\u2212 \u00b5s) + C,\nwhere the constant is selected to be the same as s \u2208 [0, 1] to match at s = 0.\nExample 3 (Equivalent loss of Tifrea et al. (2022)). The uncertainty function is probably the simplest case: an indicator function of whether |s| = |Y \u00b7 Y\u0302 | = |Y \u03b8\u22a4X | is no greater than a certain threshold \u03b3. Then for those s\u2019s that satisfy the threshold requirement, the equivalent loss is identical to the original loss (which is the logistic loss, as a reminder), while for those s\u2019s outside the threshold area, the equivalent loss must be constant. We select those constants to avoid abrupt changes at the threshold, resulting in the expressions in Section 3.2.\nExample 4 (Equivalent loss of margin loss and margin-based uncertainty). We recall that the original loss and the uncertainty function w.r.t. s = Y\u0302 \u00b7 Y are\nl(s) = max{0, 1\u2212 s},\nU\u00b5(s) =\n \n\n1 1+\u00b5s , if s \u2265 0;\n1 1\u2212\u00b5s , if s \u2264 0.\nFor the s \u2265 1 part, the indefinite integral must be constant. We select the constant to be zero. For the s \u2208 (0, 1) part,\nU\u00b5(s) \u00b7 \u2202l \u2202s = \u2212 1 1 + \u00b5s ,\nwhich indicates that \u222b\nU\u00b5(s) \u00b7 \u2202l \u2202s ds = \u2212 1 \u00b5 log(1 + \u00b5s) + C, \u2200s \u2265 0.\nWe select the constant to be 1\u00b5 log(1 + \u00b5) so that there is no discontinuity at s = 1. For the s \u2264 0 part, U\u00b5(s) \u00b7 \u2202l\n\u2202s = \u2212 1 1\u2212 \u00b5s,\nresulting in \u222b\nU\u00b5(s) \u00b7 \u2202l\n\u2202s ds =\n1 \u00b5 log(1\u2212 \u00b5s) + C, \u2200s \u2265 0.\nWe set the constant to be 1\u00b5 log(1 + \u00b5) to keep the continuity at s = 0.\nExample 5 (Equivalent loss of exponential loss and exponential uncertainty). Similarly, we state the\noriginal loss and the uncertainty function concerning s:\nl(s) = exp(\u2212s),\nU\u00b5(s) =\n \n exp(\u2212\u00b5s), if s \u2265 0; exp(\u00b5s), if s \u2264 0.\nThen, for s \u2265 0, U\u00b5(s) \u00b7 \u2202l\n\u2202s = \u2212 exp(\u2212(1 + \u00b5)s),\nof which the indefinite integral is\n\u222b\nU\u00b5(s) \u00b7 \u2202l\n\u2202s ds = exp(\u2212(1 + \u00b5)s) 1 + \u00b5 + C, \u2200s \u2265 0.\nWe select C = \u00b51+\u00b5 so that the value at s = 0 is 1. On the contrary, for s \u2264 0, U\u00b5(s) \u00b7 \u2202l\n\u2202s = \u2212 exp(\u2212(1\u2212 \u00b5)s),\nleading to \u222b\nU\u00b5(s) \u00b7 \u2202l\n\u2202s ds = exp(\u2212(1\u2212 \u00b5)s) 1\u2212 \u00b5 + C, \u2200s \u2264 0.\nThe constant is chosen to be C = \u2212 \u00b51\u2212\u00b5 to meet the value at s = 0.\nA.2 Surrogate property and proof of Proposition 2\nIn this subsection, we summarize the arguments in Bartlett et al. (2006) and provide their surrogate link function computation method for the margin-based models such as the SVM. Such a surrogate property induces a mini-max optimal bound on the excessive 0-1 risk (see Theorem 3 in Bartlett et al. (2006)). For simplicity, in this subsection, we omit the dependence on X and \u03b8, since all the excessive risk analyses hold for any certain but fixed hypothesis f\u03b8 and sample point X = x.\nWe start with the standard definitions of Bartlett et al. (2006). Assume that the loss l(Y\u0302 , Y ) is of\nthe form l(Y\u0302 \u00b7 Y ) (which is the case in all of our examples). By denoting the probability of a positive Y by p, the expected loss induced by predicting Y\u0302 is\nCp(Y\u0302 ) := pl(Y\u0302 ) + (1\u2212 p)l(\u2212Y\u0302 ).\nFor any fixed probability value p, the inferior of the expected loss is denoted by\nH(p) := inf Y\u0302 Cp(Y\u0302 ).\nIf we restrict the prediction Y\u0302 to be not Bayes-optimal (that is, to be of the different sign as 2p\u2212 1) and take the inferior, we get\nH\u2212(p) := inf Y\u0302 \u00b7(2p\u22121)\u22640 Cp(Y\u0302 ).\nNote that a binary classification loss l is said to be classification-calibrated (Bartlett et al., 2006) (or Fisher consistent (Lin, 2004)) if H\u2212(p) > H(p) for any p 6= 12 . Bartlett et al. (2006) provide a way of computing the surrogate link function \u03c8 : [0, 1]\u2192 R via\n\u03c8\u0303(z) = H\u2212 ( 1 + z\n2\n) \u2212H ( 1 + z\n2\n)\n,\n\u03c8(z) = \u03c8\u0303\u2217\u2217(z),\nwhere g\u2217\u2217 is the Fenchel-Legendre biconjugate of the function g, characterized by\nepi g\u2217\u2217 = co epi g.\nNote that those functions are convex if and only if their Fenchel-Legendre biconjugate are themselves (Bartlett et al., 2006).\nEquipped with such a surrogate link function \u03c8, Bartlett et al. (2006)\u2019s Theorem 3 shows that it can be an upper bound for the excessive 0-1 risk: for any measurable function f and any probability distribution on X \u00d7 Y = X \u00d7 {\u22121,+1},\n\u03c8\n(\nL01(f) \u2212 inf g\u2208G L01(g)\n)\n\u2264 E [l(f(X), Y )]\u2212 inf g\u2208G E [l(g(X), Y )] ,\nwhere G is the set of all measurable functions. Such an upper bound is mini-max optimal in the sense that for any non-negative loss l, any |X | \u2265 2, any 0-1 risk level \u03b6 \u2208 [0, 1], and any precision \u01eb > 0, there exists a probability distribution on X\u00d7{\u22121,+1} such that L01(f) \u2212 infg\u2208G L01(g) = \u03b6, and\n\u03c8(\u03b6) \u2264 E [l(f(X), Y )]\u2212 inf g\u2208G E [l(g(X), Y )] \u2264 \u03c8(\u03b6) + \u01eb.\nEquipped with such powerful tools, all we need to do is to find the surrogate link functions of those active learning models. But before we proceed to the particular calculation, we notice that the analysis in Bartlett et al. (2006) is designed for the margin-based models, while our Example 1 is not based on the margin but on the probability. To generalize the arguments to the probabilistic models, we transform the probability into the expectation to enable the margin-based analysis. We denote the predicted expectation of Y in a probabilistic model by\nY\u0302 := E\u0302[Y ] = 2q \u2212 1.\nExample 1 (Surrogate link function of Dagan and Engelson (1995); Culotta and McCallum (2005)). Remind that the original loss can be expressed as\nl(Y\u0302 \u00b7 Y ) = \u2212 log ( 1 + Y\u0302 \u00b7 Y 2 ) .\nThe entropy uncertainty is\nU = \u2212[q log(q) + (1\u2212 q) log(1\u2212 q)]\n= \u2212 [ 1 + Y\u0302\n2 log\n(\n1 + Y\u0302\n2\n)\n+ 1\u2212 Y\u0302\n2 log\n(\n1\u2212 Y\u0302 2\n)]\n= \u2212 [ 1 + Y\u0302 \u00b7 Y 2 log ( 1 + Y\u0302 \u00b7 Y 2 ) + 1\u2212 Y\u0302 \u00b7 Y 2 log ( 1\u2212 Y\u0302 \u00b7 Y 2 )] .\nThen the equivalent loss is\nl\u0303 = Li2(1)\u2212 Li2 ( 1 + Y\u0302 \u00b7 Y 2 ) + 1 2 [ (1 + Y\u0302 \u00b7 Y ) log ( 1 + Y\u0302 \u00b7 Y 2 ) + (1\u2212 Y\u0302 \u00b7 Y ) log ( 1\u2212 Y\u0302 \u00b7 Y 2 )] ,\nwhere Li2(\u00b7) is the Spence\u2019s function. One can take an easy check that this loss is actually identical to the equivalent loss we provide in Section 3.2 if Y\u0302 = 2q \u2212 1.\nNotice that U is a non-negative even function that only takes zero value at two endpoints, which implies that minimizing expected l\u0303 is equivalent to minimizing expected l. The minimizer Y\u0302 \u2217 can be easily obtained at the first-order stationary point\np \u00b7 ( \u2212 1 1 + Y\u0302 \u2217 ) + (1\u2212 p) \u00b7 ( 1 1\u2212 Y\u0302 \u2217 ) = 0,\nwhich is Y\u0302 \u2217 = 2p\u2212 1. Then\nH(p) = Li2(1)\u2212 [pLi2(p) + (1\u2212 p)Li2(1\u2212 p)] + [p log(p) + (1\u2212 p) log(1\u2212 p)] ,\nwhere Li2(\u00b7) is the Spence\u2019s function. The computation of H\u2212(p) is simple: the equivalent loss is convex, indicating that the inferior risk of the non-Bayes classifiers must be taken at Y\u0302 = 0. Therefore,\nH\u2212(p) = Li2(1)\u2212 Li2 ( 1\n2\n)\n\u2212 log(2).\nBy definition,\n\u03c8\u0303(z) = \u2212Li2 ( 1\n2\n)\n+ 1 + z\n2 \u00b7Li2\n(\n1 + z\n2\n)\n+ 1\u2212 z 2 \u00b7Li2\n(\n1\u2212 z 2\n) \u2212 [ 1 + z 2 \u00b7 log(1 + z) + 1\u2212 z 2 \u00b7 log(1\u2212 z) ] ,\nwhose second-order derivative is\nd2\u03c8\u0303\ndz2 = \u22121 2\n[\n(1\u2212 z) log ( 1\u2212z 2 ) + (1 + z) log ( 1+z 2 )\n1\u2212 z2\n]\n\u2265 0.\nThe convexity implies that\n\u03c8(z) = \u03c8\u0303(z).\nWe need to note that the first-order derivative of \u03c8 is\nd\u03c8 dz = 1 2\n[\nLi2\n(\n1 + z\n2\n) \u2212 Li2 ( 1\u2212 z 2 )] \u2265 0,\nwhich is zero if and only if z = 0. So the equivalent loss is classification-calibrated, and the surrogate link function around z = 0 is approximately\n\u03c8(z) \u223c d 2\u03c8\ndz2\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n\u00b7 z2 = log(2) \u00b7 z2.\nSince \u03c8(z) is bounded at z \u2208 [0, 1], we can conclude that \u03c8(z) = \u0398(z2), where \u0398 is the big theta notation referring to \u201cof the same order as\u201d rather than our denoted set of parameters.\nThe other example of the least confidence uncertainty U = min{q, 1 \u2212 q} can also be analyzed via Y\u0302 = 2q \u2212 1. By definition,\nU = min\n{\n1 + Y\u0302 2 , 1\u2212 Y\u0302 2\n}\n= 1\u2212 |Y\u0302 |\n2 .\nThe equivalent loss with respect to Y\u0302 \u00b7 Y is\nl\u0303(Y\u0302 \u00b7 Y ) =\n \n\n1 2\n( Y\u0302 \u00b7 Y \u2212 2 log(1 + Y\u0302 \u00b7 Y ) ) + log(2)\u2212 12 , if Y\u0302 \u00b7 Y \u2265 0; \u2212 12 \u00b7 Y\u0302 \u00b7 Y + log(2)\u2212 12 , if Y\u0302 \u00b7 Y \u2264 0.\nAgain, one can quickly check that this equivalent loss is identical to the form we present in Section 3.2 with Y\u0302 = 2q\u2212 1. We don\u2019t bother to adjust those constants explicitly to meet the non-negativity or any other requirements, since those equivalent losses are all bounded and we are interested in the excessive risk (which is one expected loss minus another).\nW.l.o.g. assume that p \u2265 12 . Then the first-order stationary point of Cp(Y\u0302 ) should be\n\u22121 2 \u00b7 p \u00b7 1\u2212 Y\u0302\n\u2217\n1 + Y\u0302 \u2217 +\n1 2 (1\u2212 p) = 0,\nwhich is Y\u0302 \u2217 = 2p\u2212 1. Then\nH(p) = p\u00b7 1 2 [(2p\u2212 1)\u2212 2 log(2p)]\u2212(1\u2212p)\u00b7 1 2 (1\u22122p)+log(2)\u2212 1 2 = p\u2212 1 2 \u2212p log(2p)+log(2)\u2212 1 2 , \u2200p \u2265 1 2 .\nFor p \u2264 12 , the optimal Y\u0302 \u2217 remains the same 2p\u2212 1, while \u2200p \u2264 12 ,\nH(p) = \u2212p \u00b7 1 2 (2p\u2212 1) + (1 \u2212 p) \u00b7 1 2 [(1\u2212 2p)\u2212 2 log(2(1\u2212 p))] + log(2)\u2212 1 2\n= 1 2 \u2212 p\u2212 (1 \u2212 p) log(2(1\u2212 p)) + log(2)\u2212 1 2 .\nBy the convexity of l\u0303,\nH\u2212(p) = Cp(0) = log(2)\u2212 1\n2 .\nThe derivation of \u03c8\u0303 only requires the p \u2265 12 part, hence\n\u03c8\u0303(z) = H\u2212 ( 1 + z\n2\n) \u2212H ( 1 + z\n2\n)\n= \u22121 2 z + 1 + z 2 log(1 + z),\nof which the second-order derivative is\nd2\u03c8\u0303 dz2 =\n1\n2(1 + z) \u2265 0.\nBy the convexity of \u03c8\u0303, we have\n\u03c8 = \u03c8\u0303.\nFrom the first-order derivative of \u03c8\nd\u03c8 dz = 1 2 log(1 + z),\nwe know that the surrogate link function \u03c8 is only tending to zero if and only if z itself tends zero. Thus, the equivalent loss is classification-calibrated. From the facts that\nd\u03c8 dz\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n= 0\nand d2\u03c8\ndz2\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n= 1\n2 ,\nwe know that\n\u03c8(z) \u223c 1 2 z2\naround the zero point. From the boundedness of \u03c8, we can also conclude similarly to the entropy uncertainty case that\n\u03c8(z) = \u0398(z2),\nwhere the big theta notation means \u201cof the same order as\u201d.\nExample 2 (Surrogate link function of Raj and Bach (2022)). We start with finding the Y\u0302 that minimizes the expected equivalent loss. Remind that the equivalent loss can be written in the form of Y\u0302 \u00b7 Y :\nl\u0303\u00b5 =\n   \n    \u2212 2\u00b5 ( 1\u00b5 \u2212 1) log(1 \u2212 \u00b5Y\u0302 \u00b7 Y )\u2212 2\u00b5 Y\u0302 \u00b7 Y + C, if Y\u0302 \u00b7 Y \u2264 0; \u2212 2\u00b5 ( 1\u00b5 + 1) log(1 + \u00b5Y\u0302 \u00b7 Y ) + 2\u00b5 Y\u0302 \u00b7 Y + C, if Y\u0302 \u00b7 Y \u2208 (0, 1); 0, if Y\u0302 \u00b7 Y \u2265 1,\nwhere C = 2\u00b5 ( 1 \u00b5 + 1) log(1 + \u00b5)\u2212 2\u00b5 . By the definition,\nU\u00b5 \u00b7 \u2202l \u2202Y\u0302 = \u2202l\u0303\u00b5 \u2202Y\u0302 .\nSince U\u00b5 = (1 + \u00b5|Y\u0302 |)\u22121 is a positive and even function, minimizing the expected equivalent loss is identical to minimizing the expected original loss (which is, the squared Hinge loss). By direct calculation (or referring the Example 2 in Bartlett et al. (2006)), the minimizer should be\nY\u0302 \u2217 = 2p\u2212 1.\nWithout loss of generality, we assume p \u2265 12 , which implies that 2p\u2212 1 \u2265 0. Subject to that minimizer,\nH(p) = C\n+ p \u00b7 ( \u2212 2 \u00b5 ( 1 \u00b5 + 1 ) log(1 + \u00b5(2p\u2212 1)) + 2 \u00b5 (2p\u2212 1) ) + (1\u2212 p) \u00b7 (\n\u2212 2 \u00b5\n(\n1 \u00b5 \u2212 1 ) log(1 + \u00b5(2p\u2212 1)) + 2 \u00b5 (2p\u2212 1) ) .\nSince the equivalent loss is convex, the minimized risk of the non-Bayes classifier must be\nH\u2212(p) = Cp(0) = C.\nHence we have\n\u03c8\u0303(z) = 1 + z 2 \u00b7 ( 2 \u00b5 ( 1 \u00b5 + 1 ) log(1 + \u00b5z) + 2 \u00b5 z ) + 1\u2212 z 2 \u00b7 ( 2 \u00b5 ( 1 \u00b5 \u2212 1 ) log(1 + \u00b5z) + 2 \u00b5 z )\n= 2 \u00b52 \u00b7 (1 + \u00b5z) log(1 + \u00b5z)\u2212 2 \u00b5 z.\nThe second-order derivative of (\u0303\u03c8) is\nd2\u03c8\u0303 dz2 =\n2\n1 + \u00b5z > 0,\nwhich guarantees the convexity of \u03c8\u0303. Hence\n\u03c8 = \u03c8\u0303.\nThe first-order derivative of \u03c8 is d\u03c8\ndz =\n2 \u00b5 log(1 + \u00b5z) \u2265 0,\nwhere the equality holds if and only if z = 0 for any \u00b5 > 0, indicating the classification-calibration of the equivalent loss l\u0303. By a similar Taylor expansion argument, we can conclude that\n\u03c8(z) \u223c d 2\u03c8\ndz2\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n\u00b7 z2 = 2z2.\nDue to the boundedness of the surrogate link function, we have\n\u03c8(z) = \u0398(z2),\nwhere the big theta notation stands for \u201cof the same order as\u201d.\nExample 3 (Surrogate link function of Tifrea et al. (2022)). We briefly recall the equivalent loss with respect to Y\u0302\nl\u0303\u03b3 =\n    \n    log(1 + exp(\u03b3)), if Y \u00b7 Y\u0302 \u2264 \u2212\u03b3; log(1 + exp(\u2212Y \u00b7 Y\u0302 )), if Y \u00b7 Y\u0302 \u2208 (\u2212\u03b3, \u03b3); log(1 + exp(\u2212\u03b3)), if Y \u00b7 Y\u0302 \u2265 \u03b3,\nwhere the non-constant part is identical to that of a logistic loss. For sufficiently large threshold \u03b3 so that the minimizer locates in the non-constant part, we compute the first-order condition of the minimizer (which is just that of the logistic loss) as\n\u2212p \u00b7 exp(\u2212Y\u0302 \u2217)\n1 + exp(\u2212Y\u0302 \u2217) + (1\u2212 p) \u00b7 exp(Y\u0302\n\u2217)\n1 + exp(Y\u0302 \u2217) = 0,\nwhich implies that\nY\u0302 \u2217 = log\n(\np\n1\u2212 p\n)\n.\nFor a small \u03b3, the derivative of the expected equivalent loss suggests that the minimizer should be\nY\u0302 \u2217 = \u03b3 \u00b7 sign(2p\u2212 1).\nWithout loss of generality, we assume that p \u2265 12 . Then\nY\u0302 \u2217 =\n \n \u03b3, if p \u2265 exp(\u03b3)1+exp(\u03b3) ; log (\np 1\u2212p\n)\n, if 12 \u2264 p \u2264 exp(\u03b3) 1+exp(\u03b3) .\nSubstituting above results into Cp(Y\u0302 ), we have\nH(p) =\n \n\n\u2212[p log(p) + (1 \u2212 p) log(1\u2212 p)], if 12 \u2264 p \u2264 exp(\u03b3) 1+exp(\u03b3) ; log(1 + exp(\u03b3))\u2212 \u03b3 exp(\u03b3)1+exp(\u03b3) , if p \u2265 exp(\u03b3) 1+exp(\u03b3) .\nOne can check that Cp(0) \u2264 Cp(Y\u0302 ) for any p \u2265 12 and Y\u0302 \u2264 0, implying that\nH\u2212(p) = Cp(0) = log(2).\nThen\n\u03c8\u0303(z) =\n \n\n1 2 [(1 + z) log(1 + z) + (1\u2212 z) log(1\u2212 z)] , if z \u2264 exp(\u03b3)\u22121 exp(\u03b3)+1 ; log (\n2 1+exp(\u03b3)\n)\n+ \u03b3 exp(\u03b3)1+exp(\u03b3) , if z \u2265 exp(\u03b3)\u22121 exp(\u03b3)+1 .\nApparently, \u03c8\u0303(z) is non-convex as a whole: in the first part where z is small, the function is convex and strictly increasing, while in the second part, the function is a constant. We extend the values of \u03c8\u0303(z) from small z\u2019s to large z\u2019s by defining another function\nh(z) := 1\n2 [(1 + z) log(1 + z) + (1\u2212 z) log(1\u2212 z)] .\nTo compute \u03c8(z), observe that the convex hull of the epigraph of \u03c8\u0303 can be determined by some specific point z0 \u2264 exp(\u03b3)\u22121exp(\u03b3)+1 : at the left side of z0, the epigraph is identical to that of h(z), while at the right side of z0, the epigraph is identical to that of the tangent at (z0, h(z0)). Such a tangent should contain the right-most point (1, \u03c8\u0303(1)), which means\nh(z0) + h \u2032(z0) \u00b7 (1\u2212 z0) = \u03c8\u0303(1).\nReplacing the equation with concrete expressions, we have\nlog(1+ z0) = h(z0)+h \u2032(z0) \u00b7 (1\u2212 z0) = \u03c8\u0303(1) = log(2)\u2212 log\n(\nexp\n(\n\u03b3\n1 + exp(\u03b3)\n)\n+ exp ( \u2212\u03b3 1 + exp(\u2212\u03b3) )) .\nSimplifying notations, we have\nz0 = 2 \u00b7 ( exp (\n\u03b3\n1 + exp(\u03b3)\n)\n+ exp ( \u2212\u03b3 1 + exp(\u2212\u03b3) ))\u22121 \u2212 1.\nTherefore,\n\u03c8(z) =\n \n 1 2 [(1 + z) log(1 + z) + (1\u2212 z) log(1\u2212 z)] , if z \u2264 z0; 1 2 [(1 + z) log(1 + z0) + (1 \u2212 z) log(1\u2212 z0)] , if z \u2265 z0,\nwhere z0 is some positive constant stated above. By examining the first-order derivative of \u03c8(z), we can easily find out that the equivalent loss is classification-calibrated:\nd\u03c8 dz =\n \n 1 2 [log(1 + z)\u2212 log(1\u2212 z)] , if z \u2264 z0; 1 2 [log(1 + z0)\u2212 log(1\u2212 z0)] , if z \u2265 z0.\nBy computing its Taylor expansions at z = 0, we have\n\u03c8(z) \u223c d 2\u03c8\ndz2\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n\u00b7 z2 = z2.\nFinally, we note that\n\u03c8(z) = \u0398(z2),\nwhere the big theta notation suggests \u201cat the same order as\u201d.\nExample 4 (Surrogate link function of margin loss and margin-based uncertainty). Similarly, the even\nand positive uncertainty function U leads to the same minimizer of the expected equivalent loss as the expected original margin loss, while the latter by the arguments in Bartlett et al. (2006) is\nY\u0302 \u2217 = sign\n(\np\u2212 1 2\n)\n,\nfor p 6= 12 . For p = 12 , any Y\u0302 \u2208 [\u22121, 1] will lead to the same expected equivalent loss. We compute the p \u2265 12 part, gaining\nH(p) = p \u00b7 0 + (1 \u2212 p) \u00b7 2 \u00b5 log(1 + \u00b5) = (1\u2212 p) \u00b7 2 \u00b5 log(1 + \u00b5), \u2200p \u2265 1 2 .\nThe other part p < 12 is\nH(p) = p \u00b7 2 \u00b5 log(1 + \u00b5) + (1\u2212 p) \u00b7 0 = p \u00b7 2 \u00b5 log(1 + \u00b5), \u2200p < 1 2 .\nFor computing the H\u2212(p), assume that p \u2265 12 . Then any Y\u0302 \u2208 [\u22121, 0] will be optimal among the non-Bayes classifiers, leading to\nH\u2212(p) = 2\n\u00b5 log(1 + \u00b5).\nHence,\n\u03c8\u0303(z) = H\u2212 ( 1 + z\n2\n) \u2212H ( 1 + z\n2\n)\n= log(1 + \u00b5)\n\u00b5 z.\nThe linear function is of course convex, so\n\u03c8 = \u03c8\u0303.\nExample 5 (Surrogate link function of exponential loss and exponential uncertainty). The equivalent loss concerning Y\u0302 \u00b7 Y is\nl\u0303 =\n \n\n\u2212 11+\u00b5 \u00b7 exp(\u2212(1 + \u00b5)Y\u0302 \u00b7 Y ) + \u00b5 1+\u00b5 , if Y\u0302 \u00b7 Y \u2265 0; \u2212 11\u2212\u00b5 \u00b7 exp(\u2212(1\u2212 \u00b5)Y\u0302 \u00b7 Y )\u2212 \u00b5 1\u2212\u00b5 , if Y\u0302 \u00b7 Y \u2264 0.\nSince the uncertainty function U = exp(\u2212|Y\u0302 |) is even and positive, the minimizer of the expected equivalent loss Cp(Y\u0302 ) is identical to that of the expected original loss. That is,\n\u2212p exp(\u2212Y\u0302 \u2217) + (1\u2212 p) exp(Y\u0302 \u2217) = 0,\nwhich implies that\nY\u0302 \u2217 = 1\n2 log\n(\np\n1\u2212 p\n)\n.\nWithout loss of generality, assume that p \u2265 12 . Then\nH(p) = p\n[\n\u2212 1 1 + \u00b5 \u00b7 exp ( \u22121 + \u00b5 2 log ( p 1\u2212 p )) + \u00b5 1 + \u00b5 ]\n+ (1\u2212 p) [ \u2212 1 1\u2212 \u00b5 \u00b7 exp ( 1\u2212 \u00b5 2 log ( p 1\u2212 p )) \u2212 \u00b5 1\u2212 \u00b5 ]\n= 2 1\u2212 \u00b52 \u00b7 p 1\u2212\u00b5 2 (1 \u2212 p) 1+\u00b52 + \u00b5 1\u2212 \u00b52 \u00b7 (2p\u2212 1\u2212 \u00b5).\nSince the equivalent loss is convex with respect to Y\u0302 \u00b7 Y , the minimum of expected equivalent loss when\nthe prediction is non-Bayes is\nH\u2212(p) = Cp(0) = 1.\nThen by definition,\n\u03c8\u0303(z) = H\u2212 ( 1 + z\n2\n) \u2212H ( 1 + z\n2\n)\n= 1 1\u2212 \u00b52 ( 1\u2212 \u00b5z \u2212 (1 \u2212 z) 1+\u00b52 (1 + z) 1\u2212\u00b52 ) .\nThe first-order derivative is\nd\u03c8\u0303 dz = \u2212 \u00b5 1\u2212 \u00b52 \u2212 1 2(1 + \u00b5)\n(\n1\u2212 z 1 + z\n) 1+\u00b5 2\n+ 1\n2(1\u2212 \u00b5)\n(\n1 + z 1\u2212 z\n) 1\u2212\u00b5 2\n,\nwhich is zero at z = 0. The second-order derivative is\nd2\u03c8\u0303 dz2 = 1 1\u2212 z2 \u00b7 (1 \u2212 z) \u2212 1\u2212\u00b52 (1 + z)\u2212 1+\u00b5 2 \u2265 0,\nwhich implies two facts: \u03c8\u0303(z) tends to zero if and only if z tends to zero, and \u03c8\u0303(z) is convex (henceforth \u03c8 = \u03c8\u0303). Thus, the equivalent loss is classification-calibrated.\nFrom the facts that d\u03c8\ndz\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n= 0\nand d2\u03c8\ndz2\n\u2223 \u2223 \u2223 \u2223 \u2223\nz=0\n= 1,\nwe can say that\n\u03c8(z) \u223c z2\naround z = 0. Due to the boundedness of \u03c8, we have\n\u03c8(z) = \u0398(z2),\nwhere the big theta notation is \u201cof the same order as\u201d."
        },
        {
            "heading": "A.3 Convexity and Proof of Proposition 4",
            "text": "In this subsection, we examine how the convexity requirements are fulfilled in the listed examples.\nExample 1 (Convexity of Dagan and Engelson (1995); Culotta and McCallum (2005)). W.l.o.g. we still assume Y = 1 to ease the burden of notations. For the entropy uncertainty, remind that we have already shown its partial derivative with respect to Y\u0302 by\n\u2202l\u0303\n\u2202Y\u0302 =\n1 2 log\n(\n1 + Y\u0302\n2\n)\n+ 1 2 \u00b7 1\u2212 Y\u0302 1 + Y\u0302 log\n(\n1\u2212 Y\u0302 2\n)\n.\nContinue to compute its partial derivative, we have\n\u22022 l\u0303\n\u2202Y\u0302 2 = \u2212 1 (1 + Y\u0302 )2 \u00b7 log\n(\n1\u2212 Y\u0302 2\n)\n\u2265 0,\nwhich ensures its convexity with respect to Y\u0302 .\nFor the least confidence uncertainty, the partial derivative is\n\u2202l\u0303\n\u2202Y\u0302 =\n \n \u2212 12 \u00b7 1\u2212Y\u03021+Y\u0302 , if Y\u0302 \u2265 0; \u2212 12 , if Y\u0302 \u2264 0,\nwhich implies that l\u0303 is at least C1 continuous with respect to Y\u0302 . Furthermore,\n\u22022 l\u0303\n\u2202Y\u0302 2 =\n \n\n1 (1+Y\u0302 )2 , if Y\u0302 > 0; 0, if Y\u0302 < 0.\nTherefore, the equivalent loss is convex with respect to Y\u0302 .\nWe have shown the convexity with respect to Y\u0302 for both cases. If Y\u0302 is linear with respect to \u03b8, then we can further conclude that the convexity regarding \u03b8 holds. But unlike the margin-based classifiers, the probabilistic models restrict that Y\u0302 \u2208 (\u22121, 1), where a popular model is the logistic regression model that predicts Y\u0302 = exp(\u03b8\n\u22a4X)\u22121 exp(\u03b8\u22a4X)+1 . Unlike the original cross-entropy loss, the equivalent loss under the logistic\nregression model is no longer convex with respect to the parameter \u03b8.\nExample 2 (Convexity of Raj and Bach (2022)). Since the model is linear in the sense that Y\u0302 = \u03b8\u22a4X , we only need to check the convexity with respect to Y\u0302 . First, assuming Y = 1, the derivative of l\u0303 with respect to Y\u0302 is\n\u2202l\u0303\u00b5 \u2202Y\u0302 =\n    \n   \n2(Y\u0302 \u22121) 1+\u00b5Y\u0302 , if Y\u0302 \u2264 0; 2(Y\u0302 \u22121) 1\u2212\u00b5Y\u0302 , if Y\u0302 \u2208 (0, 1); 0, if Y\u0302 \u2265 1.\nWe can see that l\u0303 is C1 continuous with respect to Y\u0302 . We further compute that\n\u22022 l\u0303\u00b5 \u2202Y\u0302 2 =\n   \n   \n2(1+\u00b5)\n(1+\u00b5Y\u0302 )2 , if Y\u0302 < 0;\n2(1\u2212\u00b5) (1\u2212\u00b5Y\u0302 )2 , if Y\u0302 \u2208 (0, 1); 0, if Y\u0302 > 1.\nHence the model is convex but not strongly convex.\nExample 3 (Convexity of Tifrea et al. (2022)). The equivalent loss is non-convex for Y\u0302 since it is a truncated logistic loss outside a region, where the truncation is to set the loss to be a constant. By the linearity of Y\u0302 on \u03b8, the model is also non-convex for \u03b8.\nExample 4 (Nonconvexity of margin loss and margin-based uncertainty). Proof of Proposition 4. Since the model is linear, we only need to examine the case where l\u0303 is convex w.r.t. Y\u0302 . At the differentiable parts, the second-order derivative of the equivalent loss w.r.t. Y\u0302 is\n\u22022 l\u0303\n\u2202Y\u0302 2 =\n\u2202\n\u2202Y\u0302\n(\n\u2202l\u0303\n\u2202Y\u0302\n)\n= \u2202U \u2202Y\u0302 \u00b7 \u2202l \u2202Y\u0302 + U \u00b7 \u2202 2l \u2202Y\u0302 2 = \u2202U \u2202Y\u0302 \u00b7 \u2202l \u2202Y\u0302 ,\nsince the Hinge loss l is piece-wise linear w.r.t. Y\u0302 . For any fixed Y\u0302 , the actual outcome Y could possibly\nbe either +1 or \u22121, indicating that\n\u2202l\n\u2202Y\u0302 =\n    \n    +1, if Y\u0302 > \u22121, Y = \u22121; \u22121, if Y\u0302 < +1, Y = +1; 0, otherwise.\nAt the positive part Y\u0302 > 0, the uncertainty function is non-increasing, which restricts the term \u2202U \u2202Y\u0302 to be non-positive. But for the case Y = \u22121, the convexity requires the term \u2202U \u2202Y\u0302 to be non-negative. Henceforth \u2202U\n\u2202Y\u0302 = 0,\nwhich implies that the uncertainty function must be piece-wise constants. To further ensure that U must be only one constant, we observe that the equivalent loss is now piece-wise linear with non-increasing slopes for Y\u0302 > 0 if Y = \u22121. In order to keep the loss continuous and convex, the slope must be constant everywhere.\nExample 5 (Convexity of exponential loss and exponential uncertainty). Similar to the arguments in Example 2, we only need to compute the second-order derivatives (w.l.o.g. assume Y = 1):\n\u22022 l\u0303\u00b5 \u2202Y\u0302 2 =\n \n (1 + \u00b5) \u00b7 exp(\u2212(1 + \u00b5)Y\u0302 ), if Y\u0302 < 0; (1\u2212 \u00b5) \u00b7 exp(\u2212(1\u2212 \u00b5)Y\u0302 ), if Y\u0302 > 0.\nThe convexity thus holds."
        },
        {
            "heading": "A.4 Lipschitzness in Section 5",
            "text": "What is different from the stream-based case is the excessive equivalent risk decomposition, due to the distributions from which the SGD\u2019s samples are drawn. For the stream-based setting, the algorithm receives a newly drawn sample X from the underlying distribution PX , while for the pool-based setting, the sample set DXn is determined and the sampling distribution is the empirical distribution P\u0302nX . As a consequence, the excessive risk for any loss function l(f ; (X,Y )) (which can be transformed into the excessive risk for the conditional expectation L(f ;X) = EY [l(f ; (X,Y ))]) should be decomposed into five terms rather than two:\nE [ l(f\u0302 ; (X,Y )) ] \u2212 E [ l(g\u2217; (X,Y )) ] = E [ L(f\u0302 ;X) ] \u2212 E [ L(g\u2217;X) ]\n= E [ L(f\u0302 ;X) ] \u2212 1 n\nn \u2211\ni=1\nL ( f\u0302 ;Xi )\n(generalization)\n+ 1\nn\nn \u2211\ni=1\nL ( f\u0302 ;Xi ) \u2212 inf f\u2208F\n1 n\nn \u2211\ni=1\nL ( f ;Xi ) (optimization)\n+ inf f\u2208F\n1 n\nn \u2211\ni=1\nL ( f ;Xi ) \u2212 1 n\nn \u2211\ni=1\nL ( f\u2217;Xi ) (non-positive)\n+ 1\nn\nn \u2211\ni=1\nL ( f\u2217;Xi ) \u2212 E [ L(f\u2217;X) ]\n(concentration)\n+ E [ L(f\u2217;X) ] \u2212 E [ L(g\u2217;X) ]\n(approximation).\nAmong the above five terms, the non-positive term and the concentration term can be dealt with easily: the non-positive term can be discarded immediately, and the concentration term can be handled by either\nthe standard concentration arguments to yield a high probability bound or the same as the generalization term. In this paper, we cope with the concentration term in the same way as the generalization term.\nWhat matters most now remains three terms: generalization, optimization, and approximation. As in the stream-based setting, we do not discuss the approximation term in this paper, since it is beyond the scope of choosing the uncertainty function. We simply assume that there is no model misspecification so that the approximation term is zero. The optimization can be dealt with easily with the convexity condition as we do in Proposition 3. For the remaining generalization term, we summarize an easy-tocheck criterion.\nTo begin with, we briefly review the classical statistical learning theory. The estimator f\u0302 we get in any algorithm is dependent on the data points DXn , so we cannot directly get the generalization bound via the concentration inequalities that rely on the i.i.d. condition. To deal with such a dependence, classical statistical learning theory usually proves the uniform convergence to establish an upper bound on the generalization term. A popular way to uniform convergence is to compute the Rademacher complexity. The Rademacher complexity of a hypothesis class F on X can be defined as\nRn(F) := E [\nsup f\u2208F\n1 n\nn \u2211\ni=1\n\u03c3if(Xi)\n]\n,\nwhere \u03c3i\u2019s are n i.i.d. samples from the uniform distribution on {\u22121,+1} and Xi\u2019s are n i.i.d. samples from the distribution P on X . If we further define the loss class as\nLL\u25e6F := {x 7\u2192 L(f ;x)|f \u2208 F},\nthen a well-known high-probability upper bound for the generalization term (for example, see Theorem 5 in Bousquet et al. (2003)) is that for any \u03b4 > 0, the following holds with probability at least 1\u2212 \u03b4:\n\u2200f \u2208 F , \u2223 \u2223 \u2223\n\u2223 \u2223\nE [ L(f ;X) ] \u2212 1 n\nn \u2211\ni=1\nL(f ;Xi)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2264 2Rn(LL\u25e6F ) +\n\u221a\nlog(2\u03b4 )\n2n . (8)\nThe 1\u2212 \u03b4 high-probability bound 8 can handle the generalization term and the concentration term easily with an upper bound of 4Rn(LL\u25e6F) +O( log(1/\u03b4)n ).\nThe next question is: how to quickly get an upper bound on the Rademacher complexity of a loss class? We hope that Rn(LL\u25e6F ) can be converted to Rn(F), since the Rademacher complexity of a function class F is generally easier to compute. For example, for a linear function class with parameter L2 norm upper bound M\u0398 and feature space L2 upper bound MX , the Rademacher complexity is upper bounded by M\u0398M\u0307X\u221a n .\nLuckily, if we can ensure the \u03b2L-Lipschitzness of the conditionally expected loss L w.r.t. f , then by Ledoux-Talagrand\u2019s contraction inequality (see Corollary 3.17 in Ledoux and Talagrand (1991)), we have\nRn(LL\u25e6F ) \u2264 \u03b2LRn(F). (9)\nLipschitz condition of those examples in Section 3 are verified in Appendix A.4. As for the \u201closs as uncertainty\u201d principle in Section 4, we note that the equivalent loss L\u0303 = 12L 2 +C is ML \u00b7 \u03b2L-Lipschitz if the original loss L is \u03b2L-Lipschitz and bounded by ML.\nWe start to check the Lipschitz condition for the equivalent loss in the examples. We recall that\n\u2202l\u0303 \u2202Y\u0302 = U \u00b7 \u2202l \u2202Y\u0302 .\nTherefore, due to the fact that the uncertainty U \u2208 [0, 1], we have \u2223\n\u2223 \u2223 \u2223\n\u2202l\u0303\n\u2202Y\u0302\n\u2223 \u2223 \u2223 \u2223 \u2264 \u2223 \u2223 \u2223 \u2223 \u2202l\n\u2202Y\u0302\n\u2223 \u2223 \u2223 \u2223 ,\nwhich implies the following:\nLemma 1. If the uncertainty function U \u2208 [0,MU ] and the original loss l(Y\u0302 , Y ) is differentiable and \u03b2-Lipschitz with respect to Y\u0302 , then the equivalent loss l\u0303(Y\u0302 , Y ) is MU \u00b7 \u03b2-Lipschitz with respect to Y\u0302 .\nMoreover, the uncertainty function U is usually decreasing to be near zero when |Y\u0302 | is large enough, which counteracts the effects of the rapid growth of many popular loss functions when Y\u0302 \u00b7 Y is negative and far enough from zero. To see this, we have a closer look at the probabilistic model in Example 1.\nExample 1 (Lipschitzness of Dagan and Engelson (1995); Culotta and McCallum (2005)). The original cross-entropy loss is not Lipschitz on the range Y\u0302 \u2208 (\u22121, 1) (or equivalently, q \u2208 (0, 1)), since the derivative of the negative logarithm will explode near the zero point. But from direct computation, for the entropy uncertainty (Dagan and Engelson, 1995), we have (w.l.o.g. assume Y = 1)\n\u2202l\u0303 \u2202Y\u0302 = U \u00b7 \u2202l \u2202Y\u0302\n= \u2212 [ 1 + Y\u0302\n2 log\n(\n1 + Y\u0302\n2\n)\n+ 1\u2212 Y\u0302\n2 log\n(\n1\u2212 Y\u0302 2\n)]\n\u00b7 ( \u2212 1 1 + Y\u0302 )\n= 1\n2 log\n(\n1 + Y\u0302\n2\n)\n+ 1 2 \u00b7 1\u2212 Y\u0302 1 + Y\u0302 log\n(\n1\u2212 Y\u0302 2\n)\n.\nSince the first-order partial derivative \u2202l\u0303 \u2202Y\u0302 is non-positive and monotonically increasing, we only need to check the limit case Y\u0302 \u2192 \u22121+ to examine the Lipschitzness. We have \u2223\n\u2223 \u2223 \u2223\n\u2202l\u0303\n\u2202Y\u0302\n\u2223 \u2223 \u2223 \u2223 \u223c 1 2 \u2223 \u2223 \u2223 \u2223 log(1 + Y\u0302 ) \u2223 \u2223 \u2223 \u2223 ,\nwhich is much smaller than the original loss\n\u2223 \u2223 \u2223 \u2223 \u2202l\n\u2202Y\u0302\n\u2223 \u2223 \u2223 \u2223 \u223c \u2223 \u2223 \u2223 \u2223 \u2212 1 1 + Y\u0302 \u2223 \u2223 \u2223 \u2223 ,\nsince by l\u2019H\u00f4pital\u2019s rule,\nlim Y\u0302 \u2192\u22121+\n\u2202l\u0303\n\u2202Y\u0302\n/\n\u2202l\n\u2202Y\u0302 = lim Y\u0302 \u2192\u22121+\n(\n\u2212 1 1 + Y\u0302\n)/\n(\n\u2212 1 (1 + Y\u0302 )2\n)\n= lim Y\u0302 \u2192\u22121+ (1 + Y\u0302 ) = 0.\nAlthough we cannot say that the equivalent loss is Lipschitz with respect to the whole (\u22121, 1), for any compact subset of (\u22121, 1), the equivalent loss is Lipschitz. We shall see that the Lipschitz constant is reduced compared to the original loss.\nAs for the least confidence uncertainty, the situation is even better: the equivalent loss is Lipschitz\nover the entire set Y\u0302 \u2208 (\u22121, 1). To see this, we w.l.o.g. assume Y = 1, and the equivalent loss is\nl\u0303(Y\u0302 \u00b7 Y ) =\n \n\n1 2\n( Y\u0302 \u2212 2 log(1 + Y\u0302 ) ) , if Y\u0302 \u2265 0; \u2212 12 \u00b7 Y\u0302 , if Y\u0302 \u2264 0.\nIts partial derivative is\n\u2202l\u0303\n\u2202Y\u0302 =\n \n \u2212 12 \u00b7 1\u2212Y\u03021+Y\u0302 , if Y\u0302 \u2265 0; \u2212 12 , if Y\u0302 \u2264 0,\nwhich implies that the equivalent loss is 12 -Lipschitz.\nExample 2 (Lipschitzness of Raj and Bach (2022)). From direct computation, the partial derivative w.r.t. Y\u0302 can be upper-bounded by \u2225\n\u2225 \u2225 \u2225 \u2225 \u2202l\u0303\n\u2202Y\u0302\n\u2225 \u2225 \u2225 \u2225 \u2225 \u2264 2 \u00b5 .\nBy assuming an almost upper bound MX on the feature space X , the equivalent loss is of course 2MX\u00b5 - Lipschitz w.r.t. \u03b8.\nExample 3 (Lipschitzness of Tifrea et al. (2022)). By the property of the logistic loss, the equivalent loss must be 1-Lipschitz w.r.t. Y\u0302 . Hence the equivalent loss is MX -Lipschitz w.r.t. \u03b8.\nExample 4 (Lipschitzness of margin loss and margin-based uncertainty). The equivalent loss is 1- Lipschitz w.r.t. Y\u0302 , which indicates its MX -Lipschitzness w.r.t. \u03b8.\nExample 5 (Lipschitzness of exponential loss and exponential uncertainty). The prediction Y\u0302 = \u03b8\u22a4X has an upper bound of\n|Y\u0302 | \u2264MX \u00b7M\u0398,\nwhere MX is the almost sure upper bound for X and M\u0398 is the upper bound for \u0398. Then the equivalent loss has an upper bound for its partial derivative w.r.t. Y\u0302 of exp ( (1\u2212\u00b5)MX \u00b7M\u0398 ) . The final Lipschitzness constant w.r.t. \u03b8 is MX \u00b7 exp ( (1 \u2212 \u00b5)MX \u00b7M\u0398 ) ."
        },
        {
            "heading": "B Proofs and Discussions",
            "text": ""
        },
        {
            "heading": "B.1 Proof of Proposition 1",
            "text": "Proof. Denote the \u03c3-field generated by \u03b8t by Ft. The general requirement for the SGD update to hold is that\nE [\u03b8t+1 \u2212 \u03b8t|Ft] = \u2212\u03b7t \u00b7 \u2202l\u0303\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n,\nwhere \u03b7t is the step size. To prove such a requirement, we first see that the only randomness that will affect \u03b8t+1 conditioned on Ft is 1{\u03bet \u2264 U(\u03b8t;Xt)}\nthat has a conditional expectation of\nE [1{\u03bet \u2264 U(\u03b8t;Xt)}|Ft] = U(\u03b8t;Xt).\nFrom the definition that\n\u03b8t+1 = \u03b8t \u2212 \u03b7t \u00b7 1{\u03bet \u2264 U(\u03b8t;Xt)} \u00b7 \u2202l\u0303\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n,\nwe can conclude the proof."
        },
        {
            "heading": "B.2 Proof of Proposition 3",
            "text": "Proof. To ease the notation, we denote 1{\u03bet \u2264 U(\u03b8t;Xt)} \u00b7 \u2202l(\u03b8;(Xt,Yt))\u2202\u03b8 \u2223 \u2223 \u03b8=\u03b8t by gt. By Proposition 1, we have\nE\u03bet [gt|\u03b8t] = \u2207\u03b8 l\u0303(\u03b8t; (Xt, Yt)).\nTake the expectation with respect to (Xt, Yt), we see that gt is further applying SGD directly on the expected equivalent loss\nE\u03bet,(Xt,Yt) [gt|\u03b8t] = E(Xt,Yt) [ \u2207\u03b8 l\u0303(\u03b8t; (Xt, Yt)) \u2223 \u2223 \u2223\u03b8t ] = \u2207\u03b8E(X,Y )[l\u0303(\u03b8t; (X,Y ))|\u03b8t].\nDenote E(X,Y )[l\u0303](\u03b8; (X,Y )) by R(\u03b8). Then from the definition, we have\n\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 = \u2016\u03b8t \u2212 \u03b7t \u00b7 gt \u2212 \u03b8\u2217\u20162\n= \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 2\u03b7t \u00b7 g\u22a4t (\u03b8t \u2212 \u03b8\u2217) + \u03b72t \u2016gt\u20162 = \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 2\u03b7t \u00b7 (gt \u2212\u2207R(\u03b8t) +\u2207R(\u03b8t))\u22a4 (\u03b8t \u2212 \u03b8\u2217) + \u03b72t \u2016gt\u20162 \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 2\u03b7t \u00b7 (gt \u2212\u2207R(\u03b8t))\u22a4 (\u03b8t \u2212 \u03b8\u2217) + 2\u03b7t \u00b7 (R(\u03b8\u2217)\u2212R(\u03b8t)) + \u03b72t \u2016gt\u20162,\nwhere the last inequality follows from the convexity of R(\u00b7) such that R(\u03b8\u2217) \u2265 R(\u03b8t)+\u2207R(\u03b8t)\u22a4(\u03b8\u2217\u2212 \u03b8t). Assume that the parameters sequence {\u03b8t}t\u22651 is adapted to an increasing sequence of \u03c3-fields {Ft}t\u22651. Since \u03b8t+1 is completely determined by \u03b8t, \u03bet, and (Xt, Yt), taking the expectation conditioned on Ft is equivalent to taking the expectation w.r.t. \u03bet and (Xt, Yt) conditioned on knowing \u03b8t. By taking the expectation w.r.t. Ft, we have\nE [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2223 \u2223Ft ]\n\u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 2\u03b7t \u00b7 E [(gt \u2212\u2207R(\u03b8t))|Ft]\u22a4 (\u03b8t \u2212 \u03b8\u2217) + 2\u03b7t \u00b7 (R(\u03b8\u2217)\u2212R(\u03b8t)) + \u03b72tE [ \u2016gt\u20162 \u2223 \u2223Ft ] = \u2016\u03b8t \u2212 \u03b8\u2217\u20162 + 2\u03b7t \u00b7 (R(\u03b8\u2217)\u2212R(\u03b8t)) + \u03b72tE [ \u2016gt\u20162 \u2223 \u2223Ft ] .\nBy rearranging the terms, we have\nR(\u03b8t) \u2264 R(\u03b8\u2217) + \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 E[\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162|Ft]\n2\u03b7t + \u03b7t 2 \u00b7 E [ \u2016gt\u20162 \u2223 \u2223Ft ] , (10)\nwhere\nE [ \u2016gt\u20162 \u2223 \u2223Ft ] =\n\n 1{\u03bet \u2264 U(\u03b8t;Xt)} \u00b7 \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2202l(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n\u2225 \u2225 \u2225 \u2225 \u2225 2 + 1{\u03bet > U(\u03b8t;Xt)} \u00b7 0\n\n\n\u2264 G2.\nSumming up inequality 10 from t = 1 to T + 1 and taking the unconditional expectation on both sides, by the tower property of the conditional expectation we have\n1\nT + 1\nT+1 \u2211\nt=1\nE [R(\u03b8t)] \u2264 R(\u03b8\u2217) + \u2016\u03b81 \u2212 \u03b8\u2217\u20162\n2\u03b7t + \u03b7t 2 \u00b7G2.\nAssume \u2016\u03b81 \u2212 \u03b8\u2217\u2016 \u2264 D. Substituting \u03b7t = DG\u221aT+1 into the above inequality, we have\n1\nT + 1\nT+1 \u2211\nt=1\nE [R(\u03b8t)] \u2264 R(\u03b8\u2217) + GD\u221a T + 1 .\nBy the convexity of R(\u00b7), we have\nR ( \u03b8\u0304T+1 ) = R\n(\n1\nT + 1\nT+1 \u2211\nt=1\n\u03b8t\n)\n\u2264 1 T + 1\nT+1 \u2211\nt=1\nR(\u03b8t),\nwhich finally verifies the proof."
        },
        {
            "heading": "B.3 Proof of Theorem 2",
            "text": "Proof. Since \u03c8 is convex by the definition in Bartlett et al. (2006), we have\n\u03c8\n(\nE\n[ L01(f\u03b8\u0304T+1)\u2212 infg\u2208G L01(g) ]) \u2264 E [ \u03c8 ( L01(f\u03b8\u0304T+1)\u2212 infg\u2208G L01(g) )] ,\nwhere the expectation is taken with respect to all the randomness in the algorithm. By the surrogate property (2), we have\nE\n[ L01(f\u03b8\u0304T+1)\u2212 infg\u2208G L01(g) ] \u2264 \u03c8\u22121 ( E [ \u03c8 ( L01(f\u03b8\u0304T+1)\u2212 infg\u2208G L01(g) )])\n\u2264 \u03c8\u22121 ( E [ EX,Y [l\u0303(f\u03b8\u0304T+1(X), Y )]\u2212 infg\u2208G EX,Y [l\u0303(g(X), Y )] ]) = \u03c8\u22121 (\nE[l\u0303(f\u03b8\u0304T+1(X), Y )]\u2212 infg\u2208G E[l\u0303(g(X), Y )] )\n= \u03c8\u22121 ( E[l\u0303(f\u03b8\u0304T+1(X), Y )]\u2212 E[l\u0303(f\u03b8\u0303\u2217(X), Y )] + E[l\u0303(f\u03b8\u0303\u2217(X), Y )]\u2212 infg\u2208G E[l\u0303(g(X), Y )] ) .\nFrom the result in Proposition 3, we can see that\nE[l\u0303(f\u03b8\u0304T+1(X), Y )]\u2212 E[l\u0303(f\u03b8\u0303\u2217(X), Y )] \u2264 GD\u221a T + 1 ."
        },
        {
            "heading": "B.4 Proof of Proposition 5",
            "text": "Proof. The algorithm we are considering is under the stream-based setting (Algorithm 1), where the newly observed sample is directly taken from the unknown distribution P . As discussed in Section 3.4, we are directly applying SGD on the expected loss L(\u03b8;X). Following the equivalent loss analyses, one can find the equivalent expected loss by\n\u2202L\u0303 \u2202\u03b8 = U \u00b7 \u2202L \u2202\u03b8 = L \u00b7 \u2202L \u2202\u03b8 .\nThen\nL\u0303 = 1\n2 L2 + C,\nwhere C is some constant. Since the constant does not affect the gradient, we choose C = 0 for simplicity. In other words, we are actually implementing SGD on the squared expected loss when we are applying the gradient-descent-update version of the uncertainty sampling algorithm. For those U \u2265 1, we permanently query the label and compensate the ratio by increasing the original descent step size \u03b7 to \u03b7 \u00b7 U , keeping the SGD rule the same.\nB.5 Discussions on existence of solution to Equation (1)\nIn Section 4.1, we have mentioned that the necessary and sufficient condition for the path integral of \u2211d\nj=1 U \u00b7 \u2202l\u2202\u03b8j d\u03b8j to not depend on the chosen path is the uncertainty U and loss l fulfill the exchangeability condition:\n\u2202U \u2202\u03b8i \u00b7 \u2202l \u2202\u03b8j = \u2202U \u2202\u03b8j \u00b7 \u2202l \u2202\u03b8i , \u2200i 6= j.\nTo see why this happens, we give a very brief argument here without bothering to concretely introduce another system of concepts in differential forms and algebraic topology. For those interested readers, please refer to the textbook of differential forms in algebraic topology (Bott et al., 1982). By de Rham\u2019s theorem, we shall see the condition that the path integral of \u2211d\nj=1 U \u00b7 \u2202L\u2202\u03b8j d\u03b8j does not depend on the path choices is equivalent to saying that it is an exact form, where the term exact means that the form itself is the (exterior) derivative of another function. In other words, saying that the path integral of some differential form depends only on the starting and ending points equals saying that it is some gradient itself.\nThe next question is: how to find all the exact forms on some Euclidean parameter space \u0398? By Poincar\u00e9\u2019s lemma, on any open ball of Rd, to say a 1-form is exact (where 1 means that it is a first-order gradient of a function) equals to say the form is a closed 1-form, where the term closed means that the form\u2019s exterior derivative is zero.\nBefore diving into finding closed forms, we try to intuitively tell what an exterior derivative is. We take R3 as an example. We shall see that the exterior derivative is a mimic of gradients, curls, and divergences. In the 3-dimensional Euclidean space, we can find the gradient of a smooth function F by\n\u2207F = ( \u2202F \u2202x , \u2202F \u2202y , \u2202F \u2202z )\u22a4 .\nIf we represent the gradient by independent vectors (dx, dy, dz), then we have\n\u2207F \u2243 \u2202F \u2202x dx+ \u2202F \u2202y dy + \u2202F \u2202z dz,\nwhich is exactly the definition of the exterior derivative dF . If F is now a vector field (Fx, Fy, Fz) \u22a4, then its curl is\n\u2207\u00d7 F = ( \u2202Fz \u2202y \u2212 \u2202Fy \u2202z , \u2202Fx \u2202z \u2212 \u2202Fz \u2202x , \u2202Fy \u2202x \u2212 \u2202Fx \u2202y )\u22a4 .\nBy representing it with independent vectors (dy \u2227 dz, dz \u2227 dx, dx \u2227 dy), we have\n\u2207\u00d7 F \u2243 ( \u2202Fz \u2202y \u2212 \u2202Fy \u2202z ) dy \u2227 dz + ( \u2202Fx \u2202z \u2212 \u2202Fz \u2202x ) dz \u2227 dx+ ( \u2202Fy \u2202x \u2212 \u2202Fx \u2202y ) dx \u2227 dy.\nBy writing F \u2243 Fxdx+ Fydy + Fydy, we have the same as the definition of exterior derivatives:\nd (Fxdx+ Fydy + Fydy) =\n(\n\u2202Fz \u2202y \u2212 \u2202Fy \u2202z\n) dy \u2227 dz + ( \u2202Fx \u2202z \u2212 \u2202Fz \u2202x ) dz \u2227 dx+ ( \u2202Fy \u2202x \u2212 \u2202Fx \u2202y ) dx \u2227 dy.\nFinally, the divergence of a vector field F = (Fx, Fy, Fz) \u22a4 is\n\u2207 \u00b7 F = \u2202Fx \u2202x + \u2202Fy \u2202y + \u2202Fy \u2202y .\nUp to a vector dx \u2227 dy \u2227 dz, we have\n\u2207 \u00b7F \u2243 d (Fxdy \u2227 dz + Fydz \u2227 dx+ Fzdx \u2227 dy) .\nIn a word, the exterior derivative is to extend the concept of \u201cdifferential\u201d from functions to vector fields.\nAll we have to do now is to find all the closed 1-forms. The closed forms are those of zero exterior\nderivatives. By the definition of exterior derivatives, we can compute the exterior derivative of \u2211d j=1 U \u00b7 \u2202L \u2202\u03b8j d\u03b8j as\nd\n\n\nd \u2211\nj=1\nU \u00b7 \u2202L \u2202\u03b8j d\u03b8j\n  = \u2211\n1\u2264i<j\u2264d\n(\n\u2202\n\u2202\u03b8i\n(\nU \u00b7 \u2202L \u2202\u03b8j\n)\n\u2212 \u2202 \u2202\u03b8j\n(\nU \u00b7 \u2202L \u2202\u03b8i\n)\n)\nd\u03b8i \u2227 d\u03b8j\n= \u2211\n1\u2264i<j\u2264d\n(\n\u2202U \u2202\u03b8i \u00b7 \u2202L \u2202\u03b8j \u2212 \u2202U \u2202\u03b8j \u00b7 \u2202L \u2202\u03b8i\n)\nd\u03b8i \u2227 d\u03b8j .\nwhich must be zero due to the definition of closed forms. This is the so-called requirement for exchangeability."
        },
        {
            "heading": "B.6 Proof of Proposition 6",
            "text": "Proof. \u2200\u01eb > 0, we can find some g\u01eb \u2208 G such that\nE[L(g\u01eb)] \u2264 inf g\u2208G E[L(g)] + \u01eb.\nFor every trajectory of X , the inequality (3) holds for any hypotheses f and g. Set g = g\u01eb. Taking expectation w.r.t. X \u223c PX on both sides, we have\nE[L\u0303(f)]\u2212 E[L\u0303(g\u01eb)] \u2265 1\n2 E\n[\n( L(f)\u2212 L(g\u01eb) )2 ] .\nBy Jensen\u2019s inequality,\n1 2 (E[L(f)]\u2212 E[L(g\u01eb)])2 \u2264 1 2 E [ ( L(f)\u2212 L(g\u01eb) )2 ]\n\u2264 E[L\u0303(f)]\u2212 E[L\u0303(g\u01eb)] \u2264 E[L\u0303(f)]\u2212 inf\ng\u2208G E[L\u0303(g)].\nHence, we have \u2200\u01eb > 0,\nE[L(f)]\u2212 inf g\u2208G\nE[L(g)]\u2212 \u01eb \u2264 E[L(f)]\u2212 E[L(g\u01eb)] \u2264 2 \u221a\nE[L\u0303(f)]\u2212 inf g\u2208G E[L\u0303(g)].\nTaking \u01eb to be arbitrarily small, we complete the proof."
        },
        {
            "heading": "B.7 Proof of Proposition 7",
            "text": "Proof. The proof is straightforward from the first two equal signs of (3). We have assumed the pointwise minimum conditional risk of g\u2217 is at least \u01eb\u2217. Then for any X \u2208 X ,\nL\u0303(f)\u2212 L\u0303(g\u2217) = 1 2 (L(f) + L(g\u2217))(L(f)\u2212 L(g\u2217)) \u2265 \u01eb\n\u2217\n2 (L(f)\u2212 L(g\u2217)).\nTaking expectation on X \u223c PX concludes the proof."
        },
        {
            "heading": "B.8 Proof of Theorem 3",
            "text": "Proof. We develop our proof based on that of Proposition 3. In the proof of Proposition 3, we utilize the term gt to ease the burden of redundant notations. We keep the notation here but replace U with L:\ngt := 1{\u03bet \u2264 L(\u03b8t;Xt)} \u00b7 \u2202l(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n.\nNote that if we were able to carry on the parameter update based on gt, then we would be doing SGD on the oracle equivalent expected loss L\u0303, and the analysis of Proposition 3 can be directly applied. But we are actually implementing the uncertainty based on an estimation of L, say L\u0302. Note that the \u03b8 term in L\u0302(\u03b8;X) does not mean that the estimation model L\u0302 is also parametrized by \u03b8 but that the model estimates the conditional expected loss when the current hypothesis is \u03b8. Therefore, the update is made w.r.t.\ng\u0302t := 1{\u03bet \u2264 L\u0302(\u03b8t;Xt)} \u00b7 \u2202l(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n.\nDenote EY [L\u0303] by R. By the definition of \u03b8t+1, we have\n\u2016\u03b8t+1 \u2212 \u03b8t\u20162 = \u2016\u03b8t \u2212 \u03b7t \u00b7 g\u0302t \u2212 \u03b8\u2217\u20162\n= \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 2\u03b7t \u00b7 (g\u0302t \u2212 gt + gt \u2212\u2207R(\u03b8t) +\u2207R(\u03b8t))\u22a4 (\u03b8t \u2212 \u03b8\u2217) + \u03b72t \u2016g\u0302t\u20162 \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 2\u03b7t \u00b7 (gt \u2212\u2207R(\u03b8t))\u22a4 (\u03b8t \u2212 \u03b8\u2217)\u2212 2\u03b7t \u00b7 (g\u0302t \u2212 gt)\u22a4 (\u03b8t \u2212 \u03b8\u2217) + 2\u03b7t \u00b7 (R(\u03b8\u2217)\u2212R(\u03b8t)) + \u03b72\u2016g\u0302t\u20162,\nwhere the last inequality is derived from the convexity of R. Similar to the proof of Proposition 3, we take the expectation conditioned on Ft on both sides, where Ft is the \u03c3-field generated by \u03b8t:\nE [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2223 \u2223Ft ] \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162 + 2\u03b7t \u00b7 E [\u2016g\u0302t \u2212 gt\u2016|Ft] \u00b7 \u2016\u03b8t \u2212 \u03b8\u2217\u2016 + 2\u03b7t \u00b7 (R(\u03b8\u2217)\u2212R(\u03b8t)) + \u03b72tE [ \u2016g\u0302t\u20162 \u2223 \u2223Ft ] .\nRearranging the terms, we have\nR(\u03b8t) \u2264 R(\u03b8\u2217) + \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2212 E [ \u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162 \u2223 \u2223Ft ]\n2\u03b7t + \u03b7t 2 \u00b7E [ \u2016g\u0302t\u20162 \u2223 \u2223Ft ] +E [\u2016g\u0302t \u2212 gt\u2016|Ft] \u00b7 \u2016\u03b8t\u2212 \u03b8\u2217\u2016. (11)\nSimilar to the proof of Proposition 3, we can easily see from the definition that\n\u2016g\u0302t\u20162 \u2264 \u2225 \u2225 \u2225 \u2225\n\u2225\n\u2202l(\u03b8; (Xt, Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n\u2225 \u2225 \u2225 \u2225 \u2225 2 \u2264 G2.\nBy assuming that \u2016\u03b8t \u2212 \u03b8\u2217\u2016 \u2264 D for all t \u2208 [T + 1], we take the unconditional expectation on both sides of inequality 11 and sum up from t = 1 to t = T + 1, then\n1\nT + 1\nT+1 \u2211\nt=1\nE[R(\u03b8t)] \u2264 R(\u03b8\u2217) + D2 2\u03b7t + \u03b7t 2 \u00b7G2 +D \u00b7 1 T + 1\nT+1 \u2211\nt=1\n\u03b4t.\nBy taking \u03b7t = D G \u221a T+1 and the convexity of R, we have\nE [ R(\u03b8\u0304T+1) ] \u2264 R(\u03b8\u2217) + GD\u221a T + 1 + D T + 1\nT+1 \u2211\nt=1\n\u03b4t."
        },
        {
            "heading": "B.9 Proof of Proposition 8",
            "text": "Proof. We give the analysis here to show that Algorithm 2 is indeed an SGD update. To simplify the notation, we abbreviate the gradient we take at time step t as gt:\ngt := \u2202l(\u03b8; (Xat , Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n.\nWe define the \u03c3-field generated by \u03b8t to be Ft. If we define the equivalent expected loss L\u0303 to be 12L2 as we do in Section 4, we have\nE\n[ St n \u00b7 gt \u2223 \u2223 \u2223 \u2223 Ft ] = St n n \u2211\ni=1\nU(\u03b8t;Xi)\nSt \u2207\u03b8E [l(\u03b8t; (Xi, Y ))|X = Xi]\n= EP\u0302n X [U(\u03b8t;X) \u00b7 \u2207\u03b8L(\u03b8t;X)] = EP\u0302n X [L(\u03b8t;X) \u00b7 \u2207\u03b8L(\u03b8t;X)] = EP\u0302n X [ \u2207\u03b8L\u0303(\u03b8t;X) ] = \u2207\u03b8EP\u0302n X [ L\u0303(\u03b8t;X) ] , (12)\nwhich indicates that Algorithm 2 is indeed an SGD update w.r.t. the expected L\u0303 under the empirical distribution P\u0302nX with step sizes {\u03b7t}Tt=1."
        },
        {
            "heading": "B.10 Proof of Proposition 10",
            "text": "Proof. Denote the indexes of the m-largest loss functions l(\u03b8t; (Xi, Yi)) by {it1, . . . , itm}. Denote the conditional expected loss as E[l(\u03b8; (X,Y ))|X ] by L(\u03b8;X). Then, the gradient of the objective (6) at \u03b8 = \u03b8t is\n1 m\nm \u2211\nk=1\n\u2207\u03b8L(\u03b8t;Xitk).\nOn the other hand, the conditional expectation of the update is\nE [\u03b8t+1 \u2212 \u03b8t|\u03b8t] = E [ \u2212\u03b7t \u00b7 \u2202l(\u03b8; (Xit , Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n\u2223 \u2223 \u2223 \u2223 \u2223 \u03b8t ]\n= \u2212\u03b7t \u00b7 1\nm\nm \u2211\nk=1\nEYt\n[\n\u2202l(\u03b8; (Xit , Yt))\n\u2202\u03b8\n\u2223 \u2223 \u2223 \u2223\n\u03b8=\u03b8t\n\u2223 \u2223 \u2223 \u2223 \u2223 \u03b8t, it = itk ]\n= \u2212\u03b7t \u00b7 1\nm\nm \u2211\nk=1\n\u2207\u03b8EYt [l(\u03b8t; (Xit , Yt))|\u03b8t, it = itk]\n= \u2212\u03b7t \u00b7 1\nm\nm \u2211\nk=1\n\u2207\u03b8L(\u03b8t;Xitk),\nwhich completes the proof."
        },
        {
            "heading": "B.11 Proof of Proposition 11",
            "text": "Proof. The sampling probability at step t is\npti = (1 \u2212 \u03b3) \u00b7 1\nn + \u03b3 \u00b7 1{i \u2208 {it1, . . . , itm}}.\nDue to the envelope theorem, we only need to prove that pt is indeed the solution to the maximization problem at time step t:\nmax p\u2208P\n\u03b32n(n\u2212m) 2m ,n, m+(n\u2212m)\u03b3 mn\nn \u2211\ni=1\npi \u00b7 L(\u03b8t;Xi).\nSuch an optimality check can be easily done by checking the KKT condition, if one notices that the uncertainty set P \u03b32n(n\u2212m) 2m ,n,m is a convex set and the objective is a linear function of p. In fact, if we remove the divergence constraint and only focus on the linear constraints, one can easily see that the maximization solution is pt, since it puts as much as possible weights on the largest m objectives. Since\nD 1 2 (\u00b7\u22121)2\n(\npt\n\u2225 \u2225 \u2225 ( 1\nn , . . . ,\n1 n )\u22a4) = m \u00b7 1 2n \u00b7 ( \u03b3 ( n m \u2212 1 ))2 + (n\u2212m) \u00b7 1 2n \u00b7 \u03b32 = \u03b32 \u00b7 n\u2212m 2m ,\nwhich means that the divergence constraint is also fulfilled by the relaxed maximization point pt. Hence the relaxed solution is also the solution to the original maximization problem."
        }
    ],
    "title": "Understanding Uncertainty Sampling",
    "year": 2023
}