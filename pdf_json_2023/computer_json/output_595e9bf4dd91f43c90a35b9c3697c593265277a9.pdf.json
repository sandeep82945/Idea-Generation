{
    "abstractText": "To date, various neural methods have been proposed for causal effect estimation based on observational data, where a default assumption is the same distribution and availability of variables at both training and inference (i.e., runtime) stages. However, distribution shift (i.e., domain shift) could happen during runtime, and bigger challenges arise from the impaired accessibility of variables. This is commonly caused by increasing privacy and ethical concerns, which can make arbitrary variables unavailable in the entire runtime data and imputation impractical. We term the co-occurrence of domain shift and inaccessible variables runtime domain corruption, which seriously impairs the generalizability of a trained counterfactual predictor. To counter runtime domain corruption, we subsume counterfactual prediction under the notion of domain adaptation. Specifically, we upper-bound the error w.r.t. the target domain (i.e., runtime covariates) by the sum of source domain error and inter-domain distribution distance. In addition, we build an adversarially unified variational causal effect model, named VEGAN, with a novel two-stage adversarial domain adaptation scheme to reduce the latent distribution disparity between treated and control groups first, and between training and runtime variables afterwards. We demonstrate that VEGAN outperforms other state-of-the-art baselines on individual-level treatment effect estimation in the presence of runtime domain corruption on benchmark datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hechuan Wen"
        },
        {
            "affiliations": [],
            "name": "Tong Chen"
        },
        {
            "affiliations": [],
            "name": "Li Kheng Chai"
        },
        {
            "affiliations": [],
            "name": "Shazia Sadiq"
        },
        {
            "affiliations": [],
            "name": "Junbin Gao"
        },
        {
            "affiliations": [],
            "name": "Hongzhi Yin"
        }
    ],
    "id": "SP:51ed59c9d4e8866dec1f6dbfd2269ac1adae26ad",
    "references": [
        {
            "authors": [
                "T.A. Glass",
                "S.N. Goodman",
                "M.A. Hern\u00e1n",
                "J.M. Samet"
            ],
            "title": "Causal inference in public health",
            "venue": "Annual review of public health, vol. 34, p. 61, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J.M. Cordero",
                "V. Cristobal",
                "D. Sant\u0131\u0301n"
            ],
            "title": "Causal inference on education policies: A survey of empirical studies using pisa, timss and pirls",
            "venue": "Journal of Economic Surveys, vol. 32, no. 3, pp. 878\u2013915, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "W. Wang",
                "X. Lin",
                "F. Feng",
                "X. He",
                "M. Lin",
                "T.-S. Chua"
            ],
            "title": "Causal representation learning for out-of-distribution recommendation",
            "venue": "WWW, pp. 3562\u20133571, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Johansson",
                "U. Shalit",
                "D. Sontag"
            ],
            "title": "Learning representations for counterfactual inference",
            "venue": "International conference on machine learning, pp. 3020\u20133029, PMLR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "L. Yao",
                "Z. Chu",
                "S. Li",
                "Y. Li",
                "J. Gao",
                "A. Zhang"
            ],
            "title": "A survey on causal inference",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD), vol. 15, no. 5, pp. 1\u201346, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Gresele",
                "J. Von K\u00fcgelgen",
                "J. K\u00fcbler",
                "E. Kirschbaum",
                "B. Sch\u00f6lkopf",
                "D. Janzing"
            ],
            "title": "Causal inference through the structural causal marginal problem",
            "venue": "ICML, pp. 7793\u20137824, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "P.R. Rosenbaum"
            ],
            "title": "Overt bias in observational studies",
            "venue": "Observational studies, pp. 71\u2013104, Springer, 2002.",
            "year": 2002
        },
        {
            "authors": [
                "M. Caliendo",
                "S. Kopeinig"
            ],
            "title": "Some practical guidance for the implementation of propensity score matching",
            "venue": "Journal of economic surveys, vol. 22, no. 1, pp. 31\u201372, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "A. Jesson",
                "S. Mindermann",
                "U. Shalit",
                "Y. Gal"
            ],
            "title": "Identifying causal-effect inference failure with uncertainty-aware models",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 11637\u201311649, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "U. Shalit",
                "F.D. Johansson",
                "D. Sontag"
            ],
            "title": "Estimating individual treatment effect: generalization bounds and algorithms",
            "venue": "International Conference on Machine Learning, pp. 3076\u20133085, PMLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Louizos",
                "U. Shalit",
                "J.M. Mooij",
                "D. Sontag",
                "R. Zemel",
                "M. Welling"
            ],
            "title": "Causal effect inference with deep latent-variable models",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Shi",
                "D. Blei",
                "V. Veitch"
            ],
            "title": "Adapting neural networks for the estimation of treatment effects",
            "venue": "Advances in neural information processing systems, vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "S. Neyman"
            ],
            "title": "On the application of probability theory to agricultural experiments. essay on principles. section 9. portions translated into english by d. dabrowska and t. speed (1990)",
            "venue": "Statistical Science, pp. 465\u2013472, 1923.",
            "year": 1923
        },
        {
            "authors": [
                "D.B. Rubin"
            ],
            "title": "Causal inference using potential outcomes: Design, modeling, decisions",
            "venue": "Journal of the American Statistical Association, vol. 100, no. 469, pp. 322\u2013331, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "P.R. Rosenbaum",
                "D.B. Rubin"
            ],
            "title": "The central role of the propensity score in observational studies for causal effects",
            "venue": "Biometrika, vol. 70, no. 1, pp. 41\u201355, 1983.",
            "year": 1983
        },
        {
            "authors": [
                "G.W. Imbens",
                "D.B. Rubin"
            ],
            "title": "Causal inference in statistics, social, and biomedical sciences",
            "year": 2015
        },
        {
            "authors": [
                "J.M. Robins",
                "A. Rotnitzky",
                "L.P. Zhao"
            ],
            "title": "Estimation of regression coefficients when some regressors are not always observed",
            "venue": "Journal of the American statistical Association, vol. 89, no. 427, pp. 846\u2013866, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "H.A. Chipman",
                "E.I. George",
                "R.E. McCulloch"
            ],
            "title": "Bart: Bayesian additive regression trees",
            "venue": "The Annals of Applied Statistics, vol. 4, no. 1, pp. 266\u2013298, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Bengio",
                "A. Courville",
                "P. Vincent"
            ],
            "title": "Representation learning: A review and new perspectives",
            "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.",
            "year": 1828
        },
        {
            "authors": [
                "Y. LeCun",
                "Y. Bengio",
                "G. Hinton"
            ],
            "title": "Deep learning",
            "venue": "nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "S. Vallender"
            ],
            "title": "Calculation of the wasserstein distance between probability distributions on the line",
            "venue": "Theory of Probability & Its Applications, vol. 18, no. 4, pp. 784\u2013786, 1974.",
            "year": 1974
        },
        {
            "authors": [
                "A. Gretton",
                "K.M. Borgwardt",
                "M.J. Rasch",
                "B. Sch\u00f6lkopf",
                "A. Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 723\u2013773, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "J. Yoon",
                "J. Jordon",
                "M. Van Der Schaar"
            ],
            "title": "Ganite: Estimation of individualized treatment effects using generative adversarial nets",
            "venue": "International Conference on Learning Representations, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "N. Kallus"
            ],
            "title": "Deepmatch: Balancing deep covariate representations for causal inference using adversarial training",
            "venue": "International Conference on Machine Learning, pp. 5067\u20135077, PMLR, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems, vol. 27, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "N. Hassanpour",
                "R. Greiner"
            ],
            "title": "Learning disentangled representations for counterfactual regression",
            "venue": "International Conference on Learning Representations, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M.J. Vowels",
                "N.C. Camgoz",
                "R. Bowden"
            ],
            "title": "Targeted vae: Structured inference and targeted learning for causal parameter estimation",
            "venue": "Openreview, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Zhang",
                "L. Liu",
                "J. Li"
            ],
            "title": "Treatment effect estimation with disentangled latent factors",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, pp. 10923\u201310930, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Gal",
                "Z. Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "international conference on machine learning, pp. 1050\u20131059, PMLR, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Qu",
                "I. Lipkovich"
            ],
            "title": "Propensity score estimation with missing values using a multiple imputation missingness pattern (mimp) approach",
            "venue": "Statistics in medicine, vol. 28, no. 9, pp. 1402\u20131414, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "I. Mayer",
                "E. Sverdrup",
                "T. Gauss",
                "J.-D. Moyer",
                "S. Wager",
                "J. Josse"
            ],
            "title": "Doubly robust treatment effect estimation with missing attributes",
            "venue": "The Annals of Applied Statistics, vol. 14, no. 3, pp. 1409\u2013 1431, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Berrevoets",
                "F. Imrie",
                "T. Kyono",
                "J. Jordon",
                "M. van der Schaar"
            ],
            "title": "To impute or not to impute?\u2013missing data in treatment effect estimation",
            "venue": "arXiv preprint arXiv:2202.02096, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Lipton",
                "J. McAuley",
                "A. Chouldechova"
            ],
            "title": "Does mitigating ml\u2019s impact disparity require treatment disparity",
            "venue": "Advances in neural information processing systems, vol. 31, 2018. 12",
            "year": 2018
        },
        {
            "authors": [
                "J. Simons",
                "S. Adams Bhatti",
                "A. Weller"
            ],
            "title": "Machine learning and the meaning of equal treatment",
            "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp. 956\u2013966, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Coston",
                "E. Kennedy",
                "A. Chouldechova"
            ],
            "title": "Counterfactual predictions under runtime confounding",
            "venue": "Advances in Neural Information Processing Systems, vol. 33, pp. 4150\u20134162, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Griliches",
                "J.A. Hausman"
            ],
            "title": "Errors in variables in panel data",
            "venue": "Journal of econometrics, vol. 31, no. 1, pp. 93\u2013118, 1986.",
            "year": 1986
        },
        {
            "authors": [
                "G. Maddala",
                "M. Nimalendran"
            ],
            "title": "17 errors-in-variables problems in financial models",
            "venue": "Handbook of statistics, vol. 14, pp. 507\u2013 528, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "F.D. Johansson",
                "U. Shalit",
                "N. Kallus",
                "D. Sontag"
            ],
            "title": "Generalization bounds and representation learning for estimation of potential outcomes and causal effects",
            "venue": "arXiv preprint arXiv:2001.07426, 2020.",
            "year": 2001
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Kullback",
                "R.A. Leibler"
            ],
            "title": "On information and sufficiency",
            "venue": "The annals of mathematical statistics, vol. 22, no. 1, pp. 79\u201386, 1951.",
            "year": 1951
        },
        {
            "authors": [
                "R. Nelken",
                "S.M. Shieber"
            ],
            "title": "Computing the kullback-leibler divergence between probabilistic automata using rational kernels",
            "venue": "Harvard Computer Science Group Technical Report TR-07-06, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "S. Moral",
                "A. Cano",
                "M. G\u00f3mez-Olmedo"
            ],
            "title": "Computation of kullback\u2013leibler divergence in bayesian networks",
            "venue": "Entropy, vol. 23, no. 9, p. 1122, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "F. Nielsen"
            ],
            "title": "On the jensen\u2013shannon symmetrization of distances relying on abstract means",
            "venue": "Entropy, vol. 21, no. 5, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Mescheder",
                "S. Nowozin",
                "A. Geiger"
            ],
            "title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks",
            "venue": "International Conference on Machine Learning, pp. 2391\u20132400, PMLR, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "J.L. Hill"
            ],
            "title": "Bayesian nonparametric modeling for causal inference",
            "venue": "Journal of Computational and Graphical Statistics, vol. 20, no. 1, pp. 217\u2013240, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "ACIC"
            ],
            "title": "Atlantic causal inference conference 2019",
            "venue": "Aug 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Yao",
                "S. Li",
                "Y. Li",
                "M. Huai",
                "J. Gao",
                "A. Zhang"
            ],
            "title": "Representation learning for treatment effect estimation from observational data",
            "venue": "Advances in Neural Information Processing Systems, vol. 31, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems, vol. 32, 2019.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Causal Effect Estimation, Runtime Domain Corruption, Adversarial Domain Adaptation\n\u2726"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In predictive analytics, causal inference is increasingly important in guiding decision-making in high-stake domains, such as healthcare [1], education [2], e-commerce [3], etc. Normally, randomized control trial (RCT) is the gold standard for estimating the causal effect. Given that implementing RCTs is costly, time-consuming, and sometimes ethically intractable, various applications alternatively turn to use the passively collected observational data to perform causal inference in a data-driven fashion [4], [5], [6]. Denoting input variables as x, treatment as t, outcome as y, the observational dataset with N samples {(xi, ti, yi)}Ni=1 commonly does not satisfy the RCT standard due to unmeasured confounders and selection bias, which are two prominent challenges in causal inference. Specifically, the untestable unconfoundedness assumption assumes no unobserved confounders. Unfortunately, such an assumption cannot be satisfied in many cases, rendering the estimation erroneous [7], [8]. Meanwhile, the selection bias between the treated and control groups causes the imbalanced covariate distributions, which could introduce undesirable spurious effect due to the imbalance [5]. In the extreme case, it can even violate the positivity assumption and result in nonidentifiable causal effect [9]. Thus, this issue further weakens\n\u2022 H. Wen, T. Chen, S. Sadiq, and H. Yin, are with the School of Information Technology and Electrical Engineering, The University of Queensland, Brisbane, Australia E-mail: h.wen@uq.edu.au, tong.chen@uq.edu.au, sadiq@itee.uq.edu.au, h.yin1@uq.edu.au \u2022 L. K. Chai is with Health and Wellbeing Queensland, Brisbane, Australia. E-mail: LiKheng.Chai@hw.qld.gov.au \u2022 J. Gao is with Business School, The University of Sydney, Sydney, Australia. E-mail: junbin.gao@sydney.edu.au\nHongzhi Yin is the corresponding author.\nthe correctness of causal effect estimation. An example to explain these two challenges is that, if only the rich can afford drug A while the poor have to use the cheaper drug B, then people\u2019s financial status could be a hidden confounder if unmeasured, resulting in invalid estimation. If measured, it causes selection bias, and the effectiveness of drug A and drug B cannot be validly compared based on the skewed distribution of variables due to people\u2019s financial status. By addressing either of the two challenges or both, several neural approaches [10], [11], [12] are made available for causal effect estimation with observational data. Despite a variety of methods that tackle distributional imbalance caused by the selection bias, such domain shifts are only restricted between the treated and control groups that are both used for training, where the runtime variables are assumed to be drawn from the same distribution as the training data. In fact, domain shift also widely exists between training and runtime data, e.g., when a model trained on one race is asked to perform predictions on a different minority race, and it challenges the generalizability of the trained model.\nOn top of that, the unavailable/missing variables and corresponding countermeasures are also largely understudied. For instance, real-world applications commonly have medical diagnostic models learned with high-quality open benchmarks, but in the deployment stage, not all endusers are able to provide the same set of variables due to accessibility issues (e.g., high-cost medical checks), privacy constraints (e.g., historical treatments), and ethical concerns (e.g., gender and race). In this paper, we refer to the coexistence of the shifted and unavailable variables in the inference data as runtime domain corruption.\nRuntime domain corruption can be interpreted as one step above observing domain/covariate shift during infer-\nar X\niv :2\n30 6.\n13 27\n1v 1\n[ cs\n.L G\n] 2\n3 Ju\nn 20\n23\n2 ence, where the model not only faces changed covariate distribution but also the ubiquitous missing values. In short, in our definition, runtime domain corruption is caused by the co-occurrence of domain shift and missing values. Compared with domain shift, runtime domain corruption more aggressively challenges the generalizability of the trained counterfactual prediction model, because variables deemed important in training might no longer be present during inference, and the domain-invariant patterns are unable to be mapped to those missing variables. Therefore, a high corruption rate of runtime variables can make the counterfactual predictor merely learned on full training data incur large generalization errors. Though one can consider discarding the unavailable variables in the training set, the reduced variables may lead to an underfitting issue. Also, for real-world deployment, it is impractical to assume prior knowledge on which variables are corrupted during runtime, especially considering the inaccessible variables can differ among individuals (e.g., users may choose to withhold different personal information).\nThis work focuses on causal inference using the Neyman-Rubin potential outcome framework [13], [14] under the runtime domain corruption circumstance. In this work, we aim to learn a robust, causal, and domaininvariant latent representation z of variable x, for which the latent distributions across various domains are wellbalanced to counter the aforementioned three challenges, i.e., unmeasured confounders, selection bias, and runtime domain corruption, simultaneously. Our main contributions are:\n\u2022 We identify an important performance bottleneck for causal inference methods, namely runtime domain corruption that combines two largely unexplored yet important settings: domain shift and unavailable variables during runtime. In our paper, we propose the first systematic investigation of it for causal effect estimation. \u2022 We derive the upper bound of the generalization error by extending the in-sample causal inference to the corrupted out-of-sample scenario. To efficiently optimize the multiple Kullback-Leibler (KL) divergence terms in our VAE-based model, we propose a two-stage domain adaptation scheme, namely the Variational autoEncoder Generative Adversarial Network (VEGAN) for unifying multiple inter-domain distances. \u2022 We compare VEGAN to state-of-the-art baselines for performing predictions on both in-sample covariates and out-of-sample, corrupted runtime covariates. The empirical results demonstrate our model\u2019s stronger robustness to runtime domain corruption."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Back in time, researchers have been seeking to approach observational data-based causal inference from various perspectives. The re-weighting method, e.g., inverse probability weighting (IPW), uses the propensity score [15], [16] to mitigate the selection bias by re-weighting each unit\u2019s treatment outcome according to its estimated probability of being assigned a treatment. However, such a method\nstrongly relies on the correctness of the estimated propensity score. To alleviate this strong dependency, the proposed doubly robust (DR) [17] method considers the outcome regression together with IPW for re-weighing purposes. The DR method tries to secure the causal effect estimation with an additional \u201cinsurance\u201d that comes from the correctness of the outcome modelling which is in fact no one can assure. In addition to re-weighting, other methods such as the nonparametric tree-based model, e.g., BART [18] combines the tree method and Bayesian inference. However, all the abovementioned methods mainly focus on estimating the average treatment effect (ATE) and are not expressive enough to handle the high-dimensional dataset for individual-level estimations.\nNowadays, with the strong expressive power of deep learning [19], [20], new algorithms are proliferating by leveraging the deep learning framework to learn the deconfounded latent representation on top of the observed covariates and model the personalized treatment effect. We relate our work to the representation learning branch in causal inference, which is overlapped with the domain adaptation field due to the unique counterfactual nature of estimating treatment effect. The TARNet [10] builds a shared feature extractor followed by a two-headed neural network to model the outcomes for each type of treatment separately. Its variants can incorporate the integral probability metric (IPM), e.g., Wasserstein distance [21], and maximum mean discrepancy (MMD) [22], to minimize the distance of the learned latent covariate distribution between treated and control groups to mitigate the selection bias. Following that, a variational autoencoder (VAE) framed CEVAE model [11] emphasizes handling the confounding problem by building robust latent representation, and its performance is stated to be more robust than many previous methods. Dragonnet [12] leverages the neural net-enhanced propensity estimation and the innovative targeted regularization for causal effect estimation to achieve an asymptotic consistent ATE estimator. In addition, other works such as GANITE [23] and DeepMatch [24] adopt generative adversarial network (GAN) [25] and build their own designated GAN learning systems. Recently, many latent variable disentanglement methods, e.g., DR-CFR [26], TVAE [27], TEDVAE [28], are proposed to discover the disentanglement of the latent instrumental, risk, and confounding factors from the observed covariates to better capture the selection bias. The unique point of difference in our work is the additional consideration of the runtime domain corruption situation where the trained causal model\u2019s performance could dramatically decline when deployed to other environments.\nIn addition, it is noted that [9] integrate the Monte Carlo Dropout [29] method to the state-of-the-art neural methods and allow the upgraded models, e.g., BTARNET, BCEVAE, to estimate epistemic uncertainty in high-dimensional conditional average treatment effect (CATE) estimation, thus to inform the decision maker to be vigilant when making recommendations if the high uncertainty present. Their method only considers the domain shift between the treated and control groups during training, and it emphasizes making no treatment recommendation if the epistemic uncertainty exceeds a certain threshold. Hence, our work differs from it as we focus on more accurate treatment effect es-\n3 timation when runtime domain corruption occurs during inference stage. It should also be noted that some existing works [30], [31], [32] have been proposed for treatment effect estimation with missing values, where the core is to leverage imputation algorithms to handle the missing values. Since runtime domain corruption also includes domain shift, the imputed target domain data could still deviate heavily from the source domain, rendering those methods inaccurate in such conditions. Furthermore, the imputation algorithm is not capable of imputing accurately when the number of missing values is large, it even becomes useless when the attributes are completely missing at the distribution level during the inference stage.\nLastly, we also relate our work to algorithmic fairness topics, e.g., disparate learning processes (DLPs), in which the ethically concerned, privacy-related features are not available or impermissible to be used during runtime [33], [34]. A similar approach to DLPs is a doubly-robust counterfactual prediction model with additional handling of the confounding problem during training [35]. However, it differs from the common causal effect estimation as it assumes that one of the potential outcomes is a known constant for a binary treatment, and is hence inapplicable to the problem studied in this paper."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 Preliminaries",
            "text": "For simplicity, we consider binary treatment t of 1 or 0 to denote the treated group and the control group, respectively. The individual treatment effect (ITE) for a variable vector x is defined as:\n\u03c4(x) = E[Y1 \u2212 Y0|x], (1)\nwhere Y1 and Y0 are the unobserved potential outcomes with treatment t = 1 and t = 0 respectively. As a common practice in causal inference research, to validly identify the true treatment effect \u03c4(x) of instance x, we make the following standard assumptions.\nAssumption 1 (Stable Unit Treatment Value Assumption) The SUTVA assumption [16] states that: a) The potential outcomes for any unit do not vary with the treatment assigned to other units. b) For each unit, there are no different forms or versions of each treatment level, which leads to different potential outcomes.\nAssumption 2 (Unconfoundedness) Treatment assignment is independent of the potential outcomes given the pre-treatment covariate x, i.e., t \u22a5 {Y0, Y1}|x.\nAssumption 3 (Positivity) For every instance x \u2208 X , we have its corresponded treatment assignment mechanism p(t|x), such that 0 < p(t = 1|x) < 1.\nThe assumptions further lead us to the proposition below.\nProposition 1 (Identifiability) The causal effect is identifiable if and only if the SUTVA, the unconfoundedness, and the positivity assumptions hold.\nProof 1 Under SUTVA and unconfoundedness, the ITE for instance x is:\nE[Y1 \u2212 Y0|x] =E[Y1|x]\u2212 E[Y0|x] =E[Y1|X = x, t = 1]\u2212 E[Y0|X = x, t = 0] =E[y1|X = x, t = 1]\u2212 E[y0|X = x, t = 0],\n(2)\nwhere y1 and y0 are the observed responses after the interventions t = 1 and t = 0 have been taken, respectively. The last terms are identifiable as we assume 0 < p(t = 1|x) < 1. The first equality is by the operation of expectation, the second equality is based on the unconfoundedness, and the third equality is by the expected value of the observed outcomes {y1, y0} equals the unobserved potential outcomes {Y1, Y0}."
        },
        {
            "heading": "3.2 Problem Definition",
            "text": "Let \u03a8 : X\u00d7{0, 1} \u2192 R be the hypothesis, our goal is to build the treatment effect regression model \u03a8t(x, t) = E[yt|X = x, T = t] with observed outcome yt based on the training data, which can accurately recover the treatment effect for test instance x\u2217, thus the causal effect for the test instance x\u2217 can be estimated as \u03a81 \u2212 \u03a80. However, the untestable unconfoundedness and selection bias challenges arise when the observational dataset does not follow the RCT standard, making the trained models \u03a81 and \u03a80 unable to accurately reflect the true treatment outcomes for x.\nWe perceive the observed covariates of treated and control groups from the conventional domain shift perspective, in which covariate x is a noisy measurement, normally less informative and more confounded [36], [37], than the domain-invariant latent representation z. Therefore, the unconfoundedness changes from t \u22a5 {Y0, Y1}|x to t \u22a5 {Y0, Y1}|z. In addition to the treated and control groups from the in-sample set, this paper uniquely considers causal effect estimation where the out-of-sample set is affected by runtime domain corruption. In what follows, we formally define the runtime domain corruption problem.\nDefinition 1 (Runtime Domain Corruption) We define each variable vector x = [x1, x2, ..., xd] \u2208 Rd as a non-zero concatenation of categorical features (i.e., encodings) and numerical features. During training, all d entries xs, for 1 \u2264 s \u2264 d, are available and assigned corresponding values. Then, during inference, runtime domain corruption occurs when: (1) the covariate distribution shifts in the test domain: ptest(x) \u0338= ptrain(x); and (2) each vector x contains an arbitrary number of unavailable variables xs\u2032 , for 1 \u2264 s\u2032 \u2264 d, which are all zeroed out by setting xs\u2032 = 0.\nRationale of zero-padding. Specifically, during runtime, the unavailable features are not straightforwardly discarded when performing prediction, instead, we pad zeros to entries that correspond to missing variables such that the dimensionality is kept unchanged. It is worth noting that, during training, a non-zero property is maintained for every instance x whose features are all available. This can be easily achieved via standard preprocessing steps, e.g., rescaling/normalization for numerical features, and using 1 and -1 to respectively represent relevant and irrelevant categorical features in multi-hot encodings. Thus, using zeropadding to mark unknown/corrupted variables during run-\n4 time is viable, because the semantics of zeros are exclusively reserved for the unknown status of variables. Also, zeropadding is a more feasible practice in real applications, as each runtime instance x may have an arbitrary number and combination of attributes missing, rendering it impractical to train a specific latent feature extractor for each case. In contrast, zero-padding is a more flexible and scalable approach for learning domain-invariant latent representations with a shared feature extractor, where all zero-valued entries of x will be filtered out during projection."
        },
        {
            "heading": "3.3 Target Domain Error Upper Bound",
            "text": "Shalit et al. [10] show that the accuracy metric for causal inference \u2013 expected Precision in Estimation of Heterogeneous Effect (PEHE), denoted as \u03f5PEHE, is upper-bounded by both the trained model error \u03f5F on factual outcomes and the distance between treated and control distributions, measured by integral probability metric (IPM). However, as their derived upper bound for \u03f5PEHE does not consider runtime domain corruption on out-of-sample variables, we fill this gap by deriving the bound in Theorem 1.\nTheorem 1 Let \u03d5 : X \u2192 Z be the invertible latent representation mapping function (a.k.a. feature extractor) with inverse \u03a6. Let \u03a8 : Z \u00d7 {0, 1} \u2192 R be the updated hypothesis that maps latent variables z \u2208 Z to each treatment\u2019s outcome. Let F = {f |f : Z \u2192 R} be a family of functions. The source domain is the observational data for treated and control groups, and the target domain is the runtime test/inference set with corrupted variables. We derive the upper bound of target domain error (i.e., generalization error) as1:\n\u03f5trPEHE \u2264 2 [ \u03f5t=1F + \u03f5 t=0 F +B\u03d5 ( IPMF (Pt=1\u03d5 ,P t=0 \u03d5 )\n+ 1\n2 IPMF (Ptr\u03d5 ,P sr \u03d5 )\n)] ,\n(3)\nwhere \u03f5tF denotes the factual training error, Pt\u03d5 is the probability measure within treatment group t in the training set, \u03f5trPEHE and \u03f5srPEHE respectively indicate the target and source domain errors, Ptr\u03d5 and Psr\u03d5 are probability measures which respectively denote the covariate distributions in target domain and source domain, and B\u03d5 is a bounded constant.\nTo prove Theorem 1, we first provide some preliminary definitions.\nDefinition 2 Let F = {f |f : Z \u2192 R} be a family of functions. The distribution distance measure \u2013 integral probability metric (IPM) between the target and source distributions Psr and Ptr over Z is defined as:\nIPMF (Ptr,Psr) = sup f\u2208F \u2223\u2223\u2223\u2223\u222b Z f(z)(ptr(z)\u2212 psr(z))dz \u2223\u2223\u2223\u2223 . (4) Definition 3 Let \u03d5 : X \u2192 Z be the latent mapping, and let \u03a8 : Z \u00d7 {0, 1} \u2192 R be the updated hypothesis over the latent space Z , the estimated ITE for variable x is:\n\u03c4\u0302(x) = \u03a81(\u03d5(x), 1)\u2212\u03a80(\u03d5(x), 0). (5)\n1. We follow some notation conventions set in [10] and [38].\nDefinition 4 The expected Precision in Estimation of Heterogeneous Effect (PEHE) of the causal model {\u03d5,\u03a8} with squared loss metric L(\u00b7, \u00b7) is defined as:\n\u03f5PEHE(\u03d5,\u03a8) = \u222b X L\u03d5,\u03a8(x)p(x)dx, (6)\nwhere we denote L(\u03c4\u0302(x), \u03c4(x)) as L\u03d5,\u03a8(x) for notation simplicity. The \u03c4(x) is the true treatment effect defined in Eq. 1 and \u03c4\u0302(x) is the estimated one defined in Eq. 5.\nLemma 2 Let \u03d5 : X \u2192 Z be the invertible latent representation mapping function with inverse \u03a6. Let \u03a8 : Z \u00d7 {0, 1} \u2192 R be the updated hypothesis. Let F = {f |f : Z \u2192 R} be a family of functions. Assume we have B\u03d5 > 0 s.t. 1B\u03d5L\u03d5,\u03a8(\u03a6(z)) \u2208 F . The tightness of target domain error w.r.t. the source domain one is bounded by the distribution distance denoted by IPM:\n|\u03f5trPEHE \u2212 \u03f5srPEHE|\n= \u2223\u2223\u2223\u2223 \u222b Z L\u03d5,\u03a8(\u03a6(z))ptr\u03d5 (z)dz\u2212 \u222b Z L\u03d5,\u03a8(\u03a6(z))psr\u03d5 (z)dz \u2223\u2223\u2223\u2223 \u2264B\u03d5IPMF (Ptr\u03d5 ,Psr\u03d5 ), (7)\nwhere \u03f5trPEHE and \u03f5 sr PEHE indicate target domain error and source domain error respectively, Ptr\u03d5 and Psr\u03d5 denote covariate distribution in target domain and source domain respectively, and B\u03d5 is a bounded constant.\nProof of Lemma 2 We denote the expected PEHE in Eq. 6 in target domain and source domain as \u03f5trPEHE and \u03f5 sr PEHE respectively, also tr and sr indicates the test set and training set where ptr(x) \u0338= psr(x) if domain corruption exists.\n|\u03f5trPEHE \u2212 \u03f5srPEHE| = \u2223\u2223\u2223\u2223\u222b\nX L\u03d5,\u03a8(x)ptr\u03d5 (x)dx\u2212 \u222b X L\u03d5,\u03a8(x)psr\u03d5 (x)dx \u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223\u222b Z L\u03d5,\u03a8(\u03a6(z))ptr\u03d5 (z)dz\u2212 \u222b Z L\u03d5,\u03a8(\u03a6(z))psr\u03d5 (z)dz \u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223B\u03d5 \u222b Z 1 B\u03d5 L\u03d5,\u03a8(\u03a6(z))(ptr\u03d5 (z)\u2212 psr\u03d5 (z))dz \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223B\u03d5 supf\u2208F \u2223\u2223\u2223\u2223\u222b Z f(z)(ptr\u03d5 (z)\u2212 psr\u03d5 (z))dz \u2223\u2223\u2223\u2223 \u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223B\u03d5IPMF (Ptr\u03d5 ,Psr\u03d5 )\u2223\u2223 =B\u03d5IPMF (Ptr\u03d5 ,P sr \u03d5 ).\n(8)\nThe first equality is by Definition 4, the second equality is by change of variable, the first inequality is by the premise that 1 B\u03d5\nL\u03d5,\u03a8 belongs to the function family F , the fourth equality is by Definition 2, the last equality is by the property that IPM is non-negative.\nProof of Theorem 1 Under the conditions of Lemma 2 and the auxiliary theorem 1 in [10], thus conclude the proof of Theorem 1:\n\u03f5trPEHE \u2264\u03f5srPEHE +B\u03d5IPMF (Ptr\u03d5 ,Psr\u03d5 ) \u2264 2[\u03f5t=1F + \u03f5t=0F\n+B\u03d5(IPMF (Pt=1\u03d5 ,P t=0 \u03d5 ) +\n1 2 IPMF (Ptr\u03d5 ,P sr \u03d5 ))],\n(9)\nwhere the first inequality is by Lemma 2 and the second inequality is by the auxiliary theorem 1 from [10]. We align the function family F to the one used in [10], as different choices of function\n5 family F will require different assumptions about the joint distribution p(z, t, y1, y0), the representation mapping function \u03d5, and the hypothesis \u03a8. Thus, we share the same bounded constant B\u03d5.\nIn summary, the upper bound given in Theorem 1 suggests that, to bring down the target domain error \u03f5trPEHE during runtime, we are essentially minimizing: (1) the prediction errors on observed outcomes; (2) the imbalance between treated and control groups; (3) the discrepancy between the training and test sets altogether. It guides our algorithm design in general for runtime causal inference. Note that if no domain corruption exists, which means Ptr\u03d5 = Psr\u03d5 and thus IPM(Ptr\u03d5 ,Psr\u03d5 ) = 0, the runtime error becomes identical to the source domain error \u03f5trPEHE = \u03f5 sr PEHE."
        },
        {
            "heading": "3.4 Variational Inference",
            "text": "Our solution is built upon the variational autoencoder (VAE). To start with, in this section we introduce the minimization of the factual error \u03f5F, the distribution disparity between treated and control groups first, and between training and runtime domains afterwards."
        },
        {
            "heading": "3.4.1 Evidence Lower Bound",
            "text": "For modelling the observed treatment outcome y, we use the maximum likelihood estimation (MLE) to approximate the parameters. For simplicity, log is commonly used to decompose the joint marginal likelihood p(y) into:\nlog p(y) = N\u2211\nk=1\nlog p(yk)\n= N1\u2211 i=1 log p(yi) + N0\u2211 j=1 log p(yj),\n(10)\nwhere y = [y1, y2, . . . , yN ] is a vector hosting all N samples\u2019 observations, N = N1 + N0, with N1 and N0 respectively denoting the number of samples in treated and control groups. Thus, to maximize the joint marginal log-likelihood of observing y, we can maximize each individual loglikelihood log p(y).\nAs we assume that there exists a latent representation z and treatment t that causally determine the observed treatment response yt, i.e., yt \u223c p(y|z, t) in a probabilistic way, while the observed proxy x does not have any causal relations but statistical correlations with y. Due to the potentially high dimensionality of z, the marginal likelihood p(y) is intractable. Here, we apply the variational methodology [39] to our scenario to tackle p(y) by establishing an encoder network \u03d5t to learn the posterior latent representation zt \u223c p\u03d5t(z|x), and a decoder network \u03a8t to estimate treatment response yt \u223c p\u03a8t(y|z, t). According to the decomposed joint likelihood in Eq. 10, we can separately derive the the evidence lower bound (ELBOt) for each of the treatment group t in a similar manner used by [39] as follows:\nNt\u2211 i=1 log p(yi) \u2265ELBOt\n=EP\u03d5t [log p\u03a8t(y|z, t)]\u2212DKL(P\u03d5t ||Qz), (11)\nwhere P\u03d5t and Qz are posterior and prior distributions respectively over the latent space Z . DKL(\u00b7) returns the\nKullback-Leibler (KL) divergence between two distributions. As such, the task of maximizing the intractable log p(yt) can be indirectly solved by pushing up its associated ELBOt, thus minimizing the factual error \u03f5tF. According to the decomposition in Eq. 10, our objective is to maximize the sum of two ELBOs for treated and control groups:\nELBO = \u2211\nt\u2208{0,1}\nELBOt. (12)\nIt is worth noting that, our derived bound ELBO can be easily extended from our binary treatment setting to scenarios that involve multiple treatments."
        },
        {
            "heading": "3.4.2 Treated/Control Domain Adaptation",
            "text": "According to the second term in Eq. 11, for t \u2208 {0, 1}, we have both KL divergence terms that regularize the posterior distribution P\u03d5t and the prior distribution Qz, i.e., DKL(P\u03d51 ||Qz) and DKL(P\u03d50 ||Qz). By pushing up the ELBO in Eq. 12, one can notice that both posteriors P\u03d51 and P\u03d50 are regularized to approach the same prior distribution Qz, e.g., standard normal distributionN (0, 1). Thus, the domain adaptation (DA) for both groups can be naturally achieved to balance their latent distributions and counter selection bias by adjusting the priors using the VAE framework. It is worth noting that KL divergence is an unbounded asymmetric distribution distance measure [40] which does not belong to IPM, so we replace it with a bounded symmetric distribution similarity measurement in Section 3.5 as a better approximation."
        },
        {
            "heading": "3.4.3 Training/Runtime Domain Adaptation",
            "text": "In addition to the DA across treated and control groups within the training set, we would also like to do DA between the entire training and runtime sets to minimize the tightness bound B\u03d5IPMF (Ptr\u03d5 ,Psr\u03d5 ) given in Theorem 1 and thus alleviate runtime domain corruption. As such, for a well-trained model, we aim to make the out-of-sample performance as good as the in-sample performance, i.e., the out-of-sample results would not deviate from the in-sample ones drastically while keeping good in-sample performance.\nIntuitively, if the VAE prediction framework is applied to the full runtime test set {(xtrj , ttrj , ytrj )}N \u2032\nj=1 on the target domain, one can end up with an objective to be maximized similar to the ELBOt presented in Eq. 11 as follows:\n\u0393\u03d5trt ,\u03a8trt = EPtr\u03d5t [log p tr \u03a8t(y|z, t)]\u2212DKL(P tr \u03d5t ||Q tr z ). (13)\nHowever, the label ytr and treatments ttr are apparently unknown in practice, and such an objective cannot be optimized. Since the only available information is the runtime covariates which can be used to extract the domaininvariant representation from DA, the second term in Eq. 13 can be utilised for such purpose with a mild modification.\nPrecisely, we alternatively walk around to minimize the KL divergence between the runtime posterior Ptr\u03d5 and the entire training set posterior Psr\u03d5 , namely DKL(Ptr\u03d5 ||Psr\u03d5 ), where \u03d5 is a shared feature extractor. Thus, to achieve the second-stage DA, our proposed ultimate evidence lower\n6 bound (ELBOulti) for the intractable joint log-likelihood p(y) is:\nlog p(y) \u2265 ELBO \u2265 ELBOulti = ELBO\u2212DKL(Ptr\u03d5 ||Psr\u03d5 ).\n(14)"
        },
        {
            "heading": "3.5 Adversarial Learning",
            "text": "Thus far, we have three DKL(\u00b7) terms in our optimization objective: two from the in-sample treated/control groups, which align the posteriors to the same priorN (0, 1), and the one from out-of-sample train/test adaptation that aligns the posteriors of training set and runtime set. As the direct calculation of KL divergence is computationally inefficient and may even be infeasible with high dimensional data [41], [42], we propose to implicitly minimize them and unify these terms into a compact generative adversarial network (GAN) [25] shown in Figure 1. Apart from that, optimizing the minimax game in GAN is equivalent to minimizing the JensenShannon divergence [25], which is a bounded symmetric distribution similarity measurement [43]. Such technique resonates with adversarial variational Bayes introduced in [44], while our motivation and implementation differ from theirs. Here, we present our Variational autoEncoder Generative Adversarial Network runtime counterfactual regression model, coined as VEGAN. In what follows, we unfold the design details of VEGAN.\nFirstly, we instantiate \u03d5, the shared feature extractor among xsrt=1, x sr t=0 and x\ntr. It includes G\u03d5 and the following two multi-layer perceptrons (MLPs) that map all the data from the original Rd into latent space Rl. Due to the variational nature of the model, the j-th latent dimension of individual i, denoted by xsri , is modelled by a Gaussian distribution with its dedicated mean \u00b5ij and variance \u03c32ij as follows:\npsr\u03d5 (zi|xi) = l\u220f\nj=1\nN (\u00b5ij , \u03c32ij), (15)\nwhere mean \u00b5ij and standard deviation \u03c3ij are respectively the j-th element of latent representations \u00b5i and \u03c3i. In VEGAN, \u00b5i,\u03c3i \u2208 Rl are denoted as:{\n\u00b5i = MLP\u00b5(G\u03d5(xsri )) \u03c3i = MLP\u03c3(G\u03d5(xsri )) , (16)\nwhich allows us to obtain the latent representation zi for the subsequent DAs and inference.\nSecondly, for the treated/control group DA, we propose an adversarial way to implicitly reduce inter-domain distribution distance. In this regard, G\u03d5 is essentially our generator for notation simplicity, and a discriminator D\u03b4 is thus designed to pair up the generator to facilitate adversarial learning. The minimax game is designed as: the discriminator D\u03b4 tries to differentiate the standard Gaussian sample n \u223c N (0, 1) from zsr learned from the training sample; in the meantime, feature extractor G\u03d5 tries to update the latent representation zsr to make it indistinguishable from n. When an equilibrium state is reached, the treated and control domains are well adapted because both latent representations zsr fall in the same distribution. Thus, the two terms DKL(P\u03d51 ||Qz) and DKL(P\u03d50 ||Qz), are minimized in an\nAlgorithm 1 Optimization of VEGAN\n1: Input: Train set {(xsri , tsri , ysri )}Ni=1, runtime variable set {(xtrj )}N \u2032\nj=1 (optional), hyperparameters (e.g., learning rate \u03b1), initialized neural networks\u2019 parameters {\u03d5,\u03a81,\u03a80, \u03b4, \u03b2};\n2: while not converged do 3: Sample a mini-batch bm \u2208 {bm}Mm=1 of size\n|bm| = |bt=1m |+ |bt=0m |, with |bt=1m | = |bt=0m |, from {(xsri , tsri , ysri )}Ni=1. Sample equal size instances from standard Gaussian N (0, 1);\n4: \u03b4 \u2190 \u03b4 \u2212 \u03b1 1|bm| (\u2211|bm| j=1 \u2207\u03b4 logD\u03b4(nj)\n+ \u2211|bm| i=1 \u2207\u03b4 log(1\u2212D\u03b4(\u03c9sri )) )\n; 5: if runtime domain corruption exists then 6: Randomly draw batch b \u2032\nm from {(xtrj )}N \u2032\nj=1 with size |b\u2032m| = |bm|;\n7: \u03b2 \u2190 \u03b2 \u2212 \u03b1 1|bm| (\u2211|bm| i=1 \u2207\u03b2 logD\u03b2(\u03c9sri )\n+ \u2211|bm| i=1 \u2207\u03b2 log(1\u2212D\u03b2(\u03c9tri )) )\n; 8: end if 9: \u03d5\u2190 \u03d5\u2212 \u03b1 ( 1 |bm| \u2211|bm|\ni=1 \u2207\u03d5 logD\u03b4(\u03c9sri ) + 1|bm| \u2211|bm| i=1 \u2207\u03d5 log(1\u2212D\u03b2(\u03c9sri ))\n+ 1|bm| \u2211|bm|\ni=1 \u2207\u03d5 logD\u03b2(\u03c9tri ) + \u2211\nt\u2208{0,1} 1 |btm| \u2211|btm| i=1 \u2207\u03d5 log p\u03a8t(yi|\u03c9i, ti) ) ,\n10: \u03a81 \u2190 \u03a81 \u2212 \u03b1|bt=1m | \u2211|bt=1m | i=1 \u2207\u03a81 log p\u03a81(yi|\u03c9i, ti = 1),\n11: \u03a80 \u2190 \u03a80 \u2212 \u03b1|bt=0m | \u2211|bt=0m |\ni=1 \u2207\u03a80 log p\u03a80(yi|\u03c9i, ti = 0); 12: end while\nadversarial way. As Figure 1 shows, the output pi = D\u03b4(wi) is the scalar probability of being a Gaussian sample, where wi = \u03b7ini + (1 \u2212 \u03b7i)zsri with \u03b7i \u2208 {1, 0} labelling the ith sample from two buckets (1 for Gaussian samples, and 0 for training samples). Note that n is resampled for every training instance i \u2208 I , where I is the collection of instances from the training set. In our supervised learning setting, we have the cross-entropy loss for the discriminator D\u03b4(\u00b7):\nl(wi) = \u03b7i logD\u03b4(wi) + (1\u2212 \u03b7i) log(1\u2212D\u03b4(wi)). (17)\nThen, in an adversarial setting, the minimization of two KL-divergence terms for treated/control domain adaptation is replaced by the following:\nmin \u03d5 max \u03b4\nEI [\u03b7i logD\u03b4(wi) + (1\u2212 \u03b7i) log(1\u2212D\u03b4(wi))]\n\u21d0\u21d2 min \u03d5 max \u03b4 EN (0,1)[logD\u03b4(n)] + EI [log(1\u2212D\u03b4(zsri ))]. (18)\nAnalogously, for the train/runtime domain adaptation, we design another discriminator D\u03b2(\u00b7) to form the second GAN system between G\u03d5(\u00b7) and D\u03b2(\u00b7), where D\u03b2(\u00b7) predicts the probability p\u2032j of the sample j, where j \u2208 J is the collection of the test set, from the source domain (i.e., training set). The only difference from the first GAN system is that, it takes the training sample as real while the runtime sample is treated as fake. Thus, DKL(Ptr\u03d5 ||Psr\u03d5 ) is replaced by the following:\nmin \u03d5 max \u03b2\nEI [logD\u03b2(zsri )] + EJ [log(1\u2212D\u03b2(ztrj ))]. (19)\n7\nFinally, to build the probabilistic model p(y|z, t), we model each of the treatment classes through two separate MLPs, namely \u03a81 and \u03a80 respectively, thus the general representation of modelling the observed outcome y for individual i is given as p\u03a8t(yi|zi, ti) = N (\u00b5\u0302i, \u03c3\u03022i ), where \u00b5\u0302t,i = \u03a8ti(z sr i , ti), and we follow [11] to set \u03c3\u0302 2 i = 1 for simplicity. In a nutshell, to promote a computationally efficient algorithm, we propose to minimize the following loss function L along with optimizing the minimax game together such that the ELBOulti in Eq. 14 will be maximized:\nmin \u03d5,\u03a81,\u03a80 max \u03b4,\u03b2\n{ EN [logD\u03b4(n)] + EZsr [log(1\u2212D\u03b4(z))]\n+ EZsr [logD\u03b2(z)] + EZtr [log(1\u2212D\u03b2(z))] + L } ,\n(20)\nwhere L = \u2212 ( EPsr\u03d51 [log p\u03a81(y|z, t = 1)] + EPsr\u03d50 [log p\u03a80(y|z, t = 0)] ) .\n(21) We summarize our VEGAN model optimization scheme in Algorithm 1. Please note that the notation changed accordingly as the reparameterization trick z = \u03c9(\u00b5\u03d5,\u03c32\u03d5, \u03f5) [39] is applied as a necessity to get the gradient \u2207\u03d5 for the feature extractor G\u03d5. Also, the original minimax game in Eq. 20 is adjusted to the double minimization tradition for gradient descent."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we evaluate the proposed VEGAN framework in dealing with the runtime domain corruption by answering the following research questions (RQs):\n\u2022 RQ1: How does VEGAN perform compared with other state-of-the-art models?\n\u2022 RQ2: How effective is the proposed dual-stage DA in VEGAN? \u2022 RQ3: Is VEGAN computationally efficient compared to other VAE-based models? \u2022 RQ4: As a classic solution to missing variables in prediction tasks, is data imputation on par with VEGAN\u2019s performance when handling runtime domain corruption? \u2022 RQ5: Is our proposed second-stage plug-in applicable to other existing methods?"
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": ""
        },
        {
            "heading": "4.1.1 Datasets and Domain Corruption Simulation",
            "text": "We utilize two popular semi-synthetic datasets in the causal inference literature, which are introduced below.\n\u2022 Infant Health and Development Program (IHDP) [45]. The IHDP dataset contains 25 covariates and 747 samples, assessing the effectiveness of early childhood interventions for low-birth-weight infants. To evaluate the causal model, the treatment outcomes are simulated according to [45]. In our test setting, seven privacy-related features are selected as target variables, i.e., {momage, sex, twin, b.marr, cig, drugs, work dur}, which are corrupted at different corruption levels (CLs), where the CL denotes the severity of the domain corruption ranging from 0% to 100%. While the rest 18 features remain unchanged. This\n8 is to mimic a typical runtime domain corruption scenario where individuals provide none or falsified privacy-related information for the trained model. We test CL \u2208 {5%, 12.5%, 20%, 33.3%, 100%} on IHDP.\n\u2022 Atlantic Causal Inference Conference (ACIC) 2019 [46]. The ACIC 2019 dataset is a high-dimensional dataset of 200 covariates and 1,000 samples, which are drawn from publicly available data and the treatment outcomes are also simulated. In our test setting, since there is no clear definition of sensitive features, we treat all covariates as target variables for runtime domain corruption. As such, we test CL in {5%, 12.5%, 20%, 33.3%}, given that CL=100% will wipe out all the covariates in ACIC.\nEach of the dataset is randomly split with 3:1 ratio for train and test. As per our definition, runtime domain corruption entails both a shift in the covariate distribution and missing values in the test set. To simulate the distribution shift, for each target feature xs \u2208 xi, we perform the following with the probability specified by each CL: (1) we add noise drawn from Gaussian distribution N (\u00b5\u0304, 0.1) to xs if xs is continuous; (2) we flip its value if xs is binary. To simulate the missing values, we scan each target feature xs and drop it (via zero-padding) with probability CL. The two corruption steps are performed independently on the same test set."
        },
        {
            "heading": "4.1.2 Baselines and Evaluation Metrics",
            "text": "We compare VEGAN with nine causal inference baselines as the following:\n\u2022 TARNet [10] is a base deep learning framework with a shared feature extractor and two decoders modelling the treated and control effects, respectively. \u2022 CFRWASS [10] is a variant of TARNet, with a latent distribution balancing regularization (Wasserstein distance) to overcome the confounding bias introduced by the imbalance between the treated and control groups. \u2022 CEVAE [11] is a variational autoencoder framework which focuses on modelling the robust latent variable to handle the confounding bias from a probabilistic perspective. \u2022 SITE [47] explores the importance of the local similarity preservation as a constraint to improve ITE estimation, and proposes a deep representation learning method to help preserve the local similarity and balance data distribution altogether. \u2022 DragonnetBase [12] is based on TARNet, but it additionally provides an end-to-end procedure for predicting propensity score to adjust the confounding bias when estimating the treatment effects. \u2022 DragonnetTR [12] is built on top of DragonnetBase, the updated model introduces the novel targeted regularization based on the non-parametric estimation theory, which provides an asymptotic property with a suitable downstream estimator. \u2022 BTARNET [9] enhances the decoders of TARNet with Monte Carlo dropout technique to quantify the uncertainty when estimating the treatment effect.\n\u2022 BCEVAE [9] takes CEVAE as a base model, and incorporates the Monte Carlo dropout into its generative network for uncertainty quantification. \u2022 TEDVAE [28] is a latent variable disentangle model based on a three-headed variational autoencoder, which tries to learn the disentangled latent instrumental, risk, and confounding factors, respectively, from the observed covariates."
        },
        {
            "heading": "4.1.3 Implementation",
            "text": "Our model is implemented with PyTorch [48]. The hyperparameters are tuned according to the models\u2019 performance on validation set. Our tuned hyperparameters are shown in Table 1, respectively. All the experiments are conducted with RTX-3090 on Ubuntu 22.04 LTS platform where GPU training is enabled, otherwise the 12th Gen Intel i7-12700K 12-Core 20-Thread CPU is used."
        },
        {
            "heading": "4.2 Performance Evaluation (RQ1)",
            "text": ""
        },
        {
            "heading": "4.2.1 Out-of-Sample Prediction under Runtime Domain Corruption",
            "text": "IHDP Dataset. For predictions on the corrupted, out-ofsample instances, we conduct the tests on the test set with five corruption ratios CL \u2208 {5%, 12.5%, 20%, 33.3%, 100%}. Notably, CL = 100% represents an extreme case where all the seven sensitive features are completely inaccessible during runtime and only the remaining 18 variables are available for prediction. As Table 2 demonstrates, VEGAN yields the second best performance when the domain corruption is relatively restrained, and obtains the highest accuracy after the corruption ratio increases to and beyond 20%. The best baseline is CFRWASS when CL is low, but it overfits the training set significantly and thus does not generalize to a higher domain corruption level, while VEGAN is more robust to the stronger corruption on IHDP\u2019s private variables.\nACIC. Since there is no clear definition for all 200 features on ACIC 2019 dataset, we allow the corruption to take place for all the features in ACIC dataset with a ratio of CL \u2208 {5%, 12.5%, 20%, 33.3%}. With this, we can mimic situations where individuals can withhold an arbitrary combination of variables in privacy-sensitive applications. Note, that we omit CL = 100% in ACIC dataset as it will set all variables to zero and thus make any predictions infeasible. As a result, VEGAN outperforms all the other models for out-of-sample prediction as shown in Table 3."
        },
        {
            "heading": "4.2.2 In-Sample Prediction without Runtime Domain Corruption",
            "text": "Besides the out-of-sample prediction under the run-time corruption, we also investigate the traditional in-sample inference, where there no corruption happens, i.e., there is neither distribution shift nor missing variables. Table 4 and 5 show the in-sample prediction results on both CATE and ITE estimation, for which our model performs the best in estimating CATE while staying competitive for ITE estimation on the IHDP dataset, and outperforms all the other models on the ACIC dataset.\n9\nTABLE 2: \u221a \u03f5PEHE of out-of-sample prediction on IHDP dataset with different corruption levels on private features.\nTABLE 3: \u221a \u03f5PEHE of out-of-sample prediction on ACIC dataset with different corruption levels on all features."
        },
        {
            "heading": "4.2.3 Volatility Analysis",
            "text": "It is noted that when the domain corruption level climbs, the fluctuations of prediction errors are small in magnitude on ACIC 2019 dataset. To better quantify the advantage of VEGAN under domain corruption on ACIC, we analyse the deviation (\u2206) of each model\u2019s performance between in-sample and corrupted prediction tasks in Figure 2, i.e., \u2206 = 100%\u00d7 |\u03f5in-sample\u2212 \u03f5corrupted|/\u03f5in-sample. \u2206 quantifies the instability of the model, as we commonly rely on models obtained with the training set and prefer lower generalization errors. All models become more volatile as CL increases, while VEGAN maintains an excellent stability with only 0.22% variation at corruption level 33.3% and achieves the best accuracy in terms of \u221a \u03f5PEHE."
        },
        {
            "heading": "4.3 Effectiveness of Second-Stage DA (RQ2)",
            "text": "As VEGAN\u2019s main highlight is the second-stage adversarial DA as a plug-in component, we conduct an ablation study to compare the performance of VEGAN and VEGANI on both datasets, where VEGANI is a degraded version with the second-stage DA removed. The results in Figure 3 indicate that, when CL is low, both two models are comparable. However, when CL goes higher, the advantage of the second-stage plug-in becomes significant. Thus, with our proposed second-stage DA, VEGAN is shown to have higher generalization ability than VEGANI across different scenarios."
        },
        {
            "heading": "4.4 Computational Efficiency & Stability of VEGAN (RQ3)",
            "text": "One core motivation for utilizing GAN to replace the straightforward KL divergence optimization is to preserve training efficiency under high dimensionality. Hence, we further test VEGAN\u2019s efficiency by comparing its training\ntime (in seconds) per 100 epochs with CEVAE and VEGANI. To ensure a fair comparison, the tests are performed on 12th Gen Intel i7-12700K 12-Cores 20-Threads CPU on Ubuntu 22.04 LTS. Figure 4 shows that VEGANI, which can be\n10\nviewed as an amplified version of CEVAE with the introductions of GAN, has significantly faster training speed (over 6\u00d7 speedup). Furthermore, the introduction of our secondstage adversarial DA in VEGAN is still able to maintain high computational efficiency, witnessed by over 4\u00d7 speedup over CEVAE.\nAs GAN is known to be unstable during training, We provide the stability analysis in terms of prediction loss convergence and equilibrium status between feature extractor and discriminators. Figure 5 shows the models\u2019 convergence on root mean square error (RMSE) during training. It is noted that a higher corruption level brings more challenges in the adversarial training, but we observe an equilibrium state in the majority of the cases. Taking the more challenging 33.3% CL in ACIC dataset as an example, both discriminators (for treated/control and training/runtime adaptations) can quickly converge to the equilibrium by returning an average binary cross-entropy loss of 0.69, which\nmeans the discriminators are completely deceived by the feature extractors, and always give 0.5 probability for the samples from each of the groups. As such, training VEGAN in an adversarial setting is completely attainable."
        },
        {
            "heading": "4.5 Comparison with Imputation Method (RQ4)",
            "text": "As imputation is a natural choice to handle missing values, we test the effectiveness of VEGAN against data imputation methods on ACIC\u2019s corrupted out-of-sample test sets. Specifically, we implement the imputation algorithm MICE [49], which has been widely adopted in treatment effect estimation [31], [32]. We denoted imputation-enhanced models with \u201c*\u201d in Table 6. The results indicate that, when the corruption rate is low, using imputation is generally helpful for slightly increasing the prediction performance compared to Table 3, but the improvements remain marginal and less significant compared with VEGAN. In short, data imputation has very limited benefits under the domain corruption setting. Furthermore, in scenarios where an attribute is completely missing for all instances, it is infeasible to impute this attribute based on its distribution within existing test samples for prediction."
        },
        {
            "heading": "4.6 Applicability of Second-Stage DA to Other Baselines (RQ5)",
            "text": "To demonstrate the applicability of our proposed secondstage adversarial DA plug-in to other state-of-the-arts, we study its compatibility with the most representative baseline TARNet. The experiments are conducted using the ACIC dataset, and the results are presented in Table 7. We denote the TARNet with adversarial plug-in as TARNet+. As the results suggest, there is a transferable benefit to the other baseline with our proposed second-stage adversarial plugin when the corruption level becomes higher, the benefit of the second-stage domain adaptation will be enlarged. When the adversarial plug-in is in use, it effectively helps TARNet reduce prediction risks under runtime domain corruption as the volatility of the TARNet+ is stabilized at around 2%."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper formalizes the runtime causal inference problem under domain corruption, where novel strategies are proposed to counter the imbalance between treated and control groups and the inter-domain discrepancy between training\n11\nand inference domain. We further adopt adversarial learning to replace the direct calculation of KL-divergence to improve computational efficiency. For our proposed approach VEGAN framework with second-stage domain adaptation, its performance exceeds other state-of-the-arts under the runtime domain corruption setting in semi-synthetic and full-synthetic benchmark datasets. In addition, the secondstage adversarial plug-in is demonstrated as applicable to the off-the-shelf models to reduce generalization errors."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "This work is supported by the Australian Research Council under the streams of Industrial Transformation Training Centre (No. IC200100022), Future Fellowship (No. FT210100624), Discovery Project (No. DP190101985), and Discovery Early Career Researcher Award (No. DE230101033)."
        }
    ],
    "title": "Variational Counterfactual Prediction under Runtime Domain Corruption",
    "year": 2023
}