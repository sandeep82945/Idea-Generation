{
    "abstractText": "For extreme multi-label classification (XMC), existing classification-based models poorly perform for tail labels and often ignore the semantic relations among labels, like treating \u201cWikipedia\u201d and \u201cWiki\u201d as independent and separate labels. In this paper, we cast XMC as a generation task (XLGen), where we benefit from pre-trained text-to-text models. However, generating labels from the extremely large label space is challenging without any constraints or guidance. We, therefore, propose to guide label generation using label cluster information to hierarchically generate lower-level labels. We also find that frequencybased label ordering and using decoding ensemble methods are critical factors for the improvements in XLGen. XLGen with cluster guidance significantly outperforms the classification and generation baselines on tail labels, and also generally improves the overall performance in four popular XMC benchmarks. In human evaluation, we also find XLGen generates unseen but plausible labels. Our code is now available at https://github.com/ alexa/xlgen-eacl-2023.",
    "authors": [
        {
            "affiliations": [],
            "name": "Taehee Jung"
        },
        {
            "affiliations": [],
            "name": "Joo-Kyung Kim"
        },
        {
            "affiliations": [],
            "name": "Sungjin Lee"
        },
        {
            "affiliations": [],
            "name": "Dongyeop Kang"
        }
    ],
    "id": "SP:e703ce7b7221971a0f224841b1284ffb4724eb20",
    "references": [
        {
            "authors": [
                "Enrique Amigo",
                "Agust\u00edn Delgado."
            ],
            "title": "Evaluating extreme hierarchical multi-label classification",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5809\u20135819, Dublin, Ireland.",
            "year": 2022
        },
        {
            "authors": [
                "Rohit Babbar",
                "Bernhard Sch\u00f6lkopf."
            ],
            "title": "Dismec: Distributed sparse machines for extreme multi-label classification",
            "venue": "Proceedings of the tenth ACM international conference on web search and data mining, pages 721\u2013729.",
            "year": 2017
        },
        {
            "authors": [
                "Peter F Brown",
                "Vincent J Della Pietra",
                "Peter V Desouza",
                "Jennifer C Lai",
                "Robert L Mercer."
            ],
            "title": "Classbased n-gram models of natural language",
            "venue": "Computational linguistics, 18(4):467\u2013480.",
            "year": 1992
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Wei-Cheng Chang",
                "Hsiang-Fu Yu",
                "Kai Zhong",
                "Yiming Yang",
                "Inderjit S Dhillon."
            ],
            "title": "Taming pretrained transformers for extreme multi-label text classification",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge",
            "year": 2020
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "ACL, pages 889\u2013898.",
            "year": 2018
        },
        {
            "authors": [
                "Eva L. Gibaja."
            ],
            "title": "A tutorial on multi-label learning",
            "venue": "ACM Computing Surveys, 47:1\u201338.",
            "year": 2015
        },
        {
            "authors": [
                "N. Gupta",
                "S. Bohra",
                "Y. Prabhu",
                "S. Purohit",
                "M. Varma."
            ],
            "title": "Generalized zero-shot extreme multi-label learning",
            "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining.",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "ICLR.",
            "year": 2020
        },
        {
            "authors": [
                "Wenpeng Hu",
                "Ran Le",
                "Bing Liu",
                "Feng Ji",
                "Jinwen Ma",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "Predictive adversarial learning from positive and unlabeled data",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, pages 7806\u20137814.",
            "year": 2021
        },
        {
            "authors": [
                "Himanshu Jain",
                "Venkatesh Balasubramanian",
                "Bhanu Chunduri",
                "Manik Varma."
            ],
            "title": "Slice: Scalable linear extreme classifiers trained on 100 million labels for related searches",
            "venue": "Proceedings of the Twelfth ACM International Conference on Web",
            "year": 2019
        },
        {
            "authors": [
                "Atsushi Kanehira",
                "Tatsuya Harada."
            ],
            "title": "Multilabel ranking from positive and unlabeled data",
            "venue": "CVPR, pages 5138\u20135146.",
            "year": 2016
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei."
            ],
            "title": "Scaling laws for neural language models",
            "venue": "arXiv:2001.08361.",
            "year": 2020
        },
        {
            "authors": [
                "Joo-Kyung Kim",
                "Young-Bum Kim."
            ],
            "title": "Pseudo labeling and negative feedback learning for largescale multi-label domain classification",
            "venue": "ICASSP, pages 7964\u20137968.",
            "year": 2020
        },
        {
            "authors": [
                "Juho Lee",
                "Yoonho Lee",
                "Jungtaek Kim",
                "Adam R. Kosiorek",
                "Seungjin Choi",
                "Yee Whye Teh."
            ],
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks",
            "venue": "ICML, pages 3744\u20133753.",
            "year": 2019
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pretraining for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "ICLR.",
            "year": 2019
        },
        {
            "authors": [
                "Jinseok Nam",
                "Eneldo Loza Menc\u00eda",
                "Hyunwoo J Kim",
                "Johannes F\u00fcrnkranz."
            ],
            "title": "Maximizing subset accuracy with recurrent neural networks in multilabel classification",
            "venue": "Advances in neural information processing systems, 30:5413\u20135423.",
            "year": 2017
        },
        {
            "authors": [
                "Yashoteja Prabhu",
                "Anil Kag",
                "Shrutendra Harsola",
                "Rahul Agrawal",
                "Manik Varma."
            ],
            "title": "Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising",
            "venue": "Proceedings of the 2018 World Wide Web Conference,",
            "year": 2018
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "JMLR, 21:1\u201367.",
            "year": 2020
        },
        {
            "authors": [
                "Wissam Siblini",
                "Pascale Kuntz",
                "Frank Meyer."
            ],
            "title": "CRAFTML, an efficient clustering-based random forest for extreme multi-label learning",
            "venue": "International Conference on Machine Learning, pages 4664\u20134673.",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Simig",
                "Fabio Petroni",
                "Pouya Yanki",
                "Kashyap Popat",
                "Christina Du",
                "Sebastian Riedel",
                "Majid Yazdani."
            ],
            "title": "Open vocabulary extreme classification using generative models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Yukihiro Tagami."
            ],
            "title": "AnnexML: Approximate nearest neighbor search for extreme multi-label classification",
            "venue": "Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages 455\u2013464.",
            "year": 2017
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Jinfeng Rao",
                "William Fedus",
                "Samira Abnar",
                "Hyung Won Chung",
                "Sharan Narang",
                "Dani Yogatama",
                "Ashish Vaswani",
                "Donald Metzler."
            ],
            "title": "Scale efficiently: Insights from pre-training and fine-tuning transformers",
            "venue": "ICLR.",
            "year": 2022
        },
        {
            "authors": [
                "Che-Ping Tsai",
                "Hung-Yi Lee."
            ],
            "title": "Order-free learning alleviating exposure bias in multi-label classification",
            "venue": "AAAI, pages 6038\u20136045.",
            "year": 2020
        },
        {
            "authors": [
                "Laurens van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-SNE",
            "venue": "Journal of Machine Learning Research, 9(86):2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "Marek Wydmuch",
                "Kalina Jasinska",
                "Mikhail Kuznetsov",
                "R\u00f3bert Busa-Fekete",
                "Krzysztof Dembczynski."
            ],
            "title": "A no-regret generalization of hierarchical softmax to extreme multi-label classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Yuanhao Xiong",
                "Wei-Cheng Chang",
                "Cho-Jui Hsieh",
                "Hsiang-Fu Yu",
                "Inderjit Dhillon."
            ],
            "title": "Extreme Zero-Shot learning for extreme text classification",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yang",
                "Fuli Luo",
                "Shuming Ma",
                "Junyang Lin",
                "Xu Sun."
            ],
            "title": "A deep reinforced sequence-toset model for multi-label classification",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5252\u20135258.",
            "year": 2019
        },
        {
            "authors": [
                "Pengcheng Yang",
                "Xu Sun",
                "Wei Li",
                "Shuming Ma",
                "Wei Wu",
                "Houfeng Wang."
            ],
            "title": "SGM: sequence generation model for multi-label classification",
            "venue": "COLING, pages 3915\u20133926.",
            "year": 2018
        },
        {
            "authors": [
                "Hui Ye",
                "Zhiyu Chen",
                "Da-Han Wang",
                "Brian Davison."
            ],
            "title": "Pretrained generalized autoregressive model with adaptive probabilistic label clusters for extreme multi-label text classification",
            "venue": "International Conference on Machine Learning, pages 10809\u201310819.",
            "year": 2020
        },
        {
            "authors": [
                "Ian EH Yen",
                "Xiangru Huang",
                "Wei Dai",
                "Pradeep Ravikumar",
                "Inderjit Dhillon",
                "Eric Xing."
            ],
            "title": "Ppdsparse: A parallel primal-dual sparse method for extreme classification",
            "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowl-",
            "year": 2017
        },
        {
            "authors": [
                "Ronghui You",
                "Zihan Zhang",
                "Ziye Wang",
                "Suyang Dai",
                "Hiroshi Mamitsuka",
                "Shanfeng Zhu."
            ],
            "title": "AttentionXML: Label tree-based attention-aware deep model for high-performance extreme multi-label text classification",
            "venue": "Advances in Neural Information",
            "year": 2019
        },
        {
            "authors": [
                "Hsiang-Fu Yu",
                "Prateek Jain",
                "Purushottam Kar",
                "Inderjit S. Dhillon."
            ],
            "title": "Large-scale multi-label learning with missing labels",
            "venue": "ICML, pages 593\u2013 601.",
            "year": 2014
        },
        {
            "authors": [
                "Hsiang-Fu Yu",
                "Kai Zhong",
                "Inderjit S Dhillon."
            ],
            "title": "Pecos: Prediction for enormous and correlated output spaces",
            "venue": "JMLR, 23:1\u201332.",
            "year": 2022
        },
        {
            "authors": [
                "Jiong Zhang",
                "Wei-cheng Chang",
                "Hsiang-fu Yu",
                "Inderjit Dhillon."
            ],
            "title": "Fast multi-resolution transformer fine-tuning for extreme multi-label text classification",
            "venue": "Advances in Neural Information Processing Systems, 34:7267\u20137280.",
            "year": 2021
        },
        {
            "authors": [
                "Ximing Zhang",
                "Qian-Wen Zhang",
                "Zhao Yan",
                "Ruifang Liu",
                "Yunbo Cao."
            ],
            "title": "Enhancing label correlation feedback in multi-label text classification via multi-task learning",
            "venue": "ACL Findings.",
            "year": 2021
        },
        {
            "authors": [
                "Daoming Zong",
                "Shiliang Sun."
            ],
            "title": "BGNN-XML: Bilateral graph neural networks for extreme multilabel text classification",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, pages 1\u201312.",
            "year": 2022
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "by the classifier from the previous cluster level, called matcher-aware negatives (MAN)",
            "year": 2021
        },
        {
            "authors": [
                "XR-Linear (Yu"
            ],
            "title": "2022) has a very similar architecture with XR-Transformer, except that it only uses simple tf-idf text features instead of transformer encoder outputs. For OVA classification, linear matchers recursively solve XMC sub-problem",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "For extreme multi-label classification (XMC), existing classification-based models poorly perform for tail labels and often ignore the semantic relations among labels, like treating \u201cWikipedia\u201d and \u201cWiki\u201d as independent and separate labels. In this paper, we cast XMC as a generation task (XLGen), where we benefit from pre-trained text-to-text models. However, generating labels from the extremely large label space is challenging without any constraints or guidance. We, therefore, propose to guide label generation using label cluster information to hierarchically generate lower-level labels. We also find that frequencybased label ordering and using decoding ensemble methods are critical factors for the improvements in XLGen. XLGen with cluster guidance significantly outperforms the classification and generation baselines on tail labels, and also generally improves the overall performance in four popular XMC benchmarks. In human evaluation, we also find XLGen generates unseen but plausible labels. Our code is now available at https://github.com/ alexa/xlgen-eacl-2023."
        },
        {
            "heading": "1 Introduction",
            "text": "Extreme multi-label classification (XMC) is a task to predict multiple relevant labels for a given input where the label space is extremely large. Conventional approaches for XMC decompose the problem into a set of binary classifications, training one-vs-all classifiers for each label. However, they encounter several issues in practical use cases.\nFirst, the labels in XMC are long-tail distributed. In other words, only a few labels have sufficient positive samples, thereby the other infrequent labels could be rarely predicted during inference as we see the heavily right-skewed distribution in the\n\u2217 Part of this work was done during an internship at Amazon Alexa AI.\ncompletely new labels from input text, e.g., soda, inferred from context that other carbonated beverages can replace diet coke.\nlong-tail in Figure 2a. Second, multi-label classification techniques such as one-by-one and label powerset (Gibaja, 2015) assume independent and identically distributed labels, while the usergenerated labels in XMC are dependent on each other. Moreover, annotated labels are only a portion of possible labels, thus, resulting in positive and unlabeled (PU) setting (Yu et al., 2014; Kanehira and Harada, 2016).\nIn this paper, we tackle extreme multi-label classification with a generative approach, called extreme multi-label generation (XLGen). In particular, we fine-tune a pre-trained Transformer-based encoder-decoder model (Raffel et al., 2020) with input documents and their known positive labels. This (label) generation approach is more intuitive and closely similar to how humans tag documents with text labels without a fine-grained ontology or\nar X\niv :2\n30 2.\n09 15\n0v 1\n[ cs\n.C L\n] 1\n7 Fe\nb 20\n23\nguideline. However, the generated labels from the extremely large label space without any constraints and/or guidance can be noisy and not cover infrequent labels. To address this issue, we propose a method to leverage label clusters into generation: first, generate cluster IDs of semantically similar labels, and then generate text labels utilizing the cluster IDs as additional contextual inputs. Specifically, we propose two XLGen architectures (XLGen-BCL, XLGen-MCG) in which such clusters are jointly trained with labels in different ways. Using clusters for label generation is motivated by showing label categories to human annotators. As an example, humans often start by setting highlevel topics first and then hierarchically create actual tags under each high-level topic. The clusters, however, are treated as additional guidance rather than a constraint since we do not restrict the model to only predict labels under the given clusters.\nSimilarly to XLGen, Simig et al. (2022) proposed GROOV, which fine-tunes T5 to generate labels in XMC. In particular, GROOV aims to implement a label order invariant training objective by randomly shuffling label orders and using multisoftmax function, which does not penalize if any first tokens of true labels are predicted regardless of the label orders. However, it does not outperform classification baselines consistently, and we empirically find that label order by frequency helps alleviate the issue in our ablation study.\nOur experiment shows that XLGen (and its variants) outperforms classification baselines on four XMC benchmarks. Furthermore, XLGen with cluster guidance (XLGen-BCL and -MCG) significantly and consistently outperforms the classification and\ngeneration baseline (XLGen-base) on tail labels, respectively. The effect on tail labels from XLGenBCL is demonstrated in Figure 2b.\nFigure 1 shows predicted or generated labels from different models. A Wikipedia page of diet coke and mentos eruption has true labels such as \u201cbeverage\u201d, \u201cfun\u201d, and \u201ceruption\u201d. We find XLGen can also generate a new positive label \u201csoda\u201d based on the context of \u201ccarbonated beverage\" in the input text. From a human evaluation (S6.1), we find newly generated labels by XLGen are highly associated with the input texts, which potentially helps automatically find new labels without manual tagging. We also show the generated labels from large language models (LLMs) like GPT-3 (Brown et al., 2020): we find that the overall performance of incontext learning is significantly less than XLGen (See \u00a74.4 for details), but LLMs could generate reasonable labels with a few examples as XLGen does."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "Extreme multi-label classification (XMC).",
            "text": "Classification-based approaches on XMC suffer from dealing with enormous label spaces under the one-vs-all classification setting (Babbar and Sch\u00f6lkopf, 2017; Yen et al., 2017; Jain et al., 2019). To address the efficiency issue, state-of-the-art XMC models partition label space to the scalable subsets via hierarchical clustering (Prabhu et al., 2018; Wydmuch et al., 2018; You et al., 2019; Chang et al., 2020; Yu et al., 2022; Tagami, 2017), graph-based approximations (Jain et al., 2019; Zong and Sun, 2022), or random forest (Siblini et al., 2018). However, they still suffer from predicting tail or unseen labels. To efficiently deal with such long-tail issues, few-shot learning frameworks and methods (Gupta et al., 2021; Xiong et al., 2022) are proposed. Rather, we show how encoder-decoder language model can improve tail label scores by fine-tuning it with guidance from label clusters.\nLMs and Generative approach in XMC. For XMC, pre-trained LMs such as XLNet (Ye et al., 2020) and Transformer (Chang et al., 2020) are used but only for encoding input texts, thus, it still relies on the classification approach for label prediction. Previously, other works address multi-label classification with generative approaches (Nam et al., 2017; Tsai and Lee, 2020; Yang et al., 2018, 2019; Zhang et al., 2021b) but in much smaller\nlabel spaces. Recently, Simig et al. (2022) also used T5 (Raffel et al., 2020) for directly generating labels in end-to-end manners for XMC, but its performance was not convincing compared to classification-based models.\nPositive and unlabeled data. In practice, XMC is inherently with positive and unlabeled (PU) setting as the label space is extremely large and it is infeasible to manually review all the labels (Kim and Kim, 2020). Multi-label performances on PU tasks can be simulated by leaving only a few labels per train instance (e.g., leaving 8 out of 10 positive labels in one instance for label deficit rate 20%) positive (Hu et al., 2021). In this work, we show how XLGen works on such PU settings in \u00a74.3."
        },
        {
            "heading": "3 Extreme Multi-label Generation",
            "text": "(XLGen) with Cluster Guidance\nClassifying a document with multiple labels can be regarded as tagging a document with possible topical labels, which is basically decoding the freeformed text labels in an encoder-decoder setting. Moreover, if encoder and decoder are trained on large text corpora, labels are generated with an understanding of their lexical variations and semantic similarities. Our baseline framework fine-tunes a pre-trained Transformer using the input text as an encoder input and the label sequences as a decoder output. In addition, to more effectively address a huge number of labels in a long-tail distribution, we propose two different architectures, XLGenBCL (\u00a73.2) and XLGen-MCG (\u00a73.3), generating labels guided by pre-computed cluster information,\ninspired by class-based language models (Brown et al., 1992).\nPre-computed clustering. We compute label clusters using K-Means algorithm, as depicted in Figure 4. We first obtain label features using average embedding of positive documents in a train set, following Chang et al. (2020), and compute the label-cluster assignment matrix. Label clusters are assigned to each training document using this matrix and ground-truth labels, and used as a multicluster prediction layer for XLGen-BCL training or sequence of cluster IDs for XLGen-MCG training."
        },
        {
            "heading": "3.1 Baseline Fine-Tuning",
            "text": "Figure 3a shows our baseline XLGen which simply fine-tunes text-to-text Transformers, e.g., T5 (Raffel et al., 2020) or BART (Lewis et al., 2020), as our encoder-decoder framework on XMC dataset. \u2022 Input: task prefix: input text \u2022 Output: A sequence of label texts For encoding, we add a prefix token \u2018MultiLabel\u2019 to the input text to inform the task type. Then, the output labels are generated as a sequence of labels in decoding. The model is fine-tuned with cross-entropy loss (Lxent) given the sequence of label texts. In practice, the order of labels in decoding significantly influences the model performance. Following Yang et al. (2018), we sort the target labels in decreasing order of the frequencies. We also investigate various ordering effects and their impact on performance in \u00a75.1."
        },
        {
            "heading": "3.2 Fine-Tuning with Cluster Prediction",
            "text": "Figure 3b shows the fine-tuning of text-to-text with an additional multi-cluster prediction layer (XLGen-BCL). By doing so, we expect the model learns label similarities and hence biases itself to generate labels relevant to the given cluster. \u2022 Input: task prefix: input text \u2022 Multi-Cluster Layer: a vector of v1, ...vk where vi = 1 if ith cluster ci is a positive cluster; otherwise vi = 0 (1 1 0 ... 1...) \u2022 Output: A sequence of label texts The multi-cluster prediction layer is a vector of 0 or 1 that corresponds to the assigned clusters of instance, and is trained using the sequence of the last layer\u2019s hidden states of the encoder with a binary cross-entropy loss, Lbce. The final objective is as follows:\nLxmc\u2212bcl = Lxent + \u03bbLbce (1)\nwhere Lxent is a cross-entropy loss term for the original text-to-text framework and Lbce is a binary cross entropy loss term for the cluster layer. \u03bb is a weighting parameter for controlling Lbce, to be chosen by dev-set performance."
        },
        {
            "heading": "3.3 Fine-Tuning with Cluster Decoding",
            "text": "XLGen-BCL utilizes a cluster prediction only as an auxiliary task to improve representations for a label prediction, thus, predicted clusters are not used in inference. Figure 3c shows the third variant, XLGen with a multi-cluster generation (MCG),\nwhich leverages predicted clusters as additional input tokens so that the cluster information can be used in inference. \u2022 Input1: task prefix: input text ; a sequence of\npositive cluster IDs (c1 c2 c11..) \u2022 Output1: A sequence of label texts \u2022 Input2: cluster prediction prefix: input text \u2022 Output2: A sequence of positive cluster IDs (c1\nc2 c11..) We add a sequence of cluster IDs to the input text so that cluster information can be used while training (Input1-Output1). On the other hand, we have a new task with a clustering prefix \u2018MultiCluster\u2019 appended to the input text and predicts the sequence of labels (Input2-Output2). In training, these two tasks are trained simultaneously. Note that in inference, the predicted cluster IDs are appended to Input1 for the final label generation."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setups",
            "text": "Datasets. We use four widely used XMC benchmark datasets; three large-scale datasets with 4K\u223c30K labels (EURLEX-4K,AMZNCAT-13K, and WIKI10-31K) and one very-large-scale datasets with 500K labels (WIKI-500K). See Table 1 for the detailed data statistics.\nBaseline and XLGen Training. We compare XLGen with three state-of-the-art baselines in XMC tasks; AttentionXML (You et al., 2019), X-Transformer (Chang et al., 2020), and XRLinear (Yu et al., 2022). Note that all baseline models partition labels using hierarchical clustering. See A.1 for the detailed setups of baselines. Note it is common to upscale scores by ensemble learning for XMC baselines. However, for a fair comparison, we do not use any ensemble models for XLGen and baselines.\nWe train XLGen with T5 because BART (Lewis et al., 2020) performs worse for our task as shown\nin Table 13. By default, we sort labels in the decreasing frequency order to provide the training target sequence and infer the labels by beam search with size 5. For XLGen-BCL and XLGen-MCG, cluster sizes are optimized by dev set performance. We get the input text embedding by averaging the last hidden states from the pre-trained T5 encoder since T5 model does not have a CLS token. See A.2 to check more details."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "Following the prior work in XMC, we report F1 score (F@k) of top-k label probabilities as a supplementary metric in A.4. However, such ranking metrics are not applicable to label generation tasks since the generative model only output positive label texts sequentially and the order of generated labels does not align with the confidence of the label; in other words, the formerly generated labels do not need to be more confident than the latter ones. Thus, we use conventional multi-label classification metrics, like micro-averaged F1 score (Mic.) and macro-averaged F1 score (Mac.), as main evaluation metrics.\nIn principle, evaluating XMC task with the ranking format is not appropriate for most cases as it requires predicting the number of correct labels as well (Amigo and Delgado, 2022). We therefore select predicted labels only when the predicted score\nis greater than the threshold optimized from the validation set as in You et al. (2019)."
        },
        {
            "heading": "4.3 Results",
            "text": "We compare performances of XLGen and baselines in full labels (Table 2), tail labels (Table 3), and PU data setting (Table 4). For tail label and PU setting, we do not include AMAZNCAT-13K as it does not have zero-occurred labels. The best scores are bold and the second best scores are underlined. See A.4 for the full scores on tail labels and PU setting.\nFull label performance. In the evaluation with full benchmark sets, all the XLGen models show outperforming or competitive performance compared to the classification-based baselines. For macro F1 scores, XLGen models hugely outperform the baselines, which empirically represents that our approach is strong at predicting infrequent but correct labels. In other words, XLGen models are less biased to predicting frequent labels. Compared to XLGen-base, both XLGenBCL and XLGen-MCG generally show better performance, which demonstrates the effectiveness of the cluster prediction as an auxiliary loss.\nTail label performance. We measure macro F1 scores only for tail labels which never or one-time occur in the train set. We find every baseline extremely suffers from the tail labels, while XLGen shows significant improvements, demonstrating\nthe power of generative models for long-tail labels. Surprisingly, XLGen even predicts neverseen, zero-occurred labels, only inferred from the semantic meaning of the input text. Similarly to full label performance, XLGen-BCL and XLGenMCG perform better than XLGen-base, indicating that guidance of label cluster improves tail label performance as well.\nPU setting. In XMC, it is infeasible to annotate all relevant labels for an input text by checking every millions of labels. Therefore, many XMC datasets are indeed in PU setting. To evaluate the robustness against the positive and unlabeled properties, following Hu et al. (2021), we make PU data setting by randomly eliminating positive labels for each instance with 50% of deficit rate.\nAs XLGen is trained with fewer positive labels in PU settings, the generated output labels tend to be fewer as well, causing lower recall than expected. To increase the recall, we generate diverse label sequences using various sampling schemes in inference, which we call ensemble generation. We combine generated results from three decoding strategies; beam search with size 5, Top P + K sampling, and sampling with 0.8 temperature.\nWe find XLGen models outperform the baselines. Specifically, XLGen-MCG shows significantly strong scores, which indicates having predicted clusters as an additional input helps predict infrequent but correct labels. In Figure 5, we additionally visualize macro F1 scores of PU settings on WIKI10-31K with various deficit rates. Although XLGen-MCG drops with an increasing deficit rate, it still shows significant gaps with baseline scores."
        },
        {
            "heading": "4.4 Feasibility of in-context learning in XMC",
            "text": "In-context learning (Brown et al., 2020) shows a potential of generating unseen but positive labels as depicted in Figure 1, such as \u201cgeyser\u201d and \u201cphysical_reaction\u201d. In order to thoroughly validate the feasibility of in-context learning in XMC problems, we select 100 samples randomly from EURLEX4K and WIKI10-31K, and predict their labels using GPT-3 (Brown et al., 2020) in zero/one/fiveshot setups. We explore a few variations of prompts by tweaking label order or selecting few-shot examples differently, and report the best scores in Table 5. The performance of in-context learning significantly improves when we use more examples in the prompt, but they are far from the performance of XLGen. Moreover, the performance gap between GPT-3 and XLGen is much larger in EURLEX-4K where labels are formally annotated than in WIKI10-31K where labels are annotated by random users without a solid guideline. Unlike other multi-label classification tasks, XMC treats an extremely large number of labels, making it difficult to predict most unseen labels based on a few examples in in-context learning. See A.3 for the details of the experimental setup for in-context learning and the performance comparison among prompt variations."
        },
        {
            "heading": "5 Ablation Study",
            "text": "We explore various factors that impact the performance of XLGen on WIKI10-31K, such as label orders (\u00a75.1) and sampling strategies (\u00a75.2) In order to reduce training costs, we mainly train XLGen-base on the base size model with epoch 5 for ablation tests. We then investigate the model performance by clustering sizes and algorithms (\u00a75.3)."
        },
        {
            "heading": "5.1 Label Orders",
            "text": "Label orders in decoder are important as XLGen sequentially generates labels. We compare three different label orders; label frequencies from high to low (Frequency), inverse label frequencies from low to high (Inverse), and shuffling where labels are randomly ordered per training epoch. Inspired by Lee et al. (2019), we also consider ignoring label orders by resetting positional embeddings of each label as initial values in decoder 1 which we call label positional invariant setting (PosInv.).\nFigure 6a shows task performance across different label orders. We find trade-offs between macro and micro F1 scores by the label frequency order (Frequency and Inverse) because inversely frequent label orders make the model generate longtail labels earlier with certainty, thus, the scores of long-tail labels could improve. On the other hand, shuffling (Simig et al., 2022) crucially downgrades the performance since with randomly shuffled labels, XLGen tends to ignore co-occurrence patterns among labels in training time. Also, we conjecture that positional invariant setting does not work well as it tweaks the original positional embeddings of pre-trained T5 model."
        },
        {
            "heading": "5.2 Decoding Strategy",
            "text": "We now explore task performances with various sampling strategies in label generation. We compare greedy search, beam search, sampling with restrictions such as Top-K (Fan et al., 2018) and Top-P (Holtzman et al., 2020), and sharpening vocabulary distributions with a temperature parameter. In Figure 6b, we find that beam search with size 5\n1But we keep the position embeddings for token sequences in one label to learn token positions.\nachieves the best scores. Interestingly, most sampling methods heavily degrade performances since our label spaces are not entirely open-ended. We also explore ensemble methods to combine label outputs from different sampling strategies. Unlike the PU setting, however, they are not helpful in the full data setup since a sufficient number of labels are already generated by a single best generation strategy. Find the Appendix A.6 for details."
        },
        {
            "heading": "5.3 Cluster Strategy",
            "text": "We show the effect of clustering algorithms and their parameters. We train XLGen-MCG fine-tuning T5-base with epoch 5. We compare two clustering methods; K-means and Agglomerative clustering, and two text representations; TF-IDF and the recent T5 encoder. We find K-means and pre-trained T5 encoder shows the best performance over other combinations, as described in Appendix A.7.\nCluster size is another important factor for model performance. For example, a larger cluster size helps find label groups at a higher granularity, while it is much harder to be accurately predicted in inference time. Here, we choose cluster size to be a power of two on average (e.g., around 30 containing 1024 labels for WIKI10-31K on average). Figure 6c shows micro F1 scores of XLGen-MCG across cluster sizes in WIKI10-31K. Here we also report the upper bound of task performance (oracle) by using ground-truth cluster information. As we expect, clustering performance decreases as the cluster size increases since it is much harder to predict clusters in a larger cluster space. In terms of label prediction, we find that the model with smaller cluster sizes (e.g., \u2264 30) outperforms the larger ones, where the peak is around 30. Although\nAttentionXML XLGen-BCL # % # %\nExisting labels\nCorrect 674 39.2% 596 39.2% Wrong 776 45.1% 457 30.0%\nPU 270 15.7% 393 25.8%\na larger cluster size helps elaborately specify labels in the same category, lower cluster prediction performance harms label performances as well and leads to a bigger performance gap compared with oracle task scores."
        },
        {
            "heading": "6 Qualitative Analysis",
            "text": "Lastly, we evaluate the quality of generated labels via human evaluation (\u00a76.1) and visualization of the semantic relations among labels (\u00a76.2)."
        },
        {
            "heading": "6.1 Human Evaluation",
            "text": "The benchmark datasets have different label qualities. For example, the labels from EURLEX-4K, annotated by the Publication Office of EU, are refined and structured while Wiki datasets are collaboratively labeled by general users, so the quality of labels is relatively lower than the other benchmarks. Hence, we conduct human evaluations in both quantitative and qualitative ways to accurately measure the potential existence of PU labels and newly-generated labels. In particular, we randomly select 100 instances from the test set and extract incorrectly predicted labels and/or newly predicted labels by XLGen and baseline models. We then ask three human annotators to annotate and decide on possibly positive labels via majority voting.\nTable 6 shows human evaluation results on the annotated WIKI10-31K. Note that the number of predicted labels by XLGen-BCL is less than true labels because XLGen does not generate label with low confidence. In AttentionXML, on the other hand, we choose top-K labels as many as the number of true labels for each instance, so it has the same total labels as true labels. Compared to the best baseline, AttentionXML, XLGen-BCL could generate more PU labels and reduce the number of wrong labels. Also, our method generates\n75 (=45+30) newly generated labels out of 1,521 where 60% (=45/75) of them are correct, showing a relatively good generation quality of new labels. Of course, we can control our model to only count the candidate labels and not any of these new labels for more accurate predictions, as measured in Table 2.\nLastly, we provide annotation examples in Table 7. As we sort the label sequence by frequency in training for XLGen, frequently generated labels such as \u201cwikipedia\" or \u201cwiki\" are predicted first, followed by long-tail labels specified in the input text. For AttentionXML, on the other hand, top predicted labels seem more aligned with the input context, although frequently generated labels still come in front. Interestingly, new labels generated by XLGen come not only from the input context, but also previously generated labels. For instance, on the Wikipedia page of diet coke and mentos eruption, a new label \u201csoda\" is generated because input text contains \u201ccarbonated beverages\" which is synonym of \u201csoda\". On the Wiki page of Vimeo, on the other hand, after XLGen generates the PU label \u201csocialnetworking\", followed by its synonyms such as \u201csocial_network\" and \u201csocial_networking\".\n6.2 Label Semantics in XLGen\nTo better understand the semantics behind labels generated by XLGen, we visualize an annotated labels of three examples from Table 7 in Figure 7. We get label embeddings from the last hidden state of the fine-tuned XLGen-BCL decoder and project them into two-dimensional T-SNE (van der Maaten and Hinton, 2008). If a single label is split by multiple tokens, we average the last hidden layers of all tokens. We observe that frequently co-occurred labels (e.g., \u201cwiki\"-\u201cwikipedia\" or \u201cweird\"-\u201cfunny\") have similar label embeddings. Also, the newly generated labels become close to the co-occurred labels (e.g., \u201csoda\" - \u201cfunny\" or \u201ceruption\" in diet coke and mentos eruptions) via XLGen optimization."
        },
        {
            "heading": "7 Conclusion",
            "text": "We apply text-to-text Transformers to extreme multi-label classification, by tweaking the classification problem as generation of label texts. As we do not control the vocabulary space of generated labels, XLGen can create completely unseen but still relevant labels, inferred from the input context and semantic relationship from the previ-"
        },
        {
            "heading": "Input Document Models Labels",
            "text": "Emily Elizabeth Dickinson (December 10, 1830\u2013 May 15, 1886) was an American poet. Born in Amherst, Massachusetts to a successful family with strong community ties, she lived a mostly introverted and reclusive life. After she studied at the Amherst Academy for seven years in her youth, she spent a short time at ...\nTrue authors biography dickinson emily journal library literature openaccess people poem poet poetry reference research to-read wiki wikipedia writers AttentionXML wiki poet writers wikipedia literature authors books writing history poets writer people poetry biography inspiration american poems luule XLGen-BCL wikipedia wiki people art books literature english poetrywriters writer poet elizabeth dickinson emilydickinson\nScreenshot of vimeo.com home page Vimeo is a video-centric social network site (owned by IAC/InterActiveCorp) which launched in November 2004. The site supports embedding, sharing, video storage, and allows user-commenting on each video page...\nTrue articles computer reference socialnetworks technologytools video web2.0 wikipedia AttentionXML video web2.0 wikipedia wiki media youtube videosvideoblogging streaming XLGen-BCL wikipedia wiki reference technology web internet social video web2.0 no_tag socialnetworking socialsoftware phd social_networking social_network vimeo\nDiet Coke and Mentos Eruption is a reaction of Diet Coke and mint Mentos candies, a bottle of Diet Coke (other carbonated beverages may be used instead) and dropping some Mentos. This causes the Coke to foam at a rapid rate and spew into the air...\nTrue beverage candy chemistry coca-cola coke dietcoke drink eruption experiment experiments video explosion food fun funny interesting mint prank science AttentionXML wikipedia fun science diet wiki funny coke tv video healthinteresting humor food XLGen-BCL wikipedia wiki science interesting fun video funny foodhumor weird humour wtf #afterdarkclub soda eruption\nTable 7: Ground-truth and predicted labels from XLGen-BCL and AttentionXML on input documents in WIKI1031K. We ask human annotators to annotate labels to be correct (blue), wrong (strikethrough), and PU (red). For XLGen, we additionally mark potentially correct labels from the newly generated labels and their relevant contexts\nin input text with yellow box. (e.g., a possibly correct label soda is newly generated based on the fact that diet coke can be replaced with other carbonated beverage.)\nously generated labels. Our experiments show that XLGen outperforms the classification baselines in general, and significantly improves the long-tail performance and PU setting. Also, we observe utilizing label cluster information helps improve the performance in various settings. XLGen is expected to more benefit from pre-trained models as they become larger and powerful (Kaplan et al., 2020)."
        },
        {
            "heading": "Limitations and Future Directions",
            "text": "First, we conduct our main experiments and additional analyses on certain languages such as English that has tremendous text corpora. Extension to the low-resource languages might be challeng-\ning since this work requires text2text pre-trained models where those languages are applicable (e.g., multilingual T5 model), as well as the corresponding XMC datasets.\nAlso, compared to the efficient classification baselines, generative models are relatively expensive in terms of memory and time. For example, our experiment requires a lot of training resource as pre-trained models have >200 millions parameters to be tuned. Thus, we use three p3.16xlarge AWS instances with 8 Nvidia V100 GPUs for training. Using more efficient version of Transformers (Tay et al., 2022) or applying distributed training should be considered for a resource reduction.\nWhile in-context learning does not show compa-\nrable performance in XMC, we do observe that as the number of examples increases from zero to one to five, in-context learning can generate reasonable unseen but positive labels. It would be interesting to explore the potential of in-context learning in XMC with more advanced prompting and example sampling in the future.\nLastly, the XMC task has a risk of being biased or overfit to small training datasets (e.g., EURLEX4K and WIKI10-31K contain only about 15,000 training examples). As with other commonly used NLP benchmarks, there is a potential risk that our proposed method may not work properly in the new test/train sets, though we anticipate that such a risk will be quite small."
        },
        {
            "heading": "Ethics Statement",
            "text": "We use the four XMC benchmark datasets which are publicly available and widely used in research 2. The datasets with social tags (e.g., WIKI10-31K and WIKI-500K) may contain inappropriate vulgarisms if they are not filtered out from the original data processing."
        },
        {
            "heading": "A Appendix",
            "text": ""
        },
        {
            "heading": "A.1 Details on Baseline Models",
            "text": "AttentionXML (You et al., 2019) is a label treebased deep learning model. It uses a shallow and wide probabilistic label tree which allows to handle millions of labels and a multi-label attention mechanism by using raw text as input to capture the most relevant part of text to each label.\nX-Transformer (Chang et al., 2020) is the first scalable approach to apply deep transformer models in XMC task. In particular, it uses a pre-trained transformer encoder to assign labels to corresponding cluster. For each hierarchical cluster level, OVA classifiers are trained by only using sample instances under the same cluster, called teacher forcing negative (TFN) strategy. Unlike AttentionXML which only uses negative sampling, X-Transformer also uses the negative instances positively predicted by the classifier from the previous cluster level, called matcher-aware negatives (MAN). Recently, Zhang et al. (2021a) proposed XR-Transformer to speed up X-Transformer\u2019s training time in recursive manner. Thus, we use XR-Transformer instead of X-Transformer for the comparison.\nXR-Linear (Yu et al., 2022) has a very similar architecture with XR-Transformer, except that it only uses simple tf-idf text features instead of transformer encoder outputs. For OVA classification, linear matchers recursively solve XMC sub-problem for each hierarchical cluster level.\nIn order to fit score outputs into [0,1], we apply sigmoid post processor implemented by the authors for XR-Transformer and XR-Linear.\nA.2 Details on XLGen Training\nWe finetune the T5-large (EURLEX-4K, WIKI10-31K) or the T5-base (AMAZONCAT13K, WIKI-500K), with epoch 10 (EURLEX-4K, AMAZONCAT-13K) or epoch 5 (WIKI10-31K, WIKI-500K) based on the data and/or label size.\nWe set up input length as 500 for all benchmark datasets and use different output length based on the label lengths in train set; 90 for EURLEX-4K\nSample Label EURLEX-4K WIKI10-31K\nMic. Mac. Mic. Mac.\n0-shot\nRandom Random 5.3 3.8 7.1 3.8 Random Frequency 9.2 6.3 7.6 4.5 Most Label Random 6.0 4.3 7.2 4.4 Most Label Frequency 5.0 3.5 6.7 3.5\n1-shot\nRandom Random 14.8 10.7 13.6 11.8 Random Frequency 16.1 10.4 20.3 13.4 Most Label Random 17.2 14.7 17.9 16.8 Most Label Frequency 15.1 12.2 18.1 16.1\n5-shot\nRandom Random 15.7 10.4 17.9 14.2 Random Frequency 13.1 9.7 23.5 16.6 Most Label Random 11.0 9.6 19.8 18.0 Most Label Frequency 12.5 11.3 21.5 15.1\nTable 9: Micro-averaged and macro-averaged F1 scores in-context learning settings on 100 randomly selected samples. We test two label ordering strategies, random and decreasing label frequency (frequency), as well as two sampling strategies, random and selecting examples with the most labels (most label). The highest scores are bold.\nand 165 for other three benchmarks. We optimize XLGen using AdamW (Loshchilov and Hutter, 2019) with learning rate 2e-4.\nFor XLGen-BCL, we set up an initial weight value \u03bb as 1.0 and reduce it to 1k for every epoch number k.\nFor cluster-based XLGen architectures, we train k-means clustering and optimize the cluster size via cross-validation from the range of {10,20,30,...,100}. In Table 8, we report optimal cluster sizes for XLGen training.\nNote that each of T5 models have 220 million (T5-base) or 770 million (T5-large) parameters to be tuned. Also for training, we use a small batch size (1) since pre-trained T5 models are large to be fitted in a single GPU machine. Due to the model size, we use two GPU machines via model parallelism for T5-large and a single GPU machine for T5-base in training. Also, due to the training cost and time, we report the performance scores from the single running of training and inference. We basically modify the T5 code from huggingface library 3, and our code will be publicly available at https://github.com/alexa/ xlgen-eacl-2023.\nA.3 In-context learning in XMC\nFor in-context learning, we use OpenAI GPT-3 text-davinci-002 model with temperature 0.7 and max tokens 256. To find the optimal prompt, we use prompt variations with different label orders\n3https://huggingface.co/\nand few-shot example sampling strategy. Furthermore, we test two label ordering strategies, random and decreasing label frequency, as well as two sampling strategies, random and selecting examples with the most labels. See Figure 8 for GPT-3 prompt input and generated output. Table 9 shows the in-context learning performances across differ-\nent label ordering and sampling strategies. The best macro-averaged F1 scores for WIKI10-31K are achieved with label frequency ordering with random sampling; however, there is no consistently outperforming strategy for EURLEX-4K."
        },
        {
            "heading": "A.4 Additional Task Performances on Benchmark Datasets",
            "text": "For the full setup, we also report ranking based scores in Table 10. In general, for supplementary metrics (F@k) XLGen shows comparable results with baselines except F@1, and F@3 in EURLEX4K and WIKI10-31K. Note that for XLGen, we just treat the order of generated labels as a rank, which might not be correct since such generated labels should have a equal priority in theory. For this reason, XLGen has lower F@k scores with smaller k. However, such score gaps between baselines and XLGen decrease as k increases, like EURLEX-4K with XLGen-BCL, or even XLGen achieves higher performances in the larger benchmarks (e.g., F@5 and F@10 in AMZNCAT-13K and WIKI-500K). Also, full micro/macro F1 scores for tail labels and PU settings are in Table 11 and Table 12, respectively."
        },
        {
            "heading": "A.5 Additional Analyses on Base Model Comparison",
            "text": "For XLGen, we can use any pre-trained text-totext models. We compare task performance of two popular text-to-text models in Table 13; T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) by finetuning XLGen-base. In general T5 model outperforms BART, therefore, we use pre-trained T5 architectures for our main experiments."
        },
        {
            "heading": "A.6 Additional Analyses on Decoding Strategy",
            "text": "Followed by Figure 6b, Table 14 shows a task performance across various decoding strategies, including different beam size for beam search and a single sampling restriction.\nAdditionally, instead of choosing single generation strategy, we can even consider to integrate generation outputs from different generation strategies. For ensemble generations, we choose three single generation strategies; beam search with size 5, Top K + P sampling and sampling with temperature 0.8 to get diverse label sequences. We also consider\ntwo different types of joining method; inner join to union all labels and outer join to intersect labels from single generations.\nTable 15 shows task performance of ensemble generations. We find that outer joining ensemble generation could improve macro F1 scores as it includes more labels than single result. However, it simultaneously drops other micro F1 scores due to the high chance to contain wrongly predicted labels as well. On the other hand, inner joining ensemble generation in general harms the performance by restricting predicted labels occurring at any single generations, though this yields higher micro F1 scores than inner joining ensemble results."
        },
        {
            "heading": "A.7 Additional Analyses on Clustering and Representation",
            "text": "We compare two clustering algorithms; K-means and Agglomerative hierarchical clustering, and two text representations for the label features; TF-IDF and the last hidden states of T5 encoder in Table 16. We find that both algorithms show comparable per-\nformances. As computing cost is more expensive in Agglomerative hierarchical clustering, we mainly use K-means in our experiments. For text representation, the pre-trained T5 encoder achieves similar or slightly better performance to TF-IDF vectors. Pre-trained T5 encoder is more efficient in training as it has much lower size of dimensionality (e.g., 768 in t5-base) than tfidf (e.g., >100,000 for both EUR-LEX and WIKI10-31K). Thus, for all experiments with clustering method, we use K-means with pre-trained T5 encoder text representation."
        },
        {
            "heading": "A.8 Additional Examples of Human Annotation",
            "text": "In Table 17, We provide more annotation examples from WIKI10-31K, following the Table 7 to show how XLGen generates labels. In Figure 9, we also provide visualizations of generated labels\nby XLGen for examples in Table 17."
        }
    ],
    "title": "Cluster-Guided Label Generation in Extreme Multi-Label Classification",
    "year": 2023
}