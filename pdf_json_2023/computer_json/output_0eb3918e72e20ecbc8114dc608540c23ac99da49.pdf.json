{
    "abstractText": "Several assistive technology solutions, targeting the group of Blind and Visually Impaired (BVI), have been proposed in the literature utilizing multi-sensor data fusion techniques. Furthermore, several commercial systems are currently being used in real-life scenarios by BVI individuals. However, given the rate by which new publications are made, the available review studies become quickly outdated. Moreover, there is no comparative study regarding the multi-sensor data fusion techniques between those found in the research literature and those being used in the commercial applications that many BVI individuals trust to complete their everyday activities. The objective of this study is to classify the available multi-sensor data fusion solutions found in the research literature and the commercial applications, conduct a comparative study between the most popular commercial applications (Blindsquare, Lazarillo, Ariadne GPS, Nav by ViaOpta, Seeing Assistant Move) regarding the supported features as well as compare the two most popular ones (Blindsquare and Lazarillo) with the BlindRouteVision application, developed by the authors, from the standpoint of Usability and User Experience (UX) through field testing. The literature review of sensor-fusion solutions highlights the trends of utilizing computer vision and deep learning techniques, the comparison of the commercial applications reveals their features, strengths, and weaknesses while Usability and UX demonstrate that BVI individuals are willing to sacrifice a wealth of features for more reliable navigation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Paraskevi Theodorou"
        },
        {
            "affiliations": [],
            "name": "Apostolos Meliones"
        }
    ],
    "id": "SP:32ffbd196e7fc55807dd896e501aa8dc406184a4",
    "references": [
        {
            "authors": [
                "W. Wang",
                "Q. Chang",
                "Q. Li",
                "Z. Shi",
                "W. Chen"
            ],
            "title": "Indoor-Outdoor Detection Using a Smart Phone Sensor",
            "venue": "Sensors 2016,",
            "year": 2016
        },
        {
            "authors": [
                "X. Teng",
                "D. Guo",
                "Y. Guo",
                "X. Zhou",
                "Z. Ding",
                "Z. Liu"
            ],
            "title": "IONavi: An Indoor-Outdoor Navigation Service via Mobile Crowdsensing",
            "venue": "ACM Trans. Sen. Netw",
            "year": 2017
        },
        {
            "authors": [
                "H. Huang",
                "Q. Zeng",
                "R. Chen",
                "Q. Meng",
                "J. Wang",
                "S. Zeng"
            ],
            "title": "Seamless Navigation Methodology Optimized for Indoor/Outdoor Detection Based on WIFI",
            "venue": "In Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "S. Real",
                "A. Araujo"
            ],
            "title": "Navigation Systems for the Blind and Visually Impaired: Past Work, Challenges, and Open Problems",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "A. Esmaeili Kelishomi",
                "A.H.S. Garmabaki",
                "M. Bahaghighat",
                "J. Dong"
            ],
            "title": "Mobile User Indoor-Outdoor Detection Through Physical Daily Activities",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "N. Zhu",
                "M. Ortiz",
                "V. Renaudin"
            ],
            "title": "Seamless Indoor-Outdoor Infrastructure-Free Navigation for Pedestrians and Vehicles with GNSSAided Foot-Mounted IMU",
            "venue": "In Proceedings of the 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN), Pisa, Italy,",
            "year": 2019
        },
        {
            "authors": [
                "J. Yan",
                "A.A. Diakit\u00e9",
                "S. Zlatanova"
            ],
            "title": "A Generic Space Definition Framework to Support Seamless Indoor/Outdoor Navigation Systems",
            "venue": "Trans. GIS 2019,",
            "year": 2019
        },
        {
            "authors": [
                "S.A. Cheraghi",
                "A. Almadan",
                "V. Namboodiri"
            ],
            "title": "CityGuide: A Seamless Indoor-Outdoor Wayfinding System for People with Vision Impairments",
            "venue": "In Proceedings of the The 21st International ACM SIGACCESS Conference on Computers and Accessibility, Pittsburgh, PA, USA,",
            "year": 2019
        },
        {
            "authors": [
                "J. Xu",
                "F. Xue",
                "A. Chiaradia",
                "W. Lu",
                "J. Cao"
            ],
            "title": "Indoor-Outdoor Navigation without Beacons: Compensating Smartphone AR Positioning Errors with 3D Pedestrian Network",
            "venue": "In Construction Research Congress",
            "year": 2020
        },
        {
            "authors": [
                "C. Costa",
                "X. Ge",
                "E. McEllhenney",
                "E. Kebler",
                "P.K. Chrysanthis",
                "D. Zeinalipour-Yazti"
            ],
            "title": "CAPRIOv2.0: A Context-Aware Unified Indoor-Outdoor Path Recommendation System",
            "venue": "In Proceedings of the 2020 21st IEEE International Conference on Mobile Data Management (MDM), Versailles, France,",
            "year": 2020
        },
        {
            "authors": [
                "K. Lee",
                "D. Sato",
                "S. Asakawa",
                "H. Kacorri",
                "C. Asakawa"
            ],
            "title": "Pedestrian Detection with Wearable Cameras for the Blind: A Two-way Perspective. 2020",
            "venue": "In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI\u201920), Honolulu, HI, USA,",
            "year": 2020
        },
        {
            "authors": [
                "A. Shelton",
                "T. Ogunfunmi"
            ],
            "title": "Developing a Deep Learning-Enabled Guide for the Visually Impaired",
            "venue": "In Proceedings of the 2020 IEEE Global Humanitarian Technology Conference (GHTC), Seattle, WA, USA,",
            "year": 2020
        },
        {
            "authors": [
                "E.I. Al Khatib",
                "M.A.K. Jaradat",
                "M.F. Abdel-Hafez"
            ],
            "title": "Low-Cost Reduced Navigation System for Mobile Robot in Indoor/Outdoor Environments",
            "venue": "IEEE Access 2020,",
            "year": 2020
        },
        {
            "authors": [
                "B. Congram",
                "T.D. Barfoot"
            ],
            "title": "Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS",
            "venue": "In Proceedings of the 2021 18th Conference on Robots and Vision (CRV), Burnaby, BC, Canada,",
            "year": 2021
        },
        {
            "authors": [
                "P. Ren",
                "F. Elyasi",
                "R. Manduchi"
            ],
            "title": "Smartphone-Based Inertial Odometry for Blind Walkers",
            "venue": "Sensors 2021,",
            "year": 2021
        },
        {
            "authors": [
                "S.S. Senjam",
                "S. Manna",
                "C. Bascaran"
            ],
            "title": "Smartphones-Based Assistive Technology: Accessibility Features and Apps for People with Visual Impairment, and Its Usage, Challenges, and Usability Testing",
            "venue": "OPTO",
            "year": 2021
        },
        {
            "authors": [
                "S. Rajak",
                "A.K. Panja",
                "C. Chowdhury",
                "S. Neogy"
            ],
            "title": "A Ubiquitous Indoor\u2013Outdoor Detection and Localization Framework for Smartphone Users",
            "year": 2021
        },
        {
            "authors": [
                "U. Das",
                "V. Namboodiri",
                "H. He"
            ],
            "title": "PathLookup: A Deep Learning-Based Framework to Assist Visually Impaired in Outdoor Wayfinding",
            "venue": "In Proceedings of the 2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),",
            "year": 2021
        },
        {
            "authors": [
                "Y.B. Bai",
                "L. Holden",
                "A. Kealy",
                "S. Zaminpardaz",
                "S. Choy"
            ],
            "title": "A Hybrid Indoor/Outdoor Detection Approach for Smartphone-Based Seamless Positioning",
            "venue": "J. Navig",
            "year": 2022
        },
        {
            "authors": [
                "J. Schyga",
                "J. Hinckeldeyn",
                "J. Kreutzfeldt"
            ],
            "title": "Meaningful Test and Evaluation of Indoor Localization Systems in Semi-Controlled Environments",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "S. Chandna",
                "A. Singhal"
            ],
            "title": "Towards Outdoor Navigation System for Visually Impaired People Using YOLOv5",
            "venue": "In Proceedings of the 2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence), Noida, India,",
            "year": 2022
        },
        {
            "authors": [
                "A. Koutris",
                "T. Siozos",
                "Y. Kopsinis",
                "A. Pikrakis",
                "T. Merk",
                "M. Mahlig",
                "S. Papaharalabos",
                "P. Karlsson"
            ],
            "title": "Deep Learning-Based Indoor Localization Using Multi-View BLE Signal",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Q. Liu",
                "C. Gao",
                "R. Shang",
                "Z. Peng",
                "R. Zhang",
                "L. Gan"
            ],
            "title": "Environment Perception Based Seamless Indoor and Outdoor Positioning System of Smartphone",
            "venue": "IEEE Sens. J. 2022,",
            "year": 2023
        },
        {
            "authors": [
                "M. Mallik",
                "A.K. Panja",
                "C. Chowdhury"
            ],
            "title": "Paving the Way with Machine Learning for Seamless Indoor\u2013Outdoor Positioning: A Survey",
            "venue": "Inf. Fusion",
            "year": 2023
        },
        {
            "authors": [
                "P. Theodorou",
                "A. Meliones"
            ],
            "title": "Gaining Insight for the Design, Development, Deployment and Distribution of Assistive Navigation Systems for Blind and Visually Impaired People through a Detailed User Requirements Elicitation, Universal Access in the Information Society (UAIS)",
            "year": 2022
        },
        {
            "authors": [
                "K. Petersen",
                "R. Feldt",
                "S. Mujtaba",
                "M. Mattsson"
            ],
            "title": "Systematic mapping studies in software engineering. In Proceedings of the 12th international conference on Evaluation and Assessment in Software Engineering (EASE\u201908)",
            "venue": "Swindon, UK,",
            "year": 2008
        },
        {
            "authors": [
                "B. Kitchenham",
                "O.P. Brereton",
                "D. Budgen",
                "M. Turner",
                "J. Bailey",
                "S. Linkman"
            ],
            "title": "Systematic literature reviews in software engineering\u2013A systematic literature review",
            "venue": "In Information and Software Technology; Elsevier: Amsterdam, The Netherlands,",
            "year": 2009
        },
        {
            "authors": [
                "A. Meliones",
                "D. Sampson"
            ],
            "title": "Blind MuseumTourer: A System for Self-Guided Tours in Museums and Blind Indoor Navigation",
            "venue": "Technologies 2018,",
            "year": 2018
        },
        {
            "authors": [
                "P. Theodorou",
                "K. Tsiligkos",
                "A. Meliones",
                "C. Filios"
            ],
            "title": "An Extended Usability and UX Evaluation of a Mobile Application for the Navigation of Individuals with Blindness and Visual Impairments Outdoors\u2014An Evaluation Framework",
            "venue": "Based on Training. Sensors",
            "year": 2022
        },
        {
            "authors": [
                "P. Theodorou",
                "K. Tsiligkos",
                "A. Meliones",
                "A. Tsigris"
            ],
            "title": "An extended usability and UX evaluation of a mobile application for the navigation of individuals with blindness and visual impairments indoors: An evaluation approach combined with training sessions",
            "venue": "Br. J. Vis. Impair",
            "year": 2022
        },
        {
            "authors": [
                "M. Schrepp",
                "J. Thomaschewski"
            ],
            "title": "Design and Validation of a Framework for the Creation of User Experience Questionnaires",
            "venue": "Int. J. Interact. Multimed. Artif. Intell. 2019,",
            "year": 2019
        },
        {
            "authors": [
                "R. Nakashima",
                "A. Seki"
            ],
            "title": "Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Characteristics",
            "venue": "In Proceedings of the 2020 IEEE International Conference on Robotics and Automation (ICRA), Paris, France,",
            "year": 2020
        },
        {
            "authors": [
                "C.X. Lu",
                "M.R.U. Saputra",
                "P. Zhao",
                "Y. Almalioglu",
                "P.P.B. de Gusmao",
                "C. Chen",
                "K. Sun",
                "N. Trigoni",
                "A. Markham"
            ],
            "title": "MilliEgo: Single-Chip MmWave Radar Aided Egomotion Estimation via Deep Sensor Fusion",
            "venue": "In Proceedings of the 18th Conference on Embedded Networked Sensor Systems, Virtual,",
            "year": 2020
        },
        {
            "authors": [
                "C. Chen",
                "S. Rosa",
                "Y. Miao",
                "C.X. Lu",
                "W. Wu",
                "A. Markham",
                "N. Trigoni"
            ],
            "title": "Selective Sensor Fusion for Neural Visual-Inertial Odometry",
            "venue": "In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2019
        },
        {
            "authors": [
                "L. Deng",
                "X. Li",
                "Y. Zhang"
            ],
            "title": "Research on Visual and Inertia Fusion Odometry Based on PROSAC Mismatched Culling Algorithm",
            "venue": "In Proceedings of the 2019 International Conference on Robotics Systems and Vehicle Technology, Wuhan, China,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "X. Li",
                "X. Zhang",
                "Y. Bai",
                "C. Zheng"
            ],
            "title": "An Attitude Estimation Method Based on Monocular Vision and Inertial Sensor Fusion for Indoor Navigation",
            "venue": "IEEE Sens. J. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Q. Zeng",
                "J. Wang",
                "Q. Meng",
                "X. Zhang",
                "S. Zeng"
            ],
            "title": "Seamless Pedestrian Navigation Methodology Optimized for Indoor/Outdoor Detection",
            "venue": "IEEE Sens. J. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "G. Galioto",
                "I. Tinnirello",
                "D. Croce",
                "F. Inderst",
                "F. Pascucci",
                "L. Giarr\u00e9"
            ],
            "title": "Sensor Fusion Localization and Navigation for Visually Impaired People",
            "venue": "In Proceedings of the 2018 European Control Conference (ECC), Snowbird, UT, USA,",
            "year": 2017
        },
        {
            "authors": [
                "D. Croce",
                "L. Giarr\u00e9",
                "F. Pascucci",
                "I. Tinnirello",
                "G.E. Galioto",
                "D. Garlisi",
                "A. Lo Valvo"
            ],
            "title": "An Indoor and Outdoor Navigation System for Visually Impaired People",
            "venue": "IEEE Access 2019,",
            "year": 2019
        },
        {
            "authors": [
                "N.A. Khan",
                "R. Ansari"
            ],
            "title": "Real-Time Traffic Light Detection from Videos with Inertial Sensor Fusion",
            "venue": "In Proceedings of the 1st ACM SIGSPATIAL Workshop on Advances on Resilient and Intelligent Cities, Seattle, WA, USA,",
            "year": 2018
        },
        {
            "authors": [
                "G. Chaudhari",
                "A. Deshpande"
            ],
            "title": "Robotic Assistant for Visually Impaired Using Sensor Fusion",
            "year": 2017
        },
        {
            "authors": [
                "V. Bharati"
            ],
            "title": "LiDAR + Camera Sensor Data Fusion on Mobiles with AI-Based Virtual Sensors to Provide Situational Awareness for the Visually Impaired",
            "venue": "In Proceedings of the 2021 IEEE Sensors Applications Symposium (SAS), Sundsvall, Sweden,",
            "year": 2021
        },
        {
            "authors": [
                "W.M. Elmannai",
                "K.M. Elleithy"
            ],
            "title": "A Highly Accurate and Reliable Data Fusion Framework for Guiding the Visually Impaired",
            "venue": "IEEE Access 2018,",
            "year": 2018
        },
        {
            "authors": [
                "H. Chen",
                "K. Wang",
                "K. Yang"
            ],
            "title": "Improving RealSense by Fusing Color Stereo Vision and Infrared Stereo Vision for the Visually Impaired",
            "venue": "In Proceedings of the 1st International Conference on Information Science and Systems, Jeju, Republic of Korea,",
            "year": 2018
        },
        {
            "authors": [
                "H. Hakim",
                "A. Fadhil"
            ],
            "title": "Navigation System for Visually Impaired People Based on RGB-D Camera and Ultrasonic Sensor",
            "venue": "In Proceedings of the International Conference on Information and Communication Technology, Baghdad, Iraq,",
            "year": 2019
        },
        {
            "authors": [
                "N. Long",
                "K. Wang",
                "R. Cheng",
                "K. Yang",
                "J. Bai"
            ],
            "title": "Fusion of Millimeter Wave Radar and RGB-Depth Sensors for Assisted Navigation of the Visually Impaired",
            "venue": "In Proceedings of the Millimetre Wave and Terahertz Sensors and Technology XI, Berlin, Germany,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhao",
                "R. Huang",
                "B. Hu"
            ],
            "title": "A Multi-Sensor Fusion System for Improving Indoor Mobility of the Visually Impaired",
            "venue": "In Proceedings of the 2019 Chinese Automation Congress (CAC), Hangzhou, China,",
            "year": 2019
        },
        {
            "authors": [
                "J. Chen",
                "A. Ruci",
                "E. Sturdivant",
                "Z. Zhu"
            ],
            "title": "ARMSAINTS: An AR-Based Real-Time Mobile System for Assistive Indoor Navigation with Target Segmentation",
            "venue": "In Proceedings of the 2022 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO),",
            "year": 2022
        },
        {
            "authors": [
                "V.V. Baskar",
                "I. Ghosh",
                "S. Karthikeyan",
                "R.J. Hemalatha",
                "T.R. Thamizhvani"
            ],
            "title": "An Indoor Obstacle Detector to Assist the Visually Impaired Person on Real-Time with a Navigator",
            "venue": "In Proceedings of the 2021 International Conference on Computational Performance Evaluation (ComPE), Shillong, India,",
            "year": 2021
        },
        {
            "authors": [
                "C.S. Silva",
                "P. Wimalaratne"
            ],
            "title": "Sensor Fusion for Visually Impaired Navigation in Constrained Spaces",
            "venue": "In Proceedings of the 2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS),",
            "year": 2016
        },
        {
            "authors": [
                "Y. Bouteraa"
            ],
            "title": "Design and Development of a Wearable Assistive Device Integrating a Fuzzy Decision Support System for Blind and Visually Impaired People",
            "venue": "Micromachines",
            "year": 2021
        },
        {
            "authors": [
                "P.T. Mahida",
                "S. Shahrestani",
                "H. Cheung"
            ],
            "title": "Indoor Positioning Framework for Visually Impaired People Using Internet of Things",
            "venue": "In Proceedings of the 2019 13th International Conference on Sensing Technology (ICST), Sydney, NSW, Australia,",
            "year": 2019
        },
        {
            "authors": [
                "A. El-Naggar",
                "A. Wassal",
                "K. Sharaf"
            ],
            "title": "Indoor Positioning Using WiFi RSSI Trilateration and INS Sensor Fusion System Simulation",
            "venue": "In Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing, Prague, Czech Republic,",
            "year": 2019
        },
        {
            "authors": [
                "M. Li",
                "J. Ammanabrolu"
            ],
            "title": "Indoor Way-Finding Method Using IMU and Magnetic Tensor Sensor Measurements for Visually Impaired Users",
            "venue": "Int. J. Intell. Robot Appl. 2021,",
            "year": 2021
        },
        {
            "authors": [
                "P. Marzec",
                "A. Kos"
            ],
            "title": "Low Energy Precise Navigation System for the Blind with Infrared Sensors",
            "venue": "In Proceedings of the 2019 MIXDES-26th International Conference \u201cMixed Design of Integrated Circuits and Systems\u201d, Rzeszow,",
            "year": 2019
        },
        {
            "authors": [
                "J. Gong",
                "X. Zhang",
                "Y. Huang",
                "J. Ren",
                "Y. Zhang"
            ],
            "title": "Robust Inertial Motion Tracking through Deep Sensor Fusion across Smart Earbuds and Smartphone",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2021
        },
        {
            "authors": [
                "M. Murata",
                "D. Ahmetovic",
                "D. Sato",
                "H. Takagi",
                "K.M. Kitani",
                "C. Asakawa"
            ],
            "title": "Smartphone-Based Localization for Blind Navigation in Building-Scale Indoor Environments",
            "venue": "Pervasive Mob. Comput",
            "year": 2019
        },
        {
            "authors": [
                "S. Chung",
                "J. Lim",
                "K.J. Noh",
                "G. Kim",
                "H. Jeong"
            ],
            "title": "Sensor Data Acquisition and Multimodal Sensor Fusion for Human Activity Recognition Using Deep Learning",
            "venue": "Sensors",
            "year": 2019
        },
        {
            "authors": [
                "S. Gill",
                "D.T.V. Pawluk"
            ],
            "title": "Design of a \u201cCobot Tactile Display\u201d for Accessing Virtual Diagrams by Blind and Visually Impaired Users",
            "venue": "Sensors 2022,",
            "year": 2022
        },
        {
            "authors": [
                "H. Xue",
                "W. Jiang",
                "C. Miao",
                "Y. Yuan",
                "F. Ma",
                "X. Ma",
                "Y. Wang",
                "S. Yao",
                "W. Xu",
                "A Zhang"
            ],
            "title": "DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data",
            "venue": "In Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing, Catania, Italy,",
            "year": 2019
        },
        {
            "authors": [
                "M.R.U. Saputra",
                "P.P.B. de Gusmao",
                "C.X. Lu",
                "Y. Almalioglu",
                "S. Rosa",
                "C. Chen",
                "J. Wahlstr\u00f6m",
                "W. Wang",
                "A. Markham",
                "N. Trigoni"
            ],
            "title": "DeepTIO: A Deep Thermal-Inertial Odometry With Visual Hallucination",
            "venue": "IEEE Robot. Autom. Lett. 2020,",
            "year": 2020
        },
        {
            "authors": [
                "S.K. Gharghan",
                "R.D. Al-Kafaji",
                "S.Q. Mahdi",
                "S.L. Zubaidi",
                "H.M. Ridha"
            ],
            "title": "Indoor Localization for the Blind Based on the Fusion of a Metaheuristic Algorithm with a Neural Network Using Energy-Efficient WSN. Arab",
            "venue": "J. Sci. Eng. 2022,",
            "year": 2023
        },
        {
            "authors": [
                "K. Patil",
                "Q. Jawadwala",
                "F.C. Shu"
            ],
            "title": "Design and Construction of Electronic Aid for Visually Impaired People",
            "venue": "IEEE Trans. Hum.-Mach. Syst",
            "year": 2018
        },
        {
            "authors": [
                "R. Elbakly",
                "M. Elhamshary",
                "M. Youssef"
            ],
            "title": "HyRise: A Robust and Ubiquitous Multi-Sensor Fusion-Based Floor Localization System",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "G. Kov\u00e1cs",
                "S. Nagy"
            ],
            "title": "Ultrasonic Sensor Fusion Inverse Algorithm for Visually Impaired",
            "venue": "Aiding Applications. Sensors",
            "year": 2020
        },
        {
            "authors": [
                "S. Yao",
                "Y. Zhao",
                "H. Shao",
                "D. Liu",
                "S. Liu",
                "Y. Hao",
                "A. Piao",
                "S. Hu",
                "S. Lu",
                "T.F. Abdelzaher"
            ],
            "title": "SADeepSense: Self-Attention Deep Learning Framework for Heterogeneous On-Device Sensors in Internet of Things Applications",
            "venue": "In Proceedings of the IEEE INFOCOM 2019-IEEE Conference on Computer Communications, Paris, France,",
            "year": 2019
        },
        {
            "authors": [
                "S.A. Junoh",
                "S. Subedi",
                "J.-Y. Pyun"
            ],
            "title": "Smartphone-Based Indoor Navigation System Using Particle Filter and Map-Constraints",
            "venue": "In Proceedings of the The 9th International Conference on Smart Media and Applications, Jeju, Republic of Korea,",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhou",
                "S. Chen",
                "J. Chen"
            ],
            "title": "Research on Indoor Positioning of Multi-Source Information Fusion Based on Improved Particle Filter",
            "venue": "In Proceedings of the 6th International Conference on Computer Science and Application Engineering, Virtual,",
            "year": 2022
        },
        {
            "authors": [
                "H.-Y. Huang",
                "C.-Y. Hsieh",
                "K.-C. Liu",
                "H.-C. Cheng",
                "S.J. Hsu",
                "C.-T. Chan"
            ],
            "title": "Multi-Sensor Fusion Approach for Improving Map-Based Indoor Pedestrian Localization",
            "venue": "Sensors 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Q. Niu",
                "N. Liu",
                "J. Huang",
                "Y. Luo",
                "S. He",
                "T. He",
                "S.-H.G. Chan",
                "X. Luo"
            ],
            "title": "DeepNavi: A Deep Signal-Fusion Framework for Accurate and Applicable Indoor Navigation",
            "venue": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
            "year": 2019
        },
        {
            "authors": [
                "W.C.S.S. Simoes",
                "L.M. da Silva",
                "V.J. da Silva",
                "V.F. de Lucena"
            ],
            "title": "A Guidance System for Blind and Visually Impaired People via Hybrid Data Fusion",
            "venue": "In Proceedings of the 2018 IEEE Symposium on Computers and Communications (ISCC), Natal,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Citation: Theodorou, P.; Tsiligkos, K.;\nMeliones, A. Multi-Sensor Data\nFusion Solutions for Blind and\nVisually Impaired: Research and\nCommercial Navigation Applications\nfor Indoor and Outdoor Spaces.\nSensors 2023, 23, 5411. https://\ndoi.org/10.3390/s23125411\nAcademic Editor: Natividad Duro\nCarralero\nReceived: 12 May 2023\nRevised: 2 June 2023\nAccepted: 5 June 2023\nPublished: 7 June 2023\nCopyright: \u00a9 2023 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: sensor fusion techniques; deep learning; computer vision; assistive technologies; comparison analysis; remote sensing; usability and user experience\n1. Introduction\nBlind and Visually Impaired (BVI) individuals face a tremendous number of challenges even for simple daily routine tasks. One of those tasks is safe autonomous navigation in outdoor and indoor spaces. Enabling the BVI individuals to independently move around has a tremendous effect on several aspects of their mental and emotional wellbeing as it fosters a higher sense of empowerment and overall satisfaction by reducing feelings of dependency, isolation, and frustration that may result from consistently relying on others for mobility. Besides the above, autonomous navigation is critical to activities such as social participation, access to education and employment leading to the personal growth and development of the BVI individual. The scientific community has proposed and demonstrated numerous solutions that span several different techniques and approaches to address these issues. However, few of them can be used immediately by the BVI individuals to improve their life as they are largely exploratory and most commonly have little to no consideration for the utility aspect of day-to-day application [1\u201324]. Furthermore, contrary to outdoor space navigation, which leverages the GPS infrastructure, indoor space navigation has no adequate solution. This was highlighted from discussions with both Orientation and Mobility specialists as\nSensors 2023, 23, 5411. https://doi.org/10.3390/s23125411 https://www.mdpi.com/journal/sensors\nSensors 2023, 23, 5411 2 of 29\nwell as BVI individuals on various occasions [25,26]. Simultaneously, there exist several commercial applications that are being used among the BVI communities. These include AriadneGPS [27], Lazarillo [28], Nav by ViaOpta [29], Seeing Assistant Move [30] and Blindsquare [31] among others with the latter being the most popular in the communities of the BVI. Between these two there is a gap. On one hand, as can be seen from the literature, multimodal solutions can provide features that can improve the quality of life of BVI individuals. The abundance of sensors with varying capabilities and costs can be exploited with the help of sensor-fusion techniques with smartphone devices playing the role of the central point of integration. On the other hand, commercial applications already provide features to improve the experience of BVI individuals via the use of smartphones, but they could significantly increase their quality of service with the adoption of mature features found in the literature. This paper, in an effort to close the gap, tries to provide a comprehensive review of the landscape of the available academic and commercial solutions. It performs a comparative analysis between these two to highlight the type of features that are part of commercial applications, the ones that are still part of the academic realm and the ones being the low-hanging fruits that can be immediately adopted from commercial applications. This effort strives to contribute to a dramatical user experience improvement. For the case of conducting the sensor fusion solutions review the selected papers consisted mainly of peer-reviewed journals and conferences from the last five years to present the most recent advances. The scope of the search involved papers demonstrating sensor fusion solutions with an emphasis on applications for BVI individuals. The collected papers were evaluated from the standpoint of the solutions\u2019 novelty, whether the employed procedure was adequately described as well as whether the proposed solution was thoroughly evaluated. For each paper, summaries were compiled and classified according to the chronological period, the sensor fusion techniques, the number and type of employed sensors as well as the practicality of the proposed solutions. For the case of conducting the commercial comparative analysis, the selected applications were chosen based on the following criteria. The actual usage and popularity, as indicated by search engines, scores and critics as found in the smartphone stores (Play and Apple Store) and, finally, from interviews and questionnaires with BVI user communities. The evaluation of the commercial applications consisted of two parts. The first part concerned the comparison of the applications feature-wise while the second, their comparison from a Usability and User Experience (UX). For the latter part, the BVI individuals were requested to complete a UX questionnaire to quantitively assess their experience. At the same time the research team was monitoring the efforts of the participants in order to measure Effectiveness and Efficiency, the remaining two aspects of the Usability evaluation. The paper\u2019s content is organized as follows. The Materials and Method section presents to the reader the selection criteria of both the papers constituting part of the literature review and of the available commercial solutions in more detail. Furthermore, it describes the application comparison methodology. Followingly, the section Results demonstrates the findings of both literature review and comparative analysis. Finally, the conclusion-discussion section summarizes the findings and presents opportunities for improvements from the literature that can be immediately adopted by the existing or new commercial applications resulting in a significant improvement in user experience. The study aims to link the state-of-the-art research on sensor fusion with the practitioners and developers of applications targeting BVI individuals. To the best of our knowledge, there is no other similar effort in the field. It does so by bringing together the most recent research literature with an exposition of their strong and weak points along with the most widely used international applications where a comparative analysis makes clear the number and type of features supported. Furthermore, there is no prior work presenting either a comprehensive list of features of the commercial applications or a Usability and UX study performed on commercial applications. Moreover, among the\nSensors 2023, 23, 5411 3 of 29\nkey takeaways of this study is a set of minimum features that outdoor and indoor blind navigation applications need to support along with improvements commercial applications could make by adopting features found in the research literature. The emphasis of the research study was on indoor space navigation as both O&M specialists and the individuals themselves admit both inadequate solutions and fragmentation of the space leading to limited awareness of the existing solutions.\n2. Materials and Methods\nThis section presents the methodology for conducting both a literature review on papers employing sensor fusion techniques in applications targeting the BVI individuals and a comparative evaluation of the commercial applications consisting of two parts. The first part concerns the comparison of the applications feature-wise while the second concerns the comparison from the standpoint of usability of two of the most popular commercial applications, namely Blindsquare and Lazarillo, with BlindRouteVision, our outdoor navigation application.\n2.1. Process of Conducting a Systematic Literature Review\nCritical to the conduct of any survey is the selection of a process to guarantee the validity, correctness, and effectiveness of the survey results. In particular, the adopted methodology (Figure 1) for conducting the systematic literature review [32,33] includes the following:\n\u2022 Definition of the research questions \u2022 Definition of the search process for relevant papers \u2022 Definition of the inclusion and exclusion criteria \u2022 Quality assessment to further refine the selection process \u2022 Acquisition of the papers \u2022 Data analysis\nSensors 2023, 23, x FOR PEER REVIEW 3 of 29\nUsability and UX study performed on commercial applications. Moreover, among the key\ntakeaways of this study is a set of minimum features that outdoor and indoor blind navi-\ngation applications need to support along with improvements commercial applications\ncould make by adopting features found i the research literature. The emphasis of the\nresearch study was on indoor space navigation as both O&M specialists and t e individ-\nuals themselves dmit both inadequ te solutions and fragmentation of the space leading\no li ited awareness of the existing s l tions.\n2. Materials and Methods\nThis section presents the methodology for conducting both a literature review on pa-\npers employing sensor fusion techniques in applications targeting the BVI individuals and\na comparative evaluation of the commercial applications consisting of two parts. The first\npart concerns the comparison of the applications feature-wise while the second concerns\nthe comparison from the standpoint of usability of two of the most popular commercial\napplications, namely Blindsquare and Lazarillo, with BlindRouteVision, our outdoor nav-\nigation application.\n2.1. Process of Conducting a Systematic Literature Review\nCritical to the conduct of any survey is the selection of a process to guarantee the\nvalidity, correctness, and effectiveness of the survey results. In particular, the adopted\nmethodology (Figure 1) for conducting the systematic literature review [32,33] includes\nthe following:\n\u2022 Definition of the research questions\n\u2022 Definition of the search process for relevant papers\n\u2022 Definition of the inclusion and exclusion criteria\n\u2022 Quality assessment to further refine the selection process\n\u2022 Acquisition of the papers\n\u2022 Data analysis\nSeveral research questions were defined to guide the review of sensor fusion solu-\ntions used for BVI applications and the commercial applications comparison.\nCentral to our approach was understanding the techniques employed in sensor fu-\nsion solutions for the designated application domain as well as uncovering the area\u2019s\ntrends, the frequency and the number of papers published. Details such as the type, char-\nacteristics and range of sensors used in these assistive technology solutions were also in-\nvestigated. Finally, special attention was also given to the assessment from a practical\nstandpoint, the cost and wearability efficiency to determine the degree to which they can\nbe easily adopted and used long-term by the BVI individuals.\nSeveral research questions were defined to guide the review of sensor fusion solutions used for BVI applications and the commercial applications comparison. Central to our approach was understanding the techniques employed in sensor fusion solutions for the designated application domain as well as uncovering the area\u2019s trends, the frequency and the number of papers published. Details such as the type, characteristics and range of sensors used in these assistive technology solutions were also investigated. Finally, special attention was also given to the assessment from a practical standpoint, the cost and wearability efficiency to determine the degree to which they can be easily adopted and used long-term by the BVI individuals.\n2.1.2. Search Process, Inclusion, and Exclusion Criteria\nAfter forming the research questions, the search process started. For the first task, various search engines and known repositories of scientific papers were selected as sources. The chosen papers consisted mainly of peer-reviewed journals and conferences where the\nSensors 2023, 23, 5411 4 of 29\nones from major publishers were prioritized, including IEEE Xplore, ACM Digital Library and MDPI among others. The scope of the search involved papers demonstrating sensor fusion solutions with an emphasis on applications for BVI individuals. This was applied by searching for a direct mention of the \u201csensor-fusion\u201d and/or \u201cnavigation\u201d term either in the abstract or inside the main text. Furthermore, at a later stage and as the previous search revealed a few results, we extended the search to cover implementations that are not part of assistive technology solutions for the BVI individuals but, nonetheless, their proposed solution could be transferred to our target domain. On the other hand, we excluded papers based on the following criteria:\n\u2022 No mention of sensor fusion whatsoever \u2022 Solutions that used similar techniques but with no application to our domain. \u2022 Papers which mention sensor fusion as related work or use the phrase \u201csensor fu-\nsion\u201d but do not elaborate on their sensor fusion procedure or they misleadingly use the term.\n\u2022 Finally, papers that use sensor fusion, but their contribution is not very novel.\n2.1.3. Quality Assessment, Acquisition and Data Analysis\nAfter gathering all the relevant papers, a second stage of filtering was applied so as to limit even further the number of papers and improve the quality of the study. In this stage, we considered further criteria including the solutions\u2019 novelty, whether the employed procedure was adequately described as well as whether the proposed solution was thoroughly evaluated. After the quality assessment stage, the pool of papers was finalized, and the data analysis commenced. For each paper, summaries were compiled and classified according to the chronological period, the sensor fusion techniques, the number and type of employed sensors as well as the practicality of the proposed solutions.\n2.2. Process of Conducting the Commercial Applications Review\nOne of the main goals of the commercial applications comparison was to gain an understanding of the employed techniques, the trends as well as the popularity of commercial applications among the BVI communities. Part of this effort was to also understand the advantages and disadvantages of those solutions\u2019 features, weight and cost-wise. A crucial issue was to compare the effectiveness and efficiency of these solutions as well as their perceived Usability and UX with the help of BVI individuals. We, also, included BlindRouteVision, an application developed by our research team in the context of the MANTO project [34], to determine the effectiveness and efficiency of our approach as well. An emphasis was also given to uncovering the factors that make a successful commercial application for the BVI in which training is a crucial parameter and conventionally requires access to sites including special navigation utilities for the blind [35]. Finally, we conclude the presentation by highlighting the features found in the state-of-the-art literature that are easily transferred to the existing solutions. Figure 2 presents graphically the methodology employed to collect the commercial applications and evaluate them. Sensors 2023, 23, x FOR PEER REVIEW 5 of 29\ni . i l li ti t l .\nSensors 2023, 23, 5411 5 of 29\n2.2.1. Description of the Search Process\nLikewise to the case of the literature review, the search of the commercial applications followed a process which was more simplified and less restrictive given the substantially smaller amount of available pool. In particular, the criteria that shaped the results were based on results from search engines, the popularity of the applications as indicated by the search engines, scores and critics of these applications as found in the smartphone stores (Play and Apple Store) and, finally, from interviews and questionnaires with BVI individuals concerning their experience with these applications as well as whether they use any of the available commercial applications as their main daily driver.\n2.2.2. Overview of the Comparative Evaluation of the Commercial Applications\nThe commercial applications\u2019 comparison process was the following. Firstly, we downloaded them on compatible smartphone devices. Subsequently, the research team performed two tasks. The first one involved the applications assessment feature-wise which led to the compilation of a comparative table while the second involved the participation of the BVI individuals. The latter required them to navigate a set of preselected routes, one known and one unknown to them, to evaluate from a Usability standpoint the provided features of the two most popular commercial applications (Blindsquare and Lazarillo) and our application (BlindRouteVision) under a given set of criteria as defined by our research team. Then the BVI individuals were requested to complete a UX questionnaire to quantitively assess their experience. At the same time, the research team was monitoring the efforts of the participants in order to measure Effectiveness and Efficiency, the remaining two aspects of the Usability evaluation. The test subjects consisted of 13 BVI individuals both experts and non-experts with assistive technologies and with or without high digital sophistication. Both the Lighthouse for the Blind of Greece and the Tactual Museum of Athens helped to reach the participants.\n2.2.3. Usability and UX Evaluation\nUsability and UX are two key terms in evaluating assistive technology applications for the BVI which are commonly overlooked and, thus, underexplored. These two have the potential to cast some light on the factors that lead to the low adoption rates of the BVI communities [36]. We have seen in the context of designing, implementing and evaluating our assistive technology applications that their exploration can lead to uncovering the system\u2019s failing aspects from the perspective of the BVI individuals [37,38]. In order to assess the degree to which users can use them correctly to achieve their purpose while being satisfied at the same time, we employed three measures, namely effectiveness, efficiency and satisfaction. According to the ISO/IEC 25010 2011 [39], they are all components of usability. The latter is defined as \u201cthe degree to which a product or system can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use\u201d. In particular, the 3 main components are defined as follows:\n\u2022 Effectiveness\u2014a measure of the degree to which users can complete a task. \u2022 Efficiency\u2014a measure of the time required by a user to complete a task. \u2022 Satisfaction\u2014a measure of the subjective quality of interaction with the application\nwhich is equivalent to UX [37].\n2.2.4. Effectiveness and Efficiency\nThe assessment methodology employed the metrics of effectiveness and efficiency and is similar to the one described in [37,38]. The former employs the metrics of completion and error rate and efficiency is measured as the ratio of successfully completed tasks per unit of time. These metrics are among the most used as they are simple and easy to understand, thus, making them popular in many studies. In particular, the Completion rate measures the number or percentage of tasks completed successfully, while the Error Rate indicates the number of errors per user. Common cases of errors include undesired outcomes arising\nSensors 2023, 23, 5411 6 of 29\nas a result of poor interaction with the system interface or insufficient information, as well as mental errors where users are unable to comprehend the system options. This was measured by the research team as the users were executing the tasks given to them. For the calculation of effectiveness and efficiency, the following formulas were used:\nE f f ectiveness = total # o f tasks success f ully completed\ntotal # o f tasks undertaken =\n\u2211Ul=1 \u2211 M i=1 taskli\nU \u2217 M (1)\nwhere U = # of participants, M = # of tasks per participant and taskli= i-th task of the l-th user. Furthermore, taskli takes the value 1 if the task is successfully completed and 0 otherwise.\nE f f iciency = \u2211Uj=1 \u2211 M i=1 tasksijtij\n\u2211Uj=1 \u2211 M i=1 tij\n\u00d7 100% (2)\nwhere tij = EndTimeij \u2212 StartTimeij, for which, in turn, EndTimeij is defined as the time required for the i-th task of the j-th user to be completed successfully or the time until the user quits.\n2.2.5. User Experience\u2014UX\nUser experience is usually measured with the deployment of questionnaires. To the best of our knowledge, there are no known questionnaires available that assess the features of assistive technology solutions targeting navigation for the BVI. The framework adopted for the purpose of this study is a modified version of the standardized UEQ+ framework [40]. The latter is a configurable, although limited, set of scales with the intent to quantitatively measure the opinion of a user on a Likert scale of 1 to 7 where the value 1 corresponds to \u201cnot at all satisfied\u201d and the value 7 corresponds to \u201cextremely satisfied.\u201d\nFor our case, the set of included scales includes the following:\n\u2022 Efficiency\u2014assesses the users\u2019 perception of the effort required to achieve the desired goal as well as how quickly the application reacts to their actions. \u2022 Dependability\u2014assesses the users\u2019 perception of the system\u2019s response predictability and consistency and the degree to which everything is under control. \u2022 Personalization\u2014assesses the users\u2019 perception of the system\u2019s configuration and adjustment to personal preferences. \u2022 Usefulness\u2014assesses the users\u2019 perception of the gains as a result of using the system. \u2022 Trustworthiness of content\u2014assesses the users\u2019 perception regarding the quality and reliability of the system\u2019s instructions. \u2022 Response behaviour\u2014assesses the users\u2019 perception of the utilized voice.\nFinally, before handing out the questionnaire to the participants, they were requested to use the following evaluation criteria to form the basis of their qualitative assessment while navigating the suggested routes:\n\u2022 Evaluation of the real-time tracking and navigation instructions \u2022 Accuracy and density of reported position \u2022 Combination with public means of transportation \u2022 Traffic light crossings \u2022 User Placement to Points of Interest (POIs) \u2022 Obstacle detection and recognition\n3. Results\nThis section presents the results of both the literature review on sensor fusion solutions for the BVI as well as the comparison of the commercial applications feature and usabilitywise following the methodology described in Section 2.\nSensors 2023, 23, 5411 7 of 29\n3.1. Literature Review\nThe review consisted of 40 papers from various scientific and engineering societies as well as well-established publishers. The full list of the paper sources includes IEEE Xplore, ACM Digital Library and MDPI. The target application domain consisted of applications providing the tools for blind indoor and outdoor navigation with sensor fusion as the enabling technology. The selected papers ranged between 2018 and 2023 (the last five years). The overall architecture of the proposed solutions is presented in Figure 3. Several sensors either homogeneous or heterogenous in nature feed the proposed sensor fusion systems. The majority of these sensor fusion systems run either on single devices, including smartphones and computer systems or in collaboration with the Cloud and/or other resources found in the near vicinity. Their goal is to make either a decision or to give an accurate estimate on various aspects associated with outdoor and indoor navigation.\nSensors 2023, 23, x FOR PEER REVIEW 7 of 29 Finally, before handing out the questionnaire to the participants, they were requested to use the following evaluation criteria to form the basis of their qualitative assessment while navigating the suggested routes: \u2022 Evaluation of the real-time tracking and navigation instructions \u2022 Accuracy and density of reported position \u2022 Combination with public means of transportation \u2022 Traffic light crossings \u2022 User Placement to Points of Interest (POIs) \u2022 Obstacle detection and recognition\n3. Results\nThis section presents the results of both the literature review on sensor fusion solu-\ntions for the BVI as well as the comparison of the commercial applications feature and\nusability-wise following the methodology described in Section 2.\n3.1. Literature Review\nThe review consisted of 40 papers from various scientific and engineering societies\nas well as well-established publishers. The full list of the paper sources includes IEEE\nXplore, ACM Digital Library and MDPI. The target application domain consisted of ap-\nplications providing the tools for blind indoor and outdoor navigation with sensor fusion\nas the enabling technology. The selected papers ranged between 2018 and 2023 (the last\nfive years).\nThe overall architecture of the proposed solutions is presented in Figure 3. Several\nsensors either homogeneous or heterogenous in nature feed the proposed sensor fusion\nsystems. The maj rity of the e sensor fusi n systems run either n single device , i clud-\ning smartphones and computer systems or in collaboration with the Cloud and/or other\nresources found in the near vicinity. Their goal is to make eit er a decision r to give an\naccurate estimate on various aspects associated with outdoor and indoor navigation.\nFigure 3. Commercial Applications Methodology.\nTables 1 and 2 describe the sensor fusion techniques respectively and the type of em-\nployed sensors.\n2D Object Detection Semantic segmentation 2D Object Tracking\n3D Point Cloud Depth Maps\nFigure 3. Commercial Applications Methodology.\nTables 1 and 2 describe the sensor fusion techniques respectively and the type of employed se sors.\nTable 1. The techniques utilized in the research papers.\nFrom the review, the following trends emerged. Most of the proposed solutions address aspects of indoor navigation with a percentage of 89% as opposed to 11% of the outdoor space solutions. Obstacle detection, a very important feature for BVI applications to possess, constitute 35% of the total while roughly half of them (46%) implement obstacle avoidance as well. The difference between these two is subtle but important. An obstacle avoidance solution, besides incorporating obstacle detection as a component, has the added feature of adjusting route navigation so as to provide the correct instructions to the users to avoid the obstacle. On the contrary, obstacle detection solutions go as far as detecting the obstacle while leaving the steps to bypass the detected obstacle to the user. Table 3 contains all the solutions of the literature review reporting, when available, whether they support obstacle detection, either static or dynamic or both, the supported range and accuracy.\nSensors 2023, 23, 5411 8 of 29\nTable 2. Type of sensors utilized in the research papers.\nSensors Definition\nCamera Devices detecting and conveying information as an image Inertial Measurement Unit\n(IMU) A device consisting of gyroscopes, accelerometers and magnetometers measuring an object\u2019s gravity and angular rate\nRGB-D Depth sensing devices that work with an RGB (Red, Green, Blue) sensor camera augmentingconventional images with depth information Millimeter Wave A special class of radar technology using short wavelength electromagnetic waves for object detection\nLidar Laser-based devices that determine the ranges of obstacles in 3D space by measuring the time requiredfor the reflected light to return to a receiver Ultrasonic Devices measuring object distance using ultrasonic sound waves\nGPS Satellite-based navigation system receivers that provide position, velocity and timing information Gyroscope Devices measuring angular velocity\nAccelerometer Devices that measure acceleration forces acting on an object to determine its position and movement. Magnetometer Devices measuring the magnetic field or the magnetic dipole moment\nBeacon Periodical transmission of information Compass Devices detecting and responding to the presence of a magnetic field\nMicrophone Devices capturing sound waves and converting them into an electrical signal Passive Infrared Resistor\n(PIR) Devices measuring infrared (IR) light radiating from objects to detect motion\nVibration Devices measuring vibration Buzzer An audio signalling device Barometer Devices measuring atmospheric pressure Wi-Fi Devices using Wi-Fi signals to transmit information. Sonar Devices measuring object distance using sonic sound waves\nThermal Camera Devices creating images from infrared radiation instead of visible light\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy\n[41\u201347]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some de ree of succes , 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with some degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will present the solutions found in the literature organized around the s nsors emplo ed. S ecifically, the prese tation is organized around the criteria of being ca era or non-camera assisted as different s ts of techniques are required for those two cases. For ea h of these t o categories, we continue by grouping the provided s lutions based on the type of sensor with the m st frequent group be ng presented first. For both case , the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single camera or a configuration of multiple cam ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, magnet meter), the mo t popular sensor among the elected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadva tages is written. On th other hand, Section 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can be u ed in r al-l fe c narios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are limited pract ca ity for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will pres nt the solutions found in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of being ca era or non-camera assisted as different s ts of techniques are required for those two cases. For a h of th se t categories, we continue by grouping the provided s luti ns based on the type of sensor with the m s frequent group be ng presented first. For both case , the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single camera or a configuration of multiple cam ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, magnet meter), the mo t popular sen or among th elected papers, are presented. For each individual paper, a summary incorporating th solution with the advantages and disadva t ges is written. On th other hand, Sect on 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in al-l fe cenarios with som degree of success, 29% of them are practical ut have a co bination f ei r high cost or requi e from the user to carry many s nsors, 24% are l mited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will pres t the solutions found in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of bei g ca era or on-camera ass sted as different s ts of techniques are required f those two cases. For a h of th se t categories, we continue by grouping the provided s luti ns bas d on the type of s nsor with the m s frequent group be ng presented first. For both case , the sensor fu on techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single cam ra or a configuration of multiple cam ras, heads t devices as well as 3D cam ra sen or . Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, ma net meter), the mo t popular sen or among th elected papers, are presented. For each individ al paper, a summ ry incorporating th solution with the advantages and disadva t ges is written. On th ot er hand, Sect on 3.1.2 concerns s lutions with no camera sensors. M st f them i corporat an IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r l-l fe cenarios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are l mited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r uppo t St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT follo ng s ctions will pres t the solutions found in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of bei g ca era or on-camera ass sted as different s ts of techniques are required f those two cases. Fo a h of th se t categories, we continue by grouping the provided s luti ns bas d on the type of s nsor with the m s frequent group be ng presented first. For both case , the se sor fu on techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single am ra or a configuration of multiple cam ras, heads t devices as well as 3D cam ra s n or . Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, ma net meter), the mo t p pular sen or among th elected papers, are presented. For each individ al paper, a summ ry incorporating th solution with the advantages and disadva t ges i ritten. On th ot er hand, Sect on 3.1.2 concerns s lutions with no camra s nsors. M st f them i corporat an IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odome-\ny (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e es matio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\n[48]\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand cam ra sensors is exp ct d given the prolife ati of martpho e d vices as he prefer ed platform o deve oping solutions for the BVI dividuals. Finally, ly 14% of these sol tions can be used in e l-life sc n rios with ome degre of uc s , 29% them are practi al but have combinat on of either high cos or req ire from the user to ar y many sensors, 24% re limited practica ty for spe ific scenarios whi e 32% are pure y xperimental.\nTable 3. Solutions and bstacle detection.\nPa er Sup ort Static Dynami Range Ac uracy [41\u201347] \u2716 \u2716\n[48] N/A N/A [49] \u2716 \u2716 [50] N/A N/A [51] N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A [5 ] N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llowing sections will prese t the so utions f und in the literature o ga iz d ar und the sensors mployed. Specifically, the pr sent tion is organized around the cr - teria of bei g camera or non-camera as isted diff re t s ts of techniques re equired for those two cases. For ach of th s two categori , w con u by groupin provided solutions based on the type of e s r with the mo t frequent group be ng presented f rst. For both cases, the sensor fusion t ch ique are de crib d in co preh nsible man er. In more detail, Section 3.1.1 covers th papers util zing a form of the cam ra sensor, be it a single cam ra or a configuration of multiple cameras, he dset devices as well as 3D camera sensors. Subs qu tly, p pe s utilizi g the IMU sensor (ac lerom ter, gyroscop , magnetometer), the most popular sensor among th sel cted papers, ar resent d. For each individ al p per, a summ ry corpor ti g the sol tion wit the a va tages and disadvantages is writt n. O the o her hand, Se tio 3.1 2 c c rns solutions wi h o c mera sensors. Most of them incorporate an I U sensor while o h options include ul rasonic and Lidar se sors as well as Blueto th Low Energy (BLE) beacon and Wi-Fi ac es points.\n3.1.1. Camera-As isted Solution An uncertainty-based adapt ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is proposed in [41] for estimating relative motion. It minim zes degr dat on fr m inac urate state estimation by d ter ining the states hat should b i clude in h estimation proces . The e degr ding stat s can ar se und r motio character stics that nullify\n[49]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of s artphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-lif s narios with some deg e of succes , 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] N/A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single ca era or a configuration of multiple ca eras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti ns An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expect d given the proliferation of smartphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with some degree of uccess, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy 1\u201347] [48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95 [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] N/A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will present the solutions found in the literature organized around the s nsors emplo ed. S ecifically, the prese tation is organized around the criteria of being ca era or non-ca era assisted as different s ts of techniques are required for those two cases. For ea h of these t o categories, we continue by grouping the provided s lutions based on the type of sensor with the m st frequent group be ng presented first. For both case , the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single ca era or a configuration of multiple ca ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, magnet meter), the mo t popular sensor among the elected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadva tages is written. On th other hand, Section 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Soluti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected giv n th proliferation of smartphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of these solutio s ca be used in r al-l fe cenarios ith som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are limited pract ca ity for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] N/A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will pres nt the solutions found in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of being camera or non-ca era assisted as different s ts of techniques are required for those two cases. For a h of th se t categories, we continue by grouping the provided s luti ns based on the type of sensor with the s frequent group be ng presented first. For both case , the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single ca era or a configuration of multiple cam ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, magnet meter), the mo t popular sen or among th elected papers, are presented. For each individual paper, a summary incorporating th solution with the advantages and disadva t ges is written. On th other hand, Sect on 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-As isted Soluti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sens rs is expected given the proliferation of smartphon devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used i r al-l fe cenarios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are l mited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] / N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will pres t the solutions found in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of bei g ca era or on-ca era ass sted as different s ts of techniques are required f those two cases. For a h of th se t categories, we continue by grouping the provided s luti ns bas d on the type of s nsor with the s frequent group be ng presented first. For both case , the sensor fu on techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single ca ra or a configuration of multiple cam ras, heads t devices as well as 3D cam ra sen or . Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, ma net meter), the mo t popular sen or among th elected papers, are presented. For each individ al paper, a summ ry incorporating th solution with the advantages and disadva t ges is ritten. On th ot er hand, Sect on 3.1.2 concerns s lutions with no camera sensors. M st f them i corporat an IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me a-As sted S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is expected given the proliferation of smartphon devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r l-l fe cenarios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are l mited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r uppo t St tic Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] N/A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT follo ng s ctions will pres t the solutions found in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of bei g camera or on-ca era ass sted as different s ts of techniques are required f those two cases. Fo a h of th se t categories, we continue by grouping the provided s luti ns bas d on the type of s nsor with the s frequent group be ng presented first. For both case , the se sor fu on techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single am ra or a configuration of multiple cam ras, heads t devices as well as 3D cam ra s n or . Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, ma net meter), the mo t p pular sen or among th elected papers, are presented. For each individ al paper, a summ ry incorporating th solution with the advantages and disadva t ges i ritten. On th ot er hand, Sect on 3.1.2 concerns s lutions with no camra s nsors. M st f them i corporat an IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-As sted Soluti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odome-\ny (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e es matio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\n[50]\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand cam ra sensors is exp ct d given the prolife ati of martpho e d vices as he prefer ed platform o deve oping solutions for the BVI dividuals. Finally, ly 14% of these sol tions can be used in e l-life sc narios with some degre f uc s , 29% them are practi al but have combinat on of either high cos or req ire from the user to ar y many sensors, 24% re limited practica ty for spe ific scenarios whi e 32% are pure y xperimental.\nTable 3. Solutions and bstacle detection.\nPa er Sup ort Static Dynami Range Ac uracy [41\u201347]\n[48] / / [49] \u2716 \u2716 [ 0] N/A N/A [ 1] N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] N/A / [ ] N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llowing sections will prese t the so utions f und in the literature o ga iz d ar und the sensors mployed. Specifically, the pr sent tion is organized around the cr - teria of bei g camera or non-ca era as isted diff re t s ts of techniques re equired for those two cases. For ach of th s two categori , w con u by groupin provided solutions based on the type of e s r with the mo t frequent group be ng presented f rst. For both cases, the sensor fusion t ch ique are de crib d in co preh nsible man er. In more detail, Section 3.1.1 covers th papers util zing a form of the cam ra sensor, be it a single cam ra or a configuration of multiple cameras, he dset devices as well as 3D camera sensors. Subs qu tly, p pe s utilizi g the IMU sensor (ac lerom ter, gyroscop , magnetometer), the most popular sensor among th sel cted papers, ar resent d. For each individ al p per, a summ ry corpor ti g the sol tion wit the a va tages and disadvantages is writt n. O the o her hand, Se tio 3.1 2 c c rns solutions wi h o c mera sensors. Most of them incorporate an I U sensor while o h options include ul rasonic and Lidar se sors as well as Blueto th Low Energy (BLE) beacon and Wi-Fi ac es points.\n3.1.1. Camera-As i ed Soluti n An uncertainty-based adapt ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is proposed in [41] for estimating relative motion. It minim zes degr dat on fr m inac urate state estimation by d ter ining the states hat should b i clude in h estimation proces . The e degr ding stat s can ar se und r motio character stics that nullify\n[51]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is xpect d given the prolif ration of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination f either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experiment l.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A /A N/A N/A [51] N/A N/A 5 m 95 [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A /A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m /A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solution An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nN/A N/A 5 m 95%\n[52]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the prolif ration of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions c n be use in real-lif sc arios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% ar limited practic lity for specific scenarios while 32% ar purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followi g ct on will pr sent the s luti s fo d in t e lit rature organiz d around the s nsors empl y d. S cifically, the e n a ion is g nized around th criter a of being r n -camer assisted as diff rent et of chniqu are required for thos two ca es. Fo each f thes two categories, we co t ue by groupi g the p ovided solutions bas d on th typ of se sor with the mos frequ nt grou being fi st. For both c ses, the sensor fusi n echnique ar descr b d in comprehensible manner. In more detail, Section 3.1.1 ov s the pap s utilizi g a f m f the camera ensor, b it a single camera or a co figuration f mu tiple c meras, headset device as w ll as 3D camer sensors. Subsequently, papers utilizing the IMU sensor (acc l meter, gyros p , mag etometer), the most popular sensor among he selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and di advantages s written. On he other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these soluti c n b u e in real-l fe c arios with some degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% ar limited pract c lity for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT e follo g t on will pr sent the solutio s fo d in t e lit rature organized around the s nsors empl d. S cifically, the r a i n is o g nized around th criteria of being r non-camer assisted as iff rent s t of chniques are required for thos two ca es. Fo ea h f these t o categories, w c t ue b grou i g the provided s lutions based on th typ of se sor with the m s frequ nt grou be ng presen ed first. For both c e , the sensor fusi n echnique are descr b d i compr hensible manner. In more etail, Sectio 3.1.1 v s the p p utiliz g f m f the camera ensor, b it a single camera or a configuration f mu tiple c m ras, headset device as w ll as 3D camera ens rs. Subseq en ly, papers utilizing the IMU senso (acc l meter, gyroscop , mag et meter), the mo t popular sensor among the elected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and di adva tages s written. On h other hand, Section 3.1.2 concerns s lutions with no camera sensors. M st f the incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of these soluti s c n be use in r al-l fe ce arios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% ar limited pract c ity for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT e follo g t on will pr s nt the solutio s fou d in the lit rature organized around the s sors mp d. S cifically, the r e ta i n is org nized around th criter a of being e r non-camera assi ted as diff rent s t of techniques are required for os two ca es. Fo a h f th se t atego ies, we cont ue by groupi g the provided s uti ns bas d th typ of se or with the m s frequ nt grou be ng presented first. For both c e , the sensor fusi n echnique re descr b d i comprehensible manner. In more etail, Section 3.1.1 ov s the p p s utilizing fo m of the camera ensor, b it a single camera o a configuration of mu tiple c m ras, headset device as w ll as 3D camera s ns rs. Subseq e tly, papers utilizi g the IMU s nso (acc l meter, gyroscop , mag et meter), the mo t popular sen or among th elected papers, are presented. For each individual paper, a summary incorporating th solution with the advantages and di adva t ges s written. On h other hand, Sect on 3.1.2 concerns s lutions with no camera sensors. M st f the incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\n0 m < R < 9 m 98%\n[53]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors i expected given the proliferation of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicali y f specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n48 N/A N/A 49 \u2716 \u2716 \u2716 0 N/A N/A N/A N/A 1 N 5 m 95% 2 0 9 m 98% 3 0.1 m < R < 3.5 m 90\u201395% 4 N/A N/A 5 N/A N/A 0.2 < R < 10 6 R > 2 m N/A\n57 \u2716 N/A [58] 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n64 R = 20 cm N/A [65\u201371]\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mer -Assisted S lutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nca era s nsors is expected given the proliferation of smartphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with some degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are lim ted pract cality f specific s enarios while 32% a e purely experime tal.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ctions will present the solutions found in the literature organized around the s nsors emplo ed. S ecifically, the prese tation is organized around the criteria of being ca era or non-camera assisted as different s ts of techniques are required for those two cases. For ea h of these t o categories, we continue by grouping the provided s lutions based on the type of sensor with the m st frequent group be ng presented first. For both case , the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single ca era or a configuration of multiple cam ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler eter, gyroscope, magnet eter), the mo t popular sensor among the elected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadva tages is written. On th other hand, Section 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate an IMU sens r while other options include ultrasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is ex ected give the proliferation of smartphone d vices as the pr - ferre platform for evel ping solutions for the BVI in ividuals. Finally, only 14% of these solutions can be us d in real-life cenarios with some deg e f succes , 29% of them are practical but have a combinatio of ither high ost or equire fro the user to carry many s sors, 24% ar limited practi ality for sp cific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 98 [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [ 4] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following se ti ns will present the olutions found i the liter ture organized around the sensors e pl yed. Specifically, the pr se tation is o ganiz around the crit ria of being camera or non-cam ra assisted as diff rent sets of techniques are equired for t ose two cases. For each of these t o catego ies, w continue by grouping the provided solutions ba ed on the type f s nsor with the ost fr quent group being prese ted first. For both cases, the sensor fusion techniques are described in c mprehensible m nner. In ore detail, Secti 3.1.1 covers the pap rs utilizing form of th c mera sensor, be it ingle camera or a configuration of ultiple camera , he dset devices as well as 3D c mera ens rs. Subsequently, papers utilizing the IMU s nsor (accelerometer, gyroscope, m gneto eter), the most popular sens r amo t elected p p rs, are presented. For each individual pap r, summary incorporati g the solution with the advantages and disadvantages is w itt n. On the ther hand, Section 3.1.2 concerns solutions with no camera e sors. Most of them inc rporate an IMU se sor while other ptio s include ultraso ic and Li r sensors as well as Bluetooth Low Energy (BLE) beac ns and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions A uncertainty-based adaptive se sor fusion framewo k for Visual-Inertial Odometry (VIO) i proposed in [41] for esti ati relative motion. It mi imiz degradation from inaccurate state e timatio by determini g the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\n0.1 m < R < 3.5 m 90\u2013 5%\n[54]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferre platform for evel ping soluti n for th BVI individuals. Fi ally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n8 N/A N/A 49 \u2716 \u2716 \u2716 \u2716 0 N/A N/A N/A N/A 1 5 m 95% 2 0 9 m 98% 3 0.1 m < R < 3.5 m 90\u201395% 4 N/A N/A 5 N/A N/A 0.2 < R < 10 6 R > 2 m N/A\n57 \u2716 N/A [58] 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n64 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh f llowing sections will pres nt solutions ou d in th l erature ganized round the se sors e pl y d. Specific lly, the tation is orga iz d aro nd the c - teria of bei g ca era or non-ca era a sist d as diff r t sets of techn qu are eq ire for t ose two cases. For each of thes w ca g ries, w c ntin e by grouping the provided solutio s ba ed on th type of sensor w th the mos fr qu n group being presented fi st. For both c ses, sensor fusion tech iques ar described i a compr ensible manner. I m re detail, Sectio 3.1.1 c vers the papers utiliz ng a form f the amera nso , be it a si gle c m ra a c nfiguratio of multiple c meras, he d t devices as ll s 3D camera sensors. Subsequently, apers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most pop lar s sor among the l cted pape , re presente . For each in iv d al pap r, a mmary incorpor ting the sol tio w th the advan ges an dis dvan ages is written. O the other ha d, S ction 3.1.2 onc rns solutions wi h no ca er se sors. Most of th m inco por e an IMU n or while t r op io s clude rasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mer -Assisted S lutio s An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nN/A N/A N/A\n[5 ]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sens rs is expec ed giv n the prolif rat on of sma tph n evic s s t e pref rred latform for develo ing sol ns for the BVI dividuals. Fi ally, only 14% of th s solu ions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32 are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 49 \u2716 \u2716 \u2716 50 N/A N/A 1 / / 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A / / 55 N/A N/A 0.2 < R < 10\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following sections will present th solutions foun in the literatur organ z around the ns rs employ . Specifically, the presentatio is rganized around th criteria f being camera or non-camera assisted as differ nt sets of techni es are required for those two cases. F r each of se tw categories, we continue by groupi g th provided solutions based on the typ of s nsor with the most frequent group b ing pr ented first. For both ca es, th sensor fusion t chniques are described i a comprehensible mann . In more det il, Section 3.1.1 cov r the a er utilizing a form of th camera sens r, be it a single c mera or a configuratio f multiple cameras, headset devices as ell as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, m gnetometer), he m st popular sensor among t select d papers, are prese t d. For each individual p pe , summary inc rporating th s lution with the advantages and disadvantag s is writte . On the other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sensor . Most of them incorporate an IMU s nsor while other options include ultrasoni and Lidar ensors as well as Bluet oth Low Energy (BLE) beaco s and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An un rtai y-bas d aptive sensor fusion framewo k for Visual-Inertial Odometry (VIO) is proposed in [41] for estimatin relative motion. It minimiz s degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\n/ / 0.2 m < R <10 m /\n[56]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expec ed giv n the proliferation of sma tphon devices s the pref rred platform for develo ing solu ions for the BVI i dividuals. Finally, only 14% of th se solu ions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limit d practicali y for specific scenarios while 32% are purely xperimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A / [51] / / 5 m 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A / / [55] N/A N/A 0.2 < R < 10 / [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following sec ions will p ese t th s lution foun i t li tur organ z round the s rs empl yed. S ecifi lly, th ese tati s rga iz d r u e criteri f bein camera o n n-ca e a ssist d as diff r set of tech iques are r quire f hose two cases. For ach of the tw categ ie , w continue by gro pi g th p ovided\nlutio s based on the type of e or with e most frequent group b i g pre ented fir . F r both ca es, the sensor fusion techniques are described i a comprehensible manner. In more detail, S ctio 3.1.1 cove the p ers ut lizing m of th c mera , be it a si gl c mera or configura o f mul ipl cameras, he s t devic s well as 3D camera sen ors. Subseque tl , pap rs u ilizing the IMU sensor (accel r me r, gy o cope,\ngnetometer), most popular sen or amo t l c d pap s, a e pre en d. For each individual pape , summary inc rporating th solution with the advantages and disadvantag s is written. On he other hand, Sec ion 3.1.2 c ncer s ol tio s w th no c m era sensor . Most of them incorporate an IMU nsor while other ptions include ultrasoni and Lidar ensors as well s Bluetooth Low Energy (BLE) beaco s and Wi-Fi acces points.\n3.1.1. Camera-Assisted Solutions An un rtainty-bas d aptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is expected given the proli eration of smartphon devices as the pref r ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with some degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are lim ted pract cali y for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A / [51] N/A N/A 5 m 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A / [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT foll ng s c i s will p es t th s lutio fou d i t lit r ture organized round he s rs empl ed. S ecifically, he ese ati s orga ized r u d e crieria of bein cam ra o n n-cam a ssis d as diff re s t of techniques are required fo hose two cases. For of the t categ ies, w continue by gro pi g the p ovided s luti n based on the type of e or with e m st frequ t group be g presented fir t. For oth case , the s n or fus on echniques are des ribed in a comprehensible manner. In more etail, S cti 3.1.1 c ve s the p ers ut lizing f m of th camera sensor, be it a si gl ca e a or c figura on of mul ipl cam ras, he s t devic s s well as 3D camera se sors. Subs q e tly, pap rs u lizing the IMU senso (accel r me er, gyro cope, ma net eter), th most popular sensor am th l cted pape s, are pre ented. For each individual paper, a summary incorporating the solution with th advantages and d sadva tag s is written. On h ther hand, Section 3.1.2 c ncer s s l tio s w th no c m era sensor . M st f them incorporate an IMU nsor while other pti ns include ultrasoni and Lidar s nsors as well a Blu oo h w En rgy (BLE) beaco and Wi-Fi access point .\n3.1.1. Camera-As isted Solutions An uncert inty-ba ed adaptive sensor fusion fram work for Vis al-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is ex ected giv the proliferation of smartphone d vices as the pr - ferre platform for evel ping solutions for the BVI in ividuals. Finally, only 14% of these solutions can be us d in real-life cenarios with some deg e f succes , 29% of them are practical but have a c mbinatio of ither high ost or equire from the user to carry many s sors, 24% are limited practicali y for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A N/A [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 98 [53] 0.1 m < R < 3.5 m 90\u201395% [ 4] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll wi g s ct ns will pr sent the olu ion fo nd i he l t r ture or anize arou d t e ens rs mploy d. Specifically, pr sentatio is o ganiz arou d the cri\nria of bei g amera no -cam assisted as diff e t ts of t chn qu s ar eq ired f r t e wo cases. F e ch of these t cat go ie , w ntinue y groupi g the provided solutions ba ed n he type f s sor with the ost f qu nt group being prese ted first. For both cas s, the sen or fusi ec q s a desc ib i c mpr h nsible m nner. In m re det il, Se t n 3.1. cov r the p pers u lizing for of the c m ra sensor, b i ingle cam ra or configur ion f multip c era , he s t ev ces a w ll as 3D c m ra e s r . Sub que tly, papers utilizing t IMU s ns r (accel ro eter, gyros ope, m gnetomet r), the ost popular sens r amo t electe p p rs, are presented. For eac i ividu l pap r, sum ary incorp ati g the solutio with th advantages and di a vantages is w itt n. On the ther ha d, S ction 3.1.2 concerns solutions with o ca - era e s r . Most of th m inc rpor te an IMU se or whil other ptio s includ ultraso ic and Li r sensors as well as Blu tooth Low Energy (BLE) beac ns and Wi-Fi acce s points.\n3.1.1. Camera-Assisted Solutions A uncertainty-based adaptive se sor fusion framewo k for Visual-Inertial Odometry (VIO) i proposed in [41] for esti ati relative motion. It mi imiz degradation from inaccurate state e timatio by determini g the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nR > 2 m\n[57]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferred latform for ev loping solu ons for the BVI individuals. F nally, onl 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 N/A 49 0 1 5 m 95 2 0 < R < 9 m 98% 3 0.1 3.5 90\u201395% 4 /A /A N/A 5 N/A N/A 0.2 m < R < 10 m\n56 R > 2 m 57 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 60 2 m < R < 12 m\n[61\u201363] 6 R = 20 cm\n6 1 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solution foun in the lite atur organized around the sensors employed. Specifi ally, the presentation is organiz d around the criteri of being camera or no -ca era assisted as different sets of techniques are required for those two cases. For ach of these two categories, we continue by grouping the provided\nlutio s based on the type of sensor with the most frequent group being presented fir t. F r both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a for of the ca era sensor, be it a single camera or configuratio of multi le ca r s, h ads t d vic s as w ll as 3D camera sen ors. Subseque tly, ap rs ut lizing th IMU e s (accelero ete , gyroscope, m gnetome er), the ost popular sen or mo g th s lecte paper , are pr se d. For each i dividual p per, a sum ary i co por t g th s lu with th vant g and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic a d Lidar sensors as well as Blu to th Low Energy (BLE) beacons a d Wi-Fi access points.\n3.1.1. Camera-Assisted Solutio s An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed latform for ev loping solu o s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with some degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n48 N/A 49 \u2716 0 /A /A 1 N/A N/A 5 m 95 2 0 < R < 9 m 98%\n53 \u2716 0.1 3.5 90\u201395% [54] /A /A N/A /A 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m /A 57 58 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll ng s cti ns will present the solution found in th literature organized around the s nsors emplo ed. S ecifically, he prese ation is organized around the criteria of being camera or non-camera assisted as different s ts of techniques are required for those two cases. For e h of these t o categories, we continue by grouping the provided s luti n based on the type of sensor with the m st freque t group be ng presented first. For both case , the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the ca era sensor, be i a single camera or c nfiguratio of multi le cam r s, h ads t evic s as well as 3D camera sen ors. Subseq e tly, ap rs ut lizin th IMU se s (acceler ete , gyroscope, magnet meter), th ost popular s n or m ng th s l cte paper , are pr se d. For eac in ividual p per, a sum ary i co p r t g th so utio wi h th advantages and disadva tages is written. On th other hand, Section 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate an IMU sens r while other options include ultraso ic and Lidar sensors a well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access\nints.\n3.1.1. C me -As d Solution An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camera sensors is ex cted give the proliferation of smartphone d vices as the pr - ferre platform for evel ping solutions for the BVI in ividuals. Finally, only 14% of these solutions can be us d in real-life cenarios with some deg e f succes , 29% of them are practical but have combinatio of ither high ost or equire from the user to carry many s sors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 [50] /A /A N/A N/A [51] N/A N/A 5 m 95 [52] 0 < R < 9 98 [53] 0.1 m < R < 3.5 m 90\u201395% [ 4] /A /A N/A /A [55] N/A N/A 0.2 m < R < 10 m /A [56] \u2716 R > 2 m /A [57] N/A /A [58] 2 cm R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 m < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following secti ns will present the olutions found i the liter ture organized around the sens rs mpl yed. Specifically, th pr se tation is o ganiz around the crit ria of being camera or non-cam ra assisted as diff rent ets of t chniques are equired for t o e two cases. For each of these t o catego ies, w continue by grouping the provided solutions ba ed on the type f s nsor with the ost fr quent group being prese ted first. For both cases, the sensor fusion techniques are described in c mprehensible m nner. In more etail, S cti 3.1.1 covers the pap rs utilizing f rm f th c mera sensor, be it ingle camera or a configuratio f multipl c mera , h dset devices s well a 3D c m r e s rs. Subseque tl , paper utilizing th IMU s nsor (accel ro et r, gyroscope,\ngnetometer), t most p pular s ns r m t lecte p pers, re pr se ted. F r each individual pap r, summary incorporati g the solution with the advantages and disadvantages is w itt n. On the ther hand, Section 3.1.2 concerns solutions with no camera sors. Mo t of them inc rporate an IMU se sor while other ptio s include ultraso ic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions A uncertainty-based adaptive se sor fusion framewo k for Visual-Inertial Odometry (VIO) i proposed in [41] for esti ati relative motion. It mi imiz degradation from inaccurate state e timatio by determini g the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\n[58]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferred platform for eveloping solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n4 / / / / 4 \u2716 \u2716 \u2716\n1 5 m 95% 2 0 m < R < 9 m 98% 3 0.1 < < 3.5 90\u201395% 4 N/A N/A / 5 N/A N/A 0.2 m < R < 10 m /\n56 > 2 [57] / / 58 2 cm < R < 4.5 m\n[59] N/A 67\u201398% 60 2 cm < R < 12 m\n[61\u201363] 6 R = 20 cm\n6 1 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followin s ctions will pr s nt the solutio foun in t lite atur rganized around the sensors employ d. Specifi lly, the pr se tati n is organiz d around the criteri of being camera or no -camera assisted as differe t sets f c nique a e required for those two cases. For ach of the tw categories, we co tinue by g ouping th pr vided\nlutio s based the type of s sor with the most frequent group being presented fir t. F r both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 c vers the papers utilizing a form f the camera sensor, be it a single camera or a configura ion f multiple ca eras, headset devices as w ll as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorpor t ng the lution with the advantages nd disadvantages is written. On the o her hand, Section 3.1.2 concerns s lutions with no cam er sensors. Most of them inc rporate a IMU nsor whil o her op s includ ultra sonic a d Lidar sensors as well as Blu o th Low Energy (BLE) beacons a d Wi-Fi acces points.\n3.1.1. Camera-Assisted Solutio s An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR EER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with so e degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] / / N/A / [49] \u2716 \u2716 \u2716 [50] N/A N/A / / [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 < < 3.5 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] / / [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f ll n s cti ns will pres nt t solutio fou in t lite atur organized around the sensors emplo ed. S ecifi lly, he r se ati n is organiz d around the criteri of bei g ca era or no -camera assisted as differe t s ts of techniques a e required for those two cases. For h of these t o categories, we continue by g ouping the pr vided\nluti based the type of s sor with the m st fr que t group be ng presented fir t. F r both case , the sensor fusion techniques are described in a comprehensible manner. In m re detail, Section 3.1.1 c vers the p pers utilizing fo m of the camera s nsor, be it a si gle camera or a configura ion f multiple cam ras, heads t devices as well s 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, magnet meter), the mo t popular sensor among the elected papers, are presented. For each individual paper, a summary incorporat ng the so ution with the advantages and disadva tages is written. On th o h r hand, Section 3.1.2 concerns s lutions with no ca - r sensors. M st f them inc porate a IMU ens r while other options include ultra soni an Lid r s nsors a well as Blu tooth ow En rgy (BLE) beacons an Wi-Fi acc s ints.\n3.1.1. Came -As i ed Solution An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can be used in r al-l fe cenarios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are limited pract ca ity for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] /A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh foll s cti ns will resent the solution foun in th lite atur organized round the s sors empl ed. Specifi ally, he pre ation is organiz d around the criteri of bei g ca era or no -camera assisted as diff r nt s ts of techniqu s are required for t os two cases. For h of th s t categories, we continue by grouping the provided\nluti ba ed n the type of sensor with the m s frequ t group be ng presented fir t. F r both case , t sensor fusion tech iques are described in a comprehensible manner. In more detail, Sectio 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single camera or a c nfiguratio of multiple cam ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler meter, gyroscope, magnet meter), the mo t popular sen or among th elected papers, are presented. For each individual pape , a summary incorporating th so ution with the advantages and disadva t es is written. O th other han , Sect n 3.1.2 concerns s lutions with no camra nsors. Most f them incorporat a IMU se s r wh l other ptions include ultrasonic n Lid r enso s as ell as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\n2 cm < R < 4.5 m\n[59]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sens rs is expec ed giv n the prolif rat on of sma tph n evic s s t e pref rred platform for evelo ing solu ns for the BVI i dividuals. Finally, only 14% of th s solu ions can be used in real-lif scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n4 / / N/A / 4 \u2716 \u2716 \u2716 \u2716\n1 5 m 95% 2 0 < R < 9 m 98% 3 0.1 < < 3.5 90\u201395% 4 N/A N/A / 5 N/A N/A 0.2 < R < 10 /\n56 > 2 [57] / / 58 2 cm < R < 4.5 m\n[59] N/A 67\u201398% 60 2 cm < R < 12 m\n[61\u201363] 6 R = 20 cm\n6 1 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following sections will pres nt t solutions foun in the literatur organ z around the ns rs employ . Specifically, the presentatio is rganized around th criteria f being camera or non-camera assisted as differ nt sets of techni es are required for those two cases. F r each of se tw categories, we continue by groupi th provided solu io s bas d on h typ of s sor with the m st frequ n g up being p e te first. For both ca es, th nsor fusi n t chn es are descr bed i a c mpreh nsib e mann . In more d t il, Sect on 3.1.1 cov r t e a r ut lizing a for of th camer s s r, be it a ingle c mera r a c nfigura ion f ultiple c r s, headse vices a ell s 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, m gnetometer), he m st popular sensor among t select d papers, are prese t d. F r each individual p pe , sum ary inc rporat ng th s lution ith the advantages and disadvantages is writte . On the o her hand, Section 3.1.2 concerns solutions with no camer sensors. Most of them incorporate a IMU ensor while other options include ultra sonic a d Lidar ensors as well as Blu t th Low En rgy (BLE) beacons a d Wi-Fi access points.\n3.1.1. Camera-Assisted S lutio s An un rtai y-bas d aptive sensor fusion framewo k for Visual-Inertial Odometry (VIO) is proposed in [41] for estimatin relative motion. It minimiz s degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR EER REVIEW 9 of 29\nand ca era sensors is expec ed giv n t e proli erati n of sma tphon devices s t pref r ed platform for develo ing solu io s for the BVI i dividuals. Finally, only 14% of th se solu i ns can be u ed in real-l fe cenarios with so degree of success, 29% of them are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic ange Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n4 / / N/A / 4 \u2716 \u2716 \u2716 \u2716\n1 5 m 95% 2 < R < 9 m 98% 3 0.1 m < R < 3.5 90\u201395% 4 N/A N/A /\nN/A N/A 0.2 < < 10 / 56 > 2\n[57] N/A / 58 2 cm < R < 4.5 m\n[59] N/A 67\u201398% 60 2 m < R < 12 m\n[61\u201363] 6 R = 20 cm\n6 1 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh foll ng s ctio s will present t s lutions f u in th literature organ z ar und he ns rs empl ed. S cifically, t e pr se tatio is rganized around the crieria f being ca ra or non-cam ra assisted as differ nt s ts of techniques are required for those two cases. For ea of these t o categories, we continue by gr uping th provided s lu i bas on h type of e s r with the m t freque t g up b ng p e ted first. F r oth ca e , the n or fus on echn q es are des r b d i a compreh nsib e manner. I re detail, S ct n 3.1.1 cover t e p p rs ut lizing fo m of th camer s nsor, be it a ingle c me a r a fig r tion f ul ple c r s, heads devices a well s 3D camera sensors. Subs q e tly, papers utilizing the IMU senso (acceler eter, gyroscope, m gneto eter), he mo t popular sensor among the elect d papers, are present d. For each individual pape , summary inc rporating th so ution with th advantages and disadv tages is written. On th other hand, Section 3.1.2 concerns s lutions with no camra sensors. M st f them incorporate an IMU sens r while other options include ultraso ic a Lidar ensors a well as Blu to th ow Energy (BLE) beacons a d Wi-Fi access ints.\n3.1.1. Cam -As i ed Solutio A un rt inty-ba d aptiv sensor fusion fram w rk for Vis al-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\na d camera sensors is ex ec ed giv the proliferation f sma tphon d vices s the pr - f rre platform for evel ing solu ions for the BVI i ividuals. Finally, only 14% of th se solu ions can be us d in real-life cenarios with some deg e f succes , 29% of them are practical but have c mbinatio of ither high ost or equire from the user to carry many s sors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n48 / / N/A N/A 9 \u2716 \u2716 \u2716 \u2716 \u2716 0 1 5 m 95% 2 0 < R < 9 m 98 3 0.1 m < R < 3.5 m 90\u201395% 4 N/A N/A / 5 N/A N/A 0.2 < R < 10 /\n56 R > 2 m [57] \u2716 N/A / 58 2 cm < R < 4 5 m\n[59] N/A 67\u201398% 60 2 cm < R < 12\n[61\u201363] 6 = 20 c\n6 1 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh followi g se ons will present t olu ions found i he l ter tur organ z a ou d the ns rs empl yed. Specifically, the pr se tatio is ganiz around the cri-\nria f being camera or on-cam ra assisted as diff r nt sets of techniques are equired for ose tw cas s. Fo each of thes t o catego i s, w c ntinue by gr upi g th p ovided solutio s ba ed n the ype f s nsor with the ost f qu nt group b i g pre ted first. For both ca s, th sensor fusi te iqu s ar escribed i c mp h nsibl m nn r. I re et il, Se ti 3.1.1 ver he pap r utilizing form of th c mer s nsor, be it ingl c mera or configuratio f multiple camera , he dset devices as well as 3D c mera ens rs. Subsequently, apers utilizing t e IMU s ns r (acceler meter, gyroscope, m gnetomet ), he most popular sensor am t e elect d p p rs, are present d. For each i dividual pap , summary inc rporati g th solution with the advantages and disadvantages is w itt . On the ther and, Section 3.1.2 concerns solutions with no camera ensors. Most of them inc rporate an IMU se sor hile ot er ptions include ultraso ic and Li r ensors as well as Blu t th L w Energy (BLE) beac ns a d Wi-Fi acc ss points.\n3.1.1. Camera-Assisted Solutio s A un rtainty-bas d aptive se sor fusion framewo k for Visual-Inertial Odometry (VIO) i proposed in [41] for esti ati relative motion. It mi imiz degradation from inaccurate state e timatio by determini g the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\n67\u2013 8\n[60]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expec ed giv n the proliferat on of sma tph n devices s t e pref rred platform for evelo ing solu ons for th BVI i ividuals. Finally, only 14% of th s solu ions can be used in real-lif scenarios with some degree of success, 29% of the are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limit d practicali y f r specific scen rios while 32% are purely xperimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy 4 4\n48 / / 49 50 N/A N/A /\n[51] / / 5 m 95% [52] 0 < R < 9 m 98% [53] 0.1 < < 3.5 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R < 10 / [56] \u2716 R > 2 N/A [57] \u2716 / / [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following sec ions will pres nt th solutions found in th lit ratu organ z around the ns rs employ . S ecific lly, the p ese tati is rganized ar und criteria f being camera or non-camera assisted as differ nt sets of techniques are required for those two cases. For each of hese two catego ies, w continue by grouping th p ov ded solutions based on the typ of nsor with the most fr q ent group b ing pr nted fir t. For both ca es, the sensor fusion techniques are de crib d i a comprehensible mann . In more det il, Section 3.1.1 cover th aper utilizing a form of th camera sensor, be it a single c mera or a configuratio f mu tiple ca eras, headset devices as ell as 3D camera sensors. Subseque tly, papers utilizing th IMU ensor (acceler me r, gyroscop , m gnetome er), he m st popular sens r among th selec d p pers, re presen d. For each i dividual p pe , summary i c rporat ng th s lu with the advant g s and disadvantages is written. On the o her hand, Section 3.1.2 concerns solutions with no camer sensors. Most of them incorporate a IMU ensor while other options include ultra sonic and Lidar ensors as well as Bluet oth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An un rtain y-bas d aptive sensor fusion framewo k for Vi u l-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR EER REVIEW 9 of 29\nand ca era sensors is expec ed giv n the proli erat on of sma tph n devices s t e pref r ed platform for evelo ing solu o s for the BVI i dividuals. Finally, only 14% of th s solu ions can be u ed in real-l fe cenarios with so e degree of success, 29% of the are practical ut have a combination f either high cost or requi e from the user to carry many s nsors, 24% are lim ted pract cali y for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynami Range Accuracy [41\u20134 ] \u2716 \u2716\n[48] / / N/A N/A [49] \u2716 [50] N/A N/A N/A N/A [51] / / 5 m 95% [52] 0 < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R < 10 / [56] \u2716 R > 2 m N/A [57] N/A / [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh follo ng s ctio s will p es nt t s lutions f u d in th literatur organ z around he ns rs empl . S ecific lly, the pr se tatio is rganized rou d th crieria f bein ca ra or non-cam ra assisted as differ nt s ts of techniques are required fo those two cases. For ea of hese t o categories, we continue by grouping th provided s luti ns based on the typ of n or with e m t frequent group b g pr ented first. For oth ca e , the s n or fus on echniques are des ribed i a comprehensible mann . In more det il, S cti n 3.1.1 cover the per utilizing fo m of th camera sensor, be it a single c e a or a c nfiguratio f multiple ca ras, heads t evices as ell as 3D camera sensors. Subs q e tly, papers utilizing th IMU s nso (acceler eter, gyrosc pe, m gneto e e ), h t popular s nsor am ng the elect d papers, are presen d. F r eac i ividu l pape , summary inc rp rating th s u wi h th advant ges and dis dva tages i writt n. On th other hand, Section 3.1.2 c ncerns s lutions wit no camra sensors. M st f them incorporate an IMU sens r while other options include ultrasonic an Lidar ensors as well as Blu t oth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An un rt in y-ba d aptive sensor fusion fram wo k for Vis al-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR EER REVIEW 9 of 29\na d ca era sensors is expec ed giv n the proli eration of sma tphon devices s the pref r ed platf m for develo ng solu io s fo the BVI i dividuals. Finally, only 14% of th se solu io s can be used in r al-l fe cenarios with so degree of success, 29% of the are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are lim ted pract ca ity for specific s en rios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u20134 ] \u2716 \u2716 \u2716\n[48] / / N/A N/A [49] \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] / / 5 m 95% [52] 0 < R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A / / [55] N/A N/A 0.2 < R < 10 / [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh follo ng ctio s will pres nt t s lutions f u d in th literature organ z round he ns rs mpl ed. S ecifi ally, the pr s tatio is rganized around the crieria f being ca ra o non-cam ra assisted as differ n s t of techniques are required f r hose two cases. For a of th t categories, w continue by gro ping th provided s luti ns based on the type of s nsor with the m frequent group b ng pre ented first. For oth ca e , the s n or fus on echniques ar des ibed i a comprehensible manner. In m e detail, S cti n 3.1.1 cov the p pers utilizing fo m of th camera sensor, be it a single c e a or a c nfiguratio f multiple cam ras, heads t evices as well as 3D c mera se sors. Subs q e tly, paper utilizi g the IMU senso (acceler ter, gyroscope, m gne o eter), h t popular s n or am g the elect d papers, are present d. For eac in ividu l pape , summary inc rp rating th solution wi h th advantages and disadva t es is written. O th other han , Sect on 3.1.2 concerns s lutions with no camera sensors. M st f them incorporat a IMU se s r wh le other options include ultrasonic nd Lid r ensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An un rt nty-ba d aptive senso fusion fram work for Vis al-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\n2 cm < R < 12 m\n[61\u201363]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the p oliferati n of sm rtphone d vices as the preferred platfor for developing so utions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of the are practical but have a combinatio of either high cost requir fr m the user to carry many sensors, 24% are limited practicality f r specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range ccuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] N/A / N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followi g secti s will pr t the s lutio s f n i th lite atu e orga ize around the sens rs empl ye . Sp cifically, pr s tati is org nize a und t c iteria of b ing camera or non-c ssist d differe t ts f t c iq s a requir for those two cases. For each of t two categories, w o tinu by gr u i g t p ovide solutions base on the ty of sen or with t most frequent group being presented first. For both cases, the s sor fusion t chniques are described i a c pre ensible man er. In more detail, Section 3.1.1 c vers th p pers utilizing form of t e camer ensor, be it a single camera or a configu at on of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing he IMU sensor ( ccelerom ter, gyrosc pe, magnetometer), he most popular sensor mo g he elected pap , are pres ted. F each individual paper, a summary incorp ting the solution with the adva tages and disadvantages is written. On the other hand, S ction 3.1.2 concerns solu ions with n camera sensors. Mos f them incorp rate an IMU s nsor while other opti ns in lu ultrasonic and Lidar sensors as well s Bluet oth Low Ener y (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nd camera sensors is ex cted given the proliferati n f s artphon d vices as the pref r ed platfor for develop ng solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l fe cenarios with some degree of success, 29% of the are practical ut have a c mbinatio f either high cost r requi e from the user to carry many s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport Static Dynamic Range ccuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] N/A / N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 c < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo g s cti s will pre t t e s luti s i th lite atu org ized ou d the ns rs mpl e cifically, th res ati is org ize a u d t c iteria of b i g camera r on-c m a sisted iff re t ts of t ch iq s ar requir for those two cas s. For a h of th t o categ ri s, w o tinu by groupi g the p ovide s lutio s base on the ty of sen or with t e m st freque t group be g pr sented first. For both cas , the s o fusi n t ch iques are describ d in a comp e e sibl manner. I more detail, Section 3.1.1 c v r th p pers tilizing fo m of the camer se or, be it a singl camera or a configu ation of multiple cam ras, heads t devices as ell as 3D c me sensors. Subseq ently, apers utilizing the IMU sens (acc ler m ter, gyrosc pe, mag et m te ), he mo t popular s ns r m ng th elected pap s, are pres nted. For each individual paper, a summary incorp ting the s lution with the advantages nd disadva tages is written. On th other hand, S ction 3.1.2 concerns s lu ions with no camera sensors. M f th m incorporate an IMU s ns while ot e opti ns in lud u trasonic and Lidar sensors as well as Blu t oth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for esti ating relative otion. It minimizes degradation from inaccurate state es imatio by determining t e states that should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nd camera sensors is ex cted giv the proliferation f smartphon d vices as the pref r ed platf m for evelop ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can be used in r al-l fe cenarios with som degree of success, 29% of them are practical ut have c mbinatio f ei r high cost or requi e from the user to carry many s nsors, 24% are limited pract ca ity for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n48 N/A N/A N/A 49 \u2716 \u2716 \u2716 \u2716 \u2716\n[50] N/A / N/A N/A 51 N/A N/A 5 m 95%\n[52] 0 m < R < 9 m 98% 53 0.1 m < R < 3.5 m 90\u201395% 54 N/A N/A N/A N/A 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 c < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh f llo g s cti s will p e t t e solutions f i t lite atu org nized u d the ns rs pl . S cifical y, the pres tati n is org ize a ou d th crit ria of b i ca ra or on-cam a siste a iff re t ts of t ch iq es ar required for thos two c s s. Fo a h f th e t o categ ri s, w tinu by grouping the p ovide s luti s based on the ty e of sensor wit the m s f equ t g oup be g presented first. For both cas , the sensor fusi t ch iqu s are d scrib d i a comp e e sibl manner. I more det il, S ti 3.1.1 cov r the p pers utilizing fo m f the camera sensor, be it a singl camera or configuration f multiple cam ras, heads t devices as well as 3D c mer se sors. Subseq ently, aper utilizing t e IMU sens (acceler meter, gyroscope, magnet e ), h mo t popular sensor am ng th ect d p pers, are presented. For each i dividual paper, a s mmary inc rporat g th solution with the advantages and disadva t ges is w itte . On th other and, Sect on 3.1.2 concerns s lutions with no camer s nsors. M t f th m incorporat an IMU sens while ot e opt ons include u trasonic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensor is expected giv the prolif ration of smartphon devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r al-l fe cenarios with so degree of success, 29% of them are prac al ut have c mbinatio f ei r high cost or requi e from the user to carry many s nsors, 24% are l mited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / N/A N/A N/A [55] / N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nTh f llo g s cti s will p e t the solutions f d in th literatu e organized arou d th ns rs pl e . S ifi ally, the pres tati n is orga ize a ound th criteria of b i ca era or on-cam ass te as iff r t ts of t ch iq es are required f t s tw c s s. For a h f th e cat g ries, w o tinu by grouping the p ovided s lut s bas d on t e type of s sor wit the m f equ t group be ng presented first. Fo bo case , the se sor fu t c niqu s are d scribed i a comprehensible manner. In more det il, Se ti 3.1.1 cov r the p pers utilizi g fo m of the camera sensor, be it a single ca ra or configuration of multiple cam ras, heads t devices as well as 3D cam ra se or . Sub eq ently, papers utilizing t e IMU senso (acceler eter, gyroscope, ma n t e r), th mo t p pula sen o among th e ct d papers, are presented. For each i divid l paper, a sum ry inc rporat g th solution with the advantages and\nisadva t es is w itte . On th ot er and, Sect on 3.1.2 concerns s lutions with no camra s n ors. M t f the i corporat an IMU sens r while ot er opt ons include ultrasonic a Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e s imatio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand came a ensor is expected given th prolif ration of smartphon devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r l-l fe cenarios with som degree of success, 29% of them are pr c al ut have co binatio f ei r high cost or requi e from the user to carry many s nsors, 24% are l ited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r uppo t St tic Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT foll g s cti s will p e t th solutions fo d in th literatu e organized aro th s s rs l . S ifically, the pres tati n is orga ize a ound the criteri of b i c er r on-cam a t as iff re t ts of tech iq es are required f t s tw c ses. For a h f th e t cat g ries, w co tinue by grouping the provided s lu i s bas d o the type f s sor with the m s frequent group be ng presented first. Fo both case , the se sor fu o t chniques are d scribed in a comprehensible manner. I re detail, Sec i 3.1.1 cov rs the p pers utilizing fo m of the camera sensor, be it a single a ra or a configurati of ultiple cam ras, heads t devices as well as 3D cam ra s or . Sub eq ently, papers tilizing the IMU senso (acceler meter, gyroscope, ma et meter), the m t p pula se or among th el cted papers, are presented. For each ind vid l paper, a sum ry i corporating th solution with the advantages and\ni adva t es i ritt n. O th ot er han , Sect on 3.1.2 concerns s lutions with no camra s n ors. M t f the corporat a IMU se s r while other options include ultrasonic d Lid r sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Solutions An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odome-\ny (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurate sta e es matio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\n[64]\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand cam ra se sors is xp ct d given the rolife ati of martpho e d vices as he pre fer ed platform o deve ping soluti n for th BVI dividuals. Finally, ly 14% of these sol tions can be used in e l-life sc narios with some degre of uc s , 29% the are practi al b have c mbinat on of either high cos or req ire fro the user to r y many sensors, 24% re limited practic ty for spe ific scenarios whi e 32% are pure y xperimental.\nTable 3. Solutions and bstacle detection.\nPa er Sup ort Static Dynami Range Ac uracy [41\u201347] \u2716\n8 N/A 49 50 / 51 N/A 5 m 95% 5 0 m < R < 9 m 98% 5 0.1 < < 3.5 90\u201395% 5 N/A\n[5 ] N/A 0.2 < < 10 / [56] R > 2 m N/A [57] N/A / [58] 2 cm < R < 4.5 N/A [59] / 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llowi g s c i will pre t the so ti f u n the lit t ga iz d aroun the sens s ploye . ecific lly, the r se t t is org ize un the cr - t ri of b i g ca ra or on-cam isted diff rent ts of tec iq s r quire for two c s. F ch of the two t gori , w n nu by g o pin provid solution based on th type of e s r with the m t freque t g oup be ng pr se t d f rst. For both cases, the r fusion t ch ique are d crib d in co reh nsible man er. In more d tail, Section 3.1.1 cove th papers til zing a m of the cam r se or, be it a si gle cam ra or a co figuration of multipl cameras, he d et device s well as 3D camera sen ors. Subs qu tl , p pe s utilizi g th IMU sensor (ac lerom t r, gyr scop ,\ngneto eter), t most popular s n r m g th sel c ed paper , resen . For each individ al p per, a summ ry corpor ti g the sol tion it th a va tages and disadvantages s writt n. O h her ha d, S tio 3.1 2 c c rns solutions wi h o c m er se sors. Mo t of them incorpor an I U sensor while o h op ions i clude asonic and Lidar se s rs as well as Blueto th Low Energy (BLE) beacon and Wi-Fi ac s points.\n3.1.1. Camera-As isted Solution An uncertainty-based adapt ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is proposed in [41] for estimating relative motion. It minim zes degr dat on fr m inac urate state estimation by d ter ining the states hat should b i clude in h estimation proces . The e degr ding stat s can ar se und r motio character stics that nullify\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand cam ra se sors is xp ct d given th rolif ti of martpho devices as he pre fer ed platform o deve oping solutio s for the BVI dividuals. Finally, nly 14% of these sol tions can be u ed in e l-l f c narios with s me degre of uc s , 29% the are practi al av c mbinat on f e ther high c s o req i e fro the user to r y many s nsors, 24% re limited pract ca ty for spe ific scenarios while 32% a e pure y xperimental.\nTable 3. Solutions and bstacle detection.\nP er up ort Static Dyna i Range Ac uracy [41\u201347] \u2716\n48 N/A [49] 50 /\n[51] N/A 5 m 95% 5 0 m < R < 9 m 98% 5 0.1 < < 3.5 90\u201395%"
        },
        {
            "heading": "5 N/A",
            "text": "[5 ] N/A 0.2 < < 10 / [56] R > 2 m N/A [57] N/A / [58] 2 cm < R < 4.5 m N/A [59] / 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llo g s c i s will prese t he so tion fo in the lit rat re ga iz d aroun th nsors plo ed. Sp cific lly, the pr se t t o is org ized ar und the cr - t ri of bei g ca r or n-c me isted iff r nt s ts of tec iqu s r require for two c e . F ch f h s t o t g r , w n nue by g o pin th provid s l tion b sed on t e type of s r with the m t freque t g oup be ng presented first. For bot cas , th s ns r fus on t chnique are d crib d i c eh nsible man er. In more et il, Secti n 3.1.1 c vers th p pers util zing fo m of th cam r sensor, be it ingle cam ra o co figur tion of multiple cam ras, he d t device as well as 3D camera ensors. Subs q tly, p pe s utilizi g th IMU se o (ac el r m ter, gyroscop , magn t eter), the most popular sens r m g th selecte p per , r presente . For each individ al p per, a umm ry corpor ti g he ol tion w t the a va tages and disadv tages is wr tt . O o her ha d, S tio 3.1 2 c c rns s lutions wi h cam er e sors. M t f them incorpor an I U sens r while oth op ions include rasonic a d Lid r se sors as well as Blu t th ow E ergy (BLE) beacons and Wi-Fi ac es points.\n3.1.1. Camera-As isted Solution An u ce t inty-based adapt ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is pr po ed in [41] for esti at ng relative tion. It minimizes degr dat on fr m inac u ate state es imatio by d ter ining t e states hat should b i cluded in h estimation pro es . Th e gr ding stat s can ar se und r motio c aracter stics that nullify\nSensors 2023, 23, x FOR PEER REVIEW of 29\nand camera se s rs is x ected giv the roliferati n of martph e d vices as the pr ferre platform for v l ping s lutio s for the BVI i ividuals. Fi lly, only 14% of thes solutions can be s d in real-lif c narios with some deg f s cc s , 29% f the are practical b have a c mbination of ither high ost or equire from the us r to arry m ny s sors, 24% are limited practicality for specific scenarios while 32 are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n8 N/A / N/A 9 0 N/A 1 N/A N/A 5 m 5\n0 9 98 0.1 m R < 3.5 m 90\u201395%\nN/A 5 N/A N/A 0.2 m < R < 10 m 6 R > 2 7 N/A 8 2 cm < R < 4.5 m N/A\n59 N/A 67\u201398% [60] 2 c < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 m N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 2 N/A N/A 3 \u2716 \u2716 \u2716 \u2716\n[74] R < 6 m N/A [75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT ol wing se ti ns will p esent the lutio s f u d i h lit ure organiz d round h senso s ployed Sp cifica ly, pr s at on is o gan z d round t\nteri f b ing cam r or n n-cam r a sisted a diff r nt t of echn ques ar q ir for two c ses. F eac f thes t o cate i , w c tinue by g uping th provid\nluti ba ed n t ty e f s n r wit th m t fr q ent g oup bei g prese ted first. For both cas s, the r fusion t ch iques are d scribed i c m re ible m nn r. In m r d tail, Secti 3.1.1 covers the pap rs utilizi g form of th c mer sensor, be it i gle amera or co fi ur i n of ultipl amera , he d t dev c as w ll as 3D c m r ens rs. Sub que tly, p e s utilizing t IMU s s r (acc lerometer, gyros op ,\nagn to eter), the most pul r se s r a t sel c ed p p r , re presente . For e h individual p per, summary incorp r ti g the solution with the adva tag s nd disadvantage is w itt n. On the h r h d, S ction 3.1.2 concerns s lutions with o cam er e sors. Mo t of th m inc rpor IMU sensor hile ther p io s include tr - s ic and Lidar se sors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mer -Assist d Solutions A u certainty-b sed adaptive se or fusio framew k for Visual-Inerti l Odometry (VIO) i proposed in [41] for esti ti relativ otion. It mi i iz degrad tion from i ccurat st te e timatio by det rmini g the states th t hould be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nR = 20 cm\n[65\u201371]\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartph ne devices as the preferred platform for developing solutions for the BVI i dividuals. Fi lly, only 14% of these solutions can be used in real-life scenarios with some degree of succ ss, 29% f them are practical but have a combination of either high cost or require from the user to arry any sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range ccuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] N/A N/A 0.2 < R < 10 / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] c 4.5 / [59] N/A 67\u201398% [ 0] 2 c < R < 12 m /\n[ 1 63] [64] R = 20 c /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following secti s will p ese t the soluti s un in th liter ture orga iz around the sensors empl ye . Sp cifically, the res n ati is organized around th criteria of being c er or n n-c mer ass sted differe t sets f tech iqu s ar requi ed for those two case . For each f t ese two categories, we co tinu by group provided solutions based on the type of ens r with the mos freq nt gro p being pre t d first. For both cases, the sensor fusion te h iques e de c ibed in a compr hensible manner. In more detail, Section 3.1.1 co ers th papers utilizing a form of the camera sensor, be it a single ca era or a configuration of multiple cameras, headset devices a well as 3D camera sensors. Subsequently, papers til zing IMU sensor (acc lerometer, gyroscope, magnetometer), the mo t p ular ensor am ng the selected papers, are pres nted. For each individual paper, a su mary incorporating the solution with the advantages n disadvantages is written. On the other han , Section 3.1.2 co cer soluti n ith no camera sensors. Most of them inc rpor te an IMU sensor while o her options include ultrasonic and Lidar sensors as w ll as Blu tooth L w Energy (BLE) b aco s and Wi-Fi acce s points.\n3.1.1. Camera-Assisted Soluti ns An uncertainty-based adaptive sensor fusion framew rk for Visual-Inertial Odometry (VIO) is proposed in [41] for estim ting relative motion. It mini izes degrad tion from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of s artphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l f cenarios with some degree of success, 29% f them are practical ut have a combination f either high cost or requi e from the user to carry m ny s nsors, 24% are limited pract cality for specific scenarios while 32% a e purely experimental.\nTable . Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347]\n[ 8] / / / / [ 9] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95 [ 2] 0 m R < 9 98% [ 3] .1 3.5 90\u201395% [ 4] N/A N/A N/A / [ 5] N/A N/A 0.2 < R < 10 / [ 6] R > 2 / [ 7] / N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 c < R < 12 m /\n[ 1 63] [64] R = 20 c /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s cti will pr e t the solutions fo in th literatu e orga iz d rou the s nsors mpl . Sp cifically, th pres tati n is org ized around t e c iteria of being c er or n-ca era assisted s d fferent sets f techniq es are required for those two case . For a h of t ese t c tegories, we continu by groupi g t pr vided s lutions based the type f nsor with the m t freq ent group b ng presented f r t. For both case , the se sor fusion tech iques are described in compr hensible ma ner. In more detail, Sectio 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it single ca era or configuration of multi le ca ras, heads t devices as well as 3D camera sensors. Subseq ently, papers uti iz ng th IMU senso (acceler meter, gyro cop , magnet meter), the mo t ular se sor a ong the elec ed papers, are presented. For e h individual p pe , a ummary inc rporating the solution with the adva tag s an disadva tage s written. On th o h r h , Section 3.1.2 concer s s lut ns wit o camera sensors. Most f them inc rporate n IMU sens r while ther options include ltr - sonic and Lidar e sors as w ll as Blu tooth ow Energy (BLE) b acons and Wi-Fi access points.\n3.1.1. Camera-As isted Soluti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for sti ting relative otion. It minimizes degradation from inaccurate state es imatio by det rmining t e states th t should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sens rs is expected given the proliferation of s artphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of thes solutio s can be used in r al-l fe cenarios ith som degre of succ ss, 29% of the are practical ut have a combination f ei r high cost or requi e from the us r to carry many s nsors, 24% are limited pract ca ity for specific scenarios while 32 a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347]\n[ 8] / N/A / / 49 \u2716 \u2716 \u2716 \u2716\n[ 0] /A N/A 1 N/A N/A 5 m 5 2 0 9 98% 3 .1 3.5 90\u201395%\n[ 4] N/A / [ 5] N/A N/A 0.2 < R < 10 / [ 6] R > 2 / [ 7] / [ 8] 4 5 / 59 N/A 67\u201398%\n[ 0] 2 c < R < 12 m / [ 1 63]\n[64] R = 20 m / [65\u201371]\n[ 2] N/A / 3\n[74] R < 6 m N/A [75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng cti s will res t th solutions fo i t lite atu e orga ized rou the s nsors pl e . S cifically, the p es tati n is a ize around the criter a of bei g camera or -ca era assisted as different s ts of t chniqu re require for thos w ca es. For a h of th s t c t g ri s, we co ti ue by groupi g th pr vided\nluti ns bas d o the type of sensor with th s f q ent group be ng t fi t. F r both case , th sensor fusion techniques are described i a comprehensible ma n r. In mor detail, Sectio 3.1.1 covers t e p pers utilizing fo m f the camera sensor, be it single ca era or co figuration of multiple ca r s, heads t devic s as well as 3D camer se so s. Subseq ently, ap utilizing the IMU se so (acceler et , gyros op , magnet meter), th mo t p pul r se r among th elec ed papers, ar presented. For e h individual paper, a summary incorporating th soluti n with the a va tag s nd disadva t g s writt . On th o h r h d, Sect o 3.1.2 concerns s lut ons with o camera se sors. Most f them incorporate a IMU sens r while other options include ultras nic nd Lidar ensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-As isted Soluti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for estimating r lative otion. It minimizes degradation from in ccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is expected given the proliferation of smartph n devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r al-l fe cenarios ith som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are l ited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St ti Dynamic Range Accuracy [41\u201347]\n[48] N/A / / / [49] \u2716 \u2716 \u2716 \u2716 [ 0] / N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] / N/A N/A / [ 5] / N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4 5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nThe f llo ng s cti will pr s t th sol tions f nd in th lite ature organized roun the s nsors mpl . S cifi ally, he res tati n is o a ize around the criteri of bei g ca era or on-ca era ass sted as ifferent s ts of t chniques are required f thos tw cas s. Fo a of th se t cat g ri s, w co tinue b grou ing the provided s luti ns bas d on t type of s nsor with the s frequ nt group be ng presented first. For bot ca e , the sensor fu on tech iques are described i a compr hen ible manner. In more detail, Sectio 3.1.1 c vers t e p per utiliz ng fo m f the camera sensor, be it a single ca ra or a confi uration of multiple am r s, heads t devices as well as 3D cam r en or . Sub q n ly, pape s ti izing the IMU s so ( cceler eter, gyroscope, ma n t met r), the mo t opular sen r am g th el cted p pers, are presented. For ach individ al pap r, a summ ry incorporating the sol tion wit the advantages and di adva t ges ritt . O th t er h nd, Sect on 3.1.2 con erns s l tions with no camera se sors. M st f th m i corporat a IMU sens r while other options include ultrasonic d Lidar sensors as well as Blu tooth ow En rgy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me -As st d S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurat st e s imatio by determining t states that hould be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is expected given the proliferati n of smartphon devices as the prefer ed platf m for dev op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r l-l fe cenarios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s nsors, 24% are l ited pract ca it for specific scenarios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r uppo t St tic Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 [ 0] / N/A N/A N/A [ 1] / N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [ 4] / N/A N/A / [ 5] / N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4 5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT follo ng cti s will pr s t th s lutio s f nd in th literature organized aro the s nsors mp e . S ecifically, the pres tati n is orga ized around the criteria of bei g c era r -ca era ass sted as different s ts of techniques are required f those t o cases. Fo a f th se t c tego ies, we ontinue by grouping the provided s uti ns bas d on the type of s sor with the s frequent group be ng presented first. For bot case , the se so fu on techniques re d scr bed in a comprehensible manner. In m re detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a single am ra or a configur tion of ultiple cam ras, heads t devices as well as 3D cam ra s n or . Subseq ntly, pa ers ti izi g th IMU s nso (acceler meter, gyroscope,\na net met r), the t p pular se or among th elected papers, are presented. For ach individ al pap r, a s mm ry incorp rating the solution with the advantages and d adv t ges itten. On th t e h nd, Sect on 3.1.2 concerns s lutions with no camr s nsors. M st f th i corp rat a IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow En rgy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me a-As sted S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odome-\ny (VIO) is p po ed in [41] for esti ati g r lative otion. It minimizes degradation from i accurate sta e es matio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\n[72]\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand cam ra sensors is exp ct d given the rolife ati of martpho e d vices as he prefer ed platform deve opi g solutions for the BVI dividuals. Finally, ly 14% of these sol tions can be used in e l-life sc narios with some degre of uc s , 29% them are practi al but have combinat on of either high cos or req ire from the user to ar y many sensors, 24% re limited practica ty for spe ific scenarios whi e 32% are pure y xperimental.\nTable 3. Solutions and bstacle detection.\nPa er Sup ort Static Dynami Range Ac uracy [4 47]\n[48] / / [49] \u2716 \u2716 [ 0] N/A / [ 1] N/A 5 m 95% [ 2] 0 < < 9 m 98% [ 3] .1 m R 3.5 m 90\u201395 [ 4] N/A / [ ] N/A 0.2 < R < 10 m N/A [ 6] > 2 / [ 7] N/A N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llowi s cti will pr se t th so tio f un i t e literature i ar und th sensors employ . Specifi lly, t e p e e t ti n is org nized ar und the cr - teri of bei g camera or o -ca era as isted diff re t s ts of techniques re eq ired for those two cases. For ach of th s two categ rie , w co u by groupi r vided s lutions based n th typ of s r with the mo t frequent group be ng presented f rst. F r both cases, the sensor fusion t ch ique are de cr b d in c preh nsible man er. I more detail, Section 3.1.1 covers th pa ers util zing a form of t e cam ra ens r, be it a single ca ra r a configuration of multi le c eras, he dset devices as w ll as 3D camera se so s. Subs q tly, p utilizi g he IMU ensor (ac l om t , gyro cop , magnetometer), th most opular se sor a ong th sel ct d papers, ar resent d. For each individ al p pe , a umm ry c rpor ti g the sol ti n wit the a va tag s an disadvantag is writt n. O he o h r hand, Se ti 3.1 2 c c r s sol ti s w o c m era sensor . Most of them incorporate an I U nsor while o h ptions include ul rasoni and Lidar se sors as well as Blueto th Low Energy (BLE) beac and Wi-Fi ac es points.\n3.1.1. Camera-As i ed Soluti n An uncertainty-based ada t ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is proposed in [41] for estimating relative otion. It minim zes degr dat on fr m inac urate state estimation by d ter ining the states hat should b i clude in h estimation proces . The e degr ding stat s can ar se und r motio character stics that nullify\nSensors 2023, 23, x FOR E R REVIEW 9 of 29\nand cam ra sensors is exp ct d given th prolif ti of smartpho devices as he pr - fer ed platform o deve oping solutio s f r the BVI dividuals. Finally, nly 14% of these sol tions can be u ed in e l-l f c narios wit so e d gre of uc s , 29% them are practi al ut av combinat on f either high c st o req i e from the user to ar y many s nsors, 24% re limited pract ca ty for spe ific scenarios while 32% a e pure y xperimental.\nTable 3. Solutions and bstacle detection.\nP er up ort Static Dynami Range Ac uracy [4 47]\n[48] / / [49] [ 0] N/A / [ 1] N/A 5 m 95% [ 2] 0 < < 9 m 98% [ 3] .1 m R 3.5 m 90\u201395 [ 4] N/A / [ ] N/A 0.2 m < R < 10 m N/A [ 6] > 2 / [ 7] N/A N/A [58] 4.5 / [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] [64] R = 20 cm /\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f ll g cti s will r se t e so tion fo in the literature organiz d ar und h s nsors e plo ed. S ecifi lly, t e rese t tio is rg ized ar und he cr - t ri of bei g r no -ca ra as isted diff re t s s of tec niques re required for those two ca es. For h of thes t o categ rie , we con ue by gr pin th provided s luti b sed n the type of e s r with the m frequ nt grou be ng pr sented first. For both cas , th s nsor fus on t ch ique are de crib d in co preh ible anner. I more det il, Section 3.1.1 c v s th p pers util zi g fo m of the cam ra sensor, b it single ca ra r configur tion of multiple ca ras, he dset device as well as 3D ca era e so s. Sub q tly, p utilizi g the IMU s nso ( c ler m t , gyrosc p , mag et meter), th o t popular sensor mong th elected p pers, ar presented. F r ach ind vid l p per, a umm ry corpor ti g he o ti n wi the a va tages and d s dva tag wr tt n. O th her hand, Se tio 3.1 2 c rns s l t s w o cam era sensor . M st f them incor orate an I U s sor while oth opti ns include ul rasoni and Lidar s sors as well a Blu h w En rgy (BLE) beaco and Wi-Fi ac es point .\n3.1.1. Camera-As i d Soluti n An u ce t inty-based adapt ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is pr po ed in [41] for estimat g relative tion. It minimizes degr at on fr m inac u ate state es imatio by d ter ining t e states hat should b i cluded in h estimation pro es . Th e gr ding stat s can ar se und r motio c aracter stics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand camera sens rs is x ected giv n the proliferati n of s artph e d vices as the pr - ferre pl tform for v l pi g s lutio s for the BVI i ividuals. Fi lly, only 14% of thes soluti ns can be us d in real-lif c narios with some deg f succ s , 29% f the are practical but have a combination of ither high ost or equire from the us r to arry m ny s sors, 24% are limited practicality for specific scenarios while 32% are purely experiment l.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [4 47]\n[ 8] / / / / 49\n[50] / / [51] / / 5 m 5 [52] 0 < < 9 98 53 .1 3.5 90\u201395 54 / 55 N/A N/A 0.2 < R < 10 56 > 2 / 7 /\n[ 8] 4.5 / 59 N/A 67\u201398%\n[ 0] 2 cm < R < 12 / [ 1 63]\n[64] R = 20 m / [65\u201371]\n[ 2] N/A / 3\n[74] R < 6 m N/A [75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sec i n will present the lutions fou d i lit r ture organized around th s nsors empl yed. Sp cifi ally, the pr s tation is organiz around t crit ri f being cam ra or n -cam ra assisted a diff rent sets of techniques are equir for t ose two cases. For ac f these t o categ ies, w continue by groupi g the provided s luti s ba ed n t e type f s ns r with th most fr q ent group bei g prese ted first. F r both cases, the se sor fusion t chniques are d scribed i c mprehensible m nn r. I m r detail, Secti 3.1.1 covers the pap rs utilizi g form of th c mera sensor, be it singl c mera r a confi ur tio of ltipl a era , he dset devic s as well as 3D c m ra e s s. Sub quently, a r utilizing t e IMU s nsor (acc l et , gyrosc p ,\nagn ometer), th most pul r se s r a t se ct p p rs, r present d. For e ch individu l p per, summary incorp ati g the soluti n with th a vantages nd di a vantag is w itt n. On the ther ha d, Sectio 3.1.2 c ncerns s l tio s w h o ca er e s r . Most of th m inc rpor t n IMU s or whil ther ptio s includ ultr - s i and Li ar se sors as well as Bluetooth Low Energy (BLE) beaco s and Wi-Fi access points.\n3.1.1. C mer -A sist d S lu i A u certainty-based adaptive sen or fusio framew k for Visual-Inerti l Odometry (VIO) i proposed in [41] for esti ti relativ otion. It mi i iz degrad tion from i ccurat st te e timatio by det rmini g the states th t hould be included in the estimation process. These degrading states can arise under motion characteristics that nullify\n[73]\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand camera sensors is expected given he prolif rati n of smartph ne devices as the preferred platform for developing solutions for the BVI i dividuals. Fi lly, only 14% of these solutions can be used in real-life scenarios with some degree of succ ss, 29% f the are practical but have a combination of either high cost or require from the user to arry many sensors, 24% are limit d practicali y for specific s enarios while 32% are purely xperimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range ccuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] .1 3.5 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R < 10 / [ 6] \u2716 R > 2 m N/A [ 7] / N/A [58] 4.5 N/A [59] N/A 67\u201398% [ 0] 2 < R < 12 m N/A\n[ 1 63] [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following section will pres t the olutio s fo nd i th literat re o ganized around the sensors employed. Specifically, the prese tati n is orga ized around th criteria of being camera or non-camera assisted a different sets of tec iques are require for those two cases. For each of t ese tw categories, we continue by gro ping the provide solutions base on the ty e of sens r with t most freq ent group being presented first. For both cases, the sensor fusion techniques are described in a compre e ible manner. In more detail, Section 3.1.1 c vers the papers utilizi g a form f t e camera sensor, be it a single ca era or a configuration of multiple c er s, headset devices as well as 3D camera sensors. Subsequently, papers utilizing t e IMU sensor (accelerom ter, gyrosc pe, magnetometer), the most popular sensor mong the selecte papers, are presented. For each individual paper, a sum ary inc rpor ting the solut on wi h the dva tag s and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions ith n camera sensors. Most of them incorpor te an IMU sensor while other options include ultrasonic and Lidar sensors as well s Bluet oth Low Ener y (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti ns An uncertainty-based adaptive sensor fusion framew rk for Visual-Inertial Odometry (VIO) is proposed in [41] for estim ting relative motion. It mini izes degrad tion from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camera sensors is exp cted given he proliferation of smartphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l f cenarios with some degree of success, 29% f the are practical ut have a combination f either high cost or requi e from the user to carry m ny s nsors, 24% are lim t d pract c li y for specific s enarios while 32% a e purely xperimental.\nTable . Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy 41\u201347] [48] / / / / [49] \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95 [52] 0 m < R < 9 98% [53] .1 3.5 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R < 10 / [ 6] \u2716 R > 2 m N/A [ 7] / N/A [58] 4.5 N/A [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m N/A\n[ 1 63] [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ction will present the l tio s f nd i the literature organized around the s nsors emplo ed. S ecifically, the prese tation is organized around t e criteria of b ing ca era or n-ca era assisted as different s ts of tec niques are require for those two cases. For ea h of t ese t o categories, we continue by groupi g the provide s lutio s base on the type f s nsor with t e m st freque t group be ng presented first. For both case , the sens r fusion t chniques are described in a comprehe sible manner. I ore detail, Section 3.1.1 c vers the p pers utilizing fo m of t e amera sensor, be it a single ca era or a configuration of multiple ca ras, heads t devices as well as 3D camera sensors. Subseq ently, papers utilizing the IMU senso (acceler me er, gyroscop , mag et meter), the mo t popular sens r a ong the elec ed p pers, re present d. For each individual p per, a summary incorpor ting the lution with the advantages nd disadva tages is written. On th other hand, Section 3.1.2 concerns s lutions with no camera sensors. M st f them incorporate n IMU s ns r while ther options includ ultr - sonic and Lidar se sors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi acces points.\n3.1.1. Camera-As isted Soluti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for sti ting relative otion. It minimizes degradation from inaccurate state es imatio by det rmining t e states th t should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camera sens rs is exp cted given the proliferation of smartphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of thes solutio s can be used in r al-l fe cenarios ith som degre of succ ss, 29% of the are practical ut have a combination f ei r high cost or requi e from the us r to carry many s nsors, 24% are lim t d pract c i y for specific s n rios while 32% a e purely xperiment l.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347]\n[ 8] / / / / 49 \u2716 \u2716 \u2716 0 N A N A / N/A 1 N/A N/A 5 m 5\n[ 2] 0 < < 9 98% [ 3] .1 3.5 90\u201395% [ 4] / N/A [ 5] N/A N/A 0.2 < R < 10 [56] R > 2 m N/A\n7 / [ 8] 4.5 N/A 59 N/A 67\u201398%\n[ 0] 2 c < R < 12 N/A [ 1 63]\n[64] R = 20 cm N/A [65\u201371]\n[ 2] N/A / 3\n[74] R < 6 m N/A [75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f llo ng s cti s will present the s lutions fo d in the literature organized around the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria f being camera or on-ca era assisted as different s ts of techniques ar requir for those two cases. For a h of th se t categories, we continue by grouping th provide s luti s base on the type of sensor with t s fr que t group be g presented first. For both case , the sensor fusion techniques are described in a compr he sible mann r. I ore detail, S cti 3.1.1 c vers the p pers utilizi g fo m of the camera sensor, be it a single ca era or a configuration f multipl ca ras, heads t devic s as well as 3D camera se sors. Subseq ently, papers utilizing th IMU senso (acceler meter, gyroscope, m g et me er), the mo t popul r se r among th elected papers, are presen ed. For each i dividual paper, a summary incorporating th solu with the advant ges nd disadva t ges is written. On th o her ha d, Sect on 3.1.2 concerns s lutions with no camer sensors. M st f them incorporate an IMU ens r while other options include ultra s nic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-As isted S luti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for estimating r lative otion. It minimizes degradation from in ccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\nS sors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd ca era sensors is exp cted given the proliferation of smartph n devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these soluti s can b used in r al-l fe cenarios ith som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s sors, 24% are l ted pract c i for specific s n rios while 32% a e purely xperimental.\nTable 3. Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] .1 3.5 90\u201395% [54] N/A N/A N/A N/A [55] / N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m N/A [ 7] / N/A [58] 4.5 N/A [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m N/A\n[ 1 63] [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nThe f llo ng s ctions will pres t the olutions found in the literature organized round the s nsors mplo ed. S ecifically, the prese tation is organized around the criteria of bei g ca era or on-cam ra ass sted as different s ts of techniques are required f those tw cases. For a of th se t categories, we continue by grouping the provided s luti s bas d on t e type of s sor with the s freque t group be ng presented first. For both case , the sensor fu on techniques are describ d in a comprehe sible manner. I ore detail, Secti n 3.1.1 covers the p pers utilizi g fo m of the camera sensor, be it a single cam ra or a c nfi uration f multiple am ras, heads t evices as well as 3D cam ra se or . Sub q ently, papers utilizing the IMU senso (acceler meter, gyroscope, ma n t me er), th o t opular s n or am g the elected papers, are presented. For eac in ivid al paper, a sum ry incorp rating th solution wi h the advantages and disadva t ges is ritten. On th ot er hand, Sect on 3.1.2 concerns s lutions with no camra sensors. M st f th i corporat an IMU sens r while other options include ultrasonic a Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me -As st d S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurat st e s imatio by determining t states that hould be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nS sors 2 23, 23, x FOR PEER REVIEW 9 of 29\na d ca era sensors is expected given the proliferati n of smartphon devices as the prefer ed platf m for dev op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r l-l fe cenarios with som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s sors, 24% are l ted pract ca i for specific s en rios while 32% a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r uppo t St tic Dynamic Range Accuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] .1 3.5 90\u201395% [54] N/A N/A N/A N/A [55] / N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m N/A [ 7] / N/A [58] 4.5 N/A [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m N/A\n[ 1 63] [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT f ll ng s ctions will pres t the s lutions found in the literature organized around the s nsors mplo ed. S ecifically, the pres tation is organized around the criteria of bei g ca ra or -camera ass sted as different s ts of techniques are required f those tw cases. Fo a h of th se t categories, we continue by grouping the provided s luti s bas d o the type of s sor with the s frequent group be ng presented first. For both c se , the se sor fu o techniques are d scribed in a comprehensible manner. I re detail, Secti n 3.1.1 covers the p pers utilizi g fo m of the camera sensor, be it a single a ra or a configur ti of ultiple cam ras, heads t devices as well as 3D cam ra s or . Subseq ently, pa ers utilizing th IMU senso (acceler meter, gyroscope,\na et me er), the mo t p pular se or among th elected papers, are presented. For each individ l pape , a sum ry incorp rating th solution with the advantages and disadva t ges i ritten. O th ot er han , Sect on 3.1.2 concerns s lutions with no camr s nsors. M st f th i corporat a IMU se s r while other options include ultrasonic d Lid r sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me a-As sted S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odome-\ny (VIO) is p po ed in [41] for esti ati g r lative otion. It minimizes degradation from i accurate sta e es matio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\n[74]\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand c m ra se sors is xp ct d given the rolife at of mar pho e d vices s he prefer ed platform deve opi g solutions for the BVI dividuals. Finally, ly 14% of these sol tions can be used in e l-life sc narios with some degre of uc s , 29% them are practi al but have combinat on of either high cos or req ire from the user to ar y any sensors, 24% re limit d practica y f spe ific scenarios whi e 32% are pure y xperimental.\nTable 3. Solutions and bstacle detection.\nPa er Sup ort Static Dynami Range Ac uracy [41\u201347]\n8 / A 49 0 N/A 1 N/A 5 m 95%\n5 0 9 m 98% 5 .1 3.5 90\u201395% 5 N/A\n[5 ] N/A 0.2 < R < 10 m / 6 R > 2 m 7 / N/A\n58 4.5 m [59] N/A 67\u201398% [ 0] 2 cm < R < 12 m N/A\n[ 1 63] [64] R = 20 m N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llowi g ctions will pr se t th so uti ns f nd in the litera ure i ar und the sens rs mployed. pecifically, the r sent tio is organize around the cr - teria of bei g camera r non-c era as isted diff re t s ts of tech iq es re equired for those two c ses. For ach of th s two categori , w co u by grou i provide solutions based n the type of s r with the m t frequent group be ng presented f rst. For both ca es, the se s r fusion t ch ique are de crib d i co pr h nsibl man r. I m re detail, Secti n 3.1. covers th papers til zing a form of the cam r se sor, be it a single ca ra or a configuration of multi le cameras, he dset devices as well as 3D camera sensors. Subs qu ntly, p s utilizi g the IMU sensor (ac lerom ter, gyroscop ,\nagneto eter), the most popular sens r ong th sel cted paper , ar resent d. For each individ al p per, a summ ry corpor ti g th sol tion wit th a va tages and disadvantages is writt n. O the her hand, Se ti 3.1 2 c c rns solutions wi o c mera sensors. Most of them incorporate an I U sensor while o h options include ul rasonic and Lidar se s rs as well as Blueto th Low Energy (BLE) beacon and Wi-Fi ac s points.\n3.1.1. Camera-As i ed Soluti n An uncertainty-based ada t ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is proposed in [41] for estimating relative otion. It minim zes degr dat on fr m inac urate state estimation by d ter ining the states hat should b i clude in h estimation proces . The e degr ding stat s can ar se und r motio character stics that nullify\nSensors 2023, 23, x FOR PE R REVIEW 9 of 29\nand c m ra se sors is exp ct d given th prolif t of smartpho devices as he pr - fer ed platform o deve oping solutio s f r the BVI dividuals. Finally, nly 14% of these sol tions can be u ed in e l-l f c narios wit some d gre of uc s , 29% them are practi al ut av combinat on f either high c st o req i e from the user to ar y many s nsors, 24% re lim ted pract ca y f spe ific s enarios while 32% a e pure y xperimental.\nTable 3. Solutions and bstacle detection.\nP er up ort Static Dynami Range Ac uracy [41\u201347]\n48 / A 49 0 N/A 1 N/A 5 m 95%\n5 0 9 m 98% 5 .1 3.5 90\u201395%"
        },
        {
            "heading": "5 N/A",
            "text": "[5 ] N/A 0.2 < < 10 m / 6 R > 2 m\n/ N/A 58 4.5\n[59] N/A 67\u201398% [ 0] 2 cm < R < 12 N/A\n[ 1 63] [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f llo g ctions will rese t he so uti ns fo d in the literature orga iz ar und th s ns rs mplo ed. pecifically, the r se t tion is rga ize around the cr - teria of bei g c mera r non-ca ra as isted iff re t s ts of tech iq es re required for those two cases. For a h f th s t o categ ri , we con ue by gr upin th provide solutions b s d on t e type of e s r with the m t freque t grou be ng pr sented first. For bot cas , th s s r fus on t ch ique are de crib d i co preh nsible an er. In m re et il, Secti n 3.1.1 c vers th p pers til zing fo m of the cam ra se sor, be it single ca ra or configur tion of multiple cam ras, he ds t devices as well as 3D ca era ensors. Subs q ntly, p pe s utilizi g the IMU se so (ac eler mete , gyroscop ,\nagn t eter), the o t popular sens r mong th selected papers, ar pr se ted. For each individ al p per, a umm ry corpor t g he ol tion wit th a va tages and disadva tages is wr tt . O th o her hand, Se tio 3.1 2 c c rns s luti ns wi h camera sensors. M st f them incorporate an I U sens r while oth options include ul rasonic a d Lidar se sors as well as Blu t th ow E ergy (BLE) beacons and Wi-Fi ac es points.\n3.1.1. Camera-As i d Soluti n An u ce t inty-based adapt ve sensor fus on framework f r Visual-Inertial Odome try (VIO) is pr po ed in [41] for estimat g relative tion. It minimizes degr at on fr m inac u ate state es imatio by d ter ining t e states hat should b i cluded in h estimation pro es . Th e gr ding stat s can ar se und r motio c aracter stics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sens rs is x ected giv n the proliferat n of s artph e d vices as the pr - ferre platform for v l ping s lutio s for the BVI i ividuals. Fi lly, only 14% of thes soluti ns can be us d in real-lif c narios with some deg f succ s , 29% f the are practical but have c mbinatio of ither high ost or equire fro the us r to rry m ny s sors, 24% are limited practicali y for specific scenarios while 32 are purely experiment l.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy [41\u201347]\n8 / / /A 9 0 N/A 1 N/A N/A 5 5\n0 9 98 .1 3.5 90\u201395%\nN/A 5 N/A N/A 0.2 < R < 10\n56 R > 2 m 7 / 8 4.5\n59 N/A 67\u201398% [ 0] 2 c < R < 12 m N/A\n[ 1 63] \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] [ 2] N/A /\n3 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh f l wi g s i will p se t the luti ns f u d th lit r ture organiz d r u d th s ns s mpl yed. Sp cifica ly, the pr nt t on i o ganiz arou d t crit r a f being c m ra or n -c m ra a ist d d ff r nt s t of ech q es ar quir f r ose two cases. For eac f hes t o ca eg ie , w continue by groupi g th provided\nluti s ba ed n t type f s ns r wit t mo t fr q ent group b i g pres ted first. For both cas s, the or fusion t chniques are d scribed i c mpreh n ible m nn r. In m r detail, Secti 3.1.1 covers the pap rs utilizi g form of th c mera sensor, be it single camera or a confi ur tion of ultiple amera , he dset devic s as well as 3D c m r ens rs. Sub quently, pa ers utilizi g t IMU s nsor (acc lerometer, gyroscop ,\nagn tometer), the most pul r se s r a t sel cted p p rs, are presented. For each individual p per, summary incorp rati g the solution with the advantages nd disadvantages is w itt n. On the ther ha d, Section 3.1.2 concerns s lutions with no camera ensors. Most of th m inc rporat IMU sensor while ther ptio s include ultr - s ic and Lidar se sors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mer -A sist d S lu i n A u certainty-based adaptive sen or fusio framew k for Visual-Inerti l Odometry (VIO) i proposed in [41] for esti ti relativ otion. It mi i iz degrad tion from i ccurat st te e timatio by det rmini g the states th t hould be included in the estimation process. These degrading states can arise under motion characteristics that nullify\n6 N\n[75\u201378]\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand camera sensors is expected given the proliferation of smartph ne devices as the preferred platform for developing solutions for the BVI i dividuals. Fi lly, only 14% of these solutions can be used in real-life scenarios with some degree of succ ss, 29% f them are practical but have a combination of either high cost or require from the user to arry many sensors, 24% are limit d practicali y for specific s enarios while 32% are purely xperimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range ccuracy [41\u201347]\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] .1 3.5 90\u201395% [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] c 4.5 / [59] \u2716 N/A 67\u201398% [ 0] 2 c < R < 12 m /\n[ 1 63] \u2716 \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will pres nt th s lu i f und in the lite atur org iz around the sensors employed. Specifically, th r entatio i o ganized aro th criteria of being camer or non-camer assiste a differe t sets f tech iqu s are equ r d for those two case . For ach f hese two cat gori s, we continu by grouping provide solutions based on the type of en r with the most fr q nt gro p being prese ted fir t. For both cases, the se sor fusio techniques re described in a comprehensible manner. In more detail, Section 3.1.1 covers the apers utilizi g a form of the camer sensor, be it a single ca era or a configuration of multiple ca eras, headset devices as well as 3D camera sensors. Subsequently, papers utilizi t IMU sensor (acc lerometer, gyroscope, magnetometer), the mo t p ular sensor am ng the elected papers, are presented. For each individual paper, a summary incorporating th olution with the a vantages an disadvantages is written. On the other han , Section 3.1.2 co cer s solutions ith no camera sensors. Most of them incorpor te an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti ns An uncertainty-based adaptive sensor fusion framew rk for Visual-Inertial Odometry (VIO) is proposed in [41] for estim ting relative motion. It mini izes degrad tion from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW of 29\nand camera sensors is expected given the proliferation of s artphon devices as the prefer ed platform for developing solutio s for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l f cenarios with some degree of success, 29% f them are practical ut have a combination f either high cost or requi e fro the user to c rry m ny s nsors, 24% are lim t d pract c li y for specific s enarios while 32% a e purely xperimental.\nTable . Solutions and obstacle detection.\nPaper upport Static Dynamic Range Accuracy [41\u201347]\n[ 8] / / / / [ 9] \u2716 \u2716 \u2716 \u2716 \u2716 [ 0] N/A N/A N/A N/A [ 1] N/A N/A 5 m 95 [ 2] 0 m R < 9 98% [ 3] .1 3.5 90\u201395% [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] / N/A [58] 4.5 / [59] \u2716 N/A 67\u201398% [ 0] 2 c < R < 12 m /\n[ 1 63] \u2716 \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ng s ction will present the solu i s f und in the litera ur orga iz rou the s n or mplo d. Sp cifically, th re tati n i org nized around t riteria of being ca er or n n-ca r assisted as diff re t sets f tech iqu s are requ red for those two case . For ea h of h se t o categories, we contin by groupi g provided s lutions ba ed the type f nsor with the m st frequ nt gro p b ng prese ted fir t. For both case , the se sor fusio techniques re described in a c mprehensible manner. In more detail, Section 3.1.1 covers the pers utilizing fo m of the c mera sensor, be it a single ca era or a configuratio of multiple ca ras, heads t devices as well as 3D c mera sensors. Subseq ently, papers utilizi th IMU senso (acc ler meter, gyroscop , magnet meter), the mo t p ular sensor a ng the elected papers, are presented. For e h individual p per, a summary incorporating the solution with the adva tag s an disadva tage is written. On th oth r h , Section 3.1.2 co cer s s lutions with o camera sensors. M st f them incorporate n IMU sens r while ther options include ultr - sonic and Lidar se sors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-As isted Soluti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr po ed in [41] for sti ting relative otion. It minimizes degradation from inaccurate state es imatio by det rmining t e states th t should be included in the estimation process. These degrading states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sens rs is expected given the proliferation of s artphon devices as the prefer ed platf m for develop ng solutio s for the BVI individuals. Finally, only 14% of thes solutio s can be used in r al-l fe cenarios ith som degre of succ ss, 29% of the are practical ut have a combination f ei r high cost or requi e fro the us r to carry many s nsors, 24% are lim t d pract c i y for specific s n rios while 32 a e purely xperiment l.\nTable . Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347]\n8 / / 49 0 /\n51 N/A N/A 5 m 5 52 0 < 9 m 98% 53 .1 3.5 90\u201395%\n[ 4] N/A / 5 N/A N/A 0.2 m < R < 10 m\n56 R > 2 m / 7 /\n[ 8] m 4.5 m / 59 \u2716 N/A 67\u201398%\n[ 0] 2 cm < R < 12 m / [ 1 63] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371]\n[ 2] N/A / 3\n[74] R < 6 m N/A [75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT follo ng s ctions will pres nt th soluti ns fou d i te ature organiz d rou the s n o mpl d. Sp cifically, pres tation is or n ze around the cr teria f bei g camer or on-ca era assisted as different sets f techniques are req r for thos two cas . For a h of h se t cat g ri s, we o tinu by grouping provided\nluti ns bas d o th ype f nsor with th s fr quent group be g presented fir t. For both case , the se sor fusio techniques re described i a c mprehensible mann r. In mor detail, Section 3.1.1 covers the p pers utilizing fo m of the c mera sensor, be it a single ca era or a configur tio of multiple cam ras, heads t devic s as well as 3D c mer sensors. Subseq e tly, pape s utilizi th IMU se so (acceler meter, gyroscope, magnet meter), the mo t p ul r se r among th elected papers, are presented. For e h individual paper, a summary incorporating th solution with the adva tag s n disadva t ge is written. On th oth r h , Sect on 3.1.2 concer s s lutions with o camera sensors. M st f them incorporate an IMU sens r while other options include ultras nic and Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-As isted S luti n An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for estimating r lative otion. It minimizes degradation from in ccurate sta e s imatio by determining t states that should be included in the estimation process. These degrad ng states can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is expected given the proliferation of s artph n devices as the prefer ed platf m for deve op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r al-l fe cenarios ith som degree of success, 29% of them are practical ut have a combination f ei r high cost or requi e from the user to carry many s sors, 24% are l ted pract c i for specific s n rios while 32 a e purely xperimental.\nTable . Solutions and obstacle detection.\nPaper upport St tic Dynamic Range Accuracy [41\u201347] \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 [ 0] N/A N/A N/A [ 1] / N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [54] N/A N/A / [55] /A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] \u2716 / N/A [58] 4.5 / [59] \u2716 N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT follo ng s c ions will p s t th so ti ns f und i te ature organized rou he s n o pl d. S ecifi ally, t e pres tation is or anize around the crit ria of b i g ca r r on-cam ra as sted as different ts of techniques are required f thos two cases. For a of th se t at g ri s, we co tinue by grouping the provided\nluti ns bas d o t e ype f nsor with the s frequent group be ng presented fir t. For both case , the sensor fu on techniques are described i a c mprehensible manner. In more detail, Section 3.1.1 covers the p pers utilizing fo m of the c mera sensor, be it a si gle cam ra or a confi r tio of multiple am ras, heads t devices as well as 3D c m r sen or . Sub q tly, pape s tilizing t e IMU s so (acceler meter, gyroscope, ma n t met r), the mo t pular sen r am g th el cted papers, are presented. For e h individ al pap r, a summ ry incorporating the solution with the adva tag s and disadva t ge i ritten. On th ot r h d, Sect on 3.1.2 concerns s lutions with o camera sensors. M st f th m i corporat an IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me -As st d S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odomet y (VIO) is p po ed in [41] for esti ating r lative otion. It minimizes degradation from inaccurat st e s imatio by determining t states that hould be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sensors is expected given the proliferati n of smartphon devices as the prefer ed platf m for dev op ng solutio s for the BVI individuals. Finally, only 14% of these solutio s can b used in r l-l fe cenarios with som degree of success, 29% of them are practical ut have co bination f ei r high cost or requi e from the user to carry many s sors, 24% are l ted pract ca i for specific s en rios while 32 a e purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r uppo t St tic Dynamic Range Accuracy [41\u201347] \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 [ 0] N/A N/A N/A [ 1] / N/A 5 m 95% [ 2] 0 m < R < 9 m 98% [ 3] \u2716 .1 3.5 90\u201395% [54] N/A N/A / [55] /A N/A 0.2 m < R < 10 m / [ 6] \u2716 R > 2 m / [ 7] \u2716 / N/A [58] 4.5 / [59] \u2716 N/A 67\u201398% [ 0] 2 cm < R < 12 m /\n[ 1 63] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] [ 2] N/A / [73] [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716\nT follo ng s c ions will p s he s tio s f und in the ite ature organized ar h s nso s mpl e . S ecifi ally, th pres tation is or anize around the criter a of bei g ca ra r -c mer ass sted as different s ts of techniques are required f tho t o cases. Fo a of th se t c t g ri s, we co tinue by grouping the provided\nluti s bas d on the ype of nsor with the s frequent group be ng presented first. For both case , the se sor fu on techniques are d scribed i a comprehensible manner. In m re detail, Section 3.1.1 covers the p pers utilizing fo m of the camera sensor, be it a si gle am ra or a config r tion of ultiple cam ras, heads t devices as well as 3D cam r s n r . Subseq e tly, pa e s tilizing t IMU s so (acceler meter, gyroscope,\na net met r), the mo t p pular se r among th elected papers, are presented. For ach individ al pap r, a s mm ry incorp rating the solution with the advantages and disadva t ges i ritten. On th ot er hand, Sect on 3.1.2 concerns s lutions with no camr s nsors. M st f th m i corporat a IMU sens r while other options include ultrasonic a d Lidar sensors as well as Blu tooth ow Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C me a-As sted S luti ns An uncert inty-based adaptive sensor fusion framework for Visual-Inertial Odome-\ny (VIO) is p po ed in [41] for esti ati g r lative otion. It minimizes degradation from i accurate sta e es matio by determining t states that should be included in the estimatio process. These degrad ng st tes can arise under motion c aracteristics that nullify\nSensors 2023, 23, 5411 9 of 29\nThe most preferred technique is the Kalman filter with a percentage of 27% followed by computer vision and deep learning approaches with the same percentage of 19%. The Kalman Filter, which is an algorithm used for estimation (correcting predictions), is favoured for its scalability. This Bayesian fusion method is particularly helpful for state propagation and updating basic input data. Next are solutions utilizing PDR with a percentage of 14% and Particle Filters with a percentage of 8%. The proposed solutions are multimodal, however, IMU sensors, a combination of accelerometer, gyroscope and magnetometer, are by far the most preferred (57%) followed by camera sensors (38%), Lidar sensors (16%), and, last but not least, ultrasonic sensors (14%). The prevalence of the IMU and camera sensors is expected given the proliferation of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental. The following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions\nAn uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify the readings of the inertial measurements such as when the user moves at a constant velocity. The level of uncertainty in the estimated states is measured using the expected value of the errors, which are calculated by reusing Jacobian matrices obtained from bundle adjustment. A challenge of visual odometry concerns scenarios with scene illumination or featureless surfaces. In an effort to address the challenge [42] proposes a new deep-learning method to accurately estimate ego-motion using low-cost mmWave radar technology. They introduce a mixed attention approach to fuse the mmWave pose estimates with data from other sensors such as inertial or visual sensors, in order to improve the overall accuracy and robustness of the ego-motion estimation. In order to improve the accuracy and robustness of monocular VIO in estimating the trajectory, the work in [43] proposes a framework that selectively fuses monocular images and inertial measurements. It is designed to handle real-life issues such as missing or corrupted data and poor sensor synchronization and includes two different feature selection strategies, namely deterministic soft fusion and stochastic hard fusion. These strategies are used to re-weight the concatenated inertial-visual features selectively, taking into account the current environment dynamics and the reliability of the data input. The work in [44] proposes a localization technique that utilizes Progressive Sampling Consensus (PROSAC) and combines monocular visual and inertial navigation for improv-\nSensors 2023, 23, 5411 10 of 29\ning the self-positioning of low-cost devices in unknown environments. The proposed solution for localization involves using a ROSAC mismatch culling approach that combines monocular vision and inertial navigation. The system in [45] implements an attitude estimation method based on monocular vision and inertial sensor fusion for indoor navigation. It attempts to mitigate the low accuracy in the results of attitude estimation based on vision and inertial sensors due to factors such as motion image blur, attitude angle processing algorithms, and data synchronization. The main method of attaining the high-precision attitude information of both the visual and inertial model is a multi-rate Kalman filter. When a new image is collected by the camera module, the visual attitude measurement algorithm is used to solve the zero-bias error of the new gyroscope, and then the results of inertial measurement and visual measurement are combined to update the attitude angle information. Detecting when the user is in an outdoor or indoor space is helpful. The system in [46] implements a pedestrian navigation methodology optimized for this case. To enhance the accuracy of the location identification, the navigation algorithm integrates signals from the light and magnetic sensor as well as from the global navigation satellite system (GNSS). In more detail, the information of the satellite and magnetic sensors are introduced into the extended Kalman filter data fusion processing to reduce the positioning error. The fusion of these sensor readings is done with the help of a decision table constructed to detect whether the device is in an outdoor or indoor space. The work in [47] proposes a solution that focuses on navigation, localization and orientation of visually impaired individuals in outdoor and indoor spaces using a nonGPS approach. The main approach of the system is to jointly utilize dead-reckoning and computer vision techniques on a smartphone-centric tracking system. Input from the gyroscope and the accelerometer are fused with the camera module-generated information to create the heading estimate. The latter is computed by a prediction-correction filter. Few efforts cover both indoor and outdoor navigation. A system describing an indoor and outdoor tracking and navigation system that uses pedestrian dead reckoning and computer vision algorithms for indoor environments and GPS for the outdoors is described in [48]. The system takes advantage of coloured tapes as reference signals to turn the smartphone camera into an additional sensor. By processing images from the camera, the system can estimate user heading and velocity, and detect landmarks to reset tracking errors. The accuracy of the system is improved by combining measurements from the smartphone\u2019s IMU and computer vision techniques via the use of an Extended Kalman Filter. Videos and inertial sensor fusion can be leveraged in systems providing real-time traffic light detection systems for BVI pedestrian navigation [49]. The inertial sensors are used to estimate the orientation, by fusing gyroscope, accelerometer and magnetic field sensor data, and motion of the camera, which is used to correct for the distortion in the image caused by the camera\u2019s tilt and to calculate the position of the traffic light in the image. The position information is then fed into a Convolutional Neural Network (CNN) trained to recognize traffic lights, enabling the algorithm to detect traffic lights with greater accuracy. The paper also proposes using the velocity and acceleration estimates from the inertial sensors to predict the position of the traffic light in subsequent frames. This reduces the computational load required for real-time detection and the spurious detection of traffic lights. The work in [50] proposes the design and implementation of a personal assistant robot for BVI individuals using sensor fusion technology for indoor navigation to provide accurate robot localization and navigation. The sensor fusion is responsible for fusing data from the camera and the lidar system to provide a map and the depth of the area. This is especially helpful in scenarios where the depth information obtained from the camera is degraded due to the configuration of the indoor space such as uniform wall colours. Combining both the Cloud and the Edge can help in providing robust solutions for indoor navigation by raising situational awareness and providing navigational aid for BVI individuals. The solution in [51] fuses data generated from the smartphone\u2019s LiDAR and\nSensors 2023, 23, 5411 11 of 29\ncamera sensor as well as applying machine learning algorithms that are partly performed in the cloud and partly in the edge. The system\u2019s main functionality is to provide a rich 3-D description of the user\u2019s front-facing navigational path by enhancing the cameracaptured image of a scene with LiDAR-generated distance information and directional information computed by the device. Additionally, the system combines sensor data fusion and geometric formulas to generate step-by-step walking instructions for the user to reach the desired destination. Another system [52] utilizing sensor fusion to address both indoor and outdoor space navigation consists of a navigation assistant and a wearable device with a camera. It combines computer vision and sensor-based technology, in a cost-efficient way, to detect multiple objects and enhance the accuracy of its collision avoidance system. The employed algorithm performing the fusion utilizes a fuzzy controller with 18 fuzzy rules that take as input the user position from the sensors and the distance of the detected obstacle via employing computer vision techniques on the camera module. As a result, it returns audio instructions as to what the user should do. The work in [53] proposes a method that involves using two RealSense R200 (Intel, Santa Clara, CA, USA) devices to create a colour stereo vision system with a short baseline for improved depth estimation. As the depth map produced by the R200 is not accurate at short distances, a stereo matching algorithm that uses the colour stereo image pair to generate a more precise depth map was developed. This new colour stereo depth map is then fused with the original depth map created by the R200, which is produced through a stereo-matching process using the infrared image pair. The fusion algorithm works on each individual pixel. A voting window is set up around the pixel in question, and any other pixel within that window will vote based on similarity. The depth value that receives the most votes becomes the final depth value for the pixel. Several systems employ an RGB-D camera as a central component. In [54] a system equipped with an RGB-D camera and ultrasonic sensor is used to navigate the BVI individuals. The proposed fusion method combines the distance measurements from the ultrasonic sensor, which correspond to the ground plane, with the depth measurements from the RGB-D camera by mapping the ultrasonic sensor\u2019s distance readings onto the RGB-D camera\u2019s point cloud. The fused data is processed using an extended Kalman filter (EKF) to estimate the user\u2019s position and orientation. The EKF incorporates the sensor measurements, the user\u2019s motion model, and an error model to estimate the user\u2019s position and orientation over time. Another system employing sensor fusion for assisting the navigation of the BVI individuals comprises an RGB-D camera with a millimeter wave radar [55]. It fuses data from those tow sensors to detect obstacles in the user\u2019s navigational path. The system acquires data on the velocity of multiple objects via a frequency-modulated continuous wave millimeter wave radar and it performs contour extraction and applies MeanShift algorithms on the output of the RGB-D sensor to verify the obstacle\u2019s position. The fusion of data from the millimeter wave radar and RGB-D sensor is achieved via the application of particle filters to obtain an accurate state estimation. Another benefit of this approach is its suitability for scenarios that have multivariate, non-linear behaviour and noise that follows a non-Gaussian noise. Following this approach successfully bypasses the complexity of applying the Joint Integrated Probabilistic Data Association (JIPDA) algorithm that is able to track and label multiple targets prior to the step of applying the Kalman Filter. The work in [56] is proposing a multi-sensor fusion system for improving indoor mobility of the visually impaired in corridor environments. The multi-sensor system employs floor-plan digitization, semantic SLAM, environmental perception, obstacle avoidance and human-machine interaction modules. The system improves its results by fusing environmental data and depth information. The environmental perception data is produced by combining RGB-D and Lidar sensor data. A system for exploring and navigating areas with BVI individuals that is both safe and adaptable to changing environments and runs in real-time is presented in [57]. The system\nSensors 2023, 23, 5411 12 of 29\nleverages the computational capabilities of the iPhone and ARKit for obstacle detection, using a combination of 2D object detection, semantic segmentation, 2D object tracking, 3D ARKit point cloud, and depth maps generated by LiDAR sensors in recent iPhone versions. In particular, the system fuses the generated input from the camera and LiDAR sensor of the iPhone. The method also uses the plane detection capability of ARKit to detect objects with planar surfaces, this covers large objects that cannot be detected by the 2D object detector. An indoor obstacle detector in combination with a navigation module to assist in real-time BVI individuals is presented in [58]. In particular, the navigational work as well as the obstacle detection portion was carried out by the combined functioning of the PIR Sensor and the Ultrasonic Sensor which are connected to the Arduino Nano processor. Detecting and identifying obstacles along the navigational path of a BVI individual is important. The system in [59] fuses the input of ultrasonic, vision and sonar sensors. The first two are used for obstacle detection and identification, respectively, while the last one speeds up the visual data processing as it allows the selection of only the regions which have an obstacle. The main approach concerning fusion is to use the Extended Kalman filter to fuse input from homogeneous sensors and rule-based fusion for data coming from heterogeneous sensors. Furthermore, camera rotations, which occur due to the body movements, are corrected by fusing the inertial measurement unit data connected to the camera.\n3.1.2. Non-Camera Assisted Solutions\nThe design and development of a wearable assistive device integrating a fuzzy decision support system, for the navigation of BVI individuals, is presented in [60]. The system consists of an acquisition system that takes input readings from the IMU and Mini-LIDAR sensor. By fusing the inputs from the IMU sensors using an Extended Kalman Filter (EKF) method data on attitude and head direction is collected while the fusion of velocity and depth data happens at the fuzzy controller to assess the level of risk on a given path during navigation. The solution in [61] proposes an indoor positioning framework for BVI individuals using the Internet of Things (IoT). The system leverages the smartphone\u2019s inertial sensors and Bluetooth-powered beacons in order to provide turn-by-turn instructions to a pedestrian to navigate between two points even in cases where external sensing is absent as in the case of a big hallway. The solution calculates the user\u2019s threshold and step length to determine the travelled distance while these values are regularly updated for a user\u2019s profile and stored in the cloud. Subsequently, the framework fuses the data generated from the travelled distance, heading and turns to estimate the current position of a user with the absolute location as it is inferred from the Bluetooth beacons. Another approach to supporting an indoor position system is with the help of Wi-Fi RSSI trilateration and INS sensor system simulation. The system [62] employs a Kalman Filter to fuse the RSSI signals with the INS data to have more accurate positioning. The IMU acceleration is integrated to determine the INS position. Reliable and accurate indoor orientation and localization can be achieved by the combination of IMU and magnetic sensors. An improved KF (Kalman filter) is designed [63] to combine data from those two sensors to achieve precise position and orientation estimation. The magnetic sensors are utilized to counterbalance the accumulated error and drift of the inertial sensors, while the inertial sensors are used to rectify errors in the magnetic fields that are related to orientation. Additionally, a parameter derived from the magnetic tensor is used to facilitate indoor obstacle avoidance and object/destination approach, as well as to indicate the reliability of the yaw angle estimated from the magnetic measurement. To address the inaccuracy of the GNS navigation systems, a system detecting obstacles and estimating the user\u2019s distance from them utilizing infrared sensors is proposed in [64]. It consists of a base station, equipped with a Bluetooth module and a microcontroller, receiving sensor data from a wearable device, equipped with IMU and infrared sensors,\nSensors 2023, 23, 5411 13 of 29\nand performing sensor fusion to estimate the user\u2019s position and orientation. The results are then transmitted back to the wearable device translated into haptic feedback so as to guide the user through the environment. The work in [65] proposes a deep learning-based solution that aims to achieve robust inertial motion tracking in various environments. It incorporates additional IMU sensors, like smart earbuds, that are less likely to be affected by motion noise than the smartphone\u2019s IMU measurements. Specifically, the fusion layer is a fully connected layer in the CNN that takes as input the features from the accelerometer and gyroscope sensors in both the smart earbuds and the smartphone. In general, the proposed sensor fusion model aims to synthesize the estimations from multiple sensors and automatically adjust their fusion weights according to their motion noise levels, in order to mitigate the impacts of the corrupted sensor. The fusion model assesses the reliability of the sensors based on a customized reliability LSTM (r-LSTM) and fuses the translation and angle increment features using two attention models. The work in [66] proposes a solution for achieving localization accurately, continuously and in real-time utilizing a probabilistic localization algorithm running on a smartphone device. The algorithm takes advantage of the inertial sensors and the Received Signal Strength (RSS) from BLE beacons and tries to resolve many of the issues existing localization approaches face when deploying them in large and complex environments, like shopping malls and hospitals. The probabilistic framework uses the Particle Filter for state estimation. A sensor data acquisition system employing multimodal sensor fusion for recognizing human activity using deep learning is presented in [67]. In addition to accelerometer data, gyroscope and magnetometer sensor data are collected and considered for multimodal sensor fusion to improve activity recognition performance. A classifier-level sensor fusion technique using a two-level ensemble model is adopted to combine class-probabilities from multiple sensor modalities, leading to an improvement in classification performance. The accuracy of each sensor on different activity types is analyzed, and custom weights are elaborated for multimodal sensor fusion, taking into account the characteristics of individual activities. Another work [68] presents the design of a tactile display for BVI individuals to access virtual diagrams. The system utilizes a compact robot base with Omni-wheels that enables seamless and unrestricted movements in a two-dimensional space. The proposed sensor fusion approach uses a single optical mouse sensor and a commercially available IMU, without the need for placement away from the center of the robot base. The displacement data from the mouse sensor is combined with the orientation angle obtained from the IMU to determine the precise x and y location and orientation of the device. A comprehensive deep learning approach to acquire meaningful insights from a variety of sensory data types and enhance the accuracy of classification and recognition tasks is presented in [69]. This framework merges unique and complementary information from multiple sensors and prioritizes high-quality data while also considering the relationships between different sensors. It utilizes both weighted-combination features and cross-sensor features to accomplish this goal. The work in [70] proposes a solution that applies deep learning thermal-inertial odometry with visual hallucination. In particular, the proposed approach utilizes fusion to selectively combine features extracted from three distinct modalities, including thermal, hallucination, and inertial features to improve pose regression. By incorporating selective fusion into the combined features, the authors observe a consistent reduction in ATE compared to the network without selective fusion. This highlights the importance of selective fusion in achieving accurate results as each feature modality has its own inherent noise and the hallucination network may produce incorrect visual features. The system in [71] supports indoor localization targeting BVI individuals by fusing a metaheuristic algorithm with a Neural Network using energy-efficient wireless sensor networks. To optimize the performance of the Artificial Neural Network (ANN) and improve localization accuracy, the authors integrated the ANN with the following six\nSensors 2023, 23, 5411 14 of 29\ndifferent metaheuristic algorithms: the backtracking search algorithm (BSA), the crow search algorithm (CSA), the gravitational search algorithm (GSA), slime mould algorithm (SMA), the particle swarm optimization (PSO), and the multiverse optimizer-ANN (MVO). Each algorithm was applied independently to determine the optimal number of neurons and learning rate for the ANN. As a result of this fusion, the authors were able to enhance the performance of the ANN and achieve a reduction in localization errors. The design and implementation of an electronic aid for BVI individuals are presented in [72]. The system complements the white cane and uses a range of ultrasonic sensors, among others, in order to detect changes in motion and potential obstacles in the BVI individuals\u2019 navigational path. In more detail, the system uses 6 ultrasonic sensors installed on a boot worn by the user and fuses utilizing a set of derived formulas to detect the events of ascending staircase, floor-level obstacles, knee-level obstacles and knee-level forward slanting obstacles. Sensors available in commodity smartphones can be used to provide accurate and robust floor localization [73]. This is done by fusing Wi-Fi and barometer sensors in a hybrid probabilistic framework, allowing for plug-n-play floor localization without requiring prior calibration or extra geo-information that is inherent in fingerprinting. The latter is achieved by automatically crowd-sourcing the construction of histograms reflecting the probable locations of installed Wi-Fi access points (APs) in building floors, leveraging pressure readings. Depth mapping with light or radio frequency for reliable indoor space positioning and multiple obstacle distance information have limitations due to the noise level, calculation complexity, reaction time and many others. To address these, the system in [74] consisting of a device using a single ultrasound source and two to three receivers attached to a headset employes ultrasonic sensor fusion to find obstacles. The results are then transmitted to the user via audio feedback. A self-attention deep learning framework for leveraging the heterogeneous sensors of low-end IoT devices to improve the accuracy, granularity and amount of information is presented in [75]. The framework automatically balances the contributions of multiple sensor inputs over time by exploiting their sensing qualities. Firstly, it employs a selfattention mechanism to learn correlations between sensors, without the need for additional supervision, to determine the sensing qualities and adjust the model concentrations for multiple sensors over time. Secondly, instead of directly learning the sensing qualities and contributions, the framework generates residual concentrations that deviate from equal contributions, which helps to improve the stability of the training process. Indoor navigation systems based on smartphones frequently utilize PDR coupled with an external source to correct the accumulated error. The works in [76,77] follow the abovementioned approach by combining PDR with BLE beacons and UWB inferred wireless position respectively. Both are fusing the inputs via an improved Particle Filter. The former approach constrains the movement of particles using a floor-map and updates the particle weights based on proximity to BLE beacons. This real-time localization method yielded promising results, without the need for a site survey when compared to fingerprinting localization. The latter approach involves a fusion filtering process that corrects the PDRcalculated positioning trajectory using the UWB positioning. This improves the accuracy and stability of the fusion positioning. A more simplified approach to improve the accuracy of indoor pedestrian localization has been adopted from [78] where the system takes input from inertial sensors, a light sensor, a smartphone-integrated Bluetooth-sensor and a digital map representation of the indoor space. Position information from the light sensor and Bluetooth is used to modify the step length and heading information, the user\u2019s step length is modified by the distance between two ceiling-mounted lamps while the heading direction can be reset by the known planning route from the map. This information is leveraged to reduce the accumulated error as a result of the PDR method. A unified fusion framework for indoor navigation addressing is presented in [79]. It can process various types of inputs, including Wi-Fi, Bluetooth, visible light, and images\nSensors 2023, 23, 5411 15 of 29\nwith geomagnetic sequences or Wi-Fi fingerprints. The framework uses convolutional or recurrent networks to extract initial features from the inputs while additional fully connected layers are inserted to map the features to a common space for effective fusion. Ensemble learning is used to reduce the impact of noisy signals on location and orientation estimations. Multiple models are trained using the training data, and their outputs are combined for fine-grained estimations. Mixture density networks are also incorporated to generalize the models with a mixture of distributions. Finally, the work in [80] presents an indoor space navigation solution based on a wearable device providing BVI individuals audio assistance while traversing a navigational route equipped with markers. The sensor fusion approach utilizes visual and radio frequency markers to create a unique and consistent representation of indoor space. A Kalman filter is used to fuse the two data groups coming from the process of fingerprinting the internal space and from the visual markers. An LSTM network predicts the results of each sensor modality including an accelerometer, gyroscope and magnetometer which are used as an indication of the classification of probabilities of each activity class and are accepted as meta-features.\n3.2. Commercial Review\u2014Applications\nThe search for the commercial applications according to the search criteria defined in Section 2 resulted in the following applications:\n\u2022 Blindsquare \u2022 Lazarillo \u2022 Ariadne GPS \u2022 Nav by ViaOpta \u2022 Seeing Assistant Move\nBlindSquare [31] is a mobile application designed for blind, deafblind and partially sighted offering location-based information and navigation in a user-friendly way. It covers both indoor and outdoor navigation. The user selects a destination via a screen reader-friendly interface. Its support of GPS provides users with accurate and reliable information about their environment, such as nearby landmarks, road intersections, and POIs such as shops, public spaces, churches and many others. With the help of BLE beacons, BlindSquare provides accurate guidance information for indoor spaces. Furthermore, it offers the capability to personalize the application with information filters preventing information overload and, thus, enhancing the user experience. The information generated by the application is emitted via voiceover. Besides supporting cities, over the years BlindSquare has extended its coverage to other spaces including parks. Despite its proven robustness, its reliance on external services, like Foursquare and OpenStreetMap among others, can potentially deteriorate the provided quality of service when these are not reachable. Moreover, BlindSquare being a paid app available only on iPhone devices may be a barrier for some users, particularly those on a limited budget. In conclusion, despite some limitations, BlindSquare is a highly effective and valuable resource for individuals with visual impairments. Its numerous benefits, such as accessibility, customization options, and accurate location data, as well as its proven robustness over the course of years, outweigh any potential drawbacks, thus constituting the most preferable application by the BVI individuals. In the near future, BlindSquare intends to provide a set of new features and improvements [81]. These include enhancements to intersection-related information such as intersection clusters, travel direction, whether the intersection is surface or is an over/underpass, and other tips to validate travel. Another upcoming feature is object detection, especially, the ones which could cause an injury to the user. These objects will include stairways, picnic tables, benches, walks/sidewalks, bollards, gates and many others. Lazarillo [28] is similarly a mobile application designed to assist BVI individuals in navigating both indoor and outdoor spaces. It provides audio instructions to guide users through physical spaces such as buildings or city streets. The app uses GPS technology\nSensors 2023, 23, 5411 16 of 29\nto determine the user\u2019s location and then delivers step-by-step directions through the user\u2019s smartphone speakers or a connected headset. Via the use of BLE beacons and custom-made digital maps, Lazarillo can provide indoor navigation as well. The app is customizable, allowing users to adjust the volume and speed of the instructions, select a preferred language, and the types of alerts they receive. Moreover, the app has a userfriendly interface, and it is easy to use, even for people who are not technically proficient. Last but not least, Lazarillo is available for both Android and iOS devices. Overall, Lazarillo is usually preferred by the BVI individuals who own Android devices as highlighted in the interviews. Ariadne GPS [27] is yet another mobile application designed to assist BVI individuals in navigating outdoor spaces. It provides audio instructions and feedback to help users navigate unfamiliar environments, locate POIs, and travel from one location to another. The app uses GPS technology to determine the user\u2019s location while the guidance instructions are delivered through the user\u2019s smartphone speakers or a connected headset. Ariadne GPS can be customized to meet the specific needs of individual users, including adjusting the volume and speed of the instructions, selecting a preferred language, and customizing the types of alerts they receive. In effect, Ariadne GPS has three primary functions: (1) Situational awareness\u2014The application can provide information about the user\u2019s location at any given time or automatically at predetermined intervals. The amount and detail of information can be either incremental for the automatic case or a full description when the user makes a request, (2) Favorites\u2014Ariadne GPS allows the user to save favourite points, including bus stops, train stations, shops, and house doors, and receive alerts when approaching them, (3) Map exploration\u2014The application\u2019s map exploration allows users to touch with their finger any location on the map displayed such as a city, street, or country, depending on the screen\u2019s zoom level, and the application will announce the name of that object. Nav by ViaOpta [29] is designed to assist BVI individuals in navigating outdoor spaces and is part of the larger ViaOpta suite of mobile applications and services developed by Novartis, a multinational pharmaceutical company. The application provides voice-guided turn-by-turn directions, haptic feedback, and real-time location tracking to help users navigate safely and efficiently. The application\u2019s user interface is designed to be simple and easy to use, with large buttons and text that are easy to read. It also offers features such as route planning, POIs, and the ability to save favourite locations. Furthermore, it informs users of intersections in close proximity along with their distances. It, also, enables the users to insert waypoints on routes so as to be closer to their liking. Nav by ViaOpta is generally fast and responsive, with minimal lag or delay in providing turn-by-turn directions and other information. The application also uses encryption to protect user data in transit and on its utilized storage service. Finally, the application is compatible with both iOS and Android mobile devices and is available in several languages, including English, Spanish, German, and French. Last but not least, Seeing Assistant Move [30] is another mobile application with the purpose of enabling turn-by-turn outdoor navigation for BVI individuals. This application is particularly easy and intuitive to use due to its support of voice commands, allowing one to access its functions immediately. The app makes use of GPS technology that allows it to track the user\u2019s movements in real-time and provides information on nearby POIs. Users can either determine the route to their destination by inserting points or leave the application to automatically determine one. Furthermore, it allows importing points of routes stored on databases supported by other systems. The application provides a route monitoring feature in order to easily return to user-specified points as well as a simulation of locations so as to familiarize with the area prior to starting the navigation. Moreover, the app also has a magnification function that allows users to zoom in on specific areas of the screen for a clearer view of the details. In terms of accessibility, the app is designed to be user-friendly for people with visual impairments or blindness, featuring a straightforward interface with large, readable buttons and text. Finally, the app has received\nSensors 2023, 23, 5411 17 of 29\nawards and recommendations from the Polish Blind Association for its functionality, it is compatible with both iOS and Android mobile devices and is available for free download from both stores.\n3.2.1. Comparative Analysis\u2014Features\nAll these applications support several features. Some of these are found on all of them to an extent, while others are exclusive to certain applications. Below, the comparative analysis of the supported features follows. Table 4 presents the total number of features that are supported by the set of selected applications and marks those that are supported per application.\nTable 4. Commercial applications and features.\nBlindsquare Lazarillo Nav by ViaOpta Ariadne GPS Seeing Assistant Move\nPlatform iOS Android & iOS Android & iOS iOS Android & iOS\nIndoor navigation\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is ex ected give the proliferation of smartp one devices as the preferr d platform for devel ping solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of them are practical bu have a combination of either high cos or require from the user to carry many sensors, 24% are limited racticality for spec fic scen ios while 32% are purely experimental.\nl 3. Solutions and obstacle detection.\nPap r Support St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A [49] \u2716 \u2716 \u2716 [50] N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe following c ions will pre ent the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteri f bei g camer or non-camera assi ted as different ets of techniques are required for those tw cases. For e ch of these tw categories, we continue by grouping the provided solutions based on the type of senso with he most frequent group being presented first. For both ca es, the sensor fusion techniques are described in a comprehensible manner. In more detail, Sec ion 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multipl camera , headset devices as well as 3D cam ra sensors. Sub equently, papers utilizing the IMU s nsor (accelerometer, gyroscope, magnetometer), th m st popular sensor among the selected papers, are presented. For e ch individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the ther hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar ensors as well as Bluetooth Low Energy (BLE) be ons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncerta nty-ba ed adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det rmining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected gi en the proliferation of smartphone devices as th preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some deg ee of success, 29% of them are practical but have a combin tio of eith r high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found i the literature organized around the sensors employed. Specifically, the pres nt ti n is organized around the criteria of being cam ra or non-camera assisted as different sets of techniqu s are required f r those tw cas s. For ach of th se two categories, w continue by grouping the p ovided\nlution based on the typ of sensor with the most fr que t g oup being pres nted first. For b th cases, the sensor fusi n techniques a e described in a compr hensible manner. I mo e detail, Section 3.1.1 c vers the pap rs utilizing a form of the camera sensor, be it a ingle camera or a configuration of multiple cameras, h ads t devices as well as 3D camera sensors. Subsequently, pap s utilizing the IMU sensor (accel rometer, gyroscope, m gneto eter), the most popular sensor among the selected p p rs, are presente . For ach individual p per, a summary incorporating the solution with the advantages and disadvantages is writte . On the other hand, Secti 3.1.2 concerns solutions with no camera ensors. Most f them incorporate an IMU sensor while oth r options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncert inty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat stim tio by d termining the states that s o ld be in luded in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is xpected given the proliferation of sm rtphone devices as the preferred platform for d veloping solutions for th BVI individuals. Finally, only 14% of these solutions can b used in real-life sc nari s with some degree of success, 29% of them are practical but h ve a comb nati n of eith r high cost or requir f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy amic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions fo nd in the literature organized arou d th s nsors mployed. Specifically, the presentation is organized around the crit ia of b ing camera or non-camera assisted as ifferent sets of techniques are required for t se two cases. F r each f these two catego ies, we co tinue by grouping the provided solutions based on the type of s nsor with th mo t frequent group being presented first. For b th cases, the s nsor fu ion t chniques are described in a comprehensible manner. In more detail, Section 3.1.1 overs the papers u ilizing a form of the camera sensor, be it a single cam ra a onfiguration of multi le cameras, headset devices as well as 3D camera sensor . Subseque tly, papers utilizing th IMU sor (accelerometer, gyroscope, magnetometer), the most p pular sensor amo g the s lect d papers, are presented. For each individu l paper, a ummary i o p rating the soluti n with the advantages and disadvantages is w itte . O the ot er hand, Section 3.1.2 concer s solutions with no camer sensor . Most f them inco porate an IMU ensor while other options include ultrasonic and Lid r sensors as well as Bluetooth Low Energy (BLE) bea ons and Wi-Fi access points.\n3.1.1. Camera-Assisted S lutions An uncertai ty-based adaptive s ns r fusion framew rk f r Visual-Inertial Odometry (VIO) is proposed i [41] for stimati g r lat ve motio . It minimizes degradation from inaccurate t t stimation by determi ing the states that should b included in the estimation process. These degrading states can arise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is exp cted given the prolife ation of smartphone devices as the preferred platform f r dev lop g solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life sc narios with some degree of success, 29% of them are pract c l bu hav a combination of i h r high cost or equire from the user to carry many sen ors, 24% are limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh followi g sections wi l prese t the solutions found i the literature organized aroun t e sen or empl ye . Specifically, p esentation is organized around the crit r a of being c mera or n n-camera assisted as different sets of techniques are required f those two cases. F r ach of these two categories, we continue by grouping the provided solutions ba ed on the ype of s sor with he most frequent group being presented first. For both cases, the sen o fusion techniques ar described in a comprehensible manner. I more det il, S cti n 3.1.1 covers the papers utilizing a form of the camera sensor, be it a sin or a configuration f multipl cameras, headset devices as well as 3D cam ra sen ors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\ng et me er), th mo t popular s nsor mong the sel cted papers, are presented. For ach ind vidual paper, a s mmary incorpora ing the solu ion with the advantages and disa vantages is writt n. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorp rate an IMU sensor while other options include ultrasonic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted S l tions An unce tainty-based adaptiv senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from accurate state estimation by determining the states that should be included in the estimation proce s. These degradi g sta es can arise under motion characteristics that nullify\nOutdoor navigation\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected give the proliferation of smart h ne devices as the referred platform for developing soluti ns for the BVI individuals. Fi ally, only 14% of these solutions can be used in real-life scenari s with some degree f success, 29% of the are practical but have a combination of either high co t r r quire from the user to carry many sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / / / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] \u2716 N/A / [58] 2 cm < R < 4.5 m / [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the s lutions found in t e literature organized around the sensors employed. Specifically, the prese tation is organized ar und the criteria of being camera or non-camera assisted as different sets of techniques re required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sens r fusion techniques are described i a c mprehensible ma ner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, he dset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyr scope, magnetometer), the most popular sensor among the selected p pers, are presented. F r each individual paper, a summary incorporating the solution with the advantages a d disadvantages is written. On the other hand, Section 3.1.2 concerns soluti ns ith o camera sensors. Most of them i c rporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can arise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand camer sensors is ex ected give the proliferation of smartp one devices as t e referr d platform for devel ping soluti ns f r the BVI individuals. Finally, only 14% of these soluti ns ca be used in real-life scenarios with some degree of success, 29% of the are pr ctic l bu have a combinati of either high cos or require fr t user to carry many sensors, 24% are limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 9 \u201395% [54] / / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] \u2716 / [58] 2 cm < R < 4.5 m N/A [59] \u2716 / 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe following c ions will pre e t the s luti ns f und in the literature rganized around the sensors employe . Specifically, the presentation is organized around the criteri f bei g camer or non-camer ssi ted s ifferent ets of techniques are required for those tw cases. For e ch of t ese tw categories, we continue by rouping the provided solutions based on the type f senso with he ost frequent group being presented first. For both ca es, the sensor fusion techniques are described in a comprehensible ma ner. In more detail, Sec ion 3.1.1 covers the papers utilizing a form f the camer sensor, be it a single camer or a configuration of multipl camer , headset evices s well as 3D cam r sensors. Sub equently, papers utilizing the IMU s nsor (accelerometer, gyroscope, magnetometer), th m st p pular sensor a ng the selected papers, are prese te . For e ch individual paper, summary incor orating the soluti n with the advantages and disadvantages is written. On the t er hand, Sectio 3.1.2 concer s soluti ns with no camera sensors. Most of them incorporate an IMU se s r while ther optio s include ltrasonic a d Lidar ensors as well as Bluetooth Low Energy (BLE) be ns a d Wi-Fi access points.\n3.1.1. Camer -Assist d Soluti ns An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sens rs is expect d given th p olif rati of smar phon devices as the preferred platform for developing so uti s f r the BVI i divi uals. Finally, only 14% of these sol tions can be used in re l-life sc arios with som degree of succ ss, 29% of them are practical but hav a combination f eithe igh cost or req ire f o the user to carry many sensors, 24% r limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A N/A N/A [51] / N/A 5 m 95% [52] 0 m < R 9 m 98% [53] \u2716 0.1 m < R 3.5 m 90\u201395% [54] / N/A N/A N/A [55] / N/A 0.2 m < R 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe f l owi g sec ion will pr se t the solutions found i the literature organized ar und t e sensors employ d. Sp cifically, the pres nt ti n is organ zed around the criteria of bei g camera or n n-cam a as isted as differe t sets of techniques are required for thos two ca s. F r each of thes two cat gories, we conti ue by grouping the provided solutions ba d n the typ f sens r with t e most frequ nt group being presented first. For b th cases, the sensor fusi n techn ques are described in a co prehensible man er. In more detail, Section 3.1.1 cov rs the pap r utilizing a form f the camera sensor, be it single camera r a configur tion of multiple ca ras, he dset devices as well as 3D ca era sensors. Sub que tly, papers utilizi g the IMU s nsor (ac elerometer, gyroscope, magnetometer), the most popular nso among the selected papers, are presented. For each indivi al p pe , a summary incorpor ti g solution with the a v ntages and d sa v ntag s is written. O the other ha d, Secti 3.1.2 c cer s solutions with n camera s nsors. M st f them incorpo ate IMU sensor while other opti ns i clude ultrasonic nd Li ar s nsors as well as Bluetooth Low E ergy ( LE) beacons a d Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s ates an arise under motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 29\nand cam ra se sors is xpec ed giv the proliferati of sm rtphone devices as the preferre platform for developing s lutions f r the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-life c narios with some degree of success, 29% of them are pract cal but ave a co b nati n of eithe high c st or req ire f om the user to carry m n sensors, 24% re limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dy amic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A N/A N/A [51] / N/A m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / N/A N/A N/A [55] / N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f lowing secti ns will prese t the solutio s fo d in the literature organized ar und t e sensors employed. Specifically, the presentation is organized around the crit ia of bei g am ra or n n-cam ra a sisted as differe t sets of techniques are required for those tw cases. F r each f these two categ ries, we conti ue by grouping the provided sol ions based on th type of sens r with th most frequent group being presented first. For b th cas , t sensor fusi n techniques re described in a co prehensible manner. In m r detail, Sectio 3.1.1 covers th papers u ilizing a form of the camera sensor, b it a single camera or configuration of multiple cameras, he dset devices as well as 3D camera sensors. Subseq e tly, p ers utilizing th IMU sensor (accelerometer, gyroscope, magnet meter), the most ular s nsor am ng the s lect d papers, re presented. For ea h individ l p per, summ ry incorp rati g the solution with the a vantages and dis dvantage is w itte . O the other hand, Section 3.1.2 c cer s solutions with no camera sens rs. M st f th m inc r orate an IMU sensor while ot er ptions include ultrasonic and Lid r sensors as w ll as Bluetooth L w Energy (BLE) bea ons nd Wi-Fi access points.\n3.1.1. Camera-Assisted Solution An ncertai ty-based adaptive s ns r fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed i [41] for stimating relat ve motion. It minimizes degradation from inaccurat sti ati by determining the states th t should b included in the estimati process. The e degr din states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is expected given the prolife ati of smartphone devices as the preferred pl tform f r e elop g olutions for the BVI i dividuals. Finally, only 14% of these sol tions n be us in r l-lif sc narios with some degree of success, 29% of them are pract c l bu hav a combin tion of ei her high cost or eq ire from the user to carry many sen rs, 24 re limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa e Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716\n[48] / / N/A N/A [49] \u2716 \u2716 [50] / / N/A N/A [51] / N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A N/A [55] / / 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe f ll wing s ctions wi l prese t the solutions found i the literature organized ar und the sensors empl y . Specifically, p esentation is organized around the cri-\nr of b i g c mer or non-camera assisted as differe t sets of techniques are required f t ose tw cases. For ach of these two categories, we conti ue by grouping the provided s lutions ba d on the ype of s s r with he most frequent group being presented first. For both cases, the s n o fusio tec niques ar described in a co prehensible manner. I more detail, Secti n 3.1.1 c vers the papers utilizing a form of the camera sensor, be it sin or a configuration of multipl cameras, he dset devices as well as 3D cam ra se sors. Subsequently, papers utilizi g the IMU sensor (accelerometer, gyroscope,\nagn t e er), th mos po ular s nsor among the sel cted papers, are presented. For ach ind vidual p per, sum ary incorpora i g the solu ion with the a vantages and disa va tages s written. O the other hand, Sectio 3.1.2 c cerns solutions with no camera se sors. M st f them incorp r te an IMU sensor while other options include ultrasonic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es ating relat v motion. I minimizes degradation from acc rate state estim tion by determining the states that should be included in the estimation proce s. The e degr d g sta es can arise under motion characteristics that nullify\nReal-time update\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is p cted giv n t prolif ration of smartphone devices a th preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of the are practical but have a combination of either high cost or require from the user to carry many\nsensors, 24% are limited practicality for specific scenarios while 32% are purely experi-\nmental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / / N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is ex ect d give the proliferation of smartp one devices as the preferr d platform for devel ping solutions for the BVI individuals. Fin lly, only 14% of these solutions can be used in real-life scenarios with s me degree of success, 29% of the are\nractical bu have a combination of either high cos or require from the user to carry many\nsensors, 24% are li ited racticality for spec fic scen ios while 32% are purely experi-\nmental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 / [58] 2 cm < R < 4.5 m N/A [59] \u2716 / 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe following c ions will pre ent the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteri f bei g camer or non-camera assi ted as different ets of techniques are required for those tw cases. For e ch of these tw categories, we continue by grouping the provided solutions based on the type of senso with he ost frequent group being presented first. For both ca es, the sensor fusion techniques are described in a comprehensible manner. In more detail, Sec ion 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multipl camera , headset devices as well as 3D cam ra sensors. Sub equently, papers utilizing the IMU s nsor (accelerometer, gyroscope, magnetometer), th m st popular sensor among the selected papers, are presented. For e ch individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the ther hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar ensors as well as Bluetooth Low Energy (BLE) be ons and Wi-Fi access points.\n3.1.1. Camera-Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det r ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\na d came a sens rs is exp ct d given th p oliferation of smar phon devices as the preferred platform for developing so uti s for the BVI individuals. Finally, only 14% of these solutions can be use in real-life sce arios with som degree of success, 29% of the are practical but hav a combination f eithe high cost or require f o the user to carry many\nsensors, 24% r limited pra ticality f r specific scenarios while 32% are purely experi-\nmental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A / N/A N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe fol owi g sec ion will pr sent the solutions found in the literature organized around t e sensors employ d. Sp cifically, the pres nt ti n is organ zed around the criteria of being camera or non-cam a as isted as different sets of techniques are required for thos two ca s. F r each of thes two cat gories, we continue by grouping the provided solutions ba d n the typ f sensor with the most frequ nt group being presented first. For both cases, the sensor fusion techn ques are described in a comprehensible manner. In more detail, Section 3.1.1 cov rs the pap r utilizing a form f the camera sensor, be it single camera r a configur tion of multiple ca ras, headset devices as well as 3D ca era sensors. Sub que tly, papers utilizing the IMU s nsor (ac elerometer, gyroscope, magnetometer), the most popular nso among the selected papers, are presented. For each individual pape , a summary incorpor ting solution with the advantages and d sadvantag s is written. On the other ha d, Secti n 3.1.2 concerns solutions with no camera s nsors. M st of them incorp ate a IMU sensor while other options include ultrasonic nd Lidar s nsors as well as Bluetooth Low E ergy (BLE) beacons and Wi-Fi access points.\n3.1.1. Cam ra-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ading s ates an arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra se sors is xpec ed giv n the proliferation of sm rtphone devices as the preferre platform for developing solutions for th BVI individuals. Finally, only 14% of these solutions can b us d in real-life c narios with some degree of success, 29% of the are pract cal but have a comb nati n of either high cost or require f om the user to carry many\nsensors, 24% are limited practicality for specific scenarios whil 32% are purely experi-\nmental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy amic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A / N/A N/A [51] N/A N/A m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lowing sections will present the solutions fo nd in the literature organized around t e sensors employed. Specifically, the presentation is organized around the crit ia of being am ra or n n-cam ra a sisted as different sets of techniques are required for those two cases. F r each f these two categ ries, we continue by grouping the provided sol ions based on th type of sensor with th most frequent group being presented first. For b th cas , t sensor fusion techniques re described in a comprehensible manner. In m r detail, Section 3.1.1 covers th papers u ilizing a form of the camera sensor, b it a single camera or configuration of multiple cameras, headset devices as well as 3D camera sensors. Subseque tly, pa ers utilizing th IMU sensor (accelerometer, gyroscope, magnetometer), the most ular s nsor among the s lect d papers, are presented. For ea h individu l paper, summ ry incorp rating the solution with the advantages and dis dvantage is w itte . O the other hand, Section 3.1.2 concer s solutions with no camera sens rs. Most f th m incor orate an IMU sensor while other options include ultrasonic and Lid r sensors as w ll as Bluetooth Low Energy (BLE) bea ons and Wi-Fi access points.\n3.1.1. Cam r -Assi ted Sol tio An ncertainty-based adaptive s ns r fusi n framework for Visual-Inertial Odo etry (VIO) s prop sed i [41] for stimating relat ve otion. It minimizes degradation from inaccurat sti ati by determining the states that should b included in the estimati process. The e degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is expected given the prolife ation of smartphone devices as the preferred platform f r develop g olutions for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 29% of the are pract c l bu hav a combination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPape Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A / N/A N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following s ctions wi l prese t the solutions found i the literature organized around the sensors empl y d. Specifically, p esentation is organized around the cri-\nr of b ing c mer or non-camera assisted as different sets of techniques are required f t ose two cases. For ach of these two categories, we continue by grouping the provided solutions ba d on the ype of s sor with he most frequent group being presented first. For both cases, the s n o fusion techniques ar described in a comprehensible manner. I more detail, Secti n 3.1.1 covers the papers utilizing a form of the camera sensor, be it a sin or a configuration of multipl cameras, headset devices as well as 3D cam ra sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\nagn t me er), th mos popular s nsor among the sel cted papers, are presented. For ach ind vidual paper, sum ary incorpora ing the solu ion with the advantages and disa va tages s written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. M st of them incorp rate an IMU sensor while other options include ultrasonic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C ra-Assisted Solutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nReport the user\u2019s location\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of the are\npractical but have combination of either high cost or require from the user to carry many\nsensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / / / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteria of being camera or non-camera assisted as different sets of techniques are required for those two cases. For each of these two categories, we continue by grouping the provided solutions based on the type of sensor with the most frequent group being presented first. For both cases, the sensor fusion techniques are described in a comprehensible manner. In more detail, Section 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multiple cameras, headset devices as well as 3D camera sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is ex ected give the proliferation of smartp one devices as the preferr d platform for devel ping solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of the are\npractical bu have a combination of ither high cos or require from the user to carry many\nsensors, 24% are li ited racticality for spec fic scen ios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] \u2716 / [58] 2 cm < R < 4.5 m N/A [59] \u2716 / 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe following c ions will pre ent the solutions found in the literature organized around the sensors employed. Specifically, the presentation is organized around the criteri f bei g camer or non-camera assi ted as different ets of techniques are required for those tw cases. For e ch of these tw categories, we continue by grouping the provided solutions based on the type of senso with he ost frequent group being presented first. For both ca es, the sensor fusion techniques are described in a comprehensible manner. In more detail, Sec ion 3.1.1 covers the papers utilizing a form of the camera sensor, be it a single camera or a configuration of multipl camera , headset devices as well as 3D cam ra sensors. Sub equently, papers utilizing the IMU s nsor (accelerometer, gyroscope, magnetometer), th m st popular sensor among the selected papers, are presented. For e ch individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the ther hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar ensors as well as Bluetooth Low Energy (BLE) be ons and Wi-Fi access points.\n3.1.1. Camera-Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det r ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\na d camera sens rs is expect d given th p olif ration of smar phon devices as the preferred platform for developing so uti s for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with som degree of success, 29% of the are\npractical but hav combination f eithe high cost or require f o the user to carry many\nsensors, 24% r limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe fol owi g sec ion will pr sent the solutions found in the literature organized around t e sensors employ d. Sp cifically, the pres nt ti n is organ zed around the criteria of being camera or non-cam a as isted as different sets of techniques are required for thos two ca s. F r each of thes two cat gories, we continue by grouping the provided solutions ba d n the typ f sensor with the most frequ nt group being presented first. For both cases, the sensor fusion techn ques are described in a comprehensible manner. In more detail, Section 3.1.1 cov rs the pap r utilizing a form f the camera sensor, be it single camera r a configur tion of multiple ca ras, headset devices as well as 3D ca era sensors. Sub que tly, papers utilizing the IMU s nsor (ac elerometer, gyroscope, magnetometer), the most popular nso among the selected papers, are presented. For each individual pape , a summary incorpor ting solution with the advantages and d sadvantag s is written. On the other ha d, Secti n 3.1.2 concerns solutions with no camera s nsors. M st of them incorp ate a IMU sensor while other options include ultrasonic nd Lidar s nsors as well as Bluetooth Low E ergy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ading s ates an arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra se sors is xpec ed giv n the proliferation of sm rtphone devices as the preferre platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can b us d in real-life c narios with some degree of success, 29% of the are\npract cal but have a comb nati n of either high cost or require f om the user to carry many\nsensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy amic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lowing sections will present the solutions fo nd in the literature organized around t e sensors employed. Specifically, the presentation is organized around the crit ia of being am ra or n n-cam ra a sisted as different sets of techniques are required for those two cases. F r each f these two categ ries, we continue by grouping the provided sol ions based on th type of sensor with th most frequent group being presented first. For b th cas , t sensor fusion techniques re described in a comprehensible manner. In m r detail, Section 3.1.1 covers th papers u ilizing a form of the camera sensor, b it a single camera or configuration of multiple cameras, headset devices as well as 3D camera sensors. Subseque tly, pa ers utilizing th IMU sensor (accelerometer, gyroscope, magnetometer), the most ular s nsor among the s lect d papers, are presented. For ea h individu l paper, summ ry incorp rating the solution with the advantages and dis dvantage is w itte . O the other hand, Section 3.1.2 concer s solutions with no camera sens rs. Most f th m incor orate an IMU sensor while other options include ultrasonic and Lid r sensors as w ll as Bluetooth Low Energy (BLE) bea ons and Wi-Fi access points.\n3.1.1. Camer -Assi ted Sol tio An ncertainty-based adaptive s ns r fusi n framework for Visual-Inertial Odo etry (VIO) s prop sed i [41] for stimating relat ve otion. It minimizes degradation from inaccurat sti ati by determining the states that should b included in the estimati process. The e degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is expected given the prolife ation of smartphone devices as the preferred platform f r develop g olutions for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 29% of the are pract c l bu hav a combination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPape Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following s ctions wi l prese t the solutions found i the literature organized around the sensors empl y d. Specifically, p esentation is organized around the cri-\nr of b ing c mer or non-camera assisted as different sets of techniques are required f t ose two cases. For ach of these two categories, we continue by grouping the provided solutions ba d on the ype of s sor with he most frequent group being presented first. For both cases, the s n o fusion techniques ar described in a comprehensible manner. I more detail, Secti n 3.1.1 covers the papers utilizing a form of the camera sensor, be it a sin or a configuration of multipl cameras, headset devices as well as 3D cam ra sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\nagn t me er), th mos popular s nsor among the sel cted papers, are presented. For ach ind vidual paper, sum ary incorpora ing the solu ion with the advantages and disa va tages s written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. M st of them incorp rate an IMU sensor while other options include ultrasonic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-Assisted Solutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nLook around\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of s artphone devices as the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these\nsolutions can be used in real-lif scenarios with some degree of success, 29% of the are\npractical but have a com ination of either high cost or require from th ser to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / / N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the s lutions found in the lit rature rganiz d around the sensors empl yed. Specifically, the p esentation is ganized around the criteria of being cam ra or - r assisted as different ets of t chniqu are requir d for those two cases. For each of thes two categories, we continue by grouping the provided solutions bas d on the type of sensor with the most frequent group being te first. For both cases, th sensor fusion techniques are described in a compre nsible manner. In more detail, Section 3.1.1 covers the pap rs utilizi g a form f the am ra ensor, be it a single cam or a co figuration of ultiple cameras, head t devices as well as 3D camer sensors. Subsequently, paper utilizing the I U sensor (accelerometer, gyros pe, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sensor while other options include ultrasonic and Lidar sensors as well as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is ex ected give the proliferation of smartp one devices as the preferr d platform for devel ping solutions for the BVI individuals. Finally, only 14% of these\nsolutions can be used in real-lif scenarios with some degree of success, 29% of the are\npractical bu have a combina ion of either high cos r require fr m the user to carry many sensors, 24% are li ited racticality for spec fic scen ios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A [49] \u2716 \u2716 \u2716 [50] / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / N/A [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] \u2716 / [58] 2 cm < R < 4.5 m N/A [59] \u2716 / 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe following c ions will pre ent the solutions found in the literature organized round the sensors mpl yed. Specifically, the p esentation is ganized around the criteri f being camer r n -camera assi ted as different ets of t chniqu are required for thos tw cases. For e ch of thes tw categories, we continue by grouping the provided soluti ns bas d on the type f senso with he ost frequent group being te first. For both ca es, the sensor fusio tec niques are describ d in a co prehensible manner. I more detail, Sec ion 3.1.1 covers the papers utilizing a form f the camera sensor, be it a single camera or a co figuration of multiple camera , hea set devices as well as 3D cam r sensors. Subs quently, papers utilizing the IMU s nsor (accelerometer, gyros ope, magnetometer), th m st popular sensor among the selected papers, are presented. For e ch individual paper, a summary incorporating the solution with the advantages and disa vantages is written. On the ther hand, Section 3.1.2 concerns solutions with no camera sensors. Most of them incorporate an IMU sens r while other options include ultrasonic and Lidar ensors as well as Bluetooth Low Energy (BLE) be ons and Wi-Fi access points.\n3.1.1. Camera-Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det r ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\na d camera sens rs is expect d given th p olif ration of smar phon devices as the preferred platform for developing so uti s for the BVI individuals. Finally, only 14% of these\nsolutions can be used in real-life scenarios with som degree of success, 29% of the are\npractical but hav a c mbination f he high cost or require f o the user to carry many sensors, 24% r limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe fol owi g sec ion will pr e t the solutio found i the literature organized around t e sensors employ d. Sp cifically, the pres nt ti n is organ zed around the criteria of being c mera or non-ca a as ist d as different sets of techniques are required for thos two ca s. F r each of th s two cat gories, we continue by grouping the provided solutions ba d n th typ f senso with the mo t f quent group being presented first. For both cases, t sensor fusion techn q es are described in a comprehensible manner. I more detail, Section 3.1.1 cov rs the pap r utilizing a form f the camera sensor, be it single camera r a c nfigur ti of m ltiple c ras, headset devices as well as 3D ca era sensor . Sub que tly, papers utilizing the IMU s nsor (ac elerometer, gyroscope, magnetometer), the most popular nso among the selected papers, are presented. For each individual pape , a summary incorpor ting solution with the advantages and d sadvan ag is written. On the other ha d, Secti n 3.1.2 concerns solutions with no camera s nsors. M st of them incorp ate a IMU sensor while other options include ultrasonic nd Lidar s nsors as well as Bluetooth Low E ergy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ading s ates an arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra se sors is xpec ed giv n the proliferation of sm rtphone devices as the preferre platform for developing solutions for the BVI individuals. Finally, only 14% of these\nsolutions can b us d in real-life c narios with some degree of success, 29% of the are\npract cal but have a c mb nati n of ei her high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy amic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lowing sections will present the solutions fo nd in the literature organized around t e sensors employed. Specifi lly, the presentation is organized around the crit i of being or n n-c m ra a si ted as different sets of techniques are required for those two cases. F r each f thes two categ ries, we continue by grouping the provided sol ions b s d on th type of sensor with th most frequent group being presented first. For b th cas , the ensor fusion techniques re described in a comprehensible manner. In m r detail, Section 3.1.1 covers th papers u ilizing a form of the camera sensor, b it a singl camera or configuration of multiple cameras, headset devices as well as 3D camera sensors. Subseque tly, pa ers utilizing th IMU sensor (accelerometer, gyroscope, magnetometer), the most ular s nsor among the s lect d papers, are presented. For ea h individu l paper, summ ry incorp rating the solution with the advantages and dis dvantage is w itte . O the other hand, Section 3.1.2 concer s solutions with no camera sens rs. Most f th m incor orate an IMU sensor while other options include ultrasonic and Lid r sensors as w ll as Bluetooth Low Energy (BLE) bea ons and Wi-Fi access points.\n3.1.1. Camer -Assi ted Sol tio An ncertainty-based adaptive s ns r fusi n framework for Visual-Inertial Odo etry (VIO) s prop sed i [41] for stimating relat ve otion. It minimizes degradation from inaccurat sti ati by determining the states that should b included in the estimati process. The e degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is expected given the prolife ation of smartphone devices as the preferred platform f r develop g olutions for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 29% of the are pract c l bu hav a c mbination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPape Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] \u2716 N/A / [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following s ctio s wi l prese t the solutions found i the literature organized around the ensors empl y d. Specifically, p esentation is organized around the crier of b ing c mer or non-ca era assisted as different sets of techniques are required f t ose tw cases. For ach f these two categories, we continue by grouping the provided solutio s ba d on the ype of s sor with he most frequent group being presented first. For both cases, the s n o fusion techniques ar described in a comprehensible manner. I more detail, Secti n 3.1.1 covers the papers utilizing a form of the camera sensor, be it a sin or a configuration of multipl ca eras, headset devices as well as 3D cam ra s ns r . Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\nagn t me er), th mos popular s nsor among the sel cted papers, are presented. For ach ind vidual paper, sum ary incorpora ing the solu ion with the advantages and disa va tages s written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. M st of them incorp rate an IMU sensor while other options include ultrasonic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. C era-Assisted Solutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nCreation of digital maps\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as the preferred platform for developi g solutions for the BVI individuals. Fi ally, only 14% of these solutions can be used in real-life sc narios with some degree of uccess, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716\n[48] N/A / / N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present the solutions fou d in he lit rature organ z d around the sensors e p oyed. Specifically, the p esentation is or a ized around th c iteria of being r or n n-camera a sisted s different sets of te hn are equired for those two cases. For each of se two ca gories, we contin by grouping the ovid d solutions based on the type of sensor w th the most frequen grou being prese ted f rst. For both cases, the sensor fus on tech iques are described i compre ensibl an er. In more detail, Section 3.1.1 cov rs the papers utilizing a form c m a ens r, be it a single camera or a configuratio of multiple cameras, he dset device as well a 3D camera sensors. Subsequently, apers utilizing the IMU sensor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are presented. For each individual paper, a summary i corp a g the s l tio with he advantages and disadvantages is written. On the other hand, S tion 3.1.2 co cerns solutions with no camera sensors. Most of th m incorpor e an IMU sens r whil o he t ons includ ult asonic and Lidar sensors as well as Blu to th Low Energy (BLE) be con and Wi-Fi acce s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of smartphone devices as the pre-\nferred platfo m fo devel ping solutions for the BVI individuals. Finally, only 14% of these\nl ti can b used in real-lif scen rio with s me d gree of success, 29% of he ar pra tic l but have a combination of either high cost or require from the user to carry many sensors, 24% are li ited practicality for pecific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / / / [51] / 5 m 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] / / / / [55] N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] / / [58] 2 cm < R < 4.5 m / [59] / 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sectio will pr s nt the s l ti n fou i th lit ratur rganized round the s nsors mpl yed. Specifi l y, the p ese tation is ganized around th c iteria of being c mera r n - am r ssi ted s fferent ets f t chniqu ar required for thos two cases. For ach f the two cat g ri s, w contin e b gr uping th p vid d soluti ns b s d on the type f senso with the ost frequent group b ing t fi t. For both c ses, the sensor fusio tec niques are describ d in a c mp ehensibl ma ner. I ore d tail, Sectio 3.1.1 cov rs the p pers utilizing a f rm f e cam a sensor, be it a singl c m a r a co figuration of multiple cameras, h a s devic s as well as 3D camer sensors. Subs que tly, p per utilizing e IMU n or accelerom t , gyros op , magnetometer), the mo t popul r s ns r among the sel cted papers, are presented. For each i dividual paper, a summary incorporating the solution with the advantages and disa vant g s is writt n. On the other hand, Section 3.1.2 c ncerns s l ti w th no cam ra s nsor . Mo f hem incorpora e an IMU s ns r while other options include ultrasoni and Lidar s nsors as w ll as Bluetoot Low Energy (BLE) beaco s and Wi-Fi access points.\n3.1.1. Camera-Assist d Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is prop sed in [41] for estimating relative motion. It minimizes degradation from inaccurate state esti ati by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\na d camera sens rs is expected given th prolif ration of smartphone devices as th preferred platform for developi g so uti s for the BVI individuals. Fi ally, nly 14% of these solutions can be u ed in real-life scenari s with som d gree of success, 29% of them are practical but have a combin tion of eith r high cost or require from the user to carry many sensors, 24% ar limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range ccuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A / [51] N/A N/A 5 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe fol wi g s ctions will pres nt the s lutio s f und i the l terature organiz d round t e sens s e p oy d. Specifica ly, the pr s ntati is organized arou d the criteria of being r or n -ca era s is ed s diff rent sets of t chniques are requir d f r hose tw cas s. F r each of two cat g rie , w continu by grouping he vid d solution based on the type f s nsor wit the mo t f equen gro p being p e te first. For b th c es, th sensor fus n tech qu s a e described in a compr hens bl man r I m re detail, Secti n 3.1.1 c v r the pa r ut lizing a for f the ca a sensor, be it a ingle c mera r configurati n f multipl c meras, h set d vices ll 3D camera sensors. Sub quently, pape utilizing the IMU sensor (accel rometer, gyroscope, magnetometer), the most popular senso among the selected pap rs, are presente . For each individual paper, a summary incorp r ting the sol ti with the advantag and disadvantag s is writt . On he other h nd, Section 3.1.2 c nc rns sol tio w th no cam era s nsor . Most of them incorporate a IMU ns r whi e other ptions include ultrasoni nd Lid r so s as well as Bluetoot Low En rgy (BLE) beaco s and Wi-F access points.\n3.1.1. Camera-Assisted S lutions An ncertainty-based adaptive se sor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat s imatio by d termining the states that s ould be in luded in the estimation proc ss. These degrading s ates an arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra se sors is xpec ed given the proliferation of sm rtphone devices as the preferre platform f r developi g solutions for th BVI individuals. Finally, only 14% of these solutions an b used in r al-life sc nari s wi h some degree of su cess, 29% of them are pract cal but have a comb nati n of either high cost or requir f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A / [51] N/A N/A m 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following sections wil present th solu ions fo nd in the lit a ure organiz d arou d the s nsor e p oye . Specifically, the presentation is organized aro nd the cri-\nia of being r or n n-ca ra assist d as different sets f tec niques are required for th se two c ses. F r e ch of se wo c te ri s, we co tinu by g uping the p ovid d sol t ons based n the type of s s r with th o t frequen group be g pres nted first. For b c ses, t s nsor fu on techniques are de cribed in co pre sibl manner. I m re d t il, Sectio 3.1.1 c v rs t paper u ilizing a f rm of he cam a se sor, b it a single c me a r co figur ti n of multiple c me as, h d et devices as well a 3D camera sensors. Subseque tly, pa ers utilizing th IMU sor (accelerometer, gyroscope, magnetometer), the most p pular sensor among the s lect d papers, are presented. For each indiv du l paper, su m ry i o p rating th so uti n with the advantages and disadvantag s itte . O the t er h d, Section 3.1.2 concer s s l tio s th no cam er sens r . Most f m inco orate an IMU s nsor while other options include ultraso i and Li sens rs as well s Bluetooth Low En rgy (BLE) b a o s and Wi-Fi access points.\n3.1. . Camera-Assisted S lutions An uncertai ty-based adaptive s ns r fusion framew rk f r Visual-Inertial Odometry (VIO) s proposed i [41] for stimating relat ve motio . It minimizes degradation from inaccurate t t stimatio by determi ing the states that should b included in the estimati n process. The e degrading states can arise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is expected given the prolife ation of smartphone devices as the preferre platform f r develop g solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life sc narios with some degree of success, 29% of them are pract c l bu hav a combination of i h r high cost or equire from the user to carry many sen ors, 24% are limited practicality fo specific scenarios while 32% are purely experiment l.\nTable 3. Solutions and obstacle detection.\nPape Support Static Dyna ic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A / [51] N/A N/A 5 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh owi g sections wi l pr se t the so ut ons found i the literature organiz d ar und th sen r e yed. Specifically, p esentation is organized around the crit of being r r n n-camera assisted s different sets of techniques are r quired f t ose two cas . F r ac of se two categ ri s, we conti ue by groupi g the p ovid d s l tions ba ed on the ype of sor with he most frequen roup being presented first. For both c s s, the se f si tech iques are described in a c mprehensibl manner. I m e detail, Se ti n 3.1.1 cov rs th aper uti izing a form of the cam a sensor, be it a sin e r c figuration f multipl cameras, he dset evices s well a 3D cam ra sen ors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\ng t me er), th most popular s nsor mong the sel cted papers, are presented. For ch i d vidual pape , a s mma y incorp ra ing the solu ion with the advantages and di a v t g s is wr tt . On the other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sensor . M t f them ncorp rate an IMU s nsor while ther options include ultras nd Lida sens rs as ell a Blueto th Low Energy (BLE) beaco s and Wi-Fi access points.\n3. .1. Ca era-Assisted Sol tions An unce tainty-based adaptiv senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from accurate state estimation by determining the states that should be included in the estimation proce s. These degradi g sta es can arise under motion characteristics that nullify\nIntersections\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected give the proliferati of smart h ne devices s the referred latform for eveloping solu ons for the BVI individuals. Finally, o ly 14% of these sol tions can be used in real-lif scenari s with some egree f suc ess, 29% of them are practical but have a combination of either high cost r req ire from the user to carry many sensors, 24% re limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPa er Support Static Dynamic Rang Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 49 \u2716 0 / N/A 1 / N/A 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] / N/A / / 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe f llowing sections will prese t t e s luti ns fou d in t e lit rat re ga iz d ar und the sensors empl yed. Specifically, the p se tatio is anized around the criteria of bei g cam ra or - r assisted as diff e t ets of t chniqu re requi for those two cases. For each of thes two c teg ries, we conti u by grouping the provided solutions bas d on t e type of sens r with the ost frequent gr up being te first. For both c ses, th sens r fusion techniques are escribed i c pre nsible manner. I more detail, Secti n 3.1.1 covers the p p rs utilizi g a form f the m r ens r, be it a single c m r a co figuratio of ultiple cameras, e d t devic s a well as 3D camer sensors. Subsequently, paper utilizi g the IMU sensor (accelerom ter, gyr s pe, magnetometer), the most popular sensor among the selected p pers, are presented. F r each individ al p per, a summary incorporating the solution with the a vantages a d disadvantag s is written. O the other han , S tio 3.1.2 c cerns sol tio s th cam era sensor . Mos of them i c r ate an IMU s ns r while othe tions include ult asoni and Lidar sensors s well a Blueto th L w Energy (BLE) be co nd Wi-Fi acce s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degr ding states can arise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand c mer sensors is ex ected give the proliferation of smartp one devices as t e referr d latform for devel ping solu ns f r the BVI individuals. Finally, only 14% of these soluti ns ca be used in real-lif sce rio with s me degree of success, 29% of he ar practic l bu have a combinatio of either high cos or require fr t e user to carry many sensors, 24% are limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 49 \u2716 \u2716 0 /"
        },
        {
            "heading": "1 N/A / 5 m 95",
            "text": "2 0 < < 9 m 98%\n53 0.1 3.5 9 \u201395 [54] / / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716\n[72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe following c ions will pr e t the s luti ns f und in th liter tur rganized round the sensors mpl ye . Specifically, the p esentati n is ganized around th criteri f being c mer r -camer ssi ted s ifferent ets f t chniqu are required for thos tw cases. For eac f t es tw cat ories, w continue b r upi g the provid d soluti ns bas d on the type f senso with he ost frequent group b ing te fi t. For both c es, the sensor fusio tec niques re describ d in comprehensible ma ner. I ore detail, Sec io 3.1.1 covers the papers utilizing a form f t e c m r se s r, be it a single c mera r a co figuratio f multiple camera , h set devic s s well as 3D cam r sensors. Subs que tly, p per utilizing the IMU n or (accelerom ter, gyros ope, magnetometer), th m t p pular sensor a ng the selected papers, are prese te . For e ch individu l paper, summary incor orati g the soluti n with the advantages and disa vant g s is written. On th t er hand, Sectio 3.1.2 c cer s s l i s w th o cam era sensor . o f h m incorp ra a IMU s r while other o s i clude ltrasoni a d Lidar s nsors s w ll as Bluetoot Low En gy (BLE) e a d Wi-Fi acc ss points.\n3.1.1. Camer -Assisted Solutions An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for esti ati g rel tive moti n. It mini izes degradatio fr inaccurate state estimation by det rmi ing the states that s ould be included in t e estimation process. These degradi g states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd c mera sens rs is expect d given th p olif rati of smar phon devices as the preferred platfor for developi g s uti s for the BVI indivi u ls. Finally, only 14% of these sol tions can be used in real-l f ce arios with som degree of success, 29% of them are practical but hav combination f eithe igh cost or req ire f o the user to carry many sensors, 24% r limited ra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / / / [49] \u2716 \u2716 \u2716 [50] / N/A N/A / [51] / N/A 5 m 95% [52] 0 < 9 m 98% [53] 0.1 m < R 3.5 m 90\u201395 [54] / N/A N/A / [55] / N/A 0.2 m < R 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R 12 /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe f l owi g sec ion will pr t the oluti fo d i the lit rature organized ar und t e sensors empl y d. Sp cifically, the p es nt ti n is gan zed around the c iteria of bei g c mera or n -cam a as ist d as differe t ets of t chniqu ar required for thos two ca s. F r each of th s two cat g rie , we conti e by grou ing the rovid d solutions ba d n th ty of sens wit t e o t f quent gr p being te first. For th c ses, t sens r fusi n techn q es are desc ibed in a co prehen ible man er. I more det il, Section 3.1.1 cov rs the pap r utilizi g a form f the camera sensor, be it single c mera r c figur ti f ltiple c r s, dset devices as well s 3D ca era sensor . Sub que tly, pap rs utilizi g the IMU s nsor (ac lerometer, gyros ope, magnetometer), the most popular ns among the selected papers, are presented. For each indivi al p pe , summary i corpor ting solution with the a v ntages and d sa v n ag is written. O the other h d, Secti 3.1.2 c nc r s s l tio w th n cam ra nsor . Most f them i corpo at IMU s s r whil other pti ns i clude ultraso i d i r s sor as well s Blu tooth Low E rgy ( LE) beaco s a d Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es mation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s ates an arise under motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 9\nand camera sensors is expected giv n the p olif ration of m rtphone devices as the preferred latform f r d velopi g solu ns for th BVI individuals. Finally, only 14% of these solutions an b used in r al-lif sc narios wi h some d gree of su c ss, 29% of the are p actical but have a combination of eith r high ost or requir from th user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 98%\n53 \u2716 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following sections wil present th solu ions found in th lit a ure organiz d arou d the s nsor p oye . Specific lly, the presentation is organized aro nd the cri-\nria of b ing r or n n-ca ra ssist d s ifferent sets f tech iques are required for t se two cases. For e ch of se wo c te o i s, we continu y g ouping the p ovid d solut ons based n the type of s s r with th o t frequen g oup be g pres nted first. For bo cases, t e s nsor fu on t chniques are de cribed in a co pre nsibl ma ner. In more d t il, Section 3.1.1 ov rs t paper utilizing a f rm of he cam a se sor, be it a single cam a a onfigurati n of multi le came as, h d et d vices as well a 3D camera s nsor . Sub equently, papers utilizing the IMU nsor (accelerometer, gyroscope, magnetometer), the most popular sensor among the selected papers, are pres nted. For each i div dual p p r, a ummary in o p ra ng th so uti n wi h th advantages and disadvantage s ritten. On the t er h d, Section 3.1.2 concerns olutio s with no camera sen or . Mo t f m inco por e an IMU s nsor whil o her optio s include ultras ic and Li a ns rs as well as Blu tooth Low Energy (BLE) beacons and Wi-Fi acce s points.\n3.1. . Camer -Assi te S l tio s An uncertai ty-based adaptive sensor fusion framew rk for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimati g r l tive motio . It minimizes degradation from inaccurate t t esti ation by determi ing the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sens rs is exp cted given he proliferation of smartphone devices as the preferre latform f r dev lopi g solu ons for the BVI individuals. Finally, only 14% of these soluti s can b used in real-life scenarios with some degree of success, 29% of the are pract c l but have a combination of ith r high cost or equire from the user to carry many sensors, 24% are limited practicality for sp cific scenarios while 32% are purely experiment l.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 95 2 0 < < 9 m 98%\n53 \u2716 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh owi g section will pr se t the so ut ons found i the literature organiz d ar und t sen or e oyed. Specifically, the presentation is organized around the crit i of being r r n n-camera assisted s differ nt sets of techniques are r quired f tho two cas . F r ac of se tw categori s, we continue by groupi g the p ovid d s l tions ba ed on th ype of ensor with the most frequen group being presented first. For both as s, the se f si tech iques are d s rib d in a c mprehensibl manner. I m e det il, S tion 3.1.1 cov rs th aper uti izing a form of the cam a sensor, be it a sin le camera or c figurati n f multipl cameras, headset evices s well a 3D cam ra sen or . Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\ngnet meter), the mo t popular s sor mong the sel cted papers, are presented. For ch i d idual pape , a s mma y inc p ra ng the solu ion with the advantages and disadvan ag s s wr tt . On the other hand, S cti 3.1.2 concerns solutions with no camera s nsors. Mo t f th m incorpora e an IMU sensor whil o her optio s include ultras ic and Lidar sens rs as ll as Blu tooth Low Energy (BLE) beacons and Wi-Fi acce s points.\n3.1.1. C era-Assisted S l tions An uncertainty-based adaptiv senso fu ion f mework for Visual-Inertial Odometry (VIO) is proposed in [41] for es mating r lat v motion. I minimizes degradation from inaccurate state estimation by deter ining the state t t should be included in the estimation proce s. These degradi g states can arise under motion characteristics that nullify\nSimulation mode\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of smartphone devices s the preerred latform for developing solu ons for the BVI individuals. Finally, only 14% f hese solu ns can be used in real-lif scenari s with some egree f suc ess, 29% of the are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Rang Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 49 \u2716 \u2716 \u2716 0 N/A / 1 N/A N/A 5 m 95 2 0 < < 9 m 8%\n53 0.1 3.5 90\u201395 [54] N/A / / / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 6 \u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present t e s luti ns fou d in t e lit rat re ga iz d around the sensors empl yed. Specifically, the p se tatio is anized ar und the criteria of being cam ra or - r assisted as diff ent ets of t chniqu are requi d for those two cases. For each of thes two c teg ries, we continu by grouping the provided solutions bas d on t e type of sensor with the ost frequent gr up being te first. For both c ses, th sensor fusion techniques are escribed i c pre nsible ma ner. I more detail, Secti n 3.1.1 covers the p p rs utilizi g a form f the m r ens r, be it a single c m r a co figuratio of ultiple cameras, e d t devic s a well as 3D camer sensors. Subsequently, paper utilizing the IMU sensor (accelerom ter, gyr s pe, magnetometer), the most popular sensor among the selected papers, are presented. F r each individual paper, a summary incorporating the solution with the advantages a d disadvantag s is written. O the other han , S tion 3.1.2 c cerns sol tio s th n ca era sensor . Mos of them incor rate an IMU s ns r while othe tions include ult asoni and Lidar sensors s well as Blueto th Low Energy (BLE) be co nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation from inaccurate state estimation by deter ini g the states that should be included in the estimation process. These degrading states can arise under motion characteristics th t nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given h proliferation of sm rtphone devices as the preferred platform for dev lopi g solutions f r the BVI individuals. Finally, only 1 % of these soluti ns c n be used i real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll wing sections will p esent the sol ons fou d in the lite atu e organiz d r u d the sensors e p oyed. Sp cifically, th pres nt tion i organ z d ar u d the crit ri of being er or n n-camera assisted s differ nt t f techniques r qui ed for thos two cases. For ach of se two cat g ries, w ontinue by grou ing th p ovid d solution b sed o the type of se sor with h most freque group being prese ted fir t. For bo h cas s, the sensor fus n t c iques are desc b d i a comprehen ibl mann r. In more detail, Section 3.1.1 cov rs the apers util zing a form f the am sensor, b it a single camera or a configu ation of multipl camer , h ad et devices s well 3D camera sensors. Subsequently, papers utilizi t IMU ensor (accelerom ter, gyroscope, magnetometer), the most popular sens among the selected papers, are presented. For each individual paper, summary incorp g th soluti n with the adva t g s nd disadvantages is written. On the other hand, S cti n 3.1.2 c nce n solut ons wi h n ca - era sensors. Most of th m incorpor e an IMU s nsor whil o her options clu ultrasonic and Lidar sensors s well as Blu tooth Low Energy (BLE) beacon nd Wi-Fi acce points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det r i ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expec d gi en he p oliferation of smar pho devices s the preferred platform for developi g solutions for the BVI individuals. Fi ally, nly 14% of these solutions can be u ed in real-life scenari s with some d g ee of success, 29% of them are practical but hav a combinatio f eithe high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A / [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A / [51] N/A N/A 5 m 95% [52] < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll wing s c ion will pres nt the s lutio s f und in the l terature organiz d round the sens s e p oy d. Specifica ly, the pr s nt ti is organ zed arou d the criteria of being r or n n-ca a s is ed s diff rent sets of t chniques are requir d f r hose two ca s. For ach of tw categorie , we continu by grouping he p vid d\nlutions ba d n the typ of s nsor with the mo t f qu n g o p being p e ente first. For both ca es, th sensor fus on tech iqu s are described in a compr hens bl man r I m e detail, Secti n 3.1.1 cov r the pa rs ut lizi g a for f the ca a sensor, be it single camera r a configur ti n of multipl cam ras, hea s t d vices ll 3D camera sensors. Subsequently, pap r utilizing the IMU s nsor (ac elerometer, gyroscope, m gneto eter), the most popular nsor among the selected p pers, are presented. For ach individual p pe , a summary incorp ra ng oluti with the advantag and d sadvantag s is writt n. On he other ha d, S cti 3.1.2 concerns solutions with no ca - era nsors. M st f th m incorp e n IMU ensor whi o h r optio s nclude ultr - sonic nd Lid r enso s as well as Blu toot Low E rgy (BLE) beacons d Wi-F acce s points.\n3.1.1. Camera-Assisted S l tions An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est m tion by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is xpected giv n he p oliferation of sm rtphone devices as the preferred platform f r d velopi g solutions for the BVI individuals. Finally, only 14% of these solutions an b us d in r al-life c narios wi h some degree of su cess, 29% of the are practical but h ve a comb nati n of eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A / [49] \u2716 \u2716 \u2716 [50] / / N/A / [51] N/A N/A m 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A / [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lowing sections wil present th solu ions fo nd in the lit a ure organiz d arou d t e s nsor p oye . Specific lly, the presentation is organized aro nd the crie ia of b ing r or n n-ca ra a sist d s ifferent sets f techniques are required for t se two cases. For e ch f se wo c te i s, we continu by g ouping the p ovid d solu ons based n th type of s s r with th ost frequen group be g pres nted first. For b cas , t e sensor fus on t chniques re de cribed in a co pre nsibl manner. In mor d t il, Section 3.1.1 ov rs t paper u ilizing a f rm of he cam a se sor, be it a single cam a a onfigurati n of multi le came as, h d et devices as well a 3D camera sensor . Subseque tly, papers utilizing th IMU sensor (accelerometer, gyroscope, magnetometer), the most ular s nsor among the s lect d papers, are presented. For ea h i div du l p p r, a ummary incorp ra ng th so ution wi h th advantages and dis dvantage s itte . O the ther h d, Section 3.1.2 co cer s olutio s with no camera sen ors. Most of m inco or e an IMU nsor whil o her optio s include ultras ic and Li ns rs as w ll as Blu tooth Low Energy (BLE) bea ons and Wi-Fi acce s points.\n3.1. . Camera-Assisted S lutio s An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odo etry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat sti ati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca ra sensors is exp cted given he prolife ation of smartphone devices as the preferre platform f r dev lop g olutions for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 29% of the are pract c l bu hav a combination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experi-\nent l.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A / [49] \u2716 \u2716 \u2716 \u2716 [50] / / N/A / [51] N/A N/A 5 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe owing s ctions wi l pr se t the so ut ons found i the literature organiz d ar und t sen ors e y . Specifically, p esentation is organized around the criof b ing r r n n-camera assisted s different sets of techniques are r quired f those two cas . For ac of se tw categori s, we continue by groupi g the p ovid d s l tions ba d on the ype of sor with he most frequen group being presented first. For both cas s, the se f si tech iques are described in a comprehensibl manner. I m e det il, S ti n 3.1.1 cov rs th aper uti izing a form of the cam a sensor, be it a sin e or c figuration of multipl cameras, headset evices s well a 3D cam ra sensors. Subsequently, papers utilizing the IMU sensor (accelerometer, gyroscope,\nagnet me er), th mo popular s nsor among the sel cted papers, are presented. For ch i d idual pape , a s mma y inc p ra ng the solu ion with the advantages and disa van ag s s wr tt . On the other hand, S cti 3.1.2 concerns solutions with no camera sensors. M t f th m incorp r e an IMU sensor whil o her optio s include ultras ic nd Lidar sens rs as ll a Blu tooth Low Energy (BLE) beacons and Wi-Fi acce s points.\n3. .1. C era-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\n3rd party app navigation\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected give the proliferation f smart h ne devices s the referred latform for developing solu ons for the BVI individuals. Finally, only 14% of these solutio s can be used in real-lif scenari s with some egree f suc ess, 29% of the are practical but have a combination f either hig cost r require from the user to carry many sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy [41\u2013 7] \u2716 \u2716 \u2716\n48 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 8%\n53 0.1 3.5 90\u2013 5 [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 6 \u201398%\n[60] 2 cm < R < 12 m / [61\u2013 3]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will present t e s luti ns fou d in t e literature ga iz d around the sensors empl yed. Specifically, the p se tatio is anized ar und the criteria of being cam ra or n- r assisted as diff ent ets of t chniqu re requi d for those two cases. For each of thes two c teg ries, we continu by grouping the provided solutions bas d on t e type of sensor with t e ost frequent gr up being te first. For both c ses, th sens r fusion techniques are escribed i c pre nsible ma ner. I more detail, Secti n 3.1.1 covers the p p rs utilizi g a form f the m r ens r, be it a single c m r a co figuratio f ultiple cameras, e d t devic s a well as 3D camer sensors. Subsequently, paper utilizing the IMU sensor (accelerom ter, gyr s pe, magnetometer), the most popular sensor among the selected p pers, are presented. F r each individual paper, a summary incorp rating the solution with t e advant ges a d disadvant g s is written. O the other han , S tion 3.1.2 c cerns sol tio s th ca era sensor . Mos of them i c r rate an IMU s ns r while othe tions include ult asoni and Lidar sensors s well as Blueto th Low Energy (BLE) be co nd Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand c mer sensors is ex ected give the proliferati of smartp one devices as t e referr d latform for devel ping solu ns f r the BVI i dividuals. Finally, only 14% of th se sol ti ns ca be used in real-lif sce rio with s me degree of success, 29% of hem ar practic l bu have a combinatio of either high cos or req ire fr t e user to carry many sensors, 24% re limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716\n48 49 \u2716 0 / 1 / / 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 9 \u201395 [54] / / 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716\n[72] N/A [73] \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716\nThe f llowing c ions will pre e t the s luti ns f und in th liter tur rganized r und the sensors mpl ye . Specifically, the p esentati n is ganized around th criteri f bei g c mer r n n-camer ssi ted as iffere t ets f t chniqu are required for thos t cases. For each f t es tw cat g ries, w conti ue b roupi g the provid d soluti ns bas d on the type f sens with he ost frequent group b ing te fi t. For both c es, the sensor fusio tec niques re describ d in co prehensible ma ner. I ore detail, Sec i 3.1.1 covers the papers utilizing a form f t e c m r se s r, be it a single c mer r a co figuratio f multiple camer , h set devic s s well as 3D cam r sensors. Subs que tly, p per utilizi g the IMU n or (accelerom ter, gyros ope, magnetometer), th m t p pular sensor a g the selected papers, are prese te . For e ch individ al p per, summary incor orati g the soluti n with the a vantages and disa vant g s is written. th t er hand, Sectio 3.1.2 c cer s s l i s w th no cam era sensor . Mo of hem incorpora a IMU s r while other o s i clude ltrasoni a d Lidar nsors as w ll as Bluetoot Low En gy (BLE) e a d Wi-Fi acc ss points.\n3.1.1. Camer -Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degr ding states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expec ed gi en he proliferation of smartphone devices s th preferred platform for developi g solutions for the BVI individuals. Fi ally, nly 14% of these solutions can be u ed in real-life scenarios with some deg ee of success, 29% of the are practical but have a combinatio of eith r high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 95 [52] < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh foll wing s ctions will pres nt the solutio s f u d i the l terature organiz d round the sens s emp y d. Specifica ly, the pr s ntati is o ga ized arou d the criteria of being r or n n-ca era s is ed s diff rent sets of t chniqu s are r quir d f r hose tw cas s. For ach of tw categorie , w continu by grouping he p vid d olution based on the typ of s nsor with the mo t f quen g o p being p e nte first. For b h ca es, h sensor fus n tech iqu s a e described in a compr hens bl man r I m e detail, Secti n 3.1.1 cov r the p rs ut lizi g a form of the ca a sensor, be it a ingle camera or a configurati n of multipl ca eras, h a s t d vices ll 3D ca era sensors. Subsequently, pap utilizing the IMU sensor (accel rometer, gyroscope, m gneto eter), the most popular sensor among the selected p p rs, are presente . For ach individual p per, a summary incorp ra ng the oluti with the advantag and disadvantages is writt . On he other hand, S cti 3.1.2 concerns solutions with no ca - era nsors. Most f th m incorpor e n IMU ensor whi o h r optio s nclude ultr - sonic nd Lid r enso s as well as Blu toot Low En rgy (BLE) beacons d Wi-F acce s points.\n3.1.1. Camera-Assisted Soluti s An uncert inty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat stimatio by d termining the states that s o ld be in luded in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 9\nand c mera se sors is xpec ed giv n the proliferati of smartphone devices as the preferr latfor for developing solu ons for the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-lif c narios with some degree of success, 29% of them are pract cal but ave a combinati n of either high cost or req ire from the user to carry m n se sors, 24% re limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support St tic Dynamic Range Accuracy [41\u201347] \u2716 \u2716\n48 / 49 \u2716 \u2716 \u2716 0 N/A N/A 1 / N/A 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f lowing sections will prese t the solutio s fou d i the literature organized ar und t e sens rs empl ye . Specifi lly, the p esentation is ganized around the crit ri of bei or n -c m ra a si ted a iff re t ets of t c niqu re r quired for th se tw c es. Fo e ch f thes two categ ries, we co ti ue by gr uping the p ovid d sol ions b s d on th type of se s with th most frequent group being te first. For b th c s , the e sor fusi techniq s re described in co prehe sible man er. I m r detail, Secti 3.1.1 c vers th papers tilizing a form f the camera se or, b it a singl c er r co figu ti of multiple c me as, he dset devices as well as 3D camer ensors. Sub eq e tly, p utilizi g th IMU sens r (accel rometer, gyros ope, magnet meter), the mo t o ular s nsor am ng the selected papers, re presented. For e h individ al p p r, su m ry inco porati g the s lution with the a vantages and dis vantag is writt . O t other an , Secti 3.1.2 c ncerns s l tio s th no cam era sens r . M st f th i c orate an IMU s sor while ot er ptions include ultraso and Lidar s s as w ll B et oth L w En rgy (BLE) b aco s nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutions An ncertai ty-based adaptive sensor fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed in [41] for estimating r lative m tion. It minimizes degradation from inaccurat estimati by determining the states th t should be included in the estimati pr cess. The e degr din states can rise und r motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is expected given the proliferati of smartphone devices as the preferred l tf rm f r eloping olu ons for the BVI i dividuals. Finally, only 14% of these sol tions an be use in r al-lif sc narios with some degree of success, 29% of them are pract c l but hav a combin tion of either high cost or eq ire from the user to carry many sens rs, 24 re limited practicality for sp cific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa e Support Static Dyna ic R nge Accuracy [41\u201347] \u2716 \u2716\n48 / 49 \u2716 \u2716 \u2716 0 N/A N/A 1 / N/A 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 / / 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nTh f ll wi g s cti s will pres t the solutions found i the literature organized ar und the ens e pl y . Specifically, the p es nt tion i ganized around the crie i of b i g c mer n -camera as ist d differe t ets of t chniqu are required f r t o tw c se . For ach f thes two categ ri s, w conti ue by grouping the provided s lutio s ba o th y of ns r with the m st frequent roup being te first. F th c ses, the s o fusi t chniques are described i c prehensible manner. I m re detail, Se tio 3.1.1 c vers the papers utilizing a form f the camera sensor, be it sin le c m r r c figuration of multipl cameras, h dset devices as well as 3D cam r s s r . Subsequently, papers utilizi g the IMU sensor (accelerometer, gyros ope,\nagn t eter), the mos po ular s nsor among the sel cted papers, are presented. For ach individual p per, u mary incorpora i g the solu ion with the a vantages and di adv t g s s written. O th other hand, Sectio 3.1.2 c cerns sol tio s w th no cam era se s r . M st f th m ncorp rate an IMU s nsor while ther options include ultraso and Lida se s rs as ll as Blueto th Low Energy (BLE) beaco s and Wi-Fi access p in s.\n3.1.1. Cam ra-Assisted Soluti ns An uncertainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odometry (VIO) is p opose i [41] for es ating relat v motion. I minimizes degradation from inacc rate state estim tion by deter ining the states that should be included in the estimation proce s. The e egr d g states can arise under motion characteristics that nullify\nVoice control\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected give the proliferation of s art h ne devices as the referred latform for developing solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree f success, 29% of the are practical but have a c m ination of either high cost r require from th ser to carry many sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 8%\n53 0.1 3.5 90\u201395 [54] / / / / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 6 \u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections w ll pr sent t e s l ti ns fou d in the literature rga iz d a ound the sensors empl yed. S cific lly, the pre ntation or ized aroun the r teria of being cam ra or o - er as is ed as diff ent s ts of t h ique are r quir d for those two cases. For each of these two categ ries, we continue by grouping the p ovid d solutions based on t e type of sensor with the ost frequent group being pr se ted first. For both c ses, th ens r f si techniques are described i a compre nsible ma ner. I more detail, Secti n 3.1.1 cover th pap rs utilizi g f rm f the m r ens r, be it a single c me r a configuratio of ultiple cameras, ead et devices as well as 3D camera sensors. Subseque tl , paper utilizing the I U se sor (accelerometer, gyr sc pe,\nagnetometer), t e most popular sensor among the selected p pers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantag s is written. O the other han , Se tion 3.1.2 c cerns sol tio s w th ca era sensor . Mos of them i c r rate an IMU s ns r while othe tions include ult asoni and Lidar sensors s well as Blueto th Low Energy (BLE) be co nd Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation fro inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion ch racteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given h proliferation of sm rtphone devices as the preferred platform for dev lopi g solutions for the BVI individuals. Finally, only 1 % of these soluti ns c n be used i real-life sce rio with s me degree of success, 29% of he ar practical but have a combination of either high cost or require from the user to carry many sensors, 24% are li ited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll ing sections will pr sent the oluti ns found in lite tur r z d r und the sensors p oyed Specifically, the pr sentati i organiz around th criteria of being ra r n n-camera ssi t d s ifferent s ts f tech iq s ar requi ed for thos two cases. For ach of se two cat g ries, w cont ue b gr u i g th p ovid d soluti n based o the type f se sor with th most frequen group b ing presented fi t. For both cas , h se sor fus o tec iques re de crib d in a comprehe ibl ma ner. I more detail, Sectio 3.1.1 cov rs the apers utilizi g a form f the c m s nsor, b it a singl camera or a configu atio of multiple ca eras, h a set devic s as w ll a 3D camera s n ors. Subs q e tly, p p r utilizi t IMU n or (ac elerom ter, y oscope, magnetometer), the mo t opular sens among the selected papers, are prese ted. For each individual paper, summary incorp ra g the soluti n with the advant g s nd disa vantages is written. On the other hand, S cti n 3.1.2 c nce n solut ons wi h n camera sensors. Most of th m incorpor e an IMU s nsor whil o her options clu ultrasonic and Lidar sensors s well as Blu tooth Low Energy (BLE) beacon nd Wi-Fi acce points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det rmi ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expec d gi en he p oliferation of smar phon devices s the preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l f cenarios with some deg ee of success, 29% of the are practical but hav a combinatio f eithe high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 m 95 [52] < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A / [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh f llowing ec io s will pr t th s lutio f in t lit ratur org nized ound the se s s e pl y d. S cific ly, the p n ti i ga zed arou the c i teria of being ca era or -ca a s is as diff e et of t h iqu ar r quired for those two ca s. For ach of th two ca egorie , we contin e by grou ing he pr vid d olutions ba d n th typ of senso with the o t fr qu nt g up being te first. For bo h c es, nsor f si t ch iq s are d c ibed in a c prehen b e manner. I m e detail, S cti n 3.1.1 cove s the pap rs utilizing a r f the camera sensor, be it ingl ca era r a c figur ti f l iple c r s, ads t devices as well as 3D camera sensors. Subseq e tl , pap rs utilizing th IMU s nsor (ac lerometer, gyros ope,\ngneto eter), t e m st p pular ns r among the selecte p pers, are prese ted. For ach individual p pe , a summary incorp ra ng olutio with the advantag s and d sadvantag s is writt n. On he other ha d, S cti 3.1.2 concerns s lutions with no ca - era nsors. M st f th m incorp e n IMU ensor whi o h r optio s nclude ultr - sonic nd Lid r enso s as well as Blu toot Low E rgy (BLE) beacons d Wi-F acce s points.\n3.1.1. Camera-Assisted S lutions An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est m tion by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is xpected giv n he p oliferation of sm rtphone devices as the preferred platform for d veloping solutions for the BVI individuals. Finally, only 14% of these solutions an b us d in r al-lif c narios with some degree of success, 29% of them are practical but have a comb nati n of eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A / N/A / [49] \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A m 95% [52] 0 m R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u20137 ] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f l wing section w l pr sent the s lutions fo d i the literature orga ized ou d t e ensors mpl yed. S cifi lly, the p e e tation i ganized aroun the cri t i of b in ca r r n -c m a a is d a iff ent ets f t h iqu r r quir d for t se two c ses. F r each f th s wo c teg ies, we continue by grouping the p ovid d solu ions b s d on th type of sensor with th ost frequent group being te first. F r b cas , t e ensor f i t chniqu s re descr b in co prehensible man er. In mor d t il, Section 3.1.1 ov rs e papers ilizing a f r f the camera se or, be it a singl cam ra a o figur ion of mult le cameras, head et devices as well as 3D\ne en or . Sub equ tl , pape utilizing th IMU sens r (accel rometer, gyros ope, agnetometer), t e ost ular s ns r amo g the s lect d papers, are presented. For ea h i div du l p p r, a ummary incorp ra ng th so ution wi h th advantages and dis vantage s w itte . O th ther h , Secti 3.1.2 co cer s olutio s with no camera sen ors. Most of m inco or e an IMU nsor whil o her optio s include ultras ic and Li ns rs as w ll as Blu tooth Low Energy (BLE) bea ons and Wi-Fi acce s points.\n3.1. . Camer -Assi ted Sol tions An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odometry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat stimati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c era sensors is expected given the proliferation of smartphone devices as the preferred l tform for eloping solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenarios with some degree of success, 29% of the are practical but hav a c mbin tion of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific sce arios while 32 are purely experimental.\nTable 3. Solutions and obstacle detection.\nPape Support Static Dyna ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n48 / 49 \u2716 \u2716 \u2716 \u2716 0 N/A / 1 N/A 5 m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT e fol wi g sectio s will present the s lutio s foun in the literature organized arou d th sens rs e ploy . Specific lly, the p e entation is orga ized around the crit ri of b ing c m a on-ca e as isted as different sets of techniques are required fo t s t o c s s. For each f thes two categ es, we conti ue by grouping the provided s lutio s ba e on the typ of sens r with the most frequent roup being presented first. F bot c ses, the se sor fusi techniques are described in a c mprehensible manner. I etail, Secti n 3.1.1 covers the p pers utilizing a form of the camera sensor, be it single c era r c nfiguration of multipl ca eras, he dset devices as well as 3D camer s ns r . Subs quently, p pers utilizing the IMU sensor (accelerometer, gyroscope, magn tometer), the most popular s nsor among the selected papers, are presented. For ach individual paper, a u a y incorporating the solution with the advantages and di t is written. On th other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sens r . M st of th m ncorp rate an IMU s nsor while ther options include ultras and Lida se sors as w ll as Blueto th Low Energy (BLE) beaco s and Wi-Fi access p i s.\n3.1.1. C m ra-Assisted S luti ns An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is p posed i [41] f esti ati g relative motion. It minimizes degradation from inaccurate stat estimation by deter ining the states that should be included in the estimation proc s. These egradi g states can arise under motion characteristics that nullify\nSupports deafblind\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected give the pro iferation f s art h ne devices as the referred latform for developing solu ons for the BVI individuals. Finally, only 14% of these olutions can be used in real-lif scenari s with some egree f suc ess, 29% of the are practical but have a combination f either hig cost r require from th ser to carry many sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Range Accuracy [41\u2013 7] \u2716 \u2716 \u2716 \u2716 \u2716\n48 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 8%\n53 \u2716 0.1 3.5 90\u2013 5% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 6 \u201398%\n[60] 2 cm < R < 12 N/A [61\u2013 3] \u2716 \u2716\n[64] \u2716 R = 20 N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followi g sect ons will pr sent t e s l t s fou d in th lit ature ganiz d a ound the sensors empl yed. S cific ly, the p e t o is g nized arou th cri teria of being cam ra r n- r as is e as diff e t et of t h ique e requ r d for thos two cases. Fo each of thes two c t gori s, we co tinue by rouping th provided solutions bas d on t e type of se sor with t ost f equ t gr u being te first. For both c s s, th ens r f si techniqu s are esc ibed i c pre nsible ma ner. In more detail, Secti n 3.1.1 covers the p p s utilizi g f r f the am ra e or, be it a single cam or co figurati n f u ti l a er s, he et devices w ll as 3D camer sensor . Sub que tl , paper utilizing th I U ( ccel ro ter, gyr s p ,\nagnetometer), t e m s po ular ensor am ng the elected p pers, are p nted. Fo each individual paper, a summary incorp rating the solution with t e advant ges a d di advant ges is written. O the other han , S ction 3.1.2 c ncerns solutions with o ca - era sensors. Mos of them i c r rate an IMU s nsor while other options i clude ltrasonic and Lidar sensors s well as Bluetooth Low Energy (BLE) be c ns nd Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given h proliferation of sm rtphone devices as the preferred platform for dev lopi g solutions f r the BVI individuals. Finally, only 1 % of these soluti ns c n be used i real-life scen rios with s me degree of success, 29% of he ar practical but have a combination of either high cost or require from the user to carry many sensors, 24% are li ited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] N/A / / N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / / N/A [51] N/A / 5 m 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] / / / / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 c < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll ing sections will p sent the l ons fou d in the lite tu r niz d r u d the sensors p oyed. Sp cifically, th pr s ntati n i organ z around th crit ria of being era or n n-camera ssist d s different s ts f techniques a r qui ed for thos two cases. For ach f se two cat g ries, w ontinue b r u i g th p ovid d solution b sed o the type of se sor with h ost freque group b ing prese ted fir t. For bo h cas , h se sor fus t c ique re c d i ompr h ibl ma ner. In ore detail, Section 3.1.1 covers the ape util zing a form f he m s s r, b it a singl camera or a configu atio f multipl ca er , h ad t d v c s as w ll a 3D camera s n ors. Subs quently, p p r utilizi t IMU n r ( c elerom t , gy o cop , magnetometer), the most popular senso among the selected papers, are presented. For each individual paper, a summary incorp g th solution with the advantages and disadvant ges is written. On th other hand, S cti n 3.1.2 c ncerns s lu io s with no ca - era sensors. Mo of h m incorpor a IMU sen or whil o her o ns i clude ultrasonic and Lidar s nsors as w ll as Blu toot Low En gy (BLE) eacon and Wi-Fi acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det r i ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expec d gi en he p oliferation of smar phon devices s the preferred platform for developi g solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-l fe cenarios with some deg ee of success, 29% of the are practical but hav a combinatio f eithe high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] / / / N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A 5 m 95 [52] < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh f ll wing s c ions will pr t th s lutio fo d in t lit ratur org niz d round the se s s emp y d. Specifica ly, the pr s nt ti i ga zed around the c iteria of being r or n -ca a s is s diffe e t sets of t chniques ar r quired f r hose two ca s. For each of two categ rie , we contin e by grou ing he vid d olutions ba d n the typ of sensor wit the o t fr qu n g up being prese te first. For bo h c s, h s s r f s o t ch iq s re desc ibed in a c prehen b m ner. I m e detail, S cti n 3.1.1 c v s the p rs ut lizin a or f the c m a s ns r, be it ingl ca era r c nfigur ti n f ul iple c r s, set evices as w ll 3D ca era sensor . Sub que tly, pap rs utilizi th IMU s r (ac eleromet , gy oscope, m gneto eter), the most popular ns r among the selected p pers, are presented. For ach individual p pe , a summary incorp rating solution with the advantag s and d sadvantag s is written. On he other h d, Secti 3.1.2 c nc rns s l tio w th no cam era nsor . M st f them i corp at an IMU ns r whi e oth r ptions include ultrasoni nd Lid r se so as well as Bluetooth Low E rgy (BLE) beaco s and Wi-F access points.\n3.1.1. Camera-Assisted Soluti s An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est mation by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is xpected giv n he p oliferation of sm rtphone devices as the preferred platform f r d veloping solutions for the BVI individuals. Finally, only 14% of these solutions can b us d in real-lif c narios with some degree of success, 29% of the are practical but have a comb nati n of eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716 \u2716\n[48] / / / N/A [49] \u2716 \u2716 \u2716 \u2716 [50] / / N/A N/A [51] N/A N/A m 95 [52] 0 < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] R = 20 cm N/A\n[65\u20137 ] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f l wing section wil pr sent th s lutions fo d in the liter t re orga ized rou d t e nsor mpl ye . Sp cifi ally, the p e e tation i ganized around the crie ia of being cam r or n -ca ra a sist d as ifferent ets f t c iqu r r quir d for th se two c ses. F r e ch of th s wo c te i s, we co tinu by g uping the p ovid d solu ons bas d n th type of s s r with th most frequent group being te first. F r b c s , t r fu i t chniqu re desc b in co reh sibl man er. I m r d t l, Secti 3.1.1 v rs p pers u ilizing a f the camera s r, be it a single c me r a o figur i n of mult le c meras, h dev ces s w ll as 3D\nme sen or . Sub qu tly, pap r utilizi th IMU e r (acc lerom ter, gy os ope, magnetometer), the most ular s nsor among the s lect d papers, are presented. For ea h indiv du l paper, a ummary incorp rating th so ution with the advantages and dis dvantag s is itt . O th ther ha , Secti 3.1.2 concer s s l tio s th no cam era sensor . Most of m incor orate an IMU nsor while other options include ultrasoni and Lid r sensors as w ll Bluetooth Low En rgy (BLE) b a o s and Wi-Fi access points.\n3.1.1. Camer -Assi ted Sol tion An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odometry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat sti ati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is exp cted given he prolife ation of smartphone devices as the preferre platform f r dev lop g olutions for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 29% of them are pract c l bu hav a co bination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / / / N/A [49] \u2716 \u2716 \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe l wi g s cti s wi l pr s t the s lutions found in the literature organized aroun t se o e y . Spe ifically, p es nt tion i ganized aro nd the criof b in c mer r non-camera assist d as different ts of t chniqu r required f those two cas s. For ac of th s two categ ri s, w conti ue by grouping the provided s l tions ba o the ype of sor with he most frequent roup being te first. For b th c s s, the se f si techniques desc ibed i c mprehensible manner. I e et il, Secti n 3.1.1 cov rs th per u i izing a for f the camera s nsor, be it a sin r c figuration of multipl am ras, h set dev ces as w ll as 3D c m r nsor . Sub que l , pap rs utilizi the IMU sens r (accelerometer, gy os ope,\nagnet me er), th mo popular s nsor among the sel cted papers, are presented. For ach i d vidual pape , s m a y incorpora ing the solu ion with the advantages and di a v nt g s s wr tt n. On the other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sens r . M t f th m ncorp rate an IMU s nsor while ther options include ultraso and Lida sens rs as ell a Blueto th Low Energy (BLE) beaco s and Wi-Fi access poin s.\n3.1.1. C mera-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nSearch places\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected give the pro iferation f s art h ne devices as the referred platform for developing solutions for the BVI individuals. Finally, only 14% of these s lutions can be used in real-lif scenari s with some degree f success, 29% of the are practical but have a com in tion f either hig cost r require from th ser to carry many sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy [41\u2013 7] \u2716 \u2716 \u2716 \u2716\n[48] / / [49] \u2716 \u2716 \u2716 [50] / / N/A [51] N/A N/A 5 m 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u2013 5 [54] / / N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A [58] 2 c < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u2013 3] [64] R = 20 m N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will pr sent the s lut ns fou d i t e lit ature organized a ound the sensors empl yed. S ecific lly, the p e e t o is anized ar un th cri teria of being camera or n n-c mera as is e as diff e t et of te h ique e requ red for those two cases. For each of thes two cat g ries, we co tinue by grouping the provided solutions bas d on t e type of sensor with t ost f equent grou being te first. For both c s s, the ens r f si techniqu s are esc ibed i a comp ehen i le ma ner. I more detail, Secti n 3.1.1 covers the p pers utilizi g f r f the camer se sor, be it a single c mer r a co figuration f ulti l ca eras, he set d vices as ell as 3D camer sensor . Sub que tl , papers utilizing the I U ( cel ro eter, yros p ,\nagnetometer), t e most popular sensor am ng the selected p pers, are presented. F r each individual paper, a summary incorp rating the solution with t e advant ges a d disadvant g s is written. O the other han , S ti n 3.1.2 c ncerns sol tio s th cam era nsor . Mos of them i c r ate an IMU s s r whil othe ptions i clude ltrasoni and Lidar sensors s well a Blueto th L w Energy (BLE) be c nd Wi-Fi acce s points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand camer sensors is ex ected give the proliferati of smartp one devices as t e referr d latform for devel ping solu ns f r the BVI i dividuals. Finally, only 14% of these sol ti ns ca be used in real-life scenarios with some degree of success, 29% of them are practic l bu have a c mbina io of either high cos r req ire fr m t e user to carry many sensors, 24% re limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support Static Dyn mic Range Accuracy [41\u201347] \u2716\n48 49 \u2716 0 N/A 1 / 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 9 \u201395 [54] N/A / 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 / 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 c N/A [65\u201371] \u2716 \u2716\n[72] N/A [73] \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716\nThe f llowing c ions will pr e t the s l ti n f u d in lit rature rga zed und the sens s mploye S ecific l y, the pre entatio is organized aroun the c i teri f bei g c mer r non-cam r ssi ed s ff t ets of te h iq s are required for thos t cases. For e c f t e e tw cat gories, we cont e by rouping the pr vided soluti ns based on the type f sens with he ost frequent group being pre ent first. For oth ca s, the ensor f si tec nique re sc ib d in a c p ehen ible ma ner. I ore d tail, Sec i n 3.1.1 covers the p pe s utilizing a f r f he cam a sensor, be it single c m r a configuratio f ultipl ca era , he s t d vices s well as 3D cam r sensor . Sub q e tl , papers utilizi g t e IMU n r (accelerom t , yro cop ,\nagnetometer), t m st pular sensor a g the selected papers, are prese te . For e ch individ al p per, summary incor orati g the soluti n with the a vantages and disa va t g s is written. O th t er h nd, Sectio 3.1.2 c cer s s l ti w th o cam era sensor . Mo f h m incorp ra a IMU s r while other opti s include ltrasoni a d Lidar s nsors s w ll as Bluetoot Low En gy (BLE) be a d Wi-Fi acc ss points.\n3.1.1. Cam r -Assist d Soluti ns An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degr ding states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sens rs is exp ct d given th p olif rati of smar phon devices as the preferred platform for developi g so uti s for the BVI i divi uals. Finally, only 14% of these sol tions can be used in real-life sce arios with som degree of success, 29% of them are practical but av c mbination f he igh cost or req ire f o the user to carry many sensors, 24% r limited pra ticality f r s ecific scenarios while 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dyn mic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / / / [49] \u2716 \u2716 [50] N/A N/A N/A / [51] / N/A 5 m 95% [52] 0 < 9 m 98% [53] \u2716 0.1 m < R 3.5 m 90\u201395 [54] N/A N/A N/A / [55] / N/A 0.2 m < R 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 m < R 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R 12 /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe f l owi g io ill pr e t th solution fou i the literature organiz d ou d t e se s rs employ d. S cific ly, e n t n is orga zed arou the cri teri f bei g camera r n-c ra s is ed as d ff e s t of te h ques ar require for those two cas s. F r each of th s two ca g ries, w conti ue by grouping the provided solutions ba d n th typ f sens with t e ost frequ nt gr u bei p sented first. For b t c s , t ns f si techn ques re e c ibed in a c rehensibl an er. I more det il, Secti n 3.1.1 c v rs the p p r u ilizi a f r f the c mera sens r, be it single c er r a c nfigur ti f ltipl c ras, h set evices a el as 3D camera sen or . Sub q e tl , p p utilizi g the IMU ns r (ac eler met r, roscope,\nag etometer), t e ost p p lar nso amo g the selecte papers, are prese ted. For each indivi al p pe , a summary incorporati solution with the a v ntages and d s va tag s is written. O the other a d, Secti 3.1.2 c nc r s sol tio s w th n cam ra nsor . Most f them i corp at IMU s sor whil other pti s i clude ultrasoni d Li r s ns r as well s Blu to th Low E ergy ( LE) beaco s a d Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutions A ncertai ty-based adaptive se sor usion fr ework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s tes an arise u der motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 29\nand ca ra se sors is xpec ed giv the proliferati of sm rtphone devices as the preferre platfor for developing solutions for the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-life c narios with some degree of success, 29% of them are pract cal but ave a c mb nati n of ei her high cost or req ire f om the user to carry m n se sors, 24% re limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support St tic Dy amic Range Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A / [49] \u2716 \u2716 [50] N/A N/A N/A / [51] / N/A m 95% [52] 0 < < 9 m 98% [53] \u2716 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A / [55] / N/A 0.2 m < R < 10 m N/A [56] \u2716 R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f l wi g sect ns w ll pr se t the s lutio s fo d in the literature orga ized a und t e se sors mpl yed. S cific lly, the pre entation is or nized aroun the cri t i of bei g amer r n -c m a a is ed as diff ent set of te h iques are required for th s t c ses. F e ch of thes two categor es, we conti ue by gr uping the provided sol ions b s d on th ype of se s r with the most frequent group being presented first. For b th s , t r fusi techniqu s re desc ibed in c preh nsibl m nner. In r det il, Secti 3.1.1 v rs p p s il zing a f r f the camera sensor, b it a si gl camera r co figur ti n of u tiple cam ras, hea s t devices s well as 3D ca era senso . Sub q e tl , p er utilizi th IMU e r (accelerom ter, gyroscope,\nagnet meter), t e ost lar s ns r am ng the s lect d papers, re presented. For ea h individ l p per, summ ry incorp rati g the solution with the a vantages and di advantage is w itt . On t ther hand, Section 3.1.2 c nc r s solution with no c mra ns rs. Most f th inc r rat an IMU se so whil t er ptions include ultraso a d Lid r ens rs s w ll B etooth L w En rgy (BLE) b a o s nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutions An ncertai ty-based adaptive s ns r fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed i [41] for stimating relat ve motion. It minimizes degradation from inaccurat stimati by determining the states th t should b included in the estimati pr cess. The e degr din states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is expected given the prolife ati of smartphone devices as the preferred pl tf rm f r e elop g olutions for the BVI i dividuals. Finally, only 14% of these sol tions an be use in r al-lif sc narios with some degree of success, 29% of them are pract c l bu hav a c mbin tion of ei her high cost or eq ire from the user to carry many sen rs, 24 re limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa e Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A / [49] \u2716 \u2716 [50] / N/A / [51] / 5 m 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] / N A N/A / [55] / N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nT fol owi g s ct o wi l pr e t the solutions found in the literature organized a und the ensors empl y . S cific lly, p e entation is org nized aroun the cri b i g c mer r n n- a era assis ed s diff e t set of t h iques are required f t os t c ses. Fo ach of th se two cate o ies, w conti ue by grouping the provided s luti s ba on the ype of s s r with he most frequent group being presented first. For th c s s, the n o f si tec niques a d c ibed in co prehensible manner. I m r det il, Secti n 3.1.1 overs the p p utilizing a f r of the camera sensor, be it sin or a configuration of mu tipl a ras, he set devices as well as 3D cam ra sens r . Sub que tl , p per utilizi g the IMU sens r (accelerometer, gyroscope,\nagn t e er), t mos po ular s nsor among the sel cted papers, are presented. For ach ind vidual p per, su mary incorpora i g the solu ion with the a vantages and di a va tages s written. O th th r hand, Secti 3.1.2 c ncerns solutions with no camera s rs. M st f them nc rp r te an IMU se s r whil other ptions include ultraso ic a d Li ar s s rs as ll a Bluetooth Low Energy (BLE) beacons and Wi-Fi access p in s.\n3.1.1. Camera-Assisted S luti ns An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is propose i [41] for es ating relat v motion. I minimizes degradation from acc rate state estim tion by determining the states that should be included in the estimation proce s. The e degr d g sta es can arise under motion characteristics that nullify\nDifferent input modes\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the pro iferation of smartphone devices s the preferred latform for developing solu ons for the BVI individuals. Finally, only 14% f hese solutions can be used in real-lif scenarios with some degree f success, 29% of the are practical but have a com ination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy 4 47\n48 49 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] / / / / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 m N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will pr sent th s l t s fou d i th lit ature rg iz d a ound the sensors empl yed. S ecific ly, the p e ent o is ganized aroun th cri teria of bei cam ra or n- r as is e as diff e t et of t h ique a e requ r d for thos two case For each of thes two ca g ri s, we co tinu by rouping th provided solutions bas d on the type of sensor with th st f equ t g ou b ing te first. For both c s s, th ensor f si techniqu s are esc ibed in a comp e n i le ma ner. I more detail, Secti 3.1.1 covers t e p pers utilizi g f r f the am r e sor, be it single c m r co figuration of ulti l ameras, ea et d vic s a well as 3D camer se sor . Sub que tl , paper utilizing the IMU ( cel ro eter, yros op ,\nagnetometer), t e m st popular sensor am ng the selected papers, are presented. For each individual paper, a summary incorporating the solution with the advantages and disadvantag s is written. On the ther hand, Section 3.1.2 c ncerns sol tio s w th n ca era sensor . Most of them incorporate an IMU s nsor while ther options i clude ltrasoni and Lidar sensors as well as Bluetooth Low Energy (BLE) beac n Wi-Fi acce s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given h proliferation of sm rtphone devices as the preferred platform for dev lopi g solutions for the BVI individuals. Finally, only 4% of these soluti ns c n be used i real-life sce ario with some degree of success, 9% of them are practical but have a combination of either high cost or require fro the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [ 1\u201347] \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 [50] N/A N/A / / [51] N/A / 5 m 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A / / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 c < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll ing sections will p sent the sol ons found in th lite tu e r niz d r u d the sensors p oyed. Specifically, th pr s nt ti n i organ z ar u d the crit ria of being era or n n-camera ssist d s differ nt s t f techniques a requi ed for thos two cases. For ach of se two cat g ries, w ontinue by rou i g th p ovid d solution b sed o the type of se sor with h ost freque group being presented fi t. For bo h c s , h se sor fus t c iques re de cri ed i ompr h ibl ma ner. I more detail, Sectio 3.1.1 cov rs the aper util zing a form f he m s s r, b it a singl c mera r a configu atio f multipl ca eras, headset dev ces s w ll 3D camera s n ors. Subs que tly, p p rs utilizi t IMU ns r (ac elerom te , gy oscop , magnetometer), the mo t popular sens among the selected papers, are presented. For each individual paper, summary incorp rati g the soluti n with the adva tages and disadvantag s is written. On th other hand, S cti n 3.1.2 c ncerns sol io s w th n ca era sensor . Most of them incorporat a IMU s n or while other o ns clud ultrasoni and Lidar sensors s well as Bluetooth Low En gy (BLE) eaco and Wi-Fi acc ss points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det rmi ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expec d gi en he p oliferation of smar phon devices s the preferred platform for developing solutions for th BVI individuals. Fi ally, nly 14% of these solutions can be used in real-lif scenarios with some d g ee of success, 29% of them are practical but hav a combinatio f eithe high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy 4 47\n48 / / / [49] 50 N/A N/A 51 N/A N/A 5 m 95% 52 0 < < 9 m 98%\n[53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 c < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh f ll wing s c i will pr s t th s lutio f d in t l teratur org nized r und the se s s empl y d. Specifica ly, the p s nt ti i ga zed around the criteria of being camera or n -ca a s is as diff e t ets of t chniqu ar r quir d f r hose two ca s. For each of th two categ rie , we continu by grouping he r vid d\nlutions ba d n the typ of s nsor wit the mo t f qu nt g o p being te first. For bo h c es, s sor fusio t ch iq s re described in a c mpr hens b m ner I m e detail, S cti n 3.1.1 cove the pa ers ut lizing a or f the ca era s ns r, be it ingl ca era r co figur ti n f mul iple c m ras, set d vices ll 3D camera sensors. Subseque tly, pap r utilizi th IMU s sor (ac eleromete , gy os ope, m gneto eter), the most popular nsor among the selected p pers, are presented. For ach individual p pe , a summary incorporating sol ti with the advantage and d sadvantag s is writt n. On the other h d, Secti 3.1.2 c nc rns sol tio w th no cam ra nsor . M st f them incorp ate an IMU s ns r while oth r ptions include ultrasoni d Li r e sors as well as Bluetoot Low E rgy (BLE) beaco s and Wi-Fi access points.\n3.1.1. Camera-Assisted S lutions An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est m tion by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 9\nand cam ra sensors is xpected giv n the p oliferation of sm rtphone devices as the preferred latform for d velopi g solu ons for the BVI individuals. Finally, only 14% of these solutions can b us d in real-life c narios wi h some degree of su cess, 29% of them are practical but h ve a comb nati n of eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range A curacy 4 47\n48 / 49 0 N/A N/A 1 N/A N/A m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 m / [61\u201363]\n[64] \u2716 R = 20 m N/A [65\u20137 ] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f l wing section wil pr sent th s lu ions fo d i the lit re orga iz d rou d t e ens r mp oye . Sp cifi ally, the p e e tation i rganized aro nd the cri-\nia of bein r or n n-ca ra a sist d a ifferent sets f tec iques ar requir d for t se two c ses. F r e ch of se wo c teg ies, w co tinue by gr uping the p ovid d solu ions based n th type of s nsor with th most frequen group be g presented first. F r b c s , t e se sor fu o t chniq e re de cr b in co re nsibl manner. I m r d t l, Secti 3.1.1 v rs paper ilizing a of he cam a s s r, be it a single c mera r a co figur io of mult le c me as, head dev ces as w ll a 3D\nme en or . Subsequ ntly, pap s utilizi th IMU se sor ( cc l rometer, gy oscope, magnetometer), the most ular s nsor amo g the s lect d papers, are presented. For ea h individu l paper, a ummary incorp rating the solution with the advantages and dis vantag s s itte . O the other h nd, Section 3.1.2 c ncer s s l tio s th no cam era sensor . Most of th incor orate an IMU s nsor while other options include ultraso and Li sens rs as w ll s B etooth Low En rgy (BLE) b a o s and Wi-Fi access points.\n3.1. . Camera-Assisted S lutions An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odometry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat stimati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca ra sensors is exp cted given the prolife ation of smartphone devices as the preferred latform f r dev lop g olu ons for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 29% of them are pract c l bu hav a co bination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic R nge Accuracy 4 47\n48 / 49 0 N/A N/A 1 N/A N/A 5 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 / [61\u201363]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh o wi g s cti s wi l pr s t the s ut o s found in the literature organiz d aroun t e ors e p y . Spe ifically, p esentation i organized aro nd the crie of b in r r non-camera assist d s different s ts of techniques r r quired f those two c s . For ach of se two categ ri s, w conti ue by groupi g the p ovid d s lutions ba d on th y e of sor with he st frequen roup being presented first. F b th c ses, the se f si t ch iques r described i a c prehensibl manner. I e et il, S ti 3.1.1 c v rs th papers u ilizing a form of the cam a s nsor, be it a sin r c figuration of multipl cameras, he dset ev ces s w ll a 3D c mer ensor . Subseque l , pap rs utilizi the IMU sensor (accelerometer, gy oscope,\nagnet me er), th mo popular s nsor among the sel cted papers, are presented. For ch ind vidual paper, summary incorpora ing the solu ion with the advantages and di a v nt g s s writt . On th other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sensor . M st of them ncorp rate an IMU s nsor while ther options include ultras nd Lida se s rs as ll a Blueto th Low Energy (BLE) beaco s and Wi-Fi access p ints.\n3. .1. Ca era-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by determining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nAudio menu\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected give the proliferation f s art h ne devices s the referred latform for developing solu ons for the BVI individuals. Finally, only 14% f hese solutions can be used in real-lif scenari s with some degree f success, 29% of the are practical but have a com in tion f either hig cost r require from th user to carry any sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy 4 47\n48 / 49 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u2013 5 [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < 12 m / [61\u2013 3]\n[64] R = 20 m N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following secti ns will pr s nt the s l t s fou d i t lit ature organized a ound the sensors empl yed. S ecific ly, the p e t io is o g nized ar un th cri teria of being camera or n -c mer as is e as diff e t et of t h iqu e requ red for those two cases. For each of thes two categ ries, we co tinue by rouping th provided solutions bas d on t e type of sensor with t e ost frequ t g oup being te first. For both cas s, the ens r f si techniqu s are desc ibed i a c mp ehen i le ma ner. I more detail, Secti n 3.1.1 covers the p pers utilizi g a f r f the c mer se s r, be it a single c mer r co figurati n f multi l ameras, he set d vic s a ell as 3D camer sensor . Sub que tl , papers utilizing the I U s r (a celero eter, yr s p ,\nagnetometer), t e m st popular sensor am ng the selected p pers, are presented. F r each individual paper, a summary incorp rating the solution with t e advant ges a d disadvant g s is written. O the other han , S ction 3.1.2 c ce ns sol ti s th o cam era sensor . Mos of them i c r ate an IMU s nsor while other tions inclu e ultrasoni and Lidar sensors s well a Blu to th L w Energy (BLE) be co nd Wi-Fi acc s points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of sm rtphone devices as the preferred latform for dev loping solu ons for the BVI individuals. Finally, only 14% of th se s luti ns c n be used i real-lif sce rio with s me degree of success, 9% of the are practical but have a combination of either high cost or require fro the user to carry many sensors, 24% are li ited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [ 1\u201347] \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 0 / N/A 1 N/A N/A 5 m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 c < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll wing sections will p esent the s l o s found i th lite atu rg n zed r u d the sensors empl yed. Specifically, th p es tati n i gan z arou d th crit ri of being ca era or n -camera ssisted as different ts f t c nique a require for thos t o cases. For ach of t se two cat g ries, w ontinue by rou ing th provid d solution b s d o the type of se sor with h ost freque t group being nte fi t. For bo h cas s, he se sor fusi t c niques re descri ed in ompr h ible ma ner. In more detail, Sectio 3.1.1 covers the aper util zing a form f e m s ns r, b it a singl camera or a co figu atio f multipl ca eras, headset dev c s s w ll 3D camer s n ors. Subs que tly, p p r utilizi t IMU n r (ac elerom te , gy os op , magnetometer), the mo t popular senso among the selected papers, are presented. For each individual paper, summary incorpora g the soluti n with the advant g s nd disadvantages is written. n th other hand, S cti n 3.1.2 c nce ns solut ons with n camera sensors. Most of th m incorpor a IMU sen or whil o her opt ns clu ultrasonic and Lidar sensors s well as Blu tooth Low En gy (BLE) beacon nd Wi-Fi acc points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det r i ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expect d gi en the p oliferation of smar phon devices as the preferred latform for developi g solu ons for th BVI individuals. Fi ally, nly 14% of these solutions can be used in real-l f cenarios with some deg ee of success, 9% of the are practical but hav a combinatio f eithe high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716\n48 / 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f ll wing sec i ill pre t the s lutio s f d i the l terature rg nized around the se s rs empl y d. Specifically, the p es nt ti is ga zed arou d the criteria of being c era or non-ca a as isted as differe t ets of t chniqu ar r quir d f r h se two ca s. For ach of th s tw categ ries, we c tinu by grouping he provided olutions ba d n the typ of sensor with the mo t f quent g p being nte first. For both c es, he s sor fusio t chniq es re desc ibed in a c mprehen b m ner. I mo e detail, S ction 3.1.1 cove s the p ers ut lizi g a or f the ca era s ns r, be it ingl ca era r a c figur ti n f ul iple ca r s, a set devices ll 3D ca era sensor . Subseque tly, pap rs utilizi th IMU s sor (ac eleromete , gy os ope, m gneto eter), the most popular nsor among the selected p pers, are presented. For ach individual p pe , a summary incorp rating olutio with the advantag and d sadvan ag is writt n. On he other ha d, Secti 3.1.2 c ncerns sol tio s w th no cam era nsor . M st f them incorp ate n IMU nsor whi e other optio s nclude ultrasoni nd Lid r enso s as well as Bluetoot Low E rgy (BLE) beaco s d Wi-F access points.\n3.1.1. Camera-Assisted Soluti s An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est mation by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 9\nand c m ra sensors is xpected giv n the p oliferation of sm rtphone devices as the preferred latfor for d veloping solu ons for the BVI individuals. Finally, only 14% of these solutions can b us d in real-lif c narios with some degree of success, 9% of the are practical but have a comb nati n of eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716\n48 N/A 49 \u2716 \u2716 0 / / 1 N/A N/A m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 m N/A [65\u20137 ] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f l wi g sections will presen th solu ions fo i the lit re orga ized arou d t e sens r pl ye . Specifi ally, the p e e tation is ganized aro nd the cri-\nia of b in m or n n-ca ra a si t d a ifferent ets of t chniqu are required for th se t o c ses. F r e ch of thes two categ ies, w conti e b gr uping the provided solu ions bas d n th type of s nsor with th most frequent group be g te first. F r b th cas , the e sor fusio t chniq e re de cribed in co re nsible manner. I m r d ta l, Secti n 3.1.1 vers h paper ilizing a f he camera s s r, b it a single c mera a co figur ion of mult le c me as, heads dev ces as w ll as 3D\nme en or . Subsequ ntly, pap s utilizi th IMU se sor ( cc l rometer, gy os ope, magnetometer), the most ular s nsor among the s lect d papers, are presented. For ea h indiv du l paper, a ummary incorp ra ng th so ution wi h the advantages and dis vantages s itte . O the ther h d, S ction 3.1.2 concer s olutio s with no camera sensors. Most of m incor or e an IMU s nsor whil o her optio s include ultraso ic and Li sens rs as w ll s Blu tooth Low Energy (BLE) b a ons and Wi-Fi acce s points.\n3.1.1. Camer -Assi ted Sol tio An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odo etry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat sti ati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is exp cted given the prolife ation of smartphone devices as the preferred latform f r dev lop g olu ons for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 9% of the are pract c l bu hav a co bination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh ollo i g secti s wi l pres t the solutio s found in the literature organiz d aroun t e o emp oy . Spe ifically, p es nt tion i organized around the crie of b in r r non-camera as ist d different sets of techniques are required f those t o case . For ach f se two categories, w continue by grouping the p ovid d s lutions ba d o th y e of sor with he st frequen group being presented first. F b th c ses, the s o f s t ch iques r described i co prehensibl manner. I re et il, S ti 3.1.1 c v rs th papers u ilizing a form of the cam a s nsor, be it a sin or co figuration of multipl cameras, h adset dev ces as w ll a 3D c mer ns r . Subseque l , pap rs utilizi the IMU sensor (accelerometer, gy oscope,\nagnet me er), th mo popular s nsor among the sel cted papers, are presented. For ach i d vidual pape , a s ma y incorpora ng the solu ion with the advantages and di a vantag s is wr tt . On the other hand, S cti 3.1.2 concerns solutions with no camera sensors. M t f th m ncorp ra e an IMU sensor whil o her optio s include ultras ic and Lidar sens rs as ell a Blu tooth Low Energy (BLE) beacons and Wi-Fi acce s points.\n3.1.1. C mera-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nPOIs\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected give the pro iferation f s art h ne devices as the referred l tform for developing solu ons for the BVI individuals. Finally, only 14% f hese solutions can be used in real-lif scenari s with some degree f success, 29% of the are practical but have a com in tion f either hig cost r require from th ser to carry any sensors, 24% are limited practicality for specific scen rios while 32 are purely experimental.\nTabl 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy 4 47\n48 49 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 8%\n53 0.1 3.5 90\u2013 5 [54] / / N/A / 55 N/A N/A 0.2 m < R < 10\n[5 ] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 6 \u201398%\n[60] 2 cm < R < 12 / [61\u2013 3]\n[64] R = 20 N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follow g secti ns w ll pre t t s l ti fou in t lit ratu e rg ized around the sens s empl ye . S cifica ly, th p e e tat on o g ize u d th r - teria of b i camera o n -c m as sted as diff re t t of ch iq e r quired for hos two case For each of the wo c g ri s, w conti ue by rouping th p vi d solutions bas d on the type of sensor with t st f equ t g ou b ing te fi st. For both c s s, the sens r fusion techniqu s are esc ibed i a c mp ehen i le ma ner. I more detail, Secti 3.1.1 cover t p pers utilizi g a for f the c mer se sor, be it single c mer r co figuration f multipl ameras, e set d vices as ell as 3D camer se sor . Sub quently, papers utilizing the I U ( cel rometer, yr s p , magnetometer), the m st popular sensor among the selected p pers, are presented. F r each individual paper, a summary incorp rating the solution with t e advant ges a d disadvant g s is written. On the ther hand, S ction 3.1.2 c ncerns sol tio s th o cam era sensor . Most of them i c rp ate an IMU s nsor while ther options include ltrasoni and Lidar sensors as well a Bluetooth L w Energy (BLE) beac s an Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand camera sensors is ex ected give the proliferati of smartp one devices as t e referr d latform for devel ing solu ns f r the BVI i dividuals. Finally, only 4% of these sol ti ns ca be used in real-life scenarios with so e degree of success, 29% of the are practic l bu have a c mbina io of either high cos r req ire fr m t e user to carry many sensors, 24% re limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support Static Dyn mic Range Accuracy 4 47\n48 49 0 / 1 / / 5 95% 2 0 m < R < 9 m 98%\n53 0.1 3.5 9 \u201395% [54] N/A N/A 55 / N/A 0.2 m < < 10 m\n[56] R > 2 m N/A 57 58 2 cm < R < 4.5 m N/A 59 67\u201398%\n[60] 2 cm < R < 12 N/A [61\u201363] \u2716\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716\n[72] N/A [73] \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716\nTh f llow c will p t the s lut o s f u in th lit r u rgan zed und the sens r mploye . S cifi l , the pre ta ion is ganize a ou th c i teri f b i g c m r non- am a si d s iff e t et of t q ar req ir d for h s t cases. Fo e f t e w c t g ries, w conti e by r upi g t p vided soluti ns based on the type f se s with he ost frequ nt group being present first. For oth ca s, the ensor f si tec nique are sc ib d in c p ehensibl ma ner. I ore d tail, Sec i n 3.1.1 ver the p p s utilizing a f r f he camer se s r, be it single cam or a configuratio of mu tipl camer , he s t d vices s well as 3D cam r sensor . Sub quentl , papers utilizi g e IMU ns r ccelerom t , gyro cop ,\nagnetometer), t m st p pular sensor a g the selected papers, are prese te . For e ch individ al p per, summary incor orati g the soluti n with the a vantages and di a vant ges is written. O the t er hand, Sectio 3.1.2 c cer s s lu i s with no ca - era sensors. Mo f h m incorp ra e an IMU se s r while other o tio s include ltrasonic a d Lidar s nsors as w ll as Bluetoot Low Energy (BLE) be ns a d Wi-Fi access points.\n3.1.1. Cam r -Assist d Soluti ns An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degr ding states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sens rs is expect d given th p olif rati of smar phon devices as the preferred latfor for developi g so uti s for th BVI indivi uals. Fin ll , only 14% of these sol tions can be used in real-life sce arios with som degree of success, 29% of them are practical but av c mbination f he igh cost or req ire f o the user to carry many sensors, 24% r limited pra ticality f r s ecific scenarios while 32 are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dyn mic Range Accuracy 4 47\n48 N/A N/A [49] 50 N/A N/A 51 / N/A 5 m 95% 52 0 m < R 9 m 98%\n[53] 0.1 m < R 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] / N/A 0.2 m < 10 N/A [56] \u2716 R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 cm < R 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe fol o i g s c ill re t he solu ions foun i the l eratur organiz d ar und s so s employ . S fic ly, e nt t n is org zed ar und the c ieri of b i g camer n n-c m ra s ist as differe t s t of technique are required for thos two cases. F each f th tw ca eg rie , conti by groupi g the provide soluti ns ba d n th typ f se s wit t o t f equ nt gro being p sented first. For b t c s , t s ns f si tec n ques e e c ibe in co prehensibl man er. I more det il, Sec i n 3.1.1 v rs the p p utilizi a for f the c mera sens r, be it si gle c mer r a c nfigur ti f ltipl c m ras, he set evices a wel as 3D camera sen or . Sub que tly, p p utilizi g the IMU ns r (ac eleromet r, gyroscope, mag etometer), the most pop lar nso a ong the selected papers, are presented. For each indivi al p pe , a summary incorporatin solution with the a v ntages and d a va tag s is written. O the other a d, Secti 3.1.2 c nc r s solution with n camera s rs. Most f them i corp at a IMU se s r whil other pti ns include ultrasonic d Li r s nsor as well as Bluetooth Low E ergy ( LE) beacons a d Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutio s A ncertai ty-based adaptive se sor usion fr ework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s tes an arise u der motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 9\nand ca ra se sors is xpec ed giv the proliferati of sm rtphone devices as the preferre latfor for develo ing solu ons for the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-life c narios with some degree of success, 29% of them are pract cal but ave a c mb nati n of ei her high cost or req ire f om the user to carry m n se sors, 24% re limited practicality for specific scenarios whil 32 are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support St tic Dy amic Range A curacy 4 47\n48 N/A 49 0 N/A N/A 1 / N/A m 95% 2 0 m < R < 9 m 98%\n53 0.1 3.5 90\u201395% [54] N/A N/A N/A N/A 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m N/A 57 58 2 cm < R < 4.5 m N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f l wi g sect ns will rese the s lutio s fo n in the literature organized ar un t s s mpl ye . S cific lly, t e p e ntation is org ized around the crit ia of bei g am r r n-c m ra a s sted as iff re t s t of t c iques are required for th t c s . Fo e ch f the e tw categor es, w co ti ue by grouping the provided sol ions b d n th typ of s s with th most f quent group being presented first. For b th c s , t r f si tech iques re desc ibed in co preh sibl manner. In r detail, Sect o 3.1.1 v rs th p p s u ilizing a f r f the camera sensor, b it a si gl cam ra co figur tion of mu tiple c meras, he s t devices s well as 3D ca era senso . Sub q e tly, p er utilizi th IMU en r (accelerom ter, gyroscope, magnet meter), the most ular s nsor am ng the s lect d papers, re presented. For ea h individ l p per, su m ry incorp rati g the solution with the a vantages and di dvantage is w itt . O t e other hand, Section 3.1.2 c ncer s s lutions ith no camra sens rs. M st f th m inc r orat an IMU senso while ot er ptions include ultrasonic a d Lid r ens rs s w ll a Bluetooth L w E rgy (BLE) bea ons nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutions An ncertai ty-based adaptive s ns r fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed i [41] for stimating relat ve motion. It minimizes degradation from inaccurat stimati by determining the states th t should b included in the estimati pr cess. The e degr din states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is expected given the prolife ati of smartphone devices as the preferred l tf m f r e elop g olu ons for the BVI i dividuals. Finally, only 14% of these sol tions an be use in r al-lif sc narios with some degree of success, 29% of them are pract c l bu hav a c bin tion of ei her high cost or eq ire from the user to carry many sen rs, 24 re limited practicality fo specific scenarios while 32 are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa e Support Static Dynamic R nge Accuracy 4 47\n48 / 49 0 N/A N/A 1 / N/A 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A N/A 55 / / 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nThe fol wi g secti ns wi l pr se t the s lutio s foun i the literature organized ar u d th s empl y . Specifically, p e entation is orga ized around the crier b i g e or non-ca e as isted as differe t s ts of techniques are required f t e t s s. For ach of th two cate es, w conti ue by grouping the provided s luti s bas on th ype of s s with h most f equent group being presented first. For th c s , t e s n o fusio techniques a d c ibed in a co prehensible manner. I mor det il, Secti 3.1.1 covers the p per utilizing a for of the camera sensor, be it sin r a configuration of multipl a ras, he set devices as well as 3D cam ra sens r . Sub que tly, p p r utilizi g the IMU sens r (accelerometer, gyroscope,\nagn t e er), th mos po ular s nsor among the sel cted papers, are presented. For ach ind vidual p per, su mary incorpora i g the solu ion with the a vantages and disa va tag s s written. O the th r hand, Sectio 3.1.2 c cerns sol tio s w th no cam era se s r . M st f them inc rp r te an IMU s nsor while other options include ultras ni and Li ar se s rs as ell a Bluetooth Low Energy (BLE) beaco s and Wi-Fi access poin s.\n3.1.1. Cam ra-Assisted S luti ns An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is propose i [41] for es ating relat v motion. I minimizes degradation from acc rate state estim tion by determining the states that should be included in the estimation proce s. The e degr d g sta es can arise under motion characteristics that nullify\nAdjustable radius\u2014POIs\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected given the pro iferation of smartphone devices s the preferred latform for developing solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-lif scenarios with some egree f suc ess, 29% of the are practical but have a combination of either high cost or require from the user to carry any sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy 4 47\n48 / 49 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 8%\n53 0.1 3.5 90\u201395 [54] / / / / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 m / 59 N/A 6 \u201398%\n[60] 2 c < 12 / [61\u201363]\n[64] R = 20 N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follow g secti ns will pr t t s l fou d i th lit at g iz a ound the sens s empl yed. S ecific l , the p nta o o g iz a u h r teria of b i ca ra o n- s s e as diff e t et of h iq a e r qui d for hos two case For each of the wo c g ri s, w continu by grouping p vid solutions bas d on the type of sensor with th st f equ t g ou b ing te first. For both cases, th ensor f si techniqu s are escribed in co pre nsible manner. I more detail, S cti 3.1.1 cove s t e p pers ut lizi g a f rm f the m r e or, be it si gle c m r co figuration of ul i l amer s, e et devic s w ll as 3D camer se sors. Subseque tl , paper utilizing th IMU ( ccel ro ter, gyr s op ,\nagnetometer), t e m s po ular ensor am ng t e el cted papers, are p nted. Fo each individual paper, a summary incorporating the solution with the advantages and disadvantag s is written. On the ther hand, S ction 3.1.2 c nce ns sol tio s w th n cam era sensor . Most of them incorpo ate an IMU s nsor while ther options i clu e ltrasoni and Lidar sensors as well a Blu tooth L w Energy (BLE) beac s an Wi-Fi acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odo etry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of sm rtphone devices as the preferred latform for dev lopi g solu ons f r the BVI individuals. Finally, only 1 % of th se s luti ns c n be used i real-life scenarios with some degree of success, 9% of them are practical but have a combination of either high cost or require fro the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic ange Accuracy [ 1\u201347] \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 0 N/A N/A 1 N/A N/A 5 m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 N/A N/A 0.2 < R < 10\n[56] R > 2 m / 57 58 2 c < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 c < R < 12 / [61\u201363]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follo ing sections will p ese t the s l o s fou d i lite atu e g z d r u d the sensors p oyed. Sp cifically, th es t ti n i organ z ar u d th crit ria of b ing era o n -cam ra ssisted iffer t t f tec niq s qui e for thos two cases. For ach of se two cat g ries, w ont ue by rou i g th p ovid d solution b sed o the type of se sor with h ost freque group being presented fir t. For o h cas , h se sor fus t c ique re sc d i compr h ibl mann r. I more d tail, Secti n 3.1.1 covers the ape util zing a form f he m s s r, b it singl c m a r a configu atio f multipl camer , h d t d v ces s w ll 3D cam ra s n ors. Subs quently, p p rs utilizi t IMU nsor ( c elerom t r, y o cope, magnetometer), the most popular sens among the selected papers, are presented. For each individual paper, summary incorpo ti g th soluti n with the adva tages and disadvantag s is written. n th other hand, S cti n 3.1.2 c ncerns sol io s w th n cam era sensor . Most of them incorporat a IMU s n or while other o ns clud ultrasoni and Lidar sensors s well as Bluetooth Low En gy (BLE) eaco and Wi-Fi acc ss points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det rmi ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expect d gi en the p oliferation of smar phon devices as the preferred latform for developi g solu ons for the BVI individuals. Fi ally, nly 14% of these solutions can be used in real-life scenarios with some deg ee of success, 9% of them are practical but hav a combinatio f eithe high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716\n48 / 49 \u2716 \u2716 \u2716 0 N/A N/A 1 N/A N/A 5 m 95% 2 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 / 57 58 2 c < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f ll wing c i will pres t the solutio f i the l terature g niz d r und th se s s emp oy d. Spe ific ly, the p s t ti i orga z ar u d th c iteria f being r o -c s is d diff re set of t c iques a equir f r h se two ca s. F r each of tw ca eg rie , we c tinu by groupi g he vid d\nlutions ba d n the typ of s nsor wit the mo t f qu g o p being p e e te first. For b h c s, t s s f s o t ch iq s re c ibed in a c pr hens b man er I m e detail, S cti n 3.1.1 c v the p rs ut lizi a or f the c a s nsor, be it ingl c era r configur ti n f mul ipl c ras, set vices ll 3D camera sen or . Sub q e tly, p p r utilizi th IMU ns r (ac eleromet r, gy oscope, m gneto eter), the m st p pular nsor among the selecte p pers, are prese ted. For ach individual p pe , a summary incorp rating sol ti with the advantage and d sadvantag s is writt n. On the other h d, Secti 3.1.2 c nc rns sol tio w th no cam ra nsor . M st f them incorp ate an IMU s ns r while other ptions include ultrasoni d Li r e sors as well as Bluetoot Low E rgy (BLE) beaco s and Wi-Fi access points.\n3.1.1. Camera-Assisted S lutions An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est m tion by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is xpected giv n the p oliferation of sm rtphone devices as the preferred latfor f r d velopi g solu ons for the BVI individuals. Finally, only 14% of these solutions can b us d in real-life c narios with some degree of success, 9% of them are practical but h ve a comb nati n of eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716\n48 / 49 \u2716 \u2716 0 N/A N/A 1 N/A N/A m 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A N/A 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m / [61\u201363]\n[64] \u2716 R = 20 N/A [65\u20137 ] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f l wi g s ction wil p sent th s lu i s fo n i the lit a orga iz d u d t e nsor p y . S ecifi lly, th p e tatio i rganized aro the cri e ia of bei g r or n - a a is d a diff e t sets f te iques are quir f r t se t o c ses. F r e ch of se w c te i s, we co ti b g uping the p ovi d solu ons based n th type of s s r with th most frequen group be g pres nted first. F r b c s , t r fus t chniqu re de c ibed in co pre sibl manner. I r d t l, Secti 3.1.1 v rs p per u ilizing a f r f he cam a sensor, be it a single c me a r a o figur i of mult le c me as, h t devices s w ll a 3D\ne a sen o . Sub qu tl , papers utilizi th IMU e r (acc lerom ter, gyroscope, agnetometer), t e ost ular s nsor amo g the s lect d papers, are presented. For ea h individu l paper, a u mary incorp rating the solution with the advantages and dis dvantag s s itte . O the other h nd, Section 3.1.2 c ncer s s l tio s th no cam era sensor . Most of th incor orate an IMU s nsor while other options include ultraso and Li sens rs as w ll s B etooth Low En rgy (BLE) b a o s and Wi-Fi access points.\n3.1. . Camera-Assisted S lutions An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odometry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat stimati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is exp cted given he prolife ation of smartphone devices as the preferre platform f r dev lop g olutions for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with some degree of success, 9% of them are pract c l bu hav a combination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716\n[48] / / / / [49] \u2716 \u2716 \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 95% [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh i g s cti ns wi l pr se t the s ut o s fou d i the literature organiz d a ound t se ors e y . S ecific lly, p e e tation i organized aroun the cri\na of b ing r r n n-ca era ssis ed diff e t sets of t iques are quire f those t o c s . For ac of se two categ ri s, we conti ue by groupi g the p ovid d s l tions ba d on the ype of sor with he most frequen roup being presented first. For b th c s s, the e o f si tech iques a desc ibed in a c mprehensibl manner. I e det il, Se ti n 3.1.1 cov rs th per uti izing a f r of the cam a sensor, be it a sin r c figuration of multipl am ras, he set evices s well a 3D c m ra sensor . Sub que l , papers utilizing the IMU sens r (accelerometer, gyroscope,\nagnet me er), t mo popular s nsor among the sel cted papers, are presented. For ch i d vidual paper, su mary incorpora ing the solu ion with the advantages and di a v nt g s s writt . On th other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sensor . M st of them ncorp rate an IMU s nsor while ther options include ultras and Lida se s rs as ll a Blueto th Low Energy (BLE) beaco s and Wi-Fi access p ints.\n3. .1. Ca era-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by determining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nFiltering POIS\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected give the proliferation f s art h ne devices as the referred l tform for developing solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-lif scenari s with some degree f success, 29% of the are practical but have a com in tion f either hig cost r require from th ser to carry any sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy 4 47\n48 / 49 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 m 98%\n53 0.1 3.5 m 90\u2013 5 [54] / / N/A / 55 N/A N/A 0.2 m < R < 10\n[5 ] R > 2 m / 57 58 2 cm < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 c < 12 / [61\u2013 3]\n[64] R = 20 m N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nT e follow g secti ns w ll pr t t e s l i fou d i t e literatu rga ized a ound the sens s employed. S cific lly, the p e tat on o ize a un r teria of b ing ca era o n -cam a ss s ed as diff e t ts of h iq e re r quir for hose two cases. For each of the wo categ ries, w continue by grouping he p ov d d solutio s bas d on t e type of sensor with t e ost frequent group being te first. For both cas s, the ens r f si techniqu s are desc ibed i a comprehen ible ma ner. I more detail, Secti n 3.1.1 cover th p pers utilizi g f r f the c mer sen or, be it a single c mer r a co figuratio f ultipl ca eras, he set d vices ell as 3D camer sensor . Sub que tl , papers utilizing th I U ns r (accelerom ter, gyros p ,\nagnetometer), t e most popular sensor among the elected p pers, are pr nted. F r each individual paper, a summary incorp rating the solution with t e advant ges a d disadvant g s is written. O the other han , S tion 3.1.2 c ce ns sol ti s th cam era sensor . Mos of them i c r ate an IMU s ns r while othe tions inclu e ult asoni and Lidar sensors s well a Blu to th L w Energy (BLE) be co nd Wi-Fi acc s points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand camer sensors is ex ected give the proliferati of smartp one devices as t e referr d latform for devel ing soluti ns f r the BVI i dividuals. Finally, only 4% of these sol ti ns ca be used in real-life scenarios with so e degree of success, 29% of them are practic l bu have a c mbina io of either high cos r req ire fr t e user to carry many sensors, 24% re limited ractic lity for spec fic scen ios while 32 re purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support Static Dyn mic Range Accuracy 4 47\n48 [49] 50 / 51 / / 5 95% 52 0 < < 9 m 98%\n[53] 0.1 m < R < 3.5 m 9 \u201395 [54] / N/A [55] / N/A 0.2 < < 10 m N/A [56] R > 2 m / [57] N/A [58] 2 c < R < 4.5 / [59] 67\u201398% [60] 2 cm < R < 12 /\n[61\u201363] [64] R = 20 N/A\n[65\u201371] \u2716 \u2716 [72] N/A [73] \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716\nTh f llow g c i ns will p e t the s lut s f un i th lit ra u rgan zed r und the sens mploye . Specifically, the nt io is rga ize a ou d th c iteri f b i g c mer non-came a si t d s iff e t ets of q es are req ir d for h s t cases. For e ch f t e e w categ ri , w conti ue by r upi g the p vi e solutions based on the type f sens with he ost frequent group being presented first. For oth ca s, the sensor fusio tec nique re esc ib d in co prehensible ma ner. I ore d tail, Sec i n 3.1.1 c ver the pape s tilizing a f r f he c era se sor, be it single c m a r a configurati f ultipl camera , h s t d vices s well as 3D cam r sensor . Sub quently, papers util zi g t e IMU s ns r (accel rom ter, gyroscope, magnetometer), th m st popular sensor am the s lect d pap s, are prese e . For e ch individ al p per, summary incor orati g the soluti n with the a vantages and disa vant g s is written. O th t er hand, Secti 3.1.2 c cer s s l i s w th o cam era nsor . Mo f h m incorp ra a IMU s r whil ther s i clude ltrasoni a d Lidar s nsors s w ll as Bluetoot Low En gy (BLE) e a d Wi-Fi acc ss points.\n3.1.1. Camer -Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degr ding states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected gi en the proliferation of smartphone devices as th preferred latform for developi g solu ons for th BVI individuals. Finally, only 14% of these solutions can be used in real-l f cenarios with some deg ee of success, 9% of the are practical but have a combinatio of eith r high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy [41\u201347] \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 95 2 0 < < 9 98%\n53 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 < R < 10\n[56] R > 2 m / 57 58 2 cm < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 c < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll wing cti s will pr nt the solutio f i t literatur ganiz d ound th sensors empl y d. S e ific ly, t p e tati i ga iz ar u th c i teria f being c ra n-c r s is diff e et of t iqu a equire f r h se tw cas s. For ach of t s tw ca egorie , w c tin e by grou i g the p ovid d olution bas d on the typ of sensor with the o t f quent g up being te first. For b h ca e , he ensor f si techniq es are d c ibed in a c mpr hen ibl manner. I mo e detail, S cti n 3.1.1 c vers the pa ers ut lizin a r f the c mera sensor, be it a ingle camera or a c figurati n f ultiple ca r s, adset evices as well as 3D ca era sensor . Subseq e tl , pap s utilizi g th IMU nsor (ac el romet r, gyros ope,\ngneto eter), t e m st p p lar sensor among the selecte p p rs, are prese te . For ach individual p per, a summary incorp ra ng the olutio with the advantag s and disadvan age is writt . On he other hand, S cti 3.1.2 concerns s lutions with no camera nsors. Most f th m incorpor e n IMU ensor whi o her optio s nclude ultrasonic nd Lid r enso s as well as Blu toot Low En rgy (BLE) beacons d Wi-F acce s points.\n3.1.1. Camera-Assisted Soluti s An uncert inty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat stimatio by d termining the states that s o ld be in luded in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 9\nand c m ra sensors is xpected given the p oliferation of sm rtphone devices as the preferred latfor for d veloping solu ons for th BVI individuals. Finally, only 14% of these solutions can b used in real-lif sc narios with some degree of success, 9% of the are practical but have a comb nati n of eith r high cost or requir f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range Accuracy [41\u201347] \u2716 \u2716\n48 N/A 49 \u2716 \u2716 0 / / 1 N/A N/A m 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f ll wi g s ction w ll p sent th s luti s fo i the literatu orga iz d u d the e s r pl y . S cifi lly, th p e tatio i rganized arou the cri ia of b i r n - a a i d a iff e t ets of te iqu re quir f r those t o c ses. F r e ch f t ese tw categ ies, we co ti e b grouping the p ovi d solutions bas d n the type of s nsor with th o t frequent group being te first. F r b th cas s, the nsor f i t chniqu s re desc b in comprehensibl man er. I more detail, Section 3.1.1 vers h papers ilizing a f r f the camera se or, be it a single camera o a o figuration of multi le cameras, hea s t devices as well as 3D\ne ensor . Sub que tl , pape utilizin th IMU ns r (accel rom ter, gyros ope, agnetometer), t e ost p pular sens r among the s lect d papers, are presented. For each i div du l p p r, a ummary in o p ra ng th so uti n wi h th advantages and disa vantage s itte . On th t er h , Secti 3.1.2 concer s olutio s with no camera sen or . Most f m inco por e an IMU s nsor whil o her optio s include ultras ic and Li ns rs as well as Blu tooth Low Energy (BLE) bea ons and Wi-Fi acce s points.\n3.1.1. Camer -Assi ted S l tio An uncertai ty-based adaptive s ns r fusion framew rk for Visual-Inertial Odo etry (VIO) is proposed i [41] for stimati g r lat ve motio . It minimizes degradation from inaccurate t t sti ation by determi ing the states that should b included in the estimation process. These degrading states can arise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is exp cted given the prolife ation of smartphone devices as the preferred latform f r dev lop g solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-lif sc narios with some degree of success, 9% of the are pract c l bu hav a co bination of i h r high cost or equire from the user to carry many sen ors, 24% are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic R nge Accuracy [41\u201347] \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 \u2716 0 / / 1 N/A N/A 5 95 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10\n[56] R > 2 m / 57 58 2 m < R < 4.5 N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh l i g s cti wi l pr e t the s lutio s fou d i the literature organiz d a ound t e o empl y . S e ific lly, p e t tion i ganized aro n the cri t of b in camer r n n- a era ssis ed diff e t ets of t iqu r equire f those t o case . F r ach f t ese two categories, w continue by grouping the provided s lutions ba d o th y e of sor with he m st frequent group being te first. F b th c s s, th en f si t chniques a described i co prehensible manner. I m r det il, S ti 3.1.1 c vers the p per utilizing a f r f the camera sensor, be it a sin or co figuration f multipl am ras, h a set devices as well as 3D cam r s n r . Sub que tl , papers utilizing the IMU sens r (accelerometer, gyros ope,\ngnet me er), t mo t popular s nsor mong the sel cted papers, are presented. For ach i d idual pape , a s a y inc pora ng the solu ion with the advantages and disa van ag s s wr tt . On the other hand, S cti 3.1.2 concerns solutions with no camera sensors. Mo t f th m incorp ra e an IMU sensor whil o her optio s include ultras ic and Lidar sens rs as ll a Blu tooth Low Energy (BLE) beacons and Wi-Fi acce s points.\n3.1.1. C mera-Assisted S l tions An unce tainty-based adaptiv senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from accurate state estimation by deter ining the states that should be included in the estimation proce s. These degradi g sta es can arise under motion characteristics that nullify\nAlert Distance\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected give the pro iferation of s art h ne devices as the referred l tform for developing solu ons for the BVI individuals. Finally, only 14% f hese solut ons can be used in real-lif scenari s with some degree f success, 29% of the are practical but have a com in tion of either high cost r require from th ser to carry any sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy [41\u201347]\n48 49 \u2716 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 8%\n53 0 1 3.5 90\u201395 [54] / / / / 55 N/A N/A 0.2 m < R < 10\n[5 ] R > 2 / 57 58 2 cm < R < 4.5 / 59 N/A 6 \u201398%\n[60] 2 c < R < 12 / [61\u201363]\n[64] R = 20 c N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follow g secti ns w ll pr t th s l i fou d i t lite atu org ized a ound the sens s empl yed. S cific ly, the p e ent t o is o ganize a u h cri teria of b i ca era o n -c m s s ed as diff e t t of h iq e e r qu r d for hos two case For each of the wo ca g ri s, w co tinue by grouping h provid d solutions bas d on the type of sensor with th st f equ t g ou b ing te first. For both c s s, the ens r f si techniques are esc ibed i a c mprehensible ma ner. I more detail, Secti 3.1.1 cover t p pers utilizing a f r f the cam a se or, be it single c mera r co figuration of multiple amer s, e set devices ll as 3D camer se sor . Sub que tl , papers utilizi I U ( ccel rom ter, gyr s ope,\nagnet meter), t e m s po ular ensor among the elected p pers, re p n ed. F each individual paper, a summary incorporating the solution with the advantages a d disadvantag s is written. On the ther hand, Section 3.1.2 c ncerns sol tio s th o ca era sensor . Most of them i c rporate an IMU s nsor while ther options i clude ltrasoni and Lidar sensors as well as Bluetooth Low Energy (BLE) beac s an Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can arise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand camera sensors is ex ected give the proliferation of smartp one devices as t e referr d latform for devel ing solu ns f r the BVI individuals. Finally, only 14% of th se soluti ns ca be used in real-life scenarios with so e degree of success, 29% of the are practic l bu have a c mbina io of either high cos r require fr m t e user to carry many sensors, 24% are limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy 4 47\n48 49 0 N/A 1 / 5 95 2 0 < < 9 m 98%\n53 0.1 3.5 9 \u201395% [54] N/A / 55 N/A N/A 0.2 < < 10 m\n[56] R > 2 m / 57 58 2 c < R < 4.5 N/A 59 67\u201398%\n[60] 2 cm < R < 12 N/A [61\u201363] \u2716 \u2716\n[64] R = 20 m N/A [65\u201371] \u2716 \u2716 \u2716\n[72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nTh f llow g c n will p t the s l t f u i h lit ra u rgan zed ound the sens mploye . S cifi l y, the nt tio is rganize a ou the c i teri f b i g c mer r non- am a assi ed s ff ent et of q s ar req ir d for h s t cases. Fo e c f t e e w cat g ri , w cont e by rouping th p vi e soluti ns based on the type f se so with he ost frequent group being pre e t first. For oth ca es, the ensor f si ec niques re describ d in c p ehen ibl ma ner. I ore d tail, S c ion 3.1.1 ove s th p p s t lizing a f f he c a sensor, be it single cam or co figurati n of mu ipl camer , h s ev c s well s 3D cam r sensors. Sub que tl , papers util zing e IMU n or accel rom te , yroscope,\nagnetomete ), t m st popul r sen or am t s l ct d pap s, re pre e e . For e ch individual paper, summary incor orating the soluti n with the advantages and di a vant ges is written. n the t er hand, Sectio 3.1.2 concer s s l ti with no ca - era sensors. Mo of hem incorp ra e an IMU se s r while other optio s include ltrasonic a d Lidar nsors as w ll as Bluetoot Low Energy (BLE) be ns a d Wi-Fi access points.\n3.1.1. Cam r -Assist d Soluti ns An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expec ed gi en he proliferation of smartphone devices s th preferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be u ed in real-l f cenari s with some d g ee of success, 29% of the are practical but have a combinatio of eith r high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range ccuracy 4 47\n48 / / / [49] 50 / / 51 N/A N/A 5 95 52 < < 9 m 98%\n[53] 0.1 < R < 3.5 m 90\u201395 [54] / / N/A / [55] N/A N/A 0.2 < R < 10 N/A [56] R > 2 m / [57] N/A N/A [58] 2 cm < R < 4.5 m / [59] N/A 67\u201398% [60] 2 c < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f ll wing s cti s will pr s t the olutio s f i the literature g niz d ound th se s rs e pl y d. S e ific lly, the p e tati i ganiz ar un th c i teria f being c m ra o no -c er as is d diff e t ets of t iqu a equire for h se tw cas s. For each of t s tw categ rie , w c tin e by grou i g the ovid d olution bas d on the typ of sensor wit the o t f quent g p being te first. For b h c s s, t e nsor f si t chniq es are de c ibed in a c pr hen b m ner. I mo e det il, S ction 3.1.1 c ve s the p rs ut lizin a r f the c mera s ns r, be it a ingl ca era r c figur ti n f l iple c m r s, s t evices as well s 3D camera sensor . Sub que tl , pap s utilizi g th IMU s r (ac l romet , gyros ope,\ngneto eter), t e most popular sens r among the selected p p rs, are presente . For ach individual p per, a summary incorporating the solution with the advantages and disadvan ag is writt . On the other h nd, Secti 3.1.2 c nc rns s l tio w th no cam ra ensor . Most f them incorporate an IMU s ns r while oth r ptions include ultrasoni d Li r e sors as well as Bluetoot Low E rgy (BLE) beaco s and Wi-Fi access points.\n3.1.1. Camera-Assisted S l tions An uncert inty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat stim tio by d termining the states that s o ld be in luded in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 9\nand ca ra se sors is xpec ed giv the proliferati of smartphone devices as the preferr latfor for develo ing solu ons for the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-life c nari s with some degree of success, 29% of them are pract cal but ave a c mbinati n of ei her high cost or req ire from the user to carry m n sensors, 24% re limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support Static Dynamic Range Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n48 N/A 49 \u2716 \u2716 0 N/A N/A 1 / N/A 5 m 95% 2 0 m < R < 9 m 98%\n53 \u2716 0.1 3.5 90\u201395% [54] N/A N/A N/A N/A 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m N/A 57 58 2 cm < R < 4.5 N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 N/A [61\u201363] \u2716 \u2716 \u2716\n[64] \u2716 R = 20 N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f l w g sect ns will r the s lutio s fou d in the literatu e orga ized a u d t e s ns mpl yed. S cific lly, the pre ntation is r nize a oun the cri t ri of bei g am ra r n -c m a a s ed as iff nt et of t h iq es are required for hos tw c s . Fo each f th w catego es, we conti ue by grouping the provided sol ions b s d on th yp of s s with th most frequent group being presented first. For b th s , t e sor f si ech iqu s re described in c prehensible m nner. In de ail, S ctio 3.1.1 ve s pap s ut l zing a f rm f the camera sensor, b a s l c ra co figu ati n of u iple c me as, he s t devices as well as 3D ca era senso s. Sub q e tl , p rs u ilizi g th IMU e sor (accelerom ter, gyroscope,\nagn t meter), t ost o lar s nsor am ng t e sel ct d papers, re pr sented. For e h individ al p p r, summ ry inco porati g the s lution with the a vantages and di dvantage is writt . On the other and, Section 3.1.2 c ncerns solutions with no camera sens rs. M st f th m i c orate an IMU se sor while ot er ptions include ultrasonic and Lidar e s as w ll a Bluet oth L w E ergy (BLE) beaco s nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solution An ncertai ty-based adaptive sensor fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed in [41] for estimating r lative m tion. It minimizes degradation from inaccurat esti ati by determining the states th t should be included in the estimati process. The e degr din states can rise und r motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c era sensors is exp cted given he prolife ation of sm rtphone devices as the preferred platform for dev lop g solutions for th BVI individ als. Finally, only 14% of these solutions can be used in real-lif scenarios with some degree of success, 29% of the are pract cal bu hav a co bination of i h r high cost or require from the user to carry many sen ors, 24% are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy 4 47\n48 / / / [49] 50 / / 51 N/A N/A 5 m 95 52 0 < < 9 m 98%\n[53] 0.1 < R < 3.5 m 90\u201395 [54] / / N/A / [55] N/A N/A 0. < R < 10 N/A [56] R > 2 / [57] N/A / [58] 2 m < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh ll i g s cti wi l pr e t th s luti s fou d i the literature o ganiz d aroun t e e pl y . Specifically, p es t tion i ganized aro nd the crite of b ing camer r n n- a era ssisted a differe t ets f t c niqu ar equire f r those t o c se . F r each f t ese two categ ies, w conti ue by groupi g the provided s lutions ba d o th ty of sor with he m st frequent roup b ing te first. F b th c s s, the se s r f si t chniques desc ibed i c mprehensible manner. I re et il, Se ti 3.1.1 c vers th p per u ilizing a for f the camera s nsor, be it a sing r c fig ration f multiple am r s, h set dev ces as w ll as 3D c r n r . Sub que l , pap rs utilizi the IMU sens (acc le omet r, gy os ope, m g et me er), th mo t popular sensor mong the selected papers, re prese ted. For each ind vidual paper, a summary incorp rating the solution with the advantages and di a v nt g s is writt . On th other hand, Section 3.1.2 c ncerns sol tio s w th no cam era sensor . M st of them ncorp rate an IMU s nsor while ther options include ultras nd Lida se sors as w ll a Blueto th Low Energy (BLE) beaco s and Wi-Fi access p ints.\n3.1.1. C era-Assisted S l tions An unc tainty-based adaptive senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for estimating relative motion. It mi imiz s degradation from accurate stat estimation by deter ining the states that should be included in the estimation process. These degrading sta es can arise under motion characteristics that nullify\nPublic transport information\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected give the pro iferation of smart h ne devices s the referred latform for developing solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-lif scenarios with some egree f suc ess, 29% of the are practical but have a combination of either high cost r require from the user to carry any sensors, 24% are limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Rang Accuracy 4 47\n48 / 49 0 / / 1 N/A N/A 5 m 95 2 0 < < 9 98%\n53 0.1 3.5 90\u201395 [54] / / / / 55 N/A N/A 0.2 < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 c < 12 / [61\u201363]\n[64] R = 20 N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT e follow g sections w ll pr t t s l i fou d i t literat rg iz d a ound the sens employed. S cific ly, the p tat o is g nize a oun r teria of b i ca ra o - ss s ed as diff e t t of h iq a e r qu d for hos two case For each of the wo c g ri s, w continu by grouping h prov d d solutio s bas d on the type of sensor with th st f equ t g u b ing te first. For both cas s, th ens r f si techniqu s are esc ibed i co p e nsi le ma ner. I more detail, Secti 3.1.1 cover t p p rs utilizi g a f r f the m r e sor, be it single c m r co figuration of ultipl a er s, ea t d vic s a w ll as 3D camer se sor . Sub que tl , paper utilizing the IMU ( cel rom ter, yr s op ,\nagnetometer), t e m s po ular ensor among the selected p pers, are p esented. F each individual paper, a summary incorporating the solution with the advantages and disadvantag s is written. On the ther hand, S ction 3.1.2 c nce ns sol tio s th cam era sensor . Most of them i c rpo ate an IMU s nsor while ther options i clu e ltrasoni and Lidar sensors as well a Blu tooth L w Energy (BLE) beac n Wi-Fi acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation fro inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion ch racteristics that nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand c mera sensors is ex ected give the proliferation of smartp one devices as t e referr d platform for devel ping soluti ns f r the BVI individuals. Finally, only 4% of these soluti ns ca be used in real-lif scen rio with s e degree of success, 29% of he ar practic l bu have a combinatio of either high cos or require fr m the user to carry any sensors, 24% are limited ractic lity for spec fic scen ios while 32 re purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy 4 4\n48 [49] 50 / 51 N/A / 5 m 95 52 0 < < 9 m 98%\n[53] 0.1 m < R < 3.5 m 90\u201395% [54] / / [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 m / [57] / [58] 2 c < R < 4.5 / [59] 67\u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 [64] R = 20 N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nTh f llow g c n will p e t the s l t f u i th lit ra u rga ized round the sens mploye . S cifi l y, the t io i ga ize a ou d th c iteri f b i g c mer n - am a a si ted s ff ent et f q ar req ir d for h s t cases. Fo each f t e w cat g ri , w cont e b gr upi g th p vi soluti ns bas d on the type f se so with he ost frequent group b ing t fi t. For oth ca s, the sensor fusi tec niques are d sc ib d in c mp ehen ibl ma ner. I ore d tail, Sec ion 3.1.1 ver th p p s utilizing a f f e c m a sensor, be it single cam or a co figuratio of mu tiple camera , h a s dev c s well s 3D cam r sensor . Sub quently, paper utilizing e IMU n r accelerom t , gyro ope, magnetomete ), th m t p pul r sen or amo g th selected papers, re pre e ted. For e ch individual paper, a summary incorporating the soluti n with the advantages and di a vant ges is written. On the t er hand, Secti 3.1.2 co cer s s l ti with o camera nsors. Mo f h m incorp ra e an IMU se s r whil ther ptions include ltrasonic a d Lidar s nsors s w ll as Bluetoot Low Energy (BLE) be ons and Wi-Fi access points.\n3.1.1. Camer -Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion framework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr m inaccurate state estimation by det r ining the states that s ould be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sens rs is exp ct d given th p olif rati of smar phon devices as the preferred platfor for developing so uti s for the BVI i dividuals. Finally, only 14% of these sol tions can be used in real-l f ce arios with som degree of success, 29% of them are practical but hav a co bination f eithe high cost or req ire f o the user to carry many sensors, 24% r limited pra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa er Support Static Dyn mic Range Accuracy 4 47\n48 N/A N/A [49] 50 N/A N/A 51 / N/A 5 m 95% 52 0 < 9 98%\n[53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A / [55] / N/A 0.2 m < R < 10 m N/A [56] R > 2 m / [57] N/A N/A [58] 2 < R < 4.5 / [59] N/A 67\u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe fol ow g s o ill r t h lutio fo i the l ratu e gan z d r u d s s r employ . S c fically, t e e nt ti n is g ze a u d the c ieri of b i g c mer no -c m a s i t d as iff r t t f t ch q ar require for hos two cas s. F each of th w ca ego e , conti by grou i g he provid d soluti ns ba d n th typ of se s with t e o t f quent gr u bei te first. For b t c ses, t s ns f si ec n q es e e c ibe in co rehen ibl an er. I more detail, S c ion 3.1.1 v s the pap u lizi a for f the c mera sens r, be it si gle c era r configur ti of l ipl c er s, set evices a el as 3D ca era sen or . Sub que tly, p p utilizi g the IMU nsor (ac ler met r, ros ope, magnetometer), the ost popular ns r amo g t e sel cted papers, are prese ted. For each individ al p pe , a summary incorpor ti g solution with the a vantages and d advan ag is w itten. O the other ha d, Secti 3.1.2 c nc rns s lutions with n camera s rs. Most of them i corp at an IMU se s r whil other pti s include ultrasonic d Lid r s n or as well s Blu to th Low E rgy ( LE) beacons a d Wi-Fi access points.\n3.1.1. Cam ra-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s ates an arise under motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 29\nand c ra se sors is xpec ed giv the proliferation of sm rtphone devices as the preferre platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can b us d in real-lif c narios with some degree of success, 29% of the are pract cal but ave a comb nati n of either high cost or require f om the user to carry m n sensors, 24% are limited practicality for specific scenarios whil 32 are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy amic Range Accuracy [41\u201347]\n[48] N/A N/A N/A [49] \u2716 \u2716 \u2716 [50] N/A / N/A N/A [51] N/A N/A m 95 [5 ] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A / N/A / [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 / [59] N/A 67\u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 \u2716 [64] R = 20 N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lo g s ct ns will re t the s lutio s fo i the literatu e orga ized ar u d t e s s mpl yed. S cifi lly, the p e ntation is ize a ound the crit i of bein r n -c m a ted a iff r nt et of t ch iq re r quired for hos tw c s . Fo each f th w categ es, we conti ue by grouping the p ovid d sol ions b s d on th yp of s so with th most frequent group being te first. For b th s , the e sor f si tech iq s re desc ibed in c mprehensibl m n er. I de il, Sectio 3.1.1 v rs p p s il zing a for f the camera se or, b t a s l c ra co figurati n of mu tiple cam ras, hea s t devices s well as 3D ca er enso . Sub q e tly, p utilizin th IMU e s r (accel rom ter, gyros ope, magn t meter), the ost lar s nsor am ng the s lect d papers, re presented. For ea h individu l paper, summ ry incorp rating the solution with the advantages and di vantage is w itt . On th other han , Secti 3.1.2 conc r s solution with no c mra s rs. M st f th m inc r o at an IMU se so whil t er ptions include ultrasonic d Lid r ens rs s w ll a Bluetooth L w E rgy (BLE) bea o s nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solution An ncertai ty-based adaptive s ns r fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed i [41] for stimating relat ve otion. It minimizes degradation from inaccurat sti ati by determining the states th t should b included in the estimati process. The e degradin states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca era sens rs is exp cted given the proliferation of sm rtphone devices as the preferre latform f r dev lopi g solu ons for th BVI individ als. Finally, only 14% of these soluti s can b used in real-life scenarios with some degree of success, 29% of the are practical but have a combination of ith r high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyna ic Range Accuracy 4 47\n48 / 49 0 / / 1 N/A N/A 5 95 2 0 < < 9 m 98%\n53 0.1 3.5 m 90\u201395 [54] / / N/A / 55 N/A N/A 0. < R < 10\n[56] R > 2 / 57 58 2 m < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 m / [61\u201363]\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh i g s cti will pr e t th s ut s fou d in the literature o ganiz d ar u d t se rs e oy . Specifically, th p esentation i organized aro nd the crit i of being r r n n- a era ssisted differ t sets f t chniques are quired for tho t o c s . F r eac of se two categ i s, we conti ue by groupi g the p ovid d s l tions ba d on th typ of ensor with the most frequen roup being pr sented first. For both c s s, the se s r f si tech iques re d s rib d in a c mprehensibl manner. I m e det il, Se ti n 3.1.1 cov rs th per uti izing a for of the cam a s nsor, be it a single c era r c fig rati f multipl camer s, he dset evices s w ll a 3D ca ra sen or . Subsequentl , pap rs utilizin the IMU senso (acc le omet r, gy oscope, m g etometer), the mo t popular se sor mong the selected papers, re prese ted. For e ch individual paper, a su mary incorporating the solution with the advantages and di adv nt g s is writt . On th other hand, Section 3.1.2 c ncerns sol tio s w th no cam era s nsor . M st of them ncorp rate an IMU s nsor while ther options include ultras and Lida se sors as w ll as Blueto th Low Energy (BLE) beaco s and Wi-Fi access p ints.\n3.1.1. Ca era-Assisted S l tions An unc rtainty-based adaptive senso fu ion f mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating r lative motion. It mi imiz s degradation from inaccurate stat estimation by determining the state t t should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nVehicle mode\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given he proliferation of smartphone devices s the preferred platform for developi g solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life sc narios with some egree f suc ess, 9% of them are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716\n[48] N/A / N/A [49] \u2716 [50] N/A N/A N/A [51] N/A N/A 5 95% [52] 0 < R < 9 m 8% [53] 0.1 < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 / [57] N/A [58] 2 c < R < 4.5 m N/A [59] N/A 6 \u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 \u2716 [64] \u2716 R = 20 m N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follow g sections will pr t t e s lu o s fou d i the lit at ga iz d around the sens rs e p yed. Spe ifically, the pr se t i rganize a ound h criteria of b ing ra or n- s ist diff t et of ec iq es r r qu for hose two cases. For each of e wo c teg ries, w co tinu by groupin he p ovid d solutions based on the type of sensor with the most frequen group being pr sented first. For both cases, th se sor fus o tech iqu s re e cribed in co p e n i m nner. In more detail, Section 3.1.1 cov rs the p pers utilizi g a form f the am sor, be it a single cam or a configuration of ulti l ca eras, head et d vic s a w ll a 3D camera sensors. Subsequently, pap r utilizi the IMU s r (a celero ter, y sc p , magnetometer), the most popular sensor am ng the selected papers, are presented. For each individual paper, a summary i corp a g the solution with the a v ntages and disadvantages is written. On the other hand, S ction 3.1.2 concerns solutions with n ca - era sensors. Most of th m incorpor e an IMU se sor whil o he opt ons inclu ul rasonic and Lidar sensors as well as Blu to th Low Energy (BLE) be cons and W -Fi acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the proliferation of smartphone devices as t e referred latfo m fo devel ing solutions for the BVI individuals. Finally, only 14% of these\nl ti can b used in real-life scenarios with some d gree of success, 29% of the are pra tic l but have a c mbina ion of either high cost r require fr the user to carry any sensors, 24% are li ited practicality for pecific scenarios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy [41\u201347]\n[48] / / [49] \u2716 \u2716 [50] / / [51] / 5 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 90\u201395 [54] / / / / [55] N/A 0.2 < < 10 m N/A [56] R > 2 m / [57] N/A [58] 2 c < R < 4.5 / [59] 67\u201398% [60] 2 cm < R < 12 /\n[61\u201363] [64] R = 20 N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follow g secti ns will pre t the s lutio s foun in the litera u e organized round the s n o mpl ye . Specificall , the p e e t ion i ga ize a ou d th c it ria of b i g c m ra non-cam i t d as iffe ent ets of t iq are req ir d for h s t o cases. For e h f the e wo at g ries, w conti ue by r upi g t e pr vided solutions based on the type of senso with the ost frequ nt group being presented first. For oth cases, the sensor fusio tec niques re describ d in co prehensible manner. I ore d tail, S cti n 3.1.1 cov th papers ut lizing a f f he c era se sor, be it singl c m a r co figurati f mul iple cameras, he s t dev c s well s 3D cam ra sensors. Subs quently, papers util zing t e IMU s nsor ( ccel rom t r, gyroscope, magnetomete ), the most popul r s n r amo t sel ct d pape s, re pre en ed. For each i dividual paper, a summary incorporating the solution with the advantages and disa vant g s is writt n. On th other hand, Secti 3.1.2 c cerns s l io s w th o cam ra nsor . Mo f h m incorp ra a IMU s r whil ther ns i clude ultrasoni and Lidar s nsors s w ll as Bluetoot Low En gy (BLE) eaco and Wi-Fi acc ss points.\n3.1.1. Camera-Assist d Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is prop sed in [41] for estimating relative motion. It minimizes degradatio from inaccurate state esti ati by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\na d c mera sens rs is expected given th prolif ration of smartphone devices as th preferred latform for developi g so u s for th BVI individuals. Finally, only 14% of these solutions can be u ed in real-l f cenarios with som degree of success, 29% of them are practical but have a combin tion of eith r high cost or require from the user to carry many sensors, 24% ar limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn ic Range ccuracy 4 47\n48 N/A 49 0 N/A N/A 1 N/A N/A 5 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] N/A N/A N/A / 55 N/A N/A 0.2 < R < 10\n[56] R > 2 m / 57 58 2 c < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 N/A [61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 c N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe fol w g cti s will pr t h lutio fo d i t li ratu gan zed a ound t sens rs empl y d. S cific ly, t p e at i ga iz a ou the c i teria f b ing c r r -c s is as iff et f t iq ar require f r h s tw cas s. F r each of t e e w ca go e , w c ti by grou ing he p ovid d solution bas d on the type f sensor with the o t f quent gr up being nte first. For b h ca , he ensor f si techn q es a e d c ibed in a c pr hen ible manner. I more detail, S ction 3.1.1 c v rs the p er ut lizin a r f the camera sensor, be it a ingle camera or a c figurati n f ltiple ca r s, a set devices as well as 3D camera sensor . Sub q e tl , pap s utilizing th IMU ens r (ac el rometer, gyros ope,\nagnetometer), t e m st p p lar senso among the selecte pap rs, are prese te . For each individual paper, a summary incorp r ng the olutio with the advantag s and disadvan age is writt . On he other hand, S ction 3.1.2 concerns s lutions with no camera s nsors. Most of th m incorpor e IMU ensor whi o her optio s nclude ultrasonic nd Lid r nso s as well as Blu toot Low En rgy (BLE) beacons d Wi-F acce s points.\n3.1.1. Camera-Assisted S lutions An ncertainty-based adaptive se sor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat s imatio by d termining the states that s ould be in luded in the estimation proc ss. These degrading s ates an arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 9\nand c m ra se sors is xpec ed given the p oliferation of sm rtphone devices as the preferre latfor for developing solu ons for th BVI individuals. Finally, only 14% of these solutions an b used in real-lif sc narios with so e degree of success, 29% of them are pract cal but have a comb nati n of either high cost or requir f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Support Static Dy a ic Range A curacy 4 47\n48 N/A 49 0 N/A N/A 1 N/A N/A 95% 2 0 < < 9 m 98%\n53 0.1 3.5 90\u201395% [54] N/A N/A N/A / 55 N/A N/A 0.2 m < R < 10 m\n[56] R > 2 / 57 58 2 cm < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 cm < R < 12 N/A [61\u201363] \u2716 \u2716 \u2716\n[64] \u2716 R = 20 m N/A [65\u201371] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe f ll w g s ction w ll pr th s lutions fo i the literatu e orga ized u d the e s r pl ye . Sp cifi lly, the p e tation i ganize a ou the cri ia of b in r n -ca a a i d a diff nt ets of t h iq re r quir d f r h se two c ses. F r e ch f th wo categ ies, we co ti e b grouping the p ovid d sol tions bas d n the type of s nsor with th o t frequent group being te first. F r b th c s , th r fu i techniqu s re desc b in compreh nsible man er. I m re detail, Section 3.1.1 cov rs h p pers ilizing a f r f the camera se or, b it a si gle camera or co figuration of multiple cameras, hea set devices as well as 3D\ne ensor . Sub que tl , pa e utilizi g th IMU r (accel rometer, gyros ope, agnetometer), t e ost p pular sens r among the s lect d papers, are presented. For each i div du l p p r, a summ ry in o p ra ng th so uti n wi h th advantages and disa vantage s itte . On th t er h , Secti 3.1.2 concer s olutio s with no camera sen r . Most f m inco por e an IMU s nsor whil o her optio s include ultras ic and Li ns rs as well as Blu tooth Low Energy (BLE) bea ons and Wi-Fi acce s points.\n3.1. . Camera-Assisted S lutio s An uncertai ty-based adaptive s ns r fusion framew rk f r Visual-Inertial Odo etry (VIO) s proposed i [41] for stimating relat ve motio . It minimizes degradation from inaccurate t t sti atio by determi ing the states that should b included in the estimati n process. The e degrading states can arise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c m ra sensors is expected given the prolife ation of smartphone devices as the preferred latform f r develop g solu ons for the BVI individuals. Finally, only 14% of these solutions can be used in real-lif sc narios with some degree of success, 29% of the are pract c l bu hav a co bination of i h r high cost or equire from the user to carry many sen ors, 24% are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstacle detection.\nPape Support Static Dynamic R nge Accuracy 4 47\n48 N/A 49 0 / / 1 N/A N/A 5 95 2 0 < < 9 m 98%\n53 0.1 3.5 m 90\u201395% [54] / / N/A / 55 N/A N/A 0.2 m < R < 10\n[56] R > 2 / 57 58 2 m < R < 4.5 m / 59 N/A 67\u201398%\n[60] 2 c < R < 12 N/A [61\u201363] \u2716 \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh l g ecti wi l pr t the s lutions found in th literatu e organized a ound th e emp y . S e ific lly, p e nt tion i o ganize a o n the cri t of b in c era r n n- am ss s e s diff e t ets of t h iq r required f ose two cases. F r a f th wo catego i s, w continue by grouping the provided s lutions ba d o th y e of sor with he m st frequent group being te first. F b th c ses, the en f si t chniques a described i comprehensible manner. I m re det il, Se ti 3.1.1 c vers the paper utilizing a f r f the camera sensor, be it a sin or co figuration f multipl am ras, h adset devices as well as 3D cam r s n rs. Sub eque tl , papers utilizing the IMU sensor (accelerometer, gyros ope,\ng t me er), t most popular s nsor mong the sel cted papers, are presented. For ach i d idual pape , a s ma y inc pora ng the solu ion with the advantages and disa va ag s s wr tt . On the other hand, S cti 3.1.2 concerns solutions with no camera sensors. Mo t f th m incorp ra e an IMU sensor whil o her optio s include ultras ic and Lidar sens rs as ll a Blu tooth Low Energy (BLE) beacons and Wi-Fi acce s points.\n3. .1. Ca era-Assisted Sol tions An unce tainty-based adaptiv senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from accurate state estimation by determining the states that should be included in the estimation proce s. These degradi g sta es can arise under motion characteristics that nullify\nCustomizable\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected give the pro iferati of s art h ne devices as the referred l tform for developing solu ons for the BVI i dividuals. Finally, only 14% f hese sol tions can be used in real-lif scenari s with some degree f success, 29% of the are practical but have a com ination of either high cost r req ire from th ser to carry many sensors, 24% re limited practicality for specific scen rios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPa er Support Static Dynamic Rang Accuracy 4 47\n48 49 0 / N/A 1 / N/A 5 m 95% 2 0 < 9 8%\n53 0.1 3.5 m 90\u201395 [54] / N/A / / 55 / N/A 0.2 < R < 10\n[5 ] R > 2 / 57 58 2 c < R < 4.5 / 59 N/A 6 \u201398%\n[60] 2 c < R < 12 / [61\u201363]\n[64] \u2716 R = 20 c N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe f llow g s cti ns will pr t th s l fou d in t lit atu rg iz a oun the sens s e pl yed. S ecific l , the p ent o is o g niz a u h cri t ria of b i ca ra o n- s s e as diff ent et of iq e r q r for os two case For ach of the wo c g ri s, w co ti ue by gro ping t pr vid solutions bas d on the type of sens r with th st f equ t g ou b ing te fi st. For both c s s, th ens r f si techniques are esc ibed i a co pre nsible ma ner. I more detail, Secti 3.1.1 covers t e p pers utilizi g a f r f the am a e or, be it single c m r co figuration of ulti le amer s, e et devices w ll as 3D camer se sor . Sub que tl , paper utilizi I U ( ccel ro ter, gyros ope,\nagnet meter), t e m s po ular ensor am ng the elected p pers, re p n ed. F each individ al p per, a summary incorporati g the solution with the a vantages a d disadvantag s is written. O the ther hand, Sectio 3.1.2 c cerns sol tio s th o ca era sensor . Most of them i c rporate an IMU s nsor while ther options i clude ltrasoni and Lidar sensors as well as Bluetooth Low Energy (BLE) beac s an Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degr ding states can arise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand c mera sensors is ex ected give the proliferation of smartp one devices as t e referr d latform for devel ing solu ns f r the BVI individuals. Finally, only 14% of th se soluti ns ca be used in real-lif scenarios with so e degree of success, 29% of the are practic l bu have a combina io of either high cos r require fr m t e user to carry any sensors, 24% are limited ractic lity for spec fic scen ios while 32% re purely experimental.\nTable 3. Solutions and obstacle detection.\nPaper Support Static Dyn mic Range Accuracy 4 47\n48 49 0 /"
        },
        {
            "heading": "1 N/A / 5 95",
            "text": "2 0 < < 9 m 98%\n53 0.1 3.5 m 9 \u201395% [54] / / 55 N/A N/A 0.2 < < 10 m\n[56] R > 2 m / 57 58 2 c < R < 4.5 N/A 59 67\u201398%\n[60] 2 cm < R < 12 N/A [61\u201363] \u2716 \u2716\n[64] R = 20 c N/A [65\u20137 ] \u2716 \u2716 \u2716\n[72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716\nTh f llow g c n will p t the s l t f u i h lit ratu e rgan ze ound the s ns r mploye S cifi l , the pr nt tion i ga ize a oun the c i\nteri f b i g c mer r n - am a assi ed s ff nt t of t h q ar required for hos t cas s. Fo e f t e w cat gories, w cont e by roupin t p ovided soluti ns bas d on the type f se so with he most frequent group being t first. For oth ca es, the ensor f si ec niques re describ d in c p ehen ibl ma ner. I ore d tail, S c ion 3.1.1 ove s th p p s t lizing a f f he c sensor, be it single cam or co figurati n of u ipl ca er , h s dev c s well s 3D cam r sensors. Subs q e tl , papers util zi g e IMU n or accel rom te , yros ope,\nagnetomete ), t m st opul r sen or am t s l ct d pap s, re pre e e . For e ch individu l paper, s mmary incor orati g the soluti n with the advantages and di a vant ges is written. n the t er hand, Sectio 3.1.2 concer s s l ti with no ca - era sensors. o of hem incorp ra e an IMU se s r while other optio s include ltrasonic a d Lidar nsors as w ll as Bluetoot Low Energy (BLE) be ns a d Wi-Fi access points.\n3.1.1. Camer -Assisted Solutions An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for esti ati g rel tive moti n. It mini izes degradatio fr inaccurate state estimation by det rmi ing the states that s ould be included in t e estimation process. These degradi g states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camera sens rs is exp ct d given th p olif rati of smar phon devices as the preferred latfor for developi g s u s for th BVI i divi uals. Fin ll , only 14% of these sol tions can be used in real-life sce arios with som degree of success, 29% of them are practical but hav c mbination f he igh cost or req ire f o the user to carry many sensors, 24% r limited ra ticality f r specific scenarios while 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dyn mic Range Accuracy [41\u201347]\n48 N/A 49 \u2716 \u2716 0 / N/A 1 / N/A 5 m 95% 2 0 m < R 9 m 98%\n53 \u2716 0.1 3.5 90\u201395% [54] / N/A N/A N/A 55 / N/A 0.2 m < 10\n[56] R > 2 m N/A 57 58 2 cm < R 4.5 N/A 59 N/A 67\u201398%\n[60] 2 cm < R 12 N/A [61\u201363] \u2716 \u2716 \u2716\n[64] \u2716 R = 20 c N/A [65\u201371] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe fol ow g s o ill r t h sol ti fou i the literatu e org niz d u s s ploy d. S c fic ly, e n t n is org n ze a ou the cri eri of b ing c m r on-c s is s diff en t of te q es are require for os two cas s. F ach f th w ca gori s, w conti ue by gro pi g the provide solutions ba d n th ty f se s with t o t f quent gr u bei p sented first. For t c se , t ns f si ec n q es e cribe in co rehensible man er. I more detail, S ction 3.1.1 ov s the pap u lizing a f r f the camera sens r, be it si gl c era r c nfigur ti of ip c eras, h set devices a el as 3D ca er sen or . Sub q e l , p per utiliz g he IMU nsor (ac eler me r, rosc pe,\nagnetometer), t e o t p p lar n or amo g t e sel c e papers, are prese ted. For each indivi al p pe , summary i corpor ting solution with the a v ntages and d a v n ag is written. O the other ha d, Sectio 3.1.2 c cer s solutions with n camera s s rs. Most f them i corp at an IMU sens r while other pti s include ultraso ic d i r s nsor as well as Blueto th Low E ergy ( LE) beacons a d Wi-Fi access points.\n3.1.1. Camera-Assisted Soluti s An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es mation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s ates an arise under motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 9\nand ca ra se sors is xpec ed giv the proliferati of sm rtphone devices as the preferre latfor for develo ing solu ons for the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-life c narios with so e degree of success, 29% of them are pract cal but ave c mb nati n of ei her high cost or req ire f om the user to carry m n sensors, 24% re limited practicality for specific scenarios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPa er Support Static Dy amic Range A curacy 4 47\n48 N/A 49 0 / N/A 1 / N/A m 95% 2 0 m < R < 9 m 98%\n53 0.1 3.5 90\u201395% [54] / N/A N/A N/A 55 / N/A 0.2 m < R < 10 m\n[56] R > 2 m N/A 57 58 2 cm < R < 4.5 N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716 \u2716 \u2716\n[64] \u2716 R = 20 cm N/A [65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f l wi g s ct ns w ll r s t th s lutio s fo d in the literature orga ized a u t s ns pl y d. S cifi lly, the pr entation is r nized aroun the cri t i of bei g r n n-c m a ed as diff ent s t of te iques are required for t os tw c s . Fo ach f the tw c t g r es, w co ti ue by gro ping the provided sol ions b s d n th yp of s s with th most f quent group being presented first. For b th s , the e sor f si ech iqu s re described in c prehensible m nner. In de ail, S ctio 3.1.1 ve s pap s l zing a f rm f the camera sensor, b a s l c r co figurati n of u iple c meras, he s t devices as well as 3D ca era senso s. Sub q e tl , p rs u ilizing th IMU e sor (accelerom ter, gyroscope,\nagn t meter), t ost l r s ns r am ng t e s l ct d papers, re pr sented. For ea individ l p per, summ ry incorp rati g the solution with the a vantages and di dvantage is w itt . On the other hand, Section 3.1.2 c ncer s solutions with no camera sens rs. M st f th m inc r orate an IMU sensor while ot er ptions include ultras ic a Lid r sens rs as w ll a Bluetooth L w E ergy (BLE) bea o s nd Wi-Fi access points.\n3.1.1. Camera-Assisted Solution A ncertai ty-based adaptive s ns r fusi n fra ework for Visual-Inertial Odometry (VIO) s pr p sed i [41] for stimating relat ve motion. It minimizes degradation from inacc rat sti ati by determining the states th t should b included in the estimati process. The e degr din states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is expected given the prolife ati of smartphone devices as the preferred l tfo m f r e elop g olu ons for the BVI i dividuals. Finally, only 14% of these sol tions an be use in r al-lif sc narios with some degree of success, 29% of them are pract c l bu hav a c mbin tion of ei her high cost or eq ire from the user to carry many sen rs, 24 re limited practicality fo specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPa e Support Static Dynamic R nge Accuracy 4 47\n48 N/A 49 0 N/A / 1 / N/A 5 m 95% 2 0 m < R < 9 m 98%\n53 0.1 3.5 90\u201395% [54] N/A N/A N/A N/A 55 / / 0.2 m < R < 10 m\n[56] R > 2 m N/A 57 58 2 cm < R < 4.5 N/A 59 N/A 67\u201398%\n[60] 2 cm < R < 12 m N/A [61\u201363] \u2716 \u2716\n[64] R = 20 cm N/A [65\u201371] \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nT fol wi g s ct o wi l pr e t the s lutions found in the literature organized a ou th n o e pl y d. S cific lly, p e entation is org nized aroun the cri bei g r r o - a er ass s ed s diff ent s t of t iques are required f t os tw as s. Fo a f th e wo cate o ies, w conti ue by gro ping the provided s luti s ba on th ype of s s with h most f equent group being presented first. For b th c s , t e n o f si ec niques ar d cribed in co prehensible manner. I m r detail, S cti n 3.1.1 ove s the pap s ut lizing a f r of the camera sensor, be t si r configuration of mu pl ca eras, he set devices as well as 3D ca ra s ns rs. Subseque tl , p p r utilizi g IMU sensor (accelerometer, gyroscope,\nagne e er), t mos po ular sensor among t e sel cted papers, are presented. For ach ind vidual p per, su ary incorpora i g the solu ion with the a vantages and di a va tages s written. O the other hand, Sectio 3.1.2 c cerns solutions with no camera se s rs. M st f them incorp r te an IMU sensor while other options include ultras nic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access poin s.\n3.1.1. Cam ra-Assisted Solutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es ating relat v motion. I minimizes degradation from acc rate state estim tion by determining the states that should be included in the estimation proce s. The e degr d g sta es can arise under motion characteristics that nullify\nBuilt-in turn by turn\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given he proliferation of smartphone devices s the preferred platform for developi g solutions for the BVI individuals. Fi ally, only 14% of these solutions can be sed in real-life sc narios with some egree of uc ess, % of the are practical but have a combination of either high cost or require from the user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detectio .\nPaper Support Static Dynamic Range Accuracy [41\u201347] \u2716\n[48] N/A / N/A [49] \u2716 [50] N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 < R < 9 m 8% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 m N/A [57] N/A [58] 2 cm < R < 4.5 N/A [59] N/A 6 \u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following sections will pr sent t e s lu o fou d i the lit at r o ga z d around the senso s e p yed. S ecifically, th p se t o i or a ized around h c iteria of being ra o n n-c mera st diff e t et of e q e ar qu r for those two cases. For each of se two c g ries, we co tin e by grouping he ovid d solutions based on the type of sensor w th the most frequen gr up being presented first. For both cases, the se sor fus o tech iques re escribed i co pre ensibl a er. In more detail, Section 3.1.1 cov rs the p pers utilizing form f the c m a s r, be it a single camera or a configuratio of multi le cameras, he dset devic s a w ll a 3D camera sensors. Subsequently, ap rs utilizi the IMU se s r (accelero ter, gy scope, magnetometer), the most popular sensor am ng the selected papers, are presented. For each individual paper, a summary i corp a g the s l tio with the advantages and disadvantages is written. On the other hand, S tion 3.1.2 co cerns solutions with no ca - era sensors. Most of th m incorpor e an IMU sens r whil o he t ons includ ult asonic and Lidar sensors as well as Blu to th Low Energy (BLE) be con and Wi-Fi acce s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of smartphone devices as the preferred platfo m fo devel ping solutions for the BVI individuals. Finally, only 14% of these\nl ti can b used in real-lif scen rios with s e d gree of success, 29% of the are pra tic l but have a combina ion of either high cost r require fr m the user to carry many sensors, 24% are li ited practicality for pecific scenarios while 32 are purely experimental.\nTabl 3. Soluti ns and obstacl detecti .\nPaper Support Static Dynamic Range Accuracy [41\u201347]\n[48] / / [49] \u2716 [50] / / [51] / 5 m 95 [52] 0 < < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395 [54] / / / / [55] N/A 0.2 m < R < 10 N/A [56] R > 2 m / [57] / [58] 2 c < R < 4.5 / [59] 67\u201398% [60] 2 cm < R < 12 m /\n[61\u201363] [64] R = 20 N/A\n[65\u20137 ] \u2716 \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh follow g cti will p es nt the s lut o fou i th lit ra u organ zed round th s ns mpl ye . Specifi l , the p e ta ion is ga ized arou d th c iteria of b i g c mera n n- am r a si t d s ff nt ts f t n qu ar req ir d for th s t o cases. For h f the two cat g ries, we conti e by gr upi t p vided solutions bas d on the type f s nso with the most f equ nt group being t first. For oth cas s, the sensor fusio tec nique are sc ib d in a c p ehensibl manner. I ore d tail, Section 3.1.1 c v r the p pe s utilizing a f r of e cam a sensor, be it singl c m a r a co figuratio of multiple cameras, hea s t d vic s as well as 3D cam r sensor . Sub quently, paper utilizing e IMU en r accelerom t , gyro op , magnetometer), the most popul r s ns r among the sel cted papers, are presented. For each i dividual paper, a summary incorporating the solution with the advantages and disa vant g s is writt n. On the other hand, Secti n 3.1.2 c cerns s l ti w th o cam ra nsor . Mo f h m incorp ra e an IMU s s r whil other ptions include ultrasoni and Lidar s nsors s w ll as Bluetoot Low Energy (BLE) beaco s and Wi-Fi access points.\n3.1.1. Camera-Assist d Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is prop sed in [41] for estimating relative motion. It minimizes degradation from inaccurate state esti ati by deter ining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is exp ct d given the p oliferation of smar phon devices as the preferred platfo for dev loping solutions for the BVI individuals. Finally, only 14% of these solutions c n be used in real-l fe cenarios with some degree of success, 29% of the are practical but hav a c mbination f e the high cost or require f o the user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32 are purely experim tal.\nTable 3. Solutions and bstacle det ction.\nPaper Support Static Dynamic Range Accuracy [41\u201347]\n[48] N/A N/A / [49] \u2716 \u2716 [50] N/A / N/A / [51] N/A N/A 5 m 95 [52] 0 < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A / N/A / [55] N/A N/A 0.2 m < R < 10 / [56] R > 2 m / [57] N/A / [58] 2 m < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followi g s on ill pre t h ol i s fo n i the l eratur org niz d r u d th se s e ploy . Sp cific lly, t e e nt ti n is org zed ar und the c iteri f b ing c mer r on-c m a s i t s d ff re t s t of techn que ar require for thos two ca s. F r ach f th two ca gori , conti by groupi g the provide oluti ns ba d n th typ f se so with th o t f quent gr u bei p sented first. For b t c s s, t s ns f si techniq es re e c ibed in com rehen ibl an er. I more det il, Sec ion 3.1.1 vers the p p s u ilizi a for f the c mera sens r, be it si le c era r a onfigur ti f ltipl c r s, h a set evices a el as 3D ca era sen or . Sub que tly, p p utilizi g the IMU ns r (ac eler met r, roscope, magnetometer), th ost popular nsor a o g the selected papers, are presented. For each indivi ual pape , a summary incorporating solution with the advantages and d advan ag is w itt n. On the other ha d, Secti n 3.1.2 conc rns s lutions with n camra nsors. Mo t f them i corp at an IMU se sor whil other pti s include ultrasonic d Li r sen r as well s Blu to th Low E rgy (BLE) beacons a d Wi-Fi access points.\n3.1.1. Cam ra-Assisted Soluti s An uncertainty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is pr posed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est ation by determining the states that should be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 9\nand camera se sors is xpec ed given the p oliferation of martphone devices as the preferre latfor for developi g solu ns for th BVI individuals. Finally, only 14% of these solutions an b used in r al-lif sc narios with some d gree of succ ss, 29% of them are p act cal but have a combination of either high ost or requir from th user to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTabl 3. S luti ns nd bs acle d t ction.\nPaper Support Static Dyna ic Range Accuracy 4 47\n48 / 49 0 N/A N/A 1 N/A N/A 5 m 95% 2 0 < 9 m 98%\n53 0.1 3.5 90\u201395 [54] N/A N/A N/A / 55 N/A N/A 0 2 m < R < 10 m\n[56] R > 2 m / 57 58 2 cm < R < 4.5 / 59 N/A 67\u201398%\n[60] 2 c < R < 12 / [61\u201363]\n[64] \u2716 R = 20 m N/A [65\u201371] \u2716 \u2716 \u2716\n[72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll w g section w l pr t th s lutio s fou i th literatu e orga iz d ou d the s r p y . Specifi lly, the pre tation is rganize a oun the cri ria of bei r r n - a a s is ed a diff nt ets f t iq es are require for h se t o c s s. F r e ch of e w c teg ies, we co ti e gr uping the p ovi d sol tions based n the type of s sor with th o t frequen g oup being pr sented first. F r bo c ses, t nsor f t chniq s are described in comprehensibl ma er. I m re d t il, Secti 3.1.1 c v rs papers tilizing a f rm f the cam a se or, b it a single c mera or co figur tion of multiple c meras, head et d vices as well a 3D\ne nsors. Sub eque tl , pa e utilizing the IMU s r (accel rometer, gyroscope, agnetometer), t e ost popular sens r among the selected papers, are pres nted. For each indiv dual paper, su m ry in o p rating th so uti n with the advantages and disa vantag s ritten. O th t er h , Secti 3.1.2 concerns s l tio s th no cam era sens r . Most f m inco orate an IMU s nsor while other options include ultraso i and Li a sens rs as well s Bluetooth Low En rgy (BLE) b aco s and Wi-Fi access points.\n3.1. . Camera-Assiste S lutions An uncertai ty-based adaptive sensor fusion framew rk f r Visual-Inertial Odometry (VIO) s proposed in [41] for estimating rel tive motio . It minimizes degradation from inaccurate t t estimatio by determi ing the states that should be included in the estimati n process. The e degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected given the prolife ation of smartphone devices as the preferred platform for develop g olutions for the BVI individuals. Finally, only 14% of these olutions an be used in r al-lif scenarios with some degree of success, 29% of the are pract cal bu hav a c mbination of ei her high cost or require from the user to carry many sen ors, 24% are limited practicality fo specific sce arios while 32 are purely experi-\nental.\nTable 3. Solutions nd obstacle detection.\nPaper Support Static Dynamic Range Accuracy 4 47\n48 N/A [49] 50 N/A 51 N/A 5 m 95 52 0 < < 9 m 98%\n[53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N A N/A / [55] N/A N A 0.2 m < R < 10 m / [56] R > 2 m / [57] N/A / [58] 2 m < R < 4.5 m / [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT e foll wi g sect s wi l pr sent the s lutio s foun in the literature organized r u t enso e pl y . S cific lly, p e nt tion i org ized around the crir a b ing c r on-ca e as isted as diff rent s t of tec niques are required f t tw c es. Fo ach f th two cate o s, w conti ue by gro ping the provided soluti s ba o th type of s s with h most f equent group being presented first. F b t c s s, t e s ns r fusi n t chniques a d c ibed i comprehensible manner. In mo et il, Secti n 3.1.1 overs the p p utilizing a for of the camera sensor, be it sing or a configuration of mu tipl am ras, h a set devices as well as 3D cam ra s ns r . Sub que tly, p p r utilizing the IMU sens r (accelerometer, gyroscope, magnet me er), th mos popular sensor among the selected papers, are presented. For each ind vidual pap r, summa y incorporating the solution with the advantages and di a vantages s written. On th th r hand, Secti n 3.1.2 concerns solutions with no camera ns rs. M st f them inc rp rate an IMU se s r whil other ptions include ultras nic a d Li ar s sors as w ll a Bluetooth Low Energy (BLE) beacons and Wi-Fi access p in s.\n3.1.1. C ra-Assisted Solutions An unce tainty-b sed adaptive sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is pr posed in [41] fo estimati g relative motion. It minimizes degradation from acc rat stat estimation by deter ining the states that should be included in the estimation proc s. The e degrad ng sta es can arise under motion characteristics that nullify\nIncreasing f equency proximity progress upda es\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand c mera sensors is expected given the proliferation of smartphone devices as he eferred latform for developing solu ons for the BVI individuals. Finally, only 14% of these sol tions can be used in re l-life scenarios with some degree f success, 29% of them are practical but have combina of either high cost r require from the ser to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r Supp rt Static Dynamic Range Accuracy [4 47]\n48 49 0 N/A N/A / 1 N/A N/A 5 95% 2 0 9 98% 3 0.1 m 3.5 90 5\n54 N/A N/A N/A N A [55] N/A N/A 0.2 m < R < 10 m / 56 R > 2 m\n[57] / / 58 2 c < R < 4.5 m 59 N/A 67\u201398% 60 2 cm < R < 12\n61 63 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716\nT e followi g s cti ns w ll r s nt t e sol tions f d i the lit r t r a iz around th sens rs employe . S cifically, th p tation g ize round r - ter a of b ing cam ra or - s isted as diff ts of t ch q e are r qui d f r those two cas . F e ch of thes two c tegories, w c ntinu y g oupi g the p v d solutio s bas d on th type of sensor wi h the most frequen gr up b ing te first. For both cases, th sensor fusion techniques are escribed in co pre nsible anner. In more detail, Secti n 3.1.1 covers the p p rs ut lizi g f the m r en or, be it a single cam or a configuration of ultipl meras, he t evices s well as 3D camer sen ors. Subseque tl , pa r utilizi t IMU sensor (accele om r, gyr s pe,\ngnet meter), t most p ular s n or among the selec ed papers, pr s ed. For each individual paper, summary incorporating the soluti n wit the advant ges an disadvantag s is written. O the other han , Section 3.1.2 c cerns sol tio s with n cam era sensor . Most of them incor rate an IMU s nsor whil other tions i clude ult asoni and Lidar sensors s well as Blueto th Low Energy (BLE) b co d Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of smartphone devices as the preferred platform for dev loping solutions for the BVI individu ls. F nally, nly 4% of th se solutions c n be used i real-life scenarios with some degree of success, 29% of them are practical but have a combination of either high cost or require fro he u er to carry many sensors, 24% are limited practicality for specific scenarios while 32% are purely experimental.\nTabl 3. Solutions and obstacle detectio .\nPaper Support S atic Dy amic Rang Accu acy [ 47]\n[48] N/A N/A / [49] [50] N/A N/A / / [51] N/A N/A 5 95% [52] 0 m < R < 9 m 98% [53] .1 < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 m N/A [57] N/A N/A [58] 2 c < R < 4.5 N/A [59] N/A 67\u201398% [60] 2 m < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh follo ing c i n will pr ent he ol tions fou d n the lite ure n z d r u d th sensors ploye . Spec fically, th pr se t ion i rgani d ar und the crit ria of being c r or n n-camera assist d s diff r nt s s f techniques ar r qui d for thos two cas . For ch f t two at g ri s, w co tinue gr u g th provid d solution b sed o th typ f se sor with th most frequ t roup b ing prese ted first. For both cas , th sensor f sion t c iques are de crib d i a comprehe sibl a ner. In more et il, ection 3.1.1 covers the apers utilizi g a he c m r enso , b it a singl camera or a configu ation of multipl ca r s, h adset evices s well as 3D camera s n ors. Subs q e l , p p utilizi t IMU se s r (ac el rom t r, yroscop ,\ngnetometer), t most opular sen among the selec e p pers, a e prese ed. For each individual paper, a summary incorporati g the solution with the adva tages and disadvantages is written. On th other han , Sect n 3.1.2 c ncern soluti s with o ca - era sen ors. Most of them inc rporat a IMU sen r while ther o ti ns include ultr - sonic and Lidar sensors as w ll as Bluetooth Low En gy (BLE) b acon and Wi-Fi acc ss p ints.\n3.1.1. Camera-Assist d Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det rmi ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expect d gi en the p oliferation of smar phon devices as the prefe red latform for developing solu ons for th BVI individuals. Finally, only 14% of these solutions can be u ed in real-life scenarios with some deg ee of success, 29% of them are practical b hav a c mbinatio f eithe high cos r require f o th user to carry many sensors, 24% re limited pra ticality f r specific scenarios while 32% are purely experimental.\nable 3. Solutions and obstac e detecti n.\nPap r Sup ort Static Dyna ic ange ccuracy 4 47\n48 49 0 N/A N/A / 1 N/A N/A 5 m 95% 2 0 9 98% 3 0.1 3.5 90 5\n54 N/A N/A N/A N A [55] N/A N/A 0.2 < R < 10 / 56 R > 2 m\n[57] / / 58 2 c < R < 4.5 m 59 N/A 67\u201398% 60 2 cm < R < 12\n61 63 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh foll wi g sec i ns will pr s t th s lutio f u n li ra ur organ z d around th sens r empl y . Sp cifically, the pres nt ti i gan zed ar un the c iteria of b ing c r or n n-ca a s ist as diffe nt se s f ech q es ar r quir d for tho e two ca . For ach of t w categories, w contin e by grou ing t e pr vid d\nlutions ba d n th ty of e sor with the ost fr qu t g oup being prese te firs . For both ca es, th sensor fusion tech iqu s are d scribed in a c pr hensible man er. I m e detail, S cti n 3.1.1 cov rs the pa rs utilizing a f the cam r sensor, be t single camera r a configur ti n f ultipl ca ras, ads t d v es s el as 3D c mera sen ors. Subseq e tl , pap rs util zing th IMU sor (ac lero e r, gyroscope,\nn to eter), t m st p p lar or among th s le e p pers, a e prese e . For ach individual p pe , a summary incorporating solution with the advantages and d sadvan g s is written. O the other ha , Secti 3.1.2 c nc rns s l ti with no cam era nsor . M st f them inc rp ate an IMU s ns r wh le oth r ons include ultrasoni nd Lidar sensors as w ll as Bl e ooth Low E ergy (BLE) b aco s and Wi-Fi access points.\n3.1.1. Camera-Assisted Sol tions An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est m tion by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand ca ra sensors is xpected giv n the proliferation of sm rtphone devices as the preferred l tform for d veloping solu ons for the BVI individ als. F nally, only 14% of these solutions an b us d in real-life c narios with some degree of success, 29% of them are practic l b h ve a c mb nati n f eith r high cost or require f om the user to carry many sensors, 24% are limited practicality for specific scenarios whil 32% are purely experimental.\nTabl 3. S lutions a d b tacle de ction.\nPap r Suppo t Stati Dy a ic ange A c racy 4 47 [48] / N/A / [49] [ 0] N/A N/A / / [ 1] N/A N/A m 95% [ 2] 0 m < R < 9 m 98% [53] 0.1 3.5 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m / [56] R > 2 m N/A [57] / / [58] 2 m < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f low ng sec i n will pr s nt th s lutions fo d n the literatu e organiz d rou d t ensors ploye . Sp c fi lly, he p e tation rganized aroun the crit ia of b ing c r or n n-ca r a sist d as ifferen sets f t chniques ar r quir d for t ose two cas . F r eac f t e tw c te i s, w continu by grouping the provi d solu i ns based n th type f se s r with th most f quent group being pr sented first. F r b t c s , t e sensor fu ion t chniqu s re describ in a co prehensible man er. I mor d t il, Secti 3.1. overs t papers ilizing f the cam ra se or, be it a single cam r a o figurati n of multi l camer s, h dset devices s well as 3D\ne a sen or . Sub eque tl , pap r u ilizing the IMU sens r (acceleromet r, gyroscop , gnetom ter), os ular s n r amo g the le d p pers, a e presen ed. For ea h individu l paper, a ummary incorp rating the solution with the advantages and dis dva tag s is itte . O th oth han , Secti 3.1.2 concer s s luti ns ith no camera sensor . Most of th m inc r orate n IMU ensor wh le ther options include ultrasonic an Lid r se s rs as w ll s Bluetoot Low En rgy (BLE) b a ons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odometry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat stimati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c ra sensors is exp cted given the prolife ation of smartphone devices as the prefer ed latform f r dev lop g olu ons for the BVI individuals. Finally, only 14% of these solutions an be used in r al-lif sc narios with so e degree of success, 29% of them are pract c l b hav a c bination of ei her high cost or equire from the user to carry many sen ors, 24 are limited practicality fo specific scenarios while 32% are purely experi-\nental.\nTable 3. Solutions and obstac et ction.\nPa Support Static Dynamic nge Accuracy 4 47\n48 49 0 N/A N/A / 1 N/A N/A 95% 2 0 9 m 98% 3 0.1 3.5 90 5\n54 N/A N/A N/A N A [55] N/A N/A 0.2 m < R < 10 m / 56 R > 2 m\n[57] / / 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 60 2 c < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe fol wing s c i ns wi l pr s t he s lutio found the literature organized ar und t ensors em l y . Spe ifically, p esentation gan zed aro n the cri-\nr a of b in cam r r non camer assist d as different ts of t c niqu r r quire f those two cas s. For ac of t tw c tegori s, we continue by g oupi g the provided\nl tions b on the ype of s sor wit e most frequent group being nte first. For b th cas s, the se fusion techniques ar described in a comprehensible manner. I re d tail, S cti n 3.1.1 c vers th apers utilizing a f the camera sensor, be it a sin or c figuration of ultipl cameras, headset evices s well as 3D cam r se or . Subseque tl , papers tilizing the IMU sensor (acceleromet r, gyros ope,\ngnet me er), t mo popula or mong the sel c ed papers, a e presen ed. For ach ind vidual paper, summary incorpora ing the solu ion with the advantages and di vantag s writt n. On the other hand, Section 3.1.2 c ncerns sol o s with no cam era ensor . M s of them incorp rat an IMU s sor wh le other options i clude ultras i and Lidar sens rs as ell Bluetoo h Low Energy (BLE) beaco s and Wi-Fi access points.\n3.1.1. C era-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by determining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nRepeating information\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd cam ra sensors s xpected given th rolife tion f art hon d v c a h ferred platform for developing solutions for he BVI individuals. Finally, only 14% of these solutions can be used in re l-life scenarios with some degree of success, 29% of them are pr ctical b have c mbina of either high cost r require from the user to carry many sensors, 24% are limited practicality for specific sce arios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r Supp rt Static Dynamic ange Ac uracy 41 47\n48 N A N A N A N A 49 0 / N/A / 1 / N/A 5 95%\n0 9 98% 0.1 m < R < 3.5 90 5\n5 N/A N/A N/A [55] N/A N/A 0.2 < < 10 m N/A 56 R > 2\n[57] N/A N/A 58 2 c < R < 4.5 m 59 N/A 67\u201398% 60 2 cm < R < 12\n61 63 [64] \u2716 R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT following s c ion will prese th olutio s fo d in t lit ure anize around th sensors employe . Sp cifically, the pr se at o is g ze r und th\nria of be g am ra or n -c a i te s iff n ts f tech q are req i e f r ho e tw cas . F ch of thes tw c e orie , w c n i y g ou i g p ovi d solution b sed on th type of s nso with the most fr qu n g being pr s ted fi . For bo h cases, the sens r fusi n techniqu ar rib i m ehensibl an e . In more detail, S cti n 3.1.1 covers th p p rs utilizing f r of th c me se or, be it a si gle cam ra or co figuration of l i l c er , t device s well s 3D camera sensors. Subseque tl , pap rs utilizing th I U se s r (a cel o t r, gyro cop ,\ng etometer), t m s popular s s r am g the s l t d p per , ar pr se t d. Fo each i dividual aper, a summary incorporating th soluti with the advant ges and disadvantages is written. On the other hand, Section 3.1.2 concerns solutions with no camera sensors. Mo t of them incorpora n IMU sensor while other options i clude ultr - sonic and Lid r sensors as well as Bluetooth L w Energy (BLE) beaco s a d Wi-F acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camer sensor is xpected give the rolif ration of ma tp on d v ces as th re ferr d platform for d vel ping solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenari s with some degree of suc ess, 29% of them a e practical b have a c mbination of either high cos or require fro he u er to carry an sensors, 24% are limited racticality for spec fic sce ios while 32% are purely experimental.\nTable 3. Solutions and obstacle detection.\nPap r Supp rt Static Dynamic ange ccuracy 41 47\n48 N A N A 49 0 / 1 N/A 5 95% 2 0 9 m 98% 3 0.1 3. 90 5 4 N/A\n[55] / N/A 0.2 0 m N/A 56 R > 2\n[57] / 58 2 c < R < 4.5 59 / 67\u201398% 60 2 m < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 [72] \u2716 N/A [73] \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716\nThe f llowing c i s will r ent s lu ions fou d n the lit at organiz round the e ors empl yed. Specifically, the pr tat s o gan z d arou d the c ieri f bei g camer o o -cam ra si ed s d ff et of t ch ique ar equire f h e t cas s. F c of the e w c tegori , we c i ue by gr pi g pr vi d solut b sed on h yp of s nso wit h m st freq nt r up b ing r nte first. Fo both ca es, th se s r fu ion t ch iqu s are d scribed in a c r n ible anner. In more detail, Sec i n 3.1.1 cove s the pa e s util zing form of the mer e sor, be t si gle cam ra or a co fi ration of mul ip ca ra , head et devic s well a 3D cam ra se s rs. Sub eq e t , p p s u ilizi g th IMU nsor (ac l o r, yroscop ,\ngnet meter), t st opul m ng the el cted pap rs, re p ese ted. F r e ch indivi ual p p r, summary incorporating the solution with the advantages and disadvantages is written. On the ther hand, Section 3.1.2 concerns solutions with no camera sensors. Mo t of them incorporat an IMU sens r while ther ption include ultra sonic and Lidar sors as w ll Bluetooth Low Energy (BLE) e ons nd Wi-Fi ccess points.\n3.1.1. Camera-Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det rmining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected gi en the proliferation of smartphone devices as th prefe red platfor for developing solutions for the BVI individuals. F nally, only 14% of these solutions can be used in real-life scenarios with some deg ee of success, 9% of them re practical but have a combin of eit r high co t r require from the ser o carry many sensors, 24% are limite practicali y for specific sce arios w ile 32% are purely experime tal.\nable 3. Solutions and obstacle detection.\nPaper Sup ort Sta ic Dy a c Ra ge Accuracy [41\u201347] \u2716 \u2716 \u2716 \u2716\n[48] N/A N/A N/A N/A 49 \u2716 \u2716 \u2716 \u2716\n[ 0] / / N/A / 1 N/A N/A 5 95%\n0 m < R < 9 98% 0.1 3. 90\u201395%\n[ ] N/A N/A / / [55] N/A N/A 0.2 m 0 m / [56] R > 2 m N/A [57] N/ / [58] 2 c < R < 4.5 N/A [59] / 67\u201398% [60] 2 c < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe follow ng ct s w ll present th soluti f nd i the lit r tu o an ze arou d th sen or e ploy . Sp cific lly, th pr ent ti n i g i ed r u the cri er a of being c r or n -c e assist d as d ff nt ts f te hn ques re q r d f r tho e tw ca . F ch of th w cat gori s, we ntin y grou i g t p ovide oluti n b ed on th ty of r wi h th ost f equ t oup being res ted fir t. For b th cases, the sensor fusi n techniques are d scribed a co pr hensible man er. In mo e et il, Sectio 3.1.1 c ve s the p p rs utilizing a f rm of the camer sensor, b it a ingle c era or co figu io of lti le c r s, ea et dev es as well as 3D c mera sen o s. Subsequ ntly, pape utiliz ng th IMU s s r ( cc r meter, gyroscope, m neto eter), the most popular ensor among the sel cted p p rs, are presente . For ach i dividual p per, a summary inc rp r ting th soluti with the dvant ges nd isadvan ges is writte . On the other hand, Secti 3.1.2 c ncerns solutions with no camera ensor . Most f them i o porat an IMU sensor while oth r o tions inclu e ultra son c and L dar sensors as well a Blu ooth L w E ergy (BLE) beacons a d Wi-Fi cces po ts.\n3.1.1. Camera-Assisted Soluti s An uncert inty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat stim tio by d termining the states that s o ld be in luded in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is xpected given he proliferation of sm rtphone devices as the preferr d pl tform for d veloping solutions for th BVI individ als. F nally, only 14% of these solutions can b used in real-life sc nari s with some degree of success, 9% of them are pr ctical but h ve a comb nati n of eith r high cost or quir f o the user to carry many sensors, 24% are imi ed ractic lity for specific sce arios whil 32% are purely experimental.\nTable 3. Solutions a d obstacle detection.\nPaper Sup ort Stati Dy ic ange Accuracy 41 47 [48] N/A / N/A / 49\n[ 0] / / N/A / 1 N/A N/A m 95%\n0 m < R < 9 m 98% 0.1 3. 90\u201395%\n[ ] N/A N/A / / [55] N/A N/A 0.2 0 m / [56] R > 2 m N/A [57] N/A / [58] 2 m < R < 4.5 m N/A [59] / 67\u201398% [60] 2 m < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followi g s tio will pr se th solutions fo nd in th lit r ture o anize u the s sors pl ye . Sp ific lly, th pr se ta io is g ni d around the cr a of b ing c m ra r no -ca ass ste as iff en s t f t chn ques are req i d f r t se t o c . F f hes o c t g i s, w c ti u y groupi the provide s luti ns bas d on h ty of s with th mo f qu nt ro p bei g resent fir t. For b th c ses, the s n or fu ion te hniq es are described i a co pr he sible anner. In re etail, Section 3.1 overs the pap rs lizing a f rm f the camera sensor, b it a ngle cam ra o a figura o of multi le ca eras, head et d v ces as well 3D\nera sensor . Subsequ tly, pape utilizing th IMU nsor (acc lerometer, gyroscope, g etometer), the most p pular se sor mo g th s lect d paper , are presented. For each individ l paper, a ummary i o p rating th soluti with the advant ges and dis dva tag s is w itte . O the ot e hand, Section 3.1.2 c ncer s solutions ith no camer se sor . Most f them inc por t a IMU ensor while ther optio s include ultra onic an L d r se so s as w ll a Bluetooth Low En rgy (BLE) bea ns a d Wi-Fi access poi t .\n3.1.1. Camer -Assisted S l tio s An uncertai ty-based adaptive s ns r fusion framew rk f r Visual-Inertial Odometry (VIO) is proposed i [41] for stimati g r lat ve motio . It minimizes degradation from inaccurate t t stimation by determi ing the states that should b included in the estimation process. These degrading states can arise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is exp cted given the prolife ation of smartphone devices as the prefer ed platfor f r dev lop g solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life sc narios with so e degree of success, 9% of them are pract c l bu hav a comb natio of ith r high ost or equire from the user to carry many sen ors, 24% are l mited practicality fo specific sce arios while 32% are purely experimental.\nTable 3. Solutions and obstacle etection.\nPa er S p or Static Dy amic ge Accuracy 41 47\n48 N A N A 49 0 / / N/A 1 N/A N/A 95%\n0 9 m 98% 0.1 3. 90 5\nN/A N/A / [55] N/A N/A 0.2 0 / 56 R > 2 m\n[57] N/A / 58 2 cm < R < 4.5 m 59 / 67\u201398% 60 2 cm < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll g s c i n w l prese t h solut ons fo nd i the lit r ture o anize ar un t e or e pl y . Specific lly, p esentation i ga i ed round the cri\nr a f b ing c m ra or n c me assis as d ffe ent s t of echn qu s are req ir d f r th s t ca . F h of h tw at gori , we cont nue y groupi g th rovide\nlutio b n th y of or wi e ost frequ t roup being resente fir t. For bot cas s, the sen o f sio techniques ar described in a comprehensibl ma ner. I or et il, n 1 overs the papers utilizi g a f rm of the camera sensor, be t a sin e r a c fig ra ion f mul pl ca ras, head et dev ces as well as 3D camera se ors. Subseque tly, pape utilizing th IMU sens r (acc lerometer, gyroscope,\ng et m er), th m t popular s n or mong the sel cted papers, are presented. For ach i d vidual paper, s mmary incorpora ing th solu i with the advant ges and disa vantag s is writt n. On the other hand, Section 3.1.2 concerns solutions with no camer se sors. Mos of them inco p rat an IMU e sor while other options include ultra sonic and Li ar se s rs as ll a Bluetoo h Low En rgy (BLE) beacons and Wi-Fi access points.\n3.1.1. C mera-Assisted S l tions An unce tainty-based adaptiv senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from accurate state estimation by determining the states that should be included in the estimation proce s. These degradi g sta es can arise under motion characteristics that nullify\nCardinal directions\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is expected give the prolifer tion f smart h ne device a the r - ferred platform for developing solutions for the BVI individuals. Finally, only 14% of these solutions can be used in real-life scenari s with some degree f success, 29% of the are practical but have combi a of ither high cost r r q ire fr m the user to carry a y sensors, 24% ar limited practicality for pecific sce rios w ile 32% are purely expe i mental.\nTable 3. Solutions and obstacle d tectio .\nPap r Supp rt Static Dynamic ang Ac uracy 41 4 [48] N/A N/A N A N A 49\n[ 0] / / / 1 N/A N/A 5 95\n0 < < 9 98% 0.1 3. 90\u201395%\n[ ] / / / / [55] N/A N/A 0.2 0 m N/A [56] R > 2 m / [57] N/A / [58] 2 cm < R < 4.5 m [59] / 6 \u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following s ctions will res n th olutio s f d i t lit r t re o anize around th sensors employe . Sp cific lly, the s tati is g n ze r und th eria of being cam ra or n n-ca er a si ted s ff nt ts o t chn que r q ired f r those two cas . F e ch of hese two c egories, we c n n e y g ou i g provi d solutions based on th ty e of senso with the mo t fr que t grou being r se te fi For both cases, the sens r fusion techniques are d scribed i c prehensible ma n r. In ore detail, Secti n 3.1.1 covers the papers utilizing a f r of the came sensor, be it a single camera or configuration of l ipl r s, he dset evic s s well s 3D camera sensors. Subseque tl , pa rs u iliz g th IMU sensor (a cele om t r, gyro cope,\ngnetome er), most p pular s nsor among the s lected p per , r pres t d. F each i dividual aper, summary incorporati th soluti wit the adva t ges a disadvantages is written. On the other hand, Section 3.1.2 concerns soluti ns ith o camera sensors. Most of them i c rp ra n IMU sensor whil other options i clude l r sonic and Lidar sensors as well Bluetooth L w Energy (BLE) b ac d Wi-F acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can arise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nand camer sensor is x ected give the roliferation of ma tp one d v ces as t e re ferr d platform for devel ping soluti ns f r the BVI individuals. Finally, only 14% of these soluti ns ca be used in real-life scenarios with some degree of success, 29% of them are practic l b have a c mbi ati of ither high cos r req ir fr t e u er to carry man sensors, 24% are limited ractic lity fo p fi ce ios while 32% re p ly xpe i\nental.\nTabl 3. Solutions and obstacle detection.\nPap r Supp rt Static Dyn mic Range ccuracy [41\u201347] \u2716 \u2716\n48 N A 49 0 /"
        },
        {
            "heading": "1 N/A 5 95",
            "text": "0 9 m 98% 0.1 m 3. 9 5\n[55] N/A N/A 0.2 0 m / 56 R > 2 m\n[57] / 58 2 c < R < 4.5 59 / 67\u201398% 60 2 m < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe following c io s will r ent he s lu i ns fo d in the lit t r aniz around the se ors employe . Specifically, th pre e at on s o g niz d arou d the c i eri f bei g cam or no -cam a s ff e ets f tech ques ar required f r ho e t cas . F e c of es tw ategori s, w c tinue rouping t e pr vi d soluti based on h type f senso with he ost freq nt up being re e ted first. For both ca es, the sens r fu ion techniques are described in a com r hen ible anner. In more detail, Sec i n 3.1.1 covers the papers util zing a form of the camer ensor, be it si gle cam a or a co fi ration of multip ca , h a et evic s ell a 3D cam a sens rs. Sub eque tly, p pers u ilizi th IMU s ns (acc l r meter, gyroscope, m gnet meter), he m st p pul ng th l c pap rs, are pr se te . F r e ch i d vi ual p p r, summ ry i co or g th solu i with the a vant ges and disadvantages is written. On the t er hand, Sectio 3.1.2 concer s soluti ns with no camera sensors. Mo t of them incorpo an IMU s or while ther ptio include ltra sonic a d Lidar sensors as well a Bluetooth Low Energy (BLE) be ns a d Wi-Fi acc ss points.\n3.1.1. Camer -Assist d Soluti ns An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand camera sensors expected gi en th proliferation of smartphone devices as th preferred platform for deve oping solutions for the BVI individuals. Finally, only 14 of these solutions can be used in r al-life scenarios with some deg ee of success, 29% of the re practical but have a combi ati of it r high c t req ire from th er t carry many sensors, 24% e limi e practicali y for specific c nari s w il 32% re urely ex me tal.\nable 3. Solutions and ob tacl detectio .\nPaper Sup ort Sta ic Dy a c a ge ccuracy 41 47\n48 N A 49 0 / / N/A N A 1 N/A N/A 5 95 2 0 m 9 m 98% 3 0.1 < R < 3.5 90 5\n54 / / N/A [55] N/A N/A 0.2 < R < 10 m / 56 R > 2 m\n[57] N/ / 58 2 c < R < 4.5 59 N/A 67\u201398% 60 2 cm < R < 12\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/ N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh following s ct will pres nt th soluti s fo i the lit r ture aniz around th s n or e ploy d. Sp c fically, the pr tatio i rgani ed r u the cri ter a of being c m ra or n -c e sist d a diffe nt ets f n qu s r r q ir f r th tw cas . F r ch of t s tw cat gori , we c tinue y groupi g the p ovided ol on ba ed on th typ of s s r with the most fr qu nt oup being pres nted fir t. For b th cases, the nsor f si n technique a e d sc ib d a compr nsible man er. I mo e et il, Sec 3.1.1 c v s the pap r utilizing a f r of the camer sensor, be it a ingle camera or o figura ion of ultipl ca er s, hea t d v es s well as 3D c mera senso s. Subseq e tl , pap utilizing th IMU s s r (acc l r met r, gyrosc e,\nn o eter), t m st p p lar e sor mong th s lecte p p r , are prese te . For ach i dividual p per, a summary incorp r ting th soluti with the dvant ges nd disadvan ges is writte . O the other han , Secti 3.1.2 concerns soluti n with no camera ensors. Most f them inc rp rate an IMU sensor while oth r op ions inclu e ultrasonic and Lidar sensors as w ll as Blue ooth Low E ergy (BLE) b acons a d Wi-Fi access points.\n3.1.1. Camera-Assisted S l tions An uncert inty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) is proposed in [41] fo estimating relative motion. It minimizes egradation from i accurate stat stim tio by d termining the states that s o ld be in luded in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand cam ra sensors is xpect d given he roliferation of m rtphone devices as the pre ferred platform for d veloping solutions for th BVI individ als. Finally, only 14% of these solutions c be used in real-life s narios with some degree of success, 29% of the are practical b hav a comb ation f ith r high cost r q re f om th ser t c rry ma y sensors, 24% imi ed ractic lity for specific cenario whil 32% are purely exp i mental.\nTable 3. Solutio s and obstacle det c on.\nPaper Sup ort Stati Dy ic ange Accuracy 4 47\n48 49 \u2716 0 / / N/A N A 1 N/A N/A 95 2 0 9 m 98% 3 0.1 < R < 3.5 m 90 5\n54 / / N/A [55] N/A N/A 0.2 m < R < 10 m / 56 R > 2\n[57] N/A / 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 60 2 < R < 12\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe followi g sec ion will r s the solutio s f i t e lit r ture o anize u d th s nsors pl y d. Sp ific lly, th pr se ta o is rganiz d arou d the cr\nt i of b ing c m ra no -ca a s sted as iffe ent s t of t n ques are req i e f r o e t o c s. F e ch f t s t at go i s, w conti groupi the pr vi d soluti ns bas d o th ty e of s sor with th mo f quent gro p bei g presented first. For b th c ses, the s n r fu ion t hniques are described i a com r he sible man er. In more d ail, Sect 3.1 overs the pap rs lizing f rm f the camer se sor, be it a gle cam r a figurat o of multi l ca er s, head et d vice s well 3D a era sensor . Subsequ tl , pap rs uti izing th IMU nsor (acceleromet r, gyroscop ,\ngnetom er), t o t p pular e s r mong th lec d p per , are presented. For each i dividu l paper, a ummary i o p rating th soluti with the advant ges and dis dva tag s is itte . On th ot ha , Sectio 3.1.2 c ncer s soluti ns ith no camer se sor . Mo t f them inc po ate IMU ensor while other optio s include ultrasonic an Lid r se s rs as w ll as Bluetoot Low En rgy (BLE) b a ons and Wi-Fi ccess points.\n3.1. . Camer -Assi ted S l tio s An uncertai ty-based adaptive s ns r fusion framew rk for Visual-Inertial Odo etry (VIO) is proposed i [41] for stimati g r lat ve motio . It minimizes degradation from inaccurate t t sti ation by determi ing the states that should b included in the estimation process. These degrading states can arise und motion characteristics that nullify\nSensor 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors i exp ted given the prolife ation of s artphone devices as the preferred platfor f r dev lop g solutions for the BVI individuals. Finally, only 14% of these solu ions can be used in real-life sc narios with so e degree of success, 29% of the are pract c b hav a co b ation of i h r i h os eq ire from the use to carry many sen ors, 4% e l mi ed practi ity f p cifi sce i s wh l 32% are purely expe i\ne tal.\nTable 3. Solutions nd obstacl e ction.\nPa er S p or Static Dy a ic ge Accuracy 41 47\n48 N A N A 49 \u2716 0 / / N/A N A 1 N/A N/A 5 95 2 0 9 m 98% 3 0.1 < R < 3.5 m 90 5\n54 / / N/A [55] N/A N/A 0.2 m < R < 10 m N/A 56 R > 2 m\n[57] N/A N/A 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 0 2 < R < 12\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh foll g s c i n w l pres t h olutio s fo nd i the lit r ture o anize round t or e pl y . Sp cific lly, p se tation i organi ed ar und the cri\nt r a of b ing c mera or n came ssi t d as d ff e t s t of ec q es ar r q ire f thos t o cases. F h of t s tw t gori s, w cont nue y group g the provided\nl tions ba ed on he yp of s sor with he most frequ nt rou being presented first. Fo both cases, the sen o f sion techniques r described in a comprehensible manner. I m r t il, 1 vers the papers utilizi g a f r of the camera sensor, be a si r a c nfig ion f mul p ca ras, head et dev ces s well as 3D camera s sors. Subsequ tl , pape utilizing th IMU sens r (acc leromet r, gyroscope,\ngnet m r), t m t popula s n or mong the sel cted papers, are presented. For ach i d vidual paper, a s mmary incorpora ing th solu i with the advant ges and dis vantag s i writt n. On the other han , Section 3.1.2 concerns solu i ns with no camra e sor . Mos of them inc rp rat an IMU s sor while other options i clude ultras nic and Lidar sens rs as ll Bluetoo h Low Energy (BLE) b acons and Wi-Fi access points.\n3.1.1. C era-Assisted S l tions An unce tainty-based adaptiv senso fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from accurate state estimation by deter ining the states that should be included in the estimation proce s. These degradi g sta es can arise under motion characteristics that nullify\nMulti-categ ry search\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd cam ra sensors s expected give th proliferation of s art h ne d vices as th referred platform for developing solutions for he BVI individuals. Finally, only 14% of these solutions can be used in re l-life scenari s with some degree f su cess, 9% of them are practical b have a c mbi ati n of ither igh cost r r q ire from the u r to carry many sensors, 2 % are limited practicality for specific sce rios w ile 32% ar purely exp i mental.\nTable 3. Solutions and obstacle d tectio .\nPap r Supp rt Static Dynamic ang Accuracy 41 47\n48 N A N A 49 0 / / / 1 / N/A 5 m 95\n0 9 98% 0.1 < < 3.5 m 90 5\n5 / / / [55] N/A N/A 0.2 < < 10 m N/A 56 R > 2\n[57] N/A N/A 58 2 c < R < 4.5 m 59 N/A 6 \u201398% 60 2 cm < R < 12\n61 63 [64] \u2716 R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT following s ctions will res nt th olutions f d i t e lit re or anize around th sensors employe . Specifically, th prese atio i g nized r und cri\nria of be ng am ra or non-c a isted as d ff n ts f tech q re r q i e f r hose two cas . F e ch of thes two c e orie , w c n i ue y g ou i g h provided solutions b sed on th type of senso with the most fr quen g p being pr s ted fir . For bo h cases, the sens r fusion techniqu ar e ribe i a ehensibl a e . In more detail, Secti n 3.1.1 covers the papers utilizing f rm of th camera sensor, be it a single cam ra or a co figuration of ltipl ras, h ds t evic s as well as 3D camera sensors. Subseque tl , pa rs u iliz g th I U se sor (accel omet r, gyroscope,\ng etometer), t mos popular s sor amo g the sel ted p pers, r prese ted. F r each i dividual paper, summary incorporati th soluti with the adva t ges a disadvantages is written. On the other hand, Section 3.1.2 concerns soluti ns ith o camera sensors. Most of them i c rporate n IMU sensor whil other options include ultr - sonic and Lid r sensors as well as Bluetooth L w Energy (BLE) beacons d Wi-F acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive se sor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can arise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera sensors is expected given the proliferation of smartphone devices as the preferred platform for dev loping solutions for the BVI individu ls. F nally, nly 14 of these soluti ns c n be used in real-life scenarios ith some degree of success, 29% of the are practical b have a c mbination of ei her high c st or require fr the u er t carry ma y sensors, 24% e li i ed practic lity for specific scenarios whil 32% e pur ly xperimental.\nTable . Solu ion a d obstacle d tection.\nPaper Support S atic Dy amic Ra ge Accuracy [ 1\u201347]\n[48] N/A N/A N/A [49] [50] N/A N/A / / [51] N/A N/A 5 m 95% [5 ] 0 < R < 9 m 98% [5 ] .1 m < R < 3.5 90\u201395% [5 ] N/A / N/A / [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 N/A [57] / N/A [58] 2 c < R < 4.5 m N/A [59] / 67\u201398% [60] 2 < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh fol ing s ctions will res t the so utions f u d i t e lit atu r ize r u d th se sors e ploy . Sp cific lly, th pre atio is ganiz a oun th c i eria f being ca n n-c era as st d s diffe en s ts f t ch ques ar r qui ed fo those two cas . For ch of he tw ateg ri , w co tinu grou ing th pr vided solution based o th ype f e sor with th most frequ t oup being pres ted first. For both cas s, the sensor fusion techniques are described in a comprehensible anner. In more detail, ction 3.1.1 covers th apers utilizi g a form of the c mer ensor, b it a single camera or a c nfigu t o of multiple ca ras, a s t vic s as w ll s 3D amera ensor . Subseq ently, pa rs utilizin IMU se sor ( c l r m t r, yro cop , magnetometer), he most opul r ens among he selected p pers, ar prese ted. For each individual paper, a summary incorporati g the solution with the advantages and disadvantages written. O the other han , S cti n 3.1.2 c ncerns soluti ns with o c - era sensors. Most of them incorpo te an IMU s n or while ther ptions include ultrasonic and Lidar sensors as w ll as Bluetooth Low Energy (BLE) b acons nd Wi-Fi ccess points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion fra ework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimating relative motion. It minimizes degradation from inaccurat tate estimation by det r i ing the states that should b included i the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand camera sensors is expect d gi en the p oliferation of smar phon devices as the prefe red platfor for developing solutions for the BVI individuals. F nally, only 14 of these solutions can be used in real-life scenarios ith some deg ee of success, 29% of the are practica b hav a c mbinati f eit high co r require f o th er t carry many sensors, 24% e limi e pra ticali y f r specific sc nari s w il 32% re urely ex r - me tal.\nable 3. Solutions and ob tacl detection.\nPaper Sup ort Sta ic Dy a c a ge ccuracy 41 47 [48] N/A N/A N/A [49] [50] / / N/A / [51] N/A N/A 5 m 95 [5 ] 0 m < < 9 98% [5 ] 0 1 m < R < 3.5 90\u201395% [5 ] / / N/A / [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 N/A [57] N/ N/A [58] 2 c < R < 4.5 N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/ N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following c io will res nt the soluti s f d in the lit ture or an ze arou d th sen or employ . Sp c fic lly, th pr nt ti i gan zed ar u the cri eria of being c ra o no -c a st a diff nt s ts f e hn ques re q r d f r those two cas . F ch of th w c t gories, we continue y groupi g t e pr vided lutions b d th ty of nsor wi h th most f equ nt g oup being prese ted first. For both cases, the sensor fusion techniques are described in a co prehensible manner. In mo e detail, Sectio 3.1.1 cove s the pap r utilizing a f r f the camera sensor, b it single c era r configur tio f ltipl ca er s, hea s t d vices s w ll as 3D camera sen o s. Subseq e tl , paper utilizing th IMU s sor (ac eleromet r, gyroscope,\nneto eter), t m st p pular so among the s lecte p per , are pre te . For ach i dividual p pe , a summary incorp rating soluti with the dvant ges and sadvan g s is written. On the other ha , Secti 3.1.2 c ncerns soluti n with no camera nsor . M st f them i p ate an IMU sensor while oth r ptions include ultrason c nd L dar sensors as w ll as Blu ooth L w E ergy (BLE) b acons and Wi-Fi acces po ts.\n3.1.1. Camera-Assisted S l tions An uncert inty-based adaptive sensor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state est m tion by determining the states that sho ld be ncluded in the estimation process. These deg ading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 f 29\nand cam ra sensors is xpected giv n the proliferation of sm rtphone devices as the preferr d pl tform for d veloping solutions for the BVI individ als. F nally, only 14% of these solutions can be us d in real-life c narios ith some degree of success, 29% of the are practic l b hav a comb nati n f eith r high cost or qu re f om the ser to c rry ma y sensors, 24% imi ed ractic lity for specific cenario whil 32% are purely exp rimental.\nTable 3. Solutio s a d obstacle det ct on.\nPaper Sup ort Stati Dy ic Range Accuracy [41\u201347]\n[48] N/A N/A N/A / [49] [ 0] / / N/A / [ 1] N/A N/A 95 [ ] 0 m < 9 m 98% [ ] 0.1 3. 90\u201395% [ ] / / / / [55] N/A N/ 0.2 0 m N/A [56] R > 2 N/A [57] N/A / [58] 2 m < R < 4.5 m N/A [59] / 67\u201398% [60] 2 m < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lowing se tions will r se th solutions f nd in liter ture organize u d t nsors pl ye . Sp ific lly, he pre enta io is ganized around the cr a of b ing cam ra o n n-cam ra a s sted a ifferen s ts of techniques are requi ed for t ose t o c . F e f he wo categ i s, we continue by groupin the pr vided solu ions bas d o th type of se so with th most frequ nt group being resente first. For b th cas , the sensor fusion techniques re described in a co prehensible manner. In m r d ail, Sect 3.1.1 overs th pap rs ilizing f rm f the camera ensor, be it a s ngle cam r a o figurat o of multi l cam r s, headset d vices s well as 3D ca era sensor . Subseque tl , pap rs uti izing h IMU sensor ( cceleromet r, gyroscop ,\ngnetome er), o t ular sens r mong h lec d p per , are presented. For ea h individu l paper, a ummary incorp rating th solutio with the advantages and dis dvantag s is w itte . O th othe ha d, Sectio 3.1.2 concer s solutions with no camera sensor . Most of th m inc r o at an IMU ensor while other options include ultra sonic an L d r se s s as w ll a Bluetooth Low En rgy (BLE) bea ons and Wi-Fi ccess point .\n3.1. . Camer -Assi ted Sol tio s An ncertai ty-based adaptive s ns r fusi n framework for Visual-Inertial Odo etry (VIO) is prop sed i [41] for stimati g r lat ve otion. It minimizes degradation from inaccurat sti ati n by determining the states that should b included in the estimatio process. These degrading states can rise und motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand cam ra sensors is exp cted given the prolife ation of smartphone devices as the prefer ed platform f r dev lop g olutions for the BVI individuals F nally, only 14% of these solutions an be used in r al-lif sc narios ith some degree of success, 29% of the are pract c b hav a c b nation of ei her igh os o equire from the use to carry many sen ors, 4 e l mi ed practi ity fo sp cific sce ari s wh l 32% are purely experi-\ne t l.\nTable 3. Solutio nd obstacl d tection.\nPa er S p or Static Dy amic R ge Accuracy [41\u201347]\n[48] N/A N/A N/A [49] [50] / / N/A / [51] N/A N/A 95 [5 ] 0 < < 9 m 98% [5 ] 0.1 < R < 3.5 90\u201395% [5 ] / / N/A / [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 / [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe foll wing s cti ns wi l prese t he olutions fo nd i the lit r ture or anize r und t ors empl y . Specifically, p ese ation is ganized ar nd the cri er a of b ing c m r or n came assi ed as diffe ent s ts of tech q es are req ired f r thos t o cas . F h of hese two cat gori , we continu y group g the provided lutions b on th ype of s sor wi e most frequent group being presented first. For both cas s, the s n o fusion techniques ar described in a comprehensible manner. I ore d t il, S cti n 3.1.1 c vers the papers utilizing a f rm of he camera sensor, be a sin r a c nfig tion of multipl ca eras, headset devices s well as 3D c m ra s ors. Subsequ tl , papers utilizing th IMU sensor (a celerome r, gyroscope,\ngn t m r), t m popular s nsor mong the sel cted papers, are presented. For ach i d vidual paper, summary incorpora ing th solu i with the advant ges and dis vantag s writt n. On the other han , Section 3.1.2 concerns soluti ns with no camer sensors. M s of them inc p rate an IMU nsor while other options include ultraso ic and Lidar sens rs as ll Bluetooth Low En rgy (BLE) b acons and Wi-Fi access points.\n3.1.1. C era-Assisted S lutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by deter ining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nSharing places\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand c mera senso s is xpected give th prolife tion f s art ne devic s a h eferred pl tform for developing solutions for he BVI individuals. Finally, only 14% of these solutions can be used in real-life scenari s with some degree f success, 29% of the are pr ctical but have a com i ti n f ither ig cost r r q ire fr m th r to carry ma y sensors, 24% are limited practicality for specific sce rios w ile 32% ar purely exp i mental.\nTable 3. Solutions and obstacle d tectio .\nPap r upp rt Static D namic ng Ac uracy 41 4 [48] N/A N/A N A N A 49\n[50] / N/A / 51 N/A N/A 5 95 5 0 < R < 9 98% 5 0.1 m < R < 3.5 90\u2013 5%\n[5 ] / / N/A / [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 / [57] N/A N/A [58] 2 cm < R < 4.5 [59] N/A 6 \u201398% [60] 2 cm < R < 12 N/A\n[61\u2013 3] \u2716 [64] R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 [72] N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u2013 8] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following s ctions will r s t the s lutions nd in t e lit r t e or anize around th sensors employe . Sp cifically, the p e e ation i ganiz d r und e cri er a of being am ra or n -c m s isted as diffe n ts f tech q re r q i e f r hose two cas . F ch of thes tw cate orie , w c nti ue y g ou i g h p ovi ed solutions b s d on th type of sensor wi h t e most fr quen gr p b ing te fir . For bo h cases, he sens r fusi n techniqu ar de ribe i a m eh nsibl a e . In more detai , Secti n 3.1.1 covers th pers utilizing f rm f th c mer sen or, be it a single camera o a co figuration f m ltipl c ras, s t devices as ell as 3D camer sensors. Subseque tl , pa ers utiliz g th I U se sor (accel met r, gyr s p ,\ngnetometer), t m st popular s nsor amo g th sel t d p pers, r pr sented. F r each i dividual paper, summary incorp rati th soluti with t e adva t ges a disadvant ges is written. On the other hand, Section 3.1.2 concerns soluti ns ith o camera sensors. Most of them i c rp rate an IMU sensor while other options include ltrasonic and Lidar sensors as well s Bluetooth Low Energy (BLE) beac ns and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An u certainty-based a aptive se sor fusion framework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It minimizes degr dation fro inaccurate state estimation by determini g the states that should be included in the estimation process. These degrading states can rise under motion ch racteristics th t nullify\nSensors 2023, 23, x FOR PEE REVIEW 9 of 29\nnd cam r sensors s ex ected give the proliferati of sma tp on d vices as t referr d platform for devel ping soluti ns f r the BVI i dividuals. Finally, only 14% of these sol ti ns ca be used in real-life scenari s with some degree of success, 29% of them are practic l bu have a c mbi a i of ith r high cos r req ir fr t e user to carry any sensors, 24% re limited ractic lity f r spec fic sce ios whil 32% re p rely x e i\nental.\nTabl 3. Solutions and obstacle detectio .\nPa r upp rt Static Dyn mic R nge Accuracy [41\u201347]\n48 N A N A 49 0 / 1 / 5 95%\n0 9 m 98% 0.1 m < R < 3.5 9 5\n5 [55] / N/A 0.2 < R < 10 m N/A 56 R > 2 m\n[57] N/A 58 2 c < R < 4.5 59 N/A 67\u201398% 60 2 m < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] N/A [73] \u2716 \u2716 \u2716 [74] R < 6 N/A\n[75\u201378] \u2716\nT e f llowing c ions will pr e t e s lu i ns fo nd n the lit u e a iz r und the se ors mploye . Specifically, th p e tat o o g iz d arou d the c i ri f be g cam r o -came ssi ted s d ff t of tech ques ar equire\nf r thos t cas s. F ch f t ese tw tegori , w co i ue y roupi g provided solut ns b sed on h yp f s ns wit he most frequ nt g up b ing pr s nted first. For both ca es, th sensor fusio tec iqu s ar d scrib d in a c pre nsible ma ner. I more detail, Sec i n 3.1.1 cove s the pa e s utilizing fo m of the c mer se sor, be it single cam a or a co figur tion of mul ipl ca , he set evic s well as 3D cam sens rs. Sub que tl , p s u ilizi th IMU s r (acc l ro et r, yroscop ,\ng et meter), m s popul r or am g th elec d pap rs, re p se t . For e ch i d vi al p p r, summ ry incor or g th soluti with the a v t ges a disa vantages is written. O the t er hand, Sectio 3.1.2 co cer s soluti ns with no camera sensors. Most of them incorporate an IMU se s r while her ptio s include ltrasonic a d Lidar ensors as well as Bluetooth Low Energy (BLE) be ns a d Wi-Fi access points.\n3.1.1. Camer -Assist d Solutions An uncerta nty-ba ed adaptive sensor fusion fr mework f r Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It mini izes degradatio fr inaccurate state estimation by det rmining the states that s ould be included in t e estimation process. These degr ding states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd cam a sens rs s exp ct d given th olif rati of smar p on d vi s as the preferred platfor for developing so uti s for the BVI i divi uals. Finally, only 14% of these sol tions can be used in real-life sce arios with som degree of success, 29% of them re pra tical but ave c mbi ation f he igh c st eq ire f o the user to carry many ensors, 2 % r limite pra ticality f s ecific arios whil 32% are purely expe i mental.\nTable 3. Soluti and obstacle detectio .\nPap r Supp r Stati Dyn mic Range Accuracy [41\u201347]\n48 N A N A 49 0 N/A N/A N/A N A 1 / N/A 5 95% 2 0 9 m 98% 3 0.1 m < R 3.5 90 5\n54 N/A N/A N/A [55] / N/A 0.2 m < R 10 N/A 56 R > 2\n[57] N/A N/A 58 2 cm < R 4.5 59 N/A 67\u201398% 60 2 m < R 12 m\n61 63 [64] \u2716 R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT e f l owi g s cti n ill r e t the lution fo nd i the lit ure or a ize a u d t e s ors pl yed. Sp c fi ally, th pres nt i n s g n zed a und the cri\nri of be g camer or -cam as is diffe e s t f techn q es r req ir d f r h two ca . F e ch of th s t ate ori s, ontinue y grou i g the provide solutio s ba d n h yp f s ns wit m s frequent g up b ing pr sented fir t. For b th c ses, t s nsor fu i n tech qu a scribed in co pre e ible man r. I more det il, Secti n 3.1.1 cov rs the pap r utilizing a f m f e camera sensor, be it si gle c r r a figur ti of ltipl r s, h ads t d vic s s well as 3D cam s. Sub q tly, papers utilizi g th IMU s nso ( c l m ter, gyroscope,\ng t meter), th o t p p l r o o g th s l cte pap rs, are pr sen ed. For each i divi al p pe , a summ y i c por ti soluti with th a v nt ges nd d sa va tag s is written. O the other a d, Secti 3.1.2 co cer s solutions with n camera s nsors. M st f them incorp ate IMU sensor whil other opti ns i clude ultrasonic d Lidar s ns rs as well as Bluetooth Low E rgy ( LE) beacons a d Wi-Fi ccess poi ts.\n3.1.1. Cam ra-Assisted Solutio s A ncertai ty-based adaptive se sor usion fr ework for Visual-Inertial Odometry (VIO) is pro osed in [41] for estimating relative motion. It mi imizes degradation from inaccurate state es ation by d termining the states that should be ncluded in the estimation proc ss. These deg ding s tes an arise u der motion char cteristics that nullify\nSensors 2023, 23, x FOR PEER EVIEW 9 of 29\nnd ca r se sors s x ect give the proliferati of sm rtphone d vices as the preferre platfor for developing solutions for the BVI i dividuals. Finally, only 14% of these sol tions can b us d in real-lif narios with some degree of success, 29% of them are pract cal but ave a c mb ation f i her high cost r req ire f om th user t carry m n se s rs, 24% r li ite practicality for specific sce arios whil 32% are purely expe i mental.\nTabl 3. Solutions a d obs acle detec ion.\nP p r Supp t St tic Dy amic Range Acc racy [41\u201347]\n[48] / N/A N/A N/A [49] [50] N/A N/A N/A [51] / N/A m 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] /A N/A 0.2 m < R < 10 m N/A [56] R > 2 N/A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 m < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716\nT f lowing s c io s will pre t th solutio s fo d in he lit ure or anize a u t n ors m loyed. Sp cific lly, th resentation is orga ized around the cri t i of be g am r or -c r a s st s iff r sets of te h ques r r q i ed f r s t ca s. F ach f t e tw categ ries, we conti ue y groupi g the provided sol io b s on h type f s s r wit h most frequ n g oup b ing p e e f rst. For b th cas , t e sor fu i tech iqu esc ib d in a co prehensible mann r. I m r etail, S cti 3.1.1 covers th p pers utilizing a f rm of the camera se sor, b it a singl c m or co figu ti f mul pl cameras, headset d vice s well as 3D c mera se sors. Subseq tl , p rs utilizi g th IMU sensor (accel ro et r, gyros ope,\ng et m ter), mo ar s sor am lected paper , re prese t . For a h individ l p per, summ ry corp rating th soluti with the a vant ges and dis dvantage is w itte . On t e ther hand, Section 3.1.2 c cer s solutions ith no camera sens rs. M st f th inc r rate an IMU sensor while ot er ptions include ultrasonic and Lid r sensors as w ll as Bluetooth L w Energy (BLE) bea ons nd Wi-Fi access points.\n3.1.1. Cam ra-Assisted Solutions An ncertai ty-based adaptive s ns r fusi n fra ework for Visual-Inertial Odometry (VIO) s prop sed i [41] for stimating relat ve motion. It minimizes degradation from inaccurat stimati by determining the states th t should b included in the estimati pr cess. The e degr din states can rise und motion characteristics that nullify\nS nsors 2023, 23, x FOR PEER REVIEW 9 of 29\nd c ra sen ors s expected given the p o ife ati of smartphone d vices as the preferred pl tf r f r e elop g olutions for the BVI i dividuals. Finally, only 14% of these sol tions an be use in r l-lif sc narios with some degree of success, 29% of them are pract c l b hav a c bi tio of i h r hi h ost r eq ire from the user to carry many se rs, 4 re limite practicality f pecifi sce ios whil 32% are purely expe i mental.\nTable 3. Solutions and obstacle de ction.\nPa Supp rt Static Dynamic nge Accuracy 41 47\n48 N A 49 0 N/A N/A N A 1 / 5 m 95% 2 0 9 m 98% 3 0.1 m < R < 3.5 m 90 5\n54 N/A N/A [55] / 0.2 m < R < 10 m N/A 56 R > 2 m\n[57] N/A N/A 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 60 2 m < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716\nT foll w g s c ion wi l pr se t h olutions found i the lite a ure organized r u d the n o s empl y d. Sp cifically, p ese tation is orga ized around the crit of b g c er o on-c era assis ed s ffe en set of echniques are required f r e t cases. F c of the e tw categori , we cont nue by gro ping the provided\nlu io s ba d on he ype of s r wi he most frequent g oup b ing presented first. Fo oth c s s, the s nso fusio tec iqu s ar escribed in a co prehensible manner. I more d tail, Sectio 3.1.1 covers the papers utilizing a for of the camera sensor, be it si r a co fig r tion of multipl ca r s, h adset devic s as well as 3D camera se sors. Subs que tly, papers util zi g th IMU senso (accel ro eter, gyroscope,\nt er), th s po ul r s or mong th sel cte papers, are presented. For a h ind vid l p per, summary i co pora i g the solu ion with the a vantages and disa va tages s written. O the other hand, Sectio 3.1.2 co cerns solutions with no camra se sors. M st f them incorp r te an IMU sensor while other options include ultrasonic and Lidar sens rs as ell a Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Cam ra-Assisted Soluti ns An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is propose i [41] for es ating relat v motion. I minimizes degradation from acc rate state estim tion by determining the states that should be included in the estimation proce s. The e degr d g sta es can arise under motion characteristics that nullify\nCall a sighted person for live suppo t\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nand camera sensors is xpected g ven the prolif tion f smartpho e devices s t pr - ferred platform for developing s lutions for the BVI individuals. F nally, o ly 14% of thes solutions can be sed in real-life sc narios with some degr e of su cess, 29% f them ar practical but have a combination of ei er high cost or r q ire from the ser t carry many sensors, 24% e limi ed practicality f r sp ific scenarios hil 32% ar ur ly experimental.\nTable 3. Solutions and obst cle detec ion.\nPaper Support Static Dyna ic R nge A cur cy [41\u201347] \u2716\n[48] N/A / [49] \u2716 [50] N/A N/A N/A N/A [51] N/A N/A 5 95% [52] 0 m < R < 9 98% [53] 0.1 m < R < 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R < 10 m N/A [56] R > 2 N/A [57] \u2716 N/A N/A [58] 2 c < R < 4.5 m [59] N/A 6 \u201398% [60] 2 c < R < 12 m N/A\n[61\u201363] \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nThe following section will rese t t e solutions f nd in t literat re o ga ized around the sensors employed. Specifically, the pr s ntatio is rga d round the criteria of being c r n n-camera ass ted s diff se s of techn ques e equi d for those two cases. For ach of the two c t g ri , we contin by groupi g he pr ide solutions based o the typ of s n or with th most frequen r up b ing present d fir . For both cases, the sensor f sion tec iqu s a e e crib d i co pre nsible m nn r. In more et il, Section 3.1.1 covers the p p rs utilizi g f r f the c mer sensor, be it a single camera or a configuratio f multipl ca r s, h t d vic s well as 3D camera sensors. Subsequently, ape utilizing th IMU se s r (acc l r meter, gyroscope, m gnetome er), the most popular sensor among th s le d papers, are pr sen d. Fo each i dividual paper, a sum ary i corporating th solu wit the advant es and disadvantages is written. On the other ha , S ction 3.1.2 concerns soluti ns with n camera sensors. Most of them inc rporate an IMU sensor while other options include ultrasonic and Lidar sensors as w ll as Bluetooth Low Energy (BLE) b aco s and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camera sensors is x ected give the rolif ti n of martp on devic s as t preferr d platform for devel ping olutions f r the BVI individu ls. Finally, o ly 14 f these soluti ns can be sed in r al-life sc nari s with so degr e f succ ss, 29% f them are practi l bu av a c mb nation of ei her high c s or r quire fr he u r t car y a y se sors, 24% e lim ed ractic lity f r spec fic sc ios whil 32% p r ly xperimental.\nTable . Soluti s and obst cle detec ion.\nPaper Suppor S atic Dy amic ange A curacy [ 1\u201347]\n48 49 0 N/A 1 N/A N/A 5 95% 2 0 9 m 98% 3 0.1 < R < 3.5 90 5\n54 N/A [55] N/A N/A 0.2 < R < 10 N/A 56 R > 2 m\n[57] N/A 58 2 m < R < 4.5 59 N/A 67\u201398% 60 2 c < R < 12\n61 63 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe fol owing c ion will r t the so tions f u d in t lit r tu n z rou d the se sors e ploy d. Sp cifically, the pres ntation is rgani ed aroun th c it ri f bei g c r n- era s t s different e s f t ch iqu ar r qui ed fo thos case . For each of the w cat gori , w continu b gr u ing the prov d d solutions b sed o the yp of enso with he most freque t roup b ing prese te fi t. For both ca , the sens r f si tec iques re cribed in comp ehensibl a ner. In more et il, ec io 3.1.1 covers th paper utilizing a f rm f the camera sensor, be it ingle camer or a configur tio of multiple camera , dset d vic s as w ll s 3D cam ra sens s. Subseque tly, p ut lizing the IMU ( c le r, gyrosc p , magnet meter), th m t popul r e sor among he elect p p rs, ar present d. For e ch indivi ual p p r, summ ry incorporati g the soluti n with th dv t g s disadvantages written. O the ther han , Section 3.1.2 concern soluti ns with no camera sen ors. Most of them inc rporate an IMU sens r while other options include ultrasonic and Lidar ensors as w ll as Bluetooth Low Energy (BLE) b ons and Wi-Fi access p ints.\n3.1.1. Camera-Assisted Solutions An uncerta nty-ba ed adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det rmining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd cam a sensors s exp cted giv n the p lif ration of smartp one d vi s as the preferred platfo for dev loping solutions for th BVI individuals. Finally, only 14% of these solutions c n be used in real-life scenarios with some d gre of success, 29% of them are practica but have a c mbinati n f h r high cost or requir fro the user to carry many s s rs, 2 % are limi d p act ca y for sp cific sc ari while 32% a e purely experim tal.\nT ble 3. Soluti and obstacle detectio .\nPaper upp Stat c D namic R g Acc racy [41\u201347] \u2716 \u2716 \u2716\n[48] N/A N/A / N/A [49] \u2716 \u2716 \u2716 [50] N/A N/A N/A [51] N/A N/A 5 m 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R 3.5 m 90\u201395% [54] N/A N/A N/A N/A [55] N/A N/A 0.2 < R 10 m / [56] R > 2 m N/A [57] \u2716 N/A N/A [58] 2 cm < R < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 m < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 \u2716 [64] \u2716 R = 20 c N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT follow ions wil r e h s lutio found i the lite a ur orga ized r und the e ors em l y . Specifi lly, he pres nta ion s org iz d a ound the crit ri f be ng c er o no -c e as i t d as differen se f t chniqu s are equir d f r tho e tw ca s. Fo ach of the t categ ies, w ntin e by g o ing t e provid\nluti s ba d n the y e of s o wi h th o f quen g up in pr se ted fir t. F r b th ca s, t e se sor fu io echniq s a escribed in a comprehe sible mann r. I more detail, S ction 3.1.1 covers the paper utilizing a f m of e camera sensor, be it a in le c m ra or a figu ti of m l ple m ras, heads d vices as well as 3D c m a n . Sub qu ntly, pap rs utilizing the IMU sensor ( c el ter, gyroscope,\nag etometer), the o t popular s nsor a o g the sel ct d pap rs, ar pr sented. For each indivi al paper, a summary inc rporati g the olution with th dvant ges an disadvanta e is writt n. On e ot er h n , Section 3.1.2 concerns solutions with no camera se s rs. Mo t f th m incorp rate n IMU sens r whil other pti s include ultras nic a d Lidar sensors as well s Blueto th Low En rgy (BLE) beacons and Wi-Fi ccess poi ts.\n3.1.1. Camera-Assisted Soluti s An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is pr posed in [41] for estimating relative motion. It minimizes degradation from inaccurate st te stimation by determinin the states that should be included in the estimatio process. Th e degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd c mera sensor is xp c ed giv n th prolif ratio of martp o e d vices as the pref rr pl tfor f r d v lopi g soluti s for the BVI individuals. Finally, only 14% of th se s luti n an b s d in r al-lif arios wit som de r e of ucc ss, 29% of th m are p act c l b av a c mbination f either hi h ost or quir fr m th ser to c rry ma y sensors, 24% limi ed ra tic l ty for sp cific enario whil 32% are purely exp rimental.\nTabl 3. Solutions and obstacle d t c ion.\nPaper Supp Sta i Dyna ic ange A curacy [41\u201347]\n48 49 0 N/A N/A N/A\nN/A N/A 5 95% 2 0 9 m 98% 3 0.1 m < R < .5 90 5\n54 N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 N/A 56 R > 2\n[57] N/A N/A 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 60 2 < R < 12\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [7 ] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT f lowing se tion will rese th solutions f und i literature organiz d r u d t e ensors mpl ye . Spe fic ly, he p senta io is ganized around the crit r a of bein m r n n-cam r s ted a iffere t ts t ch iqu re r quired for th se o c e . F e ch of th wo c teg i s, e con i e by gro ping t e p vid d sol ions as d h ype of se s with h mo t f qu t g oup being te first. For bot c s , t s nsor fusio t chniq s re described in co p hensible ma er. In m r d il, Secti 3.1. covers t papers lizing a f rm f the camera se or, be it si gle cam ra r co figura i of multiple ca eras, headset d vices as well as 3D cam r s rs. Sub quently, pa utilizing th IMU sens r (accel rometer, gyr s ope, m gnet m r), the mos o ular s nsor mong t select pap r , are pres n ed. For\nh i dividu l p per, s mm ry i cor r ting t solu with th advant ges and dis va tag is ritte . O th oth han , Secti 3.1.2 concerns solutions ith no camer sens r . Mo t f th inc r orate n IMU ensor wh le ther options include ultrasonic an Lidar se sors as w ll as Bluetoot Low En rgy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assiste Solutions An ncertainty-based adaptive sensor fusi n framework for Visual-Inertial Odometry (VIO) s prop sed in [41] for estimating rel tive otion. It minimizes degradation from inaccurat estimati by determining the states that should be included in the estimati process. The e degrading states can rise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd c m ra s n rs is expected given the p oliferatio of s artpho e devices as the pref rred platform f r dev l pi g olutions for th BVI individuals. Finally, only 14% of these s lu i s an b u d in r l-lif sc narios with so e degree f success, 29% of them are pract c l b have a c bination of either igh cos o equire fr m the use to carry many sensors, 4 e limi ed pra ti lity for sp cific s e ari s whil 32% are purely experi-\ntal.\nTable 3. Solutions and bst cl detec ion.\nPa e S p t S atic Dyna ic nge A curacy [41\u201347]\n48 49 0 N/A N/A N/A 1 N/A N/A 5 95%\n0 9 m 98% 0.1 < R < 3.5 90 5\n5 N/A N/A N/A [55] N/A N/A 0.2 m < R < 10 m / 56 R > 2\n[57] N/A N/A 58 2 cm < R < 4.5 m 59 N/A 67\u201398% 60 2 < R < 12\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nTh foll wi g s cti n will r s t the solutions f und i t e literature organized r und the o empl y d. Specifically, the p e tion i ganized ar und the crir of b i g c m r n n camera as t d differ n ets of t ch iq are required f r t o o cas . For a h of th tw cat go i s, w continu by group ng the pr vided s lutions as d o h y of ensor with m st freque t group being te first. F both as s, th se f si t chniques re d s ib d i a c mp ehensible manner. I ore detail, Se ti 3.1.1 c vers the papers utilizing a form f the camera sensor, be i sin le ca er c fig r ti n f multipl ca as, h adset devices as w ll as 3D cam r s n or . Subsequ tly, papers u ilizing th IMU sensor (acc lerometer, gyros ope,\ngn t me er), the m s pop lar s or among the sel ct d papers, are pr s n ed. For h i dividual paper, a summary i corpora ing th solu with the advant ges and disadva tag s s written. On the other hand, Section 3.1.2 concerns solutions with no camera s nsor . M s of them i corporate an IMU se sor while other options include ultras nic and Lidar se s rs as ell as Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Ca era-Assisted Solutions An uncertainty-b sed adaptiv sensor fu ion f mework for Visual-Inertial Odometry (VIO) is proposed in [41] for es mating r lat v motion. I minimizes degradation from inacc rate state estimation by determining the state t t should be included in the estimation proce s. The e degrad g states can arise under motion characteristics that nullify\nRoute pl nning\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camera senso s is xpected given th rolife tio f art ho devices a f red platform for develop solutions for he BVI individuals. Finally, only 14% f t es s luti ns can be used in real-life scena i s with some degr of su ces , 29% f the ar practical b have a c mbination f either igh cost or req ire fr m the user t carry many sensors, 24% are limited pr ctical ty for sp fic nari s whil 32% ar pu ely expe - me tal.\nTable 3. Solutions and obstacle d tection\nPaper Supp t t tic Dynamic Rang Accur c [41\u20134 ]\n48 49\n[ 0] / / / / 1 N/A N/A 5 95%\n[ 2] 0 9 98% [ 3] 0.1 3 90 5 [ 4] N/A N/A /A / [55] N/A N/A 0.2 0 / 56 R > 2\n[57] N/A / 58 2 cm < R < 4.5 m 59 / 67\u201398% 60 2 c < R < 12 m\n61 63 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716\nThe f llowing c i ns will pr se t h oluti n f u d i t e liter u orga iz d around the sensors e ploye . Sp cifi ally, the pr s a on i or anized arou d the criteria of being c e or on-ca ra a si t d as differ t sets of techniques r r qui ed for ho e two case . F ach of these tw cat g ri , we co ti ue by gr upi g he p ovi e s lution based th yp of s n or w th the mo t freque group b ing r te fir t. For both cas , the sen r fusi tec niqu s r describ d i a c m h n ib anner. In more detail, Section 3.1.1 covers th p pers utilizing for of the cam r sen or, be it a si gle camera or a co figuration of ltiple ca eras, a et device as well as 3D camera sensors. Subsequently, p pers utilizing th I U e sor (accele omete , gyroscope, magnetometer), the m st popular sens r amo g the sel ct d p pers, are pr s ted. For each i dividual paper, a summary incorporatin the soluti with the dva tages a d disadvantages is writte . On the ot r hand, Se tion 3.1.2 concerns solutions with no amera sensors. Mo t of them incorporat n IMU sensor whil other options i clude ltr sonic and Lidar sensors as well a Bluetooth L w Energy (BLE) b ac s a d Wi-F acc s points.\n3.1.1. Camera-Assisted Solutions An uncertainty-based adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state estimation by determining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nnd camer sensors is expect d giv the rolifer ti of sma tp o devices as th re f r d platform fo devel pi g solutions f r the BVI i ividuals. Finally, only 14% f hese s luti ns can be used in real-lif scenarios wit so deg ee f s c s, 29% of them are practical bu have a c mbination f either high cos or requ r fr m the us r to carry an sensors, 24% ar limite racticality for sp c fi i s whil 32% ar p rely exp rimental.\nTa le 3. Solution and bst l d cti n.\nPa er Supp rt t tic Dynamic Ra c ur cy [41\u201347] \u2716\n48 / 49\n[ 0] / / 1 N/A 5 95%\n[ 2] 0 9 98% 3 0.1 3. m 90 5 4 / /\n55 N/A N/A 0.2 0 56 R > 2\n[57] / 58 2 c < R < 4.5 59 / 67\u201398% 60 2 c < R < 2 m\n61 63 \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 [72] \u2716 N/A [73] \u2716 [74] \u2716 R < 6 m N/A\n[75\u201378] \u2716\nThe f llo ing c io s will pr nt s l i n foun th te atur g iz around the se sors employed. Specifi ally, th pr se t t i organiz d arou d the criteri f bei g c m r or no -camer assi t s diffe t of t ch iques are equired for t ose tw ca . For e ch of thes tw ca g e , w cont ue y g upi g th provide\nlutio s b se on the p of enso w h he os fr que t roup being es d f r t. F r both a , th se r fus techniqu e d scribed in a c p ehensible manner. In re detail, Sec io 3.1.1 cove s the p e s utilizing fo m of the c mera se so , be t a single cam ra or a co figuration of multipl ca era , he set devic s as well as 3D cam ra se s s. Sub equently, ap u ilizing the IMU sor (accele et , gyr scop , magnetometer), th m st popular se or am ng the elected p pers, r p esented. For e ch i divi ual paper, a sum ary incorp rating the soluti with the a va tages a disadvantag is written. On th ther hand, Sectio 3.1.2 concern solutions with no camera sensors. Most of the incorpo t an IMU sen r while other ptions include ultra sonic and Lidar s nsors as w ll Bluetooth Low Energy (BLE) e ons d Wi-Fi cc ss points.\n3.1.1. Camera-Assisted Solutions An uncerta nty-ba ed adaptive sensor fusion framework for Visual-Inertial Odometry (VIO) is proposed i [41] for estimati g relative motion. It minimizes degradation from inaccurate state estimation by det rmining the states that should be included in the estimation process. These degrading states can arise under motion characteristics that nullify\nSensors 2023, 23, x FOR PEER REVIEW 9 of 29\nd ca a sen rs s exp ct d giv n th p lif ratio of s ar p o d vi s as the pre f red platfor fo developi g so uti s fo th BVI individ als. Finally, o ly 14% of these s luti ns can be used in real-life cenarios with s m degre of s ccess, 29% of t e are prac ical but hav a c mbinati f i h g cost or equir f t e u e to carry many , 2 % r limit d pr cticali y f r specific arios while 32% are purely experim tal.\nTable 3. S luti and b tacle detec ion.\nPaper Support Static D n mic Ra ge Acc r cy [41\u201347] \u2716 \u2716\n[48] N/A N/A N/A / [49] [50] N/A N/A / [51] / N/A 5 m 95% [52] 0 m < < 9 98% [53] 0.1 m < R 3.5 90\u201395% [54] N/A N/A /A / [55] N/A N/A 0.2 < R 10 m N/A [56] R > 2 /A [57] N/A N/A [58] 2 cm < < 4.5 N/A [59] \u2716 N/A 67\u201398% [60] 2 < R < 12 m N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716 \u2716 \u2716\nT fol owi g ecti wil pr sen th sol tion foun i the lite a ur o ga ize around t s n o s em l y d. S cifi lly, the pres nt i n s org z d a ound t e crit ri f be ng camer r no -c as ist d as differe se s of techniques re requir d f r those tw ca . F ach of thes o cat g i , we onti e by g ou ing the pr vid\nl ba d n th typ f s r with th os requ nt g oup being pre e ted fir t. F r b th c s, the sor fu ion t chn q es a des ribe in compre ible mann r. In ore detail, Sec on 3.1.1 c v rs he pap utilizing a f m f e camera sensor, be it single c m ra r a o figur ti of m ltiple a ras, hea s t d vices as well as 3D cam a s Sub que ly, a r utilizing the IMU s nsor ( c el met , gyroscop ,\nag e omet r), th mo t popular nso among the sel cted papers, ar pr sented. For each i dividual pape , a summary inc rp r ti g soluti with the a vantages and d sa vantag is writte . On the other ha d, Secti 3.1.2 onc r s solutions with no camera s nsor . M st of them incorp ate n IMU sensor whil other opti s include ultrason c nd L d r s ns rs as well s Blu to th L w E rgy (BLE) beacons and Wi-Fi ccess po s.\n3.1.1. Camera-Assisted Solutions An ncertainty-based adaptive se sor usion fr mework for Visual-Inertial Odometry (VIO) is proposed in [41] for estimating relative motion. It minimizes degradation from inaccurate state es mation by d termining the states that should be ncluded in the estimation proc ss. These deg ading s ates an arise under motion characteristics that nullify\nSens rs 2023, 23, x FOR PEER REVIEW 9 of 29\nnd ca ra se sors s xpec giv th pr lif rat o of sm rtp o e d vices as the pre f re plat r for d v lopi g solutio for the BVI individ als. Finally, only 14% of th se\nluti ns c b s d i r al-l f sc narios it som e ree of succ s, 29% of th m are pract al but have a c mbi ati n of either hi h cost or require f m the user to carry many s sors, 24% a lim t p a icality for specific enarios whil 32% are purely expe ime tal.\nTable 3. S lution a d ob tacle det c ion.\nP p r Support t t Dynamic Ra ge Accur cy [4 \u201347] \u2716 \u2716\n[48] N/A N/A N/A / [49] [50] N/A N/A / [51] / N/A 95% [52] 0 m < R < 9 m 98% [53] 0.1 < R < 3.5 90\u201395% [54] N/A N/A /A / [55] N/A N/A 0.2 m < R < 10 m N/A [56] R > 2 /A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] \u2716 N/A 67\u201398% [60] 2 c < R < 12 N/A\n[61\u201363] \u2716 \u2716 \u2716 [64] \u2716 R = 20 cm N/A\n[65\u201371] \u2716 \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 \u2716 [7 ] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716 \u2716\nT e f lowing ctions wil pres nt th ol tions f n in t e li e a ure organiz d rou d t ensor mpl ye . Specifically, th present tion is rga ized ar und the crit i f b i g a a n -c ra a si t d a differ sets of t hniques ar requi ed f r s two cas s. F ach f th e t o cat g ri s, we conti e by gr uping t e provided s l tions bas d n th ype f s n r with he o t freq e g oup bei g p esen ed first. F r b th cas , s nsor fusi n techn qu s escribed in a comprehen ible ann r. I m r e ail, S ction 3.1.1 covers th p pers uti zing a form of the camera se sor, b it a single camera or configurati f mult ple cameras, headset devices as well as 3D camera se sors. Subseque tly, pa rs utilizing th IMU sensor (accel romet r, gyros ope,\nag etomet r), the mos ular e sor ong th s lected pape s, are prese ted. For ea h individ l paper, summ ry incorp rating the soluti with th advantages and di dvantage i w itte . O the other and, Sectio 3.1.2 concer s ol tions with o camera ens rs. Most f th incor rate n IMU sensor while other options includ ultraonic a d L d r s ns as w ll s Blu tooth Low E ergy (BLE) bea o s and Wi-Fi access point .\n3.1.1. Camer -Assisted Sol tions An ncertainty-based adaptive s ns r fusi n framework for Visual-Inertial Odometry (VIO) s prop sed i [41] for stimating relat ve otion. It minimizes degradation from inaccurat stimati by determining the states that should b included in the estimati process. The e degrading states can rise und motion characteristics that nullify\nSensor 2023, 23, x FOR PEER REVIEW 9 of 29\nd c ra s nsors s expected given the p ife atio of smartpho e d vices as the pre f red platf f r dev l p g olutions fo th BVI in ivid als. Finally, only 14% of these s luti ns an b used in l-lif sc narios with so e egree f success, 29% of them are pract c l bu hav a c binati of ei her high co or equire fr m the user to carry many se ors, 24 are li ited pra ticality fo specific enarios while 32% are purely experimental.\nTable 3. Solutions a d ob tacle ete io .\nPap Suppo t t tic Dynami R ge Accur cy [41\u201347] \u2716 \u2716\n[48] N/A N/A /A [49] \u2716 \u2716 [50] N/A N/A [51] / 5 95% [52] 0 m < R < 9 m 98% [53] 0.1 m < R < 3.5 90\u201395% [54] N/A / /A /A [55] N/A 0.2 m < R < 10 m N/A [56] R > 2 /A [57] N/A N/A [58] 2 cm < R < 4.5 m N/A [59] N/A 67\u201398% [60] 2 cm < R < 12 N/A\n[61\u201363] \u2716 \u2716 [64] R = 20 cm N/A\n[65\u201371] \u2716 \u2716 [72] \u2716 N/A N/A [73] \u2716 \u2716 [74] \u2716 R < 6 N/A\n[75\u201378] \u2716 \u2716\nT following sections wi l pr e t the sol ti n foun i the lite a ur organized a ound t e sen o s pl y d. Specifi lly, p esentation is orga iz d around the cri-\nr f bei g c m r r n -ca era assisted s diffe en sets of techniques are required f t tw cas . Fo ac of these t o categories, w conti ue by grouping the provided\nlu o s d o th typ of sor wi h he o t frequent g oup being presented fir t. F both cas s, th s n o fusion t chniqu s ar es ribed in a comprehen ible manner. I o e detail, Sectio 3.1.1 cov s he papers utilizing a form of the camera sensor, be it a si r a co fig r tion of multipl ca eras, headset devices as well as 3D cam ra so s. Subsequently, ap r util zing th IMU sensor (accel romet , gyroscop ,\na t r), th mos popul r s sor among th sel cted papers, ar presented. For ac i vidual p per, summary incorpora ing the solu i with the a vantages and dis va tages s written. On the other hand, Sectio 3.1.2 concerns solutions with no camera sensors. M st of th m incorp rate an IMU sensor while other options include ultras nic a d Lid r s ns rs s ell Bluetooth Low Energy (BLE) beacons and Wi-Fi access points.\n3.1.1. Camera-Assisted Solutions An unce tainty-b sed adaptiv sensor fusion framework for Visual-Inertial Odome-\nry (VIO) is proposed in [41] for es mating relat v motion. I minimizes degradation from acc rate state estimation by determining the states that should be included in the estimation proce s. The e degrad g sta es can arise under motion characteristics that nullify\nTotal 25 19 13 12 12\nSensors 2023, 23, 5411 18 of 29\nThe solutions explored are all provided for smartphone devices. The majority of them, including Lazarillo, Seeing Assistant Move and Nav by ViaOpta support both Android and iOS platforms, while Blindsquare and Ariadne GPS are found exclusively on iOS. The above table, also, indicates that Blindsquare is the solution with the most supported features, followed by Lazarillo, Nav by ViaOpta, and AriadneGPS with Seeing Assistant Move occupying the last position. All applications support outdoor navigation, route planning, real-time updates, report the user\u2019s location either during the navigation or when the user requests to receive an update on the current location, support searching and sharing of places and POIs both in the near vicinity and in the wider area and, finally, are customizable each to its extent. Exclusivity-wise, Lazarillo is the only application that supports the creation of digital maps and vehicle mode. With the former, Lazarillo can overlay information over floor plans, enabling businesses to participate and make themselves more accessible to the BVI individuals and become more involved, while the latter allows for the detection of speed changes in order to support the switch from pedestrian to vehicle mode and accommodate that use case as well. The application Nav by Opta, developed by Novartis, exclusively provides the feature to call for a sighted volunteer\u2019s help to offer live support for emergencies. Blindsquare sets itself apart as it supports the largest number of unique features including (1) a multi-category search of places and POIs, (2) progress updates in audio message format with increasing frequency as the user approaches the designated destination, (3) an adjustable radius for the POIs, (4) support for the deafblind, (5) multiinput methods including the on-screen iOS keyboard, the iOS handwriting feature, dictating text to Siri as well as a 3rd party app that allows the user to use Braille, (6) audio menu where the user can control the Blindsquare from the headset and, finally, (7) providing a simulation tool of routes for raising the user\u2019s awareness of the surrounding environment prior to actually traversing that route. The latter leverages Blindsquare\u2019s Look Around feature, which is used to plan the trip ahead of time in order to increase the user\u2019s confidence and sense of safety. Examining the full set of exclusive features, a few of them can be considered useful to have for the provision of an increased level of quality of services. A quite useful feature for all the applications to have concerns the capability to create digital maps of indoors spaces of existing buildings, as provided by Lazarillo. This gives the opportunity for more detailed representations of buildings, thus allowing better and more secure indoor navigation. Furthermore, Lazarillo\u2019s approach to bringing the business or the place in the loop is important from a scalability standpoint. In this way, the workload can be decreased and as such no single organization responsible for performing those actions can become the bottleneck in this process. Nav by Via Opta supports another interesting and very helpful feature as it allows for a BVI to call a sighted volunteer to provide support. This can be exceptionally helpful for cases of emergencies such as when a BVI is lost and cannot find another person to point them in the correct direction as well as when the BVI has been involved in an accident. On the other hand, all the features of Blindsquare are important in order to provide an increased level of quality of service for the BVI. A very popular solution regarding the support of navigational information for routes, POIs, intersections and other static objects such as trees, dumpsters, benches and the like is OpenStreetMap. The latter is an open and freely available geographic database maintained by a community of volunteers via open collaboration and is used by all the selected applications but Ariadne GPS. Given this database stores a wealth of detailed information about an area, it is a very good solution for providing enriched semantic information. However, the downside to this is the possibility that an area\u2019s info may not be accurate or even present on OpenStreetMap as the stored information depends on volunteers\u2019 actions. Another popular 3rd party service used by both Blindsquare and Lazarillo is Foursquare. The latter service provides the user with the capability to search for POIs including restaurants, cinemas, coffee shops and the like. Since this is a large database its wealth of\nSensors 2023, 23, 5411 19 of 29\ninformation can be leveraged to provide a rich experience. Similarly to the case of OpenStreetMap, the downside to this solution is the dependence on a single service and the occasional deterioration of accuracy or complete lack of information.\n3.2.2. Effectiveness and Efficiency\nGiven the quite high number of applications, including our own, and the timeconsuming aspect of the tasks, some of the participants expressed their concerns about their availability. To address this issue, we decided to proceed with the Usability and UX evaluation of two out of the five commercial applications and our own. Specifically, we evaluated Blindsquare and Lazarillo, which are the most popular, and BlindRouteVision. After gathering the required data from the trials, we calculated the two metrics. Their results are presented in Table 5.\nThe results on Effectiveness show that our application was more favourably evaluated (80.56%) over the other commercial applications with Blindsquare being very close (78%). Lazarillo received the lowest score (66.6%) among the three with the difference from the top two being significant. Likewise, the Efficiency results follow the same trend. Our application is evaluated higher than the rest of the applications (78.12%). In the second place, we find Blindsquare (73%) while the last position is taken by Lazarillo (62%) with a significant gap to the other two again. By analyzing and interpreting the results, the following conclusions can be reached. The participants favoured both BlindRouteVsion and Blindsquare almost equally, as their score indicates, while Lazarillo was the least favoured. Furthermore, a similar trend of Effectiveness and Efficiency is expected as they are closely related to the latter depending on the former. From our point of view, which was later confirmed as well as from the blind participants, the marginally higher preference for our application over the commercially available Blindsquare was due to the provision of more accurate navigation and, subsequently, the timing of events. This can be attributed to the use of the external GPS receiver from our application enabling more accurate and higher density tracking of the user location, coupled with our custom route navigation algorithm. Despite the wealth of Blindsquare\u2019s useful features, as admitted by the blind users while justifying their reasons for preferring this application for their everyday activities, it utilizes the smartphone\u2019s integrated GPS receiver, which is known to have a precision of fewer than 10 m and, thus, incorrect location reporting is frequent. In contrast, our application achieves an error of less than 1 m, thus, resulting in more precise navigation around sections critical to the safety of the BVI individuals such as corners and intersections giving users more control over the uncertain factors. Lazarillo, on the other hand, scored low on both metrics as the application\u2019s voice instruction subsystem significantly disrupted the blind users\u2019 navigation with no control of this behaviour after the navigation process had started. The issue was occurring at intersections where the voice instruction subsystem would start spelling the names of the roads overwhelming any other information. This could be due to the names of the roads as the navigation took place in Greece or even specific to the operating system as Lazarillo was evaluated on Android. Despite the cause of this issue, blind users during the tests would have to stop as it was impossible for them to move forward. After some time, it started to become annoying for them and many requested to stop the task.\nSensors 2023, 23, 5411 20 of 29\n3.2.3. User Experience\u2014UX (Satisfaction)\nThis section presents the statistical results from the evaluation of BlindRouteVision, Blindsquare and Lazarillo. Various statistics were used and detailed reports were generated including the mean value with error bars showing the 95% confidence interval of the questionnaire\u2019s scales, the consistency of the responses as measured with the help of Cronbach alpha, and, finally, the overall mean value per application of all the participants for evaluating the UX impression. The supplied scales were evaluated using a Likert scale ranging from 1 to 7, however, their results were rescaled to the range of \u22123 to 3 so as to allow the comparison of the results with the initial version of the UEQ questionnaire. Starting from our own application BlindRouteVision, it can be seen from Figure 4 that it has been ranked high on the scales of Efficiency and Dependability with a score of 1.81 and 1.73 respectively. Close to those two is the scale of Trustworthiness of Content with a score of 1.71 while the scale of Usefulness received a score of 1.39. The high scores of these scales are in line with the observations made for the effectiveness and efficiency metrics and can be attributed to the highly accurate navigation due to the combination of the external GPS receiver and our custom navigation routing algorithm. The slightly reduced rating for the scale of Usefulness is due to the application\u2019s lack of wealth of features that the other two commercial applications provide. The rest of the scales received mediocre ratings with Perspicuity, Personalization and Response Behavior having scores of 1.01, 0.98, and 0.95 respectively. These marginally positive assessments highlight future improvements for our application. Overall, the variability of the responses is low as suggested by the error bars in Figure 4 with the scale of Response Behavior having the largest observed (0.4) as there were varying opinions. Sensors 2023, 23, x FOR PEER REVIEW 21 of 29\nFigure 4. Mean values of scales\u2014BlindRouteVision.\nFigure 5 shows a bar graph of the consistency results as measured with the help of\nthe Cronbach alpha coefficient. This metric is used to determine the reliability of the user\nresponses. Although there is no generally accepted rule of thumb on the value the coeffi-\ncient should have, however, in practice, a value greater than 0.7 is sufficient to qualify the\nresults as reliable. As can be seen from Figure 5, the results are reliable.\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nR at\nin g\nUX Scales\nMean value with error bars\n0 0.2 0.4 0.6 0.8\n1\nC ro\nn b\nac h\nR es\nu lt\nUX Scales\nConsistency analysis\nFigure 4. ean values of scales BlindRouteVision.\nFigure 5 shows a bar graph of the consistency results as measured with the help of the Cronbach alpha coefficient. This metric is used to determine the reliability of the user responses. Although there is no generally accepted rule of thumb on the value the coefficient should have, however, in practice, a value greater than 0.7 is sufficient to qualify the results as reliable. As can be seen from Figure 5, the results are reliable.\nSensors 2023, 23, 5411 21 of 29\nSensors 2023, 23, x FOR PEER REVIEW 21 of 29\nFigure 4. Mean values of scales\u2014BlindRouteVision.\nFigure 5 shows a bar graph of the consistency results as measured with the help of\nthe Cronbach alpha coefficient. This metric is used to determine the reliability of the user\nresponses. Although there is no generally accepted rule of thumb on the value the coeffi-\ncient should have, however, in practice, a value greater than 0.7 is sufficient to qualify the\nresults as reliable. As can be seen from Figure 5, the results are reliable.\nFigure 5. Consistency of scales\u2014BlindRouteVision.\nNext follows the Blindsquare application, the first of the two commercials. Figure 6\nshows the overall evaluation as being a very positive one. Starting in reverse, the scale of\nResponse Behavior has the lowest ranking with a score of 0.93. This can be seen as an area\nof potential improvement for the application as it concerns the qualitative elements of the\nvoice characteristics emitting the information. The highest-ranking scale was Personaliza-\ntion with a score of 1.83 followed by Trustworthiness of Content and Usefulness with a\nscore of 1.78 and 1.73 respectively. All these high assessments are due to the wealth of\ncustomizability and the accuracy of the provided information. The rest of the scales also\nhave positive evaluations with a score of 1.5 for the scale of Dependability, a score of 1.31\nfor the scale of Efficiency and a score of 1.18 for the scale of Perspicuity. Overall, the vari-\nability of the responses is low as suggested by the error bars in Figure 6 with the scale of\n\u22123 \u22122 \u22121 0 1 2 3 R at in g\nUX Scales Mean value with error bars\n0 0.2 0.4 0.6 0.8\n1\nC ro\nn b\nac h\nR es\nu lt\nUX Scales\nConsistency analysis\nFigure 5. Consistency of scales\u2014BlindRouteVision.\nNext follows the Blindsquare application, the first of the two commercials. Figure 6 shows the overall evaluation as being a very positive one. Starting in reverse, the scale of Response Behavior has the lowest ranking with a score of 0.93. This can be seen as an area of potential improvement for the application as it concerns the qualitative elements of the voice characteristics emitting the information. The highest-ranking scale was Personalization with a score of 1.83 followed by Trustworthiness of Content and Usefulness with a score of 1.78 and 1.73 respectively. All these high assessments are due to the wealth of customizability and the accuracy of the provided information. The rest of the scales also have positive evaluations with a score of 1.5 for the scale of Dependability, a score of 1.31 for the scale of Efficiency and a score of 1.18 for the scale of Perspicuity. Overall, the variability of the responses is low as suggested by the error bars in Figure 6 with the scale of Response Behavior having the largest observed (0.4) as there were varying opinions. Sensors 2023, 23, x FOR PEER REVIEW 22 of 29\nSensors 2023, 23, 5411 22 of 29\nSensors 2023, 23, x FOR PEER REVIEW 22 of 29\nFigure 6. Mean values of scales\u2014Blindsquare.\nFigure 7 shows a bar graph of the consistency results as measured with the help of\nthe Cronbach alpha coefficient. As can be seen, the results are reliable.\nFigure 7. Consistency of scales\u2014Blindsquare.\nLast but not least, Figure 8 shows the evaluation of the Lazarillo application, the sec-\nond commercial application. Among the three applications, it received the lowest positive\nevaluation. The major factor negatively impacting this application\u2019s evaluation is the dis-\nruption caused to the navigation by the spelling of the roads at intersections as mentioned\nabove in the section describing the evaluation of the effectiveness and efficiency metrics.\nThe scales of Efficiency, Perspicuity, Dependability, Usefulness, Trustworthiness of Con-\ntent and Response behaviour received scores of 0.76, 0.77, 0.72, 0.70, 0.72 and 0.9 respec-\ntively. The scale of Personalization is the only one that receives a very high positive eval-\nuation with a score of 1.73. Lazarillo provides several customizable features that all blind\nusers admit to be very useful. However, this alone is not enough to change the marginal\npositive evaluation. Likewise to the previous applications, the overall variability of the\nresponses is low as suggested from the error bars in Figure 8 with the scale of Response\n\u22123 \u22122 \u22121 0 1 2 3 R at in g\nUX Scales\nMean value with error bars\n0 0.2 0.4 0.6 0.8\n1\nC ro\nn b\nac h\nR es\nu lt\nUX Scales\nConsistency analysis\nFigure 7. Consistency of scales Blindsquare.\nLast but not least, Figure 8 shows the evaluation of the Lazarillo application, the second commercial application. Among the three applications, it received the lowest positive evaluation. The major factor negatively impacting this application\u2019s evaluation is the disruption caused to the navigation by the spelling of the roads at intersections as mentioned above in the section describing the evaluation of the effectiveness and efficiency metrics. The scales of Efficiency, Perspicuity, Dependability, Usefulness, Trustworthiness of Content and Response behaviour received scores of 0.76, 0.77, 0.72, 0.70, 0.72 and 0.9 respectively. The scale of Personalization is the only one that receives a very high positive evaluation with a score of 1.73. Lazarillo provides several customizable features that all blind users admit to be very useful. However, this alone is not enough to change the marginal positive evaluation. Likewise to the previous applications, the overall variability of the responses is low as suggested from the error bars in Figure 8 with the scale of Response Behavior having the largest observed (0.5) as there were varying opinions. Sensors 2023, 23, x FOR PEER REVIEW 23 of 29\nSensors 2023, 23, 5411 23 of 29\nSensors 2023, 23, x FOR PEER REVIEW 23 of 29\nFigure 8. Mean values of scales\u2014Lazarillo.\nFigure 9 shows a bar graph of the consistency results as measured with the help of\nthe Cronbach alpha coefficient. As can be seen, the results are reliable.\nFigure 9. Consistency of scales\u2014Lazarillo.\nOverall, from the standpoint of UX, all of the applications received a positive evalu-\nation. The highest was received by Blindsquare with a rate of 1.46 while the lowest one by\nLazarillo with a rate of 0.9. The second place was occupied by our own application Blin-\ndRouteVision with a rate of 1.36. Despite showing the best results in the scales of Effi-\nciency and Dependability, the lack of the wealth of features found in the commercial ap-\nplications reduced its overall rate. Blindsquare received the best score for the scale of Per-\nsonalization followed closely by Lazarillo. Common to all of the applications is the low\nscore for the scale of Response behaviour indicating a low satisfaction with the voice in-\nstruction subsystem. All of them utilize the default voice systems provided by the plat-\nforms supported, which are not particularly well accepted by several blind participants.\n4. Conclusions\u2013Discussion\nThis paper had a twofold target. The first one concerned the conduct of a literature\nreview concerning the use of sensor fusion techniques in blind navigation applications\n\u22123 \u22122 \u22121 0 1 2 3 R at in g\nUX Scales\nMean value with error bars\n0 0.2 0.4 0.6 0.8\n1\nC ro\nn b\nac h\nR es\nu lt\nUX Scales\nConsistency analysis\nFigure 9. Consistency of scales Lazarillo.\nOverall, fro the standpoint of UX, all of the applications received a positive evaluation. The highest was received by Blindsquare with a rate of 1.46 while the lowest one by Lazarillo with a rate of 0.9. The second place was occupied by our own application BlindRouteVision with a rate of 1.36. Despite showing the best results in the scales of Efficiency and Dependability, the lack of the wealth of features found in the commercial applications reduced its overall rate. Blindsquare received the best score for the scale of Personalization followed closely by Lazarillo. Common to all of the applications is the low score for the scale of Response behaviour indicating a low satisfaction with the voice instruction subsystem. All of them utilize the default voice systems provided by the platforms supported, which are not particularly well accepted by several blind participants.\n4. Conclusions\u2013Discussion\nThis paper had a twofold target. The first one concerned the conduct of a literature review concerning the use of sensor fusion techniques in blind navigation applications over the last five years. The second concerned the conduct of a comparative evaluation of commercial applications popular among the BVI communities. The scope of the evaluation concerned a feature-wise comparison of all the commercial applications and a Usability assessment including the two most popular applications among the BVI in Greece, namely Blindsquare and Lazarillo, with our own application BlindRouteVision. For the first target, the main point of interest was the description of the fusion techniques used in applications targeting both outdoor and indoor space navigation for BVI individuals. The study involved the classification of the solutions in terms of the employed technique, the number and range of types of sensors, and their practicality regarding cost and wearability efficiency. The results demonstrated a limited number of solutions but with an upward trend in the number of available publications as the years progressed. Overall, most of the reviewed solutions concerned overwhelmingly the case of indoor space navigation covering 89% of the total amount. Out of the total number of applications, 35% of them supported obstacle detection while roughly half of them 46% implemented obstacle avoidance as well. Kalman Filter, computer vision and deep learning approaches were heavily utilized accounting for 27%, 14% and 8% respectively. PDR at 14% and Particle Filters at 8% were also quite common. The proposed techniques use diverse sensor technologies, but IMUs are the most favoured at 57%, while camera sensors follow closely at 38%. Lidar sensors are preferred at 16%, and ultrasonic sensors at 14%. The popularity of IMU and camera sensors can be attributed to the widespread use of smartphones as the preferred platform for developing solutions for BVI individuals. Out of the proposed solutions, only 14% are effective in real-life situations, while 29% are practical but may have high costs or require multiple sensors for users to carry. Around 24% have limited practicality for specific scenarios, while 32% are purely experimental.\nSensors 2023, 23, 5411 24 of 29\nThe second target was the comparison of commercial solutions assisting blind navigation in both outdoor and indoor spaces. The comparison study considered five applications, namely Blindsquare, Lazarillo, Ariadne GPS, Nav by ViaOpta and, finally, Seeing Assistant Move. It included a description of each application as well as a list of all the available features. Furthermore, part of the evaluation involved the Usability evaluation of Blindsquare, Lazarillo and our own application BlindRouteVision. Usability was evaluated with the metrics of Effectiveness, Efficiency and UX. The results of this study highlighted that the BVI individuals required a wealth of supported features from their application but were willing to give up to a certain extent several of them in the face of better and more precise navigation on corners and intersections, which constitute places of high risk during navigation. On the other hand, the list highlighted the fact that all applications are utilizing the existing GPS technology to support outdoor navigation. A popular choice was the integration of external third-party services such as maps (Google Maps, Apple Maps, OpenStreetMaps and the like) for providing real-time navigation. Furthermore, it made evident that a modern application targeting outdoor navigation for BVI individuals needs to have at minimum the following features. An easy way to search for places and start the navigation ideally with voice control or else have a screen reader compatible structure, accurate localization of the user\u2019s position, real-time voice guidance, easily accessible way to regain both the current position and the direction as external events can throw off even the most experienced BVI individuals, route planning ahead of time, customizability of navigation instructions in terms of the wealth of information and format (orthogonal or clockwise) as cardinal instructions, commonly found in thirdparty navigation applications, are not useful as the cause of blindness, either congenital or acquired, shapes the BVI\u2019s perception of the surrounding environment [1], more detailed navigation when crossing intersections, management of the frequency and priority of the emitted instructions as well as the support of predefined and user-defined POIs that can serve the role of navigational landmarks. In order to significantly improve the provided quality of the solution, then the capability to filter other applications\u2019 notifications and obstacle detection follow are much-needed features. When the navigation application is taking place, it should be prioritized over other applications and notifications from them should be limited. Other critical applications required by the user such as phone calls or messages should be added to a whitelist and become part of a priority hierarchy to resolve any potential conflicts. As far as obstacle detection is concerned, the existing commercial solutions do not support such functionality and, instead, it is performed manually by the BVI individual with the help of the white cane. Although all BVI individuals have acquired O&M skills allowing them to detect and avoid obstacles in their route, this is not always achieved easily or safely. A system that can proactively inform the users about impending obstacles, static and/or dynamic, can significantly improve the user\u2019s effectiveness and efficiency and, thus, increase the rates of successfully reaching the destination. Between static and dynamic obstacle detection, the latter is more critical for the safety of the user as the white cane can be used to detect static objects while among the static objects critical to safety are the ones found on the user\u2019s head level such as low hanging tree branches, low balconies, signs and the like. Another very useful feature is route simulation as it provides the opportunity for the user to better learn the route and the surrounding environment prior to traversing it. Likewise to the previous case, a modern blind indoor navigation application needs to have at minimum the following. An easy way to search for POIs once the user has entered an indoor place, like emergency places, shops, toilet facilities and public spaces among others. At the very least it should support accurate localization of the user\u2019s position, more fine-grained guidance in comparison to the outdoor case given the limited mobility and freedom of movement and error correction in case the user has stranded from the navigation path, real-time guidance, customizability of navigation instructions in terms\nSensors 2023, 23, 5411 25 of 29\nof the wealth of information and frequency of emission, and an easy way for the user to access the current position as the stimulus of the external events may be overwhelming. A significant improvement in blind indoor navigation can be accomplished by adding obstacle detection capabilities for both static and dynamic objects, and the provision of obstacle avoidance functionality. This is even more crucial for indoor spaces as quite often they are crowded, i.e., one can consider the example of a shopping mall, a hospital or a subway. Another improvement is the incorporation of beacon-based solutions such as the widely used BLE beacons. These beacons are placed at static places known to the application so as to minimize the accumulated error of the various localization techniques. Further improving the localization as well as the searching capabilities is the generation of digital maps specifically tailored to the requirements of the location. This can provide significant benefits; however, it requires the active participation of the people managing the place being mapped. The latter applies to both the scaling down of the infrastructure and to the ease of management of such dense information. Last but not least, is the use of simulation tools, to allow the user to create families of microprocessors and of other peripherals. As far as it concerns the improvements of commercial applications, the scientific literature can provide new features for indoor and outdoor space navigation. For the issue of directionality due to the smartphone GPS sensors\u2019 poor accuracy, external GPS receivers can be used as demonstrated in [7]. There the authors combine a high-accuracy external GPS receiver with a novel route navigation algorithm to achieve a better level of accuracy and guidance. For the obstacle detection feature many of the multi-sensor fusion solutions presented in the literature review can be leveraged. The main techniques employed include the use of ultrasonic sensors, computer vision and deep learning-based solutions. The use of cameras can significantly improve the commercially available applications; however, they do come with their own set of restrictions. Despite their undeniable benefits, camerabased solutions usually are associated with higher costs, as it is quite common to deploy more than one type of camera sensor and may incur an additional computational cost that may impact battery-based devices. Ultrasonic sensors, on the other hand, have a significantly lower cost on average and computational overhead but do not demonstrate the same strong results. Despite the lack of sensor-based assistive solutions, the literature review demonstrated the benefits of this approach and an increasing trend in adopting these solutions, especially with computer vision and deep learning techniques as the main driver.\nAuthor Contributions: Conceptualization, P.T. and K.T.; Data curation, A.M.; Formal analysis, P.T. and K.T.; Funding acquisition, A.M.; Methodology, P.T. and K.T.; Project administration, A.M.; Resources, P.T. and A.M.; Supervision, A.M.; Validation, P.T., K.T. and A.M.; Visualization, P.T. and K.T.; Writing\u2014original draft, P.T. and K.T.; Writing\u2014review & editing, P.T. and K.T. All authors have read and agreed to the published version of the manuscript.\nFunding: This research was funded by the Greek RTDI State Aid Action RESEARCH-CREATEINNOVATE of the National EPAnEK 2014\u20132020 Operational Program \u201cCompetitiveness, Entrepreneurship and Innovation\u201d in the framework of the MANTO project, under contract No. 593. The paper is also partially supported by the University of Piraeus Research Center.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Informed consent was obtained from all end users involved in the study. Written informed consent for publication was not required and therefore not obtained from anonymized users participating in the system evaluation.\nData Availability Statement: Not applicable.\nConflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.\nSensors 2023, 23, 5411 26 of 29\nReferences 1. Wang, W.; Chang, Q.; Li, Q.; Shi, Z.; Chen, W. Indoor-Outdoor Detection Using a Smart Phone Sensor. Sensors 2016, 16, 1563. [CrossRef] [PubMed] 2. Teng, X.; Guo, D.; Guo, Y.; Zhou, X.; Ding, Z.; Liu, Z. IONavi: An Indoor-Outdoor Navigation Service via Mobile Crowdsensing. ACM Trans. Sen. Netw. 2017, 13, 1\u201328. [CrossRef] 3. Huang, H.; Zeng, Q.; Chen, R.; Meng, Q.; Wang, J.; Zeng, S. Seamless Navigation Methodology Optimized for Indoor/Outdoor\nDetection Based on WIFI. In Proceedings of the 2018 Ubiquitous Positioning, Indoor Navigation and Location-Based Services (UPINLBS) IEEE, Wuhan, China, 22\u201323 March 2018; pp. 1\u20137. [CrossRef]\n4. Real, S.; Araujo, A. Navigation Systems for the Blind and Visually Impaired: Past Work, Challenges, and Open Problems. Sensors 2019, 19, 3404. [CrossRef] [PubMed] 5. Esmaeili Kelishomi, A.; Garmabaki, A.H.S.; Bahaghighat, M.; Dong, J. Mobile User Indoor-Outdoor Detection Through Physical Daily Activities. Sensors 2019, 19, 511. [CrossRef] [PubMed] 6. Zhu, N.; Ortiz, M.; Renaudin, V. Seamless Indoor-Outdoor Infrastructure-Free Navigation for Pedestrians and Vehicles with GNSSAided Foot-Mounted IMU. In Proceedings of the 2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN), Pisa, Italy, 30 September\u20133 October 2019; IEEE: Pisa, Italy, 2019; pp. 1\u20138. [CrossRef] 7. Yan, J.; Diakit\u00e9, A.A.; Zlatanova, S. A Generic Space Definition Framework to Support Seamless Indoor/Outdoor Navigation Systems. Trans. GIS 2019, 23, 1273\u20131295. [CrossRef] 8. Cheraghi, S.A.; Almadan, A.; Namboodiri, V. CityGuide: A Seamless Indoor-Outdoor Wayfinding System for People with Vision Impairments. In Proceedings of the The 21st International ACM SIGACCESS Conference on Computers and Accessibility, Pittsburgh, PA, USA, 28\u201330 October 2019; ACM: Pittsburgh, PA, USA, 2019; pp. 542\u2013544. [CrossRef] 9. Xu, J.; Xue, F.; Chiaradia, A.; Lu, W.; Cao, J. Indoor-Outdoor Navigation without Beacons: Compensating Smartphone AR Positioning Errors with 3D Pedestrian Network. In Construction Research Congress 2020; American Society of Civil Engineers: Tempe, AZ, USA, 2020; pp. 444\u2013452. [CrossRef] 10. Costa, C.; Ge, X.; McEllhenney, E.; Kebler, E.; Chrysanthis, P.K.; Zeinalipour-Yazti, D. CAPRIOv2.0: A Context-Aware Unified Indoor-Outdoor Path Recommendation System. In Proceedings of the 2020 21st IEEE International Conference on Mobile Data Management (MDM), Versailles, France, 30 June\u20133 July 2020; IEEE: Versailles, France, 2020; pp. 230\u2013231. [CrossRef] 11. Lee, K.; Sato, D.; Asakawa, S.; Kacorri, H.; Asakawa, C. Pedestrian Detection with Wearable Cameras for the Blind: A Two-way Perspective. 2020. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI\u201920), Honolulu, HI, USA, 25\u201330 April 2020; Association for Computing Machinery: New York, NY, USA, 2020; pp. 1\u201312. [CrossRef] 12. Shelton, A.; Ogunfunmi, T. Developing a Deep Learning-Enabled Guide for the Visually Impaired. In Proceedings of the 2020 IEEE Global Humanitarian Technology Conference (GHTC), Seattle, WA, USA, 29 October\u20131 November 2020; IEEE: Seattle, WA, USA, 2020; pp. 1\u20138. [CrossRef] 13. Al Khatib, E.I.; Jaradat, M.A.K.; Abdel-Hafez, M.F. Low-Cost Reduced Navigation System for Mobile Robot in Indoor/Outdoor Environments. IEEE Access 2020, 8, 25014\u201325026. [CrossRef] 14. Congram, B.; Barfoot, T.D. Relatively Lazy: Indoor-Outdoor Navigation Using Vision and GNSS. In Proceedings of the 2021 18th Conference on Robots and Vision (CRV), Burnaby, BC, Canada, 26\u201328 May 2021. [CrossRef] 15. Ren, P.; Elyasi, F.; Manduchi, R. Smartphone-Based Inertial Odometry for Blind Walkers. Sensors 2021, 21, 4033. [CrossRef] 16. Senjam, S.S.; Manna, S.; Bascaran, C. Smartphones-Based Assistive Technology: Accessibility Features and Apps for People with Visual Impairment, and Its Usage, Challenges, and Usability Testing. OPTO 2021, 13, 311\u2013322. [CrossRef] 17. Rajak, S.; Panja, A.K.; Chowdhury, C.; Neogy, S. A Ubiquitous Indoor\u2013Outdoor Detection and Localization Framework\nfor Smartphone Users. In Emerging Technologies in Data Mining and Information Security; Hassanien, A.E., Bhattacharyya, S., Chakrabati, S., Bhattacharya, A., Dutta, S., Eds.; Advances in Intelligent Systems and Computing; Springer: Singapore, 2021; Volume 1286, pp. 693\u2013701. [CrossRef]\n18. Das, U.; Namboodiri, V.; He, H. PathLookup: A Deep Learning-Based Framework to Assist Visually Impaired in Outdoor Wayfinding. In Proceedings of the 2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops), Kassel, Germany, 22\u201326 March 2021; IEEE: Kassel, Germany, 2021; pp. 111\u2013116. [CrossRef] 19. Bai, Y.B.; Holden, L.; Kealy, A.; Zaminpardaz, S.; Choy, S. A Hybrid Indoor/Outdoor Detection Approach for Smartphone-Based Seamless Positioning. J. Navig. 2022, 75, 946\u2013965. [CrossRef] 20. Schyga, J.; Hinckeldeyn, J.; Kreutzfeldt, J. Meaningful Test and Evaluation of Indoor Localization Systems in Semi-Controlled Environments. Sensors 2022, 22, 2797. [CrossRef] 21. Chandna, S.; Singhal, A. Towards Outdoor Navigation System for Visually Impaired People Using YOLOv5. In Proceedings of the 2022 12th International Conference on Cloud Computing, Data Science & Engineering (Confluence), Noida, India, 27\u201328 January 2022; IEEE: Noida, India, 2022; pp. 617\u2013622. [CrossRef] 22. Koutris, A.; Siozos, T.; Kopsinis, Y.; Pikrakis, A.; Merk, T.; Mahlig, M.; Papaharalabos, S.; Karlsson, P. Deep Learning-Based Indoor Localization Using Multi-View BLE Signal. Sensors 2022, 22, 2759. [CrossRef] [PubMed] 23. Liu, Q.; Gao, C.; Shang, R.; Peng, Z.; Zhang, R.; Gan, L. Environment Perception Based Seamless Indoor and Outdoor Positioning System of Smartphone. IEEE Sens. J. 2022, 22, 17205\u201317215. [CrossRef]\nSensors 2023, 23, 5411 27 of 29\n24. Mallik, M.; Panja, A.K.; Chowdhury, C. Paving the Way with Machine Learning for Seamless Indoor\u2013Outdoor Positioning: A Survey. Inf. Fusion 2023, 94, 126\u2013151. [CrossRef] 25. Theodorou, P.; Meliones, A. Gaining Insight for the Design, Development, Deployment and Distribution of Assistive Navigation Systems for Blind and Visually Impaired People through a Detailed User Requirements Elicitation, Universal Access in the Information Society (UAIS); Springer International Publishing: Cham, Switzerland, 2022. 26. In Proceedings of the IMC18-International and Mobility Conference, Warswaw, Poland, 22\u201326 May 2023. Available online: https://imc18poland.com/imc18 (accessed on 1 June 2023). 27. Ariadne\u2013GPS. Available online: https://ariadnegps.eu (accessed on 27 April 2023). 28. Lazarillo. Available online: https://lazarillo.app/theapp/ (accessed on 27 April 2023). 29. Nav by ViaOpta. Available online: https://apps.apple.com/us/app/nav-by-viaopta/id908435532 (accessed on 27 April 2023). 30. Seeing Assistant Move. Available online: https://seeingassistant.tt.com.pl/move (accessed on 27 April 2023). 31. Blindsquare. Available online: https://blindsquare.com (accessed on 27 April 2023). 32. Petersen, K.; Feldt, R.; Mujtaba, S.; Mattsson, M. Systematic mapping studies in software engineering. In Proceedings of the 12th\ninternational conference on Evaluation and Assessment in Software Engineering (EASE\u201908), Swindon, UK, 26\u201327 June 2008; GBR; BCS Learning & Development Ltd.: Swindon, UK, 2008; pp. 68\u201377.\n33. Kitchenham, B.; Brereton, O.P.; Budgen, D.; Turner, M.; Bailey, J.; Linkman, S. Systematic literature reviews in software engineering\u2013A systematic literature review. In Information and Software Technology; Elsevier: Amsterdam, The Netherlands, 2009; Volume 51, pp. 7\u201315. [CrossRef] 34. MANTO Project. Available online: https://manto.ds.unipi.gr (accessed on 27 April 2023). 35. Meliones, A.; Sampson, D. Blind MuseumTourer: A System for Self-Guided Tours in Museums and Blind Indoor Navigation. Technologies 2018, 6, 4. [CrossRef] 36. Theodorou, P.; Tsiligkos, K.; Meliones, A.; Filios, C. A Training Smartphone Application for the Simulation of Outdoor Blind Pedestrian Navigation: Usability, UX Evaluation, Sentiment Analysis. Sensors 2023, 23, 367. [CrossRef] [PubMed] 37. Theodorou, P.; Tsiligkos, K.; Meliones, A.; Filios, C. An Extended Usability and UX Evaluation of a Mobile Application for the\nNavigation of Individuals with Blindness and Visual Impairments Outdoors\u2014An Evaluation Framework Based on Training. Sensors 2022, 22, 4538. [CrossRef] [PubMed]\n38. Theodorou, P.; Tsiligkos, K.; Meliones, A.; Tsigris, A. An extended usability and UX evaluation of a mobile application for the navigation of individuals with blindness and visual impairments indoors: An evaluation approach combined with training sessions. Br. J. Vis. Impair. 2022, 38. [CrossRef] 39. ISO/IEC 25010:2011. Available online: https://www.iso.org/standard/35733.html (accessed on 2 April 2022). 40. Schrepp, M.; Thomaschewski, J. Design and Validation of a Framework for the Creation of User Experience Questionnaires. Int. J. Interact. Multimed. Artif. Intell. 2019, 5, 88\u201395. [CrossRef] 41. Nakashima, R.; Seki, A. Uncertainty-Based Adaptive Sensor Fusion for Visual-Inertial Odometry under Various Motion Charac-\nteristics. In Proceedings of the 2020 IEEE International Conference on Robotics and Automation (ICRA), Paris, France, 31 May\u201331 August 2020; pp. 3119\u20133125.\n42. Lu, C.X.; Saputra, M.R.U.; Zhao, P.; Almalioglu, Y.; de Gusmao, P.P.B.; Chen, C.; Sun, K.; Trigoni, N.; Markham, A. MilliEgo: Single-Chip MmWave Radar Aided Egomotion Estimation via Deep Sensor Fusion. In Proceedings of the 18th Conference on Embedded Networked Sensor Systems, Virtual, 16\u201319 November 2020; Association for Computing Machinery: New York, NY, USA, 2020; pp. 109\u2013122. 43. Chen, C.; Rosa, S.; Miao, Y.; Lu, C.X.; Wu, W.; Markham, A.; Trigoni, N. Selective Sensor Fusion for Neural Visual-Inertial Odometry. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA, 15\u201320 June 2019; pp. 10534\u201310543. 44. Deng, L.; Li, X.; Zhang, Y. Research on Visual and Inertia Fusion Odometry Based on PROSAC Mismatched Culling Algorithm. In Proceedings of the 2019 International Conference on Robotics Systems and Vehicle Technology, Wuhan, China, 16\u201318 October 2019; Association for Computing Machinery: New York, NY, USA, 2019; pp. 113\u2013118. 45. Wang, Z.; Li, X.; Zhang, X.; Bai, Y.; Zheng, C. An Attitude Estimation Method Based on Monocular Vision and Inertial Sensor Fusion for Indoor Navigation. IEEE Sens. J. 2021, 21, 27051\u201327061. [CrossRef] 46. Zeng, Q.; Wang, J.; Meng, Q.; Zhang, X.; Zeng, S. Seamless Pedestrian Navigation Methodology Optimized for Indoor/Outdoor Detection. IEEE Sens. J. 2018, 18, 363\u2013374. [CrossRef] 47. Galioto, G.; Tinnirello, I.; Croce, D.; Inderst, F.; Pascucci, F.; Giarr\u00e9, L. Sensor Fusion Localization and Navigation for Visually Impaired People. In Proceedings of the 2018 European Control Conference (ECC), Snowbird, UT, USA, 16\u201320 October 2017; pp. 3191\u20133196. 48. Croce, D.; Giarr\u00e9, L.; Pascucci, F.; Tinnirello, I.; Galioto, G.E.; Garlisi, D.; Lo Valvo, A. An Indoor and Outdoor Navigation System for Visually Impaired People. IEEE Access 2019, 7, 170406\u2013170418. [CrossRef] 49. Khan, N.A.; Ansari, R. Real-Time Traffic Light Detection from Videos with Inertial Sensor Fusion. In Proceedings of the 1st ACM SIGSPATIAL Workshop on Advances on Resilient and Intelligent Cities, Seattle, WA, USA, 6 November 2018; Association for Computing Machinery: New York, NY, USA, 2018; pp. 31\u201340.\nSensors 2023, 23, 5411 28 of 29\n50. Chaudhari, G.; Deshpande, A. Robotic Assistant for Visually Impaired Using Sensor Fusion. In Proceedings of the 2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), San Francisco, CA, USA, 4\u20138 August 2017; pp. 1\u20134. 51. Bharati, V. LiDAR + Camera Sensor Data Fusion on Mobiles with AI-Based Virtual Sensors to Provide Situational Awareness for the Visually Impaired. In Proceedings of the 2021 IEEE Sensors Applications Symposium (SAS), Sundsvall, Sweden, 23\u201325 August 2021; pp. 1\u20136. 52. Elmannai, W.M.; Elleithy, K.M. A Highly Accurate and Reliable Data Fusion Framework for Guiding the Visually Impaired. IEEE Access 2018, 6, 33029\u201333054. [CrossRef] 53. Chen, H.; Wang, K.; Yang, K. Improving RealSense by Fusing Color Stereo Vision and Infrared Stereo Vision for the Visually Impaired. In Proceedings of the 1st International Conference on Information Science and Systems, Jeju, Republic of Korea, 27\u201329 April 2018; Association for Computing Machinery: New York, NY, USA, 2018; pp. 142\u2013146. 54. Hakim, H.; Fadhil, A. Navigation System for Visually Impaired People Based on RGB-D Camera and Ultrasonic Sensor. In Proceedings of the International Conference on Information and Communication Technology, Baghdad, Iraq, 15\u201316 April 2019; Association for Computing Machinery: New York, NY, USA, 2019; pp. 172\u2013177. 55. Long, N.; Wang, K.; Cheng, R.; Yang, K.; Bai, J. Fusion of Millimeter Wave Radar and RGB-Depth Sensors for Assisted Navigation of the Visually Impaired. In Proceedings of the Millimetre Wave and Terahertz Sensors and Technology XI, Berlin, Germany, 10\u201311 September 2018; SPIE: Bellingham, WA, USA, 2018; 10800, pp. 21\u201328. 56. Zhao, Y.; Huang, R.; Hu, B. A Multi-Sensor Fusion System for Improving Indoor Mobility of the Visually Impaired. In Proceedings of the 2019 Chinese Automation Congress (CAC), Hangzhou, China, 22\u201324 November 2019; pp. 2950\u20132955. 57. Chen, J.; Ruci, A.; Sturdivant, E.; Zhu, Z. ARMSAINTS: An AR-Based Real-Time Mobile System for Assistive Indoor Navigation with Target Segmentation. In Proceedings of the 2022 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO), Long Beach, CA, USA, 28\u201330 May 2022; pp. 1\u20136. 58. Baskar, V.V.; Ghosh, I.; Karthikeyan, S.; Hemalatha, R.J.; Thamizhvani, T.R. An Indoor Obstacle Detector to Assist the Visually Impaired Person on Real-Time with a Navigator. In Proceedings of the 2021 International Conference on Computational Performance Evaluation (ComPE), Shillong, India, 1\u20133 December 2021; pp. 136\u2013141. 59. Silva, C.S.; Wimalaratne, P. Sensor Fusion for Visually Impaired Navigation in Constrained Spaces. In Proceedings of the 2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS), Galle, Sri Lanka, 16\u201319 December 2016; pp. 1\u20136. 60. Bouteraa, Y. Design and Development of a Wearable Assistive Device Integrating a Fuzzy Decision Support System for Blind and Visually Impaired People. Micromachines 2021, 12, 1082. [CrossRef] 61. Mahida, P.T.; Shahrestani, S.; Cheung, H. Indoor Positioning Framework for Visually Impaired People Using Internet of Things. In Proceedings of the 2019 13th International Conference on Sensing Technology (ICST), Sydney, NSW, Australia, 2\u20134 December 2019; pp. 1\u20136. 62. El-Naggar, A.; Wassal, A.; Sharaf, K. Indoor Positioning Using WiFi RSSI Trilateration and INS Sensor Fusion System Simulation. In Proceedings of the 2019 2nd International Conference on Sensors, Signal and Image Processing, Prague, Czech Republic, 8\u201310 October 2019; Association for Computing Machinery: New York, NY, USA, 2019; pp. 21\u201326. 63. Li, M.; Ammanabrolu, J. Indoor Way-Finding Method Using IMU and Magnetic Tensor Sensor Measurements for Visually Impaired Users. Int. J. Intell. Robot Appl. 2021, 5, 264\u2013282. [CrossRef] 64. Marzec, P.; Kos, A. Low Energy Precise Navigation System for the Blind with Infrared Sensors. In Proceedings of the 2019 MIXDES-26th International Conference \u201cMixed Design of Integrated Circuits and Systems\u201d, Rzeszow, Poland, 27\u201329 June 2019; pp. 394\u2013397. 65. Gong, J.; Zhang, X.; Huang, Y.; Ren, J.; Zhang, Y. Robust Inertial Motion Tracking through Deep Sensor Fusion across Smart Earbuds and Smartphone. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2021, 5, 1\u201326. [CrossRef] 66. Murata, M.; Ahmetovic, D.; Sato, D.; Takagi, H.; Kitani, K.M.; Asakawa, C. Smartphone-Based Localization for Blind Navigation in Building-Scale Indoor Environments. Pervasive Mob. Comput. 2019, 57, 14\u201332. [CrossRef] 67. Chung, S.; Lim, J.; Noh, K.J.; Kim, G.; Jeong, H. Sensor Data Acquisition and Multimodal Sensor Fusion for Human Activity Recognition Using Deep Learning. Sensors 2019, 19, 1716. [CrossRef] 68. Gill, S.; Pawluk, D.T.V. Design of a \u201cCobot Tactile Display\u201d for Accessing Virtual Diagrams by Blind and Visually Impaired Users. Sensors 2022, 22, 4468. [CrossRef] 69. Xue, H.; Jiang, W.; Miao, C.; Yuan, Y.; Ma, F.; Ma, X.; Wang, Y.; Yao, S.; Xu, W.; Zhang, A.; et al. DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory Data. In Proceedings of the Twentieth ACM International Symposium on Mobile Ad Hoc Networking and Computing, Catania, Italy, 2\u20135 July 2019; Association for Computing Machinery: New York, NY, USA, 2019; pp. 151\u2013160. 70. Saputra, M.R.U.; de Gusmao, P.P.B.; Lu, C.X.; Almalioglu, Y.; Rosa, S.; Chen, C.; Wahlstr\u00f6m, J.; Wang, W.; Markham, A.; Trigoni, N. DeepTIO: A Deep Thermal-Inertial Odometry With Visual Hallucination. IEEE Robot. Autom. Lett. 2020, 5, 1672\u20131679. [CrossRef] 71. Gharghan, S.K.; Al-Kafaji, R.D.; Mahdi, S.Q.; Zubaidi, S.L.; Ridha, H.M. Indoor Localization for the Blind Based on the Fusion of a Metaheuristic Algorithm with a Neural Network Using Energy-Efficient WSN. Arab. J. Sci. Eng. 2022, 48, 6025\u20136052. [CrossRef]\nSensors 2023, 23, 5411 29 of 29\n72. Patil, K.; Jawadwala, Q.; Shu, F.C. Design and Construction of Electronic Aid for Visually Impaired People. IEEE Trans. Hum.-Mach. Syst. 2018, 48, 172\u2013182. [CrossRef] 73. Elbakly, R.; Elhamshary, M.; Youssef, M. HyRise: A Robust and Ubiquitous Multi-Sensor Fusion-Based Floor Localization System. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2018, 2, 1\u201323. [CrossRef] 74. Kov\u00e1cs, G.; Nagy, S. Ultrasonic Sensor Fusion Inverse Algorithm for Visually Impaired Aiding Applications. Sensors 2020, 20, 3682. [CrossRef] [PubMed] 75. Yao, S.; Zhao, Y.; Shao, H.; Liu, D.; Liu, S.; Hao, Y.; Piao, A.; Hu, S.; Lu, S.; Abdelzaher, T.F. SADeepSense: Self-Attention Deep Learning Framework for Heterogeneous On-Device Sensors in Internet of Things Applications. In Proceedings of the IEEE INFOCOM 2019-IEEE Conference on Computer Communications, Paris, France, 29 April\u20132 May 2019; pp. 1243\u20131251. 76. Junoh, S.A.; Subedi, S.; Pyun, J.-Y. Smartphone-Based Indoor Navigation System Using Particle Filter and Map-Constraints. In Proceedings of the The 9th International Conference on Smart Media and Applications, Jeju, Republic of Korea, 17\u201319 September 2020; Association for Computing Machinery: New York, NY, USA, 2021; pp. 354\u2013357. 77. Zhou, C.; Chen, S.; Chen, J. Research on Indoor Positioning of Multi-Source Information Fusion Based on Improved Particle Filter. In Proceedings of the 6th International Conference on Computer Science and Application Engineering, Virtual, 21\u201323 October 2022; Association for Computing Machinery: New York, NY, USA, 2022; pp. 1\u20137. 78. Huang, H.-Y.; Hsieh, C.-Y.; Liu, K.-C.; Cheng, H.-C.; Hsu, S.J.; Chan, C.-T. Multi-Sensor Fusion Approach for Improving Map-Based Indoor Pedestrian Localization. Sensors 2019, 19, 3786. [CrossRef] [PubMed] 79. Niu, Q.; Liu, N.; Huang, J.; Luo, Y.; He, S.; He, T.; Chan, S.-H.G.; Luo, X. DeepNavi: A Deep Signal-Fusion Framework for Accurate and Applicable Indoor Navigation. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2019, 3, 1\u201324. [CrossRef] 80. Simoes, W.C.S.S.; da Silva, L.M.; da Silva, V.J.; de Lucena, V.F. A Guidance System for Blind and Visually Impaired People via Hybrid Data Fusion. In Proceedings of the 2018 IEEE Symposium on Computers and Communications (ISCC), Natal, Brazil, 25\u201328 June 2018; pp. 01261\u201301266. 81. Blindsquare\u2013Capability Maturity Model\u2013Circa 2023. Available online: https://docs.google.com/document/d/e/2PACX1vRL2RKIaGhpj2GvuVy9sR2eQOBEhN8rcEovhF5WJBm2qA3jf0-OhyMCZ9cqsig7qgN_MqGVCe5wINkV/pub (accessed on 27 April 2023).\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
        }
    ],
    "title": "Multi-Sensor Data Fusion Solutions for Blind and Visually Impaired: Research and Commercial Navigation Applications for Indoor and Outdoor Spaces",
    "year": 2023
}