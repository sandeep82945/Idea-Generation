{
    "abstractText": "The heavy-tailed behavior of the generalized extreme-value distribution makes it a popular choice for modeling extreme events such as floods, droughts, heatwaves, wildfires, etc. However, estimating the distribution\u2019s parameters using conventional maximum likelihood methods can be computationally intensive, even for moderate-sized datasets. To overcome this limitation, we propose a computationally efficient, likelihood-free estimation method utilizing a neural network. Through an extensive simulation study, we demonstrate that the proposed neural network-based method provides Generalized Extreme Value (GEV) distribution parameter estimates with comparable accuracy to the conventional maximum likelihood method but with a significant computational speedup. To account for estimation uncertainty, we utilize parametric bootstrapping, which is inherent in the trained network. Finally, we apply this method to 1000-year annual maximum temperature data from the Community Climate System Model version 3 (CCSM3) across North America for three atmospheric concentrations: 289 ppm CO2 (pre-industrial), 700 ppm CO2 (future conditions), and 1400 ppm CO2, and compare the results with those obtained using the maximum likelihood approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sweta Rai"
        },
        {
            "affiliations": [],
            "name": "Alexis Hoffman"
        },
        {
            "affiliations": [],
            "name": "Soumendra Lahiri"
        },
        {
            "affiliations": [],
            "name": "Douglas W. Nychka"
        },
        {
            "affiliations": [],
            "name": "Stephan R. Sain"
        },
        {
            "affiliations": [],
            "name": "Soutir Bandyopadhyay"
        }
    ],
    "id": "SP:66d3a6401f20692accfda8401936acaaa7d07d93",
    "references": [
        {
            "authors": [
                "W.N. Adger"
            ],
            "title": "Social capital, collective action, and adaptation to climate",
            "year": 2003
        },
        {
            "authors": [
                "T.G. 387\u2013404. Bali"
            ],
            "title": "The generalized extreme value distribution\u2019, Economics letters",
            "year": 2003
        },
        {
            "authors": [
                "A. B\u00fccher",
                "J. Segers"
            ],
            "title": "On the maximum likelihood estimator for the generalized extreme",
            "year": 2016
        },
        {
            "authors": [
                "E. Casson",
                "S. Coles"
            ],
            "title": "Spatial regression models for extremes\u2019, Extremes 1, 449\u2013468",
            "year": 1999
        },
        {
            "authors": [
                "D. Castro-Camilo",
                "R. Huser",
                "H. Rue"
            ],
            "title": "Practical strategies for gev-based regression",
            "year": 2021
        },
        {
            "authors": [
                "S. Coles",
                "J. Bawa",
                "L. Trenner",
                "P. Dorazio"
            ],
            "title": "An introduction to statistical modeling",
            "venue": "an artificial neural network\u2019,",
            "year": 2001
        },
        {
            "authors": [
                "P. Chang",
                "S.C. Doney",
                "J.J. Hack",
                "Henderson",
                "T. B"
            ],
            "title": "The community climate",
            "year": 2006
        },
        {
            "authors": [
                "D. Cooley",
                "D. Nychka",
                "P. Naveau"
            ],
            "title": "Bayesian spatial modeling of extreme precipitation",
            "venue": "Journal of Climate",
            "year": 2007
        },
        {
            "authors": [
                "A.C. Davison",
                "R. Huser"
            ],
            "title": "Statistics of extremes",
            "venue": "Annual Review of Statistics",
            "year": 2015
        },
        {
            "authors": [
                "B. Efron",
                "R.J. Tibshirani"
            ],
            "title": "An introduction to the bootstrap, CRC press",
            "venue": "El Adlouni,",
            "year": 1994
        },
        {
            "authors": [
                "F. e2744. Gerber",
                "D. Nychka"
            ],
            "title": "Fast covariance parameter estimation of spatial gaussian process",
            "year": 2021
        },
        {
            "authors": [
                "G. Hinton",
                "N. Srivastava",
                "K. Swersky"
            ],
            "title": "Neural networks for machine learning lecture 6a overview of mini-batch gradient descent",
            "venue": "Cited on 14(8),",
            "year": 2012
        },
        {
            "authors": [
                "J.R.M. Hosking",
                "J.R. Wallis",
                "E.F. Wood"
            ],
            "title": "Estimation of the generalized extreme-value distribution by the method of probability-weighted moments",
            "year": 1985
        },
        {
            "authors": [
                "W.K. Huang",
                "M.L. Stein",
                "D.J. McInerney",
                "S. Sun",
                "E.J. Moyer"
            ],
            "title": "Estimating changes in temperature extremes from millennial-scale climate simulations using generalized extreme value (gev) distributions",
            "venue": "Advances in Statistical Climatology, Meteorology and Oceanography",
            "year": 2016
        },
        {
            "authors": [
                "A.F. Jenkinson"
            ],
            "title": "The frequency distribution of the annual maximum (or minimum) values of meteorological elements",
            "venue": "Quarterly Journal of the Royal Meteorological Society",
            "year": 1955
        },
        {
            "authors": [
                "B. Jiang",
                "Wu",
                "T.-y",
                "C. Zheng",
                "W.H. Wong"
            ],
            "title": "Learning summary statistic for approximate bayesian computation via deep neural network",
            "year": 2017
        },
        {
            "authors": [
                "R.W. Katz"
            ],
            "title": "Statistics of extremes in climate change",
            "venue": "Climatic change",
            "year": 2010
        },
        {
            "authors": [
                "J. Kysel\u1ef3"
            ],
            "title": "A cautionary note on the use of nonparametric bootstrap for estimating uncertainties in extreme-value models",
            "venue": "Journal of Applied Meteorology and Climatology",
            "year": 2008
        },
        {
            "authors": [
                "A. Lenzi",
                "J. Bessac",
                "J. Rudi",
                "M.L. Stein"
            ],
            "title": "Neural networks for parameter estimation in intractable models",
            "year": 2021
        },
        {
            "authors": [
                "R. Majumder",
                "B.J. Reich",
                "B.A. Shaby"
            ],
            "title": "Modeling extremal streamflow using deep learning approximations and a flexible spatial process",
            "year": 2022
        },
        {
            "authors": [
                "E.S. Martins",
                "J.R. Stedinger"
            ],
            "title": "Generalized maximum-likelihood generalized extremevalue quantile estimators for hydrologic data",
            "venue": "Water Resources Research",
            "year": 2000
        },
        {
            "authors": [
                "T. Opitz",
                "R. Huser",
                "H. Bakka",
                "H. Rue"
            ],
            "title": "Inla goes extreme: Bayesian tail regression for the estimation of high spatio-temporal quantiles",
            "venue": "Extremes",
            "year": 2018
        },
        {
            "authors": [
                "S. Power",
                "M. Lengaigne",
                "A. Capotondi",
                "M. Khodri",
                "J. Vialard",
                "B. Jebri",
                "E. Guilyardi",
                "S. McGregor",
                "Kug",
                "J.-S",
                "M Newman"
            ],
            "title": "Decadal climate variability in the tropical pacific: Characteristics, causes, predictability, and prospects",
            "venue": "Science 374(6563),",
            "year": 2021
        },
        {
            "authors": [
                "J. Richards",
                "R. Huser"
            ],
            "title": "A unifying partially-interpretable framework for neural networkbased extreme quantile regression",
            "year": 2022
        },
        {
            "authors": [
                "M. Sainsbury-Dale",
                "A. Zammit-Mangion",
                "R. Huser"
            ],
            "title": "Fast optimal estimation with intractable models using permutation-invariant neural networks",
            "year": 2022
        },
        {
            "authors": [
                "S. Singer",
                "J. Nelder"
            ],
            "title": "Nelder-mead algorithm",
            "venue": "Scholarpedia",
            "year": 2009
        },
        {
            "authors": [
                "R. Yadav",
                "R. Huser",
                "T. Opitz"
            ],
            "title": "A flexible bayesian hierarchical modeling framework",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: Deep neural networks; Generalized Extreme Value distribution; Parameter estimation; Sufficient statistics; Extreme quantiles.\n\u2217SR, SB, and DWN\u2019s work has been partially supported by the National Science Foundation, CMMI-2210840, and SL\u2019s work has been partially supported by the National Science Foundation, CMMI-2210811. Corresponding author email: sbandyopadhyay@mines.edu.\nar X\niv :2\n30 5.\n04 34\n1v 1\n[ st"
        },
        {
            "heading": "1 Introduction",
            "text": "It is widely acknowledged that the Earth is currently undergoing a period of climate change, which is resulting in extreme variability in weather patterns and constant disturbances in the surrounding environment. In order to inform planning and mitigate risks associated with these changes, it is crucial to gain a thorough understanding of these extreme events and quantify their potential impact (see, Adger (2003), Power et al. (2021)). To estimate the probability of extreme events occurring, extreme value theory (EVT) is widely used in various fields such as econometrics (Bali (2003)), finance (Broussard & Booth (1998)), materials science (Castillo (2012)), environmental science (Katz (2010), Martins & Stedinger (2000)), and reliability engineering (Smith (1991)). For more detailed information on EVT, please refer to Fisher & Tippett (1928), Hosking et al. (1985), Smith (1985, 1990), Bali (2003), Coles et al. (2001), Gumbel (2004), Davison & Huser (2015) and the references therein. Under the univariate EVT framework, the GEV distribution is commonly used for modeling extreme events due to its flexibility and sound theoretical foundation. One of its significant characteristics is the shape parameter, which governs various tail behaviors, ranging from a restricted distribution to one with heavy tails, as described in Coles et al. (2001), Gumbel (2004), Davison & Huser (2015), Haan & Ferreira (2006). Typically, the maximum likelihood (ML) or moment-based methods are utilized to estimate GEV parameters, both of which can be intricate and time-consuming as it involves numerical computation due to domain dependence on the shape, and other parameters. As a result, recent methods for fitting a GEV distribution have looked into alternatives to utilizing the exact likelihood (see, Casson & Coles (1999), El Adlouni & Ouarda (2009), Opitz et al. (2018), Erhardt & Smith (2012), Castro-Camilo et al. (2021)). One such approach is the use of Bayesian methods, such as Approximate Bayesian Computation (ABC), which has been employed to examine the dependence for modeling max-stable processes (see, Blum (2010), Erhardt & Smith (2012), Erhardt & Sisson (2016), Sisson et al. (2018)). On the other hand, several deep-learning algorithms have recently been developed for likelihood-free inference for extreme events (see, Creel (2017), Lenzi et al. (2021), Sainsbury-Dale et al. (2022), Richards & Huser (2022), and the references therein). These works show promising results in accuracy as compared to the classical approaches, along with a potential speed-up factor in the overall estimation process. However, the application of these works mostly focuses on modeling spatial extremes using max-stable processes for higher dimensions.\nThis work is motivated by recent applications of the ABC method and the deep-learning algorithm to spatial extremes as described above, as well as the use of neural networks (NN) for time series and spatial data, as evidenced in papers such as Chon & Cohen (1997), Cremanns & Roos (2017), Gerber & Nychka (2021), Wikle & Zammit-Mangion (2022), Majumder et al. (2022). In this work, we present a new estimation method that utilizes a NN to fit univariate GEV distributions to extreme events. It is crucial to note that identifying the marginal GEV distributions is typically required for modeling any multivariate extreme process, and therefore, in this paper, we focus on the computationally efficient modeling of univariate GEV distributions. The use of informative statistics instead of full datasets for modeling extremes has been reported in several studies, including Creel & Kristensen (2013), Creel (2017), Gerber & Nychka (2021). By including both extreme quantiles and the standard quartiles (Q1, Q2, and Q3), our NN is able to effectively capture the important tail behavior in extreme event modeling. The utilization of sample quantiles as inputs for the network is supported by the concept of order statistics.\nDuring training, we input the sample quantiles and apply activation functions to generate optimal nonlinear representations, which are then used to estimate the GEV parameters. The model outputs an estimate of the GEV parameters, which define the distribution of the extreme event. To ensure a robust model, we utilized simulated values within a reasonable parameter range for training and selected a large training size. Furthermore, we also utilized a validation set to monitor the model\u2019s performance during training.\nThe use of a NN for this problem offers the following benefits.\n(a) The NN architecture is well suited for inference, allowing for the efficient estimation of parameters. In particular, the network can be quickly evaluated once trained, resulting in significant speed gains of up to 100 - 1000 times compared to traditional methods.\n(b) To address the issue of uncertainty in our parameter estimates, we adopt the bootstrapping approach, which has been widely supported by previous research (see, for example, Cooley et al. (2007), Kysely\u0300 (2008), Huang et al. (2016), Lenzi et al. (2021), Gamet & Jalbert (2022), Yadav et al. (2022), Sainsbury-Dale et al. (2022)). In particular, to generate confidence intervals, we utilized the parametric bootstrap, which is typically computationally intensive. However, the trained NN enables efficient simulation and evaluation of bootstrap samples, resulting in the rapid generation of confidence intervals.\nFinally, our model is employed to analyze the maximum temperature extremes in the CCSM3 model\u2019s run at three distinct CO2 concentrations throughout North America. By examining annual maximum temperature data, we demonstrate the accuracy and advantages of our approach compared to the classical ML method. The fast evaluation speed of the neural estimators facilitated efficient uncertainty quantification through parametric bootstrap sampling. Our findings indicate that we can produce hundreds of spatial confidence intervals within a matter of seconds.\nThe remainder of the paper is structured as follows. Section 2 offers an overview of the GEV distribution and elucidates the proposed NN model. Section 3 showcases the outcomes of our simulation study. Section 4 delves into our CCSM3 runs case study, and lastly, Section 5 recapitulates our findings, examines the behavior and limitations of our model, and presents our conclusion."
        },
        {
            "heading": "2 Methods",
            "text": "This section provides an overview of the structure of the GEV distribution and outlines our model framework. It also includes information on the approximate statistics chosen as inputs for the network and the network architecture used in our model."
        },
        {
            "heading": "2.1 Generalized Extreme-Value Distribution",
            "text": "The GEV distribution, introduced by Jenkinson (1955), with location-scale parameters (\u00b5, \u03c3) \u2208 R \u00d7 (0,\u221e) and the shape or tail-index parameter \u03be \u2208 R. It has the cumulative distribution function (CDF) denoted as\nF(x) = exp { \u2212 [1 + \u03be(x\u2212 \u00b5)/\u03c3]\u22121/\u03be } , if \u03be 6= 0\nexp {\u2212exp [\u2212(x\u2212 \u00b5)/\u03c3]} , if \u03be = 0 The support of F is determined by the interval\nS\u03b8 = {x \u2208 R : \u03c3 + \u03be(x\u2212 \u00b5) > 0}, (1)\nwhere \u03b8 = (\u00b5, \u03c3, \u03be). Therefore, the CDF F is defined only for values of x that fall within S\u03b8. When evaluating the risk associated with extreme events in extreme value analysis (EVA), return levels are a crucial component. These levels estimate the expected values of extreme quantiles that may occur within a specific time frame, or return period, represented by T . The equation for computing return levels is given by zT = F\u22121(1\u2212 T\u22121), where F\u22121 refers to the inverse CDF of the GEV distribution, and T denotes the return period. The use of the (1 \u2212 T\u22121) quantile as the threshold is important because it represents the average frequency with which this threshold is exceeded over the specified return period.\nThe GEV distribution can take on three different forms depending on the sign of its shape parameter, \u03be. These forms are the Gumbel distribution for light-tailed distributions (\u03be = 0), the Fre\u0301chet distribution for heavy-tailed distributions (\u03be > 0), and the Weibull distribution for short-tailed distributions (\u03be < 0). The sign of \u03be also determines whether the GEV distribution is upper-bounded (\u03be > 0) or lower-bounded (\u03be < 0).\nDetermining the large-sample asymptotics of the ML estimator (MLE) for the GEV parameters is challenging due to the dependence of the support S\u03b8 on the parameters \u03b8. However, Smith (1985) and Bu\u0308cher & Segers (2016) have shown that by restricting the lower bound of \u03be > \u22121/2, the asymptotic properties can be preserved. Therefore, in our simulation study, we limit the range of \u03be > \u22121/2 to ensure the validity of the MLE results."
        },
        {
            "heading": "2.2 Approximate sufficient statistics",
            "text": "In this paper, we propose an indirect inference method for estimating the unknown parameter \u03b8 \u2208 \u0398 \u2282 Rk of the underlying GEV distribution, based on a sample y = (y1, y2, . . . , yn) and using a minimal set of lower-dimensional statistics as input to a NN. The approach is specifically designed to infer the heavy-tailed behavior of the GEV distribution. It achieves this by utilizing a set of extreme quantiles from both the lower and upper ends, including the Q1, Q2, and Q3.\nWhile the use of summary statistics in indirect inference is a useful approximation method, it may not always achieve optimal asymptotic efficiency. To address this issue, it is important to carefully select informative quantiles that accurately capture the heavy-tailed behavior of the GEV distribution while minimizing the input dimensionality of the NN. To identify the optimal set of extreme quantiles, different choices of quantiles are experimented with, and their impact on the network\u2019s behavior is observed. Selecting an informative statistic is crucial for the success of indirect inference as it can impact the computational efficiency, robustness, and interpretability of the NN. Therefore, approximately sufficient statistics are selected to ensure computational efficiency and robustness.\nThis approach is motivated by previous studies (Creel & Kristensen (2013), Jiang et al. (2017)) that have shown promising results using informative statistics in statistical modeling. Still, to the\nbest of our knowledge, this is the first study that explores the use of extreme quantiles as input to a NN for the estimation of the GEV distribution parameters. The study aims to examine the ability of a given set of quantiles to estimate the parameters of the GEV distribution using a sophisticated deep NN, with the goal of providing a more efficient and accurate method for inferring the heavy-tailed nature of the distribution. Further details about the NN framework are provided in Section 2.3."
        },
        {
            "heading": "2.3 NN Framework",
            "text": "Our estimation technique involves a deep NN that takes the quantile values as its inputs and returns an approximate \u03b8 = (\u00b5, \u03c3, \u03be) as the dependent output. Let P = P(y) \u2208 Rm be the quantile/percentile values, which is a scalar vector, m < n, where m and n are the sizes of P(y) and y respectively. The function N maps P to \u03b8 such that \u03b8\u0302 = (\u00b5\u0302, \u03c3\u0302, \u03be\u0302) is obtained as the corresponding estimate\nN : Rm \u2192 \u0398; where N (P) = \u03b8\u0302.\nThe feedforward neural network (FFNN) N has L layers, where the first layer is the input layer, the final layer is the output layer, and the remaining L\u2212 2 layers are the hidden layers. Let the jth layer of N has nj neurons, j = 1, 2, . . . , L, and is characterized by the non-linear activation function fj. The interaction between the successive layers is defined by the recursive equation\nhj = fj(bj + wjhj\u22121), j = 1, 2, . . . , L,\nwhere hj is the vector output for all neurons in the j th layer, bj is the bias vector, wj is the weight matrix connecting the (j\u2212 1)th layer to the jth layer, and hj\u22121 is the vector output of the (j\u2212 1)th layer. This equation captures the computation performed by each layer of the network, where the input to each layer is the output of the previous layer after being transformed by the weight matrix, bias vector, and activation function. Thus, N maps inputs to outputs through a sequence of non-linear transformations performed over the subsequent layers. During training, the weights and biases are adjusted to minimize the discrepancy between the predicted \u03b8\u0302 and the true \u03b8 for a given input P .\nSelecting an appropriate performance loss function to measure the discrepancy between \u03b8 and \u03b8\u0302 is crucial in building a NN algorithm. A commonly used metric for measuring the performance of a NN algorithm is the mean squared error (MSE) loss, which is computed over a selected batch of the training sample and is expressed as follows:\nMSE(\u03c9) = 1\nnB nB\u2211 i=1 \u2016\u03b8i \u2212 \u03b8\u0302i(\u03c9)\u201622 = 1 nB nB\u2211 i=1 { (\u00b5i \u2212 \u00b5\u0302i(\u03c9))2 + (\u03c3i \u2212 \u03c3\u0302i(\u03c9))2 + (\u03bei \u2212 \u03be\u0302i(\u03c9))2 } , (2)\nwhere nB is the batch size, \u03c9 denotes the matrix that encodes the network\u2019s weights and biases for a given iteration stage, and \u2016 \u00b7 \u20162 is the l2 norm. Minimizing the MSE loss during training is essential for obtaining accurate estimates of \u03b8, thereby achieving a close prediction of the true parameter value.\nHowever, in the case of estimating the parameters of the GEV distribution, it is important to ensure that the estimated parameters satisfy the support constraint of GEV denoted by 1. To achieve the support constraint, we modify the MSE loss function by adding a penalty term that accounts for any violations within the selected batch\u2019s sample ynB . We express the penalty term as C(ynB , \u03b8\u0302NN), which returns a value of 1 if any yi in ynB , i = 1, 2, . . . , nB, violates the support constraint, and 0 otherwise.\nC(ynB , \u03b8\u0302NN) =\n{ 1, if yi violates domain constraint for any i = 1, 2, ..., nB\n0, otherwise.\nTherefore, we define a regularized loss function for our problem as follows:\nLoss(\u03c9) = MSE(\u03c9) + \u03bb C(ynB , \u03b8\u0302NN(\u03c9)), (3)\nwhere \u03bb is a weight that balances the MSE and the penalty term, C(ynB , \u03b8\u0302NN(\u03c9)).\nWe optimize this loss function using a suitable optimizer ( e.g. we use RMSprop for our study), which involves iteratively updating the weights of the neural network based on the gradients of the loss function. This allows us to train N to estimate the parameters while ensuring they meet the domain constraint of the GEV."
        },
        {
            "heading": "2.4 Network Training",
            "text": "To train N , we generate a comprehensive training dataset by simulating values from the GEV distribution across a range of feasible parameters \u03b8. Specifically, we generate 340, 000 parameter configurations for training and validation, uniformly sampling over the range of \u00b5 \u2208 (1, 50),\n\u03c3 \u2208 (0.1, 40), and \u03be \u2208 (\u22120.4, 0.4), with the choices of \u00b5 and \u03c3 informed by our analysis of temperature, precipitation, and wind data. Once a parameter configuration is established, we simulate a GEV sample and further standardize the sample to improve model performance. We achieve standardization by subtracting the sample mean and scaling by the sample interquartile range (IQR), which also rescales the values of \u00b5 and \u03c3. Notably, the restricted range of \u03be > \u22121/2 aligns with the literature, as the ML estimate is valid within this range and supported by the asymptotic property established in prior research Smith (1985), Bu\u0308cher & Segers (2016).\nTo serve as the input to the NN, we select a suitable set of percentiles, P . To identify an appropriate set of percentiles, we consider a range of values spanning from the 0.01th to the 99.99th percentile over the generated GEV samples given by\nP = { 0.01th, 0.1th, 1th, 10th, 25th, 50th, 75th, 90th, 99th, 99.9th, 99.99th } , (4)\nwhere we have boldfaced the extreme percentile values for emphasis. By selecting percentiles from this wide range of values, we can capture the full spectrum of the GEV samples and better understand the heavy-tailed behavior of the distribution.\nTo train the NN, we explore two scenarios: (1) generating fixed 1000-sized GEV samples across the parameter configuration for training-validation-test, and (2) generating varying-sized GEV samples to better approximate real-world conditions.\n(1) For each parameter configuration, a GEV sample of size 1000 is simulated. To optimize the simulation process, we employed both vectorization techniques and utilized the GPU support available in Google Colab. This combination allowed us to perform the simulation for 340, 000 configurations, each with a sample size of 1000, in an efficient manner, taking approximately 12.6 minutes. To facilitate train-validation purposes, we divide the 340, 000 parameter configuration into a training set (Ntrain = 300,000) and a validation set (Nvalid = 40,000) to monitor overfitting. In addition, we defined a testing set (Ntest = 10,000) to evaluate the model\u2019s behavior over the same parameter ranges mentioned above. Working with a large training set ensures the optimization of the weights involved in the network, resulting in a reliable mapping from inputs to outputs. To prevent overfitting, we employed early stopping measures during the training process, monitoring the validation loss and learning rate. The training process was stopped if there was no improvement in the validation loss.\n(2) To ensure our study\u2019s generalizability to real-world scenarios with limited observations, we investigate how the size of GEV samples used for training affects our findings. For training and validation, we use the same Ntrain and Nvalid parameter configurations, but to generate GEV samples, we choose sizes ranging from 30 to 1000. During the training and validation phase, a total of 340, 000 parameter configurations are generated, with 68, 000 configurations assigned randomly per size for GEV sample generation. For evaluation, we create a test set by fixing \u00b5 at 0 and generating a 20 \u00d7 20 regular grid of (\u03c3, \u03be) \u2208 (0.1, 40) \u00d7 (\u22120.4, 0.4). And then generate GEV samples of sizes ranging across the different sample sizes, each configuration with 100 replications. Our findings demonstrate that increasing the sample size from small to moderate values leads to a significant improvement in estimate accuracy. Refer to Section 3 for additional details.\nNote: The input layer has shape [-, 11]; 11 quantile values as inputs, and returning the estimated GEV parameters as output with shape [-, 3].\nFor both scenarios, we maintain a consistent network architecture and utilize the loss function as described in Eq. 3 to train and optimize our model, employing the RMSprop optimizer. The optimizer is initialized with a learning rate of 0.001, allowing efficient and effective adjustments to the model\u2019s parameters during training. Table 1 provides a comprehensive overview of the architecture, including the output shape per layer and the activation functions employed.\nOur NN\u2019s output layer uses a customized activation function that returns three scalar values, corresponding to the shifted \u00b5, shifted \u03c3, and \u03be, respectively. We design this activation function by combining the tanh, relu, and tanh activation functions to handle the possible range of these parameters and ensure accurate and reliable estimates. To better model heavy-tailed distributions, we employ the RMSprop compiler for algorithm optimization (see, Hinton et al. (2012)), updating the weights for every batch of 64 samples. Early-stopping criteria based on validation MSE outlined in Eq. 2 is employed to prevent overfitting, stopping the training process when there is no improvement observed in the loss. We save the best weights obtained at the 28th epoch. To implement the model, we opt for the fixed sample scenario since it is trained using fixed 1000-GEV samples and is more efficient in terms of accuracy. The model takes 260 seconds to complete one epoch, and training for 150 epochs with early stopping at the 28th epoch takes a total of 2.11 hours."
        },
        {
            "heading": "2.5 Sample Standardization",
            "text": "In this section, we will discuss the importance of standardizing the sample before training the network. The standardization process has several benefits, including improved stability, estimation, and performance. However, our main objective here is to make the network more versatile and applicable to a wider range of extreme scenarios by making it invariant to different scales and units of measurement.\nWe center and scale the GEV sample using the sample mean and IQR. Let y = (y1, y2, y3, . . . , yn) be a sample of size n from the GEV(\u00b5, \u03c3, \u03be) distribution. The standardization is expressed as\nz = y \u2212 y\u0304 IQR ,\nwhere y\u0304 is the sample mean, IQR is the sample interquartile range, and z is the standardized GEV sample. It is crucial to understand that standardizing a GEV sample does not guarantee that the standardized sample will follow a GEV distribution. However, rescaling the sample can alter its\nlocation (\u00b5) and scale (\u03c3). The adjustment of \u00b5 is given by\n\u00b5\u2212 y\u0304 IQR\nand the adjustment of \u03c3 is \u03c3\nIQR .\nTo account for these changes, we train N with transformed percentile values and then invert the transformation to the original scale to compare with the true values.\nBy implementing standardization, a NN can become more robust and generalize better to extreme events, such as precipitation and wind, measured in different units, making it more versatile. Standardization is typically performed using pairing methods such as median with interquartile range (IQR) or mean with standard deviation. In this work, we have opted for the sample mean-IQR pairing, as it yields better outcomes than other combinations."
        },
        {
            "heading": "3 Simulation Study",
            "text": "This section presents the results of simulation studies on the precision of the neural model N using a test set of Ntest parameter configurations generated from the selected parameter range described in Section 2.4."
        },
        {
            "heading": "3.1 Comparison with ML approach",
            "text": "For comparison of the model performance, we also calculate the MLEs of the GEV parameters on the test dataset. The MLEs are computed using the parameter estimation method implemented in the R package ismev (Stephenson (2011)). In particular, the ismev package uses the optim function for numerical optimization to provide the MLEs of GEV distribution using the NelderMead optimization method (see, Singer & Nelder (2009)). The accuracy of the parameter estimates from N is presented in Figure 2, Figure 3, along with the outcomes from the ML method.\nFor our simulation study, case (1), we assess the performance of our neural model N by running Ntest = 10, 000 configurations, each with 1000-GEV samples. To compare our model\u2019s estimates with MLEs, we use boxplots to visualize the differences between the estimates and the true parameter values across a range of true parameter values. Our analysis shows that the NN estimator performed similarly to the ML approach, with fewer outliers. These findings suggest that our neural model N is a promising approach for estimating GEV parameters.\nIn case 2, we use the same network architecture and parameter configuration for training and validation as in the previous case. To evaluate the behavior of the model, we generate test sets over a parameter grid of size 20\u00d7 20 of (\u03c3, \u03be) with \u00b5 = 0. For each configuration, we replicate the test sets 100 times; this is done across each sample size. In Figure 3, we compare the mean squared error (MSE) of the estimates obtained from the NN model and the ML method for the parameters \u03c3 and \u03be. We find that increasing the sample size from 72 to 1000 results in a decrease in MSE for both the NN and ML approaches. For a sample size of 72, the NN estimates have smaller MSE than the ML estimates, but for increasing sample sizes to 416 and 1000, the MLEs have smaller\nMSE than the NN estimates. However, the difference in MSE between the two approaches is small overall across the sample sizes.\nOverall, this study provides evidence that NN models can serve as a viable alternative to the traditional ML approach for modeling extreme value."
        },
        {
            "heading": "3.2 Bootstrap",
            "text": "To account for uncertainty in parameter estimates obtained from the NN, we employ a parametric bootstrap approach (Efron & Tibshirani (1994)). We generate B = 900 bootstrap samples from the original data, fit the NN to each sample, and compute a 95% CI of the true parameters. The bootstrap method incurs no additional computation costs after the NN is trained. We can produce B bootstrap replications and derive the corresponding results from the NN within seconds.\nTo evaluate the performance of the bootstrap-based CIs, we compare them to the likelihoodbased CIs computed using the standard errors of the MLEs from the Hessian matrix of the maximum likelihood approach. We use the ismev package, as described in section 3.1, for the computation. We compute the maximum likelihood-based CIs over the test set and obtain the bootstrap-based CIs using the fixed 1000 GEV sample-trained NN over 10,000 test sets and 900 bootstrap replications.\nFigure 4 presents the ratio of the bootstrap-NN CI widths to the ML-based CI widths across the true parameter values. The boxplot summarizes the spread of this ratio, and our results indicate that the bootstrap-NN CIs are slightly wider than those of the ML model, although the observed difference is minimal. Specifically, we found that the bootstrap CIs for \u00b5 and \u03be are wider than the ML CIs, compared to \u03c3. The CI width obtained from the bootstrap method may be wider than that obtained from the ML method because the bootstrap does not assume strong distributional constraints."
        },
        {
            "heading": "3.3 Timing comparison",
            "text": "The training of the neural model N was found to be computationally efficient in comparison to other estimation methods. The model was implemented on the cloud-based Python platform, Google Colab, utilizing a computing environment with 2 virtual CPUs, 32GB of RAM, and either a P100 GPU with 16GB of memory or a T4 GPU with 16GB of GPU memory and system RAM that can be expanded up to 25GB. For comparison purposes, the MLEs were calculated using the R package ismev. The calculations were performed on a laptop with a 2.3 GHz Dual Core Intel i5 processor and 8GB of RAM. The evaluation time for the NN model on 10,000 test samples was 4 seconds, while the calculation of MLEs took approximately 10.631 minutes. Based on this, we anticipate a significant speed increase of over 150 times when scaling up to the target model output."
        },
        {
            "heading": "4 Case Study",
            "text": "Another way to validate and time out our NN estimator is to reproduce the results from a substantial climate model analysis. We analyze temperature extremes in the millennial runs of the Community Climate System Model version 3 (CCSM3), a global climate model widely used in climate research, at varying atmospheric CO2 concentrations (Collins et al. (2006)). The CCSM3 model includes complete representations of the atmosphere, land, sea ice, and ocean components, is run on grids of T31 resolution for the atmosphere and land, and approximately 1\u25e6 resolution for the ocean and sea ice (Huang et al. 2016; Yeager et al. 2006).\nWe consider a control run of 1000 years at 133 spatial locations across North America and consider three CO2 concentration scenarios: pre-industrial (289 ppm), future scenarios with 700 ppm (3.4\u25e6C increase in global mean temperature) and 1400 ppm (6.1\u25e6C increase in global mean temperature) (Huang et al. 2016). The key external forcings, including solar forcing and aerosol concentrations, are fixed at pre-industrial levels. The final simulations of 1000 years are assumed to be stationary and free from climate drift after a spin-up period. The maximum daily temperature is calculated for each grid box and year from the model output.\nWe model the 1000-year annual maximum temperatures over the spatial domain using the GEV distribution. We fit a GEV distribution to each site in the domain, assuming each site has its specific GEV distribution over the 1000-year annual maxima values. To estimate the GEV parameters for each grid box, we use N as described in previous sections. Figure 5 shows the NN estimates of CCSM3 GEV parameters for the pre-industrial period and possible changes for future cases. Our results show that the GEV distribution for the pre-industrial period is in agreement with previous findings from Huang et al. (2016). Negative shape parameters are commonly observed when modeling extreme temperatures, and our model output confirms this trend. We further display the comparison of the performance of our NN model with the ML model in estimating the GEV parameters in Figure 6.\nFinally, using the same setup as in Section 3.2, we can compute a bootstrap-based confidence interval with 900 bootstrap replicates in approximately 0.4 seconds."
        },
        {
            "heading": "5 Conclusion",
            "text": "This study highlights advances in the use of deep learning algorithms for likelihood-free inference. The results indicate that a well-trained NN can estimate the parameters of complex heavy-tailed distributions, such as the GEV, with accuracy comparable to traditional ML approaches. Although there may be more variability than MLE in the estimation of the shape parameter compared to other parameters, this is expected due to the challenging nature of estimating the shape in heavytailed GEV. Additionally, our findings demonstrate a significant increase in computational speed, with a factor of 150 improvements in model evaluation times when compared to traditional ML approaches when working with large datasets. The use of NNs allows us full control over the testing and training samples and the ability to operate on a wide range of parameters. This allows us to customize the NN to meet the specific requirements of our problem and assess its reliability.\nHowever, several limitations of the NN approach must be taken into account for its use. The selection of appropriate hyperparameters is a critical step in building a NN, as it can significantly impact the performance and accuracy of the model. Hyperparameters such as the number of hidden layers, number of neurons in the hidden layer, choice of activation function, learning rate,\nand batch size must be carefully chosen through a trial and error process. This process can prove to be time-consuming and challenging due to the vast and complex search space of hyperparameters. Additionally, the optimization of weights over each layer in the NN model can result in a large number of parameters, making the model intractable. Furthermore, the selection of the parametric range for the design of the training set is of utmost importance. The choice of informative statistics used as inputs to the network must also be carefully considered, as they must provide sufficient information about the data to allow for accurate estimates.\nIn conclusion, it is imperative to consider the network architecture, hyperparameter selection, and choice of statistics when utilizing the proposed model to ensure reliable results. Our estimation of the GEV parameters has not taken into account any potential spatial or temporal effects. This opens up the possibility for future studies to examine the integration of time-dependent structures into the GEV parameter estimation, leading to improved accuracy and robustness in extreme value predictions. Furthermore, the expanded usage of this approach in the spatial modeling of extremes can provide valuable insights into the distribution and behavior of extreme events in various geographical locations. Also, this could be beneficial in other statistical modeling approaches related to a heavy-tailed distribution."
        },
        {
            "heading": "Acknowledgements",
            "text": "We extend our sincere gratitude to Whitney Huang for generously sharing the 1000 years of output from three multimillennial runs of the CCSM3 model for our case study."
        }
    ],
    "title": "Fast parameter estimation of Generalized Extreme Value distribution using Neural Networks",
    "year": 2023
}