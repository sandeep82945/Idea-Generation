{
    "abstractText": "Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generateand-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. We execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code. We perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89% on APPS-dev, 31% on APPS-test, and 48% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. Compared to other post-processing methods, our method demonstrates superior accuracy and efficiency.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kechi Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhuo Li"
        },
        {
            "affiliations": [],
            "name": "Jia Li"
        },
        {
            "affiliations": [],
            "name": "Ge Li"
        },
        {
            "affiliations": [],
            "name": "Zhi Jin"
        }
    ],
    "id": "SP:635d30845fb1a88cb55722885b58e9b4b185c5f1",
    "references": [
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman."
            ],
            "title": "Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow",
            "venue": "If you use this software, please cite it using these metadata, 58.",
            "year": 2021
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen."
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "CoRR, abs/2207.10397.",
            "year": 2022
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Jacob Hilton",
                "Reiichiro Nakano",
                "Christopher Hesse",
                "John Schulman."
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "CoRR, abs/2110.14168.",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Fried",
                "Armen Aghajanyan",
                "Jessy Lin",
                "Sida Wang",
                "Eric Wallace",
                "Freda Shi",
                "Ruiqi Zhong",
                "Wen-tau Yih",
                "Luke Zettlemoyer",
                "Mike Lewis."
            ],
            "title": "Incoder: A generative model for code infilling and synthesis",
            "venue": "CoRR, abs/2204.05999.",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring coding challenge competence with APPS",
            "venue": "Proceedings of the Neural",
            "year": 2021
        },
        {
            "authors": [
                "Jeevana Priya Inala",
                "Chenglong Wang",
                "Mei Yang",
                "Andres Codas",
                "Mark Encarnaci\u00f3n",
                "Shuvendu K Lahiri",
                "Madanlal Musuvathi",
                "Jianfeng Gao."
            ],
            "title": "Faultaware neural code rankers",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Luke Zettlemoyer"
            ],
            "title": "Mapping language to",
            "year": 2018
        },
        {
            "authors": [
                "Jianfeng Gao"
            ],
            "title": "Interactive code generation",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyi Fu"
            ],
            "title": "2022a. Codeeditor: Learning to edit",
            "year": 2022
        },
        {
            "authors": [
                "Xing Hu"
            ],
            "title": "2023b. Skcoder: A sketch-based",
            "year": 2023
        },
        {
            "authors": [
                "Xiong"
            ],
            "title": "A conversational paradigm for program",
            "year": 2022
        },
        {
            "authors": [
                "Yusuke Oda",
                "Hiroyuki Fudaba",
                "Graham Neubig",
                "Hideaki Hata",
                "Sakriani Sakti",
                "Tomoki Toda",
                "Satoshi Nakamura."
            ],
            "title": "Learning to generate pseudo-code from source code using statistical machine translation",
            "venue": "2015 30th IEEE/ACM Interna-",
            "year": 2015
        },
        {
            "authors": [
                "Richard Yuanzhe Pang",
                "He He."
            ],
            "title": "Text generation by learning from demonstrations",
            "venue": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.",
            "year": 2021
        },
        {
            "authors": [
                "Jianhao Shen",
                "Yichun Yin",
                "Lin Li",
                "Lifeng Shang",
                "Xin Jiang",
                "Ming Zhang",
                "Qun Liu."
            ],
            "title": "Generate & rank: A multi-task framework for math word problems",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event /",
            "year": 2021
        },
        {
            "authors": [
                "Freda Shi",
                "Daniel Fried",
                "Marjan Ghazvininejad",
                "Luke Zettlemoyer",
                "Sida I. Wang."
            ],
            "title": "Natural language to code translation with execution",
            "venue": "CoRR, abs/2204.11454.",
            "year": 2022
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki."
            ],
            "title": "GPT-J6B: A 6 Billion Parameter Autoregressive Language Model",
            "venue": "https://github.com/kingoflolz/ mesh-transformer-jax.",
            "year": 2021
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System",
            "year": 2020
        },
        {
            "authors": [
                "Frank F. Xu",
                "Uri Alon",
                "Graham Neubig",
                "Vincent Josua Hellendoorn."
            ],
            "title": "A systematic evaluation of large language models of code",
            "venue": "MAPS@PLDI 2022: 6th ACM SIGPLAN International Symposium on Machine Programming, San Diego, CA, USA, 13",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Graham Neubig."
            ],
            "title": "Tranx: A transition-based neural abstract syntax parser for semantic parsing and code generation",
            "venue": "arXiv preprint arXiv:1810.02720.",
            "year": 2018
        },
        {
            "authors": [
                "Daoguang Zan",
                "Bei Chen",
                "Dejian Yang",
                "Zeqi Lin",
                "Minsu Kim",
                "Bei Guan",
                "Yongji Wang",
                "Weizhu Chen",
                "Jian-Guang Lou."
            ],
            "title": "CERT: Continual pretraining on sketches for library-oriented code generation",
            "venue": "The 2022 International Joint Conference on",
            "year": 2022
        },
        {
            "authors": [
                "Kechi Zhang",
                "Ge Li",
                "Jia Li",
                "Zhuo Li",
                "Zhi Jin."
            ],
            "title": "Toolcoder: Teach code generation models to use api search tools",
            "venue": "ArXiv, abs/2305.04032.",
            "year": 2023
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Tao Yu",
                "Tatsunori B. Hashimoto",
                "Mike Lewis",
                "Wen-tau Yih",
                "Daniel Fried",
                "Sida I. Wang."
            ],
            "title": "Coder reviewer reranking for code generation",
            "venue": "CoRR, abs/2211.16490.",
            "year": 2022
        },
        {
            "authors": [
                "Inala"
            ],
            "title": "2022) and reproduce finetuned base models",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) have recently been applied to the competitive programming task. This task requires understanding a complex natural language description of a problem with example test cases and correctly implementing solutions that can span hundreds of lines. Solutions are evaluated by executing them on hidden test cases. However, existing LLMs often have low accuracy and pass rates in this task. For example, on a popular competitive programming benchmark APPS-test (Hendrycks et al., 2021), the nearly most powerful model GPT3 (Brown et al., 2020) achieves only 7% accuracy when allowed to submit only one program per task (referred to as pass@1).\n*Corresponding authors\nTo improve the performance of LLMs on the competitive programming task, we take inspiration from the process of human programming. When solving competitive programming problems, programmers usually write an initial program, execute some example test cases, and refine the code based on the test results. In this process, a programmer can take key information (e.g, program outputs or compile/runtime error message) from the test results, which helps them debug the program. We instantiate this idea by adopting a similar pipeline with a neural-based editor (in Figure 1(a)). Analyzing the code generated by a pre-trained LLM, we have found that some of the generated codes can be improved with minor modifications. Figure 1(b) shows an example of generated code by GPT3 on the APPS-test dataset. GPT3 generates code that is inconsistent with the problem description. We notice that the error message directly points out the bug in the code, with which we can quickly fix the error. It motivates us to investigate approaches to edit and improve the quality of the code generated by LLMs with the help of execution results.\nIn this work, we propose a novel generate-andedit approach to augment LLMs on the competitive\nar X\niv :2\n30 5.\n04 08\n7v 5\n[ cs\n.S E\n] 1\n1 Se\np 20\n23\nprogramming task, named Self-Edit. To mimic the above human programmers\u2019 behavior, our approach incorporates the ability of LLMs in three steps: \u2776 Generation with LLMs. We use large language models as black-box generators and generate the program based on the problem description. \u2777 Execution. Given a generated code from LLMs, we execute it on the example test case to get the execution results. We further wrap the execution results with templates as supplementary comments to include additional helpful information for editing. \u2778 Edit. We develop a fault-aware neural code editor that takes the generated code and supplementary comment as input and refines the code. Our code editor aims to improve the quality and accuracy of code generation using LLMs.\nWe conduct extensive experiments on two public competitive programming benchmarks, including APPS (Hendrycks et al., 2021) and HumanEval (Chen et al., 2021). We apply our approach to 9 popular LLMs with parameter sizes ranging from 110M to 175B to show the universality. Compared to directly generating from LLMs, we have several findings: \u2776 Our approach significantly improves the performance of LLMs. In particular, our approach improves the average of pass@1 by 89% on APPS-dev and 31% on APPS-test. Even for the chosen largest language model GPT3-175B, our relatively small editor model can improve pass@1 from 26.6% to 32.4% on the APPS-dev benchmark. \u2777 Our approach is generalizable on a different style of dataset HumanEval, improving the average of pass@1 by 48%, showing the transfer ability on the out-of-distribution benchmark.\nRecently some approaches are also proposed to post-process programs generated by LLMs (Shi et al., 2022; Inala et al., 2022; Chen et al., 2022; Zhang et al., 2022). These approaches do largescale sampling from LLMs, rerank these sampled programs, and output the final program. In comparison, our self-edit framework has two advantages: \u2776 Our approach maintains a constant sample budget and significantly reduces the computational overhead for LLMs. \u2777 Our editor directly modifies the programs and outperforms these reranking-based methods, especially with a limited sample budget such as pass@1. To our knowledge, we are the first to adopt an editing-based post-processing method for competitive programming tasks.\nThe contributions are listed as follows:\n\u2022 We propose a generate-and-edit approach\nnamed Self-Edit for large language models (LLMs) to generate high-quality code for competitive programming tasks.\n\u2022 We develop a fault-aware neural code editor that takes the generated code and error messages as input and uses them to refine the code, improving its quality and accuracy.\n\u2022 We conduct experiments on two popular datasets and nine LLMs to demonstrate the effectiveness and universality of our approach."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Code Generation",
            "text": "Code generation is a process in which source code is automatically generated based on functional requirements such as natural language descriptions (Iyer et al., 2018; Yin and Neubig, 2018; Li et al., 2023a,b,c) or pseudo code algorithms (Kulal et al., 2019; Oda et al., 2015) or a old version of code (Li et al., 2022a) or a response from programming tools (Zhang et al., 2023). One particularly challenging type of code generation task is competitive programming (Li et al., 2022c), in which models must solve problems at the level of programming competitions. This task often involves natural language descriptions and example input-output pairs. The performance of a code generation model on competitive programming tasks can serve as a measure of its ability to create complete solutions to problems. In recent years, large pre-trained language models such as AlphaCode (Li et al., 2022c) and the GPT3 (Brown et al., 2020) series have demonstrated impressive capabilities in code generation and competitive programming. Other open-source code generation models include GPT-Neo (Black et al., 2021), GPT-J (Wang and Komatsuzaki, 2021), CodeParrot (Wolf et al., 2020), PolyCoder (Xu et al., 2022), CodeGen (Nijkamp et al., 2022) and InCoder (Fried et al., 2022). We utilize the text-davinci-002 API from OpenAI and various competitive code generation models in this work."
        },
        {
            "heading": "2.2 Post-processing of LLMs for code generation",
            "text": "To find the correct code solutions based on LLMs, researchers adopt various post-processing methods to filter/rerank the original outputs from LLMs. In the domain of solving math problems, Cobbe et al. (2021) and Shen et al. (2021) chose the one\nLLM Output \u2026 print n + k \u2026\nSupplementary Comment Line 3, print n + k SyntaxError: Missing parentheses in call to \u2018print\u2019. Did you mean print(n + k)? Fix the bug.\nProblem Description \u2026find the smallest integer x greater than n, so it is divisible by the number k\u2026 -----Examples----Input 5 3 Output 6\n...\nexample test case\nEditor Output \u2026 print (n + k) \u2026\n\u662f\u5426\u8981\u589e\u52a0example testcase\u548chidden test case\u7684\u8868\u793a\u56fe\n1. LLM for generating programs\n2. Executor for running example test case\n3. Fault-aware Code Editor\nFigure 2: Pipeline of our self-edit approach.\nwith the highest rank by a trained ranker. Similar ranking methods are also used in the field of cross-domain adaptation (Li et al., 2022b). In the domain of code generation, post-processing techniques are also often used (Lahiri et al., 2022; Le et al., 2022). AlphaCode (Li et al., 2022c) and Shi et al. (2022) adopted the clustering and filtering methods based on the execution output of the generated programs. Inala et al. (2022) trained a fault-aware neural ranker to rerank the outputs with a large sample budget. Chen et al. (2022) use the large models to generate test cases for themselves and automatically rank the solutions based on the test-driven dual execution agreement. Zhang et al. (2022) reranked the LLM outputs with the generation probability of back translation.\nHowever, these existing methods require largescale sampling. They need to generate a large number of programs for post-processing. For example, AlphaCode (Li et al., 2022c) needs 1 million samples per problem, costing 105 TPU-seconds. In the real world, computing resources are precious and limited, and existing methods are ineffective in practical applications. Our self-edit approach addresses this issue by maintaining a constant sample budget and improving computational efficiency, described in Section 4.3."
        },
        {
            "heading": "3 Methodology",
            "text": "We provide an overview of the self-edit pipeline in Figure 2. Given the problem description, We first generate the initial code with LLM. Then we execute the example test case to obtain test results and construct the supplementary comment. Finally, we\n0.0 0.1 0.2 0.3 0.4 0.5\nRecursionError IndentationError\nTypeError IndexError ValueError\nPass the test case EOFError\nNameError Wrong answers\nSyntaxError\n(a) PyCodeGPT-110M-finetuned\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nRecursionError EOFError\nTime limit exceeded TypeError\nValueError IndexError\nSyntaxError NameError\nPass the test case Wrong answers\n(b) GPT3\nFigure 3: Distribution of the top 10 classes of supplementary comments in the APPS-train dataset when using the PyCodeGPT-110M-finetuned and GPT3 models, expressed as a percentage of the total number of generated programs for each class.\ntrain a fault-aware code editor model to refine the code based on the problem description, generated code, and supplementary comment."
        },
        {
            "heading": "3.1 LLMs as Black-box Generator",
            "text": "We use large language models as black-box generators with fixed parameters in our design. This design choice is motivated by the fact that training LLMs is costly, and access to LLMs is often restricted. (E.g., OpenAI only offers paid API to infer GPT3.) Using LLM as a black-box generator makes our approach flexible for using different LLMs. We investigate nine LLMs for code generation with sizes ranging from 110M to 175B. A detailed comparison is described in Table 2."
        },
        {
            "heading": "3.2 Executor and Supplementary Comments",
            "text": "After we generate the code using LLMs, we use an executor to run the example test case. We classify the execution results into three types: \u2776 Passed: The program passes the test case. \u2777 Wrong Answer: The program runs normally but gives incorrect outputs. \u2778 Error: The program terminates abnormally due to syntax error, runtime exceptions, or exceeding time limit.\nWe analyze the distribution of test results on APPS-train dataset for code generated by a relatively small model PyCodeGPT-110M and a large model GPT3-175B as shown in Figure 3. We observe that programs produced by different models yield different test result distributions. Code generated by smaller models (PyCodeGPT) tends to encounter SyntaxError issues more frequently, while large models (GPT3) show fewer SyntaxErrors, fewer RuntimeErrors, but more normally executed cases.\nIn order to construct meaningful supplementary comments for the code editor model to understand\nvarious execution results, we design the comment templates (Fig. 4) for the three types of test results. The comment template can wrap potential error messages with additional helpful information for editing. \u2776 For the code passing the example test case, we use Comment 1: \u201cPass the example test case.\u201d. \u2777 For the code producing incorrect outputs, we use Comment 2 to include the relevant input, expected output, and the actual output. We also append the instruction \u201cRewrite the code\u201d to guide the editor model to reimplement the algorithm to produce correct outputs. \u2778 For the code that terminates with errors, we use Comment 3 to include the error line number, line context, and full error message. These supplementary comments provide additional context and clarity for the generated code and are used to guide editing the code."
        },
        {
            "heading": "3.3 Fault-aware Code Editor",
            "text": "Once we have constructed the supplementary comments, we train a fault-aware editor that takes the natural language description, generated code, and supplementary comments as input and produces higher-quality refined code."
        },
        {
            "heading": "3.3.1 Code Editor Models",
            "text": "The fault-aware code edit task is formally defined as a sequence-to-sequence task: given a natural language description N , a program generated by LLM S, and accompanied supplementary comments C (Sec. 3.2), the model is required to generate higher-quality code C\u0302 that implements the natural language description and passes test cases. In our experiments, the input pair (N,S,C) is segmented into three parts and concatenated using special separator tokens, represented as [SOS], n1, n2, . . . , n|N |, [CODE], s1, . . . , s|S|, , [CMNT ], c1, . . . , c|C|, [EOS], where the lower-\ncase letters represent the token of the corresponding content in the input pair (N,S,C). We train a decoder-only model to complete the code edit task. Concretely, we implement the code editor by fine-tuning PyCodeGPT-110M on this task.\nAt inference time, we first generate multiple programs from LLMs using natural language description as input. For each generated program, we feed the example test case provided in the description into the executor to obtain a fault-aware comment. We then use the editor to generate a new program, which is the final version for further evaluation. This inference approach maintains a small sample budget compared with existing large-scale sampling and filter/reranking methods."
        },
        {
            "heading": "3.3.2 Dataset Construction for Code Editor",
            "text": "To train a fault-aware code editor, we need datasets that contain the generated program and the corresponding supplementary comments. To collect such datasets, we use different LLMs (Sec. 4.1) to generate candidate programs for problems in the APPS-train dataset. For each problem, we sample 10 programs from the LLM and then execute the example test case to get the test results and construct supplementary comments. At this point, we get the datasets of triplets (N,S,C) for different LLMs. To further obtain the ground truth program C\u0302, we collect the standard ground truth programs in the original APPS training dataset and the generated programs that pass all hidden test cases. For each LLM, we create an individual editor dataset with nearly 4.5k generated programs with comments. For each generated program, we set at most 15 ground truth programs. As we described in Figure 3, the generated programs from different LLMs have different distributions of the corresponding comments. To optimize the performance of the fault-aware code editor for each LLM, it is necessary to use training datasets specific to the corresponding LLM."
        },
        {
            "heading": "3.3.3 Training Objective of Code Editor",
            "text": "Editing for a high-quality program based on the input pair (N,S,C) is a one-of-many task because multiple correct target programs satisfy the requirements. Standard maximum likelihood objectives aim to minimize loss by considering all of the solutions in the training set (like recall), while we focus on a model\u2019s ability to edit a single correct solution based on the existing generated code within a limited budget of attempts (like precision). To\naddress this discrepancy, we follow previous work and adopt a variation of GOLD (Pang and He, 2021; Li et al., 2022c), which incorporates an off-policy importance weight into the standard maximum likelihood objective gradient:\n\u2207L(\u03b8) = \u2212 \u2211 t\u2208C\u0302 P\u03b8(t)\u2207logP\u03b8(t) (1)\nwhere \u03b8 represents the model parameters and logP\u03b8(t) is the standard log-likelihood objective for next token prediction. The additional weight P\u03b8(t) allows the model to focus on the tokens that already have a high likelihood, so the model can concentrate on these easier-to-learn ground truth solutions and increase the chance of getting at least one correct output. Such a loss setting allows editors to learn to copy part of the content from existing generated programs to obtain better outputs."
        },
        {
            "heading": "4 Experiment",
            "text": "We present extensive experiments that span two representative datasets and nine different LLMs for code generation, whose parameter counts range across four orders of magnitude. The details of the adopted LLMs are described in Section 3.1. We aim to investigate four research questions: (1) how much can fault-aware code editors improve various code generation models on competitive programming (Sec. 4.2), (2) the advantages of editor-based methods over existing ranking methods (Sec. 4.3), (3) to what extent does the supplementary comments help to refine the program (Sec. 4.4), (4) how does the number of editing rounds affect the final result (Sec. 4.5)."
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "Dataset. We consider evaluating our approach on two existing code generation datasets: (1) APPS (Hendrycks et al., 2021): a collection of 5000 training and 5000 test tasks collected from coding competitions and interview problems. The test set has three different difficulty levels: Introductory, Interview, and Competition. (2) HumanEval (Chen et al., 2021): a set of 164 test programming problems with a function signature, docstring, body, and several unit tests. Our experiments only use the APPS-train dataset to finetune the code generation models and the code editor models since it is the largest training dataset. Following previous studies (Inala et al., 2022), we adopted the same division and used a set of 598 tasks excluded from the\nAPPS training dataset for validation1. The detailed statistic of the datasets is shown in Table 1. The hidden test cases are those test cases for evaluation. They are not included in the problem description, so they are distinguished from the example test case used to obtain supplementary comments. Base LLMs. In this paper, we investigate the effectiveness of several widely used language models for code generation, including text-davinci-002 (175B) (Brown et al., 2020), CodeGen (2B, 350M) (Nijkamp et al., 2022), InCoder (1B) (Fried et al., 2022), GPT-Neo (1.3B, 125M) (Black et al., 2021), GPT-J (6B) (Wang and Komatsuzaki, 2021) and PycodeGPT (110M) (Zan et al., 2022). These models are evaluated under zero-shot or finetune experimental conditions, with additional descriptions provided as a part of Table 2. 2 Editor Model. We implement the code editor by fine-tuning PyCodeGPT-110M. We choose this model because of its relatively small parameter size and high performance. We also tried the CodeGen350M model in early experiments but found that the training speed and final performance were not as good as the model we chose.\nConsidering that LLMs shows strong in-context learning abilities that do not need training process, we also explore to design a variant of our self-edit method with in-context learning. We use the textdavinci-002 as both base model and editor model. The in-context learning self-edit performances are discussed in Section 5.2. Metrics. We use the metric pass rate pass@k for performance evaluation and take advantage of hidden test cases to determine the functional correctness of code solutions. For each problem, we submit k code solutions for evaluation. If any of the\n1https://github.com/microsoft/CodeRanker 2We do not use the CodeX model as it was in closed beta and was not available during our experiments. We choose text-davinci-002 with equal parameter size as an alternative.\nk code solutions passes all ground truth test cases, the problem is considered solved. Then pass@k is the percentage of solved problems. In our experiments, we set k = {1, 5, 10}.\nTo show the number of programs corrected by our editor, we design a new metric sol@k, which means the total number of correct programs given k samples per problem. For example, for the 5000 problems in APPS-test, we will generate 5000 \u2217 k code solutions, from which we will count the number of correct solutions as sol@k. In our experiments, we set k = 10. We show the performance of the base model and the performance after editing (denoted as edit-pass@k and edit-sol@k). Training/Inference Settings. For each finetuned LLM, we limit the maximum epochs to 10 with a learning rate of 1e-5, and choose the best checkpoint based on the validation loss on APPS-dev. We adopt the same training strategy to train faultaware code editors on each corresponding editor dataset. We set the maximum input length to 1024 and output length to 512 for our editors. To extract the supplementary comment, we choose only one example test case contained in the problem description even if it contains multiple. At inference time, we use temperature sampling with T = 0.8 both for LLM and editor outputs. We limit the sample budget of LLMs to 10. For each LLM output code, we only generate one code as the final version with our editor. Thus the usage of the editor maintains a constant sample budget. All experiments are conducted with 4 Tesla V100-32GB GPUs."
        },
        {
            "heading": "4.2 Comparison with Base LLMs",
            "text": "APPS-dev & APPS-test. We first compare with directly generating from LLMs to analyze how faultaware code editors can improve nine popular code generation models. Table 2 shows the primary results on the APPS-dev dataset for nine different code generation models. The fault-aware editor improves all code generation models despite their different sizes and training settings. The average pass@1 value across nine models increases from 6.17% to 11.67%, representing an impressive 89% improvement. For those LLMs with a particularly large number of parameters, our editor can also achieve a significant improvement. For GPT3 with 175B parameters, the improvement of our editor also achieves 5.9%, 5.0%, 8.4% on pass@{1,5,10}.\nResults on the APPS-test dataset are shown in Table 3. The test problems are more challenging\nthan APPS-dev, which we can see by the smaller pass@k numbers. Our editors maintain significant improvement for models of different sizes. The absolute improvement of pass@1 covers from 0.12% to 0.7%, showing that the editor can solve 6 to 35 more problems on this challenging benchmark. As for sol@10, our editors can additionally correct hundreds of generated codes from LLMs.\nIn some cases, we observe that the edit-pass@1 outperforms the pass@5. It demonstrates that editing the candidate code is very sample efficient. With the editor model, the number of required programs sampled from the LLM can be reduced.\nAnother interesting observation is that a smaller LLM equipped with our editor can achieve comparable performance as the super large models. For example, the GPT-Neo-125M, GPT-Neo-1.3B, and GPT-J are pretrained and finetuned with the same dataset. Using the editor can fill in the gaps in the parameter sizes of this series of models. The 125M pretrained model with a 110M editor can significantly outperform a 1.3B pretrained model and even outperform the 6B pretrained model in some cases. This finding can also be observed in other experiments, showing that our editor can offer a boost approximately equivalent to a tens of times pretrained model size increase.\nOn Different Difficulty-Level Problems. Considering that the APPS-test dataset has three difficulty levels, we further analyze the improvement on problems of different difficulty in Table 5. We choose GPT-J-6B-finetuned as the base model because it has shown promising results on this challenging benchmark and has certain representativeness. The editor can improve the base model on problems of all difficulty levels but has a relatively high pass rate improvement on simple \"Introductory\" problems. We find that the output of LLMs is poor on very difficult problems, making it too difficult for the editor to correct these solutions. Even so, our method slightly improves the \"Competition\" problems when enlarging the sample budget from 1 to 10.\nHumanEval. We also measure the transfer ability of our editor on HumanEval, a dataset of different styles, in Table 4. The HumanEval dataset requires the model to give the function body based on the function signature, comments, and example test cases. Following the executability filter in previous work (Zhang et al., 2022), in this dataset, we only edit the outputs that can not pass the example test\ncase. We also modify the input format to be similar to the format in the APPS dataset. We select several representative LLMs for evaluation within our computational capabilities. We can again see that the editor improves the performance of all code generation models on all metrics. We notice that under larger sample budget conditions, even if the pass@10 does not increase for CodeGen-2B, our editor can still correct more generated solutions. Thus the sol@10 increases significantly. These results demonstrate the ability and generality of our editor to correct out-of-distribution output codes."
        },
        {
            "heading": "4.3 Comparison with Post-processing Baseline",
            "text": "This experiment compares our self-edit approach with existing post-processing methods for code generation. We choose to compare with CodeRanker (Inala et al., 2022), a state-of-the-art reranking method on the APPS dataset. CodeRanker finetuned CodeBERT (125M) to classify the potential\nerror type and use this classification prediction to rerank the generated codes from LLMs. The supervised training task makes this method more efficient than previous filtering and reranking methods. However, our experiments (Table 6) prove that our editor outperforms this state-of-the-art method in terms of accuracy and efficiency.\nWe choose the GPT-Neo-1.3B-finetuned as the base model and finetune on the APPS-train dataset, keeping the same experimental settings as CodeRanker for a fair comparison. Our method (\"+ editor\") significantly outperforms CodeRanker (\"+ ranker\"). In particular, on APPS-test, our method can improve pass@1 from 0.14% to 0.68%, while their method can only improve from 0.14% to 0.3%. It means our method can solve 19 more problems on this challenging dataset. We also provide the performance of other reproduced base models in Table 9, where our method generally outperforms.\nMore importantly, existing post-processing\nmethods rely on sampling many outputs from LLMs. For instance, the CodeRanker requires 100 outputs for each problem and then selects k samples with their ranker model to evaluate pass@k metric. In contrast, our method only requires k = {1, 5} outputs per problem and then utilizes these outputs to generate a final solution through editing. Our approach is more efficient and effective, especially when obtaining outputs from large language models is costly. As a result, our method has greater practical significance and is more suitable for use with limited sample budgets."
        },
        {
            "heading": "4.4 Ablation on Supplementary Comments",
            "text": "To investigate the influence of supplementary comments, we remove the supplementary comments from the editor input and only use problem description and generated code to train a new editor. Other settings are kept the same. Results on APPS validation and test datasets are shown in Table 7.\nWe find that the pass rate of the modified editor decreases significantly on both datasets compared with the original editor. The modified editor can im-\nprove the APPS-dev dataset compared to the base model. However, on the more difficult APPS-test dataset, the editor model without comments shows no performance improvement. The results indicate that losing the guidance of the supplementary comment will hurt the performance of the editor model. Our experiments show that using error messages as supplementary comments for the code editor is crucial for achieving remarkable performances."
        },
        {
            "heading": "4.5 Ablation on the Number of Edit Rounds",
            "text": "In our self-edit approach, we make edits to the output of LLMs to produce the final program. It\nleads to a question: what if we make additional edits to the program after the first edit? We add an additional editing step to answer this question using our original editor. Concretely, the edited program is executed on an example test case to obtain comments and then refined by the editor model again. The results of this approach are presented in Table 7, with the column labeled \"+ edit round\" indicating the two-round editing approach.\nThe results show the two-round editing leads to a slight increase in pass@1 on APPS-dev. However, the additional edit round hurts the performance on APPS-test. We guess the reason is the gap between training and test time in the second editing round. The editor is trained to edit LLM outputs but used to edit its own output in the second edit round. In this setting, an additional editing round is not very helpful in generating better programs."
        },
        {
            "heading": "5 Discussion",
            "text": ""
        },
        {
            "heading": "5.1 Time Cost compared with Post-processing Baseline",
            "text": "For the specific issue of time cost, we use Google Colab 3 with a Tesla T4 GPU to build a demo and conduct evaluations over APPS-test dataset. We use text-davinci-002 as the base model and the average time cost is nearly 8.4s to obtain 1 sample for each question. The executor costs <0.01s, and our editor costs 3.7s to get the final output, which is acceptable in our actual experience using the demo. By contrast, the state-of-the-art reranking method CodeRanker requires >110s to obtain candidate lists and 0.53s for the following ranker. As a result, our framework achieves better performance with less total time cost and fewer LLM calls."
        },
        {
            "heading": "5.2 Performances of In-Context Learning Self-Edit",
            "text": "Given that LLMs have demonstrated strong incontext learning abilities without requiring any specific training, we leverage the capabilities of the text-davinci-002 model as both the base and editor models to develop a variant of our self-edit method that utilizes in-context learning. Specifically, we utilize in-context learning abilities of the model to self-edit its output using the supplementary comments we construct (detailed in Section 3.2) as input prompts for zero-shot inference. This approach allows the large model to edit its output program\n3https://colab.research.google.com\nwithout additional training, offering a promising solution for optimizing the potential of LLMs.\nOur experiments on APPS-test and HumanEval are presented in Table 8. Results demonstrate that our self-edit framework can be extended using incontext learning, achieving significantly better performance than smaller editors across various benchmarks. However, it is important to note that this in-context learning self-edit method still incurs a relatively large number of LLM calls. Therefore, optimizing resource requirements while exploiting the potential of LLMs remains critical. To this end, we will explore strategies to efficiently utilize the in-context learning capabilities of LLMs in our self-edit framework in future work."
        },
        {
            "heading": "6 Conclusion",
            "text": "We propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. The central component of our approach is the fault-aware code editor, which can edit and optimize the generated code. In-depth evaluations demonstrate our approach significantly improves the quality of LLMs\u2019 output code."
        },
        {
            "heading": "7 Acknowledgement",
            "text": "This research is supported by the National Natural Science Foundation of China under Grant Nos. 62072007, 62192731, 62192733, 62192730, 61832009. The AI training platform supporting this work were provided by High-Flyer AI. (Hangzhou High-Flyer AI Fundamental Research Co., Ltd.) We also would like to thank all the anonymous reviewers for constructive comments and suggestions to this paper.\nLimitations\nOur work has several limitations, which we aim to address in our future work:\nFirstly, we implement our editor with relatively small pretrained models within our computational capabilities. Our in-depth evaluations have preliminarily demostrated the effectiveness of the generateand-edit approach. We hope to further understand the performance when using different pretrained models and architectures for the editor.\nSecondly, the editor datasets we constructed are relatively small due to our computational capabilities. In our experiment, we only sample 10 programs from the LLM for each problem for dataset construction. Compared with existing post-editing methods, the dataset we use is quite small. It would be meaningful to do a detailed analysis of the impact of editor dataset size, or to experiment with other dataset construction methods. We leave this as future work.\nThirdly, We do not have strict comparison about computing resources with other post-editing methods. In Section 4.3 we compare with a state-of-theart re-reaking baseline. We both use an additional model with a similar amount of parameters, but our approach outperforms using very few samples from LLMs. As accessing LLMs is costing, our approach demonstrates both superior accuracy and efficiency.\nFinally, in our ablation study on the number of edit rounds, we faced with a gap between training and test time in the second editing round. Our existing implementation is not designed for this multiple-round editor. We hope to further try new specially designed model to implement the editor model. As large language models continue to advance, the need for effective strategies to interact with LLMs will be an important area of future research."
        },
        {
            "heading": "A Compared with CodeRanker",
            "text": "We compare with CodeRanker (Inala et al., 2022) using GPT-Neo-125M-finetuned, GPT-Neo-1.3Bfinetuned and GPT-J-6B-finetuned as the base model. For fair comparison, we choose the same base model, training dataset and test benchmark as the CodeRanker. We choose the above three base models and finetune on the APPS-train dataset to reproduce their results. The purpose of this step is to make our base model results similar to their reported base model results, so as to fairly compare the post-processing performance. In the experiments, the base model performance in our results is similar to the base model reported by CodeRanker. Full details of results are shown in Table 9. With a very small number of samples output by LLMs, our method significantly exceeds this state-of-the-art baseline."
        },
        {
            "heading": "B Qualitative analysis of Code Editor",
            "text": "In Figure 5 and 6 we show various programs generated by the GPT3, its corresponding problem description (contains example test case) and the supplementary comment. Our fault-aware code editor concatenates these as input, and generate the edited code as the final output. We find that the edited code is simialr to the GPT3 output. In particular, the first few lines of the edited output are exactly the same as the output of GPT3, and the subsequent code is also partially based on the content in GPT3 output. Through statistical analysis, we find that the common prefix between the two sequences accounted for 19.10% of the edited output on the APPS-dev and APPS-test datasets. While this does not account for similarities in the intermediate content, it is sufficient evidence to demonstrate the impact of the LLM output on the edited code. As for the HumanEval benchmark, we also show case studies in Figure 7."
        }
    ],
    "title": "Self-Edit: Fault-Aware Code Editor for Code Generation",
    "year": 2023
}