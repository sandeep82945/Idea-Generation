{
    "abstractText": "While promising performance for speaker verification has been achieved by deep speaker embeddings, the advantage would reduce in the case of speaking-style variability. Speaking rate mismatch is often observed in practical speaker verification systems, which may actually degrade the system performance. To reduce intra-class discrepancy caused by speaking rate, we propose a deep representation decomposition approach with adversarial learning to learn speaking rate-invariant speaker embeddings. Specifically, adopting an attention block, we decompose the original embedding into identity-related component and rate-related component through multi-task training. Additionally, to reduce the latent relationship between the two decomposed components, we further propose a cosine mapping block to train the parameters adversarially to minimize the cosine similarity between the two decomposed components. As a result, identity-related features become robust to speaking rate and then are used for verification. Experiments are conducted on VoxCeleb1 data and HI-MIA data to demonstrate the effectiveness of our proposed approach.",
    "authors": [
        {
            "affiliations": [],
            "name": "Fuchuan Tong"
        },
        {
            "affiliations": [],
            "name": "Siqi Zheng"
        },
        {
            "affiliations": [],
            "name": "Haodong Zhou"
        },
        {
            "affiliations": [],
            "name": "Xingjia Xie"
        },
        {
            "affiliations": [],
            "name": "Qingyang Hong"
        },
        {
            "affiliations": [],
            "name": "Lin Li"
        }
    ],
    "id": "SP:7c23624e55ca3b8ff38d8e99d48c56579ebef3fc",
    "references": [
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "X-vectors: Robust dnn embeddings for speaker recognition",
            "venue": "Proc. ICASSP. IEEE, 2018, pp. 5329\u20135333.",
            "year": 2018
        },
        {
            "authors": [
                "T. Zhou",
                "Y. Zhao",
                "J. Wu"
            ],
            "title": "ResNeXt and Res2Net structure for speaker verification",
            "venue": "arXiv preprint arXiv:2007.02480, 2020.",
            "year": 2007
        },
        {
            "authors": [
                "B. Desplanques",
                "J. Thienpondt",
                "K. Demuynck"
            ],
            "title": "ECAPA- TDNN: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification",
            "venue": "Proc. INTERSPEECH, 2020, pp. 1\u20135.",
            "year": 2020
        },
        {
            "authors": [
                "R. Peri",
                "M. Pal",
                "A. Jati",
                "K. Somandepalli",
                "S. Narayanan"
            ],
            "title": "Robust speaker recognition using unsupervised adversarial invariance",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 6614\u20136618.",
            "year": 2020
        },
        {
            "authors": [
                "J. Tai",
                "X. Jia",
                "Q. Huang",
                "W. Zhang",
                "H. Du",
                "S. Zhang"
            ],
            "title": "SEEF- ALDR: A speaker embedding enhancement framework via adversarial learning based disentangled representation",
            "venue": "Annual Computer Security Applications Conference, 2020, pp. 939\u2013950.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kwon",
                "S.-W. Chung",
                "H.-G. Kang"
            ],
            "title": "Intra-class variation reduction of speaker representation in disentanglement framework",
            "venue": "Proc. INTERSPEECH, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Zeng",
                "S. Yin",
                "D. Wang"
            ],
            "title": "Learning speech rate in speech recognition",
            "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "M. Grimaldi",
                "F. Cummins"
            ],
            "title": "Speech style and speaker recognition: A case study",
            "venue": "Tenth Annual Conference of the International Speech Communication Association, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "C.J. van Heerden",
                "E. Barnard"
            ],
            "title": "Speech rate normalization used to improve speaker verification",
            "venue": "Proceedings of the Symposium of the Pattern Recognition Association of South Africa. Citeseer, 2007, pp. 2\u20137.",
            "year": 2007
        },
        {
            "authors": [
                "A. Rozi",
                "L. Li",
                "D. Wang",
                "T.F. Zheng"
            ],
            "title": "Feature transformation for speaker verification under speaking rate mismatch condition",
            "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, 2016, pp. 1\u20134.",
            "year": 2016
        },
        {
            "authors": [
                "K.A. Lee",
                "H. Yamamoto",
                "K. Okabe",
                "Q. Wang",
                "L. Guo",
                "T. Koshinaka",
                "J. Zhang",
                "K. Shinoda"
            ],
            "title": "NEC-TT system for mixedbandwidth and multi-domain speaker recognition",
            "venue": "Computer Speech & Language, vol. 61, p. 101033, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Ma",
                "H. Yu",
                "W. Chen",
                "J. Guo"
            ],
            "title": "Short utterance based speech language identification in intelligent vehicles with timescale modifications and deep bottleneck features",
            "venue": "IEEE transactions on vehicular technology, vol. 68, no. 1, pp. 121\u2013128, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Roberts",
                "K.K. Paliwal"
            ],
            "title": "A time-scale modification dataset with subjective quality labels",
            "venue": "2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Ko",
                "V. Peddinti",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "Audio augmentation for speech recognition",
            "venue": "Sixteenth Annual Conference of the International Speech Communication Association, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "J. Villalba",
                "N. Br\u00fcmmer",
                "N. Dehak"
            ],
            "title": "Tied variational autoencoder backends for i-vector speaker recognition",
            "venue": "Proc. INTER- SPEECH, 2017, pp. 1004\u20131008.",
            "year": 2017
        },
        {
            "authors": [
                "W.H. Kang",
                "N.S. Kim"
            ],
            "title": "Adversarially learned total variability embedding for speaker recognition with random digit strings",
            "venue": "Sensors, vol. 19, no. 21, p. 4709, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Meng",
                "Y. Zhao",
                "J. Li",
                "Y. Gong"
            ],
            "title": "Adversarial speaker verification",
            "venue": "Proc. ICASSP. IEEE, 2019, pp. 6216\u20136220.",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhou",
                "T. Jiang",
                "L. Li",
                "Q. Hong",
                "Z. Wang",
                "B. Xia"
            ],
            "title": "Training multi-task adversarial network for extracting noise-robust speaker embedding",
            "venue": "Proc. ICASSP. IEEE, 2019, pp. 6196\u20136200.",
            "year": 2019
        },
        {
            "authors": [
                "Q. Wang",
                "W. Rao",
                "S. Sun",
                "L. Xie",
                "E.S. Chng",
                "H. Li"
            ],
            "title": "Unsupervised domain adaptation via domain adversarial training for speaker recognition",
            "venue": "Proc. ICASSP. IEEE, 2018, pp. 4889\u2013 4893.",
            "year": 2018
        },
        {
            "authors": [
                "Z. Chen",
                "S. Wang",
                "Y. Qian",
                "K. Yu"
            ],
            "title": "Channel invariant speaker embedding learning with joint multi-task and adversarial training",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 6574\u20136578.",
            "year": 2020
        },
        {
            "authors": [
                "C. Luu",
                "P. Bell",
                "S. Renals"
            ],
            "title": "Channel adversarial training for speaker verification and diarization",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 7094\u20137098.",
            "year": 2020
        },
        {
            "authors": [
                "K. Liu",
                "H. Zhou"
            ],
            "title": "Text-independent speaker verification with adversarial learning on short utterances",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 6569\u20136573.",
            "year": 2020
        },
        {
            "authors": [
                "J. Hu",
                "L. Shen",
                "G. Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132\u20137141.",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhou",
                "T. Jiang",
                "Z. Li",
                "L. Li",
                "Q. Hong"
            ],
            "title": "Deep speaker embedding extraction with channel-wise feature responses and additive supervision softmax loss function.",
            "venue": "in Proc. INTERSPEECH,",
            "year": 2019
        },
        {
            "authors": [
                "H. Wang",
                "D. Gong",
                "Z. Li",
                "W. Liu"
            ],
            "title": "Decorrelated adversarial learning for age-invariant face recognition",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3527\u20133536.",
            "year": 2019
        },
        {
            "authors": [
                "A. Nagrani",
                "J.S. Chung",
                "W. Xie",
                "A. Zisserman"
            ],
            "title": "Voxceleb: Large-scale speaker verification in the wild",
            "venue": "Computer Speech & Language, vol. 60, p. 101027, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Qin",
                "H. Bu",
                "M. Li"
            ],
            "title": "HI-MIA: A far-field text-dependent speaker verification database and the baselines",
            "venue": "Proc. ICASSP. IEEE, 2020, pp. 7609\u20137613.",
            "year": 2020
        },
        {
            "authors": [
                "S. Tomar"
            ],
            "title": "Converting video formats with ffmpeg",
            "venue": "Linux Journal, vol. 2006, no. 146, p. 10, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "D. Snyder",
                "D. Garcia-Romero",
                "G. Sell",
                "A. McCree",
                "D. Povey",
                "S. Khudanpur"
            ],
            "title": "Speaker recognition for multi-speaker conversations using x-vectors",
            "venue": "Proc. ICASSP. IEEE, 2019, pp. 5796\u2013 5800.",
            "year": 2019
        },
        {
            "authors": [
                "F. Tong",
                "M. Zhao",
                "J. Zhou",
                "H. Lu",
                "Z. Li",
                "L. Li",
                "Q. Hong"
            ],
            "title": "ASV-Subtools: Open source toolkit for automatic speaker verification",
            "venue": "Proc. ICASSP. IEEE, 2021, pp. 6184\u20136188.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Speaker verification is a typical biometric authentication technology that verifies the identities of speakers from their voices. Recently, deep learning based speaker verification algorithms have achieved excellent performance. The main task of deep embedding based speaker recognition system involves extracting a high dimensional embedding from an utterance that characterizes each speaker uniquely. Recently, deep speaker embeddings (x-vectors) extracted from time delay neural network (TDNN) [1] or other encoders (e. g., Res2Net [2], ECAPATDNN [3]) have become state-of-the-art for speaker verification. In terms of \u2018in-the-wild\u2019 environments and speakingstyle variability, however, the embeddings will include speakerunrelated information, which will significantly degrade their performance. Therefore, improving the robustness of speaker embeddings has become a crucial research issue for the \u2018in-thewild\u2019 speaker verification task.\nMost recently, a few works were proposed to eliminate the impact of identity-unrelated information. For example, Peri et al. [4] adopted an adversarial invariance architecture to train a network and extracted robust speaker-discriminative representations. Tai et al. [5] proposed the SEEF-ALDR framework to\n\u2020The work was done during an internship at Alibaba. This research is funded by the National Natural Science Foundation of China (Grant No. 61876160 and No. 62001405) and Fundamental Research Funds for the Central Universities (No. 20720210087).\nminimize the impact of identity-unrelated information via adversarial learning based disentangled representation. Similarly, Kwon et al. [6] improved the SEEF-ALDR framework and disentangled identity-related and identity-unrelated information using a mutual information criterion through an auto-encoder framework.\nIn this work, we focus on the speaking rate mismatch problem. This problem occurs when a speaker enrolled in a normal speaking rate but tested with a slower or faster speech utterance. Speaking rate mismatch is often observed in practical speaker verification systems. For example, people tend to speak faster when they are in a hurry, while speaking slowly due to exhaustion or illness. Small differences in speaking rate between enrollment and test utterances will not be a problem, but severe mismatch will lead to serious performance degradation. Zeng et al. [7] found that some speaker information will be discarded in fast speech, while slow speaking rate damages the spectrum of speech signals. In early works, Grimaldi et al. [8] studied the impact of speaking rate on speaker verification systems and confirmed that verification performance degraded due to speaking rate mismatch. Heerden et al. [9] used the phoneme duration as an additional feature and augmented it to the conventional Melfrequency cepstral coefficients (MFCCs) to mitigate the impact of speaking rate mismatch. Rozi et al. [10] proposed a feature transform approach that projected the speech features with slow speaking rate to those with normal speaking rate.\nIn this paper, we propose a method to effectively decompose the original embedding into two uncorrelated components: identity-related component and rate-related component. We utilize a rate estimation task with a channel-wise attention block to obtain the rate-related feature and then disentangle it from the whole original embedding. Besides, to further reduce the latent relationship between the two decomposed components, we adopt a cosine similarity loss that minimizes the cosine similarity between the identity- and rate- related features in an adversarial manner. Specifically, a cosine mapping block is introduced to find the maximum similarity between the two components, while the encoder network and attention block aim to reduce the similarity adversarially. Through the feature decomposition and adversarial training, the rate information can be significantly removed. Our proposed framework can be implemented as a simple yet effective residual branch by integrating it into existing encoders while only adding a tiny amount of parameters and computation."
        },
        {
            "heading": "2. Related works",
            "text": ""
        },
        {
            "heading": "2.1. Time-scale modification",
            "text": "Given an audio signal x(t), we can alter its rate to x(\u03b1t) by resampling and time-warping with a scale \u03b1. However, this simple method will change the audio fundamental frequency and\nar X\niv :2\n20 5.\n14 29\n4v 1\n[ ee\nss .A\nS] 2\n8 M\nay 2\n02 2\nresult in a different pitch. This can be seen from the frequency domain, supposing the Fourier transform of x(t) is X(w), then the Fourier transform of x(\u03b1t) will be \u03b1\u22121X(\u03b1\u22121w), which means the time-warping scale produces frequency components shifting. These changes result in the audio sound as if they are uttered by different people. Lee et al. [11] adopted this method for data augmentation and assigned new speaker labels to the augmented examples. That is to say, this method cannot be used to simulate different speech rates for the same person, which requires the same pitch content.\nThanks to the time-scale modification (TSM) technology, we can apply it to simulate diverse speaking rate scenarios. The TSM alters the duration of an audio signal while retaining its local frequency content without affecting the pitch content and the prosody information [12]. That is to say, the duration of the original audio can be increased or decreased, but the important speaker identification features remain unchanged. By doing so, the TSM makes the modified audio sound as if the speaker is talking at a slower or faster pace. Current methods for TSM can be roughly categorized into three categories: frequency domain, time domain, and hybrid methods. In general, time-domain methods are more effective at scaling transient signals, while frequency-domain methods excel in scaling harmonically complex audio. Hybrid methods leverage the strengths of frequency and time domain methods to produce higher quality results [13]. Recently, in the field of speech recognition and language identification, several works [12, 14] have adopted the TSM based algorithm for data augmentation."
        },
        {
            "heading": "2.2. Latent identity analysis",
            "text": "The latent identity analysis model can infer the latent variable through a statistical model from the given observations. The general formulation for latent identity analysis can be formulated as:\n\u03a6 = \u00b5+ n\u2211 i=1 Uixi (1)\nwhere \u03a6 \u2208 Rd\u00d71 refers to the speaker representation, \u00b5 is the mean of all the representations, the columns of Ui \u2208 Rd\u00d7mi span the subspace of different variation and xi \u223c N (0, I) denotes the latent variable.\nRecently, a few works [15, 16] have applied the latent variable model to speaker verification based on the variational autoencoder. However, how to smoothly apply latent variable model to extract robust speaker embeddings still remains to be explored."
        },
        {
            "heading": "2.3. Adversarial learning",
            "text": "Adversarial training technique has greatly facilitated speaker verification in domain mismatch situations. In speaker verification, the domain adversarial network usually consists of an encoder, a speaker discriminator, and a domain discriminator. The encoder aims to fool the domain discriminator through learning source embeddings that resemble the target embeddings, while the domain discriminator aims to discriminate the learned embeddings from target domain. By this minimax game between the encoder and domain discriminator, the encoder can successfully minimize the distance between the source and target data distribution in the feature space. Besides, adversarial learning has been widely explored to improve the speaker embedding robustness for noisy [4, 17, 18], cross-channel [19, 20, 21], and short utterances [22] situations.\nUnlike the general adversarial learning framework, we\nadopt a simpler adversarial training approach, i.e., minimizing the cosine distance between two features to reduce their correlation."
        },
        {
            "heading": "3. Proposed methods",
            "text": ""
        },
        {
            "heading": "3.1. Feature decomposition",
            "text": "As audio contains intrinsic identity information and other information, they can be jointly represented by the identity-related feature and variability-related feature. Thus, we can decompose the two features from the original embedding \u03a6 in a supervised manner:\nxid = V\u0303\u03a6 , xrate = U\u0303\u03a6 (2)\nwhere, xid is the identity-related feature, and xrate is the raterelated feature. V\u0303 and U\u0303 are the projection matrices, respectively. However, the identity-related feature is latent variable, implementing V\u0303 and U\u0303 in the network directly will be complicated. Since the identity-related feature is what we need in the rate-invariant speaker verification problem, and the rate-related feature can be easily obtained through a rate estimation task, we utilize a rate estimation task combining with a channel-wise attention block [23, 24] to disentangle the rate-related features from the embedding. More precisely, every input utterance is labeled by speaker-id and rate-id (slow, normal, and fast), and the identity estimation and rate estimation tasks update the parameters of the network simultaneously. Besides, an attention model is applied to perform dynamic channel-wise feature recalibration for the multi-task learning. Figure 2 shows an overview of our proposed method. We base on the assumption that if the reinforced features learned by the attention block are helpful for the variability estimation task, they should be suppressed from the identity-related features. Thus, the decomposition of the original embedding can be defined as:\nxid = (1\u2212 \u03c3 (\u03a6)) \u03a6 , xrate = \u03c3 (\u03a6) \u03a6 (3)\nwhere \u03c3 represents an attention weight learned by the attention model, and represents element-wise multiplication."
        },
        {
            "heading": "3.2. Cosine similarity adversarial learning",
            "text": "The disentangled features, i. e., xid and xrate, obtained through the attention mechanism, however, may have some latent relationship with each other. That is to say, xid may still contain the speaking rate related information which degrades the performance of speaker verification. To better guide the rate\ninformation disengaged from the original embedding, inspired by [25], we further consider minimizing the cosine similarity between xid and xrate. Intuitively, if the features of the two subspaces have low cosine similarity, their intrinsic latent relationship would also be very small. As shown in Figure 2, the cosine similarity loss is computed between the two decomposed features by the cosine mapping block. Precisely, we evaluated the cosine similarity loss as follow:\nLcos = ( x\u2032id \u00b7 x\u2032rate )2 (4) where x\u2032 is the normalized version of the fully-connected (FC) layer output embedding x\u2032, and \u00b7 represents dot production. The square function is used to constrain Lcos \u2208 [0, 1]. Adopting adversarial learning, we divide the training process into two phases, termed as the cosine similarity maximization and minimization, respectively. That is to say, we first maximize the cosine similarity by training the cosine mapping module while freezing the encoder and attention module. Then, with the cosine mapping module fixed, cosine similarity is minimized along with the identity and rate estimation tasks by updating the encoder and attention module. By doing so, it plays a minimax game during the adversarial training procedure. Finally, it renders the two features almost orthogonal and further reduces the correlation between them.\nOverall, the total objective function for the multi-task is formulated as:\nL = Lid + \u03bb1Lrate + \u03bb2Lcos (5)\nwhere Lid is the additive margin (AM) softmax loss for the identity estimation task, Lrate denotes the rate estimation softmax loss, \u03bb1 and \u03bb2 are scalar hyper-parameters to balance these three losses."
        },
        {
            "heading": "4. Experiments",
            "text": ""
        },
        {
            "heading": "4.1. Dataset",
            "text": "To demonstrate the effectiveness of our proposed method, a dataset with different speaking rates is required and the amount of samples should be large enough to avoid overfitting. However, the size of available datasets to meet the experimental requirements is limited. In order to assess our systems\u2019 robustness to speaking rate, we first created a large simulated dataset with different speaking rates based on the VoxCeleb1 dataset [26]. Then, experiments were conducted on both the augmented VoxCeleb1 simulated dataset and the HI-MIA [27] real corpus.\nThe training set of VoxCeleb1 includes 148,642 recordings uttered by 1,211 speakers, while the test set consists of 37,720 test trials, including 4,878 utterances from 40 speakers. The utterances in the VoxCeleb1 dataset were collected from online video, although not all of them were pronounced at a strictly normal speed, we assumed they were normal speaking rate utterances and labeled them as normal. To simulate different speaking rates from the original data, we adopted the FFmpeg libraries [28] based TSM algorithm to modify the original audio x(t) to x(\u03b1t) with a scaling speed factor \u03b1. Compared with the original speech, if \u03b1 < 1, a slower speech was generated and labeled as slow, if \u03b1 > 1, a faster speech was generated and labeled as fast. In our experiments, the scale \u03b1 ranged from 0.5 to 2.0 and the change step was set to 0.1. For the training set, we added one-fourth of the original data for each scale \u03b1 less than 1.0 (0.5\u20130.9), while for each scale \u03b1 which is greater than 1.0 (1.1\u20132.0), one-eighth of the original data was added. Thus, after\nbeing augmented by different speaking rates, the total training data size was about 3.5 times of the original data. Meanwhile, except for the original test trials, another 15 sets of test trials were also created, which were enrolled in normal speaking rate while tested in different specific speaking rates w. r. t a given \u03b1.\nFor the real scene speaking rate, experiments were conducted on the text-dependent speaker recognition database, HIMIA. The phrase of HI-MIA is the wake-up words \u2018Hi, Mia\u2019 in Chinese. The data was collected in a real home environment using microphone arrays and a high-fidelity microphone. The recordings of each speaker could be categorized into three subsets according to the speaking speed (i. e., slow, normal, and fast). Considering that we focused on examining the effect of different speaking rates, other factors (e. g., far-field, crosschannel) should be excluded. Thus, we only selected closetalking microphone utterances for the creation of the disjoint training set (254 speakers) and test set (42 speakers). Then, three sets of trials with different speaking rates were created as the cross-product of the utterances in the test set. In other words, all possible target and impostor samples were created from the test set."
        },
        {
            "heading": "4.2. Experimental setting",
            "text": "For all of the audios, we extracted 40-dimensional MFCCs with a 25ms window and a 10ms frame shift, and an energy-based voice active detection (VAD) was conducted to filter out nonspeech frames. Then, cepstral mean normalization (CMN) over a 3s sliding window was applied.\nFor the network implementation, we applied the extended time delay network (E-TDNN) describe in [29] as the encoder network. Our proposed framework was applied by integrating it into the segment-level. We utilized the cross-entropy loss for speaking rate prediction and the AM-Softmax loss for speakers classification. We alternately run the cosine similarity maximizing process for 20 iterations and then switched to minimizing process for 50 iterations. The empirically setting of hyperparameters \u03bb1 and \u03bb2 in Equation 5 were: \u03bb1 = 0.1, \u03bb2 = 0.1.\nSince we had only limited real-world data with different speaking rates, the model trained directly on these data would most likely to be overfitting. Therefore, we resorted to the transfer learning strategy for HI-MIA evaluations. Specifically, we utilized the models pre-trained on the VoxCeleb1 dataset for parameters initialization.\nAt testing stage, we computed the probabilistic linear discriminant analysis (PLDA) scoring and used the EER as the performance metric. All of these systems were implemented in ASV-Subtools [30]."
        },
        {
            "heading": "4.3. Evaluations on VoxCeleb1",
            "text": "In this section, experiments were conducted on the Voxceleb1 dataset. Five different systems were designed for comparisons, listed as below:\n\u2022 S1: The baseline E-TDNN trained on the VoxCeleb1 training data.\n\u2022 S2: The network trained on TSM simulated and original data (TSM aug.).\n\u2022 S3: The feature decomposition system with attention block (FD att.).\n\u2022 S4: The network trained with cosine similar adversarial learning (AL cos.).\n\u2022 S5: The system combined with feature decomposition and cosine similar adversarial learning (FD-AL).\nComparisons between these systems under different speaking rate test sets are shown in Table 1. It is seen from the experimental results of S1 that when a small degree of speech rate mismatch happened, the performance reduced slightly. At the same time, more obvious degradation occurred in the slow test sets than the fast ones, which shows that the slow utterances cause more severe distortion for the spectrum. However, when a large degree of speech speed mismatch happened, it could sharply degrade the discriminative power of speaker embeddings, and more degradation occurred in fast speech. One intuitive explanation is that as the audio speed increases, the duration of the audio becomes shorter and contains less information.\nAs expected, S2 shows that adding simulated data can alleviate the mismatch problem and improve the performance significantly. The S3 and S4 show that feature decomposition or adversarial learning can further improve performance. The two systems both achieved about 15% performance improvement in all cases compared to the S2, which trained directly on augmented data. One can also see that in most cases, S4 outperforms S3, while S3 provides better robustness when the speaking rate mismatch became serious. In S5, we achieved the best results by combining feature decomposition and cosine similarity adversarial learning, which suggests the two methods are complementary. Surprisingly, our proposed method not only improves the performance in the mismatch case but also improves the performance in the normal audio speed test set. This observation indicates that the original embeddings contain speech rate information, which would affect the verification performance, while our proposed method could eliminate speech speed information and improve the discrimination ability for speaker embeddings."
        },
        {
            "heading": "4.4. Evaluations on HI-MIA",
            "text": "In this section, experiments were conducted on the real scene speaking rate dataset, HI-MIA. We firstly conducted experiments to evaluate whether the TSM based simulated data could characterize the real-world speech rate variability in the speaker verification task. To this end, we utilized the following four different evaluation systems:\n\u2022 S6: The baseline system trained on the HI-MIA slow, normal, and fast speed utterances.\n\u2022 S7: The system trained only on the HI-MIA normal rate utterances.\n\u2022 S8: The system trained on the HI-MIA normal rate data combined with two-fold noise augmented copies. The noise data augmentation processing was followed the setup described in [1].\n\u2022 S9: The system trained on the HI-MIA normal rate data with two-fold TSM based rate augmentation versions, the \u03b1 was set to 0.8, 0.9, 1.1, and 1.2.\nExperimental results of are shown in the upper part of Table 2. We can see from the comparisons between S8 and S9 that S8 outperforms S9 in the normal rate test set, but S9 achieves better preferences under speaking rate mismatch conditions. Since the two systems were trained on the same training data size, it can be deduced that the improved performance achieved by S9 was benefited from the TSM based data augmentation. Besides, the performance of S9 is nearly similar to S6, which indicates that TSM based data augmentation could characterize the real-world speech rate variability and alleviate the realworld speaking rate mismatch problem.\nThen, we conducted experiments based on systems of S10S12 to validate the effectiveness of our proposed method in solving real scene speaking rate mismatch problems. The training datasets of system S10-S12 were the same as the training set of S6. The network structures of S10-S12 correspond to S3-S5. The lower part of Table 2 shows the results by feature decomposition and adversarial learning. We can observe that the experimental results were consistent with those on VoxCeleb1 test sets, which illustrates that our proposed method could also improve the speaker embeddings robustness in real-world speaking rates mismatch scenarios."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this paper, we proposed an attention block and cosine similarity loss to obtain rate-invariant speaker embeddings. The attention block was used to obtain rate-related feature and then this feature was disentangled from the original embedding. The cosine similarity loss and cosine mapping block were introduced to minimize the cosine similarity between identity- and rate- related features adversarially. Experiments conducted on both VoxCeleb1 based simulated data and the HI-MIA realistic dataset demonstrated the superior effectiveness of our proposed method in dealing with the speaking rate mismatch problem. In the future, we are interested in validating our method on other speaker-unrelated variabilities, such as far-field, cross-channel, and noisy environments."
        },
        {
            "heading": "6. References",
            "text": "[1] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-\npur, \u201cX-vectors: Robust dnn embeddings for speaker recognition,\u201d in Proc. ICASSP. IEEE, 2018, pp. 5329\u20135333.\n[2] T. Zhou, Y. Zhao, and J. Wu, \u201cResNeXt and Res2Net structure for speaker verification,\u201d arXiv preprint arXiv:2007.02480, 2020.\n[3] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPATDNN: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\u201d in Proc. INTERSPEECH, 2020, pp. 1\u20135.\n[4] R. Peri, M. Pal, A. Jati, K. Somandepalli, and S. Narayanan, \u201cRobust speaker recognition using unsupervised adversarial invariance,\u201d in Proc. ICASSP. IEEE, 2020, pp. 6614\u20136618.\n[5] J. Tai, X. Jia, Q. Huang, W. Zhang, H. Du, and S. Zhang, \u201cSEEFALDR: A speaker embedding enhancement framework via adversarial learning based disentangled representation,\u201d in Annual Computer Security Applications Conference, 2020, pp. 939\u2013950.\n[6] Y. Kwon, S.-W. Chung, and H.-G. Kang, \u201cIntra-class variation reduction of speaker representation in disentanglement framework,\u201d in Proc. INTERSPEECH, 2020.\n[7] X. Zeng, S. Yin, and D. Wang, \u201cLearning speech rate in speech recognition,\u201d in Sixteenth Annual Conference of the International Speech Communication Association, 2015.\n[8] M. Grimaldi and F. Cummins, \u201cSpeech style and speaker recognition: A case study,\u201d in Tenth Annual Conference of the International Speech Communication Association, 2009.\n[9] C. J. van Heerden and E. Barnard, \u201cSpeech rate normalization used to improve speaker verification,\u201d in Proceedings of the Symposium of the Pattern Recognition Association of South Africa. Citeseer, 2007, pp. 2\u20137.\n[10] A. Rozi, L. Li, D. Wang, and T. F. Zheng, \u201cFeature transformation for speaker verification under speaking rate mismatch condition,\u201d in 2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA). IEEE, 2016, pp. 1\u20134.\n[11] K. A. Lee, H. Yamamoto, K. Okabe, Q. Wang, L. Guo, T. Koshinaka, J. Zhang, and K. Shinoda, \u201cNEC-TT system for mixedbandwidth and multi-domain speaker recognition,\u201d Computer Speech & Language, vol. 61, p. 101033, 2020.\n[12] Z. Ma, H. Yu, W. Chen, and J. Guo, \u201cShort utterance based speech language identification in intelligent vehicles with timescale modifications and deep bottleneck features,\u201d IEEE transactions on vehicular technology, vol. 68, no. 1, pp. 121\u2013128, 2018.\n[13] T. Roberts and K. K. Paliwal, \u201cA time-scale modification dataset with subjective quality labels,\u201d 2020.\n[14] T. Ko, V. Peddinti, D. Povey, and S. Khudanpur, \u201cAudio augmentation for speech recognition,\u201d in Sixteenth Annual Conference of the International Speech Communication Association, 2015.\n[15] J. Villalba, N. Bru\u0308mmer, and N. Dehak, \u201cTied variational autoencoder backends for i-vector speaker recognition,\u201d in Proc. INTERSPEECH, 2017, pp. 1004\u20131008.\n[16] W. H. Kang and N. S. Kim, \u201cAdversarially learned total variability embedding for speaker recognition with random digit strings,\u201d Sensors, vol. 19, no. 21, p. 4709, 2019.\n[17] Z. Meng, Y. Zhao, J. Li, and Y. Gong, \u201cAdversarial speaker verification,\u201d in Proc. ICASSP. IEEE, 2019, pp. 6216\u20136220.\n[18] J. Zhou, T. Jiang, L. Li, Q. Hong, Z. Wang, and B. Xia, \u201cTraining multi-task adversarial network for extracting noise-robust speaker embedding,\u201d in Proc. ICASSP. IEEE, 2019, pp. 6196\u20136200.\n[19] Q. Wang, W. Rao, S. Sun, L. Xie, E. S. Chng, and H. Li, \u201cUnsupervised domain adaptation via domain adversarial training for speaker recognition,\u201d in Proc. ICASSP. IEEE, 2018, pp. 4889\u2013 4893.\n[20] Z. Chen, S. Wang, Y. Qian, and K. Yu, \u201cChannel invariant speaker embedding learning with joint multi-task and adversarial training,\u201d in Proc. ICASSP. IEEE, 2020, pp. 6574\u20136578.\n[21] C. Luu, P. Bell, and S. Renals, \u201cChannel adversarial training for speaker verification and diarization,\u201d in Proc. ICASSP. IEEE, 2020, pp. 7094\u20137098.\n[22] K. Liu and H. Zhou, \u201cText-independent speaker verification with adversarial learning on short utterances,\u201d in Proc. ICASSP. IEEE, 2020, pp. 6569\u20136573.\n[23] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132\u20137141.\n[24] J. Zhou, T. Jiang, Z. Li, L. Li, and Q. Hong, \u201cDeep speaker embedding extraction with channel-wise feature responses and additive supervision softmax loss function.\u201d in Proc. INTERSPEECH, 2019, pp. 2883\u20132887.\n[25] H. Wang, D. Gong, Z. Li, and W. Liu, \u201cDecorrelated adversarial learning for age-invariant face recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 3527\u20133536.\n[26] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, \u201cVoxceleb: Large-scale speaker verification in the wild,\u201d Computer Speech & Language, vol. 60, p. 101027, 2020.\n[27] X. Qin, H. Bu, and M. Li, \u201cHI-MIA: A far-field text-dependent speaker verification database and the baselines,\u201d in Proc. ICASSP. IEEE, 2020, pp. 7609\u20137613.\n[28] S. Tomar, \u201cConverting video formats with ffmpeg,\u201d Linux Journal, vol. 2006, no. 146, p. 10, 2006.\n[29] D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, \u201cSpeaker recognition for multi-speaker conversations using x-vectors,\u201d in Proc. ICASSP. IEEE, 2019, pp. 5796\u2013 5800.\n[30] F. Tong, M. Zhao, J. Zhou, H. Lu, Z. Li, L. Li, and Q. Hong, \u201cASV-Subtools: Open source toolkit for automatic speaker verification,\u201d in Proc. ICASSP. IEEE, 2021, pp. 6184\u20136188."
        }
    ],
    "title": "Deep Representation Decomposition for Rate-Invariant Speaker Verification",
    "year": 2022
}