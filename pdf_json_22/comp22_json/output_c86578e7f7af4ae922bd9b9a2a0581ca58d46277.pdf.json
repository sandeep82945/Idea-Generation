{
    "abstractText": "Triggered by the success of transformers in various visual tasks, the spatial self-attention mechanism has recently attracted more and more attention in the computer vision community. However, we empirically found that a typical vision transformer with the spatial self-attention mechanism could not learn accurate attention maps for distinguishing different categories of fine-grained images. To address this problem, motivated by the temporal attention mechanism in brains, we propose a spatial-temporal attention network for learning fine-grained feature representations, called STAN, where the features learnt by implementing a sequence of spatial self-attention operations corresponding to multiple moments are aggregated progressively. The proposed STAN consists of four modules: a self-attention backbone module for learning a sequence of features with selfattention operations, a spatial feature self-organizing module for facilitating the model training, a spatial-temporal feature learning module for aggregating the re-organized features via a Long Short-Term Memory network, and a context-aware module that is implemented as the forget block of the spatial-temporal feature learning module for preserving/forgetting the long-term memory by utilizing contextual information. Then, we propose a STAN-based method for open-set fine-grained recognition by integrating the proposed STAN network with a linear classifier, called STAN-OSFGR. Extensive experimental results on 3 finegrained datasets and 2 coarse-grained datasets demonstrate that the proposed STAN-OSFGR outperforms 9 state-of-the-art openset recognition methods significantly in most cases.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiayin Sun"
        },
        {
            "affiliations": [],
            "name": "Hong Wang"
        },
        {
            "affiliations": [],
            "name": "Qiulei Dong"
        }
    ],
    "id": "SP:446817e93c7789f562b510c3ede86a1517cbd201",
    "references": [
        {
            "authors": [
                "W. Dai",
                "W. Diao",
                "X. Sun",
                "Y. Zhang",
                "L. Zhao",
                "J. Li",
                "K. Fu"
            ],
            "title": "Camv: Class activation mapping value towards open set finegrained recognition",
            "venue": "IEEE Access, vol. 9, pp. 8167\u20138177, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Vaze",
                "K. Han",
                "A. Vedaldi",
                "A. Zisserman"
            ],
            "title": "Open-set recognition: a good closed-set classifier is all you need?",
            "venue": "in International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "C. Wah",
                "S. Branson",
                "P. Welinder",
                "P. Perona",
                "S. Belongie"
            ],
            "title": "The CaltechUCSD Birds-200-2011 Dataset",
            "venue": "California Institute of Technology, Tech. Rep., 2011.",
            "year": 2011
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly",
                "J. Uszkoreit",
                "N. Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint. arXiv:2010.11929, 2020.",
            "year": 2010
        },
        {
            "authors": [
                "K. Han",
                "A. Xiao",
                "E. Wu",
                "J. Guo",
                "C. Xu",
                "Y. Wang"
            ],
            "title": "Transformer in transformer",
            "venue": "Conference and Workshop on Neural Information Processing Systems, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Wang",
                "E. Xie",
                "X. Li",
                "D. Fan",
                "K. Song",
                "D. Liang",
                "T. Lu",
                "P. Luo",
                "L. Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "IEEE International Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Yuan",
                "Y. Chen",
                "T. Wang",
                "W. Yu",
                "Y. Shi",
                "Z. Jiang",
                "F.E. Tay",
                "J. Feng",
                "S. Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "IEEE International Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Liu",
                "Y. Lin",
                "Y. Cao",
                "H. Hu",
                "Y. Wei",
                "Z. Zhang",
                "S. Lin",
                "B. Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "IEEE International Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Wang",
                "Y. Qiu",
                "P. Cheng",
                "J. Zhang"
            ],
            "title": "Hybrid cnntransformer features for visual place recognition",
            "venue": "IEEE TCSVT, 2022, doi:10.1109/TCSVT.2022.3212434.",
            "year": 2022
        },
        {
            "authors": [
                "J. Sun",
                "H. Wang",
                "Q. Dong"
            ],
            "title": "MoEP-AE: Autoencoding Mixtures of Exponential Power Distributions for Open-Set Recognition",
            "venue": "IEEE TCSVT, 2022, doi:10.1109/TCSVT.2022.3200112.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zheng",
                "B. Zhong",
                "Q. Liang",
                "Z. Tang",
                "R. Ji",
                "X. Li"
            ],
            "title": "Leveraging local and global cues for visual tracking via parallel interaction network",
            "venue": "IEEE TCSVT, 2022, doi:10.1109/TCSVT.2022.3212987.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Hu",
                "K. Zhao",
                "B. Zhou",
                "H. Guo",
                "S. Wu",
                "Y. Yang",
                "J. Liu"
            ],
            "title": "Gaze target estimation inspired by interactive attention",
            "venue": "IEEE TCSVT, 2022, doi:10.1109/TCSVT.2022.3190314.",
            "year": 2022
        },
        {
            "authors": [
                "Y.-J. Wang",
                "Y.-M. Luo",
                "G.-H. Bai",
                "J.-M. Guo"
            ],
            "title": "Uformpose: A u-shaped hierarchical multi-scale keypoint-aware framework for human pose estimation",
            "venue": "IEEE TCSVT, 2022, doi:10.1109/TCSVT.2022.3213206.",
            "year": 2022
        },
        {
            "authors": [
                "S. Guo",
                "E. Rigall",
                "Y. Ju",
                "J. Dong"
            ],
            "title": "3d hand pose estimation from monocular rgb with feature interaction module",
            "venue": "IEEE TCSVT, vol. 32, no. 8, pp. 5293\u20135306, 2022, doi:10.1109/TCSVT.2022.3142787.",
            "year": 2022
        },
        {
            "authors": [
                "Z. Zhou",
                "Q. Dong"
            ],
            "title": "Self-distilled feature aggregation for self-supervised monocular depth estimation",
            "venue": "European Conference on Computer Vision, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Yuan",
                "X. Gu",
                "Z. Dai",
                "S. Zhu",
                "P. Tan"
            ],
            "title": "Neural window fully-connected crfs for monocular depth estimation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.T. Coull"
            ],
            "title": "fmri studies of temporal attention: allocating attention within, or towards, time",
            "venue": "Cognitive Brain Research, vol. 21, no. 2, pp. 216\u2013226, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "F. Sugimoto",
                "M. Kimura",
                "Y. Takeda",
                "J. Katayama"
            ],
            "title": "Temporal attention is involved in the enhancement of attentional capture with task difficulty: an event-related brain potential study",
            "venue": "NeuroReport, vol. 28, no. 12, pp. 755\u2013759, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Convolutional deep belief networks on cifar-10",
            "venue": "2010, Technical report.",
            "year": 2010
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "2009, Technical report.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Le",
                "X. Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "2015, CS 231N.",
            "year": 2015
        },
        {
            "authors": [
                "A. Bendale",
                "T.E. Boult"
            ],
            "title": "Towards open set deep networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "D. Zhou",
                "H. Ye",
                "D. Zhan"
            ],
            "title": "Learning placeholders for openset recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R. Yoshihashi",
                "W. Shao",
                "R. Kawakami",
                "S. You",
                "M. Iida",
                "T. Naemura"
            ],
            "title": "Classification-reconstruction learning for openset recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Oza",
                "V.M. Patel"
            ],
            "title": "C2ae: Class conditioned autoencoder for open-set recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Perera",
                "V.I. Morariu",
                "R. Jain",
                "V. Manjunatha",
                "C. Wigington",
                "V. Ordonez",
                "V.M. Patel"
            ],
            "title": "Generativediscriminative feature representations for open-set recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "G. Chen",
                "L. Qiao",
                "Y. Shi",
                "P. Peng",
                "J. Li",
                "T. Huang",
                "S. Pu",
                "Y. Tian"
            ],
            "title": "Learning open set network with discriminative reciprocal points",
            "venue": "European Conference on Computer Vision, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Yang",
                "X. Zhang",
                "F. Yin",
                "Q. Yang",
                "C. Liu"
            ],
            "title": "Convolutional prototype network for open set recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, pp. 2358\u20132370, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "J. Lu",
                "Y. Xu",
                "H. Li",
                "Z. Cheng",
                "Y. Niu"
            ],
            "title": "Pmal: Open set recognition via robust prototype mining",
            "venue": "AAAI Conference on Artificial Intelligence, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "A. Li",
                "J. Guo",
                "Y. Guo"
            ],
            "title": "Hybrid models for open set recognition",
            "venue": "European Conference on Computer Vision, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "X. Sun",
                "Z. Yang",
                "C. Zhang",
                "K. Ling",
                "G. Peng"
            ],
            "title": "Conditional gaussian distribution learning for open set recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Cao",
                "Y. Luo",
                "D. Klabjan"
            ],
            "title": "Open-set recognition with PAPER UNDER REVIEW 13 gaussian mixture variational autoencoders",
            "venue": "AAAI Conference on Artificial Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Guo",
                "G. Camporese",
                "W. Yang",
                "A. Sperduti",
                "L. Ballan"
            ],
            "title": "Conditional variational capsule network for open set recognition",
            "venue": "IEEE International Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Azizmalayeri",
                "M.H. Rohban"
            ],
            "title": "Ood augmentation may be at odds with open-set recognition",
            "venue": "arXiv preprint. arXiv:2206.04242, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Huang",
                "Y. Wang",
                "Q. Hu",
                "M.-M. Cheng"
            ],
            "title": "Class-specific semantic reconstruction for open set recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, doi:10.1109/TPAMI.2022.3200384.",
            "year": 2022
        },
        {
            "authors": [
                "H. Yang",
                "X. Zhang",
                "F. Yin",
                "C. Liu"
            ],
            "title": "Robust classification with convolutional prototype learning",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "G. Chen",
                "P. Peng",
                "X. Wang",
                "Y. Tian"
            ],
            "title": "Adversarial reciprocal points learning for open set recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Ge",
                "S. Demyanov",
                "R. Garnavi"
            ],
            "title": "Generative openmax for multi-class open set classification",
            "venue": "British Machine Vision Conference, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. Neal",
                "M. Olson",
                "X. Fern",
                "W. Wong",
                "F. Li"
            ],
            "title": "Open set learning with counterfactual images",
            "venue": "European Conference on Computer Vision, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "S. Kong",
                "D. Ramanan"
            ],
            "title": "Opengan: Open-set recognition via open data generation",
            "venue": "European Conference on Computer Vision, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "W. Cho",
                "J. Choo"
            ],
            "title": "Towards accurate open-set recognition via background-class regularization",
            "venue": "European Conference on Computer Vision, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "W. Moon",
                "J. Park",
                "H.S. Seong",
                "C.-H. Cho",
                "J.-P. Heo"
            ],
            "title": "Difficulty-aware simulator for open set recognition",
            "venue": "European Conference on Computer Vision, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Dilokthanakul",
                "P.A. Mediano",
                "M. Garnelo",
                "M.C. Lee",
                "H. Salimbeni",
                "K. Arulkumaran",
                "M. Shanahan"
            ],
            "title": "Deep unsupervised clustering with gaussian mixture variational autoencoders",
            "venue": "arXiv preprint. arXiv:1611.02648, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Maji",
                "E. Rahtu",
                "J. Kannala",
                "M. Blaschko",
                "A. Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint. arXiv:1306.5151, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "M. Guo",
                "T. Xu",
                "J. Liu",
                "Z. Liu",
                "P. Jiang",
                "T. Mu",
                "S. Zhang",
                "R.R. Martin",
                "M. Cheng",
                "S. Hu"
            ],
            "title": "Attention mechanisms in computer vision: A survey",
            "venue": "Computational Visual Media, vol. 8, no. 3, pp. 331\u2013368, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zhang",
                "I. Goodfellow",
                "D. Metaxas",
                "A. Odena"
            ],
            "title": "Selfattention generative adversarial networks",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "P. Ramachandran",
                "N. Parmar",
                "A. Vaswani",
                "I. Bello",
                "A. Levskaya",
                "J. Shlens"
            ],
            "title": "Stand-alone self-attention in vision models",
            "venue": "Conference and Workshop on Neural Information Processing Systems, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Ye",
                "M. Rochan",
                "Z. Liu",
                "Y. Wang"
            ],
            "title": "Cross-modal selfattention network for referring image segmentation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Fu",
                "J. Liu",
                "H. Tian",
                "Y. Li",
                "Y. Bao",
                "Z. Fang",
                "H. Lu"
            ],
            "title": "Dual attention network for scene segmentation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Choe",
                "H. Shim"
            ],
            "title": "Attention-based dropout layer for weakly supervised object localization",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "IEEE International Conference on Computer Vision, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint. arXiv:1409.1556, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "arXiv preprint. arXiv:1512.03385, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Huang",
                "Z. Liu",
                "L. van der Maaten",
                "K.Q. Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "arXiv preprint. arXiv:1608.06993, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "J. Krause",
                "M. Stark",
                "J. Deng",
                "L. Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "IEEE International Conference on Computer Vision, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "A.R. Dhamija",
                "M. G\u00fcnther",
                "T.E. Boult"
            ],
            "title": "Reducing network agnostophobia",
            "venue": "Conference and Workshop on Neural Information Processing Systems, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Fixing weight decay regularization in adam",
            "venue": "International Conference on Learning Representations, 2018.",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014open-set fine-grained image recognition, spatialtemporal attention, Long-Short Term Memory\nI. INTRODUCTION\nW ITH the rapid development of deep neural networks(DNNs), the computer vision community has witnessed significant progress in various image recognition tasks. Compared with generic image recognition, open-set finegrained image recognition (OSFGR) which aims to simultaneously classify known-class fine-grained samples and detect unknown-class fine-grained samples, is a more challenging task due to the lower interclass discrepancies, as shown in Fig. 1. Accordingly, this task requires OSFGR models to learn more discriminative features from input images [1, 2].\nRecently, the spatial self-attention mechanism has been extensively used for feature learning in the computer vision community due to its ability of capturing local discriminative information, and particularly, the transformer and its variants [4\u20138], which were explored according to the spatial selfattention mechanism, have achieved a tremendous success in various visual tasks, such as image recognition [9, 10], object tracking [11, 12], pose estimation [13, 14], and monocular depth estimation [15, 16]. However, we empirically find that regardless of whether the scenario is open-set or closedset, when a typical vision transformer is trained for object\nrecognition with a set of object images that belong to different categories but have similar appearances, it sometimes could not pay accurate attention on objects, resulting in wrong predictions. This phenomenon would be described in detail in Sec. III-A.\nTo address this problem, we propose a spatial-temporal attention network to learn more accurate attention maps from input images for recognizing fine-grained objects, called STAN, inspired by the temporal attention mechanism in brains [17, 18]. The proposed STAN network learns the spatialtemporal attention maps with four modules: (1) a self-attention backbone module (the typical swin transformer [8] is simply used as this module here) is used for extracting latent features by implementing a sequence of spatial self-attention operations; (2) a spatial feature self-organizing (SFSO) module is explored for facilitating the model training; (3) a spatialtemporal feature learning (STFL) module is explored for modeling temporal dependency in the sequence of latent features outputted from the SFSO module and aggregating\nar X\niv :2\n21 1.\n13 94\n0v 1\n[ cs\n.C V\n] 2\n5 N\nov 2\n02 2\nthese features progressively via an LSTM (Long Short-Term Memory network); (4) a context-aware (CA) module is explored for preserving/forgetting the long-term memory of the STFL module. Furthermore, we present a STAN-based method for dealing with the OSFGR task, called STAN-OSFGR, where the proposed STAN is firstly used for learning spatial-temporal features from input object images and then a linear classifier is used to classify these features.\nThe main contributions of this paper are as follows: - We empirically analyze the ability of a typical vision\ntransformer with the spatial self-attention mechanism for discriminating fine-grained object images, finding that the vision transformer could not provide accurate attention maps sometimes. This analysis could contribute to a better understanding of the vision transformer. - We propose the spatial-temporal attention network STAN for learning both effective attentions and finegrained feature representations. It is expected to be used as a feature extractor in various visual tasks, including but not limited to closed-set recognition and open-set recognition. - We design the STAN-OSFGR method for handling the OSFGR task with the proposed STAN. The priority of the designed STAN-OSFGR to nine state-of-the-art methods is demonstrated in Sec. IV.\nThe remainder of this paper is organized as follows. Sec. II reviews the related works. Sec. III empirically investigates the ability of a typical vision transformer for discriminating finegrained images, and describes the proposed STAN network and STAN-OSFGR method in detail. Sec. IV gives the experimental results. Sec. V concludes this paper."
        },
        {
            "heading": "II. RELATED WORKS",
            "text": "In this section, we review the related works on open-set recognition (OSR)/OSFGR and attention mechanisms, respectively."
        },
        {
            "heading": "A. OSR/OSFGR",
            "text": "From a technical point of view, all the existing methods for handling the OSR task could be straightforwardly used in the OSFGR task without regard to their recognition accuracies. Most of the existing OSR methods in literature have been only evaluated on several coarse-grained datasets in their original papers, such as CIFAR+10/+50 [19, 20] and TinyImageNet [21]. These methods generally learn discriminative feature representations either by utilizing only known-class samples [10, 22\u201332, 32\u201337] or by jointly utilizing both real knownclass samples and synthesized unknown-class samples [38\u2013 42]. Zhang et al. [30] proposed to train a resflow network and a classifier in a latent feature space simultaneously for calculating both likelihood scores and classification scores. Yang et al. [36] learnt the prototypes for modeling the distribution of each known class as a Gaussian mixture distribution, then the unknown classes could be detected according to whether the testing features belonged to these distributions. Cao et al. [32] modeled these distributions by using an autoencoder structure,\nGMVAE [43]. Sun et al. [10] proposed to encode the distributions of known-class sample features as multiple exponential power distributions for representing complex distributions that could not be modeled by Gaussian distributions. Kong and Ramanan [40] introduced some exposed outlier samples into the training set and used the discriminator of a GAN for detecting unknown classes. Chen et al. [37] minimized the overlap of the distributions of known-class features and those of unknown-class features by adversarial learning using the reciprocal points. Azizmalayeri and Rohban [34] revisited several training augmentations and chose the most suitable combination for the SoftMax-based OSR method using a vision transformer backbone.\nAdditionally, a few OSFGR methods [1, 2] that aim at learning feature representations for recognizing fine-grained objects in open-set scenarios have been proposed and evaluated on a few fine-grained datasets, such as CUB [3] and Aircraft [44]. Dai et al. [1] used the class activation mapping values rather than the softmax scores for preserving information in the classification scores. Vaze et al. [2] trained a closedset classification network with the ResNet50 backbone using multiple training strategies for improving the generalization ability of the closed-set recognition model on open-set classes."
        },
        {
            "heading": "B. Attention Mechanisms",
            "text": "Existing attention mechanisms in the computer vision community could be divided into four categories according to their operating domains [45]: channel attention, spatial attention, temporal attention, and branch attention. Among them, the spatial attention is more widely used. As an essential branch of spatial attention, the spatial self-attention mechanism, which captures the internal correlation of the features, has experienced from stand-alone modules [46\u201350] to integrated networks [4\u20138]. Examples of the latter include the vision transformer and its variants [4\u20138], which integrate a sequence of spatial self-attention operations in their networks. In [4], the vision transformer was proposed to rely entirely on selfattention in the form of fully-connected layers along with position encoding for handling several image tasks, which was the pioneering work to introduce transformers into the computer vision community and had the advantage in learning global information. Its variants utilized embedded subtransformer [5], pyramid architecture [6], layer-wise tokensto-tokens transformation [7], or shifted windows [8] for excavating local information that was insufficiently learnt by the vanilla vision transformer to improve the model performance.\nHowever, to our best knowledge, there is no report on whether accurate attention maps can be obtained from the fine-grained object images by a vision transformer. In this work, we investigate the ability of a typical vision transformer for discriminating fine-grained object images, and propose a spatial-temporal attention network for both learning more accurate attention maps and improving the performance of open-set fine-grained recognition, which would be described in the following section."
        },
        {
            "heading": "III. STAN FOR OSFGR",
            "text": "In this section, we firstly investigate the ability of a typical transformer in both coarse-grained and fine-grained image recognition tasks regardless of open-set or closed-set scenarios. Then we describe the proposed spatial-temporal attention network STAN in detail. Finally, we introduce the proposed STAN-OSFGR method based on the STAN network for handling the open-set fine-grained task."
        },
        {
            "heading": "A. Could a transformer pay accurate attention on objects in images?",
            "text": "Vision transformers [4\u20138] have shown their effectiveness in object recognition, mainly due to their multiple spatial self-attention operations. Then, a question is naturally raised: \u2018Could a transformer learn accurate attention maps on objects in images?\u2019 To address this question, the swin transformer [8], which is a typical vision transformer, is trained with the traditional cross-entropy classification loss for object classification by utilizing a portion of classes from the TinyImageNet dataset\n[21] (a coarse-grained dataset) and the Aircraft dataset [44] (a fine-grained dataset) respectively. For each dataset, we use Grad-CAM [51] to visualize the attention maps of 8 example images from the corresponding testing set (4 images from the known classes which are observed in the training set, while the rest from the unknown classes which are not observed in the training set), outputted from the four blocks of the swin transformer as shown in Fig. 2.\nAs seen from the examples 1\u00a9 and 2\u00a9 (which are predicted correctly) in Fig. 2 (a) and Fig. 2 (b), a higher block of the transformer is prone to pay more attention on the objects in the example images, resulting in a correct prediction. However, as seen from the examples 3\u00a9 and 4\u00a9 (which are predicted wrongly) in Fig. 2 (a) and Fig. 2 (b), the attention maps learnt by the higher blocks of the transformer are not accurate, which focus on the backgrounds (the grasses and water) or the distractors (the trunk and fences), resulting in wrong predictions.\nThe issue that the transformer focuses on the backgrounds or\ndistractors more easily for classification is not common in the coarse-grained OSR task, because images in different classes usually have significantly different appearances. However, it is a common issue in the OSFGR task where fine-grained objects usually appear in similar environments. As seen from the two sub-figures Fig. 2 (c) and Fig. 2 (d), though the images in the examples 1\u00a9 and 2\u00a9 are correctly predicted since the attention maps learnt by the higher blocks of the transformer mainly highlight the aircrafts, these attentions are universally not accurate enough, which focus on the sky or runways to different extent.\nIn sum, it is revealed to some extent from the above experimental results that the transformer could sometimes not learn accurate attention maps for coarse-grained object images, and they universally could not learn attention maps for finegrained object images which are accurate enough even some of these images could be accurately predicted."
        },
        {
            "heading": "B. Spatial-Temporal Attention Network",
            "text": "To address the above revealed issue, here, we propose a spatial-temporal attention network (called STAN) for learning fine-grained feature representations, whose architecture is shown in Fig. 3. STAN consists of four modules: a selfattention backbone module for learning sequential features with multiple spatial self-attention operations from input images, a spatial feature self-organizing (SFSO) module for transforming the learnt sequential features to facilitate the model training, a spatial-temporal feature learning (STFL) module for learning fine-grained feature representation via an LSTM, and a context-aware (CA) module that is embedded into the STFL module for selectively preserving the long-\nterm memory. In this work, we simply use swin transformer [8], which contains 4 spatial self-attention blocks and a spatial-feature classification loss as shown in Fig. 4, as the self-attention backbone module. Accordingly, a sequence of features could be obtained from the four spatial self-attention blocks of the swin transformer, and they are inputted to the SFSO module. In the following parts, the SFSO, STFL, and CA modules would be described in detail.\n1) Spatial Feature Self-Organizing (SFSO) Module: It is noted that (i) the sizes of the features outputted from the blocks of the self-attention backbone module might be different (also for the swin transformer [8]); (ii) for most of DNNs in literature [8, 52\u201354], the features learnt from their lower blocks/layers generally have a lower discriminability\nthan those from their higher blocks/layers. Hence here, the SFSO module is explored to take the sequence of features outputted from the self-attention backbone module as its input, and then re-organize them into a sequence of features so that (i) all the re-organized features are of a same size and (ii) each re-organized feature could integrate the original features outputted from higher blocks. Fig. 5 shows the architecture of the designed SFSO module.\nAs seen from Fig. 5, the sequence of features of different sizes outputted from the backbone module is firstly reshaped respectively, and then convolution (2-d convolution) operations are implemented on these feature maps in the SFSO module, so that this sequence of features of different sizes is transformed into a sequence of features of a same size. Next, the obtained features of a same size are passed through max-pooling layers respectively and further injected into a high-to-low feature aggregation structure with self-attentions, and a sequence of re-organized features where each feature integrates the original features outputted from the higher blocks of the backbone module is obtained.\n2) Spatial-Temporal Feature Learning (STFL) Module: The STFL module is explored to facilitate the model training. It takes the features re-organized by the SFSO module as its input, and outputs an aggregated feature that has temporal dependency on the re-organized features as the fine-grained feature representation. The architecture of this module is shown in Fig. 6.\nAs seen from Fig. 6, a single-layer LSTM is adopted here, which accepts the re-organized feature sequence that consists of 4 moments of features. The LSTM unit which is unrolled/unfolded through moments comprises of a (long-\nterm) memory cell (for retaining global memory of the LSTM over all of the 4 moments in 4 Cell states) and 3 blocks (the input/output/forget block) for determining what information to be added/exported/dropped from the memory. The output of the previous moment is utilized as the Hidden state, while the output of the final moment is used as the fine-grained feature representations to be classified. Besides, the initial Cell and Hidden states are instance-adaptive vectors that are mapped from the input feature at the final moment for accelerating the model training.\n3) Context-Aware (CA) Module: The CA module is explored to preserve/forget information selectively from the memory cell. It is implemented as the forget block of the STFL module. It contains three inputs: the Hidden state at the previous moment in the STFL module, the input feature at the current moment in the STFL module, and sequential contextual features obtained by splitting the current feature map at the pixel level. It outputs a soft mask, whose elements are limited to (0, 1), for masking the Cell state of the STFL module. The architecture of this module is shown in Fig. 7.\nAs seen from Fig. 7, another single-layer LSTM with n\nmoments (where n is the number of pixels in the feature map) is used here for converting the three inputs into the final soft mask. The structure of this LSTM unit adopts a standard form, whose initial Cell and Hidden states are randomized. The input at the current moment of this LSTM is a sequence of features where each split feature is concatenated with the Hidden state of the STFL module, and the output of this LSTM is concatenated with the input feature of the STFL module. The concatenated feature is passed through a fully-connected layer and a sigmoid layer for obtaining the final soft mask."
        },
        {
            "heading": "C. STAN-OSFGR",
            "text": "In this subsection, we introduce the STAN-OSFGR method for handling the OSFGR task, which integrates the proposed STAN with a linear classifier (i.e., the classifier CST shown in Fig. 3). The STAN-OSFGR method employs a joint classification loss function for training and a threshold-based strategy for inference as follows:\nLoss. The used joint classification loss function contains two loss terms, including a spatial-feature classification loss term LS and a spatial-temporal-feature classification loss term LST , as shown in Fig. 3:\n- LS : The spatial-feature classification loss term is used for measuring the difference between the predictions of the classifier CS in the self-attention backbone module and the groundtruths. It encourages the backbone module to learn a sequence of features under the spatial selfattention mechanism. Here, LS employs the traditional cross-entropy loss as:\nLS = \u2212 1\nN N\u2211 i=1 logpi (1)\nwhere N is the batch size, and pi is the classification probability obtained by implementing the SoftMax layer for the classification scores outputted from the classifier CS . - LST : The spatial-temporal-feature classification loss term is used for measuring the difference between the predictions of the introduced classifier CST and the groundtruths. It encourages the STFL module to learn fine-grained spatial-temporal feature representations. Here, LST also employs the traditional crossentropy loss as:\nLST = \u2212 1\nN N\u2211 j=1 logqj (2)\nwhere qj is the corresponding classification probability calculated from the classification scores which are outputted from the classifier CST .\nHence, the total joint classification loss function is the weighted sum of LS and LST :\nL = LS + \u03bbLST (3)\nwhere \u03bb is a hyperparameter for balancing the two loss terms. Inference. The classification scores outputted from the classifier CST are used for inference. Specifically, the maximum value of the logit vector obtained from a testing image is used as a score for detecting unknown classes as well as classifying known classes:\nscore = max k {fk} (4)\nwhere fk is the k-th (k = 1, 2, ...,K) value of the logit vector outputted from the classifier CST , and K is the number of known classes.\nThen, if the score is larger than a threshold \u03b8, the testing image will be recognized as one of the known classes; otherwise, it will be recognized as unknown classes. Besides, in the former case, the argmax index in the logit vector indicates the predicted label:\nprediction = { argmaxk\u2208{1,2,...,K}fk if score > \u03b8 unknown classes if score \u2264 \u03b8 (5)"
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": "In this section, we firstly introduce the benchmark datasets and the evaluation metrics. Then, we introduce the implementation details. Next, we conduct comparative experiments for evaluating the proposed method. Then, we analyze the\nhyperparameter in the total loss. Next, we conduct two ablation studies. Lastly, we end up with visualization."
        },
        {
            "heading": "A. Benchmark Datasets and Evaluation Metrics",
            "text": "1) Benchmark Datasets: In order to comprehensively evaluate the proposed STAN-OSFGR method, we conduct experiments under both the fine-grained and coarse-grained settings as follows:\nFor the fine-grained setting, our method is evaluated under two configurations: the standard-dataset configuration where the known-class images and unknown-class images are taken from the same dataset and the cross-dataset configuration where the known-class images and unknown-class images are taken from different datasets. The experiments under both the configurations are conducted based on three public datasets, including the CUB dataset [3], the Aircraft dataset [44], and the Stanford-Cars dataset [55]. The numbers of the split known/unknown classes in these datasets under the two configurations are listed in Table I. The standard-dataset configuration is deployed based on these datasets in the following manner:\n- CUB: The Caltech-UCSD Birds (CUB) dataset [3] contains 200 categories of bird images with label and attribute tags. Here, we use CUB-200-2011 version of this dataset as done in the state-of-the-art work [2] for handling the OSFGR task: 100 classes are selected as the known classes, while the rest as the unknown classes. Three difficulty modes are set for the unknown classes based on the attribute similarity between each unknown class and the known classes. Generally, the unknown classes in the \u2018Easy\u2019 mode are more unlike the known classes thus can be relatively easily distinguished from the known classes, the unknown classes in the \u2018Hard\u2019 mode are more similar to the known classes, thus making it more difficult for detecting semantic novelty, and the similarity and detection difficulty of the \u2018Medium\u2019 mode is between the \u2018Easy\u2019 mode and the \u2018Hard\u2019 mode. We use the same data splits with that in [2]. - Aircraft: The FGVC-Aircraft-2013b (Aircraft) dataset [44] is another commonly used benchmark dataset in\nthe fine-grained image classification task, and is also with both label and attribute tags. Its labels contain three levels: the manufacturer level with 30 categories, the family level with 70 categories, and the variant level with 100 categories. We choose the level of variants that consists of 100 categories of aircraft images, 50 classes of which are selected as the known classes, and the rest classes are further split into the three difficulty modes of unknown classes. We use the same data splits as done in [2]. - Stanford-Cars: The Stanford-Cars dataset [55] contains 196 categories of car images, whose labels depend on Make, Model, and Year. we design the following splitting manner for further evaluating the proposed method on this dataset: the first 98 classes are selected as the known classes, and the rest 98 classes are used as the unknown classes.\nThe cross-dataset configuration is deployed by taking the 50 known classes in the Aircraft dataset as the known classes, and taking all of the 200/196-class testing images in the CUB/Stanford-Cars dataset as the unknown-class images respectively.\nFor the coarse-grained setting, the proposed method is evaluated on the CIFAR+10/+50 and TinyImageNet datasets under a same data splitting manner as done in conventional OSR methods [10, 30, 37]. The split numbers of known/unknown classes are also listed in Table I.\n- CIFAR+10/+50: The CIFAR+10/+50 dataset is constructed from the two commonly-used datasets, i.e., CIFAR10 [19] and CIFAR100 [20], which contain 10 and 100 categories of natural images, respectively. The whole 10 classes of the CIFAR10 dataset are used as the known classes, while 10 and 50 classes of the CIFAR100 dataset are chosen as the unknown classes for CIFAR+10 and CIFAR+50. - TinyImageNet: The TinyImageNet contains 200 categories of natural images, Here, 20 classes of the ImageNet dataset [21] are chosen as the known classes, and non-overlapping 180 classes as the unknown classes.\n2) Evaluation Metrics: In this work, we adopt four evaluation metrics: the area under the receiver operating characteristic curve (AUROC) for evaluating the performance of open-set detection, the top-1 accuracy (ACC) for evaluating the performance of closed-set classification, the Open Set Classification Rate (OSCR) [56] for measuring the trade-off between accuracy and open-set detection rate, and the macroF1 score for measuring the performance of both open-set detection and closed-set classification simultaneously. The first two metrics (i.e., AUROC and ACC) are used for the coarsegrained setting as done in [10, 30, 34, 37, 40]. The first three metrics i.e., ACC, AUROC, and OSCR are used for the fine-grained setting under the standard-dataset configuration as done in [2, 10, 32, 40]. And the fourth metric (i.e., macro-F1 score), which takes the unknown-classes as an additional class, is used for the fine-grained setting under the cross-dataset configuration as done in [1, 10, 40]. B. Implementation Details In our work, the image size in the CUB and Aircraft datasets is set to 448 \u00d7 448, and the image size in the rest datasets is set to 224 \u00d7 224. The Swin-B is used as the backbone, which is pretrained on the ImageNet-22K dataset. The AdamW [57] optimizer with weight decay of 0.01 and learning rate of 5 \u00d7 10\u22125 is used for training the self-attention backbone module, and the SGD optimizer with weight decay of 1\u00d710\u22124 and initial learning rate of 1 \u00d7 10\u22123 is used for training the rest parts of the network."
        },
        {
            "heading": "C. Comparative Evaluation",
            "text": "1) Evaluation Under the Fine-Grained Setting: As indicated in Sec. II that only a few works (e.g., CAMV and Cross-Entropy+) have been evaluated on fine-grained datasets, in order to make a comprehensive comparison, we evaluate the proposed method in comparison to the referred CAMV\nand Cross-Entropy+, as well as seven state-of-the-art OSR methods (OpenHybrid, OpenGAN, ARPL, GCPL, GMVAEOSR, MoEP-AE-OSR, and Trans-AUG) which have not been evaluated in their original papers. The corresponding results on the Aircraft, CUB, and Stanford-Cars datasets under the standard-dataset configuration are reported in Tables II, III, and IV, respectively. The corresponding results under the crossdataset configuration are reported in Table V. The results in these tables are split by a solid line according to transformerbased (swin transformer is used here) or non-transformerbased methods. The results of a vanilla transformer-based method used in Sec. III-A (denoted as Backbone) are added for better comparison, and we also report the results of three stateof-the-art non-transformer-based methods by replacing their backbones with the swin transformer. Four points can be seen\nfrom these tables:\n- Firstly, STAN-OSFGR achieves the best performance under both the standard-dataset configuration and the cross-dataset configuration, and outperforms both the state-of-the-art OSR/OSFGR methods with the same backbone significantly in most cases, indicating that STAN-OSFGR can classify known-class fine-grained images accurately as well as detect unknown-class finegrained images effectively. - Secondly, the results of transformer-based methods are universally better than those of non-transformer-based methods, indicating that the transformer is helpful for further improving the model performance. - Besides, the results of STAN-OSFGR are significantly better than those of Backbone, indicating that STAN improves the performance of the transformer backbone effectively when handling the OSFGR task. - Furthermore, the performance improvements on AUROC are more significant than those on ACC, and such improvements can be observed more obviously on the CUB and Aircraft datasets in a harder mode. These observations indicate that STAN has the ability for detecting semantic novelty, which is one of the most desired properties for handling the OSR task.\n2) Evaluation Under the Coarse-Grained Setting: As the above experiments demonstrate, STAN-OSFGR is effective for discriminating fine-grained objects. Here, we further conduct experiments under the coarse-grained setting for investigating whether the STAN-OSFGR model could still be effective for discriminating coarse-grained objects. The corresponding results are reported in Table VI. As seen from this table, STAN does not outperform other methods significantly and achieves similar performance to MoEP-AE-OSR, mainly because the transformer has learnt accurate attention maps for most of coarse-grained objects and these attentions are effective enough for discriminating coarse-grained images."
        },
        {
            "heading": "D. Analysis of the Hyperparameter in Total Loss",
            "text": "Here, we analyze the influence of the hyperparameter \u03bb in Eqn. (3), which balances the weights of the spatialfeature classification loss LS and the spatial-temporal-feature classification loss LST in the total loss function. The proposed STAN-OSFGR method is implemented with \u03bb = {0, 0.001, 0.01, 0.1, 1, 5, 10, 50, 100} on the CUB dataset in the hard mode, and the testing results (ACC, AUROC, and\nOSCR) are reported in Fig. 8. As seen from this figure, the results vary slightly when \u03bb varies in [0.1, 10], demonstrating that the STAN-OSFGR method is not quite sensitive to the hyperparameter \u03bb."
        },
        {
            "heading": "E. Ablation Studies",
            "text": "Here, we conduct two ablation studies on modules and feature aggregation strategies, respectively. The experiments are all conducted on the CUB dataset.\n1) Ablation Study on the Modules: Firstly, we conduct an ablation study on modules for evaluating the effect of the three explored modules in STAN (i.e., the SFSO module, the STFL module, and the CA module) which are introduced in Sec. III-B, and the results are reported in Table VII. The model only with Module1 represents the Backbone model, the model only with Module1 and Module2 aggregates the sequential features obtained from the SFSO module and uses the aggregated features for discriminating fine-grained objects, the model only with Module1, Module2 and Module3 removes the CA module in STAN-OSFGR, and the model with all of the four modules represents the STAN-OSFGR model. As Table VII shows, all of the three explored modules (the SFSO module, the STFL module, and the CA module) play important roles for handling the OSFGR task, among which the STFL module (i.e., the spatial-temporal feature learning module) contributes significant improvement and the CA module (i.e., the context-aware module) further improves the\nTABLE VIII: Results on the CUB dataset by the proposed model trained with different feature aggregation strategies.\nModel ACC AUROC (Easy/Medium/Hard)\nOSCR (Easy/Medium/Hard)\nModule1-AGG 0.942 0.943/0.883/0.824 0.902/0.851/0.799 Module2-AGG 0.949 0.950/0.867/0.839 0.911/0.839/0.815 Module3-AGG 0.945 0.940/0.892/0.828 0.898/0.858/0.802 STAN-OSFGR 0.959 0.965/0.922/0.860 0.934/0.898/0.841\nBlock1 Block2 Block3 Block4 Original Images\nBackbone\nSTAN-OSFGR\nModels Attention Maps\n\u2460\n\u2461\n\u2462\nBackbone\nSTAN-OSFGR\nBackbone\nSTAN-OSFGR\nBlock1 Block2 Block3 Block4 Original Images\nBackbone\nSTAN-OSFGR\nModels Attention Maps\n\u2460\n\u2461\n\u2462\nBackbone\nSTAN-OSFGR\nBackbone\nSTAN-OSFGR\n(a) known-class examples (b) unknown-class examples\n\u2463 \u2463\nBackbone\nSTAN-OSFGR\nBackbone\nSTAN-OSFGR\nFig. 9: Visualization of the attention maps learnt by both the backbone method (i.e., the swin transformer) and the proposed method for (a) the four known-class images and (b) the four unknown-class images from the Aircraft dataset. All of these images are correctly predicted by STAN-OSFGR. The attentions in red regions are strongest, while the attentions in yellow, green, and blue regions decrease by degrees.\nperformance especially on detecting unknown classes in the \u2018Hard\u2019 mode. These observations indicate that modeling the temporal dependency improves the performance of Backbone for handling the OSFGR task uniformly and significantly, and using the contextual information further improves the ability of the model for discriminating fine-grained objects.\n2) Ablation Study on the Feature Aggregation Strategies: Considering that a temporal aggregation strategy is used in STAN-OSFGR, hence we also conduct another ablation study for comparing against three models that are configured with three different feature aggregation strategies based on STAN: Module1-AGG where the spatial features outputted from the self-attention backbone module are straightforwardly concatenated, Module2-AGG where the sequential features outputted from the SFSO module are concatenated, and Module3-AGG\nwhere the features outputted at all moments from the STFL module are concatenated. The corresponding results are reported in Table VIII. Two points can be seen from this table:\n- STAN-OSFGR achieves significantly better performance than Module1-AGG and Module2-AGG, indicating the importance of temporal aggregation. - The results of STAN-OSFGR are better than those of Module3-AGG, because the aggregation of earlier outputs weakens the effect of the temporal aggregation.\nF. Visualization\nIn order to analyze whether the attention maps learnt by the network have been improved, we take the eight examples from the Aircraft dataset (which have been used in Sec. III-A and all of these images are predicted correctly by STAN-OSFGR)\nas examples, and the corresponding attention maps learnt by two models (Backbone and STAN-OSFGR) are visualized in Fig. 9. As seen from this figure, the attention maps learnt by the higher blocks of the STAN-OSFGR model focus on fine-grained objects more accurately than those learnt by the Backbone model."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this paper, we firstly find that the vision transformer could sometimes not learn accurate attention maps for discriminating fine-grained objects. Addressing this observation, we propose the spatial-temporal attention network, STAN, which performs four modules for learning accurate attentions and fine-grained feature representations: the self-attention module, the spatial feature self-organizing module, the spatial-temporal feature learning module, and the context-aware module. Moreover, the STAN-OSFGR method is designed for handling the OSFGR task with our proposed STAN. Experimental results on both the 3 fine-grained OSR datasets and 2 coarse-grained OSR datasets demonstrate the effectiveness of STAN-OSFGR for handling the OSR tasks, especially for handling the OSFGR task.\nIn the future, we will apply the proposed STAN network in other computer vision tasks where the attention mechanisms play essential roles, such as semantic segmentation, object detection, and monocular depth estimation for learning more effective attention maps as well as feature representations."
        }
    ],
    "title": "Spatial-Temporal Attention Network for Open-Set Fine-Grained Image Recognition",
    "year": 2022
}