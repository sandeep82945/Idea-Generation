{
    "abstractText": "Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to be overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures such as temperature scaling. While temperature scaling is frequently used because of its simplicity, it is often outperformed by modified training schemes. In this work, we identify a specific bottleneck for the performance of temperature scaling. We show that for empirical risk minimizers for a general set of distributions in which the supports of classes have overlaps, the performance of temperature scaling degrades with the amount of overlap between classes, and asymptotically becomes no better than random when there are a large number of classes. On the other hand, we prove that optimizing a modified form of the empirical risk induced by the Mixup data augmentation technique can in fact lead to reasonably good calibration performance, showing that training-time calibration may be necessary in some situations. We also verify that our theoretical results reflect practice by showing that Mixup significantly outperforms empirical risk minimization (with respect to multiple calibration metrics) on image classification benchmarks with class overlaps introduced in the form of label noise.",
    "authors": [
        {
            "affiliations": [],
            "name": "Muthu Chidambaram"
        },
        {
            "affiliations": [],
            "name": "Rong Ge"
        }
    ],
    "id": "SP:be2be27a2a37de4ee2e99b4d36707c38350fa96a",
    "references": [
        {
            "authors": [
                "Mariusz Bojarski",
                "Davide Del Testa",
                "Daniel Dworakowski",
                "Bernhard Firner",
                "Beat Flepp",
                "Prasoon Goyal",
                "Lawrence D. Jackel",
                "Mathew Monfort",
                "Urs Muller",
                "Jiakai Zhang",
                "Xin Zhang",
                "Jake Zhao",
                "Karol Zieba"
            ],
            "title": "End to end learning for self-driving cars, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Jaros\u0142aw B\u0142asiok",
                "Parikshit Gopalan",
                "Lunjia Hu",
                "Preetum Nakkiran"
            ],
            "title": "A unifying theory of distance from calibration, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Muthu Chidambaram",
                "Xiang Wang",
                "Yuzheng Hu",
                "Chenwei Wu",
                "Rong Ge"
            ],
            "title": "Towards understanding the data dependency of mixup-style training",
            "venue": "CoRR, abs/2110.07647,",
            "year": 2021
        },
        {
            "authors": [
                "Jillian M. Clements",
                "Di Xu",
                "Nooshin Yousefi",
                "Dmitry Efimov"
            ],
            "title": "Sequential deep learning for credit risk monitoring with tabular financial data, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Zhipeng Ding",
                "Xu Han",
                "Peirong Liu",
                "Marc Niethammer"
            ],
            "title": "Local temperature scaling for probability calibration",
            "venue": "CoRR, abs/2008.05105,",
            "year": 2020
        },
        {
            "authors": [
                "Haitham A Elmarakeby",
                "Justin Hwang",
                "Rand Arafeh",
                "Jett Crowdis",
                "Sydney Gang",
                "David Liu",
                "Saud H AlDubayan",
                "Keyan Salari",
                "Steven Kregel",
                "Camden Richter"
            ],
            "title": "Biologically informed deep neural network for prostate cancer",
            "venue": "discovery. Nature,",
            "year": 2021
        },
        {
            "authors": [
                "Andre Esteva",
                "Brett Kuprel",
                "Roberto A Novoa",
                "Justin Ko",
                "Susan M Swetter",
                "Helen M Blau",
                "Sebastian Thrun"
            ],
            "title": "Dermatologist-level classification of skin cancer",
            "year": 2017
        },
        {
            "authors": [
                "Andre Esteva",
                "Katherine Chou",
                "Serena Yeung",
                "Nikhil Naik",
                "Ali Madani",
                "Ali Mottaghi",
                "Yun Liu",
                "Eric Topol",
                "Jeff Dean",
                "Richard Socher"
            ],
            "title": "Deep learning-enabled medical computer vision",
            "venue": "NPJ digital medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Sorin Grigorescu",
                "Bogdan Trasnea",
                "Tiberiu Cocias",
                "Gigel Macesanu"
            ],
            "title": "A survey of deep learning techniques for autonomous driving",
            "venue": "Journal of Field Robotics,",
            "year": 2020
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,",
            "year": 2017
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CoRR, abs/1512.03385,",
            "year": 2015
        },
        {
            "authors": [
                "Xiaoqian Jiang",
                "Melanie Osl",
                "Jihoon Kim",
                "Lucila Ohno-Machado"
            ],
            "title": "Calibrating predictive model estimates to support personalized medicine",
            "venue": "Journal of the American Medical Informatics Association,",
            "year": 2012
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Meelis Kull",
                "Miquel Perell\u00f3-Nieto",
                "Markus K\u00e4ngsepp",
                "Telmo de Menezes e Silva Filho",
                "Hao Song",
                "Peter A. Flach"
            ],
            "title": "Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with dirichlet calibration",
            "venue": "URL http://arxiv.org/abs/ 1910.12656",
            "year": 1910
        },
        {
            "authors": [
                "Aviral Kumar",
                "Sunita Sarawagi",
                "Ujjwal Jain"
            ],
            "title": "Trainable calibration measures for neural networks from kernel mean embeddings",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Fabian K\u00fcppers",
                "Jan Kronenberger",
                "Amirhossein Shantia",
                "Anselm Haselhoff"
            ],
            "title": "Multivariate confidence calibration for object detection",
            "venue": "In The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Balaji Lakshminarayanan",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Matthias Minderer",
                "Josip Djolonga",
                "Rob Romijnders",
                "Frances Hubis",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Dustin Tran",
                "Mario Lucic"
            ],
            "title": "Revisiting the calibration of modern neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jishnu Mukhoti",
                "Viveka Kulharia",
                "Amartya Sanyal",
                "Stuart Golodetz",
                "Philip H.S. Torr",
                "Puneet K. Dokania"
            ],
            "title": "Calibrating deep neural networks using focal loss, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey Hinton"
            ],
            "title": "When does label smoothing help",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Nixon",
                "Mike Dusenberry",
                "Ghassen Jerfel",
                "Timothy Nguyen",
                "Jeremiah Liu",
                "Linchuan Zhang",
                "Dustin Tran"
            ],
            "title": "Measuring calibration in deep learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Yaniv Ovadia",
                "Emily Fertig",
                "Jie Ren",
                "Zachary Nado",
                "D. Sculley",
                "Sebastian Nowozin",
                "Joshua Dillon",
                "Balaji Lakshminarayanan",
                "Jasper Snoek"
            ],
            "title": "Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas K\u00f6pf",
                "Edward Z. Yang",
                "Zach DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning",
            "venue": "URL http://arxiv.org/abs/1912",
            "year": 1912
        },
        {
            "authors": [
                "Rahul Rahaman",
                "Alexandre H. Thiery"
            ],
            "title": "Uncertainty quantification and deep ensembles, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Sunil Thulasidasan",
                "Gopinath Chennupati",
                "Jeff A Bilmes",
                "Tanmoy Bhattacharya",
                "Sarah Michalak"
            ],
            "title": "On mixup training: Improved calibration and predictive uncertainty for deep neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Deng-Bao Wang",
                "Lei Feng",
                "Min-Ling Zhang"
            ],
            "title": "Rethinking calibration of deep neural networks: Do not be afraid of overconfidence",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yeming Wen",
                "Dustin Tran",
                "Jimmy Ba"
            ],
            "title": "Batchensemble: An alternative approach to efficient ensemble and lifelong learning, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Yeming Wen",
                "Ghassen Jerfel",
                "Rafael Muller",
                "Michael W. Dusenberry",
                "Jasper Snoek",
                "Balaji Lakshminarayanan",
                "Dustin Tran"
            ],
            "title": "Combining ensembles and data augmentation can harm your calibration, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Ross Wightman",
                "Hugo Touvron",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "year": 2021
        },
        {
            "authors": [
                "Saining Xie",
                "Ross B. Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks. CoRR, abs/1611.05431, 2016",
            "venue": "URL http://arxiv. org/abs/1611.05431",
            "year": 2016
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Ciss\u00e9",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk",
            "venue": "minimization. CoRR,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The past decade has seen a rapid increase in the prevalence of deep learning models across a variety of applications, in large part due to their impressive predictive accuracy on unseen test data. However, as these models begin to be applied to critical applications such as predicting credit risk (Clements et al., 2020), diagnosing medical conditions (Esteva et al., 2017; 2021; Elmarakeby et al., 2021), and autonomous driving (Bojarski et al., 2016; Grigorescu et al., 2020), it is crucial that the models are not only accurate but also predict with appropriate levels of uncertainty.\nIn the context of classification, a model with appropriate uncertainty would be correct with a probability that is similar to its predicted confidence \u2013 for example, among samples on which the model predicts a class with 90% confidence, around 90% of them should indeed be the predicted class (a more formal definition is provided in Equation (2.1)).\nAs a concrete (but highly simplified) example, consider the case of applying a deep learning model for predicting whether a patient has a life-threatening illness (Jiang et al., 2012). In this situation, suppose our model classifies the patient as not having the illness but does so with high confidence. A physician using this model for their assessments may then incorrectly diagnose the patient (with potentially grave consequences). On the other hand, if the model had lower confidence in this incorrect prediction, a physician may be more likely to do further assessments.\nObtaining such models with good predictive uncertainty is the problem of model calibration, and has seen a flurry of recent work in the context of training deep learning models (Guo et al., 2017; Thulasidasan et al., 2019; Ovadia et al., 2019; Wen et al., 2020; Minderer et al., 2021). In particular, Guo et al. (2017) showed that the simple technique of temperature scaling \u2013 which introduces only a single parameter to \u201cdampen\u201d the logits of a trained model (defined formally in Section 2) \u2013 is a powerful procedure for calibrating deep learning models.\nar X\niv :2\n30 6.\n00 74\n0v 2\n[ cs\n.L G\n] 2\n9 Se\np 20\n23\nTemperature scaling falls under the class of post-training calibration techniques, which are attractive due to the fact that they can be applied to a black box model without requiring any kind of retraining. However, several empirical works have shown that temperature scaling alone can be outperformed by training-time modifications such as data augmentation (Thulasidasan et al., 2019; M\u00fcller et al., 2020) and regularized loss functions (Kumar et al., 2018; Mukhoti et al., 2020).\nIn this work, we try to understand these empirical observations theoretically by addressing the following question:\nCan we identify reasonable conditions on the data distribution for which temperature scaling provably fails to achieve good calibration, but training-time modifications still succeed?"
        },
        {
            "heading": "1.1 MAIN CONTRIBUTIONS AND OUTLINE",
            "text": "We answer this question in the affirmative, showing that temperature scaling cannot handle data distributions with certain class overlap properties, while training-time modifications can. We first define the notions of calibration and temperature scaling relevant to our work in Section 2, and then motivate conditions on the models we consider in our theory in Section 3.1. Namely, we focus on models that interpolate the training data (i.e. achieve zero training error) and satisfy a local Lipschitz-like condition. We also introduce the Mixup (Zhang et al., 2017) data augmentation, as well as a modification of it necessary for our theoretical results in Section 3.2.\nAfter establishing the necessary background, we show in our main results of Section 4 that for classification tasks in which the supports of different classes have overlaps, the performance of temperature scaling degrades with the amount of overlap between classes, while the performance of our modified Mixup procedure remains robust to such overlaps. The key idea behind our theory is that training-time calibration techniques can significantly constrain model behavior in regions away from the training data points.\nLastly, in Section 5 we show that our theoretical results accurately reflect practice by considering both synthetic data and image classification benchmarks. In Section 5.1 we show that the performance of empirical risk minimization (ERM) combined with temperature scaling quickly degrades on a simple 2-class high-dimensional Gaussian dataset as we increase the overlap between the two Gaussians. Similarly, in Section 5.2 we show that the same phenomenon occurs on standard datasets when we increase the amount of label noise in the data (thereby increasing class overlaps)."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "Calibration in deep learning. The calibration of deep learning models has received significant attention in recent years, largely stemming from the work of Guo et al. (2017) which empirically showed that modern, overparameterized models can have poor predictive uncertainty. Follow-up works (Thulasidasan et al., 2019; Rahaman & Thiery, 2021; Wen et al., 2021) have supported these findings, although the recent work of Minderer et al. (2021) showed that current state-of-the-art architectures can be better calibrated than the previous generation of models.\nMethods for improving calibration. Many different methods have been proposed for improving calibration, including: logit rescaling (Guo et al., 2017), data augmentation (Thulasidasan et al., 2019; M\u00fcller et al., 2020), ensembling (Lakshminarayanan et al., 2017; Wen et al., 2020), and modified loss functions (Kumar et al., 2018; Wang et al., 2021). The logit rescaling methods, namely temperature scaling and its variants (Kull et al., 2019; Ding et al., 2020), constitute perhaps the most applied calibration techniques, since they can be used on any trained model with the introduction of only a few extra parameters (see Section 2). However, we show in this work that this kind of post-training calibration can be insufficient for some data distributions, which can in fact require data augmentation/modified loss functions to achieve good calibration. We focus particularly on Mixup (Zhang et al., 2017) data augmentation, whose theoretical benefits for calibration were recently studied by Zhang et al. (2021) in the context of linear models and Gaussian data. Our results provide a complementary perspective to this prior work, as we address a much broader class of models and a different class of data distributions."
        },
        {
            "heading": "2 THEORETICAL PRELIMINARIES",
            "text": "Notation. We use [k] to denote {1, 2, ..., k} for a positive integer k. We consider k-class classification and use X to denote a dataset of N points (xi, yi) sampled from a distribution \u03c0(X,Y ) whose support supp(\u03c0) is contained in Rd \u00d7 [k]. We use \u03c0X and \u03c0Y to denote the respective marginal distributions of \u03c0, and use \u03c0y to denote the conditional distribution \u03c0(X | Y = y). We use d(A,B) to denote the Euclidean distance between two sets A,B \u2282 Rd, dKL(\u03c01, \u03c02) to denote the KL divergence between two distributions \u03c01 and \u03c02, and \u00b5d for the Lebesgue measure on Rd. For a function g : Rd \u2192 Rk, we use gi to denote the ith coordinate function of g. Lastly, we use \u03d5(\u00b7) to denote the softmax function, i.e. \u03d5i(g(x)) = exp ( gi(x) ) / \u2211 j\u2208[k] exp ( gj(x) ) . In everything that follows, we assume both N and k are sufficiently large, and that N = \u2126(poly(k)) for some large degree polynomial poly(k).\nCalibration. In a classification setting, we say that a model g is calibrated with respect to the ground-truth probability distribution \u03c0 if the following conditional probability condition holds:\nP(Y | \u03d5(g(X))) = \u03d5(g(X)) (2.1)\nEquation (2.1) captures the earlier mentioned intuition that, when our model predicts the probability distribution \u03d5(g(X)) over the classes [k], the true probability distribution for the classes is also \u03d5(g(X)). It is straightforward to translate Equation (2.1) into a notion of miscalibration by considering the expectation of the norm of the difference between the left and right-hand sides. In practice, however, it is common in multi-class classification to focus only on calibration with respect to the top predicted class argmaxy \u03d5\ny(g(X)). This leads to the top-class expected calibration error (ECE), which we define simply as ECE below:\nECE(g) = E(X,Y )\u223c\u03c0 [\u2223\u2223\u2223\u2223\u2223E[Y \u2208 argmaxy\u2208[k] \u03d5y(g(X)) | maxy\u2208[k]\u03d5y(g(X)) = p\u2217]\u2212 p\u2217 \u2223\u2223\u2223\u2223\u2223 ]\n(2.2)\nTop-class ECE is only one of several common notions used to measure calibration, and is known to have several theoretical and empirical drawbacks (B\u0142asiok et al., 2023). In our theoretical work, we opt to instead work with the expected KL divergence EX\u223c\u03c0X [dKL(\u03c0(Y | X), \u00b7)] as our notion of calibration error. Note that such a notion is difficult to estimate in reality as we do not know \u03c0(Y | X). However, we will consider cases where the post-training calibration procedure has access to \u03c0(Y | X). In such cases expected KL divergence is a more accurate characterization as minimizing it implies the full calibration condition of Equation (2.1), while minimizing Equation (2.2) does not (we will only be calibrated with respect to the top class).\nWith a notion of miscalibration in hand, we consider methods to improve the calibration of a trained model g. One of the most popular (and simplest) approaches is temperature scaling (Guo et al., 2017), which consists of introducing a single parameter T that is used to scale the outputs of g. The value of T is obtained by optimizing the negative log-likelihood on a calibration dataset Xcal:\nT = argmin T\u0302\u2208(0,\u221e) \u2212 1 |Xcal| \u2211 (xi,yi)\u2208Xcal log \u03d5yi(g(xi)/T\u0302 ) (2.3)\nFor our results, we will in fact consider an even more powerful (and impractical) form of temperature scaling in which we allow access to the ground-truth distribution \u03c0:\nT = argmin T\u0302\u2208(0,\u221e)\nEX\u223c\u03c0X [ dKL(\u03c0(Y | X), \u03d5Y (g(X)/T\u0302 )) ] (2.4)\nWe will henceforth refer to an optimally temperature-scaled model with respect to Equation (2.4) as gT . We will show in Section 4 that even when we allow this \u201coracle\u201d temperature scaling, we cannot hope to calibrate models g that satisfy some empirically-observed regularity properties."
        },
        {
            "heading": "3 TRAINING APPROACHES",
            "text": ""
        },
        {
            "heading": "3.1 EMPIRICAL RISK MINIMIZATION",
            "text": "In practice, models are often trained via empirical risk minimization (ERM). Due to the large number of parameters in modern models, training often leads to an interpolator that is very confident on every training data point (xi, yi) \u2208 X , as we formalize below:\nDefinition 3.1. [ERM Interpolator] For a dataset X , we say that a model g is an ERM interpolator if for every (xi, yi) \u2208 X there exists a universal constant Ci such that:\nmin s \u0338=yi gyi(xi)\u2212 gs(xi) > log k and max r,s \u0338=yi gs(xi)\u2212 gr(xi) < Ci (3.1)\nEquation (3.1) is slightly stronger than directly assuming \u03d5yi(g(xi)) \u2248 1; however, this type of significant logit separation is commonly observed in practice. Indeed, to further justify Equation (3.1) we train ResNeXt-50 (Xie et al., 2016) models on CIFAR-10, CIFAR-100, and SVHN and examine the means of the max and second max logit over the training data; results are shown in Table 1.\nIn addition to the interpolation condition of Definition 3.1, we also constrain our attention to ERM models g that satisfy a mild local-Lipschitz-like condition.\nDefinition 3.2. [\u03b3-Regular] For a point (xi, yi) \u2208 X , letting L be a universal constant, we define:\nB\u03b3(xi) = {x \u2208 Rd : \u2225xi \u2212 x\u2225 \u2264 \u03b3} (3.2) G\u03b3(xi) = {x \u2208 Rd : |gyi(xi)\u2212 gyi(x)| \u2264 L\u03b3} (3.3)\nWe say that a model g is \u03b3-regular over a set U if there exists a class y \u2208 [k] and \u0398(\u03c0X(U)N) points (xi, y) \u2208 X with xi \u2208 U such that \u03c0y(X \u2208 G\u03b3(xi) | X \u2208 B\u03b3(xi)) \u2265 1\u2212O(1/k).\nBasically, Definition 3.2 codifies the idea that the model logit gyi does not change much in a small enough neighborhood of each xi over a given set U , with high probability. Experiments providing empirical justification for this assumption are provided in Appendix B.1. We will show in Theorem 4.5 that satisfying Definitions 3.1 and 3.2 is sufficient for poor calibration for a wide class of data distributions, even when using temperature scaling with access to the ground truth distribution oracle."
        },
        {
            "heading": "3.2 MIXUP",
            "text": "In contrast, we can show that if we consider models minimizing a Mixup-like training objective instead of the usual negative log-likelihood in Equation (2.3), we can prevent major calibration issues.\nLet D\u03bb denote a continuous distribution supported on [0, 1] and let zi,j(\u03bb) = \u03bbxi + (1\u2212 \u03bb)xj (using zi,j when \u03bb is clear from context) where (xi, yi), (xj , yj) \u2208 X . Then we may define the empirical Mixup cross-entropy Jmix(g,X ,D\u03bb) as:\nJmix(g,X ,D\u03bb) = \u2212 1\nN2 \u2211 i\u2208[N ] \u2211 j\u2208[N ] E\u03bb\u223cD\u03bb [\u03bb log \u03d5yi(g(zi,j)) + (1\u2212 \u03bb) log \u03d5yj (g(zi,j))] (3.4)\nEssentially, minimizing Equation (3.4) forces a model to linearly interpolate between its predictions \u03d5yi(g(xi)) and \u03d5yj (g(xj)) over the line segment connecting the points xi and xj . This already provides some intuition for why Mixup-optimal models can be better calibrated: their predictions can change quickly as one moves away from the training data, avoiding issues that stem from \u03b3-regularity of the logits combined with interpolation.\nHowever, the line segment constraints of Jmix(g,X ,D\u03bb) will not be enough to make this intuition rigorous when the data is in Rd with d > 1, since in this case line segments are measure zero sets with respect to \u00b5d. We will thus augment Mixup to work with convex combinations of d+ 1 points as opposed to two, and refer to this new objective as d-Mixup.\nIn generalizing from Mixup to d-Mixup, it is helpful from a theoretical standpoint to constrain the set of allowed mixings Md(X ) \u2282 [N ]d+1. We will consider only mixing points at most some constant distance away from one another, and we will also preclude mixing points that are too highly\ncorrelated.1 The precise definition of Md(X ) can be found in Definition A.1 of Appendix A.2; we omit it here due to its technical nature.\nNow let D\u03bb,d denote a continuous distribution supported on the d-dimensional probability simplex \u2206d \u2282 Rd+1. Defining z\u03c3(\u03bb) = \u2211 j\u2208[d+1] \u03bbjx\u03c3j for \u03bb \u2208 supp(D\u03bb,d) and \u03c3 \u2208 Md(X ), we can define the empirical d-Mixup cross-entropy Jmix,d(g,X ,D\u03bb,d):\nJmix,d(g,X ,D\u03bb,d) = \u2212 1 |Md(X )| \u2211\n\u03c3\u2208Md(X )\nE\u03bb\u223cD\u03bb,d  \u2211 j\u2208[d+1] \u03bbj log \u03d5 y\u03c3j (g(z\u03c3(\u03bb)))  (3.5) We will henceforth use Xmix,d to denote the set of all z\u03c3. The main benefit of introducing the set Md(X ) instead of just generalizing Equation (3.4) to mixing over [N ]d+1 is that it allows us to use a reparameterization trick with which we can characterize the d-Mixup optimal prediction at every mixed point z\u03c3 . We state only an informal version of this result below and defer a formal statement and proof to Appendix A.2. Lemma 3.3. [Informal Optimality Lemma] Every g\u2217 \u2208 arginfg Jmix,d(g,X ,D\u03bb,d) (where the arginf is over all extended Rd-valued functions) satisfies \u03d5y(g\u2217(z)) = \u03bey(z)/ \u2211 s\u2208[k] \u03bes(z) for almost every z\u03c3 \u2208 Xmix,d, where \u03bey(z) corresponds to the expected weight of class y points over all mixing sets \u03c3 \u2208 Md(X ) from which we can obtain z.\nWe note that this lemma is analogous to Lemma 2.3 in the work of Chidambaram et al. (2021), but avoids restrictions on the function class being considered and is non-asymptotic. Since we can characterize optimal predictions over Xmix,d, we can define d-Mixup interpolators as follows. Definition 3.4. [d-Mixup Interpolator] For a dataset X , we say that g is a d-Mixup interpolator if \u03d5y(g(z)) = \u03d5y(g\u2217(z)) \u00b1 O(1/k) for almost every z \u2208 Xmix,d and y \u2208 [k], with g\u2217 \u2208 arginfg Jmix,d(g,X ,D\u03bb,d).\nIn Theorem 4.6, we will show that d-Mixup interpolators can achieve good calibration on a subclass of distributions for which ERM interpolators perform poorly. Remark 3.5. In practice it is unreasonable to mix d + 1 points when d is large. However, we conjecture that due to the structure of practical models (i.e. neural networks), even mixing two points as in traditional Mixup is sufficient for achieving neighborhood constraints like those induced by d-Mixup. We introduce d-Mixup because we make no such structural assumptions in our theory."
        },
        {
            "heading": "4 MAIN THEORETICAL RESULTS",
            "text": "In this section, we show that even for simple data distributions, ERM interpolators can prevent temperature scaling from producing well-calibrated models, while modifications in the training process (Mixup) can potentially address this issue. Prior to proving our main results, we begin first with a 1-dimensional example that contains the key ideas of our analysis. The full proofs of all results in this section can be found in Appendix A."
        },
        {
            "heading": "4.1 WARM-UP: A SIMPLE 1-D EXAMPLE",
            "text": "Definition 4.1. [\u03b1-Overlapping Intervals] Let \u03c4(y) denote the parity of a nonnegative integer y and let \u03b2y = \u230a(y \u2212 1)/2\u230bk + \u03b1\u03c4(y \u2212 1) for y \u2208 [k], where \u03b1 \u2208 [0, 1] is a parameter of the distribution. Then we define \u03c0(X,Y ) to be the distribution on R \u00d7 [k] such that \u03c0Y is uniform over [k] and \u03c0(X | Y = y) is uniform over [\u03b2y, \u03b2y + 1].\nDefinition 4.1 corresponds to a distribution in which consecutive class-conditional densities are supported on overlapping intervals (whose overlap is determined by the parameter \u03b1) with a spacing of k between each pair of classes (see Figure 1). The spacing of k between pairs of classes is introduced only to simplify the d-Mixup analysis; it is not necessary for proving the negative results regarding ERM interpolators, and will not feature when we generalize to Definition 4.4.\n1This means we do not mix points with themselves in d-Mixup; however, when \u03c0X has a density, this makes little difference since we can mix in a neighborhood of any point.\nOur first result shows that when considering ERM interpolators for distributions of the type in Definition 4.1, so long as the interpolators satisfy \u03b3-regularity for sufficiently large \u03b3, they will be poorly calibrated in the overlapping regions of the data distribution. Proposition 4.2. Let X consist of N i.i.d. draws from the distribution \u03c0 specified in Definition 4.1, with a parameter \u03b1. Then with probability at least 1\u2212 k exp(\u2212\u2126(N/k)) over the randomness of X , the set S of all models g that are ERM interpolators for X and k/(4N)-regular over each overlapping region in supp(\u03c0X) is non-empty (in fact, uncountable). Furthermore, the predictive distribution \u03c0\u0302T (Y | X) = \u03d5Y (gT (X)) of the optimally temperature-scaled model gT for any g \u2208 S satisfies:\nEX\u223c\u03c0X [dKL(\u03c0(Y | X), \u03c0\u0302T (Y | X))] \u2265 \u0398((1\u2212 \u03b1\u2212 1/k) log k) (4.1) Thus, for \u03b1 = O(1), even with oracle temperature scaling every g \u2208 S is asymptotically no better than random. In contrast, as the separation \u03b1 \u2192 1, the bound in Equation (4.1) becomes vacuous.\nProof Sketch. We can show that S is non-trivial using Chernoff bound arguments, and then use k/(4N)-regularity to show that there is a significant fraction of supp(\u03c0X) on which every g \u2208 S predicts incorrect probabilities. The key idea is then that temperature scaling will only improve incorrect predictions for ERM interpolators to uniformly random (i.e. 1/k), whereas the correct prediction in an overlapping region is 1/2 for each of the overlapping classes.\nOn the other hand, for d-Mixup, each point in the overlapping regions can be obtained as a mixture of points from the overlapping classes, so we will have non-trivial probabilities for both classes. Proposition 4.3. Let X be as in Proposition 4.2 and p(k) denote a polynomial in k of degree at least one. Then taking D\u03bb,1 to be uniform, every 1-Mixup interpolator g for X with the property that \u03d5y(g(x)) \u2264 1\u2212 \u2126(1/p(k)) for every x \u2208 supp(\u03c0X) \\ Xmix,1 and y \u2208 [k] satisfies with probability at least 1\u2212 k3 exp ( \u2212\u2126(N/k3) ) :\nEX\u223c\u03c0X [dKL(\u03c0(Y | X), \u03c0\u0302(Y | X))] \u2264 \u0398(1) (4.2) Note that this result is independent of the separation parameter \u03b1.\nProof Sketch. We can show with high probability that Xmix,1 covers most of supp(\u03c0X) uniformly, and then we can use Lemma 3.3 to precisely characterize the 1-Mixup predictions over Xmix,1. The added stipulation that \u03d5y(g(x)) \u2264 1\u2212\u2126(1/poly(k)) is necessary, since we cannot hope to prove an upper bound if g is allowed to behave arbitrarily on supp(\u03c0X) \\ Xmix,1, and we also expect this in practice due to the regularity of models considered. A key takeaway from Proposition 4.3 is that the upper bound on the Mixup error we obtain is independent of the parameter \u03b1 of our distribution; we will see that this is also the case in practice in Section 5."
        },
        {
            "heading": "4.2 GENERALIZING TO HIGHER DIMENSIONS",
            "text": "By extending the idea of overlapping regions in supp(\u03c0X) from our 1-D example, we can generalize the failure of \u03b3-regular ERM interpolators to higher-dimensional distributions. Definition 4.4. [General Data Distribution] Given a parameter \u03b1 \u2208 [0, 1], we define \u03c0 to be any distribution whose support is contained in Rd \u00d7 [k] satisfying the following constraints:\n1. (Classes are roughly balanced) \u03c0Y (Y = y) = \u0398(1/k).\n2. (Constant class overlaps) Letting M denote a nonnegative integer constant, there exist \u0398(k) classes y for which there are classes s1(y), s2(y), ..., sm(y) for some 1 \u2264 m \u2264 M with \u03c0y(supp(\u03c0y) \u2229 supp(\u03c0si(y))) \u2265 1\u2212 \u03b1, and all other s\u2032 \u2208 [k] satisfy \u03c0X(supp(\u03c0y) \u2229 supp(\u03c0s\u2032)) = 0.\n3. (Overlap density is proportional to measure) \u03c0y(X \u2208 A) = \u0398(\u00b5d(A)) and \u03c0si(y)(X \u2208 A) = \u0398(\u00b5d(A)) for every A \u2286 supp(\u03c0y) \u2229 supp(\u03c0si(y)).\nDefinition 4.4 is quite broad in that we make no assumptions on the behavior of the class-conditional densities outside of the overlapping regions. We now generalize Proposition 4.2. Theorem 4.5. Let X consist of N i.i.d. draws from any distribution \u03c0 satisfying Definition 4.4, and let r \u2208 R be such that the sphere with radius r in Rd has volume k/(2MN). Then the result of Proposition 4.2 still holds for the set Sd of ERM interpolators for X which are r-regular over each overlapping region in supp(\u03c0X).\nTo generalize Proposition 4.3, however, we need further restrictions on \u03c0. Mainly, we need to have significant spacing between non-overlapping classes (as in Definition 4.1), and we need to restrict the class-conditional densities such that mixings in Xmix,d are not too skewed towards a small subset of classes. The precise formulation of this assumption can be found in Appendix A.2. Theorem 4.6. Let X consist of N i.i.d. draws from any distribution \u03c0 satisfying Definition 4.4 and Assumption A.3, and let p(k) be as in Proposition 4.3. Then the result of Proposition 4.3 still holds when considering d-Mixup interpolators g for X where the mixing distribution D\u03bb,d is uniform over the d-dimensional probability simplex."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We now verify that the theoretical phenomenon of ERM calibration performance decreasing with distributional overlap also manifests in practice. For all of the experiments in this section, we train ResNeXt-50 models (Xie et al., 2016) due to the continued use of the ResNet (He et al., 2015) family of models as strong baselines (Wightman et al., 2021), and we report the mean performance over 5 random initializations with 1 standard deviation error bounds. For metrics, we report negativelog-likelihood (NLL), ECE with binning using 15 uniform bins (as was done by Guo et al. (2017)), and also adaptive calibration error (ACE) (Nixon et al., 2020) which is ECE but computed using equal-weight bins. We also use confidence histograms and reliability diagrams to visualize calibration. The former corresponds to histograms of the predicted probabilities for the correct labels, while the latter correspond to binning the top-class probability predictions and plotting the average accuracy in each bin. These visualizations are generated using the calibration library of K\u00fcppers et al. (2020).\nAll models were trained for 200 epochs using Adam (Kingma & Ba, 2015) with the standard hyperparameters of \u03b21 = 0.9, \u03b22 = 0.999, a learning rate of 0.001, and a batch size of 500 on a single A5000 GPU using PyTorch (Paszke et al., 2019). We did not use any additional training heuristics such as Dropout (Srivastava et al., 2014). In preliminary experiments, we found that minor changes to these hyperparameters did not affect our experimental results so long as the training horizon was sufficiently long (so as to achieve interpolation of training data). For each training dataset considered in this section, we set aside 10% of the data for calibration."
        },
        {
            "heading": "5.1 SYNTHETIC DATA",
            "text": "We first consider a synthetic data model which closely resembles Definition 4.1: binary classification on overlapping Gaussian data. Namely, we consider class 1 points to be drawn from N (0, Id300) (i.e. 300-dimensional mean zero Gaussian), and class 2 to be drawn from N (\u00b5, Id300). The probability of overlap between the two classes is controlled by the choice of the mean \u00b5 of class 2.\nWe consider \u00b5 \u2208 {0.25 \u2217 1, 0.05 \u2217 1, 0.01 \u2217 1} (where 1 here denotes the all-ones vector in R300) to cover a range of overlaps (0.05 corresponds to roughly 1/ \u221a d here). We sample 4000 training data points and 1000 test data points according to the aforementioned distribution, with class 1 and class 2 being chosen uniformly.\nWe compare the performance of ERM with temperature scaling (TS) to that of Mixup without temperature scaling (which means the set aside calibration data is not used). Our implementation of temperature scaling follows that of Guo et al. (2017). For Mixup, we consider the usual Mixup formulation with uniform mixing distribution as well as d-Mixup with d = 2, 4 (i.e. mixing 3 and 5 points respectively) with uniform mixing (although we do not enforce the technical conditions in Definition A.1 for d-Mixup). Results are reported in Table 2.\nOur results are what we would expect from our theory \u2013 the NLL of the ERM model increases sharply as we increase the distributional overlap, while the Mixup models have comparatively small increases in NLL. Additionally, we find that the d-Mixup models perform better than the ERM and regular Mixup models as we increase the amount of overlap. Of note is the fact that this occurs even though this synthetic dataset consists of only 2 classes, suggesting that it is perhaps possible to improve our theoretical observations to a non-asymptotic setting with more precise analysis.\nA reasonable concern with comparing model NLL is that it can correlate strongly with generalization performance. However, we can verify that in fact the underlying issue is one of calibration and not of generalization by comparing confidence histograms and reliability diagrams for the models being considered. Figure 2 compares the ERM + TS model to the 4-Mixup model; we see that the average top-class confidence for the 4-Mixup model concentrates around 0.5 (as desired for high overlap), while the ERM confidence is bimodal at 0 and 1 (i.e. extremely overconfident, even when making mistakes), and that the test accuracy of both models is roughly the same. Also, the observed underconfidence at lower probabilities for both models can be attributed to the lack of predictions at those probabilities. Additional plots for other models and levels of separation can be found in Appendix B.2.\nWe observe a similar phenomenon to that of the NLL results when comparing model ECE, although we note that the ACE performance remains roughly the same across different overlap levels (this is outside the scope of our theory, since this is a byproduct of the binning procedure used for ACE). The main takeaway from this set of experiments is that Mixup, and particularly the d-Mixup variants, have\na significant regularization effect on model confidence. This regularization effect is more pronounced as we increase d (as we would expect), but comes at a trade-off of additional computational overhead (computing convex combinations of d+ 1 points per batch)."
        },
        {
            "heading": "5.2 IMAGE CLASSIFICATION BENCHMARKS",
            "text": "We can also verify that the phenomena observed in synthetic data translates to the more realistic benchmarks of CIFAR-10, CIFAR-100, and SVHN. To artificially introduce (more) overlaps in the datasets, we add label noise to the training data. In order to follow the style of Definition 4.4, our label noise procedure consists of randomly pairing up each class in the data with another distinct class, and then flipping the label of the original class points to be the label of the paired up class according to the desired level of label noise.\nResults for ERM + TS and Mixup in terms of test NLL are shown in Table 3. Results in terms of ECE and ACE can be found in Appendix B.3; they follow similar trends to NLL (notably, ACE does not remain consistent like it did in the synthetic data case). We also trained d-Mixup models on the same data, but we found that the confidence regularization effect for d > 2 led to underconfidence on these datasets, so we report just the results for Mixup.\nWe see in Table 3 the same behavior we observed in Section 5.1; the NLL for the ERM + TS models jumps quickly with increasing label noise while the Mixup NLL increases comparatively slowly. While in the cases of CIFAR-10 and CIFAR-100 some of this gap can be associated to improved test error of the Mixup models, we find for SVHN that ERM actually outperforms Mixup in terms of test error and still has massive gaps in terms of NLL. Furthermore, we show in Appendix B.3 that Mixup also dominates in terms of ECE and ACE across all datasets and virtually every label noise level. Finally, comparing confidence histograms and reliability diagrams in the manner of Section 5.1 shows the same behavior on the classification benchmarks: ERM + TS confidences cluster around 1 while Mixup confidences are more evenly distributed, contributing to improved calibration."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "The key finding of our work is that when considering interpolating models on data distributions with overlaps, temperature-scaling-type methods alone can be insufficient for obtaining good calibration, both in theory and in practice. We show empirically that such interpolating models have very skewed confidence distributions (i.e. always predicting near 1), which re-affirms earlier empirical findings in the literature and suggests that temperature scaling is forced to push such models to be closer to random (as seen in Theorem 4.5). On the other hand, we also show that minimizing the Mixupaugmented loss instead of the standard empirical risk can lead to good calibration even when the data distribution has significant overlaps. Towards this end, we introduce d-Mixup, and show that mixing more than two points with Mixup can in fact be beneficial for calibration due to the added regularization, particularly in high noise/overlap settings. One clear direction suggested by our work is the idea of developing better neighborhood constraints around training points; we study the constraints introduced by Mixup, but we anticipate there are likely better alternatives for improving calibration that rely on non-linear constraints, and we leave this to future work."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Rong Ge and Muthu Chidambaram are supported by NSF Award DMS-2031849, CCF-1845171 (CAREER), CCF-1934964 (Tripods), and a Sloan Research Fellowship. Muthu would like to thank Kai Xu for helpful discussions during the early stages of this project."
        },
        {
            "heading": "A FULL PROOFS",
            "text": "Here we provide full proofs of all results in Section 4. For convenience, we first recall all of the relevant definitions and assumptions from Sections 2 and 4.\nDefinition 3.1. [ERM Interpolator] For a dataset X , we say that a model g is an ERM interpolator if for every (xi, yi) \u2208 X there exists a universal constant Ci such that:\nmin s \u0338=yi gyi(xi)\u2212 gs(xi) > log k and max r,s \u0338=yi gs(xi)\u2212 gr(xi) < Ci (3.1)\nDefinition 3.2. [\u03b3-Regular] For a point (xi, yi) \u2208 X , letting L be a universal constant, we define:\nB\u03b3(xi) = {x \u2208 Rd : \u2225xi \u2212 x\u2225 \u2264 \u03b3} (3.2) G\u03b3(xi) = {x \u2208 Rd : |gyi(xi)\u2212 gyi(x)| \u2264 L\u03b3} (3.3)\nWe say that a model g is \u03b3-regular over a set U if there exists a class y \u2208 [k] and \u0398(\u03c0X(U)N) points (xi, y) \u2208 X with xi \u2208 U such that \u03c0y(X \u2208 G\u03b3(xi) | X \u2208 B\u03b3(xi)) \u2265 1\u2212O(1/k). Definition 3.4. [d-Mixup Interpolator] For a dataset X , we say that g is a d-Mixup interpolator if \u03d5y(g(z)) = \u03d5y(g\u2217(z)) \u00b1 O(1/k) for almost every z \u2208 Xmix,d and y \u2208 [k], with g\u2217 \u2208 arginfg Jmix,d(g,X ,D\u03bb,d). Definition 4.1. [\u03b1-Overlapping Intervals] Let \u03c4(y) denote the parity of a nonnegative integer y and let \u03b2y = \u230a(y \u2212 1)/2\u230bk + \u03b1\u03c4(y \u2212 1) for y \u2208 [k], where \u03b1 \u2208 [0, 1] is a parameter of the distribution. Then we define \u03c0(X,Y ) to be the distribution on R \u00d7 [k] such that \u03c0Y is uniform over [k] and \u03c0(X | Y = y) is uniform over [\u03b2y, \u03b2y + 1]. Definition 4.4. [General Data Distribution] Given a parameter \u03b1 \u2208 [0, 1], we define \u03c0 to be any distribution whose support is contained in Rd \u00d7 [k] satisfying the following constraints:\n1. (Classes are roughly balanced) \u03c0Y (Y = y) = \u0398(1/k).\n2. (Constant class overlaps) Letting M denote a nonnegative integer constant, there exist \u0398(k) classes y for which there are classes s1(y), s2(y), ..., sm(y) for some 1 \u2264 m \u2264 M with \u03c0y(supp(\u03c0y) \u2229 supp(\u03c0si(y))) \u2265 1\u2212 \u03b1, and all other s\u2032 \u2208 [k] satisfy \u03c0X(supp(\u03c0y) \u2229 supp(\u03c0s\u2032)) = 0.\n3. (Overlap density is proportional to measure) \u03c0y(X \u2208 A) = \u0398(\u00b5d(A)) and \u03c0si(y)(X \u2208 A) = \u0398(\u00b5d(A)) for every A \u2286 supp(\u03c0y) \u2229 supp(\u03c0si(y)).\nA.1 PROOFS OF PROPOSITION 4.2 AND THEOREM 4.5\nProposition 4.2. Let X consist of N i.i.d. draws from the distribution \u03c0 specified in Definition 4.1, with a parameter \u03b1. Then with probability at least 1\u2212 k exp(\u2212\u2126(N/k)) over the randomness of X , the set S of all models g that are ERM interpolators for X and k/(4N)-regular over each overlapping region in supp(\u03c0X) is non-empty (in fact, uncountable). Furthermore, the predictive distribution \u03c0\u0302T (Y | X) = \u03d5Y (gT (X)) of the optimally temperature-scaled model gT for any g \u2208 S satisfies:\nEX\u223c\u03c0X [dKL(\u03c0(Y | X), \u03c0\u0302T (Y | X))] \u2265 \u0398((1\u2212 \u03b1\u2212 1/k) log k) (4.1)\nThus, for \u03b1 = O(1), even with oracle temperature scaling every g \u2208 S is asymptotically no better than random. In contrast, as the separation \u03b1 \u2192 1, the bound in Equation (4.1) becomes vacuous.\nProof. We begin by first verifying that the set S is large with high probability. Firstly, there are uncountably many strong ERM interpolators g, since the interpolator constraint is only on the finitely many points in X (so behavior is unconstrained elsewhere on the input space), and with probability 1 these points do not coincide.\nIn order to show that k/(4N)-regularity is feasible over each overlapping region, let us focus on a single class y < k with \u03c4(y) = 1 (i.e. the class y is odd-numbered). In this case, the overlapping region is [\u03b2y + \u03b1, \u03b2y + 1] (see Definition 4.1). By a Chernoff bound, there are \u0398((1\u2212 \u03b1)N/k) class y points in this region with probability 1\u2212 exp(\u2212\u2126((1\u2212 \u03b1)N/k)).\nNow conditioning on each class y point x in the region, the probability that a class y + 1 point falls in a k/(4N)-neighborhood of x is at most 1/(2N) (since the class-conditional density of the class y + 1 points is uniform over [\u03b2y + \u03b1, \u03b2y + 1 + \u03b1], and the class prior is uniform over [k]). Thus, by a union bound the probability that x has no class y + 1 neighbors is at least 1/2. Now by another Chernoff bound, there are \u0398((1\u2212 \u03b1)N/k) class y points with no k/(4N)-neighborhood neighbors with probability at least 1\u2212 exp(\u2212\u2126((1\u2212 \u03b1)N/k)), which allows us to estbalish k/(4N)-regularity over [\u03b2y + \u03b1, \u03b2y + 1]. Repeating this logic for each overlapping region (conditioning appropriately) and taking a union bound shows that the set S is large with probability 1\u2212 k exp(\u2212\u2126((1\u2212 \u03b1)N/k)). With k/(4N)-regularity established, our approach is straightforward: we will lower bound the expected KL divergence by considering just the divergence over the regions consisting of the unions of the regularity neighborhoods, over which temperature scaling can at best bring the softmax outputs to uniform over k (1/k). However, the ground truth conditional distribution over the regularity neighborhoods is uniform over only two classes (since the neighborhoods are in a region of overlap between two classes).\nAs before, for simplicity we will focus on a single class since the argument that follows can be iterated for each subsequent class. To keep notation brief, let us define:\nH\u03c0(\u03c0\u03021A) = E\u03c0(Y |X)[\u22121A log \u03c0\u0302(Y | X)] (A.1)\nIn other words, the cross-entropy restricted to a region A. Now consider a class 1 < y < k with \u03c4(y) = 0 (even parity); over its support [\u03b2y, \u03b2y + 1] we observe that:\nH\u03c0(\u03c01[\u03b2y,\u03b2y+1]) = \u2212 1\nk \u222b \u03b2y+1 \u03b2y \u2211 y\u2208[k] \u03c0(Y = y | X = x) log \u03c0(Y = y | X = x) dx\n= log 2\nk (A.2)\nSo the entropy over the support of class y of the ground truth conditional distribution is a constant divided by k. We will now show that the cross-entropy over the same region for the temperature-scaled predictive distribution \u03c0\u0302T (Y | X) is lower bounded by \u0398((1\u2212 \u03b1) log(k)/k). To do so, we will constrain our attention as mentioned earlier to the entropy over just the regions specified by k/(4N)-regularity. Let U denote the union of all of the k/(4N)-confidence neighborhoods around class y \u2212 1 points in the region [\u03b2y, \u03b2y + 1]. Note that we are focusing on the neighborhoods around class y \u2212 1 points (this corresponds to the odd-numbered class in our discussion above), because in these neighborhoods the class y softmax output will be extremely small (due to the interpolation property applied to g on the class y \u2212 1 points). We have that:\nH\u03c0(\u03c0\u0302T1[\u03b2y,\u03b2y+1]) \u2265 \u2212 1\nk min T>0 \u222b U 1 2 log \u03d5y(g(x)/T ) dx (A.3)\nNow we can lower bound the integrand in Equation (A.3) at each point (xi, y \u2212 1) \u2208 X with xi \u2208 U as:\n\u2212 log \u03d5y(g(xi)/T ) = \u2212 log exp(gy(xi)/T ) exp ( g(y\u22121)(xi/T ) ) + \u2211 s\u0338=(y\u22121) exp(g s(xi)/T )\n\u2265 \u2212 log exp(g y(xi)/T ) exp ( g(y\u22121)(xi/T ) ) + (k \u2212 1) exp((gy(xi)\u2212 Ci)/T )\n(A.4)\nWhere Ci above is from the ERM interpolation property of g. Applying k/(4N)-regularity we can translate the bound in Equation (A.4) to all x \u2208 U at the cost of introducing a O(k/N) correction to g(y\u22121)(xi/T ) and a O(1/k) correction to \u03c0X(U). The former error term will be irrelevant to the calculation (asymptotically), while the latter error term will show up in the final bound. Letting XU denote all points xi \u2208 U with (xi, y \u2212 1) \u2208 X and recalling that \u00b51(U) denotes the 1-dimensional Lebesgue measure of the set U , we then obtain: H\u03c0(\u03c0\u0302T1[\u03b2y,\u03b2y+2]) \u2265\n\u0398 ( \u2212\u00b51(U)\u2212O(1/k)\nk max xi\u2208XU\nlog exp(gy(xi)/T ) exp ( g(y\u22121)(xi/T ) ) + (k \u2212 1) exp((gy(xi)\u2212 Ci)/T ) ) (A.5)\nNow we consider two cases; T < 1 and T \u2265 1. In the former case, since exp ( g(y\u22121)(xi) ) \u2265 k exp(gy(xi)), we have that the term inside the log is bounded above by 1/k. In the latter case, since Ci is a constant, the term inside the log is also bounded above by 1/k. Thus we get:\nH\u03c0(\u03c0\u0302T1[\u03b2y,\u03b2y+1]) \u2265 \u0398 ( (\u00b51(U)\u2212O(1/k)) log k\nk\n) (A.6)\nFinally, we observe that U consists of the union of \u0398((1\u2212 \u03b1)N/k) neighborhoods of size \u0398(k/N), so \u00b51(U) = \u0398(1\u2212\u03b1). Iterating this argument over each of the \u230ak/2\u230b overlapping regions, we obtain the desired result.\nAs mentioned, the proof of the more general result follows the proof of Proposition 4.2 extremely closely, and we thus recommend reading the above proof before proceeding to this one (as we skip some details).\nTheorem 4.5. Let X consist of N i.i.d. draws from any distribution \u03c0 satisfying Definition 4.4, and let r \u2208 R be such that the sphere with radius r in Rd has volume k/(2MN). Then the result of Proposition 4.2 still holds for the set Sd of ERM interpolators for X which are r-regular over each overlapping region in supp(\u03c0X).\nProof. As in the proof of Proposition 4.2, there are uncountably many ERM interpolators for X . Now for showing that there is a subset of such interpolators satisfying uniform r-confidence over overlapping regions with high probability, let us consider a class y whose support overlaps with the supports of other classes s1(y), ..., sm(y) (as defined in Definition 4.4).\nLet A \u2282 supp(\u03c0y) denote the overlap between the support of y and an arbitrary one of the aforementioned classes si(y). As before, by a Chernoff bound, there are \u2126((1\u2212 \u03b1)N/k) class y points falling in A with probability at least 1\u2212 exp(\u2212\u2126((1\u2212 \u03b1)N/k)), since by assumption \u03c0y(A) \u2265 1\u2212 \u03b1. Conditioning on each class y point x in A, the probability that a point from classes s1(y), ..., sm(y) falls in an r-neighborhood of x is at most m/(2MN). As before, this implies by a union bound and another Chernoff bound that there are \u0398((1\u2212 \u03b1)N/k) class y points in the overlapping region with no r-neighborhood neighbors with probability at least 1\u2212 exp(\u2212\u2126((1\u2212 \u03b1)N/k)). Repeating this logic and taking another union bound, it follows that the set S is large with probability at least 1\u2212 k exp(\u2212\u2126((1\u2212 \u03b1)N/k)). The rest of the proof carries over similarly from the proof of Proposition 4.2. Let y be any class with overlaps, as considered above. Similarly, let s be any class overlapping with y whose region of overlap we denote as A, once again as above. Defining H\u03c0(\u00b7) as in Equation (A.1), we see that:\nH\u03c0 ( \u03c01supp(\u03c0s) ) = \u0398\n( 1\nk\n) (A.7)\nNow letting U denote the union of all r-neighborhoods around class y points in the overlapping region A, we also get:\nH\u03c0(\u03c0\u0302T1supp(\u03c0s)) \u2265 \u2212\u0398 ( 1\nk min T>0 \u222b U log \u03d5s(g(x)/T ) dx ) (A.8)\nAs in the proof of Proposition 4.2, we can lower bound the integrand in Equation (A.8) at each point (xi, y) \u2208 X with xi \u2208 U as:\n\u2212 log \u03d5s(g(xi)/T ) \u2265 \u2212 log exp(gs(xi)/T )\nexp(gy(xi/T )) + (k \u2212 1) exp((gs(xi)\u2212 Ci)/T ) (A.9)\nOnce again letting XU denote all points xi \u2208 U with (xi, y) \u2208 X , we obtain from r-regularity: H\u03c0(\u03c0\u0302T1\u03c0s) \u2265 \u0398 ( \u2212\u00b5d(U)\u2212O(1/k)\nk max xi\u2208XU\nlog exp(gs(xi)/T )\nexp(gy(xi/T )) + (k \u2212 1) exp((gs(xi)\u2212 Ci)/T ) ) (A.10)\nWhich, by identical logic to the proof of Proposition 4.2, gives: H\u03c0(\u03c0\u0302T1\u03c0s) \u2265 \u0398 ( (\u00b5d(U)\u2212O(1/k)) log k\nk\n) (A.11)\nObserving again that \u00b5d(U) = \u2126(1\u2212 \u03b1) and applying this logic to all \u0398(k) overlapping regions in supp(\u03c0X), we obtain the desired result.\nA.2 MIXUP OPTIMALITY LEMMA AND PROOFS OF PROPOSITION 4.3 AND THEOREM 4.6\nFirst, we provide a proper definition of the mixing set Md(X ), which was intuitively described in Section 3.2 of the main paper. Definition A.1. Given a dataset X , we define:\nMd(X ) = { \u03c3 \u2208 [N ]d+1 : x\u03c3i \u2212 x\u03c3d+1 \u0338= 0 \u2200i \u2208 [d]\nand\n\u2329 x\u03c3i \u2212 x\u03c3d+1 , x\u03c3j \u2212 x\u03c3d+1 \u232a\u2225\u2225x\u03c3i \u2212 x\u03c3d+1\u2225\u2225\u2225\u2225x\u03c3j \u2212 x\u03c3d+1\u2225\u2225 \u2264 12d \u2200i, j \u2208 [d], i \u0338= j and\n\u2225\u2225x\u03c3i \u2212 x\u03c3j\u2225\u2225 \u2264 C(X ) \u2200i, j \u2208 [d+ 1]} Where C(X ) is a possibly data-dependent constant.\nWe recall that the definition of Xmix,d is all z such that z = \u2211\nj\u2208[d+1] \u03bbjx\u03c3j for some \u03bb \u2208 supp(D\u03bb,d) and \u03c3 \u2208 Md(X ). Now we prove a formal version of Lemma 3.3, which we will rely on in the proofs of Proposition 4.3 and Theorem 4.6. Lemma A.2. Let f\u03bb denote the density of D\u03bb,d and define:\nL\u03c3 = [ x\u03c31 \u2212 x\u03c3d+1 x\u03c32 \u2212 x\u03c3d+1 . . . x\u03c3d \u2212 x\u03c3d+1 ] (A.12)\n\u039b\u03c3(z) = L\u22121\u03c3 (z \u2212 x\u03c3d+1) 1\u2212 \u2211 j\u2208[d] L\u22121\u03c3 (z \u2212 x\u03c3d+1)j  (A.13) \u03bey(z) =\n\u2211 \u03c3\u2208Md(X ) 1z\u2208conv({x\u03c3i} d+1 i=1 ) \u2223\u2223det(L\u22121\u03c3 )\u2223\u2223f\u03bb (\u039b\u03c3(z)) \u2211 j: y\u03c3j=y \u039b\u03c3(z)j (A.14)\nThen any g\u2217 \u2208 arginfg Jmix,d(g,X ,D\u03bb,d) satisfies \u03d5y(g\u2217(z)) = \u03bey(z)/ \u2211\ns\u2208[k] \u03bes(z) for almost every z \u2208 Xmix,d.\nProof. First, writing out the expectation in Equation (3.5), we have:\nJmix,d(g,X ,D\u03bb,d) = \u2212 1 |Md(X )| \u2211\n\u03c3\u2208Md(X )\n\u222b \u2206d \u2211 i\u2208[d+1] \u03bbi log \u03d5 y\u03c3i g  \u2211 j\u2208[d+1] \u03bbjx\u03c3j  f(\u03bb) d\u03bb (A.15)\nNow we make the substitution z = \u2211\nj\u2208[d+1] \u03bbjx\u03c3j . By the definition of Md(X ), the transformation L\u03c3 in Equation (A.12) is invertible, so \u039b\u03c3(z) from Equation (A.13) is well-defined. Now applying Fubini\u2019s Theorem, we may write:\n\u2113(\u03c3, z) = \u2212 \u2223\u2223det(L\u22121\u03c3 )\u2223\u2223f\u03bb (\u039b\u03c3(z)) \u2211\nj\u2208[d+1]\n\u039b\u03c3(z)j log \u03d5 y\u03c3j (g(z)) (A.16)\nJmix,d(g,X ,D\u03bb,d) = 1\n|Md(X )| \u222b Xmix,d \u2211 \u03c3\u2208Md(X ) 1z\u2208conv({x\u03c3i} d+1 i=1 ) \u2113(\u03c3, z) dz (A.17)\nWe observe that \u2211\n\u03c3\u2208Md(X ) 1z\u2208conv({x\u03c3i} d+1 i=1 ) \u2113(\u03c3, z) is strictly convex as a function of \u03d5(g(z)). Now we can optimize the aforementioned term as a function of \u03d5(g(z)) under the constraint that\u2211\ny\u2208[k] \u03d5 y(g(z)) = 1. This is straightforward to do with Lagrange multipliers, and the solution is of the form \u03d5y(g(z)) = \u03bey(z)/ \u2211\ns\u2208[k] \u03bes(z) where \u03bey(z) is defined as in Equation (A.14), so the result follows.\nWith Lemma A.2, we can prove Proposition 4.3. Proposition 4.3. Let X be as in Proposition 4.2 and p(k) denote a polynomial in k of degree at least one. Then taking D\u03bb,1 to be uniform, every 1-Mixup interpolator g for X with the property that \u03d5y(g(x)) \u2264 1\u2212 \u2126(1/p(k)) for every x \u2208 supp(\u03c0X) \\ Xmix,1 and y \u2208 [k] satisfies with probability at least 1\u2212 k3 exp ( \u2212\u2126(N/k3) ) :\nEX\u223c\u03c0X [dKL(\u03c0(Y | X), \u03c0\u0302(Y | X))] \u2264 \u0398(1) (4.2)\nNote that this result is independent of the separation parameter \u03b1.\nProof. We first partition supp(\u03c0X) into subregions each of which have measure \u0398(1/k2) (the reason for this choice will be made clear shortly), so that there are \u0398(k3) total subregions. By a Chernoff bound, there are \u0398(N/k3) points in each of these subregions with probability at least 1 \u2212 exp ( \u2212\u2126(N/k3) ) . Thus, by a union bound, the probability that every subregion contains\n\u0398(N/k3) points is at least 1 \u2212 k3 exp ( \u2212\u2126(N/k3) ) . Consequently, with the same probability,\n\u03c0X(supp(\u03c0X) \\ Xmix,d) = O(1/k2). To see this, observe that for z to be in Xmix,d, we need only have one point to the left of z and one point to the right of z, both within a constant distance of one another (note that the other conditions in Md(X ) are vacuously satisfied since we are in dimension 1). By our high probability arguments, there is at most some O(1/k2) Lebesgue measure region in each pair of overlapping class intervals with no points in X , from which it follows that \u03c0X(supp(\u03c0X) \\ Xmix,d) = O(1/k2). Now for y \u2208 [k], consider z \u2208 supp(\u03c0y) \u2229 Xmix,d. We will show that every 1-Mixup interpolator g for X with D\u03bb,1 being uniform satisfies \u03d5y(g(z)) \u2208 [\u0398(1), 1] when z falls only in the support of y and \u03d5y(g(z)) \u2208 [\u0398(1), 1\u2212\u0398(1)] in the overlapping region, which will be sufficient for showing the desired result.\nWe first handle the non-overlapping region of supp(\u03c0y). Here we need to consider the effect of the separation parameter \u03b1; if \u03b1 = O(1/k) (i.e. the classes have very high overlap), the behavior in this region will be asymptotically irrelevant to the final KL-divergence calculation (since it will be a O(log(k)/k) term). On the other hand, if \u03b1 = \u2126(1/k), then our partitioning of the input space was fine enough that we may claim from our high probability arguments that there are \u0398(\u03b1N/k) class y points in some subregion [\u03b2y, \u03b2y +\u0398(\u03b1)]. Similar logic also applies when considering the overlapping region, but this time the aforementioned logic is applied to 1\u2212 \u03b1. Thus, in what follows, we will assume we are in the regime of \u03b1 \u2208 [\u2126(1/k), 1\u2212 \u2126(1/k)]. Suppose without loss of generality that \u03c4(y) = 1 and that y < k (i.e. y is an odd-numbered class), and consider z \u2208 [\u03b2y, \u03b2y +\u03b1)\u2229Xmix,d (this is, as mentioned, the non-overlapping part of supp(\u03c0y)). Clearly z can only be obtained from \u03c3 \u2208 Md(X ) with y\u03c31 = y or y\u03c32 = y, since the other class supports are spaced k away and z \u2208 supp(\u03c0y) \\ supp(\u03c0y+1) (i.e. we have to mix with class y). This implies that for g\u2217 \u2208 arginfg Jmix,1(g,X ,D\u03bb,1), we have \u03bes(z) = 0 for s \u0338= y, y+1 (where \u03bey is as in Equation (A.14)). Now we consider two sub-cases: z \u2208 [\u03b2y, \u03b2y+\u03b1/2) and z \u2208 [\u03b2y+\u03b1/2, \u03b2y+\u03b1). In the first sub-case, if y\u03c3j = y then \u03bbj \u2265 1/2 since z = \u03bb1x\u03c31 + (1\u2212 \u03bb1)x\u03c32 must necessarily fall closer to whichever class y point produced it, which implies that \u03bey(z) \u2265 \u03b1y+1(z). For the latter sub-case, this is no longer true, since some points in [\u03b2y + \u03b1/2, \u03b2y + \u03b1) may fall closer to class y + 1. However, by our initial arguments there are \u0398(\u03b1N/k) class y points in [\u03b2y, \u03b2y + \u03b1/2), which implies there are \u0398(\u03b1N2/k2) choices of \u03c3 \u2208 Md(X ) which can produce z \u2208 [\u03b2y + \u03b1/2, \u03b2y + \u03b1) with a constant weight associated with class y. Since there are O(\u03b1N2/k2) total choices of \u03c3 \u2208 Md(X ) that can produce z \u2208 [\u03b2y + \u03b1/2, \u03b2y + \u03b1) (since we need to mix with a point to the left of z), we get that \u03bey(z) = \u2126(\u03b1y+1(z)) in this case. Now from Definition 3.4, it immediately follows that every 1-Mixup interpolator g satisfies \u03d5y(g(z)) \u2208 [\u0398(1), 1] for z \u2208 [\u03b2y, \u03b2y + \u03b1).\nLet us now consider z \u2208 [\u03b2y + \u03b1, \u03b2y + 1] \u2229 Xmix,d (the overlapping part of supp(\u03c0y)). We can apply similar logic to that used in handling the sub-case of z \u2208 [\u03b2y + \u03b1/2, \u03b2y + \u03b1) considered above; namely, for every z \u2208 [\u03b2y + \u03b1, \u03b2y + 1], there are \u0398((z \u2212 \u03b2y)(\u03b2y + 1 + \u03b1 \u2212 z)N2/k2) choices of \u03c3 \u2208 Md(X ) which can produce z with a constant weight associated with class y (consider our partitioning of the space), and so as before we get that \u03bey(z) = \u2126(\u03b1y+1(z)). However, there are also the same order of choices of \u03c3 \u2208 Md(X ) which can produce z with a constant weight associated with class y + 1, so we also get \u03b1y+1(z) = \u2126(\u03bey(z)). Thus, it follows that g satisfies \u03d5y(g(z)) \u2208 [\u0398(1), 1\u2212\u0398(1)] for z \u2208 [\u03b2y + \u03b1, \u03b2y + 1] \u2229 Xmix,d. Based on the above, we see that H\u03c0(\u03c0\u03021Xmix,1) = \u0398(1) (where we recall that H\u03c0 is defined as in Equation (A.1)). Furthermore, since \u03d5y(g(x)) \u2264 1\u2212\u2126(1/poly(k)) on supp(\u03c0X) \\ Xmix,1, we have H\u03c0(\u03c0\u03021supp(\u03c0X)\\Xmix,1) = O(log(k)/k), from which the overall result follows.\nBefore proving Theorem 4.6, we need to formally state the necessary assumptions alluded to in the main text. Once again, these generalize the core facets of the simple data distribution from Definition 4.1.\nAssumption A.3. In what follows, X is understood to be N i.i.d. draws from \u03c0 and Xmix,d is obtained from X as defined above and in Section 3.2. With this in mind, we restrict the class of distributions \u03c0 from Definition 4.4 such that:\n1. (Non-overlapping classes are separated) For y, s \u2208 [k], if \u03c0X(supp(\u03c0y) \u2229 supp(\u03c0s)) = 0, then d(supp(\u03c0y), supp(\u03c0s)) = \u03c9(1). Additionally, supp(\u03c0y) = \u0398(1) for all y \u2208 [k].\n2. (Mixed points cover support) With probability at least 1 \u2212 k3 exp ( \u2212\u2126(N/k3) ) ,\n\u03c0X(supp(\u03c0X) \\ Xmix,d) = O(1/k).\n3. (Mixed points are balanced across overlapping classes) Let us define for y \u2208 [k] and z \u2208 Rd: Myd(X , z) = { \u03c3 \u2208 Md(X ) : \u2203\u03bb \u2208 supp(D\u03bb,d) \u2223\u2223\u2223\u2223 z = \u2211 j\u2208[d+1] \u03bbjx\u03c3j\nand \u2211\nj:y\u03c3j=y\n\u03bbj = \u0398(1)\n} (A.18)\nThen with probability at least 1\u2212 k3 exp ( \u2212\u2126(N/k3) ) , for every y \u2208 [k] and z \u2208 Xmix,d \u2229 supp(\u03c0y) we have that |Myd(X , z)| = \u2126(f(z, \u03b1)Nd+1/kd+1) for some function f , or we have for every \u03c3 \u2208 Md(X ) such that z =\n\u2211 j\u2208[d+1] \u03bbjx\u03c3j with \u03bb \u2208 supp(D\u03bb,d):\u2211\nj:y\u03c3j=y\n\u03bbj \u2265 \u2211\ni:y\u03c3i \u0338=y\n\u03bbi (A.19)\nThe third part of Assumption A.3 appears complicated, but it just generalizes one of the key ideas in the proof of Proposition 4.2: either our mixed points always fall closer to one class (i.e. they are far away from the overlapping region), or there are a lot of possible mixings that can produce the mixed point. Ideally we would have structured Assumption A.3 to be purely in terms of the shape of \u03c0 and not in terms of the high probability behavior of the mixed points Xmix,d, but various edge cases in high dimensions led us to structuring the assumption this way.\nWith this assumption, it is straightforward to prove the generalization of Proposition 4.2.\nTheorem 4.6. Let X consist of N i.i.d. draws from any distribution \u03c0 satisfying Definition 4.4 and Assumption A.3, and let p(k) be as in Proposition 4.3. Then the result of Proposition 4.3 still holds when considering d-Mixup interpolators g for X where the mixing distribution D\u03bb,d is uniform over the d-dimensional probability simplex.\nProof. Assumption A.3 allows us to largely lift the proof of Proposition 4.2 to higher dimensions. Indeed, from the second part of Assumption A.3 we already begin with the fact that \u03c0X(supp(\u03c0X) \\ Xmix,d) = O(1/k) with probability at least 1\u2212 k3 exp ( \u2212\u2126(N/k3) ) .\nNow as before, for y \u2208 [k], consider z \u2208 supp(\u03c0y)\u2229Xmix,d. By the first part of Assumption A.3 and Definition A.1, z can only be obtained by mixing points from class y and/or its overlapping neighbors s1(y), ..., sm(y).\nIf z /\u2208 supp(\u03c0si(y)) for every si(y) (i.e. it is not in the overlapping region), then by the third part of Assumption A.3 it immediately follows that \u03d5y(g(z)) \u2208 [\u0398(1), 1] (note that unlike in dimension 1 the expression for \u03bey in Lemma A.2 is more complicated now, but due to assuming a uniform mixing density and near orthogonality of mixed points in Definition A.1 we can simplify to essentially expressions like in the 1-d case). On the other hand, if z = \u2211 j\u2208[d+1] \u03bbjx\u03c3j \u2208 supp(\u03c0si(y)) for some si(y), then we cannot simultaneously satisfy:\u2211 j:y\u03c3j=y \u03bbj \u2265 \u2211 r:y\u03c3r \u0338=y \u03bbr and \u2211 j:y\u03c3j=si(y) \u03bbj \u2265 \u2211 r:y\u03c3r \u0338=si(y) \u03bbr (A.20)\nThis implies that the other condition in the third part of Assumption A.3 is satisfied for both y and si(y); i.e. |Myd(X , z)| = \u2126(f(z, \u03b1)Nd+1/kd+1) (see Equation (A.18)). This again implies that \u03d5y(g(z)) \u2208 [\u0398(1), 1 \u2212 \u0398(1)]. As in the final part of the proof of Proposition 4.3, from this we get H\u03c0(\u03c0\u03021Xmix,d) = \u0398(1) and we are done from the fact that \u03d5\ny(g(x)) \u2264 1 \u2212 \u2126(1/poly(k)) on supp(\u03c0X) \\ Xmix,d."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENT PLOTS",
            "text": "B.1 MODEL LOGIT BEHAVIOR\nTo verify that the locally Lipschitz-like model behavior of Definition 3.2 is reasonable in our setting, we consider the following experimental setup. For every training point in each dataset we consider, we uniformly sample 500 points (due to compute constraints) from the surfaces of hyperspheres centered at the original training point with radii ranging from 1 to 10. The cutoff radius of 10 was chosen as that was roughly the minimum distance to a neighboring training point of a different class across the considered datasets.\nFor all sampled points, we compute the trained model (following the training setup of Section 5) logit gap (to account for translation invariance) between the logit for the correct class (or the original point) and the largest logit belonging to a different class. We then compute the mean over all logit gaps at each of the different radii considered, and plot how the mean logit gap changes as we move farther away from the original training points. Results are shown in Figure 3.\nAs can be seen in Figure 3, the mean logit gaps for each dataset barely move over this radius, suggesting that assuming Lipschitz-like behavior of the logits with high probability should be reasonable in small neighborhoods. However, we acknowledge that to truly make this claim one\nwould need to prove guarantees about the sample complexity needed to get appropriate coverage in each neighborhood. Our goal with the experiments in this section has only been to provide an initial attempt at such justification; we think that exploring how model logit/confidence behavior changes as one moves away from the training data contains many interesting challenges for future work.\nB.2 OMITTED PLOTS FOR SYNTHETIC DATA\nFigures 4 to 6 display the confidence histograms and reliability diagrams for ERM + TS as well as all of the Mixup models (Mixup, 2-Mixup, and 4-Mixup) for the different settings of Gaussian test data in Section 5.1. As was originally seen in Table 2, the calibration performance for ERM markedly declines as we move from \u00b5 = 0.25 to \u00b5 = 0.05.\nThe reason for this is visible in the confidence diagrams: at the \u00b5 = 0.25 level, the ERM confidences are still very bimodal, but the accuracy in the top bin is close to perfect. On the other hand, when moving to \u00b5 = 0.05, the accuracy in the top bin drops significantly while the accuracy in the bottom bin increases, which corresponds to the drop in calibration performance since nearly all predictions fall in these two bins.\nB.3 OMITTED PLOTS AND TABLES FOR IMAGE CLASSIFICATION BENCHMARKS\nTables 4 and 5 correspond to the ECE and ACE versions of 3. Unlike in the case of Gaussian data, here we see that both the ECE and ACE monotonically increase with label noise for the ERM + TS models (the only exception being SVHN at the 25% label noise level). We also find, as mentioned in the main text, that Mixup dominates in ECE and ACE as well.\nFigures 7 to 9 contain the confidence histograms and reliability diagrams for the ERM + TS and Mixup models across CIFAR-10, CIFAR-100, and SVHN. Unlike in the case of Gaussian test data, we find non-trivial gaps in test accuracy between the ERM and Mixup models on CIFAR-10 and CIFAR-100, which certainly contributes to some of the gap in reported negative log-likelihood. However, we can see in Figure 9 that ERM actually outperforms Mixup in terms of test accuracy in this case (across different label noise settings), and still suffers a massive gap in NLL. Additionally, the confidence histograms are telling, as we once again find that the ERM + TS predictions cluster heavily in the largest probability bin, which is again the cause for poor calibration performance."
        }
    ],
    "year": 2023
}