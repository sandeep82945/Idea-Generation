{
    "abstractText": "We introduce Control4D, an innovative framework for editing dynamic 4D portraits using text instructions. Our method addresses the prevalent challenges in 4D editing, notably the inefficiencies of existing 4D representations and the inconsistent editing effect caused by diffusion-based editors. We first propose GaussianPlanes, a novel 4D representation that makes Gaussian Splatting more structured by applying plane-based decomposition in 3D space and time. This enhances both efficiency and robustness in 4D editing. Furthermore, we propose to leverage a 4D generator to learn a more continuous generation space from inconsistent edited images produced by the diffusion-based editor, which effectively improves the consistency and quality of 4D editing. Comprehensive evaluation demonstrates the superiority of Control4D, including significantly reduced training time, high-quality rendering, and spatial-temporal consistency in 4D portrait editing. The link to our project website is: https://control4darxiv.github.io/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruizhi Shao"
        },
        {
            "affiliations": [],
            "name": "Jingxiang Sun"
        },
        {
            "affiliations": [],
            "name": "Cheng Peng"
        },
        {
            "affiliations": [],
            "name": "Zerong Zheng"
        },
        {
            "affiliations": [],
            "name": "Boyao Zhou"
        },
        {
            "affiliations": [],
            "name": "Hongwen Zhang"
        },
        {
            "affiliations": [],
            "name": "Yebin Liu"
        }
    ],
    "id": "SP:020bf3147255df85207ac5b8324e39d1d8a89bf8",
    "references": [
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Omer Bar-Tal",
                "Dolev Ofri-Amar",
                "Rafail Fridman",
                "Yoni Kasten",
                "Tali Dekel"
            ],
            "title": "Text2live: Text-driven layered image and video editing",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "arXiv preprint arXiv:2211.09800,",
            "year": 2022
        },
        {
            "authors": [
                "Ang Cao",
                "Justin Johnson"
            ],
            "title": "Hexplane: A fast representation for dynamic scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Yukang Cao",
                "Yan-Pei Cao",
                "Kai Han",
                "Ying Shan",
                "Kwan- Yee K Wong"
            ],
            "title": "Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models",
            "venue": "arXiv preprint arXiv:2304.00916,",
            "year": 2023
        },
        {
            "authors": [
                "Duygu Ceylan",
                "Chun-Hao Paul Huang",
                "Niloy J Mitra"
            ],
            "title": "Pix2video: Video editing using image diffusion",
            "venue": "arXiv preprint arXiv:2303.12688,",
            "year": 2023
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Chen",
                "Yongwei Chen",
                "Ningxin Jiao",
                "Kui Jia"
            ],
            "title": "Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2303.13873,",
            "year": 2023
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yilun Du",
                "Yinan Zhang",
                "Hong-Xing Yu",
                "Joshua B Tenenbaum",
                "Jiajun Wu"
            ],
            "title": "Neural radiance flow for 4d view synthesis and video processing",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Patrick Esser",
                "Johnathan Chiu",
                "Parmida Atighehchian",
                "Jonathan Granskog",
                "Anastasis Germanidis"
            ],
            "title": "Structure and content-guided video synthesis with diffusion models",
            "venue": "arXiv preprint arXiv:2302.03011,",
            "year": 2023
        },
        {
            "authors": [
                "Sara Fridovich-Keil",
                "Giacomo Meanti",
                "Frederik Rahb\u00e6k Warburg",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "K-planes: Explicit radiance fields in space, time, and appearance",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Communications of the ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Jiatao Gu",
                "Lingjie Liu",
                "Peng Wang",
                "Christian Theobalt"
            ],
            "title": "Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "arXiv preprint arXiv:2110.08985,",
            "year": 2021
        },
        {
            "authors": [
                "Shuyang Gu",
                "Dong Chen",
                "Jianmin Bao",
                "Fang Wen",
                "Bo Zhang",
                "Dongdong Chen",
                "Lu Yuan",
                "Baining Guo"
            ],
            "title": "Vector quantized diffusion model for text-to-image synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuwei Guo",
                "Ceyuan Yang",
                "Anyi Rao",
                "Yaohui Wang",
                "Yu Qiao",
                "Dahua Lin",
                "Bo Dai"
            ],
            "title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning",
            "venue": "arXiv preprint arXiv:2307.04725,",
            "year": 2023
        },
        {
            "authors": [
                "Ayaan Haque",
                "Matthew Tancik",
                "Alexei A Efros",
                "Aleksander Holynski",
                "Angjoo Kanazawa"
            ],
            "title": "Instruct-nerf2nerf: Editing 3d scenes with instructions",
            "venue": "arXiv preprint arXiv:2303.12789,",
            "year": 2023
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "venue": "arXiv preprint arXiv:2210.02303,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans",
                "Alexey Gritsenko",
                "William Chan",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Video diffusion models",
            "venue": "arXiv preprint arXiv:2204.03458,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew G Howard",
                "Menglong Zhu",
                "Bo Chen",
                "Dmitry Kalenichenko",
                "Weijun Wang",
                "Tobias Weyand",
                "Marco Andreetto",
                "Hartwig Adam"
            ],
            "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
            "venue": "arXiv preprint arXiv:1704.04861,",
            "year": 2017
        },
        {
            "authors": [
                "Xin Huang",
                "Ruizhi Shao",
                "Qi Zhang",
                "Hongwen Zhang",
                "Ying Feng",
                "Yebin Liu",
                "Qing Wang"
            ],
            "title": "Humannorm: Learning normal diffusion model for high-quality and realistic 3d human generation",
            "venue": "arXiv preprint arXiv:2310.01406,",
            "year": 2023
        },
        {
            "authors": [
                "Mustafa I\u015f\u0131k",
                "Martin R\u00fcnz",
                "Markos Georgopoulos",
                "Taras Khakhulin",
                "Jonathan Starck",
                "Lourdes Agapito",
                "Matthias Nie\u00dfner"
            ],
            "title": "Humanrf: High-fidelity neural radiance fields for humans in motion",
            "venue": "arXiv preprint arXiv:2305.06356,",
            "year": 2023
        },
        {
            "authors": [
                "Dadong Jiang",
                "Zhihui Ke",
                "Xiaobo Zhou",
                "Xidong Shi"
            ],
            "title": "4d-editor: Interactive object-level editing in dynamic neural radiance fields via 4d semantic segmentation",
            "venue": "arXiv preprint arXiv:2310.16858,",
            "year": 2023
        },
        {
            "authors": [
                "Bernhard Kerbl",
                "Georgios Kopanas",
                "Thomas Leimk\u00fchler",
                "George Drettakis"
            ],
            "title": "3d gaussian splatting for real-time radiance field rendering",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2023
        },
        {
            "authors": [
                "Levon Khachatryan",
                "Andranik Movsisyan",
                "Vahram Tadevosyan",
                "Roberto Henschel",
                "Zhangyang Wang",
                "Shant Navasardyan",
                "Humphrey Shi"
            ],
            "title": "Text2video-zero: Text-toimage diffusion models are zero-shot video generators",
            "venue": "arXiv preprint arXiv:2303.13439,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tianye Li",
                "Mira Slavcheva",
                "Michael Zollhoefer",
                "Simon Green",
                "Christoph Lassner",
                "Changil Kim",
                "Tanner Schmidt",
                "Steven Lovegrove",
                "Michael Goesele",
                "Richard Newcombe"
            ],
            "title": "Neural 3d video synthesis from multi-view video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Weiyu Li",
                "Rui Chen",
                "Xuelin Chen",
                "Ping Tan"
            ],
            "title": "Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d",
            "venue": "arXiv preprint arXiv:2310.02596,",
            "year": 2023
        },
        {
            "authors": [
                "Yu-Jhe Li",
                "Kris Kitani"
            ],
            "title": "3d-clfusion: Fast text-to-3d rendering with contrastive latent diffusion",
            "venue": "arXiv preprint arXiv:2303.11938,",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: Highresolution text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2211.10440,",
            "year": 2022
        },
        {
            "authors": [
                "Haotong Lin",
                "Sida Peng",
                "Zhen Xu",
                "Yunzhi Yan",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Efficient neural radiance fields for interactive free-viewpoint video",
            "venue": "In SIGGRAPH Asia Conference Proceedings,",
            "year": 2022
        },
        {
            "authors": [
                "Jia-Wei Liu",
                "Yan-Pei Cao",
                "Weijia Mao",
                "Wenqiao Zhang",
                "David Junhao Zhang",
                "Jussi Keppo",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Devrf: Fast deformable voxel radiance fields for dynamic scenes",
            "venue": "arXiv preprint arXiv:2205.15723,",
            "year": 2022
        },
        {
            "authors": [
                "Lingjie Liu",
                "Marc Habermann",
                "Viktor Rudnev",
                "Kripasindhu Sarkar",
                "Jiatao Gu",
                "Christian Theobalt"
            ],
            "title": "Neural actor: Neural free-view synthesis of human actors with pose control",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Rundi Wu",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Sergey Zakharov",
                "Carl Vondrick"
            ],
            "title": "Zero-1-to-3: Zero-shot one image to 3d object, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jonathon Luiten",
                "Georgios Kopanas",
                "Bastian Leibe",
                "Deva Ramanan"
            ],
            "title": "Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis",
            "venue": "arXiv preprint arXiv:2308.09713,",
            "year": 2023
        },
        {
            "authors": [
                "Gal Metzer",
                "Elad Richardson",
                "Or Patashnik",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Latent-nerf for shape-guided generation of 3d shapes and textures",
            "venue": "arXiv preprint arXiv:2211.07600,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2022
        },
        {
            "authors": [
                "Thu Nguyen-Phuoc",
                "Chuan Li",
                "Lucas Theis",
                "Christian Richardt",
                "Yong-Liang Yang"
            ],
            "title": "Hologan: Unsupervised learning of 3d representations from natural images",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Thu H Nguyen-Phuoc",
                "Christian Richardt",
                "Long Mai",
                "Yongliang Yang",
                "Niloy Mitra"
            ],
            "title": "Blockgan: Learning 3d object-aware scene representations from unlabelled images",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "arXiv preprint arXiv:2112.10741,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Lars Mescheder",
                "Michael Oechsle",
                "Andreas Geiger"
            ],
            "title": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Jonathan T Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Steven M Seitz",
                "Ricardo Martin-Brualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Peter Hedman",
                "Jonathan T Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Ricardo Martin- Brualla",
                "Steven M Seitz"
            ],
            "title": "Hypernerf: A higherdimensional representation for topologically varying neural radiance fields",
            "venue": "arXiv preprint arXiv:2106.13228,",
            "year": 2021
        },
        {
            "authors": [
                "Sida Peng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Po",
                "Gordon Wetzstein"
            ],
            "title": "Compositional 3d scene generation using locally conditioned diffusion",
            "venue": "arXiv preprint arXiv:2303.12218,",
            "year": 2023
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988,",
            "year": 2022
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-nerf: Neural radiance fields for dynamic scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yi-Ling Qiao",
                "Alexander Gao",
                "Ming Lin"
            ],
            "title": "Neuphysics: Editable neural geometry and physics from monocular videos",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Lutz Sch\u00f6nberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Hoigi Seo",
                "Hayeon Kim",
                "Gwanghyun Kim",
                "Se Young Chun"
            ],
            "title": "Ditto-nerf: Diffusion-based iterative text to omnidirectional 3d model",
            "venue": "arXiv preprint arXiv:2304.02827,",
            "year": 2023
        },
        {
            "authors": [
                "Ruizhi Shao",
                "Zerong Zheng",
                "Hanzhang Tu",
                "Boning Liu",
                "Hongwen Zhang",
                "Yebin Liu"
            ],
            "title": "Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Qiuhong Shen",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "title": "Anything- 3d: Towards single-view anything reconstruction in the wild",
            "venue": "arXiv preprint arXiv:2304.10261,",
            "year": 2023
        },
        {
            "authors": [
                "Yichun Shi",
                "Peng Wang",
                "Jianglong Ye",
                "Mai Long",
                "Kejie Li",
                "Xiao Yang"
            ],
            "title": "Mvdream: Multi-view diffusion for 3d generation",
            "venue": "arXiv preprint arXiv:2308.16512,",
            "year": 2023
        },
        {
            "authors": [
                "Uriel Singer",
                "Adam Polyak",
                "Thomas Hayes",
                "Xi Yin",
                "Jie An",
                "Songyang Zhang",
                "Qiyuan Hu",
                "Harry Yang",
                "Oron Ashual",
                "Oran Gafni"
            ],
            "title": "Make-a-video: Text-to-video generation without text-video data",
            "venue": "arXiv preprint arXiv:2209.14792,",
            "year": 2022
        },
        {
            "authors": [
                "Uriel Singer",
                "Shelly Sheynin",
                "Adam Polyak",
                "Oron Ashual",
                "Iurii Makarov",
                "Filippos Kokkinos",
                "Naman Goyal",
                "Andrea Vedaldi",
                "Devi Parikh",
                "Justin Johnson"
            ],
            "title": "Text-to-4d dynamic scene generation",
            "venue": "arXiv preprint arXiv:2301.11280,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Liangchen Song",
                "Anpei Chen",
                "Zhong Li",
                "Zhang Chen",
                "Lele Chen",
                "Junsong Yuan",
                "Yi Xu",
                "Andreas Geiger"
            ],
            "title": "Nerfplayer: A streamable dynamic scene representation with decomposed neural radiance fields",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 2023
        },
        {
            "authors": [
                "Jingxiang Sun",
                "Bo Zhang",
                "Ruizhi Shao",
                "Lizhen Wang",
                "Wen Liu",
                "Zhenda Xie",
                "Yebin Liu"
            ],
            "title": "Dreamcraft3d: Hierarchical 3d generation with bootstrapped diffusion prior",
            "venue": "arXiv preprint arXiv:2310.16818,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaxiang Tang",
                "Jiawei Ren",
                "Hang Zhou",
                "Ziwei Liu",
                "Gang Zeng"
            ],
            "title": "Dreamgaussian: Generative gaussian splatting for efficient 3d content creation",
            "venue": "arXiv preprint arXiv:2309.16653,",
            "year": 2023
        },
        {
            "authors": [
                "Junshu Tang",
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Ran Yi",
                "Lizhuang Ma",
                "Dong Chen"
            ],
            "title": "Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior",
            "venue": "arXiv preprint arXiv:2303.14184,",
            "year": 2023
        },
        {
            "authors": [
                "Ayush Tewari",
                "Justus Thies",
                "Ben Mildenhall",
                "Pratul Srinivasan",
                "Edgar Tretschk",
                "Wang Yifan",
                "Christoph Lassner",
                "Vincent Sitzmann",
                "Ricardo Martin-Brualla",
                "Stephen Lombardi"
            ],
            "title": "Advances in neural rendering",
            "venue": "In Computer Graphics Forum,",
            "year": 2022
        },
        {
            "authors": [
                "Edgar Tretschk",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Michael Zollh\u00f6fer",
                "Christoph Lassner",
                "Christian Theobalt"
            ],
            "title": "Nonrigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Chaoyang Wang",
                "Ben Eckart",
                "Simon Lucey",
                "Orazio Gallo"
            ],
            "title": "Neural trajectory fields for dynamic novel view synthesis",
            "venue": "arXiv preprint arXiv:2105.05994,",
            "year": 2021
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaodan Du",
                "Jiahao Li",
                "Raymond A Yeh",
                "Greg Shakhnarovich"
            ],
            "title": "Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation",
            "venue": "arXiv preprint arXiv:2212.00774,",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "Lingjie Liu",
                "Yuan Liu",
                "Christian Theobalt",
                "Taku Komura",
                "Wenping Wang"
            ],
            "title": "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Zhengyi Wang",
                "Cheng Lu",
                "Yikai Wang",
                "Fan Bao",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Pratul P Srinivasan",
                "Jonathan T Barron",
                "Ira Kemelmacher-Shlizerman"
            ],
            "title": "Humannerf: Free-viewpoint rendering of moving people from monocular video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Guanjun Wu",
                "Taoran Yi",
                "Jiemin Fang",
                "Lingxi Xie",
                "Xiaopeng Zhang",
                "Wei Wei",
                "Wenyu Liu",
                "Qi Tian",
                "Xinggang Wang"
            ],
            "title": "4d gaussian splatting for real-time dynamic scene rendering",
            "venue": "arXiv preprint arXiv:2310.08528,",
            "year": 2023
        },
        {
            "authors": [
                "Jiajun Wu",
                "Chengkai Zhang",
                "Tianfan Xue",
                "Bill Freeman",
                "Josh Tenenbaum"
            ],
            "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Jay Zhangjie Wu",
                "Yixiao Ge",
                "Xintao Wang",
                "Weixian Lei",
                "Yuchao Gu",
                "Wynne Hsu",
                "Ying Shan",
                "Xiaohu Qie",
                "Mike Zheng Shou"
            ],
            "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation",
            "venue": "arXiv preprint arXiv:2212.11565,",
            "year": 2022
        },
        {
            "authors": [
                "Wenqi Xian",
                "Jia-Bin Huang",
                "Johannes Kopf",
                "Changil Kim"
            ],
            "title": "Space-time neural irradiance fields for free-viewpoint video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Zhen Xu",
                "Sida Peng",
                "Haotong Lin",
                "Guangzhao He",
                "Jiaming Sun",
                "Yujun Shen",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "4k4d: Real-time 4d view synthesis at 4k resolution",
            "venue": "arXiv preprint arXiv:2310.11448,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Yu",
                "Zerong Zheng",
                "Kaiwen Guo",
                "Pengpeng Liu",
                "Qionghai Dai",
                "Yebin Liu"
            ],
            "title": "Function4d: Real-time human volumetric capture from very sparse consumer rgbd sensors",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        },
        {
            "authors": [
                "Zerong Zheng",
                "Han Huang",
                "Tao Yu",
                "Hongwen Zhang",
                "Yandong Guo",
                "Yebin Liu"
            ],
            "title": "Structured local radiance fields for human avatar modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zerong Zheng",
                "Xiaochen Zhao",
                "Hongwen Zhang",
                "Boning Liu",
                "Yebin Liu"
            ],
            "title": "Avatarrex: Real-time expressive fullbody avatars",
            "venue": "arXiv preprint arXiv:2305.04789,",
            "year": 2023
        },
        {
            "authors": [
                "Daquan Zhou",
                "Weimin Wang",
                "Hanshu Yan",
                "Weiwei Lv",
                "Yizhe Zhu",
                "Jiashi Feng"
            ],
            "title": "Magicvideo: Efficient video generation with latent diffusion models",
            "venue": "arXiv preprint arXiv:2211.11018,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The realm of 4D scene reconstruction has witnessed advancements with the advent of dynamic neural 3D representation [12, 35, 47, 51, 59]. These innovations have significantly enhanced our ability to capture and represent dynamic scenes. Despite these advances, the interactive editing of these 4D scenes still poses substantial\nchallenges. The primary challenge involves ensuring both spatial-temporal consistency and high quality in 4D editing.\nAvailable 4D editing techniques [26, 52], while effective for fundamental tasks like object removal or color modification, often fall short in delivering interactive and advanced editing functionalities. Recently, a groundbreaking framework based on text-to-image (T2I) diffusion model [50] has emerged for 3D generation and editing. It integrates a neural 3D representation such as NeRF with an image diffusion model and achieves text-to-3D generation [8, 33, 39, 50] or editing [17] by iteratively aligning images rendered from the 3D representation with those generated by the diffusion model. This diffusion-based framework allows for more flexible and enhanced editing through textual control.\nBuilding on this framework, a straightforward approach to 4D editing involves transitioning from a 3D to a 4D representation. However, it faces two primary challenges: First, 4D representations such as dynamic NeRFs require dense sampling along the rays to render images, which is slow and highly memory-intensive [12, 51, 59]. Such inefficiency significantly increases the time required for editing in 4D scenarios. On the other hand, current T2I diffusion models lack consistency in editing different images [17]. This inconsistency is more apparent in 4D editing, as the results vary across different spatial perspectives and over time, making 4D editing extremely challenging.\nIn this paper, we address these challenges and present Control4D, a novel method for efficient, high-quality, and consistent 4D dynamic portrait editing with text as input. Firstly, to enhance the efficiency of 4D representation, we\nar X\niv :2\n30 5.\n20 08\n2v 2\n[ cs\n.C V\npropose to extend an explicit 3D representation, Gaussian Splatting, to a 4D dynamic representation. Gaussian Splatting is an emerging representation that has demonstrated its efficiency in training and rendering for 3D reconstruction [27] and generation [67]. However, as it uses discrete Gaussian point clouds where every point is independent from each other, it easily introduces noise during the 4D editing process, where the edited images are not consistent in both space and time. To address this issue, we first propose to construct the spatial structure to describe the attributes of discrete Gaussian points by a unified, structured tri-plane [7] representation. Specifically, we project each Gaussian point onto three feature planes and employ an MLP to integrate features and derive their attributes, which not only ensures efficiency but also enhances robustness. Then, we extend Gaussian Splatting to 4D by defining a canonical Gaussian point cloud and allowing each point to move with time. To regularize the flow of discrete points, we also project their positions with time into 9 planes [59] to make the flow more structured. With the tri-planar structure for the canonical space and the 4D plane-based structure for the 4D flow, we introduce GaussianPlanes representation, which significantly reduces the time cost and improves spatiotemporal consistency in 4D editing.\nAlthough GaussianPlanes significantly improves the efficiency of representation, implementing 4D editing based on it still encounters a bottleneck. This bottleneck lies in the T2I diffusion model, as the diffusion-based editor adopts a 2D generation process and produces inconsistent edits in 4D space across time and viewpoints. Consequently, when optimized with these inconsistent images, the dynamic scene model tends to diverge or produce blurry and smoothed outcomes. To overcome this challenge, we propose a 4D generator to mitigate the issue of inconsistent supervision arising from the edited dataset. The key insight of our method is to learn a more continuous GAN latent space based on the edited images produced by the editor, avoiding direct but inconsistent supervision. Specifically, we introduce additional latent properties to GaussianPlanes and incorporate it with a 2D super-resolution module, constructing a 4D generator, capable of producing high-resolution images based on the rendered latent features. Simultaneously, we employ a discriminator to learn the generation distribution from the edited images, which then provides discrimination signals for updating the generator. To ensure stable training, we extract multi-level information from the edited images and utilize it to facilitate the generator\u2019s learning process.\nWe conduct comprehensive evaluation of our approach using a diverse collection of dynamic portraits. To validate the efficacy of our design, we conduct ablation studies and compare our method with a 4D extension of InstructNeRF2NeRF [17]. The evaluation demonstrates the efficiency and remarkable capabilities of our method in achiev-\ning both photo-realistic rendering and spatio-temporal consistency in 4D portrait editing. To sum up, our main contributions are listed as follows: \u2022 We propose an efficient and robust 4D representation\nGaussianPlanes for 4D editing by applying plane-based decomposition to structure Gassian Splatting in both space and time. \u2022 We introduce a 4D generator to learn from the 2D diffusion-based editor, which reduces the effect of inconsistent supervision signals and enhances the quality of 4D editing. \u2022 Building upon the proposed GaussianPlanes and 4D generator, We introduce Control4D, a novel framework for flexible 4D portrait editing with text, which significantly reduces the training time, achieves high-quality rendering, and ensures spatio-temporal consistency."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. 2D Diffusion Models",
            "text": "Diffusion models iteratively transform random samples into ones resembling target data [9, 20, 29, 64]. Enhanced with pre-trained models [53], they solve multi-modal tasks like text-to-image generation [19, 44, 54]. VQdiffusion[15] and LDMs [55] bolster performance by operating within an autoencoder\u2019s latent space. Although these models have found success, temporally inconsistent issues emerge in videos and 4D scenes.\nResearch has also concentrated on diffusion-based video generation and editing. Video Diffusion Models (VDM)[22] use U-Net architecture to train image and video data jointly, while approaches like ImagenVideo[21] enable high-resolution video generation. Various methods aim to transfer text-image generation to text-video, but due to training costs, many focus on text-prompted video editing [2, 6, 11, 28, 31, 62, 79, 86]. These efforts underscore the potential of text-based video editing, yet challenges related to temporal consistency, quality generation, and viewpoint alterations persist."
        },
        {
            "heading": "2.2. NeRF-Based 3D Generation and Editing",
            "text": "NeRFs [40] have gained widespread popularity for producing realistic 3D scene reconstruction and novel views based on calibrated photographs, and have been further developed in numerous subsequent studies [69]. Nevertheless, NeRFs still pose a challenge for editing purposes, primarily due to their underlying representation.\nNERF editing researchers have focused on utilizing GANs [13] and Diffusion models for their powerful generative capabilities. GAN-based methods have seen a proliferation of novel architectures that combine implicit or explicit 3D representations with neural rendering techniques, achieving promising results [7, 14, 42, 43, 78, 87]. How-\never, voxel-based GANs face challenges such as high memory requirements and computational burden when training high-resolution 3D GANs. On the other hand, diffusionbased methods have two primary approaches for extending 2D editing to 3D NeRFs. The first involves using Stable Diffusion with score distillation sampling (SDS) loss to generate 3D NeRFs using the 2D diffusion-prior, as seen in DreamFusion [50] and its follow-ups [5, 8, 24, 32, 33, 37, 49, 57, 60, 61, 63, 66\u201368, 72, 75]. However, these methods can only generate isolated objects lacking fine-level control over synthesized outputs. The second approach utilizes dataset update(DU) to guide NeRF convergence iteratively, as seen in Instruct-NeRF2NeRF [17], but it has network convergence issues and can be cost-intensive."
        },
        {
            "heading": "2.3. NeRF for Dynamic Scenes",
            "text": "To expand the success of NeRF into the temporal domain, researchers have pursued the strategy of modeling scenes in 4D domain with time dimension. DyNeRF [30] proposes a keyframe-based training strategy to extend NeRF with time-conditioning. VideoNeRF [80] learns a spatiotemporal irradiance field directly from a single video and resolves the shape-motion ambiguities in monocular inputs by incorporating depth estimation. Meanwhile, NeRFlow [10] and DCT-NeRF[71] utilize point trajectories to regularize network optimization. Park et al.[46, 47]; Pumarola et al. [51]; Tretschk et al. [70] adopt a similar framework that introduce a separate MLP to predict scene deformations for multi-view and monocular videos, respectively.Another approach for dynamic scenes is DeVRF [35], which adopts a voxel-based representation to model both the 3D canonical space and the 4D deformation field. Additionally, methods including Neuralbody [48] and [36, 76, 84, 85] leverage parametric body templates as semantic priors to achieve photo-realistic novel view synthesis of complex human performances. Recently, to achieve higher quality with lower memory, NeRFPlayer [65] have decomposed the 4D space into regions of static, deforming, and newly appeared content. Meanwhile, more compact and efficient representations, such as [4, 12, 25, 81] are proposed, significantly boosting the rendering quality and efficiency."
        },
        {
            "heading": "2.4. Gaussian Splatting",
            "text": "3D Gaussian Splatting (3DGS) [27] offers a high-quality, swift alternative to Neural Radiance Fields (NeRF), leveraging differentiable 3D Gaussians for efficient rasterization. Unlike NeRF and other implicit 3D representations [45, 73] which render images based on volume rendering, 3D-GS employs a splatting method for image rendering, resulting in real-time speed. The successor, 4D Gaussian Splatting (4DGS) [38, 77], extends this with per-frame dense tracking and novel view synthesis for dynamic scenes by utilizing a lightweight deformation field to model Gaussian motions\nand shape changes."
        },
        {
            "heading": "3. Overview",
            "text": "To achieve high-quality, efficient, and consistent 4D portrait editing, we first extend the Gaussian Splatting to 4D representation and structure it through a spatial-temporal planebased decomposition (Sec. 4). To address the issue of inconsistencies in edited images generated by diffusion-based editors, we integrate a 4D Editor with GaussianPlanes to effectively mitigate instability and blurring issues and achieve the realism and quality of 4D editing (Sec. 5). Building on the GaussianPlanes and 4D Editor, we finally introduce several efficient training strategies for 4D editing in the Control4D framework (Sec. 5.3).\nAs shown in Fig. 2, our framework consists of the GaussianPlanes and a 4D generator. Given multi-view videos, we first reconstruct the 4D portrait based on GaussianPlanes. Subsequently, we edit the reconstructed rendering results and latent features through a multi-level generator to obtain the edited results. Simultaneously, we employ an iterative approach to achieve dataset update through a 2D diffusion-based editor, which is a ControlNet [83] in practice. The outputs of the editor serve as real images, while the generator\u2019s results function as fake images for the discriminator\u2019s input. As the GAN training progresses, we progressively incorporate the generator\u2019s outputs to refine the inputs of the 2D diffusion-based editor, facilitating training convergence. Ultimately, the discrimination outcomes are utilized to compute the GAN loss, driving the iterative refinement of both the generator and discriminator. This methodology ensures the efficient and precise realization of 4D editing through our GAN-based framework."
        },
        {
            "heading": "4. GaussianPlanes",
            "text": "In this section, we propose an efficient and robust 4D representation GaussianPlanes for 4D portrait editing. The key idea is to structure the discrete points of Gaussian Splatting in both space and time. First, we introduce the spatial tri-plane decomposition, which makes Gaussian Splatting structured in the spatial domain (Sec. 4.1). Following this, we expand Gaussian Splatting into 4D representation and structure the flow of each Gaussian point by performing a temporal-spatial plane-based decomposition (Sec. 4.2)."
        },
        {
            "heading": "4.1. GaussianPlanes in 3D",
            "text": "Gaussian Splatting is an emerging explicit 3D representation that utilizes a set of Gaussian point clouds to represent 3D scenes. Each point is described with attributes of the center position x \u2208 R3, the rotation quaternion r \u2208 R4, the scale factor s \u2208 R3, the opacity value \u03b1 \u2208 R and the color feature c \u2208 R3. The rendering process of Gaussian Splatting involves projecting the Gaussian point cloud\nonto the rendering viewpoint according to camera parameters, followed by rasterization and volume rendering. Since each point in the Gaussian point cloud is independent and unstructured, noise easily occurs during optimization. To enhance robustness, we propose a spatial tri-plane decomposition to represent the attributes of the Gaussian points. Specifically, we decompose the color ci, opacity \u03b1i and rotation ri of i-th Gaussian point into tri-plane features:\nci = fc(F xy c (xi, yi), F xz c (xi, zi), F yz c (yi, zi)), \u03b1i = f\u03b1(F xy \u03b1 (xi, yi), F xz \u03b1 (xi, zi), F yz \u03b1 (yi, zi)), ri = fr(F xy r (xi, yi), F xz r (xi, zi), F yz r (yi, zi)),\n(1)\nwhere F xy, F xz, F yz are the decomposed feature planes, and f is an MLP that fuses features to predict specific attributes. In this way, although the Gaussian points remain independent, their attributes are structured and low-rank in spatial space, which helps to reduce noise and improve the robustness of Gaussian Splatting. The scale factor s and center position x are not decomposed, as splitting Gaussian points would abruptly halve the scale factor and the center position of each point is used for querying attributes itself."
        },
        {
            "heading": "4.2. GaussianPlanes in 4D",
            "text": "To extend Gaussian Splatting for 4D editing, we regard the Gaussian point cloud at the first frame as the canonical space and represent the 4D scene at different times by deforming the canonical Gaussian point cloud. Specifically, we define the flow x\u0302, r\u0302 for both position and rotation attributes of Gaussian points. Then, for time t, we move each Gaussian point in the canonical space (t = 0) with the flow:\nxi(t) = xi(0) + x\u0302i(t), ri(t) = ri(0) + r\u0302i(t). (2)\nIn this way, we enhance temporal consistency since the Gaussian point cloud at all times corresponds to its canonical space. However, the flow of each gaussian point is still discrete and independent. To further structure the flow of Gaussian points in space and time, we adopt spatial-temporal plane-based decomposition proposed by Tensor4D [59] and decompose the flow attributes of i-th point into nine feature planes:\nx\u0302i(t) = fx\u0302(xi, yi, zi, t) = \u03c03(\u03a03(Fx\u0302)), r\u0302i(t) = fr\u0302(xi, yi, zi, t) = \u03c03(\u03a03(Fr\u0302)), (3)\nwhere \u03a03, \u03c03 are the hierarchical 4D decomposition in Tensor4D and F represents feature planes. Through spatial triplanar decomposition and 4D plane-based decomposition, we structure the 4D Gaussian Splatting to enhance its consistency while maintaining efficiency."
        },
        {
            "heading": "5. 4D Editing with GaussianPlanes",
            "text": "To solve another challenge raised by diffusion-based editors, we propose a GaussianPlane-based 4D generator to edit 4D scene from the 2D inconsistent editing images with stable optimization. Instead of utilizing direct supervision with the edited images [3], our method learns a continuous generation space via GAN [13] to establish a connection between GaussianPlanes and dynamically edited images. Specifically, we integrate GaussianPlanes with a 2D GAN-based super-resolution module into a 4D generator and learn a generation space from the edited images generated by the diffusion model. Leveraging its generative capabilities, the 4D generator can effectively distill knowledge from the diffusion-based editor and distinguish between the rendering images (fake samples) and edited images (real samples). Subsequently, GaussianPlanes can be\noptimized within a continuous generative space supervised by the discrimination loss. With such a learning-to-generate mechanism, our method effectively alleviates blurry effects, resulting in high-fidelity and consistent 4D editing. In the following, we will introduce 1) integrating GaussianPlanes with GAN for 4D scene generation; and 2) the generation with multi-level guidance."
        },
        {
            "heading": "5.1. Connecting GAN to GaussianPlanes",
            "text": "To enable the generative ability of GaussianPlanes and higher rendering resolution, we build a 4D generator by connecting the GaussianPlanes representation with a GANbased super-resolution module. To this end, we first augment each point in the GaussianPlanes with latent features [7] as additional attributes. Here we assume the latent features follow a normal distribution, so in practice we augment the Gaussian attributes with their means \u00b5 and variances \u03c3, thus enabling subsequent sampling. We also adopt the same tri-plane decomposition for these latent distribution parameters. Then we can render a \u201cdistribution parameter map\u201d to sample the latent features, which will be fed into a super-resolution module G. Meanwhile, we also render an RGB image for auxiliary supervision. The distribution map consist of a latent mean map and a latent variance map, denoted as I\u00b5 and I\u03c3 , respectively, which capture the mean and variance of latent features. By leveraging this distribution map, we then proceed to sample a latent feature map that will be fed into G:\nIl = I\u00b5 + tI\u03c3, t \u223c N(0, 1). (4)\nThen, we concatenate the rendered RGB images Ir and the latent feature maps Il and feed them into the superresolution module to synthesize high-resolution images:\nIG = G(Ir, Il). (5)\nAs mentioned above, the edited images are temporally inconsistent due to the frame-by-frame editing. To avoid the discrete and inconsistent issue of direct supervision, our method learns a more continuous generation space via GAN from these edited images. Specifically, the generated images IG are considered as fake samples, while the edited images are regarded as real samples. The GAN loss can be formulated as follows:\nLD = D(IG)\u2212D(Ied) + Lgp LG = \u2212D(IG),\n(6)\nwhere Ied are the edited images generated by diffusionbased editor, D is the discriminator and Lgp is the Wasserstein GAN gradient penalty loss [1]."
        },
        {
            "heading": "5.2. Multi-level Generation with Guidance",
            "text": "When training GAN with the loss in Eqn. 6, we observe that the learning process often suffers from mode collapse\nissue. This may be caused by the fact that there is a limited number of edited images, and it is easy for the discriminator to learn how to distinguish between different sources of samples. To stabilize the learning process, we propose to extract multi-level information from the edited images and use these global and local cues to guide the learning of the generator. As shown in Fig. 3, during the training process, we construct two networks\u2014global encoder Eg and local encoder El\u2014to extract the global code and local feature maps of the edited image Ied, respectively. With these conditions as additional inputs, our generator can synthesize images on three levels:\nI1G = G(Ir, Il, Eg(Ir)) I2G = G(Ir, Il, Eg(Ied)) I3G = G(Ir, El(Ied), Eg(Ied))\n(7)\nThroughout the progression from level 1 to level 3, the generator produces images that gradually approach real edited images:\n\u2022 At level 1, the generator directly synthesis images based on Tensor4D. \u2022 At level 2, global information from the real edited images is introduced as conditions, guiding the generator to produce results consistent with the overall style of the real images. \u2022 At level 3, both the global and local information from the real edited images is used as conditions, enabling the network to generate images that exhibit consistency in both the overall pattern and finer details with the real edited images.\nTo facilitate training, we also utilize different losses at different levels:\nL1 = \u2212D(I1G) L2 = \u2212D(I2G) + LP (I2G, Ied) L3 = \u2212D(I3G) + LP (I3G, Ied) + \u2225I3G \u2212 Ied\u22251\n(8)\nLevel 1 employs the original GAN loss. At level 2, a perceptual loss is introduced as an additional constraint to enforce consistency in the global style. At level 3, the loss function simultaneously incorporates L1 loss, perceptual loss, and GAN loss as penalties, as the consistency in details and global style is desired. This multi-level information guides the generator to converge progressively towards the generation space of the diffusion model, improving training stability in single scenarios and accelerating convergence compared to the original GAN training process."
        },
        {
            "heading": "5.3. Training Strategy",
            "text": "To address the high iterative optimization cost associated with using the diffusion-based editor, we propose several strategies to further improve the efficiency of 4D editing. Staged Training Strategy. We adopt a staged training strategy that facilitates convergence. First, we fix the flow in the static stage and focus solely on editing the canonical space. This simplifies the editing process from 4D to 3D static editing, resulting in faster convergence. Once the editing of the canonical space has converged, we proceed to train GaussianPlanes across the entire 4D sequences. We also adopt a smaller noise timestep t \u2208 U(0.02, 0.6) for the diffusionbased editor in the dynamic stage since most of the editing effect is done in the static stage. Batch-based Dataset Update. To improve the editing consistency across different images, instead of editing a sin-\ngle image per iteration like InstructN2N, we group several images as a batch and edit them simultaneously. In editing each batch, we incorporate an attention module [16] for multi-frame image generation into our diffusion-based editor to capture the temporal-spatial correspondences and thereby improve editing consistency."
        },
        {
            "heading": "6. Experiment",
            "text": "We primarily conduct experiments on the dynamic Tensor4D dataset, which captures dynamic half-body human videos by four sparsely positioned, fixed RGB cameras. The calibration is performed using a checkerboard. Each data sample captures a diverse range of human motions in 1-2 minute duration. For our experiments, we extract 2- second segments, consisting of 50 frames, from the fulllength videos for 4D reconstruction and editing. Furthermore, to showcase the capabilities of our method in 360- degree scenes, we also select scanned human models from Twindom [82] dataset for additional evaluation. Please refer to the suppl. for more experiment details."
        },
        {
            "heading": "6.1. Qualitative Evaluation",
            "text": ""
        },
        {
            "heading": "6.1.1 Static scene",
            "text": "Since the task of 4D editing with text has not been addressed in previous works, we first conduct evaluation on static scene in order to validate the efficiency of our proposed methods. To validate the efficiency of our proposed GAN, we first conduct a comparison between NeRF+GAN and instruct-NeRF2NeRF under static scenes. We select some human models from the Twindom dataset and sampled 180 viewpoints randomly within a 360-degree range to render images. Subsequently, we evaluate NeRF+GAN and\ninstruct-NeRF2NeRF for editing with prompt \u201dTurn him into Elon Musk\u201d. In Fig. 4, we present the results after 50,000 iterations of training. Observing the results, it is evident that our GAN can generate images of high quality, exhibiting rich detail and enhanced realism. In contrast, the instruct-NeRF2NeRF outputs appear smoother, with some issues observed in the blending of side views. This comparison highlights the significant advantage of our GAN in terms of editing capabilities."
        },
        {
            "heading": "6.1.2 Dynamic scene",
            "text": "In dynamic scenarios, we compare our proposed method and the baseline method that only utilize GaussianPlanes, and the results are presented in Figure 5. We also present the results of different individuals engaged in various actions, which can be referenced in Figure 6. In the baseline approach, where GAN-based generation is not utilized, GaussianPlanes is directly tasked with fitting a dynamically changing editing dataset in both space and time. This direct fitting process often leads to the optimization of smooth results that may lack consistency and high-quality details. Our proposed method incorporates GAN-based generation, leveraging the GAN to learn a more continuous 4D generation space. This allows us to leverage the smooth supervisory signals for optimization. Thus, our method generates consistent and high-quality results that exhibit improved fidelity and capture finer details in the dynamic editing process. The comparison between the baseline approach and our method demonstrates the effectiveness of our proposed 4D generator in enhancing the overall quality and consistency of the generated results."
        },
        {
            "heading": "6.2. Quantitative Experiment",
            "text": "We conducted quantitative experiments in 5 static and 4 dynamic scenarios. The results are presented in Tab. 1. First, we compare the diffusion-based editor including Instructpix2pix [3] and ControlNet [83] in the context of portrait editing. ControlNet exhibited better consistency between the subject and the editing prompt than Instruct-\npix2pix. We further compared our method, Control4D, with the baseline approaches including Tensor4D, Tensor4D+GAN, and GaussianPlanes to validate the efficiency of our proposed representation and GAN. We evaluated the Fre\u0301chet Inception Distance (FID) metric [18] between the edited dataset and generated images. We also compute CLIP cosine similarity [53] between the generated images and text. Compared with Tensor4D and Tensor4D+GAN, our method achieves superior performance, which demonstrates the efficiency of GaussianPlanes. The results also reveals that our method outperforms the baseline and InstructNeRF2NeRF [17] significantly, demonstrating the effectiveness of our proposed 4D editing pipeline."
        },
        {
            "heading": "7. Conclusions",
            "text": "In conclusion, Control4D is a novel approach for efficient, high-fidelity and temporally consistent editing in dynamic 4D scenes. It utilizes an efficient 4D representation GaussianPlanes and a 2D diffusion-based editor. By utilizing plane-based decomposition to struct Gaussian Splat-\nting, GaussianPlanes ensure both efficiency and robustness for 4D editing. To tackle with the inconsistency caused by diffusion-based editor, Control4D leverages a GAN to generate from the editor, avoiding direct supervision. Experimental results demonstrate Control4D\u2019s effectiveness in achieving photo-realistic and consistent 4D editing, surpassing previous approaches in real-world scenarios. It represents a significant advancement in text-based image editing, particularly for dynamic scenes.\nLimitations. Due to utilizing a canonical Gaussian point clouds with flow representation, our approach relies on learning flow within the 4D scenes to exhibit simplicity and smoothness. This poses challenges for our method in effectively handling rapid and extensive non-rigid movements. Furthermore, our method is constrained by ControlNet, which limits the granularity of edits to a coarse level. Consequently, it is unable to perform precise expression or action edits. Our method also requires iterative optimizations for the editing process and cannot be accomplished in a single step."
        },
        {
            "heading": "B. More Comparisons",
            "text": "We conducted further comparisons with InstructNeRF2NeRF on their dataset. As shown in Fig. 13, our method noticeably surpasses InstructNeRF2NeRF in terms of realism and quality. Additionally, our optimization process is extremely fast, completing editing tasks in just 5 minutes, whereas InstructNeRF2NeRF requires at least about 5 hours. This makes our method 60 times more\nefficient than InstructNeRF2NeRF."
        },
        {
            "heading": "C. More Ablation Study",
            "text": "GaussianPlanes. We conducted more ablation studies on GaussianPlanes. As shown in Fig. 9, when the spatial triplane decomposition is not used, the results exhibit significant noise. Without the decomposition of flow, the edited results become noticeably blurred, with evident occurrences of jittering, which can be clearly observed in the supplementary video. This demonstrates that our proposed plane decomposition method makes Gaussian Splatting more structured and significantly enhances its robustness. 4D generator. More ablation experiments were conducted on our proposed 4D generator. As illustrated in Fig. 7, without the use of the generator and relying solely on GaussianPlanes, the images noticeably lose many high-quality de-\ntails and appear blurry. This validates the role of our 4D generator in enhancing quality.\nMulti-level guidance. Further ablation studies were performed on multi-level guidance. As shown in Fig. 11, when only GAN loss is used, mode collapse occurs easily due to the small size of the dataset. When only the first and third levels are used, the results become blurred. This indicates that our progressive guidance strategy improves the stability of the GAN and gradually enhances the quality of the rendered images."
        },
        {
            "heading": "D. More Results",
            "text": "Our method is applicable not only to the editing of halfbody and heads but also to complex 4D scenes and fullbody human editing. More results are illustrated in Fig. 12, 10, and 8. For dynamic editing effects, please refer to our supplementary video."
        },
        {
            "heading": "E. Social Impact",
            "text": "The primary goal of our method is to provide users with an advanced tool for dynamic human editing in complex 4D scenes. While our approach enables intricate editing of fullbody humans and facilitates creative expression in digital environments, it also raises concerns about potential misuse, such as creating deceptive or misleading content. This challenge is not exclusive to our method but is a common issue across various generative modeling techniques. Additionally, in line with ethical considerations, our approach underscores the importance of diversity, including aspects of gender, race, and cultural representation. It is crucial for ongoing and future research in generative modeling to\ncontinuously engage with and reevaluate these ethical considerations to ensure responsible use and positive societal impact."
        }
    ],
    "title": "Control4D: Efficient 4D Portrait Editing with Text",
    "year": 2023
}