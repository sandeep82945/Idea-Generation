{
    "abstractText": "In this paper, we present the results of the HSE-NN team in the 4th competition on Affective Behavior Analysis in-the-wild (ABAW). The novel multi-task EfficientNet model is trained for simultaneous recognition of facial expressions and prediction of valence and arousal on static photos. The resulting MT-EmotiEffNet extracts visual features that are fed into simple feed-forward neural networks in the multi-task learning challenge. We obtain performance measure 1.3 on the validation set, which is significantly greater when compared to either performance of baseline (0.3) or existing models that are trained only on the s-Aff-Wild2 database. In the learning from synthetic data challenge, the quality of the original synthetic training set is increased by using the super-resolution techniques, such as Real-ESRGAN. Next, the MT-EmotiEffNet is finetuned on the new training set. The final prediction is a simple blending ensemble of pre-trained and fine-tuned MT-EmotiEffNets. Our average validation F1 score is 18% greater than the baseline convolutional neural network. As a result, our team took the first place in the learning from synthetic data challenge and the third place in the multi-task learning challenge.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrey V. Savchenko"
        },
        {
            "affiliations": [],
            "name": "A.V. Savchenko"
        }
    ],
    "id": "SP:622a470da7ec2d483f74adf68d2c57a2a6176848",
    "references": [
        {
            "authors": [
                "T. Baltrusaitis",
                "A. Zadeh",
                "Y.C. Lim",
                "L.P. Morency"
            ],
            "title": "OpenFace 2.0: Facial behavior analysis toolkit",
            "venue": "Proceedings of the 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG",
            "year": 2018
        },
        {
            "authors": [
                "D. Deng",
                "B.E. Shi"
            ],
            "title": "Estimating multiple emotion descriptors by separating description and inference",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2392\u20132400",
            "year": 2022
        },
        {
            "authors": [
                "P. Foret",
                "A. Kleiner",
                "H. Mobahi",
                "B. Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "arXiv preprint arXiv:2010.01412",
            "year": 2020
        },
        {
            "authors": [
                "E. Jeong",
                "G. Oh",
                "S. Lim"
            ],
            "title": "Multi-task learning for human affect prediction with auditory-visual synchronized representation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2438\u20132445",
            "year": 2022
        },
        {
            "authors": [
                "D. Kollias"
            ],
            "title": "ABAW: Learning from synthetic data & multi-task learning challenges",
            "venue": "arXiv preprint arXiv:2207.01138",
            "year": 2022
        },
        {
            "authors": [
                "D. Kollias"
            ],
            "title": "ABAW: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2328\u20132336",
            "year": 2022
        },
        {
            "authors": [
                "D. Kollias",
                "S. Cheng",
                "M. Pantic",
                "S. Zafeiriou"
            ],
            "title": "Photorealistic facial synthesis in the dimensional affect space",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops. pp. 0\u20130",
            "year": 2018
        },
        {
            "authors": [
                "D. Kollias",
                "S. Cheng",
                "E. Ververas",
                "I. Kotsia",
                "S. Zafeiriou"
            ],
            "title": "Deep neural network augmentation: Generating faces for affect analysis",
            "venue": "International Journal of Computer Vision 128(5), 1455\u20131484",
            "year": 2020
        },
        {
            "authors": [
                "D. Kollias",
                "M.A. Nicolaou",
                "I. Kotsia",
                "G. Zhao",
                "S. Zafeiriou"
            ],
            "title": "Recognition of affect in the wild using deep neural networks",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 1972\u20131979. IEEE",
            "year": 2017
        },
        {
            "authors": [
                "D. Kollias",
                "V. Sharmanska",
                "S. Zafeiriou"
            ],
            "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
            "venue": "arXiv preprint arXiv:2105.03790",
            "year": 2021
        },
        {
            "authors": [
                "D. Kollias",
                "P. Tzirakis",
                "M.A. Nicolaou",
                "A. Papaioannou",
                "G. Zhao",
                "B. Schuller",
                "I. Kotsia",
                "S. Zafeiriou"
            ],
            "title": "Deep affect prediction in-the-wild: Aff-Wild database and challenge, deep architectures, and beyond pp",
            "venue": "1\u201323",
            "year": 2019
        },
        {
            "authors": [
                "D. Kollias",
                "S. Zafeiriou"
            ],
            "title": "Expression, affect, action unit recognition: Aff-Wild2, multi-task learning and arcface",
            "venue": "arXiv preprint arXiv:1910.04855",
            "year": 2019
        },
        {
            "authors": [
                "D. Kollias",
                "S. Zafeiriou"
            ],
            "title": "Va-stargan: Continuous affect generation",
            "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems. pp. 227\u2013 238. Springer",
            "year": 2020
        },
        {
            "authors": [
                "D. Kollias",
                "S. Zafeiriou"
            ],
            "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
            "venue": "arXiv preprint arXiv:2103.15792",
            "year": 2021
        },
        {
            "authors": [
                "A. Mollahosseini",
                "B. Hasani",
                "M.H. Mahoor"
            ],
            "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
            "venue": "IEEE Trans. Affective Computing 10(1), 18\u201331",
            "year": 2017
        },
        {
            "authors": [
                "M. Pourmirzaei",
                "G.A. Montazer",
                "F. Esmaili"
            ],
            "title": "Using self-supervised auxiliary tasks to improve fine-grained facial representation",
            "venue": "arXiv preprint arXiv:2105.06421",
            "year": 2021
        },
        {
            "authors": [
                "A. Savchenko",
                "A. Alekseev",
                "S. Kwon",
                "E. Tutubalina",
                "E. Myasnikov",
                "S. Nikolenko"
            ],
            "title": "Ad lingua: Text classification improves symbolism prediction in image advertisements",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics (COLING). pp. 1886\u20131892",
            "year": 2020
        },
        {
            "authors": [
                "A.V. Savchenko"
            ],
            "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
            "venue": "19th Int. Symposium on Intelligent Systems and Informatics (SISY). pp. 119\u2013124. IEEE",
            "year": 2021
        },
        {
            "authors": [
                "A.V. Savchenko"
            ],
            "title": "Video-based frame-level facial analysis of affective behavior on mobile devices using EfficientNets",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2359\u20132366",
            "year": 2022
        },
        {
            "authors": [
                "M. Tan",
                "Q. Le"
            ],
            "title": "EfficientNet: Rethinking model scaling for convolutional neural networks",
            "venue": "Int. Conf. Mach. Learn. pp. 6105\u20136114",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "L. Xie",
                "C. Dong",
                "Y. Shan"
            ],
            "title": "Real-ESRGAN: Training real-world blind super-resolution with pure synthetic data",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 1905\u20131914",
            "year": 2021
        },
        {
            "authors": [
                "Z. Wen",
                "W. Lin",
                "T. Wang",
                "G. Xu"
            ],
            "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
            "venue": "arXiv preprint arXiv:2109.07270",
            "year": 2021
        },
        {
            "authors": [
                "S. Zafeiriou",
                "D. Kollias",
                "M.A. Nicolaou",
                "A. Papaioannou",
                "G. Zhao",
                "I. Kotsia"
            ],
            "title": "Aff-Wild: Valence and arousal \u2018in-the-wild\u2019 challenge",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 1980\u20131987. IEEE",
            "year": 2017
        },
        {
            "authors": [
                "W. Zhang",
                "F. Qiu",
                "S. Wang",
                "H. Zeng",
                "Z. Zhang",
                "R. An",
                "B. Ma",
                "Y. Ding"
            ],
            "title": "Transformer-based multimodal information fusion for facial expression analysis",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. pp. 2428\u20132437",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords: Facial expression recognition, multi-task learning, learning from synthetic data, 4th Affective Behavior Analysis in-the-Wild (ABAW), EfficientNet"
        },
        {
            "heading": "1 Introduction",
            "text": "The problem of affective behavior analysis in-the-wild is to understand people\u2019s feelings, emotions, and behaviors. Human emotions are typically represented using a small set of basic categories, such as anger or happiness. More advanced representations include a discrete set of Action Units (AUs) from the Facial Action Coding System (FACS) Ekman\u2019s model and Russell\u2019s continuous encoding of affect in the 2-D space of arousal and valence. The former shows how passive or active an emotional state is, whilst the latter shows how positive or negative it is. Though the emotion of a person may be identified using various signals, such as voice, pronounced utterance, body language, etc., the most accurate results are obtained with facial analytics.\nar X\niv :2\n20 7.\n09 50\n8v 3\n[ cs\n.C V\n] 2\n0 O\nct 2\nDue to the high complexity of labeling emotions, existing emotional datasets for facial expression recognition (FER) are small and dirty. As a result, the trained models learn too many features specific to a concrete dataset, which is not practical for in-the-wild settings [6]. Indeed, they typically remain not robust to the diversity of environments and video recording conditions, so they can be hardly used in real-world settings with uncontrolled observation conditions. Hence, a lot of attention has been recently brought towards mitigating algorithmic bias in models and, in particular, cross-dataset studies.\nThis problem has become a focus of many researchers since an appearance of a sequence of the Affective Behavior Analysis in-the-wild (ABAW) challenges that involve different parts of the Aff-Wild [11,23] and Aff-Wild2 [9,12] databases. The organizers encourage participants to actively pre-train the models on other datasets by introducing all the new requirements. For example, one of the tasks of the third ABAW competition was the multi-task-learning (MTL) for simultaneous prediction of facial expressions, valence, arousal, and AUs [6,10]. Its winners [2] did not use the MTL small static training set (s-Aff-Wild2). Indeed, they proposed the Transformer-based Sign-and-Message Multi-Emotion Net that was trained on a large set of all video frames from the Aff-Wild2. The runner-up proposed the auditory-visual representations [4] that were also trained on initial video files. Similarly, the team that took fourth place in the MTL challenge developed the transformer-based multimodal framework [24] trained on the video frames from the Aff-Wild2 dataset that let them become the winners of FER and AU sub-challenges.\nAs a result, in the fourth ABAW competition [5], such usage of other data from Aff-Wild2 except the small training set is prohibited. It seems that only one successful participant (the third place) of the previous MTL challenge, namely, multi-head EfficientNet [19], and the baseline VGGFACE convolutional neural network (CNN) [6] satisfy new requirements. Moreover, a new challenge has been introduced that encourages the refinement of the pre-trained models on a set with synthetic faces generated from a small part of the Aff-Wild2 dataset [8,13,7].\nIn this paper, we discuss our solution for all tasks from the ABAW4 competition. It is proposed to improve the EfficientNet-based model [19] by pretraining a model on the AffectNet dataset [15] not only for FER but for additional prediction of valence and arousal. The visual embeddings are extracted from the penultimate layer of the resulting MT-EmotiEffNet, while the valence, arousal, and logits for each emotion are obtained at the output of its last layer. The multi-output feed-forward neural network is trained using the s-Aff-Wild2 database for the MTL challenge. The best validation results are obtained by simple blending [17] of its predictions with action unit features from the OpenFace 2 toolkit [1]. In the Learning from Synthetic Data (LSD) challenge, we propose to increase the quality of the original synthetic training set by using the super-resolution techniques, such as Real-ESRGAN [21], and fine-tuning the MT-EmotiEffNet on the new training set. The final prediction is a simple blend-\ning of scores at the output of pre-trained and fine-tuned MT-EmotiEffNets. The source code for the proposed solutions are made publicly available1.\nThis paper is organized as follows. Section 2 introduces the MT-EmotiEffNet model and the training procedures for both tasks. Experimental results are presented in Section 3. Concluding comments are discussed in Section 4."
        },
        {
            "heading": "2 Proposed Approach",
            "text": ""
        },
        {
            "heading": "2.1 Multi-task learning challenge",
            "text": "The main task of human affective behavior analysis is FER. It is a typical problem of image recognition in which an input facial image X should be associated with one of CEXPR > 1 categories (classes), such as anger, surprise, etc. There exist several commonly-used expression representations, such as estimation of Valence V and Arousal A (typically, V,A \u2208 [\u22121, 1]) and AU detection [6]. The latter task is a multi-label classification problem, i.e., prediction of a binary vector AU = [AU1, ..., AUCAU ], where CAU is the total number of AUs, and AUi \u2208 {0, 1} is a binary indicator of the presence of the i-th AU in the photo. 1 https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/\nsrc/ABAW\nIn this paper, we propose the novel model (Fig. 1) for the MTL challenge from ABAW4 [6], in which the method from [19] was modified as follows:\n1. Emotional feature extractor is implemented with the novel MT-EmotiEffNet model based on EfficientNet-B0 architecture [20], which was pre-trained not only for FER but for additional valence-arousal estimation. 2. The valence and arousal for the input facial photo are predicted by using only the output of the last layer of the MT-EmotiEffNet, i.e., they are not concatenated with embeddings extracted at its penultimate layer. 3. Three heads of the model for facial expressions, Valence-Arousal and AUs, respectively, are trained sequentially, i.e., we do not need additional masks for missing data.\nLet us consider the details of this pipeline. Its main part, namely, the MTEmotiEffNet model, was pre-trained using PyTorch framework identically to EfficientNet-B0 from [18] on face identification task. The facial regions in the training set are simply cropped by a face detector without margins or face alignment. Next, the last classification layer is replaced by a dense layer with 10 units for valence, arousal, and C(e) = 8 emotional categories (Neutral, Happy, Sad, Surprise, Fear, Anger, Disgust, Contempt) from the AffectNet dataset [15], respectively. The loss function is computed as a sum of Concordance Correlation Coefficients (CCCs) [14] and the weighted categorical cross-entropy [15]:\nL(X, y(V ), y(A), y(e)) = 1\u2212 0.5(CCC(z(V ), y(V )) + CCC(z(A), y(A)))\u2212 \u2212 log softmax(zy(e)) \u00b7 max\nc\u2208{1,...,C(e)} Nc/Ny(e) , (1)\nwhere X is the training image, y(e) \u2208 {1, ..., Ce} is its emotional class label, y(V ) and y(A) are the ground-truth valence and arousal, Nc is the total number of training examples of the c-th class, zy(e) is the FER score, i.e., y (e)-th output of the last (logits) layer, z(V ) and z(A) are the outputs of the last two units in the output layer, and softmax is the softmax activation function.\nThe imbalanced training set with 287,651 facial photos provided by the authors of the AffectNet [15] was used to train the MT-EmotiEffNet model, while the official balanced set of 4000 images (500 per category) was used for validation. At first, all weights except the new head were frozen, and the model was learned in 3 epochs using the Adam optimizer with a learning rate of 0.001 and SAM (Sharpness-Aware Minimization) [3]. Finally, we trained all weights of the model totally of 6 epochs with a lower learning rate (0.0001).\nIt is important to emphasize that the MT-EmotiEffNet feature extractor is not refined on the s-Aff-Wild2 dataset for the MTL challenge. Thus, every input X and reference Xn image is resized to 224x224 and fed into our CNN. We examine two types of features: (1) facial image embeddings (output of the penultimate layer) [18]; and (2) logits (predictions of emotional unnormalized probabilities at the output of the last layer). The outputs of penultimate layer [18] are stored in the D = 1280-dimensional embeddings x and xn, respectively. The concatenation of 8 FER logits, Valence, and Arousal at the output of the model are\nstored in the 10-dimensional logits l and ln. We experimentally noticed that the valence and arousal for the MTL challenge are better predicted by using the logits only, while the facial expression and AUs are more accurately detected if the embeddings x and logits l are concatenated.\nThe remaining part of our neural network (Fig. 1) contains three output layers, namely, (1) CEXPR = 8 units with softmax activation for recognition of one of eight emotions (Neutral, Anger, Disgust, Fear, Happiness, Sadness, Surprise, Other); (2) two neurons with tanh activation functions for Valence-Arousal prediction; and (3) CAU = 12 output units with sigmoid activation for AU detection. The model was trained using the s-Aff-Wild2 cropped aligned set provided by the organizers [6]. This set contains N = 142, 333 facial images {Xn}, n \u2208 {1, ..., N}, for which the expression en \u2208 {1, ..., CEXPR}, CAU -dimensional binary vector AUn of AUs, and/or valence Vn and arousal An are known. Some labels are missed, so only 90,645 emotional labels, 103,316 AU labels, and 103,917 values of Valence-Arousal are available for training. The validation set contains 26,876 facial frames, for which AU and VA are known, but only 15,440 facial expressions are provided. The three heads of the model, i.e., three fully-connected (FC) layers, were trained separately by using the Tensorflow 2 framework and the procedure described in the paper [19] for the uni-task learning.\nFinally, we examined the possibility to improve the quality by using additional facial features. The OpenFace 2 toolkit [1] extracted pose, gaze, eye, and AU features from each image. It was experimentally found that only the latter features are suitable for the FER part of the MTL challenge. Hence, we trained MLP (multi-layered perceptron) that contains an input layer with 35 AUs from the OpenFace, 1 hidden layer with 128 units and ReLU activation, and 1 output layer with CEXPR units and softmax activation. The component-wise weighted sum of the CEXPR outputs of this model and the emotional scores (posterior probabilities) at the output of \u201cFC layer, softmax\u201d (Fig. 1) is computed in the \u201cBlending\u201d layer, which returns the emotional class that corresponds to the maximal component of this weighted sum. The best weight in a blending is estimated by maximizing the average F1 score of FER on the validation set."
        },
        {
            "heading": "2.2 Learning from synthetic data challenge",
            "text": "In the second sub-challenge, it was required to solve the FER task and associate an input facial image with one of CEXPR = 6 basic expressions (Surprise, Fear, Disgust, Anger, Happiness, Sadness). The task is to refine the pre-trained model by using only information from 277,251 synthetic images that have been generated from some specific frames from the Aff-Wild2 database. The validation set contains 4,670 images for the same subjects.\nThe proposed pipeline (Fig. 2) uses the MT-EmotiEffNet from the previous Subsection in a straightforward way. Foremost, we noticed that the quality of provided synthetic facial images with a resolution of 112x112 is too low for our model that was trained on photos with a rather large resolution (224x224). Hence, it was proposed to enhance these images by using contemporary super-\nresolution (SR) techniques, such as Real-ESRGAN [21]. In particular, the pretrained model RealESRGAN x4plus with scale 2 was used for all training images.\nSince the required set of 6 expressions is a subset of eight emotions from AffectNet, the simplest solution is to ignore the training set of synthetic images, and predict expression for an image X with the pre-trained MT-EmotiEffNet by using only 6 scores (emotional posterior probabilities) s(pre)(X) from the last (softmax) layer [18] that is associated with required categories.\nTo use the provided synthetic data, we additionally fine-tuned the MTEmotiEffNet model on the training set at the output of Real-ESRGAN. At first, the output layer was replaced with the new fully-connected layer with CEXPR units, and the weights of the new head were learned during 3 epochs. The remaining weights were frozen. The weighted cross-entropy similar to (1) was minimized by the SAM [3] with Adam optimizer and learning rate of 0.001. Moreover, we examine the subsequent fine-tuning of all weights on synthetic data during 6 epochs with a learning rate of 0.0001. We can use either model with a new head or completely fine-tuned EfficientNet to predict CEXPR-dimensional vector of scores s(ft)(X) for the input image. To make a final decision, blending of pre-trained and fine-tune models is again applied by computing the weighted sum of scores: s(X) = w \u00b7 s(pre)(X) + (1 \u2212 w) \u00b7 s(ft)(X) [17]. The final decision is made in favor of the expression that corresponds to the maximal component of the vector s(X). The best value of the weight hyperparameter w \u2208 [0, 1], is estimated by maximizing the average F1 score on the validation set."
        },
        {
            "heading": "3 Experimental study",
            "text": ""
        },
        {
            "heading": "3.1 FER for static images",
            "text": "In this subsection, we compare the proposed MT-EmotiEffNet with several existing techniques for static photos from the validation set of AffectNet [15]. The accuracy of FER with 8 emotional categories and CCC and RMSE (root mean squared error) for valence and arousal estimates are summarized in Table 1.\nThough the MT-EmotiEffNet is not the best FER model, it has 0.6% greater accuracy when compared to a single-task model with identical training settings and architecture (EfficientNet-B0) [18]. One can conclude that taking valencearousal into account while training the model makes it possible to simultaneously solve multiple affect prediction tasks and extract better emotional features. The RMSE for valence prediction of the AlexNet [15] is slightly better than the RMSE of our model. However, as we optimized the CCC for valence and arousal (1), these metric is higher when compared to baseline EfficientNet in all cases."
        },
        {
            "heading": "3.2 Multi-task-learning challenge",
            "text": "In this subsection, the proposed pipeline (Fig. 1) for the MTL challenge is examined. At first, we study various features extracted by either known models (OpenFace AUs, EfficientNet-B0 [18] ) and our MT-EmotiEffNet. Visual embeddings x and/or logits l are fed into MLP with 3 heads. Three training datasets were used, namely, cropped and cropped aligned (hereinafter \u201caligned) provided by the organizers of this challenge and the aligned faces after Real-ESRGAN [21] as we proposed in Subsection 2.2.\nThe ablation results are presented in Table 2. Here, we used the performance metrics recommended by the organizers of the challenge, namely, PV A is the average CCC for valence and arousal, PEXPR is the macro-average F1 score for FER, PAU is the macro-average F1 score for AU detection, and PMTL = PV A + PEXPR + PAU . The best result in each column is marked in bold.\nThe proposed MT-EmotiEffNet has 0.06-0.07 greater overall quality PMTL when compared to EfficientNet-B0 features [18] even if all heads are trained simultaneously as described in the original paper [19]. If the heads are trained separately without adding masks for missing values, PMTL is increased by 0.02- 0.04. Moreover, it was experimentally found that valence and arousal are better predicted by using the logits only. Hence, we developed a multi-head MLP (Fig. 1) that feeds logits to the Valence-Arousal head but concatenates logits with embeddings for FER and AU heads. As a result, PV A was increased to 0.447, and the whole model reached PMTL = 1.276. Finally, we noticed that AU features at the output of OpenFace may be used for rather accurate FER, so that their ensemble with our model is characterized by the best average F1 score for facial expressions PEXPR = 0.357. It is remarkable that AU features from OpenFace are not well suited for the AU detection task in the MTL challenge, and their blending with our model cannot significantly increase PAU .\nThe detailed results of the proposed model (Fig. 1) for AU detection with the additional tuning of thresholds for AU detection, FER, and Valence-Arousal estimation are shown in Table 3, Table ,4 and Table 5, respectively. A comparison of our best model to existing baselines is summarized in Table 6. As one can notice, the usage of the proposed MT-EmotiEffNet significantly increased the performance of valence-arousal prediction and expression recognition. The AUs are also detected with up to 0.03 greater F1 scores. As a result, the performance measure PMTL of our model is 0.15 and 1.02 points greater when compared to the best model from the third ABAW challenge trained on s-Aff-Wild2 dataset only [19] and the baseline of the organizers [5], respectively."
        },
        {
            "heading": "3.3 Learning from synthetic data challenge",
            "text": "In this Subsection, our results for the LSD challenge are presented. We examine various EfficientNet-based models that have been pre-trained on AffectNet and\nfine-tuned on both an original set of synthetic images provided by the organizers and its enhancement by using SR techniques [21]. The official validation set of the organizers without SR was used in all experiments to make the metrics directly comparable. The macro-averaged F1 scores of individual models are presented in Table 7.\nHere, even the best pre-trained model is 9-10% more accurate than the baseline of the organizers (ImageNet ResNet-50) that reached an F1 score of 0.5 on the validation set [5]. As a result, if we replace the last classification layer with a new one and refine the model by training only the weights of the new head, the resulting F1 score will not be increased significantly. It is worth noting that the MT-EmotiEffNet is preferable to other emotional models in all these cases. Thus, our multi-task pre-training seems to provide better emotional features when compared to existing models. As was expected, the fine-tuning of all weights lead to approximately the same results for the same EfficientNet-B0 architecture. It is important to emphasize that the fine-tuning of the model on the training set after Real-ESRGAN leads to a 3-4% greater F1 score after tuning the whole model. Even if only a new head is learned, performance is increased by 1% when SR is applied.\nThe ablation study of the proposed ensemble (Fig. 2) is provided in Table 8. We noticed that as the synthetic data have been generated from subjects of the validation set, but not of the test set, the baseline\u2019s performance on the validation and test sets (real data from Aff-Wild2) is equal to 0.50 and 0.30, respectively [5]. Thus, the high accuracy on the validation set does not necessarily lead to the high testing quality. Hence, we estimated the F1 score not only on the official validation set from this sub-challenge but also on the validation set from the MTL competition. The frames with Neutral and Other expressions were removed to obtain a new validation set of 8,953 images with 6 expressions.\nAccording to these results, the original validation set seems to be not representative. For example, fine-tuning all weights leads to an increase in the validation F1 score by 6-9%. However, it decreases the performance by 6% for the new validation set if facial expressions from other subjects should be predicted. It is important to highlight that the pre-trained models are characterized by high F1 scores (41-42%) in the latter case. The proposed MT-EmotiEffNet is still 1% more accurate than EfficientNet-B0 [18]. What is more important, the\nfine-tuning of the new head of our model increased performance by 1%, while the same fine-tuning of EfficientNet-B0 caused significant degradation of the FER quality. The best results are achieved by blending the predictions of a pre-trained model and its fine-tuned head. In fact, this blending can be considered as adding a new classifier head to visual embeddings extracted by MT-EmotiEffNet, so only one inference in a deep CNN is required. For sure, the best results on the original validation set are provided by an ensemble with the model with re-training of all weights. The difference in confusion matrices of pre-trained MT-EmotiEffNet and our blending is shown in Fig. 3. Hence, our final submission included both ensembles that showed the best F1 score on the original validation set and the new one from the MTL challenge."
        },
        {
            "heading": "4 Conclusions",
            "text": "In this paper, we presented the multi-task EfficientNet-based model for simultaneous recognition of facial expressions, valence, and arousal that was pre-trained on static photos from the AffectNet dataset. Based on this model, we introduced two novel pipelines for MTL (Fig. 1) and LSD (Fig. 2) challenges in the fourth ABAW competition [6]. It was experimentally demonstrated that the proposed model significantly improves the results of either baseline VGGFACE CNN [5] or single-task EfficientNet-B0 [18] for both tasks (Tables 6, 8). It is worth noting that the best performance in both challenges is obtained by original pre-trained MT-EmotiEffNet without a need for fine-tuning of all weights on Aff-Wild2 data. Thus, the well-known issue of affect analysis techniques, namely, the subgroup distribution shift, is partially overcome in our models by training simple MLP-based predictors on top of facial emotional features extracted by the MTEmotiEffNet."
        },
        {
            "heading": "Acknowledgements",
            "text": "The research is supported by RSF (Russian Science Foundation) grant 20-71- 10010. The work in Section 3 was implemented in the framework of the Basic Research Program at the National Research University Higher School of Economics (HSE University), Russia in 2022."
        }
    ],
    "title": "HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images",
    "year": 2022
}