{
    "abstractText": "Physically-based rendering (PBR) is key for immersive rendering effects used widely in the industry to showcase detailed realistic scenes from computer graphics assets. A well-known caveat is that producing the same is computationally heavy and relies on complex capture devices. Inspired by the success in quality and efficiency of recent volumetric neural rendering, we want to develop a physically-based neural shader to eliminate device dependency and significantly boost performance. However, no existing lighting and material models in the current neural rendering approaches can accurately represent the comprehensive lighting models and BRDFs properties required by the PBR process. Thus, this paper proposes a novel lighting representation that models direct and indirect light locally through a light sampling strategy in a learned light sampling field. We also propose BRDF models to separately represent surface/subsurface scattering details to enable complex objects such as translucent material (i.e., skin, jade). We then implement our proposed representations with an end-to-end physically-based neural face skin shader, which takes a standard face asset (i.e., geometry, albedo map, and normal map) and an HDRI for illumination as inputs and generates a photo-realistic rendering as output. Extensive experiments showcase the quality and efficiency of our PBR face skin shader, indicating the effectiveness of our proposed lighting and material representations.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jing Yang"
        },
        {
            "affiliations": [],
            "name": "Hanyuan Xiao"
        },
        {
            "affiliations": [],
            "name": "Wenbin Teng"
        },
        {
            "affiliations": [],
            "name": "Yunxuan Cai"
        },
        {
            "affiliations": [],
            "name": "Yajie Zhao"
        }
    ],
    "id": "SP:1b7ae60093b4ad2037c676041142cacd5ab28979",
    "references": [
        {
            "authors": [
                "Sai Bi",
                "Zexiang Xu",
                "Pratul Srinivasan",
                "Ben Mildenhall",
                "Kalyan Sunkavalli",
                "Milo\u0161 Ha\u0161an",
                "Yannick Hold-Geoffroy",
                "David Kriegman",
                "Ravi Ramamoorthi"
            ],
            "title": "Neural reflectance fields for appearance acquisition",
            "venue": "arXiv preprint arXiv:2008.03824,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Bi",
                "Zexiang Xu",
                "Kalyan Sunkavalli",
                "Milo\u0161 Ha\u0161an",
                "Yannick Hold-Geoffroy",
                "David Kriegman",
                "Ravi Ramamoorthi"
            ],
            "title": "Deep reflectance volumes: Relightable reconstructions from multi-view photometric images",
            "venue": "arXiv preprint arXiv:2007.09892,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Bi",
                "Stephen Lombardi",
                "Shunsuke Saito",
                "Tomas Simon",
                "Shih-En Wei",
                "Kevyn Mcphail",
                "Ravi Ramamoorthi",
                "Yaser Sheikh",
                "Jason Saragih"
            ],
            "title": "Deep relightable appearance models for animatable faces",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Mark Boss",
                "Raphael Braun",
                "Varun Jampani",
                "Jonathan T Barron",
                "Ce Liu",
                "Hendrik Lensch"
            ],
            "title": "Nerd: Neural reflectance decomposition from image collections",
            "year": 2020
        },
        {
            "authors": [
                "Mark Boss",
                "Varun Jampani",
                "Raphael Braun",
                "Ce Liu",
                "Jonathan Barron",
                "Hendrik Lensch"
            ],
            "title": "Neuralpil: Neural pre-integrated lighting for reflectance decomposition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yajing Chen",
                "Fanzi Wu",
                "Zeyu Wang",
                "Yibing Song",
                "Yonggen Ling",
                "Linchao Bao"
            ],
            "title": "Self-supervised learning of detailed 3d face reconstruction",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Zhang Chen",
                "Anpei Chen",
                "Guli Zhang",
                "Chengyuan Wang",
                "Yu Ji",
                "Kiriakos N Kutulakos",
                "Jingyi Yu"
            ],
            "title": "A neural rendering framework for free-viewpoint relighting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Paul Debevec"
            ],
            "title": "The light stages and their applications to photoreal digital actors",
            "venue": "SIGGRAPH Asia,",
            "year": 2012
        },
        {
            "authors": [
                "Paul Debevec",
                "Tim Hawkins",
                "Chris Tchou",
                "Haarm-Pieter Duiker",
                "Westley Sarokin",
                "Mark Sagar"
            ],
            "title": "Acquiring the reflectance field of a human face",
            "venue": "In Proceedings of the 27th annual conference on Computer graphics and interactive techniques,",
            "year": 2000
        },
        {
            "authors": [
                "Abhijeet Ghosh",
                "Graham Fyffe",
                "Borom Tunwattanapong",
                "Jay Busch",
                "Xueming Yu",
                "Paul Debevec"
            ],
            "title": "Multiview face capture using polarized spherical gradient illumination",
            "venue": "In Proceedings of the 2011 SIGGRAPH Asia Conference,",
            "year": 2011
        },
        {
            "authors": [
                "Andrew Hou",
                "Michel Sarkis",
                "Ning Bi",
                "Yiying Tong",
                "Xiaoming Liu"
            ],
            "title": "Face relighting with geometrically consistent shadows",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "James T. Kajiya"
            ],
            "title": "The rendering equation",
            "venue": "SIGGRAPH Comput. Graph.,",
            "year": 1986
        },
        {
            "authors": [
                "Brian Karis",
                "Epic Games"
            ],
            "title": "Real shading in unreal engine 4",
            "venue": "Proc. Physically Based Shading Theory Practice,",
            "year": 2013
        },
        {
            "authors": [
                "Shichen Liu",
                "Tianye Li",
                "Weikai Chen",
                "Hao Li"
            ],
            "title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning",
            "venue": "The IEEE International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Stephen Lombardi",
                "Tomas Simon",
                "Gabriel Schwartz",
                "Michael Zollhoefer",
                "Yaser Sheikh",
                "Jason Saragih"
            ],
            "title": "Mixture of volumetric primitives for efficient neural rendering",
            "venue": "arXiv preprint arXiv:2103.01954,",
            "year": 2021
        },
        {
            "authors": [
                "Shugao Ma",
                "Tomas Simon",
                "Jason Saragih",
                "Dawei Wang",
                "Yuecheng Li",
                "Fernando De La Torre",
                "Yaser Sheikh"
            ],
            "title": "Pixel codec avatars",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Wan-Chun Ma",
                "Tim Hawkins",
                "Pieter Peers",
                "Charles-Felix Chabert",
                "Malte Weiss",
                "Paul E Debevec"
            ],
            "title": "Rapid acquisition of specular and diffuse normal maps from polarized spherical gradient illumination",
            "venue": "Rendering Techniques,",
            "year": 2007
        },
        {
            "authors": [
                "Nelson Max"
            ],
            "title": "Optical models for direct volume rendering",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 1995
        },
        {
            "authors": [
                "Abhimitra Meka",
                "Christian Haene",
                "Rohit Pandey",
                "Michael Zollh\u00f6fer",
                "Sean Fanello",
                "Graham Fyffe",
                "Adarsh Kowdle",
                "Xueming Yu",
                "Jay Busch",
                "Jason Dourgarian"
            ],
            "title": "Deep reflectance fields: highquality facial reflectance field inference from color gradient illumination",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Abhimitra Meka",
                "Rohit Pandey",
                "Christian H\u00e4ne",
                "Sergio Orts-Escolano",
                "Peter Barnum",
                "Philip DavidSon",
                "Daniel Erickson",
                "Yinda Zhang",
                "Jonathan Taylor",
                "Sofien Bouaziz"
            ],
            "title": "Deep relightable textures: volumetric performance capture with neural rendering",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Oliver Nalbach",
                "Elena Arabadzhiyska",
                "Dushyant Mehta",
                "H-P Seidel",
                "Tobias Ritschel"
            ],
            "title": "Deep shading: convolutional neural networks for screen space shading",
            "venue": "In Computer graphics forum,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Rebain",
                "Wei Jiang",
                "Soroosh Yazdani",
                "Ke Li",
                "Kwang Moo Yi",
                "Andrea Tagliasacchi"
            ],
            "title": "Derf: Decomposed radiance fields",
            "venue": "arXiv preprint arXiv:2011.12490,",
            "year": 2020
        },
        {
            "authors": [
                "Pratul P Srinivasan",
                "Boyang Deng",
                "Xiuming Zhang",
                "Matthew Tancik",
                "Ben Mildenhall",
                "Jonathan T Barron"
            ],
            "title": "Nerv: Neural reflectance and visibility fields for relighting and view synthesis",
            "year": 2012
        },
        {
            "authors": [
                "Mohammed Suhail",
                "Carlos Esteves",
                "Leonid Sigal",
                "Ameesh Makadia"
            ],
            "title": "Light field neural rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tiancheng Sun",
                "Jonathan T Barron",
                "Yun-Ta Tsai",
                "Zexiang Xu",
                "Xueming Yu",
                "Graham Fyffe",
                "Christoph Rhemann",
                "Jay Busch",
                "Paul E Debevec",
                "Ravi Ramamoorthi"
            ],
            "title": "Single image portrait relighting",
            "venue": "ACM Trans. Graph.,",
            "year": 2019
        },
        {
            "authors": [
                "Tiancheng Sun",
                "Kai-En Lin",
                "Sai Bi",
                "Zexiang Xu",
                "Ravi Ramamoorthi"
            ],
            "title": "Nelf: Neural lighttransport field for portrait view synthesis and relighting",
            "venue": "arXiv preprint arXiv:2107.12351,",
            "year": 2021
        },
        {
            "authors": [
                "Ayush Tewari",
                "Tae-Hyun Oh",
                "Tim Weyrich",
                "Bernd Bickel",
                "Hans-Peter Seidel",
                "Hanspeter Pfister",
                "Wojciech Matusik",
                "Mohamed Elgharib",
                "Christian Theobalt"
            ],
            "title": "Monocular reconstruction of neural face reflectance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Justus Thies",
                "Michael Zollh\u00f6fer",
                "Matthias Nie\u00dfner"
            ],
            "title": "Deferred neural rendering: Image synthesis using neural textures",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Dor Verbin",
                "Peter Hedman",
                "Ben Mildenhall",
                "Todd Zickler",
                "Jonathan T Barron",
                "Pratul P Srinivasan"
            ],
            "title": "Ref-nerf: Structured view-dependent appearance for neural radiance fields",
            "venue": "arXiv preprint arXiv:2112.03907,",
            "year": 2021
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "Highresolution image synthesis and semantic manipulation with conditional gans",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lior Yariv",
                "Yoni Kasten",
                "Dror Moran",
                "Meirav Galun",
                "Matan Atzmon",
                "Basri Ronen",
                "Yaron Lipman"
            ],
            "title": "Multiview neural surface reconstruction by disentangling geometry and appearance",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Yu",
                "Vickie Ye",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "arXiv preprint arXiv:2012.02190,",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Zhang",
                "Lifan Wu",
                "Changxi Zheng",
                "Ioannis Gkioulekas",
                "Ravi Ramamoorthi",
                "Shuang Zhao"
            ],
            "title": "A differential theory of radiative transfer",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Kai Zhang",
                "Fujun Luan",
                "Qianqian Wang",
                "Kavita Bala",
                "Noah Snavely"
            ],
            "title": "Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xiuming Zhang",
                "Sean Fanello",
                "Yun-Ta Tsai",
                "Tiancheng Sun",
                "Tianfan Xue",
                "Rohit Pandey",
                "Sergio Orts-Escolano",
                "Philip Davidson",
                "Christoph Rhemann",
                "Paul Debevec"
            ],
            "title": "Neural light transport for relighting and view synthesis",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2021
        },
        {
            "authors": [
                "Xiuming Zhang",
                "Pratul P Srinivasan",
                "Boyang Deng",
                "Paul Debevec",
                "William T Freeman",
                "Jonathan T Barron"
            ],
            "title": "Nerfactor: Neural factorization of shape and reflectance under an unknown illumination",
            "venue": "arXiv preprint arXiv:2106.01970,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Zhou",
                "Sunil Hadap",
                "Kalyan Sunkavalli",
                "David W Jacobs"
            ],
            "title": "Deep single-image portrait relighting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Physically-based rendering (PBR) provides a shading and rendering method to accurately represent how light interacts with objects in virtual 3D scenes. Whether working with a real-time rendering system in computer graphics or film production, employing a PBR process will facilitate the creation of images that look like they exist in the real world for a more immersive experience. Industrial PBR pipelines take the guesswork out of authoring surface attributes like transparency since their methodology and algorithms are based on physically accurate formulae and resemble real-world materials. This process relies on onerous artist tuning and high computational power in a long production cycle. In recent years, academia has shown incredible success using differentiable neural rendering in extensive tasks such as view synthesis (Mildenhall et al., 2020), inverse rendering (Zhang et al., 2021a), and geometry inference (Liu et al., 2019). Driven by the efficiency of neural rendering, a natural next step would be to marry neural rendering and PBR pipelines. However, none of the existing neural rendering representations supports the accuracy, expressiveness, and quality mandated by the industrial PBR process.\nA PBR workflow models both specular reflections, which refers to light reflected off the surface, and diffusion or subsurface scattering, which describes the effects of light absorbed or scattered internally. Pioneering works of differentiable neural shaders such as Softras (Liu et al., 2019) adopted the Lambertian model as BRDF representation, which only models the diffusion effects and results in low-quality rendering. NeRF (Mildenhall et al., 2020) proposed a novel radiance field representation for realistic view-synthesis under an emit-absorb lighting transport assumption without explicitly modeling BRDFs or lighting, and hence is limited to a fixed static scene with no scope for relighting. In follow-up work, NeRV (Srinivasan et al., 2020) took one more step by explicitly modeling directional light, albedo, and visibility maps to make the fixed scene relightable. The indirect illumination was achieved by ray tracing under the assumption of one bounce of incoming light.\n\u2217Equal contributions. We would like to thank Marcel Ramos and Chinmay Chinara at Vision and Graphics Lab (VGL), for their valuable help on data preparation and paper writing.\nar X\niv :2\n30 4.\n05 47\n2v 1\n[ cs\n.C V\n] 1\n1 A\npr 2\n02 3\nHowever, this lighting model is computationally very heavy for real-world environment illumination when more than one incoming directional lights exist. To address this problem, NeRD (Boss et al., 2020) and PhySG (Zhang et al., 2021a) employ a low-cost global environment illumination modeling method using spherical gaussian (SG) to extract parameters from HDRIs. Neural-PIL (Boss et al., 2021) further proposed a pre-trained light encoding network for a more detailed global illumination representation. However, it is still a global illumination representation assuming the same value for the entire scene, which is not true in the real world, where illumination is subjected to shadows and indirect illumination bouncing off of objects in different locations in the scene. Thus it\u2019s still an approximation but not an accurate representation of the environmental illumination. Regarding material (BRDF) modeling, all the current works adopt the basic rendering parameters (such as albedo, roughness, and metalness) defined in the rendering software when preparing the synthetic training data. However, they will fail in modeling intricate real-world objects such as participating media (e.g., smoke, fog) and translucent material (organics, skins, jade), where high scattering and subsurface scattering cannot be ignored. Such objects require more effort and hence attract more interest in research in their traditional PBR process.\nIn this work, we aim to design accurate, efficient lighting/ illumination and BRDF representations to enable the neural PBR process, which will support high-quality and photo-realistic rendering in a fast and lightweight manner. To achieve this goal, we propose a novel lighting representation - a Light Sampling Field to model both the direct and indirect illumination from HDRI environment maps. Our Light Sampling Field representation faithfully captures the direct illumination (incoming from light sources) and indirect illumination (summary of all indirect incoming lighting from surroundings) given an arbitrary sampling location in a continuous field. Accordingly, we propose BRDF representations in the format of surface specular, surface diffuse, and subsurface scattering for modeling real-world object material. This paper mainly evaluates the proposed representations with a novel volumetric neural physically-based shader for human facial skin. We trained with an extensive high-quality database, including real captured ground truth images as well as synthetic images for illumination augmentation. We also introduce a novel way of integrating surface normals into volumetric rendering for higher fidelity. Coupled with proposed lighting and BRDFs models, our light transport module delivers pore-level realism in both on- and underneath-surface appearance unprecedentedly. Experiments show that our Light Sampling Field is robust enough to learn illumination by local geometry. Such an effect usually can only be modeled by ray tracing. Therefore, our method compromises neither efficiency nor quality with the Light Sampling Field when compared to ray tracing.\nThe main contributions of this paper are as follows: 1) A novel volumetric lighting representation that accurately encodes the direct and indirect illumination positionally and dynamically given an environment map. Our local representation enables efficient modeling of complicated shading effects such as inter-reflectance in neural rendering for the first time as far as we are aware. 2) A BRDF measurement representation that supports the PBR process by modeling specular, diffuse, and subsurface scattering separately. 3) A novel and lightweight neural PBR face shader that takes facial skin assets and environment maps (HDRIs) as input and efficiently renders photo-realistic, highfidelity, and accurate images comparable to industrial traditional PBR pipelines such as Maya. Our face shader is trained with an image database consisting of extensive identities and illuminations. Once trained, our models will extract lighting models and BRDFs from input assets, which works well for novel subjects/ illumination maps. Experiments show that our PBR face shader significantly outperforms the state-of-the-art neural face rendering approaches with regard to quality and accuracy, which indicates the effectiveness of the proposed lighting and material representations."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Volumetric Neural Rendering Volumetric rendering models the light interactions with volume densities of absorbing, glowing, reflecting, and scattering materials (Max, 1995). A neural volumetric shader trains a model from a set of images and queries rendered novel images. The recent state-of-the-art was summarized in a survey (Tewari et al., 2020). In addition, Zhang et al. (2019) introduced the radiance field as a differentiable theory of radiative transfer. Neural Radiance Field (NeRF) (Mildenhall et al., 2020) further described scenes as differentiable neural representation and the following raycasting integrated color in terms of the transmittance factor, volume density, and the voxel diffuse color. Extensions to NeRF were developed for better image encoding (Yu et al., 2020), ray marching (Bi et al., 2020a), network efficiency (Lombardi et al., 2021; Yariv et al., 2020), realistic shading (Suhail et al., 2022) and volumetric radiative decomposition (Bi et al., 2020b; Rebain\net al., 2020; Zhang et al., 2021c; Verbin et al., 2021). In particular, NeRV (Srinivasan et al., 2020), NeRD (Boss et al., 2020; 2021) decomposed the reconstructed volume into geometry, SVBRDF, and illumination given a set of images even under varying lighting conditions. RNR (Chen et al., 2020b) assumed the environmental illumination as distant light and is able to decompose the scene into an albedo map with a 10-order spherical harmonics (SH) of incoming directions.\nPortrait and Face Relighting Early single-image relighting techniques utilize CNN-based image translation (Nalbach et al., 2017; Thies et al., 2019). Due to the lack of 3D models, image translation approaches cannot recover surface materials or represent realistic high-fidelity details, thus neural volumetric relighting approaches are widely adopted recently. Ma et al. (2021) proposed a lightweight representation to decode only the visible pixels during rendering. Bi et al. (2021) The face relighting utilized strong priors. Zhou et al. (2019) fitted a 3D face model to the input image and obtained refined normal to help achieve relighting. Chen et al. (2020a) relit the image by using spherical harmonics lighting on a predicted 3D face. Hou et al. (2022) introduced a shadow mask estimation module to achieve novel face relighting with geometrically consistent shadows. With high-quality volumetric capture in lightstage (Debevec et al., 2000) to obtain training data, this trend achieved the following: regression of a one-light-at-a-time (OLAT) image for relighting (Meka et al., 2019), encoding the feature tensors for a Phong shading into UV space and relighting using an HDRI map (Meka et al., 2020), a neural renderer that can predict the non-diffuse residuals (Zhang et al., 2021b). Bi et al. (2021) proposed neural networks to learn relighting implicitly but lacked modeling both surface and subsurface reflectance properties following physical light transport. Similar to our approach most, Sun et al. (2021) inferred both light transport and density, and enabled relighting and view synthesis from a sparse set of input images."
        },
        {
            "heading": "3 METHODS",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "We use the rendering equation Kajiya (1986) to estimate the radiance L at a 3D point x with the outgoing direction \u03c9o: L(x,\u03c9o) = \u222b \u03c9i\u2208\u2126+ f(x,\u03c9o,\u03c9i)Li(x,\u03c9i)(\u03c9i \u00b7nx)d\u03c9i, where f(x,\u03c9o,\u03c9i) is the BRDF representation and Li(x,\u03c9i) measures the radiance of incident light with direction \u03c9i.\nGenerally, the incident light could be categorized as direct vs. indirect light. Fig. 1 illustrates an example of direct light (path (a), (b)) and indirect light (path (c)) in human face skin, where subsurface scattering happens in the deeper dermis layer, causing the nearby area to receive indirect light. Therefore, the rendering formulation can be split into separate components with direct and indirect lighting:\nL(x,\u03c9o)= \u222b \u03c9i\u2208\u2126+ fs(x,\u03c9o,\u03c9i)L d i (x,\u03c9i)(\u03c9i \u00b7nx)d\u03c9i+ \u222b \u03c9i\u2208\u2126 fss(x,\u03c9o,\u03c9i)L id i (x,\u03c9i)(\u03c9i \u00b7nx)d\u03c9i (1) where fs, fss represent the BRDF evaluation of surface and subsurface, respectively. Following the render equation, we design a lightweight physically-based rendering method by learning different BRDF representations and modeling the direct and indirect lights, as we will introduce in the next few sections."
        },
        {
            "heading": "3.2 ILLUMINATION AND BIDIRECTIONAL REFLECTANCE DISTRIBUTION LEARNING",
            "text": "We propose a lighting model and material model to construct the Light Sampling Field and estimate the BRDF representations, as will be introduced in the next several sections."
        },
        {
            "heading": "3.2.1 LIGHTING MODEL",
            "text": "Considering the interaction between light and different skin layers, we propose a novel light modeling method using HDRIs that decomposes lighting into direct illumination and indirect illumination.\nFor direct illumination, We use light importance sampling to simulate external light sources, such as light bulbs. And we implement ray tracing for the specular reflectance effect. For indirect illumination, we introduce a Light Sampling Field that models location-aware illumination using SH. This learned local light-probe models subsurface scattering and inter-reflectance effects.\nImportance Light Sampling for Direct Illumination. Direct radiance comes from the HDRI map directly. In order to compute the contribution of pixels in HDRI to different points in our radiance field, we use a SkyDome to represent direct illumination by projecting the HDRI environment map onto a sphere. Each pixel on the sphere is regarded as a distant directional light source. Hence, direct lighting is identical at all locations in the radiance field. Such representation preserves specular reflection usually achieved by ray tracing methods. We take two steps to construct the representation. Firstly, we uniformly sample a point grid of size N = 800 on the sphere, where each point is a light source candidate. Secondly, we apply our importance light sampling method to filter valid candidates by two thresholds: 1) an intensity threshold that clips the intensity of outliers with extreme values; 2) an importance threshold that filters out outliers in textureless regions. Fig. 2a illustrates this process.\nLight Sampling Field for Indirect Illumination. Indirect illumination models the incoming lights reflected or emitted from surrounding objects, which is achieved by ray tracing with the assumption of limited bounce times in the traditional PBR pipeline. Inspired by the volumetric lightmaps used in Unreal Engine (Karis & Games, 2013), which stores precomputed lighting in sampled points and use them for interpolation at runtime for modeling indirect lighting of dynamic and moving objects, We adopt a continuous Light Sampling Field for accurately modeling the illumination variation in different scene positions. We use Spherical Harmonics (SH) to model the total incoming lights at each sampled location separately. We compute the local SH by multiplying fixed Laplace\u2019s SH basis with predicted SH parameters. Specifically, we use SH of degree l = 1 for each color channel (RGB). Therefore, we acquire 3 (color channels) \u00d7 4 (basis) = 12-D vector as local SH representation. We downsample the HDRI map to 100\u00d7 150 resolution and project it to a sphere. Each pixel on the map is considered an input lighting source. We use the direction and color of each pixel as the lighting embedding to feed into a light field sampling network for inference of\ncoefficients of local SH. We visualize our Light Sampling Field with selected discrete sample points in Fig. 2b. Learning Light Sampling Field. We design a network (Fig. 3b) to predict the spherical Harmonics coefficient Cmk of a continuous light field. The inputs of this network are the lighting embedding zl, the positional encoding of 3D location x, and view direction \u03c9. Conditioned on the lighting representations, the network succeeds in predicting the accurate and location-aware lighting. Fig. 6b evaluates our lighting model."
        },
        {
            "heading": "3.2.2 MATERIAL MODEL",
            "text": "The choice of reflectance parameters usually relies on artists\u2019 tuning and therefore requires high computational power and results in a long production cycle, but sometimes not very ideal. To tackle the problem, we propose a lightweight material network (Fig. 3a) to estimate the BRDF parameters including specular strength \u03b3 \u2208 R and skin scattering \u03b7 \u2208 R3 through learnable parameters. The parameters are crucial to represent the reflectance property of both surface and subsurface. Together with input albedo \u03c1, we can construct a comprehensive BRDF to model surface reflection and subsurface scattering. It consists of a surface specular component fss, a surface diffuse component fsd, and a subsurface scattering component fsss. Refer to Sec. 3.3 for a detailed light transport and Fig.5 for an evaluation of modeling subsurface scattering."
        },
        {
            "heading": "3.3 LIGHT TRANSPORT",
            "text": "Light transport defines a light path from the luminaire to the receiver. We introduce the light transport that connects our lighting model and material model in Fig. 4. We also detail the rendering equation in this section to match the light transport along the light path.\nVolume Casting. Our lighting model includes direct illumination and indirect illumination. Light transports from explicit lights and casts along the light path. We, therefore, define the light transport along the light path, p, in the volume by the following:\nL(x\u0302,\u03c9o) = \u222b \u221e 0 \u03c4 (t) \u00b7 \u03c3 (p(t)) \u00b7 L (p(t),\u03c9o) dt (2)\nwhere L(x\u0302,\u03c9o) is the total radiance at x\u0302 along the light path direction \u03c9o. \u03c3(x) is the volume density, which is converted from the input geometry. \u03c4(x) = exp(\u2212 \u222b x 0 \u03c3(p(t))dt) is the visibility that indicates whether the location x is visible on the light path, where p(t) represents the 3D location on the light path along \u03c9o from x\u0302 and defined as p(t) = x\u0302+ t\u03c9o.\nWe use explicit geometry to construct a more reliable density field. More specifically, we locate the intersection coordinates, x0, between any arbitrary light path and the input geometry. For any point x along the light path \u03c9, we define the density \u03c3 as the following central Gaussian distribution\nfunction: \u03c3(x) = \u03b1\u03c3 \u00b7 exp (\u2212dG(x)2/(2\u03b42)), where dG(x) is the distance between x and the intersection x0, \u03b1\u03c3 and \u03b4 are two scalars that determine the magnitude and standard deviation of the gaussian distribution.\nMaterial Scattering. The light transport between the scene objects and light sources is characterized by the rendering equation. Eqn. 1 introduce the rendering covering direct illumination and indirect illumination. Also, classified by reflection location, the reflected radiance L(x,\u03c9o) has two components: surface reflectance and subsurface volume scattering. To have a comprehensive light transport representation, we further develop the equation with the dissection of light as well as the specialized BSSRDF components:\nL(x,\u03c9o)= \u222b \u03c9i\u2208\u2126+ fss(x,\u03c9o,\u03c9i)L d i (x,\u03c9i)|\u03c9i \u00b7 nx|d\u03c9i\n\ufe38 \ufe37\ufe37 \ufe38 surface specular reflectance with direct light\n+ \u222b \u03c9i\u2208\u2126+ fsd(x,\u03c9o,\u03c9i)L d i (x,\u03c9i)|\u03c9i \u00b7 nx|d\u03c9i\n\ufe38 \ufe37\ufe37 \ufe38 surface diffuse reflectance with direct light\n+ \u222b \u03c9i\u2208\u2126 fsss(x,\u03c9o,\u03c9i)L id i (x,\u03c9i)|\u03c9i \u00b7 nx|d\u03c9i\n\ufe38 \ufe37\ufe37 \ufe38 subsurface scattering with indirect light\n(3)\nhere Ldi (x,\u03c9i) and L id i (x,\u03c9i) are the incoming direct and indirect radiance from direction \u03c9i at point x, respectively. fss, fsd, and fsss are the different counterparts of light transport parameterized by material representations. We show the complete light transport in Eqn. 4 consists of our light and material representation.\nL(x,\u03c9o)=\u03b3x \u00b7 \u222b \u03c9i\u2208\u2126+\nLdi (x,\u03c9i)|\u03c9o \u00b7R(\u03c9i,nx)|ed\u03c9i\ufe38 \ufe37\ufe37 \ufe38 surface specular reflectance with direct light\n+ \u03c1sx \u03c0 \u00b7 \u222b \u03c9i\u2208\u2126+\nLdi (x,\u03c9i)|\u03c9i \u00b7 nx|d\u03c9i\ufe38 \ufe37\ufe37 \ufe38 surface diffuse reflectance with direct light\n+ \u03c1ssx + \u03b7x \u03c0 \u00b7 \u222b \u03c9i\u2208\u2126\nLidi (x,\u03c9i)|\u03c9i \u00b7 nx|d\u03c9i\ufe38 \ufe37\ufe37 \ufe38 subsurface scattering with indirect light\n(4)\nwhere \u03b3x and \u03b7x are the predicted specular strength and scattering from material network at x. R(\u03c9i,nx) denotes the reflection direction of \u03c9i at the surface with normal nx and e denotes the specular exponent. Also, \u03c1sx and \u03c1 ss x are surface and subsurface albedo at x sampled from the input albedo map correspondingly with geometry."
        },
        {
            "heading": "4 IMPLEMENTATION DETAILS",
            "text": "In order to construct the density field \u03c3, we set \u03b1\u03c3 and \u03b4 to be 10 and 0.5, respectively. We further compare the rendering results of other values and visualize them in the Appendix. In the constructed radiance field, to sample rays, we draw 1024 random rays per batch. Along each ray, we sample 64 points for the shading model. The low-frequency location of 3D points and direction of rays are transformed to high-frequency input via positional encoding and directional encoding respectively (Mildenhall et al., 2020). The length of encoded position and view direction is 37 and 63 respectively in the material network and the Light Sampling Field network. Also, importance light sampling takes 800 light samples z \u2208 R3 from the HDRI input for direct lighting. We further downsample the input HDRI and embedded all pixels as a light embedding zl \u2208 R6\u00d715000 to feed into the Light Sampling Field network. We use an 8-layer MLP with 256 neurons in each layer for both networks. For the material network, encoded sample locations are fed in the first layer of the MLP, while the encoded view direction is later fed in layer 4. The output of material MLP for each queried 3D point is specular strength \u03b3 \u2208 R, and scattering \u03b7 \u2208 R3. The Light Sampling Field network has a similar network structure but also has direct light embedding in layer 4 as input and outputs encoded spherical coefficients Cmk \u2208 R12 for indirect lighting. During light transport, we obtain a weighted value for each ray based on \u03c4 distribution among the sampled points along the ray. After introducing values from pre-processed albedo and normal maps, the value of each component on rays is gathered and visualized as an image with pixels representing their intensities following our rendering Eqn. 4. Finally, the rendered RGB values are constrained with ground truth by an MSE loss. In our application, MLP modules can converge in 50, 000 iterations (2.6 hour) on a single Tesla V100, with decent results on the same level of detail as reference images."
        },
        {
            "heading": "5 EXPERIMENTS AND EVALUATION",
            "text": ""
        },
        {
            "heading": "5.1 TRAINING DATASET",
            "text": "Our training dataset is composed of a synthetic image dataset and a Lightstage-scanned image dataset. In synthetic dataset, we used a professionally-tuned Maya face shader to render 40-view colored images under all combinations between 21 face assets and 101 HDRI+86 OLAT illumination. Lightstage-scan dataset consists of 16-view captured colored images of 48 subjects in 27 expressions under white illumination. We carefully selected subjects in both dataset preparation to cover diverse ages, skin colors, and gender. Further details can be found in Appendix. A."
        },
        {
            "heading": "5.2 EVALUATION AND ANALYSIS",
            "text": "Material Model Evaluation. We conduct a comparison experiment with only surface scattering (BRDF) in Fig. 5, which presents two BRDF materials (middle two columns) and our proposed material with layer-by-layer decomposition (right four columns). From the comparison, CookTorrance BRDF has a more specular effect than Lambertian yet saves rubber-alike appearance from the uncanny valley. Apart from surface scattering, our method also predicts subsurface scattering to achieve a vivid look around the nose tip and ears by depicting red blood cells and vessel color.\nLighting Model Evaluation. We show our captured real data under a real fixed HDRI illumination in Fig. 6a. Fig. 6b illustrates uniform white ball illumination within a scene rendered by SH lighting (degree l = 1), Spherical Gaussian (SG) Neural-PIL(Boss et al., 2021) and our method, respectively. Compared with other models, our proposed method delivers the highest fidelity of illumination with the widest spectrum of light as well as a lighting field sensitive to lighting distribution. We further provide an extensive ablation study to validate our light sampling for modeling direct illumination in Fig. 15.\nEvaluation of Light and Material Modeling in indirect illumination. We evaluate our light and material components in Fig. 7. In (c), we use pre-calculated SH of degree l = 1 to model the global illumination and albedo as a diffuse scattering to render the face, resulting in a face image with strong shadows and an unnatural appearance. In (b), we introduce learned subsurface scattering in material but still use the same pre-calculated SH of degree l = 1 as global illumination, which results in color shift and artifacts. In (c), we further introduce local SH and infer a light sampling field to replace the pre-calculated SH. Together with the full spectrum of material layers, we achieve realistic rendering effects. In particular, we demonstrate interreflection effects in the zoom-in box. The shadow is softened by modeling the scattering and positional illumination.\nInverse Rendering. Our method can also achieve highfidelity inverse rendering with multi-view images (under various illumination) instead of geometry and texture maps as input. To make this possible, we additionally imple-\nmented MLP to predict a density field. We present our results under novel illumination in Fig. 8.\n5.3 QUALITATIVE RESULTS\nRendering on General Object Assets. We picked orange and meat in addition to face subjects to show that our method is generalizable on diverse objects with multi-layered structures in Fig. 9. Through testing on different organic materials, we show consistent sharpness and realistic appearance, especially in accomplishing specular reflection on orange and accurate soft shadow on meat.\nMaya Comparison. We compare our renderings with Maya, an\nindustry-level rendering engine, under HDRI or OLAT (one-light-at-a-time) illumination and present zoom-in pore-level details in Fig. 10. The zoom-in inspections show comparable or even better\nrendering results in their sharpness or illumination. Skin wrinkles and forehead specularity are particularly rich and sharp from the proposed method. At testing time, with the same rendering assets and queries (2500 \u00d7 2500 in resolution), our method requires the training images with only 800 \u00d7 800 in resolution. Under OLAT settings, our method casts hard shadows and soft shadows as accurately as Maya. More comparisons and qualitative results can be found in Fig. 17 and Fig. 18.\nQualitative Comparison. In Fig. 16, we compare the rendering results of FRF (Tewari et al., 2021), NeLF (Sun et al., 2021), SIPR (Sun et al., 2019), Neural-PIL (Boss et al., 2021), and our methods under HDRI and OLAT illuminations. We present more testing performance of our trainedonce models on other novel subjects in other datasets in Fig.19."
        },
        {
            "heading": "5.4 QUANTITATIVE RESULTS",
            "text": "We evaluated PSNR, SSIM, and LPIPS in quantitative measurements (Table 1) with Maya rendering as a benchmark. Specifically, LPIPS evaluates the perception on the VGG-19 network. Ours outperforms other baseline methods in all three metrics. Moreover, We do not provide the SSIM score of NeLF due to the slight view difference."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We demonstrate that the prior neural rendering representation for physically-based rendering fails to accurately model environment lighting or capture subsurface details. In this work, we propose a differentiable light sampling field network that models dynamic illumination and indirect lighting in a lightweight manner. In addition, we propose a flexible material network that models subsurface scattering for complicated materials such as the human face. Experiments on both synthetic and realworld datasets demonstrate that our light sampling field and material network collectively improve the rendering quality under complicated illumination compared with prior works. In the future, we will focus on modeling more complicated materials such as translucent materials and participated media. We will also collect datasets based on general objects and apply them for extensive tasks such as inverse rendering."
        },
        {
            "heading": "7 ACKNOWLEDGMENT",
            "text": "This research is sponsored by the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005. Army Research Office also sponsored this research under Cooperative Agreement Number W911NF-20-2-0053. We would also would like to acknowledge Sony Corporation of America R&D Center, US Lab for their support. Statements and opinions expressed, and content included, do not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. Further, the views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "heading": "A DATASETS",
            "text": "A.1 LIGHT STAGE\nFace Capture. We use the Light Stage (Debevec, 2012) to capture data for training purposes. The Light Stage features controllable lights and cameras, allowing us to capture multiview images using polarized spherical gradient illumination (Ma et al., 2007; Ghosh et al., 2011). By decomposing the specular and diffuse surface reflections from the captured images, we can generate image space diffuse albedo and high-frequency normals. We further fit the reconstructed 3D face to our template mesh for consistent UV space textures.\nOLAT Mapping. To simulate OLAT illumination, we approximate a directional light source by using an area light source positioned on a sphere. We then convert this area light into a High Dynamic Range Image (HDRI) map using equirectangular mapping. We illustrate the segmentations and corresponding OLAT mapping in Fig. 11.\nA.2 TRAINING DATASET\nOur training dataset consists of 1) a Light Stage-scanned multi-view colored image dataset under white illumination shown in Fig. 12 (a), 2) a synthetic multi-view colored image dataset rendered via a professionally-tuned Maya face shader shown in Fig. 12 (b). In the following sections, we introduce data composition and settings for synthetic data rendering and lightstage scanning.\nSynthetic Image Dataset Our input rendering assets to Maya renderer are 101 HDRI environment maps, 86 OLAT environment maps, and 21 face assets: the HDRI data covers various illuminations from outdoor open areas to small indoor areas; OLAT environment map cover 86 different regions in SkyDome as directional light; 21 subjects cover a variety of skin color, age, and gender. Each face asset consists of a coarse mesh, an albedo map, and a high-frequency normal map. We rendered 40 fixed-view RGBA-space images under all combinations of illumination and face assets. In total, we acquired 37, 160 images in 800\u00d7 800-pixel resolution for our synthetic image dataset.\nLightstage-scanned Real Image Dataset Synthetic data is insufficient to train a model with output rendering close to the real world. We utilized lightstage to additionally capture multi-view images for 48 subjects under uniform white illumination. Besides the diverse skin colors, gender, and age of subjects, the lightstage-scanned dataset consists of 27 expressions of each subject. We set up 16 fixed cameras covering different viewpoints of the frontal view. In total, we acquired 20, 736 images in 3008\u00d7 4096-pixel resolution for our real image dataset."
        },
        {
            "heading": "B MORE RESULTS AND ANALYSIS",
            "text": "Density Field Construction. We show how \u03b1\u03c3 and \u03b4 affect the constructed density field in Fig. 13. Larger \u03b4 results in a more evenly distributed density at samples along the light path. The constructed density field hence presents a coarse boundary around the input geometry. Therefore, the rendered results tend to have a blurry appearance. However, smaller \u03b4 approaches Dirac delta distribution, where only samples close to the intersections have valid density values. The rendered results thus have black stripes. Larger \u03b1\u03c3 alleviates the false density construction and results in accurate density values around the input geometry.\nHigh-frequency Surface Normal. To investigate the effectiveness of high-frequency surface normal as input, we compare results by re-\nplacing high-frequency input normal maps with a low-frequency one extracted from coarse geometry (Fig. 14). Given other input assets unchanged, rendered results with low-frequency normal maps show the incapability of extracting high-frequency information from input images. Therefore, high-frequency normal maps are crucial in achieving pore-level sharpness even though training data contains high-frequency details on images.\nAblation Study on Light Sampling We validate the Importance Light Sampling for simulating various external light sources with an extensive ablation study in Fig. 15. Under the same illumination conditions, the Importance Light Sampling can generate soft and appropriate diffuse reflection while preserving the accurate lighting distribution from the input HDRI. It ensures that the rendered images maintain the high degree of realism and fidelity to the original lighting conditions. In contrast, Uniform Spherical Sampling, while capable of representing the lighting environment with the same\nnumber of sampled lights, tends to produce hard shadows and may result in less detailed and overexposed images.\nQualitative Comparison. In Fig. 16, we compare the rendering results of FRF, NeLF, SIPR, Neural-PIL, and our methods under HDRI and OLAT (one-light-at-a-time) illuminations, as well as using multi-view images as input instead of geometry. To be more concrete, we identify different methods\u2019 input, output, and functionality in Table 2. Our method stands out for its ability to produce clear hard shadows resulting from full occlusion by face geometry, and soft shadows caused by indirect illumination. In contrast, Neural-PIL and NeLF do not model directional light and were not trained on OLAT data, so we compare them only under HDRI illumination. SIPR is an imagebased relighting method that models the scene in 2D, and cannot be queried from novel viewpoints. FRF, Neural-PIL, and NeLF, on the other hand, models in 3D. Neural-PIL inherits a per-scene-pertrain manner as NeRF and, thus is not generalizable on different subjects. In addition, we provide rendering results from Maya, a top-notch industrial renderer, as a reference for comparison.\nMore Qualitative Results. Fig. 18 presents a comprehensive collection of qualitative results. Each row showcases the rendering inputs, including the geometry, albedo map, and normal map, followed by six rendered images. The first three show different facial expressions under all-white illumination, while the last three display neutral expressions under different lighting conditions. By utilizing our photo-realistic neural renderer, we are able to render images at any resolution without compromising quality.\nRendering Speed. In addition to the rendering results, we also compare the rendering speed in Table 3. With the same specification of output and environment, our method is able to achieve up to 47-49 times faster with engineering acceleration (e.g. multi-thread processing)\nTesting on Other Datasets. To demonstrate that high-quality training data does not determine the robustness of the method, we evaluated our trained model over other available resources in Fig. 19. We converted three datasets to match our input as follows,\n\u2022 Triplegangers. We used FaceX 3DMM to fit and align geometries; albedo map was directly transferred from source; normal map was inferred using (Wang et al., 2018).\n\u2022 3D Scan Store. We used 3DMM to fit and align geometries; albedo and normal maps were provided and not further processed.\n\u2022 FaceScape. Geometries and albedo maps were provided and not further processed. The normal map was unavailable and not used.\nOur method delivers promising fidelity on all three testing datasets. First, our method is capable of adapting to different mesh topologies. For example, meshes in the Triplegangers dataset has denser vertices in the front face than behind while meshes in FaceScape have more uniform density, but our trained-once model performs equally well in both testing datasets. Second, our method does not sacrifice fidelity when input normal is unavailable during testing. Thanks to explicit geometry volume and robust model, pixels in the albedo map are precisely mapped onto the surface and therefore, yield no noise in rendering.\nFinally, we acquired and tested our pre-trained model over the Generated Photos dataset containing only low-frequency albedo and normal map in Fig. 20. The dataset is generated by some anonymous method with only a single-view online image as input. Our method not only outputs a clear silhouette but also shows realistic pre-level detail when high-frequency input is unavailable.\n(a) HDRI\n(b) OLATFigure 17: We show more qualitative comparisons of our method and Maya under (a) HDRI and (b) OLAT, with zoom-in on the images."
        }
    ],
    "title": "TION FOR PHYSICALLY-BASED NEURAL RENDERING",
    "year": 2023
}