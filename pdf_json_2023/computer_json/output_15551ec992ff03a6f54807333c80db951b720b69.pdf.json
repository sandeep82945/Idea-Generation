{
    "abstractText": "As the use of artificial intelligence (AI) models becomes more prevalent in industries such as engineering and manufacturing, it is essential that these models provide transparent reasoning behind their predictions. This paper proposes the AI-Reasoner, which extracts the morphological characteristics of defects (DefChars) from images and utilises decision trees to reason with the DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e. charts) and textual explanations to provide insights into outputs made by masked-based defect detection and classification models. It also provides effective mitigation strategies to enhance data pre-processing and overall model performance. The AI-Reasoner was tested on explaining the outputs of an IE Mask R-CNN model using a set of 366 images containing defects. The results demonstrated its effectiveness in explaining the IE Mask R-CNN model\u2019s predictions. Overall, the proposed AI-Reasoner provides a solution for improving the performance of AI models in industrial applications that require defect analysis.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiajun Zhang"
        },
        {
            "affiliations": [],
            "name": "Georgina Cosma"
        },
        {
            "affiliations": [],
            "name": "Sarah Bugby"
        },
        {
            "affiliations": [],
            "name": "Axel Finke"
        }
    ],
    "id": "SP:86c9bc404cc583a86f0751a5331bcbc3775ac8af",
    "references": [
        {
            "authors": [
                "I. Ahmed",
                "G. Jeon",
                "F. Piccialli"
            ],
            "title": "From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 8, pp. 5031\u20135042, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Cheng",
                "D. HongGui",
                "F. YuXin"
            ],
            "title": "Effects of Faster Regionbased Convolutional Neural Network on the Detection Efficiency of Rail Defects under Machine Vision",
            "venue": "2020 IEEE 5th Information Technology and Mechatronics Engineering Conference (ITOEC), 2020, pp. 1377\u20131380.",
            "year": 2020
        },
        {
            "authors": [
                "J. Lian",
                "W. Jia",
                "M. Zareapoor",
                "Y. Zheng",
                "R. Luo",
                "D.K. Jain",
                "N. Kumar"
            ],
            "title": "Deep-Learning-Based Small Surface Defect Detection via an Exaggerated Local Variation-Based Generative Adversarial Network",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 2, pp. 1343\u2013 1351, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "H. Dong",
                "K. Song",
                "Y. He",
                "J. Xu",
                "Y. Yan",
                "Q. Meng"
            ],
            "title": "PGA-Net: Pyramid Feature Fusion and Global Context Attention Network for Automated Surface Defect Detection",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 16, no. 12, pp. 7448\u20137458, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Yang",
                "Z. Liu",
                "G. Duan",
                "J. Tan"
            ],
            "title": "Mask2Defect: A Prior Knowledge-Based Data Augmentation Method for Metal Surface Defect Inspection",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 10, pp. 6743\u20136755, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Ni",
                "Z. Ma",
                "J. Liu",
                "B. Shi",
                "H. Liu"
            ],
            "title": "Attention Network for Rail Surface Defect Detection via Consistency of Intersection-over- Union(IoU)-Guided Center-Point Estimation",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 3, pp. 1694\u20131705, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Acciani",
                "G. Brunetti",
                "G. Fornarelli"
            ],
            "title": "Application of neural networks in optical inspection and classification of solder joints in surface mount technology",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 2, no. 3, pp. 200\u2013209, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "C. Phua",
                "L.B. Theng"
            ],
            "title": "Semiconductor Wafer Surface: Automatic Defect Classification with Deep CNN",
            "venue": "2020 IEEE REGION 10 CONFERENCE (TENCON), 2020, pp. 714\u2013719.",
            "year": 2020
        },
        {
            "authors": [
                "L. Wen",
                "Y. Wang",
                "X. Li"
            ],
            "title": "A New Cycle-consistent Adversarial Networks With Attention Mechanism for Surface Defect Classification With Small Samples",
            "venue": "IEEE Transactions on Industrial Informatics, vol. 18, no. 12, pp. 8988\u20138998, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "G. Fanci",
                "Z. Chune",
                "X. Ke"
            ],
            "title": "Image defect inspection based on human visual characteristics",
            "venue": "IET Conference Proceedings, pp. 377\u2013380(3), January 2011. [Online]. Available: https://digital-library. theiet.org/content/conferences/10.1049/cp.2011.1028",
            "year": 2011
        },
        {
            "authors": [
                "C. Laofor",
                "V. Peansupap"
            ],
            "title": "Defect detection and quantification system to support subjective visual quality inspection via a digital image processing: A tiling work case study",
            "venue": "Automation in Construction, vol. 24, pp. 160\u2013174, 2012. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0926580512000271",
            "year": 2012
        },
        {
            "authors": [
                "S. Dong",
                "X. Sun",
                "S. Xie",
                "M. Wang"
            ],
            "title": "Automatic defect identification technology of digital image of pipeline weld",
            "venue": "Natural Gas Industry B, vol. 6, no. 4, pp. 399\u2013403, 2019. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2352854019300749",
            "year": 2019
        },
        {
            "authors": [
                "X. Yan",
                "L. Gao"
            ],
            "title": "A feature extraction and classification algorithm based on improved sparse auto-encoder for round steel surface defects",
            "venue": "Mathematical Biosciences and Engineering, vol. 17, no. 5, pp. 5369\u20135394, 2020. [Online]. Available: https://www.aimspress.com/ article/doi/10.3934/mbe.2020290",
            "year": 2020
        },
        {
            "authors": [
                "M. Lee",
                "J. Jeon",
                "H. Lee"
            ],
            "title": "Explainable AI for domain experts: a post Hoc analysis of deep learning for defect classification of TFT\u2013LCD panels",
            "venue": "Journal of Intelligent Manufacturing, 2021, publisher Copyright: \u00a9 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",
            "year": 2021
        },
        {
            "authors": [
                "J. Lorentz",
                "T. Hartmann",
                "A. Moawad",
                "F. Fouquet",
                "D. Aouada"
            ],
            "title": "Explaining Defect Detection with Saliency Maps",
            "venue": "International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems. Springer, 2021, pp. 506\u2013518.",
            "year": 2021
        },
        {
            "authors": [
                "V. Bento",
                "M. Kohler",
                "P. Diaz",
                "L. Mendoza",
                "M.A. Pacheco"
            ],
            "title": "Improving deep learning performance by using Explainable Artificial Intelligence (XAI) approaches",
            "venue": "Discover Artificial Intelligence, vol. 1, no. 1, pp. 1\u201311, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "R.-K. Sheu",
                "L.-C. Chen",
                "M.S. Pardeshi",
                "K.-C. Pai",
                "C.-Y. Chen"
            ],
            "title": "AI Landing for Sheet Metal-Based Drawer Box Defect Detection Using Deep Learning (ALDB-DL)",
            "venue": "Processes, vol. 9, no. 5, 2021. [Online]. Available: https://www.mdpi.com/2227-9717/9/5/768",
            "year": 2021
        },
        {
            "authors": [
                "C. Seiffer",
                "H. Ziekow",
                "U. Schreier",
                "A. Gerling"
            ],
            "title": "Detection of Concept Drift in Manufacturing Data with SHAP Values to Improve Error Prediction",
            "venue": "DATA ANALYTICS 2021: The Tenth International Conference on Data Analytics, October 3-7, 2021, Barcelona, Spain, 2021, pp. 51\u201360.",
            "year": 2021
        },
        {
            "authors": [
                "G. Kolappan Geetha",
                "S.-H. Sim"
            ],
            "title": "Fast identification of concrete cracks using 1D deep learning and explainable artificial intelligencebased analysis",
            "venue": "Automation in Construction, vol. 143, p. 104572, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S0926580522004423",
            "year": 2022
        },
        {
            "authors": [
                "S. Zhou",
                "H. Liu",
                "K. Cui",
                "Z. Hao"
            ],
            "title": "JCS: An Explainable Surface Defects Detection Method for Steel Sheet by Joint Classification and Segmentation",
            "venue": "IEEE Access, vol. PP, pp. 1\u20131, 10 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A.S. Barnard"
            ],
            "title": "Explainable prediction of N-V-related defects in nanodiamond using neural networks and Shapley values",
            "venue": "Cell Reports Physical Science, vol. 3, no. 1, p. 100696, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S2666386421004215",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhang",
                "G. Cosma",
                "J. Watkins"
            ],
            "title": "Image Enhanced Mask R-CNN: A Deep Learning Pipeline with New Evaluation Measures for Wind Turbine Blade Defect Detection and Classification",
            "venue": "Journal of Imaging, vol. 7, no. 3, 2021. [Online]. Available: https://www.mdpi.com/2313-433X/7/3/46",
            "year": 2021
        },
        {
            "authors": [
                "S.M. Lundberg",
                "S.-I. Lee"
            ],
            "title": "A Unified Approach to Interpreting Model Predictions",
            "venue": "Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4765\u20134774. [Online]. Available: http://papers.nips.cc/ paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Morphological analysis, AI-Reasoner, defect characteristics, explainable AI.\nI. INTRODUCTION\nAs artificial intelligence (AI) models become increasingly prevalent in industries such as manufacturing, there is a growing need to explain the reasoning behind machine learning (ML) model outputs. Developing a comprehensive understanding of ML model outputs empowers end-users to make informed decisions based on the results [1]. Moreover, this understanding equips developers with vital information to effectively address and overcome any limitations inherent in the model. To address this challenge, researchers have developed a field of study known as explainable AI (XAI). XAI techniques use various strategies to provide interpretations for the AI model and its outputs, allowing non-experts to understand\nThis research is funded through joint funding by the School of Science at Loughborough University with industrial support from Railston & Co Ltd.\nhow the models arrive at their predictions. These strategies may involve generating explanations that can be understood by humans, visualising the model\u2019s decision-making process, or identifying the key features that the model relies on to make its predictions. By providing explanations of an AI model\u2019s outputs, XAI can help to increase the transparency and trustworthiness of the model and enable non-experts to make informed decisions based on the model\u2019s predictions. This can ultimately lead to improved safety, quality, and efficiency in industrial applications.\nIn the application of ML to defect detection [2]\u2013[6] and analysis [7]\u2013[9], there have been some attempts to provide explainability of outputs. Fanci et al. [10] used image morphology analysis and human visual characteristics (i.e. area, roundness, ratio, brightness, and texture) to identify image defects, and demonstrated that their method could successfully distinguish defects. Lafor and Peansupap [11] proposed a defect detection system that quantifies defects into a feature list (i.e. region, edges, scale, interest points, and texture) and which achieved 94% detection accuracy in a tiling defect detection task, and effectively reduced engineers\u2019 subjective judgements of aesthetic faults. Dong et al. [12] extracted geometry-, texture- and vision-based features from images to perform weld defect analysis using a support vector machine model, showing that defects could be identified with high accuracy over 90% in a pipeline weld defect detection task. Yan and Gao [13] extracted a set of optical image features, including colour-, texture- and shape-based features, from steel surface defect images and used those for a classification task for an engineering enterprise, and their model\u2019s performance reached an average of 98% accuracy.\nOne approach to XAI is through the use of saliency map methods [14]\u2013[18], that can explain neural network models by highlighting interesting regions in a defect image. Another\nar X\niv :2\n30 7.\n11 64\n3v 3\n[ cs\n.C V\n] 1\napproach is through the use of simplification and featurerelevance methods [19]\u2013[21] that can provide ML model explanations by extracting and analysing defect features from images and tabular data. These methods can help identify the most important features that a model relies on to make its predictions, enabling non-experts to better understand how the model derived its decisions.\nThis paper proposes the AI-Reasoner that facilitates reasoning with AI outputs. The contributions are:\n\u2022 A new set of defect characteristics (DefChars) for describing defects based on their morphological characteristics. DefChar values represent quantitative information about defects, including their colour, shape, and meta aspects. DefChars can be utilised in AI-based models for reasoning and defect analysis tasks. \u2022 A novel AI-Reasoner that extracts DefChars from images and generates a set of charts and textual descriptions to provide visualisation and reasoning with AI outputs. \u2022 Empirical demonstration of the AI-Reasoner on the outputs of a Mask region-based convolutional neural network (R-CNN) model using a set of 366 images containing defects. The results demonstrate the usefulness of the proposed DefChars and AI-Reasoner in explaining the model\u2019s predictions. The AI-Reasoner offers effective data pre-processing mitigation strategies aimed at enhancing model performance, and which can also save experimental time. Furthermore, this paper discusses the suitability of shapley additive explanations (SHAP) for Reasoning with Defect Predictions."
        },
        {
            "heading": "II. PROPOSED AI-REASONER",
            "text": "This section presents the AI-Reasoner\u2019s components, as illustrated in Fig. 1."
        },
        {
            "heading": "A. Detection and Classification",
            "text": "A mask-based deep learning (DL) model can be utilised to detect or classify defect regions. Such a model can be a Mask R-CNN that takes images and predicts the presence or categories of the defects."
        },
        {
            "heading": "B. Feature Extraction: Predictions to Reasoning Targets",
            "text": "The AI-Reasoner is able to reason the AI predictions for a detection task or a classification task. Let X be an image. Assume that this image shows I defects in pairwise disjoint (i.e. non-overlapping) regions. In other words, X \\ (X1\u222a\u00b7 \u00b7 \u00b7\u222aXI) is the remaining part of the image which does not contain any defects. Furthermore, let Li = L(Xi) \u2208 {1, . . . ,K} denote the type (category) of the defect in the ith region.\nAn AI model for defect detection will then output a prediction consisting of J pairs (X\u03021, L\u03021), . . . , (X\u0302J , L\u0302J), where X\u0302j \u2286 X is a predicted defect region and L\u0302j = L\u0302(X\u0302j) \u2208 {1, . . . ,K} is the predicted type of defect in that region.\nThroughout this work, it is assumed that a predicted defect can always be matched to at most one true defect; and that a true defect can always be matched to at most one predicted defect, e.g., via the intersection over union (IoU) value.\nFor the remainder of this work, reasoning targets (i.e. true positives and false negatives) will be defined as follows.\n\u2022 Detection. If only detection (but not classification) of defects is of interest, one can consider which of the true defects have been detected (or not) and set C = (C1, . . . , CI) and D = (D1, . . . , DI), where\nCi = { 1, if Xi matches some X\u0302j , 0, otherwise,\n(1)\nDi = { 1, if Xi does not match any X\u0302j , 0, otherwise,\n(2)\nfor i = 1, . . . , I . \u2022 Classification. If only classification (of already\nknown/correctly detected) defects is of interest (in which case I = J), one can set C \u2032 = (C \u20321, . . . , C \u2032 I) and D\u2032 = (D\u20321, . . . , D \u2032 I), where\nC \u2032i = { 1, if L\u0302i = Li, 0, otherwise, D\u2032i = { 1, if L\u0302i \u0338= Li, 0, otherwise,\nfor i = 1, . . . , I .\n\u2022 Joint detection and classification. If both detection and classification of defects are of interest, one can specify Ci and Di as in (1) and (2) and then set\nC \u2032i = { 1, if Ci = 1 and L\u0302(Xi) = Li, 0, otherwise,\nD\u2032i = { 1, if Ci = 1 and L\u0302(Xi) \u0338= Li, 0, otherwise,\nfor i = 1, . . . , I , where L\u0302(Xi) is the predicted category of the ith true defect. Finally, set C = (C1, . . . , CI), C \u2032 = (C \u20321, . . . , C \u2032 I), D = (D1, . . . , DI), D \u2032 = (D\u20321, . . . , D \u2032 I)."
        },
        {
            "heading": "C. Feature Extraction: Images to DefChar",
            "text": "DefChars are a new set of defect characteristics that are extracted from images by analysing the red, green and blue (RGB) values and the polygon-shaped defect regions/masks in an image. Table I presents the complete set of DefChars. Color characteristics: extract colour information using the hue, saturation, and brightness (HSV) values, converted from RGB values. HSV values can provide more intuitive colour properties than RGB values. The colour complexity features capture the frequency distribution differences of the HSV values between the defect and background areas. Shape characteristics: extract shape information from polygon annotations, including bounding boxes, vertices and edges. Shape complexity indicates the shape irregularity of the defect by calculating four values: the edge ratio, follow turning, reverse turning and small turning. Meta characteristics: \u2018Defect size\u2019 provides an indication of the severity of the defect. \u2018Distance to the nearest neighbour defect\u2019 provides information about a defect\u2019s environment."
        },
        {
            "heading": "D. AI Reasoning",
            "text": "The AI-Reasoner uses an ensemble decision tree (DT) to reason with the outputs of an AI-based defect detection model. Algorithm 1 introduces the pseudocode for the proposed AI-Reasoner, which comprises: PlantForest, ValidateForest, ClimbForest, AnalyseForest, SummariseForest and ExplainForest.\nEmpirical evaluations were carried out to determine the optimal parameters for the ensemble DTs. These are the recommended parameters when using the proposed AI-Reasoner. PlantForest takes the DefChar matrix E and reasoning target vector T as inputs where T is the reasoning target C, D, C \u2032 or D\u2032 (described in Section II-B). Then, 200 DTs are created with the untrimmed setting (i.e. max depth = -1: unrestricted tree depth, min split = 2: at least two samples in a node when splitting it, and min leaf = 1: at least one sample contained in a leaf node) for exploring reasons behind the predictions of an AI-based defect detection model. All trained DTs are stored in a list M for the next step. ValidateForest evaluates the learning performance of each trained DT model M using the metrics (true positive rate (TPR) and true negative rate (TNR)) defined in Section II-E.\nAlgorithm 1 AI-Reasoner. Input: DefChar matrix E of size m\u00d738, where m represents the number of defects in the dataset. Reasoning target vector T = C or T = D or T = C \u2032 or T = D\u2032, where C, D, C \u2032 and D\u2032 are as in Section II-B Output: Reasoning result R (charts and descriptions)\n1: M \u2190 PlantForest(E, T ) 2: V \u2190 ValidateForest(M,E, T ) 3: print V % defects have been correctly reasoned 4: O \u2190 ClimbForest(M) 5: A\u2190 AnalyseForest(O) 6: S \u2190 SummariseForest(A) 7: R\u2190 ExplainForest(S) 8: return R\nThe input data (DefChar matrix E and reasoning target vector T ) are utilised to assess how many defects were correctly learnt by each trained DT. Then, a learning score V provides the overall learning performance of the AI-Reasoner and the equation is shown in (3). ClimbForest parses all trained DTs and recursively reads the nodes in each tree and extracts tree information (i.e. DefChar features, split condition, the sample distributions in the node and its child nodes). The extracted information of every node is stored in a list O for further steps. AnalyseForest analyses the extracted list of DTs\u2019 nodes O and computes a set of values to determine the importance of each node in each DT. A decision score, DS, is the average split ratio in which the parent node divides the samples into its two child nodes:\nDS = 1\n2 (\u2223\u2223\u2223\u2223TC1 \u2212 FC1N1 \u2223\u2223\u2223\u2223+ \u2223\u2223\u2223\u2223TC0 \u2212 FC0N0 \u2223\u2223\u2223\u2223), where\n\u2022 N1 is the number of samples whose reasoning target values are 1 in a node; \u2022 N0 is the number of samples whose reasoning target values are 0 in a node; \u2022 TC1 is the number of samples whose reasoning target values are 1 in its true child node; \u2022 TC0 is the number of samples whose reasoning target values are 0 in its true child node; \u2022 FC1 is the number of samples whose reasoning target values are 1 in its false child node; \u2022 FC0 is the number of samples whose reasoning target values are 0 in its false child node.\nFurthermore, the distinguish score, TS, measures the percentage in which the parent node isolates the samples in its two child nodes:\nTS = \u2223\u2223\u2223\u2223TC1N1 \u2212 TC0N0 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223FC1N1 \u2212 FC0N0 \u2223\u2223\u2223\u2223.\nFinally, usage of samples, U , is the percentage of samples contained in a node:\nU = N1 +N0\nN ,\nwhere N is the total number of samples, i.e. at the root node. A set of additional values are computed. The node importance index, IDX, is calculated to state a node\u2019s capability in distinguishing samples that have different reasoning target\nvalues:\nIDX = U \u00d7 ((1 + DS)\u00d7 TS +Bdeg +Bsta).\nDecision degree bonus (Bdeg) and decision status bonus (Bsta) are calculated based on decision degree (DEG) and decision status (STA) (shown in Table II), and these two values are used to amplify the IDX value. DEG expresses the sample isolation degree (i.e. empty, weak, middle, strong and full) according to TS. STA expresses the node\u2019s splitting status (i.e. confirmation, half reduction, and reduction) according to\nthe values of TC1, TC0, FC1, FC0. Decision direction (DIR) is a boolean signal that indicates which child node contains more samples that reasoning target values are 1. The output is an extended list A, which contains the list O and these analysed values. SummariseForest computes the importance of each DefChar by summarising the list A. Firstly, all analysed nodes are divided into groups by DefChar; hence 38 groups of nodes are constructed where each node group corresponds to a unique DefChar. Next, DefChar Importance Index (DIS), DefChar Usage Frequency (DUF), DefChar Overall Score (DOS), DefChar Effective Range (DER) are calculated to quantify the importance of each DefChar as follows.\n\u2022 DIS is calculated by averaging the node importance indices IDX in the grouped nodes; a higher value indicates that the DefChar can easily affect the AI model\u2019s detection/classification performance. \u2022 DUF is calculated by averaging the occurrence of the DefChar used in the ensemble DT; a higher value implies that the DefChar is essential to split the reasoning targets. \u2022 DOS is the overall score for each DefChar calculated by multiplying the DIS and DUF with a 3:1 ratio. \u2022 DER is a value range to show the DefChar value interval that can affect the AI model\u2019s predictions.\nThe metrics defined in this section are utilised by ExplainForest to create the AI-Reasoner\u2019s output (i.e. charts and textual descriptions) for enabling the end-user to reason with a model\u2019s outputs. ExplainForest visualises the array-format reasoning result S using charts and provides textual descriptions that include mitigation strategies R to help users gain reasons behind the AI results and to take steps in improving the dataset that was used to train the models. Sample outputs are shown in the paper\u2019s Github repository1."
        },
        {
            "heading": "E. Metrics for Evaluating the AI-Reasoner\u2019s Learning",
            "text": "TPR and TNR evaluate the learning performance of each DT and are defined as\nTPR = TP\nTP + FN , TNR =\nTN\nTN+ FP ,\nwhere\n1https://github.com/edgetrier/AI-Reasoner\n\u2022 TP is the total number of defects whose reasoning target values are 1 and were correctly learnt by the DT; \u2022 TN is the total number of defects whose reasoning target values are 0 and were correctly learnt by the DT; \u2022 FP is the total number of defects whose reasoning target values are 1 and were not correctly learnt by DT; \u2022 FN is the total number of defects whose reasoning target values are 0 and were not correctly learnt by DT.\nTPR measures the ability of a model to correctly identify positive instances. TNR measures the ability of a model to correctly identify negative instances.\nThe learning score evaluates the overall learning performance of the reasoning model:\nLearning Score = 1\nn n\u2211 i=1 TPRi +TNRi 2 , (3)\nwhere n is the number (i.e. n = 200) of DTs that comprise the reasoning model; i is the index of a DT; TPRi and TNRi are the TPR and the TNR of the ith DT."
        },
        {
            "heading": "III. EXPERIMENT METHODOLOGY \u2013 UTILISING DEFCHARS TO REASON WITH AI MODELS",
            "text": "The proposed AI-Reasoner is applied to reason with the outputs of the image-enhanced mask R-CNN (IE-MRCNN) model proposed by Zhang et al. [22] which can detect the presence and types of wind turbine blade defects. Their paper evaluates the model\u2019s defect detection and type classification performance on an augmented dataset of defects (v1), the augmented dataset greyscaled (v2), and the augmented dataset after image enhancement (but not greyscaled) (v3). Their prediction results are shown in Table III."
        },
        {
            "heading": "A. Dataset",
            "text": "The dataset utilised for the experiments was provided by Zhang et al. [22]. Each defect is represented as a set of DefChar features stored in a 366 \u00d7 38 matrix E. The ground truth label (i.e. region and type) of each defect is stored in a set of pairs (X,L) where each pair (Xi, Li) corresponds to the ith row of matrix E. The predicted label of each defect is stored in a set of pairs (X\u0302, L\u0302)."
        },
        {
            "heading": "B. Methodology",
            "text": "Step 1: Apply the Mask R-CNN model to the dataset containing 366 defects with ground truth labels (i.e. regions X , types L) and obtain predicted labels (i.e. regions X\u0302 , types L\u0302) (described in Section III-A). Step 2: Extract the DefChar matrix E (366\u00d738) of the images and rescale the DefChar values using the min-max scaling method (described in Section II-C). Step 3: Convert and merge each image\u2019s ground truth labels (X,L) and predicted labels (X\u0302, L\u0302) of the Mask R-CNN model to four separate reasoning target vectors C, D, C \u2032 and D\u2032. Vectors C and D hold the reasoning targets of the correct and incorrect model outputs of the detection task, and vectors C \u2032 and D\u2032 hold the reasoning targets of the correct and incorrect outputs of the classification task (see Section II-B). Step 4: Apply the AI-Reasoner (see Section II-D) to the DefChar matrix E and each reasoning target vector T \u2208 {C,D,C \u2032, D\u2032}. Conduct four experiments, each with a different reasoning target: 1) reasoning with the outputs that were correctly detected; 2) reasoning with the outputs that were not detected; 3) reasoning with the outputs whose types were correctly classified; and 4) reasoning with the outputs whose types were not correctly classified. Step 5: Evaluate the learning performance of the AI-Reasoner across each experiment using the learning score metric described in Section II-E. The learning scores are averaged across four reasoning targets and presented in Table V that also contains the results when tuning the DT with different parameter settings (max depth, min split and min leaf) and using different DefChar combinations (see Table IV). Step 6: AI-Reasoner interprets the outputs, presents charts, textual explanations, and suggests mitigation strategies to the\nuser for improving prediction performance. These strategies are presented to the end-user in textual format (see Section IV-B). The user can follow the proposed mitigation strategies to improve their dataset and the model\u2019s performance."
        },
        {
            "heading": "IV. RESULTS",
            "text": ""
        },
        {
            "heading": "A. Reasoning Performance when using DefChars",
            "text": "This section describes the learning performance of the AIReasoner when using different parameters and combinations of DefChars. The learning scores for each parameter setting and combination are computed by averaging the learning scores across the four reasoning targets (see Section III-B step 4) and are shown in Table V. The highest learning score of each model is marked in bold text. The AI-Reasoner achieves a higher learning score if the DT is set with a deep tree depth, a small number of splits, and a small number of leaves. The learning score gradually increased from 50.31% (max depth = 1, min split = 2, and min leaf = 1) to 100% (max depth = infinity, min split = 2, and min leaf = 1) when applying all DefChars. The AI-Reasoner achieved the highest learning score of 100% when using all DefChars and the DT setting (max depth = infinity, min split = 2, and min leaf = 1); hence, this DefChar combination and DT settings are applied to the DTs that comprise AI-Reasoner.\nB. Interpretation of the AI-Reasoner\u2019s Outputs\nThe outputs of AI-Reasoner contain 38 charts in total for a reasoning target. Sample outputs are shown in the project\u2019s Github2 and in Fig. 2. Each chart illustrates the value range (i.e. DER) of a DefChar for a defect that was undetected or misclassified by the IE-MRCNN model. The charts provide the calculated scores (i.e. DIS, DUF and DOS) indicating the importance of a DefChar. These scores are shown at the top of each chart. The reasons for undetected cases include low hue range, neighbouring defects, and low saturation; and the reasons for misclassified cases include low hue range and strong saturation in the background, and narrow hue differences between the inside and outside of the defected area. Based on these findings, the AI-Reasoner suggests the following mitigation strategies: Greyscaling the defects may reduce misclassified but increase the undetected cases. Enhancing the\n2https://github.com/edgetrier/AI-Reasoner\nimages by normalising the colours of the defects may increase the detected cases and reduce the misclassified cases.\nZhang et al.\u2019s [22] experiments revealed that using a greyscaled dataset (v2) reduced the misclassified and increased the undetected cases compared to when using the augmented dataset (v1); and when using the image-enhanced dataset (v3), the detected cases were slightly decreased but the misclassified cases were reduced (see Table III). These results are mostly consistent with the mitigation strategies proposed by the AIReasoner."
        },
        {
            "heading": "C. Discussion on the suitability of SHAP for Reasoning with Defect Predictions",
            "text": "This section explains the outputs of a pre-trained CNN model using the SHAP algorithm [23], an XAI technique for reasoning with model outputs. SHAP cannot support objectdetection models and hence a simple CNN model was utilised for the type classification task instead of Zhang et al.\u2019s [22] IE-MRCNN model. Fig. 3 shows SHAP\u2019s output when given four defect images. The important regions are highlighted with blue and red colours based on the analysis of the hidden layers of the CNN. The main observations when comparing SHAP with the proposed AI-Reasoner for defect detection, SHAP is\nnot compatible with masked-based models that provide multichannel outputs (i.e. matrices that indicate the presence or absence of specific objects or regions within an image) or\nhave complex-structured designs (e.g. YOLO, Faster R-CNN, and Mask R-CNN). Whereas, the AI-Reasoner is compatible with masked-based models. SHAP does not provide details about a defect\u2019s characteristics that lead to its misclassification, whereas the AI-Reasoner captures that information in the form of DefChar charts and provides reasoning in the form of charts and textual explanations with mitigation strategies for improving the performance of the model."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "This paper proposes DefChars and an AI-Reasoner that extracts DefChars from images and utilises DTs to reason with AI outputs. The AI-Reasoner provides the end-user with charts, textual explanations and recommendations on improving the dataset pre-processing in order to improve a model\u2019s performance. Future work includes experiments with additional datasets, considering other prediction errors (e.g. false positive, duplicated predictions, etc.), DefChar applications, and exploring the AI-Reasoner\u2019s capability in saving experimental time via the mitigation strategies it provides."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "The authors acknowledge the expert guidance and datasets provided by Jason Watkins, Chris Gibson, and Andrew Rattray of Railston & Co Ltd."
        }
    ],
    "title": "Morphological Image Analysis and Feature Extraction for Reasoning with AI-based Defect Detection and Classification Models",
    "year": 2023
}