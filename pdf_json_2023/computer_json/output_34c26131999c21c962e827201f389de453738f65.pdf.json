{
    "abstractText": "Significant progress has been made in sentiment analysis over the past few years, especially due to the application of deep neural language models. However, there is a problem of transferability of trained models from one domain to another, especially for less studied languages such as Russian. We propose an approach to build cross-domain sentiment analysis models based on a two-stage procedure: first, we finetune a pre-trained RuBERT language model on a combined non-domain corpus, and then fine-tune this model on a small domain corpus. We conducted large-scale experiments with 30 sentiment annotated corpora across 12 domains. In order to increase the representativeness of news texts with high-quality annotation, we created a novel RuNews corpus, containing 1,823 news articles annotated by sentiment. The results show that finetuning the model using a small number (about several hundred) of annotated domain texts can significantly improve the performance of sentiment analysis for a new domain (on average by 4.6 p.p.). We also obtained the state-of-the-art results for 7 out of 14 test corpora. INDEX TERMS BERT, cross-domain models, neural language models, sentiment analysis",
    "authors": [
        {
            "affiliations": [],
            "name": "Anastasia V. Kotelnikova"
        },
        {
            "affiliations": [],
            "name": "Sergey V. Vychegzhanin"
        },
        {
            "affiliations": [],
            "name": "Evgeny V. Kotelnikov"
        }
    ],
    "id": "SP:c1accc9e09160892f3cb93ad09ecfef2e4044db7",
    "references": [
        {
            "authors": [
                "S. Poria",
                "D. Hazarika",
                "N. Majumder",
                "R. Mihalcea"
            ],
            "title": "Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research",
            "venue": "IEEE Trans. Affect. Comput., 2020, doi: 10.1109/TAFFC.2020.3038167.",
            "year": 2020
        },
        {
            "authors": [
                "B. Pang",
                "L. Lee",
                "S. Vaithyanathan"
            ],
            "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques",
            "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP 2002), 2002, pp. 79\u201386. doi: 10.3115/1118693.1118704.",
            "year": 2002
        },
        {
            "authors": [
                "P.D. Turney"
            ],
            "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews",
            "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, USA, 2002, pp. 417\u2013424. doi: 10.3115/1073083.1073153.",
            "year": 2002
        },
        {
            "authors": [
                "H. Jiang",
                "P. He",
                "W. Chen",
                "X. Liu",
                "J. Gao",
                "T. Zhao"
            ],
            "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online, 2020, pp. 2177\u20132190. doi: 10.18653/v1/2020.acl-main.197.",
            "year": 2020
        },
        {
            "authors": [
                "T. Al-Moslmi",
                "N. Omar",
                "S. Abdullah",
                "M. Albared"
            ],
            "title": "Approaches to Cross-Domain Sentiment Analysis: A Systematic Literature Review",
            "venue": "IEEE Access, vol. 5, pp. 16173\u201316192, 2017, doi: 10.1109/ACCESS.2017.2690342.",
            "year": 2017
        },
        {
            "authors": [
                "R.K. Singh",
                "M.K. Sachan",
                "R.B. Patel"
            ],
            "title": "360 degree view of cross-domain opinion classification: a survey",
            "venue": "Artif. Intell. Rev., vol. 54, no. 2, Art. no. 2, 2021, doi: 10.1007/s10462-020-09884-9.",
            "year": 2021
        },
        {
            "authors": [
                "A. Farahani",
                "S. Voghoei",
                "K. Rasheed",
                "H.R. Arabnia"
            ],
            "title": "A Brief Review of Domain Adaptation",
            "venue": "Advances in Data Science and Information Engineering, Cham, 2021, pp. 877\u2013894. doi: 10.1007/978-3-030-71704-9_65.",
            "year": 2021
        },
        {
            "authors": [
                "M. Hu",
                "Y. Wu",
                "S. Zhao",
                "H. Guo",
                "R. Cheng",
                "Z. Su"
            ],
            "title": "Domain- Invariant Feature Distillation for Cross-Domain Sentiment Classification",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China, 2019, pp. 5559\u20135568. doi: 10.18653/v1/D19-1558.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Du",
                "M. He",
                "L. Wang",
                "H. Zhang"
            ],
            "title": "Wasserstein based transfer network for cross-domain sentiment classification",
            "venue": "Knowl.-Based Syst., vol. 204, p. 106162, 2020, doi: 10.1016/j.knosys.2020.106162.",
            "year": 2020
        },
        {
            "authors": [
                "T. Manshu",
                "W. Bing"
            ],
            "title": "Adding Prior Knowledge in Hierarchical Attention Neural Network for Cross Domain Sentiment Classification",
            "venue": "IEEE Access, vol. 7, pp. 32578\u201332588, 2019, doi: 10.1109/ACCESS.2019.2901929.",
            "year": 2019
        },
        {
            "authors": [
                "A. Golubev",
                "N. Loukachevitch"
            ],
            "title": "Transfer Learning for Improving Results on Russian Sentiment Datasets,\u201d in Proceedings of the International Conference \u201cDialogue 2021",
            "venue": "2021, pp. 268\u2013 277. doi: 10.28995/2075-7182-2021-20-268-277.",
            "year": 2021
        },
        {
            "authors": [
                "S. Smetanin",
                "M. Komarov"
            ],
            "title": "Deep transfer learning baselines for sentiment analysis in Russian",
            "venue": "Inf. Process. Manag., vol. 58, no. 3, Art. no. 3, 2021, doi: 10.1016/j.ipm.2020.102484.",
            "year": 2021
        },
        {
            "authors": [
                "E. Kotelnikov"
            ],
            "title": "Current Landscape of the Russian Sentiment Corpora,\u201d in Proceedings of the International Conference \u201cDialogue 2021",
            "venue": "2021, pp. 433\u2013444. doi: 10.28995/2075-7182- 2021-20-433-444.",
            "year": 2021
        },
        {
            "authors": [
                "A. Geethapriya",
                "S. Valli"
            ],
            "title": "An Enhanced Approach to Map Domain-Specific Words in Cross-Domain Sentiment Analysis",
            "venue": "Inf. Syst. Front., vol. 23, no. 3, Art. no. 3, 2021, doi: 10.1007/s10796- 020-10094-5.",
            "year": 2021
        },
        {
            "authors": [
                "X. Qu",
                "Z. Zou",
                "Y. Cheng",
                "Y. Yang",
                "P. Zhou"
            ],
            "title": "Adversarial Category Alignment Network for Cross-domain Sentiment Classification",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, 2019, pp. 2496\u20132508. doi: 10.18653/v1/N19-1258.",
            "year": 2019
        },
        {
            "authors": [
                "K. Zhang",
                "H. Zhang",
                "Q. Liu",
                "H. Zhao",
                "H. Zhu",
                "E. Chen"
            ],
            "title": "Interactive Attention Transfer Network for Cross-Domain Sentiment Classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2019, vol. 33 (01), pp. 5773\u20135780. doi: 10.1609/aaai.v33i01.33015773.",
            "year": 2019
        },
        {
            "authors": [
                "L. Li",
                "W. Ye",
                "M. Long",
                "Y. Tang",
                "J. Xu",
                "J. Wang"
            ],
            "title": "Simultaneous Learning of Pivots and Representations for Cross-Domain Sentiment Classification",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 2020, vol. 34 (05), pp. 8220\u20138227. doi: 10.1609/aaai.v34i05.6336.",
            "year": 2020
        },
        {
            "authors": [
                "B. Myagmar",
                "J. Li",
                "S. Kimura"
            ],
            "title": "Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models",
            "venue": "IEEE Access, vol. 7, pp. 163219\u2013163230, 2019, doi: 10.1109/ACCESS.2019.2952360.",
            "year": 2019
        },
        {
            "authors": [
                "J. Zhou",
                "J. Tian",
                "R. Wang",
                "Y. Wu",
                "W. Xiao",
                "L. He"
            ],
            "title": "SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis",
            "venue": "Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain (Online), 2020, pp. 568\u2013579. doi: 10.18653/v1/2020.coling-main.49.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Lei",
                "Y. Li"
            ],
            "title": "A novel scheme of domain transfer in documentlevel cross-domain sentiment classification",
            "venue": "J. Inf. Sci., 2021, doi: 10.1177/01655515211012329.",
            "year": 2021
        },
        {
            "authors": [
                "C. Zhao",
                "S. Wang",
                "D. Li"
            ],
            "title": "Multi-source domain adaptation with joint learning for cross-domain sentiment classification",
            "venue": "Knowl.- Based Syst., vol. 191, p. 105254, 2020, doi: 10.1016/j.knosys.2019.105254.",
            "year": 2020
        },
        {
            "authors": [
                "J. Yuan",
                "Y. Zhao",
                "B. Qin"
            ],
            "title": "Learning to share by masking the non-shared for multi-domain sentiment classification",
            "venue": "Int. J. Mach. Learn. Cybern., 2022, doi: 10.1007/s13042-022-01556-0.",
            "year": 2022
        },
        {
            "authors": [
                "J. Blitzer",
                "M. Dredze",
                "F. Pereira"
            ],
            "title": "Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",
            "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic, 2007, pp. 440\u2013447. Accessed: May 28, 2022. [Online]. Available: https://aclanthology.org/P07-1056",
            "year": 2007
        },
        {
            "authors": [
                "I. Mozeti\u010d",
                "M. Gr\u010dar",
                "J. Smailovi\u0107"
            ],
            "title": "Multilingual Twitter Sentiment Classification: The Role of Human Annotators",
            "venue": "PLoS ONE, vol. 11, no. 5, Art. no. 5, 2016, doi: 10.1371/journal.pone.0155036.",
            "year": 2016
        },
        {
            "authors": [
                "I. Chetviorkin",
                "P. Braslavskiy",
                "N. Loukachevitch"
            ],
            "title": "Sentiment Analysis Track at ROMIP 2011,\u201d in Proceedings of the International Conference \u201cDialogue 2012",
            "venue": "2012, vol. 2, pp. 1\u201314.",
            "year": 2012
        },
        {
            "authors": [
                "I. Chetviorkin",
                "N. Loukachevitch"
            ],
            "title": "Sentiment Analysis Track at ROMIP 2012,\u201d in Proceedings of the International Conference \u201cDialogue 2013",
            "venue": "2013, vol. 2, pp. 40\u201350.",
            "year": 2013
        },
        {
            "authors": [
                "N. Loukachevitch",
                "P. Blinov",
                "E. Kotelnikov",
                "Y. Rubtsova",
                "V. Ivanov",
                "E. Tutubalina"
            ],
            "title": "SentiRuEval: Testing object-oriented sentiment analysis systems in Russian,\u201d in Proceedings of the International Conference \u201cDialogue 2015",
            "venue": "2015, vol. 2, pp. 3\u201313.",
            "year": 2015
        },
        {
            "authors": [
                "Y.V. Adaskina",
                "P.V. Panicheva",
                "A.M. Popov"
            ],
            "title": "Syntax-based Sentiment analysis of tweet in Russian,\u201d in Proceedings of the International Conference \u201cDialogue 2015",
            "venue": "2015, vol. 2, pp. 1\u201311.",
            "year": 2015
        },
        {
            "authors": [
                "N.V. Loukachevitch",
                "Y.R. Rubtsova"
            ],
            "title": "SentiRuEval-2016: overcoming time gap and data sparsity in tweet sentiment analysis,\u201d in Proceedings of the International Conference \u201cDialogue 2016",
            "venue": "2016, pp. 416\u2013426.",
            "year": 2016
        },
        {
            "authors": [
                "K. Arkhipenko",
                "I. Kozlov",
                "J. Trofimovich",
                "K. Skorniakov",
                "A. Gomzin",
                "D. Turdakov"
            ],
            "title": "Comparison of neural network architectures for sentiment analysis of russian tweets,\u201d in Proceedings of the International Conference \u201cDialogue 2016",
            "venue": "2016, pp. 50\u201359.",
            "year": 2016
        },
        {
            "authors": [
                "A. Zvonarev",
                "A. Bilyi"
            ],
            "title": "A Comparison of Machine Learning Methods of Sentiment Analysis Based on Russian Language Twitter Data",
            "venue": "Proceedings of the 11th Majorov International Conference on Software Engineering and Computer Systems (MICSECS 2019), Saint Petersburg, December, 2019, vol. 2590.",
            "year": 2019
        },
        {
            "authors": [
                "D.R. Baymurzina",
                "D.P. Kuznetsov",
                "M.S. Burtsev"
            ],
            "title": "Language model embeddings improve sentiment analysis in Russian,\u201d in Proceedings of the International Conference \u201cDialogue 2019",
            "venue": "2019, pp. 53\u201362.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Kuratov",
                "M. Arkhipov"
            ],
            "title": "Adaptation of deep bidirectional multilingual transformers for Russian language,\u201d in Proceedings of the International Conference \u201cDialogue 2019",
            "venue": "2019, pp. 333\u2013340.",
            "year": 2019
        },
        {
            "authors": [
                "A. Golubev",
                "N. Loukachevitch"
            ],
            "title": "Improving Results on Russian Sentiment Datasets",
            "venue": "Artificial Intelligence and Natural Language, Cham, 2020, pp. 109\u2013121. doi: 10.1007/978-3-030- 59082-6_8.",
            "year": 2020
        },
        {
            "authors": [
                "M. Pontiki"
            ],
            "title": "SemEval-2016 Task 5: Aspect Based Sentiment Analysis",
            "venue": "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), San Diego, California, 2016, pp. 19\u201330. doi: 10.18653/v1/S16-1002.",
            "year": 2016
        },
        {
            "authors": [
                "O. Koltsova",
                "S. Alexeeva",
                "S. Kolcov"
            ],
            "title": "An opinion word lexicon and a training dataset for Russian sentiment analysis of social media,\u201d in Proceedings of the International Conference \u201cDialogue 2016",
            "venue": "2016, pp. 277\u2013287.",
            "year": 2016
        },
        {
            "authors": [
                "A. Rogers",
                "A. Romanov",
                "A. Rumshisky",
                "S. Volkova",
                "M. Gronas",
                "A. Gribov"
            ],
            "title": "RuSentiment: An Enriched Sentiment Analysis Dataset for Social Media in Russian",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, Santa Fe, New Mexico, USA, 2018, pp. 755\u2013763. Accessed: May 28, 2022. [Online]. Available: https://aclanthology.org/C18-1064",
            "year": 2018
        },
        {
            "authors": [
                "V. Rybakov",
                "A. Malafeev"
            ],
            "title": "Aspect-Based Sentiment Analysis of Russian Hotel Reviews",
            "venue": "Supplementary Proceedings of the 7th International Conference on Analysis of Images, Social Networks and Texts (AIST-SUP 2018), Moscow, Russia, July 5-7, 2018, pp. 75\u201384.",
            "year": 2018
        },
        {
            "authors": [
                "S. Smetanin",
                "M. Komarov"
            ],
            "title": "Sentiment Analysis of Product Reviews in Russian using Convolutional Neural Networks",
            "venue": "2019 IEEE 21st Conference on Business Informatics (CBI), 2019, vol. 1, pp. 482\u2013486. doi: 10.1109/CBI.2019.00062.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Rubtsova"
            ],
            "title": "Automatic Term Extraction for Sentiment Classification of Dynamically Updated Text Collections into Three Classes",
            "venue": "Knowledge Engineering and the Semantic Web, Cham, 2014, pp. 140\u2013149. doi: 10.1007/978-3-319-11716-4_12.",
            "year": 2014
        },
        {
            "authors": [
                "J. Devlin",
                "M.-W. Chang",
                "K. Lee",
                "K. Toutanova"
            ],
            "title": "BERT: Pretraining of Deep Bidirectional Transformers for Language Understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Minneapolis, Minnesota, 2019, pp. 4171\u20134186. doi: 10.18653/v1/N19-1423.",
            "year": 2019
        },
        {
            "authors": [
                "A. Vaswani"
            ],
            "title": "Attention is All You Need",
            "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2017, vol. 30, pp. 6000\u2013 6010.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Wu"
            ],
            "title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.",
            "venue": "arXiv, 2016. doi: 10.48550/arXiv.1609.08144",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2023 1\ndue to the application of deep neural language models. However, there is a problem of transferability of\ntrained models from one domain to another, especially for less studied languages such as Russian. We propose an approach to build cross-domain sentiment analysis models based on a two-stage procedure: first, we finetune a pre-trained RuBERT language model on a combined non-domain corpus, and then fine-tune this model on a small domain corpus. We conducted large-scale experiments with 30 sentiment annotated corpora across 12 domains. In order to increase the representativeness of news texts with high-quality annotation, we created a novel RuNews corpus, containing 1,823 news articles annotated by sentiment. The results show that finetuning the model using a small number (about several hundred) of annotated domain texts can significantly\nimprove the performance of sentiment analysis for a new domain (on average by 4.6 p.p.). We also obtained the state-of-the-art results for 7 out of 14 test corpora.\nINDEX TERMS BERT, cross-domain models, neural language models, sentiment analysis\nI. INTRODUCTION Sentiment analysis (also known as opinion mining) is a research field that aims at understanding the underlying sentiment of unstructured content [1]. Despite a twenty-year history (the first papers appeared in 2002 [2], [3]), sentiment analysis still attracts increasing attention from researchers: in 2021 about 5,700 articles on this topic were indexed in the Scopus citation database compared to 4,800 articles in 20201. At present, it is possible to achieve a sufficiently high performance of sentiment analysis, mainly due to deep neural language models. For example, for the binary classification problem (positive/negative) Stanford Sentiment Treebank v2 (SST-2), the classification accuracy using the SMART-RoBERTa model reached 97.5% [4].\nHowever, there is an important problem of transferability of trained models from one domain to another, that is, the development of cross-domain models [5], [6]. The aim of cross-domain sentiment classification (domain adaptation for sentiment analysis) is to generalize a model trained on labeled source domain data to a target domain in which there are few or no labeled data [7], [8]. For example, if we have a sentiment analysis model for restaurant reviews, we would like to use it to analyze bank reviews, perhaps with minimal fine-tuning on text about banks.\n1 We used the following search query: TITLE-ABS-KEY ( ( sentiment AND analysis ) OR ( opinion AND mining ) ) AND PUBYEAR = year, where year = {2020, 2021}.\nThe problem of model transferability is related to the complexity of text labeling in a new domain. Neural network models of sentiment analysis require a sufficient amount of training data. However, in practice, such data may not be available for a new domain, and labeling large-scale data is long and expensive [9], [10].\nMost of the research in sentiment analysis, as in NLP in general, is done for the English language. Other languages, including Russian, are less explored. For example, in recent years, studies have been carried out on Russian-language neural network models for sentiment analysis [11], [12], but there were practically no studies on cross-domain models for the Russian language.\nIn addition, works devoted to the Russian language do not use all available Russian-language corpora with high-quality annotation. For example, Smetanin and Komarov [12] do not use the ROMIP and Russian Hotel Reviews corpora, while Kotelnikov [13] does not apply the RuSentiment and LinisCrowd corpora. At the same time, these labeled corpora are a valuable source of information for neural network models.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n2 VOLUME XX, 2023\nIn this paper we aim to close these gaps and study deep neural language models for cross-domain sentiment analysis in Russian using all the available Russian-language corpora with high-quality annotation.\nLet's define a combined non-domain corpus for the given domain as a set of all available (training) corpora, except for the corpus of the given domain. The following research questions were raised:\nRQ1: Will adding a combined non-domain corpus to a domain corpus improve the performance for the given domain?\nRQ2: Will training on a combined non-domain corpus\ncompensate for the lack of the domain corpus?\nRQ3: Will training on a combined non-domain corpus significantly simplify fine-tuning to the given domain by adding a relatively small number of texts in this domain?\nTo answer these questions, we conduct several series of experiments with 30 sentiment annotated corpora across 12 domains. In order to increase the representativeness of news texts with high-quality annotation, we create a novel RuNews corpus, containing 1,823 news articles annotated by sentiment.\nFirst, we compare the performance of a model trained on an in-domain corpus only with a model trained on all the available corpora2 (RQ1). Second, we test how the performance of a model trained only on a combined nondomain corpus varies (RQ2). Third, we measure the change in model performance by adding small portions of the target domain texts to the combined non-domain corpus (RQ3). It turns out that adding even a small number of target domain texts significantly improves the performance of sentiment analysis. Finally, we compare the results of our models with the best results from other studies.\nThus, the contribution of our work is as follows:\n\u2022 a new approach of building models for cross-domain sentiment analysis is proposed: first, we fine-tune a pre-\ntrained deep neural model on a combined non-domain corpus, and then fine-tune this model on a small domain corpus;\n\u2022 the universal cross-domain Russian-language model for sentiment analysis is made publicly available;\n\u2022 an overview of the existing Russian-language corpora for sentiment analysis is provided;\n\u2022 a novel RuNews corpus of news annotated by sentiment is introduced;\n\u2022 the influence of the size of the added corpus on the performance of sentiment analysis is investigated;\n\u2022 new state-of-the-art performance scores were obtained for several Russian-language corpora. The remainder of the paper is organized as follows. Section II describes previous work on cross-domain sentiment analysis: models and methods are classified by architecture types and the number of training domains for English and Russian languages. Section III presents the materials and methods of our study: text corpora, including the novel\n2 We call such a model \u201cuniversal\u201d.\nRuNews corpus, the BERT neural language model, and our approach to answering research questions. Section IV characterizes our results: first, the experimental setup is given,\nthen the results of experiments are presented and discussed, and lastly our performance scores are compared with the stateof-the-art results. Section V concludes the paper."
        },
        {
            "heading": "II. RECENT WORK",
            "text": "This section provides the classification of cross-domain sentiment analysis models by architecture types and the number of training domains, as well as the current work in this area for English and Russian languages."
        },
        {
            "heading": "A. THE ENGLISH LANGUAGE",
            "text": "1) CLASSIFICATION OF MODELS BY ARCHITECTURE TYPE According to the types of architecture used, the existing variety of models for cross-domain sentiment analysis can be classified into two broad categories [7]: models with shallow and deep architectures. In shallow domain adaptation, the alignment of domain distributions is done using instancebased and feature-based techniques. For example, the method of cross-domain sentiment analysis by refining feature extraction proposed by Geethapriya and Valli [14] builds a bipartite graph from domain-specific (non-pivots) and domain-independent (pivots) words. Based on the links between the words in the graph, the sentiment knowledge is transferred from the source domain to the target domain through domain-independent features using the method of spectral feature alignment.\nDeep domain adaptation uses deep neural networks, which are a powerful tool giving outstanding results in many natural language processing tasks. However, they require a large amount of labeled training data and also assume that the source and target domain data come from the same distribution. In this category of models, we should highlight adversarial-based, attention-based and transformer-based types of adaptation.\nAdversarial domain adaptation models seek to minimize mismatch in distribution between domains to obtain transferable and domain invariant features using adversarial learning. Qu et al. [15] proposed an adversarial category\nalignment network that attempts to improve category consistency between source and target domains. Authors increase the discrepancy of the two sentiment classifiers to obtain different representations to find ambiguous features near decision boundaries. Minimizing this discrepancy, the generator then learns to create more distinguishing features\nthat are further away from the category boundaries.\nAttention-based domain adaptation models use the attention mechanism to capture important words and sentences in a text. The hierarchical attentional network with prior knowledge proposed by Manshu and Bing [10] can\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2023 3\nobtain both domain-specific and domain-independent features by adding prior knowledge. The neural network architecture consists of a sentiment dictionary match layer, a three-layer convolutional neural network (CNN), and a hierarchical attention network. The first of these layers selects important sentiment words from the sentiment dictionary for further classification. A three-layer CNN is used to store context information. The hierarchical attention network assigns proper weights to sentiment related sentences and words.\nZhang et al. [16] proposed an interactive attention transfer network that can transfer sentiment across domains given both sentence and aspect information. The neural network architecture consists of two attention networks. The first one determines common features between domains through domain classification. The second network aims to extract information from the aspects using common features as a bridge. Afterwards, the authors conduct interactive attention learning for those two networks so that both the sentences and the aspects can influence the final sentiment representation.\nWasserstein based Transfer Network proposed by Du et al. [9] uses the attentional recurrent neural network to account for features with important semantic information and minimizes the Wasserstein distance between source and target domain feature representations, which helps to capture significant domain-invariant features.\nTransformer-based domain adaptation models use deep neural networks built on the Transformer architecture with a self-attention mechanism. Transferable Pivot Transformer [17] can learn pivots and contextual representations simultaneously. The model consists of two networks. The first network is a pivot selector that learns to detect transferable n-gram pivots from contexts. The second network is a transferable transformer that learns to generate domain-invariant representations by modeling the correlation between pivot and non-pivot words.\nA number of works within the Transformer-based approach are devoted to training procedures for language models, in particular, the BERT model. Myagmar et al. [18] fine-tuned the BERT and XLNet models for cross-domain sentiment analysis. On a test dataset, the authors found that BERT and XLNet were able to outperform previous state-ofthe-art approaches in accuracy using 20 and 120 times fewer training data, respectively.\nDu et al. [19] developed a BERT model post-training procedure designed to solve two tasks: the domaindistinguish task and the target domain masked language model task. The first task enables BERT to distill the specific features for different domains; the second task allows injecting the target domain knowledge.\nZhou et al. [20] proposed a BERT-based SentiX language model that does not require fine-tuning for cross-domain tasks. The model learns domain-independent features from domain-invariant sentiment knowledge, including sentiment lexicons, emoticons, and ratings.\nLei and Li [21] created a framework based on the following procedure for fine-tuning the BERT model. First, BERT is fine-tuned with the labeled reviews of the source domain.\nNext, the trained BERT classifier is used to label the reviews of the target domain. Target domain reviews for which the highest probability of positive or negative sentiment was predicted are accepted as correctly classified and added to the set of source domain reviews for the next iteration of finetuning.\n2) CLASSIFICATION BY THE NUMBER OF TRAINING DOMAINS Based on the number of training domains, cross-domain sentiment analysis methods and techniques can be classified into single-source and multi-source domain adaptation. Single-source domain adaptation aims at adapting models trained on single labeled source domain to an unlabeled target domain. In multi-source domain adaptation task, multiple source domains are used to train models.\nIn all the works cited above, the problem of single-source domain adaptation was considered. There are several papers where multi-source domain adaptation is examined. Zhao et al. [22] proposed a framework with multi-source domain adaptation and joint learning. This framework uses bidirectional gated recurrent units (GRU) and convolutional neural networks for deep feature extraction and effectively transfers sentiment features from different domains through soft parameter transfer.\nThe BertMasker model proposed by Yuan et al. [23], first learns to select domain-related tokens from texts, then mask those tokens from the original text and form sentiment representations of texts. As the masked tokens are domainrelated, they are appropriate for learning domain-aware sentiment representations of texts from different domains. The final text representations for sentiment classification are obtained by concatenation of domain-agnostic and domainaware sentiment representations.\nThe multi-domain dataset from Amazon [24] is often used as an English text corpus for training and testing methods. It consists of four different domains: kitchen, electronics, DVDs, and books. Mozetic [25] shows that the quality of classification models depends on the quality and size of training data rather than on the type of the model being trained.\nClassification by the number of source domains is important because each new domain introduces some number of new ways of expressing sentiment. If a model was trained on the texts of several domains rather than just one, it means that the model learned about more sentiment expressions. This allows the model to become more robust to transfer to other domains.\nIn the present work, we consider the task of the multi-source domain adaptation. There are four different types of sources of texts that we use to train models (reviews, tweets, social media posts, news articles), while in the previous studies, training data consist of reviews only. In the works considered, the largest amount of training data is about 20,000 examples (e.g.,\n[23]); in our study, the amount of training data is an order of magnitude larger and is estimated as 250,000 examples.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n4 VOLUME XX, 2023"
        },
        {
            "heading": "B. THE RUSSIAN LANGUAGE",
            "text": "Currently, there is a lack of research on cross-domain sentiment analysis of the Russian-language texts. In most of the existing works, the approaches are aimed at solving the problem of in-domain sentiment analysis. Early work on the text sentiment analysis in Russian is associated with the ROMIP workshop, within which in 2011-2012 the competitions in data mining systems were held. In 2011, to analyze the sentiment, the contestants were given three text corpora composed of reviews of books, movies, and digital cameras, and in 2012 a corpus of news quotes was suggested. The best classification performance on these corpora was shown by shallow architecture models based on the SVM classifier [26], [27].\nThe next competition for data mining systems was held as part of the SentiRuEval-2015 workshop [28], where corpora of tweets about banks and telecommunication companies were proposed for entity-based sentiment analysis. The best results in this competition were shown by the SVM with features that included word and letter n-grams, syntactic links presented as triples (a head word, a dependent word, type of relation), and techniques involving rule-based factextraction [29]. The SentiRuEval-2016 competition [30] also analyzed tweets about banks and telecommunication companies. The best classification scores on both corpora were obtained using a two-layer GRU on the top of the pretrained Word2Vec model [31].\nZvonarev and Bilyi [32] used logistic regression, XGBoost, and CNN classifiers to analyze the sentiment of the RuTweetCorp corpus. Baymurzina et al. [33] explored a shallow-and-wide CNN and bidirectional GRU on the top of FastText and ELMo embeddings to analyze the sentiment of the RuSentiment corpus.\nAfter the BERT neural network model was introduced in 2018, deep domain adaptation aroused particular interest among researchers. Works for the Russian language appeared, in which, using the BERT model, the obtained results exceeded the results of previous studies on the existing Russian-language corpora annotated by sentiment. Kuratov and Arkhipov [34] created a Russian version of the BERT model called RuBERT based on the original multilingual BERT model and applied it to analyze the sentiment of the RuSentiment corpus.\nGolubev and Loukachevich [35] tested CNN, LSTM, BiLSTM neural network architectures and two variants of Russian BERT: RuBERT and Conversational RuBERT. The authors showed that the Conversational RuBERT model achieves the best results on news quotes from the ROMIP2012 and tweets from the SentiRuEval-2015 and SentiRuEval-2016. Subsequently, Golubev and Loukachevich [11] used an additional train dataset created with the distant supervision technique. The best results were obtained using a three-step approach of sequential training on general, thematic and benchmark train samples.\nSmetanin and Komarov [12] investigated the performance of text sentiment classification using four fine-tuned models: multilingual BERT, RuBERT, and two versions of the multilingual Universal Sentence Encoder model. The\nperformance of the models was evaluated on the SentiRuEval-2016, SentiRuEval-2015, RuTweetCorp, RuSentiment, LinisCrowd, Kaggle Russian News Dataset and RuReviews corpora. The best classification performance scores for all the corpora were obtained using the RuBERT model.\nThe works presented above for the Russian language solve the problem of in-domain sentiment analysis. Kotelnikov [13] investigates the problem of cross-domain sentiment analysis with several sources in relation to the Russian language. Two series of experiments were carried out, in which the training corpora ROMIP-2011, SentiRuEval2015, RuReviews and Russian Hotel Reviews were combined in different ways. The combined corpora were used to fine-tune the RuBERT model, which was then tested on the test part of the ROMIP-2011, ROMIP-2012, SentiRuEval-2015, RuReviews and Russian Hotel Reviews corpora. The results of the experiments showed that, on average, the performance of the models increases with an increase in the number of training corpora.\nIn this paper, we propose a new training procedure for the RuBERT neural language model for cross-domain sentiment analysis. Our approach differs from the existing ones by two-\nstep fine-tuning: on the first step we use a large non-domain corpus, and on the second step we fine-tune a model on a small domain corpus. We conducted large-scale experiments using all the available Russian-language corpora with high-quality annotation (30 corpora in total). Also, we created a novel RuNews corpus, containing 1,823 news articles annotated by\nsentiment."
        },
        {
            "heading": "III. MATERIALS AND METHODS",
            "text": ""
        },
        {
            "heading": "A. TEXT CORPORA",
            "text": ""
        },
        {
            "heading": "1) EXISTING RUSSIAN CORPORA",
            "text": "The characteristics of text corpora for sentiment analysis are described in detail in [13]: source of texts, domain, corpus size and text sizes, number and ratio of sentiment classes, annotation method, split into training and test parts. Table I shows the characteristics of the Russian-language corpora used in this work.\nThe sources of texts in these corpora are: 1) reviews of goods, works of art and organizations; 2) news articles; 3) tweets; 4) social media posts.\nCorpora annotation was carried out using four methods: 1) expert annotation with \ud835\udc5b annotators; 2) crowdsourcing; 3) use of the author\u2019s (user\u2019s) rating of the review; 4) automatic labeling based on emoticons.\nThe number of sentiment classes varies from two to five. In this paper we consider the problem of sentiment analysis with three classes: positive, negative, and other. The \u201cother\u201d class combines both the texts with no pronounced sentiment (neutral texts) and the texts which contain both sentiments in a comparable volume (contradictory texts). We decided to\ncombine the neutral and contradictory classes into one because\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2023 5\nit is often difficult to distinguish between two of them: e.g., review sites tend to have three sentiment classes, and users are forced to place both neutral and contradictory reviews in the third class.\nA detailed description of the corpora from Table I is presented in [13], so here we will give a brief overview here.\nThe ROMIP corpora were created as part of the sentiment analysis competitions of the same name in 20113 and 20124 [26], [27] and include movie, book, and digital camera reviews. In another series of competitions \u2013 SentiRuEval 20155 and 20166 \u2013 there was created a corpus of car and restaurant reviews for aspect-based sentiment analysis and a corpus of tweets about banks and telecommunication companies for entity-based sentiment analysis [28], [30]. The corpus of restaurant reviews was expanded as part of the Russian track at the SemEval-20167 international competition [36]. Corpora of social media posts LinisCrowd8 [37] and RuSentiment9 [38], corpora of hotel reviews Russian Hotel Reviews10 [39] and reviews about women\u2019s\n3 http://romip.ru/ru/2011/index.html. 4 http://romip.ru/ru/2012/tracks.html. 5 https://www.dialog-21.ru/evaluation/2015/sentiment. 6 https://www.dialog-21.ru/evaluation/2016/sentiment. 7 https://alt.qcri.org/semeval2016. 8 https://linis-crowd.org.\nclothing and accessories RuReviews11 [40] appeared later. RuTweetCorp12 tweet corpus was automatically labeled based on emoticons [41]. The corpus of Kazakh news Kaggle Russian News Dataset13 is also known."
        },
        {
            "heading": "2) NOVEL RUNEWS CORPUS",
            "text": "In order to increase the representativeness of news texts with high-quality annotation, a novel RuNews14 corpus was created, containing 1,823 news articles annotated by sentiment. The articles for annotation were selected randomly and evenly by time and topic from a large news corpus, including 300,000 messages (RIA Novosti, Interfax, RBC, Lenta) for the period from 08/01/2000 to 07/01/2019. The articles cover a wide range of topics: Politics, Economics, Science, Culture, IT, Auto, Sports. Annotation was carried out by three annotators on a three-point scale (positive / negative / neutral), the final label was formed on the basis of the labels given by the majority of annotators (Fleiss\u2019 kappa=0.501).\n9 https://github.com/text-machine-lab/rusentiment. 10 https://goo.gl/DTEpxs. 11 https://github.com/sismetanin/rureviews. 12 https://study.mokoron.com. 13 https://www.kaggle.com/c/sentiment-analysis-in-russian. 14 https://github.com/kotelnikov-ev/RuNews.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n3) CORRESPONDENCE BETWEEN TRAINING AND TEST CORPORA In the study, a total of 30 corpora were used \u2013 12 training and 18 test corpora. We assigned a single training corpus to each test corpus, depending on the domain and the source of the texts (Table II). The split into training and test parts followed the original works, with the exception of the LinisCrowd corpus, which was not split in the original version. We divided this corpus into training and test parts according to the following principle: we referred texts annotated by only one user to training ones, and texts annotated by several users, to test ones. The RuReviews corpus was used only for model training.\nWe did not use the RuTweetCorp corpus due to the low quality of labeling (automatic, based on emoticons) and the Kaggle Russian News Dataset corpus due to the fact that the method for labeling its training part is unknown, and there are no sentiment labels of the test part texts.\nThe information about tokens and imbalance (percentage and standard deviation) is given in tables A.I-A.IV of the Appendix.\nFrom Table I and Tables A.I-A.II of the Appendix, it can be seen that the corpora are characterized by a significant imbalance by class \u2013 the maximum standard deviation by class is for the training corpora \ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (42.3%), \ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (33.3%), \ud835\udc4511\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (32.8%) and test corpora \ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 (42.7%) and \ud835\udc4512\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 (37.1%). On average, the class imbalance in terms of the standard deviation is 23.0% for the training and test corpora. At the same time, the combination of training corpora allows balancing the distribution by classes to a large extent: the standard deviation becomes 11.1% (48.8% positive, 23.6% negative, 27.6% other)."
        },
        {
            "heading": "B. BERT",
            "text": "To classify texts by sentiment, we use a deep neural network language model BERT (Bidirectional Encoder Representations from Transformers) [42], which showed the best results for sentiment analysis in Russian [12], [35].\nThe BERT model is a multilayer Encoder from the Transformer architecture [43]. Encoder works on the basis of self-attention and generates distributed representations of input text tokens, which can be used for sentiment classification.\nWorking with BERT usually involves two steps [42]: in the first step, the model is trained to solve masked language modeling and next sentence prediction tasks using large text corpora. The result is a pre-trained language model. At the second stage, this model is fine-tuned to a specific task, e.g., sentiment analysis, by additional training on a specially labeled corpus. In BERT, input texts are represented using subword tokenization [44]. The maximum input size for BERT is 512 tokens. In addition to word tokens, special tokens are used, e.g., the [CLS] token, which is always specified first and serves to represent the text as a whole. The token vector [CLS] generated at the output as a result of the model can be used for classification. For this purpose, BERT uses a twolayer neural network by default.\nIn our work, as a pre-trained language model, we used the Russian version of BERT \u2013 RuBERT, proposed in [34]. To train this model, a multilingual version of BERTBASE (12 layers, hidden layer size 768) was taken and further finetuned on the Russian-language Wikipedia and the corpus of news articles.\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2023 7"
        },
        {
            "heading": "C. OUR APPROACH",
            "text": "To answer the formulated research questions RQ1\u2013RQ3, we conducted four series of experiments (A\u2013D), shown in Fig. 1.\nIn series A, we use traditional transfer learning: the pretrained RuBERT model is fine-tuned on a domain corpus, and then the performance is evaluated on the test corpus corresponding to this domain (see Table II). Non-domain corpus for the given domain is not used.\nExperiment series B is conducted to answer RQ1: will adding a combined non-domain corpus to a domain corpus improve the performance for the given domain? In series B, we explore the impact of text corpora from other domain on the performance of sentiment analysis in that domain. To this end, we train the pre-trained model on joint dataset that includes both the domain corpus and corpora of other domains. After that, we can compare the results of series of\nexperiments A and B and draw conclusions about the usefulness of non-domain corpora for sentiment analysis.\nWith the help of experiment series C, we answer RQ2: will training on a combined non-domain corpus compensate for the lack of the domain corpus? In this series, we explore the possibility of transferring the model fine-tuned on the corpora of other domains only (we call such a model \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc5b\ud835\udc51), to a new domain. To this end, we train the pre-trained model on all available corpora except the domain corpus and then evaluate the model on the corpus corresponding to the given domain. The results are also compared with series A and B.\nExperiment series D is necessary to answer RQ3: will training on a combined non-domain corpus significantly simplify fine-tuning to the given domain by adding a relatively small number of texts in this domain? To this end, we first fine-tune the pre-trained model on all corpora except the domain corpus (as in series C \u2013 we obtain \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc5b\ud835\udc51 model), and then fine-tune this model using small sub-corpora of the domain data (so we create \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc51 model). As small subcorpora we use three subsets of each domain corpus with the following sizes (in tokens) \u2013 0.5\ud835\udc41, 0.75\ud835\udc41, and 1.0\ud835\udc41, where \ud835\udc41 \u2013 the size of the training corpus with the minimum number of tokens (\ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f\n\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b , \ud835\udc41 = 37,556 tokens). Thus, in our experiments, a small sub-corpus is a dataset of about 20\u201340 thousand tokens, which is approximately 100\u2013200 texts with an average size of 200 tokens. The results of experiment series D are compared with series B and C."
        },
        {
            "heading": "IV. RESULTS AND DISCUSSION",
            "text": ""
        },
        {
            "heading": "A. EXPERIMENTAL SETUP",
            "text": "Neural language models were trained using the Google Colab Pro service on NVIDIA Tesla P100 and V100 video cards. In order to reduce the influence of random initialization of weights, five runs were performed for each experiment with different seed values, the results of which were averaged.\nIn preliminary experiments, we selected the optimal hyperparameters for the RuBERT model: the number of epochs from the range [1, 10], the learning rate from the set {10\u20133, 10\u20134, 10\u20135, 10\u20136}, and the batch size from the set {2, 6, 12}. The optimal value for the learning rate was 10\u20135 and we refined it in this neighborhood: [1,9]\u00d710\u20135. As a result, the following values of hyperparameters were the best:\n\u2022 number of epochs: 5; \u2022 learning rate: 2\u221910\u20135.\n\u2022 batch size: 12. For the weight decay we took default value 0.01. In the experiments, the pre-trained RuBERT model was applied [34]. The F1-score was used to evaluate the performance.\nText preprocessing includes the following steps: 1. converting texts to lower case; 2. replacing of URLs, e-mails, mentions (@user) and\nphone numbers with tokens \u201curl\u201d, \u201cmail\u201d, \u201cuser\u201d, \u201cphone\u201d, respectively;\n3. replacing sequences of repeated longer than two\ncharacters by a sequence consisting of two characters.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n8 VOLUME XX, 2023"
        },
        {
            "heading": "B. RQ1: DOMAIN AND NON-DOMAIN TRAINING DATA",
            "text": "To answer RQ1, two series of experiments A and B were conducted. In experiment series A, training was carried out on each training corpus separately; testing was carried out on the test corpus from the same domain (see Table II). Thus, 12 models were trained. The results of the first experiment series are shown in the second column of Table III.\nIn series B, a single universal model was trained on all the 12 training corpora; testing was carried out on all the 18 test corpora (see Table II). The results of experiment series B are shown in the third column of Table III.\nOn average for the test corpora, training the universal model on all the training corpora (series B) improves the performance by 3.6 percentage points (0.6235 vs. 0.5878) compared to training on a single corresponding corpus (series A). We calculated the correlation between the difference in the performance scores of the universal model (series B) and individual models (series A), on the one hand, and the size of the training domain corpus, on the other hand. At first glance, there is a fairly strong negative correlation \u2013 the Pearson coefficient is \u20130.42: with an increase in the size of the training corpus, the difference in performance scores between the models decreases. However, this correlation value is mainly due to a significant increase in the performance of the universal model compared to the two models built on the basis of the smallest training corpora \u2013 restaurant reviews \ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (200 texts) and car reviews \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b (203 texts). If these models are excluded from the comparison, the performance difference between the models is on average 0.1 p.p., and the Pearson coefficient is only \u2013 0.02: there is no correlation between the difference in the performance scores of the models and the corpus size.\nThus, the answer to RQ1 (will adding a combined nondomain corpus to a domain corpus improve the performance for the given domain?), is negative if low-resource domains\nare not taken into account: adding a significant amount of training texts from other domains does not improve the performance of sentiment analysis compared to the domain corpus. For example, adding corpora from other domains (153,483 texts) to the \ud835\udc4615\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b corpus of reviews of telecommunication companies, containing 4,839 texts, improves the performance of the test \ud835\udc4616\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpus by only 0.4 p.p. (0.6705 vs. 0.6746).\nAt the same time, if the domain corpus is small (about several hundred texts), adding a corpus from other domains can significantly improve the performance: from 14.6 p.p. for \ud835\udc4616\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 up to 29.1 p.p. for \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61.\nDespite the fact that, on average, adding training corpora does not affect the performance of sentiment analysis, there is an important pattern: the higher the class imbalance of the training corpus is, the greater the positive effect of expanding the training data is (the correlation coefficient without taking into account low-resource domains is 0.36). For training corpora with a class imbalance standard deviation greater than 0.2 (see Table A.I of the Appendix), the performance of the universal model, compared to individual models, is higher, on average, by 0.9 p.p.\nConversely, the imbalance of both training and test corpora\nstrongly affects the performance of sentiment analysis: the higher the class imbalance is, the worse the classification performance is (the correlation coefficient for training corpora is \u2013 0.37, for test corpora \u2013 0.40). Fig. 2 shows the dependence of the universal model performance on the class imbalance of the test corpora.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2023 9"
        },
        {
            "heading": "C. RQ2: NON-DOMAIN TRAINING DATA",
            "text": "To answer RQ2, the results of two series of experiments \u2013 A and C \u2013 were compared. In experiment series C, 12 models were trained \u2013 a separate model for each domain. When training the model for the given domain, all the training corpora were used, except for one for this domain. The model was tested on this domain corpus. The results of this experiment series are shown in the fourth column of Table III.\nExcluding the domain corpus (series C) reduces the performance compared to training on a domain corpus only (series A) by an average of 3.9 p.p. However, for lowresource domains (car and restaurant reviews), a large training corpus that does not include the domain data turns out to be better than a small domain corpus: on average, the performance increases by 14.5 p.p.\nThus, the answer to RQ2 (will training on a combined non-domain corpus compensate for the lack of the domain corpus?) is generally negative. However, for low-resource domains (about several hundred training texts), training on a large non-domain corpus allows obtaining better results."
        },
        {
            "heading": "D. RQ3: ADDING SMALL DOMAIN DATA",
            "text": "To answer RQ3, experiment series D was carried out, during which the models obtained in series C were used.\nWe conducted 12 experiments (according to the number of domains and training corpora). In each of the experiments, the \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc5b\ud835\udc51 model from series C was first selected: out of five runs of series C for each domain, the model whose results were closest to the average results for five runs was selected. This was done to improve the fairness of the results. The \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc5b\ud835\udc51 models were trained on all the training corpora with the exception of one corpus for the given domain.\nIn experiment series D, the selected \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc5b\ud835\udc51 model was finetuned on several subcorpora of various sizes of the domain corpus, as a result, the set of \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc51 models was obtained. The size of the subcorpora was determined as follows. The training corpus with the minimum total number of tokens was chosen: \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b , \ud835\udc41 = 37,556 tokens (see Table A.III in the Appendix). This number was set as the maximum number of tokens used when the domain fine-tuning.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n10 VOLUME XX, 2023\nTo build \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc51 models, three different subcorpora of texts randomly selected from the domain corpus were applied. The size of these subcorpora in tokens was 50%, 75% and 100% of \ud835\udc41 tokens. When the selection was carried out, the balance of texts of different classes was maintained, and 10% of the shortest and longest texts were excluded (except for the \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b corpus, where the entire corpus was 100 percent used). For each of the three sizes of subcorpora, five runs of training the \ud835\udc53\ud835\udc61\ud835\udc40\ud835\udc51 model were carried out, at each run the texts in the subcorpora were selected randomly; the results were\naveraged.\nThe results of experiment series D are shown in Table IV. In the same table, for comparison, the results of experiment series B (universal model training on all the training corpora) and C (model training on all the corpora, except for the domain one \u2013 0.0\ud835\udc41) are shown. Adding even a small subset of domain texts to the training data can improve the performance of the model on average: 0.5\ud835\udc41 (18,778) tokens are enough to improve the F1-score by an average of 3.6 p.p., 0.75\ud835\udc41 (28,167) tokens \u2013 by 4.2 p.p., \ud835\udc41 (37,556) tokens \u2013 by 4.6 p.p.\nTABLE IV RESULTS OF EXPERIMENT SERIES D COMPARED WITH RESULTS OF SERIES B AND C (F1-SCORE). BOLD INDICATES THE BEST RESULTS FOR SERIES B, C, AND D; THE BEST RESULTS FOR SERIES C AND D ARE UNDERLINED\nTest corpora\nSeries B Series\nC Series D\nAll\ncorpora +0.0\ud835\udc41 +0.5\ud835\udc41 +0.75\ud835\udc41 +\ud835\udc41\n\ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6128 0.5155 0.5101 0.5219 0.5286 \ud835\udc3f\ud835\udc36\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.5539 0.5102 0.4889 0.4835 0.4868\n\ud835\udc4511\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6587 0.6060 0.6216 0.6307 0.5993\n\ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6809 0.6579 0.6971 0.7009 0.7361\n\ud835\udc4511\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.5441 0.5197 0.5215 0.4932 0.5295\n\ud835\udc4512\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.4929 0.4265 0.4289 0.4663 0.4372\n\ud835\udc4512\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.4490 0.4306 0.3890 0.3936 0.3977\n\ud835\udc4512\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.5333 0.4834 0.5169 0.5125 0.5144\n\ud835\udc4512\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6789 0.4143 0.6589 0.6661 0.6704\n\ud835\udc45\ud835\udc41\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6149 0.5798 0.5725 0.5802 0.5788 \ud835\udc45\ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.7518 0.4627 0.6560 0.6702 0.6862\n\ud835\udc4616\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6120 0.5373 0.7178 0.7122 0.6948\n\ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.7146 0.6987 0.6391 0.6489 0.6646\n\ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6464 0.6169 0.6718 0.6981 0.7085\n\ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6517 0.5273 0.7007 0.7120 0.6825\n\ud835\udc4615\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6685 0.6245 0.6004 0.6043 0.6051 \ud835\udc4616\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6848 0.6713 0.5103 0.5276 0.5651\n\ud835\udc4616\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 0.6746 0.5966 0.6191 0.6170 0.6251\nMean 0.6235 0.5488 0.5845 0.5911 0.5950\nAn increase in performance when adding domain texts occurs for 13 out of 18 corpora. For 6 out of 13 corpora, there is an increase of more than 5 p.p. There is a fairly strong negative correlation between the difference between the results of models with domain finetuning on \ud835\udc41 tokens (series D) and models without domain fine-tuning (series C), on the one hand, and the class imbalance of the test corpus, on the other hand \u2013 the Pearson coefficient is \u20130.52: the more imbalanced the test corpus is, the less effective adding the domain texts is.\nFor four out of five corpora, for which the performance in series D has fallen, the drop is in the range of [\u20133.4 .. \u20132.0] p.p. Only for the \ud835\udc4616\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpus, the drop is 10.6 p.p. For this corpus, we analyzed the effects that occur when training using the \ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b corpus on different sets of tokens of different size with different numbers of epochs and identified an overfitting. The \ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b corpus is characterized by a significant class imbalance (only 7% of positive texts) and short texts of tweets (average length is 32 tokens). Finetuning the model, which initially shows the performance F1score=0.6713 on the \ud835\udc4616\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpus, on a small number of tokens of this corpus (0.5\ud835\udc41=18,778 tokens) already on the first epoch leads to overfitting and a drop in performance on the \ud835\udc4616\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpus to 0.42. With further training, the performance increases to 0.51 on five epochs. The more data is fed into training, the higher the results are, but the performance remains quite low even with \ud835\udc41 added tokens (F1-score=0.5651).\nThus, the answer to RQ3 (will training on a combined non-domain corpus significantly simplify fine-tuning to the given domain by adding a relatively small number of texts in this domain?) is positive. Additional fine-tuning of the model using a small number of annotated domain texts can significantly improve the performance of sentiment analysis for a new domain. In our study, a small number of texts are understood as a corpus of about 20\u201340 thousand tokens, which is approximately 100\u2013200 texts with an average size of 200 tokens. In our experiments, the use of such an additional corpus increased the F1-score by an average of 4.6 p.p.\nWe also compared the results of series D with those of series B (the universal model trained on all the available data). Adding a small domain subcorpus does not allow achieving the performance of the universal model: such a model outperforms individual fine-tuned models for 14 out of 18 corpora (on average by 2.8 p.p.). However, it should be taken into account that the universal model is built on the training data, which, as a rule, includes a much larger number of domain texts than models with additional training on small subcorpora."
        },
        {
            "heading": "E. STATE-OF-THE-ART RESULTS",
            "text": "We did not aim to obtain the best results to date for the Russian-language test corpora. In addition, a direct comparison of the results obtained in different articles is\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2023 11\ndifficult since the data preprocessing and the analysis of the results in them differ15.\nHowever, Table V summarizes the best results both from\nour study and from other works with the same corpora.\nTable V shows that in our study the best results are currently obtained for 7 out of 14 corpora, for which other results are known.\nIn general, for 8 out of 18 test corpora, the best model was the universal model trained on all the corpora (series B). For six corpora, the best model was the model fine-tuned on the domain corpus (Series A). For no corpora the best model was obtained by training on all the corpora, except for the domain one (series C). And interestingly, for the \ud835\udc4616\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpus, the best model was obtained in experiment series D when finetuning with less than 20 thousand tokens, for the \ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpus \u2013 with less than 30 thousand tokens, for the \ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 and \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 corpora \u2013 with less than 40 thousand tokens."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "Thus, the following answers to the research questions raised in the paper were given.\nThe answer to RQ1 (will adding a combined non-domain corpus to a domain corpus improve the performance for the given domain?) is negative if low-resource domains are not\n15 For example, when analyzing the SentiRuEval-2015 tweet corpus, some authors consider rating statistics not for tweets as a whole, but for individual entities.\ntaken into account: adding a significant amount of training texts from other domains does not improve the performance compared to the domain corpus. The answer to RQ2 (will training on a combined nondomain corpus compensate for the lack of the domain corpus?) is generally negative. However, for low-resource domains (about several hundred training texts), training on a large non-domain corpus allows obtaining better results.\nThe answer to RQ3 (will training on a combined nondomain corpus significantly simplify fine-tuning to the given domain by adding a relatively small number of texts in this domain?) is positive. Fine-tuning a sentiment analysis universal model using a small number of annotated domain texts will significantly improve the performance for a new domain. In our study, a small number of texts are understood as a corpus of about 20\u201340 thousand tokens, which is approximately 100\u2013200 texts with an average size of 200 tokens. In our experiments, the use of such an additional corpus increased the F1-score by an average of 4.6 p.p.\nThis gives hope that with a universal model trained on a large number of corpora from different domains, when a new test corpus appears, it will be enough to properly annotate a small number of texts in the corresponding domain and finetune the universal model on these data to obtain a good performance. The universal model (series B), built in the course of our study is made publicly available16.\n16 https://github.com/kotelnikov-ev/CrossDomainSentimentModel.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\n12 VOLUME XX, 2023\nAPPENDIX: TEXT CORPUS STATISTICS TABLE A.I\nCLASS DISTRIBUTION IN TRAINING CORPORA. IMB (IMBALANCE) \u2013 STANDARD DEVIATION IN THE PERCENTAGE DISTRIBUTION BY CLASS; THE HIGHER IMB,\nTHE MORE IMBALANCED THE CORPUS\nTrain corpora Positive texts Negative texts Neutral texts\nTotal texts Imb # % # % # %\n\ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 41,509 93.1% 3,085 6.9% 0 0.0% 44,594 42.3%\n\ud835\udc3f\ud835\udc36\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 2,227 7.7% 12,254 42.5% 14,372 49.8% 28,853 18.4%\n\ud835\udc4511\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 17,620 79.7% 2,060 9.3% 2,418 10.9% 22,098 32.8% \ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 7,614 80.5% 1,004 10.6% 842 8.9% 9,460 33.3% \ud835\udc4511\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 10,455 70.6% 1,886 12.7% 2,467 16.7% 14,808 26.4% \ud835\udc4512\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 1,115 26.2% 1,864 43.8% 1,281 30.1% 4,260 7.5% \ud835\udc45\ud835\udc48\ud835\udc45\ud835\udc4e\ud835\udc50\ud835\udc50\ud835\udc52\ud835\udc60 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 29,999 33.3% 30,000 33.3% 30,000 33.3% 89,999 0.0%\n\ud835\udc45\ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 9,170 38.0% 3,654 15.1% 11,300 46.8% 24,124 13.4% \ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 354 7.2% 1,059 21.7% 3,470 71.1% 4,883 27.3% \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 115 56.7% 30 14.8% 58 28.6% 203 17.4% \ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 136 68.0% 28 14.0% 36 18.0% 200 24.6% \ud835\udc4615\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 908 18.8% 1,585 32.8% 2,346 48.5% 4,839 12.1%\nTABLE A.II\nCLASS DISTRIBUTION IN TEST CORPORA\nTrain corpora Positive texts Negative texts Neutral texts\nTotal texts Imb # % # % # %\n\ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 5,709 93.6% 393 6.4% 0 0.0% 6,102 42.7%\n\ud835\udc3f\ud835\udc36\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 1,355 9.5% 6,751 47.3% 6,154 43.2% 14,260 16.9% \ud835\udc4511\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 146 64.0% 14 6.1% 68 29.8% 228 23.8% \ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 128 61.8% 37 17.9% 42 20.3% 207 20.2% \ud835\udc4511\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 185 70.3% 28 10.6% 50 19.0% 263 26.4% \ud835\udc4512\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 100 77.5% 9 7.0% 20 15.5% 129 31.4% \ud835\udc4512\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 351 85.4% 7 1.7% 53 12.9% 411 37.1% \ud835\udc4512\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 266 65.2% 63 15.4% 79 19.4% 408 22.6% \ud835\udc4512\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 1,448 31.7% 1,890 41.3% 1,235 27.0% 4,573 6.0% \ud835\udc45\ud835\udc41\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 147 8.1% 550 30.2% 1,126 61.8% 1,823 22.0%\n\ud835\udc45\ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 943 36.0% 258 9.8% 1,420 54.2% 2,621 18.2% \ud835\udc4616\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 69 67.0% 15 14.6% 19 18.4% 103 23.9% \ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 346 7.6% 654 14.4% 3,534 77.9% 4,534 31.7% \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 98 49.0% 26 13.0% 76 38.0% 200 15.1% \ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 146 71.9% 26 12.8% 31 15.3% 203 27.3% \ud835\udc4615\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 342 9.1% 847 22.4% 2,585 68.5% 3,774 25.5% \ud835\udc4616\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 302 9.1% 762 23.1% 2,238 67.8% 3,302 25.0% \ud835\udc4616\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 182 8.3% 1,008 45.9% 1,008 45.9% 2,198 17.7%\nTABLE A.III\nTOKEN DISTRIBUTION IN TRAINING CORPORA\nTrain corpora Total texts Tokens\nTotal Mean Median Min Max Std\n\ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 44,594 6,679,225 149.8 101 5 7,634 164.8\n\ud835\udc3f\ud835\udc36\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 28,853 6,418,945 222.5 201 4 5,977 137.2\n\ud835\udc4511\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 22,098 1,780,442 80.6 39 3 4,582 155.1 \ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 9,460 741,291 78.4 53 3 728 81.8 \ud835\udc4511\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 14,808 1,866,274 126.0 50 3 5,024 205.3 \ud835\udc4512\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 4,260 238,338 56.0 47 15 1,009 43.3 \ud835\udc45\ud835\udc48\ud835\udc45\ud835\udc4e\ud835\udc50\ud835\udc50\ud835\udc52\ud835\udc60 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 89,999 3,036,013 33.7 24 3 346 31.3\n\ud835\udc45\ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 24,124 599,367 24.9 16 3 343 28.0 \ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 4,883 156,098 32.0 30 5 76 9.4 \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 203 37,556 185.0 161 34 1,372 137.4 \ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 200 40,188 200.9 202 72 327 68.7 \ud835\udc4615\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b 4,839 148,125 30.6 30 6 90 12.0\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor Name: Preparation of Papers for IEEE Access (February 2017)\nVOLUME XX, 2023 13\nTABLE A.IV\nTOKEN DISTRIBUTION IN TEST CORPORA\nTest corpora Total texts Tokens\nTotal Mean Median Min Max Std\n\ud835\udc45\ud835\udc3b\ud835\udc45\u210e\ud835\udc5c\ud835\udc61\ud835\udc52\ud835\udc59 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 6,102 871,280 142.8 101 14 3,488 141.4\n\ud835\udc3f\ud835\udc36\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 14,260 3,462,190 242.8 213 6 7,060 221.6 \ud835\udc4511\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 228 92,463 405.5 278 33 2,692 396.0 \ud835\udc4511\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 207 78,091 377.3 243 59 4,542 523.9 \ud835\udc4511\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 263 205,828 782.6 440 15 7,021 942.1 \ud835\udc4512\ud835\udc4f\ud835\udc5c\ud835\udc5c\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 129 42,301 327.9 178 18 3,769 510.3 \ud835\udc4512\ud835\udc50\ud835\udc4e\ud835\udc5a \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 411 214,520 522.0 178 11 6,657 980.9 \ud835\udc4512\ud835\udc5a\ud835\udc5c\ud835\udc63 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 408 40,012 98.1 63 9 1,000 111.9 \ud835\udc4512\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 4,573 266,234 58.2 51 17 496 31.8 \ud835\udc45\ud835\udc41\ud835\udc5b\ud835\udc52\ud835\udc64\ud835\udc60 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 1,823 603,082 330.8 273 24 3,797 247.2\n\ud835\udc45\ud835\udc46\ud835\udc5d\ud835\udc5c\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 2,621 62,729 23.9 15 3 265 26.7 \ud835\udc4616\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 103 21,609 209.8 215 75 340 69.7 \ud835\udc4615\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 4,534 138,802 30.6 29 6 76 9.6 \ud835\udc4615\ud835\udc50\ud835\udc4e\ud835\udc5f \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 200 34,353 171.8 164 37 369 58.4 \ud835\udc4615\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc61 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 203 43,296 213.3 220 67 347 69.3 \ud835\udc4615\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 3,774 116,540 30.9 30 4 71 11.7 \ud835\udc4616\ud835\udc4f\ud835\udc4e\ud835\udc5b\ud835\udc58 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 3,302 115,642 35.0 35 7 82 11.7 \ud835\udc4616\ud835\udc61\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50 \ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61 2,198 73,183 33.3 33 7 68 11.1"
        }
    ],
    "title": "Cross-domain sentiment analysis based on small in-domain fine-tuning",
    "year": 2023
}