{
    "abstractText": "Koch, Strassle, and Tan [SODA 2023], show that, under the randomized exponential time hypothesis, there is no distribution-free PAC-learning algorithm that runs in time n log s) for the classes of n-variable size-s DNF, size-s Decision Tree, and log s-Junta by DNF (that returns a DNF hypothesis). Assuming a natural conjecture on the hardness of set cover, they give the lower bound n . This matches the best known upper bound for n-variable size-s Decision Tree, and log s-Junta. In this paper, we give the same lower bounds for PAC-learning of n-variable size-s Monotone DNF, size-s Monotone Decision Tree, and Monotone log s-Junta by DNF. This solves the open problem proposed by Koch, Strassle, and Tan and subsumes the above results. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time, and can compute the target function on all the points of the support of the distribution in polynomial time.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nader H. Bshouty"
        }
    ],
    "id": "SP:a08a3afc975daaf493c94b5027f171f4ac910a54",
    "references": [
        {
            "authors": [
                "Michael Alekhnovich",
                "Mark Braverman",
                "Vitaly Feldman",
                "Adam R. Klivans",
                "Toniann Pitassi"
            ],
            "title": "The complexity of properly learning simple concept classes",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2008
        },
        {
            "authors": [
                "Chris Calabro",
                "Russell Impagliazzo",
                "Valentine Kabanets",
                "Ramamohan Paturi"
            ],
            "title": "The complexity of unique k-sat: An isolation lemma for k-cnfs",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2008
        },
        {
            "authors": [
                "Holger Dell",
                "Thore Husfeldt",
                "D\u00e1niel Marx",
                "Nina Taslaman",
                "Martin Wahlen"
            ],
            "title": "Exponential time complexity of the permanent and the tutte polynomial",
            "venue": "ACM Trans. Algorithms,",
            "year": 2014
        },
        {
            "authors": [
                "Andrzej Ehrenfeucht",
                "David Haussler"
            ],
            "title": "Learning decision trees from random examples",
            "venue": "Inf. Comput.,",
            "year": 1989
        },
        {
            "authors": [
                "Thomas R. Hancock",
                "Tao Jiang",
                "Ming Li",
                "John Tromp"
            ],
            "title": "Lower bounds on learning decision lists and trees",
            "venue": "Inf. Comput.,",
            "year": 1996
        },
        {
            "authors": [
                "Lisa Hellerstein",
                "Devorah Kletenik",
                "Linda Sellie",
                "Rocco A. Servedio"
            ],
            "title": "Tight bounds on proper equivalence query learning of DNF",
            "venue": "In COLT 2012 - The 25th Annual Conference on Learning Theory, June",
            "year": 2012
        },
        {
            "authors": [
                "Russell Impagliazzo",
                "Ramamohan Paturi"
            ],
            "title": "On the complexity of k-sat",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2001
        },
        {
            "authors": [
                "Russell Impagliazzo",
                "Ramamohan Paturi",
                "Francis Zane"
            ],
            "title": "Which problems have strongly exponential complexity",
            "venue": "J. Comput. Syst. Sci.,",
            "year": 2001
        },
        {
            "authors": [
                "Caleb Koch",
                "Carmen Strassle",
                "Li-Yang Tan"
            ],
            "title": "Superpolynomial lower bounds for decision tree learning and testing",
            "venue": "CoRR, abs/2210.06375,",
            "year": 2022
        },
        {
            "authors": [
                "Bingkai Lin"
            ],
            "title": "A simple gap-producing reduction for the parameterized set cover problem",
            "venue": "46th International Colloquium on Automata, Languages, and Programming,",
            "year": 2019
        },
        {
            "authors": [
                "Craig A. Tovey"
            ],
            "title": "A simplified np-complete satisfiability problem",
            "venue": "Discret. Appl. Math., 8(1):85\u2013",
            "year": 1984
        },
        {
            "authors": [
                "Leslie G. Valiant"
            ],
            "title": "A theory of the learnable",
            "venue": "Commun. ACM,",
            "year": 1984
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 1.\n08 48\n6v 2\n[ cs\n.D S]\n3 0\nJa n\nKoch, Strassle, and Tan [SODA 2023], show that, under the randomized exponential time hypothesis, there is no distribution-free PAC-learning algorithm that runs in time nO\u0303(log log s) for the classes of n-variable size-s DNF, size-s Decision Tree, and log s-Junta by DNF (that returns a DNF hypothesis). Assuming a natural conjecture on the hardness of set cover, they give the lower bound n\u2126(log s). This matches the best known upper bound for n-variable size-s Decision Tree, and log s-Junta.\nIn this paper, we give the same lower bounds for PAC-learning of n-variable size-s Monotone DNF, size-s Monotone Decision Tree, and Monotone log s-Junta by DNF. This solves the open problem proposed by Koch, Strassle, and Tan and subsumes the above results.\nThe lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time, and can compute the target function on all the points of the support of the distribution in polynomial time."
        },
        {
            "heading": "1 Introduction",
            "text": "In the distribution-free PAC learning model [12], the learning algorithm of a class of functions C has access to an unknown target function f \u2208 C through labeled examples (x, f(x)) where x are drawn according to an unknown but fixed probability distribution D. For a class of hypothesis H \u2287 C, we say that the learning algorithm A PAC-learns C by H in time T and error \u01eb if for every target f \u2208 C and distribution D, A runs in time T and outputs a hypothesis h \u2208 H which, with probability at least 2/3, is \u01eb-close to f with respect to D. That is, satisfies Prx\u223cD[f(x) 6= h(x)] \u2264 \u01eb.\nKoch et al., [9], show that, under the randomized exponential time hypothesis (ETH), there is no PAC-learning algorithm that runs in time nO\u0303(log log s) for the classes of n-variable size-s DNF, size-s Decision Tree and log s-Junta by DNF. Assuming a natural conjecture on the hardness of set cover, they give the lower bound n\u2126(log s). Their lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time and can compute the target function on all the points of the support of the distribution in polynomial time.\nIn this paper, we give the same lower bounds for PAC-learning of the classes n-variable size-s Monotone DNF, size-s Monotone Decision Tree and Monotone log s-Junta by DNF. This solves the open problem proposed by Koch, Strassle, and Tan [9].\n\u2217Center for Theoretical Sciences, Guangdong Technion, (GTIIT), China."
        },
        {
            "heading": "1.1 Our Results",
            "text": "In this paper, we prove the following three Theorems. Theorem 1. Assuming randomized ETH, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log s)-Junta, size-s Monotone DT and size-s Monotone DNF by DNF with \u01eb = 1/(16n) must take at least\nnc log log s log log log s\ntime. Theorem 2. Assuming a plausible conjecture on the hardness of Set-Cover 1, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log s)-Junta, size-s Monotone DT and size-s Monotone DNF by DNF with \u01eb = 1/(16n) must take at least\nnc log s\ntime. Theorem 3. Assuming randomized ETH, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log s)-Junta, size-s Monotone DT and size-s Monotone DNF by size-s DNF with \u01eb = 1/(16n) must take at least\nnc log s\ntime. All the above lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time and can compute the target on all the points of the support of the distribution in polynomial time.\nIn the following two subsections, we give the technique used in [9] to prove Theorem 1 for (log s)-Junta, and the technique we use here to extend the result to Monotone (log s)-Junta."
        },
        {
            "heading": "1.2 Previous Technique",
            "text": "In [9], Koch, Strassle, and Tan show that under the randomized exponential time hypothesis, there is no PAC-learning algorithm that runs in time nO\u0303(log logn) for the class of log n-Junta2 by DNF. The results for the other classes follow immediately from this result, since all other classes contain log n-Junta. All prior works [1, 5] ruled out only poly(n) time algorithms.\nThe result in [9] uses the hardness result of (k, k\u2032)-Set-Cover where one needs to distinguish between instances that have set cover of size at most k from instances that have minimum-size set cover greater than k\u2032:\n1. For some parameters k and k\u2032, assuming randomized ETH, there is a constant \u03bb < 1 such that (k, k\u2032)-Set-Cover cannot be solved in time n\u03bbk.\nFirst, for each set cover instance S, they identify each element in the universe with an assignment in {0, 1}n and construct in polynomial time a target function \u0393S : {0, 1}n \u2192 {0, 1} and a distribution DS that satisfies:\n1See Conjecture 1. 2k-Junta are Boolean functions that depend on at most k variables\n2. The instance S has minimum-size set cover opt(S) if and only if the function \u0393S is a conjunction of opt(S) unnegated variables3 over the distribution DS .4\nFor a DNF F and x \u2208 {0, 1}n, they define widthF (x) to be the size of the smallest term T in F that satisfies T (x) = 1. They then show that\n3. Any DNF F with expected width Ex\u223cDS [widthF (x)] \u2264 opt(S)/2 is (1/(2N))-far from \u0393 S\nwith respect to DS where N is the size5 of S. That is, Prx\u223cDS [F (x) 6= \u0393 S(x)] \u2265 1/(2N).\nThey then use the following gap amplification technique. They define the function \u0393S\u2295\u2113 : ({0, 1}\u2113)n \u2192 {0, 1} where for y = (y1, . . . , yn), yi = (yi,1, . . . , yi,\u2113) \u2208 {0, 1}\n\u2113, i \u2208 [n], we have \u0393S\u2295\u2113(y) = \u0393\nS(\u2295y1, . . . ,\u2295yn) and \u2295yi = yi,1 + \u00b7 \u00b7 \u00b7 + yi,\u2113. They also extend the distribution DS to a distribution DS\u2295\u2113 over domain ({0, 1} \u2113)n and prove that\n4. \u0393S\u2295\u2113(y) is a (opt(S)\u2113)-Junta over D S \u2295\u2113. 5. Any DNF formula F with expected depth Ey\u223cDS \u2295\u2113 [widthF (y)] \u2264 opt(S)\u2113/4 is (1/(4N))-far\nfrom \u0393S\u2295\u2113 with respect to D S \u2295\u2113.\nItem 4 follows from the definition of \u0393S\u2295\u2113 and item 2. To prove Item 5, they show that if, to the contrary, there is a DNF F of expected width at most opt(S)\u2113/4 that is 1/(4N)-close to \u0393S\u2295\u2113 with respect to DS\u2295\u2113, then there is j \u2208 [\u2113] and a projection of all the variables that are not of the form yi,j that gives a DNF F \u2217 of expected width at most opt(S)/2 that is 1/(2N)-close to \u0393S with respect to DS . Then, by item 3, we get a contradiction.\nThey then show that\n6. Any size-s DNF that is (1/(4N))-close to \u0393S\u2295\u2113 with respect to D S \u2295\u2113 has average width Ey\u223cDS\n\u2295\u2113\n[widthF (y)] \u2264 4 log s.\nIf F is (1/(4N))-close to \u0393S\u2295\u2113 with respect to D S \u2295\u2113, then, by items 5 and 6, 4 log s \u2265 Ey\u223cDS\n\u2295\u2113\n[widthF (y)] \u2265 opt(S)\u2113/4 and then s \u2265 2 opt(S)\u2113/16. Therefore,\n7. Any DNF of size less than 2opt(S)\u2113/16 is (1/(4N))-far from \u0393S\u2295\u2113 with respect to D S \u2295\u2113.\nNow, let k = O\u0303(log log n). Suppose, to the contrary, that there is a PAC-learning algorithm for log n-Junta by DNF with error \u01eb = 1/(8N) that runs in time t = n\u03bbk/2 = nO\u0303(log logn), where \u03bb is the constant in item 1. Given a (k, k\u2032)-Set-Cover instance, we run the learning algorithm for \u0393S\u2295\u2113 for \u2113 = log n/k. If the instance has set cover at most k, then by item 4, \u0393 S \u2295\u2113 is log n-Junta. Then the algorithm learns the target and outputs a hypothesis that is (1/(8N))-close to \u0393S\u2295\u2113 with respect to DS\u2295\u2113.\nOn the other hand, if the instance has a minimum-size set cover of at least k\u2032, then any learning algorithm that runs in time t = n\u03bbk/2 = nO\u0303(log logn) cannot output a DNF of size more than t terms. By item 7, any DNF of size less than 2k\n\u2032 logn/(16k) \u2264 2opt(S)\u2113/16 is (1/(4N))-far from \u0393S\u2295\u2113 with respect to DS\u2295\u2113. By choosing the right parameters k and k \u2032, we have 2k \u2032 logn/(16k) > t, and therefore, any DNF that the algorithm outputs has error of at least 1/(4N).\n3Their reduction gives a conjunction of negated variable. So here, we are referring to the dual function. 4That is, there is a term T with opt(S) variables such that for every x in the support of DS , \u0393S(x) = T (x). 5N is the number of sets plus the size of the universe in S .\nTherefore, by estimating the distance of the output of the learning algorithm from \u0393S\u2295\u2113 with respect to DS\u2295\u2113, we can distinguish between instances that have set cover of size less than or equal to k from instances that have a minimum-size set cover greater than k\u2032 in time t = n\u03bbk/2. Thus, we got an algorithm for (k, k\u2032)-Set-Cover that runs in time n\u03bbk/2 < n\u03bbk. This contradicts item 1 and finishes the proof of the first lower bound.\nAssuming a natural conjecture on the hardness of set cover, they give the lower bound n\u2126(log s). We will discuss this in Section 5."
        },
        {
            "heading": "1.3 Our Technique",
            "text": "In this paper, we also use the hardness result of (k, k\u2032)-Set-Cover . As in [9], we identify each element in the universe with an assignment in {0, 1}n and use the function \u0393S and the distribution DS that satisfies:\n1. The instance S has minimum-size set cover opt(S) if and only if the function \u0393S is a conjunction of opt(S) variables over the distribution DS .\nWe then build a monotone target function \u0393S\u2113 and use a different approach to show that any DNF of size less than 2opt(S)\u2113/20 is (1/(8N) \u2212 2\u2212opt(S)\u2113/20)-far from \u0393S\u2113 with respect to D S \u2113 .\nWe define, for any odd \u2113, the monotone function \u0393S\u2113 : ({0, 1} \u2113)n \u2192 {0, 1} where for y =\n(y1, . . . , yn), yi = (yi,1, . . . , yi,\u2113) we have \u0393 S \u2113 (y) = \u0393 S(Majority(y1), . . . ,Majority(yn)) where Majority is the majority function. A distribution DS\u2113 is also defined such that\n2. Pry\u223cDS \u2113 [\u0393S\u2113 (y) = 0] = Pry\u223cDS\u2113 [\u0393S\u2113 (y) = 1] = 1/2.\nIt is clear from the definition of \u0393S\u2113 and item 1 that\n3. \u0393S\u2113 (y) is a monotone (opt(S)\u2113)-Junta over D S \u2113 .\nWe then define the monotone size of a term T to be the number of unnegated variables that appear in T . We first show that\n4. For every DNF F : ({0, 1}\u2113)n \u2192 {0, 1} of size |F | \u2264 2opt(S)\u2113/5 that is \u01eb-far from \u0393S\u2113 with respect to DS\u2113 , there is another DNF F\n\u2032 of size |F \u2032| \u2264 2opt(S)\u2113/5 with terms of monotone size at most opt(S)\u2113/5 that is (\u01eb\u2212 2\u2212opt(S)\u2113/20)-far from \u0393S\u2113 with respect to D S \u2113 .\nThis is done by simply showing that terms of large monotone size in the DNF F have a small weight according to the distribution DS\u2113 and, therefore, can be removed from F with the cost of \u22122\u2212opt(S)\u2113/20 in the error.\nWe then, roughly speaking, show that\n5. Let F \u2032 be a DNF of size |F \u2032| \u2264 2opt(S)\u2113/5 with terms of monotone size at most opt(S)\u2113/5. For every y \u2208 ({0, 1}\u2113)n in the support of DS\u2113 that satisfies \u0393 S \u2113 (y) = 1, either\n\u2022 F \u2032(y) = 0 or \u2022 F \u2032(y) = 1, and at least 1/(2N) fraction of the points z below y in the lattice ({0, 1}\u2113)n\nthat are in the support of DS\u2113 satisfies F \u2032(z) = 1 and \u0393S\u2113 (z) = 0.\nBy item 5, either 1/(4N) fraction of the vectors y that satisfy \u0393S\u2113 (y) = 1 satisfy F \u2032(y) = 0 or (1 \u2212 1/(4N))/(2N) > 1/(4N) fraction of the points z that satisfy \u0393S\u2113 (z) = 0 satisfy F \u2032(z) = 1. Therefore, with item 2, we get that F \u2032 is 1/(8N)-far from \u0393S\u2113 with respect to D S \u2113 . This, with item 4, implies that\n6. If F : ({0, 1}\u2113)n \u2192 {0, 1} is a DNF of size |F | < 2opt(S)\u2113/20, then F is (1/(8N)\u2212 2\u2212opt(S)\u2113/20)far from \u0393S\u2113 with respect to D S \u2113 .\nThe rest of the proof is almost the same as in [9]. See the discussion in subsection 1.1 after item 7."
        },
        {
            "heading": "1.4 Upper Bounds",
            "text": "The only known distribution-free algorithm for log s-Junta is the trivial algorithm that, for every set of m = log s variables S = {xi1 , . . . , xim}, checks if there is a function that depends on S and is consistent with the examples. This algorithm takes nO(log s) time.\nFor size-s decision tree and monotone size-s decision tree, the classic result of Ehrenfeucht and Haussler [4] gives a distribution-free time algorithm that runs in time nO(log s) and outputs a decision tree of size nO(log s).\nThe learning algorithm is as follows: Let T be the target decision tree of size s. First, the algorithm guesses the variable at the root of the tree T and then guesses which subtree of the root has size at most s/2. Then, it recursively constructs the tree of size s/2. When it succeeds, it continues to construct the other subtree.\nFor size-s DNF and monotone size-s DNF, Hellerstein et al. [6] gave a distribution-free proper\nlearning algorithm that runs in time 2O\u0303( \u221a n).\nTo the best of our knowledge, all the other results in the literature for learning the above classes are either restricted to the uniform distribution or use, in addition, a black box queries or returns hypotheses that are not DNF."
        },
        {
            "heading": "2 Definitions and Preliminaries",
            "text": "In this section, we give the definitions and preliminary results that are needed to prove our results."
        },
        {
            "heading": "2.1 Set Cover",
            "text": "Let S = (S,U,E) be a bipartite graph on N = n+ |U | vertices where S = [n], and for every u \u2208 U , deg(u) > 0. We say that C \u2286 S is a set cover of S if every vertex in U is adjacent to some vertex in C. The Set-Cover problem is to find a minimum-size set cover. We denote by opt(S) the size of a minimum-size set cover for S.\nWe identify each element u \u2208 U with the vector (u1, . . . , un) \u2208 {0, 1} n where ui = 0 if and only if (i, u) \u2208 E. We will assume that those vectors are distinct. If there are two distinct elements u, u\u2032 \u2208 U that have the same vector, then you can remove one of them from the graph. This is because every set cover that covers one of them covers the other.\nDefinition 1. The (k, k\u2032)-Set-Cover problem is the following: Given as input a set cover instance S = (S,U,E), and parameters k and k\u2032. Output Yes if opt(S) \u2264 k and No if opt(S) > k\u2032."
        },
        {
            "heading": "2.2 Hardness of Set-Cover",
            "text": "Our results are conditioned on the following randomized exponential time hypothesis (ETH) Hypothesis: [2, 3, 7, 8, 11]. There exists a constant c \u2208 (0, 1) such that 3-SAT on n variables cannot be solved by a randomized algorithm in O(2cn) time with success probability at least 2/3.\nThe following is proved in [10]. See also Theorem 7 in [9]\nLemma 1. [10]. Let k \u2264 12 log logN log log logN and k \u2032 = 12\n(\nlogN log logN\n)1/k be two integers. Assuming ran-\ndomized ETH, there is a constant \u03bb \u2208 (0, 1) such that there is no randomized N\u03bbk time algorithm that can solve (k, k\u2032)-Set-Cover on N vertices with high probability."
        },
        {
            "heading": "2.3 Concept Classes",
            "text": "For the lattice {0, 1}n, and x, y \u2208 {0, 1}n, we define the partial order x \u2264 y if xi \u2264 yi for every i. When x \u2264 y and x 6= y, we write x < y. If x < y, we say that x is below y, or y is above x. A Boolean function f : {0, 1}n \u2192 {0, 1} is monotone if, for every x \u2264 y, we have f(x) \u2264 f(y). A literal is a variable or negated variable. A term is a conjunction (\u2227) of literals. A clause is a disjunction (\u2228) of literals. A monotone term (resp. clause) is a conjunction (resp. disjunction) of unnegated variables. The size of a term T , |T |, is the number of literals in the term T . A DNF (resp. CNF) is a disjunction (resp. conjunction) of terms (resp. clauses). The size |F | of a DNF (resp. CNF) F is the number of terms (resp. clauses) in F . A monotone DNF (resp. monotone CNF) is a DNF (resp. CNF) with monotone terms (resp. clauses).\nWe define the following classes\n1. size-s DNF and size-s Monotone DNF are the classes of DNF and monotone DNF, respectively, of size at most s.\n2. size s-DT and size-s Monotone DT are the classes of decision trees and monotone decision trees, respectively, with at most s leaves.\n3. k-Junta andMonotone k-Junta are the classes of Boolean functions and monotone Boolean functions that depend on at most k variables.\nIt is well known that\nMonotone (log s)-Junta\u2282 size-s Monotone DT \u2282 size-s Monotone DNF . (1)"
        },
        {
            "heading": "2.4 Functions and Distributions",
            "text": "For any set R, we define U(R) to be the uniform distribution over R. For a distribution D over {0, 1}n and two Boolean functions f and g, we define distD(f, g) = Prx\u223cD[f(x) 6= g(x)]. Here, bold letters denote random variables. If distD(f, g) = 0, then we say that f = g over D. For a class of functions C, we say that f is C over D if there is a function g \u2208 C such that f = g over D.\nDefinition 2. (\u0393S and DS) Let S = (S,U,E) be a set cover instance with S = [n]. Recall that we identify each element u \u2208 U with the vector (u1, . . . , un) \u2208 {0, 1}\nn where ui = 0 if and only if (i, u) \u2208 E. We define the partial function \u0393S : {0, 1}n \u2192 {0, 1} where \u0393S(x) = 0 if x \u2208 U and \u0393S(1n) = 1. We define the distribution DS over {0, 1}n where DS(x) = 1/2 if x = 1n, DS(x) = 1/(2|U |) if x \u2208 U , and DS(x) = 0 otherwise. We will remove the superscript S when it is clear from the context and write \u0393 and D.\nFact 1. We have\n1. C \u2286 S is a set cover of S = (S,U,E), if and only if \u0393(x) = \u2227\ni\u2208C xi over D. 2. In particular, If T is a monotone term of size |T | < opt(S), then there is u \u2208 U such\nthat T (u) = 1.\nProof. Let C be a set cover of S. First, we have \u0393(1n) = 1. Now, since C is a set cover, every vertex u \u2208 U is adjacent to some vertex in C. This is equivalent to: for every assignment u \u2208 U , there is i \u2208 C such that ui = 0. Therefore, \u2227i\u2208Cui = 0 for all u \u2208 U . Thus, \u0393(x) = \u2227\ni\u2208C xi over D. The other direction can be easily seen by tracing backward in the above proof.\nFor an odd \u2113, define \u22060 = {a \u2208 {0, 1}\u2113|wt(a) = \u230a\u2113/2\u230b} and \u22061 = {a \u2208 {0, 1}\u2113|wt(a) = \u2308\u2113/2\u2309}, where wt(a) is the Hamming weight of a. Notice that |\u22060| = |\u22061| =\n( \u2113 \u230a\u2113/2\u230b ) .\nDefinition 3. (\u0393\u2113, D\u2113, \u2206 0 n and \u2206 1 n) For an odd \u2113, define \u2206 1 n = (\u2206 1)n and6 \u22060n := \u222au\u2208U \u220fn i=1 \u2206 ui = \u222au\u2208U(\u2206u1 \u00d7 \u2206u2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u2206un). Define the distribution D\u2113 : ({0, 1}\u2113)n \u2192 [0, 1] to be D\u2113(y) = 1/(2|\u22061n|) = 1/(2|\u2206 1|n) if y \u2208 \u22061n, D\u2113(y) = 1/(2|\u2206 0 n|) = 1/(2|U | \u00b7 |\u2206\n0|n) if y \u2208 \u22060n, and D\u2113(y) = 0 otherwise. We define the partial function \u0393\u2113 over the support \u2206 0 n \u222a\u2206 1 n of D\u2113 to be 1 if y \u2208 \u2206 1 n and 0 if y \u2208 \u22060n.\nWe note here that the distribution D\u2113 is well-defined. This is because: First, the sum of the distribution of the points in \u22061n is 1/2. Second, for two different u, u\n\u2032 \u2208 U , we have that \u220fn\ni=1\u2206 ui and \u220fn i=1 \u2206 u\u2032i are disjoint sets. Therefore, |\u22060n| = |U | \u00b7 |\u2206 0|n, and therefore, the sum of\nthe distribution of all the points in \u22060n is half. In particular,\nFact 2. We have Pr y\u223cD\u2113 [\u0393\u2113(y) = 1] = Pr y\u223cD\u2113 [\u0393\u2113(y) = 0] = PrD\u2113 [\u22061n] = PrD\u2113 [\u22060n] = 1 2 .\nFor y \u2208 ({0, 1}\u2113)n, we write y = (y1, . . . , yn), where yj = (yj,1, yj,2, . . . , yj,\u2113) \u2208 {0, 1} \u2113. Let (Majority(yi))i\u2208[n] = (Majority(y1), . . . ,Majority(yn)) whereMajority is the majority function.\nFact 3. If C \u2286 S is a set cover of S, then \u0393\u2113(y) = \u0393((Majority(yi))i\u2208[n]) = \u2227\ni\u2208C Majority(yi) over D. In particular, \u0393\u2113 is Monotone opt(S)\u2113-Junta over D.\nProof. First notice that Majority(x) = 1 if x \u2208 \u22061 and Majority(x) = 0 if x \u2208 \u22060. Therefore, for x \u2208 \u2206\u03be, \u03be \u2208 {0, 1} we have Majority(x) = \u03be.\nFor y \u2208 \u22061n = (\u2206 1)n, (Majority(yi))i\u2208[n] = 1 n and \u0393\u2113(y) = 1 = \u0393(1 n). For y \u2208 \u22060n = \u222au\u2208U (\u2206 u1 \u00d7\u2206u2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u2206un), there is u such that y \u2208 \u2206u1 \u00d7\u2206u2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u2206un .\nThen, (Majority(yi))i\u2208[n] = u and \u0393\u2113(y) = \u0393((Majority(yi))i\u2208[n]) = \u0393(u) = 0.\nFor t \u2208 [\u2113], \u03be \u2208 {0, 1} and u \u2208 {0, 1}\u2113, we define ut\u2190\u03be \u2208 {0, 1}\u2113 the vector that satisfies\nut\u2190\u03bei =\n{\nui i 6= t \u03be i = t .\nLet z \u2208 ({0, 1}\u2113)n. For j \u2208 [\u2113]n and a \u2208 {0, 1}n, define zj\u2190a = (zj1\u2190a11 , . . . , z jn\u2190an n ). For a set\nV \u2286 {0, 1}n, we define zj\u2190V = {zj\u2190v|v \u2208 V }. We define one(z) =\n\u220fn i=1{mi|zi,mi = 1} = {m1|z1,m1 = 1} \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 {mn|zn,mn = 1}.\n6Here \u2206\u03be = \u22060 if \u03be = 0 and \u22061 if \u03be = 1.\nFact 4. Let w \u2208 \u22061n, j \u2208 one(w), and T be a term that satisfies T (w) = 1. Then\n1. wj\u2190U \u2286 \u22060n. 2. |wj\u2190U | = |U |. 3. If T j(y1,j1 , . . . , yn,jn) is the conjunction of all the variables that appear in T of the form yi,ji,\nthen T (wj\u2190a) = T j(a).\nProof. We first prove item 1. Let u \u2208 U and i be any integer in [n]. Since w \u2208 \u22061n, we have wi \u2208 \u2206 1. Since j \u2208 one(w), we have wi,ji = 1. Therefore, w ji\u2190ui i \u2208 \u2206 ui for all i \u2208 [n] and wj\u2190u \u2208 \u220fn i=1 \u2206 ui . Thus, wj\u2190u \u2208 \u22060n for all u \u2208 U . To prove item 2, let u, u\u2032 be two distinct elements of U . There is i such that ui 6= u\u2032i. Therefore wji\u2190uii 6= w ji\u2190u\u2032i i and w j\u2190u 6= wj\u2190u \u2032\n. We now prove item 3. Let T \u2032 be the conjunction of all the variables that appear in T that are not of the form yi,ji. Then T = T \u2032 \u2227 T j . Since T (w) = 1, we have T \u2032(w) = 1. Since the entries of wj\u2190a are equal to those in w on all the variables that are not of the form yi,ji, we have T \u2032(wj\u2190a) = 1. Therefore, T (wj\u2190a) = T \u2032(wj\u2190a) \u2227 T j(wj\u2190a1,j1 , . . . , w j\u2190a n,jn ) = T j(a).\nWe now give a different way of sampling according to the distribution D\u2113.\nFact 5. Let S be a Set-Cover instance. The following is an equivalent way of sampling from D\u2113.\n1. Draw \u03be \u2208 {0, 1} u.a.r.7 2. Draw w \u2208 \u22061n u.a.r. 3. If \u03be = 1 then output y = w.\n4. If \u03be = 0 then\n(a) draw j \u2208 one(w) u.a.r. (b) draw v \u2208 wj\u2190U u.a.r. (c) output y = v.\nIn particular, for any event X,\nPr y\u223cU(\u22060n) [X] = Pr w\u223cU(\u22061n),j\u223cU(one(w)),y\u223cU(wj\u2190U ) [X].\nProof. Denote the above distribution by D\u2032. By Item 1 in Fact 4, if w \u2208 \u22061n and j \u2208 one(w), then wj\u2190U \u2286 \u22060n. Therefore, for z \u2208 \u2206 1 n, Pry\u223cD\u2032 [y = z|\u03be = 0] = 0 and then\nPr y\u223cD\u2032 [y = z] = Pr \u03be\u223cU({0,1}) [\u03be = 1] \u00b7 Pr y\u223cU(\u22061n)\n[y = z] = 1\n2|\u22061n| =\n1\n2|\u22061|n .\nFor z \u2208 \u22060n, suppose z \u2208 \u2206 u1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u2206un where u \u2208 U . In the sampling according to D\u2032 and when \u03be = 0, since for j \u2208 one(w), the elements of wj\u2190U are below w, we have Pr y\u223cD\u2032 [y = z|w 6> z] = 0. Therefore,\nPr y\u223cD\u2032 [y = z] = Pr \u03be\u223cU({0,1}) [\u03be = 0] \u00b7 Pr w\u223cU(\u22061n) [w > z] \u00b7\n\u00b7 Pr j\u223cU(one(w)) [z \u2208 wj\u2190U |w > z,w \u2208 \u22061n] \u00b7 Pr v\u223cU(wj\u2190U ) [v = z|z \u2208 wj\u2190U ]. (2)\n7Uniformly at random.\nNow, since, for x \u2208 \u22060, the number of elements in \u22061 that are above x is \u2308\u2113/2\u2309, we have that the number of w \u2208 \u22061n = (\u2206 1)n that are above z \u2208 \u2206u1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u2206un is \u2308\u2113/2\u2309n\u2212wt(u). Therefore,\nPr w\u223cU(\u22061n)\n[w > z] = \u2308\u2113/2\u2309n\u2212wt(u)\n|\u22061n| . (3)\nNow let w > z and w \u2208 \u22061n. Since for two different u, u \u2032 \u2208 U , we have \u220fn i=1\u2206 ui and \u220fn i=1 \u2206 u\u2032i are disjoint sets, and since z \u2208 \u2206u1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7\u2206un , we have z \u2208 wj\u2190U if and only if z = wj\u2190u. Therefore, the number of elements j \u2208 one(w) that satisfy z \u2208 wj\u2190U is the number of elements j \u2208 one(w) that satisfy z = wj\u2190u. This is the number of elements j \u2208 one(w) that satisfies for every ui = 0, zi,ji = 0. For a j u.a.r. and a fixed i where ui = 0, the probability that zi and wi differ only in entry ji is 1/\u2308\u2113/2\u2309. Therefore,\nPr j\u223cU(one(w))\n[z \u2208 wj\u2190U |w > z,w \u2208 \u22061n] = 1\n\u2308\u2113/2\u2309n\u2212wt(u) . (4)\nFinally, by item 2 in Fact 4, since |wj\u2190U | = |U |, we have\nPr v\u223cU(wj\u2190U )\n[v = z|z \u2208 wj\u2190U ] = 1\n|wj\u2190U | =\n1\n|U | . (5)\nBy (2), (3), (4), and (5), we have\nPr y\u223cD\u2032\n[y = z] = 1 2 \u00b7 \u2308\u2113/2\u2309n\u2212wt(u) |\u22061n| \u00b7\n1\n\u2308\u2113/2\u2309n\u2212wt(u) \u00b7\n1\n|U | =\n1\n2|U | \u00b7 |\u22061n| =\n1\n2|U | \u00b7 |\u22060|n ."
        },
        {
            "heading": "3 Main Lemma",
            "text": "In this section, we prove\nLemma 2. Let S = (S,U,E) be a set cover instance. If F : ({0, 1}\u2113)n \u2192 {0, 1} is a DNF of size |F | < 2opt(S)\u2113/20, then distD\u2113(F,\u0393\u2113) \u2265 1/(8|U |) \u2212 2 \u2212opt(S)\u2113/20.\nNote that Lemma 2 is used to prove Theorem 1 and 2. To prove Theorem 3, we will need Lemma 4, a stronger version of Lemma 2.\nTo prove the lemma, we first establish some results. For a term T , let TM be the conjunction of all the unnegated variables in T . We define the\nmonotone size of T to be |TM|.\nClaim 1. Let S = (S,U,E) be a set cover instance and \u2113 \u2265 5. If F : ({0, 1}\u2113)n \u2192 {0, 1} is a DNF of size |F | < 2opt(S)\u2113/20, then there is a DNF, F \u2032, of size |F \u2032| \u2264 2opt(S)\u2113/20 with terms of monotone size at most opt(S)\u2113/5 such that distD\u2113(\u0393\u2113, F \u2032) \u2264 distD\u2113(\u0393\u2113, F ) + 2 \u2212opt(S)\u2113/20.\nProof. Let T be a term of monotone size at least opt(S)\u2113/5. Let bi denote the number of unnegated variables of T of the form yi,j and let Ti be their conjunction. Then TM = \u2227ni=1Ti and \u2211n i=1 bi = |TM| \u2265 opt(S)\u2113/5. If, for some i, bi > \u2308\u2113/2\u2309, then the term Ti is zero on all \u22060\u222a\u22061, and therefore,\nT is zero on all \u22060n \u222a\u2206 1 n. Thus, it can be just removed from F . So, we may assume that bi \u2264 \u2308\u2113/2\u2309 for all i. First,\nPr y\u223cD\u2113 [T (y) = 1|\u0393\u2113(y) = 1] = Pr y\u223cU(\u22061n) [T (y) = 1] \u2264 Pr y\u223cU(\u22061n) [TM(y) = 1]\n=\nn \u220f\ni=1\nPr yi\u223cU(\u22061) [Ti(yi) = 1]\n=\nn \u220f\ni=1\n( \u2113\u2212bi \u2308\u2113/2\u2309\u2212bi )\n( \u2113 \u2308\u2113/2\u2309 )\n=\nn \u220f\ni=1\n(\n1\u2212 bi \u2113\n)(\n1\u2212 bi\n\u2113\u2212 1\n)\n\u00b7 \u00b7 \u00b7\n(\n1\u2212 bi\n\u2308\u2113/2\u2309 + 1\n)\n\u2264\nn \u220f\ni=1\n\u2113 \u220f\nj=\u2308\u2113/2\u2309+1 exp(\u2212bi/j) =\nn \u220f\ni=1\nexp\n\n\u2212bi\n\u2113 \u2211\nj=\u2308\u2113/2\u2309+1 1/j\n\n\n= exp\n \u2212|TM| \u2113 \u2211\nj=\u2308\u2113/2\u2309+1 1/j\n\n \u2264 2\u2212|TM|/2 \u2264 2\u2212opt(S)\u2113/10. (6)\nLet F \u2032 be the disjunction of all the terms in F of monotone size at most opt(S)\u2113/5. Let T (1), . . . , T (m) be all the terms of monotone size greater than opt(S)\u2113/5 in F . Then, by (6) and the union bound,\nPr y\u223cD\u2113 [F (y) 6= F \u2032(y)|\u0393\u2113(y) = 1] \u2264 Pr y\u223cD\u2113 [\u2228mi=1T (i)(y) = 1|\u0393\u2113(y) = 1]\n\u2264 2\u2212opt(S)\u2113/10m \u2264 2\u2212opt(S)\u2113/20. (7)\nand (Here we abbreviate F \u2032(y), F (y) and \u0393\u2113(y) by F \u2032, F and \u0393\u2113)\ndistD\u2113(\u0393\u2113, F \u2032) = Pr y\u223cD\u2113 [F \u2032 6= \u0393\u2113]\n= 1\n2 Pr y\u223cD\u2113\n[F \u2032 6= \u0393\u2113|\u0393\u2113 = 1] + 1\n2 Pr y\u223cD\u2113\n[F \u2032 6= \u0393\u2113|\u0393\u2113 = 0] (8)\n= 1\n2 Pr y\u223cD\u2113\n[F \u2032 6= F |\u0393\u2113 = 1] +\n1 2 Pr y\u223cD\u2113 [F 6= \u0393\u2113|\u0393\u2113 = 1] + 1 2 Pr y\u223cD\u2113 [F \u2032 6= \u0393\u2113|\u0393\u2113 = 0] (9)\n\u2264 2\u2212opt(S)\u2113/20 + 1\n2 Pr y\u223cD\u2113\n[F 6= \u0393\u2113|\u0393\u2113 = 1] + 1\n2 Pr y\u223cD\u2113 [F 6= \u0393\u2113|\u0393\u2113 = 0] (10)\n= 2\u2212opt(S)\u2113/20 + distD\u2113(\u0393\u2113, F ).\nIn (8), we used Fact 2. In (9), we used the probability triangle inequality. In (10), we used (7) and the fact that if F \u2032(y) 6= 0, then F (y) 6= 0.\nWe now prove\nClaim 2. Let z \u2208 \u22061n. Let F be a DNF with terms of monotone size at most \u2308\u2113/2\u2309(opt(S)\u2212 1)/2 that satisfies F (z) = 1. Then\nPr j\u223cU(one(z)),y\u223cU(zj\u2190U )\n[F (y) = 1] \u2265 1\n2|U | .\nProof. Since F (z) = 1, there is a term T in F that satisfies T (z) = 1. Let Y0 = {yi,m|zi,m = 0} and Y1 = {yi,m|zi,m = 1}. Since T (z) = 1, every variable in Y0 that appears in T must be negated, and every variable in Y1 that appears in T must be unnegated. For j \u2208 one(z), define q(j) to be the number of variables in {y1,j1 , . . . , yn,jn} that appear in T (y). All those variables appear unnegated in T because j \u2208 one(z). Recall that TM is the conjunction of all unnegated variables in T . Then |TM| \u2264 \u2308\u2113/2\u2309(opt(S)\u2212 1)/2. Each variable in TM contributes \u2308\u2113/2\u2309n\u22121 to the sum \u2211\nj\u2208one(z) q(j) and |one(z)| = \u2308\u2113/2\u2309n. Therefore,\nE j\u223cU(one(z)) [q(j)] = |TM| \u2308\u2113/2\u2309 \u2264 opt(S)\u2212 1 2 .\nBy Markov\u2019s bound, at least half the elements j \u2208 one(z) satisfies q(j) \u2264 opt(S) \u2212 1. Let J = {j \u2208 one(z)|q(j) < opt(S)}. Then Prj\u223cU(one(z))[j \u2208 J ] \u2265 1/2. Consider j \u2208 J and let T\nj be the conjunction of all the variables that appear in T of the form yi,ji. Then |T\nj| = q(j) \u2264 opt(S) \u2212 1. By Fact 1, there is u \u2208 U such that T j(u) = 1. By Fact 4, we have T (zj\u2190u) = T j(u) = 1. Then F (zj\u2190u) = 1. Since by item 1 in Fact 4, |zj\u2190U | = |U |, we have\nPrj\u223cU(one(z)),y\u223cU(zj\u2190U )[F (y) = 1|j \u2208 J ] \u2265 1\n|U | .\nTherefore,\nPr j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F (y) = 1] \u2265 Pr j\u223cU(one(z)) [j \u2208 J ]\u00b7 Pr j\u223cU(one(z)),y\u223cU(zj\u2190U )\n[F (y) = 1|j \u2208 J ] \u2265 1\n2|U | .\nWe are now ready to prove\nLemma 2. Let S = (S,U,E) be a set cover instance, and let \u2113 \u2265 5. If F : ({0, 1}\u2113)n \u2192 {0, 1} is a DNF of size |F | < 2opt(S)\u2113/20, then distD\u2113(F,\u0393\u2113) \u2265 1/(8|U |) \u2212 2 \u2212opt(S)\u2113/20.\nProof. By Claim 1, there is a DNF, F \u2032, of size |F \u2032| \u2264 2opt(S)\u2113/20 with terms of monotone size at most opt(S)\u2113/5 such that distD\u2113(\u0393\u2113, F \u2032) \u2264 distD\u2113(\u0393\u2113, F ) + 2 \u2212opt(S)\u2113/20. Therefore, it is enough to prove that distD\u2113(\u0393\u2113, F \u2032) \u2265 1/(8|U |).\nIf Pr y\u223cU(\u22061n)\n[F \u2032(y) 6= 1] \u2265 1/(4|U |), then by Fact 2, we have\ndistD\u2113(\u0393\u2113, F \u2032) \u2265 Pr\ny\u223cD\u2113 [\u0393\u2113(y) 6= F \u2032(y)|\u0393\u2113(y) = 1] Pr y\u223cD\u2113 [\u0393\u2113(y) = 1] = 1 2 Pr y\u223cU(\u22061n) [F \u2032(y) 6= 1] \u2265 1 8|U | .\nIf Pr y\u223cU(\u22061n)\n[F \u2032(y) 6= 1] < 1/(4|U |), then by Fact 2 and 5, and Claim 2,\ndistD\u2113(\u0393\u2113, F \u2032) \u2265 Pr\ny\u223cD\u2113 [\u0393\u2113(y) 6= F \u2032(y)|\u0393\u2113(y) = 0] Pr y\u223cD\u2113 [\u0393\u2113(y) = 0]\n= 1\n2 Pr y\u223cU(\u22060n) [F \u2032(y) = 1]\n= 1\n2 Pr z\u223cU(\u22061n),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F \u2032(y) = 1]\n\u2265 1\n2 Pr z\u223cU(\u22061n),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F \u2032(y) = 1|F \u2032(z) = 1] \u00b7 Pr z\u223cU(\u22061n) [F \u2032(z) = 1]\n\u2265 1\n2\n1\n2|U |\n(\n1\u2212 1\n4|U |\n)\n\u2265 1\n8|U | ."
        },
        {
            "heading": "4 Superpolynomial Lower Bound",
            "text": "In this section, we prove the first results of the paper. First, we prove the following result for Monotone (log n)-Junta.\nLemma 3. Assuming randomized ETH, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log n)-Junta by DNF with \u01eb = 1/(16n) must take at least\nn c log log n log log log n\ntime. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time and can compute the target on all the points of the support of the distribution in polynomial time.\nProof. Consider the constant \u03bb in Lemma 1. Let c = min(1/40, \u03bb/4). Suppose there is a PAC learning algorithm A for Monotone (log n)-Junta by DNF with \u01eb = 1/(16n) that runs in time nc log log n log log log n . We show that there is k such that for\nk\u2032 = 1\n2\n(\nlogN\nlog logN\n)1/k\n,\n(k, k\u2032)-Set-Cover can be solved in time N4ck \u2264 N\u03bbk. By Lemma 1, the result then follows. Let S = (S,U,E) be an N -vertex (k, k\u2032)-Set-Cover instance where\nk = 1\n2\nlog logN\nlog log logN and k\u2032 =\n1\n2\n(\nlogN\nlog logN\n)1/k\n.\nLet\n\u2113 = logN\nk\nand consider \u0393\u2113 and D\u2113. Consider the following algorithm B\n1. Input S = (S,U,E) an instance for (k, k\u2032)-Set-Cover .\n2. Construct \u0393\u2113 and D\u2113. 3. Run A using \u0393\u2113 and D\u2113. If it runs more than N 4ck steps, then output No . 4. Let F be the output DNF.\n5. Estimate \u03b7 = distD\u2113(F,\u0393\u2113). 6. If \u03b7 \u2264 116N , output Yes , otherwise output No .\nThe running time of this algorithm is N4ck \u2264 N\u03bbk. Therefore, it is enough to prove the following\nClaim 3. Algorithm B solves (k, k\u2032)-Set-Cover .\nProof. Yes case: Let S = (S,U,E) be a (k, k\u2032)-Set-Cover instance and opt(S) \u2264 k. Then, opt(S) \u00b7 \u2113 \u2264 k\u2113 = logN , and by Fact 3, \u0393\u2113 is Monotone logN-Junta. Therefore, w.h.p., algorithm A learns \u0393\u2113 and outputs a DNF that is \u03b7 = 1/(16N) close to the target with respect to D\u2113. Since B terminates A after N\n4ck time, we only need to prove that A runs at most N4ck time. The running time of A is\nN c log logN log log logN < N4ck.\nNo Case: Let S = (S,U,E) be a (k, k\u2032)-Set-Cover instance and opt(S) > k\u2032. By Lemma 2, any DNF, F , of size |F | < 2k\n\u2032\u2113/20 satisfies distD\u2113(F,\u0393\u2113) \u2265 1/(8|U |) \u2212 2 \u2212k\u2032\u2113/20. First, we have\n(2k)2k =\n(\nlog logN\nlog log logN\n) log logN\nlog log logN < logN\nlog logN .\nTherefore, since c \u2264 1/40,\nk\u2032 = 1\n2\n(\nlogN\nlog logN\n)1/k\n> 1\n2 (2k)2 > 80ck2.\nSo k\u2032\u2113/20 > (k\u2113)(4ck) and 2k \u2032\u2113/20 > (2k\u2113)4ck = N4ck.\nNow since the algorithm runs in time N4ck, it cannot output a DNF F of size more than N4ck < 2k \u2032\u2113/20, and by Lemma 2,\ndistD\u2113(F,\u0393\u2113) \u2265 1\n8|U | \u2212\n1\nN4ck \u2265\n1\n9N .\nSo it either runs more than N4ck steps and then outputs No in step 3 or outputs a DNF with an error greater than 1/(9N) > 1/(16N) and outputs No in step 6.\nNotice that the learning algorithm knows \u0393\u2113 and D\u2113. It is also clear from the definition of \u0393\u2113 and D\u2113 that the learning algorithm can draw a sample according to the distribution D\u2113 in polynomial time and can compute the target \u0393\u2113 on all the points of the support of the distribution in polynomial time.\nWe now prove\nTheorem 1. Assuming randomized ETH, there is a constant c such that any PAC learning algorithm for n-variable size-s Monotone DT and size-s Monotone DNF by DNF with \u01eb = 1/(16n) must take at least\nnc log log s log log log s\ntime. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time and can compute the target on all the points of the support of the distribution in polynomial time.\nProof. By Lemma 3, assuming randomized ETH, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log n)-Junta by DNF with \u01eb = 1/(16n) runs in time\nnc log log n log log logn .\nNow by (1) and since s = n, the result follows."
        },
        {
            "heading": "5 Tight Bound Assuming some Conjecture",
            "text": "A plausible conjecture on the hardness of Set-Cover is the following.\nConjecture 1. [9] There are constants \u03b1, \u03b2, \u03bb \u2208 (0, 1) such that, for k < N\u03b1, there is no randomized N\u03bbk time algorithm that can solve (k, (1 \u2212 \u03b2) \u00b7 k lnN)-Set-Cover on N vertices with high probability.\nWe now prove\nTheorem 2. Assuming Conjecture 1, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log s)-Junta, size-s Monotone DT and size-s Monotone DNF by DNF with \u01eb = 1/(16n) must take at least\nnc log s\ntime. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time and can compute the target on all the points of the support of the distribution in polynomial time.\nProof. We give the proof for Monotone (log s)-Junta. As in the proof of Theorem 1, the result then follows for the other classes.\nConsider the constants \u03b1, \u03b2 and \u03bb in Conjecture 1. Let c = min(\u03bb/10, (1 \u2212 \u03b2)/(20 log e)). Suppose there is a PAC learning algorithm A for Monotone (log s)-Junta by DNF with \u01eb = 1/(16n) that runs in time nc log s. We show that there is k < N\u03b1, k = \u03c9(1), such that (k, k\u2032)-SetCover can be solved in time N\u03bbk where k\u2032 = (1 \u2212 \u03b2)k lnN . By Conjecture 1, the result then follows.\nConsider the following algorithm B\n1. Input S = (S,U,E) an instance for (k, k\u2032)-Set-Cover .\n2. Construct \u03935 and D5.\n3. Run A using \u03935 and D5 with s = 2 5k. If it runs more than N5ck steps, then output No . 4. Let F be the output DNF.\n5. Estimate \u03b7 = distD5(F,\u03935). 6. If \u03b7 \u2264 116N , output Yes , otherwise output No .\nSince c < \u03bb/10, the running time of this algorithm is N5ck < N\u03bbk. Therefore, it is enough to prove the following\nClaim 4. Algorithm B solves (k, k\u2032)-Set-Cover .\nProof. Yes case: Let S = (S,U,E) be a (k, k\u2032)-Set-Cover instance and opt(S) \u2264 k. Then, 5 \u00b7 opt(S) \u2264 5k = log s, and by Fact 3, \u03935 is Monotone log s-Junta. Therefore, w.h.p., algorithm A learns \u03935 and outputs a DNF that is \u03b7 = 1/(16N) close to the target with respect to D5. Since B terminates A after N5ck time, we only need to prove that A runs at most N5ck time.\nThe running time of A is nc log s \u2264 N5ck.\nNo Case: Let S = (S,U,E) be a (k, k\u2032)-Set-Cover instance and opt(S) > k\u2032 = (1\u2212 \u03b2)k lnN . By Lemma 2, any DNF, F , of size |F | < 2k\n\u2032/4 satisfies distD5(F,\u03935) \u2265 1/(8|U |) \u2212 2 \u2212k\u2032/4. Since,\nc < (1\u2212 \u03b2)/(20 log e),\n2k \u2032/4 = 2 (1\u2212\u03b2)k lnN 4 = N (1\u2212\u03b2)k 4 log e > N5ck,\nany DNF, F , that the learning outputs satisfies\ndistD5(F,\u03935) \u2265 1 8|U | \u2212 2\u2212k \u2032/4 \u2265 1 8N \u2212 1 N5ck \u2265 1 9N .\nTherefore, with high probability the algorithm answer No ."
        },
        {
            "heading": "6 Strictly Proper Learning",
            "text": "In this section, we prove\nTheorem 3. Assuming randomized ETH, there is a constant c such that any PAC learning algorithm for n-variable Monotone (log s)-Junta, size-s Monotone DT and size-s Monotone DNF by size-s DNF with \u01eb = 1/(16n) must take at least\nnc log s\ntime. The lower bound holds, even if the learner knows the distribution, can draw a sample according to the distribution in polynomial time and compute the target on all the points of the support of the distribution in polynomial time.\nWe first prove the following stronger version of Lemma 2\nLemma 4. Let S = (S,U,E) be a set cover instance, and let \u2113 \u2265 5. If F : ({0, 1}\u2113)n \u2192 {0, 1} is a DNF of size |F | < 2opt(S)\u2113/16, then distD\u2113(F,\u0393\u2113) \u2265 1/(8|U |).\nTo prove this lemma, we will give some more results. Recall that, for a term T , TM is the conjunction of all the unnegated variables in T . We define the monotone size of T to be |TM|. For a DNF F = T1 \u2228 T2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Ts and z \u2208 ({0, 1}\u2113)n, we define the monotone width of z in F as\nmwidthF (z) :=\n{\nminTi(z)=1 |(Ti)M| F (z) = 1 0 F (z) = 0 .\nWe define F\u22121(1) = {z|F (z) = 1} and\n\u2126 = \u22061n \u2229 F \u22121(1).\nClaim 5. Let F be a DNF with\nE z\u223cU(\u2126)\n[mwidthF (z)] \u2264 opt(S) \u00b7 \u2113/4.\nThen\nPr z\u223cU(\u2126),j\u223cU(one(z)),y\u223cU(zj\u2190U )\n[F (y) = 1] \u2265 1\n2|U | .\nProof. Let z \u2208 \u2126. Then F (z) = 1 and z \u2208 \u22061n. Let T z be the term in F with |T zM| = mwidthF (z) that satisfies T z(z) = 1. Let Y0 = {yi,m|zi,m = 0} and Y1 = {yi,m|zi,m = 1}. Since T z(z) = 1, every variable in Y0 that appears in T z must be negated, and every variable in Y1 that appears in T z must be unnegated. For j \u2208 one(z), define qz(j) to be the number of variables in {y1,j1 , . . . , yn,jn} that appear in T z(y). All those variables appear unnegated in T because j \u2208 one(z). Each variable in T zM contributes \u2308\u2113/2\u2309 n\u22121 to the sum \u2211 j\u2208one(z) qz(j) and |one(z)| = \u2308\u2113/2\u2309 n. Therefore,\nE j\u223cU(one(z)) [qz(j)] = |T zM| \u2308\u2113/2\u2309 = mwidthF (z) \u2308\u2113/2\u2309 .\nNow,\nE z\u223cU(\u2126),j\u223cU(one(z))\n[qz(j)] =\nE z\u223cU(\u2126)\n[mwidthF (z)]\n\u2308\u2113/2\u2309\n\u2264 opt(S)\n2 .\nBy Markov\u2019s bound,\nPr z\u223cU(\u2126),j\u223cU(one(z))\n[qz(j) < opt(S)] \u2265 1\n2 .\nSuppose for some z \u2208 \u2126 and j \u2208 one(z), we have qz(j) < opt(S). Let T j be the conjunction of all the variables that appear in T zM of the form yi,ji. Then |T j | = qz(j) < opt(S). By Fact 1, there is u \u2208 U such that T j(u) = 1. By Fact 4, we have T z(zj\u2190u) = T j(u) = 1. Then F (zj\u2190u) = 1. Since by item 1 in Fact 4, |zj\u2190U | = |U |, we have\nPr z\u223cU(\u2126),j\u223cU(one(z)),y\u223cU(zj\u2190U )\n[F (y) = 1|qz(j) < opt(S)] \u2265 1\n|U | .\nTherefore,\nPr z\u223cU(\u2126),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F (y) = 1] \u2265 Pr z\u223cU(\u2126),j\u223cU(one(z)) [qz(j) < opt(S)] \u00b7\nPr z\u223cU(\u2126),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F (y) = 1|qz(j) < opt(S)]\n\u2265 1\n2|U | .\nClaim 6. Let S = (S,U,E) be a set cover instance, and let \u2113 \u2265 5. If F : ({0, 1}\u2113)n \u2192 {0, 1} is a DNF and E\nz\u223cU(\u2126) [mwidthF (z)] \u2264 opt(S)\u2113/4, then distD\u2113(F,\u0393\u2113) \u2265 1/(8|U |).\nProof. If Pr y\u223cU(\u22061n) [F (y) 6= 1] \u2265 1/(4|U |), then by Fact 2, we have\ndistD\u2113(\u0393\u2113, F ) \u2265 Pr y\u223cD\u2113 [\u0393\u2113(y) 6= F (y)|\u0393\u2113(y) = 1] Pr y\u223cD\u2113\n[\u0393\u2113(y) = 1] = 1\n2 Pr y\u223cU(\u22061n) [F (y) 6= 1] \u2265\n1\n8|U | .\nIf Pr y\u223cU(\u22061n) [F (y) 6= 1] < 1/(4|U |), then by Fact 2 and 5, and Claim 5,\ndistD\u2113(\u0393\u2113, F ) \u2265 Pr y\u223cD\u2113 [\u0393\u2113(y) 6= F (y)|\u0393\u2113(y) = 0] Pr y\u223cD\u2113 [\u0393\u2113(y) = 0]\n= 1\n2 Pr y\u223cU(\u22060n) [F (y) = 1]\n= 1\n2 Pr z\u223cU(\u22061n),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F (y) = 1]\n\u2265 1\n2 Pr z\u223cU(\u22061n),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F (y) = 1|F (z) = 1] \u00b7 Pr z\u223cU(\u22061n) [F (z) = 1]\n= 1\n2 Pr z\u223cU(\u2126),j\u223cU(one(z)),y\u223cU(zj\u2190U ) [F (y) = 1] \u00b7 Pr z\u223cU(\u22061n) [F (z) = 1]\n\u2265 1\n2\n1\n2|U |\n(\n1\u2212 1\n4|U |\n)\n\u2265 1\n8|U | .\nClaim 7. Let F be a size-s DNF formula for s \u2265 2 such that distD\u2113(F,\u0393\u2113) \u2264 1/4, then\nE y\u223cU(\u2126)\n[mwidthF (y)] \u2264 4 log s.\nProof. First, we have\n3 4 \u2264 Pr y\u223cD\u2113 [F (y) = \u0393\u2113(y)]\n= 1\n2 Pr y\u223cD\u2113\n[F (y) = \u0393\u2113(y)|\u0393\u2113(y) = 1] + 1\n2 Pr y\u223cD\u2113 [F (y) = \u0393\u2113(y)|\u0393\u2113(y) = 0]\n\u2264 1\n2 Pr y\u223cU(\u22061n) [F (y) = 1] +\n1 2 .\nTherefore, Pry\u223cU(\u22061n)[F (y) = 1] \u2265 1/2. Let F = T1 \u2228 T2 \u2228 \u00b7 \u00b7 \u00b7 \u2228 Ts. For y \u2208 \u2126, let \u03c9(y) \u2208 [s] be the minimum integer such that mwidthF (y) = |(T\u03c9(y))M| and T\u03c9(y)(y) = 1. Then, by (6),\nPr y\u223cU(\u2126) [Ti(y) = 1] = Pr y\u223cU(\u22061n) [Ti(y) = 1|F (y) = 1] =\nPr y\u223cU(\u22061n) [Ti(y) = 1]\nPr y\u223cU(\u22061n)\n[F (y) = 1] \u2264 2\u2212|(Ti)M|/2+1.\nNow, by the concavity of log,\n1 2 E y\u223cU(\u2126) [mwidthF (y)]\u2212 1 = E y\u223cU(\u2126) [ log ( 2mwidthF (y)/2\u22121 )]\n\u2264 log\n(\nE y\u223cU(\u2126)\n[ 2mwidthF (y)/2\u22121 ]\n)\n= log\n\n\n\u2211 i\u2208[s] 2|(Ti)M|/2\u22121 Pr y\u223cU(\u2126) [\u03c9(y) = i]\n\n\n\u2264 log\n\n\n\u2211 i\u2208[s] 2|(Ti)M|/2\u22121 Pr y\u223cU(\u2126) [Ti(y) = 1]\n\n\n\u2264 log\n\n\n\u2211 i\u2208[s] 2|(Ti)M|/2\u221212\u2212|(Ti)M|/2+1\n\n\n= log s.\nTherefore, E y\u223cU(\u2126) [mwidthF (y)] \u2264 4 log s.\nWe are now ready to prove Lemma 4\nProof. If distD\u2113(F,\u0393\u2113) > 1/4, then the result follows. Now suppose distD\u2113(F,\u0393\u2113) \u2264 1/4. If s = |F | < 2opt(S)\u2113/16, then by Claim 7, E\ny\u223cU(\u2126) [mwidthF (y)] \u2264 4 log s = opt(S)\u2113/4. Then by Claim 6,\ndistD\u2113(F,\u0393\u2113) \u2265 1/(8|U |).\nThe proof of Theorem 3 is the same as the proof of Theorem 14 in [9]. We give the proof for completeness.\nProof. Consider the constant \u03bb in Lemma 1. Let c = \u03bb/6. Suppose there is a PAC learning algorithm A for Monotone (log s)-Junta by size-s DNF with \u01eb = 1/(16n) that runs in time nclog s. We show that there is k such that for\nk\u2032 = 1\n2\n(\nlogN\nlog logN\n)1/k\n,\n(k, k\u2032)-Set-Cover can be solved in time N5ck \u2264 N\u03bbk. By Lemma 1, the result then follows.\nLet S = (S,U,E) be an N -vertex (k, k\u2032)-Set-Cover instance where\nk = 1\n2\nlog logN\nlog log logN and k\u2032 =\n1\n2\n(\nlogN\nlog logN\n)1/k\n.\nConsider the following algorithm B\n1. Input S = (S,U,E) an instance for (k, k\u2032)-Set-Cover . 2. Construct \u03935 and D5. 3. Run A using \u03935 and D5 with s = 2 5k and n = N . If it runs more than N5ck steps, then\noutput No .\n4. Let F be the output DNF.\n5. If |F | > s then output No .\n6. Estimate \u03b7 = distD5(F,\u03935). 7. If \u03b7 \u2264 116N , output Yes , otherwise output No .\nThe running time of this algorithm is N5ck \u2264 N\u03bbk. Therefore, it is enough to prove the following\nClaim 8. Algorithm B solves (k, k\u2032)-Set-Cover .\nProof. Yes case: Let S = (S,U,E) be a (k, k\u2032)-Set-Cover instance and opt(S) \u2264 k. Then, size(\u03935) \u2264 2\n5\u00b7opt(S) \u2264 25k = s, and by Fact 3, \u03935 is Monotone log s-Junta. Therefore, w.h.p., algorithm A learns \u03935 and outputs a DNF that is \u03b7 = 1/(16N) close to the target with respect to D5. Since B terminates A after N\n5ck time, we only need to prove that A runs at most N5ck time. The running time of A is\nnc log s = N c log s \u2264 N5ck.\nNo Case: Let S = (S,U,E) be a (k, k\u2032)-Set-Cover instance and opt(S) > k\u2032. By Lemma 4, any DNF, F , of size |F | < 25k\n\u2032/16 satisfies distD5(F,\u03935) \u2265 1/(8|U |). First, we have, for large N\nk\u2032 = 1\n2\n(\nlogN\nlog logN\n)1/k\n> 32k.\nTherefore, any DNF, F, of size |F | < 210k satisfies distD5(F,\u03935) \u2265 1/(8|U |). We have 210k > s. So, B either runs more than N5ck steps and then outputs No in step 3 or outputs a DNF of size more than s and then outputs No in step 4 or outputs a DNF of size at most s with distD5(F,\u03935) \u2265 1/(8|U |) > 1/(8N) > 1/(16N) and outputs No in step 6."
        }
    ],
    "title": "Superpolynomial Lower Bounds for Learning Monotone Classes",
    "year": 2023
}