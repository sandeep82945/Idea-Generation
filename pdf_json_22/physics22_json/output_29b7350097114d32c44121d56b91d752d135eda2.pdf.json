{
    "abstractText": "Quantum neural networks (QNNs), an interdisciplinary field of quantum computing and machine learning, have attracted tremendous research interests due to the specific quantum advantages. Despite lots of efforts developed in computer vision domain, one has not fully explored QNNs for the real-world graph property classification and evaluated them in the quantum device. To bridge the gap, we propose quantum graph convolutional networks (QuanGCN), which learns the local message passing among nodes with the sequence of crossing-gate quantum operations. To mitigate the inherent noises from modern quantum devices, we apply sparse constraint to sparsify the nodes\u2019 connections and relieve the error rate of quantum gates, and use skip connection to augment the quantum outputs with original node features to improve robustness. The experimental results show that our QuanGCN is functionally comparable or even superior than the classical algorithms on several benchmark graph datasets. The comprehensive evaluations in both simulator and real quantum machines demonstrate the applicability of QuanGCN to the future graph analysis problem.",
    "authors": [
        {
            "affiliations": [],
            "name": "Kaixiong Zhou"
        },
        {
            "affiliations": [],
            "name": "Zhenyu Zhang"
        },
        {
            "affiliations": [],
            "name": "Shengyuan Chen"
        },
        {
            "affiliations": [],
            "name": "Tianlong Chen"
        },
        {
            "affiliations": [],
            "name": "Xiao Huang"
        },
        {
            "affiliations": [],
            "name": "Zhangyang Wang"
        },
        {
            "affiliations": [],
            "name": "Xia Hu"
        }
    ],
    "id": "SP:0c7151d3181a085b5906f484740f157042e9f69f",
    "references": [
        {
            "authors": [
                "Yudong Cao",
                "Jonathan Romero",
                "Jonathan P Olson",
                "Matthias Degroote",
                "Peter D Johnson",
                "M\u00e1ria Kieferov\u00e1",
                "Ian D Kivlichan",
                "Tim Menke",
                "Borja Peropadre",
                "Nicolas PD Sawaya"
            ],
            "title": "Quantum chemistry in the age of quantum computing",
            "venue": "Chemical reviews,",
            "year": 2019
        },
        {
            "authors": [
                "Abhinav Kandala",
                "Antonio Mezzacapo",
                "Kristan Temme",
                "Maika Takita",
                "Markus Brink",
                "Jerry M Chow",
                "Jay M Gambetta"
            ],
            "title": "Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets",
            "year": 2017
        },
        {
            "authors": [
                "Edward Farhi",
                "Jeffrey Goldstone",
                "Sam Gutmann"
            ],
            "title": "A quantum approximate optimization algorithm",
            "venue": "arXiv preprint arXiv:1411.4028,",
            "year": 2014
        },
        {
            "authors": [
                "Aram W Harrow",
                "Avinatan Hassidim",
                "Seth Lloyd"
            ],
            "title": "Quantum algorithm for linear systems of equations",
            "venue": "Physical review letters,",
            "year": 2009
        },
        {
            "authors": [
                "Patrick Rebentrost",
                "Masoud Mohseni",
                "Seth Lloyd"
            ],
            "title": "Quantum support vector machine for big data classification",
            "venue": "Physical review letters,",
            "year": 2014
        },
        {
            "authors": [
                "Peter W Shor"
            ],
            "title": "Polynomial-time algorithms for prime factorization and discrete logarithms on a quantum computer",
            "venue": "SIAM review,",
            "year": 1999
        },
        {
            "authors": [
                "Lov K Grover"
            ],
            "title": "A fast quantum mechanical algorithm for database search",
            "venue": "In Proceedings of the twenty-eighth annual ACM symposium on Theory of computing,",
            "year": 1996
        },
        {
            "authors": [
                "Maxwell Henderson",
                "Samriddhi Shakya",
                "Shashindra Pradhan",
                "Tristan Cook"
            ],
            "title": "Quanvolu- tional neural networks: powering image recognition with quantum circuits",
            "venue": "Quantum Ma- chine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad H Amin",
                "Evgeny Andriyash",
                "Jason Rolfe",
                "Bohdan Kulchytskyy",
                "Roger Melko"
            ],
            "title": "Quantum boltzmann machine",
            "venue": "Physical Review X,",
            "year": 2018
        },
        {
            "authors": [
                "Seth Lloyd",
                "Maria Schuld",
                "Aroosa Ijaz",
                "Josh Izaac",
                "Nathan Killoran"
            ],
            "title": "Quantum embed- dings for machine learning",
            "venue": "arXiv preprint arXiv:2001.03622,",
            "year": 2020
        },
        {
            "authors": [
                "Seth Lloyd",
                "Masoud Mohseni",
                "Patrick Rebentrost"
            ],
            "title": "Quantum principal component anal- ysis",
            "venue": "Nature Physics,",
            "year": 2014
        },
        {
            "authors": [
                "Hanrui Wang",
                "Jiaqi Gu",
                "Yongshan Ding",
                "Zirui Li",
                "Frederic T Chong",
                "David Z Pan",
                "Song Han"
            ],
            "title": "Quantumnat: Quantum noise-aware training with noise injection, quantization and normalization",
            "year": 2022
        },
        {
            "authors": [
                "Hanjun Dai",
                "Bo Dai",
                "Le Song"
            ],
            "title": "Discriminative embeddings of latent variable models for structured data",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "David K Duvenaud",
                "Dougal Maclaurin",
                "Jorge Iparraguirre",
                "Rafael Bombarell",
                "Timothy Hirzel",
                "Al\u00e1n Aspuru-Guzik",
                "Ryan P Adams"
            ],
            "title": "Convolutional networks on graphs for learning molecular fingerprints",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Kaixiong Zhou",
                "Qingquan Song",
                "Xiao Huang",
                "Daochen Zha",
                "Na Zou",
                "Xia Hu"
            ],
            "title": "Multi- channel graph neural networks",
            "venue": "In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Will Hamilton",
                "Zhitao Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kaixiong Zhou",
                "Xiao Huang",
                "Daochen Zha",
                "Rui Chen",
                "Li Li",
                "Soo-Hyun Choi",
                "Xia Hu"
            ],
            "title": "Dirichlet energy constrained learning for deep graph neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tianlong Chen",
                "Kaixiong Zhou",
                "Keyu Duan",
                "Wenqing Zheng",
                "Peihao Wang",
                "Xia Hu",
                "Zhangyang Wang"
            ],
            "title": "Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jin Zheng",
                "Qing Gao",
                "Yanxuan L\u00fc"
            ],
            "title": "Quantum graph convolutional neural networks",
            "venue": "40th Chinese Control Conference (CCC),",
            "year": 2021
        },
        {
            "authors": [
                "Guillaume Verdon",
                "Trevor McCourt",
                "Enxhell Luzhnica",
                "Vikash Singh",
                "Stefan Leichenauer",
                "Jack Hidary"
            ],
            "title": "Quantum graph neural networks",
            "venue": "arXiv preprint arXiv:1909.12264,",
            "year": 2019
        },
        {
            "authors": [
                "Stefan Dernbach",
                "Arman Mohseni-Kabir",
                "Siddharth Pal",
                "Don Towsley"
            ],
            "title": "Quantum walk neural networks for graph-structured data",
            "venue": "In International Conference on Complex Net- works and their Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Kerstin Beer",
                "Megha Khosla",
                "Julius K\u00f6hler",
                "Tobias J Osborne"
            ],
            "title": "Quantum machine learn- ing of graph-structured data",
            "venue": "arXiv preprint arXiv:2103.10837,",
            "year": 2021
        },
        {
            "authors": [
                "Kaixiong Zhou",
                "Qingquan Song",
                "Xiao Huang",
                "Xia Hu"
            ],
            "title": "Auto-gnn: Neural architecture search of graph neural networks",
            "venue": "arXiv preprint arXiv:1909.03184,",
            "year": 2019
        },
        {
            "authors": [
                "Mingchen Sun",
                "Kaixiong Zhou",
                "Xin He",
                "Ying Wang",
                "Xin Wang"
            ],
            "title": "Gppt: Graph pre- training and prompt tuning to generalize graph neural networks",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Hongyang Gao",
                "Shuiwang Ji"
            ],
            "title": "Graph u-nets",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zhitao Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "Will Hamilton",
                "Jure Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Kaixiong Zhou",
                "Xiao Huang",
                "Yuening Li",
                "Daochen Zha",
                "Rui Chen",
                "Xia Hu"
            ],
            "title": "Towards deeper graph neural networks with differentiable group normalization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Karsten M Borgwardt",
                "Cheng Soon Ong",
                "Stefan Sch\u00f6nauer",
                "SVN Vishwanathan",
                "Alex J Smola",
                "Hans-Peter Kriegel"
            ],
            "title": "Protein function prediction via graph kernels",
            "venue": "Bioinfor- matics,",
            "year": 2005
        },
        {
            "authors": [
                "Aasa Feragen",
                "Niklas Kasenburg",
                "Jens Petersen",
                "Marleen de Bruijne",
                "Karsten Borgwardt"
            ],
            "title": "Scalable kernels for graphs with continuous attributes",
            "venue": "Advances in neural information pro- cessing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Paul D Dobson",
                "Andrew J Doig"
            ],
            "title": "Distinguishing enzyme structures from non-enzymes without alignments",
            "venue": "Journal of molecular biology,",
            "year": 2003
        },
        {
            "authors": [
                "Felix Wu",
                "Amauri Souza",
                "Tianyi Zhang",
                "Christopher Fifty",
                "Tao Yu",
                "Kilian Weinberger"
            ],
            "title": "Simplifying graph convolutional networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: Quantum machine learning \u00b7 quantum neural networks \u00b7 graph convolutional networks \u00b7 noise mitigation."
        },
        {
            "heading": "1 Introduction",
            "text": "Quantum computing is emerging as a powerful computational paradigm [1,2,3,4,5,6], showing impressive efficiency in tackling traditionally intractable problems, including cryptography [7] and database search [8]. With trainable weights in quantum circuits, quantum neural networks (QNNs), such as quantum convolution [9] and quantum Boltzmann machine [10], have achieved speed-up over classical algorithms in machine learning tasks, including metric learning [11] and principal component analysis [12].\nDespite the successful outcomes in processing structured data (e.g., images [13,13]), QNNs are rarely explored for the graph analysis. Graphs are ubiquitous in the real-world systems, such as biochemical molecules [14,15,16] and social networks [17,18,19,20], where graph convolutional networks (GCN) has become the de-facto-standard analysis\n4 The Conference Quantum Techniques in Machine Learning (QTML), 2022.\nar X\niv :2\n21 1.\n07 37\n9v 1\n[ qu\nan t-\nph ]\n9 N\ntool [21]. By passing the large-volume and high-dimensional messages along edges of the underlying graph, GCN learns the effective node representations to predict graph property. Given the time-costly graph computation, QNNs could provide the potential acceleration for via the superposition and entanglement of quantum circuits.\nHowever, the existing quantum GCN algorithms cannot be directly applied for the real-world graph analysis. They are either developed for the image recognition or quantum physics [22,23], or are only the counterpart simulations in the classical machines [24,25]. Even worse, most of them do not provide the open-source implementations. To tackle these challenges, as shown in Figure 1, we propose quantum graph convolutional networks (QuanGCN) towards the graph classification tasks in real-world applications. Specifically, we leverage differentiable pooling layer to cluster the input graph, where each node is encoded by a quantum bit (qubit). The crossing-qubit gate operations are used to define the local message passing between nodes. QuanGCN delivers the promising classification accuracy in the real quantum system of IBMQ-Quito.\nThe existing quantum devices suffer from non-negligible error rate in the quantum gates, which may lead to the poor generation of QuanGCN. To mitigate the noisy impact, we propose to apply the sparse constraint and skip connection. While the sparse constraint sparsifies the pooled graph and reduces the scales of crossing-qubit gate operations, the skip connection augments the quantum outputs with the classical node features. In summary, we make the following three contributions: (1) The first QuanGCN to address the real-world graph property classification tasks; (2) Two noise mitigation techniques used to improve model\u2019s robustness; (3) The extensive experiments in validating the effectiveness of QuanGCN compared with classical algorithms."
        },
        {
            "heading": "2 Methodology",
            "text": "We represent an undirected graph as G = (A,X), where A \u2208 Rn\u00d7n denotes adjacency matrix, and X \u2208 Rn\u00d7d denotes feature matrix, n is the number of nodes, and the ith row xi at matrix X is the feature vector of node vi. The goal of graph classification task is to predict label of each graph (e.g., biochemical molecule property). Specifically, given a set of graphs {(G1, y1), (G2, y2), \u00b7 \u00b7 \u00b7 } where yg is the corresponding label of graphGg , we learn the representation vector hg to classify the entire graph: yg = f(hg)."
        },
        {
            "heading": "2.1 Preliminary of Graph Convolutional Networks",
            "text": "The node embedding x(l)i \u2208 Rd at the l-th layer of a graph neural networks is generally learned according to [17,26,27]:\nx (l) i = Aggregate({aijx (l\u22121) j W (l) : j \u2208 N (i) \u222a vi}). (1)\nN (i) denotes the set of neighbors adjacent to node vi; aij denotes the edge weight connecting nodes vi and vj , which is given by the (i, j)-th element at matrix A; Aggregate denotes the permutation-invariant function to aggregate the neighborhood embeddings,\nand combine them with the node itself, i.e., vi. The widely-used aggregation modules include sum, add, and mean functions. W (l) \u2208 Rd\u00d7d is trainable projection matrix. Considering a L-layer GCN, READOUT function (e.g., sum or mean) collects all the node embeddings from the final iteration to obtain the graph representation: hg = READOUT(x (L) i |i = 1, \u00b7 \u00b7 \u00b7 , n), which is used for the graph classification task."
        },
        {
            "heading": "2.2 Quantum Graph Convolutional Networks",
            "text": "We propose QuanGCN to realize the graph representation learning in the classicalquantum hybrid machine in Figure 1, which is consisted of three key components.\nPooling Quantum State Encoder. This state is responsible to encode the classical node features into quantum device, where each node is is represented by a qubit. Since the existing quantum machines have a limited number q of qubits, it is intractable to encode the graphs with thousands of nodes. In this work, we leverage a differentiable pooling module to cluster each graph to a fixed q-node coarsened graph. Specifically, let S = Pool(A,X) \u2208 Rn\u00d7q denote the clustering matrix, where Pool is trainable module of MLP or other advanced pooling networks [28,29,30]. Each row at matrix S indicates the probabilities of a node being pooled to the q clusters. We could obtain the adjacency matrix and node features of coarsened graph as follow:\nAp = S TAS \u2208 Rq\u00d7q; Xp = STX \u2208 Rq\u00d7d. (2)\nWe encode the node features of pooled graph with rotation gates. To simplify the analysis and without loss of generality, we assume node feature dimension to be d = 1. The high dimensional feature could be encoded by repeating the process or using complicated quantum gates. To be specific, let |\u03c6\u3009 = |0, ..., 0\u3009 denote the ground statevector in q-qubit quantum system. The computation on a quantum system is implemented by a sequence of parameterized quantum gates on statevector |\u03c6\u3009. Parameterized by node features, we use a sequence of Ry gates to encode the pooled graph as: |\u03c6\u3009 = Ry(xp, [q]) \u00b7 \u00b7 \u00b7Ry(x1, [1]) \u00b7 |\u03c6\u3009. Ry(xi, [i]) denotes the single-qubit quantum gate rotating the i-th qubit along y-axis, at which the rotation angle is characterized by node\nfeature xi (i.e., the i-th row in Xp). In other word, the node features are memorized in the quantum system by the rotation angles of quantum states.\nQuantum Graph Convolution. As defined in Eq. (1), the representation of node vi is computed by incorporating the self-loop information and aggregating the neighborhood embeddings. In the quantum counterpart, we use the quantum gates of U1 and CU1 to model the self-loop and node-pairwise message passing, respectively:\n|\u03c6\u3009 = q\u22c2\nj=1&j 6=i\nCU1(a\u0302ij , [j, i]) \u00b7 U1(a\u0302ii, [i]) \u00b7 |\u03c6\u3009. (3)\nCU1(a\u0302i,j , [j, i]) is a two-qubit quantum gate, where the ordered pair [j, i] means the j-th quantum circuit is a control qubit and the i-th one is target qubit. The unitary operation working on qubit i is parameterized edge weight a\u0302ij , i.e., the (i, j)-th element at matrix Ap. Symbol \u22c2 denotes the sequential gate operations. U1(a\u0302ii, [i]) is a single-qubit quantum gate working on qubit i, which is parameterized by the self-loop weight a\u0302ii. By applying Eq. (3) to all the qubits, we model the quantum message passing between node pairs. A following trainable quantum layer is then used as shown in Figure 1.\nMeasurement. After L layers of quantum graph convolutions, we measure the expectation values with Pauli-Z gate and obtain the classical float value from each qubit. The measurements are concatenated to predict the graph labels as described in Section 2.1."
        },
        {
            "heading": "2.3 Noise Mitigation Techniques",
            "text": "In the real quantum systems, noises often appear due to the undesired gate operation. To mitigate noise in our QuanGCN, we propose to apply the following two techniques.\nPooling sparse constraint. The operation error generally increases with the number of included quantum gates. One of the intuitive solutions to relieve noise is to sparsify the adjacency matrix of pooled graph, where most of the edge weights are enforced to be zero. In this way, the applied quantum gate CU1 or U1 could be treated as an identity operation, which rotates the target qubit with the angle of zero. Specifically, we adopt the entropy constraint to learn the sparse adjacency matrix:Lsparse = \u2212 \u2211 i \u2211 j a\u0302ij log a\u0302ij , which is co-optimized with the graph classification loss.\nSkip connection. We mitigate the quantum noise from the architectural perspective by introducing the skip connection. In Figure 1, we concatenate the quantum measurements with the input classical features, which is not sensitive to the quantum noise."
        },
        {
            "heading": "3 Experimental Setting and Results",
            "text": "Dataset. We adopt four graph datasets, including two bioinformatic datasets of ENZYMES, PROREINS [31,32], and two social network datasets of MUTAG and IMDBBINARY [33]. They contain 600, 1113, 188, and 1000 graphs, respectively.\nImplementations. We adopt the classical baselines of MLP, simplified graph convolutions (SGC) [34], GCN, and the graph pooling method of Diffpool [29]. SGC uses MLP to learn the node presentations based on the preprocessed node features. For QNN algorithms, besides QuanGCN, we include quantum MLP (QuanMLP) and quantum SGC (QuanSGC), where their MLP layers are replaced by the quantum layer of U3CU3. The numbers of qubits and graph convolutional layers are set to 4 and 2, respectively.\nComparison of classical and quantum neural networks. We compare the classification accuracies in Table 1, where the mean results and standard variances are reported with 10 random runs. It is observed that our QuanGCN obtains the comparable or even superior results than the cassical algorithms, while generally outperforming QuanMLP and QuanSGC on the benchmark datasets. These results validate the effectiveness of quantum graph convolution in dealing with the graph data. By modeling the time-expensive message passsing in the efficient quantum device, QuanGCN provides the potential speed-up over the classical algorithms. Similar to other QNNs, QuanGCN is accompanied with higher variance due to the indeterminate quantum operations.\nTesting in quantum simulator and real machine. In Table 2, we deploy the above well-trained QNNs in Qiskit simulator and quantum computer of IBMQ-Quito to evaluate their inference performances. Since the inference in real quantum computer has to pay plenty of queuing time, we test QNNs only once in the real device. Comparing with the inference performances in GPUs (i.e., in Table 1), QNNs generally have lower accuracies due to the high error rates existing inherently in the quantum devices. Notably, QuanGCN instead obtains the better performances. One of the possible reasons is due to the graph pooling, which highly reduces the crossing-qubit gate usages and the resultant noises. The quantum graph convolution over the pooled graph provides the more informative encoding for the underlying graph structure.\nNoise mitigation results. To address the inherent noisy impacts, we apply the skip connection to all the QNNs, and use the sparse constraint to regularize the graph pooling in QuanGCN. We compare them with one popular noise cancellation baseline [13], which randomly inserts quantum gates during model training to improve robustness. The comparison results in Tabel 3 show that the technique of skip connection is consistently effective to mitigate noise in all models. In QuanGCN, the combination of skip connection and sparse constraint obtains the best noise mitigation performances."
        },
        {
            "heading": "4 Conclusion",
            "text": "In this work, we propose and implement QuanGCN towards addressing the graph property classification tasks in the real-world applications. To mitigate the noisy impact in the real quantum machine, we propose techniques of skip connection and sparse constraint to improve model\u2019s robustness. The extensive experiments on the benchmark graph datasets demonstrate the potential advantage and applicability of quantum neural networks to the graph analysis, which is a new probem introduced to quantum domain."
        }
    ],
    "title": "QuanGCN: Noise-Adaptive Training for Robust Quantum Graph Convolutional Networks",
    "year": 2022
}