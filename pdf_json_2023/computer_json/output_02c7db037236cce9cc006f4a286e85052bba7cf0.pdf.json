{
    "abstractText": "In this paper, we investigate a distributed interval optimization problemwhich is modelled with optimizing a sum of convex interval objective functions subject to global convex constraints, corresponding to agents over the time-varying network. We first reformulate the distributed interval optimization problem as a distributed constrained optimization problem by scalarization of the distributed interval optimization problem. Then, we show that the Pareto optimal solutions of the reformulated problem. The optimal solutions of the distributed constrained optimization problem is equivalent to Pareto optimal solutions to the distributed interval optimization problem, and we design a distributed subgradient-free algorithm for the distributed constrained optimization problems through constructing random differences of reformulated optimal objective functions. Moreover, we prove that for the proposed algorithm, a Pareto optimal solution can be achieved almost surely over the time-varying network. Finally, we give a numerical example to illustrate the effectiveness of the proposed algorithm. INDEX TERMS distributed interval optimization, time-varying network, Pareto optimal solution, subgradient-free algorithm.",
    "authors": [
        {
            "affiliations": [],
            "name": "YINGHUI WANG"
        },
        {
            "affiliations": [],
            "name": "JIUWEI WANG"
        },
        {
            "affiliations": [],
            "name": "XIAOBO SONG"
        },
        {
            "affiliations": [],
            "name": "YANPENG HU"
        }
    ],
    "id": "SP:f65ecccf10e3c249ad3042ba3961f0a6023ecbd6",
    "references": [
        {
            "authors": [
                "A. Nedic",
                "A. Ozdaglar"
            ],
            "title": "Distributed subgradient methods for multiagent optimization",
            "venue": "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48\u201361, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "P. Yi",
                "Y. Hong",
                "F. Liu"
            ],
            "title": "Distributed gradient algorithm for constrained optimization with application to load sharing in power systems",
            "venue": "Systems & Control Letters, vol. 83, pp. 45\u201352, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Nedic",
                "A. Ozdaglar",
                "P.A. Parrilo"
            ],
            "title": "Constrained consensus and optimization in multi-agent networks",
            "venue": "IEEE Transactions on Automatic Control, vol. 55, no. 4, pp. 922\u2013938, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "S.S. Ram",
                "A. Nedi\u0107",
                "V.V. Veeravalli"
            ],
            "title": "Distributed stochastic subgradient projection algorithms for convex optimization",
            "venue": "Journal of optimization theory and applications, vol. 147, no. 3, pp. 516\u2013545, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "Y. Zhang",
                "Y. Lou",
                "Y. Hong",
                "L. Xie"
            ],
            "title": "Distributed projection-based algorithms for source localization in wireless sensor networks",
            "venue": "IEEE Transactions on Wireless Communications, vol. 14, no. 6, pp. 3131\u20133142, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "A. Cherukuri",
                "J. Cort\u00e9s"
            ],
            "title": "Distributed generator coordination for initialization and anytime optimization in economic dispatch",
            "venue": "IEEE Transactions on Control of Network Systems, vol. 2, no. 3, pp. 226\u2013237, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "G. Shi",
                "B.D. Anderson",
                "U. Helmke"
            ],
            "title": "Network flows that solve linear equations",
            "venue": "IEEE Transactions on Automatic Control, vol. 62, no. 6, pp. 2659\u20132674, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Zeng",
                "S. Liang",
                "Y. Hong",
                "J. Chen"
            ],
            "title": "Distributed computation of linear matrix equations: An optimization perspective",
            "venue": "arXiv preprint arXiv:1708.01833, 2017.",
            "year": 1833
        },
        {
            "authors": [
                "Y. Wang",
                "P. Lin",
                "Y. Hong"
            ],
            "title": "Distributed regression estimation with incomplete data in multi-agent networks",
            "venue": "Science China Information Sciences, vol. 61, no. 9, p. 092202, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X. Zeng",
                "Y. Peng",
                "Y. Hong"
            ],
            "title": "Distributed algorithm for robust resource allocation with polyhedral uncertain allocation parameters",
            "venue": "Journal of Systems Science and Complexity, vol. 31, no. 1, pp. 103\u2013119, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "V. Kekatos",
                "G.B. Giannakis"
            ],
            "title": "Distributed robust power system state estimation",
            "venue": "IEEETransactions on Power Systems, vol. 28, no. 2, pp. 1617\u2013 1626, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "S. Sra",
                "S. Nowozin",
                "S.J. Wright"
            ],
            "title": "Optimization for machine learning",
            "year": 2012
        },
        {
            "authors": [
                "A. Neumaier"
            ],
            "title": "Interval methods for systems of equations",
            "venue": "Cambridge university press,",
            "year": 1990
        },
        {
            "authors": [
                "J. Rohn"
            ],
            "title": "Positive definiteness and stability of interval matrices",
            "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 15, no. 1, pp. 175\u2013184, 1994.",
            "year": 1994
        },
        {
            "authors": [
                "V. Levin"
            ],
            "title": "Nonlinear optimization under interval uncertainty,\u2019\u2019Cybernetics and Systems",
            "venue": "Analysis, vol. 35,",
            "year": 1999
        },
        {
            "authors": [
                "B.Q. Hu",
                "S. Wang"
            ],
            "title": "A novel approach in uncertain programming part i: New arithmetic and order relation for interval numbers",
            "venue": "Journal of Industrial & Management Optimization, vol. 2, no. 4, pp. 351\u2013371, 2006.",
            "year": 2006
        },
        {
            "authors": [
                "L. Wu",
                "M. Shahidehpour",
                "Z. Li"
            ],
            "title": "Comparison of scenario-based and interval optimization approaches to stochastic scuc",
            "venue": "IEEE Transactions on Power Systems, vol. 27, no. 2, pp. 913\u2013921, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "I. Hisao",
                "T. Hideo"
            ],
            "title": "Multiobjective programming in optimization of the interval objective function",
            "venue": "European Journal of Operational Research, vol. 48, no. 2, pp. 219\u2013225, 1990.",
            "year": 1990
        },
        {
            "authors": [
                "H.-C. Wu"
            ],
            "title": "On interval-valued nonlinear programming problems",
            "venue": "Journal of Mathematical Analysis and Applications, vol. 338, no. 1, pp. 299\u2013 316, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "S.-T. Liu",
                "R.-T. Wang"
            ],
            "title": "A numerical solution method to interval quadratic programming",
            "venue": "Applied mathematics and computation, vol. 189, no. 2, pp. 1274\u20131281, 2007.",
            "year": 2007
        },
        {
            "authors": [
                "C. Jiang",
                "X. Han",
                "G. Liu",
                "G. Liu"
            ],
            "title": "A nonlinear interval number programming method for uncertain optimization problems",
            "venue": "European Journal of Operational Research, vol. 188, no. 1, pp. 1\u201313, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "A. Jayswal",
                "I. Stancu-Minasian",
                "I. Ahmad"
            ],
            "title": "On sufficiency and duality for a class of interval-valued programming problems",
            "venue": "Applied Mathematics and Computation, vol. 218, no. 8, pp. 4119\u20134127, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M. Hlad\u0131k"
            ],
            "title": "Interval linear programming: A survey",
            "venue": "Linear programming-new frontiers in theory and applications, pp. 85\u2013120, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "A. Bellet",
                "Y. Liang",
                "A.B. Garakani",
                "M.-F. Balcan",
                "F. Sha"
            ],
            "title": "A distributed frank-wolfe algorithm for communication-efficient sparse learning",
            "venue": "Proceedings of the 2015 SIAM International Conference on Data Mining. SIAM, 2015, pp. 478\u2013486.",
            "year": 2015
        },
        {
            "authors": [
                "R. Tempo",
                "G. Calafiore",
                "F. Dabbene"
            ],
            "title": "Randomized algorithms for analysis and control of uncertain systems: with applications",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "H. Ishii",
                "R. Tempo"
            ],
            "title": "Distributed randomized algorithms for the pagerank computation",
            "venue": "IEEE Transactions on Automatic Control, vol. 55, no. 9, pp. 1987\u20132002, 2010.",
            "year": 1987
        },
        {
            "authors": [
                "H.-F. Chen",
                "T.E. Duncan",
                "B. Pasik-Duncan"
            ],
            "title": "A kiefer-wolfowitz algorithm with randomized differences",
            "venue": "IEEE Transactions on Automatic Control, vol. 44, no. 3, pp. 442\u2013453, 1999.",
            "year": 1999
        },
        {
            "authors": [
                "A.R. Conn",
                "K. Scheinberg",
                "L.N. Vicente"
            ],
            "title": "Introduction to derivativefree optimization",
            "venue": "Siam, 2009,",
            "year": 2023
        },
        {
            "authors": [
                "J.C. Duchi",
                "M.I. Jordan",
                "M.J. Wainwright",
                "A. Wibisono"
            ],
            "title": "Optimal rates for zero-order convex optimization: The power of two function evaluations",
            "venue": "IEEE Transactions on Information Theory, vol. 61, no. 5, pp. 2788\u20132806, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "X. Gao",
                "B. Jiang",
                "S. Zhang"
            ],
            "title": "On the information-adaptive variants of the admm: An iteration complexity perspective",
            "venue": "Journal of Scientific Computing, no. 4, pp. 1\u201337, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Nesterov",
                "V. Spokoiny"
            ],
            "title": "Random gradient-free minimization of convex functions",
            "venue": "Universit\u00e9 catholique de Louvain, Center for Operations Research and Econometrics (CORE), Tech. Rep., 2011.",
            "year": 2011
        },
        {
            "authors": [
                "K.S. Anit",
                "J. Dusan",
                "B. Dragana",
                "K. Soummya"
            ],
            "title": "Distributed zeroth order optimization over random networks: A kiefer-wolfowitz stochastic approximation approache",
            "venue": "arXiv:1803.07836, 2018.",
            "year": 1803
        },
        {
            "authors": [
                "D. Hajinezhad",
                "M. Hong",
                "A. Garcia"
            ],
            "title": "Zeroth order nonconvex multiagent optimization over networks",
            "venue": "arXiv:1710.09997, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Yuan",
                "D.W. Ho"
            ],
            "title": "Randomized gradient-free method for multiagent optimization over time-varying networks",
            "venue": "IEEE transactions on neural networks and learning systems, vol. 26, no. 6, pp. 1342\u20131347, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "D. Yuan",
                "D.W. Ho",
                "S. Xu"
            ],
            "title": "Zeroth-order method for distributed optimization with approximate projections.\u2019",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2015
        },
        {
            "authors": [
                "H.-F. Chen"
            ],
            "title": "Stochastic approximation and its applications",
            "venue": "Springer Science & Business Media,",
            "year": 2006
        },
        {
            "authors": [
                "E. Hazan"
            ],
            "title": "Introduction to online convex optimization",
            "venue": "Foundations and Trends\u00ae in Optimization, vol. 2, no. 3-4, pp. 157\u2013325, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "F.H. Clarke",
                "R.J. Stern",
                "Y.S. Ledyaev",
                "R.R. Wolenski"
            ],
            "title": "Nonsmooth analysis and control theory",
            "venue": "Graduate Texts in Mathematics, vol. 178, no. 7, pp. 137\u2013151, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "B.T.B.T.P. Polyak"
            ],
            "title": "Introduction to optimization",
            "year": 1987
        },
        {
            "authors": [
                "J.-P. Aubin",
                "A. Cellina"
            ],
            "title": "Differential inclusions: set-valued maps and viability theory",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "A.K. Bhurjee",
                "G. Panda"
            ],
            "title": "Efficient solution of interval optimization problem",
            "venue": "Mathematical Methods of Operations Research, vol. 76, no. 3, pp. 273\u2013288, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "T. Maeda"
            ],
            "title": "On optimization problems with set-valued objective maps: existence and optimality",
            "venue": "Journal of Optimization Theory and Applications, vol. 153, no. 2, pp. 263\u2013279, 2012.",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS distributed interval optimization, time-varying network, Pareto optimal solution, subgradient-free algorithm.\nI. INTRODUCTION\nRECENTLY, distributed optimization and control in anetwork environment have been increasingly attracted much attention, which are much effective in many large-scale problems than the centralized designs, in the casewhen agents only have the local information and exchange information over the network with their neighbours. In fact, distributed algorithms for various (constrained) optimization problems have been widely studied with potential applications to sensor networks, smart grids, and equation solutions referring to [1]\u2013 [8]. Note that the connectivity is a key issue in the distributed design. Although fixed topologies are still required for distributed optimization designs in some situation, time-varying jointly connected networks have been considered in many algorithms such as [1], [3], [7], [9].\nHowever, objective functions or constraints of some practical optimization problems may not be accurately or explicitly described. For example, some conditions in power\nsystems may be time-varying or uncertain, and the data in data mining can be inaccurate (see [10], [11], and [12]). Motivated by such background, the interval optimization is studied in [13]\u2013[15], which provides a framework to capture the uncertainty in optimization. In fact, interval optimization problems (IOP), first proposed by [13] and further studied in [14], [15] and references therein, have been widely studied in many different research areas including economic systems [16] and smart grids [17]. In the interval optimization problem setup, the objective functions are interval-valued, that is they are described by intervals rather than real numbers, through interval-valued maps. The well-defined partial orderings and convexity of interval-valued maps [16], [18], [19], provide existance guarantees of solutions of maximization and minimization of interval optimization problems. Up to now, the literature (referring to [20]\u2013[23]) has provided various programming methods, particularly based on Wolfe\u2019s method or Lamke\u2019s algorithm (Lingo software provides us with different\nVOLUME 11, 2023 1\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nalgorithm boxs to solve linear programming problems), to deal with centralized interval optimization problems.\nWith this background, it is nature for us to consider the construction of effective algorithms for distributed interval optimization problems over multi-agent networks. However, the problems are still under investigation. One reason may be that it is so easy to make the Wolfe or Lamke\u2019s ideas distributed and very few papers with related theoretical results have been published on the topic [24]. Another reason is that the partial order resulting from the interval makes the method based on gradients of objective functions become hard, especially when we only have local information in a distributed design.\nRandomization and stochastic methods have proved to be powerful tools in systems and control. For example, for the control of uncertain systems, they are useful ingredients compared to the classical robustness methods [25]. For the study in network dynamics, they are quite natural and have close relationship with the real systems and can improve the overall performance of the system when, for example, distributed algorithms being developed [26].\nThe motivation of this paper is to propose a distributed algorithm for interval optimization problems, thanks to the recent results on distributed subgradient-free algorithms, which overcomes the difficulty of obtaining subgradient information of local interval-valued functions. Zeroth-order/subgradientfree algorithms, have been widely investigated in [27]\u2013[31] and references listed therein, due to its applications in the cases where obtaining gradient/subgradient information is, sometimes, computationally costly and even impracticable. To achieve this goal, we propose a distributed subgradientfree stochastic algorithm for a class of interval optimization problems from a numerical programming perspective. The contributions of this paper are summarized as follows: (a) Following the pace of the rapid development of data\nscience and engineering systems, we extend the centralized interval optimization problem [14], [15] to a distributed one. Through well-defined partial orderings and convexity [16], [18], [19] of the distributed interval optimization problem, we scale the distributed interval optimization problem as a solvable distributed optimization problem, subject to global convex constraints. In this reformulation, the optimal solutions of the distributed constrained optimization problem are equivalent to Pareto optimal solutions to the distributed interval optimization problem. Since the well-known methods such as Wolfe\u2019s and Lamke\u2019s methods cannot be easily extended to distributed versions, we employ zeroth order or subgradient-free ideas in the distributed design. (b) We design a new distributed subgradient-free algorithm with random differences for solving the reformulated distributed constrained non-smooth optimization problem, because the subgradient of the interval optimization problem is hard to be obtained. The algorithm adopts random differences to approximate subgradients of local reformulated objective functions, which is different from\nmany existing distributed gradient/subgradient-free algorithms c.f., [32]\u2013[35] though it is consistent with them when the local objective function is smooth. (c) We give theoretical and numerical analysis to the proposed algorithm, which is related to the distributed stochastic optimization algorithm. We first prove the consensus of estimates and achievement of the global minimization with probability one of the proposed algorithm, and further establish the mean-square convergence rate of O( 1\u221a\nk ). The convergence results matches\nthe best of the first-order stochastic algorithms [4], [36], [37] with diminishing step-size.\nThe rest of the paper is organized as follows. Preliminaries related to the analysis and design of distributed interval optimization is given in Section II. Then the distributed interval optimization problem is formulated and the corresponding distributed algorithm is introduced in Section III, while the proposed algorithm is analyzed in Section IV. Following that, a numerical example is given in Section V. Finally, some concluding remarks are addressed in Section VI. Notations. Let Rp be the p-dimensional Euclidean space. Denote Rp+ as its non-negative orthants. \u2016 \u00b7 \u2016 denotes the Euclidean norm. Denote the sets of all non-empty compact intervals ofR by C(R).\nII. MATHEMATICAL PRELIMINARIES In this section, we introduce mathematical preliminaries about convex analysis [3], [38], [39], probability theory [40], [41] and interval optimization, respectively.\nA. CONVEX ANALYSIS Here are some concepts on convex analysis [38], [39].\nDefinition 1. [38][Sub-gradient]Let f (x) : Rp \u2192 R be a non-smooth convex function. Vector-valued functionOf (x) \u2208 \u2202f (x) \u2282 Rp is called the subgradient of f (x) if for any x, y \u2208 dom(f ), the following inequality holds:\nf (x)\u2212 f (y)\u2212 \u2329 Of (y), x \u2212 y \u232a > 0.\nLemma 1. [39](Lebourg\u2019s Mean Value Theorem) Let x, y \u2208 X. Suppose f (x) : Rm \u2192 R is Lipschitz on an open set containing line segment [x, y]. Then, there exists a point u \u2208 (x, y) such that\nf (x)\u2212 f (y) \u2208 \u3008\u2202f (u), x \u2212 y\u3009.\nThen we summarize some inequalities on Euclidean norm [3], [39] to be used in this paper.\nLemma 2. ( [4]) Let x1, x2, . . . , xn be vectors inRp. Then n\u2211\ni=1\n\u2225\u2225\u2225xi \u2212 1n n\u2211\ni=1\nxj \u2225\u2225\u22252 6 n\u2211\ni=1 \u2225\u2225\u2225xi \u2212 x\u2225\u2225\u22252, \u2200x \u2208 Rp. Denote the projection of x onto set X by PX (x), i.e.,\nPX (x) = arg miny\u2208X \u2225\u2225x \u2212 y\u2225\u2225, where X is a closed bounded convex set inRp. The following results are on the projection operators in Euclidean norm:\n2 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nLemma 3. [3], [38]Let X be a a closed convex set in Rp. Then for any x \u2208 Rp, it holds that (a) \u2329 x \u2212 PX (x), y\u2212 PX (x) \u232a 6 0, for all y \u2208 X\n(b) \u2225\u2225PX (x)\u2212 PX (y)\u2016 6 \u2225\u2225x \u2212 y\u2225\u2225, for all x, y \u2208 Rm.\n(c) \u2329 x \u2212 y,PX (y)\u2212 PX (x) \u232a 6 \u2212 \u2225\u2225PX (x)\u2212 PX (y)\u2225\u22252, for all y \u2208 Rm.\n(d) \u2225\u2225x \u2212 PX (x)\u2225\u22252 + \u2225\u2225y \u2212 PX (x)\u2225\u22252 6 \u2225\u2225x \u2212 y\u2225\u22252, for any y \u2208 X.\nB. PROBABILITY THEORY Denote (\u2126,F ,P) as the probability space, where \u2126 is the whole event space,F is the \u03c3-algebra on \u2126, and P is the probability measure on (\u2126,F). Then, definitions of convergence in (\u2126,F ,P) and convergence of super-martingales theorem is given.\nDefinition 2. [40][Convergence in (\u2126,F ,P)] (a) x1, x2, . . . , xk . . . is a sequence of random variables (r.\nv.) in (\u2126,F ,P). If P(xk \u2192 x) = 1, then xk converges x almost surely (a. s.). (b) x1, x2, . . . , xk . . . is a sequence of random variables (r. v.) in (\u2126,F ,P). If E\u2016xk \u2212 x\u2016p \u2192 0, then xk converges to x in Lp.\nLemma 4. ( [41]) In (\u2126,F ,P) , denote {F(k)}k\u22651 as a sequence of increasing sub-\u03c3-algebras on F . {h(k)}k\u22651, {v(k)}k\u22651 and {w(k)}k\u22651 are variable sequences in R such that for each k, h(k), v(k) and w(k) are F(k)measurable. Both {v(k)}k\u22651 and {w(k)}k\u22651 are nonnegative and \u2211\u221e k=1 w(k) <\u221e. Moreover, {h(k)}k\u22651 is bounded from below uniformly. If\nE[h(k + 1)|F(k)] 6 (1 + \u03b7(k))h(k)\u2212 v(k) + w(k), \u2200k > 1 holds almost surely, where \u03b7(k) > 0 are constants with\u2211\u221e k=1 \u03b7(k) < \u221e, then {h(k)}k\u22651 converges almost surely\nwith \u2211\u221e\nk=1 v(k) <\u221e.\nC. INTERVAL OPTIMIZATION 1) Orderings on C(R) and some properties of interval-valued map G DefineA = [aL , aR],B = [bL , bR] are two non-empty compact intervals in P(Rq). Now, we introduce some orderings on P(R).\nDefinition 3. [16], [18] For any A,B \u2208 P(R). denote: (a) A 5L B iff aL 6 bL; (b) A 5U B iff aR 6 bR; (c) A 5 B iff A 5L B and A 5U B.\nDefinition 4. [16], [18] For any A,B \u2208 P(R), denote: (a) A <L B iff aL < bL; (b) A <U B iff aR < bR; (c) A < B iff A <L B and A <U B; (d) A \u2264 B iff A <L B and A 5U B, or A 5L B and A <U B.\nLet G : Rp \u21d2 R be any interval-valued map, where G(\u00b7) is an interval with respect to x. We give the definition\nof Lipschitz continuity and convexity of the interval-valued map G : Rp \u21d2 R as follows:\nDefinition 5. [42] (Lipschitz Continuity) Let G : Rp \u21d2 R be any interval-valued map. G is locally Lipschitz at x if there exist K > 0 and a neighborhood W of x such that\nG(x1) \u2286 G(x2) + K\u2016x1 \u2212 x2\u2016, \u2200x1, x2 \u2208 W .\nWe say that G is locally Lipschitz at x if there exist a neighbourhood W of x and a constant K > 0, such that\nG(x1) \u2286 B ( G(x2),K\u2016x1 \u2212 x2\u2016 ) .\nWe denote by\nB(A, %) = {y|d(y,A) 6 %},\nthe ball of radius % around subset A, where y is chosen from a metric space.\nDefinition 6. [19](Convexity) Let G : Rp \u21d2 Rq be any interval-valued map. G is convex (lower-convex or upper convex) on \u2126 if, \u2200x1, x2 \u2208 \u2126, \u2200\u03b1 \u2208 [0, 1],\nG ( \u03b1x1 + (1\u2212 \u03b1)x2 ) 5 (5L or 5 U)\u03b1G(x1) + (1\u2212 \u03b1)G(x2).\nRemark 1. Suppose that G is compact-valued and convex, G(\u00b7) = [L(\u00b7),R(\u00b7)]. Then according to Definitions 3 and 4, L(\u00b7), R(\u00b7) : Rp \u2192 R are convex functions with respect to x \u2208 Rp. Namely, for any x1, x2 \u2208 Rp and any t \u2208 [0, 1], the following inequalities hold:\nL ( tx1 + (1\u2212 t)x2 ) 6 tL(x1) + (1\u2212 t)L(x2),\nR ( tx1 + (1\u2212 t)x2 ) 6 tR(x1) + (1\u2212 t)R(x2).\n2) Interval optimization problems with interval-valued Map Let G : Rp \u21d2 R be any interval-valued map. Now we consider the following interval optimization problem:\n(IOP) min x G(x) s. t. x \u2208 \u2126\nwhere G(x) = [L(x),R(x)] is any non-empty compact interval inR.\nExample 1. Motivated by the article [43], we give an example of interval valued function. Consider a function G : Rp \u21d2 R. Without loss of generality, consider c as an order set, which is influenced by orders maintained on the presence of components of G(x). If G(x1, x2) = c1x21 + c2x1ec3x2 , then c = [c1, c2, c3]>. Suppose c1, c2, c3 lies in intervals C1, C2 and C3, respectively. Ci = [ciL , c i R], ci(ti) = (1 \u2212 ti)ciL + ticiR, ti \u2208 [0, 1], i = 1, 2, 3. Thus C(t) = [c1(t), c2(t), c3(t)]> \u2208 C3v , where Ckv stands for kdimensinal column vecter whose elements are vectors. For the given interval vector C3v , {Gc(t)(x1, x2) = c1(t)x21 + c2(t)x1ec3(t)x2 : R2 \u21d2 R, c(t) \u2208 C3v } is an interval, where L(x) = mint Gc(t)(x1, x2) and R(x) = maxt Gc(t)(x1, x2).\nIn light of definitions of L(x) and R(x) of the example, we see that we can not get the explicit expressions of L(x) and\nVOLUME 11, 2023 3\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nR(x), and this IOP problems can be solved through set-valued optimization rather than vector valued optimization.\nBased on quasi orderings of compact intervals in C(R) given in Definitions 3 and 4, we define the Parato optimal solution to (IOP):\nDefinition 7. [44] A point x\u2217 \u2208 \u2126 is said to be a Pareto optimal solution (PO) to (IOP) iff it holds that G(x\u0304) 5 G(x\u2217) for some x\u0304 \u2208 \u2126 implies G(x\u2217) 5 G(x\u0304).\nRemark 2. There is no solution to interval optimization problem given in fig. 1 . However, [x1, x2] are Pareto optimal solutions to this given problem.\n(a) For y 6 x1, we have R(y) > R(x1) and L(y) > L(x1), which means that G(y) = G(x1). (b) For y > x2, we have R(y) > R(x2) and L(y) > L(x2), which means that G(y) = G(x2). (c) For x1 6 y 6 x2, we have R(y) 6 R(x1), L(y) > L(x1), R(y) > R(x2) and L(y) 6 L(x2) according to Definition 7. Therefore, [x1, x2] are Pareto optimal solutions to this given problem.\nAssociated with (IOP), consider the following interval optimazation problem with its scalarization:\n(SIOP) min x \u03bbL(x) + (1\u2212 \u03bb)R(x)\ns. t. x \u2208 \u2126\nwhere \u03bb \u2208 [0, 1]. The following lemma holds according to [44]. We gave its proof here just for self-reminder.\nLemma 5. We assume that G is compact-valued and convex with respect to x:\n(a) If there exists a real number \u03bb \u2208 (0, 1) such that x\u2217 \u2208 \u2126 is an optimal solution to (SIOP), then x\u2217 \u2208 \u2126 is a Pareto optimization to (IOP). (b) A point x\u2217 \u2208 \u2126 is a Pareto optimization to (IOP), then there exists a real number \u03bb \u2208 [0, 1] such that x\u2217 \u2208 \u2126 is an optimal solution to (SIOP).\nProof. (a) Given a real number \u03bb \u2208 (0, 1) and let x\u2217 \u2208 \u2126 be an optimal solution to (SIOP). Suppose that there exists a point x\u0304 \u2208 \u2126, such that G(x\u0304) 5 G(x\u2217), which implies L(x\u0304) 6 L(x\u2217) and R(x\u0304) 6 R(x\u2217). Therefore,\n\u03bbL(x\u0304) + (1\u2212 \u03bb)R(x\u0304) 6 \u03bbL(x\u2217) + (1\u2212 \u03bb)R(x\u2217)\nwhich contradicts that x\u2217 is an optimal solution to (SIOP). (b) Let x\u2217 \u2208 \u2126 be a Pareto optimal solution to (IOP). Since G is compact-valued and convex with respect to x, according to Remark 1, L(x) and U(x) are convex functions. Following Definition 7, there exists a vector \u03bb = [a, b]> 6= 0, a > 0, b > 0, such that\n\u03bb> [ L(x\u2217) R(x\u2217) ] 6 \u03bb> [ L(x) R(x) ] holds for all x \u2208 \u2126. Define \u03bb\u0304 = [ aa+b , b a+b ], we have\n\u03bb\u0304> [ L(x\u2217) R(x\u2217) ] 6 \u03bb\u0304> [ L(x) R(x) ] ,\nwhich proves that there exists a real number \u03bb \u2208 [0, 1] such that x\u2217 \u2208 \u2126 is an optimal solution to (SIOP).\nIII. DISTRIBUTED INTERVAL OPTIMIZATION Consider the following distributed interval optimization problem over an n-agent network:\n(DIOP) min x\nG(x) = n\u2211\ni=1\nGi(xi)\ns. t. xi = xj, xi \u2208 X (1) where x = [ x>1 , x > 2 , . . . , x > n ]> \u2208 Rnp, xi \u2208 Rp, and Gi : Rp \u21d2 R is a compact and convex interval-valued function. In this setting, the state of an agent i is the estimate of solution to problem (DIOP). Each agent i knows local functions Gi and global constraint X . We first make the following assumption on the local functions and constraints for the distributed interval optimal problem (DIOP):\nAssumption 1. (a) Gi(x) is a convex, compact, Lipschitz continuous interval-valued function.\n(b) X is a non-empty, compact, convex constraint set inRp.\nConsider solving (DIOP) over a time-varying multigenerator network. Define a directed network G(k) =( N , E(k),W (k) ) as the communication topology among generators , where N = {1, 2, ...n} is the agent set, the edge set E(k) \u2282 N \u00d7 N represents information communication at time k and W (k) = [ wij(k) ] ij represents adjacency matrix at time k . Each agent interacts with its neighbors in G(k) = (N , E(k),W (k)) at time k . We make the following assumption about communication topology G(k) =( N , E(k),W (k) ) .\nAssumption 2. The graph G(k) = ( N , E(k),W (k) ) satisfies:\n4 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n(a) There exists a constant \u03b7 with 0 < \u03b7 < 1 such that, \u2200k > 0 and \u2200i, j, wii(k) > \u03b7; wij(k) > \u03b7 if (j, i) \u2208 E(k). (b) W (k) is doubly stochastic, i. e. \u2211m\ni=1 wij(k) = 1 and\u2211m j=1 wij(k) = 1. (c) There is an integer \u03ba > 1 such that \u2200k > 0 and \u2200(j, i) \u2208 N \u00d7N ,\n(j, i) \u2208 E(k) \u222a E(k + 1) \u222a \u00b7 \u00b7 \u00b7 \u222a E(k + \u03ba\u2212 1).\nAssumption 2 reveals that each agent i can collect information from all its neighbors periodically. It is also a widely used connectivity condition for distributed time-varying network designs(see [1], [3]).\nDefine the function f : Rnp \u00d7 Rn \u2192 R and fi : Rp \u00d7 [0, 1]\u2192 R as\nf ( x,\u03bb ) , n\u2211 i=1 fi ( xi, \u03bbi ) (2)\nfi ( xi, \u03bbi ) , \u03bbiLi(x) + (1\u2212 \u03bbi)Ri(x) (3)\nwhere i = 1, 2, . . . , n, x = [ x>1 , x > 2 , . . . , x > n ]> \u2208 Rnq and \u03bb = [ \u03bb1, \u03bb2, . . . , \u03bbn\n]> \u2208 Rn. Remark 3. Note that both L(x) and R(x) are separable, that is\nL(x) = n\u2211\ni=1\nLi(xi),\nand\nR(x) = n\u2211\ni=1\nRi(xi).\nWe can transform the distributed interval optimization problem into a classic scalar valued distributed optimization problem. Let \u03bb = \u03bb01n with \u03bb0 \u2208 (0, 1). To solve problem (1), we solve the following distributed optimization problem:\nmin x\nf ( x,\u03bb ) = n\u2211 i=1 fi ( xi, \u03bbi ) s. t. xi = xj, xi \u2208 X\n\u03bbi = \u03bbj (4)\nwhere agent i knows the information of fi, xi, \u03bbi \u2208 (0, 1) and its neighborhood information.\nRemark 4. Reformulated problem (4) degenerates to a standard distributed constrainted optimization problem [4] when each agent i choose a common parameter \u03bbi = 0 or \u03bbi = 1.\nRemark 5. According to Definitions 5-6 and Assumption 1, we have: (a) Each fi ( x, \u03bb ) is convex with respect to x, i. e. for any\nx1, x2: fi ( \u03b1x1 + (1\u2212 \u03b1)x2, \u03bb ) 6 \u03b1fi ( x1, \u03bb ) + (1\u2212 \u03b1)fi ( x2, \u03bb ) ,\nwhere \u03b1 \u2208 [0, 1]. (b) Each fi ( x, \u03bb ) is convex with respect to \u03bb.\n(c) Each fi ( x, \u03bb ) is Lipschitz continuous with respect to x, i.\ne. for all x1, x2 and \u03bb:\u2225\u2225fi(x1, \u03bb)\u2212 fi(x2, \u03bb)\u2225\u2225 6 L\u2016x1 \u2212 x2\u2016. (d) Each fi ( x, \u03bb ) is Lipschitz continuous with respect to \u03bb i.\ne. for all \u03bb1, \u03bb2 and x:\u2225\u2225fi(x, \u03bb1)\u2212 fi(x, \u03bb2)\u2225\u2225 6 K\u2016\u03bb1 \u2212 \u03bb2\u2016. (e)\n\u2225\u2225\u2202fix (x, \u03bb)\u2225\u2225 6 L and \u2202\u2225\u2225fi\u03bb(x, \u03bb)\u2225\u2225 6 K. Proof of (e). Suppose there exists a vector x, such that we can choose a subgradient Ofix (x, \u03bb) \u2208 \u2202fix (x, \u03bb), where \u2016Ofix (x, \u03bb)\u2016 > L. Suppose y = x + Ofix (x, \u03bb), according to the definition of subgradient given in Definition 1, we have\nfi(y, \u03bb)\u2212 fi(x, \u03bb) > \u3008Ofix (x, \u03bb), y\u2212 x\u3009 > \u2225\u2225Ofix (x, \u03bb)\u2225\u22252 > L\u2225\u2225Ofix (x, \u03bb)\u2225\u2225\n> L \u2225\u2225y\u2212 x\u2225\u2225,\nwhich contradicts the Lipschitz continuity of fi ( x, \u03bb ) with respect to x. By an analogous proof, \u2202fi\u03bb(x, \u03bb) 6 K .\nLemma 6. If (x\u2217,\u03bb\u2217) \u2208 Rnp \u00d7Rn, is an optimal solution to problem (4), then x\u2217 is a Pareto solution to problem (1).\nSince the differentiability of f (x,\u03bb) with respect to x does not hold in general, we propose a Distributed (sub)gradientfree inteval-valued algorithm 1 for solving reformulated problem (4). In (6),4i(k) = [ 41i (k),42i (k), . . . ,4 p i (k) ]> .4\u2212i (k) =[\n1 41i (k) , 142i (k) , . . . , 14pi (k)\n]> , where { 4qi (k) } k>0, q =\n1, 2, . . . , p, k = 1, 2, . . . is a sequence of mutually independent and identically distributed random variables with zero mean. The measurements y+i (k) and y \u2212 i (k) are given by\ny+i (k) = fi ( \u03bei(k) + c(k)4i (k), \u03bbi(k) ) ,\ny\u2212i (k) = fi ( \u03bei(k)\u2212 c(k)4i (k), \u03bbi(k) ) .\nDefine F(k) = \u03c3 { xi(k), xi(k \u2212 1), . . . , xi(0), i = 1, 2, . . . , n;\u03bbi(k), \u03bbi(k\u22121), . . . , \u03bbi(0), i = 1, 2, . . . , n;4i(k\u2212 1),4i(k \u2212 2), . . . ,4i(0), i = 1, 2, . . . , n } , where F(k) is the \u03c3-algebra created by the whole history of Distributed (sub)gradient-free inteval-valued algorithm (Algorithm 1) up to moment k (referring to [4]). We further make the following hypotheses on the dither signal4i(k):\nAssumption 3. (a) For any fixed (i, q), let { 4qi (k) } k>0 be\na sequence of independent and identically distributed (i. i. d.) random variables. For all k > 0 and for any (i, q)\u2223\u22234qi (k)\u2223\u2223 < M1, \u2223\u2223\u2223\u2223 14qi (k) \u2223\u2223\u2223\u2223 < M2, E[ 14qi (k) ] = 0.\n(b) For i 6= j or q 6= r, { 4qi (k) } k>0 and { 4rj (k) } k>0 are\nmutually independent of each other.\nVOLUME 11, 2023 5\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAlgorithm1Distributed (sub)gradient-free inteval-valued algorithm Input: Total numbers of iteration T , step-size \u03b9(k). Initialize: \u03bei \u2208 X for all i = 1, 2, . . . n.\n1: for k = 0, . . .T do 2: Average of local observations xi(k):\n\u03bei(k) = n\u2211\nj=1\nwij(k)xj(k). (5)\n3: Calculation of local measurement di(k)\ndi(k) =\n[ y+i (k)\u2212 y \u2212 i (k) ] 4\u2212i (k)\n2c(k) , (6)\n4: Descent Step:\n\u03be\u0302i(k) = \u03bei(k)\u2212 \u03b9(k)di(k). (7)\nProjection Step: xi(k + 1) = PX ( \u03be\u0302i(k) ) . (8)\n5: Average of local observations \u03bbi(k):\n\u03bbi(k + 1) = n\u2211\nj=1\nwij(k)\u03bbi(k). (9)\n6: end for where di(k) is used as an estimate for \u2202fi i(k) ( \u03bei(k), \u03bbi(k) ) .\nThen, we introduce conditions on the step-size \u03b9(k) of Algorithm 1 and c(k) used in the randomized differences (6):\nAssumption 4. (a) \u03b9(k) > 0, \u2211\u221e\nk=1 \u03b9(k) <\u221e. (b) c(k) > 0, c(k)\u2192 0. (c) \u2211\u221e k=1 \u03b9(k) c(k) =\u221e, \u2211\u221e k=1 \u03b92(k) c2(k) <\u221e.\nThe chosen of unite parameter \u03b9(k)c(k) satisfies the stochastic approximation stepsize condition in [27], [36].\nIV. PROPERTIES OF DISTRIBUTED (SUB)GRADIENT-FREE STOCHASTIC ALGORITHM In this section, we first analyze that the estimate (xi(k), \u03bbi(k)) converges to a optimal point (x\u2217, \u03bb\u2217) in consensus and almost surely is reached by Algorithm 1. Then we give the mean square convergence rate of Algorithm 1.\nDenote the transition matrix of W (k) as \u03a8(k, s) = W (k)W (k \u2212 1) \u00b7 \u00b7 \u00b7W (s), k > s, where [ \u03a8(k, s) ] ij is the ij-th element of \u03a8(k, s). The following lemma about \u03a8(k, s) holds ture, given in Proposition 1 of [1].\nLemma 7. Under Assumptions 2, \u2223\u2223\u2223[\u03a8(k, s)]ij \u2212 1n \u2223\u2223\u2223 6\n\u03b4\u03b2k\u2212s, \u2200k > s, where \u03b4 = 2 ( 1 + \u03b7\u2212K0 ) / ( 1 \u2212 \u03b7\u2212K0 ) , with\nK0 = ( n\u2212 1 ) \u03ba and \u03b2 = ( 1\u2212 \u03b7\u2212K0 )1/K0 < 1.\nFirst, we introduce a theorem regarding convergence analysis of the proposed algorithm.\nTheorem 1. With Assumptions 1-4,\n(a) all the sequences {\u03bbi(k)}, i \u2208 N reach consensus (which is depended by initail parameters \u03bbi(0)\u2032s) almost surely by Algorithm 1. (b) all the sequences {xi(k)}, i \u2208 N by Algorithm 1 converge to the same optimal point x\u2217 in consensus and almost surely.\nThe proof of of Theorem 1 relies on the following lemmas. The first lemma gives an upper bound for the Euclidean norm of di(k) in expection; the second lemma analyzes the consensus in L1 norm of estimates xi(k) in Algorithm 1; the third lemma analyzes the lower bound of the cross term of di(k) and (\u03bei(k) \u2212 x\u2217)in expection and in conditional expection with respect to F(k), where x\u2217 is the optimal solution of (4) for fixed common point \u03bb\u2217; the fourth one analyzes that {xi(k)}, i \u2208 N converge to the same random variable almost surely and the last one analyzes that {xi(k)}, i \u2208 N converge to x\u2217 in L2. The proofs of these lemmas are given inAppendix.\nLemma 8. Let Assumption 1 and 3 hold. Then the first order moments and second moments of di(k) are bounded by\nE \u2225\u2225di(k)\u2225\u2225 6 L, E\u2225\u2225di(k)\u2225\u22252 6 L2 ,\nwhere L is the Lipschitz constant with respect to x in Remark 5.\nLemma 9. With Assumptions 1-4, for given common point \u03bb\u2217, the consensus of estimate xi(k) in L1 is achieved by Algorithm 1, that is, for i, j = 1, 2, . . . , n,\nlim k\u2192\u221e\nE \u2225\u2225xi(k)\u2212 xj(k)\u2225\u2225 = 0.\nLemma 10. With Assumptions 1 and 3, the cross term of di(k) and \u03bei(k)\u2212 \u03be\u2217 is lower bounded (a) in conditional expection with respect to F(k) as follows:\nE [\u2329 di(k), xi(k)\u2212 \u03be\u2217 \u232a\u2223\u2223F(k)] >fi ( x\u0304(k), \u03bb\u0304(k) ) \u2212 fi ( x\u2217, \u03bb\u2217 ) \u2212 L\n\u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225\u2212 B \u2212K \u2225\u2225\u03bbi(k)\u2212 \u03bb\u0304(k)\u2225\u2225\u2212 K\u2016\u03bbi(k)\u2212 \u03bb\u2217\u2225\u2225\u2212 c(k)L\u2225\u22254i (k)\u2225\u2225,\n(b) in expection as follows:\nE [\u2329 di(k), \u03bei(k)\u2212 x\u2217 \u232a] >E [ fi ( x\u0304(k), \u03bb\u2217 ) \u2212 fi ( x\u2217, \u03bb\u2217 )] \u2212 LE\n\u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225 \u22122KE\u2016\u03bbi(k)\u2212 \u03bb\u2217\n\u2225\u2225\u2212 c(k)LE\u2225\u22254i (k)\u2225\u2225\u2212 B, where L is the Lipschitz constant with respect to x, K is the Lipschitz constant with respect to \u03bb given in Remark 5, B is a positive constant.\nLemma 11. With Assumptions 1-4, for fixed common point \u03bb\u2217, all the sequences {xi(k)}, i \u2208 N converge to the same random variable consensusly and almost surely by Algorithm 1.\n6 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nLemma 12. Set \u03b9(k) = 1k1+ and c(k) = 1 k\u03b4 with 1 2 + > \u03b4 > > 0. With Assumptions 1-4, we have n\u2211\ni=1\nE \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 6 M1k2+2 + M2k + M3k +\u03b4 ,\nwhere M1, M2 and M3 are constants.\nProof of Theorem 1. (a) We prove that for i, j = 1, 2, . . . , n,\nlim k\u2192\u221e \u2225\u2225\u03bbi(k)\u2212 \u03bbj(k)\u2225\u2225 = 0 a. s. According to the definition of transition matrix \u03a8(k, s) and the definition of \u03bbi(k + 1) in (9), we have:\n\u03bbi(k + 1) = n\u2211\nj=1\n[ \u03a8(k, 0) ] ij\u03bbj(0). (10)\nDefine \u03bb\u0304(k + 1) = 1n \u2211n\ni=1 \u03bbi(k + 1). According to Assumption 1 and by an analoglous induction, the following equality holds:\n\u03bb\u0304(k + 1) = 1\nn n\u2211 i=1 \u03bbi(0). (11)\nTherefore, \u2200i \u2208 N ,\u2225\u2225\u03bbi(k + 1)\u2212 \u03bb\u0304(k + 1)\u2225\u2225 6 n\u2211 j=1 \u2223\u2223\u2223[\u03a8(k, 0)]ij \u2212 1n \u2223\u2223\u2223\u2225\u2225\u03bbj(0)\u2225\u2225. (12) Plugging in the estimate of \u03a8(k, s) in Lemma 7, we have\u2225\u2225\u03bbi(k + 1)\u2212 \u03bb\u0304(k + 1)\u2225\u2225 6 n\u03b4\u03b2k max 16i6n\n\u2225\u2225\u03bbi(0)\u2225\u2225. (13) Therefore,\nlim k\u2192\u221e \u2225\u2225\u03bbi(k)\u2212 \u03bb\u0304(k)\u2225\u2225 = 0, \u2200i \u2208 N . (14) (b) According to Lemma 11, limk\u2192\u221e \u2211n i=1\n\u2225\u2225xi(k) \u2212 x\u2217\u2225\u22252 converges to a non-negative random variable almost surely. According to Lemma 12, we have\nlim k\u2192\u221e\nE \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 = 0,\nwhich means that {xi(k)}, i \u2208 N generated from Algorithm 1 converge to the optimal solution x\u2217 in L2. Therefore,\nlim k\u2192\u221e n\u2211 i=1 \u2225\u2225xi(k)\u2212 x\u2217\u2225\u222522 = 0, a.s. (15) Then we give the following convergence rate result of the\nproposed algorithm.\nTheorem 2. Set \u03b9(k) = 1k1+ and c(k) = 1 k\u03b4 with 1 2 + > \u03b4 >\n> 0. With Assumptions 1-4, for distributed (sub)gradientfree inteval-valued algorithm (Algorithm 1), we have\nn\u2211 i=1 E \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 \u223c O(max{ 1k , 1k1+2 \u22122\u03b4})\nn\u2211 i=1 E \u2225\u2225\u03bbi(k)\u2212 \u03bb\u2217\u2225\u22252 \u223c O(\u03b2k).\nProof. According to Lemma 12, we have\nn\u2211 i=1 E \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 \u223c O(max{ 1k , 1k1+2 \u22122\u03b4}). (16)\nFollowing (13) in the proof of Theorem 1, we obtain\nn\u2211 i=1 E \u2225\u2225\u03bbi(k)\u2212 \u03bb\u2217\u2225\u22252 \u223c O(\u03b2k). (17)\nThe proof is completed.\nRemark 6. It directly follows from Theorem 2 that the optimal values for and \u03b4 are = 1\n2 and \u03b4 =\n1 2 , respectively,\nwhich in turn indicate that \u03b9k = 1 k 3 2 , ck = 1 k 1 2 , and\nn\u2211 i=1 E \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 \u223c O( 1\u221a k ) n\u2211\ni=1\nE \u2225\u2225\u03bbi(k)\u2212 \u03bb\u2217\u2225\u222522 \u223c O( 1\u221ak ) .\nThe rate matches not only the best rate for centralized stochastic approximation algorithms, see [36] and references therein, but also the best rate given for first-order stochastic subgradient algorithms with a diminishing step-size [37].\nV. SIMULATION In this section, we demonstrate simulations of the Distributed (sub)gradient-free inteval-valued algorithm. To be specific, we consider the following distributed interval-valued quadratic problem:\nmin G(x) = 5\u2211 i=1 [\u03c51i, \u03c52i]\u2016x \u2212 \u03c1i\u20162\ns. t. \u2016x\u2016 6 X ,\nwhere \u03c51i, \u03c51i \u2208 R and \u03c1i \u2208 Rp. This problem is motivited from centralized quadratic interval-valued learning [43] and distributed optimization [45]. We illustrate the proposed algorithm with X := { x \u2223\u2223\u2016x\u2016 6\n100 } , with [\u03c51i, \u03c52i] = [0.5, 2], with \u03c11 = 3 , \u03c12 = 2, \u03c13 = 1, \u03c14 = 0, \u03c15 = \u22121, respectively. Further, we specify some choices of parameters that are used in simulations of the proposed algorithm. First, we set the step-size \u03b9(k) = 1\nk 3 2\nand the parameter c(k) = 1 k 1 2 used in randomized differences. Second, we initialize \u03bb1(0) = 0.1, \u03bb2(0) = 0.3, \u03bb3(0) = 0.5, \u03bb4(0) = 0.7, \u03bb5(0) = 0.9 and xi(0)\u2019s = 0 respectively. Then, let\u2019s investigate the convergence performance of the Distributed (sub)gradient-free inteval-valued algorithm. The simulation results are based on a 5-agent time-varying network, whose communication topology between the agents can be described by Fig. 2. Fig.3 and Fig. 4 provide convergence performance of the proposed algorithm. We can obtain a pareto solution as (0.500, 0.996) for 500 iterations.\nVOLUME 11, 2023 7\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 2. Topology of the 5-agent network.\nVI. CONCLUSION This paper investigated the distributed interval optimization problem, subject to local convex constraints. The objective functions are compact, interval-valued functions and the network for the distributed design is time-varying. Through constructing random differences, a distributed subgradient-free methodology was developed to find a Pareto optimal solution of distributed interval optimization problem. Moreover, we proved that a Pareto optimal solution can be achived with probability one over time-varying network and gave a numerical example to illustrate the effectiveness of the proposed algorithm.\nAPPENDIX A PROOF OF LEMMA 8 According to the definition of di(k) in (6), we have\ndi(k) =\n[ y+i (k)\u2212 y \u2212 i (k) ] 4\u2212i (k)\n2c(k) (18)\nwhere \u2016y+i (k)\u2212 y \u2212 i (k)\u2016 = \u2225\u2225fi(\u03bei(k) + c(k)4i (k), \u03bbi(k))\u2212 fi ( \u03bei(k)\u2212 c(k)4i (k), \u03bbi(k)\n\u2225\u2225 6 2Lc(k)\u2225\u22254i (k)\u2225\u2225 according to Remark 5. Due to Assumption 3(a), we have\nE \u2225\u2225\u2225\u2225\u2225 [ y+i (k)\u2212 y \u2212 i (k) ] 4\u2212i (k) 2c(k) \u2225\u2225\u2225\u2225\u2225 6 L, (19) and\nE \u2225\u2225\u2225\u2225\u2225 [ y+i (k)\u2212 y \u2212 i (k) ] 4\u2212i (k) 2c(k) \u2225\u2225\u2225\u2225\u2225 2 6 L2. (20)\nAPPENDIX B PROOF OF LEMMA 9 For all i \u2208 N , k > 0, define pi(k + 1) = xi(k + 1)\u2212 \u03bei(k) = xi(k + 1)\u2212 n\u2211\nj=1\nwij(k)xj(k).\n(21)\nas the error between xi(k+1) and \u03bei(k). According to Lemma 5(b) and the fact that X is a closed, convex set, we get\u2225\u2225pi(k + 1)\u2225\u2225\n= \u2225\u2225\u2225\u2225PX( n\u2211 j=1 wij(k)xj(k)\u2212 \u03b9(k)di(k) ) \u2212 n\u2211 j=1 wij(k)xj(k) \u2225\u2225\u2225\u2225 6\u03b9(k)\n\u2225\u2225di(k)\u2225\u2225. (22) Rewrite (9) compactly in terms of \u03a8(k, s) and the definition of pi(k + 1) as follows:\nxi(k + 1) = n\u2211\nj=1\n[ \u03a8(k, 0) ] ijxj(0) + pi(k + 1)\n+ k\u2211 s=1 n\u2211 j=1 [ \u03a8(k, s) ] ijpj(s), (23)\n8 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nfor k > s. Define x\u0304(k + 1) = 1n \u2211n\ni=1 xi(k + 1). Moreover, with Assumption 1(b) and by an analoglous induction, the following equality holds:\nx\u0304(k + 1) = 1\nn n\u2211 i=1 xi(0) + 1 n k+1\u2211 s=1 n\u2211 j=1 pj(s) (24)\nTherefore, \u2200i \u2208 N ,\n\u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225 6 n\u2211 j=1 \u2223\u2223\u2223[\u03a8(k, 0)]ij \u2212 1n \u2223\u2223\u2223\u2225\u2225xj(0)\u2225\u2225 + \u2225\u2225pi(k + 1)\u2225\u2225+ 1n n\u2211 j=1\n\u2225\u2225pj(k + 1)\u2225\u2225 +\nk\u2211 s=1 n\u2211 j=1 \u2223\u2223\u2223[\u03a8(k, s)]ij \u2212 1n \u2223\u2223\u2223\u2225\u2225pj(s)\u2225\u2225. (25)\nTaking the expectation of (25), we get E \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225 (26)\n6 n\u2211\nj=1 \u2223\u2223\u2223[\u03a8(k, 0)]ij \u2212 1n \u2223\u2223\u2223\u2225\u2225xj(0)\u2225\u2225+ E\u2225\u2225pi(k + 1)\u2225\u2225 + 1\nn n\u2211 j=1 E \u2225\u2225pj(k + 1)\u2225\u2225+ k\u2211 s=1 n\u2211 j=1 \u2223\u2223\u2223[\u03a8(k, s)]ij \u2212 1n \u2223\u2223\u2223E\u2225\u2225pj(s)\u2225\u2225. (27)\nPlugging in the estimate of \u03a8(k, s) in Lemma 7 and the estimate of pi(k + 1) in (22), we have\nE \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225\n6n\u03b4\u03b2k max 16i6n \u2225\u2225xi(0)\u2225\u2225+ \u03b9(k)E\u2225\u2225di(k)\u2225\u2225+ \u03b9(k)n n\u2211\ni=1\nE \u2225\u2225di(k)\u2225\u2225\n+\u03b4 k\u2211 s=1 \u03b2k\u2212s n\u2211 i=1 \u03b9(s\u2212 1)E \u2225\u2225di(s\u2212 1)\u2225\u2225. (28)\nFrom Theorem 1, we have E \u2225\u2225di(k)\u2225\u2225 6 L. Therefore,\nE \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225 6 n\u03b4\u03b2k max\n16i6n \u2225\u2225xi(0)\u2225\u2225+ 2\u03b9(k)L + \u03b4n\nk\u2211 s=1 \u03b9(s\u2212 1)\u03b2k\u2212sL. (29)\nSince \u2211\u221e\nk=1 \u03b9(k) 2 < \u221e with Assumption 4(a) and\u2211\u221e\nk=1 \u03b9(k) c(k) < \u221e with Assumption 4(c), we obtain limk\u2192\u221e \u03b9(k) = 0 and limk\u2192\u221e \u03b9(k) c(k) = 0. According to\nLemma 3.1 in [4], we obtain limk\u2192\u221e \u2211k\ns=1 \u03b9(s\u22121)\u03b2k\u2212s = 0 and limk\u2192\u221e \u2211k s=1 \u03b9(s\u22121) c(s\u22121)\u03b2 k\u2212s = 0. Therefore,\nlim k\u2192\u221e\nE \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225 = 0, \u2200i \u2208 N . (30)\nAPPENDIX C PROOF OF LEMMA 10 (a) Define\n[Ci(k)]1 = \u03bei(k) + c(k)4i (k) [Ci(k)]2 = \u03bei(k)\u2212 c(k)4i (k)\n(31)\nAccording to Lemma 1: fi ( [Ci(k)]1, \u03bbi(k) ) \u2212 fi ( [Ci(k)]2, \u03bbi(k) ) \u2208 \u2329 \u2202fi\u03bei(k)+\u03b8ic(k)4i(k) ( \u03bei(k) + \u03b8ic(k)4i (k), \u03bbi(k) ) ,\n2c(k)4i (k) \u232a , (32)\nwhere \u03b8i \u2208 [\u22121, 1] is a constant. Therefore, there exists \u03c2i \u2208 \u2202fi\u03bei(k)+\u03b8ic(k)4i(k) ( \u03bei(k) + \u03b8ic(k)4i (k), \u03bbi(k) ) such that\nfi ( [Ci(k)]1, \u03bbi(k) ) \u2212 fi ( [Ci(k)]2, \u03bbi(k) ) = \u2329 \u03c2i, 2c(k)4i (k) \u232a .\n(33) By taking conditional expectation of \u2329 di(k), \u03bei(k)\u2212 x\u2217 \u232a with respect to F(k) and noticing (33), we obtain the following inequality:\nE [\u2329 di(k), \u03bei(k)\u2212 x\u2217 \u232a\u2223\u2223F(k)] = Di(k), (34) VOLUME 11, 2023 9\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nwhere Di(k) = E [ (\u03c2i) > 4i (k) [ 4i (k) ]\u2212> (\u03bei(k)\u2212 x\u2217) \u2223\u2223F(k)]. We further formulate Di(k) as follows:\nDi(k) = E [ (\u03c2i) > ( 4i (k) [ 4i (k) ]\u2212> \u2212 I)(\u03bei(k) \u2212 x\u2217)\n\u2223\u2223F(k)]+ E[\u2329\u03c2i, \u03bei(k)\u2212 x\u2217\u232a\u2223\u2223F(k)]. (35) By Definition 1 and Remark 5, we obtain\nE[ \u2329 \u03c2i, \u03bei(k)\u2212 x\u2217 \u232a |F(k)]\n=E[ \u2329 \u03c2i, \u03bei(k) + \u03b8ic(k)4i (k)\u2212 \u03b8ic(k)4i (k)\u2212 x\u2217 \u232a |F(k)]\n>E[fi ( \u03bei(k) + \u03b8ic(k)4 (k)i, \u03bbi(k) ) \u2212 fi ( x\u2217, \u03bbi(k) ) |F(k)]\n\u2212 \u2223\u2223c(k)\u2223\u2223LE\u2225\u2225\u03b8i 4i (k)\u2225\u2225 >E[fi ( \u03bei(k) + \u03b8ic(k)4i (k), \u03bbi(k) ) \u2212 fi ( x\u0304(k), \u03bbi(k) ) |F(k)]\n+fi ( x\u0304(k), \u03bbi(k) ) \u2212 fi ( x\u2217, \u03bbi(k) ) \u2212 \u2223\u2223c(k)\u2223\u2223LE\u2225\u2225\u03b8i 4i (k)\u2225\u2225\n>fi ( x\u0304(k), \u03bb\u0304(k) ) \u2212 fi ( x\u2217, \u03bb\u2217 ) + fi ( x\u0304(k), \u03bbi(k) ) \u2212fi ( x\u0304(k), \u03bb\u0304(k) ) \u2212 fi ( x\u2217, \u03bbi(k) ) + fi ( x\u2217, \u03bb\u2217\n) \u2212L \u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225\u2212 2\u2223\u2223c(k)\u2223\u2223LE\u2225\u2225\u03b8i 4i (k)\u2225\u2225\n>fi ( x\u0304(k), \u03bb\u0304(k) ) \u2212 fi ( x\u2217, \u03bb\u2217 ) \u2212 L \u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225 \u2212K \u2225\u2225\u03bbi(k)\u2212 \u03bb\u0304(k)\u2225\u2225\u2212 K\u2016\u03bbi(k)\u2212 \u03bb\u2217\u2225\u2225\u2212 2c(k)LE\u2225\u22254i (k)\u2225\u2225,\n(36)\nand\u2223\u2223\u2223\u2223E[(\u03c2i)>(4i (k)[4i (k)]\u2212> \u2212 I)(\u03bei(k)\u2212 x\u2217)\u2223\u2223F(k)]\u2223\u2223\u2223\u2223 =\n\u2223\u2223\u2223\u2223E[(\u03c2i \u2212\u2202fixi(k)(xi(k), \u03bbi(k)))>(4i (k)[4i (k)]\u2212> \u2212I ) (\u03bei(k)\u2212 x\u2217) \u2223\u2223F(k)]\u2223\u2223\u2223\u2223 6 B. (37)\nwhereB is a positive constant. Combining (36), (28) with (34) gives\nE [\u2329 di(k), xi(k)\u2212 \u03be\u2217 \u232a\u2223\u2223F(k)] >fi ( x\u0304(k), \u03bb\u0304(k) ) \u2212 fi ( x\u2217, \u03bb\u2217 ) \u2212 L\n\u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225\u2212 B \u2212K \u2225\u2225\u03bbi(k)\u2212 \u03bb\u0304(k)\u2225\u2225\u2212 K\u2016\u03bbi(k)\u2212 \u03bb\u2217\u2225\u2225\u2212 c(k)L\u2225\u22254i (k)\u2225\u2225.\n(38)\n(b) By Definition 1 and Remark 5, we obtain E[ \u2329 \u03c2i, \u03bei(k)\u2212 x\u2217 \u232a |F(k)]\n=E[ \u2329 \u03c2i, \u03bei(k) + \u03b8ic(k)4i (k)\u2212 \u03b8ic(k)4i (k)\u2212 x\u2217 \u232a |F(k)]\n>E[fi ( \u03bei(k) + \u03b8ic(k)4 (k)i, \u03bbi(k) ) \u2212 fi ( x\u2217, \u03bbi(k) ) |F(k)]\n\u2212 \u2223\u2223c(k)\u2223\u2223LE\u2225\u2225\u03b8i 4i (k)\u2225\u2225 >E[fi ( \u03bei(k) + \u03b8ic(k)4i (k), \u03bbi(k) ) \u2212 fi ( x\u0304(k), \u03bbi(k) ) |F(k)]\n+fi ( x\u0304(k), \u03bbi(k) ) \u2212 fi ( x\u2217, \u03bbi(k) ) \u2212 \u2223\u2223c(k)\u2223\u2223LE\u2225\u2225\u03b8i 4i (k)\u2225\u2225\n>fi ( x\u0304(k), \u03bb\u2217 ) \u2212 fi ( x\u2217, \u03bb\u2217 ) + fi ( x\u0304(k), \u03bbi(k) ) \u2212fi ( x\u0304(k), \u03bb\u2217 ) \u2212 fi ( x\u2217, \u03bbi(k) ) + fi ( x\u2217, \u03bb\u2217\n) \u2212L \u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225\u2212 2\u2223\u2223c(k)\u2223\u2223LE\u2225\u2225\u03b8i 4i (k)\u2225\u2225\n>fi ( x\u0304(k), \u03bb\u2217 ) \u2212 fi ( x\u2217, \u03bb\u2217 ) \u2212 L \u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225 \u22122K\u2016\u03bbi(k)\u2212 \u03bb\u2217 \u2225\u2225\u2212 2c(k)LE\u2225\u22254i (k)\u2225\u2225. (39)\nSimilar to the proof of part (a), we can get E [\u2329 di(k), xi(k)\u2212 \u03be\u2217 \u232a\u2223\u2223F(k)] >fi ( x\u0304(k), \u03bb\u2217 ) \u2212 fi ( x\u2217, \u03bb\u2217 ) \u2212 L\n\u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225\u2212 2L \u22122K\u2016\u03bbi(k)\u2212 \u03bb\u2217\n\u2225\u2225\u2212 c(k)L\u2225\u22254i (k)\u2225\u2225. (40) The proof of second part of Lemma 10 can be given by taking expection to both side of (40).\nAPPENDIX D PROOF OF LEMMA 11 (a) We prove that for i, j = 1, 2, . . . , n,\nlim k\u2192\u221e \u2225\u2225xi(k)\u2212 xj(k)\u2225\u2225 = 0 a. s. From Lemma 9, limk\u2192\u221e E\n\u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225 = 0 holds. Still\n0 6 E [ lim inf k\u2192\u221e \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225] 6 lim inf k\u2192\u221e E \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u2225 = 0, (41)\nwhich yields E [ lim inf k\u2192\u221e \u2225\u2225xi(k + 1) \u2212 x\u0304(k + 1)\u2225\u2225] = 0. Therefore, lim inf\nk\u2192\u221e \u2225\u2225xi(k + 1) \u2212 x\u0304(k + 1)\u2225\u2225 = 0 holds almost surely. Since \u2211n i=1\n\u2225\u2225xi(k + 1) \u2212 x\u0304(k + 1)\u2225\u22252 6\u2211n i=1\n\u2225\u2225xi(k + 1) \u2212 x\u0304(k)\u2225\u22252 according to Lemma 2 and\u2225\u2225xi(k + 1) \u2212 x\u0304(k)\u2225\u22252 6 \u2225\u2225\u03be\u0302i(k) \u2212 x\u0304(k)\u2225\u22252 according to Lemma 3,\nn\u2211 i=1 \u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u22252 6\nn\u2211 i=1 \u2225\u2225\u03be\u0302i(k)\u2212 x\u0304(k)\u2225\u22252 6\nn\u2211 i=1 n\u2211 j=1 wij(k) \u2225\u2225xj(k)\u2212 x\u0304(k)\u2225\u22252 + \u03b92(k) n\u2211 i=1 \u2225\u2225di(k)\u2225\u22252 +2\u03b9(k)\nn\u2211 i=1 \u2225\u2225di(k)\u2225\u2225 n\u2211 j=1 wij(k) \u2225\u2225xj(k)\u2212 x\u0304(k)\u2225\u2225. (42)\nAccording to Assumption 2(b), n\u2211 i=1 n\u2211 j=1 wij(k) \u2225\u2225xj(k)\u2212 x\u0304(k)\u2225\u22252 = n\u2211 i=1 \u2225\u2225xi(k)\u2212 x\u0304(k)\u2225\u22252. (43)\nThus, taking the conditional expectation of both side of (42) yields\nn\u2211 i=1 E [\u2225\u2225xi(k + 1)\u2212 x\u0304(k + 1)\u2225\u22252\u2223\u2223F(k)]\n6 n\u2211\ni=1 \u2225\u2225xj(k)\u2212 x\u0304(k)\u2225\u22252 + n\u2211 i=1 \u03b92(k)E \u2225\u2225di(k)\u2225\u22252\n+ n\u2211 i=1 2n\u03b9(k)E \u2225\u2225di(k)\u2225\u2225E\u2225\u2225xi(k)\u2212 x\u0304(k)\u2225\u2225. (44)\n10 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nAccording to Assumption 4 and Lemma 8(b) \u221e\u2211 k=1 n\u2211 i=1 \u03b92(k)E \u2225\u2225di(k)\u2225\u22252 <\u221e.\nAccording to Theorem 6.2 of [4], \u2211\u221e k=1 \u03b9(k) \u2225\u2225xi(k) \u2212\nx\u0304(k) \u2225\u2225 < \u221e with probability 1. Through Lemma 8(a),\u2211\u221e\nk=1 \u2211n i=1 2n\u03b9(k)E \u2225\u2225di(k)\u2225\u2225E\u2225\u2225xi(k) \u2212 x\u0304(k)\u2225\u2225 < \u221e. Therefore, limk\u2192\u221e\n\u2225\u2225xi(k) \u2212 x\u0304(k)\u2225\u2225 = 0 holds almost surely by Lemma 4.\n(b) Clearly, \u2225\u2225xi(k + 1) \u2212 x\u2217\u2225\u22252 6 \u2225\u2225\u03be\u0302i(k) \u2212 x\u2217\u2225\u22252 according\nto the properties of Euclidean norm in Lemma 3. Then,\u2225\u2225xi(k + 1)\u2212 x\u2217\u2225\u22252 6 \u2225\u2225\u03bei(k)\u2212 x\u2217\u2225\u22252 + \u03b92(k)\u2225\u2225di(k)\u2225\u22252 \u2212 2\u03b9(k) \u2329 di(k), \u03bei(k)\u2212 x\u2217 \u232a . (45)\nTaking conditional expection of both sides of (45), we obtain for all k = 0, 1, 2, . . .,\nE [\u2225\u2225xi(k + 1)\u2212 x\u2217\u2225\u22252\u2223\u2223F(k)]\n6E [\u2225\u2225\u03bei(k)\u2212 x\u2217\u2225\u22252\u2223\u2223F(k)]+ \u03b92(k)E[\u2225\u2225di(k)\u2225\u22252\u2223\u2223F(k)]\n\u22122\u03b9(k)E [\u2329 di(k), \u03bei(k)\u2212 x\u2217 \u232a\u2223\u2223F(k)]. (46) By the double stochasticity of matrix W (k) in Assumption 2(b),\nn\u2211 i=1 E [\u2225\u2225\u03bei(k)\u2212 x\u2217\u2225\u22252\u2223\u2223F(k)] 6 n\u2211 i=1 \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252, n\u2211\ni=1\nE [\u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225\u2223\u2223F(k)] 6 n\u2211\ni=1 \u2225\u2225xi(k)\u2212 x\u0304(k)\u2225\u2225. (47)\nThen, with probability 1, for i \u2208 N , it holds n\u2211\ni=1\nE [\u2225\u2225xi(k + 1)\u2212 x\u2217\u2225\u22252\u2223\u2223F(k)]\n6 n\u2211\ni=1 [\u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 + [Oi(k)]1 + [Oi(k)]2 + [Oi(k)]3 + [ Oi(k) ] 4 + [ Oi(k) ] 5 + [ Oi(k) ] 6 \u2212 Ji(k) ] , (48)\nwhere [ Oi(k) ] 1 = \u03b92(k)E [\u2225\u2225di(k)\u2225\u22252\u2223\u2223F(k)][ Oi(k) ] 2 = 2\u03b9(k)LE \u2225\u2225xi(k)\u2212 x\u0304(k)\u2225\u2225[ Oi(k) ] 3 = 4\u03b9(k)c(k)LE \u2225\u22254 (k)i\u2225\u2225[ Oi(k) ] 4 = 2\u03b9(k)LE \u2225\u2225\u03bbi(k)\u2212 \u03bb\u0304(k)\u2225\u2225[ Oi(k) ] 5 = 2\u03b9(k)LE \u2225\u2225\u03bbi(k)\u2212 \u03bb\u2217\u2225\u2225[ Oi(k) ] 6\n= 2\u03b9(k)B Ji(k) = 2\u03b9(k) [ fi ( x\u0304(k), \u03bb\u0304(k) ) \u2212 fi ( x\u2217, \u03bb\u2217 )] .\nAccording toAssumption 4 and Lemma 8, \u2211\u221e\nk=1\n[ Oi(k) ] 1 <\n\u221e. By the proof in part (a), \u2211\u221e\nk=1\n[ Oi(k) ] 2 < \u221e.\nBy Assumption 3-4, \u2211\u221e\nk=1\n[ Oi(k) ] 3 < \u221e. By Theo-\nrem 1, \u2211\u221e\nk=1\n[ Oi(k) ] 4 < \u221e and \u2211\u221e k=1 [ Oi(k) ] 5 <\n\u221e. By Assumption 4, \u2211\u221e\nk=1\n[ Oi(k) ] 6 < \u221e. There-\nfore, \u2211\u221e\nk=1 \u2211n i=1 [[ Oi(k) ] 1 + [ Oi(k) ] 2 + [ Oi(k) ] 3\n+[ Oi(k) ] 4 + [ Oi(k) ] 5 + [ Oi(k) ] 6 ] < \u221e. From Lemma\n7, the sequence \u2211n\ni=1 \u2225\u2225xi(k) \u2212 x\u2217\u2225\u22252 converges almost surely with \u2211\u221e k=1 \u2211n i=1 Ji(k) < \u221e. Therefore, the\nsequence \u2211n\ni=1 \u2225\u2225\u03bei(k) \u2212 \u03be\u2217\u2225\u22252 converges to a random variable with probability 1. The proof is completed.\nAPPENDIX E PROOF OF LEMMA 12\nBy taking expectation to both sides of (45), we obtain\nE \u2225\u2225xi(k + 1)\u2212 x\u2217\u2225\u22252 6 E\u2225\u2225\u03bei(k)\u2212 x\u2217\u2225\u22252 + \u03b92(k)E\u2225\u2225di(k)\u2225\u22252\n\u2212 2\u03b9(k)E [\u2329 di(k), \u03bei(k)\u2212 x\u2217 \u232a] . (49)\nBy the double stochasticity of matrix W (k) given in Assumption 2(b), we have the following inequalities\nn\u2211 i=1 E \u2225\u2225\u03bei(k)\u2212 x\u2217\u2225\u22252 = n\u2211 i=1 E \u2225\u2225\u2225 n\u2211 j=1 wij(k)xj(k)\u2212 x\u2217 \u2225\u2225\u22252\n6 n\u2211\ni=1\nE \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252, (50)\nn\u2211 i=1 E \u2225\u2225\u03bei(k)\u2212 x\u0304(k)\u2225\u2225 = n\u2211 i=1 E \u2225\u2225\u2225 n\u2211 j=1 wij(k)xj(k)\u2212 x\u0304(k) \u2225\u2225\u2225\n6 n\u2211\ni=1\nE \u2225\u2225xi(k)\u2212 x\u0304(k)\u2225\u2225. (51)\nBy taking summation of both sides of (49) for k = 1, 2, . . .T and i = 1, 2, . . . n and noticing (50), (51) and Lemma 10, we have\nk\u2211 s=1 n\u2211 i=1 E \u2225\u2225xi(s+ 1)\u2212 x\u2217\u2225\u22252\n6 k\u2211\ns=1 n\u2211 i=1 E \u2225\u2225xi(s)\u2212 x\u2217\u2225\u22252 + 4K k\u2211 s=1 n\u2211 i=1 \u03b9(s)E \u2225\u2225\u03bbi(s)\u2212 \u03bb\u2217\u2225\u2225\n+2L k\u2211\ns=1 n\u2211 i=1 \u03b9(s)E \u2225\u2225xi(s)\u2212 x\u0304(s)\u2225\u2225+ 2nB k\u2211 s=1 \u03b9(s)\n+4L k\u2211\ns=1 n\u2211 i=1 \u03b9(s)c(s)E \u2225\u22254i (s)\u2225\u2225+ k\u2211 s=1 n\u2211 i=1 \u03b92(k)E \u2225\u2225di(s)\u2225\u22252\n\u22122 k\u2211\ns=1 n\u2211 i=1 \u03b9(s)E [ fi ( x\u0304(s), \u03bb\u2217 ) \u2212 fi ( x\u2217, \u03bb\u2217 )] . (52)\nVOLUME 11, 2023 11\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTherefore, n\u2211\ni=1\nE \u2225\u2225xi(k + 1)\u2212 x\u2217\u2225\u22252\n6 k\u2211\ns=1 n\u2211 i=1 \u03b92(k)E \u2225\u2225di(s)\u2225\u22252 + 4K k\u2211 s=1 n\u2211 i=1 \u03b9(s)E \u2225\u2225\u03bbi(s)\u2212 \u03bb\u2217\u2225\u2225\n+2L k\u2211\ns=1 n\u2211 i=1 \u03b9(s)E \u2225\u2225xi(s)\u2212 x\u0304(s)\u2225\u2225+ 2nB k\u2211 s=1 \u03b9(s)\n+4L k\u2211\ns=1 n\u2211 i=1 \u03b9(s)c(s)E \u2225\u22254i (s)\u2225\u2225. (53)\nNoticing that \u03b9(s) = 1s1+ , c(s) = 1 s\u03b4 , and 1 2 + > \u03b4 > 0. By Lemma 8, for the first term on the right hand side of (53), we have\nk\u2211 s=1 n\u2211 i=1 \u03b92(s)E \u2225\u2225di(s)\u2225\u22252 6 n k\u2211 s=1 \u03b92(k)L2 6 M1 k2+2 . (54)\nSince X is bounded in Rm, for x \u2208 X , there exists a constant Mx such that \u2225\u2225x\u2225\u2225 6 Mx . Still, \u03bb \u2208 [0, 1]. For the terms on the right hand side of (53), we have\n2L k\u2211\ns=1 n\u2211 i=1 \u03b9(s)E \u2225\u2225xi(s)\u2212 x\u0304(s)\u2225\u2225 6 4nLMx k\u2211 s=1 \u03b9(s) 6 M21 k ,\n(55)\n4K k\u2211\ns=1 n\u2211 i=1 \u03b9(s)E \u2225\u2225\u03bbi(s)\u2212 \u03bb\u2217\u2225\u2225 6 2nK k\u2211 s=1 \u03b9(s) 6 M22 k .\n(56)\nAccording to Assumption 3-4, for the fouth term on the right hand side of (53), we have\n2nB k\u2211\ns=1\n\u03b9(s) 6 M23 k . (57)\nFor the last term on the right hand side of (53), we have\n4L k\u2211\ns=1 n\u2211 i=1 \u03b9(s)c(s)E \u2225\u22254i (s)\u2225\u2225 6 M3k +\u03b4 . (58)\nSince M1,M21,M22,M23,M3 are positive constants in the above inequalities, we have\nn\u2211 i=1 E \u2225\u2225xi(k)\u2212 x\u2217\u2225\u22252 6 M1k2+2 + M2k + M3k +\u03b4 , (59)\nwhere M21 +M22 +M23 = M2.\nREFERENCES [1] A. Nedic and A. Ozdaglar, \u2018\u2018Distributed subgradient methods for multi-\nagent optimization,\u2019\u2019 IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48\u201361, 2009. [2] P. Yi, Y. Hong, and F. Liu, \u2018\u2018Distributed gradient algorithm for constrained optimization with application to load sharing in power systems,\u2019\u2019 Systems & Control Letters, vol. 83, pp. 45\u201352, 2015. [3] A. Nedic, A. Ozdaglar, and P. A. Parrilo, \u2018\u2018Constrained consensus and optimization in multi-agent networks,\u2019\u2019 IEEE Transactions on Automatic Control, vol. 55, no. 4, pp. 922\u2013938, 2010.\n[4] S. S. Ram, A. Nedi\u0107, and V. V. Veeravalli, \u2018\u2018Distributed stochastic subgradient projection algorithms for convex optimization,\u2019\u2019 Journal of optimization theory and applications, vol. 147, no. 3, pp. 516\u2013545, 2010. [5] Y. Zhang, Y. Lou, Y. Hong, and L. Xie, \u2018\u2018Distributed projection-based algorithms for source localization in wireless sensor networks,\u2019\u2019 IEEE Transactions on Wireless Communications, vol. 14, no. 6, pp. 3131\u20133142, 2015. [6] A. Cherukuri and J. Cort\u00e9s, \u2018\u2018Distributed generator coordination for initialization and anytime optimization in economic dispatch,\u2019\u2019 IEEE Transactions on Control of Network Systems, vol. 2, no. 3, pp. 226\u2013237, 2015. [7] G. Shi, B. D. Anderson, and U. Helmke, \u2018\u2018Network flows that solve linear equations,\u2019\u2019 IEEE Transactions on Automatic Control, vol. 62, no. 6, pp. 2659\u20132674, 2017. [8] X. Zeng, S. Liang, Y. Hong, and J. Chen, \u2018\u2018Distributed computation of linear matrix equations: An optimization perspective,\u2019\u2019 arXiv preprint arXiv:1708.01833, 2017. [9] Y. Wang, P. Lin, and Y. Hong, \u2018\u2018Distributed regression estimation with incomplete data in multi-agent networks,\u2019\u2019 Science China Information Sciences, vol. 61, no. 9, p. 092202, 2018. [10] X. Zeng, Y. Peng, and Y. Hong, \u2018\u2018Distributed algorithm for robust resource allocation with polyhedral uncertain allocation parameters,\u2019\u2019 Journal of Systems Science and Complexity, vol. 31, no. 1, pp. 103\u2013119, 2018. [11] V. Kekatos and G. B. Giannakis, \u2018\u2018Distributed robust power system state estimation,\u2019\u2019 IEEETransactions on Power Systems, vol. 28, no. 2, pp. 1617\u2013 1626, 2013. [12] S. Sra, S. Nowozin, and S. J. Wright, Optimization for machine learning. Mit Press, 2012. [13] A. Neumaier, Interval methods for systems of equations. Cambridge university press, 1990, vol. 37. [14] J. Rohn, \u2018\u2018Positive definiteness and stability of interval matrices,\u2019\u2019 SIAM Journal on Matrix Analysis and Applications, vol. 15, no. 1, pp. 175\u2013184, 1994. [15] V. Levin, \u2018\u2018Nonlinear optimization under interval uncertainty,\u2019\u2019Cybernetics and Systems Analysis, vol. 35, no. 2, pp. 297\u2013306, 1999. [16] B. Q. Hu and S. Wang, \u2018\u2018A novel approach in uncertain programming part i: New arithmetic and order relation for interval numbers,\u2019\u2019 Journal of Industrial & Management Optimization, vol. 2, no. 4, pp. 351\u2013371, 2006. [17] L. Wu, M. Shahidehpour, and Z. Li, \u2018\u2018Comparison of scenario-based and interval optimization approaches to stochastic scuc,\u2019\u2019 IEEE Transactions on Power Systems, vol. 27, no. 2, pp. 913\u2013921, 2012. [18] I. Hisao and T. Hideo, \u2018\u2018Multiobjective programming in optimization of the interval objective function,\u2019\u2019 European Journal of Operational Research, vol. 48, no. 2, pp. 219\u2013225, 1990. [19] H.-C. Wu, \u2018\u2018On interval-valued nonlinear programming problems,\u2019\u2019 Journal of Mathematical Analysis and Applications, vol. 338, no. 1, pp. 299\u2013 316, 2008. [20] S.-T. Liu and R.-T. Wang, \u2018\u2018A numerical solution method to interval quadratic programming,\u2019\u2019 Applied mathematics and computation, vol. 189, no. 2, pp. 1274\u20131281, 2007. [21] C. Jiang, X. Han, G. Liu, and G. Liu, \u2018\u2018A nonlinear interval number programming method for uncertain optimization problems,\u2019\u2019 European Journal of Operational Research, vol. 188, no. 1, pp. 1\u201313, 2008. [22] A. Jayswal, I. Stancu-Minasian, and I. Ahmad, \u2018\u2018On sufficiency and duality for a class of interval-valued programming problems,\u2019\u2019 Applied Mathematics and Computation, vol. 218, no. 8, pp. 4119\u20134127, 2011. [23] M. Hlad\u0131k, \u2018\u2018Interval linear programming: A survey,\u2019\u2019 Linear programming-new frontiers in theory and applications, pp. 85\u2013120, 2012. [24] A. Bellet, Y. Liang, A. B. Garakani, M.-F. Balcan, and F. Sha, \u2018\u2018A distributed frank-wolfe algorithm for communication-efficient sparse learning,\u2019\u2019 in Proceedings of the 2015 SIAM International Conference on Data Mining. SIAM, 2015, pp. 478\u2013486. [25] R. Tempo, G. Calafiore, and F. Dabbene, Randomized algorithms for analysis and control of uncertain systems: with applications. Springer Science & Business Media, 2012. [26] H. Ishii and R. Tempo, \u2018\u2018Distributed randomized algorithms for the pagerank computation,\u2019\u2019 IEEE Transactions on Automatic Control, vol. 55, no. 9, pp. 1987\u20132002, 2010. [27] H.-F. Chen, T. E. Duncan, and B. Pasik-Duncan, \u2018\u2018A kiefer-wolfowitz algorithm with randomized differences,\u2019\u2019 IEEE Transactions on Automatic Control, vol. 44, no. 3, pp. 442\u2013453, 1999. [28] A. R. Conn, K. Scheinberg, and L. N. Vicente, Introduction to derivativefree optimization. Siam, 2009, vol. 8.\n12 VOLUME 11, 2023\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nY. Wang et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[29] J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono, \u2018\u2018Optimal rates for zero-order convex optimization: The power of two function evaluations,\u2019\u2019 IEEE Transactions on Information Theory, vol. 61, no. 5, pp. 2788\u20132806, 2013. [30] X. Gao, B. Jiang, and S. Zhang, \u2018\u2018On the information-adaptive variants of the admm: An iteration complexity perspective,\u2019\u2019 Journal of Scientific Computing, no. 4, pp. 1\u201337, 2014. [31] Y. Nesterov and V. Spokoiny, \u2018\u2018Random gradient-free minimization of convex functions,\u2019\u2019 Universit\u00e9 catholique de Louvain, Center for Operations Research and Econometrics (CORE), Tech. Rep., 2011. [32] K. S. Anit, J. Dusan, B. Dragana, and K. Soummya, \u2018\u2018Distributed zeroth order optimization over random networks: A kiefer-wolfowitz stochastic approximation approache,\u2019\u2019 arXiv:1803.07836, 2018. [33] D. Hajinezhad, M. Hong, and A. Garcia, \u2018\u2018Zeroth order nonconvex multiagent optimization over networks,\u2019\u2019 arXiv:1710.09997, 2017. [34] D. Yuan and D. W. Ho, \u2018\u2018Randomized gradient-free method for multiagent optimization over time-varying networks,\u2019\u2019 IEEE transactions on neural networks and learning systems, vol. 26, no. 6, pp. 1342\u20131347, 2015. [35] D. Yuan, D. W. Ho, and S. Xu, \u2018\u2018Zeroth-order method for distributed optimization with approximate projections.\u2019\u2019 IEEE Transactions on Neural Networks and Learning Systems, vol. 27, no. 2, pp. 284\u2013294, 2015. [36] H.-F. Chen, Stochastic approximation and its applications. Springer Science & Business Media, 2006, vol. 64. [37] E. Hazan et al., \u2018\u2018Introduction to online convex optimization,\u2019\u2019 Foundations and Trends\u00ae in Optimization, vol. 2, no. 3-4, pp. 157\u2013325, 2016. [38] J.-B. Hiriart-Urruty and C. Lemar\u00e9chal, Fundamentals of convex analysis. Springer Science & Business Media, 2012. [39] F. H. Clarke, R. J. Stern, Y. S. Ledyaev, and R. R. Wolenski, \u2018\u2018Nonsmooth analysis and control theory,\u2019\u2019 Graduate Texts in Mathematics, vol. 178, no. 7, pp. 137\u2013151, 1998. [40] R. Durrett,Probability: theory and examples. Cambridge university press, 2010. [41] B. T. B. T. P. Polyak, Introduction to optimization. Chapman and Hall, 1987. [42] J.-P. Aubin and A. Cellina, Differential inclusions: set-valued maps and viability theory. Springer Science & Business Media, 2012, vol. 264. [43] A. K. Bhurjee and G. Panda, \u2018\u2018Efficient solution of interval optimization problem,\u2019\u2019 Mathematical Methods of Operations Research, vol. 76, no. 3, pp. 273\u2013288, 2012. [44] T. Maeda, \u2018\u2018On optimization problems with set-valued objective maps: existence and optimality,\u2019\u2019 Journal of Optimization Theory and Applications, vol. 153, no. 2, pp. 263\u2013279, 2012. [45] A. Nedi\u0107 and A. Olshevsky, \u2018\u2018Stochastic gradient-push for strongly convex functions on time-varying directed graphs,\u2019\u2019 IEEE Transactions on Automatic Control, vol. 61, no. 12, pp. 3936\u20133947, 2016.\nYINGHUI WANG received the B.S. degree in mathematics and applied mathematics from Shandong University in 2010. She received her Ph.D. degree in the Academy of Mathematics and Systems Science, Chinese Academy of Sciences, in 2019. She is currently a lecturer in Key Laboratory of Knowledge Automation for Industrial Processes of Ministry of Education, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing 100083, China.\nHer current research interests include distributed optimization, distributed control, and distributed computation of network systems.\nJIUWEI WANG received the B.S. degree in mathematics and applied mathematics from Sichuan University in 2010. He received his Ph.D. degree in the Academy of Mathematics and Systems Science, Chinese Academy of Sciences, in 2019. He current research interests include resource allocation, econometrics and management science.\nXIAOBO SONG received the B.S degree in Engineering in Automation from the Yanshan University, Qinhuangdao, China, in 2017.She is currently working on M.S. degree in Control Engineering at University of Science and Technology Beijing, Beijing, China. Her current research interests include distributed optimization, distributed control, and distributed computation of network systems.\nYANPENG HU received the B.S. degree in Mechanical Engineering from Beijing Institute of Technology, and received the M.S. degree and Ph.D. degree in Aircraft Design Engineering from Beihang University. He is currently a associate professor in School of Automation and Electrical Engineering, University of Science and Technology Beijing. His research interest includes aircraft design, aircraft flight dynamics and control. Email: huyanpeng@ustb.edu.cn.\nVOLUME 11, 2023 13\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
        }
    ],
    "title": "Distributed Interval Optimization Over Time-varying Networks: A numerical Programming Perspective",
    "year": 2023
}