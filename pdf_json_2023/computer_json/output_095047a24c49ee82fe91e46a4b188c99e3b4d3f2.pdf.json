{
    "abstractText": "3D single object tracking with LiDAR points is an important task in the computer vision field. Previous methods usually adopt the matching-based or motion-centric paradigms to estimate the current target status. However, the former is sensitive to the similar distractors and the sparseness of point clouds due to relying on appearance matching, while the latter usually focuses on short-term motion clues (eg. two frames) and ignores the long-term motion pattern of target. To address these issues, we propose a mixed paradigm with two stages, named MTM-Tracker, which combines motion modeling with feature matching into a single network. Specifically, in the first stage, we exploit the continuous historical boxes as motion prior and propose an encoder-decoder structure to locate target coarsely. Then, in the second stage, we introduce a feature interaction module to extract motion-aware features from consecutive point clouds and match them to refine target movement as well as regress other target states. Extensive experiments validate that our paradigm achieves competitive performance on large-scale datasets (70.9% in KITTI and 51.70% in NuScenes). The code will be open soon at https://github.com/LeoZhiheng/ MTM-Tracker.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhiheng Li"
        },
        {
            "affiliations": [],
            "name": "Yubo Cui"
        },
        {
            "affiliations": [],
            "name": "Shuo Li"
        },
        {
            "affiliations": [],
            "name": "Zheng Fang"
        }
    ],
    "id": "SP:f07998990ecce7ee26e2523bb6529136aca67795",
    "references": [
        {
            "authors": [
                "H. Qi",
                "C. Feng",
                "Z. Cao",
                "F. Zhao",
                "Y. Xiao"
            ],
            "title": "P2b: Point-tobox network for 3d object tracking in point clouds",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "C. Zheng",
                "X. Yan",
                "J. Gao",
                "W. Zhao",
                "W. Zhang",
                "Z. Li",
                "S. Cui"
            ],
            "title": "Box-aware feature enhancement for single object tracking on point clouds",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "J. Shan",
                "S. Zhou",
                "Z. Fang",
                "Y. Cui"
            ],
            "title": "Ptt: Point-track-transformer module for 3d single object tracking in point clouds",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "Y. Cui",
                "Z. Fang",
                "J. Shan",
                "Z. Gu",
                "S. Zhou"
            ],
            "title": "3d object tracking with transformer",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Hui",
                "L. Wang",
                "M. Cheng",
                "J. Xie",
                "J. Yang"
            ],
            "title": "3d siamese voxelto-bev tracker for sparse point clouds",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Fan",
                "K. Wang",
                "H. Zhang",
                "J. Tian"
            ],
            "title": "Accurate 3d single object tracker with local-to-global feature refinement",
            "venue": "IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 12211\u201312218, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wu",
                "C. Sun",
                "J. Wang"
            ],
            "title": "Multi-level structure-enhanced network for 3d single object tracking in sparse point clouds",
            "venue": "IEEE Robotics and Automation Letters, vol. 8, no. 1, pp. 9\u201316, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "L. Bertinetto",
                "J. Valmadre",
                "J.F. Henriques",
                "A. Vedaldi",
                "P.H.S. Torr"
            ],
            "title": "Fully-convolutional siamese networks for object tracking",
            "venue": "European Conference on Computer Vision Workshops (G. Hua and H. J\u00e9gou, eds.), pp. 850\u2013865, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "B. Li",
                "J. Yan",
                "W. Wu",
                "Z. Zhu",
                "X. Hu"
            ],
            "title": "High performance visual tracking with siamese region proposal network",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8971\u2013 8980, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "L. Zhang",
                "L. Bertinetto",
                "W. Hu",
                "P.H. Torr"
            ],
            "title": "Fast online object tracking and segmentation: A unifying approach",
            "venue": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 1328\u20131338, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zheng",
                "X. Yan",
                "H. Zhang",
                "B. Wang",
                "S. Cheng",
                "S. Cui",
                "Z. Li"
            ],
            "title": "Beyond 3d siamese tracking: A motion-centric paradigm for 3d single object tracking in point clouds",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8111\u2013 8120, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "K. Lan",
                "H. Jiang",
                "J. Xie"
            ],
            "title": "Temporal-aware siamese tracker: Integrate temporal context for 3d object tracking",
            "venue": "Proceedings of the Asian Conference on Computer Vision, pp. 399\u2013414, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "K. Cho",
                "B. Van Merri\u00ebnboer",
                "C. Gulcehre",
                "D. Bahdanau",
                "F. Bougares",
                "H. Schwenk",
                "Y. Bengio"
            ],
            "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
            "venue": "arXiv preprint arXiv:1406.1078, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "D. Guo",
                "J. Wang",
                "Y. Cui",
                "Z. Wang",
                "S. Chen"
            ],
            "title": "Siamcar: Siamese fully convolutional classification and regression for visual tracking",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6269\u20136277, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Chen",
                "B. Zhong",
                "G. Li",
                "S. Zhang",
                "R. Ji"
            ],
            "title": "Siamese box adaptive network for visual tracking",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 6668\u2013 6677, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "L. u. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, vol. 30, Curran Associates, Inc., 2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Chen",
                "B. Yan",
                "J. Zhu",
                "D. Wang",
                "X. Yang",
                "H. Lu"
            ],
            "title": "Transformer tracking",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8126\u20138135, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "N. Wang",
                "W. Zhou",
                "J. Wang",
                "H. Li"
            ],
            "title": "Transformer meets tracker: Exploiting temporal context for robust visual tracking",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1571\u20131580, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Giancola",
                "J. Zarzar",
                "B. Ghanem"
            ],
            "title": "Leveraging shape completion for 3d siamese tracking",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Zhou",
                "Z. Luo",
                "Y. Luo",
                "T. Liu",
                "L. Pan",
                "Z. Cai",
                "H. Zhao",
                "S. Lu"
            ],
            "title": "Pttr: Relational 3d point cloud object tracking with transformer",
            "venue": "CVPR, pp. 8531\u20138540, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C.R. Qi",
                "O. Litany",
                "K. He",
                "L.J. Guibas"
            ],
            "title": "Deep hough voting for 3d object detection in point clouds",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Cui",
                "J. Shan",
                "Z. Gu",
                "Z. Li",
                "Z. Fang"
            ],
            "title": "Exploiting more information in sparse point cloud for 3d single object tracking",
            "venue": "IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 11926\u201311933, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Hui",
                "L. Wang",
                "L. Tang",
                "K. Lan",
                "J. Xie",
                "J. Yang"
            ],
            "title": "3d siamese transformer network for single object tracking on point clouds",
            "venue": "arXiv preprint arXiv:2207.11995, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Yan",
                "Y. Mao",
                "B. Li"
            ],
            "title": "Second: Sparsely embedded convolutional detection",
            "venue": "Sensors, vol. 18, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "X. Zhu",
                "W. Su",
                "L. Lu",
                "B. Li",
                "X. Wang",
                "J. Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "2021.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Teed",
                "J. Deng"
            ],
            "title": "Raft: Recurrent all-pairs field transforms for optical flow",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pp. 402\u2013419, Springer, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T.-X. Xu",
                "Y.-C. Guo",
                "Y.-K. Lai",
                "S.-H. Zhang"
            ],
            "title": "Cxtrack: Improving 3d point cloud tracking with contextual information",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1084\u20131093, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Geiger",
                "P. Lenz",
                "R. Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pp. 3354\u20133361, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "H. Caesar",
                "V. Bankiti",
                "A.H. Lang",
                "S. Vora",
                "V.E. Liong",
                "Q. Xu",
                "A. Krishnan",
                "Y. Pan",
                "G. Baldan",
                "O. Beijbom"
            ],
            "title": "nuscenes: A multimodal dataset for autonomous driving",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Yin",
                "X. Zhou",
                "P. Krahenbuhl"
            ],
            "title": "Center-based 3d object detection and tracking",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11784\u201311793, 2021.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "I. INTRODUCTION\nRecently, 3D Single Object Tracking (SOT) has received widespread attention in 3D computer vision and has broad application prospect in autonomous driving and mobile robot. Although existing 3D SOT approaches [1]\u2013[7] have achieved promising results, it is noteworthy that most of them inherit the Siamese structure from 2D SOT [8]\u2013[10] and regard 3D tracking as a matching problem (in Fig. 1(a)), which ignores the difference between image and point clouds. Specifically, for 2D SOT, the dense and rich image textures make target matching in consecutive frames relatively simple. However, limited by sparse and textureless nature of point clouds, the 3D matching-based trackers are sensitive to similar objects and prone to fail when tracking occluded target.\nTo break through the limitations of the matching paradigm, some works try to exploit motion or temporal information. Specially, as shown in Fig. 1(b), M2-Track [11] proposes a motion-centric paradigm which first segments two consecutive point clouds and then estimates the relative displacement of target based on foreground points. Besides, to use multiview target appearance, TAT [12] selects some high-quality\nThis work was supported in part by National Natural Science Foundation of China under Grants 62073066 and U20A20197, in part by Fundamental Research Funds for Central Universities under Grant N2226001, and in part by 111 Project under Grant B16009. (Corresponding author: Zheng Fang)\nThe authors are all with the Faculty of Robot Science and Engineering, Northeastern University, Shenyang 110819, China. Zhiheng Li and Zheng Fang are also with the National Frontiers Science Center for Industrial Intelligence and Systems Optimization, Northeastern University, Shenyang 110819, China and also with Key Laboratory of Data Analytics and Optimization for Smart Industry, Ministry of Education, Northeastern University, Shenyang 110819, China. (e-mail: fangzheng@mail.neu.edu.cn)\ntemplates from the previous frames and propagates temporal features to the current frame. Nevertheless, both of them have some shortcomings: 1) M2-Track [11] utilizes the short-term motion clues between two frames but ignores the long-term motion pattern presented in historical frames. Meanwhile, erroneous segmentation makes it hard to identify target from distractors. 2) In the case of using TAT [12] to track distant target, the point clouds are usually so sparse that it is difficult to match target accurately even integrating incomplete points from previous frames.\nDifferent from aggregating multi-template features or estimating target motion via two adjacent point cloud frames, we observe that the center and corners of historical boxes could also describe the target\u2019s key states (eg. consecutive location and orientation), which are usually ignored before. Therefore, these historical boxes could be utilized as a strong prior to model the motion pattern of target and realize rough localization. Meanwhile, owing to only processing a series of boxes, we could avoid extensive computational consumption, compared to dealing with numerous points. However, despite being simple and efficient, it inevitably accumulates location errors due to some low-quality predicted boxes in long-term tracking. Fortunately, benefiting from the accurate geometry information provided by the point clouds, it is easy to further eliminate errors via feature matching.\nBased on the above viewpoints, we propose a mixed track-\nar X\niv :2\n30 8.\n11 87\n5v 2\n[ cs\n.C V\n] 1\n8 D\nec 2\n02 3\ning paradigm (in Fig. 1(c)), called MTM-Tracker (MotionTo-Matching Tracker), which integrates the motion modeling and feature matching in the two-stage framework. In the first stage, the core problem is to estimate target motion via a set of historical boxes. A naive approach is to adopt LSTM [13] or GRU [14] to aggregate previous motion clues in the hidden features. However, if any previous box contains significant noise, the predicted target motion would be largely disrupted. Therefore, we present a Box Motion Predictor (BMP) based on Transformer encoder-decoder module to adaptively learn spatio-temporal transformation of target and suppress noise by attention mechanism. After that, in the second stage, we aim to refine target location through appearance matching of point clouds. We first extract geometry features from sparse points and then introduce a Reciprocating Interaction Module (RIM) which not only propagates target-specific features in up-down and bottom-top streams but also constructs location associations among the search and template regions. In this way, our tracker could perceive the target displacement in feature patches. Moreover, we introduce an Iterative Refinement Module (IRM) to match motion-aware features between two frames and obtain a correlation volume, which is utilized to refine target motion layer by layer. Finally, by gathering the refined motion and regressing other attributes, we could realize robust tracking for a special object.\nOverall, our contributions are as follows: 1) A novel mixed paradigm for 3D SOT, which combines the long-term motion modeling with appearance matching; 2) We propose a twostage pipeline that contains three proposed modules to realize motion prediction, feature interaction and motion refinement; 3) Experiments on the KITTI and NuScenes datasets show that our method achieves competitive performance, and ablation studies verify the effectiveness of the proposed modules."
        },
        {
            "heading": "II. RELATED WORK",
            "text": "A. 2D Single Object Tracking\nFor decades, 2D single object tracking (SOT) has become a fundamental task in image perception and aims to estimate the state of an arbitrary target in the video sequences based on initial status. With the rapid development of deep learning, most 2D SOT trackers [8], [9], [15], [16] employ the Siamese network for feature matching and obtain a similarity map to locate target. For example, pioneering approach SiamFC [8] used weight-shared network to extract features and adopted cross-correlation layer to highlight target on response map. However, due to missing bounding box regression, SiamFC cannot achieve satisfactory results. Thus, SiamRPN [9] introduced classification and proposal regression branches to improve the target localization. Additionally, SiamMask [10] performed both target tracking and segmentation through a unified network. SiamCAR [15] and SiamBAN [16] adopted pixel-wise regression manner to avoid setting the numerous anchors in advance. In recent year, encouraged by impressive performance of Transformer [17], numerous works [18], [19] attempt to construct semantic relation of target between two frames through attention mechanism. Specially, TransT [18] replaced similarity calculation with Transformer to propagate\ntarget feature to the search region. TrDiMP [19] introduced Transformer structure to convey rich temporal clues across frames. However, due to lacking depth information in RGB images, it is non-trivial to use 2D methods to track target in 3D space.\nB. 3D Single Object Tracking\nDue to the emergence of LiDAR sensor, many works [1]\u2013 [7], [20], [21] refer to Siamese structures and exploit point clouds to realize 3D tracking. Initially, SC3D [20] executed Kalman Filter to generate bunches of candidate shapes and chose the best candidate by the Siamese network. However, SC3D cannot be end-to-end trained and consumes much time due to hand-crafted filter. To solve the above issues, P2B [1] generated target-specific features to predict potential centers and utilized VoteNet [22] to predict accurate target proposals in an end-to-end manner. After that, some works like [2], [5] follow the P2B pipeline, but the difference is that BAT [2] exploited box-aware features to strengthen similarity learning. And the V2B [5] regressed target center on the dense bird\u2019s eye view (BEV) feature through Voxel-to-BEV network. At present, more methods [3], [4], [21], [23], [24] show excellent performance by using attention mechanism. Specifically, PTT [3] embeded Transformer blocks into center voting and proposal generation stages. LTTR [4] proposed Transformer encoder-decoder structure to learn semantic relations among different regions. PTTR [21] adopted attention-based feature matching to enhance point feature and integrated target clues.\nAlthough matching-based methods have achieved success in certain, they ignore that tracking is a continuous process in space-time, and break motion association among historical frames. Thus, these methods are sensitive to distractors in the surroundings. Focusing on this problem, M2-Track [11] proposed a motion-based method that modelled relative motion of target instead of feature matching. However, due to only considering motion between two frames, M2-Track still does not fully exploit the temporal information. On the contrary, although recent TAT [12] exploited multi-template features in historical frames, TAT did not consider their motion relation. Therefore, in this paper, we propose a new paradigm which exploits historical boxes as strong prior to model continuous target motion and further refine target location by geometry information from point clouds."
        },
        {
            "heading": "III. METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A. Problem Statement",
            "text": "Unlike multi-object tracking, SOT only involves one target throughout tracking process and does not rely on detection methods to discover new targets. Thus, given a 3D bounding box B0 of target in the first frame, most 3D SOT trackers locate target in subsequent frames according to the last predicted box Bt\u22121 and two continuous point clouds (Pt ,Pt\u22121). Additionally, the size of target is usually assumed to remain unchanged [1], [2], thus we only require to predict the center (x,y,z) and orientation \u03b8 of target."
        },
        {
            "heading": "B. Overall Architecture",
            "text": "Different from previous work only using Bt\u22121, we consider historical boxes Bt\u22121t\u2212N = [Bt\u2212N , ...,Bt\u22121] in the N previous frames as prior motion information, which can guide tracker to locate target from sparse LiDAR data. As shown in Fig. 2, we propose two-stage network MTM-Tracker, which exploits historical boxes to estimate coarse target motion and utilizes geometry information from point cloud to estimate the target box Bt . Specifically, in the first stage, given a historical box sequence, we introduce a Box Motion Predictor (BMP) which leverages the box keypoints to estimate target motion roughly. Then, in the second stage, we present a Reciprocating Interaction Module (RIM) to learn spatial relations among search and template regions through top-down and bottom-up feature interaction. Moreover, to refine the coarse motion in BMP, we propose an Iterative Refinement Module (IRM) to initialize a motion map and incrementally refine it by geometry correlation. We will detail each module in the following subsections."
        },
        {
            "heading": "C. Stage I: Box Motion Prediction",
            "text": "Box Keypoints Representation. We observe that the spatial status of any previous box can be efficiently determined by its eight corners and one center point. Thus, we arrange them in a predefined order to form a keypoint vector C \u2208R9K , where K represents the coordinate of keypoints. And a group of historical boxes can be formulated as Ct\u22121t\u2212N = [Ct\u2212N , ...,Ct\u22121]. After that, we calculate the keypoint offsets for every adja-\ncent box, so that the relative motion in each time step can be denoted as \u2206Ct\u22121t\u2212N = [Ct\u2212N \u2212Ct\u2212N+1, ...,Ct\u22122 \u2212Ct\u22121]. Box Motion Predictor. After obtaining historical movement corresponding to each keypoint, our goal is to predict current motion through these spatial-temporal information. However, due to some low-quality predicted boxes, historical keypoints may deviate from the real position and destroy overall motion prediction. Inspired by Transformer [17], we propose a Box Motion Predictor based on encoder-decoder structure, aiming to construct global association among keypoints and suppress noise by learnable weights. The detail of BMP is illustrated in Fig. 3. Specifically, we first apply fully connected layer to the historical motion \u2206Ct\u22121t\u2212N \u2208R(N\u22121)\u00d79K and add sinusoidal positional embedding [17] to generate a motion token Em \u2208 R(N\u22121)\u00d7D for N time steps, where D is channel dimension. Then, we feed Em to Transformer encoder to learn globalrange motion feature E\u0303m. The process can be formulated as:\nQm = EmW q,Km = EmW k,V m = EmW v (1) E\u0302m = LN ( MHA(Qm,Km,V m)+Qm ) (2)\nE\u0303m = LN ( FFN(E\u0302m)+ E\u0302m ) (3)\nwhere Q,K,V are query, key and value embedding obtained by projection matrices W q,W k,W v. And the MHA, LN, FFN represent multi-head attention, layer normalization and feedforward network respectively. Furthermore, for Transformer decoder, we convert the latest motion vector \u2206Ct\u22121t\u22122 to motion token En and combine it with E\u0303m to decode current motion \u2206Ctt\u22121. Note that the decoder is similar to the encoder except that we generate the query embedding through En and project E\u0303m to key and value embedding respectively. Finally, through averaging the coordinate of keypoint offsets \u2206Ctt\u22121, we obtain coarse motion vector V tt\u22121 = {\u2206xtt\u22121,\u2206ytt\u22121,\u2206ztt\u22121} for target center and can locate target in the current frame."
        },
        {
            "heading": "D. Stage II: Feature Interaction and Motion Refinement",
            "text": "Although stage I can obtain a rough estimation, it is still difficult to reflect the current state of target due to completely depending on historical boxes information. To have a more accurate prediction, we further exploit geometry information from point cloud at time t and t \u22121 to refine target location.\n1) Point Feature Extraction: Firstly, we crop out template points with the same size as search branch to fully use contextual information surrounding target. Then, following [4], [23], we divide point clouds into uniformly sized voxels and apply a weight-shared 3D sparse convolution backbone [25] to aggregate local geometry features. To get a dense representation, we merge the z-axis and feature channel for voxel feature and get 2D BEV feature F \u2208RH\u00d7L\u00d7D to feed feature interaction module in the following network, where H,L,D are the height, width and channel dimensions respectively.\n2) Reciprocating Interaction Module: In order to perceive target motion, we propose Reciprocating Interaction Module (RIM) to construct motion relation for search and template regions through top-down and bottom-up feature interaction. Top-Down Interaction. To exploit prior information in the template frame, we first generate mask Ox \u2208 RH\u00d7L\u00d71 which sets 0 or 1 to denote feature grid whether occupied by the target. Then, we concatenate template feature Fx with mask Ox along feature channel and perform multi-layer interaction with search stream. For each layer, we adopt a convolution block to downsample features with downsampling rates of 2i, where i \u2208 {0,1,2} in the corresponding layer. Then, we design an interactive attention module (IAM) to learn targetspecific associations. As displayed in Fig. 4, for the template and search features, we utilize pooling operation to get global features and generate attention weight by following equation:\nFattn = \u03c3 ( Conv2D ( AvgPool(F)\u2299MaxPool(F) )) (4)\nwhere \u2299 means concatenation operation along channel dimension. The AvgPool and MaxPool denote average pooling and max pooling respectively, and \u03c3 represents the sigmoid function. Then, Fxattn and F s attn are utilized to enhance target-\nspecific feature in Fx and Fs by cross manner:\nHs = (Fxattn \u2297Fs)+Fs, Hx = (Fsattn \u2297Fx)+Fx (5)\nwhere \u2297 denotes the multiply operation. In this way, we can guide the network to learn spatio-temporal relation of target in consecutive frames. Meanwhile, by repeating the above steps, we can obtain multi-scale feature pairs {Hsi ,Hxi } with specific receptive fields. Thus, these features can capture the movements for targets with different speeds and sizes. Bottom-Up Interaction. Inspired by attention mechanism, we apply the deformable attention [26] to align multi-scale features and aggregate target information. Specifically, we concatenate each feature pair {Hsi ,Hxi } and generate temporal features Hsxi \u2208 RH\u00d7L\u00d72D. After that, we set the K reference points gik to each scale of Hsxi and use two linear layers to generate K sampling offsets \u2206gik and attention weights Aik. Then, sampling offsets \u2206gik are added to the coordinates gik of reference points to sample reference features Gik from Hsxi by bilinear interpolation. Finally, the original feature Hsxi can be aggregated by reference features Gik with corresponding weights Aik and obtain refined feature H\u0302sxi . The process could be formulated as follows:\nAik = Linear(Hsxi ), \u2206gik = Linear(H sx i ) (6)\nGik = S(Hsxi ,gik +\u2206gik) (7)\nH\u0302sxi = L\n\u2211 l=1\nWl( I\n\u2211 i=1\nK\n\u2211 k=1 Alik \u00b7Glik) (8)\nwhere S( \u00b7 ) and Wl mean bilinear sampling and learnable weights. L, I,K are the number of attention heads, scale layers, and reference points respectively. For output features H\u0302sxi , we pick out the high resolution H\u0302 sx 0 and further split it into F\u0302x \u2208RH\u00d7L\u00d7D and F\u0302s \u2208RH\u00d7L\u00d7D based on concatenation order for search and template features."
        },
        {
            "heading": "PERFORMANCE COMPARISON ON THE KITTI DATASET. MEAN",
            "text": "3) Iterative Motion Refinement: Inspired by optical flow estimation [27], we propose an Iterative Refinement Module (IRM) to refine coarse motion V tt\u22121 through dense geometry features F\u0302x and F\u0302s, which could become a bridge to connect motion and matching paradigms. The details are as follows: Motion Initialization. We initialize the motion map M0 = ( f 1, f 2) \u2208RH\u00d7L\u00d72 to 0 everywhere, where f 1 and f 2 means motion components along the X and Y axes. Then, we utilize \u2206xtt\u22121 and \u2206y t t\u22121 in coarse motion vector V t t\u22121 from BMP to fill f 1 and f 2 respectively. Correlation Volume Generation. Given dense features F\u0302x and F\u0302s, we employ dot product to them and form a 4D correlation volume W \u2208RH\u00d7L\u00d7H\u00d7L, which represents similarity degree for each pixel pair between F\u0302x(u,v) and F\u0302s(u,v). Iterative Refinement. In this part, our goal is to iteratively update M0 in N times and get a refined MN . As displayed in Fig. 4, for each iteration, we estimate an increment \u2206M and add it to current motion: Mn+1 = Mn +\u2206Mn. Specifically, based on Mn, we project each pixel p = (u,v) in F\u0302x to its estimated correspondence in F\u0302s : p\u2032 = (u+ f 1(u),v+ f 2(v)). After that, we generate coordinate indices within radius r of each pixel p\u2032 to sample correlation feature Fnc \u2208 RW\u00d7L\u00d7r 2 from volume W . Besides, we employ several convolutional blocks to process the correlation feature and combine it with Mn to generate \u2206Mn. The process could be defined as:\nR = ConvB ( ConvB(Fnc )\u2299ConvB(Mn) ) \u2299Mn (9)"
        },
        {
            "heading": "PERFORMANCE COMPARISON ON THE NUSCENES DATASET. PED IS AN ABBREVIATION FOR PEDESTRIAN. \u2020 REPRESENTS THE RESULTS WE TRAIN AND TEST WITH THE OFFICIAL CODE.",
            "text": "where ConvB is the block consisting of 2D convolution and ReLU layer. As a result, we could exploit geometry features to eliminate the estimation bias caused by the first stage as much as possible and get a more accurate motion map MN ."
        },
        {
            "heading": "E. Target Regression and Loss Function",
            "text": "To obtain target location (xt ,yt), we suppose that target is located in the center of template frame following the [1]\u2013 [3]. Therefore, we can use center coordinate to index target motion (\u2206\u0302xtt\u22121, \u2206\u0302y t t\u22121) from motion map M\nN and obtain the current target location through (xt ,yt) = (xt\u22121+ \u2206\u0302xtt\u22121,yt\u22121+ \u2206\u0302ytt\u22121). Then, we utilize search feature F\u0302s to predict a zaxis map Zt \u2208RH\u00d7L\u00d71 and an orientation map \u0398t \u2208RH\u00d7L\u00d71. Finally, we exploit target location (xt ,yt) to index the vertical location zt and orientation \u03b8t from the predicted map.\nOur method is trained with box keypoint loss Lkp, motion refinement loss Lmt and regression loss Lreg as follows:\nLkp = 1\nNk\nNk\n\u2211 n=1 (\u2206Cn \u2212 \u02c6\u2206Cn)2 (11)\nLmt = 1\nNpos\nN\n\u2211 n=1 I \u2223\u2223Mn \u2212 M\u0302n\u2223\u2223 , Lreg = 1Npos N \u2211 n=1 I |\u03b3n \u2212 \u03b3\u0302n| (12)\nwhere I is the indicator function denoting occupation situation, \u03b3n is the object orientation and z-axis location. Nk, Npos and N mean the number of box keypoints, occupied grids and all grids respectively. The whole training loss is the sum of the above losses with special weights."
        },
        {
            "heading": "IV. EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A. Experimental Settings",
            "text": "Datasets. We evaluate our proposed approach on the widely used datasets: KITTI [29] and NuScenes [30]. For KITTI,"
        },
        {
            "heading": "Car",
            "text": ""
        },
        {
            "heading": "Pedestrian",
            "text": ""
        },
        {
            "heading": "ABLATION STUDIES ON MODEL COMPONENTS. BMP MEANS BOX MOTION PREDICTOR. RIM REPRESENTS RECIPROCATING INTERACTION",
            "text": ""
        },
        {
            "heading": "ABLATION STUDIES ON BOX MOTION PREDICTOR. SETTING USED IN OUR FINAL MODEL ARE UNDERLINED.",
            "text": "following the previous work [1]\u2013[3], we split the scenes 0- 16 for training, scenes 17-18 for validation, and scenes 19-20 for testing. For NuScenes with larger data volumes and more target categories, we follow implementation of [11] to divide 850 scenes into training and validation sets. Evaluation Metric. Consistent with present approaches, we employ One Pass Evaluation (OPE) to measure the Success and Precision for tracking results. Success denotes Intersection Over Union (IOU) between ground-truth and predicted box, while Precision measures Area Under Curve (AUC) for distance between two box centers ranging from 0 to 2 meters. Training and Inference Details. In data processing, we set the search and template region as [-3.2m, 3.2m] for the X and Y axes, and [-3m, 1m] for the Z axis. Following [4], [23], we voxelize point cloud with the voxel size [0.025m, 0.025m, 0.05m]. Note that we employ random offsets to ground truth during training to simulate the historical boxes with errors in testing. For KITTI and nuScenes datasets, we train 40 epochs for our proposed method with the Adam optimizer and an initial learning rate of 0.001. The training procedure spends about 3 hours on a NVIDIA 3090 GPU with batch size 128. During inference, our tracker exploits several historical boxes and two point cloud frames to capture target frame-by-frame,\nmeanwhile running speed achieves 24.7 FPS on single GPU."
        },
        {
            "heading": "B. Comparison with State-of-the-arts",
            "text": "Results on KITTI. As displayed in Tab. I, our MTM-Tracker outperforms other methods in all categories. Specifically, our method surpasses previous state-of-the-art method CXTrack [28] of 3.4%/3.1% in the average performance. We speculate that compared with CXTrack which depends on contextual information of point clouds across two frames, our paradigm can overcome occlusion better and relieve interference from the other similar objects due to extra modeling target motion during multiple frames. Moreover, our method surpasses M2Track [11] significantly (\u21918.0%/5.0%). We think although motion-based M2-Track is better than matching approaches, the inaccurate segmentation and short-term motion modeling may still limit its tracking performance. In contrast, owing to considering long-term motion prior and appearance matching at the same time, our method could locate target accurately. Besides, we exhibit some tracking results in Fig. 5. Results on NuScenes. As illustrated in Tab. II, our MTMTracker obtains the best performance in nuScenes dataset for most categories and demonstrates superior results compared with previous matching paradigm [1], [2], [6], [7], [20], [23]. Specially, the BEV-based method [23] is hard to regress large"
        },
        {
            "heading": "COMPARISON OF DIFFERENT COMPONENTS IN RECIPROCATING INTERACTION MODULE. TD AND BU STREAM MEAN TOP-DOWN AND",
            "text": ""
        },
        {
            "heading": "ABLATION STUDIES ON MULTI-SCALE FEATURES IN RECIPROCATING",
            "text": "objects by directly using CenterHead [31] since target center is usually empty. Different from [23], our BEV-based MTMTracker utilizes motion clues to estimate a coarse center on BEV and further refine it through correlation features. In this way, we not only mitigate the above issue in [23] but also achieve performance balance among all categories. Notably, our MTM-Tracker surpasses M2-Track [11] in the Pedestrian by 4.98% Success and 11.88% Precision, and obtains certain improvement for the large objects, such as Trailer, Truck and Bus. Meanwhile, we outperform M2-Track by 2.47%/4.44% on Mean metrics, proving superiority of overall performance in our Motion-to-Matching paradigm."
        },
        {
            "heading": "C. Ablation Studies",
            "text": "Based on KITTI dataset, we conduct comprehensive experiments in Car and Pedestrian categories to validate the effectiveness of our proposed method. Model Components. The 1st row in Tab. III shows that when BMP is not used to provide motion prior to the second stage, it will cause some adverse effects. Especially for Pedestrians that are easily confused, this effect is more significant. Then, as demonstrated in the 2nd row, the Mean performance will drop 7.4%/6.3% without feature interaction, which proves the effectiveness of our RIM. Moreover, in Fig. 6, we compare the tracking trajectories generated by BMP-only and our twostage tracker. The result shows that due to the cumulative errors, BMP loses target when it turns, while the two-stage tracker maintains robust tracking since the second stage can exploit point clouds to refine the target states. Finally, Fig. 7 shows that the coarse prediction from BMP can be corrected through feature matching in the second stage. Box Motion Prediction. In the case of utilizing the same number of historical boxes, we compare our BMP with other sequence-to-sequence models including the LSTM [13] and GRU [14]. As shown in Tab. IV, the LSTM and GRU are lower than BMP by 3.2%/1.8% and 1.3%/1.5% on Mean performance. We speculate that due to the appearance of some inaccurate predicted boxes in historical sequence, the"
        },
        {
            "heading": "EFFECTS OF DIFFERENT REGRESSION STRATEGIES ON PERFORMANCE.",
            "text": "unreliable motion information would be encoded into hidden features in the [13], [14] and hinder final motion prediction. However, owing to the attention mechanism, our BMP could suppress negative impacts from low-quality boxes via learnable weights and model global motion pattern better. Box Keypoints Selection. In Tab. IV, we explore the effect of choosing different box keypoints on tracking performance. Specifically, in the 1st row, lacking explicit center constraint will lead to performance degradation (\u21931.5%/2.0% in average metrics). Besides, in the 2nd row, using continuous center still cannot realize optimal prediction. However, when adding the overall motion provided by box corners, the performance can improve 0.8%/1.4%. Thus, we believe that the box center and corners are both indispensable in box motion prediction. Historical Boxes Input. As shown in Fig. 8(a), we study the impact on performance when different numbers of historical boxes are utilized to estimate box motion. When the number increases from one to two, MTM-Tracker achieves significant improvement, especially \u21915.2% Success in Pedestrian class. Then, the performance will improve steadily until the boxes increase to five. After that, feeding too many historical boxes leads to performance fluctuations. We think that a few boxes already provide enough motion clues, such as velocity, while further increasing boxes will bring excessive complexity and redundant information to cause the negative effects in certain. Reciprocating Interaction. In Tab. V, we validate effectiveness of our Reciprocating Interaction Module. As illustrated in the 1st row, the top-down stream without IAM leads to a decrease of 1.0%/1.0% in Mean, verifying that proposed interactive attention plays an essential role in learning spatial relation. Then, the 2nd row confirms the necessity of using template mask to distinguish target from background. Additionally, from the 3rd , 4th and 5th row, we can observe that our deformable operation could achieve the best result due to adaptively aligning multi-scale features and integrating information in deformable manner. Moreover, Tab. VI shows that benefiting from perceiving target from multi-scale features, our method could catch target at various speeds and achieve\nbetter performance. Iterative Refinement. In Fig. 8(b), we demonstrate the relationship between the number of iterations and performance. We can see that as iteration increases, the performance will first rise obviously and tend to stabilize but be slightly lower than optimal value. We speculate that excessive iteration will cause motion increment in each iteration to be too small and affect the final result. Nonetheless, as shown in Fig. 8(b), our method still outperforms the previous state-of-the-art method CXTrack [28] with remarkable margins in most cases. Regression Strategy. For the orientation and z-axis location regression, we compare performance when predicting relative and absolute values, and the results are displayed in Tab. VII. First, if using \u2206z and \u2206\u03b8 as regression results which are estimated by BMP and refined by the second stage, the Mean will decline 11.3% and 9.9%. We think this is because the changes in the z-axis location and orientation of target are usually too small between two frames. Thus, it is difficult to refine relative values accurately and will cause accumulative errors, resulting in deviating from the ground truth. Then, in the 2nd and 3rd row, we replace \u2206z and \u2206\u03b8 with absolute predictions respectively and observe that the z-axis regression strategy has more significant impact on performance. Finally, in the 4th row, the performance can be further improved when the absolute values z and \u03b8 are applied."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this work, we investigate the limitations of existing 3D SOT methods and propose a novel mixed tracking paradigm, which integrates motion modeling with appearance matching via a unified network. Additionally, we propose a two-stage pipeline MTM-Tracker containing motion prediction, feature interaction and motion refinement. The comprehensive experiments prove the effectiveness of our proposed modules and demonstrate that our method has competitive performance in various aspects."
        }
    ],
    "title": "Motion-to-Matching: A Mixed Paradigm for 3D Single Object Tracking",
    "year": 2023
}