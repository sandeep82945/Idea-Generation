{
    "abstractText": "A fundamental issue in machine learning is the robustness of the model with respect to changes in the input. In natural language processing, models typically contain a first embedding layer, transforming a sequence of tokens into vector representations. While the robustness with respect to changes of continuous inputs is well-understood, the situation is less clear when considering discrete changes, for instance replacing a word by another in an input sentence. Our work formally proves that popular embedding schemes, such as concatenation, TF-IDF, and Paragraph Vector (a.k.a. doc2vec), exhibit robustness in the H\u00f6lder or Lipschitz sense with respect to the Hamming distance. We provide quantitative bounds for these schemes and demonstrate how the constants involved are affected by the length of the document. These findings are exemplified through a series of numerical examples.",
    "authors": [
        {
            "affiliations": [],
            "name": "R\u00e9mi Catellier"
        },
        {
            "affiliations": [],
            "name": "Samuel Vaiter"
        },
        {
            "affiliations": [],
            "name": "Damien Garreau"
        }
    ],
    "id": "SP:e95401a6501bbe529f2203138605ecb117230b76",
    "references": [
        {
            "authors": [
                "R.P. Agarwal",
                "S. Deng",
                "W. Zhang"
            ],
            "title": "Generalization of a retarded Gronwall-like inequality and its applications",
            "venue": "Appl. Math. Comput.,",
            "year": 2005
        },
        {
            "authors": [
                "W. Alghamdi",
                "H. Hsu",
                "H. Jeong",
                "H. Wang",
                "P.W. Michalak",
                "S. Asoodeh",
                "F.P. Calmon"
            ],
            "title": "Beyond adult and compas: Fairness in multi-class prediction",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "V.I. Arnold"
            ],
            "title": "Ordinary Differential Equations",
            "year": 1978
        },
        {
            "authors": [
                "S. Arora",
                "Y. Li",
                "Y. Liang",
                "T. Ma",
                "A. Risteski"
            ],
            "title": "A latent variable model approach to PMI-based word",
            "venue": "embeddings. TACL,",
            "year": 2016
        },
        {
            "authors": [
                "I. Bailleul",
                "R. Catellier"
            ],
            "title": "Non-explosion criteria for rough differential equations driven by unbounded vector fields",
            "venue": "Ann. Fac. Sci. Toulouse Math.,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Bengio",
                "R. Ducharme",
                "P. Vincent"
            ],
            "title": "A neural probabilistic language model",
            "venue": "In NeurIPS,",
            "year": 2000
        },
        {
            "authors": [
                "P. Bojanowski",
                "E. Grave",
                "A. Joulin",
                "T. Mikolov"
            ],
            "title": "Enriching word vectors with subword information",
            "venue": "TACL,",
            "year": 2017
        },
        {
            "authors": [
                "T. Brown",
                "B. Mann",
                "N. Ryder",
                "M. Subbiah",
                "J.D. Kaplan",
                "P. Dhariwal",
                "A. Neelakantan",
                "P. Shyam",
                "G. Sastry",
                "A Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "S. Bubeck",
                "M. Sellke"
            ],
            "title": "A universal law of robustness via isoperimetry",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "S. Bubeck",
                "R. Eldan",
                "Y.T. Lee",
                "D. Mikulincer"
            ],
            "title": "Network size and size of the weights in memorization with two-layers neural networks",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "D. Cer",
                "Y. Yang",
                "Kong",
                "S.-y",
                "N. Hua",
                "N. Limtiaco",
                "R.S. John",
                "N. Constant",
                "M. Guajardo-Cespedes",
                "S. Yuan",
                "C Tar"
            ],
            "title": "Universal sentence encoder",
            "venue": "arXiv preprint arXiv:1803.11175,",
            "year": 2018
        },
        {
            "authors": [
                "M. Cisse",
                "P. Bojanowski",
                "E. Grave",
                "Y. Dauphin",
                "N. Usunier"
            ],
            "title": "Parseval networks: Improving robustness to adversarial examples",
            "venue": "In ICML,",
            "year": 2017
        },
        {
            "authors": [
                "F.M. Dannan"
            ],
            "title": "Integral inequalities of Gronwall-BellmanBihari type and asymptotic behavior of certain second order nonlinear differential equations",
            "venue": "J. Math. Anal. Appl.,",
            "year": 1985
        },
        {
            "authors": [
                "J. Devlin",
                "Chang",
                "M.-W",
                "K. Lee",
                "Toutanova",
                "K. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In NAACL,",
            "year": 2018
        },
        {
            "authors": [
                "P. Gage"
            ],
            "title": "A new algorithm for data compression",
            "venue": "C Users Journal,",
            "year": 1994
        },
        {
            "authors": [
                "B. Gao",
                "L. Pavel"
            ],
            "title": "On the properties of the softmax function with application in game theory and reinforcement learning",
            "venue": "arXiv preprint arXiv:1704.00805,",
            "year": 2017
        },
        {
            "authors": [
                "D. Garreau",
                "D. Mardaoui"
            ],
            "title": "What does LIME really see in images",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "M. Hein",
                "M. Andriushchenko"
            ],
            "title": "Formal guarantees on the robustness of a classifier against adversarial manipulation",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "K.S. Jones"
            ],
            "title": "A statistical interpretation of term specificity and its application in retrieval",
            "venue": "J. Doc.,",
            "year": 1972
        },
        {
            "authors": [
                "Kim",
                "Y.-H"
            ],
            "title": "Gronwall, Bellman and Pachpatte type integral inequalities with applications",
            "venue": "Nonlinear Anal. Theory Methods Appl.,",
            "year": 2009
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "In ICLR,",
            "year": 2015
        },
        {
            "authors": [
                "Q. Le",
                "T. Mikolov"
            ],
            "title": "Distributed representations of sentences and documents",
            "venue": "In ICML,",
            "year": 2014
        },
        {
            "authors": [
                "Y. Li",
                "L. Xu",
                "F. Tian",
                "L. Jiang",
                "X. Zhong",
                "E. Chen"
            ],
            "title": "Word embedding revisited: A new representation learning and explicit matrix factorization perspective",
            "venue": "AISTAT,",
            "year": 2015
        },
        {
            "authors": [
                "L. Logeswaran",
                "H. Lee"
            ],
            "title": "An efficient framework for learning sentence representations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "H.P. Luhn"
            ],
            "title": "A statistical approach to mechanized encoding and searching of literary information",
            "venue": "IBM J. Res. Dev.,",
            "year": 1957
        },
        {
            "authors": [
                "T. Mikolov",
                "K. Chen",
                "G.S. Corrado",
                "J. Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "venue": "arXiv preprint arXiv:1301.3781,",
            "year": 2013
        },
        {
            "authors": [
                "T. Mikolov",
                "I. Sutskever",
                "K. Chen",
                "G.S. Corrado",
                "J. Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "NeurIPS,",
            "year": 2013
        },
        {
            "authors": [
                "F. Morin",
                "Y. Bengio"
            ],
            "title": "Hierarchical probabilistic neural network language model",
            "venue": "In AISTAT,",
            "year": 2005
        },
        {
            "authors": [
                "B.G. Pachpatte"
            ],
            "title": "On some new nonlinear retarded integral inequalities",
            "venue": "J. Inequal. Pure Appl. Math,",
            "year": 2004
        },
        {
            "authors": [
                "J. Pennington",
                "R. Socher",
                "C.D. Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "In EMNLP,",
            "year": 2014
        },
        {
            "authors": [
                "M.E. Peters",
                "M. Neumann",
                "M. Iyyer",
                "M. Gardner",
                "C. Clark",
                "K. Lee",
                "L. Zettlemoyer"
            ],
            "title": "Deep contextualized word representations",
            "year": 2018
        },
        {
            "authors": [
                "R. \u0158eh\u016f\u0159ek",
                "P. Sojka"
            ],
            "title": "Software Framework for Topic Modelling with Large Corpora",
            "venue": "In Proc. of the LREC 2010 Workshop on New Challenges for NLP Frameworks,",
            "year": 2010
        },
        {
            "authors": [
                "J.M. Steele"
            ],
            "title": "The Cauchy-Schwarz master class: an introduction to the art of mathematical inequalities",
            "year": 2004
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Zaremba",
                "I. Sutskever",
                "J. Bruna",
                "D. Erhan",
                "I. Goodfellow",
                "R. Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "D. Wei",
                "H. Wu",
                "M. Wu",
                "Chen",
                "P.-Y",
                "C. Barrett",
                "E. Farchi"
            ],
            "title": "Convex bounds on the softmax function with applications to robustness verification",
            "venue": "In AISTATS,",
            "year": 2023
        },
        {
            "authors": [
                "L. Weng",
                "H. Zhang",
                "H. Chen",
                "Z. Song",
                "Hsieh",
                "C.-J",
                "L. Daniel",
                "D. Boning",
                "I. Dhillon"
            ],
            "title": "Towards fast computation of certified robustness for ReLU networks",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Weng",
                "T.-W",
                "H. Zhang",
                "Chen",
                "P.-Y",
                "J. Yi",
                "D. Su",
                "Y. Gao",
                "Hsieh",
                "C.-J",
                "L. Daniel"
            ],
            "title": "Evaluating the robustness of neural networks: An extreme value theory approach",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wu",
                "M. Schuster",
                "Z. Chen",
                "Q.V. Le",
                "M. Norouzi",
                "W. Macherey",
                "M. Krikun",
                "Y. Cao",
                "Q. Gao",
                "K Macherey"
            ],
            "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation",
            "venue": "arXiv preprint arXiv:1609.08144,",
            "year": 2016
        },
        {
            "authors": [
                "H. Xu",
                "S. Mannor"
            ],
            "title": "Robustness and generalization",
            "venue": "Mach. Learn.,",
            "year": 2012
        },
        {
            "authors": [
                "W.E. Zhang",
                "Q.Z. Sheng",
                "A. Alhazmi",
                "C. Li"
            ],
            "title": "Adversarial attacks on deep-learning models in natural language processing: A survey",
            "venue": "ACM Trans. Intell. Syst. Technol.,",
            "year": 2020
        },
        {
            "authors": [
                "Agarwal"
            ],
            "title": "Our approach is inspired by proofs of Gr\u00f6nwall-Bellman-Bahouri type lemmas, see for example Dannan",
            "venue": "Pachpatte",
            "year": 2004
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Recent advances in natural language processing (NLP) have exceeded all expectations. In particular, the advent of large language models such as BERT (Devlin et al., 2018) and GPT (Brown et al., 2020) are transforming radically the way we interact with computers. They typically rely on a deep neural network (DNN) architecture and are trained on a variety of tasks such as sentiment analysis, translation, and text summarization.\nA known issue with DNNs is the existence of adversarial examples: examples modified in order to radically change the output of the model. Initially popularized in the context of image classification (Szegedy et al., 2014), such examples also exist in NLP and a flourishing literature exists on this topic (Zhang et al., 2020). This problem has sparked\n1Universite\u0301 Co\u0302te d\u2019Azur, CNRS, LJAD, France 2Inria, France 3CNRS, France. Correspondence to: Damien Garreau <damien.garreau@unice.fr>.\nProceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).\na tremendous interest into the robustness of models with respect to small changes in the input. In this paper, we focus on the robustness of the vectorization NLP pipelines: the transformation of the input document into a vector representation. We will consider documents as ordered sequences of tokens, not necessarily corresponding to words. For instance, GPT 2 uses Byte Pair encoding (Gage, 1994; Sennrich et al., 2016), which relies on tokens corresponding to sub-words.\nAs far as we reckon, there are essentially three main schools of thought when it comes to vectorization: (i) concatenation of vectors corresponding to each token of the document. These vectors are often called word vectors when the tokens are individual words. They can either be one-hot representations of the tokens, or obtained by a mapping learned from data. A celebrated approach to produce word vectors is word2vec (Mikolov et al., 2013a;b), which transports semantic properties to the embedding space. Many other methods exist, such as GloVe (Pennington et al., 2014), EMF (Li et al., 2015), WordPiece (Wu et al., 2016), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). Positional information is typically added to the token embeddings. (ii) TF-IDF (term frequency - inverse document frequency), taking words as tokens and simply considering the frequencies of each individual word in the document. These frequencies are reweighted by an overall importance term to take into account the lesser importance of frequently appearing words such as articles. This is the historical approach to text vectorization (Luhn, 1957; Jones, 1972). (iii) ad hoc approaches. Notably, Paragraph Vector (also known as doc2vec (Le & Mikolov, 2014)) extends the ideas of word2vec. Although we will focus on doc2vec in this work, we emphasize that there exists other ad hoc approaches, such as skip-thought vectors (Kiros et al., 2015), quick-thought (Logeswaran & Lee, 2018), or universal sentence encoder (Cer et al., 2018).\nA priori, vectorizers are not designed to be robust to small changes. Even when modifying a single word of the input document, the embedding could change drastically. Thus, we ask the following question:\nAre text vectorizers provably robust with respect to modifying a small subset of the document?\nar X\niv :2\n30 3.\n07 20\n3v 2\n[ cs\n.C L\n] 1\n2 Ju\nn 20\n23\nTypical notions of robustness in machine learning deals with continuous input data: changing slightly the observation means that for instance its \u21132-norm evolves infinitesimally. The challenge of our analysis is the fundamentally discrete nature of text data. Changing a word in a document is usually not innocuous \u2013 one can think of extreme cases where the meaning of this word is flipped \u2013 and vectorizers sensitive to the semantics of input documents should capture this phenomenon. Nevertheless, we show that the answer is positive for all vectorizers that we study. Another difficulty is that the mathematical formalization of some of these vectorizers was not the main concern of the community. A necessary first step is thus to give an unequivocal definition of our objects of interest.\nContributions. In this paper, we analyze the robustness of vectorizers as their local regularity (Lipschitz, Ho\u0308lder) with respect to the Hamming distance (Section 2). We prove: \u2022 the 1/2-Ho\u0308lder continuity of concatenation of token and positional embeddings (Proposition 3.1); \u2022 the Lipschitz continuity of TF-IDF (Proposition 4.1), and the 1/2-Ho\u0308lder continuity of it normalized variant (Proposition 4.2); \u2022 the Lipschitz continuity of doc2vec (Theorem 5.1). As a necessary step to derive the latter, we make two new mathematical contributions (see Appendix), we propose: \u2022 a local Lipschitz analysis of the softmax (Theorem H.6); \u2022 a Gro\u0308nwall\u2013Bellman\u2013Bahouri result (Theorem G.1) needed when casting the doc2vec analysis as an ODE problem. The code for all experiments of the paper is available at https://github.com/dgarreau/ vectorizer-robustness.\nRelated work. (Adversarial examples). A major motivation for studying robustness is its impact on the existence of adversarial examples. In the case of DNNs, robustness often means Lipschitz continuity with respect to the inputs. For instance, one can show that a network having a small Lipschitz constant prevents the existence of small adversarial changes. More precisely, Hein & Andriushchenko (2017) provide a lower bound on the norm of the input manipulation needed to change the classifier decision inversely proportional to the Lipschitz constant of the network. This was later extended by Weng et al. (2018b) to DNNs with ReLU activations. Quantitatively, Weng et al. (2018a) show that fully connected layers have a Lipschitz constant potentially as large as the operator norm of the weight matrix. From a practical point of view, it has also been noticed that enforcing the Lipschitz constants of the layers to remain low does improve the robustness (Cisse et al., 2017). (Generalization & interpolation). It is known that robust algorithms generalize better. In particular, Xu & Mannor (2012) derive generalization bounds for generic algorithms depending in their robustness. The definition of robustness\nhere includes Lipschitz continuous DNNs. More recently, Bubeck & Sellke (2021) extending (Bubeck et al., 2020) showed that in order to train Lipschitz continuous models, one has to take a large number of parameters. (Theory of vectorizers). Surprisingly, the robustness of vectorizers received little attention until now on the theoretical side, and all previous works on robustness assume continuous input. Nevertheless, there exist some theoretical works on similar problems. Most notably, Arora et al. (2016) analyze a large class of word vectorizers and explain how the intriguing alignment properties observed experimentally appear.\nNotations. For u \u2208 Rp, we denote by \u2225u\u2225 its Euclidean norm. Let g : R\u00d7 Rd \u2192 R be a function. The derivative in the time variable (\u00b5) is denoted by \u2202\u00b5g whereas \u2207g (resp. \u22072g) denotes the Jacobian (resp. the Hessian) of g in the space variable. We let 1 = (1, . . . , 1)\u22a4 \u2208 Rd. For a matrix R, \u03c3min(R) is its smallest singular value. For a given set S , |S| is its cardinal."
        },
        {
            "heading": "2. Framework",
            "text": "Let us now present the mathematical framework in which we perform our analysis. We consider tokens from a finite dictionary D, identified as [D] := {1, . . . , D}. A document x built on D is a finite sequence of elements of D, and we write [D]\u2217 for the set of all documents. Thus the central object of our work, a vectorizer, is simply a mapping \u03c6 : [D]\u2217 \u2192 Rd, where d is the dimension of the embedding. The length of x will be denoted by T (x), and therefore x can be written as (x1, . . . , xT (x)). The set of all documents over D of length T will be denoted [D]T \u2282 [D]\u2217. When there is no ambiguity, we remove the dependency in x from our notation, e.g., T (x) becomes T .\nAs discussed in the related work, robustness is often synonym with Lipschitz continuity of the model \u2013 distance between outputs lies within a constant factor of the distance between inputs. As distance between input documents x and x\u0303 of same length, we consider the Hamming distance, which is the number of indices such that xt and x\u0303t differ:\ndH(x, x\u0303) := |{t \u2208 [T ] : xt \u0338= x\u0303t}| .\nThe distance between outputs will simply be measured by the Euclidean norm in Rd. In definitive, for a given document length T , what we call Lipschitz continuity of the vectorizer \u03c6 can be written as\n\u2200x, x\u0303 \u2208 [D]T , \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 \u2264 C dH(x, x\u0303) , (1)\nwhere C is called the Lipschitz constant. Another way to quantify robustness is to allow for an exponent in Eq. (1):\n\u2200x, x\u0303 \u2208 [D]T , \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 \u2264 C dH(x, x\u0303)\u03b2 , (2)\nwith 1 \u2265 \u03b2 > 0. This is known as Ho\u0308lder continuity, and coincides with Lipschitz continuity whenever \u03b2 = 1. While it is known that Lipschitz continuity implies Ho\u0308lder continuity on the real line when \u03b2 \u2264 1, this is not the case here, since dH takes values in N. Thus in our setting, Lipschitz continuity is a weaker notion of robustness than Ho\u0308lder continuity.\nOften we obtain more precise results, depending explicitly on the set of indices such that the documents differ. To this extent, for a given subset S of [T ], we define the set of S-close documents BS(x) of x \u2208 [D]T as\nBS(x) = {x\u0303 \u2208 [D]T : xi = x\u0303i for i \u0338\u2208 S} .\nSaid alternatively, x\u0303 \u2208 BS(x) if it is obtained by replacing the tokens of x with indices belonging to S by arbitrary tokens in D. We note thatBS(x) is a subset of the Hamming ball of radius |S|. Let us consider for instance the document x = \u201cthe quick brown fox\u201d and the set of perturbed indices S = {2, 3} Here, x has length T = 4, |S| = 2, and an element of BS(x) is the document x\u0303 = \u201cthe slow blue fox.\u201d"
        },
        {
            "heading": "3. Warm-up: concatenation",
            "text": "Concatenation embeddings generally proceed by first mapping each token xt of x to a vector u(xt, t) \u2208 Rd. In a second step, these vector representations are concatenated together to form \u03c6(x). We assume that the representation u(xt, t) can be written as\nu(xt, t) = [ue(xt);up(t)] \u2208 Rd , (3)\nwhere ue \u2208 Rde denotes vector representations of individual tokens, while up \u2208 Rdp encodes positional information, and we define d := de + dp.\nToken embeddings. As noted in the introduction, there are essentially two widespread choices for ue: either use sparse representations for individual tokens or use dense representations. The first approach is often synonymous with the use of one-hot encodings, hence considering the mapping ue : j 7\u2192 1j as a building brick, where, for any j \u2208 D, we define 1j the j-th vector of the canonical basis of RD. This has the advantage of simplicity. One caveat is that, although sparse, one-hot vectors have dimensionality de = D\u2014the size of the dictionary. Regarding dense embeddings, as discussed in the introduction, the mapping j 7\u2192 ue(j) is learned from data and can encompass some semantic properties. In all these examples, ue(j) typically has dimensionality de \u226a D (for instance, gensim takes de = 100 in its word2vec implementation).\nPositional embeddings. A common choice is to learn positional embeddings, jointly with token embeddings. It is also possible to use deterministic positional embeddings,\nsuch as one-hot vectors \u2014 up(t) = 1t \u2208 RTmax , where Tmax is a maximal document size, or more complicated functions of t. For instance, the original transformers architecture uses a sinusoidal transformation of t as positional embedding (Vaswani et al., 2017). Further, it is also possible to incorporate additional positional information in the embedding \u2013 for instance BERT incorporates segment position information corresponding to the index of the sentence the token belongs to (Devlin et al., 2018, Figure 2). Finally, one can simply ignore up altogether, relying simply on the order of the u(xt) to convey the positional information. Let us note that when de = dp, one can add ue and up in Eq. (3) instead of concatenating them, a possibility to which our analysis is robust.\nConcatenation. For a given u, the embedding \u03c6(x) of a document x is formed by concatenating the u(xt, t)s for t \u2208 [T ]. Formally, if T \u2265 Tmax, then the concatenation \u03c6(x) of (x1, . . . , xT ) is defined as\n\u03c6(x) := [u(x1, 1); . . . ;u(xTmax , Tmax)] \u2208 RdTmax , and if T < Tmax, as (zero-padding),\n\u03c6(x) := [u(x1, 1); . . . ;u(xT , T ); 0; . . . ; 0] \u2208 RdTmax . Since the embedding is explicit in this case, it is straightforward to show the following: Proposition 3.1 (Robustness of concatenation). Let x \u2208 [D]T , S \u2286 [T ], and x\u0303 \u2208 BS(x). Then\n\u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 \u2264 max j \u0338=k\n\u2225ue(j)\u2212 ue(k)\u2225 \u00b7 \u221a |S| \u2227 Tmax .\nIn particular, for small perturbation of the input document, concatenation is 1/2-Ho\u0308lder with respect to the Hamming distance. Closer inspection of the proof reveals that the constant depends only on the perturbed tokens: if the changes made are close from the point of view of ue, then \u03c6(x) and \u03c6(x\u0303) remain close."
        },
        {
            "heading": "4. TF-IDF transform",
            "text": "Let x be a document of length T built on D. In this section, we will assume that tokens correspond to individual words. Forgetting the sequential nature of natural language, one can simply look at the words appearing in x with repetitions \u2013 this is informally called a bag-of-words representation. Any given word j \u2208 D appears in this representation with multiplicity mj(x). The TF-IDF transform of x is a vector \u03c6(x) \u2208 RD, with each coordinate of \u03c6(x) corresponding to a word of the dictionary. Component-wise, \u03c6(x) is a product of two terms: the term frequency fj and the inverse document frequency vj :\n\u2200j \u2208 D, { fj := mj T ,\nvj := log |C| |{z\u2208C s.t. j\u2208z}| , (4)\nwhere C is a set of documents. We will assume that vj > 0. The exact expressions appearing in Eq. (4) can vary depending on implementation, we use here the most common definitions (in particular, they are the default choices used by scikit-learn (Pedregosa et al., 2011)). The (nonnormalized) TF-IDF of x can be written \u03c6(x)j = fjvj for all j \u2208 D. Intuitively, one wants to quantify the importance of each word in the document, while ignoring common words appearing in many documents such as articles. Finally, it is common to normalize \u03c6(x), generally using the Euclidean norm. We denote by \u03d5(x) := \u03c6(x)/ \u2225\u03c6(x)\u2225 the normalized TF-IDF of x."
        },
        {
            "heading": "4.1. Robustness results",
            "text": "As we saw in the previous section, the TF-IDF transform of a given document can be given in closed-form as a function of the word multiplicities and the given coefficients. This allows a simple analysis, at least in the non-normalized case. Proposition 4.1 (Robustness of non-normalized TF-IDF). Let x \u2208 [D]T , S \u2286 [T ], and x\u0303 \u2208 BS(x). Let mmax be the maximal word multiplicity in x and vmax be the maximal inverse document frequency over D. Then\n\u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 \u2264 4mmaxvmax |S| T .\nIn other words, non-normalized TF-IDF is Lipschitz continuous for the Hamming distance, with Lipschitz constant inversely proportional to the common length of the documents. In reality, the dependency in T is slightly more complicated since nothing prevents mmax from being as large as T in pathological cases (when all the words of the document are identical). In any case, we uncover a satisfying fact about TF-IDF: small changes in long documents do not matter much. Taking into account the normalization, we have a similar result: Proposition 4.2 (Robustness of normalized TF-IDF). Let x \u2208 [D]T . Let vmin be the minimal inverse document frequency associated to the words of x. Let S \u2286 [T ] such that |S| \u2264 \u2225\u03c6(x)\u2225 /(4mmaxvmax) and x\u0303 \u2208 BS(x). Then\n\u2225\u03d5(x)\u2212 \u03d5(x\u0303)\u2225 \u2264 4m 1/2 maxv 1/2 maxD1/4\nv 1/2 min\n\u221a |S| T .\nIn plain words, normalized TF-IDF is 1/2-Ho\u0308lder with respect to the Hamming distance. Again, the constant appearing decreases with the length of the base document. A close inspection of the proof also reveals that the D is actually equal to D(x), the size of the local dictionary."
        },
        {
            "heading": "4.2. Experimental validation",
            "text": "In order to check the accuracy of Proposition 4.2, we ran some numerical experiments. We considered movie reviews\nfrom the IMDB dataset as documents and the TF-IDF implementation from scikit-learn with L2 normalization.\nInfluence of the document length. In a first set of experiments, we investigated the behavior of \u2225\u03d5(x)\u2212 \u03d5(x\u0303)\u2225 with respect to the length T of x. To this extent, for several documents, we created a sequence of growing documents by considering the first t words of the documents, with t ranging from 5 to T . For each value of t, we replaced 5 words in the intermediary document and repeated this experiment several time. The words to replace were chosen uniformly at random in the document, and the replacements uniformly at random in D, and we estimated the supremum of \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 by taking the maximum over these repetitions. Proposition 4.2 predicts that, since |S| is kept constant here, the supremum of \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 over all possible replacements should be upper bounded by 1/ \u221a T (up to numerical constants). This appears to be empirically true (see Figure 1).\nInfluence of the number of removals. In a second set of experiments, we looked at the dependency of \u2225\u03d5(x)\u2212 \u03d5(x\u0303)\u2225 with respect to |S|. This time keeping x fixed, we gradually increased the number of replaced words from 1 to T . Since T is fixed, Proposition 4.2 predicts that the supremum of \u2225\u03d5(x)\u2212 \u03d5(x\u0303)\u2225 over all possible replacements should behave at most as \u221a |S|. This also appears to be empirically true, see Figure 2.\n5. Paragraph Vector (doc2vec) We now turn to the most challenging part of our analysis, doc2vec. On a high level, a token embedding matrix is learned jointly with a document embedding matrix on a corpus, aiming to predict correctly a missing token in a\ngiven context. The key difference with other vectorizers is that, at inference time, another minimization problem is solved by the model. Different documents yield different optimization problems, and therefore it is quite challenging to see where the resulting minimizer is located with respect to the original embedding.\n5.1. A primer on doc2vec\nThe key idea underlying paragraph vector is neural probabilistic language modeling (Bengio et al., 2000): predict words of a document knowing (i) the context of the missing word in the document, and (ii) some global information about the document, encoded as a vector q \u2208 Rd. Thus the key concept is the probability of observing word j at position t given some context c(t) and vector q. This is written informally as P (j|c(t), q), and we describe its exact formulation in the next paragraphs. Two models are proposed in Le & Mikolov (2014): distributed memory (PVDM) model, similar to the continuous bag of words model of Mikolov et al. (2013a), and distributed bag of words (PVDBOW) model, similar to the skip gram model. We first focus on the PVDM model, PVDBOW being a simplified version thereof, referring to Figure 3 for a visual help.\nLocal information. For a document x with length T , for any \u03bd < t < T \u2212 \u03bd, we define the neighborhood of t as\n\u03b3(t) := (t\u2212 \u03bd, . . . , t\u2212 1, t+ 1, . . . , t+ \u03bd) . (5)\nHere, \u03bd is an hyperparameter often called context size (or window size), quantifying the breath of the context considered by the model. To this neighborhood corresponds the context\nc(t) := (xt\u2212\u03bd , . . . , xt\u22121, xt+1, . . . , xt+\u03bd) . (6)\nif concatenation is used (see bottom layer of Figure 3).\nProjecting and lifting. This local information is then projected into Rd, with d\u226a D, the embedding space. At this stage, the document vector q \u2208 Rd is added to the local representation. This intermediary representation is lifted back to RD. PVDM relies on two matrices P and R such that each context is mapped to\nyt := R(Pht + q) = \u03c0t +Rq \u2208 RD ,\nwhere \u03c0t := RPht \u2208 RD. Here, P has size d \u00d7 D for PVDMmean, and d\u00d7 2\u03bdD for PVDMconcat, while R has size D \u00d7 d. When tokens are words, the columns of P are called word vectors, since they correspond to d dimensional embeddings for individual words. We refer to the intermediate layers of Figure 3 for a visual help.\nPrediction. Finally, the prediction for xt is encoded as the softmax of yt, where the softmax \u03c3 : RD \u2192 RD is defined for u \u2208 RD as\n\u03c3(u) = ( euj\u2211D k=1 e uk ) 1\u2264j\u2264D . (7)\nIn particular, all components of \u03c3(yt) lie between 0 and 1 and sum to one, and reading coordinate j of \u03c3(yt) can be interpreted as reading the predicted probability of token j. To summarize, \u03c3(yt) encodes a discrete distribution over D that depends on the context of xt and the document vector q (topmost layer of Figure 3).\nTraining. Let us call x(1), . . . , x(N) the documents in our training set, with lengths T1, . . . , TN . To each of these documents correspond an embedding q(i) \u2208 Rd, which can be seen as the columns of a matrixQ \u2208 Rd\u00d7N , each giving rise to y(i)t . The columns of Q are often referred to as document vectors. The key idea here is to learn P,Q, andR so that the predicted tokens at position t are accurate for all documents. Seeing \u03c3(y(i)t ) as a discrete probability distribution on D,\na natural way to compare it to the groundtruth (x(i)t ) is to compute the cross-entropy between the distribution putting mass one at x(i)t and \u03c3(y (i) t ), that is,\n\u2113 (i) t := \u2212 log \u03c3(y(i)t )x(i)t := \u03c8x(i)t (y (i) t ) ,\nwhere we defined \u03c8 := \u2212 log \u03c3 coordinate-wise. The optimization problem solved by PV is written\nMinimize P,Q,R N\u2211 i=1 1 Ti \u2211 t\u2208x(i) \u03c8 x (t) t (y (i) t ) , (8)\nwhere t \u2208 x(i) means t ranging from \u03bd + 1 to Ti \u2212 \u03bd \u2212 1. Problem (8) is solved by stochastic gradient descent, or ADAM (Kingma & Ba, 2015).\nInference. Let us describe the embedding of a new document x, assuming that the model was trained on a corpus. The way inference works for the PV model is to keep P and R fixed, and to optimize solely in q \u2208 Rd\nMinimize q\u2208Rd\n1\nT \u2211 t\u2208x \u03c8xt(yt) . (9)\nAn important observation is that q 7\u2192 \u03c8xt(\u03c0t + Rq) is a convex function, although not strictly (see Appendix). Therefore, a regularization term is often added to Eq. (9), a point which we will clarify in the next section. Also noting that q has only d parameters, solving PV inference (9) efficiently is not too challenging.\nThe case of PVDBOW. PVDBOW is another model falling under the PV umbrella. In a nutshell, following the idea of the distributed bag of word model, PVDBOW\nworks the other way around and uses only the representation of the document to predict tokens. At position t, no local information is taken into account and we put \u03c0t = 0 in that case. The predicted token distribution for the document is encoded as before (as \u03c3(yt) = \u03c3(Rq)), and its quality also measured as \u03c8xt(yt) for all tokens in the document, leading to the same optimization problems. To summarize, PVDBOW is a simplified, lightweight version of PVDM, simply obtained by taking \u03c0t = 0 in our framework. In particular, there is no matrix P , which leads to fewer parameters, and thus easier training and inference, a fact which was pointed out by Le & Mikolov (2014). Nevertheless, they recognize that PVDBOW still performs well as an embedding, and recommend considering as an embedding the concatenation of PVDM and PVDBOW.\nHierarchical softmax and negative sampling. In practice, as advocated by Le & Mikolov (2014), two additional expedients are used. First, the softmax is replaced by hierarchical softmax (Morin & Bengio, 2005). In a nutshell, each call of \u03c3 has a computational cost linear in D, which can be as large as 105 in practice. A solution is to replace the softmax by a tree-based approximation thereof, which computation is much faster. Second, following Mikolov et al. (2013a), it is common to incorporate tokens with a negative association to the token to predict when computing \u2113t, leading to faster training. These two possibilities are non-trivial modifications to the PV model and we do not consider them in our analysis."
        },
        {
            "heading": "5.2. Robustness result",
            "text": "Before stating our robustness result, let us explain why it is challenging and outline the proof technique. As detailed\nin the previous section, the embedding of a document x of length T is found by solving\nq0 = argmin q\u2208Rd\n{ F (q) + \u03b1\n2 \u2225q\u22252\n} , (10)\nwhere F (q) := 1T \u2211\nt\u2208x \u03c8xt(\u03c0t +Rq). The regularization term \u03b1 \u2225q\u22252 /2 with \u03b1 > 0 ensures uniqueness of the solution. Indeed, the softmax is invariant by translation by a vector proportional to 1, and solutions to (9) are not unique. As before, consider x\u0303, a modified version of x where tokens with indices in S have been replaced by others. The embedding q1 of x\u0303 is found by solving\nq1 = argmin t\u2208x\n{ G(q) + \u03b1\n2 \u2225q\u22252\n} , (11)\nwhere G(q) := 1T \u2211\nt\u2208x \u03c8x\u0303t(\u03c0\u0303t + Rq), and \u03c0\u0303t is defined analogously to \u03c0t. The main challenge here is that q0 and q1 are solutions of distinct optimization problems, which can be quite different if |S| is large.\nFrom discrete to continuous. The solution we propose to connect between these two problems is to interpolate smoothly between them. There are many ways to do this, and we settle for the simplest: linear interpolation. More precisely, we define for all \u00b5 \u2208 [0, 1] and q \u2208 Rd by\n\u03a8lin(\u00b5, q) := (1\u2212 \u00b5)F (q) + \u00b5G(q) . (12)\nSubsequently, for all \u00b5 \u2208 [0, 1], we can solve the following regularized optimization problem:\nq(\u00b5) := argmin q\u2208Rd\n{ \u03a8lin(\u00b5, q) + \u03b1\n2 \u2225q\u22252\n} , (13)\ngiving rise to a continuous trajectory in the embedding space (see Figure 4 for an illustration). One can think of q(\u00b5) as the embedding of a fictitious document traveling halfway between x and x\u0303 as \u00b5 ranges from 0 to 1.\nDynamics of interpolation. This approach is powerful, since it allows us to transform a problem which is discrete in nature (elements of a sum are modified) to a continuous one (time parameter varies). In particular, the dynamics of \u00b5 7\u2192 q(\u00b5) are described by an ordinary differential equation (ODE). Indeed, for all \u00b5 \u2208 [0, 1], since q 7\u2192 \u03a8lin(\u00b5, q) + \u03b12 \u2225q\u2225\n2 is a strongly convex function, q(\u00b5) is the (unique) critical point of q \u2192 \u2207\u03a8lin(\u00b5, q) + \u03b1q, where \u2207 denotes derivative with respect to the space coordinate (q). That is, for all \u00b5 \u2208 [0, 1],\n\u2207\u03a8lin(\u00b5, q(\u00b5)) + \u03b1q(\u00b5) = 0 .\nDifferentiating, we get that for all \u00b5 \u2208 [0, 1],( \u22072\u03a8lin(\u00b5, q(\u00b5)) + \u03b1 I ) q\u2032(\u00b5) + \u2202\u00b5\u2207\u03a8lin(\u00b5, q(\u00b5)) = 0 ,\n(14)\nwhere g\u2032 denotes derivative with respect to the time coordinate (\u00b5) and I the identity matrix. Let us set\n\u03a6lin(\u00b5, q) := \u2212 ( \u22072\u03a8lin(\u00b5, q) + \u03b1 I )\u22121 \u2202\u00b5\u2207\u03a8lin(\u00b5, q) .\n(15) Then, Eq. (14) can be rewritten as q\u2032(\u00b5) = \u03a6lin(q(\u00b5), \u00b5).\nSpectrum of the Hessian of the log-softmax. Looking back at the ODE problem, it appears that one needs to understand precisely the behavior of \u03a6lin. Intuitively, an ill-behaved function could lead to the explosion of the solution of the ODE, preventing the existence of reasonable bounds on \u2225q(\u00b5)\u2212 q(0)\u2225 for large \u00b5. This understanding relies on the control of the smallest positive eigenvalue of \u22072\u03a8lin, \u03bb1(\u00b5, q). Coming back to the definition of \u03a8lin (Eq. (12)), F , and G, we see that \u03bb1 closely related to \u03bbmin, the smallest positive eigenvalue of the Hessian of the logsoftmax, for which we have precise results (Lemma H.5 and Theorem H.9).\nGro\u0308nwall-type result. Once that a precise control is achieved on \u03a6lin, one may have hoped to use standard Gro\u0308nwall type inequalities such as Pachpatte (2004) to obtain quantitative bounds on \u2225q(1)\u2212 q(0)\u2225. However, in our setting, the growth of \u03a6lin prevents us from getting explicit bounds and we had to prove a new result (Theorem G.1) which is actually true in a more general setting than that of doc2vec. Specifying this result, we get:\nTheorem 5.1 (Bounded trajectories). Let x \u2208 [D]T , S \u2286 [T ], and x\u0303 \u2208 BS(x). Suppose that R \u2208 RD\u00d7d is such that \u03c3min(R) > 0 and Im(R) \u2282 1\u22a5. Let \u00b5 7\u2192 q(\u00b5) be the solution of ODE (14). Then, there exist two constants c = c(\u03b1) > 0 and L = L(\u2225q(0)\u2225) > 0 depending explicitly on P,R, \u03bd, and D such that, whenever |S| /T \u2264 c,\nsup \u00b5\u2208[0,1] \u2225q(\u00b5)\u2212 q(0)\u2225 \u2264 L |S| T .\nSince \u03c6(x) = q(0) and \u03c6(x\u0303) = q(1), a corollary of Theorem 5.1 is that the doc2vec embedding is Lipschitz continuous with respect to the Hamming distance, with Lipschitz constant at most inversely proportional to the document lengTheorem Coming back to our initial question, Theorem 5.1 guarantees that, for documents of reasonable length and small perturbations, doc2vec embeddings can not vary too greatly. We emphasize that Theorem 5.1 is true for all three doc2vec models.\nThe key assumption here is that |S| is small enough. We argue that it is only natural to ask so: indeed, if one is allowed to modify every single token of x, this yield a completely different document (although having the same length), which could a priori be embedded anywhere. The other main assumptions concern the matrix R. Experimentally, we observe that \u03c3min(R) > 0 holds (see Section I.3). Requiring that Im(R) \u2282 1\u22a5 is not too restricting: because\nof the translation invariance by 1 of the softmax, one can always normalizeR by removing the average line from each line. The main limitation of Theorem 5.1 is the dependency of c and L in the problems parameters. Exact expression can be found in Appendix (Theorem F.7)."
        },
        {
            "heading": "5.3. Experimental validation",
            "text": "In order to verify the validity of Theorem 5.1, we ran similar experiments to those presented in Section 4. We considered again movie reviews from the IMDB dataset. As vectorizer, we trained doc2vecmodels from scratch on a subset of the IMDB dataset (103 reviews). The associated dictionary has size D = 18, 416: we took tokens as words of the English dictionary. Note that one can also consider sub-word tokens, but in that case replacing a word in the document usually implies replacing several tokens. We chose d = 50 as dimension of the embedding. We took \u03bd = 5 as context size parameter.\nWe present results of experiments regarding the influence of the document length in Figure 5. Theorem 5.1 predicts that, since |S| is kept constant here, the supremum of \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 over all replacements should be upper bounded by 1/T (up to numerical constants). This appears to be empirically true.\nWe present results of experiments regarding the influence\nof the number of replaced words in Figure 6. Here we took the number of replaced words from \u03bd + 1 to T \u2212 \u03bd \u2212 1 to avoid for border effects. Since T is fixed, Theorem 5.1 predicts that the supremum of \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u2225 over all possible replacements should behave at most linearly in |S|. This appears to be empirically true.\nWe present in Appendix (Section I) additional results with another implementation, gensim (R\u030cehu\u030ar\u030cek & Sojka, 2010). In particular, this implementation uses hierarchical softmax. The results are consistent with the behavior presented here."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this paper, we proved that several popular text vectorizers are robust, in the sense that they are either Lipschitz or Ho\u0308lder continuous with respect to the Hamming distance. Proving this robustness was possible for concatenation and TF-IDF thanks to elementary computations, but required a much more challenging mathematical analysis for doc2vec requiring two new results (local Lipschitz continuity of the softmax and a new Gro\u0308nwall\u2013Bellman\u2013Bahouri non-explosion lemma).\nLet us outline future research directions. First, we studied the robustness of the true solution of (8) and (9). In practice, this problem is solved thanks to gradient descent, and it would be interesting to measure the impact of this approximation. A second line of work would consist in obtaining refined results when we put a random model on the distribution of the words of the document, similarly to what is done in (Arora et al., 2016)."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported by the French government under the management of the Agence Nationale de la Recherche, grant agreements GraVa ANR-18-CE40-0005, NEMATIC ANR-21-CE45-0010, and NIM-ML ANR-21-CE23-000501. D.G. acknowledges the support of EU Horizon 2020 project AI4Media (contract no. 951911) and would like to thank Charbel Yachouchi for his preliminary work. S.V. would like to thank Nicolas Patry for fruitful discussion about NLP embeddings."
        },
        {
            "heading": "A. General organization",
            "text": "This Appendix is organized as follows: in Section B (resp. C) we collect the missing proofs for Section 3 (resp. 4) of the main paper.\nThe next five sections are dedicated to the proof of Theorem 5.1: First, in Section D, we formally prove that the dynamics of the interpolation scheme between two minimizers follow an ordinary differential equation (ODE). We actually show a more general result and provide technical conditions on the interpolation \u03a8 under which we are able to formulate the interpolation between minimization problems as an ODE. Next, in Section E, we derive quantitative bounds for the solution of this ODE. We show how to specialize this result in the doc2vec setting in Section F, proving Theorem 5.1 in the process. The main tool used to obtain these bounds is a general Gro\u0308nwall-Bellman-Bahouri type result for ODE with exponentially-growing coefficients. This result (Theorem G.1), as well as all other technical results concerning ODEs, is stated and proved in Section G . In order to specialize our result to the doc2vec setting, we needed a fine-grained study of the (log-)softmax function. In particular, we derive a new bound on the softmax function (Theorem H.9), which is proved in Section H.\nWe conclude this Appendix with additional experimental results supporting our claims in Section I."
        },
        {
            "heading": "B. Omitted proofs for concatenation",
            "text": "B.1. Proof of Proposition 3.1\nBy definition of \u03c6 and Pythagoras theorem,\n\u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u22252 = \u2211\nt\u2208S\u2229[Tmax] \u2225u(xt, t)\u2212 u(x\u0303t, t)\u22252 .\nBy definition of u (Eq. (3)), one has\nu(xt, t)\u2212 u(x\u0303t, t) = [ue(xt)\u2212 u(x\u0303t); 0] , (16)\nand therefore \u2225u(xt, t)\u2212 u(x\u0303t, t)\u22252 = \u2225ue(xt)\u2212 u(x\u0303t)\u22252 .\nWe deduce that \u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u22252 \u2264 |S \u2229 [Tmax]| \u00b7max\nj \u0338=k \u2225ue(j)\u2212 ue(k)\u22252 .\nRemark B.1 (Concatenation v.s. sum). Replacing the concatenation by a sum in the definition of u (Eq. (3)) does not change the proof. Indeed, the key step Eq. (16) remains unchanged in that case: the key idea here is that position tokens are the same for words in the same position, and cancel out when forming the difference."
        },
        {
            "heading": "C. Omitted proofs for TF-IDF vectorization",
            "text": "C.1. Proof of Proposition 4.1\nBy definition, we can write\n\u03c6(x) = D\u2211 j=1 fjvj1j = 1 T D\u2211 j=1 mjvj1j .\nSimilarly, since x\u0303 has same length as x,\n\u03c6(x\u0303) = 1\nT D\u2211 j=1 m\u0303jvj1j ,\nwhere we let m\u0303j denote the multiplicity of word j in document x\u0303. We deduce that\n\u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u22252 = 1 T 2 D\u2211 j=1 (mj \u2212 m\u0303j)2v2j .\nBy letting vmax be the maximal inverse document frequency on D, we already see that\n\u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u22252 \u2264 v 2 max\nT 2 D\u2211 j=1 (mj \u2212 m\u0303j)2 .\nIn the previous display, only terms such that mj \u0338= m\u0303j count. Using the inequality between p-norms, we have\n\u2211 mj \u0338=m\u0303j (mj \u2212 m\u0303j)2 \u2264  \u2211 mj \u0338=m\u0303j |mj \u2212 m\u0303j | 2 . Now, by the triangle inequality, \u2211\nmj \u0338=m\u0303j |mj \u2212 m\u0303j | \u2264 \u2211 mj \u0338=m\u0303j mj + \u2211 mj \u0338=m\u0303j m\u0303j .\nWe notice that these two sums are equal: every removed word has to appear somewhere. Moreover, |{j s.t. mj \u0338= m\u0303j | \u2264 2 |S|, since modifying one word changes at most two multiplicities, and this happens at most |S| times. Therefore, we have proved that \u2211\nmj \u0338=m\u0303j |mj \u2212 m\u0303j | \u2264 4mmax |S| , (17)\nwhere we recall that mmax is the maximal multiplicity of words of x. Backtracking, we have\n\u2225\u03c6(x)\u2212 \u03c6(x\u0303)\u22252 \u2264 v 2 max\nT 2 \u00b7 16m2max |S|2 ,\nand we can conclude by simply taking the square root of this last display.\nC.2. Proof of Proposition 4.2\nWe notice that\n\u2225\u03d5(x)\u2212 \u03d5(x\u0303)\u22252 = 1 + 1\u2212 2\u03d5(x)\u22a4\u03d5(x\u0303) = 2\u2212 2 \u03c6(x) \u22a4\u03c6(x\u0303)\n\u2225\u03c6(x)\u2225 \u2225\u03c6(x\u0303)\u2225 . (18)\nIn this last term we recognize the cosine similarity between \u03c6(x) and \u03c6(x\u0303). Since we are working under the assumptions of Lemma C.1, we have\n\u03c6(x)\u22a4\u03c6(x\u0303) \u2225\u03c6(x)\u2225 \u2225\u03c6(x\u0303)\u2225 \u2265 1\u2212 8mmaxvmax |S| \u2225\u03c6(x)\u2225 .\nComing back to Eq. (18), we see that\n\u2225\u03d5(x)\u2212 \u03d5(x\u0303)\u22252 \u2264 16mmaxvmax |S|\u2225\u03c6(x)\u2225 .\nWe conclude by using Lemma C.2 and taking the square root.\nC.3. Auxilliary results\nWe have the following result, key to the proof of Prop. 4.2, and of independent interest:\nLemma C.1 (Cosine similarity robustness). Let x be a document. Let S \u2286 [T ] such that |S| \u2264 \u2225\u03c6(x)\u2225 /(4mmaxvmax) and x\u0303 \u2208 BS(x). Then\n\u03c6(x)\u22a4\u03c6(x\u0303) \u2225\u03c6(x)\u2225 \u2225\u03c6(x\u0303)\u2225 \u2265 1\u2212 8mmaxvmax |S| \u2225\u03c6(x)\u2225 . (19)\nProof. By homogeneity, we can multiply numerator and denominator in Eq. (19) by T and deal with multiplicities instead of frequencies in this proof. We first focus on the numerator and write\n\u03c6(x)\u22a4\u03c6(x\u0303) = \u03c6(x)\u22a4(\u03c6(x) + \u03c6(x\u0303)\u2212 \u03c6(x)) = \u2225\u03c6(x)\u22252 + D\u2211\nj=1\nmj(m\u0303j \u2212mj)v2j , (20)\nby definition of \u03c6. Using Cauchy-Schwarz inequality, we find that\nD\u2211 j=1 mj(mj \u2212 m\u0303j)v2j \u2264 \u221a\u2211 j mjv2j \u221a\u2211 j (mj \u2212 m\u0303j)2v2j .\nIn the first part of the right-hand side we recognize \u2225\u03c6(x)\u2225, and in the second part, the same quantity bounded in the proof of Proposition 4.1. We deduce that \u2211\nj\nmj(mj \u2212 m\u0303j)v2j \u2264 \u2225\u03c6(x)\u2225 \u00b7 4mmaxvmax |S| .\nComing back to Eq. (20), we have proved that\n\u03c6(x)\u22a4\u03c6(x\u0303) \u2265 \u2225\u03c6(x)\u22252 \u2212 4mmaxvmax \u2225\u03c6(x)\u2225 |S| ,\nwhich is positive under our assumption. Let us now look into the denominator of Eq. (19). Using the triangle inequality and Proposition 4.1, we write \u2225\u03c6(x\u0303)\u2225 \u2264 \u2225\u03c6(x)\u2225+ 4mmaxvmax |S| . Putting everything together, we have\n\u03c6(x)\u22a4\u03c6(x\u0303) \u2225\u03c6(x)\u2225 \u2225\u03c6(x\u0303)\u2225 \u2265 \u2225\u03c6(x)\u22252 \u2212 4mmaxvmax \u2225\u03c6(x)\u2225 |S| \u2225\u03c6(x)\u2225 \u00b7 (\u2225\u03c6(x)\u2225+ 4mmaxvmax |S|) = 1\u2212 u 1 + u ,\nwith u := 4mmaxvmax |S| / \u2225\u03c6(x)\u2225. Again, by our assumption, u \u2208 (0, 1). It is straightforward to show that (1\u2212 u)/(1 + u) \u2265 1\u2212 2u for all u \u2208 (0, 1), and we deduce the result.\nWe also have the following:\nLemma C.2 (Lower bound on \u2225\u03c6(x)\u2225). Let x be a document. Let vmin be the minimum inverse document frequency for words contained in x and D(x) the size of the local dictionary. Then\n\u2225\u03c6(x)\u2225 \u2265 Tvmin\u221a D(x) .\nProof. Straightforward from the definitions and the comparison of p-norms."
        },
        {
            "heading": "D. Dynamics of interpolation",
            "text": "Recall that we are considering, for all \u00b5 \u2208 [0, 1], the following minimization problem:\nq(\u00b5) := argmin q\u2208Rd\n{ \u03a8lin(\u00b5, q) + \u03b1\n2 \u2225q\u22252\n} . (21)\nIn this section, we show that under mild regularity assumptions on \u03a8, q is the unique solution of the following ODE:( \u22072\u03a8lin(\u00b5, q(\u00b5)) + \u03b1 I ) q\u2032(\u00b5) + \u2202\u00b5\u2207\u03a8lin(\u00b5, q(\u00b5)) = 0 . (22)\nNotation. For any matrix M \u2208 RA\u00d7B , let us define the operator norm of M as\n\u2225M\u2225op := sup {\u2225Mv\u2225\n\u2225v\u2225 , v \u2208 R B \\ {0}\n} .\nFor any \u03c1 > 0, we also define Bd(\u03c1) the open Euclidean ball of center 0 and radius \u03c1. Finally, for a1, a2 > 0, define a1 \u2228 a2 := max(a1, a2). We can now state the required assumptions on \u03a8.\nAssumption D.1 (Convexity). Let d \u2265 1. We suppose that \u03a8 \u2208 C1,2([0, 1]\u00d7 Rd;R) and that, for all (\u00b5, q) \u2208 [0, 1]\u00d7 Rd, \u22072\u03a8(\u00b5, q) is a positive semi-definite matrix.\nSince \u03b1 > 0, A.D.1 this guarantees that q(\u00b5) is uniquely-defined for each \u00b5. Next, we define some quantities related to the local Lipschitz continuity of \u03a8 and its derivatives.\nDefinition D.2 (Local Lipschitz semi-norms). Let \u03a8 \u2208 C1,2([0, 1]\u00d7 Rd;R). For all \u03c1 > 0, let us define\nL1(\u03c1) := sup \u00b5\u2208[0,1]\nq \u0338=q\u0303\u2208Bd(0,\u03c1)\n\u2225\u2225\u22072\u03a8(\u00b5, q)\u2212\u22072\u03a8(\u00b5, q\u0303)\u2225\u2225 op\n\u2225q \u2212 q\u0303\u2225 , L2(\u03c1) := sup\u00b5\u2208[0,1] q \u0338=q\u0303\u2208Bd(0,\u03c1)\n\u2225\u2202\u00b5\u2207\u03a8(\u00b5, q)\u2212 \u2202\u00b5\u2207\u03a8(\u00b5, q\u0303)\u2225 \u2225q \u2212 q\u0303\u2225 , (23)\nand M(\u03c1) := sup\n\u00b5\u2208[0,1] q\u2208Bd(0,\u03c1)\n\u2225\u2202\u00b5\u2207\u03a8(\u00b5, q)\u2225 . (24)\nOur second assumption on \u03a8 at this stage is that these quantities are all finite.\nAssumption D.3 (Global Lipschitz continuity). Let \u03a8 \u2208 C1,2([0, 1]\u00d7 Rd;R). Suppose that\nsup \u03c1>0\n( L1(\u03c1) + L2(\u03c1) ) < +\u221e and sup\n\u03c1>0 M(\u03c1) < +\u221e,\nwhere L1(\u03c1), L2(\u03c1), and M(\u03c1) are defined in Eq. (23) and Eq. (24).\nIn this setting, we are able to prove the following result:\nTheorem D.4 (Equivalence ODE/minimization problem). Assume that \u03a8 satisfies A.D.1 and A.D.3. Then \u00b5 7\u2192 q(\u00b5) is differentiable on [0, 1], and q is the unique solution of Eq. (22).\nNote that under assumption A.D.1 the matrix \u22072\u03a8(\u00b5, q)+\u03b1 I is invertible. One can then rewrite Eq. (22) in a more standard form, namely\nq\u2032(\u00b5) = \u2212 ( \u22072\u03a8(\u00b5, q(\u00b5)) + \u03b1 I )\u22121 \u2202\u00b5\u2207\u03a8(\u00b5, q(\u00b5)) . (25)\nThus, to study the ODE problem, one needs the regularity properties (local Lipschitz continuity, boundedness...) of the function \u03a6 : (\u00b5, q) \u2208 [0, 1]\u00d7 Rd 7\u2192 \u03a6(\u00b5, q) := \u2212 ( \u22072\u03a8(\u00b5, q) + \u03b1 I )\u22121 \u2202\u00b5\u2207\u03a8(\u00b5, q) . (26)\nThe interplay between \u2202\u00b5\u2207\u03a8 and \u22072\u03a8 here is crucial. Indeed, in Section F we will see that when specified in the doc2vec case, the term in \u2202\u00b5 gives the desired quantity |S| T whereas the term in \u22072\u03a8 has to be handled using precise properties on the softmax function. Theorem D.4 is standard in the ODE literature and holds as soon as the quantities appearing in Eq. (25) are well-behaved. More precisely, this is the case c = 0 of Theorem G.1 in Section G. We now simply check that the assumptions of Theorem G.1 are satisfied in the setting of Theorem D.4. This is achieved by Lemma D.5 and Lemma D.6. We start by a result upper bounding the norm of the inverse Hessian.\nLemma D.5 (Norm of inverse Hessian). Let \u03a8 : [0, 1]\u00d7 Rd \u2192 R. Assume that A.D.1 holds. Then,\n\u2200\u00b5, q \u2208 [0, 1]\u00d7 Rd, \u2225\u2225(\u22072\u03a8(\u00b5, q) + \u03b1 I)\u22121\u2225\u2225\nop \u2264 1 \u03b1 . (27)\nThe proof of Lemma D.5 exploits the fact that \u22072\u03a8 is a non-negative symmetric matrix and can be diagonalized in orthonormal basis with non-negative eigenvalues. The regularization of the minimization problem with the addition of the term \u03b12 \u2225q\u2225 can be translated with the addition of the term \u03b1 I to the previous Hessian matrix, which then becomes a positive definite symmetric matrix. One then only has to estimate the smallest eigenvalue of the matrix to conclude.\nProof. By A.D.1, for all \u00b5 \u2208 [0, 1], q 7\u2192 \u03a8(\u00b5, q) is convex and, for any \u00b5, q \u2208 [0, 1] \u00d7 Rd, \u22072\u03a8(\u00b5, q) is a positive semi-definite matrix with non-negative eigenvalues. From these, N0(\u00b5, q) = Rank ( \u22072\u03a8(\u00b5, q) ) of them are non-zero, and they can be ranked as 0 < \u03bb1(\u00b5, q) \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbN0(\u00b5,q)(\u00b5, q) .\nMoreover, there exists an orthogonal matrix P (\u00b5, q) (meaning that P (\u00b5, q)P (\u00b5, q)\u22a4 = I) such that\nP (\u00b5, q)\u22072\u03a8(\u00b5, q)P (\u00b5, q)\u22a4 = diag(0, . . . , 0, \u03bb1(\u00b5, q), . . . , \u03bbN0(\u00b5, q)) .\nFurthermore since \u22072\u03a8(\u00b5, q) is a symmetric matrix, its range and its kernel are orthogonal complements, Ker ( \u22072\u03a8(\u00b5, q) ) \u2295\u22a5 Im(\u22072\u03a8(\u00b5, q)) = Rd and\nh \u2208 Im(\u22072\u03a8(\u00b5, q)) if, and only if, P (\u00b5, q)h = (0, . . . , 0, h1, \u00b7 \u00b7 \u00b7 , hN0).\nHence P (\u00b5, q) ( \u22072\u03a8(\u00b5, q) + \u03b1 I ) P (\u00b5, q)\u22a4 = diag(\u03b1, . . . , \u03b1, \u03bb1(\u00b5, q) + \u03b1, . . . , \u03bbN0(\u00b5, q) + \u03b1) ,\nwhich implies that \u22072\u03a8(\u00b5, q) + \u03b1 I is an invertible positive definite matrix such that\nP (\u00b5, q) ( \u22072\u03a8(\u00b5, q) + \u03b1 I )\u22121 P (\u00b5, q)\u22a4 = diag\n( 1\n\u03b1 , . . . ,\n1 \u03b1 ,\n1\n\u03bb1(\u00b5, q) + \u03b1 , . . . ,\n1\n\u03bbN0(\u00b5, q) + \u03b1\n) .\nFrom the last display, one readily sees that the maximum eigenvalue of ( \u22072\u03a8(\u00b5, q) + \u03b1 I )\u22121 is 1/\u03b1, proving our claim.\nThe next lemma shows how regularity assumptions on \u03a8 translate into regularity conditions for \u03a6.\nLemma D.6 (Global-Lispchitz continuity of \u03a6). Let \u03a8 such that A.D.1 and A.D.3 hold. Then \u03a6 is globally Lipschitz continuous in q uniformly in \u00b5 \u2208 [0, 1]. Moreover, for all \u03c1 > 0,\nsup \u00b5\u2208[0,1] q \u0338=q\u0303\u2208Rd\n\u2225\u03a6(\u00b5, q)\u2212 \u03a6(\u00b5, q\u0303)\u2225 \u2225q \u2212 q\u0303\u2225 \u2264 1 \u03b1 ( sup \u03c1>0 L2(\u03c1) + ( sup\u03c1>0 L1(\u03c1) )( sup\u03c1>0M(\u03c1) ) \u03b1 ) . (28)\nThe proof of Lemma D.6 relies on the following identity, which is true for any non-negative symmetric matricesA,B \u2208 Rd\u00d7d and vectors X,Y \u2208 Rd:\n(A+ \u03b1 I)\u22121X \u2212 (B + \u03b1 I)\u22121Y = \u2212(A+ \u03b1 I)\u22121(A\u2212B)(B + \u03b1 I)\u22121X + (B + \u03b1 I)\u22121(X \u2212 Y ) . (29)\nLemma D.5 allows us to conclude.\nProof. Let q, q\u0303 \u2208 Bd(0, \u03c1). Using Eq. (29), we have\n\u03a6(\u00b5, q)\u2212 \u03a6(\u00b5, q\u0303) =\u2212 ( (\u22072\u03a8(\u00b5, q) + \u03b1I)\u22121 \u2212 (\u22072\u03a8(\u00b5, q\u0303) + \u03b1I)\u22121 ) \u2202\u00b5\u2207\u03a8(\u00b5, q)\n\u2212 (\u22072\u03a8(\u00b5, q\u0303) + \u03b1I)\u22121 (\u2202\u00b5\u2207\u03a8(\u00b5, q)\u2212 \u2202\u00b5\u2207\u03a8(\u00b5, q\u0303)) =\u2212 (\u22072\u03a8(\u00b5, q) + \u03b1I)\u22121 ( \u22072\u03a8(\u00b5, q\u0303)\u2212\u22072\u03a8(\u00b5, q) ) (\u22072\u03a8(\u00b5, q\u0303) + \u03b1I)\u22121\u2202\u00b5\u2207\u03a8(\u00b5, q) (30)\n\u2212 (\u22072\u03a8(\u00b5, q\u0303) + \u03b1I)\u22121 (\u2202\u00b5\u2207\u03a8(\u00b5, q)\u2212 \u2202\u00b5\u2207\u03a8(\u00b5, q\u0303)) . (31)\nTaking the norm and using Lemma D.5 (in particular Inequality (27)), we have for \u03c1 = \u2225q\u2225 \u2228 \u2225q\u0303\u2225,\n\u2225\u03a6(\u00b5, q)\u2212 \u03a6(\u00b5, q\u0303)\u2225 \u2264 1 \u03b12 \u2225\u2225\u22072\u03a8(\u00b5, q)\u2212\u22072\u03a8(\u00b5, q\u0303)\u2225\u2225 op \u2225\u2202\u00b5\u2207\u03a8(\u00b5, q)\u2225\n+ 1\n\u03b1 \u2225\u2202\u00b5\u2207\u03a8(\u00b5, q)\u2212 \u2202\u00b5\u2207\u03a8(\u00b5, q\u0303)\u2225\n\u2225\u03a6(\u00b5, q)\u2212 \u03a6(\u00b5, q\u0303)\u2225 \u2264 1 \u03b1\n( L1(\u03c1)M(\u03c1)\n\u03b1 + L2(\u03c1)\n) \u2225q \u2212 q\u0303\u2225 .\nTaking the supremum for \u00b5 \u2208 [0, 1], q \u0338= q\u0303 belonging to Bd(0, \u03c1) and \u03c1 > 0 yields the claim.\nWe now have all the tools to prove Theorem D.4.\nProof of Theorem D.4. Note that in that setting, using Lemma D.5, for all \u00b5 \u2208 [0, 1], q 7\u2192 \u03a8(\u00b5, q) + \u03b12 \u2225q\u2225 2 is a strongly convex function and has a unique minimum, which is also the unique critical point of the gradient q 7\u2192 \u2207\u03a8(\u00b5, q) + \u03b1q. Let q0 \u2208 Rd be such that\n{q0} = argmin\u03a8(0, q) + \u03b1\n2 \u2225q\u22252 .\nThanks to Lemma D.6, \u03a6 satisfies the hypothesis of Theorem G.1, with\na = 1\n\u03b1 sup \u03c1>0\nM(\u03c1), b = 1\n\u03b1\n( sup\u03c1>0 L1(\u03c1) sup\u03c1>0M(\u03c1)\n\u03b1 + sup \u03c1>0 L2(\u03c1)\n) , and c = 0. (32)\nLet \u00b5 \u2208 [0, 1] \u2192 q(\u00b5) be the unique solution up to time 1 to the ODE\nq\u2032(\u00b5) = \u2212 ( \u22072\u03a8(\u00b5, q(\u00b5)) + \u03b1 I )\u22121 \u2202\u00b5\u2207\u03a8(\u00b5, q(\u00b5)) = \u03a6(\u00b5, q(\u00b5)), q(0) = q0 .\nAccording to Theorem G.1 applied to \u039b = \u03a6, it exists and is well-defined up until \u00b5 = 1.\nRemark that when differentiating in \u00b5 \u2208 [0, 1] the function \u00b5 7\u2192 \u2207\u03a8(\u00b5, q(\u00b5)) + \u03b1q(\u00b5), we have( \u22072\u03a8(\u00b5, q(\u00b5)) + \u03b1 I ) q\u2032(\u00b5) + \u2202\u00b5\u2207\u03a8(\u00b5, q(\u00b5)) = ( \u22072\u03a8(\u00b5, q(\u00b5)) + \u03b1 I ) (q\u2032(\u00b5)\u2212 \u03a6(\u00b5, q(\u00b5))) = 0 .\nHence \u2207\u03a8(\u00b5, q(\u00b5)) + \u03b1q(\u00b5) = \u2207\u03a8(0, q(0)) + \u03b1q(0) = 0 .\nThus, for any \u00b5 \u2208 [0, 1], {q(\u00b5)} = argmin { \u03a8(\u00b5, q) + \u03b1\n2 \u2225q\u22252\n} ,\nwhich is the promised result.\nRemark D.7 (Crude bounds under mild assumptions). Using the same standard result (condition c = 0 in Theorem G.1) could naturally give us some crude bounds on \u2225q(\u00b5)\u2212 q(0)\u2225, relying only on assumptions A.D.1 and A.D.3. More precisely, these bounds would strongly depend on \u03b1 and improve as \u03b1\u2192 \u221e. Namely, using Eq. (32) and Theorem D.4 one have for all \u00b5 \u2208 [0, 1],\n\u2225q(\u00b5)\u2212 q(0)\u2225 \u2264 \u00b5 \u03b1 \u00b7 sup \u03c1 M(\u03c1) \u00b7 exp\n( 1\n\u03b1\n( 1\n\u03b1 ( sup \u03c1>0 L1(\u03c1) )( sup \u03c1 M(\u03c1) ) + sup \u03c1>0 L1(\u03c1) )) \u00b5 ) .\nThis is not the regime we aim at, since \u03b1 is a small, fixed regularization constant whose role is simply to ensure that the minimization problem is well-posed."
        },
        {
            "heading": "E. Quantitative bounds on the trajectory",
            "text": "Let us recall that q is the minimizer of the interpolated problem (21). In the previous section, we have made two assumptions (A.D.1 and A.D.3), guaranteeing that q is well-defined and is the unique solution to the ODE (22). In this section, we show how to obtain quantitative bounds on \u2225q(0)\u2212 q(\u00b5)\u2225 by studying the ODE (22). To derive these bounds, we now make two additional assumptions on \u03a8. The first one is an algebraic assumption which greatly improves the computations.\nAssumption E.1 (Common kernel). We assume that there exists a fixed subspace E \u2282 Rd such that dimE = N0 and for all (\u00b5, q) \u2208 [0, 1]\u00d7 Rd\nKer ( \u22072\u03a8(\u00b5, q) ) = E\u22a5, Im(\u22072\u03a8(\u00b5, q)) = E, and \u2202\u00b5\u2207\u03a8(\u00b5, q) \u2208 E .\nThe second one is a refined local-Lipschitz assumption (a quantitative version of A.D.3), which will allow us to use the case c \u0338= 0 in the Gronwall-Bahouri-Bellman type result Theorem F.7. Assumption E.2 (quantitative (local)-Lipschitz continuity). Recall L1 and L2 from Definition D.2, and M from Eq. (24). For any \u00b5, q, define \u03bb1(\u00b5, q) the smallest positive eigenvalue of \u22072\u03a8(\u00b5, q). For any \u03c1 > 0, define\nw\u22121(\u03c1) := inf \u00b5\u2208[0,1]\nq\u2208Bd(0,\u03c1)\n\u03bb1(\u00b5, q) .\nWe assume that there exist positive constants (\u0393i)i\u2208\u22121,...,2 and non negative constants (\u03b3i)i\u2208\u22121,...,2, such that for all \u03c1 > 0,\nL1(\u03c1) \u2264 \u03931 e\u03b31\u03c1 , L1(\u03c1) \u2264 \u03932 e\u03b32\u03c1 , M(\u03c1) \u2264 \u03930 e\u03b30\u03c1 ,\nand w\u22121(\u03c1) \u2265 1\n\u0393\u22121 e \u2212\u03b3\u22121\u03c1 .\nUnder these stronger assumptions, we can obtain the following:\nTheorem E.3 (Quantitative bounds on the trajectory). Assume that \u03a8 satisfies A.D.1, A.E.1, and A.E.2. Suppose furthermore that\n4\u0393\u22121(\u03930\u0393\u22121\u03931 + \u03932) < exp ( \u22122 ( (\u03b3\u22121 + \u03b30 + \u03b31) \u2228 \u03b32 ) ( \u2225q0\u2225+ \u0393\u22121\u03930 e(\u03b3\u22121+\u03b30+\u03b31)\u2228\u03b32\u2225q0\u2225 )) . (33)\nThen \u00b5 7\u2192 q(\u00b5) is differentiable on [0, 1], it is the unique solution of Eq. (22) and furthermore\n\u2200\u00b5 \u2208 [0, 1], \u2225q(\u00b5)\u2212 q0\u2225 \u2264 2\u00b5\u0393\u22121\u03930 e( \u22112 i=\u22121 \u03b3i)\u2225q0\u2225 .\nThe proof of Theorem E.3 follows the same path as the proof of Theorem D.4, with analogues of Lemmas D.5 and D.6. The crucial differences come from the fundamental use of A.E.1, which somehow allows us to diagonalize the Hessian \u22072\u03a8 for all \u00b5, q, and thus allows is to use estimates on the smallest positive eigenvalue of the Hessian. In practical cases, this assumption will not allow us to use global-Lipchitz estimates. We therefore introduce A.E.2 to deal with that. These two ingredients allow us to use the case c > 0 in the Gro\u0308nwall-Bahouri-Bellman type lemma (Theorem G.1).\nThe following Lemma gives an improve bounds for the norm of the inverse of the Hessian, using the algebraic requirement on the Hessian. Its proof is similar to the proof of Lemma D.5, and we only point out how to modify it.\nLemma E.4 (Quantitative norm of inverse Hessian). Let \u03a8 : [0, 1]\u00d7 Rd \u2192 R. Assume that A.D.1 and A.E.1 hold. Then\u2225\u2225\u2225(\u22072\u03a8(\u00b5, q) + \u03b1 I)\u22121 Im(\u22072\u03a8(\u00b5,q))\u2225\u2225\u2225 op \u2264 1 \u03bb1(\u00b5, q) , (34)\nwhere f |E denotes the restriction of f to the set E.\nProof. Remind that from the proof of Lemma D.5, for all (q, \u00b5) \u2208 Rd \u00d7 [0, 1], we have\nP (\u00b5, q) ( \u22072\u03a8(\u00b5, q) + \u03b1 I )\u22121 P (\u00b5, q)\u22a4 = diag\n( 1\n\u03b1 , . . . ,\n1 \u03b1 ,\n1\n\u03bb1(\u00b5, q) + \u03b1 , . . . ,\n1\n\u03bbN0(\u00b5, q) + \u03b1\n) .\nAssuming that A.E.1 holds, we have for all (\u00b5, q) \u2208 [0, 1]\u00d7 Rd, N0(\u00b5, q) = N0. Restricting to E, we see readily that the largest eigenvalue becomes 1/(\u03b1+ \u03bb1(\u00b5, q)).\nHere again, by using the algebraic requirements on \u03a8 and the local-Lipshcitz bound we are able to derive a local-Lipschitz continuity result for \u03a6. Here again, the proof is quite similar to the one of Lemma E.5.\nLemma E.5 (Local-Lispchitz continuity of \u03a6). Let \u03a8 such that A.D.1, and A.E.1 hold. Then \u03a6 is locally-Lipschitz continuous in q uniformly in \u00b5 \u2208 [0, 1]. More precisely, for all q, q\u0303 \u2208 Rd and all \u00b5 \u2208 [0, 1];\n\u2225\u03a6(\u00b5, q)\u2212 \u03a6(\u00b5, q\u0303)\u2225 \u2264 1 w\u22121(\u2225q\u0303\u2225)\n( L1(\u2225q\u2225 \u2228 \u2225q\u0303\u2225)M(\u2225q\u2225)\nw\u22121(\u2225q\u2225) + L2(\u2225q\u2225 \u2228 \u2225q\u0303\u2225)\n) \u2225q \u2212 q\u0303\u2225 .\nIf additionally A.E.2 holds, we get \u2225\u03a6(\u00b5, q)\u2212 \u03a6(\u00b5, q\u0303)\u2225 \u2264 2\u0393\u22121(\u03930\u0393\u22121\u03931 + \u03932) e ( (\u03b3\u22121+\u03b30+\u03b31)\u2228\u03b32 ) \u2225q\u2225\u2228\u2225q\u0303\u2225 \u2225q \u2212 q\u0303\u2225 , (35)\nand \u2225\u03a6(\u00b5, q)\u2225 \u2264 \u0393\u22121\u03930 e(\u03b3\u22121+\u03b30)\u2225q\u2225 .\nProof. Since \u03a8 satisfies A.E.1, for all (\u00b5, q) \u2208 [0, 1]\u00d7 Rd and all q\u0303 \u2208 Rd \u2202\u00b5\u2207\u03a8(\u00b5, q) \u2208 Im(\u22072\u03a8(\u00b5, q\u0303)), and we can use we can use the second part of Lemma D.5, namely Inequality (34). Indeed, Eq. (30), in norm, is upper bounded by\n1\n\u03bb1(\u00b5, q) + \u03b1 L1(\u2225q\u2225 \u2228 \u2225q\u0303\u2225)\n1\n\u03bb1(\u00b5, q\u0303) + \u03b1 M(\u2225q\u2225) \u2225q \u2212 q\u0303\u2225 ,\nwhile (31) is bounded by 1\n\u03bb1(\u00b5, q\u0303) + \u03b1 L2(\u2225q\u2225 \u2228 \u2225q\u0303\u2225) \u2225q \u2212 q\u0303\u2225 .\nSumming these last two displays and using the definition of w\u22121 and the bounds of A.E.2 allows us to conclude.\nProof of Theorem E.3. Remark that thanks to Lemma E.5, \u03a6 satisfies the condition of Theorem G.1 with\na = \u0393\u22121\u03930, b = 2\u0393\u22121(\u03930\u0393\u22121\u03931 + \u03932) and c = (\u03b3\u22121 + \u03b30 + \u03b31) \u2228 \u03b32.\nFurthermore, Eq. (35) can be translated into 2b < exp ( \u22122c ( \u2225q0\u2225+ a ec\u2225q0\u2225 )) .\nwhich is exactly the condition of application of Theorem G.1. It ensure that there exists a unique solution \u00b5 7\u2192 q(\u00b5) to Eq. (22). Following the proof of Theorem D.4 we can conclude easily.\nF. Specializing our results for doc2vec In the previous sections, we have seen that, under some technical assumptions on \u03a8, the mapping q is solution to an ODE, and we proved some bounds on \u2225q(\u00b5)\u2212 q(0)\u2225 (by means of Theorem E.3). In this section, we check that these assumptions are satisfied for the \u03a8 occurring when considering doc2vec embeddings. That is, \u03a8 = \u03a8lin, where \u03a8lin is defined by Eq. (12). This is embodied as Theorem F.7, which is Theorem 5.1 with explicit constants. We first prove a useful bound on the norm of \u03c0t:\nLemma F.1 (Bound on \u03c0t). Define \u03a0 := 2\u03bd\u03c3max(R) \u00b7 sup\ni \u2225P:,i\u2225 .\nThen, for any document x and any position t \u2208 x, it holds that\n\u2225\u03c0t\u2225 \u2264 \u03a0 .\nWe emphasize that Lemma F.1 is true regardless of the model used (PVDMmean, PVDMconcat, PVDBOW), even though this bound can be strengthened for specific models. Moreover, it only depends on the P and R matrices, which are fixed matrices after training.\nProof. Recall that we defined \u03c0t = RPht. For PVDBOW, ht = 0 and there is nothing to prove. Otherwise, let us first write\n\u2225\u03c0t\u2225 = \u2225RPht\u2225 \u2264 \u03c3max(R) \u00b7 \u2225Pht\u2225\nand focus on \u2225Pht\u2225. Let us assume that we work with PVDMconcat. Since, in that case, ht is the concatenation of 2\u03bd arbitrary one-hot vectors, Pht is the sum of 2\u03bd arbitrary columns of P . Using the triangle inequality, we deduce that \u2225Pht\u2225 is smaller than 2\u03bd times the largest norm of a column of P . When PVDMmean is used, the reasoning is similar. Ignoring the 1/(2\u03bd) factor (which we consider to be part of P ), the bound is the same.\nSince the matrix R appears in all the definition of the embeddings, one needs some (mild) assumptions on R. The first one ensures that the condition number of R is not equal to +\u221e. Assumption F.2 (Condition number of R). Let us R \u2208 RD\u00d7d. We assume that Im(R) \u2282 1\u22a5, and further that the smallest singular value of R is non-negative, that is,\n\u03c3min(R) > 0 .\nThe requirement for the range of R is needed here in order to work in the setting of Lemma H.6 and H.8, and then use the nice bounds for the (local)-Lipschitz constant of the softmax and its Jacobian. Lemma F.3. Suppose that A.F.2 hold. Then \u03a8lin satisfies A.D.1.\nProof. Recall that S denotes the set of modified words. Coming back to the definition of F and G, we see that, when forming the difference F \u2212G, many cancellations happen. To be more precise, replacing a word at position t only modifies \u03c0s for s belonging to the neighborhood of t. Thus\nG(q)\u2212 F (q) = \u2211 t\u2208E ( \u03c8x\u0303t(\u03c0\u0303t +Rq)\u2212 \u03c8xt(\u03c0t +Rq) ) , (36)\nwhere E \u2286 {s \u2208 [T ], |s\u2212 t| \u2264 \u03bd with t \u2208 S}. In particular, there is a numerical constant \u2113 > 0 such that |E| \u2264 \u2113\u03bd |S|. From the definition of \u03a8lin, Eq. (36), and Lemma H.1, we deduce that\n\u2207\u03a8lin(\u00b5, q) =R\u22a4 ( \u00b5 1\nT \u2211 t\u2208E ( \u2207\u03c8xt(\u03c0t +Rq)\u2212\u2207\u03c8x\u0303t(\u03c0\u0303t +Rq) ) + 1 T \u2211 t\u2208x \u2207\u03c8xt(\u03c0t +Rq) ) R\n=R\u22a4 (\n\u2212 \u00b5 1 T \u2211 t\u2208E ( \u03c3(\u03c0t +Rq)\u2212 \u03c3(\u03c0\u0303t +Rq) ) + \u00b5 1 T \u2211 t\u2208E ( 1xt \u2212 1x\u0303t ) \u2212 1 T \u2211 t\u2208x ( \u03c3(\u03c0t +Rq)\u2212 1xt\n)) ,\n\u2202\u00b5\u2207\u03a8lin(\u00b5, q) =R\u22a4 ( 1\nT \u2211 t\u2208E ( 1xt \u2212 1x\u0303t ) \u2212 1 T \u2211 t\u2208E ( \u03c3(\u03c0t +Rq)\u2212 \u03c3(\u03c0\u0303t +Rq)\n))\n=R\u22a4 ( 1\nT \u2211 t\u2208E \u222b 1 0 (( 1xt \u2212 1x\u0303t ) \u2212\u2207\u03c3(u(\u03c0t \u2212 \u03c0\u0303t) +Rq)(\u03c0t \u2212 \u03c0\u0303t) ) du ) and\n\u22072\u03a8lin(\u00b5, q) = R\u22a4 ( \u00b5 1\nT \u2211 t\u2208E (\u2207\u03c3(\u03c0t +Rq)\u2212\u2207\u03c3(\u03c0\u0303t +Rq)) + 1 T \u2211 t\u2208x\n\u2207\u03c3(\u03c0t +Rq) ) R, (37)\nwhere we remind that \u2207\u03c3 = diag(\u03c3) \u2212 \u03c3\u03c3\u22a4. Hence, \u22072\u03a8lin(\u00b5, \u00b7) is a symmetric non-negative matrix and \u03a8lin satisfies A.D.1.\nNext, we show that \u03a8lin satisfies A.E.1. Lemma F.4. Suppose that A.F.2 holds. For all \u00b5 \u2208 [0, 1] and all q \u2208 Rd,\nKer ( \u22072\u03a8lin(\u00b5, q) ) = {0} ,\nand \u03a8lin satisfies A.E.1 with N0 = d. Let us recall that we defined \u03bb1 the smallest non-zero eigenvalue of the Hessian of \u03a8lin. Then, for all (\u00b5, q) \u2208 [0, 1]\u00d7 Rd, it holds that\n\u03bb1(\u00b5, q) \u2265 e\u22122 \u221a 2\u03a0 1\nD \u03c3min(R)\n2 e\u22122 \u221a 2\u03c3max(R)\u2225q\u2225 .\nProof. Let us remind from Lemma H.5 the definition of \u03bbmin, namely for z \u2208 RD, \u03bbmin(z) = min ( Spec ( diag (\u03c3(z))\u2212 \u03c3(z)\u03c3(z)\u22a4 ) \\{0} ) .\nFor q, y \u2208 Rd and since Ry \u2208 1\u22a5 (thanks to A.F.2), the minimax theorem allows us to write (using Eq. (37))\n\u27e8\u22072\u03a8lin(\u00b5, q)y, y\u27e9 =\u00b5 1 T \u2211 t\u2208x \u27e8(\u2207\u03c3(\u03c0\u0303t +Rq)) (Ry), (Ry)\u27e9\n+ (1\u2212 \u00b5) 1 T \u2211 t\u2208x \u27e8(\u2207\u03c3(\u03c0t +Rq)) (Ry), (Ry)\u27e9\n\u2265 1 T \u2211 t\u2208x (\u00b5\u03bbmin(\u03c0\u0303t +Rq) + (1\u2212 \u00b5)\u03bbmin(\u03c0t +Rq)) \u2225Ry\u22252 .\nHere we have crucialy used A.F.2 and in particular the fact that Im(R) \u2282 1\u22a5 and that \u03c0t \u2208 1\u22a5 in order to make \u03bbmin appears. Let us set\n\u03c3(1)(z) = min i\u2208[D] \u03c3i(z) .\nThanks to Lemma H.5, one has\n\u27e8\u22072\u03a8lin(\u00b5, q)y, y\u27e9 \u2265 1 T \u2211 t\u2208x ( \u00b5D\u03c3(1)(\u03c0\u0303t +Rq) 2 + (1\u2212 \u00b5)D\u03c3(1)(\u03c0t +Rq)2 ) D \u2225Ry\u22252 .\nFurthermore, thanks to Theorem H.9,\n\u03c3(1)(z) \u2265 1 D e\u2212\n\u221a 2\u2225q\u2225,\nand we have\n\u27e8\u22072\u03a8lin(\u00b5, q)y, y\u27e9 \u2265 1 T \u2211 t\u2208x ( \u00b5 exp ( \u22122 \u221a 2 \u2225\u03c0\u0303t +Rq\u2225 ) + (1\u2212 \u00b5) exp ( \u22122 \u221a 2 \u2225\u03c0t +Rq\u2225 )) 1 D \u2225Ry\u22252\n\u2265 e\u22122 \u221a 2\u03a0 e\u22122 \u221a 2\u2225Rq\u2225 1\nD \u2225Ry\u22252\n\u2265 e\u22122 \u221a 2\u03a0 e\u22122 \u221a 2\u03c3max(R)\u2225q\u2225 1\nD \u03c3min(R)\n2 \u2225y\u22252 ,\nwhere we remind that \u03a0 is defined in Lemma F.1. This implies that Ker ( \u22072\u03a8lin(\u00b5, q) ) = {0}, that \u03a8lin fulfills A.E.1 with N0 = d, and that\n\u03bb1(\u00b5, q) \u2265 e\u22122 \u221a 2\u03a0 1\nD \u03c3min(R)\n2 e\u22122 \u221a 2\u03c3max(R)\u2225q\u2225 . (38)\nNext, we show that \u03a8lin satisfies A.E.2.\nLemma F.5 (Local Lipschitz continuity of \u03a8lin). Suppose that A.F.2 holds. Then \u03a8lin satisfies A.E.2 with\n\u0393\u22121 = D e 2 \u221a 2\u03a0 1\n\u03c3min(R)2 , \u03930 = 4\u2113\u03bd\u03c3max(R) |S| T , \u03931 =\n8 e6 \u221a 2\u03a0 (D \u2212 1)\u03c3max(R) 3 , \u03932 = 4\u2113\u03bd\u03a0e4 \u221a 2\u03a0 D \u2212 1 \u03c3max(R) 2 |S| T ,\nand \u03b3\u22121 = 2 \u221a 2\u03c3max(R) , \u03b30 = 0 , \u03b31 = 3 \u221a 2\u03c3max(R) , and \u03b32 = 2 \u221a 2\u03c3max(R) .\nProof. We have for all \u00b5 \u2208 [0, 1] and all q \u2208 Rd, \u2225\u2225\u2202\u00b5\u2207\u03a8lin(\u00b5, q)\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225R\u22a4 ( 1 T \u2211 t\u2208E ( 1xt \u2212 1x\u0303t ) \u2212 1 T \u2211 t\u2208E ( \u03c3(\u03c0t +Rq)\u2212 \u03c3(\u03c0\u0303t +Rq) ))\u2225\u2225\u2225\u2225\u2225 \u22644\u03c3max(R)\n|E| T\n\u22644\u2113\u03bd\u03c3max(R) |S| T ,\nwhere we have used the fact that \u2225\u03c3\u2225 \u2264 1 and the previous bound gives the value of \u03930 and \u03b30. Thanks to Lemma H.6 \u03c3 is locally-Lipschitz continuous and thanks to Lemma H.8, \u2207\u03c3 is also locally-Lipschitz continuous, hence for q, q\u0303 \u2208 Rd\n\u2225\u2225\u2202\u00b5\u2207\u03a8lin(\u00b5, q)\u2212 \u2202\u00b5\u2207\u03a8lin(\u00b5, q\u0303)\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225R\u22a4 1T \u2211\nt\u2208E\n(\u222b 1 0 (\u03c3(u(\u03c0t \u2212 \u03c0\u0303t) +Rq)\u2212 \u03c3(u(\u03c0t \u2212 \u03c0\u0303t) +Rq\u0303)) (\u03c0t \u2212 \u03c0\u0303t) du )\u2225\u2225\u2225\u2225\u2225\n\u22644 1 D \u2212 1 e\n2 \u221a 2(2\u03a0+\u03c3max(R)(\u2225q\u2225\u2228\u2225q\u0303\u2225)) \u2113\u03bd\u03c3max(R) 2 |S| T \u03a0 \u2225q \u2212 q\u0303\u2225\n\u22644\u2113\u03bd\u03a0e 4 \u221a 2\u03a0\nD \u2212 1 |S| T e2 \u221a 2\u03c3max(R)(\u2225q\u2225\u2228\u2225q\u0303\u2225) \u03c3max(R) 2 \u2225q \u2212 q\u0303\u2225 ,\nwhere we have used Lemma H.6 and the bound DD\u22121 \u2264 2, which gives the value of \u03932 and \u03b32. Finally, let us remark that\u2225\u2225\u22072\u03a8lin(\u00b5, q)\u2212\u22072\u03a8lin(\u00b5, q)\u2225\u2225 op \u2264\u00b5 1 T \u2211 t\u2208x \u2225\u2225R\u22a4 (\u2207\u03c3(\u03c0\u0303t +Rq)\u2212\u2207\u03c3(\u03c0\u0303t +Rq\u0303))R\u2225\u2225 + (1\u2212 \u00b5) 1\nT \u2211 t\u2208x \u2225\u2225R\u22a4 (\u2207\u03c3(\u03c0t +Rq)\u2212\u2207\u03c3(\u03c0t +Rq\u0303))R\u2225\u2225 \u2264 8 e 6 \u221a 2\u03a0\n(D \u2212 1)\u03c3max(R) 3 e3\n\u221a 2\u03c3max(R)(\u2225q\u2225\u2228\u2225q\u0303\u2225) \u2225q \u2212 q\u0303\u2225 ,\nwhich gives the value of \u03931 and \u03b31. Finally, Eq. 38 gives directly that\nw\u22121(\u03c1) = e \u22122\n\u221a 2\u03a0 1\nD \u03c3min(R)\n2 e\u22122 \u221a 2\u03c3max(R)\u2225q\u2225 ,\nwhich concludes the proof.\nNext, we show that \u2225q0\u2225 is not too large. Lemma F.6 (Bound on \u2225q0\u2225). Suppose that A.F.2 holds. Then\n\u2225q0\u2225 \u2264 \u221a 2\u03c3max(R)\n\u03b1 .\nWe demonstrate Lemma F.6 in practice in Section I.2. The key idea behind the proof is that the regularization term \u03b12 \u2225q\u2225 2 prevents q from escaping to infinity.\nProof. Let us recall that\nq0 = argmin q\u2208Rd\n{ 1\nT \u2211 t\u2208x \u03c8xt(\u03c0t +Rq) + \u03b1 2 \u2225q\u22252\n} .\nIn view of Lemma F.3, q0 is the unique solution of the following equation: R\u22a4 ( 1\nT \u2211 t\u2208x ( \u03c3(\u03c0t +Rq)\u2212 1xt )) + \u03b1q = 0 . (39)\nFrom Eq. (39), we deduce that\n\u2225q0\u2225 = 1\nT\u03b1 \u2225\u2225\u2225\u2225\u2225R\u22a4 (\u2211\nt\u2208x\n( \u03c3(\u03c0t +Rq0)\u2212 1xt ))\u2225\u2225\u2225\u2225\u2225 . By definition of \u03c3max(R) and the triangle inequality, this is upper bounded by\n\u03c3max(R)\nT\u03b1 \u2211 t\u2208x \u2225\u03c3(\u03c0t +Rq0)\u2212 1xt\u2225 . (40)\nBut we notice that, for any q \u2208 Rd and t \u2208 x,\n\u2225\u03c3(\u03c0t +Rq)\u2212 1xt\u22252 = \u2211 j \u0338=xt \u03c3j(\u03c0t +Rq) 2 + (\u03c3xt(\u03c0t +Rq)\u2212 1)2\n= \u2211 j \u03c3j(\u03c0t +Rq) 2 + 1\u2212 2\u03c3xt(\u03c0t +Rq)\n\u2225\u03c3(\u03c0t +Rq)\u2212 1xt\u22252 \u22642 ,\nwhere we have used the fact that \u2225\u03c3\u2225 \u2264 1 and \u03c3i \u2265 0. Hence each term in Eq. (40) is upper bounded by \u221a 2. Keeping in mind that the summation over t \u2208 x has at most T terms, we deduce the result.\nWe are now ready to apply case c > 0 of Theorem G.1 to obtain the promised quantitative bounds. Theorem F.7 (Quantitative bounds for doc2vec embeddings). Let \u03a8lin defined in Eq. (12), and suppose A.F.2 holds. Let us define\nA := 4\u2113\u03bdD e2 \u221a 2\u03a0 \u03c3max(R)\n\u03c3min(R)2 ,\nB := 64\u2113\u03bdD \u03c3max(R)\n2\n\u03c3min(R)2 e10\n\u221a 2\u03a0 ( \u03c3max(R) 2\n\u03c3min(R)2 +\n\u03a0\nD \u2212 1\n) ,\nand C := 5 \u221a 2\u03c3max(R) .\nSuppose that |S| T \u2264 1 2B e\u22122(AC+1) e C \u221a 2\u03c3max(R) \u03b1 , (41) Then\nsup \u00b5\u2208[0,1] \u2225q(\u00b5)\u2212 q0\u2225 \u2264 2A eC\u2225q0\u2225 |S| T . (42)\nProof. Remark that \u03a8lin satisfies A.(D.1) (Lemma F.3), A.(E.1) (Lemma F.4) and A.(E.2) (Lemma F.5). Therefore the assumptions of Theorem E.3 are satisfied. Let us note that\n\u03b30\u03b3\u22121 = A |S| T ,\n2\u0393\u22121(\u0393\u22121\u03931\u03930 + \u03932) \u2264 B |S| T\nand (\u03b3\u22121 + \u03b31) \u2228 \u03b32 = C . Remark that in that setting, thanks to Lemma F.6, \u2225q0\u2225 \u2264 \u221a 2\u03c3max(R)\n\u03b1 . We also have the following straightforward bounds:\n|S| T \u2264 1 and C \u2225q0\u2225 \u2264 eC\u2225q0\u2225 \u2264 eC \u221a 2\u03c3max(R) \u03b1 .\nUsing Eq. (41), one necessarily have\n2B |S| T\n\u2264 e\u22122(AC+1) eC \u221a 2\u03c3max(R) \u03b1\n\u2264 e\u22122C(\u2225q0\u2225+A |S| T e C\u2225q0\u2225) .\nThis guarantees that Eq. (33) and one can apply Theorem E.3, which yields the desired result."
        },
        {
            "heading": "G. Gro\u0308nwall-Bahouri-Bellman type result",
            "text": "In this section, we collect all results related to ODEs. In our setting, as seen in Lemma D.6, and in view of A.E.2, the coefficients of Eq. 22 are not globally Lispchitz (although locally-Lipschitz). Thus, while local existence and uniqueness of solutions to Eq. (22) is a given (small \u00b5 regime), existence up to time 1 and non-explosion of the solutions is much more challenging to achieve (large \u00b5 regime). Unfortunately, this is the regime that we are interested into: the local behavior of the ODE at \u00b5 = 0 does not tell us anything interesting, since what we aim at is the comparison between the starting point (\u00b5 = 0) and final point (\u00b5 = 1) of the dynamic. Our strategy is to use an ad hoc extension of the Gro\u0308nwall-Bahouri-Bellman lemma to deal with our specific setting.\nOur approach is inspired by proofs of Gro\u0308nwall-Bellman-Bahouri type lemmas, see for example Dannan (1985); Agarwal et al. (2005); Kim (2009); Pachpatte (2004). It relies on an explicit integration of the integral inequality which will pop up in the computations. Note that, instead of generic local constants L, M , and w\u22121, and in view of Section F, we will suppose that all those quantity are locally bounded by some exponential functions. Our derivation is very close to that of Pachpatte inequality (Pachpatte, 2004), but here we keep track of the constants. In doing, so we gain an explicit criteria for non-explosion of the solutions up to time \u00b5 = 1. To view other applications of non-explosion on the time-one map, one could also consult (Bailleul & Catellier, 2020) and the references therein.\nTheorem G.1 (Gro\u0308nwall-Bahouri-Bellman type inequality). Let \u039b : [0, 1] \u00d7 Rd \u2192 Rd be a continuous function and a, b, c > 0 be numerical constants such that, for all q, q\u0303 \u2208 Rd,\nsup \u00b5\u2208[0,1]\n\u2225\u039b(\u00b5, q)\u2225 \u2264 a ec\u2225q\u2225 , (43)\nand sup\n\u00b5\u2208[0,1] \u2225\u039b(\u00b5, q)\u2212 \u039b(\u00b5, q\u0303)\u2225 \u2264 b ec(\u2225q\u2225\u2228\u2225q\u0303\u2225) \u2225q \u2212 q\u0303\u2225 . (44)\nLet q0 \u2208 Rd such that either c = 0 or 2b < exp ( \u22122c ( \u2225q0\u2225+ a ec\u2225q0\u2225 )) . (45)\nThen, there exists a unique function q : [0, 1] \u2192 Rd such that q(0) = 0 and for all \u00b5 \u2208 [0, 1]\nq\u2032(\u00b5) = \u039b(\u00b5, q(\u00b5)) . (46)\nFurthermore, for all \u00b5 \u2208 [0, 1],\n\u2225q(\u00b5)\u2212 q0\u2225 \u2264 { 2\u00b5a ec\u2225q0\u2225 if c > 0, \u00b5a eb if c = 0 .\nProof. Step 1: Existence of the map satisfying (46). Note that since \u039b is locally Lipschitz continuous, thanks to the Cauchy-Lipschitz/Picard-Lindelo\u0308f theorem (see Arnold (1978, Chapter 2)), there exists an open interval I\u22c6 of [0, 1] and a unique function q : I\u22c6 \u2192 Rd such that q is the unique solution to Eq. (46). Note that an open interval of [0, 1] which contains 0 is necessarily of the form [0, \u03c4) with 0 < \u03c4 < 1 or [0, 1]. Remark also that for all \u00b5 \u2208 I\u22c6, thanks to the regularity assumption on \u039b, on I\u22c6, \u00b5 7\u2192 \u039b(\u00b5, q(\u00b5)) is continuous and for \u00b5 \u2208 I\u22c6 the following integral equation is satisfied:\nq(\u00b5) = q0 + \u222b \u00b5 0 \u039b(\u00b5\u0303, q(\u00b5\u0303)) d\u00b5\u0303 .\nStep 2: . Taking the norm in the previous display and using the triangle inequality, we see that \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u222b \u00b5 0 \u2225\u039b(\u00b5\u0303, q0)\u2225 d\u00b5\u0303+ \u222b \u00b5 0 \u2225\u039b(\u00b5\u0303, q(\u00b5))\u2212 \u039b(\u00b5\u0303, q0)\u2225 d\u00b5\u0303 .\nUsing our assumptions on \u039b, namely Eqs. (43) and (44), we obtain \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u00b5a ec\u2225q0\u2225 +b \u222b \u00b5 0 ec(\u2225q(\u00b5\u0303)\u2225+\u2225q0\u2225) \u2225q(\u00b5\u0303)\u2212 q0\u2225 d\u00b5\u0303 .\nSince \u2225q(\u00b5\u0303)\u2225 \u2212 \u2225q(q0)\u2225 \u2264 \u2225q(\u00b5\u0303)\u2212 q0\u2225, we deduce that \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u00b5a ec\u2225q0\u2225 +b e2c\u2225q0\u2225 \u222b \u00b5 0 ec\u2225q(\u00b5\u0303)\u2212q0\u2225 \u2225q(\u00b5\u0303)\u2212 q0\u2225 d\u00b5\u0303 .\nLet us define for all \u00b5 \u2208 I\u22c6, Q(\u00b5) = { a ec\u2225q0\u2225 +b e2c\u2225q0\u2225 1\u00b5 \u222b \u00b5 0 ec\u2225q(\u00b5\u0303)\u2212q0\u2225 \u2225q(\u00b5\u0303)\u2212 q0\u2225 d\u00b5\u0303 if \u00b5 > 0 ,\na ec\u2225q0\u2225 if \u00b5 = 0 .\nNote that 1\u00b5 \u222b \u00b5 0 ec\u2225q(\u00b5\u0303)\u2212q0\u2225 \u2225q(\u00b5\u0303)\u2212 q0\u2225 d\u00b5\u0303\u2192\u00b5\u21920 ec\u2225q(0)\u2212q0\u2225 \u2225q(0)\u2212 q0\u2225 = 0 and Q is continuous in \u00b5 = 0. Furthermore, for \u00b5 \u2208 I\u22c6\\{0},\nQ\u2032(\u00b5) = \u2212 1 \u00b52 b e2c\u2225q0\u2225 \u222b \u00b5 0 ec\u2225q(\u00b5\u0303)\u2212q0\u2225 \u2225q(\u00b5\u0303)\u2212 q0\u2225 d\u00b5\u0303+ 1 \u00b5 b ec\u2225q(\u00b5)\u2212q0\u2225 \u2225q(\u00b5)\u2212 q0\u2225 . (47)\nWith this notation in hand, for any \u00b5 \u2208 I\u22c6\\{0}, \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u00b5Q(\u00b5). Since we restrict our attention to \u00b5 \u2264 1, we have\n\u2225q(\u00b5)\u2212 q0\u2225 \u2264 Q(\u00b5) . (48)\nStep 3: Differential inequality. Since x 7\u2192 x ecx is a non-decreasing function, we have for \u00b5 \u2208 I\u22c6\\{0}\nQ\u2032(\u00b5) \u2264 b e2c\u2225q0\u2225 \u2225q(\u00b5)\u2212 q0\u2225 \u00b5 ec\u00b5 \u2225q(\u00b5)\u2212q0\u2225 \u00b5\n\u2264 b e2c\u2225q0\u2225 Q(\u00b5) ecQ(\u00b5) , (49)\nwhere we used Eq. (47) for a direct bound one the derivative and Eq. (48) to obtain the last display.\nStep 4: Cauchy-Lipschitz setting (c = 0). Suppose for a moment that c = 0 so that we are in the standard setting of global Cauchy-Lipschitz/Picard Lindelo\u0308f Theorem and standard Gro\u0308nwall Lemma. We have for \u00b5 \u2208 I\u22c6\nlog (Q(\u00b5) a ) \u2264 b\u00b5 ,\nI\u22c6 = [0, 1] and \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u00b5Q(\u00b5) \u2264 a\u00b5 eb\u00b5 .\nStep 5: Gro\u0308nwall-Bahouri-Bellman integration (c > 0). Suppose now that c > 0. Let \u03b2 > 0. Let us remark that for all x \u2265 0, x ecx \u2264 1c e2cx, and we have\nQ\u2032(\u00b5) \u2264 b c e2c\u2225q0\u2225 e2cQ(\u00b5) . (50)\nMultiplying both sides of Eq. (50) by e\u22122cQ(\u00b5), one recognize (up to constants) the derivative of e\u22122cQ. Integrating from 0 to \u00b5, we have proved that\ne\u22122ca e c\u2225q0\u2225 \u2212 e\u22122cQ(\u00b5)\n2 \u2264 b e2c\u2225q0\u2225 \u00b5 . (51)\nWhen e\u22122ca e\nc\u2225q0\u2225 \u22122b e2c\u2225q0\u2225 \u00b5 > 0 , (52) we have\necQ(\u00b5) \u2264 ( e\u22122ca e c\u2225q0\u2225 \u22122b e2c\u2225q0\u2225 \u00b5 )\u2212 12 . (53)\nFurthermore, whenever Eq (45) holds (namely Eq. (52) is true for all \u00b5 \u2208 [0, 1]) we can take I\u22c6 = [0, 1], since Eq. (53) guaranty that Q does not explode. For \u00b5 \u2208 I\u22c6\\{0} which satisfies Eq. (52), when using the previous bound and Eq. (49), we have the following inequality :\nQ\u2032(\u00b5) \u2264 b e2c\u2225q0\u2225 ( e\u22122ca e c\u2225q0\u2225 \u22122b e2c\u2225q0\u2225 \u00b5 )\u2212 12 Q(\u00b5) .\nWhen dividing by Q(\u00b5) and integrating, we get\nlog(Q(\u00b5))\u2212 log(Q(0)) \u2264 exp ( b e2c\u2225q0\u2225 \u222b \u00b5 0 ( e\u22122ca e c\u2225q0\u2225 \u22122b e2c\u2225q0\u2225 \u00b5\u0303 )\u2212 12 d\u00b5\u0303 ) .\nTherefore, for all \u00b5 which satisfies Eq. (52), \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u00b5Q(\u00b5) \u2264\u00b5a ec\u2225q0\u2225 exp (\u222b \u00b5\n0\nb e2c\u2225q0\u2225 ( e\u22122ca e c\u2225q0\u2225 \u22122b e2c\u2225q0\u2225 \u00b5\u0303 )\u2212 12 d\u00b5\u0303 ) \u2264\u00b5a ec\u2225q0\u2225 exp (( e\u22122ca e c\u2225q0\u2225 ) 1 2 \u2212 ( e\u22122ca e c\u2225q0\u2225 \u22122b e2c\u2225q0\u2225 \u00b5 ) 1 2 ) (54)\n\u2264\u00b5a ec\u2225q0\u2225 exp (( 2b e2c\u2225q0\u2225 \u00b5 ) 1 2 ) ,\nwhere we have use that for 0 \u2264 v < v\u0303, \u221a v\u0303 \u2212 \u221av \u2264 \u221a v\u0303 \u2212 v. Note that Eq. (54) makes sense since Eq. (52) is satisfied. Finally, one can use Eq. (52) and write\n2b e2c\u2225q0\u2225 \u00b5 \u2264 2b e2c\u2225q0\u2225 \u2264 e\u22122ac e2\u2225q0\u2225 \u2264 1 ,\nwhich gives \u2225q(\u00b5)\u2212 q0\u2225 \u2264 \u00b52a ec\u2225q0\u2225 ,\nwhich is the wanted result.\nRemark G.2 (Improving Theorem F.7). There are several open avenues to improve Theorem F.7. One possibility is to keep a finer the dependency in \u00b5 when bounding \u2225q(\u00b5)\u2212 q0\u2225 (namely keeping the \u00b5 factor when deriving Eq. (48)). A second possible improvement is to use a finer inequality than y \u2264 ey when deriving Eq. (50). Unfortunately, in both cases, we were unsuccessful in integrating these more complicated expressions in a tractable form (derivation leading to Eq. (51))."
        },
        {
            "heading": "H. Technical results related to the softmax function",
            "text": "In this section, we collect all technical facts related to the softmax function used throughout the proofs. Let us recall that we defined the softmax function from RD to RD as \u03c3(x) = (\u03c31(x), . . . , \u03c3D(x))\u22a4, where for all i \u2208 [D],\n\u03c3i(x) = exi\u2211D j=1 e xj .\nWe also defined, for all x \u2208 RD and all i \u2208 [D],\n\u03c8i(x) = \u2212 log(\u03c3i(x)) .\nH.1. Basics on the softmax function\nWe start by recalling elementary properties of the softmax function.\nLemma H.1 (Softmax derivatives). We have\n\u2202\n\u2202xk \u03c3\u2113(x) = { \u03c3k(x)(1\u2212 \u03c3k(x)) if k = \u2113 \u2212\u03c3k(x)\u03c3\u2113(x) otherwise.\nIn a more concise way, \u2207\u03c3(x) = diag(\u03c3(x))\u2212 \u03c3(x)\u03c3(x)\u22a4 .\nA straightforward consequence of Lemma H.1 is the computation of the first derivatives of \u03c8i (these are very standard computations, see for instance Proposition 1 and 2 in Gao & Pavel (2017)).\nLemma H.2 (Gradient of \u03c8i). We have\n\u2202\n\u2202xk \u03c8i(x) = { \u22121 + \u03c3k(x) if k = i \u03c3k(x) otherwise.\nIn more concise notation, \u2207\u03c8i = \u03c3 \u2212 1i.\nSimilarly, we have:\nLemma H.3 (Hessian of \u03c8i). We have\n\u22022\n\u2202xk\u2202x\u2113 \u03c8i(x) = { \u03c3 (x)k (1\u2212 \u03c3 (x)k) if k = \u2113 \u2212\u03c3 (x)k \u03c3 (x)\u2113 otherwise.\nIn more concise notation, \u22072\u03c8i = \u2207\u03c3 = diag(\u03c3)\u2212 \u03c3\u03c3\u22a4. (55)\nCorollary H.4 (Convexity of log-softmax). For any i \u2208 [D], the function \u03c8i is convex.\nThe proof of the previous fact relies on the Courant minimax theorem, which gives the value of the eigenvalue of a real symmetric matrix. Furthermore, we also use that fact that a function such that its Hessian is a non-negative symmetric matrix is convex.\nProof. Let x, v \u2208 RD. Since\u2211i \u03c3i(x) = 1, we have \u27e8\u22072\u03c8i(x)v, v\u27e9 =\n\u2211 k \u03c3k(x)v 2 k \u2212 \u2211 k \u03c3k(x) \u2211 \u2113 \u03c3\u2113(x)v\u2113vk\n= \u2211 k \u03c3k(x)v 2 k \u2212 (\u2211 k \u03c3k(x)vk )2 = \u2211 k \u03c3k(x) ( vk \u2212 \u2211 \u2113 \u03c3\u2113(x)v\u2113 )2 \u2265 0 ,\nHence, thanks to the Courant minimax principle, all the eigenvalues of \u22072\u03c8i are non-negative. Hence \u22072\u03c8i is a non-negative symmetric matrix, and \u03c8i is a convex function.\nThe following proposition controls the spectrum of the Hessian of \u03c8i, that is the gradient of the softmax, in function of the minimal and maximal values of the softmax function.\nLemma H.5 (Spectrum of the softmax Jacobian). Let \u03c1 > 0. For x \u2208 RD, let us define\n\u03c3(1)(x) := min i\u2208[D] \u03c3i(x) ,\nand \u03c3(D)(x) := max\ni\u2208[D] \u03c3i(x) .\nLet us define \u03bbmin(x) := min {Spec (\u2207\u03c3(x)) \\{0}} ,\nand \u03bbmax(x) := max {Spec (\u2207\u03c3(x))} .\nThen D\u03c32(1)(x) \u2264 \u03bbmin(x) \u2264 \u03bbmax(x) \u2264 D\u03c32(D)(x) .\nProof. According to Lemma H.3, \u2207\u03c3(x) = diag(\u03c3(x))\u2212 \u03c3(x)\u03c3(x)\u22a4 .\nThis matrix is symmetric, and according to Corollary H.4, its eigenvalues are non-negative real numbers. Since \u2211\ni \u03c3i(x) = 1, one has \u2207\u03c3(x)1 = 0 , where, as before, 1 = (1, . . . , 1)\u22a4. Since for all i \u2208 [D], \u03c3i(x) \u0338= 0, if v \u2208 Ker (\u2207\u03c3(x)) then necessarily for all i \u2208 [D], vi\u03c3i(x) \u2212 \u03c3i(x) \u2211 j \u03c3j(x)vj = 0, and v = v11. Hence, Im(\u2207\u03c3(x)) = 1\u22a5 and Ker (\u2207\u03c3(x)) = Vec (1). Using the Courant minimax characterization of eigenvalues, we have\n\u03bbmin(x) = min v\u22081\u22a5 \u2225v\u2225=1 \u27e8\u2207\u03c3(x)v, v\u27e9 = min v\u22081\u22a5 \u2225v\u2225=1 v\u22a4 (\u2207\u03c3(x)) v and \u03bbmax(x) = max v\u22081\u22a5 \u2225v\u2225=1 \u27e8\u2207\u03c3(x)v, v\u27e9 = max v\u22081\u22a5 \u2225v\u2225=1 v\u22a4 (\u2207\u03c3(x)) v .\n(56) Note then that for v \u2208 1\u22a5 (and dropping the x dependency),\nv\u22a4 (\u2207\u03c3(x)) v = D\u2211 i=1 \u03c3iv 2 i \u2212 ( D\u2211 i=1 \u03c3jvj )2 .\nNow, the Cauchy-Schwarz inequality guarantees that the previous display is non-negative, but this is not enough to conclude. We resort to the four-letter identity (Steele (2004, Exercise 3.7), see also Garreau & Mardaoui (2021, Proposition 13)) to write\nv\u22a4 (\u2207\u03c3(x)) v = D\u2211 i=1 \u03c3iv 2 i \u2212 ( D\u2211 i=1 \u03c3ivi )2 = \u2211 j<k \u03c3j\u03c3k(vk \u2212 vj)2 . (57)\nKeeping in mind that the \u03c3is are non-negative, this last identity gives \u03c32(1) \u2211 j<k \u03c3j\u03c3k(vk \u2212 vj)2 \u2264 v\u22a4 (\u2207\u03c3(x)) v \u2264 \u03c32(D) \u2211 j<k \u03c3j\u03c3k(vk \u2212 vj)2 . In the term ( \u03c3(1)\n)2 \u00b7\u2211 j<k (vk \u2212 vj)2 .\nwe recognize (D2 times) the variance of the vis. More precisely,\n1\nD2 \u2211 j<k (vk \u2212 vj)2 = 1 D D\u2211 i=1 vi \u2212 1 D \u2211 j vj 2 . Since v \u2208 Vec (1)\u22a5, we know that\u2211j vj = 0, and the previous display reduces to (1/D times) the norm of v. Whenever \u2225v\u2225 = 1 and v \u2208 1\u22a5 we have shown\nD\u03c3(1)(x) 2 \u2264 v\u22a4 (\u2207\u03c3(x)) v \u2264 D\u03c3(D)(x)2.\nComing back to the characterization of the eigenvalues given by Eq. (56), we deduce the result.\nThe previous bound, associated with estimates on the infimum and supremum of the softmax function on balls gives estimates on the (local)-Lipschitz constant of the softmax. Lemma H.6 (local-Lipschitz continuity of the softmax). For all x, y \u2208 RD such that x, y \u2208 1\u22a5,\n\u2225\u03c3(x)\u2212 \u03c3(y)\u2225 \u2264 D (D \u2212 1)2 exp\n( 2 \u221a D D \u2212 1(\u2225x\u2225 \u2228 \u2225y\u2225) ) \u2225x\u2212 y\u2225 .\nIn order to prove the previous lemma, one only has to remember that the operator norm for real symmetric matrices is the greatest eigenvalue, and use the fundamental theorem of analysis.\nProof. Let x, y \u2208 1\u22a5. We write \u2225\u03c3(x)\u2212 \u03c3(y)\u2225 = \u2225\u2225\u2225\u2225\u222b 1\n0\n\u2207\u03c3(u(x\u2212 y) + y)(x\u2212 y) du \u2225\u2225\u2225\u2225\n\u2264 \u222b 1 0 \u2225\u2207\u03c3(u(x\u2212 y) + y)\u2225op \u2225x\u2212 y\u2225 du.\nOne can then use Theorem H.9 and Lemma H.5, and we have for all u \u2208 [0, 1], \u2225\u2207\u03c3(u(x\u2212 y) + y)\u2225op = \u03bbmax(u(x\u2212 y) + y)\n\u2264 D\u03c3(D)(u(x\u2212 y) + y)2\n\u2264 D  1 1 + (D \u2212 1) e\u2212 \u221a D D\u22121\u2225u(x\u2212y)+y\u2225 2\n\u2264 D e 2 \u221a D D\u22121\u2225u(x\u2212y)+y\u2225\n(D \u2212 1)2\n\u2264 D (D \u2212 1)2 e\n2 \u221a\nD D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) .\nPutting everything together, we have\n\u2225\u03c3(x)\u2212 \u03c3(y)\u2225 \u2264 D (D \u2212 1)2 e\n2 \u221a\nD D\u22121\u2225x\u2225\u2228\u2225y\u2225 \u2225x\u2212 y\u2225 ,\nwhich is the desired result.\nRemark H.7 (Lipschitz continuity of the softmax). Note that usually, the Lipschitz continuity of the softmax is considered, but with respect to the Frobenius norm. One can obtain a crude bound starting from the squared Frobenius norm of the Jacobian, namely \u2211\ni \u03c32i (1\u2212 \u03c3i)2 + \u2211 i\u0338=j \u03c32i \u03c3 2 j . (58)\nSince the Frobenius norm is always greater than the operator norm, this implies the result for a (global) Lispchitz constant equal to 1. A finer study of Eq. (58) yields a better Lipschitz constant for \u03c3. This is what Alghamdi et al. (2022) do, proving 1/2-Lipschitz continuity for the softmax function (Proposition 1 in Appendix A.4).\nIn view of the specific form of the gradient of the softmax, this implies that we have (almost) the same local-Lipschitz constant for the gradient of the softmax.\nCorollary H.8 (local-Lispchitz continuity of the softmax Jacobian). For all x, y \u2208 1\u22a5,\n\u2225\u2207\u03c3(x)\u2212\u2207\u03c3(y)\u2225op \u2264 2D2 (D \u2212 1)3 e 3 \u221a D D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) \u2225x\u2212 y\u2225 .\nThe proof is a direct consequence of the particular form of the Jacobian (see Lemma H.1) and of the fact that\n|\u03c3i(x)\u2212 \u03c3i(y)| \u2264 \u2225\u03c3(x)\u2212 \u03c3(y)\u2225 .\nProof. Let x, y \u2208 1\u22a5. We have\n\u2225\u2207\u03c3(x)\u2212\u2207\u03c3(y)\u2225op = sup v\u2208RD \u2225v\u2225=1 v\u22a4 (\u2207\u03c3(x)\u2212\u2207\u03c3(y)) v .\nFurthermore, using the same argument as in the proof of Lemma H.5, one can only consider v \u2208 1\u22a5 with \u2225v\u2225 = 1. Applying Eq. (57) to x and y and forming the difference, we obtain\nv\u22a4 (\u2207\u03c3(x)\u2212\u2207\u03c3(y)) v = \u2211 i<k ( \u03c3i(x)\u03c3k(x)\u2212 \u03c3i(y)\u03c3k(y) ) (vi \u2212 vk)2\n= \u2211 i<k ( \u03c3i(x)\u2212 \u03c3i(y) ) \u03c3k(x)(vi \u2212 vk)2 + \u2211 i<k \u03c3i(y) ( \u03c3k(x)\u2212 \u03c3k(y) ) (vi \u2212 vk)2\nEach of these terms can be bounded, using successively the local Lipschitz continuity of the softmax (Lemma H.6) and the definition of \u03c3(D). The last display is upper bounded by(\nD (D \u2212 1)2 e 2 \u221a D D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) \u03c3(D)(x) +\nD (D \u2212 1)2 e 2 \u221a D D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) \u03c3(D)(y) )\u2211 i<k (vi \u2212 vk)2 \u2225x\u2212 y\u2225 ,\nwhich, in turn, is smaller than\n(\u03c3(D)(x) + \u03c3(D)(y)) D (D \u2212 1)2 e 2 \u221a D D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) \u2211 i<k (vi \u2212 vk)2 \u2225x\u2212 y\u2225 .\nUsing the bound on \u03c3(D) given by Theorem H.9, we have\nv\u22a4 (\u2207\u03c3(x)\u2212\u2207\u03c3(y)) v \u2264 2D (D \u2212 1)3 e\n3 \u221a\nD D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) \u2225x\u2212 y\u2225 \u2211 i<k (vi \u2212 vk)2.\nUsing again the same argument as in the proof of Lemma H.6, we have \u2211\ni<k(vi \u2212 vk)2 = D, and finally for v \u2208 1\u22a5 with \u2225v\u2225 = 1, we have\nv\u22a4(\u2207\u03c3(x)\u2212\u2207\u03c3(y))v \u2264 2D 2 (D \u2212 1)3 e 2 \u221a D D\u22121 (\u2225x\u2225\u2228\u2225y\u2225) \u2225x\u2212 y\u2225 ,\nwhich gives the wanted result by taking the supremum on v.\nH.2. Minimization of the softmax function\nIn this section, we study the extremal values of the softmax function. The reason of this study is the close connection of these extremal values with the spectrum of the softmax and log-softmax function. Intuitively, the trivial bound \u03c3i(x) \u2264 1 can be greatly strengthened when the norm of x is constrained: \u03c3i(x) = 1 is achieved only when xi \u2192 +\u221e, which can not be if x lives in a ball of radius \u03c1.\nTheorem H.9 (Bounding the softmax function). Let \u03c1 > 0 and D \u2265 2. We have\nmin i\u2208[D] inf \u2225x\u2225\u2264\u03c1\u2211 j xj=0\n\u03c3i(x) = 1 1 + (D \u2212 1) e \u221a D D\u22121\u03c1 .\nand max i\u2208[D]\nsup \u2225x\u2225\u2264\u03c1\u2211 j xj=0\n\u03c3i(x) = 1 1 + (D \u2212 1) e\u2212 \u221a D D\u22121\u03c1 .\nRemark H.10 (Bounding the softmax). The softmax function is ubiquitous in machine learning, and many bounds can be found in the literature (Wei et al., 2023). Generally, these bounds are pointwise, and not applicable in our case since we need a global bound on the ball of radius \u03c1 (with the additional constraint \u2211 j xj = 0 coming from our algebraic assumption).\nProof. Step 1: the infimum is achieved and invariant by permutation. For any x \u2208 RD such that \u2225x\u2225 \u2264 \u03c1, \u03c3i(x) \u2208 (0, 1) for all i \u2208 [D]. Furthermore, \u2207\u03c3i(x) = \u03c3i(x)1i \u2212 \u03c3i(x)\u03c3(x) , where we remind that (11, . . . ,1D) is the canonical basis of RD. Hence \u2207\u03c3i(x) \u0338= 0 and the supremum is achieved on the sphere. Note that B0(\u03c1) := { x \u2208 RD : \u2225x\u2225 = \u03c1,\u2211j xj = 0} is a compact set, and the infimum is a minimum. Consider i0 \u2208 [D] and y \u2208 B0(\u03c1) a joint minimizer such that\n\u03c3i0(y) = min i\u2208[D] min x\u2208B0(\u03c1) \u03c3i(x) . (59)\nRemark that Eq. (59) is invariant by permutation, i.e., for any permutation \u03c4 : [D] \u2192 [D], we have\n\u03c3\u03c4(i0)(\u03c4 \u00b7 y) = \u03c3i0(y) = min i\u2208[D] min x\u2208B0(\u03c1) \u03c3i(x) ,\nwhere \u03c4 \u00b7 y = (y\u03c4(i))i\u2208{1,...,D}. Hence, one can suppose without loss of generality that i0 = 1. Step 2: the coordinates of a minimizer are equal under z 7\u2192 z e\u2212z except at i0. In this setting, since we have for all i \u2208 {2, . . . , D}, \u03c31(y) \u2264 \u03c3i(y) this implies that y1 \u2264 yi. Using the fact that \u2211 j yj = 0, when summing the previous inequality for all i \u2208 [D], one gets y1 \u2264 0. Note that in fact y1 < 0. Indeed, if y1 = 0, we have yi = 0 for all i \u2208 [D] and \u2225y\u2225 = 0 \u0338= \u03c1. We are in the setting of a minimization problem under constrains, namely y solves\nminimize \u03c3(x) subject to \u2225x\u22252 = \u03c12, \u27e8x,1\u27e9 = 0 .\nUsing the Lagrange-Multiplier Theorem, there exist \u03b1, \u03b2 \u2208 R such that for the aforementioned solution y we have \u2207\u03c3(y) + \u03b1\u2207 ( \u2225\u00b7\u22252 \u2212 \u03c12 ) (y) + \u03b2\u2207 (\u27e8\u00b7,1\u27e9) (y) = 0 ,\nwhich translate into\n\u03c31(y)\u2212 \u03c31(y)2 + 2\u03b1y1 + \u03b2 = 0 \u2212\u03c31(y)\u03c3i(y) + 2\u03b1yi + \u03b2 = 0 for i \u2208 {2, . . . , D} .\nRemark that \u03b2 = 0 and \u03b1 \u0338= 0. Indeed, by summing all these previous equality, and using that\u2211 yi = 0 and\u2211\u03c3i = 1, one gets D\u03b2 = 0 and \u03b2 = 0. Remind that y1 < 0, and since\n\u03c31(y)\u2212 \u03c31(y)2 + 2\u03b1y1 = 0 ,\nif \u03b1 = 0 then \u03c31(y)(1\u2212 \u03c31(y)) = 1, which is not possible. Hence \u03b1 \u0338= 0. We also have that for all i, j \u2208 {2, . . . , D},\n\u03c31(y) = 2\u03b1yi \u03c3i(y) = 2\u03b1yj \u03c3j(y) .\nUsing that fact that \u03b1 \u0338= 0, this implies that yieyi = y2 ey2 for all i \u2208 {2, . . . , D} and that\n0 = y1 + D\u2211 i=2 yi = y1 + ( D\u2211 i=2 y2 e \u2212y2 eyi ) = y1 + ( D\u2211 i=1 eyi \u2212 ey1 ) y2 e \u2212y2 = y1 + e y1 1\u2212 \u03c31(y) \u03c31(y) y2 e \u2212y2 .\nAs a consequence, for all i \u2208 {2, . . . , D},\nyi e \u2212yi = y2 e \u2212y2 = \u2212y1 e\u2212y1 \u03c31(y)\n1\u2212 \u03c31(y) . (60)\nStep 3: expression of the minimum in function of the solution of z e\u2212z = c. Since y1 < 0, the previous equality (60) implies that yi > 0 for i \u2208 {2, . . . , D}. For any 0 < c < e\u22121, the equation x e\u2212x = c has exactly two solutions, which we call 0 < y\u2212(c) < 1 < y+(c). Let us define\nn = \u2223\u2223\u2223\u2223{2 \u2264 i \u2264 D, yi = y\u2212(\u2212y1 e\u2212y1 \u03c31(y)1\u2212 \u03c31(y) )}\u2223\u2223\u2223\u2223\nthe number of \u201cnegative\u201d solutions. By definition of n, we necessarily have\n\u03c31(y) = ey1\ney1 +n ey\u2212 +(D \u2212 1\u2212 n) ey+ . (61)\nRecall that \u2211\nj yj = 0 and \u2225y\u2225 = \u03c1, hence\ny1 + ny\u2212 + (D \u2212 1\u2212 n)y+ = 0 (62) y21 + ny 2 \u2212 + (D \u2212 1\u2212 n)y2+ = \u03c12 . (63)\nWhen n = D \u2212 1, one can solve the previous equations and we have y1 = \u03c1 \u221a D D\u22121 and yj = \u03c1 \u221a 1 D(D\u22121) for all j \u2208 {2, \u00b7 \u00b7 \u00b7 , D}, and \u03c31(y) = 1 1+(D\u22121) e \u221a D D\u22121 \u03c1 .\nSince the problem here is symmetric in y\u2212 and y+, one can suppose that 1 \u2264 n \u2264 D \u2212 2. Hence rewriting Eq. (62), we obtain y+ = \u2212 ( n\nD \u2212 1\u2212 ny\u2212 + 1 D \u2212 1\u2212 ny1 ) .\nReplacing the value of y+ by the right-hand side of the previous display in Eq. (63), we obtain( n+ n2\nD \u2212 1\u2212 n\n) y2\u2212 + 2\nn D \u2212 1\u2212 ny1y\u2212 \u2212 ( \u03c12 \u2212 y21 ( 1 +\n1\nD \u2212 1\u2212 n\n)) = 0 .\nDividing by ( n+ n 2\nD\u22121\u2212n\n) , we get\ny2\u2212 + 2 D \u2212 1y1y\u2212 \u2212 ( D \u2212 1\u2212 n n(D \u2212 1) \u03c1 2 \u2212 D \u2212 n n(D \u2212 1)y 2 1 ) = 0 .\nWe can see the previous display as a quadratic equation in y\u2212, which we now solve. There exists \u03b5 \u2208 {\u22121, 1} such that\ny\u2212 = \u2212y1 \u2212 \u03b5\n\u221a D\u22121\u2212n\nn \u2206(y1)\nD \u2212 1 ,\nwhere \u2206(y1) = \u221a (D \u2212 1)\u03c12 \u2212Dy21 .\nNote that in this setting one necessarily have\n\u2212 \u221a D\nD \u2212 1\u03c1 \u2264 y1 \u2264 0 , (64)\nsince we have already seen that the minimization problem under constrains has a solution, y\u2212 and y+ exist and 1 \u2264 n \u2264 D\u22122. When the previous condition is not satisfied, necessarily in the case n = D \u2212 1 or n = 0 holds which has already been treated.\nFinally, when using the fact that y+ = \u2212 1D\u22121\u2212n (y1 + ny\u2212),\ny+ = \u2212y1 + \u03b5\n\u221a n\nD\u22121\u2212n\u2206(y1)\nD \u2212 1 .\nAnd since y+ > y\u2212, we have\n\u03b5\n\u221a n D \u2212 1\u2212 n > \u2212\u03b5 \u221a D \u2212 1\u2212 n n ,\nand we conclude that \u03b5 = 1, i.e.,\ny\u2212 = \u2212y1 \u2212\n\u221a D\u22121\u2212n\nn \u2206(y1)\nD \u2212 1 (65)\ny+ = \u2212y1 +\n\u221a n\nD\u22121\u2212n\u2206(y1)\nD \u2212 1 . (66)\nTaking a step back, we have managed to express all coordinates as an explicit function of y1.\nStep 4: closed-form expression of the minimum. Replacing y\u2212 and y+ in Eq. (61) by the expression obtained in Eqs. (65) and (66), we have to minimize the function of y1 defined\ng(y1) = ey1\ney1 +n e \u2212y1\u2212\n\u221a D\u22121\u2212n\nn \u2206(y1) D\u22121 + (D \u2212 1\u2212 n)e \u2212y1+\n\u221a n\nD\u22121\u2212n\u2206(y1) D\u22121\n= ( 1 + e\u2212 D D\u22121y1 ( n e\u2212 \u221a D\u22121\u2212n n \u2206(y1) D\u22121 +(D \u2212 1\u2212 n) e \u221a n D\u22121\u2212n\u2206(y1) D\u22121 ))\u22121\n= ( 1 + e\u2212 D D\u22121y1 \u221a n(D \u2212 1\u2212 n)\n\u00d7 (\u221a\nn D \u2212 1\u2212 n e \u2212 \u221a D\u22121\u2212n n \u2206(y1) D\u22121 +\n\u221a D \u2212 1\u2212 n\nn e\n\u221a n\nD\u22121\u2212n\u2206(y1) D\u22121\n))\u22121\nNote that for y1 satisfying Eq. (64) y1 is non-positive. It is elementary to show that y 7\u2192 \u2206(y) is an increasing function on R\u2212. Moreover, for all a > 0, h : x 7\u2192 a e\u2212 x a + 1a e\nax is an increasing function on R+. Thus y 7\u2192 h(\u2206(y)/(D \u2212 1)) is a decreasing mapping on R\u2212. Hence, by taking a =\n\u221a D\u22121\u2212n\nn , we have\nh\n( \u2206(y1)\nD \u2212 1\n) \u2264 h(0) = \u221a D \u2212 1\u2212 n\nn +\n\u221a n\nD \u2212 1\u2212 n ,\nand \u221a (D \u2212 1\u2212 n)nh ( \u2206(y)\nD \u2212 1\n) \u2264 \u221a (D \u2212 1\u2212 n)n (\u221a D \u2212 1\u2212 n\nn +\n\u221a n\nD \u2212 1\u2212 n\n) = D \u2212 1 .\nUsing this last display, we write\ng(y1) \u2265 1\n1 + (D \u2212 1) e\u2212 DD\u22121y1 .\nThe right-hand side is an increasing function of y1, whose minimum value is \u2212 \u221a D\u22121 D \u03c1, and this gives\ng(y1) = 1 1 + (D \u2212 1) e \u221a D D\u22121\u03c1 . (67)\nThus equality in the key bound is reached for\ny = ( \u2212 \u221a D \u2212 1 D \u03c1, \u221a 1 D(D \u2212 1)\u03c1, . . . , \u221a\n1 D(D \u2212 1)\u03c1 )\u22a4 ,\nwith value given by Eq. (67).\nStep 5: Proof for the maximum. Following the same reasoning as in the proof of Theorem H.9, we show that the maximum is reached for the point (\u221a\nD \u2212 1 D\n\u03c1,\u2212 \u221a\n1 D(D \u2212 1)\u03c1, . . . ,\u2212 \u221a\n1 D(D \u2212 1)\u03c1 )\u22a4 ,\nand the coordinate \u03c31, and we get the wanted result."
        },
        {
            "heading": "I. Additional experimental results",
            "text": "In this section we collect additional experimental results.\nI.1. Illustration of Theorem 5.1 with another implementation\nIn Figure 7 and 8, we present a replication of the experiment presented in Section 5.2 of the main paper. This time, we used the gensim implementation of the doc2vec model. The main difference is the use of hierarchical softmax instead of softmax. Despite this difference, the empirical results remain consistent with our theoretical claims and experimental results with an ad hoc implementation. We conjecture that the hierarchical softmax has similar algebraic properties to the softmax, in particular kernel stability, which would justify conducting the same analysis.\nI.2. Illustration of Lemma F.6\nIn Figure 9, we illustrate the bound provided by Lemma F.6. We consider the 5 longest examples of the IMDB dataset and create artificial documents of increasing length as before. We observe no asymptotic dependency in T , as predicted by the theoretical result.\nI.3. Singular values of R\nIn Figure 10, we empirically check that the singular values of the (learned) R are well-behaved. We considered the matrices from our local model and report the histogram of their singular values in log scale in Figure 10."
        }
    ],
    "title": "On the Robustness of Text Vectorizers",
    "year": 2023
}