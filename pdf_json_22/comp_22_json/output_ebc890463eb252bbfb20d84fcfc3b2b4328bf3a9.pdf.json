{
    "abstractText": "Classical methods for model order selection often fail in scenarios with low SNR or few snapshots. Deep learningbased methods are promising alternatives for such challenging situations as they compensate lack of information in the available observations with training on large datasets. This manuscript proposes an approach that uses a variational autoencoder (VAE) for model order selection. The idea is to learn a parameterized conditional covariance matrix at the VAE decoder that approximates the true signal covariance matrix. The method is unsupervised and only requires a small representative dataset for calibration after training the VAE. Numerical simulations show that the proposed method outperforms classical methods and even reaches or beats a supervised approach depending on the considered snapshots.",
    "authors": [
        {
            "affiliations": [],
            "name": "Michael Baur"
        },
        {
            "affiliations": [],
            "name": "Franz Wei\u00dfer"
        },
        {
            "affiliations": [],
            "name": "Benedikt B\u00f6ck"
        },
        {
            "affiliations": [],
            "name": "Wolfgang Utschick"
        }
    ],
    "id": "SP:b39eca8e683b734960813502420a72e77c7d82a8",
    "references": [
        {
            "authors": [
                "H. Krim",
                "M. Viberg"
            ],
            "title": "Two Decades of Array Signal Processing Research: The Parametric Approach",
            "venue": "IEEE Signal Process. Mag., vol. 13, no. 4, pp. 67\u201394, 1996.",
            "year": 1996
        },
        {
            "authors": [
                "A. Barthelme",
                "W. Utschick"
            ],
            "title": "ChainNet: Neural Network-Based Successive Spectral Analysis",
            "venue": "arXiv preprint arXiv:2105.03742, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "P. Stoica",
                "Y. Selen"
            ],
            "title": "Model-Order Selection: A review of information criterion rules",
            "venue": "IEEE Signal Process. Mag., vol. 21, no. 4, pp. 36\u201347, 2004.",
            "year": 2004
        },
        {
            "authors": [
                "J. Ding",
                "V. Tarokh",
                "Y. Yang"
            ],
            "title": "Model Selection Techniques: An Overview",
            "venue": "IEEE Signal Process. Mag., vol. 35, no. 6, pp. 16\u201334, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "M. Wax",
                "T. Kailath"
            ],
            "title": "Detection of Signals by Information Theoretic Criteria",
            "venue": "IEEE Trans. Acoust., vol. 33, no. 2, pp. 387\u2013392, 1985.",
            "year": 1985
        },
        {
            "authors": [
                "O. Bialer",
                "N. Garnett",
                "T. Tirer"
            ],
            "title": "Performance Advantages of Deep Neural Networks for Angle of Arrival Estimation",
            "venue": "2019 IEEE Int. Conf. Acoust. Speech Signal Process. Brighton, UK: IEEE, 2019, pp. 3907\u20133911.",
            "year": 2019
        },
        {
            "authors": [
                "A. Barthelme",
                "R. Wiesmayr",
                "W. Utschick"
            ],
            "title": "Model Order Selection in DoA Scenarios via Cross-entropy Based Machine Learning Techniques",
            "venue": "2020 IEEE Int. Conf. Acoust. Speech Signal Process. Barcelona, Spain: IEEE, 2020, pp. 4622\u20134626.",
            "year": 2020
        },
        {
            "authors": [
                "A. Barthelme",
                "W. Utschick"
            ],
            "title": "A Machine Learning Approach to DoA Estimation and Model Order Selection for Antenna Arrays with Subarray Sampling",
            "venue": "IEEE Trans. Signal Process., vol. 69, pp. 3075\u20133087, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Zhou",
                "T. Li",
                "Y. Li",
                "R. Zhang",
                "Y. Ruan"
            ],
            "title": "Source Number Estimation via Machine Learning Based on Eigenvalue Preprocessing",
            "venue": "IEEE Commun. Lett., vol. 26, no. 10, pp. 2360\u20132364, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "venue": "Proc. 2nd Int. Conf. Learn. Represent., Banff, Canada, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "W. Xia",
                "S. Rangan",
                "M. Mezzavilla",
                "A. Lozano",
                "G. Geraci",
                "V. Semkin",
                "G. Loianno"
            ],
            "title": "Generative Neural Network Channel Modeling for Millimeter-Wave UAV Communication",
            "venue": "IEEE Trans. Wirel. Commun., vol. 21, no. 11, pp. 9417\u20139431, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Caciularu",
                "D. Burshtein"
            ],
            "title": "Blind Channel Equalization Using Variational Autoencoders",
            "venue": "2018 IEEE Int. Conf. Commun. Work. Kansas City, USA: IEEE, 2018, pp. 1\u20136.",
            "year": 2018
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "Unsupervised Linear and Nonlinear Channel Equalization and Decoding Using Variational Autoencoders",
            "venue": "IEEE Trans. Cogn. Commun. Netw., vol. 6, no. 3, pp. 1003\u20131018, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "V. Lauinger",
                "F. Buchali",
                "L. Schmalen"
            ],
            "title": "Blind Equalization and Channel Estimation in Coherent Optical Communications Using Variational Autoencoders",
            "venue": "IEEE J. Sel. Areas Commun., vol. 40, no. 9, pp. 2529\u2013 2539, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Baur",
                "B. Fesl",
                "M. Koller",
                "W. Utschick"
            ],
            "title": "Variational Autoencoder Leveraged MMSE Channel Estimation",
            "venue": "2022 56th Asilomar Conf. Signals, Syst. Comput. Pacific Grove, USA: IEEE, 2022, pp. 527\u2013532.",
            "year": 2022
        },
        {
            "authors": [
                "D.P. Kingma",
                "M. Welling"
            ],
            "title": "An Introduction to Variational Autoencoders",
            "venue": "Found. Trends\u00ae Mach. Learn., vol. 12, no. 4, pp. 307\u2013392, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Liaw",
                "E. Liang",
                "R. Nishihara",
                "P. Moritz",
                "J.E. Gonzalez",
                "I. Stoica"
            ],
            "title": "Tune: A Research Platform for Distributed Model Selection and Training",
            "venue": "arXiv preprint arXiv:1807.05118, 2018.",
            "year": 1807
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "Proc. 3rd Int. Conf. Learn. Represent., San Diego, USA, 2015.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014Variational autoencoder, generative model, model order, machine learning, direction of arrival estimation.\nI. INTRODUCTION Model order (MO) selection determines the number of impinging wavefronts incident at a receiver. The MO is an essential quantity for direction of arrival (DoA) estimation, both for classical [1] and current deep learning (DL) methods [2]. Most well-known MO selection approaches utilize information criteria (IC) [3]. More current treatment of model selection is covered in [4], where DL methods are left out, however.\nA popular IC-based method for MO selection reaches back to the 80s [5]. The method is based on a subspace decomposition of the sample covariance matrix and performs well in cases with high signal-to-noise ratio (SNR) and many snapshots. In contrast, for low SNR or few snapshots, the sample covariance matrix is a bad estimate of the true covariance matrix. Consequently, the method fails in these cases. DL methods are promising candidates to perform well in such difficult situations. As a result of the repeated presentation of samples during the offline training phase, the DL model extracts overall prior information of the data and can compensate for lack of knowledge in observations during the deployment phase, e.g., if only a few snapshots are available. The strong performance of DL-based methods is demonstrated in [6]\u2013[9], which use relatively simple neural network architectures to determine the MO based on the (preprocessed) snapshots. The methods are supervised, requiring access to a dataset, where observations are labeled with their corresponding MO.\nThis work is funded by the Bavarian Ministry of Economic Affairs, Regional Development and Energy within the project 6G Future Lab Bavaria. The authors acknowledge the financial support by the Federal Ministry of Education and Research of Germany in the programme of \u201cSouvera\u0308n. Digital. Vernetzt.\u201d. Joint project 6G-life, project identification number: 16KISK002\nIf the exact signal model and the exact model of the antenna array were available, it would be possible to generate unlimited amounts of labeled data. This assumption, however, only holds for idealistic circumstances, e.g., calibrated antenna arrays, that do not hold in reality. Under realistic conditions, data would only be available in the form of measurements without any labels. These aspects motivate the investigation of unsupervised learning methods because they do not require labeling. Unsupervised methods additionally offer to include model imperfections in the framework directly. The variational autoencoder (VAE) is an unsupervised framework that learns the data distribution by maximizing a lower bound to the data log-likelihood [10]. It belongs to the class of generative models, which means that the model can generate entirely new samples from the learned distribution. Despite its popularity in image processing and related disciplines, the VAE is rarely employed in communications tasks. A current publication investigates the generative modeling performance of the VAE in a millimeter-wave UAV scenario [11]. Channel equalization is another domain where the VAE is applied successfully [12]\u2013 [14], as well as channel estimation [15].\nMotivated by the performance of the VAE channel estimator in [15], we propose a method for MO selection based on a VAE. Our method is unsupervised, and only a small representative dataset is required to distinctly assign the MO after training of the VAE. The method is supposed to fill the gaps where classical methods for MO selection fail, i.e., at low SNR and few snapshots. The contributions are as follows. By parameterizing the covariance matrix of the conditionally Gaussian distribution at the VAE decoder with an oversampled discrete fourier transform (DFT) matrix, we can learn an approximation of the eigenvalues of the true signal covariance matrix. We leverage the approximation to determine the MO with a custom evaluation routine based on entropy. Numerical simulations show the advantage of the proposed framework over IC-based methods in the considered scenarios. Moreover, the proposed method can beat a supervised MO selection method in a single snapshot scenario."
        },
        {
            "heading": "II. SYSTEM MODEL",
            "text": "An antenna array with N elements receives signals from L sources in the far field, which characterize the impinging wavefronts. The received signal vector y(t) \u2208 CN at snapshot t of in total T snapshots is expressed as\ny(t) = A(\u03b8)s(t) + n(t), t = 1, . . . , T (1)\n\u00a9 2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any\ncopyrighted component of this work in other works.\nar X\niv :2\n21 0.\n15 40\n7v 3\n[ ee\nss .S\nP] 2\n8 Ju\nn 20\n23\nwith the array manifold A(\u03b8) \u2208 CN\u00d7L, the DoA \u03b8, the transmitted signal s(t) \u223c NC(0,Cs), and additive white Gaussian noise n(t) \u223c NC(0, \u03c32nI). The columns of the array manifold are defined by the steering vectors of the employed array geometry and evaluated at the respective angles. Transmitted signals are assumed to originate from uncorrelated sources with different powers, i.e., the matrix Cs is diagonal with positive and potentially dissimilar elements on its diagonal. The covariance matrix of the received signal is\nCy = R+ \u03c3 2 nI, R = A(\u03b8)CsA(\u03b8) H . (2)\nNote that this result only holds for a fixed \u03b8, which is assumed to be constant over the snapshots.\nWe consider a uniform linear array (ULA) with halfwavelength spacing. The steering vector of a ULA at angle \u03b8 is a(\u03b8) = [1, exp(j\u03c0 sin(\u03b8)), . . . , exp(j\u03c0(N \u2212 1) sin(\u03b8))]H. We furthermore set tr(Cs) = 1, which allows us to define the SNR as 1/\u03c32n."
        },
        {
            "heading": "III. MODEL ORDER SELECTION TECHNIQUES",
            "text": "The goal of MO selection in a typical DoA scenario is to determine the number of sources L. In principle, the number of non-zero eigenvalues of R gives the MO. It is evident that R is not accessible during operation, which requires us to leverage the available snapshots y(t) to determine the MO, as it is the only information we receive, besides structural information, e.g., of the antenna array.\nA. Information Criteria\nClassical approaches for MO selection are often based on IC [3]. They evaluate the quality of a model for given data and account for the degrees of freedom by an additive penalty term. Among the most well-known IC rules are the Akaike information criterion (AIC) or the Bayesian information criterion (BIC). The latter is also known as maximum description length (MDL), the term we use in this work.\nAn approach to apply the AIC and MDL to the MO selection task for the system model in (1) is presented in [5]. The authors compute a maximum likelihood estimate for every MO. Afterward, they obtain the MO by a subspace decomposition of the sample covariance matrix C\u0302 = 1T \u2211T t=1 y(t)y(t)\nH . The performance of such an approach is highly dependent on the quality of C\u0302 in estimating the true covariance matrix of the snapshots. Consequently, for a low number of snapshots or low SNR, the AIC and MDL methods, as they are defined in [5], are error-prone in determining the correct MO. For cases where C\u0302 is of good quality, the IC based methods perform well. Therefore, this work aims to develop a method for MO selection that works well in cases not covered by IC and is additionally unsupervised.\nB. Variational Autoencoder Preliminaries\nConsider a VAE [10] as Fig. 1 shows it. A data sample y is put into the encoder to yield a sample z of the variational distribution q\u03d5(z|y), referred to as a latent sample. The latent sample is fed into the decoder to obtain a conditional\ncovariance matrix Cy|z . The training objective of a VAE during stochastic optimization is to maximize the evidence lower bound (ELBO), which is given by\nL\u03d5,\u03d1(y) = Eq\u03d5(z|y) [log p\u03d1(y|z)]\u2212DKL(q\u03d5(z|y) \u2225 p(z)). (3) The ELBO is a lower bound to the data log-likelihood. The outcome of the ELBO maximization are estimates for the distributions q\u03d5(z|y) and p\u03d1(y|z). A common choice for the involved probability distributions in the ELBO is Gaussians. Thus, we set p(z) = N (0, I), p\u03d1(y|z) = NC(0,Cy|z), and q\u03d5(z|y) = N (\u00b5z|y,diag(\u03c32z|y)). We can compute the ELBO in closed-form with these definitions, which is explained in detail in [15]. As a result, we have L\u03d5,\u03d1(y) = 1\n2 1T\n( log\u03c32z|y \u2212 \u00b52z|y \u2212 \u03c32z|y ) \u2212 log detCy|z \u2212 yH C\u22121y|zy\n(4) as optimization objective, with the all-ones vector 1. We obtain \u00b5z|y and \u03c3z|y with the encoder and Cy|z with the decoder. The subscripts \u03d5 and \u03d1 correspond to the encoder and decoder neural network weights, respectively, and parameterize \u00b5z|y , \u03c3z|y , and Cy|z . Finally, using the reparameterization trick to obtain the latent vector z yields the VAE proposed in the related literature [10]. For further background details, we refer the reader to [15], [16]. In the former [15], the authors describe a similar application case."
        },
        {
            "heading": "C. Model Order Selection by Variational Autoencoding",
            "text": "Recall that the VAE maximizes a lower bound to the data log-likelihood. An intuitive way to determine the MO in combination with a VAE is to work with the conditional covariance matrix at the decoder Cy|z . If it is a good estimate of the actual covariance matrix Cy , it should be possible to infer the MO based on Cy|z . Our choice for the distribution p\u03d1(y|z) = NC(0,Cy|z) differs from the standard version in the VAE literature. The conditional covariance matrix is usually either assumed to be diagonal or a scaled identity. Furthermore, the mean value of p\u03d1(y|z) is commonly a learnable non-zero vector. While this choice achieves good results, e.g., for generating new samples, it is unsuited for our purposes because the true covariance matrix has a rich structure due to the given antenna array. For instance, the covariance matrix is Toeplitz structured for a ULA and uncorrelated sources, which we cannot model with the standard version of the VAE.\nTo this end, the authors in [15] parameterize the channel covariance matrix as a circulant matrix F H diag(c\u0303)F with a DFT matrix F \u2208 CN\u00d7N and c\u0303 \u2208 RN+ to approximate the Toeplitz channel covariance matrix of a ULA. The parameterization performs very well for channel estimation. However, in our experiments for MO selection, we discovered that the eigenvalues of Cy|z , which are c\u0303 + \u03c32n1 with the circulant parameterization, are not representative for the MO. Unfortunately, this property prevents the direct application of IC rules to determine the MO. For example, a sample with a true MO of one might have a single eigenvalue larger than the noise variance and other smaller but non-negligible eigenvalues in addition to the single dominant eigenvalue. This relation comes from the fact that the row vectors of F are steering vectors of the ULA evaluated at different angles. Hence, the learned eigenvalues express the variance of the signal space in terms of the available steering vectors in F . If an eigendirection is not collinear to one of the steering vectors, it is represented as a linear combination of the Fourier basis with more basis vectors than the actual MO tells. Consequently, we have more eigenvalues greater than the noise variance as the actual MO.\nOne possible adaption to the circulant parameterization to cope with this problem is to use an oversampled DFT matrix F\u0303 \u2208 CKN\u00d7N with an oversampling factor K \u2208 N+. Such a matrix contains a finer grid of directions in the signal space, which allows us to put more energy into fewer eigenvalues. The drawback is that the right-inverse of F\u0303 does not exist. Only the left-inverse exists, which is F\u0303 H. The non-existence of the right-inverse results in higher computational complexity during the training phase because the inverse of the conditional covariance matrix\nCy|z = F\u0303 H diag(cy|z)F\u0303 = F\u0303 H diag(c+ \u03c32n1)F\u0303 , (5)\nwith cy|z, c \u2208 RKN+ , must be computed for every y. Note that c is the output of the decoder. However, this is acceptable as all additional effort happens during the training phase and can be done offline and in advance. So, since the IC rules are not directly applicable, the question arises: how can we determine the MO with the described VAE that we train on unlabeled data originating from the system model in (1)?\nSince the eigenvalues of F\u0303 H diag(c)F\u0303 are representative for the MO and defined by the vector c, which determines the weighting of the rows in F\u0303 , it should be possible to determine the MO based on c. However, since F\u0303 is an oversampled DFT matrix, it is still not possible to say that, e.g., m dominant values in c allow to infer an MO of m. Instead, the energy distribution over c is tied to the MO. We should therefore interpret c as a feature vector. Normalizing the vector by its sum yields a non-negative vector that sums to one. However, a criterion to separate the normalized vectors based on their MO is needed. The entropy is a suitable candidate for this task as it measures how evenly the energy is distributed over the values in c. The entropy is high if the total energy is distributed over many values of c, which is a sign of a high MO. In contrast, the entropy is small if almost all the energy concentrates on one value, indicating a low MO.\nIn our experiments, it was better to learn a covariance matrix at the decoder for every snapshot separately and then average the c after the training than to learn c for all snapshots jointly. More precisely, let the vector c(t) be obtained at the VAE decoder for the encoder input y(t). In the next step, the quantities\nc\u0302 = 1\nT T\u2211 t=1 c(t), c\u0304 = c\u0302\u2211KN i=1 c\u0302i (6)\nare computed, which averages the decoder outputs over the snapshots and normalizes the resulting vector such that the sum of the elements is one. Afterward, we calculate the entropy of c\u0304 by treating all elements c\u0304i in the vector as probabilities for different outcomes of a random variable, i.e.,\nH(c\u0304) = KN\u2211 i=1 \u2212c\u0304i log(c\u0304i). (7)\nIf we calculate the entropies according to this procedure, we obtain a histogram as in Fig. 2. An ex-post view shows that, from left to right, the different colors belong to MO one to four. We excluded the data of MO zero because we find it by comparing the highest value in c\u0302 with the noise variance, which does not require further steps. Consequently, a value smaller than \u03c32n refers to MO zero. The histogram shows that entropies can provide a good measure of the classification of observations in terms of their underlying MO. What remains is to find suitable thresholds for entropy values that allow the determination of the MO. More precisely, since we want to remain unsupervised, we use a one-dimensional Gaussian mixture model (GMM) to model the entropy distribution to obtain thresholds. Finally, we determine the MO with the GMM component corresponding to the calculated entropy value. Exemplarily, in Fig. 2, an entropy value of 1.4 would be assigned to the GMM component with the second largest mean value, which represents MO two. The proposed method is not limited to using a GMM. Any tool that yields thresholds for entropy values to assign the correct MO may be used."
        },
        {
            "heading": "IV. RESULTS",
            "text": "This section first provides details regarding implementing the VAE architecture. Afterward, we present MO selection results for the proposed method for one and five snapshots.\nA. Implementation Details\nThe employed architectures follow the principles in [15]. Since the snapshots are complex-valued, we stack their real and imaginary parts to create input vectors with two convolutional channels (CCs). We decided to incorporate convolutional layers (CLs) in the neural networks inside the VAE as they have demonstrated superior performance compared to networks solely built from linear layers (LLs) in vast amounts of prior work. The neural networks in every VAE have the following architectures. The encoder consists of three times a building block of a CL, a batch normalization (BN) layer, and a ReLU activation function. Each CL has kernel size seven. The input samples are mapped from two to 64, to 128, and to 192 CCs. We use a stride of one. The three blocks are followed by two LLs that map to \u00b5z|y and \u03c3z|y , which are of dimensionality 16. With \u00b5z|y and \u03c3z|y , the reparameterization trick is performed to obtain one sample z of q\u03d5(z|y). The decoder architecture is analog to the encoder architecture, just flipped symmetrically and transposed CLs are used instead of standard CLs. At the output of the decoder, a LL maps to c. We found the architecture with a random search over the hyperparameter space for the configuration that yields the highest ELBO value with the Tune package [17]. We implement our neural networks with the help of PyTorch and optimize them with the Adam optimizer [18] with a learning rate of 10\u22124. The VAEs are trained for an SNR range from -16 to 26 dB until the ELBO saturates to also include border SNR samples. After the training, we use the model that yields the highest ELBO value. Note that the training with an SNR range implies that the proposed approach is SNR independent."
        },
        {
            "heading": "B. Numerical Simulations",
            "text": "We create training and evaluation data according to the system model in (1) for MO zero to four. During the training phase, we create 104 new samples per MO in every epoch and train with a batch size of 16. Each training sample consists of ten snapshots. After training, we create another 103 entirely new samples per MO for evaluation purposes. Please note that the numbers 104 and 103 do not refer to the number of snapshots but to the number of samples in the training and evaluation data, respectively. Furthermore, labeling the samples with their true MO relates to the evaluation phase of the proposed methods. VAE training remains unsupervised. The information in the plots applies to the number of snapshots in the evaluation samples, i.e., either five or one snapshots are considered. Although the number of snapshots in the evaluation data differs from the training data, which is ten, we achieved better results when training with data featuring ten snapshots. The same VAEs are used for the evaluations in Fig. 3 and 4. The entries of Cs = diag(cs) are sampled from a uniform distribution between 1/8 and 1 and afterward normalized such that they sum to one. The DoA of the ULA is sampled from a uniform distribution between \u2212\u03c0/2 and \u03c0/2. The oversampling factor is K = 4, and the number of antennas is N = 64.\nIn total, we evaluate four different VAE-based approaches for MO selection. The first model employs the method based on the entropy of c\u0304 from (6) described in Section III-C. It assumes knowledge of the noise variance \u03c32n and is called VAE-c. Moreover, to explore the performance gap if working with c\u0302 instead of the eigenvalues of F\u0303 H diag(c\u0302)F\u0303 , we apply the same entropy-based method to the eigenvalues. In particular, we apply the method to the eigenvalues of\nF\u0303 H diag(c\u0302)F\u0303 = 1\nT T\u2211 t=1 F\u0303 H diag(c(t))F\u0303 . (8)\nThis approach is named VAE-e. Both VAE-c and VAE-e can be implemented as versions where the noise variance is left as an optimization variable. We also implement these versions and term them VAE-c-\u03c32 and VAE-e-\u03c32. Please note again that, for all of our VAE-based approaches, the same VAE model is used for every SNR.\nThe proposed method discriminates between different MOs, but with a possible ambiguity that arises from strongly underrepresented MO data. The ambiguity can be reasoned based on Fig. 2, where underrepresented MOs lead to shifts of the entropy thresholds. Thus, an additional representative dataset with few samples that contains every MO remains essential to determine the entropy thresholds of a MO after VAE training. As outlined in Section III-C, we find the thresholds with a GMM. For our simulations, we assume the evaluation data is representative and can be used for this task. During the evaluation of the VAEs, every snapshot y(t) is fed into the encoder separately to obtain \u00b5z|y(t), which is put into the decoder to receive the respective c(t). Hence, we discard \u03c3z|y(t) and skip the sampling process in the latent space.\nWe also implement the supervised approach from [7], named CovNet, to find out how well our methods perform compared to a supervised method. In our experiments, CovNet delivers better evaluation performance on one- and fivesnapshot data when we train it on ten-snapshot data, which is why we also train CovNet solely on ten-snapshot data and use this model for all evaluations.\nIn Fig. 3, we display the performance concerning MO selection for evaluation data with five snapshots. The ordinate displays the percentage of correctly determined MOs between zero and four, which means that we have five MOs in total. As can be seen, the IC methods AIC and MDL deteriorate to guessing the MO since the sample covariance matrix is of bad quality. Furthermore, we observe that the supervised CovNet method outperforms all other methods. The VAEbased methods manage to shrink the performance gap between CovNet at high SNR but stay a few percentages below CovNet. When working with c\u0302 from (6) instead of the eigenvalues from (8), the curves of VAE-c and VAE-e show that a few percentages of accuracy are lost with VAE-c. The same holds for VAE-e-\u03c32 and VAE-c-\u03c32. Leaving the noise as an additional optimization parameter decreases the performance from low SNR to approximately 10 dB. From this point on, there is almost no gap between VAE-e and VAE-e-\u03c32, and\nVAE-c and VAE-c-\u03c32, highlighting that VAE-e-\u03c32and VAEc-\u03c32estimate the noise variance with low error.\nFig. 4 presents the same evaluation as in Fig. 3 but for evaluation data with one snapshot. We do not plot the results of the IC-based methods here as they would work with a rank one sample covariance matrix. In this case, the sample covariance matrix only has one non-zero eigenvalue, making the application of the IC-based methods useless. In Fig. 4, the VAEs that know about the noise variance can beat CovNet for every SNR value. Only VAE-c-\u03c32 performs worse than CovNet from 0 to 10 dB. This shows that the proposed approach can also beat a supervised method in MO selection if only a single snapshot is considered. Additionally, the performance gap between VAE-e and VAE-c is almost negligible for a single snapshot.\nWith VAE-c and VAE-c-\u03c32, we have low complexity methods that only require a forward pass through the neural networks and evaluation of the entropy routine from Section III-C. If it is possible to invest more computational complexity, VAEe and VAE-e-\u03c32 provide performance gains compared to their pendants VAE-c and VAE-c-\u03c32, respectively."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "In this manuscript, we present methods for MO selection based on a VAE. The main idea is to learn a parameterized covariance matrix at the VAE decoder. For the considered ULA, this is done with the help of an oversampled DFTmatrix. Simulation results highlight that the proposed methods are suitable for scenarios with very low snapshots, where ICbased approaches fail. Comparisons with a supervised method highlight that the proposed methods can compete with a supervised approach and even beat it in a single snapshot scenario. Interesting future steps include the investigation of other array geometries, correlated sources, and the optimal oversampling factor of the DFT-matrix."
        }
    ],
    "title": "Model Order Selection with Variational Autoencoding",
    "year": 2023
}