{
    "abstractText": "Current target detection methods have achieved high accuracy for detecting large and medium-sized targets. However, due to factors such as the small number of pixels and features available for targets in images, the detection performance for small targets is generally unsatisfactory. In addition, the real-time performance of target detection is also critical. In conclusion, a modified lightweight architecture for real-time small target detection, i.e., MBAB-YOLO, is proposed based on You Only Look Once (YOLO) model by combining channel-wise attention block, space-attention block and multi-branchConvNet (Convolutional Neural network) structure. Specifically, our method is more suitable for the rich scale information of small targets through proposed adaptive multi-receptive-field focusing, and then combines proposed blended attention block (BAB) to re-calibrate small target information to make it more prominent and improve the discriminability of small target features. Finally, extensive experiments have been conducted on the open source data set for the proposed real-time small target detection method, i.e., MBAB-YOLO. The results of ablation experiment and contrast experiment show that our method has excellent performance, not only with high detection accuracy, but also with fast detection speed. Compared with the various benchmark methods, it achieves a good trade-off between the two aspects mentioned above. In addition, this paper gives a comprehensive and detailed review of the current work about small target detection from different several perspectives, which can be used as a reference for future researchers. INDEX TERMS Deep learning, target detection, channel-wise attention, space-attention, YOLO",
    "authors": [
        {
            "affiliations": [],
            "name": "Jun Zhang"
        },
        {
            "affiliations": [],
            "name": "Yizhen"
        },
        {
            "affiliations": [],
            "name": "Xiaohui Yu"
        },
        {
            "affiliations": [],
            "name": "Hongjing Bi"
        },
        {
            "affiliations": [],
            "name": "Zhipeng Chen"
        },
        {
            "affiliations": [],
            "name": "Huafeng Li"
        },
        {
            "affiliations": [],
            "name": "Runtao Yang"
        },
        {
            "affiliations": [],
            "name": "Jingjun Tian"
        }
    ],
    "id": "SP:2355f9ca94446f2fde6b4d929967dbd8ead4c1aa",
    "references": [
        {
            "authors": [
                "X Li",
                "Z Xie",
                "T Lai"
            ],
            "title": "NAS-WFPN: Neural Architecture Search Weighted Feature Pyramid Networks for Object Detection[C]// International Workshops on Security, Privacy and Anonymity in Computation, Communication and Storage",
            "year": 2020
        },
        {
            "authors": [
                "B Chen",
                "G Ghiasi",
                "H Liu"
            ],
            "title": "MnasFPN: Learning Latency- Aware Pyramid Architecture for Object Detection on Mobile Devices[C]// 2020",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2020
        },
        {
            "authors": [
                "S Jo",
                "Y Ju",
                "K Cho"
            ],
            "title": "SATI: Scalable And Traffic efficient data dissemination Infrastructure for sensor-based distributed information services[J",
            "year": 2022
        },
        {
            "authors": [
                "L Cai",
                "H Li",
                "W Dong"
            ],
            "title": "Micro-expression recognition using 3D DenseNet fused Squeeze-and-Excitation Networks[J",
            "venue": "Applied Soft Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Q Oxu",
                "X Qiao",
                "C Liu"
            ],
            "title": "Automated ECG classification using a non-local convolutional block attention module[J",
            "venue": "Computer Methods and Programs in Biomedicine,",
            "year": 2021
        },
        {
            "authors": [
                "S Bell",
                "L Zitnick C",
                "K Bala"
            ],
            "title": "Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks[J",
            "year": 2015
        },
        {
            "authors": [
                "R Girshick",
                "F Iandola",
                "T Darrell"
            ],
            "title": "Deformable Part Models are Convolutional Neural Networks[J",
            "year": 2015
        },
        {
            "authors": [
                "B Li",
                "Y Yao",
                "J Tan"
            ],
            "title": "Equalized Focal Loss for Dense Long- Tailed Object Detection[J",
            "year": 2022
        },
        {
            "authors": [
                "Yaocheng",
                "W Li",
                "Y Zhang"
            ],
            "title": "SNIPER Based Multi-Target and Multi-Scale Aerial Image Processing Method[J",
            "venue": "Journal of Physics: Conference Series,",
            "year": 2020
        },
        {
            "authors": [
                "J Liu",
                "D Li",
                "R Zheng"
            ],
            "title": "RankDetNet: Delving into Ranking Constraints for Object Detection[C]// Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "T Vu",
                "H Jang",
                "X Pham T"
            ],
            "title": "Cascade RPN: Delving into High- Quality Region Proposal Network with Adaptive Convolution[J",
            "year": 2019
        },
        {
            "authors": [
                "Y Chen P",
                "W Hsieh J",
                "M Gochoo"
            ],
            "title": "Light-Weight Mixed Stage Partial Network for Surveillance Object Detection with Background Data Augmentation[C]/",
            "venue": "IEEE International Conference on Image Processing",
            "year": 2021
        },
        {
            "authors": [
                "S Singh"
            ],
            "title": "A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumors through Image Subtraction[J",
            "year": 2022
        },
        {
            "authors": [
                "W Jing",
                "F Tian",
                "J Zhang"
            ],
            "title": "Feature Super-Resolution Based Facial Expression Recognition for Multi-scale Low-Resolution Faces[J",
            "year": 2020
        },
        {
            "authors": [
                "C Deng",
                "M Wang",
                "L Liu"
            ],
            "title": "Extended Feature Pyramid Network for Small Object Detection[J",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "J Rabbi"
            ],
            "title": "Tiny Object Detection in Remote Sensing Images: End-to- End Super-Resolution and Object Detection with Deep Learning[J",
            "year": 2020
        },
        {
            "authors": [
                "L Chen",
                "C Liu",
                "F Chang"
            ],
            "title": "Adaptive Multi-Level Feature Fusion and Attention-Based Network for Arbitrary-Oriented Object Detection in Remote Sensing Imagery[J",
            "year": 2021
        },
        {
            "authors": [
                "J Rabbi",
                "N Ray",
                "M Schubert"
            ],
            "title": "Small-Object Detection in Remote Sensing Images with End-to-End Edge-Enhanced GAN and Object Detector Network[J",
            "year": 2020
        },
        {
            "authors": [
                "Zhang",
                "Kaibing",
                "Gao"
            ],
            "title": "Learning Multiple Linear Mappings for Efficient Single Image Super-Resolution[J",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2015
        },
        {
            "authors": [
                "X Li",
                "J Chen",
                "Z Cui"
            ],
            "title": "Single Image Super-Resolution Based on Sparse Representation with Adaptive Dictionary Selection[J",
            "venue": "International Journal of Pattern Recognition & Artificial Intelligence,",
            "year": 2016
        },
        {
            "authors": [
                "J Yang",
                "L Xiao",
                "Q Zhao Y"
            ],
            "title": "Hybrid Local and Nonlocal 3-D Attentive CNN for Hyperspectral Image Super-Resolution[J",
            "venue": "IEEE Geoscience and Remote Sensing Letters,",
            "year": 2020
        },
        {
            "authors": [
                "F Chen",
                "C Zhu",
                "Z Shen"
            ],
            "title": "NCMS: Towards accurate anchor free object detection through 2 norm calibration and multi-feature selection[J",
            "venue": "Computer Vision and Image Understanding,",
            "year": 2020
        },
        {
            "authors": [
                "Min",
                "S. H",
                "Choi"
            ],
            "title": "Bimodal fusion of low-level visual features and high-level semantic features for near-duplicate video clip detection[J",
            "venue": "SIGNAL PROCESSING IMAGE COMMUNICATIONS,",
            "year": 2011
        },
        {
            "authors": [
                "X Ye",
                "F Xiong",
                "J Lu"
            ],
            "title": "M2-Net: A Multi-scale Multi-level Feature Enhanced Network for Object Detection in Optical Remote Sensing Images[C]/",
            "venue": "International Conference on Digital Image Computing: Techniques and Applications",
            "year": 2020
        },
        {
            "authors": [
                "X Yang",
                "J Yan"
            ],
            "title": "SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing[J",
            "year": 2020
        },
        {
            "authors": [
                "Y Huang",
                "H Yao",
                "S Zhao"
            ],
            "title": "Towards more efficient and flexible face image deblurring using robust salient face landmark detection[J",
            "venue": "Multimedia Tools & Applications,",
            "year": 2015
        },
        {
            "authors": [
                "Z Fang",
                "J Ren",
                "H Sun"
            ],
            "title": "SAFDet: A Semi-Anchor-Free Detector for Effective Detection of Oriented Objects in Aerial Images[J",
            "venue": "Remote Sensing,",
            "year": 2020
        },
        {
            "authors": [
                "C Sun",
                "Y Xu",
                "Z Wu"
            ],
            "title": "ReAFFPN: Rotation-equivariant Attention Feature Fusion Pyramid Networks for Aerial Object Detection[J",
            "year": 2022
        },
        {
            "authors": [
                "S Ali",
                "A Siddique",
                "F Ates H"
            ],
            "title": "Improved YOLOv4 for Aerial Object Detection[C]/",
            "venue": "29th Signal Processing and Communications Applications Conference (SIU)",
            "year": 2021
        },
        {
            "authors": [
                "D Liang",
                "Q Geng",
                "Z Wei"
            ],
            "title": "Anchor Retouching via Model Interaction for Robust Object Detection in Aerial Images[J",
            "year": 2021
        },
        {
            "authors": [
                "C Wang"
            ],
            "title": "Multi-Sector Oriented Object Detector for Accurate Localization in Optical Remote Sensing Images[J",
            "venue": "Remote Sensing,",
            "year": 2021
        },
        {
            "authors": [
                "F Yang",
                "W Li",
                "H Hu"
            ],
            "title": "Multi-Scale Feature Integrated Attention-Based Rotation Network for Object Detection in VHR Aerial Images[J",
            "venue": "Sensors (Basel,",
            "year": 2020
        },
        {
            "authors": [
                "J Ponciano J",
                "M Roetner",
                "A Reiterer"
            ],
            "title": "Object Semantic Segmentation in Point Clouds\u2014Comparison of a Deep Learning and a Knowledge-Based Method[J",
            "venue": "International Journal of Geosciences,",
            "year": 2021
        },
        {
            "authors": [
                "Y Wang",
                "Q Mao",
                "H Zhu"
            ],
            "title": "Multi-Modal 3D Object Detection in Autonomous Driving: a Survey[J",
            "year": 2021
        },
        {
            "authors": [
                "J Redmon",
                "S Divvala",
                "R Girshick"
            ],
            "title": "You Only Look Once: Unified, Real-Time Object Detection[C]/",
            "venue": "Computer Vision & Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "L Qian",
                "A Jn",
                "A Jy"
            ],
            "title": "A Depthwise Separable Dense Convolutional Network with Convolution Block Attention Module for COVID-19 Diagnosis on CT Scans[J",
            "venue": "Computers in Biology and Medicine,",
            "year": 2021
        },
        {
            "authors": [
                "X Zou",
                "Z Wu",
                "W Zhou"
            ],
            "title": "YOLOX-PAI: An Improved YOLOX, Stronger and Faster than YOLOv6[J",
            "year": 2022
        },
        {
            "authors": [
                "R Siyal A",
                "Z Bhutto",
                "S Muhammad"
            ],
            "title": "Still Image-based Human Activity Recognition with Deep Representations and Residual Learning[J",
            "venue": "International Journal of Advanced Computer Science and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Sungho",
                "Kwon",
                "Yongshin"
            ],
            "title": "Proposing the Development of a One to One Learning Environment to Enhance Students` Learning Capability[J",
            "venue": "Korean Journal of the Learning Sciences,",
            "year": 2014
        },
        {
            "authors": [
                "L Mekhalfi M",
                "C Nicolo",
                "Y Bazi"
            ],
            "title": "Contrasting YOLOv5, Transformer, and EfficientDet Detectors for Crop Circle Detection in Desert[J",
            "venue": "IEEE Geoscience and Remote Sensing Letters,",
            "year": 2022
        },
        {
            "authors": [
                "D Li",
                "X Hu",
                "S Wang"
            ],
            "title": "Hyperspectral Images Ground Object Recognition",
            "venue": "Based on Split Attention[C]//",
            "year": 2021
        },
        {
            "authors": [
                "Y Liu",
                "G Zhang",
                "H Wang"
            ],
            "title": "An Efficient Super-Resolution Network",
            "venue": "Based on Aggregated Residual Transformations[J]. Electronics,",
            "year": 2019
        },
        {
            "authors": [
                "T Alipour-Fard",
                "E Paoletti M",
                "M Haut J"
            ],
            "title": "Multibranch Selective Kernel Networks for Hyperspectral Image Classification[J",
            "venue": "IEEE Geoscience and Remote Sensing Letters,",
            "year": 2020
        },
        {
            "authors": [
                "L Xie",
                "C Huang"
            ],
            "title": "A Residual Network of Water Scene Recognition Based on Optimized Inception Module and Convolutional Block Attention Module[C]/",
            "venue": "6th International Conference on Systems and Informatics (ICSAI)",
            "year": 2019
        },
        {
            "authors": [
                "N Jiang",
                "X Yu",
                "X Peng"
            ],
            "title": "SM+: Refined Scale Match for Tiny Person Detection[C]// 2021",
            "year": 2021
        },
        {
            "authors": [
                "Q Zhou",
                "X Li",
                "L He"
            ],
            "title": "TransVOD: End-to-end Video Object Detection with Spatial-Temporal Transformers[J",
            "year": 2022
        },
        {
            "authors": [
                "M Durve",
                "S Orsini",
                "A Tiribocchi"
            ],
            "title": "Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications[J",
            "year": 2023
        },
        {
            "authors": [
                "R Selvaraju R",
                "A Das",
                "R Vedantam"
            ],
            "title": "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradientbased Localization[J",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nINDEX TERMS Deep learning, target detection, channel-wise attention, space-attention, YOLO\nI. INTRODUCTION With the continuous development of deep learning and the constant reduction of hardware cost, deep learningbased target detection methods have made significant progress. Compared to medium and large target detection, small target detection has the characteristics of less target feature information, imbalanced data distribution, and susceptibility to environment, which lead to low accuracy in small target detection. Small target detection has extensive applications in tasks such as maritime rescue, surveillance recognition, unmanned aerial vehicle (UAV) identification, remote sensing satellite, and marine life detection. Therefore, studying the small target detection method and improving its accuracy and efficiency is of great significance. Due to the successive down-sampling operation, deep learning-based target detection method filters the correlated\nnoise during feature extraction, enhancing the feature representation of the target, while also causing small targets to lose information in the forward propagation of the network. To this end, some scholars have proposed different multi-scale feature fusion structures based on feature pyramid network (FPN) [1], such as path aggregation network (PANet) [2], neural architecture search network (NAS-Net) [3], deep feature pyramid networks (DFPN) [4], Bidirectional Feature Pyramid Networks (BiFPN) [5], etc. However, among these networks, the fusion between different layers is only simple summation, ignoring the relevance of the target in the scene, which has limited improvement for small target detection. Specifically, squeeze excitation network (SE-Net) [6], convolutional block attention module (CBAM) [7], frequency channel attention network (FcaNet) [8] and other methods model small targets from different perspectives of channel-wise\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\nattention and space-attention to obtain attention weight matrices in two dimensions, thus enhancing small target feature representation and suppressing other targets and complex environmental information. However, these attention network designs ignore the effect of different convolutional kernels on small target detection. To address the above problems and enhance the real-time performance of the network, this paper proposes a lightweight architecture for real-time small target detection, i.e., MBAB-YOLO. Specifically, the method uses YOLOv5s (the s version of You Only Look Once model) as the baseline structure for small target detection, and then improves it with proposed blended attention block (BAB) and multi-branch-ConvNet (Convolutional Neural network) structure. The main contributions of this paper are as follows: 1) We combined channel-wise attention (CA) block and space-attention (SA) block, and reorganized the connection structure to propose BAB. BAB can obtain the rich global spatial attention weight matrix, enhance small target feature information, and suppress irrelevant information such as the background. 2) We proposed a novel multi-branching blended attention block (MBAB) by combining multi-branchConvNet structure and BAB mechanism. MBAB can adaptively adjust the receptive field size based on the scale of the input target, and enhance the feature representation of small targets. 3) To improve the feature extraction capability for small targets, we improved the core residual block, i.e., C3, of YOLOv5 and combined MBAB with C3 to propose a feature extraction residual block based on CSPNet (cross stage partial network) (CSP-MBAB, abbr, CMBAB). CMBAB can focus more attention on small targets during feature extraction, enhancing the feature information of small targets. Meanwhile, a new prediction branch and small target detection head are introduced in the P2 layer, which has more shallow information and is beneficial for small target detection. 4) This paper comprehensively introduced the general and the specific research status of small target detection, as well as YOLO family, which can undoubtedly serve as a significant reference for later researchers. 5) Extensive contrast experiments and ablation experiments have verified the trade-off between accuracy and efficiency of proposed method, demonstrating its superiority as a lightweight architecture for real-time small target detection."
        },
        {
            "heading": "II. RELATEDWORKS",
            "text": "The small target in the COCO dataset is an absolute definition, which refers to the target that smaller than 32x32 pixels in an image. However, in practical applications, a more common approach is to use the ratio of the target size to the original image, which is referred to as a relative\ndefinition. For example, the target with a ratio smaller than 0.1 can be considered the small. In general, there is no strict definition for the small target, and it needs to be determined based on the actual engineering application. In the development of target detection, it has been gradually discovered that detecting the small target is more challenging than detecting the medium to large target.\nRegardless of whether it is a relative or absolute definition, the small target typically has fewer pixels, lower resolution, and lack feature information. After continuous exploration, several reasons that contribute to the low detection accuracy of small target have been revealed: (1) Lack of feature information: Due to the small number of pixels in the small target, deep neural networks, which undergo dozens or hundreds of convolution and pooling operations, will down-sample the image to reduce the computational cost and expand the receptive field, generating the thumbnail image. However, this will cause a significant loss of information in the small target, making the information of the small target in the feature map less and less. (2) Information loss in forward propagation of neural networks: During the forward propagation procedure, the semantic information of the feature map becomes stronger while the positional information gradually gets lost, making it difficult to locate the coordinates of the target. (3) Unequal distribution of sample quantities in the dataset: If the number of small target in the training set is distributed unevenly compared to the medium and large target, it will result in the network having lower adaptability to different sizes of the target during learning, leading to a decrease in detection accuracy. In the COCO dataset, images containing targets of all three sizes (small, medium, and large) account for 52.3% of the total samples, with the proportion of large & medium targets and small targets being 70.7%, 83.0%, respectively. This reasonable distribution of samples makes the COCO a common dataset for small target detection. Obtaining an class-balanced dataset is also a major challenge in practical applications. (4) Setting of anchors: Due to the varying sizes and aspect ratios of targets to be detected, it is difficult to set anchors that match the actual situation. Existing methods\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\nuse multiple sets of anchors or calculate anchors based on the training data set, but the generalization ability is poor when detecting unseen targets. (5) Inappropriate loss function: In deep learning models, the loss function is used to perform gradient descent to optimize the model parameters. Choosing an appropriate loss function is particularly important. In existing algorithms, IoU (Intersection over Union) is an important part of the loss function, which determines the accuracy of target localization in detection. However, the sensitivity of IoU for small targets is different from that of medium and large targets. As shown in Figure 1, when the predicted bounding box for small targets and large targets deviate from the ground truth diagonal by 1 and 4 pixels, respectively, the IoU of small targets drops sharply from 0.53 to 0.06, while that of large targets drops from 0.90 to 0.65, with a slower rate of change compared to small targets. To comprehensively and in detail summarize the current status of small target detection methods, this section is divided into three parts: general small target detection methods, small target detection methods in specific field, and commonly used industrial target detection methods, which are also the baseline method of this paper, i.e. YOLO."
        },
        {
            "heading": "A. GENERAL SMALL TARGET DETECTION METHODS",
            "text": "In target detection methods, it is common to start from the aspects of multi-scale features, contextual information, loss functions, etc. BELL et al. [9] proposed the InsideOutside Net (ION), a target detection network that utilizes information inside and outside the Region of Interest (ROI), integrates context information outside the ROI using spatial recursive neural networks, and extracts feature information using skip pooling. DAI et al. [10] proposed variable convolution, which improves on the limitations of fixed convolution in extracting spatial information. LIN et al. [11] proposed Focal Loss, which dynamically adjusts the contribution of detection results to the loss function based on their confidence, solving the problem of imbalance between positive and negative samples encountered during training of single-stage detectors. SINGH et al. [12] proposed SNIPER, a method that solves the problem of long training time and high resource consumption associated with image pyramids in multi-scale training. By appropriately processing context areas around the annotated values at a suitable scale, training speed is greatly improved. LI et al. [13] proposed DetNet, a backbone network specifically designed for target detection. Compared with ResNet-50, DetNet-59 significantly improves the detection of small targets on the COCO dataset, with AP_50 increasing by 6.4 to reach 66.4. CAI et al. [14] pointed out that most current target detection algorithms use an IoU threshold of 0.5 to determine positive and negative samples. However, using such a wide threshold can lead to a lot of interference, and increasing the threshold can lead to a decrease in detection performance. Therefore, they proposed Cascade R-CNN, a multi-stage target detection architecture that trains using different stages and IoU\nthresholds in a cascaded manner, avoiding the overfitting problem during training and the mismatching problem during inference. As the factors limiting the detection performance of small target are increasingly cognized, various methods have been proposed in recent years to improve the accuracy. The following is a comprehensive introduction according to different principles.\na. Multi-scale feature fusion methods In the target detection task, as the network infers, the features and locational information of the small target gradually get lost in the feature map. The feature pyramid can produce multi-scale features, in which all layers, including the high-resolution layer, have strong semantic information. However, because the multi-scale features in the feature pyramid network are independently computed, the speed is slow. In addition, as deep convolutional networks compute feature hierarchy layer by layer, significant semantic differences are introduced due to the difference in depth. Overall, as the network deepens, it becomes increasingly difficult to preserve the features of small targets, which greatly affects their detection performance. The features of shallow networks have more detailed locational and small target information. Therefore, multi-scale feature fusion of shallow and deep features is an effective solution.\nLiterature [15] proposed the Feature Pyramid Networks (FPN) for target detection. The FPN structure, as shown in Figure 2, consists of three main parts: the bottom-up path, the top-down path, and the lateral connection. The bottomup path is the forward propagation process in neural networks. After the continuous convolution operations, the feature maps usually become smaller, achieving the goal of producing multi-scale features. The top-down path uses upsampling operations to extract strong semantic features from high-level feature maps and then fuses them with the original feature maps through the lateral connection. FPN, which combines high-resolution and high-semantic information, was applied to Faster RCNN for small target detection, achieving an average precision of 17.8. Before feature fusion, FPN performs the 1\u00d71 convolution on the features of different layers to reduce the feature channels. However, since the large semantic gap between features is not considered, the directly fused features would reduce the ability of multi-scale representation. Specifically, the feature fusion of FPN is performed top-down, leading to the loss of feature information in the highest layer due to channel reduction. After feature fusion, the features of each candidate region are selected from one layer of feature\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\nmaps based on the scale of the proposal, ignoring other layers that also contain rich information, thus affecting the final detection performance. PANet [16] is a structure that adds the bottom-up path, the adaptive feature pooling, and the final detection and segmentation block to the backbone of FPN, as shown in Figure 3. For models that use the regression method for prediction, such as R-CNN, FPN, YOLOv3, and YOLOv4, the detailed information in the low-level feature map is important for coordinate regression. However, most models perform coordinate regression on high-level feature maps and lose a lot of detailed information after passing through the backbone network. In FPN, different feature levels are assigned to different sizes of the proposal regions, with smaller proposal regions assigned to lower-level features and larger proposal regions assigned to higher-level features. Although the prediction of FPN is based on multilevel features, each ROI still extracts features based on a single layer.\nTo address this problem, PANet adds the bottom-up enhancement branch, as shown by the green dashed line in Figure 3, which provides the detailed information required for the coordinate regression, while the original path indicated by the red dashed line is used to transmit semantic information about categories, fully utilizing both low-level and high-level features. In addition, the adaptive feature pooling replaces single-layer features with multi-layer features, and the ROI features obtained from different layers are fused together to obtain the final feature, which is used for subsequent prediction. Specifically, PANet is selected as one of the baseline modules in this paper.\nb. Data augmentation methods In [17], the data augmentation method is used to improve the accuracy of small target detection. The authors analyzed the detection performance of Mask R-CNN [18] on the MSCOCO dataset and identified two reasons for the poor performance of the model on small targets: 1) there are few images containing small targets and 2) even if the images contain small targets, the proportion is too small. Therefore, the authors over-sampled small target samples and enhanced each image by repeatedly copying and pasting small targets. During the training phase, images containing small targets were over-sampled to solve the problem of a small number of images containing small targets in the dataset. The sample size was balanced by controlling the number of times the image was copied, that is, the oversampling rate.\nSince the MS-COCO dataset provides instance segmentation masks, it is convenient to copy from the original location of the target. Therefore, based on oversampling, the copy-and-paste idea was adopted to paste small targets to any other location in the image while generating new labels, and the pasted small targets could be randomly transformed by scaling, rotating, and so on. The data augmentation method starts with the data level to solve the problem of uneven sample distribution in the dataset. By augmenting the data, the number of small targets in the image is increased, thereby increasing the number of matching anchors and improving the contribution of the loss function calculation during the training phase, resulting in better small target detection. The experimental results show that the accuracy of small target detection was improved by 7.1%.\nc. Super-resolution methods In order to improve the localization ability of smallsized images, literature [19] proposed a new superresolution method, i.e., Feature Super-Resolution (FSR), which is different from traditional image super-resolution method. Literature [20] was the first to apply GAN (Generative Adversarial Network) to small target detection tasks, proposing Perceptual GAN. The generator, composed of multiple residual blocks, learns residual representations between targets of different sizes to reduce the gap between small and large targets by enhancing the representation of small targets. The discriminator is composed of the adversarial branch and the perceptual branch. Specifically, the adversarial branch distinguishes between the reconstructed small targets and the large targets, while the perceptual branch is used for classification and regression for target detection. The perceptual branch is first trained with large target features, followed by training the generator with small targets and training the adversarial branch with both large and small targets. However, this method only considers images containing either small or large targets, and adversarial training is difficult for the discriminator to distinguish between the features of large targets and the small target super-resolution representations output by the generator. Literature [21] pointed out that small targets are difficult to distinguish from the background or other similar targets due to the lack of feature information, and proposed MultiTask Generative Adversarial Network (MTGAN). The generator is the super-resolution network that reconstructs small and blurry images by the upsampling operation to restore detailed information in the image. The multi-task module of discriminator includes judging the authenticity of the image, classification, and regression. During training, the loss from classification and regression is backpropagated to the generator, enabling it to reconstruct more details. This method first uses the baseline detector to obtain the target and background of the image. Since the generator performs super-resolution operation on the image, the reconstructed image is not the feature map, so it is\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\nnecessary to extract features again, resulting in a expensive computational cost. Literature [22] pointed out the issues of Perceptual GAN network lacking direct supervised signals and MTGAN having excessive computational complexity. They considered that using appropriate high-resolution target features as supervised signals for training the SR (SuperResolution) model, and the receptive field that matches the input low-resolution features and target high-resolution features can improve the performance of feature superresolution. The authors added four additional parts on base of the Faster R-CNN base detector: SR feature generator, SR feature discriminator, SR target extractor, and small predictor. As the SR feature generator based on GAN model, it generates high-resolution features with the features extracted by the SR target extractor as the target, under the guidance of SR feature discriminator. The small predictor is used to predict the category and location confidence of small targets, while the original large predictor is used to detect large targets. The authors elaborated on the mismatching problem between the receptive fields of high and low-resolution features and used dilated convolutions to match the receptive field, but did not experimentally explain the matching process, so there may still be the mismatching in receptive fields. Literature [23] proposed an improvement to the S2ANET [24] called the S2ANET-SR model, where both the original image and the reduced image are simultaneously inputted to the detection network. To enhance the feature extraction ability of small targets, a SR enhancement module for the reduced image is designed, and perceptual loss & texture matching loss are proposed as the supervision. The mean Average Precision (mAP) on the DOTA dataset reached 74.47%. Literatures [25][26] combined the CycleGAN and Residual Feature Aggregation (RFA) to improve the current SR framework for enhancing detection performance. The method of small object detection based on superresolution adds an SR module to the base detector, resulting in the increasing computational cost. The use of the lookup table can reduce the computational cost, but the single-layer based lookup table method limits the scalability and generalization ability of model. Therefore, MA et al. [27] proposed a serial-parallel lookup table framework to address this problem and achieve efficient image superresolution framework. In response to the non-local operation in the image SR algorithm, which tends to be global in the receptive field of deep networks, resulting in inaccurate correlation calculations between deep features, and the problem of large computational complexity based on full-image calculation of feature similarity, literature [28] proposed Non-Local Sparse Attention (NLSA), which significantly reduces the computational cost and increases the effectiveness of the non-locality operation. Literature [29] proposed an efficient non-local contrast attention\nmodule to address the influence of noise on image superresolution. The relative independence of reconstruction and detection algorithms and the computational cost limit their integration to some extent. In addition, the SR network is difficult to train and relies heavily on massive datasets. Therefore, small target detection based on SR still has significant development potential in the future."
        },
        {
            "heading": "B. SMALL TARGET DETECTION METHODS IN SPECIFIC FIELD",
            "text": "Detection of small targets in pedestrian, face, and remote sensing images is an application about specific field. Similar to the general task of small target detection, it also faces challenges such as small scale and limited features, but there are also some differences. Specifically, the distribution of targets in the specific field is usually more dense. In addition, the detection targets for pedestrians and faces are relatively singular, only needing to judge whether the target is the object to be detected, without classification loss. However, detection of small targets in remote sensing images is more complex, as the images are taken from the air angle, and there are difficulties such as target rotation angles.\na. Small target detection in pedestrian and face ZHU et al. [30] proposed the FSAF (Feature Selective Anchor-Free) module by adding an anchor-free branch after each layer of FPN. Each added branch predicts the same target, and during the backpropagation phase, the layer with the smallest loss is selected to establish the supervision signal, avoiding the defect of anchor-based detectors only dividing the belonging layer based on the target scale during the prediction process. LIU et al. [31] abandoned the anchor and sliding window-based methods and used extracted high-level semantic features to predict the center and scale of pedestrians. Since the scale of small targets is too small, predicting the center point is conducive to locating small targets and is a valuable idea for small target detection. YU et al. [32] proposed a scale matching method for detecting small pedestrians to address the problem of scale mismatch between the data used for detector learning and the data used for network pre-training. ZHANG et al. [33] proposed a scale-balanced face detection architecture to better handle the issue of varying face scales. The used VGG16 occupies about 80% of the inference time, and using a more efficient network can improve detection speed. Literature [34] discussed the problem of detecting small faces from three aspects: scale, resolution, and context. ZHU et al. [35] pointed out that the low IoU between anchors and faces resulting in poor detection performance, so they proposed the EMO (Expected Max Overlapping) score to evaluate the degree of matching between the two and proposed a new anchor design strategy to achieve a high IoU. BAI et al. [36] used the generator of GAN to reconstruct and deblur high-resolution faces, and the discriminator of\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\nGAN was used to identify faces. ZHANG et al. [37] proposed a strategy to dynamically adjust the training weight based on the difficulty of detection. A score representing the difficulty level of each image was assigned during the training phase, and images with high scores were included in a subset for the next round of training. Unlike small target detection method in remote sensing images, pedestrian and face detection do not need to consider the rotation direction of the target, which reduces the difficulty of detection to some extent. When the general target detector is applied to small and dense face detection tasks, the size of the anchor will not match the receptive field, and small anchors will produce a large number of negative samples during the matching process. Therefore, most scholars tend to study the matching and setting strategies of anchors, and face detection has achieved good results now.\nb. Small target detection in the remote sensing image Remote sensing images are captured from a high altitude perspective and have complex spatial scenes with various target types. Target detection in the remote sensing images faces difficulties such as small and dense target scales, complex backgrounds, and arbitrary distribution directions. DING et al. [38] proposed a Dataset of Object deTection in Aerial images, i.e., DOTA. PANG et al. [39] proposed a unified self-enhanced network called the Remote Sensingbased Convolutional Neural Network (R2-CNN), which consists of the lightweight network Tinny-Net, the global attention module, the classifier, and the detector. The Tinny-Net makes the network highly efficient in terms of computation and memory consumption, and the global attention module provides strong robustness against false positives. YANG et al. [40] proposed a feature fusion structure to solve the problem of small targets by exploring anchor sampling angles and feature fusion. Li et al. [41] proposed a new semantic representation method to improve the performance of detecting remote sensing images. They first designed an enhanced feature pyramid network to better extract visual features with hierarchical differences, then introduced semantic segmentation to guide the detection of horizontal proposals, and finally proposed an ROI module that fuses multiplelayer features to learn target-based semantic representations on the existing features. YANG et al. [42] observed that targets in aerial images are clustered, so they integrated clustering with target detection and proposed the ClusDet network. HAN et al. [43] incorporated the rotation-equivariant network into the detector, enabling it to predict the direction of the target when extracting rotation-invariant features. The proposed rotation-equivariant detector ReDet can solve the problem of arbitrary distribution directions of aerial targets. QIN et al. [44] proposed a multi-head rotated target detector called MRDet to predict the classification confidence, location, scale, and direction of the final bounding box separately. They divided the detection task\ninto multiple sub-tasks, and each detection head was specially designed to learn the features that were most suitable for the corresponding task. YI et al. [45] extended the horizontal landmark-based target detector to the directional target detection task to address the severe imbalance problem between positive and negative anchors encountered by current anchor-based twostage detectors when detecting targets in aerial images with arbitrary and densely arranged directions. The authors first detected the center landmark of the target and then regressed the bounding box aware vectors (BBAVectors) to capture the directional bounding box. WEI et al. [46] applied Transformers to small target detection and proposed CG-Net (Calibrated-Guidance) to enhance the relationship between channels in a feature transformer manner. This method can adaptively determine the calibration weights of each channel, and by aggregating all weighted channels together, it can represent each channel again. WANG et al. [47] proposed a visual model for remote sensing tasks based on ViT (Vision Transformer) and a new rotation-variable window attention to replace the full attention in the original Transformers. This method learns better target representations by extracting rich context from the generated different windows. SHAMSOLMOALI et al. [48] introduced the image pyramid into SSD (Single Shot MultiBox Detector) and proposed IPSSD (Image Pyramid Single-Shot Detector) for detecting small targets in remote sensing images. Although image pyramids can extract more semantic features, they inevitably bring additional computational and memory costs. Target detection in the remote sensing images belongs to the scope of specific small target detection. The methods in the above literature include commonly used techniques such as attention mechanism and feature fusion, as well as unconventional methods such as introducing Transformers and using multiple detection heads to predict classification confidence and location separately. Overall, remote sensing image detection is a rapidly developing field with vast potential for further growth."
        },
        {
            "heading": "C. YOLO FAMILY",
            "text": "Target detection based on deep learning can be divided into two categories: two-stage detection and single-stage detection [49]. The former is a coarse-to-fine process, while the latter is an end-to-end one-step process [50]. Generally, the localization and classfication accuracy of two-stage detection is higher, while the speed of single-stage detection is faster. Typically, single-stage detection attempts to directly classify each RoI as either background or target [51]. That is, it can directly give the category probability and location coordinates of the target through the single stage, and the typical representatives include YOLO family [52].\na. Fundamental theory\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\nThe basic framework of YOLOv1, as shown in Figure 4, first adjusts the size of the input image to 448\u00d7448, and then sends it to the backbone structure to extract features. Then, the network predicts the results and achieves end-toend target detection. YOLOv1 abandons the traditional sliding window technique. It divides the input image into S\u00d7S grids, and each grid is responsible for detecting the targets whose centers fall within that grid. Each grid predicts B bounding boxes and their confidence scores. The confidence score includes the probability of the bounding\nbox containing an target and the accuracy of the bounding box. Each bounding box predicts 5 elements, i.e., (x, y, w, h, c), representing the location, size, and confidence score of the bounding box. Each grid predicts (B\u00d75+C) values, where C is the number of categories. Then, the network prediction is performed using the Non-Maximum Suppression (NMS) algorithm. Subsequent models in YOLO family have inherited this basic idea.\nb. Evolution of backbone network Detectors typically consist of two parts: the Backbone network, which is the basic network used for extracting features and is usually pre-trained on the ImageNet dataset, and the Head for predicting target categories and bounding boxes [53][54]. In recent years, the Neck has been constructed between the Backbone and Head to aggregate different feature maps. The following will provide a detailed analysis of the evolution of the backbone network in YOLO family. YOLO V1. YOLOv1 [52] uses the Backbone network similar to GoogleNet [55], consisting of 24 convolutional layers and 2 fully connected layers. It is pre-trained on ImageNet dataset and then transferred to the detection task, and validated on the VOC (Visual Object Classes) dataset [56]. YOLOv1 divides the input image into the grids of 7\u00d77, and predicts two bounding boxes for each grid, resulting in a total of 7\u00d77\u00d72 bounding boxes. It can detect up to 49 targets, which makes it less effective at detecting dense and small targets. YOLO V2. YOLOv2 [57] uses the VGG network as the reference and constructs a new Backbone network called Darknet-19 based on YOLOv1. YOLOv1 directly predicts bounding boxes using fully connected layers, which causes inaccurate localization due to significant loss of spatial information. Therefore, YOLOv2 introduces anchors to replace the fully connected layers in v1 for predicting bounding boxes. Meanwhile, YOLOv2 resizes the input to\n416\u00d7416 and obtained the feature map of 13\u00d713 with odd dimensions, resulting in only one center for each grid. This center point is used to predict the target falling into that location, making it easier to detect that particular class of targets. Figure 5 shows the method in YOLOv2 used to predict bounding boxes.\nYOLOv2 [57] proposes a groundbreaking method that jointly training classification and detection, extending detection to targets with a lack of samples. This work significantly improves prediction accuracy while maintaining the advantage of fast inference. YOLO V3. The basic network of YOLOv3 [53] is Darknet-53, which borrows the residual structure of ResNet [58] to deepen the network structure while preventing the problem of convergence difficulty caused by the gradient explosion. During forward propagation process, the pooling layer and fully connected layer are removed, and the size of the tensor is changed by changing the stride of the convolution kernel. Similar to v2, Darknet-53 reduces the output features to 1/28 of the input, so it is usually required that the resolution of input image be a multiple of 32. At the same time, YOLOv3 uses tensor concatenation to expand the dimension of the tensor and extract more information. Specifically, the intermediate layer and the later layer of Darknet-53 are concatenated after upsampling. Darknet-53 has 53 convolutional layers from the 0th to the 74th layer, and the rest are residual layers [53]. The 75th to 105th are the feature fusion layers of YOLOv3, which adds multiscale detection (equivalent to the Neck) using 3 scales. The output of each scale is 52\u00d752, 26\u00d726, and 13\u00d713, respectively, for detecting small, medium, and large targets, in which each scale predicts 3 anchors. In summary, the number of predicted anchors in YOLOv3 is more than 10 times that of YOLOv2, and they are performed at different scales, so the overall precision of detecting small targets have been greatly improved. Therefore, it has become one of the milestone architectures in single-stage detection.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\nYOLO V4. YOLOv4 [54] summarizes various improvement methods after v3, which are divided into free and special packages. The former represents modules that improve training without affecting inference speed, while the latter represents modules that have little impact on\ninference time but offer higher performance return, such as the CSP (Cross Stage Partial) [59] structure used in the Backbone, which maintains high inference speed while still having high accuracy. Meanwhile, YOLOv4 is more suitable for training on a single GPU. Bochkovskiy et al. [54] found that when the model is optimal for classification, it is not necessarily optimal for detection. For example, the classification accuracy of CSPResNeXt-50 is higher than that of CSPDarknet-53, but the latter has higher detection accuracy. Therefore, YOLOv4 chooses CSPDarknet-53 as the backbone network. Regarding the Backbone, the overall architecture of YOLOv4 is the same as YOLOv3, but improvements have been made to each substructure. Figure 6 shows two network structures: Darknet-53 and CSPDarknet-53 [59]. The black color represents Darknet-53, and the CSPDarknet-53 network only needs to be replaced with the structure in the red box, and the filter values are changed to the red values in parentheses. YOLOv4 removes the last pooling layer, fully connected layer, and Softmax layer, and its Backbone has five CSP modules [54]. For the Neck, YOLOv4 introduces the Spatial Pyramid Pooling (SPP) and PANet modules. SPP significantly increases the receptive field and separates important contextual features without reducing running speed. PANet replaces FPN used in YOLOv3 for parameter aggregation and uses tensor connections instead of the original short connections. For the Head, YOLOv4 inherits the multi-scale structure from YOLOv3 for prediction. YOLO V5. YOLOv5 [60] has a similar basic structure to YOLOv4, with the main difference being the scaling of\ndifferent channel sizes. Based on model size, YOLOv5 offers five different models: YOLOv5-n/s/m/l/x. As mentioned above, it can be seen that the methods in the YOLO family directly divide the image into several regions and predict the bounding box and probability for each region at the same time, which greatly improves the detection speed."
        },
        {
            "heading": "III. THE PROPOSED METHOD",
            "text": ""
        },
        {
            "heading": "A. OVERALL ARCHITECTURE",
            "text": "Compared with the background region, small targets are very small in size and lack self-information. Directly inputting images containing small targets into YOLOv5 will cause the network to treat them as common targets and ignore their special characteristics. Firstly, inspired by [7][61], this paper improves the channel-spatial attention mechanism under a single receptive field and proposes a multi-branching blended attention block, i.e., MBAB, combined with the multi-branch-ConvNet structure. Compared with [7], the proposed module can more effectively mine the feature information of small targets with a tiny increase in computational cost and dynamically allocate blended attention weights according to the contribution of feature maps of different scales to small targets. Then, an improved feature extraction module CMBAB is introduced at the end of the Backbone to enhance the feature extraction capability of the core network, and MBAB and CMBAB are introduced in the upsampling and down-sampling operations of the Neck's multi-scale fusion to enhance the feature expression ability about small targets. Finally, the P2 detection branch is added to the PANet for detecting small targets. In summary, the overall architecture of the MBAB-YOLO is shown in Figure 7."
        },
        {
            "heading": "B. MULTI-BRANCHING BLENDED ATTENTION BLOCKMBAM",
            "text": "Most deep learning-based target detection methods use ConvNets, but different convolution kernels have different sensitivities to targets of different sizes. Specifically, Szeqedy et al. [62] proposed GoogleNet, which achieved certain superiority by using the Inception structure, consisting of four network blocks with different convolution kernels. Later, Xie et al. [63] proposed ResNeXt, which introduced group convolution in the bottleneck of ResNet and used a multi-branching structure in the base architecture, demonstrating the effectiveness through extensive experiments. SE-Net enhanced effective target features and suppressed background information by adding channel-wise attention mechanisms to adaptively recalibrate features. SK-Net (Selective Kernel Network) [64] used two different convolutional kernel branches, also introduced channel-wise attention mechanisms for fusion features, and then adaptively split the branch network for re-calibration. ResNeSt [61] improved SK-Net by using different n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\nconvolution kernels and using the dilated convolution to share computations. After introducing the channel-wise attention block, the features were re-calibrated with n attention mechanisms for different receptive fields. CBAM (Convolutional Block Attention Module) redefined channel-wise attention block by adding the sum of the mean and maximum values between channels, and introduced spatial-attention in the same way. The concatenated and blended channel and spatial attention mechanisms were used to re-calibrate the feature map. Experimental results showed that its effect was superior to that of a single attention mechanism. Inspired by these methods, this paper improves the blended attention block and combines the multi-branch-ConvNet structure with the blended attention mechanism, and verifies the effectiveness of the proposed method through extensive experiments. The main design inspirations of the proposed MBAB in this paper is as follows: First, the input feature map F is processed with different convolution kernels to obtain multiple branches, and then the multiple branches are summed to obtain the blended feature map F(b). Then, the attention weight matrices XSA and XCA are separately calculated along the spatial and channel dimensions of the blended feature map F(b), and the weights are fused along the spatial and channel dimensions to obtain the mixed attention weight matrix XBAB. The blended attention weight is then redistributed according to the contribution of each branch, and the feature map under different convolution kernels is re-calibrated. Finally, the weighted feature maps are summed to obtain the blended attention-weighted feature map. For small targets, during the multi-branching stage, with the training of the network, the features of small targets will be assigned different weights on branches with different receptive fields, where positive targets will be assigned larger\nweights while negative targets will be assigned smaller weights. Through this multi-branching structure, the model will focus more attention on learning effective features, thereby enhancing its generalization ability. Inspired by [7], two convolutional attention modules, channel-wise attention (CA) block and space-attention (SA) block, are designed. The structure of CA is shown in Figure 8, and its calculation is shown in (1):  mod ( ( )) ( ( ) )CA c cX Sig f AdaAvgPooling F f AdaMaxPooling F \n(1) in which, the size of the input feature map F is N\u00d7C\u00d7H\u00d7W; Sigmod is the activation function; AdaAvgPooling is the global adaptive average pooling; AdaMaxPooling is the global adaptive max pooling; and fc is the fully connected network. First, global adaptive average pooling and max pooling are applied to the feature map F, then the two obtained channel weights are passed through the fully connected layer fc (Conv & Relu & Conv), and finally the two different weights of the fully connected are summed and Sigmod is activated to obtain the channel attention XCA, whose size is N\u00d7C\u00d71\u00d71.\nThe structure of SA is shown in Figure 9, and its calculation is shown in (2):\n mod ( ( ( ),max( ) ))SA cX Sig f cat mean F F (2) in which, the input feature map F has a size of N\u00d7C\u00d7H\u00d7W, where mean is the mean function, max is the max function, and cat is the matrix concatenation function. Firstly, the mean and maximum are computed spatially for the feature map F, resulting in the size of N\u00d71\u00d7H\u00d7W. Then, the weight matrices for the mean and maximum are concatenated spatially to obtain the double-channel spatial attention weight. Finally, double-channel composite spatial attention XSA is\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\nobtained through the 1\u00d71 fully connected convolution layer, resulting in the size of N\u00d71\u00d7H\u00d7W.\nIn reference to [7], after the channel-wise attention is applied, the feature space is re-calibrated, then spaceattention is concatenated and the feature space is recalibrated again, resulting in two rounds of attentionweighted feature maps. However, the output is the weighted feature map that is not conducive to combining the multibranch-ConvNet. In addition, the concatenation operation is simple and effective, but it does not adequately consider the impact of reasonable connection methods on small targets. To improve the CBAM network structure and facilitate its combination with the multi-branch-ConvNet without increasing computational complexity, we propose a blended attention block (BAB), as shown in Figure 10. The calculation of BAB is shown in (3):\n( ( ), ( ( )))BABX mul CA F SA F CA F  (3)\nin which, the input feature map F has a size of N\u00d7C\u00d7H\u00d7W, CA is channel-wise attention block, output size is N\u00d7C\u00d71\u00d71; SA is space-attention block, output size is N\u00d71\u00d7H\u00d7W; mul is matrix multiplication, and XBAB is the uncalibrated blended attention weights of CA and SA, with the output size of N\u00d7C\u00d7H\u00d7W. The multi-branching attention module in [61] only uses channel-wise attention and has certain effect on feature extraction for small targets, but lacks consideration of spatial dimensions and is not comprehensive. Therefore, the blended attention module is introduced into the multi-branching network in this paper, and the multi-branching blended attention block MBAB is proposed, whose network structure is shown in Figure 11.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\nAssuming that the input feature map F has a size of N\u00d7C\u00d7H\u00d7W and the number of split branches is S, a series of mappings {F1, F2, ..., FS} are obtained through transformations with different convolution kernels. In addition, for each mapping, the element-wise sum fusion is performed to obtain the multi-branching blended feature map, denoted as FMB, with the formula shown in (4):\n1\nS\nMB i i F F   (4) Then, the FMB is used as the input of the proposed BAB, and the output is the multi-branching blended attention weight matrix, denoted as XMB, where XMB= BMB(FMB), with a size of N\u00d7C\u00d7H\u00d7W. XMB is then divided into d groups (d is the hyperparameter), and each group is called a base, denoted as XMBd, where XMBd = f(XMB), and f is the 1\u00d71 convolution with a size of N\u00d7C/d\u00d7H\u00d7W. Therefore, the multi-branching blended attention weight matrix now has S\u00d7d groups. Then, the global context attention weight matrix is fused to obtain S groups of blended attention weight matrix, denoted as XMBS, where XMBS = f(XMBd), and f is the 1\u00d71 convolution with a size of N\u00d7C*S\u00d7H\u00d7W. Finally, the blended attention weight is allocated to each branch. In [61], the allocation weight is calculated using softmax function. On the contrast, in order to reduce computation complexity, the Faster Normalization (FN) method is used instead of the original softmax method, with negligible additional cost. The FN is shown in (5):\n \n1\n( ) , 1, 2,3...\n( )\ni i S\ni i\nW F O i S\nW F \n  \n(5)\nin which, Oi represents the contribution of different receptive field branches to the overall blended attention size. The overall blended attention size reassigns the blended attention of each branch based on Oi, where \u03b3 is the constant, usually taken as a very small value of 0.00001 to prevent regularization failure caused by a denominator of zero. After the weights of each branch are allocated, the multi-branching feature map is re-scaled and the corresponding elements are summed to obtain the feature map Fout weighted by multibranching blended attention. Fout is shown in (6).\n1 ( ( ) )\nS i\nout MBS i i F FN X F    (6) in which, S is the number of branches; i is the rescaled branch; and Fi is the i-th branch. The final output size is N \u00d7 C \u00d7 H \u00d7 W."
        },
        {
            "heading": "C. Feature Extraction Module-CMBAB",
            "text": "The core module C3 for feature extraction in YOLOv5 mainly uses the CSPNet network structure to stack Bottleneck residual blocks. To improve its structure, the 3\u00d73 convolution in the residual block is replaced with the multibranching blended attention block, i.e., MBAB. To reduce the number of parameters, the modified residual blocks are no longer stacked, and the feature extraction module called CMBAB, based on CSPNet and MBAB, is proposed. Its network structure is shown in Figure 12.\nFirstly, assuming the input feature map Fin has a size of N\u00d7C\u00d7H\u00d7W. Then, after 1\u00d71 convolution, it is used as the input of the residual block and denoted as Fres-in. After passing through the residual connection with MBAB, the output feature map is denoted as Fres-out = add(Fres-in, MBAB(f(Fres-in))), where add represents element-wise summation, and f is the 1\u00d71 convolution. Finally, by combining with the CSPNet structure, CMBAB is obtained with an output denoted as Fout =f(cat(f(Fin), Fres-out)), where cat denotes matrix concatenation operation. In terms of module size, when the C3 stacks more than one Bottleneck residual block, the number of parameters in CMBAB is lower than that of C3. In feature extraction and processing for small targets, CMBAB can utilize the proposed MBAB mechanism to obtain more abundant features of small targets than C3, thereby enhancing feature representation and improving detection performance."
        },
        {
            "heading": "IV. EXPERIMENT",
            "text": ""
        },
        {
            "heading": "A. DATASET",
            "text": "The dataset used in the experiment is the TinyPerson dataset, which is a small target dataset with high-quality annotations [66]. The images in this dataset are mostly taken by aerial photography at a long distance with the large background, which is the typical small target detection scenario. The TinyPerson dataset contains two categories, i.e., earth person and sea person, with a total of 1,610 images, in which 794 in the training set and 816 in the testing set, including 72,651 human target annotations."
        },
        {
            "heading": "B. EVALUATION METRICS",
            "text": "In order to evaluate the effectiveness of the model, this paper uses Average Precision (AP), mean Average Precision (mAP), Giga Floating-point Operations Per Second (GFLOP/s), and Frame Per Second (FPS) as the evaluation criteria. Assuming that the number of correctly detected targets in the results is TP, the number of incorrectly detected targets in the results is FP, and the number of targets that were not\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\ndetected among the correct targets is FN, the precision rate P (Precision) and recall rate R (Recall) are defined by (7) and (8):\nTPP TP FP  \n(7)\nTPR TP FN  \n(8)\nTo establish a two-dimensional coordinate axis with the xaxis as recall rate and the y-axis as precision rate, simultaneously draw the Precision-Recall (PR) curve, and the area surrounded by the PR curve is the size of the AP, as shown in (9):\nAPI PdR  (9) mAP is the mean average precision of multiple categories, where n represents the total number of categories, and the formula is shown in (10):\n1\n1 i\nn\nmAP AP i I I n    (10) GFLOP/s is the floating-point operation per second, usually used to measure the computational complexity of the model. Frame rate represents the number of images that can be detected per second (unit: frame/s), which is used to measure whether the algorithm has real-time performance. It is generally believed that FPS greater than 30 frame/s indicates real-time detection effect. AP50 and mAP50 represent the average precision and mean average precision when IoU threshold is 0.5. Generally, the higher the IoU, the larger the intersection between the predicted target and the ground truth, and the closer to the expected target. At this time, the larger the detection accuracy indicates the stronger the prediction ability of model. It is obvious that small targets have fewer pixels, and if a larger IoU is used, the accuracy will be very low, which cannot measure the effect of small target detection algorithms well. Therefore, this paper chooses the compromise solution of IoU of 0.5."
        },
        {
            "heading": "C. EXPERIMENT SETTINGS",
            "text": "The hardware environment for the experiment consists of an Intel Core i7-10750H CPU @ 2.60GHz, 16GB RAM, and NVIDIA GeForce GTX 1660Ti GPU. The software environment consists of Windows 11 system, python3.7, PyTorch 1.8.3, and cuda11.2. Figure 13 shows the curve of regression loss during training. The batch size is set to 4, the training is conducted for 150 epochs, with the first three epochs being warm-up. The optimizer used is SGD with an initial learning rate of 0.01, momentum of 0.937, and learning rate decay using the cosine strategy. As can be seen from Figure 13, the regression loss during training can smoothly decrease and achieve the desired effect. Except for necessary improvements, all hyper-parameters for the models in this experiment are set to default (not necessarily optimal)\nand training, validation, and testing are performed under this setting."
        },
        {
            "heading": "D. ABLATION EXPERIMENTS",
            "text": "Since the default input size of YOLOv5s is 640\u00d7640, but the targets in the TinyPerson dataset are small targets in aerial images with distant backgrounds, and the image sizes are much larger than the default size. Obviously, the larger the input resolution, the more advantageous it is for detecting small targets, but larger resolutions will result in more expensive computational costs, and the FPS for detecting images will also be lower.\nTABLE I INFLUENCE OF DIFFERENT INPUT\nRESOLUTIONS ON ABLATION EXPERIMENTS. THE VALUE IN THE INPUT RESOLUTION FIELD REPRESENTS WIDTH ONLY, AND HEIGHT EQUALS WIDTH. IN FPS1280, 1280 MEANS THAT THE RESOLUTION OF THE IMAGE DURING THE\nTESTING IS 1280\u00d71280. Methods YOLOv5s MBAB-YOLO\nInput_resolutions 640 960 1280 640 960 1280 Num_params(106) 7.02 7.02 7.02 7.37 7.37 7.37\nSizes(MB) 56.8 1 57.0 2 57.3 1 60.5 9 61.4 4 62.6 2 GFLOP/s 15.8 33.9 57.3 19.9 42.6 74.4 mAP50(%) 32.2\n7 41.3 4 47.9 2 38.1 6 45.8 2 52.0 7\nFPS1280 122 122 122 74 74 74\nTherefore, this paper tests YOLOv5s and proposed method at different resolutions, and the test results are shown in TABLE I. Based on the TinyPerson dataset, when the resolution of YOLOv5s trained is 960\u00d7960 and 1280\u00d71280, the mAP50 has increased by 9.07% and 15.65% respectively compared to 640\u00d7640, and the FPS is 122 frames per second at a testing resolution of 1280\u00d71280. As a contrast, MBABYOLO has increased the mAP50 by 7.66% and 13.91% respectively compared to 640\u00d7640 at resolutions of 960\u00d7960 and 1280\u00d71280, and the FPS is 74 frames per second at a testing resolution of 1280\u00d71280. This indicates that increasing the image resolution of the model can improve its accuracy under the same target detection network structure. However, increasing the resolution will increase the computational cost several times, and the training time will also increase significantly. For example, under the\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\nexperimental configuration of MBAB-YOLO, if 1280\u00d71280 resolution is used as the network input, one epoch of training takes 26 minutes, and the computation complexity is 74.4GFLOP/s, while for input at a resolution of 640\u00d7640, it only takes about 9 minutes for one epoch of training and the computation complexity is 19.9GFLOP/s. Correspondingly, the size of the generated weight file will also increase, making it more difficult to deploy the model. However, if different models trained with different resolutions are used for the same network structure, and the input resolution for detection is the same, it will not affect FPS, which means that the input resolution of the model can be appropriately increased during training to improve detection accuracy. In\naddition, excessively high resolutions can also cause overfitting, so blindly increasing the resolution is not recommended. To validate the effectiveness of proposed MBAB and CMBAB, and to investigate the impact of additional small target detection head on the results, experiments are conducted to evaluate the effects of different modules on the results. As usual, YOLOv5s is used as the baseline model, with the training resolution of 1280\u00d71280 and the testing resolution of 1280\u00d71280. A total of 150 epochs are trained with pre-trained weights to accelerate training process, and the experimental results are shown in TABLE II.\nIMPACT OF ADDITIONAL DETECTION HEAD. From TABLE II, it can be seen that \u03b1 is the baseline model without improvement, and \u03b2 adds a P2 detection head on base of the baseline model. Since the P2 detection layer has more shallow information, it is more favorable for small target detection. The experimental results show that compared with \u03b1, the number of layers in \u03b2 increases from 270 to 328, GFLOP/s increases from 57.27 to 65.39, and the parameter quantity increases from 7.02M to 7.17M, but the mAP50 for small targets increases by 1.78%, and the FPS still meets the requirements of real-time detection. Therefore, it is worthwhile to increase a small amount of computation to achieve better small target detection performance. IMPACT OF BLENDED ATTENTION BLOCK. MBAB can adaptively combine the blended attention mechanism to adjust different receptive fields that are more suitable for small targets, and thereby obtain fully weighted and re-calibrated feature maps for small targets. \u03b3 introduces MBAB in front of the SPPF (Spatial Pyramid Pooling-Fast) layer of Backbone and the final part of the up-sampling and down-sampling in the Neck, respectively. Compared with \u03b2, the number of layers, GFLOP/s, and parameter quantity of \u03b3 increase by 168, 10.77, and 0.45M, respectively, but the mAP50 increases by 2.01%. In addition, \u03b4 is based on \u03b3, and replaces the original C3 module in CSPNet structure, which is not attention-weighted, with the CMBAB structure, before the downsampling of Neck inputting Head. Compared with \u03b3, \u03b4 increases the number of layers by 91, decreases GFLOP/s by 1.76, decreases parameter quantity by 0.25, and at the same time, increases mAP50 by 0.36%. The experimental\nresults show that adding the MBAB mechanism and replacing C3 with the lighter CMBAB structure in the feature extraction can enable the model to obtain more abundant small target features. In summary, compared with the baseline \u03b1, proposed \u03b4 in this paper has increased the mAP50 by 4.15%, significantly improving the detection accuracy of small targets, and the FPS reaches 74 frame/s, showing the worthy real-time detection capabilities."
        },
        {
            "heading": "E. CONTRAST EXPERIMENTS",
            "text": "Based on the TinyPerson dataset, contrast experiments are conducted about MBAB-YOLO and several benchmark methods for real-time small target detection, i.e., CBAM, PPYOLO [67], DETR [68], YOLOv7 [69], YOLOX [70], and YOLOv5, in which YOLOX and PP-YOLO use the s version, YOLOv7 uses the tiny version, and YOLOv5s uses the newer 7.0 version. The experimental results are shown in TABLE III. According to TABLE III, among the compared algorithms, YOLOv5 with CBAM has the highest mAP50 of 50.61%, which is 1.46% lower than that of MBAB-YOLO. However, the other parameters in our method are similar to CBAM, ensuring a stable balance between detection accuracy, speed, parameter quantity, and model size. YOLOv7-tiny has the fastest detection speed of 131.21 frames per second, with the smallest parameter quantity, model size, and GFLOP/s, but its mAP50 is 6.84% lower than that of MBAB-YOLO. Obviously, MBAB-YOLO focuses more attention on small targets, dynamically recalibrating small targets in feature maps of different scales, not only improving detection accuracy but also ensuring\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJun Zhang,Yizhen Meng, et al: MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection\nspeed and real-time performance, which has the worthy advantages in real-time detection of small targets tasks.\nF. VISUALIZATIONS In order to intuitively verify the influence of the proposed multi-branching blended attention block on the features of small targets, as well as the impact on the final detection of small targets, representative images from the TinyPerson testing set were used for validation. Figure 14 shows the baseline YOLOv5 model without BAB, and Figure 15 shows the YOLOv5 model added BAB, with Grad-CAM [71] used for heatmap visualization. From the comparisons of the two figures, we can see that the proposed blended attention block can focus the features of small targets more effectively at different angles when the number of targets is large and the targets are small, which is shown in the Figure 15 as clearer target boundaries and obvious color differences from the environment.\nFigure 16 and Figure 17 show the practical detection results of YOLOv5 before and after adding BAB respectively, where the red box represents the human label detected by the model, and the yellow box highlights the differences. From the Figure 16 and Figure 17, it can be seen that under different conditions, such as on land or on sea, during the day or in the evening, the baseline model misses small targets, while the addition of proposed blended attention can better detect small targets. In summary, the proposed multibranching blended attention block can effectively improve the detection performance of small targets."
        },
        {
            "heading": "V. FUTURE WORKS",
            "text": "The main approach proposed in this paper to improve the YOLOv5s model for small target detection is the multibranching blended attention block, and it does not consider combining with other related methods (such as data augmentation and self-attention mechanism, etc). Therefore, in future research, our study can be conducted on how to\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3 VOLUME XX, 2017\ncombine MBAB-YOLO with more advanced methods to achieve high-performance small target detection. On the whole, in the development of target detection method, people have gradually discovered problems and proposed corresponding solutions. The accuracy of small target detection has gradually improved, but there is still a lot of room for improvement. MULTI-SCALE FEATURE FUSION. After several years of research, the multi-scale feature fusion in ConvNets has achieved good results in the effectiveness and efficiency, but there is still a lack of the mathematical principle and the interpretability. Currently, most feature fusion structures rely on empirical design and experimental improvements. In other words, feature fusion structures, even feature extraction models, can be seen as numerical fitting results of a large amount of data, lacking reasonable explanations. Therefore, there is still a large space for interpretation in the subsequent development. In addition, Transformer [72] has become a research hotspot in the field of computer vision, and multiscale fusion of visual Transformer is also a relatively new processing solution. EVALUATION METRICS. Anchor-based target detection methods have performed very well for medium and large targets, and the performance of small target detection has gradually improved. Due to the sensitivity of anchors to small targets, they may cause slow convergence or even convergence difficulties during training. Although there are now methods to address such problems and achieve good results, the effective way to eliminate the disadvantages of anchors for small targets is to remove anchors and use anchor-free methods. Some existing research has also proven that anchor-free methods can achieve the same effect as anchor-based methods. Anchor-free detectors determine the location of the target through point priors, which are more suitable for detecting small targets compared to anchor-based detectors. Small targets detection requires higher localization accuracy than large targets, so suitable evaluation metrics can greatly improve the location accuracy. Including the center distance between the predicted values and the ground truth values in the evaluation metrics can improve the accuracy of small target localization. SUPER-RESOLUTION. Super-resolution is a popular direction in the field of computer vision, and there is still a lot of room for development in small target detection. Since the super-resolution reconstruction process is relatively independent of target detection, it limits the integration of the two. Small targets have less feature information, while superresolution methods can effectively solve this problem. Therefore, using super-resolution reconstruction to enrich the details of small targets, and then converting small target detection problems into medium and large target detection can improve the detection accuracy. THE OPTIMIZATION OF YOLO. Although the YOLO family has made significant progress after years of development, further research is needed to solve more practical problems, such as rotated bounding boxes, 3D\ntargets, few samples, aerial scenes, and how to optimize and deploy the researched algorithm with TensorRT. In addition, although the YOLO series are the leaders in speed-accuracy balance in the field of target detection, their main work is aimed at computer terminals. Currently, edge computing has become an important trend in the development of artificial intelligence (AI). How to make YOLO lighter and faster for embedded AI computing devices such as Nvidia Jetson TX2, Nano, and Raspberry Pi is a worthy question to ponder."
        },
        {
            "heading": "VI. CONCLUSION",
            "text": "In general, our improvements are mainly based on YOLOv5s. In terms of the Backbone, the proposed multibranching blended attention block based on CSPNet, i.e., CMBAB, is introduced at the end. In terms of the feature fusion network, the small target detection branch is added, and proposed MBAB modules are inserted after each layer of the feature pyramid fusion and CMBAB & MBAB modules are inserted after each layer of down-sampling. Finally, the proposed small target detection method MBAB-YOLO is experimentally evaluated on the open source dataset, and the results show that MBAB-YOLO has excellent detection performance, with high accuracy and fast speed, which can meet real-time detection needs."
        }
    ],
    "title": "MBAB-YOLO:A Modified Lightweight Architecture for Real-Time Small Target Detection",
    "year": 2023
}