{
    "abstractText": "Ahypergraph is a data structure composed of nodes and hyperedges, where each hyperedge is an any-sized subset of nodes. Due to the flexibility in hyperedge size, hypergraphs represent group interactions (e.g., co-authorship by more than two authors) more naturally and accurately than ordinary graphs. Interestingly, many real-world systems modeled as hypergraphs contain edge-dependent node labels, i.e., node labels that vary depending on hyperedges. For example, on co-authorship datasets, the same author (i.e., a node) can be the primary author in a paper (i.e., a hyperedge) but the corresponding author in another paper (i.e., another hyperedge). In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we proposeWHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end,WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.",
    "authors": [
        {
            "affiliations": [],
            "name": "Minyoung Choe"
        },
        {
            "affiliations": [],
            "name": "Sunwoo Kim"
        },
        {
            "affiliations": [],
            "name": "Jaemin Yoo"
        },
        {
            "affiliations": [],
            "name": "Kijung Shin"
        }
    ],
    "id": "SP:ea6d42c24d54df79dc955917fc3d8f02e1736f58",
    "references": [
        {
            "authors": [
                "Ralph Abboud",
                "Ismail Ilkan Ceylan",
                "Martin Grohe",
                "Thomas Lukasiewicz"
            ],
            "title": "The surprising power of graph neural networks with random node initialization",
            "year": 2020
        },
        {
            "authors": [
                "Amir Hosein Khas Ahmadi"
            ],
            "title": "Memory-based graph networks",
            "venue": "Ph.D. Dissertation. University of Toronto (Canada)",
            "year": 2020
        },
        {
            "authors": [
                "Ryan Aponte",
                "Ryan A Rossi",
                "Shunan Guo",
                "Jane Hoffswell",
                "Nedim Lipka",
                "Chang Xiao",
                "Gromit Chan",
                "Eunyee Koh",
                "andNesreen Ahmed"
            ],
            "title": "AHypergraphNeural Network Framework for Learning Hyperedge-Dependent Node Embeddings",
            "year": 2022
        },
        {
            "authors": [
                "Song Bai",
                "Feihu Zhang",
                "Philip HS Torr"
            ],
            "title": "Hypergraph convolution and hypergraph attention",
            "venue": "Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi"
            ],
            "title": "Laplacian eigenmaps for dimensionality reduction and data representation",
            "venue": "Neural computation 15,",
            "year": 2003
        },
        {
            "authors": [
                "Austin R Benson",
                "Rediet Abebe",
                "Michael T Schaub",
                "Ali Jadbabaie",
                "Jon Kleinberg"
            ],
            "title": "Simplicial closure and higher-order link prediction",
            "venue": "Proceedings of the National Academy of Sciences 115,",
            "year": 2018
        },
        {
            "authors": [
                "Phillip Bonacich"
            ],
            "title": "Power and centrality: A family of measures",
            "venue": "AJS 92,",
            "year": 1987
        },
        {
            "authors": [
                "Fanchen Bu",
                "Geon Lee",
                "Kijung Shin"
            ],
            "title": "Hypercore Decomposition for Non-Fragile Hyperedges: Concepts, Algorithms, Observations, and Applications",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Ming Chen",
                "Zhewei Wei",
                "Zengfeng Huang",
                "Bolin Ding",
                "Yaliang Li"
            ],
            "title": "Simple and deep graph convolutional networks",
            "year": 2020
        },
        {
            "authors": [
                "Eli Chien",
                "Chao Pan",
                "Jianhao Peng",
                "Olgica Milenkovic"
            ],
            "title": "You are allset: A multiset function framework for hypergraph neural networks. In ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Uthsav Chitra",
                "Benjamin Raphael"
            ],
            "title": "Random walks on hypergraphs with edge-dependent vertex weights",
            "year": 2019
        },
        {
            "authors": [
                "Minyoung Choe",
                "Sunwoo Kim",
                "Jaemin Yoo",
                "Kijung Shin"
            ],
            "title": "Classification of Edge-dependent Labels of Nodes in Hypergraphs (Code, Datasets, and Online Appendix)",
            "year": 2023
        },
        {
            "authors": [
                "Harold Cramer"
            ],
            "title": "Mathematical methods of statistics, Princeton Univ",
            "venue": "PUP",
            "year": 1946
        },
        {
            "authors": [
                "Manh Tuan Do",
                "Se-eun Yoon",
                "Bryan Hooi",
                "Kijung Shin"
            ],
            "title": "Structural patterns and generative models of real-world hypergraphs",
            "year": 2020
        },
        {
            "authors": [
                "Yihe Dong",
                "Will Sawin",
                "Yoshua Bengio"
            ],
            "title": "HNHN: hypergraph networks with hyperedge neurons",
            "venue": "In ICML Graph Representation Learning and Beyond Workshop",
            "year": 2020
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Xavier Bresson"
            ],
            "title": "A generalization of transformer networks to graphs",
            "venue": "In AAAI Workshop on Deep Learning on Graphs: Methods and Applications",
            "year": 2021
        },
        {
            "authors": [
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Thomas Laurent",
                "Yoshua Bengio",
                "Xavier Bresson"
            ],
            "title": "Graph neural networks with learnable structural and positional representations. In ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Yifan Feng",
                "Haoxuan You",
                "Zizhao Zhang",
                "Rongrong Ji",
                "Yue Gao"
            ],
            "title": "Hypergraph neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Xuemei Gu",
                "Lijun Chen",
                "Mario Krenn"
            ],
            "title": "Quantum experiments and hypergraphs: Multiphoton sources for quantum interference, quantum computation, and quantum entanglement",
            "venue": "PRA 101,",
            "year": 2020
        },
        {
            "authors": [
                "Deepesh Hada",
                "Shirish Shevade"
            ],
            "title": "HyperTeNet: Hypergraph and Transformer-based Neural Network for Personalized List Continuation",
            "venue": "In ICDM",
            "year": 2021
        },
        {
            "authors": [
                "Koby Hayashi",
                "Sinan G Aksoy",
                "Cheong Hee Park",
                "Haesun Park"
            ],
            "title": "Hypergraph random walks, laplacians, and clustering",
            "venue": "In CIKM",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Jing Huang",
                "Jie Yang"
            ],
            "title": "Unignn: a unified framework for graph and hypergraph neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Hyunjin Hwang",
                "Seungwoo Lee",
                "Kijung Shin"
            ],
            "title": "HyFER: A Framework for Making Hypergraph Learning Easy, Scalable and Benchmarkable",
            "venue": "In WWW Workshop on Graph Learning Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Jiayang Jiang",
                "Michael Mitzenmacher",
                "Justin Thaler"
            ],
            "title": "Parallel peeling algorithms",
            "venue": "TOPC 3,",
            "year": 2017
        },
        {
            "authors": [
                "Jinwoo Kim",
                "Saeyoon Oh",
                "Seunghoon Hong"
            ],
            "title": "Transformers Generalize DeepSets and Can be Extended to Graphs & Hypergraphs",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In ICLR",
            "year": 2015
        },
        {
            "authors": [
                "Risi Kondor",
                "Jean-Philippe Vert"
            ],
            "title": "Diffusion kernels. kernel methods in computational biology",
            "year": 2004
        },
        {
            "authors": [
                "Kirill Kovalenko",
                "Miguel Romance",
                "Ekaterina Vasilyeva",
                "David Aleja",
                "Regino Criado",
                "Daniil Musatov",
                "Andrei M Raigorodskii",
                "Julio Flores",
                "Ivan Samoylenko",
                "Karin Alfaro-Bittner"
            ],
            "title": "Vector centrality in hypergraphs",
            "venue": "Chaos, Solitons & Fractals",
            "year": 2022
        },
        {
            "authors": [
                "Devin Kreuzer",
                "Dominique Beaini",
                "Will Hamilton",
                "Vincent L\u00e9tourneau",
                "Prudencio Tossou"
            ],
            "title": "Rethinking graph transformers with spectral attention",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Geon Lee",
                "Minyoung Choe",
                "Kijung Shin"
            ],
            "title": "How do hyperedges overlap in real-world hypergraphs?-patterns, measures, and generators",
            "year": 2021
        },
        {
            "authors": [
                "Geon Lee",
                "Minyoung Choe",
                "Kijung Shin"
            ],
            "title": "HashNWalk: Hash and Random Walk Based Anomaly Detection in Hyperedge Streams",
            "year": 2022
        },
        {
            "authors": [
                "Geon Lee",
                "Jihoon Ko",
                "Kijung Shin"
            ],
            "title": "Hypergraph motifs: concepts, algorithms, and discoveries",
            "venue": "PVLDB 13,",
            "year": 2020
        },
        {
            "authors": [
                "Juho Lee",
                "Yoonho Lee",
                "Jungtaek Kim",
                "Adam Kosiorek",
                "Seungjin Choi",
                "Yee Whye Teh"
            ],
            "title": "2019. Set transformer: A framework for attention-based permutation-invariant neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Jianbo Li",
                "Jingrui He",
                "Yada Zhu"
            ],
            "title": "E-tail product return prediction via hypergraph-based local graph cut",
            "year": 2018
        },
        {
            "authors": [
                "Pan Li",
                "Yanbang Wang",
                "Hongwei Wang",
                "Jure Leskovec"
            ],
            "title": "Distance encoding: Design provably more powerful neural networks for graph representation learning",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Jianhua Lin"
            ],
            "title": "Divergence measures based on the Shannon entropy",
            "venue": "IEEE Transactions on Information theory 37,",
            "year": 1991
        },
        {
            "authors": [
                "Liheng Ma",
                "Reihaneh Rabbany",
                "Adriana Romero-Soriano"
            ],
            "title": "Graph attention networks with positional embeddings",
            "venue": "In PAKDD",
            "year": 2021
        },
        {
            "authors": [
                "Gr\u00e9goire Mialon",
                "Dexiong Chen",
                "Margot Selosse",
                "Julien Mairal"
            ],
            "title": "Graphit: Encoding graph structure in transformers",
            "year": 2021
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "year": 2013
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "NeurIPS",
            "year": 2013
        },
        {
            "authors": [
                "Michael Molloy"
            ],
            "title": "Cores in random hypergraphs and Boolean formulas",
            "venue": "Random Structures & Algorithms 27,",
            "year": 2005
        },
        {
            "authors": [
                "Mathias Niepert",
                "Mohamed Ahmed",
                "Konstantin Kutzkov"
            ],
            "title": "Learning convolutional neural networks for graphs",
            "year": 2016
        },
        {
            "authors": [
                "Lawrence Page",
                "Sergey Brin",
                "Rajeev Motwani",
                "Terry Winograd"
            ],
            "title": "The PageRank citation ranking: Bringing order to the web",
            "venue": "Technical Report. Stanford InfoLab",
            "year": 1999
        },
        {
            "authors": [
                "Wonpyo Park",
                "Woong-Gi Chang",
                "Donggeon Lee",
                "Juntae Kim"
            ],
            "title": "GRPE: Relative Positional Encoding for Graph Transformer",
            "venue": "In ICLR Machine Learning for Drug Discovery Workshop",
            "year": 2022
        },
        {
            "authors": [
                "Ruihong Qiu",
                "Zi Huang",
                "Tong Chen",
                "Hongzhi Yin"
            ],
            "title": "Exploiting positional information for session-based recommendation",
            "venue": "TOIS 40,",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "JMLR 21,",
            "year": 2020
        },
        {
            "authors": [
                "Ryoma Sato",
                "Makoto Yamada",
                "Hisashi Kashima"
            ],
            "title": "Random features strengthen graph neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Peter Shaw",
                "Jakob Uszkoreit",
                "Ashish Vaswani"
            ],
            "title": "Self-attention with relative position representations",
            "venue": "In NAACL",
            "year": 2018
        },
        {
            "authors": [
                "A Swati",
                "S Ashish",
                "M Nitish",
                "K Rohan",
                "C Denzil"
            ],
            "title": "Dblp records and entries for key computer science conferences",
            "year": 2016
        },
        {
            "authors": [
                "Francesco Tudisco",
                "Desmond J Higham"
            ],
            "title": "Node and edge nonlinear eigenvector centrality for hypergraphs",
            "venue": "Communications Physics 4,",
            "year": 2021
        },
        {
            "authors": [
                "Francesco Tudisco",
                "Desmond J Higham"
            ],
            "title": "Core-periphery detection in hypergraphs",
            "venue": "SIMODS 5,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "Haorui Wang",
                "Haoteng Yin",
                "Muhan Zhang",
                "Pan Li"
            ],
            "title": "Equivariant and stable positional encoding for more powerful graph neural networks. In ICLR",
            "year": 2022
        },
        {
            "authors": [
                "Minjie Wang",
                "Da Zheng",
                "Zihao Ye",
                "Quan Gan",
                "Mufei Li",
                "Xiang Song",
                "Jinjing Zhou",
                "Chao Ma",
                "Lingfan Yu",
                "Yu Gai"
            ],
            "title": "Deep graph library: A graph-centric, highly-performant package for graph neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Michael M Wolf",
                "Alicia M Klinvex",
                "Daniel M Dunlavy"
            ],
            "title": "Advantages to modeling relational data using hypergraphs versus graphs. In HPEC",
            "year": 2016
        },
        {
            "authors": [
                "Xin Xia",
                "Hongzhi Yin",
                "Junliang Yu",
                "Qinyong Wang",
                "Lizhen Cui",
                "Xiangliang Zhang"
            ],
            "title": "Self-supervised hypergraph convolutional networks for sessionbased recommendation",
            "year": 2021
        },
        {
            "authors": [
                "Naganand Yadati",
                "Madhav Nimishakavi",
                "Prateek Yadav",
                "Vikram Nitin",
                "Anand Louis",
                "Partha Talukdar"
            ],
            "title": "Hypergcn: A new method for training graph convolutional networks on hypergraphs",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Se-eun Yoon",
                "Hyungseok Song",
                "Kijung Shin",
                "Yung Yi"
            ],
            "title": "How much and when do we need higher-order information in hypergraphs? a case study on hyperedge prediction",
            "year": 2020
        },
        {
            "authors": [
                "Jiaxuan You",
                "Rex Ying",
                "Jure Leskovec"
            ],
            "title": "Position-aware graph neural networks",
            "venue": "In ICML",
            "year": 2019
        },
        {
            "authors": [
                "Jiawei Zhang",
                "Haopeng Zhang",
                "Congying Xia",
                "Li Sun"
            ],
            "title": "Graph-bert: Only attention is needed for learning graph representations",
            "year": 2020
        },
        {
            "authors": [
                "R Zhang",
                "Y Zou",
                "J Ma"
            ],
            "title": "Hyper-SAGNN: a self-attention based graph neural network for hypergraphs",
            "venue": "In ICLR",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "In this work, we introduce a classification of edge-dependent node labels as a new problem. This problem can be used as a benchmark task for hypergraph neural networks, which recently have attracted great attention, and also the usefulness of edge-dependent node labels has been verified in various applications. To tackle this problem, we proposeWHATsNet, a novel hypergraph neural network that represents the same node differently depending on the hyperedges it participates in by reflecting its varying importance in the hyperedges. To this end,WHATsNet models the relations between nodes within each hyperedge, using their relative centrality as positional encodings. In our experiments, we demonstrate that WHATsNet significantly and consistently outperforms ten competitors on six real-world hypergraphs, and we also show successful applications of WHATsNet to (a) ranking aggregation, (b) node clustering, and (c) product return prediction.\nCCS CONCEPTS \u2022 Information systems \u2192 Data mining; Social networks; \u2022 Computing methodologies\u2192 Machine learning.\nKEYWORDS Hypergraph, Graph Neural Network, Edge-Dependent Node Label"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Real-world relationships are complex and often go beyond pairwise relations. For example, a research paper is usually coauthored by a group of researchers, and an email is often sent to multiple receivers. A hypergraph is a natural representation of such group relations [7, 8, 16, 34, 36]. It consists of nodes and hyperedges, and each hyperedge is a set of any number of nodes (see Figure 1). Hypergraph modeling is used in various fields, including recommendation systems [22, 50, 61], and physics [21], leading to better performance than ordinary-graph modeling in node clustering [60], interaction prediction [64], anomaly detection [35], etc.\nTo leverage the advantages of hypergraph modeling, several hypergraph neural networks [3, 5, 12, 17, 20, 26, 27, 62] have been proposed. They commonly involve propagating node embeddings\nto incident hyperedges for updating, which are subsequently propagated back to the nodes. Most of them have been evaluated on node classification tasks, focusing on the global properties of nodes.\nHowever, in many real-world hypergraphs, node properties vary depending on the hyperedges they are involved in. For instance, in co-authorship, the same researcher can be the primary author in one paper but the corresponding author in another paper, as shown in Figure 1. In emails, the same person can be a receiver or a sender in different emails, and in online Q&A platforms, a person can be a questioner and an answerer in different posts. These edgedependent node properties have proven useful in various tasks, such as ranking aggregation [13], node clustering [23], product-return prediction [38], and anomaly detection [35].\nIn this work, we introduce a classification of edge-dependent node labels as a new problem for hypergraphs. The problem works as an effective hypergraph-learning benchmark task that is complementary to common benchmark tasks (e.g., node classification, hyperedge prediction, and clustering) for three reasons. First, it evaluates the capability of models in capturing features unique to hypergraphs. Edge-dependent node labels are unique to hypergraphs and cannot be easily expressed if we decompose hyperedges into pairwise edges between nodes. Second, as shown in our experiments, using node and hyperedge embeddings obtained by existing hypergraph neural networks leads to limited performance for the problem. Lastly andmost importantly, it has extensive real-world applications. For example, it can be used to predict the toxicity of chemical compounds (nodes) in specific reactions (hyperedges), offer guidance to students (nodes) who are expected to make limited contributions in group projects (hyperedges), and classify homonyms (nodes) based on the context of sentences (hyperedges). Moreover, the outputs of this problem (i.e., edge-dependent node properties) have proven to be useful in various applications. ar X iv :2\n30 6.\n03 03\n2v 1\n[ cs\n.S I]\n5 J\nun 2\n02 3\nIn order to tackle the new classification task, we proposeWHATsNet (Within-Hyperedge Attention Transformer Network). Most existing hypergraph neural networks do not explicitly consider edge-dependent relationships between node pairs within each hyperedge. We designWithinATT, an attention mechanism where the edge-dependent embedding of each node is computed by attending to the other nodes in each hyperedge. Inspired by the usefulness of positional encodings in the attention mechanism, we additionally use the centrality order of nodes within each hyperedge as the positional encoding, which makes attention in WithinATT even more edge-dependent. The effectiveness of WHATsNet on the proposed task is demonstrated through extensive experiments.\nOur contributions are summarized as follows:\n\u2022 New Problem: To the best of our knowledge, we are the first to address the problem of classifying edge-dependent node labels. This problem is based on a unique property of hypergraphs and closely related to real-world applications. \u2022 Effective Model: To tackle the problem, we design WHATsNet, a novel hypergraph neural network with attention and positionalencoding schemes that explicitly consider edge-dependent relationships between nodes. \u2022 Extensive Experiments: Using six real-world hypergraphs, we demonstrate the superiority of WHATsNet over 10 competitors on the considered task. We also show the usefulness of WHATsNet in three applications: ranking aggregation, node clustering, and product-return prediction.\nFor reproducibility, we make the code and data available at [14]. The rest of this paper is organized as follows. In Section 2, we review related works. In Section 3, we introduce preliminaries. In Section 4, we describe our new problem with its applications. In Section 5, we presentWHATsNet, a hypergraph neural network for edge-dependent node classification. In Section 6, we provide experimental results. Lastly, we make conclusions in Section 7."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "In this section, we briefly survey related works on hypergraph neural networks and positional encodings."
        },
        {
            "heading": "2.1 Hypergraph Neural Networks",
            "text": "Hypergraph neural networks generalize graph neural networks into higher-order relationships in hypergraphs by allowing propagation between nodes through hyperedges. Early models including HGNN [20] and HyperGCN [62] first transform a given hypergraph into an ordinary graph and then apply graph convolutions, losing high-order relationships embedded in hyperedges. HNHN [17] performs two steps of message passing using a nonlinear function to prevent equivalence to convolutions on the clique-expanded graph 1. The first step updates hyperedge embeddings and the second updates node embeddings. HNHN can vary weights on the contribution of incident embeddings during aggregation through hyperparameters.\nHCHA [5] and HAT [27] apply attention mechanisms to improve the aggregation of the incident node or hyperedge embeddings. The 1A clique-expanded graph is an ordinary graph obtained by replacing each hyperedge with the clique composed by the constituent nodes.\nattention weights are calculated from the concatenation of node and hyperedge embeddings, which change during training.\nUniGNN [26] andAllSet [12] generalize graph neural networks to hypergraphs. UniGCNII is a special case of UniGNN that addresses over-smoothing, i.e., the convergence of embeddings of different nodes in deep (hyper)graph neural networks, by extending GCNII [11] to hypergraphs. AllSet is a framework composed of two multiset functions and generalizes most models, including HGNN, HyperGCN, HNHN, and HCHA. AllSetTransformer replaces the multiset function with SetTransformer [37] and aggregates incident embeddings with attention to global learnable vectors.\nA recent study with a similar motivation to ours presents a framework named HNN [3] for jointly learning hyperedge embeddings and a set of hyperedge-dependent node embeddings. But HNN makes the hyperedge-dependent node embeddings by simply concatenating the embeddings of nodes and incident hyperedges.\nHowever, the importance of each node may vary depending on the other nodes it interacts with within each hyperedge, which may not be captured only by the relationship between the node and the hyperedge. Most existing hypergraph neural networks do not explicitly consider edge-dependent relationships between pairs of nodes within each hyperedge. Such relationships have been considered in a few studies in a way different from ours, but none of them directly apply to the considered problem. HyperSAGNN [67] is designed for hyperedge prediction, and it uses two kinds of embeddings: (1) static embeddings, which are obtained from node features independently of hyperedges, and (2) dynamic embeddings, which are calculated by aggregating the embeddings of the other nodes within hyperedges using pairwise attentions. The model is trained to minimize the discrepancy between the static embedding and dynamic embedding of each node. While HyperSAGNN uses pairwise attention, its goal is not to obtain hyperedge-dependent node representations. Higher-order Transformer [29] considers all subsets of nodes in each hyperedge,2 and thus, computational and memory cost increases exponentially with the size of hyperedges. The model has been applied only to uniform hypergraphs where the size of every hyperedge is identical, and it is non-trivial to apply it to real-world hypergraphs with hyperedges of varying sizes."
        },
        {
            "heading": "2.2 Positional Encodings",
            "text": "It is known that many graph neural networks (GNNs) have difficulty in discriminating the positions of different nodes if the nodes share similar local structures [65]. To improve the representation power, various positional encodings for graphs have been proposed. Such positional encodings, which we call absolute positional encodings, are often added or concatenated to the node features, and they are based on random discriminating features [1, 52], Weisfeiler\u2013Lehman [66], Laplacian Eigenvectors [6, 18, 33], random walk [2, 19, 39], etc. Alternatively, GNNs that are able to capture positional information of nodes [19, 58, 65] can be used.\nIn Transformer-based GNNs, such positional encodings are usually used to calculate attention. Graph Transformer [18] and SAN [33] use Laplacian eigenvectors, while Graph-Bert [66] uses the Weisfeiler algorithm [46]. Graphormer [63] adds the global degree centrality to node features to capture both semantic correlations 22\ud835\udc52 for each hyperedge \ud835\udc52 \u2286 V , where V is the set of nodes\nand node centrality differences. Its performance supports the effectiveness of employing node centrality as the positional encoding, which we also use in a different way for positional encoding.\nBesides the absolute positional encodings, relative positional encodings [51, 53] are also used in Transformer-based GNNs to affect attentionmechanisms based on the relative distance between a node pair (e.g., between a query and a key). The relative distance can be computed based on positive definite kernels on graphs [42], learnable positional embeddings [41], shortest path distance [49, 63], multi-step transition probability [68], etc. To the best of our knowledge, positional encodings specialized to hypergraphs have not been studied. In this work, we propose WithinOrderPE, where the centrality order (i.e., ranking) of nodes within each hyperedge is used to encode the relative position within each hyperedge."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "In this section, we introduce concepts necessary to describe our method. The frequently-used symbols are summarized in Table 1."
        },
        {
            "heading": "3.1 Hypergraphs",
            "text": "AhypergraphG = (V, E) consists of a set of nodesV = {\ud835\udc631, . . . , \ud835\udc63\ud835\udc41 } and a set of hyperedges E = {\ud835\udc521, . . . , \ud835\udc52\ud835\udc40 } \u2286 2V . Each hyperedge \ud835\udc52 \u2208 E is a non-empty subset ofV . We use N\ud835\udc63 = {\ud835\udc52 \u2208 E : \ud835\udc63 \u2208 \ud835\udc52} to denote the set of hyperedges incident to the node \ud835\udc63 , i.e., the set of hyperedges that contain \ud835\udc63 ."
        },
        {
            "heading": "3.2 Attention Functions",
            "text": "Scaled Dot-Product Attention. For\ud835\udc5b\ud835\udc5e query vectorsQ \u2208 R\ud835\udc5b\ud835\udc5e\u00d7\ud835\udc51\ud835\udc58 and \ud835\udc5b\ud835\udc58 key-value vector pairs K \u2208 R\ud835\udc5b\ud835\udc58\u00d7\ud835\udc51\ud835\udc58 and V \u2208 R\ud835\udc5b\ud835\udc58\u00d7\ud835\udc51\ud835\udc63 , where \ud835\udc51\ud835\udc58 and \ud835\udc51\ud835\udc63 represent vector sizes, the scaled dot-product attention computes the weighted sum of value vectors as follows:\n\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(Q,K,V) = \ud835\udc60\ud835\udc5c \ud835\udc53 \ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 (QK \u22a4\u221a\ufe01\n\ud835\udc51\ud835\udc58\n)V,\nwhere \u221a\ufe01 \ud835\udc51\ud835\udc58 is typically used to avoid exploding gradients [57]. Note that the more similar a query vector and a key vector are (i.e., the larger their dot product is), the larger the corresponding weight is. Multihead Attention. The multihead attention [57] of dimension \ud835\udc51\ud835\udc58 consists of \u210e attention modules of dimension \ud835\udc51\ud835\udc58/\u210e, whose outputs are concatenated for the final output, as follows:\n\ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc56\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(Q,K,V) = \ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc50\ud835\udc4e\ud835\udc61 (O1, \u00b7 \u00b7 \u00b7 ,O\u210e)WO,\nwhereO\ud835\udc56 = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(Q\ud835\udc56 ,K\ud835\udc56 ,V\ud835\udc56 ) = \ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(QWQ\ud835\udc56 ,KW K \ud835\udc56 ,VWV \ud835\udc56 ). That is, Q, K, and V are mapped into subspaces Q\ud835\udc56 \u2208 R\ud835\udc5b\ud835\udc5e\u00d7(\ud835\udc51\ud835\udc58/\u210e) , K\ud835\udc56 \u2208 R\ud835\udc5b\ud835\udc58\u00d7(\ud835\udc51\ud835\udc58/\u210e) , and V\ud835\udc56 \u2208 R\ud835\udc5b\ud835\udc58\u00d7(\ud835\udc51\ud835\udc63/\u210e) , respectively, by learnable parametersWQ\n\ud835\udc56 \u2208 R\ud835\udc51\ud835\udc58\u00d7(\ud835\udc51\ud835\udc58/\u210e) ,WK \ud835\udc56 \u2208 R\ud835\udc51\ud835\udc58\u00d7(\ud835\udc51\ud835\udc58/\u210e) , andWV \ud835\udc56 \u2208\nR\ud835\udc51\ud835\udc63\u00d7(\ud835\udc51\ud835\udc63/\u210e) . In our experiments, the matrixWO \u2208 R\ud835\udc51\ud835\udc63\u00d7\ud835\udc51\ud835\udc63 is fixed to the identity matrix, and the number of heads \u210e is fixed to 4. Multihead Attention Block.Weuse themultihead attention block (MAB) [57] as a component of our model. It consists of a multihead attention module, a feed-forward layer, residual connections [25], and layer normalization [4], as follows:\n\ud835\udc40\ud835\udc34\ud835\udc35(Q,K) = \ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(H + \ud835\udc39\ud835\udc52\ud835\udc52\ud835\udc51\ud835\udc39\ud835\udc5c\ud835\udc5f\ud835\udc64\ud835\udc4e\ud835\udc5f\ud835\udc51 (H))),\nwhere H = \ud835\udc3f\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f\ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(Q + \ud835\udc40\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc56\u210e\ud835\udc52\ud835\udc4e\ud835\udc51\ud835\udc34\ud835\udc61\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b(Q,K,K)). Note that the feed-forward layer does not change the dimensionality of the input matrix. Attention with Inducing Points. The scaled dot-product attention module requires all-pair dot-products between the query vectors and key vectors, making its time complexity quadratic, specifically\ud835\udc42 (\ud835\udc5b\ud835\udc5e\ud835\udc5b\ud835\udc58 ). SetTransformer [37] introduces an attention scheme that reduces the complexity to be linear. With\ud835\udc5a \u226a min(\ud835\udc5b\ud835\udc5e, \ud835\udc5b\ud835\udc58 ) trainable inducing points, denoted by \ud835\udc3c , the all-pair dot-product MAB(Q,K) is approximated by MAB(Q,MAB(I,K)) whose time complexity is \ud835\udc42 (\ud835\udc5a(\ud835\udc5b\ud835\udc5e + \ud835\udc5b\ud835\udc58 )). We adopt this strategy in our model for efficiency."
        },
        {
            "heading": "4 PROBLEM DEFINITION: EDGE-DEPENDENT NODE CLASSIFICATION IN A HYPERGRAPH",
            "text": "We formally define the edge-dependent node classification problem and introduce its direct applications to real-world tasks."
        },
        {
            "heading": "4.1 Problem Formulation",
            "text": "In classical node classification, a single label is assigned to a node. However, in hypergraphs, the labels of nodes can vary depending on the hyperedges that the nodes belong to. For example, in a coauthorship hypergraph, the role of a researcher is categorized into the first, the last, and the middle author, and the position is likely to change in other publications. In this work, we introduce the edge-dependent node classification problem, formalized as follows.\nProblem 1 (Edge-dependent node classification). Given (a) a hypergraph G = (V, E), (b) a set of edge-dependent node labels in E\u2032 \u2282 E (i.e., \ud835\udc66\ud835\udc63,\ud835\udc52 ,\u2200\ud835\udc63 \u2208 \ud835\udc52,\u2200\ud835\udc52 \u2208 E\u2032), and optionally (c) a node feature matrix X, the problem is to correctly predict the unknown edge-dependent node labels in E \\ E\u2032 (i.e., \ud835\udc66\ud835\udc63,\ud835\udc52 ,\u2200\ud835\udc63 \u2208 \ud835\udc52,\u2200\ud835\udc52 \u2208 E \\ E\u2032).\nThis problem serves as an effective benchmark task and complements common hypergraph learning benchmarks for the following reasons: First, the edge-dependent node labels are unique to hypergraphs and cannot be reduced to node labels in the clique-expanded graphs (see Footnote 1). Thus, we can evaluate the capability of methods in capturing these unique characteristics of hypergraphs. Second, as demonstrated in the experiment section 6, existing hypergraph neural networks have limitations in perfectly addressing\nthis problem. Lastly, the predictive outputs (i.e., edge-dependent node labels) can directly be applied to various applications."
        },
        {
            "heading": "4.2 Applications",
            "text": "Predicted edge-dependent node labels give information about the importance of nodes within each hyperedge, and leveraging this information has proven useful in a variety of applications. Ranking Aggregation. The task is to predict the overall (global) ranking of entities based on partial (local) rankings. For example, in a multiplayer game where participants compete in matches, the aim of the task is to determine the global ranking of players using their local rankings frommatches, enabling fair and competitive matches. For this task, we can construct a hypergraph where (a) each node is a player, (b) each hyperedge is a match, (c) and the participants of a match are labeled based on their contribution levels during the match, as described in [13]. A similar task can be considered for a co-authorship dataset. When the contribution level of each author in each publication is given, which is determined by the order of authors in each publication, the objective is to infer meaningful rankings for all authors. Clustering. Clustering refers to the problem of grouping similar elements, and the problem is commonly considered for data analysis. Hayashi et al. [23] designed a node clustering algorithm that leverages edge-dependent node weights, demonstrating its highquality clusters for real-world hypergraphs. Edge-dependent node labels, inferred by resolving Problem 1, can be directly transformed into node weights for clustering purposes. For example, in a coauthorship dataset, the weights of authors (nodes), determined by their positions (e.g., first, last, or others) within each publication (hyperedge), can be utilized to enhance clustering results. Product Return Prediction. In E-commerce, predicting customers\u2019 intention to return products is helpful to take proactive actions. Li et al. [38] showed that the product return probability of each target basket can be accurately estimated by using hypergraphs where nodes and hyperedges are baskets and products, respectively, and baskets are labeled based on the number of each product they contain. The authors showed that these edge-dependent node labels are informative in predicting product returns. For instance, multiple purchases of the same product within a basket may indicate that\nthe customer purchases the same product with different colors or sizes, increasing the likelihood of product return.\n5 PROPOSED MODEL: WHATSNET WeproposeWHATsNet (Within-HyperedgeAttentionTransformer Network) for edge-dependent node classification, with each layer consisting of two message passing steps: (1) updating hyperedge embeddings by aggregating node embeddings belonging to each hyperedge, and (2) updating node embeddings by aggregating hyperedge embeddings that include the corresponding node. In each step, input embeddings of nodes (or hyperedges) are adapted using an attention module calledWithinATT, and then aggregated into hyperedge (or node) embeddings.WithinATT considers edgedependent (or node-dependent) relations between adjacent nodes (or hyperedges) by incorporating edge-dependent positional encodings namedWithinOrderPE. The embeddings generated from the final layer are utilized to predict edge-dependent node labels.\n5.1 WithinATT: Attention to Other Nodes within Hyperedges\nIn many real-world hypergraphs, the importance or role of a node is shaped by the other nodes within the same hyperedge. For instance, if a grown-up researcher co-authors a paper only with students, she is likely to be the last author of the paper. Conversely, if she co-authors with other grown-up researchers, then the probability of her being the last author of that paper is relatively low.\nMotivated by this observation, we devise WithinATT based on Transformer [57], and it adapts a node embedding by attending to the other nodes in the same hyperedge. Specifically, it uses a set of node embeddings as queries, keys, and values in the attention mechanism. It, thereby, models and utilizes relations between nodes by taking the dot-product of all node pairs within the hyperedge.\nHowever, calculating the dot-product for every node pair has computational complexity quadratic in the hyperedge size, making it challenging to use in large-scale real-world hypergraphs. To overcome this issue, we adopt the inducing point method (described in Section 3.2) in SetTransformer [37], which performs comparably to all-pair attention while being significantly more efficient.\nLet V(\ud835\udc59 )\ud835\udc52 = {X (\ud835\udc59 ) \ud835\udc62 : \ud835\udc62 \u2208 \ud835\udc52} denote the set of the embeddings of nodes in a hyperedge \ud835\udc52 in the \ud835\udc59-th layer, which are in the form of a matrix in R |\ud835\udc52 |\u00d7\ud835\udc51\ud835\udc59 . Along with V(\ud835\udc59 )\ud835\udc52 , WithinATT uses\ud835\udc5a number of \ud835\udc51\ud835\udc59 -dimensional trainable inducing points I\ud835\udc64 \u2208 R\ud835\udc5a\u00d7\ud835\udc51\ud835\udc59 , where\ud835\udc5a is typically much smaller than max\ud835\udc52\u2208E |\ud835\udc52 |. In our experiments,\ud835\udc5a is fixed to 4. Then,WithinATT is formally expressed as follows:\nWithinATT(V(\ud835\udc59 )\ud835\udc52 ; I\ud835\udc64) = MAB(V (\ud835\udc59 ) \ud835\udc52 ,MAB(I\ud835\udc64 ,V (\ud835\udc59 ) \ud835\udc52 )) . (1)\nEven though I\ud835\udc64 is shared across all hyperedges, training the inducing points can be seen as finding a proper projection function for any input onto a lower-dimensional space. Additionally, the inner MAB (defined in Section 3.2) would summarize the node embeddings in \ud835\udc52 using the well-trained I\ud835\udc64 . Through these outputs, the outer MAB would generate edge-dependent representations of nodes within \ud835\udc52 , leveraging the attention between the input node embeddings and the summary of them. Consequently, the resulting representations serve as an approximation of the attention between all node pairs while avoiding the quadratic complexity.\n5.2 WithinOrderPE: Using Centrality for Positional Encoding\nWe expect that edge-dependent node labels are closely related to the relative positions of nodes within each hyperedge, which can be measured, for example, by centrality relative to the other nodes in each hyperedge. For instance, in co-authorship datasets, degree centrality indicates the number of papers authored by each researcher. Hence, the author with the highest centrality among all authors of a paper is more likely to be the last author.\nBased on this reasoning, we deviseWithinOrderPE, a positional encoding that facilitates edge-dependent attention between nodes within each hyperedge. Specifically, we use the relative order of node centrality within the hyperedge for WithinOrderPE. We define the order of each element \ud835\udc4e in a set \ud835\udc34 as follows:\n\ud835\udc42\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc5f (\ud835\udc4e,\ud835\udc34) = \u2211\ufe01\n\ud835\udc4e\u2032\u2208\ud835\udc34 1(\ud835\udc4e \u2032 \u2264 \ud835\udc4e) . (2)\nThen, given node centralities F \u2208 R\ud835\udc41\u00d7\ud835\udc51\ud835\udc53 where \ud835\udc51\ud835\udc53 is the number of centrality measures, corresponding to the dimensionality of positional encodings, we defineWithinOrderPE of a node \ud835\udc63 within a hyperedge \ud835\udc52 as follows:\nWithinOrderPE(\ud835\udc63, \ud835\udc52; F) =\u2225\ud835\udc51\ud835\udc53 \ud835\udc56=1 1 |\ud835\udc52 |\ud835\udc42\ud835\udc5f\ud835\udc51\ud835\udc52\ud835\udc5f (F\ud835\udc63,\ud835\udc56 , {F\ud835\udc62,\ud835\udc56 : \u2200\ud835\udc62 \u2208 \ud835\udc52}),\n(3) where \u2225 represents the concatenation of the orders with respect to different centrality measures. We simply addWithinOrderPE to the node embeddings and feed them into WithinATT. In our experiments, we leverage four types of node centrality: degree, eigenvector centrality, PageRank, and coreness. Details of them are provided in Appendix B.2.\nSimilarly, for the message passing from hyperedges to nodes, we explicitly give the target node\u2019s order within the source hyperedge. In other words, the same position encoding is used for each hyperedge-node pair in both directions of message passing. This is because the role of a hyperedge \ud835\udc52 with respect to the target node \ud835\udc63 \u2208 \ud835\udc52 is also affected by the importance of node \ud835\udc63 in \ud835\udc52 .\n5.3 WHATsNet: Our Final Model We introduceWHATsNet, a hypergraph neural network that integrates the afore-described modules. In each layer, hyperedge embeddings are first updated and then node embeddings are updated. Before updating hyperedge embeddings, edge-dependent node embeddings are computed by incorporating edge-dependent relationships between nodes within each hyperedge via WithinATT (Eq. (5)) andWithinOrderPE (Eq. (4)). The hyperedge embedding is then updated by aggregating these edge-dependent node embeddings with weights determined by the dot-product with the hyperedge embedding from the previous layer. This process can be simplified using MAB (defined in Section 3.2) (Eq. (6)). Specifically, a hyperedge \ud835\udc52\u2019s embedding H(\ud835\udc59 )\ud835\udc52 is updated as follows:\nV(\ud835\udc59 )\ud835\udc52,\u229e = {X (\ud835\udc59\u22121) \ud835\udc63 \u229eWithinOrderPE(\ud835\udc63, \ud835\udc52)) : \ud835\udc63 \u2208 \ud835\udc52}, (4)\nV\u0303(\ud835\udc59 )\ud835\udc52 = WithinATT(V (\ud835\udc59 ) \ud835\udc52,\u229e), (5) H(\ud835\udc59 )\ud835\udc52 = MAB(H (\ud835\udc59\u22121) \ud835\udc52 , V\u0303 (\ud835\udc59 ) \ud835\udc52 ) (6)\nThe notation \u229e denotes the addition of positional encodings and embeddings, after matching the dimension of the former with that of the latter through a learnable weight matrix,W \u2208 R\ud835\udc51\ud835\udc53 \u00d7\ud835\udc51\ud835\udc59\u22121 3.\nSimilarly, node embeddings are updated by aggregating (Eq. (9)) node-dependent hyperedge embeddings fromWithinATT (Eq. (8)) andWithinOrderPE (Eq. (7)). Specifically, a node \ud835\udc63 \u2019s embedding X(\ud835\udc59 )\ud835\udc63 is updated as follows:\nE(\ud835\udc59 )\ud835\udc63,\u229e = {H (\ud835\udc59 ) \ud835\udc52 \u229eWithinOrderPE(\ud835\udc63, \ud835\udc52)) : \ud835\udc52 \u2208 N\ud835\udc63}, (7)\nE\u0303(\ud835\udc59 )\ud835\udc63 = WithinATT(E (\ud835\udc59 ) \ud835\udc63,\u229e), (8) X(\ud835\udc59 )\ud835\udc63 = MAB(X (\ud835\udc59\u22121) \ud835\udc63 , E\u0303 (\ud835\udc59 ) \ud835\udc63 ) (9)\nOne can stack more than one WithinATT module before aggregation to make attention between nodes within each hyperedge even more edge-dependent. We stack two WithinATT modules in our experiments."
        },
        {
            "heading": "5.4 Application to Edge-Dependent Node",
            "text": "Classification\nGiven node and hyperedge embeddings, we predict edge-dependent node labels with a single-layer perceptron classifier, denoted as\ud835\udf13 , which takes the concatenation of node and hyperedge embeddings as its input. Let X(\ud835\udc3f) and H(\ud835\udc3f) be the node and hyperedge embeddings generated from the last \ud835\udc3f-th layer of WHATsNet respectively. Then, the label of a node \ud835\udc63 in a hyperedge \ud835\udc52 is predicted as follows:\n\ud835\udc66 (\ud835\udc63,\ud835\udc52 ) = arg max(\ud835\udf13 ( [X (\ud835\udc3f) \ud835\udc63 \u2225 H (\ud835\udc3f) \ud835\udc52 ])). (10)\nEq. (10) can readily be applied to other hypergraph neural networks, such as HNHN [17], that generate separate node and hyperedge embeddings as their outputs. Thus, any accuracy gain of WHATsNet with Eq. (10), compared to such models, is attributed to its ability to generate more informative node and hyperedge embeddings.\nAlternatively, one can utilize intermediate edge-dependent node embeddings V\u0303(\ud835\udc3f)\ud835\udc52 \u2208 R |\ud835\udc52 |\u00d7\ud835\udc51\ud835\udc3f in Eq. (5), which are generated inside aWHATsNet layer:\n\ud835\udc66 (\ud835\udc63,\ud835\udc52 ) = arg max(\ud835\udf13 (V\u0303 (\ud835\udc3f) \ud835\udc52 [\ud835\udc63])) (11)\n3Formally, A \u229e B = A + BW where A \u2208 R\ud835\udc5b\u00d7\ud835\udc51\ud835\udc34 , B \u2208 R\ud835\udc5b\u00d7\ud835\udc51\ud835\udc35 andW \u2208 R\ud835\udc51\ud835\udc35\u00d7\ud835\udc51\ud835\udc34 .\nTable 2: Summary of real-world hypergraphs with edge-dependent labels.\nDataset Num. ofNodes Num. of Hyperedges Max. of Node Deg. Max. of Hyperedge Size Sum of Hyperedge Size Num. of Class 0 Num. of Class 1 Num. of Class 2 Corr. w/ Centrality Avg. Entropy\nCoauth-DBLP 108,484 91,266 236 36 321,011 91,266 138,479 91,266 0.19 0.13 Coauth-AMiner 1,712,433 2,037,605 752 115 5,129,998 2,037,605 1,652,332 1,503,061 0.24 0.13 Email-Enron 21,251 101,124 18,168 948 1,186,521 635,268 450,129 101,124 0.10 0.28 Email-Eu 986 209,508 8,659 59 541,842 209,508 332,334 - 0.24 0.48 Stack-Biology 15,490 26,823 1,318 12 56,257 26,290 18,444 11,523 0.29 0.10 Stack-Physics 80,936 200,811 6,332 48 479,809 194,575 201,121 84,113 0.30 0.12\nwhere [\ud835\udc63] means indexing the vector corresponding to node \ud835\udc63 . In this way, one can avoid conducting the explicit concatenation with hyperedge embeddings, since V\u0303(\ud835\udc3f)\ud835\udc52 already has such information.\nTable 11 in Appendix B compares the two approaches of Eq. (10) and (11).4 There is no clear superiority between the two choices, and both of them consistently outperform the best competitor. Still, we choose the first approach as our default choice because it is more general and memory-efficient: we can store the node and hyperedge embeddings separately and then concatenate them for any downstream task utilizing edge-dependent node embeddings.5"
        },
        {
            "heading": "5.5 Complexity Analysis",
            "text": "We analyze the time and space complexities of WHATsNet. Time Complexity. WithinATT requires dot products between the input embeddings and inducing points, and the aggregation involves dot products between the input embeddings and a single query. Thus, the total time complexity of the inference step of WHATsNet without WithinOrderPE is \ud835\udc42 (\u2211\ud835\udc3f\n\ud835\udc59=1 ( \u2211 \ud835\udc52\u2208E ( |\ud835\udc52 |\ud835\udc5a\ud835\udc51\ud835\udc59 ) +\u2211\n\ud835\udc63\u2208V ( |\ud835\udc41\ud835\udc63 |\ud835\udc5a\ud835\udc51\ud835\udc59 ))) = \ud835\udc42 (\ud835\udc5a \u2211\ud835\udc3f \ud835\udc59=1 \ud835\udc51\ud835\udc59 \u2211 \ud835\udc52\u2208E |\ud835\udc52 |), where the last equal-\nity is from \u2211 \ud835\udc63\u2208V |N\ud835\udc63 | = \u2211 \ud835\udc52\u2208E |\ud835\udc52 |. As forWithinOrderPE, we first compute four node centralities of each node (spec., degree, coreness, eigenvector centrality, and PageRank), whose complexity is \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |) (see Appendix C for details). Next, we compute the order of nodes within hyperedges by sorting nodes based on each centrality value, whose time complexity is \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 | log |\ud835\udc52 |). Thus, the complexity of computingWithinOrderPE is\ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 | log |\ud835\udc52 |), and the total time complexity of WHATsNetwithWithinOrderPE is \ud835\udc42 (\ud835\udc5a\u2211\ud835\udc3f \ud835\udc59=1 \ud835\udc51\ud835\udc59 \u2211 \ud835\udc52\u2208E |\ud835\udc52 | + \u2211 \ud835\udc52\u2208E |\ud835\udc52 | log |\ud835\udc52 |). Space Complexity. We consider full-batch training. Storing the input hypergraph G = (V, E) and the \ud835\udc51\ud835\udc53 -dimensional positional encodings obtained byWithinOrderPE requires \ud835\udc42 (\ud835\udc51\ud835\udc53 \u2211 \ud835\udc52\u2208E |\ud835\udc52 |) space. We first consider the message passing from nodes to hyperedges at the \ud835\udc59-th layer. WithinATT requires (a) node embeddings, whose size\ud835\udc42 ( |\ud835\udc49 |\ud835\udc51\ud835\udc59 ), (b) a weightmatrix for adjusting the dimension of positional encodings, whose size is \ud835\udc42 (\ud835\udc51\ud835\udc53 \ud835\udc51\ud835\udc59 ), and (c) \ud835\udc5a inducing points of \ud835\udc51\ud835\udc59 dimension, whose size is \ud835\udc42 (\ud835\udc5a\ud835\udc51\ud835\udc59 ). It also requires two attention matrices between the inputs and\ud835\udc5a inducing points, whose sizes are |\ud835\udc52 | by\ud835\udc5a and\ud835\udc5a by |\ud835\udc52 | for each hyperedge \ud835\udc52 . Thus, the additional space required for the node-to-hyperedge message passing is \ud835\udc42 (\ud835\udc5a\ud835\udc51\ud835\udc59 + \ud835\udc51\ud835\udc53 \ud835\udc51\ud835\udc59 + |\ud835\udc49 |\ud835\udc51\ud835\udc59 +\ud835\udc5a \u2211 \ud835\udc52\u2208E |\ud835\udc52 |).\nSimilarly, the message passing from hyperedges to nodes at the \ud835\udc59-th layer requires \ud835\udc42 (\ud835\udc5a\ud835\udc51\ud835\udc59 + \ud835\udc51\ud835\udc53 \ud835\udc51\ud835\udc59 + |\ud835\udc38 |\ud835\udc51\ud835\udc59 + \ud835\udc5a \u2211 \ud835\udc63\u2208V |N\ud835\udc63 |). Since 4We remove WithinOrderPE from the last layer of WHATsNet when applying the approach of Eq. (11), because if not, the node centralities affect the classification performance too much. 5which is\ud835\udc42 (\ud835\udc41 +\ud835\udc40 ) instead of\ud835\udc42 (\ud835\udc41\ud835\udc40 ) when \ud835\udc41 nodes and\ud835\udc40 hyperedges exist.\n\u2211 \ud835\udc63\u2208V |N\ud835\udc63 | = \u2211 \ud835\udc52\u2208E |\ud835\udc52 |, the total space complexity of WHATsNet\nwith \ud835\udc3f layers is\ud835\udc42 ((\ud835\udc5a+\ud835\udc51\ud835\udc53 + |V|+ |E|) \u2211\ud835\udc3f \ud835\udc59=1 \ud835\udc51\ud835\udc59 + (\ud835\udc3f\ud835\udc5a+\ud835\udc51\ud835\udc53 ) \u2211 \ud835\udc52\u2208E |\ud835\udc52 |)."
        },
        {
            "heading": "5.6 Comparison to Existing Models",
            "text": "To efficiently capture the edge-dependent relationships between node pairs within hyperedges, we incorporate the encoder part of SetTransformer [37] as a component of the message passing in our model. It is important to note that SetTransformer is primarily designed to obtain proper representations of sets, and its primary focus is not on hypergraph representation learning.\nWhile both WHATsNet and SetTransformer aggregate input embeddings using a weighted sum, there are notable differences in the aggregation methods. SetTransformer uses weights obtained by attending to a global learnable parameter, whereas WHATsNet attends to the previous embedding of the target. Additionally,WHATsNet incorporates WithinOrderPE into attention, enhancing its ability to capture edge-dependent relationships within hyperedges.\nThere is another approach, called AllSetTransformer [12], which integrates SetTransformer into hypergraph neural networks. However, it should be noted that AllSetTransformer does not adopt the encoder part of SetTransformer but instead adopts its decoder with modifications to the number of stacking feed-forward layers in the multi-head attention block. Therefore, the usage of SetTransformer in AllSetTransformer differs from howWHATsNet utilizes it."
        },
        {
            "heading": "6 EXPERIMENTAL RESULTS",
            "text": "We evaluateWHATsNet by answering the following questions:\n\u2022 Q1. Does WHATsNet accurately predict the edge-dependent labels of nodes? \u2022 Q2. Does WHATsNet classify the same node differently depending on hyperedges? \u2022 Q3. Does each component of WHATsNet make a meaningful contribution to the performance? \u2022 Q4. How can we applyWHATsNet to real-world downstream tasks? DoesWHATsNet show usefulness in these tasks?"
        },
        {
            "heading": "6.1 Experimental settings",
            "text": "Datasets. We use six real-world hypergraphs from three domains:\n\u2022 Co-authorship (DBLP [54] and AMinerAuthor6): Each hyperedge indicates a publication, and the nodes in it are the authors of the publication. Edge-dependent node labels indicate the orders of authors (first, last, or others) in each publication. We exclude publications where the authors are in alphabetical order, which may not reflect the difference in the authors\u2019 contributions. 6https://www.aminer.org/aminernetwork\nTable 3: Results of Edge-Dependent Node Classification: Mean and standard deviation of Micro-F1 and Macro-F1 scores over\nfive independent runs are reported. Best scores are in bold. Note thatWHATsNet performs best in all datasets.\nDataset Metric BaselineU BaselineP HNHN HGNN HCHA HAT UniGCNII HNN HST AST WHATsNet\nCoauthDBLP MicroF1 0.333 \u00b1 0.001 0.346 \u00b1 0.001 0.486 \u00b1 0.004 0.540 \u00b1 0.004 0.451 \u00b1 0.007 0.503 \u00b1 0.004 0.497 \u00b1 0.003 0.488 \u00b1 0.006 0.564 \u00b1 0.004 0.495 \u00b1 0.038 0.605 \u00b1 0.002 MacroF1 0.330 \u00b1 0.001 0.332 \u00b1 0.001 0.478 \u00b1 0.008 0.519 \u00b1 0.002 0.334 \u00b1 0.048 0.483 \u00b1 0.006 0.476 \u00b1 0.002 0.482 \u00b1 0.006 0.549 \u00b1 0.003 0.487 \u00b1 0.040 0.595 \u00b1 0.002\nCoauthAMiner MicroF1 0.334 \u00b1 0.000 0.339 \u00b1 0.000 0.520 \u00b1 0.002 0.566 \u00b1 0.002 0.468 \u00b1 0.020 0.543 \u00b1 0.002 0.520 \u00b1 0.001 0.543 \u00b1 0.002 0.596 \u00b1 0.007 0.577 \u00b1 0.005 0.630 \u00b1 0.005 MacroF1 0.332 \u00b1 0.000 0.333 \u00b1 0.000 0.514 \u00b1 0.002 0.551 \u00b1 0.004 0.447 \u00b1 0.040 0.533 \u00b1 0.003 0.507 \u00b1 0.001 0.533 \u00b1 0.002 0.583 \u00b1 0.008 0.570 \u00b1 0.002 0.623 \u00b1 0.007\nEmailEnron MicroF1 0.334 \u00b1 0.001 0.439 \u00b1 0.001 0.738 \u00b1 0.028 0.725 \u00b1 0.004 0.666 \u00b1 0.010 0.817 \u00b1 0.001 0.734 \u00b1 0.010 0.763 \u00b1 0.003 0.779 \u00b1 0.067 0.796 \u00b1 0.014 0.826 \u00b1 0.001 MacroF1 0.300 \u00b1 0.001 0.333 \u00b1 0.001 0.637 \u00b1 0.023 0.674 \u00b1 0.003 0.464 \u00b1 0.002 0.753 \u00b1 0.004 0.656 \u00b1 0.010 0.679 \u00b1 0.007 0.681 \u00b1 0.123 0.719 \u00b1 0.020 0.760 \u00b1 0.004\nEmailEu MicroF1 0.500 \u00b1 0.001 0.525 \u00b1 0.001 0.643 \u00b1 0.004 0.633 \u00b1 0.001 0.620 \u00b1 0.000 0.669 \u00b1 0.001 0.630 \u00b1 0.005 OutOfMemory 0.671 \u00b1 0.001 0.666 \u00b1 0.005 0.671 \u00b1 0.000 MacroF1 0.493 \u00b1 0.001 0.499 \u00b1 0.001 0.552 \u00b1 0.014 0.533 \u00b1 0.008 0.497 \u00b1 0.001 0.638 \u00b1 0.002 0.565 \u00b1 0.013 OutOfMemory 0.640 \u00b1 0.002 0.624 \u00b1 0.021 0.646 \u00b1 0.003\nStackBiology MicroF1 0.335 \u00b1 0.000 0.368 \u00b1 0.001 0.640 \u00b1 0.005 0.689 \u00b1 0.002 0.589 \u00b1 0.007 0.661 \u00b1 0.005 0.610 \u00b1 0.004 0.618 \u00b1 0.015 0.694 \u00b1 0.002 0.571 \u00b1 0.054 0.742 \u00b1 0.003 MacroF1 0.326 \u00b1 0.000 0.334 \u00b1 0.003 0.592 \u00b1 0.006 0.624 \u00b1 0.007 0.465 \u00b1 0.060 0.606 \u00b1 0.005 0.433 \u00b1 0.007 0.568 \u00b1 0.013 0.631 \u00b1 0.006 0.446 \u00b1 0.081 0.686 \u00b1 0.004\nStackPhysics MicroF1 0.333 \u00b1 0.001 0.370 \u00b1 0.000 0.506 \u00b1 0.053 0.686 \u00b1 0.004 0.622 \u00b1 0.003 0.708 \u00b1 0.005 0.671 \u00b1 0.022 0.683 \u00b1 0.005 0.755 \u00b1 0.010 0.728 \u00b1 0.039 0.770 \u00b1 0.003 MacroF1 0.322 \u00b1 0.001 0.332 \u00b1 0.000 0.422 \u00b1 0.043 0.630 \u00b1 0.002 0.481 \u00b1 0.007 0.643 \u00b1 0.009 0.492 \u00b1 0.016 0.617 \u00b1 0.005 0.666 \u00b1 0.013 0.646 \u00b1 0.046 0.707 \u00b1 0.004\n\u2022 Email (Enron7 and Eu [48]): Each hyperedge represents an email, and the nodes in it represent the people involved in the email. Edge-dependent labels distinguish (1) the sender, (2) the receivers, and (3) the CC\u2019ed. \u2022 StackOverflow (Biology8 and Physics8): Each hyperedge represents a post, and the nodes in it are the users contributing to this post. Edge-dependent labels distinguish (1) the questioner, (2) answerers chosen by the questioner, and (3) the other answerers.\nTable 2 shows some statistics for the datasets. We measure the correlation coefficient between within-edge node-centrality orders and edge-dependent node labels, using the average of Cramer\u2019s coefficient matrix [15]. We also report the average entropy in each node\u2019s label distribution. Note that the average entropy is non-zero in all datasets, implying that node labels do vary across hyperedges. Competitors. We implemented all models using Deep Graph Library (DGL) [59]. In all competitors, edge-dependent labels are predicted by a single-layer perceptron from the concatenation of the global embeddings of nodes and hyperedges, as in ours (Eq. (10)). As competitors, we use seven hypergraph neural networks that generate both hyperedge and node embeddings and thus can be used for edge-dependent node classification: HNHN [17], HGNN [20], HCHA [5], HNN [3], HAT [27], UniGCNII [26], and AllSetTransformer (AST) [12]. We do not include HyperGCN [62] as a competitor because it does not provide hyperedge embeddings. In addition, we consider HypergraphSetTransformer (HST), which extends SetTransformer [37] to hypergraphs by applying it to obtain the representations of sets of incident nodes and hyperedges in the two stages of message passing. Lastly, we consider two simple approaches for comparison: (a) BaselineU, which predicts node labels uniformly at random, and (b) BaselineP, which randomly assigns labels proportionally to their global distribution. Other Settings. Since external features are absent, we create initial node features by adopting 2nd-order randomwalks on hypergraphs as in [67]. Specifically, we construct fixed-length random walks for each node and obtain embeddings using a skip-gram model [43, 44]. All parameters are initialized via Xavier initialization [24] and trained with Adam optimizer [30]. WithinOrderPE incorporates four centrality measures: degree, coreness, eigenvector centrality, and PageRank (refer to Appendix B.2). We randomly divide all hyperedges in each hypergraph into training (60%), validation (20%), 7https://www.cs.cmu.edu/ enron/ 8https://archive.org/download/stackexchange\nand test (20%) sets. For the edge-dependent node classification task, we stop training when the number of epochs reaches 100 or the accuracy on the validation set no longer improves for 25 epochs. Whereas, for downstream tasks, we stop training when the number of epochs reaches 300 or the mean of Micro-F1 and MacroF1 scores does not change for 10 epochs. We use the search space described in Appendix A for hyperparameter tuning. We select the hyperparameter values that maximize the mean of Micro-F1 and Macro-F1 scores on the validation set and report its performance on the test set over five independent runs."
        },
        {
            "heading": "6.2 Q1. Edge-Dependent Node Classification",
            "text": "To evaluate the performance of WHATsNet for the edge-dependent node classification, we measure the predictive performances using Micro-F1 and Macro-F1 scores on test data, averaged over five runs. Table 3 shows that WHATsNet consistently achieves the highest scores in terms of both Micro-F1 and Macro-F1. While HST performs well overall and ranks second, the statistical analysis using the Wilcoxon signed-rank test indicates thatWHATsNet is significantly better than HST with a p-value less than 0.05 in almost all cases (only except for Macro-F1 in Email-Enron and Micro-F1 in Email-Eu). Some other models, including HGNN and HAT, perform well on specific datasets but fail to achieve overall success. HNN runs out of memory on the email-Eu dataset due to the large size of the random-walk hyperedge transition matrix.9\nWe would like to emphasize that HST, AST, and WHATsNet utilize different aggregation methods, and relationships among nodes within each hyperedge (or among hyperedges incident to each node) are explicitly considered only in HST andWHATsNet. Additionally, positional encoding is used only inWHATsNet (see Section 5.6). Thus, the fact that AST underperformsHST andWHATsNet supports the effectiveness of WithinATT, and the fact that WHATsNet outperforms HST supports the usefulness of WithinOrderPE and our aggregation method.\nFigure 3 visually demonstrates the effectiveness of WHATsNet using the Coauthorship-DBLP dataset. The visualization showcases (a) the embeddings of hyperedges containing a specific node and (b) the concatenated embeddings of all incident pairs of nodes and hyperedges in the test set. Notably, the embeddings exhibit clear distinctions based on the edge-dependent labels. 9Implementation details of HNN can be found in [14].\nTable 4: Jensen-Shannon divergence (JSD) between Ground-truth and Predicted Node Label Distributions: The average and\nstandard deviation of the average JSD over five independent runs are reported. The lower JSD is, the better a model preserves node-level label distributions. Best scores are in bold.WHATsNet yields the lowest JSD in all datasets.\ndependent node labels predicted by WHATsNet (1) consistently outperforms using hypergraphs without such labels, (2) tends to outperform using edge-dependent node labels obtained by AST or HST (especially for clustering), and (3) performs comparably to and sometimes even better (especially for ranking aggregation) than using ground-truth labels.\n(a) Ranking Aggregation (Accuracy)\nMethod Halo H-Index\nRW [13] w/ Ground Truth 0.711 0.675\nRW [13] w/ WHATsNet 0.714 0.693 RW [13] w/ HST 0.707 0.695 RW [13] w/ AST 0.706 0.696\nRW [13] w/o Labels 0.532 0.654\n(b) Clustering (NMI: The higher, the better)\nMethod DBLP AMiner\nRDC-Spec [23] w/ GroundTruth 0.221 0.359\nRDC-Spec [23] w/WHATsNet 0.184 0.352 RDC-Spec [23] w/ HST 0.166 0.339 RDC-Spec [23] w/ AST 0.168 0.332\nRDC-Spec [23] w/o Labels 0.163 0.338\n(c) Product Return Prediction (F1)\nMethod Synthetic E-tail\nHyperGO [38] w/ GroundTruth 0.738\nHyperGO [38] w/WHATsNet 0.723 HyperGO [38] w/ HST 0.724 HyperGO [38] w/ AST 0.721\nHyperGO [38] w/o Labels 0.718\nTable 6: Ablation Study of WHATsNet: The performance of\nWHATsNet is largely improved by WithinATT and WithinOrderPE, as shown in the average rankings of models.\nDataset Metric w/o WithinATT w/oWithinOrderPE WHATsNet\nCoauthDBLP MicroF1 0.581 \u00b1 0.004 0.591 \u00b1 0.003 0.605 \u00b1 0.002 MacroF1 0.577 \u00b1 0.003 0.584 \u00b1 0.003 0.595 \u00b1 0.002\nCoauthAMiner MicroF1 0.604 \u00b1 0.010 0.583 \u00b1 0.095 0.630 \u00b1 0.005 MacroF1 0.592 \u00b1 0.013 0.536 \u00b1 0.174 0.623 \u00b1 0.007\nEmailEnron MicroF1 0.812 \u00b1 0.008 0.825 \u00b1 0.001 0.826 \u00b1 0.001 MacroF1 0.747 \u00b1 0.014 0.762 \u00b1 0.004 0.760 \u00b1 0.004\nEmailEu MicroF1 0.651 \u00b1 0.019 0.670 \u00b1 0.000 0.671 \u00b1 0.000 MacroF1 0.630 \u00b1 0.018 0.638 \u00b1 0.002 0.646 \u00b1 0.003\nStackBiology MicroF1 0.723 \u00b1 0.002 0.732 \u00b1 0.002 0.742 \u00b1 0.003 MacroF1 0.656 \u00b1 0.005 0.672 \u00b1 0.004 0.686 \u00b1 0.004\nStackPhysics MicroF1 0.752 \u00b1 0.005 0.765 \u00b1 0.002 0.770 \u00b1 0.003 MacroF1 0.675 \u00b1 0.010 0.688 \u00b1 0.008 0.707 \u00b1 0.004\nAVG. Ranking MicroF1 2.83 2.17 1.00 MacroF1 2.83 2.00 1.17"
        },
        {
            "heading": "6.3 Q2. Node Label Distribution Preservation",
            "text": "In the task of edge-dependent node classification, each node may have different labels for different edges. That is, each node has its own ground-truth label distribution (node-level label distribution), which describes how many times it has each label. In this experiment, we aim to check how well WHATsNet preserves such node-level label distributions compared to the other baseline approaches. To this end, we measure the Jensen-Shannon divergence (JSD) [40] between the ground-truth and the predicted node-level label distribution for each node in the test data. The average JSD over all nodes is used as a metric to evaluate how well the model captures the ground-truth node-level label distributions. As shown in Table 4, WHATsNet outperforms all other models, with the lowest JSD in all datasets. This result suggests that WHATsNet preserves well the ground-truth node-level label distributions.\n6.4 Q3. Ablation Study of WHATsNet For an ablation study, we compare the performance of WHATsNet with two variants of it in Table 6: (a)WHATsNet w/o WithinATT, whereWithinATT is removed and only the aggregation is used for message passing, and (b)WHATsNet w/oWithinOrderPE, which computes WithinATT without positional encodings. Importance of WithinATT. As shown in Table 6,WHATsNet consistently outperforms the variant without WithinATT. This result demonstrates the importance of WithinATT in achieving accurate edge-dependent node label classification. That is, considering relationships among nodes within each hyperedge is crucial for precise classification. In Appendix B.3, we also empirically confirm that the improvement is also contributed byWithinATT among hyperedges containing each node, by using a variant without it. Figure 4 visually illustrates the effectiveness of WithinATT by demonstrating WithinATT makes node embeddings better distinguished based on edge-dependent labels. Usefulness of WithinOrderPE.Table 6 also indicates thatWHATsNet also performs better than the variantwithoutWithinOrderPE in terms of both Micro F1 and Macro F1 scores across most datasets. This result supports thatWithinOrderPE contributes to the improvement in performance. However, in the Email-Enron dataset, WithinOrderPE does not have a significant impact, which aligns with the lowest correlation between node labels and node centrality orders in Table 2. In Appendix B.1, we show that WithinOrderPE outperforms several positional encoding schemes, including global node-centrality ranks."
        },
        {
            "heading": "6.5 Q4. Usefulness in Downstream Tasks",
            "text": "To evaluate the usefulness of WHATsNet, we conduct experiments on three downstream tasks: ranking aggregation, clustering, and product return prediction (see Section 4.2). We compare the performance of the tasks using three different types of inputs: hypergraphs with (a) ground-truth edge-dependent node labels, (b)\nThe embeddings of all publications of \u201cHans-Peter Kriegel\u201d visualized using LDA. (b) The concatenated embeddings of all incident pairs of nodes and hyperedges in the test set, visualized in the same manner. Note that they are clearly clustered based on the edge-dependent labels.\nlabels predicted by hypergraph neural networks (spec., WHATsNet, HST, and AST) trained for our problem (i.e., edge-dependent node classification problem), and (c) no labels, respectively. We aim to demonstrate that even imperfect edge-dependent node labels predicted by the trained model (especially, by WHATsNet) lead to better performances, compared to those without labels. Refer to [14] for details of the edge-dependent node labels used in each task. Ranking Aggregation. This task aims to correctly predict the global node ranks using local node ranks in subsets (hyperedges). We use a recent random-walk-based method [13], where a random walker selects a node in each hyperedge proportionally to its rank within it, and the global node ranking is determined by the stationary distribution. We use two real-world datasets: (1) the Halo 2 game dataset, where scores (edge-dependent node labels) of players (nodes) in matches (hyperedges) of up to 8 players are given, and the global rankings of players need to be inferred; (2) the AMiner dataset, where the order of authorship (i.e., edge-dependent labels) in each publication is given to predict the rankings of all authors in terms of their H-index. For evaluation, we measure the accuracy in identifying the node with a higher rank for each node pair. Clustering. From two co-authorship datasets, (1) DBLP and (2) AMiner, we group publications by venues as their ground-truth clusters. We use RDC-Spec [23], a well-performing method that uses spectral clustering and weighs authors differently according to their order (edge-dependent label) in each publication, as the backbone hypergraph clustering algorithm. For evaluation, we measure Normalized mutual information (NMI). Product Return Prediction. We use a state-of-the-art method, HyperGo [38], as the predictor for the returned product. It utilizes the information about the count of products in each basket, which corresponds to edge-dependent labels. We evaluate by F1 score of the predicted return probability of products in each target basket. Results. As shown in Table 5, across all the examined applications, the utilization of imperfect edge-dependent node labels predicted by WHATsNet consistently leads to performance improvements compared to the results obtained without labels. Furthermore, these enhancements generally surpass those achieved by HST and AST. Surprisingly, in ranking aggregation, the performance is even higher than what is achieved by using ground-truth labels. We would like to emphasize thatWHATsNet can be applied to any algorithm that\nembeddings in StackOverflow-Biology (a) before and (b) after theWithinATTmodule are visualized using LDA.WithinATT makes node embeddings better distinguished based on edge-dependent labels, represented by different colors.\nutilizes edge-dependent labels, andWHATsNet is not specialized to any of the methods considered for the tasks. The three application tasks are described in greater detail in [14]."
        },
        {
            "heading": "7 CONCLUSIONS",
            "text": "In this work, we propose the edge-dependent node classification problem, which is a new benchmark task for hypergraph neural networks with various real-life applications. In order to tackle this problem, we devise WHATsNet, a hypergraph neural network for considering edge-dependent relationships between node pairs within each hyperedge. In our experiments with 6 real-world hypergraphs, WHATsNet consistently outperformed 10 competitors. Our contributions are summarized as follows:\n\u2022 NewProblem: We formulate the edge-dependent node classification problem with six real-world datasets and three applications. \u2022 Effective Model: We propose WHATsNet, a novel hypergraph neural network equipped with the WithinATT module and WithinOrderPE. They are designed to capture edge-dependent relationships between node pairs within each hyperedge. \u2022 Extensive Experiment: We empirically show the advantage of WHATsNet over 10 competitors and the effectiveness of each component of it. We also demonstrated the usefulness of WHATsNet in three applications.\nFor reproducibility, we make the code and data available at [14]. Acknowledgements.Thisworkwas supported byNational Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF-2020R1C1C1008296) and Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00157, Robust, Fair, Extensible Data-Centric Continual Learning) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST))."
        },
        {
            "heading": "A PARAMETER SETTING",
            "text": "The search spaces of hyperparameters are given in Table 7. For all models, we fix the hidden dimension to 64, the final embedding dimension to 128, the number of the inducing points to 4, the number of attention layers to 2, and the dropout ratio to 0.7.\nWe employ sampling for efficiency in hyperedge-to-node message passing (but not in node-to-hyperedge message passing) and set the size of sampling (i.e. a set of hyperedges sampled among those incidents to each node) as follows: (a) All methods do not use sampling in Coauthorship-DBLP, (b) HST, AST, andWHATsNet sample 40 hyperedges in the other datasets, and (c) HNHN, HGNN, HCHA,HAT, andUniGCNII do not sample in StackOverflow-Biology but sample 40 or 100 hyperedges in the other datasets.\nExceptionally, for HCHA [5] and HNN [3], we use full-batch training without sampling. For a fair comparison, we tune their hyperparameters in a larger search space: {0.005, 0, 01, 0.03, 0.05, 0.1} for learning rates, {1, 2} for the number of layers, and {0.3, 0.5, 0.7} for dropout ratios, while we fix the dimension of final node and hyperedge embeddings to 128, the hidden dimension to 64, and the number of epochs to 300 with early stopping."
        },
        {
            "heading": "B ADDITIONAL ABLATION STUDIES B.1 Positional Encoding",
            "text": "We examine the usefulness of WithinOrderPE by replacing it with alternative positional encoding schemes for graph neural networks (see Section 2.2). In our study, we initialize node features using random walks (see Section 6.1) so that they capture global positional information. Thus, we compare WithinOrderPE with two relative positional encodings: Shaw et al. [53] and GraphIT [42], using diffusion kernels [31] (DK) and the p-step random-walk kernel [42] (PRWK) for calculating distances between nodes. We also consider learnable positional encodings called LSPE [19], which are added to node embeddings (as WithinOrderPE is added) and updated together with other parameters. In addition, we consider WholeOrderPE, a variant of WithinOrderPE that uses the global centrality order among all nodes, instead of the relative order within each hyperedge. Since the relative positional encoding methods pose challenges when used with inducing points, we do not employ inducing points consistently, and forWHATsNet, we consider two versions with and without inducing points. The base model for comparing positional encoding schemes is defined as follows:\n?\u0303? (\ud835\udc59 ) \ud835\udc52 = \ud835\udc40\ud835\udc34\ud835\udc35(\ud835\udc49 (\ud835\udc59\u22121) \ud835\udc52 ,\ud835\udc49 (\ud835\udc59\u22121) \ud835\udc52 ; \ud835\udc43\ud835\udc38) (12)\n\ud835\udc3b (\ud835\udc59 ) \ud835\udc52 = MAB(\ud835\udc3b (\ud835\udc59\u22121) \ud835\udc52 , ?\u0303? (\ud835\udc59 ) \ud835\udc52 ) (13)\n\ud835\udc4b (\ud835\udc59 ) \ud835\udc63 = MAB(\ud835\udc4b (\ud835\udc59\u22121) \ud835\udc63 ,WithinATT(\ud835\udc38 (\ud835\udc59 ) \ud835\udc63 )) (14)\nOnly Eq. (12) depends on positional encoding schemes \ud835\udc43\ud835\udc38. To specifically investigate the impact of node positional encoding schemes, we do not employ any positional encodings for hyperedges.\nThe results are shown in Table 8, whereWithinOrderPE outperforms all other positional encoding methods, particularly surpassing WholeOrderPE. This highlights the effectiveness of incorporating relative order within hyperedges for positional encodings. It is worth noting that WHATsNet with inducing points, which offers improved efficiency, achieves competitive performance compared toWHATsNet without inducing points.\nB.2 Node-centrality measures We investigate the impact of node-centrality measures used in WithinOrderPE on the performance of WHATsNet. We consider individually the following seven node-centrality measures: \u2022 Degree: The number of hyperedges a node belongs to. \u2022 Coreness [10, 28, 45]: The maximum \ud835\udc58 such that a node belongs to the \ud835\udc58-core which is the maximal sub-hypergraph where every node has at least degree \ud835\udc58 within it. \u2022 Eigenvector Centrality [9] and PageRank [47]: The eigenvector centrality and the PageRank score on a clique-expanded graph where each edge weight\ud835\udc64\ud835\udc62,\ud835\udc63 = |{\ud835\udc52 \u2208 E : \ud835\udc63 \u2208 \ud835\udc52 and \ud835\udc62 \u2208 \ud835\udc52}|. \u2022 H-Coreness [56]: Hypergraph core-periphery scores determining the proximity of nodes to the hypergraph core. \u2022 H-Eigenvector [55]: Node and hyperedge centralities on hypergraphs using the max centrality model. Different positional encodings are used for different message-passing directions. \u2022 Vector Centrality [32]: A vectorial measure of the roles of each node at different orders of interactions. The dimension of this measure is one less than the maximum hyperedge size.\nInWHATsNet, we use only four centrality measures (spec., degree, coreness, eigenvector, and PageRank) for computational efficiency (see Appendix C for details). We additionally considerWHATsNetall, which uses all seven centrality measures together. Table 9 shows that the performance of WHATsNet is not highly sensitive to the choice of node centrality measures, and especially it consistently outperforms HST (the strongest baseline approach) regardless of the choice of centrality measures. Moreover, the results indicate that increasing the dimensionality of WithinOrderPE tends to lead to performance improvement, which is evident from the high performance of vector centrality,WHATsNet, andWHATsNet-all, which utilize higher-dimensional WithinOrderPE.\nB.3 Advantage of usingWithinATT for both directions\nRecall that we devise the WithinATT with the intention of capturing edge-dependent relationships between nodes within each hyperedge. However, we also utilize this module for the propagation from hyperedges to nodes. To validate the effectiveness of usingWithinATT in both directions, we replace the message passing from hyperedges to nodes with the aggregation methods used in HNHN (WHATsNet + HNHN) and HAT (WHATsNet + HAT).\nTable 10 demonstrates that WHATsNet outperforms both WHATsNet + HNHN and WHATsNet + HAT, providing justification for employing the WithinATT module in both directions. It is also worth noting thatWHATsNet + HNHN andWHATsNet + HAT outperform their corresponding vanilla models, HNHN and HAT.\nB.4 The number of inducing points We explore how the performance of WHATsNet changes with respect to the numbers of inducing points (2, 4, and 8). Table 10 shows that the performance improves as more inducing points are employed. However, considering memory constraints, using 4 inducing points is favorable as it requires half the memory while achieving comparable performance to using 8 inducing points.\nB.5 Inputs to the final classifier As discussed in Section 5.4, we consider two possible inputs for a single-layer perceptron classifier: (a) intermediate edge-dependent node embeddings, referred to as WHATsNet-IM, which are generated byWithinATT inside aWHATsNet (Eq. (11)) and (b) the concatenation of final output node and hyperedge embeddings from\nWHATsNet (Eq. (10)). Table 11 indicates that one approach does not consistently outperform the other;WHATsNet-IM performs better in Coauth-AMiner, Email-Enron, and Email-Eu datasets, while WHATsNet performs better in the remaining three datasets. It is also worth noting that both approaches are superior to HST, which is overall the best competitor."
        },
        {
            "heading": "C TIME COMPLEXITY OF NODE CENTRALITY MEASURES",
            "text": "We analyze the time complexity of computing each node\u2019s centrality using each of the four node measures used for WithinOrderPE in WHATsNet. We assume \ud835\udc42 ( |V| + |E|) \u2208 \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |) for a hypergraph G = (V, E) for simplicity. Degree. The time complexity is \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |) if we increment the degree of each of |\ud835\udc52 | members of each hyperedge \ud835\udc52 \u2208 E. Coreness. The time complexity is \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |), as proven in Theorem 1 of [10]. Eigenvector centrality. The time complexity is \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |). If we let \ud835\udc3c be the |V| by |E | incidence matrix of G and \ud835\udc5b\ud835\udc5b\ud835\udc67 (\ud835\udc3c ) be the number of non-zero entries (i.e., \ud835\udc5b\ud835\udc5b\ud835\udc67 (\ud835\udc3c ) = \u2211\ud835\udc52\u2208E |\ud835\udc52 |), the eigenvector centralities of all nodes in the clique-expanded graph, whose adjacency matrix is \ud835\udc34 = \ud835\udc3c \ud835\udc3c\ud835\udc47 , are equivalent to the leading left singular vector of \ud835\udc3c . This can be computed by Power Iteration in \ud835\udc42 (\ud835\udc5b\ud835\udc5b\ud835\udc67 (\ud835\udc3c )\ud835\udc47 ) time, where \ud835\udc47 is the maximum number of iterations. In practice (and in our setting), \ud835\udc47 is set to a constant, and thus the time complexity is \ud835\udc42 (\ud835\udc5b\ud835\udc5b\ud835\udc67 (\ud835\udc3c )) = \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |), which is even lower than that of materializing the clique-expanded graph. PageRank. The time complexity is also \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |) since its calculation is almost the same as that of the eigenvector centrality. We repeat (at most\ud835\udc47 times) computing r\u2190 \ud835\udefd\ud835\udc3c\ud835\udc3c\ud835\udc47 r + (1\u2212 \ud835\udefd)1 for a |V|dimensional vector r and a constant \ud835\udefd . Note that computing \ud835\udc3c \ud835\udc3c\ud835\udc47 r can be done in\ud835\udc42 (\ud835\udc5b\ud835\udc5b\ud835\udc67 (\ud835\udc3c )) time by two steps: (1) computing r\u2032 \u2190 \ud835\udc3c\ud835\udc47 r and (2) computing \ud835\udc3cr\u2032. Since \ud835\udc47 is set to a constant in practice (and in our setting), the time complexity is \ud835\udc42 (\ud835\udc5b\ud835\udc5b\ud835\udc67 (\ud835\udc3c )) = \ud835\udc42 (\u2211\ud835\udc52\u2208E |\ud835\udc52 |)."
        }
    ],
    "title": "Classification of Edge-dependent Labels of Nodes in Hypergraphs",
    "year": 2023
}