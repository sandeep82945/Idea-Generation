{
    "abstractText": "In Koopman operator theory, a finite-dimensional nonlinear system is transformed into an infinite but linear system using a set of observable functions. However, manually selecting observable functions that span the invariant subspace of the Koopman operator based on prior knowledge is inefficient and challenging, particularly when little or no information is available about the underlying systems. Furthermore, current methodologies tend to disregard the importance of the invertibility of observable functions, which leads to inaccurate results. To address these challenges, we propose the so-called FlowDMD, a Flow-based Dynamic Mode Decomposition that utilizes the Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages the intrinsically invertible characteristics of the CF-INN to learn the invariant subspaces of the Koopman operator and accurately reconstruct state variables. Numerical experiments demonstrate the superior performance of our algorithm compared to state-of-the-art methodologies.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuhuang Meng"
        },
        {
            "affiliations": [],
            "name": "Jianguo Huang"
        },
        {
            "affiliations": [],
            "name": "Yue Qiub"
        }
    ],
    "id": "SP:bd3b38c62774ad44d8433baf7e896e95d4501bc4",
    "references": [
        {
            "authors": [
                "S.L. Brunton",
                "J.L. Proctor",
                "J.N. Kutz"
            ],
            "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "venue": "Proceedings of the National Academy of Sciences 113 ",
            "year": 2016
        },
        {
            "authors": [
                "Z. Long",
                "Y. Lu",
                "X. Ma",
                "B. Dong"
            ],
            "title": "PDE-Net: Learning PDEs from data",
            "venue": "in: Proceedings of the 35th International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "M. Raissi"
            ],
            "title": "Deep hidden physics models: Deep learning of nonlinear partial differential equations",
            "venue": "Journal of Machine Learning Research 19 ",
            "year": 2018
        },
        {
            "authors": [
                "R. Fuentes",
                "R. Nayek",
                "P. Gardner",
                "N. Dervilis",
                "T. Rogers",
                "K. Worden",
                "E. Cross"
            ],
            "title": "Equation discovery for nonlinear dynamical systems: A Bayesian viewpoint",
            "venue": "Mechanical Systems and Signal Processing 154 ",
            "year": 2021
        },
        {
            "authors": [
                "S. Kim",
                "P.Y. Lu",
                "S. Mukherjee",
                "M. Gilbert",
                "L. Jing",
                "V. \u010ceperi\u0107",
                "M. Solja\u010di\u0107"
            ],
            "title": "Integration of neural network-based symbolic regression in deep learning for scientific discovery",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems 32 ",
            "year": 2021
        },
        {
            "authors": [
                "B.O. Koopman"
            ],
            "title": "Hamiltonian systems and transformation in Hilbert space",
            "venue": "Proceedings of the National Academy of Sciences 17 ",
            "year": 1931
        },
        {
            "authors": [
                "P.J. Schmid"
            ],
            "title": "Dynamic mode decomposition and its variants",
            "venue": "Annual Review of Fluid Mechanics 54 ",
            "year": 2022
        },
        {
            "authors": [
                "J.H. Tu",
                "C.W. Rowley",
                "D.M. Luchtenburg",
                "S.L. Brunton",
                "J.N. Kutz"
            ],
            "title": "On dynamic mode decomposition: Theory and applications",
            "venue": "Journal of Computational Dynamics 1 ",
            "year": 2014
        },
        {
            "authors": [
                "M.R. Jovanovi\u0107",
                "P.J. Schmid",
                "J.W. Nichols"
            ],
            "title": "Sparsity-promoting dynamic mode decomposition",
            "venue": "Physics of Fluids 26 ",
            "year": 2014
        },
        {
            "authors": [
                "N. Takeishi",
                "Y. Kawahara",
                "Y. Tabei",
                "T. Yairi"
            ],
            "title": "Bayesian dynamic mode decomposition",
            "venue": "in: Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence",
            "year": 2017
        },
        {
            "authors": [
                "H. Arbabi",
                "I. Mezic"
            ],
            "title": "Ergodic theory",
            "venue": "dynamic mode decomposition, and computation of spectral properties of the Koopman operator, SIAM Journal on Applied Dynamical Systems 16 ",
            "year": 2017
        },
        {
            "authors": [
                "S. Le Clainche",
                "J.M. Vega"
            ],
            "title": "Higher order dynamic mode decomposition",
            "venue": "SIAM Journal on Applied Dynamical Systems 16 ",
            "year": 2017
        },
        {
            "authors": [
                "N.B. Erichson",
                "L. Mathelin",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Randomized dynamic mode decomposition",
            "venue": "SIAM Journal on Applied Dynamical Systems 18 ",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhang",
                "C.W. Rowley",
                "E.A. Deem",
                "L.N. Cattafesta"
            ],
            "title": "Online dynamic mode decomposition for time-varying systems",
            "venue": "SIAM Journal on Applied Dynamical Systems 18 ",
            "year": 2019
        },
        {
            "authors": [
                "M.J. Colbrook",
                "L.J. Ayton",
                "M. Sz\u0151ke"
            ],
            "title": "Residual dynamic mode decomposition: robust and verified Koopmanism",
            "venue": "Journal of Fluid Mechanics 955 ",
            "year": 2023
        },
        {
            "authors": [
                "J.N. Kutz",
                "S.L. Brunton",
                "B.W. Brunton",
                "J.L. Proctor"
            ],
            "title": "Dynamic mode decomposition: Data-driven modeling of complex systems",
            "venue": "SIAM",
            "year": 2016
        },
        {
            "authors": [
                "M.O. Williams",
                "I.G. Kevrekidis",
                "C.W. Rowley"
            ],
            "title": "A data\u2013driven approximation of the Koopman operator: Extending dynamic mode decomposition",
            "venue": "Journal of Nonlinear Science 25 ",
            "year": 2015
        },
        {
            "authors": [
                "M.O. Williams",
                "C.W. Rowley",
                "I.G. Kevrekidis"
            ],
            "title": "A kernel-based method for data-driven Koopman spectral analysis",
            "venue": "Journal of Computational Dynamics 2 ",
            "year": 2015
        },
        {
            "authors": [
                "S.E. Otto",
                "C.W. Rowley"
            ],
            "title": "Linearly recurrent autoencoder networks for learning dynamics",
            "venue": "SIAM Journal on Applied Dynamical Systems 18 ",
            "year": 2019
        },
        {
            "authors": [
                "Q. Li",
                "F. Dietrich",
                "E.M. Bollt",
                "I.G. Kevrekidis"
            ],
            "title": "Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the Koopman operator",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science 27 ",
            "year": 2017
        },
        {
            "authors": [
                "E. Yeung",
                "S. Kundu",
                "N. Hodas"
            ],
            "title": "Learning deep neural network representations for Koopman operators of nonlinear dynamical systems",
            "venue": "in: American Control Conference, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "N. Takeishi",
                "Y. Kawahara",
                "T. Yairi"
            ],
            "title": "Learning Koopman invariant subspaces for dynamic mode decomposition",
            "venue": "in: Advances in Neural Information Processing Systems, volume 30",
            "year": 2017
        },
        {
            "authors": [
                "B. Lusch",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Deep learning for universal linear embeddings of nonlinear dynamics",
            "venue": "Nature Communications 9 ",
            "year": 2018
        },
        {
            "authors": [
                "O. Azencot",
                "N.B. Erichson",
                "V. Lin",
                "M. Mahoney"
            ],
            "title": "Forecasting sequential data using consistent Koopman autoencoders",
            "venue": "in: International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "S. Pan",
                "K. Duraisamy"
            ],
            "title": "Physics-informed probabilistic learning of linear embeddings of nonlinear dynamics with guaranteed stability",
            "venue": "SIAM Journal on Applied Dynamical Systems 19 ",
            "year": 2020
        },
        {
            "authors": [
                "M. Li",
                "L. Jiang"
            ],
            "title": "Deep learning nonlinear multiscale dynamic problems using Koopman operator",
            "venue": "Journal of Computational Physics 446 ",
            "year": 2021
        },
        {
            "authors": [
                "S.T. Dawson",
                "M.S. Hemati",
                "M.O. Williams",
                "C.W. Rowley"
            ],
            "title": "Characterizing and correcting for the effect of sensor noise in the dynamic mode decomposition",
            "venue": "Experiments in Fluids 57 ",
            "year": 2016
        },
        {
            "authors": [
                "R. Wang",
                "Y. Dong",
                "S.O. Arik",
                "R. Yu"
            ],
            "title": "Koopman neural operator forecaster for time-series with temporal distributional shifts",
            "venue": "in: The Eleventh International Conference on Learning Representations",
            "year": 2023
        },
        {
            "authors": [
                "D.J. Alford-Lago",
                "C.W. Curtis",
                "A.T. Ihler",
                "O. Issan"
            ],
            "title": "Deep learning enhanced dynamic mode decomposition",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science 32 ",
            "year": 2022
        },
        {
            "authors": [
                "P. Bevanda",
                "J. Kirmayr",
                "S. Sosnowski",
                "S. Hirche"
            ],
            "title": "Learning the Koopman eigendecomposition: A diffeomorphic approach",
            "venue": "in: American Control Conference, IEEE",
            "year": 2022
        },
        {
            "authors": [
                "H. Lu",
                "D.M. Tartakovsky"
            ],
            "title": "Prediction accuracy of dynamic mode decomposition",
            "venue": "SIAM Journal on Scientific Computing 42 ",
            "year": 2020
        },
        {
            "authors": [
                "G. Papamakarios",
                "E. Nalisnick",
                "D.J. Rezende",
                "S. Mohamed",
                "B. Lakshminarayanan"
            ],
            "title": "Normalizing flows for probabilistic modeling and inference",
            "venue": "Journal of Machine Learning Research 22 ",
            "year": 2021
        },
        {
            "authors": [
                "I. Kobyzev",
                "S.J. Prince",
                "M.A. Brubaker"
            ],
            "title": "Normalizing flows: An introduction and review of current methods",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 43 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Dinh",
                "D. Krueger",
                "Y. Bengio"
            ],
            "title": "Nice: Non-linear independent components estimation",
            "venue": "arXiv preprint arXiv:1410.8516 ",
            "year": 2014
        },
        {
            "authors": [
                "L. Dinh",
                "J. Sohl-Dickstein",
                "S. Bengio"
            ],
            "title": "Density estimation using real NVP",
            "venue": "in: International Conference on Learning Representations",
            "year": 2017
        },
        {
            "authors": [
                "D.P. Kingma",
                "P. Dhariwal"
            ],
            "title": "Glow: Generative flow with invertible 1x1 convolutions",
            "venue": "in: Advances in Neural Information Processing Systems, volume 31",
            "year": 2018
        },
        {
            "authors": [
                "A.N. Gomez",
                "M. Ren",
                "R. Urtasun",
                "R.B. Grosse"
            ],
            "title": "The reversible residual network: Backpropagation without storing activations",
            "venue": "in: Advances in Neural Information Processing Systems, volume 30",
            "year": 2017
        },
        {
            "authors": [
                "A. Logg",
                "G.N. Wells",
                "J. Hake"
            ],
            "title": "Dolfin: A C++/Python finite element library",
            "venue": "in: Automated Solution of Differential Equations by the Finite Element Method: The FEniCS Book, Springer",
            "year": 2012
        },
        {
            "authors": [
                "N. Demo",
                "M. Tezzele",
                "G. Rozza"
            ],
            "title": "Pydmd: Python dynamic mode decomposition",
            "venue": "Journal of Open Source Software 3 ",
            "year": 2018
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein"
            ],
            "title": "L",
            "venue": "Antiga, et al., Pytorch: An imperative style, highperformance deep learning library, in: Advances in Neural Information Processing Systems, volume 32",
            "year": 2019
        },
        {
            "authors": [
                "X. Glorot",
                "Y. Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "in: Proceedings of the thirteenth International Conference on Artificial Intelligence and Statistics",
            "year": 2010
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "in: International Conference on Learning Representations",
            "year": 2015
        },
        {
            "authors": [
                "S. Ruder"
            ],
            "title": "An overview of gradient descent optimization algorithms",
            "venue": "CoRR abs/1609.04747 ",
            "year": 2016
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational physics 378 ",
            "year": 2019
        },
        {
            "authors": [
                "S. Wang",
                "X. Yu",
                "P. Perdikaris"
            ],
            "title": "When and why PINNs fail to train: A neural tangent kernel perspective",
            "venue": "Journal of Computational Physics 449 ",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "In Koopman operator theory, a finite-dimensional nonlinear system is transformed into an infinite but linear system using a set of observable functions. However, manually selecting observable functions that span the invariant subspace of the Koopman operator based on prior knowledge is inefficient and challenging, particularly when little or no information is available about the underlying systems. Furthermore, current methodologies tend to disregard the importance of the invertibility of observable functions, which leads to inaccurate results. To address these challenges, we propose the so-called FlowDMD, a Flow-based Dynamic Mode Decomposition that utilizes the Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages the intrinsically invertible characteristics of the CF-INN to learn the invariant subspaces of the Koopman operator and accurately reconstruct state variables. Numerical experiments demonstrate the superior performance of our algorithm compared to state-of-the-art methodologies.\nKeywords: Koopman operator, Generative models, Invertible neural networks\n\u22c6This work is partially supported by the National Natural Science Foundation of China (NSFC) under grant number 12101407.\n\u2217Corresponding author. Email addresses: mengyh@shanghaitech.edu.cn (Yuhuang Meng),\nhuangjg@shanghaitech.edu.cn (Jianguo Huang), qiuyue@cqu.edu.cn (Yue Qiu) 1Yuhuang Meng and Jianguo Huang contributed equally to this paper.\nPreprint submitted to ArXiv July 3, 2023\nar X\niv :2\n30 6.\n17 39\n6v 1\n[ m\nat h.\nN A\n] 3\n0 Ju"
        },
        {
            "heading": "1. Introduction",
            "text": "Nonlinear dynamic systems are widely prevalent in both theory and engineering applications. Since the governing equations are generally unknown in many situations, it can be challenging to study the systems directly based on the first principles. Fortunately, the data about the systems of interest could be available by experiments or observations. Instead, one could seek to understand the behavior of the nonlinear system through the data-driven approaches [1, 2, 3, 4, 5].\nThe Koopman operator [6], which embeds the nonlinear system of interest into an infinite dimensional linear space by observable functions has attracted lots of attention. The Koopman operator acts on the infinite dimensional Hilbert space and aims to capture the full representations of the nonlinear systems. Dynamic mode decomposition (DMD) calculates the spectral decomposition of the Koopman operator numerically by extracting dynamic information from the collected data. Concretely, DMD devises a procedure to extract the spectral information directly from a data sequence without an explicit formulation of the Koopman operator, which is efficient for handling high dimensional data [7]. Variants of DMD are proposed to address challenges in different scenarios [8, 9, 10, 11, 12, 13, 14, 15].\nThe selection of observable functions plays an essential role in the DMD algorithm. Exact DMD [8] exploits the identity mapping as the observables. This implies that one uses a linear system to approximate a nonlinear system with given data [16]. This would yield inaccurate or even completely mistaken outcomes. Furthermore, the short-term prediction of Exact DMD might be acceptable for some cases, but the long-term prediction is probably unreliable. Typically, prior knowledge is required to select the observable functions that span the invariant subspace of the Koopman operator. However, the invariant subspace is not simply available. In order to overcome the limitations of the Exact DMD algorithm and capture the full feature of the nonlinear system, several data-driven selection strategies for observable functions have been proposed. Extended DMD (EDMD) [17] lifts the state variables from the original space into a higher dimensional space using the dictionary functions. The accuracy and rate of convergence of EDMD depend on the choice of the dictionary functions. Therefore, EDMD needs as many dictionary functions as possible. This implies that the set of dictionary functions (nonlinear transformations) should be sufficiently complex, which results in enormous computational cost. Kernel based DMD (KDMD) [18] differs from EDMD in that it utilizes the kernel trick to exploit the implicit expression of dictionary functions, whereas EDMD uses the explicit expression of dictionary functions. Nonetheless, both EDMD and KDMD are prone to overfitting [19], which leads to large generalization error. How to efficiently choose the observable functions that span the invariant subspace of the Koopman operator\nbecomes a significant challenge. In contrast to EDMD and KDMD, observable functions can be represented by neural networks. Dictionary learning [20] couples the EDMD with a set of trainable dictionary functions, where dictionary functions are represented by a fully connected neural network and an untrainable component. Fixing the partial dictionary function facilitates the reconstruction of the state variables, however, this setting implicitly assumes that linear term lies in the invariant subspace of the Koopman operator. Yeung et al. [21] select low-dimensional dictionary functions more efficiently using deep neural networks.\nAutoencoder (AE) neural networks have been widely applied to learn the optimal observable functions and reconstruction functions in Koopman embedding [19, 22, 23, 24, 25, 26]. Concretely, the invariant subspace of the Koopman operator and reconstruction functions are represented by the encoder and decoder network in AE, respectively. Lusch et al. [23] utilize neural networks to identify the Koopman eigenfunctions and introduced an auxiliary network to cope with the dynamic systems with continuous spectrum. Azencot et al. [24] propose the Consistent Koopman AE model that combines the forward-backward DMD method [27] with the AE model. This approach extracts the latent representation of high-dimensional nonlinear data and eliminates the effect of noise in the data simultaneously. Pan and Duraisamy [25] parameterize the structure of the transition matrix in linear space and construct an AE model to learn the residual of the DMD. Li and Jiang [26] utilize deep learning and the Koopman operator to model the nonlinear multiscale dynamical problems, where coarse-scale data is used to learn the fine-scale information through a set of multiscale basis functions. Wang et al. [28] propose Koopman Neural Forecaster (KNF) combining AE with Koopman operator theory to predict the data with distributional shifts.\nRepresenting Koopman embedding by dictionary learning or AE networks has several drawbacks. Firstly, the reconstruction in dictionary learning partially fixes the dictionary functions, which leads to a low level of interpretability of the model. Secondly, the encoder and decoder in an AE model are trained simultaneously, but neither of them is invertible, cf. [29] for more details. Moreover, due to the structural noninvertibility of the encoder and decoder, it typically requires a large amount of training data in order to obtain accurate representations, which makes the AE model prone to overfitting. Alford-Lago et al. [29] analyze the property of both the encoder and decoder in AE and proposed the deep learning dynamic mode decomposition (DLDMD). Bevanda et al. [30] constructed a conjugate map between the nonlinear system and its Jacobian linearization, which is learned by a diffeomorphic neural network.\nIn this paper, we develop a novel architecture that incorporates physical knowledge to learn the Koopman embedding. Specifically, we apply the coupling flow invertible neural networks (CF-INN) to learn the observable functions and reconstruction functions. The invertibility of the learned observable functions makes our method more flexible than dictionary learning or AE learning. Our contributions are three-folds:\n1. We utilize an structurally invertible mapping to reconstruct state variables, which increases the interpretability of the neural network and alleviates the overfitting of AE.\n2. The difficulty of learning the observable functions and observable functions is reduced by exploiting their structural invertibility of neural networks. Therefore, the reconstruction error in the loss function could be eliminated.\n3. As the physical information is embedded into the model, the number of parameters is reduced to achieve comparable accuracy with other methods. Additionally, the parameters to be optimized are reduced dramatically since the learned mappings and their inverse share the same parameters.\nThis paper is organized as follows. In Section 2, we briefly review the Koopman operator theory and DMD. In Section 3, we present the structure of CF-INN and introduce how to learn the invariant subspace of the Koopman operator and the reconstruction functions. In Section 4, several numerical experiments are performed to demonstrate the performance of our method, and we summarize our work in Section 5."
        },
        {
            "heading": "2. Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1. Koopman operator theory",
            "text": "Consider the nonlinear autonomous system in discrete form,\nxk+1 = f(xk), xk \u2208 M \u2282 Rm, (1)\nwhere M represents the set of state space, f : M \u2192 M is an unknown nonlinear map, and k is the time index.\nDefinition 1 (Koopman operator [16]). For the nonlinear system (1), the Koopman operator K is an infinite-dimensional linear operator that acts on all observable functions g : M \u2192 C such that\nKg(x) = g(f(x)).\nHere, g(x) \u2208 H and H represents the infinite dimensional Hilbert space.\nThrough the observable functions, the nonlinear system (1) could be transformed into an infinite-dimensional linear system using the Koopman operator,\ng(xk+1) = g(f(xk)) = Kg(xk). (2)\nNote that the Koopman operator is linear, i.e., K(\u03b11g1(x)+\u03b12g2(x)) = \u03b11g1(f(x))+ \u03b12g2(f(x)), with g1(x), g2(x) \u2208 H and \u03b11, \u03b12 \u2208 R. As K is an infinite-dimensional operator, we denote its eigenfunctions and eigenvalues by {\u03bbi, \u03c6i(x)}\u221ei=0 such that K\u03c6i(x) = \u03bbi\u03c6i(x), where \u03c6i(x) : M \u2192 R, \u03bbi \u2208 C.\nThe Koopman eigenfunctions define a set of intrinsic measurement coordinates, then a vector-valued observable function g(x) = [g1(x), \u00b7 \u00b7 \u00b7 , gn(x)]T could be written in terms of the Koopman eigenfunctions,\ng(xk) = g1(xk)... gn(xk)  = \u221e\u2211 i=1 \u03c6i(xk) < \u03c6i, g1 >... < \u03c6i, gn >  = \u221e\u2211 i=1 \u03c6i(xk)vi, (3)\nwhere vi refers to the i-th Koopman mode with respect to the Koopman eigenfunction \u03c6i(x). Combining (2) and (3), we have the decomposition of a vector-valued observable functions\ng(xk+1) = Kg(xk) = K \u221e\u2211 i=1 \u03c6i(xk)vi = \u221e\u2211 i=1 \u03bbi\u03c6i(xk)vi.\nFurthermore, the decomposition could be rewritten as\ng(xk) = \u221e\u2211 i=1 \u03bbki\u03c6i(x0)vi.\nIn practice, we need a finite-dimensional representation of the infinite-dimensional Koopman operator. Denote the n-dimensional invariant subspace of the Koopman operator K by Hg, i.e., \u2200g(x) \u2208 Hg,Kg(x) \u2208 Hg. Let {gi(x)}ni=1 be one set of basis of Hg, this induces a finite-dimensional linear operator K [16], which projects the Koopman operator K onto Hg, i.e., for the n-dimensional vector-valued observable functions g(x) = [g1(x), \u00b7 \u00b7 \u00b7 , gn(x)]T , we have\ng(xk+1) = g1(xk+1)... gn(xk+1)  = Kg1(xk)... Kgn(xk)  = K g1(xk)... gn(xk)  = Kg(xk) (4)"
        },
        {
            "heading": "2.2. Dynamic mode decomposition",
            "text": "DMD approximates the spectral decomposition of the Koopman operator numerically. Given the state variables {x0,x1, \u00b7 \u00b7 \u00b7 ,xp} and a vector-valued observable function g(x) = [g1(x), \u00b7 \u00b7 \u00b7 , gn(x)]T , then we get the sequence {g(x0),g(x1), \u00b7 \u00b7 \u00b7 ,g(xp)}, where each g(xk) \u2208 Rn is the observable snapshot of the k-th time step. According to (4), we have\ng(xk+1) = Kg(xk),\nwhere K \u2208 Rn\u00d7n is the matrix form of the finite-dimensional operator. For the two data matrices, X = [g(x0), \u00b7 \u00b7 \u00b7 ,g(xp\u22121)] and Y = [g(x1), \u00b7 \u00b7 \u00b7 ,g(xp)], where X and Y are both in Rn\u00d7p, which satisfies Y = KX. Therefore, K can be represented by\nK = YX\u2020,\nwhere X\u2020 denotes the Moore-Penrose inverse of X. The Exact DMD algorithm developed by Tu et al. [8] computes dominant eigenpairs (eigenvalue and eigenvector) of K without the explicit formulation of K. In Algorithm 1, we present the DMD algorithm on the observable space, which is a general form of the Exact DMD algorithm. When using the identical mapping as the observable functions, i.e., g(x) = x, Algorithm 1 is identical to the Exact DMD algorithm.\nAlgorithm 1 DMD on observable space [16, 31]\n1. Compute the (reduced) SVD ofX, X = Ur\u03a3rV \u2217 r, whereUr \u2208 Cn\u00d7r, \u03a3r \u2208 Rr\u00d7r, Vr \u2208 Cp\u00d7r. 2. Compute K\u0303 = U\u2217rYVr\u03a3 \u22121 r . 3. Compute the eigen-pairs of K\u0303: K\u0303W = W\u039b. 4. Reconstruct the eigen-pairs of K, where eigenvalues of K are diagonal entries of \u039b, the corresponding eigenvectors of K(DMD modes) are columns of \u03a6 = YVr\u03a3 \u22121 r W. 5. Approximate the observation data via DMD, g\u0302(xk) = \u03a6\u039b kb, where b = \u03a6\u2020g(x0). 6. Reconstruct the state variables x\u0302k = g \u22121(g\u0302(xk)) = g \u22121 (\u03a6\u039bkb)."
        },
        {
            "heading": "2.3. State reconstruction",
            "text": "Koopman operator theory utilizes observable functions g to transform the nonlinear system (1) into a linear system while preserving the nonlinearity. Evolving the nonlinear system (1) is computationally expensive or even impossible when f is\nunknown, whereas evolving through the Koopman operator (2) offers a promising and computationally efficient approach.\nFigure 1 illustrates the relation between the nonlinear evolution f and the Koopman operator evolution where the system evolves linearly in the observation space H. By computing the Koopman eigenvalues and modes, we can make predictions of the observable functions g(x). We could reconstruct the state x by the inverse of the observable functions g\u22121(x) provided that g(x) is invertible. The invertibility of observable functions is essential to ensure the reconstruction accuracy and the interpretability of the outcomes.\nTypical observable functions g(x) selection are performed manually based on prior knowledge. Exact DMD takes the identical mapping, while the EDMD utilizes a set of pre-defined functions such as polynomials, Fourier modes, radial basis functions, and so forth [17]. However, these methods can be inaccurate and inefficient for Koopman embeddings learning. Deep neural networks, as efficient global nonlinear approximators, could be applied to represent the observable function g(x) and the reconstruction function g\u22121(x). Several studies have demonstrated that the encoder and decoder networks in AE correspond to g(x) and g\u22121(x), respectively [19, 22, 23, 24, 25, 26].\nIn practical applications, it is not always guaranteed that g(x) is invertible. In the learning Koopman embedding via AE, the invertibility of g(x) is enforced through numerical constraints, i.e., the reconstruction error \u2225x \u2212 g\u22121(g(x))\u222522, which tends to result in overfitting and suboptimal performance [29]. Besides, the reconstruction error is trained simultaneously with the prediction error and the linearity error [23]. The weights assigned to each loss term are hyperparameters that can be challenging to tune. In this paper, we propose a structurally invertible mapping learning framework, which eliminates the need for the reconstruction term in the loss function and yields more robust and accurate results. We present the details of our method in Section 3."
        },
        {
            "heading": "3. Learning Koopman embedding by invertible neural networks",
            "text": "In this section, we first briefly review the AE neural network and demonstrate the limitation of this class of neural networks in the Koopman embedding learning. Then, we introduce our method to overcome this limitation."
        },
        {
            "heading": "3.1. Drawback of AE in the Koopman embedding learning",
            "text": "Most of the work use the Autoencoder (AE) neural networks as the backbone to learn the invariant subspace of the Koopman operator and reconstruct the state variables. AE as the frequently-used unsupervised learning structure of neural networks, consists of two parts, i.e., the encoder E and the decoder D. AE learns these two mappings (functions) E and D by optimizing\nmin E,D\nEx\u223cm(x)[loss(x,D \u25e6 E(x))]. (5)\nHere m(x) denotes the distribution of the input data, loss(x, y) describes the difference between x and y, and E(\u00b7) represents the expectation.\nDefinition 2. Let f1 : S \u2192 S \u2032 be an arbitrary mapping, and it is said to be invertible if there exists a mapping f2 : S \u2032 \u2192 S such that\nf1 \u25e6 f2 = I, f2 \u25e6 f1 = I,\nwhere I is the identity mapping. Then, f2 is said to be the inverse mapping of f1.\nLet E and D be two mappings learned by AE such that D \u25e6 E \u2248 I. However, the reverse order of the mapping E \u25e6 D is not always a good approximation to the identity mapping, moreover, E and D are generally not invertible [29]. The main reason is that while AE strives to reach D\u25e6E \u2248 I, it omits the additional constraint E \u25e6D \u2248 I which requires the latent variable data to train. Unfortunately, the latent variables are not accessible, thus rendering it impossible for AE to satisfy E \u25e6D \u2248 I and D \u25e6 E \u2248 I simultaneously.\nAE learns an identity mapping I from a training data set S, i.e., for any x \u2208 S,D\u25e6E(x) \u2248 x. For data out of the set S, the mapping learned by AE may perform badly. In other words, AE may have poor generalization capability. Next, we use a preliminary experiment to demonstrate this limitation. The details of this numerical example are given in Section 4.1. We use the structure of AE defined in [26] and randomly generate 120 trajectories to train the AE, and the results are depicted by Figure 2.\nFigure 2 compares the input data points out of the distribution of the training data with the corresponding reconstructed data points using the trained AE model. Figure 2(a) shows the density distribution of training data set S , which provides a rough illustration of the data space S. For the reconstruction test of AE, we generate three types of data, i.e., the sin-shaped scatters, the S-shaped scatters, and scatters from the standard 2-d normal distribution. We plot the corresponding input points (blue) and reconstructed data points (red) of the AE. The results shown in the next three subfigures illustrate that AE can reconstruct the input data points nearby the training data set S very well. But for the data points far away from S, AE performs badly. The same situation happens in learning the Koopman embedding. Specifically, in the training process of AE, one aims to find the Koopman invariant space by minimizing the error of the Koopman embedding learning and the reconstruction error. However, minimizing the error between latent variables and their corresponding reconstruction denoted by loss(x, E \u25e6 D(x)) is intractable. This result is in poor stability and generalization capability."
        },
        {
            "heading": "3.2. Structure of CF-INN",
            "text": "We have shown that the mapping learned by AE performs poorly, which inspires us that invertibility can greatly reduce computational complexity and yields better\ngeneralization capability. Next, we introduce an invertible neural network to overcome the drawback of AE. Let g\u03b8(x) : X \u2192 Y denote the input-output mapping of the invertible neural network, where \u03b8 represents the parameters of the neural network. Let f\u03b8 be the inverse mapping of g\u03b8 which shares the same parameters with g\u03b8. Then we can reconstruct x in the backward direction by f\u03b8(y) : Y \u2192 X. In generative tasks of machine learning, the forward generating direction is called the flow direction and the backward direction is called the normalizing direction. Next, we introduce the concept of coupling flows, which belongs to the invertible neural networks.\nDefinition 3 (Coupling flow [32]). Let m \u2208 N and m \u2265 2, for a vector z \u2208 Rm and 2 \u2264 q \u2264 m\u2212 1, we define zup as the vector (z1, . . . , zq)\u22a4 \u2208 Rq and zlow as the vector (zq+1, . . . , zm)\n\u22a4 \u2208 Rm\u2212q. A coupling flow (CF) , denoted by hq,\u03c4 , has the following form,\nhq,\u03c4 (zup, zlow) = (zup, \u03c4(zlow, \u03c3(zup))),\nwhere \u03c3 : Rq \u2192 Rl, and \u03c4(\u00b7, \u03c3(y)) : Rm\u2212q \u00d7 Rl \u2192 Rm\u2212q is a bijection mapping for any y \u2208 Rq.\nA coupling flow defined in Definition 3 is invertible if and only if \u03c4 is invertible and its inverse h\u22121q,\u03c4 (zup, zlow) = (zup, \u03c4\n\u22121(zlow, \u03c3(zup))) [33]. The key point of making the CF invertible is the invertibility of \u03c4 . One of the mostly used CF is the affine coupling function (ACF) [34, 35, 36], where \u03c4 is an invertible element-wise function.\nDefinition 4 (Affine coupling function [33]). Define an affine coupling function by the mapping \u03a8q,s,t from Rq \u00d7 Rm\u2212q to Rm such that\n\u03a8q,s,t(zup, zlow) = (zup, (zlow + t(zup))\u2299 s(zup)), (6)\nwhere \u2299 is the Hadamard product, s, t : Rq \u2192 Rm\u2212q are two arbitrary vector-valued mappings.\nDefinition 4 defines the forward direction of computations, and the backward direction of computations is given by \u03a8\u22121q,s,t(zup, zlow) = (zup, zlow \u2298 s(zup) \u2212 t(zup)), where \u2298 denotes the element-wise division of vectors. The mappings s and t in Definition 4 can be any nonlinear functions, neural networks such as fully-connected neural network (FNN) are typically used to parameterize t and s.\nLet \u03a81, . . . ,\u03a8L be a sequence of L affine coupling functions and define g\u03b8 = \u03a8L \u25e6 \u03a8L\u22121 \u25e6 \u00b7 \u00b7 \u00b7\u03a81, where \u03b8 represents the parameters of {\u03a8i}Li=1. The resulted vector-valued function g\u03b8 is an invertible neural network and called by coupling flow invertible neural network (CF-INN) in this paper. Moreover, for any \u03a8i, the division\nindex q of the input vector x is user-guided. In this paper, we set q = \u2308m/2\u2309, where \u2308\u00b7\u2309 is the rounding function. Furthermore, in order to mix the information sufficiently, we can flip the ACF by using the form \u03a8\u0304q,s,t(zup, zlow) = ((zup+t(zlow))\u2299s(zlow), zlow). We plot the computation process of an ACF and a flipped ACF in Figure 3, where the network structure diagram left shows the forward direction and the network structure diagram right shows the backward direction. The red area is an ACF block and consists of a standard ACF and a flipped ACF, which is a CF-INN of depth 2.\nWhen the depth (L) of a CF-INN is large, its training becomes challenging. The main curse is that the dividend term s is too small in \u03a8 in the backward direction computations. This can be solved by replacing the affine coupling functions with residual coupling functions. Similar idea has also been applied in the residual term of ResNet.\nDefinition 5 (Residual coupling functions [37]). Define a residual affine coupling function (RCF) by the map \u03a8q,s,t from Rq \u00d7 Rm\u2212q to Rm such that\n\u03a8q,t(zup, zlow) = (zup, zlow + t(zup)),\nwhere t : Rq \u2192 Rm\u2212q is a nonlinear mapping.\nRCFs are simplifications of ACFs and when we connect a RCF with a flipped RCF, we obtain a RCF block, which is a simplified ACF block in Figure 3."
        },
        {
            "heading": "3.3. Loss function for Koopman embedding",
            "text": "In this paper, we use the CF-INN to learn the Koopman invariant subspace and the reconstructions simultaneously, where the forward direction of CF-INN is represented by g\u03b8 and its backward direction is represented by f\u03b8. The observable\nfunctions evolve linearly in the Koopman invariant subspace. Hence, the linearity constrained loss function that represents the DMD approximation error is given by\nLlinear = T\u2211 t=1 ||g\u03b8(xt)\u2212 \u03a6\u039bt\u03a6\u2020g\u03b8(x0)||2 = T\u2211 t=1 ||g\u03b8(xt)\u2212 g\u0302\u03b8(xt)||2,\nwhere g\u0302\u03b8(xt) = \u03a6\u039b t\u03a6\u2020g\u03b8(x0) is the DMD approximation of the observable functions {g(xt)}Tt=1 by using Algorithm 1. To reconstruct the states xt, the inverse mapping of g, i.e, f\u03b8 corresponds to the backward direction of CF-INN. f\u03b8 shares the same network structure and parameters with g\u03b8. Therefore, the computational cost is greatly reduced, compared with AE that another neural network is required to parameterize the inverse mapping of g\u03b8. The reconstruction loss due to the DMD approximation error is given by\nLrec = T\u2211 t=1 ||xt \u2212 f\u03b8(g\u0302\u03b8(xt))||2.\nThe optimal parameters \u03b8\u2217 is given by\n\u03b8\u2217 = argmin \u03b8 Llinear + \u03b1Lrec,\nwhere \u03b1 is a user-guard hyperparameter.\nCompared with other Koopman embedding learning frameworks, the loss function in our approach is much more simplified. We summarize our CF-INN framework for the Koopman embedding learning in Figure 4 and our method is called FlowDMD since this framework uses a flow model based Dynamic Model Decomposition to compute the finite dimensional Koopman operator approximation and reconstruct system states."
        },
        {
            "heading": "4. Numerical experiments",
            "text": "In this section, we use three numerical examples to demonstrate the efficiency of our method for learning the Koopman embedding and compare its performance with LIR-DMD [26] and Exact DMD. We use the Python library FEniCS [38] to compute the numerical solutions of PDEs, the Python library PyDMD [39] to complete the calculations of Exact DMD, and the Python library PyTroch [40] to train the neural networks. Besides, the Xavier normal initialization scheme [41] is utilized to initialize the weights of all neural networks, while the biases of all nodes are set to zero. All the networks are trained by the Adam optimizer [42] with an initial learning rate of 10\u22123. In order to find the optimal parameters of the network, we use ReduceLROnPlateau [43] to adjust the learning rate during the training process for all numerical examples. For fairness, all the methods share the same training strategies. Denote x as the \u201ctrue\u201d value of the states and x\u0302 as its reconstruction. We use three metrics to evaluate different methods synthetically., i.e., the relative L2 error\nRL2E(t) = ||x\u0302t \u2212 xt||2\n||xt||2 ,\nthe mean squared error (MSE),\nMSE(t) = ||x\u0302t \u2212 xt||22\nm ,\nand the total relative L2 error\nTRL2E = \u221a\u2211T t=1 ||x\u0302t \u2212 xt||22\u2211T\ni=1 ||xt||22 ."
        },
        {
            "heading": "4.1. Fixed-point attractor",
            "text": "The fixed-point attractor example [23] is given by{ xt+1,1 = \u03bbxt,1,\nxt+1,2 = \u00b5xt,2 + (\u03bb 2 \u2212 \u00b5)x2t,1.\nThe initial state is chosen randomly by x0,1 \u223c U(0.2, 4.2), x0,2 \u223c U(0.2, 4.2) and \u03bb = 0.9, \u00b5 = 0.5. We divide the data set into three parts where the ratio of training, validation, and test is 60%, 20%, and 20%, respectively. The number of neurons of each layer for the encoder network in LIR-DMD is 2, 10, 10, 3 and the number of neurons of decoder network is 3, 10, 10, 2. This results in 345 trainable parameters for\nLIR-DMD. We use three ACFs for this problem. The mappings t and s are parameterized by FNN with three layers and the width of each layer is 1,8,2, respectively. This results in 102 trainable parameters in total.\nWe randomly choose one example from the test set and plot its results in Figure 5. Both Figure 5(a) and Figure 5(b) show that the reconstruction calculated by LIRDMD and FlowDMD are better than that by the Exact DMD and the difference of trajectories between LIR-DMD and FlowDMD is very small. Figure 5(c) and Figure 5(d) illustrate that the reconstruction error of FlowDMD is the smallest. In the first 30 time steps, LIR-DMD has a similar error to FlowDMD. The error of FlowDMD increases much more slowly than that of LIR-DMD for the following 30 time steps. We conclude that FlowDMD has better generalization ability than LIR-DMD.\nWe test FlowDMD, LIR-DMD and Exact DMD using 40 randomly generated examples and the results are depicted by Figure 6. We use the total relative L2 error to evaluate the reconstruction results of trajectories. For FlowDMD, the reconstruction error is the lowest among almost all of the test examples, and the average total relative L2 error is only 0.3%. Compared with LIR-DMD, FlowDMD has better generalization ability and learning ability of the Koopman invariant subspace."
        },
        {
            "heading": "4.2. Burgers\u2019 equation",
            "text": "The 1-D Burgers\u2019 equation [44] is given by \u2202u \u2202t + u \u2202u \u2202x = 0.01 \u03c0 \u22022u \u2202x2 x \u2208 (\u22121, 1), t \u2208 (0, 1],\nu(1, t) = u(\u22121, t) = 0, u(x, 0) = \u2212\u03be \u2217 sin(\u03c0x),\n(7)\nwhere \u03be is a random variable that satisfies a uniform distribution U(0.2, 1.2). We use the finite element method with 30 equidistant grid points for the spatial discretization and the implicit Euler method with a step size of 0.01 for temporal discretization. We generate 100 samples of \u03be for the initial state and compute the corresponding solutions. The examples are then divided into three parts, with proportions 60% for training, 20% for validation, and 20% for test. We test the performance of the Exact DMD, LIR-DMD, and FlowDMD. The rank of Exact DMD is 3 and the same rank is also used in LIR-DMD and FlowDMD to embed the Koopman linearity. The structure of the encoder network for LIR-DMD is [30, 40, 50, 40], and the decoder network is [40, 50, 40, 30] where the numbers in the brackets represent the width of each layer and we use RCFs to replace ACFs. This results in an invertible neural network of depth of 3 with one RCF block and one RCF. In each RCF, the width of each layer in FNN to parameterize the mapping t is 15, 40, 15, which results in 7530 parameters in FlowDMD, whereas LIR-DMD has 10650 parameters.\nFigure 7 depicts that FlowDMD has the smallest absolute reconstruction error and total relative reconstruction error. Figure 8(a) and Figure 8(b) show that the reconstruction error of Exact DMD and LIR-DMD increase with time, but FlowDMD maintains in a very low level. Figure 9 summarizes the TRL2E of reconstruction on all test examples and depicts that the FlowDMD has the smallest error on almost all test examples, where the average TRL2E of FlowDMD is 1.5%. For some test examples, Exact DMD has the same TRL2E with FlowDMD, but for most test\nexamples, FlowDMD performs better than Exact DMD. The TRL2E of LIR-DMD are bigger than FlowDMD over all the test examples and are slightly better than Exact DMD for some test examples."
        },
        {
            "heading": "4.3. Allen-Cahn equation",
            "text": "The 1-D Allen-Cahn equation [44] is given by \u2202u \u2202t \u2212 \u03b31 \u22022u \u2202x2 + \u03b32 ( u3 \u2212 u ) = 0, x \u2208 (\u22121, 1), t \u2208 (0, 1],\nu(0, x) = \u03be \u2217 x2 cos(2\u03c0x), u(t,\u22121) = u(t, 1),\n(8)\nwhere \u03b31 = 0.0001, \u03b32 = 5, and \u03be \u223c N (\u22120.1, 0.04). We use the finite element method with 20 equidistant grid points for the spatial discretization and the implicit Euler with a step size of 0.02 for the temporal discretization. Furthermore, we generate 100 samples of \u03be and use FEniCS to compute the numerical solutions. The data set is segmented according to a ratio of 60%, 20%, 20%, respectively to be used as\nthe training set, the validation set, and the test set. The structure of the encoder network for LIR-DMD is [20, 30, 40, 30] and the decoder network is [30, 40, 30, 20], where the numbers in the bracket indicate the width of each layer. This results in 6190 parameters for LIR-DMD. For FlowDMD, we also use RCFs to replace the ACFs. The neural network for FlowDMD consists of one RCF block and one RCF, which results in a network with depth L = 3. In each RCF, the width of each layer of the FNN to parameterize t is 10, 20, 10. Finally, we obtain 2580 parameters for FlowDMD. The rank of Exact DMD is 3, and the same rank is also used in LIR-DMD and FlowDMD to embed the Koopman linearity.\nFigure 10 clearly shows that FlowDMD can reconstruct the original state most accurately. It reveals that the absolute error of both exact DMD and LIR-DMD increase over time, but FlowDMD can maintain the error in a low level all the time. Numerical results show that FlowDMD is more robust and generalizes better than Exact DMD and LIR-DMD. The error of the state reconstruction for three methods are given in Figure 11. At the beginning time, FlowDMD has the biggest relative error because the norm of the true state variables is too small, which leads to a\nlarge relative error. As time evolves, the error of FlowDMD reaches the lowest level among all three methods. In Figure 12, we use the test data set to evaluate the generalization ability. The FlowDMD has almost the smallest total relative L2 error in most examples and the average of the total relative L2 error is 9%. It also shows that the fluctuation of error for FlowDMD is smaller than that of LIR-DMD, which demonstrates that FlowDMD has a better generalization ability and is more robust than LIR-DMD."
        },
        {
            "heading": "4.4. Sensitivity study",
            "text": "Here, we study the sensitivity of FlowDMD systematically with respect to the following four aspects:\n1. The neural network initialization.\n2. The hyperparameter \u03b1 in the loss function.\n3. The structure of neural networks.\n4. The rank r used by DMD in Algorithm 1.\nWe study the sensitivity of FlowDMD using the Allen-Cahn equation in Section 4.3."
        },
        {
            "heading": "4.4.1. Sensitivity with respect to the neural network initialization",
            "text": "In order to quantify the sensitivity of FlowDMD with respect to the initialization, we consider the same data set with Section 4.3. Simultaneously, we fix the structure for FlowDMD to include only one RCF block and one RCF. Each RCF has a FNN to parameterize t where the width of each layer is 10, 20, 10. Moreover, all FNNs use the rectified linear unit (ReLU) as activation functions. We use 15 random seeds to initialize models and train all the models with the same setting. In Figure 13, we report the total relative L2 error between the reconstructed states and the\u201ctrue\u201d states. Evidently, the TRL2E remains stable for different initializations of neural networks, as demonstrated by the consistent results obtained within the following interval,\n[\u00b5TRL2E\u2212\u03c3TRL2E, \u00b5TRL2E+\u03c3TRL2E] = [6.5\u00d710\u22122\u22121.6\u00d710\u22122, 6.5\u00d710\u22122+1.6\u00d710\u22122]"
        },
        {
            "heading": "4.4.2. Sensitivity with respect to \u03b1",
            "text": "We utilize the same training set with Section 4.3 and select \u03b1 from the list [0.01, 0.1, 1, 10, 100]. As shown in Table 1, the different weights \u03b1 in the loss function\nhave little influence on the final results. We observe that the error is minimized when \u03b1 = 10, which suggests the use of an adaptive weight selection algorithm. The gradient flow provided by the neural tangent kernel [45] can be employed to adjust the weight \u03b1 and accelerate the training process, and we leave this for our future work."
        },
        {
            "heading": "4.4.3. Sensitivity with respect to the structure of neural networks",
            "text": "We study the impact of the number of RCFs and the number of neurons in the FNN to parameterize the mapping t on the performance of the FlowDMD. Specifically, the sensitivity of FlowDMD is being quantified with respect to two parameters: the number of RCFs (Nf ) and the number of neurons (Nn) in the middle layer of the FNN. Here, the FNN used to parameterize t is restricted to a three layer structure of [10, Nn, 10]. The results are summarized in Table 2, which indicate that the reconstruction of FlowDMD has little to do with its structure while adding more neurons or more RCFs will not improve the final results to a big extent."
        },
        {
            "heading": "4.4.4. Sensitivity with respect to the rank of DMD",
            "text": "As we increase the rank r used for the DMD computations in Algorithm 1, we include more physical information, but the computation time also increases. In this study, we investigate how the DMD rank affects the model and its reconstruction. The results in Table 3 show that as we increase the rank r, the corresponding error decreases rapidly."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose a coupling flow invertible neural network approach to learn both the observable functions and reconstruction functions for the Koopman operator learning. Our method generate more accurate Koopman embedding model and better approximations of the Koopman operator than state-of-the-art methods. Our FlowDMD is structurally invertible, which simplifies the loss function and improves the accuracy of the state reconstruction. Numerical experiments show that our approach is more accurate, efficient, and interpretable than the state-of-the-art methods."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors would like to thank Mengnan Li and Lijian Jiang for sharing their code."
        }
    ],
    "title": "Physics-informed invertible neural network for the Koopman operator learning ",
    "year": 2023
}