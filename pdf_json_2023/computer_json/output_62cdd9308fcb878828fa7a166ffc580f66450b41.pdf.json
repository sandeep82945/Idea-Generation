{
    "abstractText": "Molecular conformer generation (MCG) is an important task in cheminformatics and drug discovery. The ability to efficiently generate low-energy 3D structures can avoid expensive quantum mechanical simulations, leading to accelerated virtual screenings and enhanced structural exploration. Several generative models have been developed for MCG, but many struggle to consistently produce high-quality conformers. To address these issues, we introduce CoarsenConf, which coarse-grains molecular graphs based on torsional angles and integrates them into an SE(3)-equivariant hierarchical variational autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained atomic coordinates of subgraphs connected via rotatable bonds, creating a variable-length coarse-grained latent representation. Our model uses a novel aggregated attention mechanism to restore fine-grained coordinates from the coarse-grained latent representation, enabling efficient generation of accurate conformers. Furthermore, we evaluate the chemical and biochemical quality of our generated conformers on multiple downstream applications, including property prediction and oracle-based protein docking. Overall, CoarsenConf generates more accurate conformer ensembles compared to prior generative models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Danny Reidenbach"
        },
        {
            "affiliations": [],
            "name": "Aditi S. Krishnapriyan"
        }
    ],
    "id": "SP:b90763406812ce4532a2060d237b2d9425065894",
    "references": [
        {
            "authors": [
                "Sereina Riniker",
                "Gregory A. Landrum"
            ],
            "title": "Better informed distance geometry: Using what we know to improve conformation generation",
            "venue": "Journal of Chemical Information and Modeling, 55(12):2562\u20132574,",
            "year": 2015
        },
        {
            "authors": [
                "Benson Chen",
                "Xiang Fu",
                "Regina Barzilay",
                "Tommi S. Jaakkola"
            ],
            "title": "Fragment-based sequential translation for molecular optimization",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Brooke E. Husic",
                "Nicholas E. Charron",
                "Dominik Lemm",
                "Jiang Wang",
                "Adri\u00e0 P\u00e9rez",
                "Maciej Majewski",
                "Andreas Kr\u00e4mer",
                "Yaoyi Chen",
                "Simon Olsson",
                "Gianni de Fabritiis",
                "Frank No\u00e9",
                "Cecilia Clementi"
            ],
            "title": "Coarse graining molecular dynamics with graph neural networks",
            "venue": "The Journal of Chemical Physics, 153(19),",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Kmiecik",
                "Dominik Gront",
                "Michal Kolinski",
                "Lukasz Wieteska",
                "Aleksandra Elzbieta Dawid",
                "Andrzej Kolinski"
            ],
            "title": "Coarse-grained protein models and their applications",
            "venue": "Chemical Reviews, 116(14):7898\u20137936,",
            "year": 2016
        },
        {
            "authors": [
                "Soojung Yang",
                "Rafael Gomez-Bombarelli"
            ],
            "title": "Chemically transferable generative backmapping of coarse-grained proteins",
            "venue": "ArXiv, abs/2303.01569,",
            "year": 2023
        },
        {
            "authors": [
                "Wujie Wang",
                "Minkai Xu",
                "Chen Cai",
                "Benjamin Kurt Miller",
                "Tess E. Smidt",
                "Yusu Wang",
                "Jian Tang",
                "Rafael Gomez-Bombarelli"
            ],
            "title": "Generative coarse-graining of molecular conformations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Axelrod",
                "Rafael G\u00f3mez-Bombarelli"
            ],
            "title": "Geom, energy-annotated molecular conformations for property prediction and molecular generation",
            "venue": "Scientific Data,",
            "year": 2022
        },
        {
            "authors": [
                "Kexin Huang",
                "Tianfan Fu",
                "Wenhao Gao",
                "Yue Zhao",
                "Yusuf Roohani",
                "Jure Leskovec",
                "Connor W Coley",
                "Cao Xiao",
                "Jimeng Sun",
                "Marinka Zitnik"
            ],
            "title": "Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development",
            "venue": "Proceedings of Neural Information Processing Systems, NeurIPS Datasets and Benchmarks,",
            "year": 2021
        },
        {
            "authors": [
                "Gregor Simm",
                "Jose Miguel Hernandez-Lobato"
            ],
            "title": "A generative model for molecular distance geometry",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jinhua Zhu",
                "Yingce Xia",
                "Chang Liu",
                "Lijun Wu",
                "Shufang Xie",
                "Yusong Wang",
                "Tong Wang",
                "Tao Qin",
                "Wengang Zhou",
                "Houqiang Li",
                "Haiguang Liu",
                "Tie-Yan Liu"
            ],
            "title": "Direct molecular conformation generation",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Gengmo Zhou",
                "Zhifeng Gao",
                "Qiankun Ding",
                "Hang Zheng",
                "Hongteng Xu",
                "Zhewei Wei",
                "Linfeng Zhang",
                "Guolin Ke"
            ],
            "title": "Uni-mol: A universal 3d molecular representation learning framework",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Bowen Jing",
                "Gabriele Corso",
                "Regina Barzilay",
                "Tommi S. Jaakkola"
            ],
            "title": "Torsional diffusion for molecular conformer generation",
            "venue": "In ICLR2022 Machine Learning for Drug Discovery,",
            "year": 2022
        },
        {
            "authors": [
                "Philipp Pracht",
                "Fabian Bohle",
                "Stefan Grimme"
            ],
            "title": "Automated exploration of the low-energy chemical space with fast quantum chemical methods",
            "venue": "Phys. Chem. Chem. Phys.,",
            "year": 2020
        },
        {
            "authors": [
                "Dylan M. Anstine",
                "Olexandr Isayev"
            ],
            "title": "Generative models as an emerging paradigm in the chemical sciences",
            "venue": "Journal of the American Chemical Society,",
            "year": 2023
        },
        {
            "authors": [
                "Elman Mansimov",
                "Omar Mahmood",
                "Seokho Kang",
                "Kyunghyun Cho"
            ],
            "title": "Molecular geometry prediction using a deep generative graph neural network",
            "venue": "Scientific reports,",
            "year": 2019
        },
        {
            "authors": [
                "Minkai Xu",
                "Wujie Wang",
                "Shitong Luo",
                "Chence Shi",
                "Yoshua Bengio",
                "Rafael Gomez-Bombarelli",
                "Jian Tang"
            ],
            "title": "An end-to-end framework for molecular conformation generation via bilevel programming",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Minkai Xu",
                "Shitong Luo",
                "Yoshua Bengio",
                "Jian Peng",
                "Jian Tang"
            ],
            "title": "Learning neural generative dynamics for molecular conformation generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Chence Shi",
                "Shitong Luo",
                "Minkai Xu",
                "Jian Tang"
            ],
            "title": "Learning gradient fields for molecular conformation generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shitong Luo",
                "Chence Shi",
                "Minkai Xu",
                "Jian Tang"
            ],
            "title": "Predicting molecular conformation via dynamic graph score matching",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Minkai Xu",
                "Lantao Yu",
                "Yang Song",
                "Chence Shi",
                "Stefano Ermon",
                "Jian Tang"
            ],
            "title": "Geodiff: A geometric diffusion model for molecular conformation generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "John Jumper",
                "Richard Evans",
                "Alexander Pritzel",
                "Tim Green",
                "Michael Figurnov",
                "Olaf Ronneberger",
                "Kathryn Tunyasuvunakool",
                "Russ Bates",
                "Augustin \u017d\u00eddek",
                "Anna Potapenko",
                "Alex Bridgland",
                "Clemens Meyer",
                "Simon A.A. Kohl",
                "Andrew J. Ballard",
                "Andrew Cowie",
                "Bernardino Romera-Paredes",
                "Stanislav Nikolov",
                "Rishub Jain",
                "Jonas Adler",
                "Trevor Back",
                "Stig Petersen",
                "David Reiman",
                "Ellen Clancy",
                "Michal Zielinski",
                "Martin Steinegger",
                "Michalina Pacholska",
                "Tamas Berghammer",
                "Sebastian Bodenstein",
                "David Silver",
                "Oriol Vinyals",
                "Andrew W. Senior",
                "Koray Kavukcuoglu",
                "Pushmeet Kohli",
                "Demis Hassabis"
            ],
            "title": "Highly accurate protein structure prediction with alphafold",
            "venue": "doi: 10.1038/s41586-021-03819-2. URL https://doi.org/10.1038/",
            "year": 2021
        },
        {
            "authors": [
                "Paraskevi Gkeka",
                "Gabriel Stoltz",
                "Amir Barati Farimani",
                "Zineb Belkacemi",
                "Michele Ceriotti",
                "John D. Chodera",
                "Aaron R. Dinner",
                "Andrew L. Ferguson",
                "Jean-Bernard Maillet",
                "Herv\u00e9 Minoux",
                "Christine Peter",
                "Fabio Pietrucci",
                "Ana Silveira",
                "Alexandre Tkatchenko",
                "Zofia Trstanova",
                "Rafal Wiewiora",
                "Tony Leli\u00e8vre"
            ],
            "title": "Machine learning force fields and coarse-grained variables in molecular dynamics: Application to materials and biological systems",
            "venue": "Journal of Chemical Theory and Computation,",
            "year": 2020
        },
        {
            "authors": [
                "Jaehyeok Jin",
                "Alexander J. Pak",
                "Aleksander E.P. Durumeric",
                "Timothy D. Loose",
                "Gregory A. Voth"
            ],
            "title": "Bottom-up coarse-graining: Principles and perspectives",
            "venue": "Journal of Chemical Theory and Computation, 18(10):5759\u20135791,",
            "year": 2022
        },
        {
            "authors": [
                "Marloes Arts",
                "Victor Garcia Satorras",
                "Chin-Wei Huang",
                "Daniel Zuegner",
                "Marco Federici",
                "Cecilia Clementi",
                "Frank No\u00e9",
                "Robert Pinsler",
                "Rianne van den Berg"
            ],
            "title": "Two for one: Diffusion models and force fields for coarse-grained molecular dynamics, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Shriram Chennakesavalu",
                "David J Toomer",
                "Grant M Rotskoff"
            ],
            "title": "Ensuring thermodynamic consistency with invertible coarse-graining",
            "venue": "The Journal of Chemical Physics,",
            "year": 2023
        },
        {
            "authors": [
                "Octavian Ganea",
                "Lagnajit Pattanaik",
                "Connor Coley",
                "Regina Barzilay",
                "Klavs Jensen",
                "William Green",
                "Tommi Jaakkola"
            ],
            "title": "Geomol: Torsional geometric generation of molecular 3d conformer ensembles",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "W. Kabsch"
            ],
            "title": "Automatic processing of rotation diffraction data from crystals of initially unknown symmetry and cell constants",
            "venue": "Journal of Applied Crystallography,",
            "year": 1993
        },
        {
            "authors": [
                "Irina Higgins",
                "Loic Matthey",
                "Arka Pal",
                "Christopher Burgess",
                "Xavier Glorot",
                "Matthew Botvinick",
                "Shakir Mohamed",
                "Alexander Lerchner"
            ],
            "title": "beta-VAE: Learning basic visual concepts with a constrained variational framework",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Evan Walter Clark Spotte-Smith",
                "Samuel M. Blau",
                "Xiaowei Xie",
                "Hetal D. Patel",
                "Mingjian Wen",
                "Brandon Wood",
                "Shyam Dwaraknath",
                "Kristin Aslaug Persson"
            ],
            "title": "Quantum chemical calculations of lithium-ion battery electrolyte and interphase species",
            "venue": "Scientific Data,",
            "year": 2021
        },
        {
            "authors": [
                "Michael G. Taylor",
                "Daniel J. Burrill",
                "Jan Janssen",
                "Enrique R. Batista",
                "Danny Perez",
                "Ping Yang"
            ],
            "title": "Architector for high-throughput cross-periodic table 3d complex building",
            "venue": "Nature Communications,",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Bannwarth",
                "Sebastian Ehlert",
                "Stefan Grimme"
            ],
            "title": "Gfn2-xtb\u2014an accurate and broadly parametrized self-consistent tight-binding quantum chemical method with multipole electrostatics and density-dependent dispersion contributions",
            "venue": "Journal of Chemical Theory and Computation, 15(3):1652\u20131671,",
            "year": 2019
        },
        {
            "authors": [
                "Paul G. Francoeur",
                "Tomohide Masuda",
                "Jocelyn Sunseri",
                "Andrew Jia",
                "Richard B. Iovanisci",
                "Ian Snyder",
                "David R. Koes"
            ],
            "title": "Three-dimensional convolutional neural networks and a cross-docked data set for structure-based drug design",
            "venue": "Journal of Chemical Information and Modeling, 60(9):4200\u20134215,",
            "year": 2020
        },
        {
            "authors": [
                "Jerome Eberhardt",
                "Diogo Santos-Martins",
                "Andreas F. Tillack",
                "Stefano Forli"
            ],
            "title": "Autodock vina 1.2.0: New docking methods, expanded force field, and python bindings",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaqi Guan",
                "Wesley Wei Qian",
                "Xingang Peng",
                "Yufeng Su",
                "Jian Peng",
                "Jianzhu Ma"
            ],
            "title": "3d equivariant diffusion for target-aware molecule generation and affinity prediction",
            "venue": "arXiv preprint arXiv:2303.03543,",
            "year": 2023
        },
        {
            "authors": [
                "Xingang Peng",
                "Shitong Luo",
                "Jiaqi Guan",
                "Qi Xie",
                "Jian Peng",
                "Jianzhu Ma"
            ],
            "title": "Pocket2mol: Efficient molecular sampling based on 3d protein pockets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Danny Reidenbach",
                "Micha Livne",
                "Rajesh K. Ilango",
                "Michelle Lynn Gill",
                "Johnny Israeli"
            ],
            "title": "Improving small molecule generation using mutual information machine",
            "venue": "In ICLR 2023 - Machine Learning for Drug Discovery workshop,",
            "year": 2023
        },
        {
            "authors": [
                "Chence Shi",
                "Minkai Xu",
                "Zhaocheng Zhu",
                "Weinan Zhang",
                "Ming Zhang",
                "Jian Tang"
            ],
            "title": "Graphaf: a flow-based autoregressive model for molecular graph generation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Wengong Jin",
                "Regina Barzilay",
                "T. Jaakkola"
            ],
            "title": "Hierarchical generation of molecular graphs using structural motifs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yinan Huang",
                "Xing Peng",
                "Jianzhu Ma",
                "Muhan Zhang"
            ],
            "title": "3dlinker: An e(3) equivariant variational autoencoder for molecular linker design",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Keir Adams",
                "Connor W. Coley"
            ],
            "title": "Equivariant shape-conditioned generation of 3d molecules for ligand-based drug design",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Wengong Jin",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Antibody-antigen docking and design via hierarchical structure refinement",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriele Corso",
                "Hannes St\u00e4rk",
                "Bowen Jing",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Diffdock: Diffusion steps, twists, and turns for molecular docking",
            "venue": "arXiv preprint arXiv:2210.01776,",
            "year": 2022
        },
        {
            "authors": [
                "Victor Garcia Satorras",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "E(n) equivariant graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kristof T. Sch\u00fctt",
                "Oliver T. Unke",
                "Michael Gastegger"
            ],
            "title": "Equivariant message passing for the prediction of tensorial properties and molecular spectra",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Congyue Deng",
                "Or Litany",
                "Yueqi Duan",
                "Adrien Poulenard",
                "Andrea Tagliasacchi",
                "Leonidas J. Guibas"
            ],
            "title": "Vector neurons: A general framework for so(3)-equivariant networks",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Hannes St\u00e4rk",
                "Octavian Ganea",
                "Lagnajit Pattanaik",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Equibind: Geometric deep learning for drug binding structure prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Nathaniel Thomas",
                "Tess Smidt",
                "Steven Kearnes",
                "Lusann Yang",
                "Li Li",
                "Kai Kohlhoff",
                "Patrick Riley"
            ],
            "title": "Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds",
            "venue": "arXiv preprint arXiv:1802.08219,",
            "year": 2018
        },
        {
            "authors": [
                "Ronald J. Williams",
                "David Zipser"
            ],
            "title": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks",
            "venue": "Neural Computation, 1(2):270\u2013280,",
            "year": 1989
        },
        {
            "authors": [
                "Paul C.D. Hawkins",
                "A. Geoffrey Skillman",
                "Gregory L. Warren",
                "Benjamin A. Ellingson",
                "Matthew T. Stahl"
            ],
            "title": "Conformer generation with omega: Algorithm and validation using high quality structures from the protein databank and cambridge structural database",
            "venue": "Journal of Chemical Information and Modeling, 50(4):572\u2013584,",
            "year": 2010
        },
        {
            "authors": [
                "R\u00e9mi Flamary",
                "Nicolas Courty",
                "Alexandre Gramfort",
                "Mokhtar Z. Alaya",
                "Aur\u00e9lie Boisbunon",
                "Stanislas Chambon",
                "Laetitia Chapel",
                "Adrien Corenflos",
                "Kilian Fatras",
                "Nemo Fournier",
                "L\u00e9o Gautheron",
                "Nathalie T.H. Gayraud",
                "Hicham Janati",
                "Alain Rakotomamonjy",
                "Ievgen Redko",
                "Antoine Rolet",
                "Antony Schutz",
                "Vivien Seguy",
                "Danica J. Sutherland",
                "Romain Tavenard",
                "Alexander Tong",
                "Titouan Vayer"
            ],
            "title": "Pot: Python optimal transport",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "David Ryan Koes",
                "Matthew P. Baumgartner",
                "Carlos J. Camacho"
            ],
            "title": "Lessons learned in empirical scoring with smina from the csar 2011 benchmarking exercise",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2013
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Molecular conformer generation (MCG) is a fundamental task in computational chemistry. The objective is to predict stable low-energy 3D molecular structures, known as conformers. Accurate molecular conformations are crucial for various applications that depend on precise spatial and geometric qualities, including drug discovery and protein docking. For MCG, traditional physics-based methods present a trade-off between speed and accuracy. Quantum mechanical methods are more accurate but computationally slow, while stochastic cheminformatics-based methods like RDKit EKTDG [1] provide more efficient but less accurate results. As the difficulty of computing low-energy structures increases with the number of atoms and rotatable bonds in a molecule, there has been interest in developing machine learning (ML) methods to generate accurate conformer predictions efficiently.\nExisting generative MCG ML models can be broadly classified based on two primary criteria: (1) the choice of the architecture to model the distribution of low-energy conformers and (2) the manner in which they incorporate geometric information to model 3D conformers. A majority of these methods utilize a single geometric representation, such as distance matrices, atom coordinates, or torsional angles, and pair them with various probabilistic modeling techniques, including variational autoencoders (VAEs) and diffusion models. A recurring limitation of these prior approaches is that they are restricted to modeling only one specific aspect of geometric information i.e., coordinates or angles. By restricting themselves to either a pure coordinate or angle space, they often fail to fully leverage the full geometric information inherent to the problem. Consequently, in this work, we introduce a more flexible molecular latent representation that seamlessly integrates multiple desired geometric modalities, thereby enhancing the overall accuracy and versatility of conformer generation.\nTo create this flexible representation, we use a process known as coarse-graining to distill molecular information into\nar X\niv :2\n30 6.\n14 85\n2v 2\n[ cs\n.L G\n] 1\n9 O\nct 2\n02 3\na simplified, coarse-grained (CG) representation based on specific coarsening criteria. This is analogous to previous 2D fragment-based generative techniques [2]. In these scenarios, the fragmentation of molecules into prominent substructures led to significant enhancements in representation learning and generative tasks. Coarse-graining has seen success in related ML applications like molecular dynamics and protein modeling [3, 4]. However, these ML approaches have primarily explored rigid coarsening criteria, such as distance-based nearest neighbor methods, which represent all inputs with a fixed granularity or number of beads (i.e. CG nodes, N ). ML models that use fixed-length coarse-graining often necessitate N be fixed for all input molecules [5]. This approach may not be suitable for all scenarios, especially when navigating multi-modal datasets of drug-like molecules of varying sizes (multiple N ), which better reflects real-world conditions.\nTo address the limitations of fixed-length CG representations, we introduce Aggregated Attention for variable-length coarse-graining (see Fig. 1). This methodology allows a single latent representation to accommodate molecules with different numbers of fine-grained (FG) atoms and CG beads. The inherent flexibility of the attention mechanism allows input molecules to be fragmented along torsion angles, enabling the modeling of interatomic distances, 3D atom coordinates, and torsion angles in an equivariant manner, regardless of the molecule\u2019s shape or size. Through Aggregated Attention, we also harness information from the entire learned representation, unlike preceding approaches that restrict the backmapping (from CG back to FG) to a subset [6]. The adaptability of our learned variable-length representations enables more accurate generation.\nOur full innovations are encapsulated in CoarsenConf, an SE(3)-equivariant hierarchical VAE. CoarsenConf aggregates information from fine-grained atomic coordinates to create a flexible subgraph-level representation, improving the accuracy of conformer generation. Unlike prior MCG methods, CoarsenConf generates low-energy conformers with the ability to model atomic coordinates (FG and CG), distances, and torsion angles directly via variable-length coarse-graining.\nOur main contributions are as follows:\n\u2022 We present CoarsenConf, a novel conditional hierarchical VAE. CoarsenConf learns a coarse-grained or subgraphlevel latent distribution for SE(3)-equivariant conformer generation. To our knowledge, this is the first method to use coarse-graining in the context of MCG.\n\u2022 CoarsenConf is the first model capable of handling variable-length coarse-to-fine generation using an Aggregated\nAttention strategy. CoarsenConf employs a single flexible variable-length node-level latent representation that can uniquely represent molecules of any size with any number of coarse-grained nodes. Furthermore, variable-length coarse-graining circumvents having to train separate generative models for each number of CG beads to represent the same molecular dataset accurately (100+ models for MCG), which is a limitation of fixed-length methods [5].\n\u2022 We predominantly outperform prior methods on GEOM-QM9 and GEOM-DRUGS for RMSD precision and property prediction benchmarks [7]. We also produce a lower overall RMSD distribution across all conformers, achieving this with an order of magnitude less training time compared to prior methods.\n\u2022 We evaluate CoarsenConf on multiple downstream applications to assess the chemical and biochemical quality of our generated conformers, including oracle-based protein docking [8] (the affinity of generated conformers to bind to specific protein pockets) under both flexible and rigid conformational energy minimizations. Despite lacking prior knowledge about the protein or the downstream task, CoarsenConf generates significantly better binding ligand conformers for known protein binding sites when compared to prior MCG methods for both oracle scenarios."
        },
        {
            "heading": "2 Background",
            "text": "Notations. We represent each molecule as a graph G = (V,E), where V is the set of vertices representing atoms and E is the set of edges representing inter-atomic bonds. Each node v in V describes the chosen atomic features such as element type, atomic charge, and hybridization state. Each edge euv in E describes the corresponding chemical bond connecting u and v, and is labeled with its bond type. Following Simm and Hernandez-Lobato [9], each molecular graph is expanded to incorporate auxiliary edges connecting all atoms within a 4\u00c5 radius to enhance long-range interactions in message passing. The spatial position of each atom in V is represented by a 3D coordinate vector r \u2208 R3, such that the full molecule conformation is represented by the matrix X \u2208 R|V|\u00d73.\nProblem Definition. Molecular conformation generation (MCG) is a conditional generative process that aims to model the conditional distribution of 3D molecular conformations X , given the 2D molecule graph G, i.e., p(X|G). While prior works have shown some success with learning 3D conformations starting with only the 2D information, many require complex and compute-intensive architectures [10, 11]. Recently, Jing et al. [12] demonstrated good performance on the GEOM-DRUGS dataset by priming the method with easy-to-obtain 3D approximations via RDKit ETKDG [1]. Jing et al. [12] showed that RDKit is highly effective at generating conformations with correct bond distances and, as a result, can constrain the problem to a diffusion process over only torsion angles.\nWe thus formalize MCG as modeling the conditional distribution p(X|R), where R is the RDKit generated atomic coordinates. This is functionally the same underlying distribution as p(X|G), as we use RDKit as a building block to provide an approximation starting from only 2D information, R = RDKit ETKDG(G). We will show that more robust conformers are generated by conditioning on approximations without imposing explicit angular and distance constraints.\nClassical Methods for Conformer Generation. A molecular conformer refers to the collection of 3D structures that are energetically favorable and correspond to local minima of the potential energy surface. CREST [13] uses semi-empirical tight-binding density functional theory (DFT) for energy calculations, which, while computationally less expensive than ab-initio quantum mechanical (QM) methods, still requires approximately 90 core hours per drug-like molecule [7]. Though CREST was used to generate the \u201cground truth\u201d GEOM dataset, it is too slow for downstream applications such as high-throughput virtual screening.\nCheminformatics methods, such as RDKit ETKDG, are commonly used to quickly generate approximate low-energy conformations of molecules. These methods are less accurate than QM methods due to the sparse coverage of the conformational space resulting from stochastic sampling. Additionally, force field optimizations are inherently less accurate than the above QM methods. RDKit ETKDG employs a genetic algorithm for Distance Geometry optimization that can be enhanced with a molecular mechanics force field optimization (MMFF).\nDeep Learning Methods for Conformer Generation. Several probabilistic deep learning methods for MCG have been developed [14], such as variational autoencoders in CVGAE [15] and ConfVAE [16], normalizing flows in CGCF [17], score-based generative models in ConfGF [18] and DGSM [19], and diffusion models in GeoDiff [20] and Torsional Diffusion [12]. GraphDG [9] forgoes modeling coordinates and angles, relying solely on distance geometry. DMCG [10] and Uni-Mol [11] present examples of effective large models, the first mimicking the architecture of AlphaFold [21] and the second using large-scale SE(3)-equivariant transformer pre-training.\nMolecular Coarse-graining. Molecular coarse-graining refers to the simplification of a molecule representation by grouping the fine-grained (FG) atoms in the original structure into individual coarse-grained (CG) beads with a rule-based mapping.1 Coarse-graining has been widely utilized in protein design [4] and molecular dynamics [22], and analogously fragment-level or subgraph-level generation has proven to be highly valuable in diverse 2D molecule design tasks [2]. Breaking down generative problems into smaller pieces can be applied to several 3D molecule tasks. For instance, CGVAE [6] learns a latent distribution to back map or restore FG coordinates from a fixed number of CG beads effectively. We note that various coarse-graining strategies exist [23, 24, 3, 25], and many require the ability to represent inputs with a non-fixed granularity. To handle this, CoarsenConf uses a flexible variable-length CG representation that is compatible with all coarse-graining techniques.\nWe include an extensive discussion on further relevant work in Appendix \u00a7A. We discuss prominent 2D and 3D equivariant autoregressive molecular generation, protein docking, and structure-based drug discovery (SBDD) techniques, as well as our formal definition of SE(3)-equivariance."
        },
        {
            "heading": "3 Methods",
            "text": "Coarse-graining Procedure. We first define a rotatable bond as any single bond between two non-terminal atoms, excluding amides and conjugated double bonds, where the torsion angle is the angle of rotation around the central bond. Formally, the torsion angle \u03c4abcd is defined about bond (b, c) \u2208 E where (a, b) are a choice of reference neighbors s.t a \u2208 N(b) \\ c and d \u2208 N(c) \\ b.\n1We use the terms CG graph nodes and beads interchangeably.\nWe coarsen molecules into a single fragment or bead for each connected component, resulting from severing all rotatable bonds. This choice in CG procedure implicitly forces the model to learn over torsion angles, as well as atomic coordinates and inter-atomic distances. We found that using a more physically constrained definition of torsional angles, as defined by Ganea et al. [26], in the CG procedure led to a significant increase in performance compared to that used in Jing et al. [12]. This is because the latter allows rotations around double and triple bonds, while the former does not. An example of the coarse-graining procedure is in Fig. 1. For formal definitions, see Appendix \u00a7C.\nLearning Framework. CoarsenConf is a conditional generative model that learns p(X|R) where X is the low-energy 3D conformation, and R is the RDKit approximate conformation. Specifically, we optimize p(X|R) by maximizing its variational lower bound with an approximate conditional posterior distribution q\u03d5(z|X,R) and learned conditional prior p\u03c8(z|R):\nlog p(X|R) \u2265Eq\u03d5(z|X,R) log p\u03b8(X|R, z)\ufe38 \ufe37\ufe37 \ufe38 Lreconstruction +Eq\u03d5(z|X,R) log p\u03c8(z|R)\nq\u03d5(z|X,R)\ufe38 \ufe37\ufe37 \ufe38 Llatent regularization +Lauxiliary, (1)\nwhere q\u03d5(z|X,R) is the hierarchical equivariant encoder model, p\u03b8(X|R, z) is the equivariant decoder model to recover X from R and z, and p\u03c8(z|R) is the learned prior distribution. The reconstruction loss, Lrecon., is implemented as MSE(A(Xtrue, Xmodel), Xmodel), where A is the Kabsch alignment function that provides an optimal rotation matrix and translation vector to minimize the mean squared error (MSE) [27]. The second term, Lreg., can be viewed as a regularization over the latent space and is implemented as \u03b2DKL(q\u03d5(z|X,R) \u2225 p\u03c8(z|R)) [28]. More details on the geometric auxiliary loss function are in Appendix \u00a7B.\nEncoder Architecture. CoarsenConf\u2019s encoder, shown in Fig. 2(I), operates over SE(3)-invariant atom features h \u2208 Rn\u00d7D, and SE(3)-equivariant atomistic coordinates x \u2208 Rn\u00d73. A single encoder layer is composed of three modules: fine-grained, pooling, and coarse-grained. Full equations for each module can be found in Appendix \u00a7D.1, \u00a7D.2, \u00a7D.3, respectively. The encoder module takes in x and h from the ground truth and RDKit conformer and creates coarse-grained latent representations for each, Z and Z\u0303 \u2208 RN\u00d7F\u00d73, where N is the number of CG beads, and F is the latent dimensions.\nEquivariant Latent Space. As Z holds a mixture of equivariant spatial information, we maintain equivariance through the reparametrization trick of the VAE (Fig. 2(II)). Specifically, we define the posterior and prior means (\u00b5\u03d5, \u00b5\u03c8) and standard deviations (\u03c3\u03d5, \u03c3\u03c8), as follows:\nPosterior : \u00b5\u03d5 = VN-MLP(Z, Z\u0303),\nPrior : \u00b5\u03c8 = VN-MLP(Z\u0303),\nlog(\u03c32\u03d5) = MLP(Z, Z\u0303), log(\u03c32\u03c8) = MLP(Z\u0303). (2)\nWe use an invariant MLP to learn the variance and apply it to the x, y, and z directions to enforce equivariance. We note that the conditional posterior is parameterized with both the ground truth and RDKit approximation, whereas the learned conditional prior only uses the RDKit.\nDecoder Architecture: Channel Selection. We sample from the learned posterior (training) and learned prior (inference) to get Z = \u00b5 + \u03f5\u03c3, where \u03f5 is noise sampled from a standard Gaussian distribution as the input to the decoder. Given Z is still in CG space, we need to perform variable-length backmapping to convert back to FG space so that we can further refine the atom coordinates to generate the low energy conformer. The variable-length aspect is crucial because every molecule can be coarsened into a different number of beads, and there is no explicit limit to the number of atoms a single bead can represent. Unlike CGVAE [6], which requires training a separate model for each choice in CG granularity N , CoarsenConf is capable of reconstructing FG coordinates from any N (illustrated in Fig. 2(III) and Fig. 1).\nCGVAE defines the process of channel selection (CS) as selecting the top-k latent channels, where k is the number of atoms in a CG bead of interest. Instead of discarding all learned information in the remaining F \u2212 k channels in the\nlatent representation, we use a novel aggregated attention mechanism. This mechanism learns the optimal mixing of channels to reconstruct the FG coordinates, and is illustrated in Fig. 3. The attention operation allows us to actively query our latent representation for the number of atoms we need, and draw upon similarities to the learned RDKit approximation that has been distilled into the latent space through the hierarchical encoding process. Channel selection translates the CG latent tensor Z \u2208 RN\u00d7F\u00d73 into FG coordinates xCS \u2208 Rn\u00d73.\nDecoder Architecture: Coordinate Refinement. Once channel selection is complete, we have effectively translated the variable-length CG representation back into the desired FG form. From here, we explore two methods of decoding that use the same underlying architecture. xCS can be passed in a single step through the decoder or can be grouped into its corresponding CG beads, but left in FG coordinates to do a bead-wise autoregressive (AR) generation of final coordinates (Fig. 2(IV)). CoarsenConf is the first MCG method to explore AR generation. Unlike prior 3D AR methods, CoarsenConf does not use a pre-calculated molecular fragment vocabulary, and instead conditions directly on a learned mixture of previously generated 3D coordinates and invariant atom features.\nThe decoder architecture is similar to the EGNN-based FG module in the encoder, but has one key difference. Instead of learning raw atom coordinates, we learn to predict the difference between the RDKit reference and ground truth conformations. As the goal of MCG is to model the conditional distribution p(X|R), we simplify the learning objective by setting X = R + \u2206X , and learn the optimal distortion \u2206X from the RDKit approximation. The simplification follows, as we have ensured Lrecon. is no worse than that of the RDKit approximation, which is trivial to obtain, compared to the cost of model training and inference.\nSee Appendix \u00a7E for formal discussions of the decoder architecture and message-passing equations."
        },
        {
            "heading": "4 Experiments",
            "text": "We evaluate MCG on RMSD spatial accuracy (\u00a74.1), property prediction (\u00a74.1), and biochemical quality through flexible (\u00a74.3) and rigid (\u00a74.4) oracle-based protein docking. We include the following models for comparison with CoarsenConf, which are previous MCG methods that use the same train/test split: Torsional Diffusion (TD) [12], GeoMol (GM) [26], and when possible, GeoDiff (GD) [20]. For our model configuration and compute resource breakdown, see Appendix \u00a7F. The details of CoarsenConf\u2019s initial RDKit structures, as well as how CoarsenConf learns to avoid the distribution shift found in TD, can be found in Appendix \u00a7G."
        },
        {
            "heading": "4.1 GEOM Benchmarks: 3D Coordinate RMSD",
            "text": "We use the GEOM dataset [7], consisting of QM9 (average 11 atoms) and DRUGS (average 44 atoms), to train and evaluate our model. We use the same train/val/test splits from Ganea et al. [26] (QM9: 106586/13323/1000 and DRUGS: 243473/30433/1000).\nProblem setup. We report the average minimum RMSD (AMR) between ground truth and generated conformers, and Coverage for Recall and Precision. Coverage is defined as the percentage of conformers with a minimum error under a specified AMR threshold. Recall matches each ground truth conformer to its closest generated structure, and Precision measures the overall spatial accuracy of the generated conformers. Following Jing et al. [12], we generate two times the number of ground truth conformers for each molecule. More formally, for K = 2L, let {C\u2217l }l\u2208[1,L] and {Ck}k\u2208[1,K] respectively be the sets of ground truth and generated conformers:\nCOV-Precision := 1\nK \u2223\u2223\u2223\u2223{k \u2208 [1..K] : minl\u2208[1..L] RMSD(Ck, C\u2217l ) < \u03b4}\u2223\u2223\u2223\u2223, AMR-Precision := 1\nK \u2211 k\u2208[1..K] minl\u2208[1..L] RMSD(Ck, C\u2217l ), (3)\nwhere \u03b4 is the coverage threshold. The recall metrics are obtained by swapping ground truth and generated conformers. We also report the full RMSD error distributions, as the AMR only gives a small snapshot into overall error behavior.\nAdvantages and limitations of RMSD-based metrics. 3D coordinate-based metrics are commonly used to evaluate ML methods because these models are typically trained using spatial error-based loss functions (i.e., MSE). However, for domain scientists, these metrics can be somewhat challenging to interpret. Low spatial error (measured by RMSD) is not directly informative of free energy, the primary quantity of interest to scientists [29, 30]. Additionally, current spatial benchmarks can be categorized into two distinct types: precision and recall. Each of these metrics comes with its own advantages and limitations.\n(1) Precision measures the generation accuracy. It tells us if each generated conformer is close to any one of the given ground truth structures, but it does not tell us if we have generated the lowest energy structure, which is the most important at a standardized temperature of 0 K. At industrial temperatures, the full distribution of generated conformers is more important than the ability to generate a single ground truth conformer (for the full RMSD error distribution, see Fig. 4).\n(2) Recall compares each ground truth to its closest generated conformer. However, in many applications, we are only concerned with obtaining the lowest energy conformer, not all feasible ones. Furthermore, Recall is severely biased by the number of generated conformers for each molecule. Results worsen by up to ~60% for all models when we move from the previously set standard sampling budget for each molecule of 2L to min(L/32, 1), where L is the number of ground truth conformers (see Appendix Fig. 7). As we sample more molecules, we greatly influence the chance of reducing the AMR-Recall. Because of this dependency on the number of samples, we focus on the Precision metrics, which were consistent across all tested sample size budgets. We note that L is 104 on average for GEOM-DRUGS [15], so even L/32 is a reasonable number.\nResults. In Tab. 1, we outperform all models on QM9, and yield competitive results with TD on DRUGS when using an optimal transport (OT) loss (see Appendix \u00a7I- \u00a7J for more details). CoarsenConf also achieves the lowest overall error distribution, as seen in Fig. 4. CoarsenConf-OT uses an OT loss with the same decoder architecture as in Fig. 2, but is no longer autoregressive (see Appendix Eq. 11 for a formal definition). We also see in Appendix Fig. 7 that the recall is heavily dependent on the sampling budget, as performance gaps shrink from ~50% to ~5%. CoarsenConfOT was trained for 15 hours (2 epochs) compared to TD\u2019s 11 days, both on a single A6000. Furthermore, when limited to the same equivariance, CoarsenConf-OT performs predominantly better (Appendix Tab. 6). As CoarsenConf (autoregressive, without the OT loss) results in a lower overall DRUGS RMSD distribution (Fig. 4), we use this model for the remaining downstream tasks.\nTable 2: Property prediction: Mean absolute error of generated vs. ground truth ensemble properties for E, HOMOLUMO gap \u2206\u03f5, Emin (kcal/mol), and dipole moment \u00b5 (debye).\nE \u00b5 \u2206\u03f5 Emin\nGeoMol 28.80 1.475 4.186 0.267 Torsional Diffusion 16.75 1.333 2.908 0.096 CoarsenConf 12.41 1.250 2.522 0.049"
        },
        {
            "heading": "4.2 GEOM Benchmarks: Property Prediction",
            "text": "Problem setup. We generate and relax min(2L, 32) conformers (L ground truth) for 100 molecules from GEOMDRUGS using GFN2-xTB with the BFGS optimizer. We then predict various properties, including Energy (E), HOMO-LUMO Gap (\u2206\u03f5), minimum Energy (Emin) in kcal/mol, and dipole moment (\u00b5) in debye via xTB [31]. The mean absolute error of the generated ensemble properties compared to ground truth is reported.\nResults. Tab. 2 demonstrates CoarsenConf\u2019s ability to generate the lowest energy structures with the most accurate chemical properties. For further discussions, please see Appendix \u00a7H."
        },
        {
            "heading": "4.3 Flexible Oracle-based Protein Docking",
            "text": "We evaluate MCG models, pretrained on GEOM-DRUGS, using nine protein docking oracle functions provided by the Therapeutics Data Commons (TDC) [8].\nProblem setup. Starting with a known 2D ligand2 molecule, protein, and desired 3D protein binding pocket, we measure conformer quality by comparing the predicted binding affinity of generated conformers of each MCG method. TDC\u2019s protein docking oracle functions take in a ligand SMILES string, generate a 3D conformer, and try multiple poses, before ultimately returning the best binding affinity via Autodock Vina\u2019s flexible docking simulation. We augment TDC with the ability to query ML models pretrained on GEOM-DRUGS, instead of the built-in RDKit +\n2No ground truth 3D structures. All ligand SMILES taken from Protein Data Bank: https://www.rcsb.org/\nMMFF approach for MCG. For each evaluated MCG method, we generate 50 conformers for each of the nine ligands and report the best (lowest) binding affinity. Given that the ligand and protein identity and the protein pocket are fixed, this task measures the quality of 3D conformer coordinates through their binding efficacy to the specified pocket. We note that this task is indicative of real-world simulation workflows.\nResults. CoarsenConf significantly outperforms prior MCG methods on the TDC oracle-based affinity prediction task (Tab. 3). CoarsenConf generates the best ligand conformers for 8/9 tested proteins, with improvements of up to 53% compared to the next best method. CoarsenConf is 1.46 kcal/mol better than all methods when averaged over all 9 proteins, which corresponds to a 14.4% improvement on average compared to the next best method."
        },
        {
            "heading": "4.4 Rigid Oracle-based Protein Docking",
            "text": "We evaluate MCG models on rigid oracle-based protein docking. We use the 166000 protein-ligand complexes from the CrossDocked [32] training set (Appendix \u00a7K for more details).\nProblem setup. Similar to the flexible docking task (\u00a74.3), we can generate conformers of known ligands for known protein pockets, but now have them only undergo a rigid pocket-specific energy minimization before predicting the binding affinity. To handle the fact that MCG generates ligand structures in a vacuum and has no information about the target protein, we assume the centroid of the ground truth ligand is accurate, and translate each generated structure to match the center.\nWe use AutoDock Vina [33] and its local BFGS energy optimization (similar to that done with xTB), i.e. a relaxation of all tested structures (including the ground truth), following the SBDD benchmarking framework [34]. We report the difference between the generated structure\u2019s minimum binding affinity and that of the ground truth 3D ligand. Unlike the docking simulation, Vina\u2019s energy minimization does not directly adjust the torsion angles or internal degrees of freedom within the ligand. Instead, it explores different atomic positions and orientations of the entire ligand molecule within the binding site of the protein to find energetically favorable binding poses. By further isolating the MCG-generated structures, this task better evaluates the generative capacity of MCG models. While the original SBDD task [35, 34] reports the Vina minimization score as its main metric, it requires the 3D ligand to be generated from scratch. Here, we use the SBDD framework to isolate the generated ligand conformer as the only source of variability to evaluate the biological and structural quality of MCG models in an unbiased fashion.\nResults. We report the results for 100,000 unique conformer-protein interactions; note that there is a large cost to run the binding affinity prediction (see Appendix \u00a7K for more details). We also emphasize that the presented evaluation is not to be confused with actual docking solutions, as a low-energy conformer is not always guaranteed to be the best binding pose. Instead, we employ an unbiased procedure to present empirical evidence for how CoarsenConf can generate input structures to Vina that significantly outperform prior MCG models in achieving the best binding affinities.\nFig. 5 further demonstrates CoarsenConf\u2019s superior performance on orders of magnitude more protein complexes than the prior flexible oracle task. CoarsenConf decreases the average error by 56% compared to TD, and is the only method\nnot to exhibit bimodal behavior with error greater than zero. We hypothesize that the success of MCG methods in matching ground truth structures is influenced by the complexity of protein pockets. In simpler terms, open pockets better facilitate Vina\u2019s optimization, but the initial position generated by MCG remains crucial. Furthermore, if the initial MCG-generated structure did not matter, the distributions for each MCG model would be identical. We also note that the lowest energy conformer is not always the optimal ligand structure for binding, but in our experiments, it yields the best input for the Vina-based oracles. Overall, CoarsenConf best approximates the ground truth ligand conformers of CrossDocked and generates the best structures for Vina\u2019s rigid energy relaxation and binding affinity prediction."
        },
        {
            "heading": "5 Conclusion",
            "text": "We present CoarsenConf, a novel approach for robust molecular conformer generation that combines an SE(3)equivariant hierarchical VAE with geometric coarse-graining techniques for accurate conformer generation. By utilizing easy-to-obtain approximate conformations, our model effectively learns the optimal distortion to generate low-energy conformers. CoarsenConf possesses unrestricted degrees of freedom, as it can adjust atomic coordinates, distances, and torsion angles freely. CoarsenConf\u2019s CG procedure can also be tailored to handle even larger systems, whereas prior methods are restricted to full FG or torsion angle space. Our experiments demonstrate the effectiveness of CoarsenConf compared to existing methods. Our study also extends recent 3D molecule-protein benchmarks to conformer generation, providing valuable insights into robust generation and downstream biologically relevant tasks."
        },
        {
            "heading": "6 Acknowledgements",
            "text": "This work was supported by Laboratory Directed Research and Development (LDRD) funding under Contract Number DE-AC02-05CH11231. We also acknowledge generous support from Google Cloud. We thank Nithin Chalapathi, Rasmus Hoeegh Lindrup, Geoffrey Negiar, Sanjeev Raja, Daniel Rothchild, Aayush Singh, and Kevin Yang for helpful discussions and feedback. We also thank Minxai Xu, Michelle Gill, Micha Livne, and Maria Korshunova for helpful feedback on downstream tasks, as well as Bowen Jing and Gabriel Corso for their support in using Torsional Diffusion and sharing their benchmarking results for GeoDiff."
        },
        {
            "heading": "A Related Work",
            "text": "Autoregressive Molecule Generation. Autoregressive models provide control over the generative process by enabling direct conditioning on prior information, allowing for a more precise and targeted generation of output. Autoregressive generation has shown success in 2D molecule tasks using SMILE-based methods, as seen in MolMIM [36], as well as graph-based atom-wise and subgraph-level techniques, as shown in GraphAF [37] and HierVAE [38]. Similarly, 3DLinker [39] and SQUID [40] showcase the usefulness of 3D autoregressive molecule generation and their ability to leverage conditional information in both atom-wise and subgraph-level settings for 3D linkage and shape-conditioned generative tasks respectively. We note that, unlike prior methods [40], CoarsenConf does not require a predefined fragment vocabulary. HERN [41] further demonstrates the power of hierarchical equivariant autoregressive methods in the task of computational 3D antibody design. Similarly, Pocket2Mol [35] uses autoregressive sampling for structure-based drug design.\nProtein Docking and Structure-based Drug Design. Protein docking is a key downstream use case for generating optimal 3D molecule structures. Recent research has prominently explored two distinct directions within this field. The first is blind docking, where the goal is to locate the pocket and generate the optimal ligand to bind [42]. The second is structure-based drug design (SBDD), where optimal 3D ligands are generated by conditioning on a specific protein pocket. Specifically, the SBDD task focuses on the ability to generate ligands that achieve a low AutoDock Vina score for the CrossDocked2020 [32] dataset. AutoDock Vina [33] is a widely used molecular docking software that predicts the binding affinity of ligands (drug-like molecules) to target proteins. Autodock Vina takes in the 3D structures of the ligand, target protein, and binding pocket and considers various factors such as van der Waals interactions, electrostatic interactions, and hydrogen bonding between the ligand and target protein to predict the binding affinity. We demonstrate how SBDD can be adapted to construct comprehensive MCG benchmarks. In this framework, we evaluate the generative abilities of MCG models by measuring the binding affinities of generated comforters and comparing them to the provided ground truth ligand conformers for a wide array of protein-ligand complexes.\nSE(3)-Equivariance. Let X and Y be the input and output vector spaces, respectively, which possess a set of transformations G: G\u00d7 X \u2192 X and G\u00d7 Y \u2192 Y. The function \u03d5 : X \u2192 Y is called equivariant with respect to G if, when we apply any transformation to the input, the output also changes via the same transformation or under a certain predictable behavior, i.e.,\nDefinition 1 The function \u03d5 : X 7\u2192 Y is G-equivariant if it commutes with any transformation in G,\n\u03d5(\u03c1X(g)x) = \u03c1Y(g)\u03d5(x),\u2200g \u2208 G, (4)\nwhere \u03c1X and \u03c1Y are the group representations in the input and output space, respectively. Specifically, \u03d5 is called invariant if \u03c1Y is the identity.\nBy enforcing SE(3)-equivariance in our probabilistic model, p(X|R) remains unchanged for any rototranslation of the approximate conformer R. CoarsenConf\u2019s architecture is inspired by recent equivariant graph neural network architectures, such as EGNN [43] and PaiNN [44], as well as Vector Neuron multi-layer perceptron (VN-MLP) [45]."
        },
        {
            "heading": "B Loss function",
            "text": "As described in \u00a73, CoarsenConf optimizes the following loss function:\nMSE(A(X,Xtrue)) + \u03b21DKL(q\u03d5(z|X,R) \u2225 p\u03c8(z|R)) + \u03b22 1 |E\u2217| \u2211\n(i,j)\u2208E\u2217 ||rij \u2212 rtrueij ||2, (5)\nwhere A is the Kabsch alignment function [27], E\u2217 are all the 1 and 2-hop edges in the molecular graph, with rij corresponding to the distance between atoms i and j. We note that both \u03b21 and \u03b22 play a crucial role in the optimization. \u03b21 has to be set low enough (1e\u22123) to allow the optimization to focus on the MSE when the differences between the model-based X and the ground truth are very close, due to the RDKit distortion parameterization.\nFor the QM9 experiments, \u03b21 is annealed starting from 1e\u22126 to 1e\u22121, increasing by a factor of 10 each epoch. \u03b22 controls the distance auxiliary loss and also had to be similarly annealed. We found that when \u03b22 = 0, CoarsenConf still learned to improve upon the aligned MSE loss by 50%, as compared to RDKit. Our error analysis showed that the resulting molecules either had extremely low distance error with high MSE, or vice-versa. Therefore, when the learning objective is unconstrained, our model learns to violate distance constraints by placing atoms in low-error but unphysical positions.\nFor QM9, by slowly annealing the distance loss, we allow our model to reach a metaphysical unstable transition state where distances are violated, but the aligned coordinate error is better. We then force the model to respect distance constraints. In the case of DRUGS, we found that this transition state was too difficult for the model to escape from, and we report the results using \u03b22 = 0.5 in Tab. 1. In Appendix \u00a7J, we further explore this idea and experiment with different annealing schedules for DRUGS. We note that as CoarsenConf learns the torsion angles in an unsupervised manner because of the chosen CG strategy, we leave explicit angle optimization to future work."
        },
        {
            "heading": "C Coarse-graining",
            "text": "We elaborate on the coarse-graining procedure introduced in \u00a73. Following Wang et al. [6], we represent fine-grained (FG) molecular conformers as x = {xi}ni=1 \u2208 Rn\u00d73. Similarly, the coarse-grained (CG) conformers are represented by X = {XI}NI=1 \u2208 RN\u00d73 where N < n. Let [n] and [N ] denote the set {1, 2, ..., n} and {1, 2, ..., N} respectively. The CG operation can be defined as an assignment m : [n] \u2192 [N ], which maps each FG atom i in [n] to CG bead I \u2208 [N ], i.e., bead I is composed of the set of atoms CI = (k \u2208 n | m(k) = I). XI is initialized at the center of mass = 1|CI | \u2211 j\u2208CI xj .\nWe note that CoarsenConf coarsens input molecules by first severing all torsion angles \u03c4abcd, with k torsion angles resulting in k + 1 connected components or CG beads. This allows us, on average, to represent QM9 molecules with three beads and large drug molecules (n > 100) with 29 beads. We opted for a torsion angle-based strategy as it allows for unsupervised control over torsion angles, as well as the ability to rotate each subgraph independently. The CG strategy can be altered for various applications going forward."
        },
        {
            "heading": "D Encoder Equations",
            "text": "D.1 Fine-grain Module We describe the encoder, shown in Fig. 2(I). The model operates over SE(3)-invariant atom features h \u2208 Rn\u00d7D, and SE(3)-equivariant atomistic coordinates x \u2208 Rn\u00d73. A single encoder layer is composed of three modules: finegrained, pooling, and coarse-grained. Full equations for each module can be found in Appendix \u00a7D.1, \u00a7D.2, \u00a7D.3, respectively.\nThe fine-grained module is a graph-matching message-passing architecture. It differs from St\u00e4rk et al. [46] by not having internal closed-form distance regularization and exclusively using unidirectional attention. It aims to effectively match the approximate conformer and ground truth by updating attention from the former to the latter.\nThe FG module is responsible for processing the FG atom coordinates and invariant features. More formally, the FG is defined as follows:\nmj\u2192i = \u03d5 e(h (t) i ,h (t) j , \u2225x (t) i \u2212 x (t) j \u2225 2,fj\u2192i),\u2200(I, J) \u2208 E \u222a E\u2032,\nuj\u2032\u2192i = aj\u2032\u2192iWh (t) j\u2032 ,\u2200i \u2208 V, j \u2032 \u2208 V\u2032,\nmi = 1 |N(i)| \u2211 j\u2208N(i) mj\u2192i,\u2200i \u2208 V \u222a V\u2032,\nui = \u2211 j\u2032\u2208V\u2032 uj\u2032\u2192i,\u2200i \u2208 V, and u\u2032i = 0,\nx (t+1) i = \u03b7x \u00b7 x (0) i + (1\u2212 \u03b7x) \u00b7 x (t) i + \u2211 j\u2208N(i) (x (t) i \u2212 x (t) j )\u03d5 x(mj\u2192i),\nh (t+1) i = (1\u2212 \u03b7h) \u00b7 h (t) i + \u03b7h \u00b7 \u03d5 h(h (t) i ,mi,ui,fi),\u2200i \u2208 V \u222a V \u2032,\n(6)\nwhere f represents the original invariant node features ht=0, aj\u2192i are SE(3)-invariant attention coefficients derived from h embeddings, N(i) are the graph neighbors of node i, and W is a parameter matrix. (V,E) and (V\u2032,E\u2032) refer to the low-energy and RDKit approximation molecular graphs, respectively. The various \u03d5 functions are modeled using shallow MLPs, with \u03d5x outputting a scalar and \u03d5e and \u03d5h returning a D-dimensional vector. \u03b7x and \u03b7h are weighted update parameters for the FG coordinates x and invariant features h respectively. We note that attention flows in a single direction from the RDKit approximation to the ground truth to prevent leakage in the parameterization of the learned prior distribution.\nD.2 Pooling Module The pooling module takes in the updated representations (h and x) of both the ground truth molecule and the RDKit reference from the FG module. The pooling module is similar to the FG module, except it no longer uses attention and operates over a pooling graph. Given a molecule with n atoms and N CG beads, the pooling graph consists of n+N nodes. There is a single directional edge from all atoms to their respective beads. This allows message passing to propagate information through the predefined coarsening strategy.\nThe pooling module is responsible for learning the coordinates and invariant features of each coarse-grained bead by pooling FG information in a graph-matching framework. More formally, the pooling module is defined as follows:\nmj\u2192I = \u03d5 e(H (t) I ,h (t) j , \u2225X (t) I \u2212 x (t) j \u2225 2,fj\u2192I),\u2200(I, J) \u2208 E \u222a E\u2032,\nmI = 1 |N(I)| \u2211\nj\u2208N(I)\nmj\u2192I ,\u2200I \u2208 V \u222a V\u2032,\nX (t+1) I = \u03b7X \u00b7X (0) I + (1\u2212 \u03b7X) \u00b7X (t) I + \u2211 j\u2208N(I) (X (t) I \u2212 x (t) j )\u03d5 x(mj\u2192I),\nH (t+1) I = (1\u2212 \u03b7H) \u00b7H (t) I + \u03b7H \u00b7 \u03d5 h(H (t) I ,mI ,fI),\u2200I \u2208 V \u222a V \u2032,\n(7)\nwhere capital letters refer to the CG representation of the pooling graph. The pooling module mimics the FG module without attention on a pooling graph, as seen in Fig. 6(II). The pooling graph contains a single node for each atom and CG bead, with a single edge from each FG atom to its corresponding bead. It is used to learn the appropriate representations of the CG information. As the pooling graph only contains edges from fine-to-coarse nodes, the fine-grain coordinates and features remain unchanged. The pooling graph at layer t uses the invariant feature H from the CG module of layer t\u2212 1 to propagate information forward through the neural network. The main function of the pooling module is to act as a buffer between the FG and CG spaces. As a result, we found integrating the updated CG representation useful for building a better transition from FG to CG space.\nD.3 Coarse-grain Module The coarse-grained module uses the updated CG representations (H \u2208 RN\u00d7D and X \u2208 RN\u00d73) from the pooling module to learn equivariant CG features (Z and Z\u0303 \u2208 RN\u00d7F\u00d73) for the ground truth molecule and the RDKit reference. F is fixed as a hyperparameter for latent space size. N is allowed to be variable-length to handle molecules resulting from any coarsening procedure. The CG features are learned using a graph-matching point convolution [47] with similar unidirectional attention as the FG module. Prior to the main message-passing operations, the input features undergo equivariant mixing [39] to further distill geometric information into the learned CG representation.\nThe CG module is responsible for taking the pooled CG representation from the pooling module and learning a node-level equivariant latent representation. We note that we use simple scalar and vector operations to mix equivariant and invariant features without relying on computationally expensive higher-order tensor products. In the first step, invariant CG features H and equivariant features v \u2208 RF\u00d73 are transformed and mixed to construct new expressive intermediate features H \u2032, H \u2032\u2032,v\u2032 by,\nH \u2032I = \u03d51(h (t) I , \u2225VN-MLP1(v (t) I )\u2225) \u2208 R D, (8a) H \u2032\u2032I = \u03d52(h (t) I , \u2225VN-MLP2(v (t) I )\u2225) \u2208 R F , (8b)\nv\u2032I = diag{\u03d53(H (t) I )} \u00b7 VN-MLP3(v (t) I ) \u2208 R F\u00d73. (8c)\nNext, a point convolution [47, 44, 39] is applied to linearly transform the mixed features H \u2032, H \u2032\u2032,v\u2032 into messages:\nmHI\u2190J = Ker1(\u2225rI,J\u2225)\u2299H \u2032J , (9a) mvI\u2190J = diag {Ker2(\u2225rI,J\u2225)} \u00b7 v\u2032J + ( Ker3(\u2225rI,J\u2225)\u2299H \u2032\u2032J ) \u00b7 r\u22a4I,J , (9b)\nuJ\u2032\u2192I = aJ\u2032\u2192IWH (t) J\u2032 ,\u2200I \u2208 V, J \u2032 \u2208 V\u2032, (9c) uI = \u2211 J\u2032\u2208V\u2032 uJ\u2032\u2192I ,\u2200I \u2208 V, and u\u2032I = 0, (9d)\nHt+1I = (1\u2212 \u03b7H) \u00b7H \u2113 I + \u03b7H \u00b7 MLP(H\u2113I , \u2211 J\u2208N(I) mHI\u2190J , uI),\u2200I \u2208 V \u222a V\u2032, (9e)\nvt+1I = (1\u2212 \u03b7v) \u00b7 v \u2113 I + \u03b7v \u00b7 VN-MLP4(v\u2113I , \u2211 J\u2208N(I) mvI\u2190J),\u2200I \u2208 V \u222a V\u2032, (9f)\nwhere each Ker refers to a learned RBF kernel, rIJ is the difference between XI and XJ , and aJ\u2192I are SE(3)-invariant attention coefficients derived from the learned invariant features H . \u03b7H and \u03b7v control the mixing of the learned invariant and equivariant representations.\nWe note that for t > 0, the HI from the CG module are used in the next layer\u2019s pooling module, creating a cyclic dependency to learn an information-rich CG representation. This is shown by the dashed lines in Fig. 2(I). The cyclic flow of information grounds the learned CG representation to the innate FG structure. All equivariant CG features v are initialized as zero and are slowly built up through each message passing layer. As point convolutions and VN operations are strictly SO(3)-equivariant, we subtract the molecule\u2019s centroid from the atomic coordinates prior to encoding, making it effectively SE(3)-equivariant.\nThe modules in each encoder layer communicate with the respective module of the previous layer. This hierarchical message-passing scheme results in an informative and geometrically grounded final CG latent representation. We note that the pooling module of layer \u2113 uses the updated invariant features H from the CG module of layer \u2113\u2212 1, as shown by the dashed lines in Fig. 2(I)."
        },
        {
            "heading": "E Decoder Architecture",
            "text": "We sample from the learned posterior (training) and learned prior (inference) to get Z = \u00b5 + \u03f5\u03c3, where \u03f5 is noise sampled from a standard Gaussian distribution as the input to the decoder. We note the role of the decoder is two-fold. The first is to convert the latent coarsened representation back into FG space through a process we call channel selection. The second is to refine the fine-grain representation autoregressively to generate the final low-energy coordinates.\nChannel Selection. To explicitly handle all choices of coarse-graining techniques, our model performs variable-length backmapping. This aspect is crucial because every molecule can be coarsened into a different number of beads, and there is no explicit limit to the number of atoms a single bead can represent. Unlike CGVAE [6], which requires training a separate model for each choice in granularity N , CoarsenConf is capable of reconstructing FG coordinates from any N (illustrated in Fig. 2(III)).\nCGVAE defines the process of channel selection as selecting the top k latent channels, where k is the number of atoms in a CG bead of interest. Instead of discarding all learned information in the remaining F \u2212 k channels in the latent representation, we use a novel aggregated attention mechanism. This mechanism learns the optimal mixing of channels to reconstruct the FG coordinates and is illustrated in Fig. 3. The attention operation allows us to actively query our latent representation for the number of atoms we need, and draw upon similarities to the learned RDKit approximation that has been distilled into the latent space through the encoding process. Channel selection translates the CG latent tensor Z \u2208 RN\u00d7F\u00d73 into FG coordinates xcs \u2208 Rn\u00d73.\nCoordinate Refinement. Once channel selection is complete, we have effectively translated the variable-length CG representation back into the desired FG form. From here, xcs is grouped into its corresponding CG beads but left in FG\ncoordinates to do a bead-wise autoregressive generation of final low-energy coordinates (Fig. 2(IV)). As there is no intrinsic ordering of subgraphs, we use a breadth-first search that prioritizes larger subgraphs with large out-degrees. In other words, we generate a linear order that focuses on the largest, most connected subgraphs and works outward. We believe that by focusing on the most central component first, which occupies the most 3D volume, we can reduce the propagation of error that is typically observed in autoregressive approaches. We stress that by coarse-graining by torsion angle connectivity, our model learns the optimal torsion angles in an unsupervised manner, as the conditional input to the decoder is not aligned. CoarsenConf ensures each next generated subgraph is rotated properly to achieve a low coordinate and distance error.\nLearning the Optimal Distortion. The decoder architecture is similar to the EGNN-based FG layer in the encoder. However, it differs in two important ways. First, we mix the conditional coordinates with the invariant atom features using a similar procedure as in the CG layer instead of typical graph matching. Second, we learn to predict the difference between the RDKit reference and ground truth conformations. This provides an upper error bound and enables us to leverage easy-to-obtain approximations more effectively.\nMore formally, a single decoder layer is defined as follows:\n\u00b5(t) = 1 |Vprev| \u2211\nk\u2208Vprev\nxk, (10a)\nh\u0303i = \u03d5 m(h (t) i ,x (t) i ,\u00b5 (t), \u2225x(t)i \u2212 \u00b5 (t)\u22252),\u2200i \u2208 Vcur, (10b)\nmj\u2192i = \u03d5 e(h\u0303 (t) i , h\u0303 (t) j , \u2225x (t) i \u2212 x (t) j \u2225 2, \u2225x(t)i \u2212 x (t) ref,j\u2225 2, \u2225x(t)i \u2212 x (t) ref,i\u2225 2),\u2200(i, j) \u2208 Ecur, (10c)\nmi = 1 |N(i)| \u2211 j\u2208N(i) mj\u2192i,\u2200i \u2208 Vcur, (10d)\nuj\u2032\u2192i = aj\u2032\u2192iWh (t) j\u2032 ,\u2200i \u2208 Vcur, j \u2032 \u2208 Vprev, (10e) ui = \u2211\nj\u2032\u2208Vprev\nuj\u2032\u2192i,\u2200i \u2208 Vcur, (10f)\nx (t+1) i = x (t) ref,i + \u2211 j\u2208N(i) (x (t) i \u2212 x (t) j )\u03d5 x(mj\u2192i),\u2200i \u2208 Vcur, (10g)\nh (t+1) i = (1\u2212 \u03b2) \u00b7 h (t) i + \u03b2 \u00b7 \u03d5 h(h\u0303 (t) i ,mi,ui,fi),\u2200i \u2208 Vcur, (10h)\nwhere (Vcur, Ecur) and (Vprev, Eprev) refer to the subgraph currently being generated and the set of all previously generated subgraphs, i.e., the current state of the molecule. \u03d5m, \u03d5e, \u03d5x, and \u03d5h refer to separate shallow MLPs for the feature mixing, edge message calculation, coordinate update, and invariant feature update, respectively. Eq. 10(a-b) creates a mixed feature for each atom consisting of the current FG invariant feature and 3D position vectors (h and x), and the previous centroid \u00b5 and respective centroid distances. Eq. 10(c-d) defines the message passing operation that uses the aforementioned mixed features h\u0303 and a series of important distances between the model-based conformer and RDKit reference.Eq. 10(e-f) apply the same unidirectional attention updates seen in the encoder architecture. Eq. 10(g-h) update the position and feature vector for each atom using the above messages and attention coefficients, with f representing the original invariant node features h\u2113=0 and \u03b2 a weighted update parameter. We emphasize that Eq. 10(g) formulates the overall objective as learning the optimal distortion of the RDKit reference to achieve the low-energy position i.e., x\u2217 = xref +\u2206x. The CG autoregressive strategy allows CoarsenConf to handle extremely large molecules efficiently, as the max number of time steps is equal to the max number of CG beads. CoarsenConf is trained using teacher forcing [48], which enables an explicit mixing of low-energy coordinates with the current FG positions from channel selection Eq. 10(a-b)."
        },
        {
            "heading": "F Model Configuration",
            "text": "Model. We present the model configuration that was used to generate the results in \u00a74.1 - \u00a74.4. Overall, the default model has 1.9M parameters: 1.6M for the encoder and 300K for the decoder. We note that as CoarsenConf uses graph\nmatching, half the encoder parameters are used for each of the two inputs representing the same molecule in different spatial orientations. For both the encoder and decoder, we use five message-passing layers, a learning rate of 1e\u22123 with an 80% step reduction after each epoch, and a latent space channel dimension (F ) of 32. All other architectural parameters, such as feature mixing ratios or nonlinearities, were set following similar architectures [39, 45, 46]. We present further ablations in Appendix \u00a7J. We note that the ability to share weights between the inputs as well as between each layer in the encoder is left as a hyperparameter. This could allow the encoder to see a 2x or 5x reduction in model size, respectively.\nCompute. The QM9 model was trained and validated for five epochs in 15 hours using a single 40GB A100 GPU. We used a batch size of 600, where a single input refers to two graphs: the ground truth and RDKit approximate conformer. The DRUGs model was trained and validated for five epochs in 50 hours using distributed data-parallel (DDP) with 4 40GB A100 GPUs with a batch size of 300 on each GPU. For DRUGs, the GPU utilization was, on average, 66% as few batches contain very large molecules. In the future, lower run times can be achieved if the large molecules are more intuitively spaced out in each batch.\nWe note DDP has a negative effect on overall model benchmark performance due to the gradient synchronization but was used due to compute constraints. Without DDP, we expect the training time to take around 7 days, which is on par with Torsional Diffusion (4-11 days). We demonstrated that CoarsenConf achieves as good or better results than prior methods with less data and time, and these results can be further optimized in future work. We provide evidence of the negative effects of DDP in Appendix \u00a7J.\nOptimal Transport reduces compute requirements. The optimal transport (OT) models were trained on 2 epochs on a single A6000 GPU for 8 and 15 hours total for QM9 and DRUGS, respectively. For OT details, see Appendix Eq. 11. Here, both models use the first 5 ground truth conformers. In real-world applications like polymer design, the availability of data is frequently limited and accompanied by a scarcity of conformers for each molecule. The current datasets, QM9 and DRUGS, do not mimic this setting very well. For example, on average, QM9 has 15 conformers per molecule, and DRUGS has 104 per molecule\u2014both datasets have significantly more conformers than in an experimental drug design setting. Given this, rather than training on the first 30 conformers as done in Torsional Diffusion, we train on the first five (typically those with the largest Boltzmann weight) for QM9 and DRUGS, respectively."
        },
        {
            "heading": "G RDKit Approximate Conformers",
            "text": "Generating Approximate Conformers. For CoarsenConf\u2019s initial conditional approximations, we only use RDKit + MMFF when it can converge (~90% and ~40% convergence for QM9 and DRUGS, respectively). We emphasize that RDKit only throws an error when MMFF is not possible but often returns structures with a non-zero return code, which signifies incomplete and potentially inaccurate optimizations. Therefore, in generating the RDKit structures for training and evaluation, we filter for MMFF converged structures. We default to the base EKTDG-produced structures when either the optimization cannot converge, or MMFF does not yield enough unique conformers. CoarsenConf ultimately offers a solution that can effectively learn from traditional cheminformatics methods. This aspect of MMFF convergence has not been discussed in prior ML for MCG methods, and we leave it to future cheminformatics research to learn the causes and implications of incomplete optimizations.\nEliminating distribution shift with explicit conditioning. Both CoarsenConf and TD optimize p(X|R) but utilize the RDKit approximations R in different ways. TD learns to update the torsion angles of R, while CoarsenConf leverages CG information to inform geometric updates (coordinates, distances, and torsion angles) to translate R to X . Unlike TD, which uses a preprocessing optimization procedure to generate substitute ground truth conformers that mimic p(R), CoarsenConf directly learns from both X and R through its hierarchical graph matching procedure. This directly addresses the distributional shift problem. We hypothesize that this, along with our angle-based CG strategy, leads to our observed improvements. Overall, CoarsenConf provides a comprehensive framework for accurate conformer generation that can be directly applied for downstream tasks such as oracle-based protein docking."
        },
        {
            "heading": "H GEOM Benchmark Discussion",
            "text": "xTB energy and property prediction. We note the issues surrounding the RMSD metrics have always existed, and prior MCG methods have introduced energy-based benchmarks that we describe and report in Tab. 2. We note these energies are calculated with xTB, and thus are not very accurate compared to density functional theory (DFT), as it is limited by the level of theory used to produce the energies further discussed in [7]. Therefore, since current benchmarks mainly focus on gauging the effectiveness of the machine learning objective and less on the chemical feasibility and downstream use of the generated conformers, we use oracle-based protein docking-based to evaluate conformer quality on downstream tasks. These evaluations are highly informative, as molecular docking is a crucial step in the drug discovery process, as it helps researchers identify potential drug candidates and understand how they interact with their target proteins. The combination of RMSD, xTB energy, and downstream docking tasks presents a more comprehensive evaluation of generated conformers."
        },
        {
            "heading": "I QM9 Experimental Details",
            "text": "Both CoarsenConf and CoarsenConf-OT were trained on 5 conformers per ground truth molecule, compared to Torsional Diffusion\u2019s 30. We hypothesize that since CoarsenConf uses a one-to-one loss function, we are able to maintain high recall, whereas the OT model finds an optimal matching that focuses on precision. By adding more ground truth conformers, we hypothesize our model can better cover the true conformer space, improving recall, as the OT setting would not be as biased toward precision."
        },
        {
            "heading": "J DRUGS Extended Benchmarks",
            "text": "Evaluation Details. All models in Tab. 1 were benchmarked with Torsional Diffusion\u2019s (TD) evaluation code and retrained if generated molecules were not public (using their public instructions). We note that TD uses higher-order tensor products to maintain equivariance (\u2113 = 2). In contrast, GeoMol, GeoDiff, and CoarsenConf use scalar-vector operations that are theoretically analogous to \u2113 = 1. CoarsenConf-OT uses an optimal transport (OT) loss with the same decoder architecture as in Fig. 2, but is no longer autoregressive. GeoDiff\u2019s code would not load, so we were able to evaluate the GeoDiff generated DRUGS molecules from the Torsional Diffusion authors\u2019 evaluation on the same test set.\nWe copy the results from Tab. 1 and provide additional results, including TD for rotation order \u2113 = 1, OMEGA [49], and RDKit. This allows for a closer comparison to the scalar and vector operations that CoarsenConf employs to maintain equivariance. Using a lower rotation order results in slightly worse results in nearly all categories. We further discuss the implications of the choice in equivariant representation in Appendix \u00a7L.\nOptimal Transport. In practice, our model generates a set of conformers, {Ck}k\u2208[1..K], that needs to match a variable-length set of low-energy ground truth conformers, {C\u2217l }l\u2208[1..L]. In our case, the number L of true conformers, or the matching between generated and true conformers is not known upfront. For these reasons, we introduce an optimal transport-based, minimization-only, loss function [26]:\nLOT = min T\u2208QK,L \u2211 k,l TklL(Ck,C \u2217 l ),\nL(Ck,C \u2217 l ) = MSE(Ck,C \u2217 l ) + distance error(Ck,C \u2217 l ),\n(11)\nwhere T is the transport plan satisfying QK,L = {T \u2208 RK\u00d7L+ : T1L = 1K1K ,T T1K = 1 L1L}. The minimization w.r.t. T is computed quickly using the Earth Mover Distance and the POT library [50]. As the OT loss focuses more on finding the optimal mapping from generated conformers to ground truth reference, we removed the autoregressive decoding path of CoarsenConf and replaced it with a single pass with the same decoder architecture. The underlying loss function, which is tasked to minimize MSE coordinate error, and interatomic distance error is the same in both (Eq. 5), the autoregressive (AR) and non-AR OT-based loss functions. The OT version additionally finds the optimal mapping between the generated and ground truth structures, which better aligns with the AMR and Coverage benchmarks.\nHyperparameter Ablations. We experimented with increasing the latent channels (F ) from 32 to 64 and 128, and introducing a step-wise distance loss and KL regularization annealing schedule, as done in the QM9 experiments. Both these experiments resulted in slightly worse performance when limited to 2 conformers per training molecule. We hypothesize that due to the DRUGs molecules being much larger than those in QM9, more training may be necessary, and a more sensitive annealing schedule may be required.\nGEOM-DRUGS Recall Results. Fig. 7 demonstrates extensive Precision and Recall results for a wide range of tested sampling budgets for GEOM-DRUGS. We see that only Precision is stable across nearly all values. Due to the extreme sensitivity of the Recall metric and little difference in model performance for reasonable sampling budgets, we focus on Precision for QM9 and DRUGS. We also note that while CoarsenConf-OT saw worse recall results for QM9, this was\nnot the case for DRUGS. In the case of DRUGS, CoarsenConf-OT achieves the learning objective of instilling force field optimizations as the lower error bound and does so with very little training and inference time."
        },
        {
            "heading": "K Oracle-based Protein Docking",
            "text": "We utilize the oracle-based protein docking task as molecules with higher affinity (more negative) have more potential for higher bioactivity, which is significant for real-world drug discovery. We use the CrossDocked2020 trainset consisting of 166000 protein-ligand interactions (2,358 unique proteins and 11,735 unique ligands) and its associated benchmarks, as it has been heavily used in Structure-based drug discovery as defined by [35, 34].\nThe CrossDocked2020 dataset is derived from PDBBind but uses smina [51], a derivative of AutoDock Vina with more explicit scoring control, to generate the protein-conditioned ligand structures to yield ground truth data. We note that based on the raw data, 2.2 billion conformer-protein interactions are possible, but we filtered out any ground-truth example that AutoDock Vina failed to score. Furthermore, in the TDC oracle-based task, each ligand is known to fit well in the given protein. CrossDocked2020, on the other hand, consists of various ligand-protein interactions, not all of which are optimal, making the overall task more difficult.\nWe note that while it takes on the orders of hours to generate 1.2M conformers (100 conformers per molecule), it takes on the orders of ~weeks to months to score each conformer for up to the 2,358 unique proteins for each evaluated method (evaluation time is 100x the time to score the ground truth data as we generate 100 conformers per molecule). As a result, we report the results for the first 100,000 conformer-protein interactions."
        },
        {
            "heading": "L Limitations",
            "text": "As demonstrated in \u00a74.1-\u00a74.4, CoarsenConf significantly improves the accuracy and reduces the overall data usage and runtime for conformer generation. However, CoarsenConf also has some limitations that we will discuss in this section.\nAutoregressive generation. While CoarsenConf improves accuracy with reduced training time and overall data, autoregressive generation is the main bottleneck in inference time. We linearize the input molecule based on spatially significant subgraphs and then process each one autoregressively. For a model with k torsion angles, we need k + 1 passes through our decoder. Coarse-graining is an effective strategy to reduce the number of decoder passes compared to traditional atom-wise autoregressive modeling. For example, for a given molecule, the number of torsion angles (which we use to coarse-grain) is significantly less than the number of atoms. Our choice of coarse-graining strategy allows us to break the problem into more manageable subunits, making autoregressive modeling a useful strategy, as it provides greater flexibility and control by allowing conditional dependence. CoarsenConf is a good example of the trade-offs that exist between generative flexibility and speed. We target this limitation by introducing a non-autoregressive version with an optimal transport loss. We see this improves the overall GEOM results, at the slight cost of a higher right tail of the error distribution.\nOptimal Transport. While CoarsenConf-OT trained with a non-autoregressive decoder with an optimal transport loss significantly outperforms prior methods and accomplishes the goal of effectively learning from traditional cheminformatics methods, the recall results still have room for improvement, especially for QM9. While CoarsenConf (no OT) achieves competitive results, we believe that continuing to focus on how to better integrate physics and cheminformatics into machine learning will be crucial for improving downstream performance. Due to the above concerns, we chose to evaluate our non-OT model on the property prediction and protein docking tasks, as wanted to use our best model denoted by the lowest overall RMSD error distribution.\nApproximate structure error. The success of learning the optimal distortion between low-energy and RDKit approximate structure depends on having reasonable approximations. While CoarsenConf relaxes the rigid local structure assumption of Torsional Diffusion in a way that leverages the torsional flexibility in molecular structures, it still depends on an approximate structure. This is a non-issue in some instances, as RDKit does well. In more experimental cases for larger systems, the RDKit errors may be too significant to overcome. We emphasize that the underlying framework of CoarsenConf is adjustable and can learn from scratch, not only the distortion from approximate RDKit structures. In some cases, this may be more appropriate if the approximations have particularly high error. We leave to future work to explore the balance between the approximation error and the inductive bias of learning from approximate structures, as well as methods to maintain flexibility while avoiding the issues of conditioning on out-of-distribution poor approximations.\nEquivariance. As CoarsenConf uses the EGNN [43] framework as its equivariant backbone and thus only scalar and vector operations, there is no simple way to incorporate higher-order tensors. As the value of using higher-order tensors is still actively being explored, and in some cases, the costs outweigh the benefits, we used simple scalar and vector operations and avoided expensive tensor products. We leave exploring the use of higher-order equivariant representations to future work, as it is still an ongoing research effort [52]."
        }
    ],
    "title": "CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation",
    "year": 2023
}