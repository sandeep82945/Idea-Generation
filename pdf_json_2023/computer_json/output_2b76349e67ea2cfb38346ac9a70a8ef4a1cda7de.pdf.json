{
    "abstractText": "In the past years, the application of neural networks as an alternative to classical numerical methods to solve Partial Differential Equations has emerged as a potential paradigm shift in this century-old mathematical field. However, in terms of practical applicability, computational cost remains a substantial bottleneck. Classical approaches try to mitigate this challenge by limiting the spatial resolution on which the PDEs are defined. For neural PDE solvers, we can do better: Here, we investigate the potential of stateof-the-art quantization methods on reducing computational costs. We show that quantizing the network weights and activations can successfully lower the computational cost of inference while maintaining performance. Our results on four standard PDE datasets and three network architectures show that quantization-aware training works across settings and three orders of FLOPs magnitudes. Finally, we empirically demonstrate that Pareto-optimality of computational cost vs performance is almost always achieved only by incorporating quantization.",
    "authors": [
        {
            "affiliations": [],
            "name": "Winfried van den Dool"
        },
        {
            "affiliations": [],
            "name": "Tijmen Blankevoort"
        },
        {
            "affiliations": [],
            "name": "Max Welling"
        },
        {
            "affiliations": [],
            "name": "Yuki M. Asano"
        }
    ],
    "id": "SP:8111419e1418a0e8917170352b44f49ff34075de",
    "references": [
        {
            "authors": [
                "Peter Bauer",
                "Alan Thorpe",
                "Gilbert Brunet"
            ],
            "title": "The quiet revolution of numerical weather prediction",
            "venue": "Nature, 525(7567):47\u201355,",
            "year": 2015
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv e-prints, pages arXiv\u2013",
            "year": 2013
        },
        {
            "authors": [
                "Kaifeng Bi",
                "Lingxi Xie",
                "Hengheng Zhang",
                "Xin Chen",
                "Xiaotao Gu",
                "Qi Tian"
            ],
            "title": "Pangu-weather: A 3d high-resolution model for fast and accurate global weather forecast",
            "venue": "arXiv e-prints, pages",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Rianne van den Berg",
                "Max Welling",
                "Jayesh K Gupta"
            ],
            "title": "Clifford neural layers for pde modeling",
            "venue": "arXiv e-prints, pages",
            "year": 2022
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Daniel E Worrall",
                "Max Welling"
            ],
            "title": "Message passing neural pde solvers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Shuhao Cao"
            ],
            "title": "Choose a transformer: Fourier or galerkin",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "L. Espeholt",
                "S. Agrawal"
            ],
            "title": "S\u00f8nderby. Deep learning for twelve hour precipitation forecasts",
            "venue": "Nat Commun,",
            "year": 2022
        },
        {
            "authors": [
                "Steven K Esser",
                "Jeffrey L McKinstry",
                "Deepika Bablani",
                "Rathinakumar Appuswamy",
                "Dharmendra S Modha"
            ],
            "title": "Learned step size quantization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jayesh K. Gupta",
                "Johannes Brandstetter"
            ],
            "title": "Towards multispatiotemporal-scale generalized pde modeling, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Shudong Huang",
                "Wentao Feng",
                "Chenwei Tang",
                "Jiancheng Lv"
            ],
            "title": "Partial differential equations meet deep neural networks: A survey",
            "venue": "arXiv e-prints, pages",
            "year": 2022
        },
        {
            "authors": [
                "C.E. Iles",
                "R. Vautard",
                "J. Strachan",
                "S. Joussaume",
                "B.R. Eggen",
                "C.D. Hewitt"
            ],
            "title": "The benefits of increasing resolution in global and regional climate simulations for european climate extremes",
            "venue": "Geoscientific Model Development,",
            "year": 2020
        },
        {
            "authors": [
                "Benoit Jacob",
                "Skirmantas Kligys",
                "Bo Chen",
                "Menglong Zhu",
                "Matthew Tang",
                "Andrew Howard",
                "Hartwig Adam",
                "Dmitry"
            ],
            "title": "Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Sambhav Jain",
                "Albert Gural",
                "Michael Wu",
                "Chris Dick"
            ],
            "title": "Trained quantization thresholds for accurate and efficient fixed-point inference of deep neural networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tom Kimpson",
                "E. Adam Paxton",
                "Matthew Chantry",
                "Tim Palmer"
            ],
            "title": "Climate-change modelling at reduced floating-point precision with stochastic rounding",
            "venue": "Quarterly Journal of the Royal Meteorological Society,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv e-prints, pages arXiv\u20131412,",
            "year": 2014
        },
        {
            "authors": [
                "Thorsten Kurth",
                "Shashank Subramanian",
                "Peter Harrington",
                "Jaideep Pathak",
                "Morteza Mardani",
                "David Hall",
                "Andrea Miele",
                "Karthik Kashinath",
                "Anima Anandkumar"
            ],
            "title": "Fourcastnet: Accelerating global high-resolution weather forecasting using adaptive fourier neural operators",
            "venue": "In Proceedings of the Platform for Advanced Scientific Computing Conference,",
            "year": 2023
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Borislavov Kovachki",
                "Kamyar Azizzadenesheli",
                "Kaushik Bhattacharya",
                "Andrew Stuart",
                "Anima Anandkumar"
            ],
            "title": "Fourier neural operator for parametric partial differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: Stochastic gradient descent with warm restarts",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Philipp Neumann",
                "Peter D\u00fcben",
                "Panagiotis Adamidis",
                "Peter Bauer",
                "Matthias Br\u00fcck",
                "Luis Kornblueh",
                "Daniel Klocke",
                "Bjorn Stevens",
                "Nils Wedi",
                "Joachim Biercamp"
            ],
            "title": "Assessing the scales in numerical weather and climate predictions: will exascale be the rescue",
            "venue": "Philosophical Transactions of the Royal Society A,",
            "year": 2019
        },
        {
            "authors": [
                "Maziar Raissi",
                "Paris Perdikaris",
                "George E Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computational physics,",
            "year": 2019
        },
        {
            "authors": [
                "David A Randall",
                "Richard A Wood",
                "Sandrine Bony",
                "Robert Colman",
                "Thierry Fichefet",
                "John Fyfe",
                "Vladimir Kattsov",
                "Andrew Pitman",
                "Jagadish Shukla",
                "Jayaraman Srinivasan"
            ],
            "title": "Climate models and their evaluation",
            "year": 2007
        },
        {
            "authors": [
                "Jeff Rasley",
                "Samyam Rajbhandari",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD",
            "year": 2020
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "Unet: Convolutional networks for biomedical image segmentation",
            "venue": "18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Tapio Schneider",
                "Jo\u00e3o Teixeira",
                "Christopher S Bretherton",
                "Florent Brient",
                "Kyle G Pressel",
                "Christoph Sch\u00e4r",
                "A Pier Siebesma"
            ],
            "title": "Climate goals and computing the future of clouds",
            "venue": "Nature Climate Change,",
            "year": 2017
        },
        {
            "authors": [
                "Sangeetha Siddegowda",
                "Marios Fournarakis",
                "Markus Nagel",
                "Tijmen Blankevoort",
                "Chirag Patel",
                "Abhijit Khobare"
            ],
            "title": "Neural network quantization with ai model efficiency toolkit (aimet)",
            "venue": "arXiv e-prints, pages",
            "year": 2022
        },
        {
            "authors": [
                "J. Slingo",
                "P. Bates",
                "P. et al. Bauer"
            ],
            "title": "Ambitious partnership needed for reliable climate prediction",
            "venue": "Nat. Clim. Chang.,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "In many scientific fields, mathematical models of observed phenomena are expressed in terms of Partial Differential Equations, with the solution commonly represented by a function of a single time variable and one or more spatial variables. While many PDEs can be described compactly (e.g. the famous Schro\u0308dinger\u2019s equation from quantum physics, or Navier-Stokes\u2019 equations from fluid dynamics) it is usually impossible to write down explicitly the formulas for their exact solutions. Rather, there is a vast amount of scientific research on methods to numerically approximate solutions.\nWhen it comes to practical applications, computational cost and available resources play a significant role in this field of research. For most methods a natural trade-off occurs where one may use available resources to either speed\n\u2217an initiative of Qualcomm Technologies, Inc.\nup the computation, or increase the resolution on which the PDE is defined, leading to a slower but more accurate result. This latter strategy is of particular interest for PDEs that are highly non-linear and potentially exhibit chaotic behavior. Small eddies in a turbulent fluid may, for example, be overlooked when the spatial resolution is too low, leading to increasing errors in the solution function over time. One of the most important fields where computational resources form a bottleneck is climate prediction. The challenge of determining how many degrees the planet will warm over the next decades is a computational one, with supercomputers running massive ensemble-type methods. Insufficient resolution is quite often the main cause for sub-optimal modeling accuracy [21, 1, 16]. Indeed, over time climate models have become better for a large part simply because the resolution of the grid that they are defined on was allowed to increase, due to more efficient computation and more available resources [24, 26, 11].\nIt is precisely in this context of the computational cost of the traditional solvers that neural PDE solvers are becoming an interesting alternative. A single forward pass in a neural network is extremely fast compared to the iterative solving procedure of classical methods, and most of the operations are large matrix multiplications that can be easily handled by GPUs. While a neural network does need training first, possibly requiring data generated by classical solvers, its efficiency benefit comes from \u201crecycling\u201d: After training is complete, the neural network has learned to generalize to different initial conditions, whereas conventional solvers only solve the PDE for one specific configuration of initial conditions at a time. In practice, a deep learning weather model would have to be trained only once, but could then be applied every day to predict the next day\u2019s weather. Furthermore, when training data is sufficiently abundant in the real world, using that data directly comes with the added advantage that complex phenomena do not need to be first accurately modeled. Indeed, neural PDE solvers trained on real weather data alone have shown promising results [3, 7, 16].\nOur goal is not to improve on state-of-the-art models in the conventional sense. Rather, our starting point is to borrow from existing networks and architectures and focus on\nar X\niv :2\n30 8.\n07 35\n0v 1\n[ cs\n.L G\n] 1\n4 A\nug 2\n02 3\ntheir reducing their inference cost. With neural PDE solvers becoming mainstream for practical use, this measure becomes more important than test loss alone. Our contribution is in finding the optimal way to deal with the loss and computational cost trade-off. There are several orthogonal approaches to achieve lower computational costs. On the one hand, as in classical PDE solvers, the foremost way to make computation manageable is to reduce the resolution on which the PDE is defined. On the other hand, deep learning researchers have developed a variety of different techniques specifically for neural networks, most notably quantization, the focus of this work. Overall, we make the following contributions:\n\u2022 We provide an evaluation of 3 neural network-based PDE solvers [9, 17, 6] under 5 weight quantization scenarios. We use state-of-the-art quantization methods to provide exemplary benchmarks on 4 of the most common datasets.\n\u2022 We are the first to investigate both spatial resolution and model (weights and activations) resolution simultaneously as hyper-parameters in designing neural PDE-solvers. We develop a hybrid approach of applying quantization and modifying spatial resolution depending on the problem to achieve compute- and accuracy-optimal results.\n\u2022 We provide an extensive analysis of the trade-off between computational cost and errors for different quantized models and demonstrate that a certain level of quantization is almost always necessary to be Paretooptimal on the accuracy-cost curve."
        },
        {
            "heading": "2. Related Works",
            "text": "Neural PDE solvers. There are already many deep learning methods that aim to solve PDEs [6, 23, 9, 4, 5, 20, 10]. We base our research on the most common neural PDE surrogate architectures, namely the Fourier Neural Operator [17], UNet [23, 9] and a type of Transformer [6]. In terms of datasets, there are two recent larger-scale attempts at proper benchmarking of the field, namely PDEArena [9] and PDEBench [27]; from both of which we borrow our training and evaluation datasets.\nQuantization. With regards to quantization methods we rely on AIMET [25]1, which provides state-of-the-art quantization techniques. In particular, we utilize Quantization Aware Training [12, 13], utilizing the straight-through estimator [2] to approximate gradients of rounding operators. We allow quantization ranges to be trainable parameters as well, as in [8].\n1AIMET is a product of Qualcomm Innovation Center, Inc. (BSD-3)\nLow precision PDE solving. Recently, there has been increasing interest in running classical methods for solving climate models with lower precision as well. In [14] reduced precision of Float32 and Float16 has been used, as opposed to the standard precision of Float64. However, these solvers are based on traditional approaches, and not on neural networks.\nIn contrast to previous PDE surrogates, i.e. (deep learning) functions that aim to approximate the original PDE solution, we especially care for computational cost. While it is generally investigated that quantization decreases the accuracy of deep learning models as well as that standard PDE solvers\u2019 accuracy depends on their grid size, we explicitly compare both techniques in terms of how much they reduce computational costs. To make this possible we take a particular interest in the cost-loss trade-off of both techniques separately as well as their combined effects. Also, while a classification algorithm may exhibit a clear breaking point at which it can no longer accurately predict the majority of correct labels, the prediction of a PDE solution is measured in terms of (mean squared) errors between functions. As such there is no clear definition of what a \u201ccorrect\u201d prediction is, and consequentially there is no clear measure of when a PDE surrogate is successful. This means that a wide regime of models of varying qualities becomes interesting to investigate, both in the low-cost as well as the low-loss regions of the cost-loss trade-off."
        },
        {
            "heading": "3. Method",
            "text": ""
        },
        {
            "heading": "3.1. Theoretic background and notations",
            "text": "As we work with synthetic data, we are given numeric solutions to a given known PDE. Let X \u2282 Rd be a spatial domain and [0, T ] be a time window on which u : X \u00d7 [0, T ] \u2192 Rn is the solution to the partial differential equation\n\u2202u \u2202t = F\n( x,u, \u2202u\n\u2202x , \u22022u \u2202x2 , . . .\n) , (1)\nwith initial condition u0(x) at time t = 0 and boundary conditions defined by the operator B[u](t,x) = 0 when u is on the boundary \u2202X of the domain X . Here F can be any function, though we specify the precise form for different datasets in the Supplementary Information (SI). Assume that at some time t, u(t,x), as well as prior values of u, are known. Let \u03c4 > 0 be some step-size with t + \u03c4 < T . We can then try to predict one time step into the future by\nu(t+ \u03c4,x) = u(t,x) + \u222b t+\u03c4 t \u2202u(t,x) \u2202t dt. (2)\n\u2200t \u2208 [0, T \u2212 \u03c4 ],x \u2208 X\nGiven that theoretically the value of u(t,x) determines, through its spatial derivates and the PDE, all future values, a\nfirst attempt at defining a useful operator may be to replace the right-hand side of the above equation by an operator F\u03c4 such that\nu(t+ \u03c4,x) = F\u03c4 [u(t,x)] \u2200t \u2208 [0, T \u2212 \u03c4 ],x \u2208 X, (3)\nindeed ignoring values of u(t\u2032,x) for t\u2032 < t. We are then interested in approximating this operator F\u03c4 by a neural network G. However, in practice the spatial and time domains are discretized on regular grids X \u2282 X and T \u2282 [0, T ] with |X | = Nx and |T | = Nt. Crucially, when u(t,x) is defined on X rather than the full domain X , its spatial derivatives are no longer determined, breaking the connection from u(t,x) to \u2202u\u2202t through the PDE F . Consequentially, the full future cannot be exactly determined from u(x, t) defined on X alone. Note that even knowing the spatial derivatives exactly on the grid X at time t is not enough, as they are required at all later times as well if we are to solve the integral in equation 2. Now the values of u(x, t\u2032) for t\u2032 < t may still provide helpful information, and we may want to include those as inputs in our neural network approximation of F . One can think for example that such values may allow the backward difference approximation of \u2202u(t,x)/\u2202t as an alternative to using the PDE F with the approximated spatial derivatives.\nIncorporating multiple past time steps, the neural network approximation is given by the operator G, defined by\nu(tn + \u03c4,x) = G\u03c4 [(u(ti,x))ni=1] (4) for ti, tn + \u03c4 \u2208 T ,x \u2208 X .\nIn what follows we assume u to be defined on T \u00d7 X ."
        },
        {
            "heading": "3.2. Training setup",
            "text": "Let the true u(x, t) be defined everywhere on our grid T \u00d7 X . For each such trajectory u(x, t) in a mini-batch we first randomly select a starting time, after which we take a fixed set of subsequent input indices Tinput \u2282 T , and target indices, Ttarget \u2282 T . We have Nt \u2212 |Tinput| input steps available, but we are only training on |Ttarget| time steps. Therefore, to make full use of the available data we repeat each epoch \u2308(Nt \u2212 |Tinput|)/|Ttarget|\u2309 times, selecting new random indices every time so that in expectation the full dataset is used every epoch.\nFor |Ttarget| > 1 outputs we may use either temporal bundling (letting G output multiple subsequent time steps in one forward pass, see for instance [5]), or a recurrent approach, where the last outputs are fed to the network as new inputs, or a combination of both. Similar to [5] we may apply pushforward, meaning that we only backpropagate the loss through the last part of the target indices.\nLet u(t,x) be the true targets and u\u0302(t,x) the corresponding neural network predictions for (t,x) \u2208 Ttarget\u00d7X .\nThe loss, assuming no pushforward is applied, is then defined by\u2211\nx\u2208X \u2211 t\u2208Ttarget \u2211Nfields\ni=1 \u2225ui(x, t)\u2212 u\u0302i(x, t)\u222522 |X ||Ttarget|Nfields , (5)\nwhere Nfields is the dimensionality of u(t,x)."
        },
        {
            "heading": "3.3. Quantization",
            "text": "By default, the weights and activations of the neural network G are defined as floating point numbers using 32-bit precision. We can, however, store the weights and activations as integer values, to make (matrix) multiplications much more efficient. A floating-point weight matrix W can be expressed as a single scalar multiplied by a matrix of integer values, and a remainder term \u03f5.\nW = sW \u00b7Wint + \u03f5W\nSimilarly a vector of activations v may be expressed as\nv = sv \u00b7 (vint \u2212 zv) + \u03f5v,\nwhere we have added a zero-point zv to allow assymetric quantization of the activations. Matrix-vector multiplication now becomes:\nWv = (sW \u00b7Wint + \u03f5W )(sv \u00b7 (vint \u2212 zv) + \u03f5v) (6) = sW svWintvint \u2212 sW svWintzv + error terms\nAs the second term depends only on the weight matrix W and quantization parameters sW , sv and zv , it can be computed beforehand. If we then ignore the error terms, only the first term is remaining during inference: a matrix-vector multiplication of integer values. For a given bitwidth b, there are 2b possible integer values to choose from. Given a zero-point z and scale factor s, the quantization grid limits qmin and qmax are then determined by \u2212sz and s(2b\u22121\u2212z). Any values lying outside this range will be clipped to its limits, incurring a clipping error. The full quantization function q(\u00b7) is given by\nxint = q(x; sx, zx, b) = C (\u230a x\nsx\n\u2309 + zx; 0, 2 b \u2212 1 ) , (7)\nwhere \u230a\u00b7\u2309 is the round-to-nearest operator and C is a clamping function defined as:\nC(x; a, c) =  a, x < a\nx, a \u2264 x \u2264 c c, x > c\n(8)\nTo get a quantized network we start with a pre-trained floating point network and then optimize for the values of\ns and z that minimize the error terms that result from both clipping and rounding errors. This happens independently per layer, using arbitrary dummy data or training data in the forward pass. Next, we also apply Quantization Aware Training (QAT). This is a fine-tuning step, further training the quantized network using stochastic gradient descent on the original loss function. Training with quantized weights and activations is possible using the straight-through estimator [2]. Noting that the majority of the computational cost comes from (matrix) multiplications, we only quantize the inputs and weights of neural network layers that are based on matrix multiplications, and let the outputs (together with the biases) be accumulated in floating point format."
        },
        {
            "heading": "3.4. Changing resolutions",
            "text": "We change resolutions using bilinear interpolation for the 2D datasets and linear interpolation for the 1D datasets. We leave experimentation with learnable resolution operators for future research. However, we have chosen (bi)linear interpolation deliberately for its speed, which is the prime reason to change resolutions in the first place. If the network is defined on a different resolution than the data, we apply resolution-altering operators before and after it during both validation and training, thus also backpropagating losses through the resolution-altering operators."
        },
        {
            "heading": "3.5. Model cost computation",
            "text": "The proxy we use for model inference efficiency is based on counting multiplication and addition operations per layer. For a given quantized network module with M multiplication and A addition operations, and integer bitwidths of bw, ba for the weights and activations respectively, the cost is defined as\nM \u00b7 bw \u00b7 ba +A \u00b7 ba,\nwhere multiplications are considered between weights and inputs, and addition operations only apply to outputs. (We do not quantize bias vectors.) The full network cost is the sum of the cost over all layers. If a network works on a different resolution, we also take the costs of altering the resolution before and after the forward pass into account. Details on the number of multiplications and additions per layer are provided in the code. We have observed no increased losses from quantizing any parameter to bitwidth 16 under any circumstance, and therefore assume that all floating point operations can be harmlessly replaced by their Int16 counterpart. This enables us to measure the computational cost of non-quantized operations as if they were Int16 operations and to generally use fixed-point integer operations as a measure throughout all our comparisons of different model inference costs. To find the number of multiplication and addition operations in a given module we use [22],\nslightly adapted for our needs. The deepspeed model profiler lets you choose either FLOPs or MACs as a measure for module cost. We use the fact that one MAC operation consists of a single addition and multiplication and assume that the remaining operations are all addition-type in terms of complexity. Hence, when deepspeed outputs X FLOPs and Y MACs for a certain network module, we know that there are Y multiplications and X \u2212 Y remaining FLOPs that we consider additions. A few exceptions and special cases, as well as deliberate deviations from deepspeed, can be found in Appendix A.3."
        },
        {
            "heading": "4. Experiments",
            "text": "We use three popular neural-surrogate architectures: FNO, UNet and Transformer, applied to datasets based on 4 PDEs: Diffusion-Sorption (1D), Burgers\u2019 (1D), NavierStokes (2D) and Darcy (2D). The datasets are described in more detail in Appendix A.1. We show benchmark results of the original networks, as well as the results of the networks after quantization and scaling. For each result, we present the test loss as well as the computational cost of inference. We first compare quantization and reduction of spatial resolution as two orthogonal approaches to decrease computational cost, showing their impacts separately. Finally, we apply both methods simultaneously on higherresolution versions of the datasets."
        },
        {
            "heading": "4.1. Implementation",
            "text": "For a given spatial resolution, we first train the floating point network in a regular fashion, i.e., aiming for the lowest possible loss. We use Adam [15] with weight decay and a cosine annealing learning rate scheduler [18] with a linear warmup. Next, we quantize the network, first finding the best quantization parameters using AIMET\u2019s built-in optimization tool on 20% of the training data. Then we finetune the quantized network, training again using Adam and cosine annealing with linear warmup, but without weight decay, with a smaller learning rate, and for fewer epochs. Specific parameters for training and fine-tuning are provided in Appendix A.2, as well as the length of the input and output trajectories, i.e., the number of time steps used. If a model that operates on a different resolution than the original data resolution is unrolled several times, i.e., when training and testing on a longer output trajectory, the resolution is only altered after the first input, and after the output just before the loss is computed: intermediate values that are fed back into the network are kept in the network resolution. The quantization regimes used throughout the experiments are: [w4a4, w4a8, w8a8, w8a16], with wXaY refering to quantizing weights to bitwidth X and activations to bitwidth Y.2. We do not quantize the\n2We found it better to use a higher activation bitwidth than weight bitwidth when using different bitwidths for weights and activations\nfirst and last layer of each model, having observed that this improves performance with negligible impact on inference cost. Unless mentioned otherwise, we use scaling regimes, i.e., factors by which we scale input data resolution, of [0.7, 0.5, 0.3, 0.2]. For 2D data, we scale both dimensions by this factor, thus the actual number of spatial points depends quadratically on the scaling factor."
        },
        {
            "heading": "4.2. Results",
            "text": "To briefly introduce the models and datasets3 used we present an overview of the unquantized, standard-resolution models\u2019 loss and inference cost on all datasets in Tables 1 and 2. Here we use the number of floating point operations (flops), as taken from deepspeed [22], directly as a measure of model inference cost. We note that, as there is no clear measure of when a PDE surrogate is successful (compared to e.g. classification algorithms), a wide regime of varying MSE losses and costs is interesting to investigate. In Tables 1 and 2 we can see that the PDEs have vastly different possible solutions in terms of MSE and cost."
        },
        {
            "heading": "4.2.1 Quantization compared to reducing resolution",
            "text": "In what follows we refer to models being applied on reduced resolution as \u201cscaled\u201d models, although it is actually their input data that is scaled. Implicitly such models are smaller as a result of them being applied on lower-resolution data, but we do not change hyperparameters like layer width or number of layers.\n3Datasets were solely downloaded and evaluated by QUVA.\nThe results of our first set of experiments are presented in Figure 1, where we compare quantization against spatial scaling across models and architectures. The plots can be interpreted as follows: If the triangles (quantized models) lie below the circles (scaled models), quantization is a more robust way to reduce inference costs. We observe this is\nthe case for all model and dataset combinations, except for the U-Net on the Darcy dataset. The reason why scaling is relatively more successful on the Darcy dataset can be understood by looking at some examples. Comparing Figure 3 to Figure 2, the Darcy targets are already quite blurry. Generally, the dataset appears less sensitive to details in the input, making it potentially less likely that important information is lost in the process of changing spatial resolution in a forward pass."
        },
        {
            "heading": "4.2.2 Combining quantization and scaling",
            "text": "The datasets we use have been generated in a higher resolution than is actually used in the experiments. Note that this is common practice: the numerical solvers are applied to a much finer grid to make sure that the data is a relatively accurate representation of the underlying PDE [9, 27, 17]. However, we now consider the hypothetical situation where one is indeed interested in minimizing loss on the higher resolution. Results for these experiments are found in Figures 4.\nFirst, we find that when the original resolution is relatively high, reducing it may not affect performance very much for some of these datasets (e.g., Figure 4a), and yields a good reducing in compute cost. However, as more scaling is applied, we note that switching to a lower-bitwidth quantization regime quickly becomes better than scaling alone. In particular, we find that across almost all PDEs and neural PDE solvers, the cost vs error Pareto-optimal curve is achieved by quantization. In other words, we find that quantization is another \u201cknob\u201d that one can turn to reduce costs, besides only changing resolution. Not only is this \u201cknob\u201d available, but the results show that is essential for optimal efficiency. Note that the particular choice of how much quantization and how much scaling ought to be applied depends on the operating point, that can, for example, be specified by a lower bound on the MSE."
        },
        {
            "heading": "5. Discussion and conclusion",
            "text": "We have shown that quantization can be an effective solution to make neural PDE-solvers more cost-efficient. In particular, it can achieve better results in terms of prediction loss vs inference cost trade-off than the conventional method of making computational cost manageable, which is reducing spatial resolution. To some extent the results\ndepend on the type of data at hand: If the original data is of extremely, and perhaps unnecessarily, high resolution, there may be at first a clear benefit to be obtained by simply reducing spatial resolution. However, we similarly find that there is no increase in loss by quantizing network weights and activations to Int16, with weights even quantized to Int8 in some cases. We note that both cost-reducing methods, quantization and reducing spatial resolution, have a certain breaking point, depending on the model and dataset at hand, at which their effect on prediction loss increases rapidly. Our results indicate that when one of these methods reaches its breaking point, one can further improve on the loss vs inference trade-off by continuing with the other method. To achieve Pareto-optimal solutions it is necessary to apply both methods simultaneously.\nIn this paper, we have manually selected levels of quantization and resolution scaling, showing empirically the im-\nportance of quantization as a new knob that can be turned to achieve better results for similar inference cost in existing models. However, in future research, we will investigate how a neural PDE-solver network can automatically detect the appropriate scaling or quantization levels to optimize the prediction loss vs inference cost trade-off. Another direction for future research is to experiment with real-world weather and climate prediction data. Although our current research is based on toy data, we actually expect the results to be even more pronounced when working with real-world data: The recurring theme (see for instance [21, 19, 1]) is that insufficient computational power inhibits the use of higher data resolutions which would solve many modeling problems and prediction inaccuracies. We propose quantization as a strategy to free up computational resources, allowing models to be defined on these desired higher resolutions."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank SURF for the support in using the National Supercomputer Snellius. This work is financially supported by Qualcomm Technologies Inc., the University of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy."
        },
        {
            "heading": "A. Appendix",
            "text": "A.1. PDEs and Data\nWe train our neural networks on datasets that contain solutions u for different boundary and initial conditions so that it is able to generalize across these conditions, without the need for retraining. Datasets are obtained as follows:\n\u2022 Generate a pair of initial conditions u0(x) and boundary conditions B[u](t,x) = 0 and evaluate these values on the relevant subsets of our grid T \u00d7 X .\n\u2022 Use a conventional high-accuracy numerical solver to obtain u(t,x) for all (x, t) \u2208 T \u00d7 X .\n\u2022 Pick a series of input indices, and subsequent target indices, from the time interval T , starting from a possibly randomly chosen location. The values of u(\u00b7,x) at these indices will form the inputs and targets in our training scheme.\nBurger\u2019s equation The Burger\u2019s equation is a common PDE that arises in fluid dynamics and nonlinear wave phenomena. In 1D the PDE, given the domain that we use, is given by\n\u2202u \u2202t + u \u2202u \u2202x = \u03bd \u03c0\n\u22022u \u2202x2 , x \u2208 (0, 1) t \u2208 (0, 2] (9)\nwhere u represents the speed of the fluid at a certain place and time, and \u03bd is the viscosity coefficient. The Burger\u2019s equation describes the conservation of mass and momentum in a one-dimensional fluid flow, taking into account both convection effects (u\u2202u\u2202x ) and diffusion effects (\u03bd \u22022u \u2202x2 ).\nWe use the 1D Burger\u2019s equation dataset from [27]. It is defined with a spatial resolution of 1024, with periodic boundary conditions, and temporal resolution of 200. The dataset consists of 9000 train and 1000 test trajectories started from samples of different initial conditions that are formed using a superposition of randomly chosen sinusoidal waves. A viscosity coefficient of \u03bd = 0.001 is used.\nDarcy\u2019s Law The steady state 2D Darcy flow equation is a partial differential equation (PDE) that describes the flow of fluid through a porous medium. We use the PDE and domain expressed as\n\u2212\u2207 (a(x)\u2207u(x)) = f(x), x \u2208 (0, 1)2, (10) u(x) = 0, x \u2208 \u2202(0, 1)2,\nwhere a(x) is a diffusion coefficient based on the permeability of the porous medium and the dynamic viscosity of the fluid, u(x) represents the pressure of the fluid, and f\nrepresents any external sources or sinks of fluid within the domain. We set f to constant 1 and train an operator that maps a(x) to the solution u(x). We use the Darcy flow dataset from [17]. It is defined on a spatial grid of 421 \u00d7 421. We use 1024 train elements ((a(x), u(x)) pairs) and 100 validation elements. Details for how a(x) is randomly generated for each data element can be found in [6].\nNavier-Stokes equation The 2D Navier-Stokes Equation and domain that we use for our experiments is given by\n\u2202v(x, t)\n\u2202t = \u2212v(x, t) \u00b7 \u2207v(x, t) + \u03bd\u22072v(x, t) (11)\n\u2212 \u2207p(x, t) + f(x), x \u2208 (0, 32)2, t \u2208 (0, 21].\nIt describes the flow of a fluid in terms of its velocity components v, the viscosity \u03bd, and a buoyancy term f . We assume incompressibility, so \u2207 \u00b7 v = 0, and Dirichlet boundary conditions (v = 0). The dataset is taken from [9]. A viscosity of \u03bd = 0.01 is used, and a buoyancy factor of f = (0, 0.5)T . While generating the data, the pressure field p is solved first, before subtracting its spatial gradients. In addition to the two velocity field components a scalar field s(x) is introduced that is being transported through the velocity field. Its evolution is determined by\n\u2202s \u2202t = \u2212v(x, t)\u2207s, (12)\nwith Neumann boundaries \u2202s\u2202x = 0 on the edge of the domain. For more details, see [9, 4]. The full dataset consists of 2080 train samples and 1088 test samples.\nDiffusion-Sorption Equation The diffusion-sorption equation models a diffusion process that is retarded by a sorption process. The 1D PDE is given by:\n\u2202u \u2202t = D/R(u)\n\u22022u \u2202x2 x \u2208 (0, 1) t \u2208 (0, 500], (13)\nwhere D = 0.0005 is the effective diffusion coefficient, and R(u) = 1 + 2.16u\u22120.126 is the retardation factor hindering the diffusion process. This equation is applicable to, for example, groundwater contaminant transport. The boundary conditions are u(t, 0) = 1 and u(t, 1) = D \u2202u\u2202x (t, 1) The dataset, taken from [27], is discretized into 1024 spatial steps and 501 time steps. There are 9000 train trajectories and 1000 test trajectories, each based on different randomly generated initial conditions using u(0, x) \u223c U(0, 0.2) for x \u2208 (0, 1).\nA.2. Hyperparameter specifications\nWe summarize the hyperparameters used per dataset in Table 3.\nIn the second session of experiments we did not subsample the spatial grid, except for the Darcy dataset for which we subsampled every 2 grid points. The first three scaling levels applied in figure 4 correspond (from left to right) to 0.01, 0.02, 0.05 for the DiffSorp data, 0.02, 0.05, 0.1 for the Burgers data, 0.1, 0.2, 0.5 for the Navier-Stokes data and 0.05, 0.1, 0.2 for the Darcy data. The loss measure used in all datasets is the MSE as described in equation 5. However, for the Darcy dataset, we also normalize each element in the sum by dividing by the squared targets, and take the squared root of the resulting sum.\nThe UNet is taken from [9], but in order to make its size comparable to the other models we use 16 hidden channels for the Navier-Stokes dataset and 8 hidden channels for the other datasets. The FNO model is taken from [17], using 4 layers, a width of 128, 32 modes for the 2D datasets, and 16 modes for the 1D datasets. The Transformer is taken from [6]. It uses 6 encoder layers, 128 hidden channels and a Galerkin attention type for the 2D datasets, and 4 encoder layers, 32 hidden channels and Fourier attention type for the 1D datasets.\nA.3. Inference Cost Calculation Details\nWe describe a few differences compared to the regular deepspeed library [22]. Most standard deep learning operations rely on big matrix multiplications and as such deepspeed outputs the number of MACs used in their corresponding modules. On the other hand, there are some operations that have no MACs, and deepspeed simply outputs\nthe number of FLOPs. However, because we care to differentiate addition and multiplication operations for our proxy measure of inference cost, we add some manual changes to deepspeed so that multiplications are properly accounted for.\n\u2022 We assume bilinear interpolation to be three times the cost of linear interpolation, and for linear interpolation we assume 2 multiplications and 4 additions per output point.\n\u2022 The FNO model uses Fast Fourier Transforms, which are not encountered for in deepspeed. To be able to take these into account in our proxy for model inference cost we assume a complexity of N\u2308log2 N\u2309 (additions and multiplications), which we divide by 2 when the real-valued FFT is used.\n\u2022 In deepspeed no MACs are assigned to the einsum operator. Although it can in theory represent various different types of computations, in our code we only use it for basic matrix multiplications (in the FNO model). We thus change the deepspeed output accordingly."
        }
    ],
    "title": "Efficient Neural PDE-Solvers using Quantization Aware Training",
    "year": 2023
}