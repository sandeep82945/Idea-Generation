{
    "abstractText": "In this paper, we propose a CNN fine-tuning method which enables users to give simultaneous feedback on two outputs: the classification itself and the visual explanation for the classification. We present the effect of this feedback strategy in a skin lesion classification task and measure how CNNs react to the two types of user feedback. To implement this approach, we propose a novel CNN architecture that integrates the Grad-CAM technique for explaining the model\u2019s decision in the training loop. Using simulated user feedback, we found that fine-tuning our model on both classification and explanation improves visual explanation while preserving classification accuracy, thus potentially increasing the trust of users in using CNN-based skin lesion classifiers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Md Abdul Kadir"
        },
        {
            "affiliations": [],
            "name": "Fabrizio Nunnari"
        },
        {
            "affiliations": [],
            "name": "Daniel Sonntag"
        }
    ],
    "id": "SP:a5b4c1947d7a144e3814727da78e3c9c93bbe060",
    "references": [
        {
            "authors": [
                "D. Alvarez-Melis",
                "T.S. Jaakkola"
            ],
            "title": "Towards robust interpretability with self-explaining neural networks",
            "year": 2018
        },
        {
            "authors": [
                "C. Barata",
                "J.S. Marques",
                "M. Emre Celebi"
            ],
            "title": "Deep attention model for the hierarchical diagnosis of skin lesions",
            "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops",
            "year": 2019
        },
        {
            "authors": [
                "A. Chattopadhay",
                "A. Sarkar",
                "P. Howlader",
                "V.N. Balasubramanian"
            ],
            "title": "Gradcam++: Generalized gradient-based visual explanations for deep convolutional networks",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV) (Mar 2018)",
            "year": 2018
        },
        {
            "authors": [
                "N.C.F. Codella",
                "V. Rotemberg",
                "P. Tschandl",
                "M.E. Celebi",
                "S.W. Dusza",
                "D. Gutman",
                "B. Helba",
                "A. Kalloo",
                "K. Liopyris",
                "M.A. Marchetti",
                "H. Kittler",
                "A. Halpern"
            ],
            "title": "Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (ISIC)",
            "venue": "CoRR abs/1902.03368",
            "year": 1902
        },
        {
            "authors": [
                "B. Ghai",
                "Q.V. Liao",
                "Y. Zhang",
                "R. Bellamy",
                "K. Mueller"
            ],
            "title": "Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience",
            "venue": "arXiv e-prints arXiv:2001.09219",
            "year": 2001
        },
        {
            "authors": [
                "A. Holzinger",
                "M. Plass",
                "K. Holzinger",
                "G.C. Crisan",
                "C.M. Pintea",
                "V. Palade"
            ],
            "title": "A glass-box interactive machine learning approach for solving np-hard problems with the human-in-the-loop",
            "year": 2017
        },
        {
            "authors": [
                "M. Jafari",
                "N. Karimi",
                "E. Nasr-Esfahani",
                "S. Samavi",
                "S. Soroushmehr",
                "K. Ward",
                "K. Najarian"
            ],
            "title": "Skin lesion segmentation in clinical images using deep learning",
            "venue": "23rd International Conference on Pattern Recognition (ICPR)",
            "year": 2016
        },
        {
            "authors": [
                "S. Jain",
                "V. jagtap",
                "N. Pise"
            ],
            "title": "Computer aided melanoma skin cancer detection using image processing",
            "venue": "Procedia Computer Science",
            "year": 2015
        },
        {
            "authors": [
                "A.R. Lopez",
                "X. Giro-i Nieto",
                "J. Burdick",
                "O. Marques"
            ],
            "title": "Skin lesion classification from dermoscopic images using deep learning techniques",
            "venue": "13th IASTED international conference on biomedical engineering (BioMed)",
            "year": 2017
        },
        {
            "authors": [
                "F. Nunnari",
                "M.A. Kadir",
                "D. Sonntag"
            ],
            "title": "On the overlap between grad-cam saliency maps and explainable visual features in skin cancer",
            "venue": "Machine Learning and Knowledge Extraction",
            "year": 2021
        },
        {
            "authors": [
                "V. Petsiuk",
                "A. Das",
                "K. Saenko"
            ],
            "title": "RISE: randomized input sampling for explanation of black-box models",
            "year": 2018
        },
        {
            "authors": [
                "H. Raghavan",
                "O. Madani",
                "R. Jones"
            ],
            "title": "Active learning with feedback on features and instances",
            "venue": "J. Mach. Learn. Res",
            "year": 2006
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "A. Das",
                "R. Vedantam",
                "M. Cogswell",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization",
            "year": 2016
        },
        {
            "authors": [
                "D. Sonntag",
                "F. Nunnari",
                "H.J. Profitlich"
            ],
            "title": "The Skincare project, an interactive deep learning system for differential diagnosis of malignant skin lesions",
            "venue": "Technical Report. arXiv:2005.09448 [cs, eess] (May 2020),",
            "year": 2005
        },
        {
            "authors": [
                "E.P. Stuntebeck",
                "J.S. Davis",
                "G.D. Abowd",
                "M. Blount"
            ],
            "title": "Healthsense: Classification of health-related sensor data through user-assisted machine learning",
            "venue": "Proceedings of the 9th Workshop on Mobile Computing Systems and Applications. p. 1\u20135. HotMobile \u201908,",
            "year": 2008
        },
        {
            "authors": [
                "S. Teso"
            ],
            "title": "Toward faithful explanatory active learning with self-explainable neural nets",
            "venue": "Interactive Adaptive Learning 2444,",
            "year": 2019
        },
        {
            "authors": [
                "S. Teso",
                "K. Kersting"
            ],
            "title": "Explanatory interactive machine learning",
            "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
            "year": 2019
        },
        {
            "authors": [
                "B. Zhou",
                "A. Khosla",
                "L. A",
                "A. Oliva",
                "A. Torralba"
            ],
            "title": "Learning Deep Features for Discriminative Localization",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Keywords: Skin lesion \u00b7 XAI (Explainable Artificial Intelligence) \u00b7 SENN (Self Explainable NN) \u00b7 XIL (Xplanatory Interactive Learning)"
        },
        {
            "heading": "1 Introduction",
            "text": "A Convolutional Neural Network (CNN) can detect malignant skin lesions [8]; however, it cannot produce by default the explanation behind a prediction. In image classification, there are several ways to explain a prediction [13, 18, 11]. Nonetheless, in some cases, the explanations can be misleading, and by default the network does not provide the flexibility of learning from a given correction. As a result, the acceptance of such algorithms in the medical domain is quite rare. Differently, in the same situation, given the availability of correct feedback, decision-makers can perceive the reason for the mistake and take the necessary action to avoid the same kind of mistake in the future. These limitations of neural networks fall under the category of lack of interactivity.\nIn the domain of image classification, researchers introduced visual techniques to introduce explainability in deep learning-based image classification. They visualize the discriminatory regions of an image based on specific class identity [13]. Highlighting class discriminative regions in an image is an example of\nar X\niv :2\n30 4.\n01 39\n9v 1\n[ cs\n.C V\n] 3\nexplainability. This helps in spotting biased CNNs that wrongly identify the location of interest used to achieve the classification result. For example, sometimes classification performance is good, but the model classifies based on unrelated features. How can we tell the CNN that it is looking at the wrong image region and at the same time improve its classification accuracy?\nWe address this problem through the user-feedback approach is depicted in Fig. 1. We assume to start from a model that has been trained solely on classification data. Whenever a new sample is passed for classification, the resulting classification and visual explanation can be, independently, correct or wrong. If the correction is applied to the classification results, the solution is rather trivial. One simply needs to collect a batch of corrected labels and use them to perform transfer learning, or fine-tuning, on the starting model. Technically, this is done with the same data formats used for the original training. However, the challenge comes when the correction is applied to the visual explanation image.\nIn this paper, we propose a novel fine-tuning method that accepts humancorrected visual explanation images as part of the forward/backward propagation loop. On the technical level, our contribution consists of: i) the implementation a modified differentiable version of the Grad-CAM [13] explanation technique (so that corrected explanation images can be included in the back-propagation phase), and ii) the definition of a training procedure that takes into account simultaneously the classification and the visual feedback. With this approach, the difference between the original explanation image and the corrected version can be used as additional term of a loss function that includes, together with a classification error, also a visual explanation discrepancy.\nExperiments are conducted in the domain of skin lesion classification, simulating how dermatologists could possibly identify wrong classifications and explanations and provide correction feedback."
        },
        {
            "heading": "2 Related work",
            "text": "This section presents the researches related to explainability and interactivity in the machine learning and the deep learning domains. At the beginning phase of their development, machine learning and deep learning algorithms were blackbox models, except for linear and tree-based models. Black-box models only predict, but they do not present the reason behind their prediction. There are two ways for explanation: explaining the model itself, or explaining the reason for a specific prediction. They are known as global and local methods. In this research, we will only focus on the local methods because the research aims to improve the explanation of a sample-based prediction. First, we will cover the different explanation generation techniques of CNN-based and typical ML-based image classifiers. After that, we will discuss existing interactive methods where users provide feedback to NNs/AI algorithms to improve accuracy. We will also present the limitation of these methods."
        },
        {
            "heading": "2.1 Class activation mapping",
            "text": "A class activation technique generally works in CNNs. In this technique, the discriminatory classification features are extracted from the activation of any convolutional layer. Generating class activation map (CAM) is a technique for localizing class-specific significant features used in explaining convolutional neural networks. CAM has a remarkable localization capability. Zhou [18] describe a procedure for generating CAM using global average pooling (GAP) on convolution layers. GAP is the weighted sum of the convolutional feature map. In some CNN, a neural decision is made from the weighted sum of the GAP outputs. According to the authors, we can spot important image regions by linearly combining the weights of the output layer with the activation of the last convolution layer. Selvaraju [13] propose an explanation technique known as Grad-CAM, which is an extended version of CAM. It utilizes any target class flow gradients through the final convolution layer. Grad-CAM++ [3] is an extended version of Grad-CAM. In \"Improved Visual Explanations for Deep Convolutional Networks\", Chattopadhay [3] propose this generalized method. It can produce improved visualization behavior of CNNs\u2019 predictions and performs better in visualizing multiple instances of an object during a classification. It produces an explanation similar to how Grad-CAM produces an explanation, but the only difference is that it only considers the positive gradient of the output class. Barata [2] proposed a hierarchical CNN-LSTM attention model that uses hierarchical information about classes and then produces attention mapping and hierarchical classification results. The attention map hints at how the classification algorithm looks at the objects in an image.\nIn this research we leverage class activation mapping to generate post-hoc explanation of our model while tuning. Any gradient based explanation technique can adapted to our method."
        },
        {
            "heading": "2.2 Interactive and explainable AI",
            "text": "Teso and Kersting [17] argue that interactive learning places the user into the loop, but the learner stays as a black-box for the user. They also suggest a novel explanatory interactive learning (XIL) framework that can overcome the limitation of interactive learning. Moreover, it can help the user gain trust in the learner by introducing completeness, directability, and understandability. In XIL, a user gives feedback to the learner\u2019s output in an active learning manner when required. The proposed framework utilize LIME [12] as a local explainer and an additional component. They call the framework Caipirinhas (CAIPI). They use three functionalities, labeling the unlabeled data using user input, fitting the model on labeled and unlabeled features, and explaining a prediction using the local explainer. By introducing counter example, authors allow CAIPI to learn from the user\u2019s feedback on the label and the explanation. Counterexamples are nothing but original input images with randomized irrelevant regions. There are three scenarios during the interaction between the learner and the user: the prediction and the explanation are correct, both are wrong, or only the label is correct. CAIPI focuses mainly on the last. It trains itself from the user\u2019s feedback. After the evaluation, the authors see that the trust/distrust of the user increase based on the interactions. CAIPI\u2019s performance increases due to the feedback explanation. However, the counterexample requires more disk space to store and GPU performance to retrain the model. The main difference between CAIPI and this research is how we train the network on feedback. CAIPI augments the original training data based on user feedback and then fine-tune or train a new model. There is no change in the objective function of the model. Also, storing original training data is necessary for retraining. However, the approach we follow in this research only requires post-deployment test images and feedback. Initial training data is unnecessary.\nAccording to Teso [16], the Explanatory Active Learning (EAL) [17] algorithm depends on a post-hoc explainer, and it can generate a fragile and unfaithful explanation. He says the self-explainable active learning model is a solution to that. It is a combination of active learning and self-explainable neural networks (SENNs) [1]. Ghai [5] introduce Explainable Active Learning (XAL) in An Empirical Study of How Local Explanations Impacts Annotator Experience in 2020. Stuntebeck [15] propose a human-in-loop machine learning framework. This framework collects data from the patient using health sensors and trains a machine learning model on that data. Sometimes, due to the inefficiency of the sensors, the model prediction becomes wrong. To overcome this problem, they involve the user in the learning loop. Occasionally, the model gives the prediction, and the user gives feedback on the prediction by comparing what they are experiencing. Based on the feedback, the model tune itself. This framework is similar to this research involving humans in the learning loop. However, the feedback in this framework is only a yes or no decision. Holzinger et al. [6] argues that, while automatic ML suffers in performance because of insufficient training data, it is also true that interactive ML has the flexibility to allow a user to select suitable features heuristically from a vast search space. As a re-\nsult, it can reduce the complexity of NP-hard using outside knowledge (Human intervention). The authors demonstrated the effectiveness of interactive machine learning and showed how to open a black-box technique to a glass-box one, enabling humans to interact with an algorithm. In the Skincare project, Sonntag et al. [14] describe the functionality and interface of an interactive decision support system for differential diagnosis of malignant skin lesions. The methods in the report give generic ideas and importance for interactive machine learning.\nBesides lesion segmentation [7], several pieces of research are published in skin lesion classification [9]. We see that there are several approaches for skin cancer classification. Many of the methods have human-level accuracy. However, the application of these methods is constantly challenged by critique for legitimate reasons; for example, the proposed classifiers are black-box models, need training updates on new data, and lack interactivity with humans and the environment. So, in this research, we explore the interactive side of skin cancer classification."
        },
        {
            "heading": "3 Methods",
            "text": "Our method consists of a deep neural architecture that explains decisions to users and gives them the possibility to perform corrections that improve the model\u2019s performance. Fig. 1 provides an overview of the system.\nFrom the left side, we see a skin lesion fed into a convolution neural network, which predicts the class of the lesion and shows the areas that mostly contributed to the result to a dermatologist (on the right side). If the dermatologist is not convinced of the classification result or with its explanation, s/he gives a feedback. Based on the feedback, we re-train the model.\nThe remaining of this section explains the details of the implementation and describes how we simulated the feedback of dermatologists to validate our approach."
        },
        {
            "heading": "3.1 Integrating the explanation with the classification results",
            "text": "Out method starts from a pre-trained CNN which able to perform image classification. A classifier based on convolutional networks is typically composed of two functional blocks: the convolutional layers (CL) block and the the fully connected layers (FCL) block. Images (X) are fed into the CL at first, and the outputs of CL are flattened and passed through the FCL to generate the output (y). The output of the last convolutional layer is analyzed by explanation local techniques like Grad-CAM to produce a saliency map. A saliency map is a grey-scale image, with the same resolution of the convolutional layer, whose pixels with higher luminance are associated to the areas of the image that mostly contributed to the classification, while areas with dark pixels were mostly ignored.\nThe main idea of the method proposed in this paper, is to provide the results of a classification in terms both of classification result and saliency map to an expert, ask him to perform corrections, and use the corrected result (or a batch of corrections) for further fine tuning of the model. The fine tuning will take into\naccount both the class and the saliency map into a new composite loss function. This is done by attaching to the original model a differentiable branch able to extract the saliency map during a forward pass.\nHowever, from a human perspective, correcting a grey-scale image might be too difficult and time consuming. In fact, in the context of skin cancer detection, many datasets available for the masking tasks provide binary masking images. Hence, we accommodate users\u2019 usability by converting greyscale saliency maps into binary explanation maps, where the users corrections consists only in switching pixels status between black and white, or viceversa. The conversion from saliency into explanation map (yexp) can be performed by simple thresholding [10].\nThe new loss function of the extended model has two components, classification loss and explanation loss. The classification loss is the loss of a pre-trained network. However, the explanation loss is a newly introduced function, and it punishes the overall cost based on the difference between generated explanation and user feedback on explanation. The loss function can be written as\nL = (1\u2212 \u03bb)Lcls(y, y\u0302) + \u03bbLexp(yexp, y\u0302exp) (1)\nHere Lcls and Lexp are the classification and explanation loss respectively. The hyper-parameter \u03bb modulates the contribution of the two terms, and can be set during the model tuning. The two losses are defined as:\nLcls(y, y\u0302) = \u2212 \u2211 i y\u0302i log(yi) (2)\nLexp(yexp, y\u0302exp) = \u2211 i J(yexpi , y\u0302expi) (3)\nHere, Lcls is the well-know categorical cross-entropy, while Lexp is the result of the Jaccard index between the output explanation map and the corrected one. An explanation map yexp:\nyexp = T (Ay) (4)\nis the thresholded version of the result of the Grad-CAM saliency map Ay, which is defined as:\nAy = ReLU( \u2211 ackA k) (5)\nwhere Ak is the activation in a convolution layer k. Given Z as the total number of pixel in Ak (with resolution i\u00d7 j), for a given class c, ack is computed as:\nack = 1\nZ \u2211 i \u2211 j \u2206yc \u2206Akij (6)"
        },
        {
            "heading": "3.2 Implementation details",
            "text": "Algorithm 1 reports a formalized description of the approach. The main aspect that is worth an explanation, is that the Grad-CAM algorithm needs an alteration of the last layer in order to compute its saliency map. For this reason, each iteration of the fine-tuning algorithm requires two forward passes. The first, for the classification, is performed on the original model, while the second is performed on a temporary copy of the model, which is modified to get the saliency map and destroyed and the end of the batch iteration.\nAlgorithm 1 Training the self-explainable model Require: e . e: number of epoch Require: m = F (\u03b8) . m: pre-trained model Require: X,Y, Z . X: input sample, Y : label , Z: binary mask (i.e. explanation ground truth) Require: N . N : total samples Require: Lcls . Lcls: classification loss function Require: Lexp . Lexp: explanation loss function Require: \u03b3 . \u03b3: learning rate i = 1 while i \u2264 e do\nn = 0 while n \u2264 N do\nmg \u2190 deepcopy(m) ack \u2190 1R \u2211 p \u2211 q \u03b4mg(x)[c]\n\u03b4Ak ij\n. \u03b4mg(x)[c]\n\u03b4Ak ij\n: class-specific gradient on layer k for image x\nAk, y\u0302 \u2190 m(x) . Ak: receptive field of layer k, y\u0302: predicted class S \u2190 ReLU(Ak \u00d7 ack) . S: saliency map \u02c6yexp \u2190 th(S, t) . t: threshold value L(\u03b8)\u2190 (1\u2212 \u03bb)Lcls(y, y\u0302) + \u03bbLexp(z, \u02c6yexp) . \u03bb: hyper-parameter for loss balance \u03b8 \u2190 \u03b8 \u2212 \u03b3\u2207L(\u03b8) . \u2207L(\u03b8): gradient with respect to loss\nend while end while"
        },
        {
            "heading": "3.3 Simulation of user feedback",
            "text": "Getting user feedback for an experiment is very costly and time-consuming especially in the medical domain. Hence, instead of performing tests with real users, we exploited an existing dataset of skin lesion images associated to both classes and explanation maps, and used it to simulate users\u2019 feedback. The ISIC 2018 [4] dataset of skin lesion attributes contains images of skin lesions and masks of five different attributes, pigment network, negative network, streaks, milia like cyst, and globules (Fig. 3). These attribute maps are binary masks locating the different attributes. The union of all of these attribute maps provides a comprehensive indication of what are the pixel areas that would lead a human practitioner towards his/her decision.\nThe ISIC2018 dataset (Fig. 2) is imbalanced. Preliminary experiments showed us that such imbalance affects the results of the simulation. Hence, we equalize the data per class by upsampling in the simulation set. We know that the maximum sample belongs to the nevus class (1951 samples), but there are 437\nexamples for the MEL class and only 172 samples for the BKL class. We increase the number of the MEL and BKL samples to 1951 samples by coping them randomly."
        },
        {
            "heading": "4 Experiments and results",
            "text": "This sections described the two experiments conducted with our approach: the first aiming at understanding the contribution of the explanation loss function on a batch of data feedback, and the second aiming to analysing the behavior of the model when data feedback are provided in smaller batches."
        },
        {
            "heading": "4.1 Training on different loss functions",
            "text": "We use three different loss functions in the experiment: classification loss, explanation loss, and combined loss. The purpose of training the model on classification loss is to see how models perform when feedback is given only to classification data. During the training on classification, the model gets feedback only on classification output. So, the loss function only contains the classification part. We observe training and validation loss and average Jaccard index during the tuning process. Differently, the objective of using explanation loss is to see how the model performs when there is only explanation feedback. The explanation loss is the Jaccard loss which compares the predicted and feedback explanations. Finally, the combined loss combines both losses by a regularization parameter \u03bb, which balances the two losses. The purpose is to see how classification and explanation feedback improve model performance.\nWe keep 10% of the data untouched to check the model performance and to see if the model learns from feedback, another 10% for validation and the remaining 80% for fine tuning the model through user simulation.\nTable 1 presents the final result of the simulation. The baseline model performs 0.71 average sensitivity on the original test set (from ISIC 2019), and 0.73 when tested on the full simulation set (ISIC 2018). When performing a test on the 10% of the simulation set, using only the classification loss leads to 0.70 sensitivity and a Jaccard index as low as 0.10. Combining the classification and the explanation loss keeps the 0.70 sensitivity and increases the Jaccard index to 0.127, hence increasing the explanation power of the model. When testing using only the explanation loss, the Jaccard index boosts to 0.18, but the sensitivity drops to 0.66.\nHence, it seems that including an explanation loss term doesn\u2019t affect the classification capabilities of the mdoel but it is able to improve its explanation power."
        },
        {
            "heading": "4.2 Sliced simulation",
            "text": "In this section, we present how a gradual provision of tuning data improves our explainable model\u2019s performance. Fig. 4(a) shows how model accuracy increases over an increasing amount of data. The x-axis represents data slices. Each slice has 238 samples. We have a total of 20 slices, and the model is iteratively finetuned on all the slices. We plotted the accuracy evolution for the three kinds of loss functions: classification only, explanation only, and combined. For the first two cases, we see that the accuracy has no significant improvement over the slices, while we see more accuracy fluctuation when combining classification and explanation loss functions. However, overall, accuracy remains stable at the end of the slices provision.\nFig. 4(b) presents the change in the average Jaccard index over the slices. Looking at the classification loss function\u2019s graph, we see that the average Jaccard index is not increasing. There is a slight improvement when we use combined loss. However, we see notable improvement of the average Jaccard index while\nusing the explanation loss function. Table 2 shows the summary of the sliced simulation. After tuning the model on the first slice, we see that the average Jaccard index is 0.15. However, after the 20th slice, we see that the Jaccard index increases to 0.19. On the other hand, the accuracy reduces to 0.72 from 0.77 by keeping the average sensitivity the same when using the explanation loss function. These results conclude that model explanation performance increases without reducing classification accuracy when the model gets correction feedback in smaller chunks.\nFrom those experiments, it seems that there is a slight better ending performance in the model after training over 20 smaller feedback slices rather than on the full 80% of the simulation set. However, the margins are small and this observed behaviour needs to be further analysed on bigger datasets to be confirmed."
        },
        {
            "heading": "5 Conclusion",
            "text": "We presented an approach for increasing the performances of CNN-based skin cancer classification by including users feedback in a post-train, fine-tuning stage. This approach transforms a black-box VGG16 model into a self-explainable neural network (SENN), which classifies (categories) and explains (saliency maps) at the same time. The implementation consists of augmenting a pre-trained VGG16 architecture with a differentiable implementation of the Grad-CAM algorithm.\nWe tested this approach by simulating practitioner\u2019s feedback using the ISIC 2018 dataset. Our experiments show that our SENN is able, thanks to users feedback, to significantly increase its explanation power whithout compromising its classification accuracy, thus potentially increasing the trust of practitioners into computer-assisted diagnosis systems.\nThe main limitation of this approach is that it is applicable only when saliency maps offer a resolution that is high enough to provide users with a significantly fine-grained explanatory picture. Unfortunately, this is not the case in very deep neural networks, where the last convolutional stage (usually the preferred one to extract saliency maps) is composed by a high number of filters, but very few convoluted areas. Hence, at the moment, the trade-off is to use less classification accurate networks with higher resolution explanation maps, in order to include humans in the correction freedback loop.\nFuture work will focus on testing this approach with the involvement or real practitioners and on different datasets, to verify how well these results generalize to other domains."
        }
    ],
    "title": "Fine-tuning of explainable CNNs for skin lesion classification based on dermatologists\u2019 feedback towards increasing trust",
    "year": 2023
}