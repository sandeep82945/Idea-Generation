{
    "abstractText": "In recent years there has been an increase in the number of research and developments in deep learning solutions for object detection applied to driverless vehicles. This application benefited from the growing trend felt in innovative perception solutions, such as LiDAR sensors. Currently, this is the preferred device to accomplish those tasks in autonomous vehicles. There is a broad variety of research works on models based on point clouds, standing out for being efficient and robust in their intended tasks, but they are also characterized by requiring point cloud processing times greater than the minimum required, given the risky nature of the application. This research work aims to provide a design and implementation of a hardware IP optimized for computing convolutions, rectified linear unit (ReLU), padding, and max pooling. This engine was designed to enable the configuration of features such as varying the size of the feature map, filter size, stride, number of inputs, number of filters, and the number of hardware resources required for a specific convolution. Performance results show that by resorting to parallelism and quantization approach, the proposed solution could reduce the amount of logical FPGA resources by 40 to 50%, enhancing the processing time by 50% while maintaining the deep learning operation accuracy.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jo\u00e3o Silva"
        },
        {
            "affiliations": [],
            "name": "Pedro Pereira"
        },
        {
            "affiliations": [],
            "name": "Rui Machado"
        },
        {
            "affiliations": [],
            "name": "Pedro Melo-Pinto"
        }
    ],
    "id": "SP:2c11794fe9f6526682f0129e63b9ab012d24a98f",
    "references": [
        {
            "authors": [
                "S. Kato",
                "E. Takeuchi",
                "Y. Ishiguro",
                "Y. Ninomiya",
                "K. Takeda",
                "T. Hamada"
            ],
            "title": "An open approach to autonomous vehicles",
            "venue": "IEEE Micro",
            "year": 2015
        },
        {
            "authors": [
                "J. Cui",
                "L.S. Liew",
                "G. Sabaliauskaite",
                "F. Zhou"
            ],
            "title": "A review on safety failures, security attacks, and available countermeasures for autonomous vehicles",
            "venue": "Ad Hoc Netw",
            "year": 2019
        },
        {
            "authors": [
                "Y. Li",
                "L. Ma",
                "Z. Zhong",
                "F. Liu",
                "D. Cao",
                "J. Li",
                "M.A. Chapman"
            ],
            "title": "Deep learning for lidar point clouds in autonomous driving: A review",
            "venue": "arXiv 2020,",
            "year": 2020
        },
        {
            "authors": [
                "X. Chen",
                "H. Ma",
                "J. Wan",
                "B. Li",
                "T. Xia"
            ],
            "title": "Multiview 3d object detection network for autonomous driving",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "J. Ku",
                "M. Mozifian",
                "J. Lee",
                "A. Harakeh",
                "S.L. Waslander"
            ],
            "title": "Joint 3d proposal generation and object detection from view aggregation",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "A.H. Lang",
                "S. Vora",
                "H. Caesar",
                "L. Zhou",
                "J. Yang",
                "O. Beijbom"
            ],
            "title": "Pointpillars: Fast encoders for object detection from point clouds",
            "venue": "arXiv 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Zhou",
                "O. Tuzel"
            ],
            "title": "Voxelnet: End-to-end learning for point cloud based 3d object detection",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "C.R. Qi",
                "W. Liu",
                "C. Wu",
                "H. Su",
                "L.J. Guibas"
            ],
            "title": "Frustum pointnets for 3d object detection from rgb-d data",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "B. Wang",
                "J. An",
                "J. Cao"
            ],
            "title": "Voxel-fpn: Multi-scale voxel feature aggregation in 3d object detection from point clouds",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "K. Jia"
            ],
            "title": "Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "C.R. Qi",
                "H. Su",
                "K. Mo",
                "L.J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "B. Yang",
                "W. Luo",
                "R. Urtasun"
            ],
            "title": "PIXOR: Real-time 3d object detection from point clouds",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Y.-H. Chen",
                "S. Member",
                "T.-J. Yang",
                "J. Emer",
                "V. Sze"
            ],
            "title": "Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices",
            "venue": "arXiv 2019,",
            "year": 2022
        },
        {
            "authors": [
                "G. Desoli",
                "N. Chawla",
                "T. Boesch",
                "S. Singh",
                "E. Guidetti",
                "F. de Ambroggi",
                "T. Majo",
                "P. Zambotti",
                "M. Ayodhyawasi",
                "H Singh"
            ],
            "title": "14.1 a 2.9tops/w deep convolutional neural network soc in fd-soi 28nm for intelligent embedded systems",
            "venue": "In Proceedings of the 2017 IEEE International Solid-State Circuits Conference",
            "year": 2017
        },
        {
            "authors": [
                "C. Zhang",
                "P. Li",
                "G. Sun",
                "Y. Guan",
                "B. Xiao",
                "J. Cong"
            ],
            "title": "Optimizing Fpga-Based Accelerator Design for Deep Convolutional Neural Networks; Association for Computing Machinery, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "A. Jahanshahi"
            ],
            "title": "Tinycnn: A tiny modular CNN accelerator for embedded FPGA",
            "venue": "arXiv 2019,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Shen",
                "M. Ferdman",
                "P. Milder"
            ],
            "title": "Maximizing CNN accelerator efficiency through resource partitioning",
            "venue": "arXiv 2017,",
            "year": 2017
        },
        {
            "authors": [
                "J. Jo",
                "S. Kim",
                "I.C. Park"
            ],
            "title": "Energy-efficient convolution architecture based on rescheduled dataflow",
            "venue": "IEEE Trans. Circuits Syst. I Regul. Pap. 2018,",
            "year": 2018
        },
        {
            "authors": [
                "K. O\u2019Shea",
                "R. Nash"
            ],
            "title": "An introduction to convolutional neural networks",
            "venue": "arXiv 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Y. LeCun"
            ],
            "title": "Lenet-5, Convolutional Neural Networks. Available online: Http://yann.lecun.com/exdb/lenet (accessed on 9 March 2021)",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "J. Zou",
                "K. He",
                "J. Sun"
            ],
            "title": "Accelerating very deep convolutional networks for classification and detection",
            "venue": "arXiv 2015,",
            "year": 2015
        },
        {
            "authors": [
                "S. Targ",
                "D. Almeida",
                "K. Lyman"
            ],
            "title": "Resnet in resnet: Generalizing residual architectures",
            "venue": "arXiv 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Y.H. Chen",
                "T. Krishna",
                "J.S. Emer",
                "V. Sze"
            ],
            "title": "Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks",
            "venue": "IEEE J. Solid-State Circuits",
            "year": 2017
        },
        {
            "authors": [
                "C.Y. Lo",
                "C.-W. Sham"
            ],
            "title": "Energy Efficient Fixed-point Inference System of Convolutional Neural Network",
            "venue": "In Proceedings of the 2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS),",
            "year": 2020
        },
        {
            "authors": [
                "A. Ansari",
                "T. Ogunfunmi"
            ],
            "title": "Empirical analysis of fixed point precision quantization of CNNs",
            "venue": "In Proceedings of the 2019 IEEE 62nd International Midwest Symposium on Circuits and Systems (MWSCAS), Dallas, TX, USA,",
            "year": 2019
        },
        {
            "authors": [
                "C.Y. Lo",
                "F.C.-M. Lau",
                "C.-W. Sham"
            ],
            "title": "FixedPoint Implementation of Convolutional Neural Networks for Image Classification",
            "venue": "In Proceedings of the 2018 International Conference on Advanced Technologies for Communications (ATC), Ho Chi Minh City, Vietnam,",
            "year": 2018
        },
        {
            "authors": [
                "F. Libano",
                "B. Wilson",
                "M. Wirthlin",
                "P. Rech",
                "J. Brunhaver"
            ],
            "title": "Understanding the impact of quantization, accuracy, and radiation on the reliability of convolutional neural networks on fpgas",
            "venue": "IEEE Trans. Nucl. Sci",
            "year": 2020
        },
        {
            "authors": [
                "Xilinx Inc"
            ],
            "title": "Convolutional Neural Network with INT4 Optimization on Xilinx Devices 2 Convolutional Neural Network with INT4 Optimization on Xilinx Devices; Xilinx Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Fu",
                "E. Wu",
                "A. Sirasao"
            ],
            "title": "8-Bit Dot-Product Acceleration White Paper (wp487)",
            "venue": "Xilinx Inc.: San Jose, CA,",
            "year": 2017
        },
        {
            "authors": [
                "M.P. Vestias",
                "R.P. Duarte",
                "J.T. de Sousa",
                "H. Neto"
            ],
            "title": "Hybrid dot-product calculation for convolutional neural networks in FPGA",
            "venue": "In Proceedings of the 2019 29th International Conference on Field Programmable Logic and Applications (FPL),",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Citation: Silva, J.; Pereira, P.;\nMachado, R.; N\u00e9voa, R.; Melo-Pinto,\nP.; Fernandes, D. Customizable\nFPGA-Based Hardware Accelerator\nfor Standard Convolution Processes\nEmpowered with Quantization\nApplied to LiDAR Data. Sensors 2022,\n22, 2184. https://doi.org/\n10.3390/s22062184\nAcademic Editors: Dimitrie C.\nPopescu and Saber Fallah\nReceived: 17 February 2022\nAccepted: 9 March 2022\nPublished: 11 March 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional affil-\niations.\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nKeywords: convolutional neural network (CNN); hardware accelerator; field-programmable gate array (FPGA); light detection and ranging (LiDAR); quantization; object detection"
        },
        {
            "heading": "1. Introduction",
            "text": "The increased focus on research and development on intelligent systems has been growing with different technologies providing different applications on a variety of complex systems. Concerning autonomous vehicles, the main motivation centers on reducing human interference while driving, thereby reducing the likelihood of road accidents caused by human error, improving road safety [1,2]. With that, a highly detailed perception of objects surrounding every vehicle is required, enriching the perception capabilities of these vehicles, allowing thus an efficient capture of information about the localization, classification, and tracking of such vehicles. In this scope, LiDAR sensors have been highlighted as a technology that allows a description of the vehicle surrounding by means of point cloud data, being exploited in the literature as an augmentation to RGB cameras as a standalone solutions [3\u20135]. Currently, deep learning models are widely used to process point cloud data, provided from LiDAR sensors, to extract relevant information that may be used in a mechanism for object detection\nSensors 2022, 22, 2184. https://doi.org/10.3390/s22062184 https://www.mdpi.com/journal/sensors\nand localization [6\u201312]. Different studies suggest a higher precision rate during object detection and classification using deep learning models aside from the classical point cloud algorithms that manually extract features using machine learning techniques [3,6]. However, these models have some drawbacks regarding real-time execution (being often incapable of providing inference time lower than the sampling rate of LiDAR sensors, 10/20 Hz) and the required resources for computation. Deep learning models take advantage of recent developments of convolution neural network architectures, resulting in pipelined architectures with different configurations that may vary regarding the number of convolution blocks, its parameters (stride size, kernel, pooling operation), presence/absence of activation functions, and normalization methods through each pipeline structure. Most of those systems are deployed in graphics processing units. Besides providing higher performance, it is necessary to deploy those systems in edge devices, limited by tight timing constraints, which usually are not considered on GPUs or central processing units implementation approaches [13\u201317]. This work is not intended to create a fully convolutional neural network on an FPGA, instead, the main goals center on the implementation of a configurable convolution module, evaluation of the impact on performance by applying optimization methods such as quantization and parameter sharing, and integration of the convolution module on different CNN architectures. To this end, parallelism approaches for the processing element (PE) were studied and implemented and performance regarding PE gains was compared. Our convolution module which implements a PE based on work [18] provides an improvement of that work by increasing inside the same module the ability to compute rectified linear unit (ReLU), padding, and max pooling. On top of that, the convolution module can be configured to match the features of a convolution block addressed in the literature as part of an object detection model. It means that varying the size of the feature map, filter size, stride, number of inputs, number of filters, and the number of hardware resources required for a specific convolution it is only necessary to change its parameters at the instantiating time. Furthermore, the quantization technique\u2019s impact on the performance of a 3D object detection model, regarding metrics accuracy and inference time, was studied and implemented. Both developments were correctly validated in different applications, where we verified the correct operation and process efficiency on both image and point cloud processing. To the best of our knowledge, this is the first work studying the quantization influence on model performance as a function of the model depth. The paper is organized as follows: Section 2 describes some applied techniques on 3D object detectors based on point clouds data. The section ends with a description of different techniques and optimization methods applied to CNN architectures are also discussed. Section 3 describes the proposed architecture and filter iteration through each instantiated convolution block. Section 4 describes the most relevant parts regarding hardware implementation, block interface, and interactions. Section 5 presents obtained results for three different studies: (1) generic image convolution with filter applications changing the values of its parameters regarding convolution block, presenting an evaluation between processing time, level of parallelism, and consequently resources usage; (2) study about quantization influence on a simple CNN architecture, using the MNIST dataset; (3) the last validation promotes a replacement of a software convolution layer of the PointPillars model running on a laptop with values obtained from the hardware processing using the implemented IP. Finally, Section 6 provides thesis conclusion as well as some considerations for future development."
        },
        {
            "heading": "2. State of The Art",
            "text": "This section provides a brief description of related works and improvements on hardware accelerators for convolutional blocks. Convolution neural network (CNN) architectures are widely used in image recognition and are efficient for object detection, localization, and classification [19]. There are several key CNN architectures, namely LeNET [20], VGGNet [21], and ResNet [22], generally speaking, they are typically built using fundamental\nlayers like convolution, pooling, and fully connected. Convolution layers perform a vital role in how CNN architecture operates, being responsible for around 90% of all computation. Thus, this section focuses on presenting a brief description of 3D point cloud model architectures and developments of hardware accelerators already implemented."
        },
        {
            "heading": "2.1. Deep Learning for 3D Point Cloud",
            "text": "Recent works on 3D point cloud models present a sequential architecture that is split into three stages: (1) data representation, (2) feature extraction, and (3) detection modules. Stage (1) processes the data from the LiDAR sensor and organizes it as a structure that can be easily readable and processed by the following stage. Concerning literature, those structures are created as \u201cVoxels\u201d, \u201cFrustums\u201d, \u201cPillars\u201d, or 2D projections [6,7,9,10]. Stage (2) presents the feature extraction process for a given point cloud. The last stage (3) is defined by its output values which lead to possible object detection. Those outputs describe the probability of object classification, bounding box regression, and object orientation. These models have in common the conversion of the input into a pseudo-image upon the first stage, (1) data representation, which means that 2D representations are applied in further convolution layers. The convolution operation is a fundamental process for feature extraction, providing object classification and bounding box regression."
        },
        {
            "heading": "2.2. Convolution Implementations in FPGAs",
            "text": "Convolutional layers present in CNN architectures introduce high computational costs, due to the extensive number of arithmetic operations, parameter sharing, and memory access. These issues not only increase the amount of hardware resources required but also hampers some complex CNN from achieving their full potential as they are not able to output inferences in a real-time manner. Therefore, migrating convolutional blocks to hardware aims at mitigating those problems, providing a hardware architecture optimized for these operations and, consequently, more reliable, efficient, and time-consuming with fewer resources. Research works [14,18,23] provide advanced architectures that take advantage of parallel computation.\n2.2.1. Sliding Window Dataflow\nIn the proposed architecture in work [23], the processing element unit has a MAC unit and multiply and accumulate operation. Besides the MAC unit, PE blocks hold on-chip memories for input data, weight values, and outputs results. The proposed architecture in work [14] presents parallel multipliers to compute all output products in a single clock cycle, and an adder tree to aggregate all outcomes. The adder tree is predefined to hold a fixed number of multiply operations, which means is limited to a certain convolution process regarding filter size. Both works [14,23] provide hardware blocks optimized for processing time. However, these solutions rely on redundant on-chip memory access, promoting high energy consumption.\n2.2.2. Rescheduled Dataflow Optimized for Energy Efficiency\nIn work [18], a processing element is featured with a multiply and an accumulate operation, MAC unit. Input feature map data and weight data needed for the convolution process are loaded from off-chip memory to on-chip memory. Once on-chip memory is connected to the processing units, memory access requires low energy consumption as data transfer is faster than between off-chip and on-chip memories. Data present on on-chip memory are never discarded, providing access reduction to off-chip memory, decreasing system latency. After finishing the convolution process, the output values are sent to an on-chip memory reserved to hold output values. If necessary, output data stored on output on-chip memory is transferred to off-chip memory for data analysis."
        },
        {
            "heading": "2.3. Optimization Methods",
            "text": "Deep learning algorithms are usually implemented in software using 32 bit floatingpoint values (FP32). Migrating a deep learning algorithm to an ASIC or an FPGA requires for a bit width reduction which is possible using the quantization technique [24\u201326]. A quantized model and a non-quantized model execute the same operations, however, a quantized model with bit-width reduction promotes a memory reduction and allows the execution of more operations per cycle. This memory reduction allows a more compact model representation, which leads to a better deployment in a hardware platform. For hardware, implementation is intended to convert a 32 bit floating-point value to a 16/8/4 bit fixed-point value INT (fixed-point expression), respectively [27]. The bit reduction may lead to a considerable accuracy gap on full precision models as suggested by [28]. Therefore, it is necessary to achieve a trade-off regarding model accuracy, model parameters, and hardware (HW) performance. The work in [29,30] presents a method that takes full advantage of a DSP block for 8 bit quantization. However, the trade-off between accuracy and inference time might be required and applied whenever possible, therefore, this study provides insights about the model degradation for various model depths, i.e., number of layers."
        },
        {
            "heading": "3. Convolution Hardware-Based Block",
            "text": "The proposed block IP was designed taking into consideration the developments in deep learning models for object detection addressed in the last five years. Our work based on rescheduled dataflow, exploited by work [18], provides a different implementation since we create an IP capable of computing not only convolution but also rectified linear units, padding, and max pooling, that can be configured by simply changing its parameters and adapting it to different CNN architectures. Besides that, quantization was applied to each weight value which leads to a reduction in bit-width leading to parameter sharing and promoting a DSP resource usage decrease. Therefore, the parameters and the range of values that might change from model to model were identified, in order to tailor the block for any desired architecture whenever required."
        },
        {
            "heading": "3.1. Block Architecture",
            "text": "As Figure 1 depicts, the architecture of the convolution block proposed in this work comprises the following three distinct modules: processing element, rectified linear unit, and max pooling. The convolution block has six parameters that provide different architecture configurations. Five of the six parameters are related to the theoretical convolution process. The ability to change the six parameters provides an advantage to other works since different configurations are possible. We defined the number and set of configurable parameters according to the convolutions layers implemented in the literature for 3D object detection. These parameters are as follows: (1) feature map size, (2) weight/filter/kernel size, (3) stride value, (4) padding value, (5) maxpool enable/disable, and (6) number of DSP block per convolution block.\nInside the convolution block, on Figure 1, processing data flows through modules sequentially, i.e., PE result output is forwarded towards the following module, ReLU, which after performing its operations, forwarded its output to the next module, maxpool, whenever parameter (5) is enabled. A \u201ccontroller\u201d module ensures precise BRAM addresses management, providing an ordered data transfer from a block RAM to a convolution block."
        },
        {
            "heading": "3.2. Processing Element",
            "text": "The processing element module is considered a low-level block inside the proposed architecture. All convolution processes, meaning multiply and accumulate operations, are carried out by the PE module. At the same clock instant, three new values are fed to the input ports of each PE module, as Figure 2 illustrates: (1) feature map value, (2) filter (weight) value, and (3) previous PE output value.\nWe explore the DSP block usage to reduce the amount of resources required, as DSP templates provide a favorable trade-off between developed configuration and resources usage. Flexibility during architecture implementation is ensured as every DSP signal and parameter is changeable during instantiation, otherwise using another of the three types of inference leads to lower flexibility and higher resource consumption."
        },
        {
            "heading": "3.3. Memory Access and Dataflow",
            "text": "In order to reduce the limitations of the sliding window approach discussed in state of the art, namely on memory access and redundant data, our solution proposes a distinct approach of the research works [14,18], being thus inspired by the research work [23]. The proposed architecture follows sequential processing data, meaning that each FM value is fed one by one to the processing module. This mechanism ensures that the system architecture only changes when applying filters with different dimensions, providing a scalable and stable architecture. The dependency data flow chart in Figure 3 illustrates graphically how each outcome from a multiply operation must be connected to other multiply operations, where: (1) F_xx refers to input FM data; (2) W_xx refers to weight/filter/kernel data; (3) O_xx refers to output FM data; (4) for each vertical line, with 9 values corresponding filter size, all data are computed simultaneously; (5) each vertical line is processed with one delay clock cycle; (6) black arrows refer to input data of the next processing element, added up with the corresponding multiply outcome.\nThis architecture provides a modular configuration, meaning that changing the input FM dimensions or filter dimensions leads to easy reconfiguration without losing architecture integrity and maintaining the correct convolution function. Equation (1) presents the required processing time as a function of the characteristics of the input data, namely FM size (FM), number of input FM (num.FM); level of parallelism required (Num.PEs), and board clock (ClockFreq.).\nFM2 Num.PEs \u00d7 Num.It \u00d7 Num.FM\nClockFreq.(MHz)) (\u00b5s) (1)\nAs Figure 4 depicts, for a filter of 3 \u00d7 3 the processing element module is built with nine DSP blocks. As illustrated, in every N DSP block, a ShiftRAM is placed, N being equal to filter size (3 for the above example). Each ShiftRAM introduces a predetermined delay which serves as synchronization for data processing in the next DSP block. The predetermined delay depends on feature map and filter dimensions. The delay value is obtained by subtracting the FM dimension from the filter dimension. At the end of this section, a different configuration is present regarding quantization models, for that, DSP block and ShiftRAMs usage decrease due to bit width reduction making it possible to have two multiplication and one accumulate operation for a single DSP block."
        },
        {
            "heading": "3.4. Board Resources-Driven Architecture",
            "text": "To provide a convolution block that can be tailored for a certain application, three new configurable parameters were added which will influence the number of PE modules for simultaneously processing. These parameters are as follows: (1) DSP available, (2) BRAM available, and (3) BRAM memory bit. The parameters (2) and (3) emerge from memory limitation, allowing the user to specify the amount of memory available for a set of convolution blocks. Considering hardware resource limitations as a constraint of many CNN hardware implementations, parameter (1) DSP availability is added to indicate the maximum number of DSP blocks that can be distributed through the PEs components of our convolution block. In order for our block to automatically adapt its architecture to the resources available on the target board, parameters (2) and (3) are considered for specifying the amount of filters that can be simultaneously processed. It will ensure that the number of convolution blocks is limited by the memory available. The minimum memory required for one filter application is given by output FM dimension and output data bit width. Equations (2) and (3) provide information on the number of parallel filters and number of PEs modules that can be instantiated:\nNumbero f Parallel f ilter = BRAMavailable \u00d7 BRAMmem.bit\nOUTFMsize \u00d7 OUTFMsize \u00d7 OUTwidth (2)\nNumbero f PEblocks = Numbero f f ilterstoapply\nNum.ParallelFilters \u00d7 KERNELsize \u00d7 KERNELsize (3)"
        },
        {
            "heading": "3.5. Filters Iteration\u2014Control Module",
            "text": "Each convolution block only has access to the values corresponding to a single filter, meaning that for the proposed convolution block architecture, the number of blocks required to instantiate and to assure the correct operation in parallel matches the number of filters desired to apply simultaneously. As illustrated in Figure 5, all convolution blocks are instantiated inside a layer block, which has a state machine that controls each stage for data processing. Each stage has distinct functionalities since reading memory from input BRAMs, data transfer to lower levels modules, and write memory to output BRAMs. As Figure 5 presents, in idle stage (1), a reset is performed to all internal registers ensuring transition for the second stage, providing weights load from BRAM to PE modules. State machine continues in load weights stage (2) until all weight values are transferred to PE modules. Data transfer ends after Kernel_size*Kernel_size clock cycles. In load in/process FM stage (3), the convolution process begins, all FM data are transferred and processed one by one, as mentioned at the beginning of the design section. The final stage (4), indicates that a new FM was generated being the correspondent output data in memory."
        },
        {
            "heading": "3.6. Optimization Methods",
            "text": "This section presents two methods for optimizing the convolution operations on the proposed convolution block architecture by including features such as parameter sharing and quantization. The quantization process is considered an efficient and fundamental approach for compression of such model targeting resource-constraint devices, such as edge devices. Due to input feature map and weight bit width reduction performed by quantization technique from 32 bit floating point to 8 bit fixed point, the previously described architecture should be updated to handle the new data format.\nArchitecture Reconfiguration with 8 bit Quantization\nIn research work [29], two parallel MAC operations compute two dot products. For implementing these two MAC operations, it is necessary to use input port D from the DSP48Ex block. For the two multiply operations, it is necessary to apply the following equation P = (AB + DB) + C. The value that port A receives is arithmetically left-shifted by 18 bits. Data in the D register are stored in the least significant bits positions and data in the A register are left-shifted 18 bits to ensure that the outcome from the pre-adder module does not lose any weight value for each computation. As Figure 6 illustrates, the reconfigured architecture has, for a single DSP block, two MAC modules, promoting fewer shift RAM blocks usage since more DSP blocks are directly connected. As Figure 6 presents, four DSP blocks have a direct connection which refers to the filter/kernel dimension (in this case a filter of 4 \u00d7 4 is applied). The proposed architecture needs to ensure that outcomes from MAC modules are correctly sent for the following DSP blocks. In case of a direct connection, each result is directly sent to the C input port. This new DSP block receives as input the first weight value of the following four consecutive Weight values. After finishing processing the first four weight values, represented in blue, results from the last MAC module are sent to the first shift RAM. Output data from a shift RAM has the same delay as presented for the first proposed architecture, which varies regarding weight/filter/kernel dimension, FM dimension, and stride value. The shift RAM block receives one value that holds two different results regarding blue and red DSP block computation. For a correct accumulation of the first red DSP block, output data from shift RAM need to be 8 bit right-shifted."
        },
        {
            "heading": "4. Implementation",
            "text": "All code was implemented using Verilog with all the modules previously described being implemented on the programmable logic (PL). To validate the implemented hardware IP, after test bench validation for behavioral and timing simulations, it is necessary to validate the IP on an FPGA board. The tests present in the results section were deployed on a Zybo Z7:Zynq-7000 board [32]. To evaluate the consumed resources by the proposed IP an input image of 252 \u00d7 252 dimension was used, which is a typical size widely adopted in deep learning models. To build an entire CNN architecture with our convolution IP, it is only required to instantiate the correct number of convolution blocks while ensuring that data flow is correctly performed. Using our IP interface, the user is able to configure at design time each parameter addressed to a convolution. This means that for the application of the module in a real CNN deployment it is mandatory to determine the correct number of convolution layers and each parameter value, before implementing it on an FPGA. As found in the literature, a major problem regarding CNN implementation in hardware centers on resources limitations, such as DSP blocks and memory. Considering that constraint, three parameters (DSP available, BRAM available, and BRAM memory bit), which are used to define the number of filters processed at the same time, were added. Figure 7 and Table 1 presents the resources required for implementing a convolution of a 3 \u00d7 3 filter to an image of 252 \u00d7 252 using only one PE block (9 DSP blocks) with a clock source of 100 MHz. The figure shows the resources required for IP (design_1_layer_2_0_0_synth_1) at the bottom. The total of resources required for managing the ARM processor, DMA module, blocks of RAM, and our IP is displayed as impl_1. Additionally in the figure are illustrated how resources are split on the FPGA device. The resources related to the implemented IP are highlighted with red, inside the purple square box. The other resources are related to AXI modules, DMA controller, BRAMs, and processing system."
        },
        {
            "heading": "5. Results",
            "text": "The tests presented in this section were obtained with each image loaded to an SD card that is connected to the Zybo board. Using the ARM processor, provided by the Zybo Z7 board [32], the DMA controller was configured to store input image data in DDR memory. Thus, before the convolution process begins, the DMA controller sends input images to the RAM blocks that are directly connected to the proposed IP."
        },
        {
            "heading": "5.1. Generic Convolution",
            "text": "To evaluate the correct functionality of the convolution block, it was applied different filters to a set of figures, changing the values of the stride, padding, and maxpool parameters. The dimensions of input FM are 252 \u00d7 252, with a filter application of 3 \u00d7 3. Figure 8 depicts each output from applying the same 3 \u00d7 3 sharpen filter using a stride of 1 and 3, resulting in outputs of 250 \u00d7 250 and 84 \u00d7 84, respectively.\n5.1.1. Parallelism Influence on Processing Time\nConsidering that in the previous example we used a 252 \u00d7 252 image with a filter application of 3 \u00d7 3, the study begins with the simple case which uses only one PE block, providing less resource usage and higher processing time. Each test iteration increases the number of PE blocks to a maximum of 100 PEs. For a filter of 3 \u00d7 3 and applying up to 100 PEs blocks, 900 DSP blocks are required. As illustrated by Figure 9, the instantiation of only one PE module leads to a higher processing time. The application of two PE modules in parallel leads to a reduction in 50% of the previous processing time. Applying three PE modules in parallel reduces the processing time by 65%. As illustrated, after 20/30 PE modules in parallel the processing time reduction is less relevant.\nThe blue curve in Figure 9, represents theoretical processing time values previously computed regarding PEs block usage, the orange curve represents processing time values during hardware IP processing measured in the same previous conditions. A slight deviation in theoretical values and hardware measures appears on the graph since for \u201ctheoretical time\u201d we do not consider the initial clock cycles that do not generate valid outcomes, as explained in the design section.\n5.1.2. Block RAM Influence on PEs/Number of Parallel Filter/Processing Time\nFor the following example, the IP parameters were configured as: kernel size = 3, FM size = 252, padding = 0, stride = 1, maxpool = 0, input FM channels = 64, num of filters = 32, DSP block available = 1000, and clock source of 100 MHz. As Figure 10 presents, when only one block of RAM is used only one filter is applied (orange line), consequently, the maximum possible number of PE blocks are used. Increasing the number of blocks of RAM leads to an increase in the number of parallel filters. Consequently, the number of PE blocks per convolution block decreases since more PE blocks are spread to different filters reducing the number of available PEs for a single convolution block. Increasing the number of parallel filters provides a decrease in processing time, as illustrated in the graph on the right side. However, after the two lines intersect in the upper graph, processing time increases. This can be explained once more filters are computed simultaneously which results in fewer DSP blocks allocated per convolution\nblock. Thus, once fewer DSP blocks are used in a single convolution block the processing time increases."
        },
        {
            "heading": "5.2. Quantization Influence Study",
            "text": "This section presents a study regarding the quantization impact on two different deep learning models based on a CNN architecture. The MNIST model is used for handwritten digit detection in an image. The other model refers to a 3D object detector and classifier model based on point clouds, namely, PointPillars. The two deep learning models supported by the software version use the input data and weight values in a 32 bit floatingpoint format. This study intends to convert the aforementioned data from single-precision to an 8 bit fixed-point format.\n5.2.1. MNIST Dataset Model\nThe model configuration used for validation is built with two convolutions layers, fully connected and softmax. Figure 11 illustrates what changes were performed to validate the IP functionality. In the first study, the values were hardware computed using the developed IP. In a second study, with an integration of eight new convolution layers, the quantization methods through all layers were implemented. As represented inside the red square, the input values of the first convolution layer are sent to hardware IP. The output values from hardware processing, which are quantized due to hardware conversion from 32 bit floating-point format to 8 bit fixed-point format return to the software (SW) model to feed the next convolution layer, the input values of the second layer are replaced with hardware values. The fixed-point representation is expressed as Qi.d, where i refers to the integer part of the fixed point, in other words, the number of bits present on the left side of the fixed point, and d refers to the number of bits on the right side of the fixed point, as a decimal fraction. The weights values for the first convolution layer were quantized with three different quantization levels: Q2_6, Q3_5, and Q4_4. For each one of them, the output FM\nquantization level was tested with different configurations providing a better study on classification score due to weight and output FM value quantization.\nConvolution Weights and Bias Quantized\nIn a second iteration, illustrated in Figure 11 by the green box, the study was extended to apply quantization to all layers of the MNIST model. To the previous MNIST model a few more convolution layers were added, with that we intend to verify the quantization impact on deeper CNN architectures, so we added eight (8) new convolution layers. The quantization was performed using the same IP that was used in the previous study.\nWith this, we can obtain a quantization result that is approximately close to a possible implementation of the entire architecture in HW. This means if we intend to implement the entire model on hardware, which is not the goal with this study, the results expected will be close enough to the results presented in Figure 12. For this study, only Q1_7 and Q2_6 configurations were applied, another type of configuration such as Q3_5 or Q4_4 leads to worse results since the number of fractional bits is reduced, leading to accuracy loss. The previous study only uses as input one image, this study applies four different images with handwritten digits of 1, 2, 3, and 4. The graphs depicted in Figures 13 and 14 present the obtained score for the correct classification. In other words, associates each of the four images with a correct classification (image 1\u2014digit 1, image 2\u2014digit 2, and so on). The blue bar on graphs represents the score values obtained using an SW-only version. The gray bars, from left to right, represent the score obtained by applying quantization to all layers of the MNIST model and quantization only on convolution layers, respectively. As expected, applying quantization to all layers leads to a slight score reduction. This reduction, presenting a higher error on image 2 with 1.88%, varies depending on which image is used for classification. Nonetheless, it is almost insignificant, leading to a robust classification for each of the four different images.\n5.2.2. PointPillars Model Quantized Convolution Weights\nOn the PointPillars model, we verified how quantized convolution weights of backbone convolution layers affect score detection and interest over union (IoU). For this analysis, we adopt the performance metric mean average precision (mAP) while using the Kitti dataset. The mAP is used to measure the accuracy of an object detector regarding its precision and recall. Using a Python script, it was possible to obtain a result checkpoint using different frames of the Kitti dataset, the result checkpoint provides mAP values for the evaluation scenarios of bounding box (BBOX), bird\u2019s eye view (BEV), 3D, and average orientation similarity (AOS). Figure 15 presents the mAP metric value for each of the four scenarios regarding the three difficulty levels, using SW-only version with 32-bit weight values and quantized version with 8-bit. In Figure 15, below each scenario presents three bars regarding the three difficulty levels (from left to right refers to easy, moderate, and hard). Each difficulty level value for the SW-only version is directly compared with the correspondent value for a quantized version, which means, for the BBOX metric, it is possible to evaluate the SW and Q_CONV result value that combines the same bar color. In this case, the SW-only version for the easy level results in an mAP of 83.73% while the quantized version results in 80.84%. The same evaluations are applied for the remaining metrics. This mAP degradation can be explained by the precision loss of the feature maps, but also due to the fact that the outputted classification scores are affected as will be further shown, suggesting that the score threshold should be adjusted to reduce the metric performance loss.\nDifferent frames were evaluated through Kitti Viewer, thus classification score and IoU values were collected using checkpoint files resulting from the previous step, i.e., evaluate using the SW-only version model and quantized model version (quantized weight values from the backbone convolution layer). After analyzing every frame regarding classification score, IoU, and object distance to LiDAR sensor, the graphs in Figure 16 were plotted. Each blue dot represents a detected object for a given frame, describing object depth regarding the LiDAR sensor and its correspondent classification score value and IoU value. This study tries to establish a direct relationship between the obtained classification score value for a given object and its distance to the LiDAR sensor.\nAs showcased in the graphs of Figure 16, greater object accumulation can be indicated for classification scores higher than 0.6. These objects are located at a maximum distance of 30 m from the LiDAR sensor, representing 65.5% of objects for the SW-only version using 32 bit weights. For the quantized version, using weights with 8 bits, object accumulation continues high under 30 m distance from the sensor, with higher score values (total of 65.98% of objects). As for the IoU metric, the graph in Figure 16 presents a higher object concentration with higher IoU values for distances up to 30 m, resulting in 62.02% of the total objects. For the quantized model, regarding IoU values, the graph presents a higher scatter of the object\u2019s distribution, due to bit reduction, which leads to some null values, resulting in a quantitative reduction to 51.55%. The two graphs in Figure 17 illustrate how the score and IoU error is spread regarding object distance from the LiDAR sensor. In each graph, it is possible to verify that some error values are negative, meaning that the classification score or IoU value is higher for the quantized weights model version, providing an improvement of detection accuracy once score and IoU metrics increase. The graph that represents the error score shows a higher accumulation of values for an error under 0.05 and over \u22120.05, representing a point accumulation of 61.63% for all objects. This point distribution, as expected from previous graphs, is located in the range below the 30-meter distance from the LiDAR sensor, represented by the red box. For the IoU metric, higher point accumulation occurs for an error under 0.25 and a distance less than 30 m, representing 55.43% of all objects.\nConvolution Layer Hardware Replacement\nAs described in work [6], the backbone stage is built with sixteen convolution layers through each of the three blocks, which requires a lot of DSP and memory hardware resources, only two layers of block 3 were hardware processed. The inference process was done using a robotic operating system (ROS) platform that runs a new point cloud frame while initiating the PointPillars network for object detection and classification.\nTables 2 and 3 present the obtained results for each evaluated metric regarding point cloud object detection. The score metric represents the probability of an object belonging to a certain class, in this example, cars. The location metric, expressed in meters, provides a spatial object identification on the point cloud. Each object position is given regarding the LiDAR sensor. The bounding box metric represents the BBOX around an object. Rotation_y and Alpha are related to the observation angle for each object.\nFrom the obtained results, it is possible to notice there is not a huge divergence between the SW and hybrid versions, which uses values from hardware processing. During the hybrid model inference process it was verified that the number of false positives increases for the same score threshold value used during SW-only inference. To filter some of the false positives, which do not provide any valuable information about object detection or classification, the threshold value was increased from 0.5 to 0.75. However, the hybrid model presents one detection loss regarding the furthest car (car 6), as Figure 18 depicts and for the lack of metric results in Table 3. This detection loss was expected as expressed from a previous study exploited in Figure 16, which provides a visual perception that further objects tend to be less detected and present lower accuracy."
        },
        {
            "heading": "6. Conclusions",
            "text": "This paper had as its main goal the design and implementation of a convolution block, with the particularity of being totally customizable and applicable to any 3D object\ndetection model. To increase parallelism for each convolution operation, the possibility of having several processing elements operating at the same time was added to the proposed architecture to improve inference time at the cost of energy and consumed logical resources. Along with the development and implementation of the proposed generic convolution block, a study was conducted regarding the influence of quantization and parameter sharing. The quantization process, which reduces the bit-width of each parameter value, enables the second possible optimization that is related to \u201cparameter sharing\u201d. The bitwidth reduction of weight values promotes a decrease of DSP usage of around 40% to 50%. The developed IP was validated using RGB and point cloud data, in each evaluation it was verified that the proposed solution was capable of adaption regarding different model requirements. In the case of the model using point cloud data, with the PointPillars model it was possible to verify that higher scores and IoU values tend to appear near the LiDAR sensor, around less than 30 m. Furthermore, the quantization process affected both score and IoU values up a 10% decrease. Using the developed IP we visually confirmed the correct operation of the proposed solution. It was also possible to qualitatively validate the correct operation of the integration setup, the model being able to detect objects within a range of 30 m from the LiDAR.\nAuthor Contributions: Conceptualization, J.S., P.P., R.M. and D.F.; methodology, J.S., P.P., R.M. and D.F.; software, J.S. and P.P.; validation, J.S., P.P., R.M., R.N., P.M.-P. and D.F.; formal analysis, R.M., P.M.-P. and D.F.; investigation, J.S., P.P., R.M. and D.F.; resources, R.M., P.M.-P. and D.F.; data curation, J.S., P.P. and R.N.; writing\u2014original draft preparation, J.S. and P.P.; writing\u2014review and editing, J.S., P.P., R.M. and D.F.; visualization, R.N. and P.M.-P.; supervision, R.M., R.N., P.M.-P. and D.F.; project administration, R.M., P.M.-P. and D.F.; funding acquisition, P.M.-P. All authors have read and agreed to the published version of the manuscript.\nFunding: This work is supported by European Structural and Investment Funds in the FEDER component, through the Operational Competitiveness and Internationalization Programme (COMPETE 2020) (Project no. 037902; Funding Reference: POCI-01-0247-FEDER-037902).\nConflicts of Interest: The authors declare no conflict of interest.\nReferences 1. Kato, S.; Takeuchi, E.; Ishiguro, Y.; Ninomiya, Y.; Takeda, K.; Hamada, T. An open approach to autonomous vehicles. IEEE Micro 2015, 35, 60\u201368. [CrossRef] 2. Cui, J.; Liew, L.S.; Sabaliauskaite, G.; Zhou, F. A review on safety failures, security attacks, and available countermeasures for autonomous vehicles. Ad Hoc Netw. 2019, 90, 101823. [CrossRef] 3. Li, Y.; Ma, L.; Zhong, Z.; Liu, F.; Cao, D.; Li, J.; Chapman, M.A. Deep learning for lidar point clouds in autonomous driving: A review. arXiv 2020, arXiv:2005.09830. 4. Chen, X.; Ma, H.; Wan, J.; Li, B.; Xia, T. Multiview 3d object detection network for autonomous driving. arXiv 2016, arXiv:1611.07759. 5. Ku, J.; Mozifian, M.; Lee, J.; Harakeh, A.; Waslander, S.L. Joint 3d proposal generation and object detection from view aggregation. arXiv 2017, arXiv:1712.02294. 6. Lang, A.H.; Vora, S.; Caesar, H.; Zhou, L.; Yang, J.; Beijbom, O. Pointpillars: Fast encoders for object detection from point clouds. arXiv 2018, arXiv:1812.05784. 7. Zhou, Y.; Tuzel, O. Voxelnet: End-to-end learning for point cloud based 3d object detection. arXiv 2017, arXiv:1711.06396. 8. Qi, C.R.; Liu, W.; Wu, C.; Su, H.; Guibas, L.J. Frustum pointnets for 3d object detection from rgb-d data. arXiv 2017, arXiv:1711.08488. 9. Wang, B.; An, J.; Cao, J. Voxel-fpn: Multi-scale voxel feature aggregation in 3d object detection from point clouds. arXiv 2019, arXiv:1907.05286. 10. Wang, Z.; Jia, K. Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection. arXiv 2019, arXiv:1903.01864. 11. Qi, C.R.; Su, H.; Mo, K.; Guibas, L.J. Pointnet: Deep learning on point sets for 3d classification and segmentation. arXiv 2016, arXiv:1612.00593. 12. Yang, B.; Luo, W.; Urtasun, R. PIXOR: Real-time 3d object detection from point clouds. arXiv 2019, arXiv:1902.06326. 13. Chen, Y.-H.; Member, S.; Yang, T.-J.; Emer, J.; Sze, V.; Member, S. Eyeriss v2: A flexible accelerator for emerging deep neural\nnetworks on mobile devices. arXiv 2019, arXiv:1807.07928\n14. Desoli, G.; Chawla, N.; Boesch, T.; Singh, S.; Guidetti, E.; de Ambroggi, F.; Majo, T.; Zambotti, P.; Ayodhyawasi, M.; Singh, H.; et al. 14.1 a 2.9tops/w deep convolutional neural network soc in fd-soi 28nm for intelligent embedded systems. In Proceedings of the 2017 IEEE International Solid-State Circuits Conference (ISSCC), San Francisco, CA, USA, 5\u20139 February 2017; pp. 238\u2013239. 15. Zhang, C.; Li, P.; Sun, G.; Guan, Y.; Xiao, B.; Cong, J. Optimizing Fpga-Based Accelerator Design for Deep Convolutional Neural Networks; Association for Computing Machinery, Inc.: New York, NY, USA, 2015; pp. 161\u2013170. 16. Jahanshahi, A. Tinycnn: A tiny modular CNN accelerator for embedded FPGA. arXiv 2019, arXiv:1911.06777. 17. Shen, Y.; Ferdman, M.; Milder, P. Maximizing CNN accelerator efficiency through resource partitioning. arXiv 2017, arXiv:1607.00064. 18. Jo, J.; Kim, S.; Park, I.C. Energy-efficient convolution architecture based on rescheduled dataflow. IEEE Trans. Circuits Syst. I Regul. Pap. 2018, 65, 4196\u20134207. [CrossRef] 19. O\u2019Shea, K.; Nash, R. An introduction to convolutional neural networks. arXiv 2015, arXiv:1511.08458. 20. LeCun, Y. Lenet-5, Convolutional Neural Networks. Available online: Http://yann.lecun.com/exdb/lenet (accessed on 9 March 2021). 21. Zhang, X.; Zou, J.; He, K.; Sun, J. Accelerating very deep convolutional networks for classification and detection. arXiv 2015, arXiv:1505.06798. 22. Targ, S.; Almeida, D.; Lyman, K. Resnet in resnet: Generalizing residual architectures. arXiv 2016, arXiv:1603.08029. 23. Chen, Y.H.; Krishna, T.; Emer, J.S.; Sze, V. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE J. Solid-State Circuits 2017, 52, 127\u2013138. [CrossRef] 24. Lo, C.Y.; Sham, C.-W. Energy Efficient Fixed-point Inference System of Convolutional Neural Network. In Proceedings of the 2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS), Springfield, MA, USA, 9\u201312 August 2020. 25. Ansari, A.; Ogunfunmi, T. Empirical analysis of fixed point precision quantization of CNNs. In Proceedings of the 2019 IEEE 62nd International Midwest Symposium on Circuits and Systems (MWSCAS), Dallas, TX, USA, 4\u20137 August 2019; pp. 243\u2013246. 26. Lo, C.Y.; Lau, F.C.-M.; Sham, C.-W. FixedPoint Implementation of Convolutional Neural Networks for Image Classification.\nIn Proceedings of the 2018 International Conference on Advanced Technologies for Communications (ATC), Ho Chi Minh City, Vietnam, 18\u201320 October 2018.\n27. Libano, F.; Wilson, B.; Wirthlin, M.; Rech, P.; Brunhaver, J. Understanding the impact of quantization, accuracy, and radiation on the reliability of convolutional neural networks on fpgas. IEEE Trans. Nucl. Sci. 2020, 67, 1478\u20131484. [CrossRef] 28. Xilinx Inc. Convolutional Neural Network with INT4 Optimization on Xilinx Devices 2 Convolutional Neural Network with INT4 Optimization on Xilinx Devices; Xilinx Inc.: San Jose, CA, USA, 2020. 29. Fu, Y.; Wu, E.; Sirasao, A. 8-Bit Dot-Product Acceleration White Paper (wp487); Xilinx Inc.: San Jose, CA, USA, 2017. 30. Vestias, M.P.; Duarte, R.P.; de Sousa, J.T.; Neto, H. Hybrid dot-product calculation for convolutional neural networks in FPGA.\nIn Proceedings of the 2019 29th International Conference on Field Programmable Logic and Applications (FPL), Barcelona, Spain, 8\u201312 September 2019; pp. 350\u2013353.\n31. Xilinx Inc. Ultrascale Architecture DSP Slice User Guide; Xilinx Inc.: San Jose, CA, USA, 2021. 32. Diligent Inc. Zybo Z7 Board Reference Manual; Diligent Inc.: New York, NY, USA, 2018."
        }
    ],
    "title": "Customizable FPGA-Based Hardware Accelerator for Standard Convolution Processes Empowered with Quantization Applied to LiDAR Data",
    "year": 2022
}