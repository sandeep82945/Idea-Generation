{
    "abstractText": "Analogy is one of the core capacities of human cognition; when faced with new situations, we often transfer prior experience from other domains. Most work on computational analogy relies heavily on complex, manually crafted input. In this work, we relax the input requirements, requiring only names of entities to be mapped. We automatically extract commonsense representations and use them to identify a mapping between the entities. Unlike previous works, our framework can handle partial analogies and suggest new entities to be added. Moreover, our method\u2019s output is easily interpretable, allowing for users to understand why a specific mapping was chosen. Experiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. We hope this work will advance computational analogy by paving the way to more flexible, realistic input requirements, with broader applicability.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shahar Jacob"
        },
        {
            "affiliations": [],
            "name": "Chen Shani"
        },
        {
            "affiliations": [],
            "name": "Dafna Shahaf"
        }
    ],
    "id": "SP:f9086abda878f62f8945bbbba0a4d0d04cfbe535",
    "references": [
        {
            "authors": [
                "Carl Allen",
                "Timothy Hospedales."
            ],
            "title": "Analogies explained: Towards understanding word embeddings",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 223\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Yuanzhi Li",
                "Yingyu Liang",
                "Tengyu Ma",
                "Andrej Risteski."
            ],
            "title": "A latent variable model approach to pmi-based word embeddings",
            "venue": "Transactions of the Association for Computational Linguistics, 4:385\u2013399.",
            "year": 2016
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "2020b. Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "David J Chalmers",
                "Robert M French",
                "Douglas R Hofstadter."
            ],
            "title": "High-level perception, representation, and analogy: A critique of artificial intelligence methodology",
            "venue": "Journal of Experimental & Theoretical Artificial Intelligence, 4(3):185\u2013211.",
            "year": 1992
        },
        {
            "authors": [
                "Xiao",
                "Hao Zhou."
            ],
            "title": "E-kar: A benchmark for rationalizing natural language analogical reasoning",
            "venue": "arXiv preprint arXiv:2203.08480.",
            "year": 2022
        },
        {
            "authors": [
                "Hsiao-Yu Chiang",
                "Jose Camacho-Collados",
                "Zachary Pardos."
            ],
            "title": "Understanding the source of semantic regularities in word embeddings",
            "venue": "Proceedings of the 24th Conference on Computational Natural Language Learning, pages 119\u2013131, Online.",
            "year": 2020
        },
        {
            "authors": [
                "Morteza Dehghani",
                "Ken Forbus."
            ],
            "title": "Qcm: A qp-based concept map system",
            "venue": "the 23nd International Workshop on Qualitative Reasoning (QR09), pages 16\u201321.",
            "year": 2009
        },
        {
            "authors": [
                "Oren Etzioni",
                "Michele Banko",
                "Stephen Soderland",
                "Daniel S Weld."
            ],
            "title": "Open information extraction from the web",
            "venue": "Communications of the ACM, 51(12):68\u201374.",
            "year": 2008
        },
        {
            "authors": [
                "Oren Etzioni",
                "Michael Cafarella",
                "Doug Downey",
                "Stanley Kok",
                "Ana-Maria Popescu",
                "Tal Shaked",
                "Stephen Soderland",
                "Daniel S Weld",
                "Alexander Yates."
            ],
            "title": "Web-scale information extraction in knowitall: (preliminary results)",
            "venue": "Proceedings of the",
            "year": 2004
        },
        {
            "authors": [
                "Thomas G Evans."
            ],
            "title": "A heuristic program to solve geometric-analogy problems",
            "venue": "Proceedings of the April 21-23, 1964, spring joint computer conference, pages 327\u2013338.",
            "year": 1964
        },
        {
            "authors": [
                "Brian Falkenhainer",
                "Kenneth D Forbus",
                "Dedre Gentner."
            ],
            "title": "The structure-mapping engine: Algorithm and examples",
            "venue": "Artificial intelligence, 41(1):1\u2013",
            "year": 1989
        },
        {
            "authors": [
                "Kenneth Forbus",
                "Jeffrey Usher",
                "Andrew Lovett",
                "Kate Lockwood",
                "Jon Wetzel"
            ],
            "title": "Cogsketch: Sketch understanding for cognitive science research and for education",
            "venue": "Topics in Cognitive Science,",
            "year": 2011
        },
        {
            "authors": [
                "Robert M French."
            ],
            "title": "The computational modeling of analogy-making",
            "venue": "Trends in cognitive Sciences, 6(5):200\u2013205.",
            "year": 2002
        },
        {
            "authors": [
                "Dedre Gentner."
            ],
            "title": "Structure-mapping: A theoretical framework for analogy",
            "venue": "Cognitive science, 7(2):155\u2013170.",
            "year": 1983
        },
        {
            "authors": [
                "Dedre Gentner",
                "Kenneth D Forbus."
            ],
            "title": "Computational models of analogy",
            "venue": "Wiley interdisciplinary reviews: cognitive science, 2(3):266\u2013276.",
            "year": 2011
        },
        {
            "authors": [
                "Alex Gittens",
                "Dimitris Achlioptas",
                "Michael W Mahoney."
            ],
            "title": "Skip-gram- zipf+ uniform= vector additivity",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 69\u201376.",
            "year": 2017
        },
        {
            "authors": [
                "Douglas R Hofstadter",
                "Emmanuel Sander."
            ],
            "title": "Surfaces and essences: Analogy as the fuel and fire of thinking",
            "venue": "Basic books.",
            "year": 2013
        },
        {
            "authors": [
                "Keith J Holyoak."
            ],
            "title": "Analogical thinking and human intelligence",
            "venue": "Advances in the psychology of human intelligence, 2:199\u2013230.",
            "year": 1984
        },
        {
            "authors": [
                "Keith J Holyoak",
                "Paul Thagard",
                "Stuart Sutherland."
            ],
            "title": "Mental leaps: analogy in creative thought",
            "venue": "Nature, 373(6515):572\u2013572.",
            "year": 1995
        },
        {
            "authors": [
                "Aniket Kittur",
                "Lixiu Yu",
                "Tom Hope",
                "Joel Chan",
                "Hila Lifshitz-Assaf",
                "Karni Gilon",
                "Felicia Ng",
                "Robert E Kraut",
                "Dafna Shahaf."
            ],
            "title": "Scaling up analogical innovation with crowds and ai",
            "venue": "Proceedings of the National Academy of Sciences, 116(6):1870\u2013",
            "year": 2019
        },
        {
            "authors": [
                "Douglas B Lenat",
                "Mayank Prakash",
                "Mary Shepherd."
            ],
            "title": "Cyc: Using common sense knowledge to overcome brittleness and knowledge acquisition bottlenecks",
            "venue": "AI magazine, 6(4):65\u201365.",
            "year": 1985
        },
        {
            "authors": [
                "Hugo Liu",
                "Push Singh."
            ],
            "title": "Conceptnet\u2014a practical commonsense reasoning tool-kit",
            "venue": "BT technology journal, 22(4):211\u2013226.",
            "year": 2004
        },
        {
            "authors": [
                "Donald G McNeil Jr",
                "Mr Od\u00f3n."
            ],
            "title": "Car mechanic dreams up a tool to ease births",
            "venue": "The New York Times, 13.",
            "year": 2013
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in neural information processing systems, 26.",
            "year": 2013
        },
        {
            "authors": [
                "Marvin Minsky."
            ],
            "title": "Society of mind",
            "venue": "Simon and Schuster.",
            "year": 1988
        },
        {
            "authors": [
                "Melanie Mitchell."
            ],
            "title": "Abstraction and analogymaking in artificial intelligence",
            "venue": "Annals of the New York Academy of Sciences, 1505(1):79\u2013101.",
            "year": 2021
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge bases? arXiv preprint arXiv:1909.01066",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych."
            ],
            "title": "Sentencebert: Sentence embeddings using siamese bertnetworks",
            "venue": "arXiv preprint arXiv:1908.10084.",
            "year": 2019
        },
        {
            "authors": [
                "Walter Ralph Reitman"
            ],
            "title": "Cognition and thought: an information processing approach",
            "year": 1965
        },
        {
            "authors": [
                "Julien Romero",
                "Simon Razniewski."
            ],
            "title": "Inside quasimodo: Exploring construction and usage of commonsense knowledge",
            "venue": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, pages 3445\u20133448.",
            "year": 2020
        },
        {
            "authors": [
                "Maarten Sap",
                "Ronan Le Bras",
                "Emily Allaway",
                "Chandra Bhagavatula",
                "Nicholas Lourie",
                "Hannah Rashkin",
                "Brendan Roof",
                "Noah A Smith",
                "Yejin Choi."
            ],
            "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
            "venue": "Proceedings of the AAAI Con-",
            "year": 2019
        },
        {
            "authors": [
                "Oren Sultan",
                "Dafna Shahaf."
            ],
            "title": "Life is a circus and we are the clowns: Automatically finding analogies between situations and processes",
            "venue": "Proceedings of the 2022 conference on empirical methods in natural language processing (EMNLP).",
            "year": 2022
        },
        {
            "authors": [
                "David P Swain."
            ],
            "title": "The water-tower analogy of the cardiovascular system",
            "venue": "Advances in Physiology Education, 24(1):43\u201350.",
            "year": 2000
        },
        {
            "authors": [
                "Peter D Turney."
            ],
            "title": "The latent relation mapping engine: Algorithm and experiments",
            "venue": "Journal of Artificial Intelligence Research, 33:615\u2013655.",
            "year": 2008
        },
        {
            "authors": [
                "Asahi Ushio",
                "Luis Espinosa Anke",
                "Steven Schockaert",
                "Jose Camacho-Collados"
            ],
            "title": "Bert is to nlp what alexnet is to cv: Can pre-trained language models identify analogies",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Yates",
                "Michele Banko",
                "Matthew Broadhead",
                "Michael J Cafarella",
                "Oren Etzioni",
                "Stephen Soderland."
            ],
            "title": "Textrunner: open information extraction on the web",
            "venue": "Proceedings of Human Language Technologies: The Annual Conference of the",
            "year": 2007
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16426\u201316442 December 6-10, 2023 \u00a92023 Association for Computational Linguistics\nExperiments show that our model correctly maps 81.2% of classical 2x2 analogy problems (guess level=50%). On larger problems, it achieves 77.8% accuracy (mean guess level=13.1%). In another experiment, we show our algorithm outperforms human performance, and the automatic suggestions of new entities resemble those suggested by humans. We hope this work will advance computational analogy by paving the way to more flexible, realistic input requirements, with broader applicability."
        },
        {
            "heading": "1 Introduction",
            "text": "One of the pinnacles of human cognition is the ability to find parallels across distant domains and transfer ideas between them. This analogous reasoning process enables us to learn new information faster and solve problems based on prior experience (Minsky, 1988; Hofstadter and Sander, 2013; Holyoak, 1984; PJM, 1966).\nThe most seminal work in computational analogy is Gentner\u2019s Structure Mapping Theory (SMT) (Gentner, 1983) and its implementation, Structure Mapping Engine (SME) (Falkenhainer et al., 1989). In a nutshell, SMT assumes input from two domains: base and target. It maps between objects\nin a base domain and objects in a target domain according to common relational structure, rather than on object attributes.\nFor example, consider the Rutherford model of the hydrogen atom, where the atom was explained in terms of the (better-understood) solar system (Falkenhainer et al., 1989): a planet revolving around the sun is mapped to an electron revolving around the nucleus. The mapping is due to shared relations between objects (revolving around, being attracted to), not object attributes (round, small).\nOne of the main criticisms brought against SME and its follow-up work is their need for extensive hand-coded input \u2013 structured representations of both the entities and their relations (see Figure 1 for the input to the atom/solar system mapping).\nChalmers et al. (1992) argued that too much human creativity is required to construct this input, and the analogy is already effectively given in the representations: \u201cA brief examination [...] shows that the discovery of the similar structure in these representations is not a difficult task. The representations have been set up in such a way that the common structure is immediately apparent. Even for a computer program, the extraction of such common structure is relatively straightforward.\u201d\nSome follow-up works avoid hand-coding LISPlike representations, generating them from sketches (Forbus et al., 2011), qualitative simulators (Dehghani and Forbus, 2009), etc. However, they still require much knowledge engineering, and thus are hard to scale. Nowadays, when the web is full of information about potential domains to transfer ideas from (McNeil Jr and Od\u00f3n, 2013), such representations do not tap into the potential of web-scale analogies for augmenting human creativity.\nThe method with the simplest input we are aware of is Latent Relation Mapping Engine (LRME) (Turney, 2008), which requires only two lists of entities to be mapped. Given two entities, they search for phrases containing both in a large corpus and\n16426\nuse them to generate simple patterns. For example, \u201ca sun-centered solar system illustrates\u201d gives rise to patterns such as \u201ca X * Y illustrates\u201d. However, such patterns are extremely simple and brittle, and LRME requires exact string matches between the domains (so \u201crevolve around\u201d is different from \u201crotate around\u201d).\nIn this work, we develop FAME, a Flexible Analogy Mapping Engine. FAME\u2019s input requirements are minimal, requiring only two sets of entities. We apply state-of-the-art NLP and IR techniques to automatically infer commonsense relations between the entities using a variety of data sources, and construct a mapping between the domains. Importantly, we do not require identical phrasings of relations. Moreover, our output is interpretable, showing how the mapping was chosen.\nUnlike previous works, we drop the strong bijectivity assumption and let the algorithm decide which entities to include in the mapping. Meaning, we allow for entities to remain unmapped. Our algorithm can also generate new suggestions for the non-mapped entities. This paves the road to algorithms that can handle even more limited input \u2013 for example, using domain names (solar system, atom) as input, or just a single mapped entity pairs (e.g., turn white blood cells into policemen and see how the analogy unfolds). Our contributions are:\n\u2022 A novel, scalable, and interpretable approach for automatically mapping two domains based on commonsense relational similarities. Our algorithm handles partial mappings and suggests additional entities. \u2022 We extend the work of Romero and Razniewski (2020) to discover salient knowl-\nedge about pairs of entities. \u2022 Our model\u2019s accuracy is 81.2% on simple,\n2x2 problem s(guess level=50%). On larger problems, it achieves 77.8% perfect mappings (guess level=13.1%). In another experiment, we outperform humans (90% vs. 70.2%) and demonstrate that our automatic suggestions resemble human suggestions. We release code and data.1"
        },
        {
            "heading": "2 Problem Definition",
            "text": "An analogy is a mapping from a base domain B into a target domain T . The mapping is based on relations, and not object attributes. Base objects are not mapped into objects that resemble them; rather, there is a common relational structure, and they are mapped to objects that play similar roles. We follow the formulation of Sultan and Shahaf (2022), brought here for completeness:\nEntities and Relations. LetB = {b1, ..., bn} and T = {t1, ..., tm} be two sets of entities. For example: B = {sun, Earth, gravity, solar system, Newton}, T = {nucleus, electrons, electricity, atom, Faraday}.\nLet R be a set of relations. A relation is a set of ordered entity pairs with some meaning. The exact representation is purposely vague, as we do not restrict ourselves to strings, embeddings, etc. Intuitively, relations should capture notions like \u201crevolve around\u201d.\nIn our example, relations between B and T include the Earth revolve around the Sun, like electrons orbit the nucleus; the Earth creates a force field of gravity, similar to electrons creating electric force fields; the Sun and the Earth are part of\n1https://github.com/shaharjacob/FAME\nthe solar system, as the nucleus and electrons are part of the atom; Newton discovered gravity, as Faraday is credited with discovering electric force.\nNote that relation is an asymmetric function, as the pairs are ordered; e.g., Newton discovered gravity, but gravity did not discover Newton.\nSlightly abusing notation, we denote the set of relations that hold between two entities e1, e2 as R(e1, e2) \u2286 2R. For example, R(Earth, Sun) contains {revolve around, attracted to}, etc. For clarity, we sometimes use RB , RT to emphasize that the entities belong to the B, T domain. Similarity. Let sim be a similarity metric between two sets of relations, sim : 2R \u00d7 2R \u2192 [0,\u221e). Intuitively, when applied to singletons, we want our similarity metric to capture how relations are like each other. For example, \u201crevolve around\u201d is similar to \u201corbit\u201d and (to a lesser degree) \u201cspiral\u201d. When applied to sets of relations, we want sim to be higher if the two sets share many distinct relations. For example, {revolve around, attracted to} should be more similar to {orbit, drawn into} than to {revolve around, orbit} (as the last set does not include any relation similar to attraction). In Section 3.2 we present our sim implementation.\nGiven one pair from B and one from T , we define similarity in terms of their relations. SinceR is asymmetric, we consider both directions:\nsim\u2217(b1, b2, t1, t2) =\nsim(RB(b1, b2),RT (t1, t2))+ sim(RB(b2, b1),RT (t2, t1))\nObjective. Our goal is to output a mappingM : B \u2192 T \u222a\u22a5 such that no two B entities are mapped to the same T entity (Table 1). Mapping into \u22a5 means the entity was not mapped to any entity in the T domain.\nWe look for the mappingM\u2217 that captures the best inter-domain analogical structure similarity by\nmaximizing the relational similarity:\nargmax M\nn\u22121\u2211\nj=1\nn\u2211\ni=j+1\nsim\u2217(bj , bi,M(bj),M(bi))\nNote: if bi or bj maps to \u22a5, sim\u2217 is defined to be 0."
        },
        {
            "heading": "3 Analogous Matching Algorithm",
            "text": "We wish to find the best mapping from B to T . We first extract relations between entity pairs from the same domain (Section 3.1). Then, we compute similarity between entity pairs that could be mapped (Section 3.2). Finally, we build the mapping (Section 3.3)."
        },
        {
            "heading": "3.1 Relation Extraction",
            "text": "Automatically extracting relations is a key part of our algorithm, as it eliminates the need for extensive manual curation of the input. We focus on commonsense relations (e.g., the Earth revolves around the Sun), as opposed to situational relations (e.g., the book is on the table). This broadly falls under open information extraction (OIE), the task of generating a structured representation of the information in a text. There has been a lot of work in this area, especially attempts to automate the construction of commonsense datasets (Etzioni et al., 2008, 2004; Yates et al., 2007; Lenat et al., 1985; Sap et al., 2019).\nGiven two entities, we automatically extract relations from multiple sources: ConceptNet. A commonsense dataset, containing about 1.5M nodes (Liu and Singh, 2004). For each entity, we receive a list of (predicate, entity), which we filtered to match the second entity (single or plural form). The predicates serve as our relations. Open Information Extraction. A database automatically extracted from a large web corpus (Etzioni et al., 2008). It contains over 5B triplets of the form (subject, predicate, object). We searched for a match between both entities in the (subject, object) fields, and used the predicates as our relations. GPT-3 (text-davinci-001).2 We used a generative pretrained large language model (LM) as a knowledge base in a few-shot manner (Petroni et al., 2019; Brown et al., 2020b). We input a prompt of four analogies, e.g., \u201cQ: What are the relations between gravity and Newton?, A: Newton discovered gravity. A: Newton invented gravity.\u201d (see Section\n2GPT-3 is the only data source that is not freely available. All queries needed for this paper accumulated to less than $50.\nA.2.3 for the full prompt). GPT-3 outputs up to three sentences per query. We kept only sentences of the form <entity> <text> <entity>, treating the <text> as the relation.\nQuasimodo. A commonsense knowledge base that focuses on salient properties of objects (Romero and Razniewski, 2020). It contains more than 3.5M triplets of (subject, predicate, object). It considers questions instead of statements. For instance, if people search for an answer to \u201cWhy is the sky blue?\u201d, this implies that the sky is blue. Whenever our two entities appeared in the (subject, object) fields, we extracted their predicates as relations.\nQuasimodo++. A relation extraction method that we develop, inspired by Quasimodo. Quasimodo was constructed using questions about a single entity; we extended it to questions exploring relations between pairs of entities. We used Google\u2019s query auto-completion to tap into the query logs, asking questions containing both desired entities, such as \u201cHow does Earth * Sun\u201d, \u201cHow is Earth * Sun\u201d, and \u201cWhy does Sun * Earth\u201d for every pair of entities (see Figure 2 for an example). The exact regular expressions we used can be found in Section A.1.\nWe presented here the knowledge sources we implemented. We note that our algorithm is easy to extend to new sources and that we expect that its robustness will increase with coverage."
        },
        {
            "heading": "3.2 Scoring Entity Pairs",
            "text": "We wish to calculate sim\u2217(bi, bj , tk, tp) for bi,j \u2208 B, tk,p \u2208 T , 1 \u2264 i < j \u2264 n, 1 \u2264 k 6= p \u2264 m.\nIn Section 2 we specified desiderata of sim, especially that it is higher if the two sets share many distinct relations. We now present our implementation of sim.\nWithout loss of generality, let us consider sim(RB(b1, b2),RT (t1, t2)). We first extract all relations RB(b1, b2),RT (t1, t2). Next, we calcu-\nlate the score between each relation inRB(b1, b2) and each relation inRT (t1, t2). We create a complete bipartite graph where the left side nodes are the relations ofRB(b1, b2), and the right side nodes are the relations ofRT (t1, t2) (Figure 3). The edge weights (w) are the cosine similarity of the nodes\u2019 sBERT embedding (Reimers and Gurevych, 2019).\nWe remove non-informative relations by extracting the top-frequent n-grams (n = {1, 2, 3, 4}) from Wikipedia and setting their score to zero. Edges that did not reach a threshold (chosen using hyper-parameter search, see Section 3.3) were set to zero.\nNext, we cluster similar relations on each side (e.g., \u201crevolve around\u201d and \u201ccircle around\u201d) to avoid double-counting. We use hierarchical agglomerative clustering based on the cosine embedding similarity (threshold = 0.5; see Section 3.3). The weight of edges between two clusters is the maximal weight of an edge between their nodes (see Figure 3; colors correspond to clusters).\nFinally, we apply Maximum-Weight Bipartite Matching on the clusters (see Section 3.3). The similarity score sim(RB(b1, b2),RT (t1, t2)) is defined as the sum of the remaining edges."
        },
        {
            "heading": "3.3 Building a Mapping",
            "text": "Using the score mappings between pairs, we can compose larger mappings. We use beam-search, starting from the most promising pair-mappings found in Section 3.2. In each iteration, we expand the 20 most promising partial mappings, testing each possible mapping between single entities of B and T (that are consistent with the current partial\nmapping \u2013 i.e., a B entity cannot map to multiple T entities). When expansions stop increasing the score, we stop the search and select the mapping with the highest score.\nFigure 4 shows a snippet from our UI. Input appears on the top. FAME\u2019s output mapping is represented as a graph: nodes correspond to single entity mappings (e.g., Sun to nucleus). Edges represent the shared relational structure. Each edge contains some of the shared relations between the mapped pairs corresponding to its endpoints (e.g., \u201cmore massive than\u201d) and their similarity score (note the edges are directional). To ease visualization, we show at most two relations per edge. The thickness of an edge corresponds to its weight.\nA note on the solution space. In previous works n = m andM is a bijective function. Meaning, M is both injective (one-to-one; each element in the target is the image of at most one element in the source) and surjective (onto; all the target terms are covered). In other words, no entity is left unmapped. In that case, the solution space\u2019s cardinality is n!.\nWe allow for n 6= m and for entities to remain unmapped. Without loss of generality let n \u2264 m. The cardinality is then (\u2211n i=0 ( n i ) m! (m\u2212i)! ) \u2212 (n \u00b7m), where i is the number of matched entities. We subtract n \u00b7m because we do not allow for a mapping of size 1; our algorithm starts by mapping pairs and then adds single-entity mapping at each iteration of the beam search.\nThis relaxation of the bijective constraint drastically increases the space; for n = 7, n! = 5, 040, while our space is of size 130, 922.\nHyper-Parameter Search. We constructed a new dataset to set our model\u2019s hyper-parameters (See Appendix A). The dataset contains 36 analogical mapping problems created by ten volunteers, not from our research team. We showed them example analogies and asked them to generate new ones. An expert from our team verified their output, discarding 4 analogies. Domain size varied between 3 to 5 (average size=3.4).\nOn the problems generated by the volunteers, FAME achieves 83.3% perfect mappings (the whole mapping is correct). If we consider single mappings separately, it achieves 89.4% accuracy."
        },
        {
            "heading": "4 Entity Suggestion",
            "text": "One of the main limitations of previous analogical mapping algorithms is their inability to automatically expand analogies. This is especially interesting in our case, as we allow for unmapped entities; thus, suggesting new entities could identify potential mapping candidates for the unmapped entities.\nFor example, let B = {Sun, Earth, gravity, Newton} and T = {nucleus, electron, electricity}. The correct mapping is Sun\u2192 nucleus, Earth\u2192 electron, gravity\u2192 electricity, leaving Newton with no mapping. Our goal is to suggest candidate entities that preserve the relational structure.\nIntuitively, we look at the relations Newton shares with other B entities (e.g., discovered gravity), and try to see which T entity plays a corresponding role (i.e., who discovered electricity?).\nMore formally, suppose we wish to find candidates t\u2217 for mapping to bn. We first extract the relations of Rb(bi, bn), \u2200i \u2208 [n] (denoted as Rbi).\nWe then iterate over all relations r \u2208 Rbi and use the pair {M(bi), r} to extract suggestions for t\u2217.\nWe use Open Information Extraction, Quasimodo, and Quasimodo++. While our method was previously used to extract relations given a pair of two entities, we now use it to extract entities given a pair of {entity, relation}. This entails filtering on the predicate field in our commonsense datasets and changing the queries in Quasimodo++.\nAs suggestions tend to be noisy, we cluster all extracted entities (similarly to the clustering from Section 3.2). We remove clusters of size < 2.\nFor each suggestion cluster, we rerun our analogous matching algorithm with a representative entity from that cluster (the closest to the cluster\u2019s center of mass). We pick the cluster whose representative resulted in the mapping with the highest score. As the commonsense datasets we work with operate mostly on string matching, small changes (e.g., Benjamin Franklin/Ben Franklin) could sometimes result in slightly different results. Thus, we perform one final round, with all entities from our chosen cluster, and pick the highest score mapping."
        },
        {
            "heading": "5 Evaluation",
            "text": "In this section, we evaluate FAME. We test its ability to identify the correct mapping (Section 5.1), and compared it to both related works (Section 5.2) and human performance (Section 5.3)."
        },
        {
            "heading": "5.1 Performance on Analogy Problems",
            "text": "2x2 problems. One of the things that might have held computational analogy back is the lack of high-quality, large-scale datasets. Most datasets are small and focus on classical 2x2 problems (A : B :: C : D), similar to SAT questions.\nWe start by testing FAME on this standard type of analogies. We use 80 problems from Green et al. (2010), split into 40 near and 40 far analogies (e.g., for \u201canswer:riddle\u201d, near analogy is \u201csolution:problem\u201d, far analogy is \u201ckey:lock\u201d). While the dataset is small, we believe it is still interesting to explore. Our algorithm managed to perfectly map 85% of near analogies and 77.5% of far ones. Random guess baseline is 33.3% (see Section 3.3).\nExtended problems. Encouraged by the results of the 2x2 problems, we explore more complex problems. We decided to extend the Green far analogies (which are harder than the near ones). We had three experts go over the dataset together and brainstorm potential extensions. On four problems, the experts did not manage to agree on any additional mappings, leaving us with 36 extended problems (average domain size 3.3).\nOur algorithm perfectly mapped 77.8% of the extended problems. Random baseline is 13.1% on average. As we relax the bijection assumption, FAME\u2019s average guess level is even lower \u2013 2.2% (see Section 3.3). If we look beyond the top-rated solution, our algorithm has the correct solution in its top-2 guesses 83.3% of the time and 91.7% for top-3.\nError analysis. We found 3 main causes of error: \u2022 Coverage (for example, we could not find\na relation between \u201choof\u201d and \u201choofprint\u201d). This prompted us to ablate the knowledge sources FAME uses (Table 2). Results show the importance of the generative LM approach. Open IE is also important, especially for the more complex analogies (far and extended). Some sources, such as ConceptNet, did not seem to contribute much. \u2022 Noisy relations that are either peculiar or plain wrong (e.g., \u201ca footballer can iron\u201d). \u2022 Embedding similarity (for example, \u201cproduce\u201d and \u201cis produced by\u201d have a high similarity score). This is exacerbated by ambiguity (e.g., the word \u201cpen\u201d referred to \u201cpigpen\u201d and not to the writing instrument)."
        },
        {
            "heading": "5.2 Comparison to Related Work",
            "text": "SME line of work. We had difficulty comparing FAME to SME (Falkenhainer et al., 1989) and its extensions, due to their complex input requirements. LRME (Turney, 2008) is closest to our setting, but no code or demo is available. Thus, we compare to their published results on a set of 20 problems.\nLRME\u2019s entities include nouns, verbs, and adjectives. Since FAME expects noun phrases, we filtered out all other input terms (one problem has only a single noun, so we are left with 19 problems). It is hard to compare in this setup (and unfortunately, authors did not report which partial mappings were correct). Still, LRME\u2019s accuracy was 75%, whereas FAME achieved 84.2%.\nWhile the size of the problems is smaller when restricted to nouns, we believe the noun-only setting is harder. The verbs and adjectives often provide hints that significantly constrain the search space. For example, in problem A6 (Turney, 2008) (mapping a projectile to a planet) there is one adjective in each domain (parabolic, elliptical). Those adjectives can only apply to one or two of the nouns (i.e., you cannot have parabolic earth, air, or gravity), effectively giving away the noun mapping.\nAs a side note, we also believe that our nounonly input is a cleaner problem setting, as it is often easier to automatically identify the entities in a domain than to identify the attributes and verbs relevant for the analogy. In the words of LRME\u2019s authors, \u201cLRME is not immune to the criticism of Chalmers et al. (1992), that the human who generates the input is doing more work than the computer that makes the mapping.\u201d We believe FAME is a step in the right direction in this regard.\nPretrained LMs. In the absence of a baseline, we turn to a generative pretrained large LM known to have impressive commonsense abilities \u2013 GPT-3.5 (text-davinci-002). We used 4 random examples from the hyper-parameter search dataset. After some experimentation with prompt engineering, we chose two variants (see A.2.3).\nThe results are summarized in Table 3. GPT-3.5 does well on the 2x2 datasets (Green et al., 2010). However, both datasets appear on the web, and perhaps GPT-3.5 was exposed to them during training (data leakage). In particular, we found some of the answers via a simple web search (Figure A.6).\nMoreover, GPT-3.5\u2019s performance drops on the extended set, where problems are complex and do not appear on the web. Interestingly, it does not even manage to return a valid mapping in some of the cases. This exercise improves our understanding of FAME\u2019s strengths and weaknesses.\nE-KAR dataset. Chen et al. (2022) recently released a relevant dataset, E-KAR, for rationalizing analogical reasoning. The dataset consists of multiple-choice problems from civil service\nexams in China. For example, for the source triplet \u201ctea:teapot:teacup\u201d, the correct answer is \u201ctalents:school:enterprise\u201d. The reasoning is that both teapot and teacup are containers for tea. After the tea is brewed in the teapot, it is transported into the teacup. Similarly, both school and enterprise are organizations. After talents are educated in school, they are transported into enterprise.3\nThe E-KAR test set has no labels, so we used their validation set (N=119) to test FAME. As our task is different, we only took source entities (as B) and entities from the correct answer (as T ). We filtered questions without nouns, resulting in N=101.\nFAME found the right mapping 68.3% of the time. A closer examination of FAME\u2019s mistakes revealed that\u223c 75% of them occurred due to relation types that are not at all covered by our framework: either ternary relations (soldier:doctor:military doctor\u2192 car:electric vehicle:electric car; the last term is a combination of the first two) or relations based on sharing some attribute (so \u201cboth containers for holding tea\u201d is mapped to \u201cboth are organizations\u201d). Some of the attribute-based mappings work at the whole-set level, so each entity on B could map to each entity on T (yellow:red:white \u2192 sad:happy:angry). Thus, we conclude there is a big gap between FAME and E-KAR\u2019s assumptions."
        },
        {
            "heading": "5.3 Comparison to People",
            "text": "We compare FAME with human thinking in a 2- phase experiment.4 In the closed-world phase, the participants received ten structure mapping problems, in which they were asked to match instances from B to T . The domains included between 3-5 entities (Table A.4). Participants were instructed to map each B entity into exactly one T entity.\n3Interestingly, the authors of this paper thought that the \u201cpassengers:bus:taxi\u201d answer was the correct one, based on containment and size relations.\n4The experiment received ethics committee approval. See full instructions in Section A.4.\nIn the open-world phase, participants received five mapped problems, but one entity was left blank (Table A.5). Participants were instructed to fill in the blank with an entity that preserves the analogy. Participants. We recruited 304 participants using social media. The compensation was a chance to win one of three $30 vouchers. 76.6% of our participants were between the ages 18-35 and 17.2% are between 36-45 (self-reported). Closed-world mapping. FAME missclassified one problem compared to gold standard (A9, Table A.4), achieving 90% accuracy (human baseline was 70.2%; see full distribution in Table A.4).\nProblem A6 has the lowest human accuracy (35.5%), and is also the largest one (|B| = |T | = 5). A closer examination of its confusion matrix reveals that while FAME correctly mapped water to heat and pressure to temperature, 15% of people switched the two. This might be due to the strong semantic pairing of water and temperature. FAME is immune to this, as it relays on relations.\nOn average, each participant mapped the problem the same as FAME 78% of the times. Overall, FAME outperforms humans, and most of the disagreement is due to human errors. Open-world entity suggestion. We presented participants with five mapped problems where one entity was left blank (Table A.5) and asked them to fill in the black while preserving the analogy.\nFor all five problems, an entity from FAME\u2019s top two completions appeared in humans\u2019 top three completions (Table A.6). Meaning, our algorithm\u2019s top suggestions are similar to humans\u2019. Only in one example (B5) one of the top two algorithm\u2019s completions appeared third in humans\u2019 (in the rest it is first or second). We suspect that this confusion in B5 occurred because gravity and Newton reminded participants of the term apple.\nFigure 5 shows a word cloud for answers to problem B1. While most responses are quite similar, some participants returned creative and appropriate solutions (e.g., treasure chest, jewelry box, car)."
        },
        {
            "heading": "6 Related Work",
            "text": "Computational analogy-making dates back to the 1960s (Evans, 1964; Reitman, 1965). Analogymaking approaches are broadly categorized as symbolic, connectionist, and hybrid (French, 2002; Mitchell, 2021; Gentner and Forbus, 2011).\nSymbolic approaches usually represent input as structured sets of logic statements. Our work falls\nunder this branch, as well as SME (Falkenhainer et al., 1989) and its follow-up work. LRME (Turney, 2008) is the closest to our work, as it automatically extracts the relations. Unlike FAME, LRME requires exact matches of relations across different domains. We also focus on nouns only, making the problem harder, and relax the bijection assumption, allowing for automatically extending analogies.\nNLP. Analogy-making received relatively little attention in NLP. The best-known task is word analogies, often used to measure embeddings\u2019 quality (inspired by Word2Vec\u2019s \u201cking - man + woman = queen\u201d example (Mikolov et al., 2013)). Follow-up work explored embeddings\u2019 linear algebraic structure (Arora et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019) or compositional nature (Chiang et al., 2020), neglecting relational similarity. A recent work on analogies between procedural texts (Sultan and Shahaf, 2022) did study relational similarity, but extracted the relations from the input texts, with no commonsense augmentations.\nRecently, there have been efforts to study LMs\u2019 analogical capabilities (Ushio et al., 2021; Brown et al., 2020a). Findings indicate they struggle with abstract and complex relations and results depend strongly on LM\u2019s architecture and parameters.\nKittur et al. (2019) combined NLP and crowds for product analogies without explicitly modeling entities and relations, but instead automatically extracting schemas of the product."
        },
        {
            "heading": "7 Conclusions and Future Work",
            "text": "Detecting deep structural similarity across distant domains and transferring ideas between them is central to human thinking. We presented FAME, a novel method for analogy making. Compared to previous works, FAME is more expressive, scal-\nable, robust and interpretable. It also allows partial matches and automatic entity suggestions to extend the analogies.\nFAME correctly maps 81.2% of classical 2x2 analogy problems. On larger problems, it achieves 77.8% perfect mappings (mean guess level=13.1%). FAME also outperforms humans in solving analogy mapping problems (90% vs. 70.2%). Interestingly, our automatic suggestions of new entities resemble those suggested by humans.\nIn future work, we plan to improve coverage and extend our framework to more than just binary relations, as sometimes the key to an analogy is a relation involving more than two objects. In addition, we plan to improve our similarity measure, to address both context (to solve ambiguity) and the difference between active and passive relations. We plan to explore different forms of input, such as algorithms that take as input very partial domains, perhaps even just domain names (e.g., solar system, atom) and populate the domains with entities, or algorithms incorporating user feedback.\nTo conclude, we hope FAME will pave the way for analogy-making algorithms that require lessrestrictive inputs and can scale up and tap into the vast amount of potential inspiration the web offers, augmenting human creativity."
        },
        {
            "heading": "8 Ethical Considerations & Limitations",
            "text": "While FAME can assist humans by inspiring nontrivial solutions to problems, it has been shown that humans struggle with detecting caveats in presented analogies (Holyoak et al., 1995). For example, the cardiovascular system is often taught to medical students in terms of water supply system (Swain, 2000). However, this analogy might also confuse them, as it ignores important differences between water and blood (e.g., blood clots). Thus, while our output is interpretable, it might still mislead people, and it is important to alert the users to this possibility.\nAnother issue is the fact that FAME\u2019s coverage highly depends on external resources (ConceptNet, Google AutoComplete, etc.). This might be particularly problematic when applied to low-resource languages. As the relations we look for are commonsense relations, rather than cultural or situational ones, using automatic translation might ameliorate the problem.\nLastly, we also note these resources evolve over time, and thus if one is interested in reproducibility,\nit is necessary to save snapshots of the extracted relations."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the reviewers for their insightful comments. This work was supported by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and innovation programme (grant no. 852686, SIAM).\nIn memory of the more than one thousand victims of the horrific massacre carried out by Hamas terrorists on October 7th, 2023."
        },
        {
            "heading": "Q: Find an analogical mapping between the entities",
            "text": "\u201ceraser\u201d, \u201cpaper\u201d and \u201cpencil\u201d and the entities \u201ckeyboard\u201d, \u201cdelete\u201d and \u201cscreen\u201d. A: eraser:pencil:paper::delete:keyboard:screen\n(or) A: eraser -> delete, pencil -> keyboard, paper -> screen\nA.2.3 Possible leakage Example answers for chosen analogies from Green eval dataset found via a simple web search can be found in Figure 6\nA.3 Repository\nTo ease the access and usage of our code we use Docker. Its main goal is to shift the cross-platform installation burden from the user to the developer. Unfortunately, we cannot share our Docker due to\nanonymity concerns (username). We will include it in the non-anonymized version.\nWe provide a React based web interface, currently available only locally. This system is used to visualize the graphs created by the algorithm\u2019s mapping output. In addition, it visualizes the relations between entities, their similarity, and the clustering. This interface is useful for assisting in developing, debugging and understanding the algorithm\u2019s output. The demo is accessible using our repository1.\nA.4 Experiments Snippets of the experimental setup (including instructions) can be found in Figures 7, 8.\nTable 4 depicts the ten analogical proportion problems used in the structure mapping experiment (closed-world mappings in Section 5.2). Accuracy denotes the percentage of human participants who mapped from B to T correctly. Results show this task is non-trivial even for humans.\nTable 6 illustrates the experimental setup for the second phase of our experiment, in which participants received a solved mapping problem with one entity left out (open-World in Section 5.2).\nTable 5 contains all solved analogy problems used in the second phase of the experiment (entity suggestion, see open-World in Section 5.2). Participants were given with the complete mapping, but with a missing entity (as presented here).\nA.5 E-kar Table 7 shows an example of a problematic problem from E-KAR dataset."
        }
    ],
    "title": "FAME: Flexible, Scalable Analogy Mappings Engine",
    "year": 2023
}