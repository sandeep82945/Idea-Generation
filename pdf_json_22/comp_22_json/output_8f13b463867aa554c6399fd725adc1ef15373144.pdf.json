{
    "abstractText": "Cross-modal representation learning learns a shared embedding between two or more modalities to improve performance in a given task compared to using only one of the modalities. Cross-modal representation learning from different data types \u2013 such as images and time-series data (e.g., audio or text data) \u2013 requires a deep metric learning loss that minimizes the distance between the modality embeddings. In this paper, we propose to use the contrastive or triplet loss, which uses positive and negative identities to create sample pairs with different labels, for cross-modal representation learning between image and timeseries modalities (CMR-IS). By adapting the triplet loss for cross-modal representation learning, higher accuracy in the main (time-series classification) task can be achieved by exploiting additional information of the auxiliary (image classification) task. We present a triplet loss with a dynamic margin for single label and sequence-to-sequence classification tasks. We perform extensive evaluations on synthetic image and time-series data, and on data for offline handwriting recognition (HWR) and on online HWR from sensorenhanced pens for classifying written words. Our experiments show an improved classification accuracy, faster convergence, and better generalizability due to an improved cross-modal representation. Furthermore, the more suitable generalizability leads to a better adaptability between writers for online HWR. INDEX TERMS contrastive learning, cross-modal retrieval, online handwriting recognition, optical character recognition, representation learning, sensor-enhanced pen, sequence-based learning, triplet learning",
    "authors": [
        {
            "affiliations": [],
            "name": "FELIX OTT"
        },
        {
            "affiliations": [],
            "name": "DAVID R\u00dcGAMER"
        },
        {
            "affiliations": [],
            "name": "Lucas Heublein"
        },
        {
            "affiliations": [],
            "name": "Bernd Bischl"
        },
        {
            "affiliations": [],
            "name": "Christopher Mutschler"
        }
    ],
    "id": "SP:893334c34474bc5689040c2e1f5245ea918a66c4",
    "references": [
        {
            "authors": [
                "Y. Peng",
                "X. Huang",
                "Y. Zhao"
            ],
            "title": "AnOverview of Cross-media Retrieval: Concepts, Methodologies, Benchmarks and Challenges",
            "venue": "Trans. on Circuits and Systems for Video Technology, vol. 28(9), Apr. 2017, pp. 2372\u20132385.",
            "year": 2017
        },
        {
            "authors": [
                "H. Lee",
                "J. Lee",
                "J.Y.-H. Ng",
                "P. Natsev"
            ],
            "title": "Large Scale Video Representation Learning via Relational Graph Clustering",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "N. Sarafianos",
                "X. Xu",
                "I.A. Kakadiaris"
            ],
            "title": "Adversarial Representation Learning for Text-to-Image Matching",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Seoul, Korea, 2019, pp. 5814\u20135824.",
            "year": 2019
        },
        {
            "authors": [
                "V. Ranjan",
                "N. Rasiwasia",
                "C.V. Jawahar"
            ],
            "title": "Multi-Label Cross-Modal Retrieval",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Santiago de Chile, Chile, Dec. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "X. Huang",
                "Y. Peng",
                "M. Yuan"
            ],
            "title": "MHTN: Modal-Adversarial Hybrid Transfer Network for Cross-Modal Retrieval",
            "venue": "Trans. on Cybernetics, vol. 50(3), 2020, pp. 1047\u20131059.",
            "year": 2020
        },
        {
            "authors": [
                "F. Faghri",
                "D.J. Fleet",
                "H.R. Kiros",
                "S. Fidler"
            ],
            "title": "VSE++: Improving Visual-Semantic Embeddings with Hard Negatives",
            "venue": "British Machine Vision Conf. (BMVC), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Chen",
                "H. Hu",
                "H.Wu",
                "Y. Jiang",
                "C.Wang"
            ],
            "title": "Learning the Best Pooling Strategy for Visual Semantic Embedding",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "J.H. Lim",
                "P.O.O. Pinheiro",
                "N. Rostamzadeh",
                "C. Pal",
                "S. Ahn"
            ],
            "title": "Neural Multisensory Scene Inference",
            "venue": "Advances in Neural Information Processing Systems (NIPS), vol. 32(807), 2019, pp. 8996\u20139006.",
            "year": 2019
        },
        {
            "authors": [
                "F.M. Hafner",
                "A. Bhuyian",
                "J.F.P. Kooij",
                "E. Granger"
            ],
            "title": "Cross-Modal Distillation for RGB-Depth Person Re-Identification",
            "venue": "Computer Vision and Image Understanding (CVIU), vol. 103352, Jan. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V. Vapnik",
                "R. Izmailov"
            ],
            "title": "Learning Using Privileged Information: Similarity Control and Knowledge Transfer",
            "venue": "Journal of Machine Learning Research (JMLR), Sep. 2015, pp. 2023\u20132049.",
            "year": 2015
        },
        {
            "authors": [
                "A. Momeni",
                "K. Tatwawadi"
            ],
            "title": "Understanding LUPI (Learning Using Privileged Information)",
            "venue": "2018. [Online]. Available: https://web.stanford. edu/~kedart/files/lupi.pdf",
            "year": 2018
        },
        {
            "authors": [
                "Y. Wei",
                "Y. Zhao",
                "C. Lu",
                "S. Wei",
                "L. Liu",
                "Z. Zhu",
                "S. Yan"
            ],
            "title": "Cross- Modal Retrieval with CNN Visual Features: A New Baseline",
            "venue": "Trans. on Cybernetics, vol. 47(2), Mar. 2016, pp. 449\u2013460.",
            "year": 2016
        },
        {
            "authors": [
                "T.-T. Do",
                "T. Tran",
                "I. Reid",
                "V. Kumar",
                "T. Hoang",
                "G. Carneiro"
            ],
            "title": "A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019, pp. 10 404\u201310 413.",
            "year": 2019
        },
        {
            "authors": [
                "M. Long",
                "Y. Cao",
                "L. Wang",
                "M.I. Jordan"
            ],
            "title": "Learning Transferable Features with Deep Adaptation Networks",
            "venue": "Intl. Conf. on Machine Learning (ICML), vol. 37, Jul. 2015, pp. 97\u2013105.",
            "year": 2015
        },
        {
            "authors": [
                "F. Schroff",
                "D. Kalenichenko",
                "J. Philbin"
            ],
            "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, MA, Jun. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "F. Ott",
                "D. R\u00fcgamer",
                "L. Heublein",
                "B. Bischl",
                "C. Mutschler"
            ],
            "title": "Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Lisboa, Portugal, Oct. 2022, pp. 5934\u20135943.",
            "year": 2022
        },
        {
            "authors": [
                "S. Deldari",
                "H. Xue",
                "A. Saeed",
                "D.V. Smith",
                "F.D. Salim"
            ],
            "title": "COCOA: Cross Modality Contrastive Learning for Sensor Data",
            "venue": "Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), vol. 6(3), article 108, Sep. 2022, pp. 1\u201328.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Jain",
                "C.I. Tang",
                "C. Min",
                "F. Kawsar",
                "A. Mathur"
            ],
            "title": "ColloSSL: Collaborative Self-Supervised Learning for Human Activity Recognition",
            "venue": "Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), Feb. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S. Venkataramanan",
                "E. Kijak",
                "L. Amsaleg",
                "Y. Avrithis"
            ],
            "title": "Align- Mix: Improving Representations by Interpolating Aligned Fetaures",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2022, pp. 19 174\u201319 183.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Wan",
                "Q. Zou"
            ],
            "title": "Learning Metric Features for Writer-Independent Signature Verification using Dual Triplet Loss",
            "venue": "Intl. Conf. on Pattern Recognition (ICPR), Milan, Italy, Jan. 2021, pp. 3853\u20133859.",
            "year": 2021
        },
        {
            "authors": [
                "W. Li",
                "X. Yang",
                "M. Kong",
                "L. Wang",
                "J. Huo",
                "Y. Gao",
                "J. Luo"
            ],
            "title": "Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning",
            "venue": "arXiv preprint arXiv:2107.10419, Jul. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Kim",
                "D. Kim",
                "M. Cho",
                "S. Kwak"
            ],
            "title": "Proxy Anchor Loss for Deep Metric Learning",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2020, pp. 3238\u20133247.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Zhen",
                "P. Rai",
                "H. Zha",
                "L. Carin"
            ],
            "title": "Cross-Modal Similarity Learning via Pairs, Preferences, and Active Supervision",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI), Jan. 2015, pp. 3203\u20133209.",
            "year": 2015
        },
        {
            "authors": [
                "D. Zhang",
                "Z. Zheng"
            ],
            "title": "Joint Representation Learning with Deep Quadruplet Network for Real-Time Visual Tracking",
            "venue": "IEEE Intl. Joint Conf. on Neural Networks (IJCNN), Glasgow, UK, Jul. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A.F. Biten",
                "A. Mafla",
                "L. G\u00f3mez",
                "D. Karatzas"
            ],
            "title": "Is an Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching",
            "venue": "IEEE/CVF Winter Conf. on Applications of Computer Vision (WACV), Waikoloa, HI, Jan. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Diao",
                "Y. Zhang",
                "L. Ma",
                "H. Lu"
            ],
            "title": "Similarity Reasoning and Filtration for Image-Text Matching",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI), vol. 35(2), 2021, pp. 1218\u20131226.",
            "year": 2021
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark",
                "G. Krueger",
                "I. Sutskever"
            ],
            "title": "Learning Transferable Visual Models From Natural Language Supervision",
            "venue": "Intl. Conf. on Machine Learning (ICML), vol. 139, 2021, pp. 8748\u20138763.",
            "year": 2021
        },
        {
            "authors": [
                "M.M.M. Fahmy"
            ],
            "title": "Online Signature Verification and Handwriting Classification",
            "venue": "Journal on Ain Shams Engineering (ASEJ), vol. 1(1), Sep. 2010, pp. 59\u201370.",
            "year": 2010
        },
        {
            "authors": [
                "R. Plamondon",
                "S.N. Srihari"
            ],
            "title": "On-line and Off-line Handwriting Recognition: A Comprehensive Survey",
            "venue": "Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 22(1), Jan. 2000, pp. 63\u201384.",
            "year": 2000
        },
        {
            "authors": [
                "F. Alimoglu",
                "E. Alpaydin"
            ],
            "title": "Combining Multiple Representations and Classifiers for Pen-based Handwritten Digit Recognition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), vol. 2, Ulm, Germany, Aug. 1997.",
            "year": 1997
        },
        {
            "authors": [
                "J.K. Chen",
                "W. Xie",
                "Y.K. He"
            ],
            "title": "Motion-based Handwriting Recognition",
            "venue": "arXiv preprint arXiv:2101.06022, Jan. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Schrapel",
                "M.-L. Stadler",
                "andM. Rohs"
            ],
            "title": "Pentelligence: Combining Pen Tip Motion and Writing Sounds for Handwritten Digit Recognition",
            "venue": "Proc. of the CHI Conf. on Human Factors in Computing Systems, Apr. 2018, pp. 1\u201311.",
            "year": 2018
        },
        {
            "authors": [
                "J.-S. Wang",
                "Y.-L. Hsu",
                "C.-L. Chu"
            ],
            "title": "Online Handwriting Recognition Using an Accelerometer-Based Pen Device",
            "venue": "Intl. Conf. on Computer Science and Engineering (CSE), Jul. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "T. Deselaers",
                "D. Keysers",
                "J. Hosang",
                "H.A. Rowley"
            ],
            "title": "GyroPen: Gyroscopes for Pen-Input with Mobile Phones",
            "venue": "Trans. on Human- Machine Systems, vol. 45(2), Apr. 2015, pp. 263\u2013271.",
            "year": 2015
        },
        {
            "authors": [
                "F. Ott",
                "M. Wehbi",
                "T. Hamann",
                "J. Barth",
                "B. Eskofier",
                "C. Mutschler"
            ],
            "title": "The OnHW Dataset: Online Handwriting Recognition from IMU- Enhanced Ballpoint Pens with Machine Learning",
            "venue": "Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), vol. 4(3), article 92, Canc\u00fan, Mexico, Sep. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "F. Ott",
                "D. R\u00fcgamer",
                "L. Heublein",
                "B. Bischl",
                "C.Mutschler"
            ],
            "title": "Joint Classification and Trajectory Regression of OnlineHandwriting using aMulti- Task Learning Approach",
            "venue": "IEEE/CVFWinter Conf. for Applications on Computer Vision (WACV), Waikoloa, HI, Jan. 2022, pp. 266\u2013276.",
            "year": 2022
        },
        {
            "authors": [
                "A. Kla\u00df",
                "S.M. Lorenz",
                "M.W. Lauer-Schmaltz",
                "D. R\u00fcgamer",
                "B. Bischl",
                "C. Mutschler",
                "F. Ott"
            ],
            "title": "Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift",
            "venue": "IJCAI-ECAI Intl. Workshop on Spatio-Temporal Reasoning and Learning (STRL), vol. 3190, Vienna, Austria, Jul. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Ott",
                "D. R\u00fcgamer",
                "L. Heublein",
                "T. Hamann",
                "J. Barth",
                "B. Bischl",
                "C. Mutschler"
            ],
            "title": "Benchmarking Online Sequence-to-Sequence and Character-based Handwriting Recognition from IMU-Enhanced Pens",
            "venue": "International Journal on Document Analysis and Recognition (IJDAR), vol. 25(12), Sep. 2022, p. 385\u2013414.",
            "year": 2022
        },
        {
            "authors": [
                "A. Vinciarelli",
                "M.P. Perrone"
            ],
            "title": "Combining Online and Offline Handwriting Recognition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Edinburgh, UK, Aug. 2003, pp. 844\u2013848.",
            "year": 2003
        },
        {
            "authors": [
                "S. Fogel",
                "H. Averbuch-Elor",
                "S. Cohen",
                "S. Mazor",
                "R. Litman"
            ],
            "title": "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2020, pp. 4324\u20134333.",
            "year": 2020
        },
        {
            "authors": [
                "R. Hussain",
                "A. Raza",
                "I. Siddiqi",
                "K. Khurshid",
                "C. Djeddi"
            ],
            "title": "A Comprehensive Survey of Handwritten Document Benchmarks: Structure, Usage and Evaluation",
            "venue": "EURASIP Journal on Image and Video Processing, vol. 46, Dec. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "R. Bertolami",
                "H. Bunke"
            ],
            "title": "Hidden Markov Model-based Ensemble Methods for Offline Handwritten Text Line Recognition",
            "venue": "Pattern Recognition, vol. 41(11), Nov. 2018, pp. 3452\u20133460.",
            "year": 2018
        },
        {
            "authors": [
                "P. Dreuw",
                "P. Doetsch",
                "C. Plahl",
                "H. Ney"
            ],
            "title": "Hierarchical Hybrid MLP/HMM or Rather MLP Features for a Discriminatively Trained Gaussian HMM: A Comparison for Offline Handwriting Recognition",
            "venue": "IEEE Intl. Conf. on Image Processing (ICIP), Brussels, Belgium, Sep. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "N. Li",
                "J. Chen",
                "H. Cao",
                "B. Zhang",
                "P. Natarajan"
            ],
            "title": "Applications of Recurrent Neural Network Language Model in Offline Handwriting Recognition and Word Spotting",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Hersonissos, Greece, Sep. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "J. Pastor-Pellicer",
                "S.E. na Boquera",
                "M.J. Castro-Bleda",
                "F. Zamora- Mart\u00ednez"
            ],
            "title": "A Combined Convolutional Neural Network and Dynamic ProgrammingApproach for Text LineNormalization",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Tunis, Tunisia, Aug. 2015, pp. 341\u2013345.",
            "year": 2015
        },
        {
            "authors": [
                "S. Espa\u00f1a-Boquera",
                "M. Castro-Bleda",
                "J. Gorbe-Moya",
                "F. Zamora- Martinez"
            ],
            "title": "Improving Offline Handwritten Text Recognition with Hybrid HMM/ANNModels",
            "venue": "Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 33(4), Apr. 2011, pp. 767\u2013779.",
            "year": 2011
        },
        {
            "authors": [
                "A. Poznanski",
                "L.Wolf"
            ],
            "title": "CNN-N-Gram for HandwritingWord Recognition",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, Jun. 2016, pp. 2306\u20132314.",
            "year": 2016
        },
        {
            "authors": [
                "T. Wang",
                "Y. Zhu",
                "L. Jin",
                "C. Luo",
                "X. Chen",
                "Y. Wu",
                "Q. Wang",
                "M. Cai"
            ],
            "title": "Decoupled Attention Network for Text Recognition",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI), vol. 34(7), Apr. 2020, pp. 12 216\u201312 224.",
            "year": 2020
        },
        {
            "authors": [
                "A. Sharma",
                "R. Ambati",
                "D.B. Jayagopi"
            ],
            "title": "Towards Faster Offline Handwriting Recognition using Temporal Convolutional Networks",
            "venue": "NCVPRIPG 2019 Communications in Computer and Information Science (CCIS), Springer, Singapore, vol. 1249, Nov. 2020, pp. 344\u2013354.",
            "year": 2019
        },
        {
            "authors": [
                "A. Sharma",
                "D.B. Jayagopi"
            ],
            "title": "Towards Efficient Unconstrained Handwriting Recognition using Dilated Temporal Convolutional Network",
            "venue": "Expert Systems with Applications, vol. 164, Feb. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Chowdhury",
                "L. Vig"
            ],
            "title": "An Efficient End-to-End Neural Model for Handwritten Text Recognition",
            "venue": "British Machine Vision Conference (BMVC), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Sueiras",
                "V. Ruiz",
                "A. Sanchez",
                "J.F. Velez"
            ],
            "title": "Offline Continuous Handwriting Recognition Using Sequence-to-Sequence Neural Networks",
            "venue": "Neurocomputing, vol. 289(C), May 2018, pp. 119\u2013128.",
            "year": 2018
        },
        {
            "authors": [
                "R.R. Ingle",
                "Y. Fujii",
                "T. Deselaers",
                "J. Baccash",
                "A.C. Popat"
            ],
            "title": "A Scalable Handwritten Text Recognition System",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Sydney, Australia, Sep. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Michael",
                "R. Labahn",
                "T. Gr\u00fcning",
                "J. Z\u00f6llner"
            ],
            "title": "Evaluating Sequenceto-Sequence Models for Handwritten Text Recognition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Sydney, NSW, Sep. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Graves",
                "J. Schmidhuber"
            ],
            "title": "Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks",
            "venue": "Advances in Neural Information Processing Systems (NIPS), Dec. 2008, pp. 545\u2013552.",
            "year": 2008
        },
        {
            "authors": [
                "T. Bluche"
            ],
            "title": "Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition",
            "venue": "Advances in Neural Information Processing Systems (NIPS), Dec. 2016, pp. 838\u2013846.",
            "year": 2016
        },
        {
            "authors": [
                "P. Voigtlaender",
                "P. Doetsch",
                "H. Ney"
            ],
            "title": "Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Shenzhen, China, Oct. 2016, pp. 228\u2013233.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Chen",
                "Y.Wu",
                "F. Yin",
                "C.-L. Liu"
            ],
            "title": "Simultaneous Script Identification and Handwriting Recognition via Multi-Task Learning of Recurrent Neural Networks",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017, pp. 525\u2013530.",
            "year": 2017
        },
        {
            "authors": [
                "T. Bluche",
                "J. Louradour",
                "andR.Messina"
            ],
            "title": "Scan, Attend andRead: End-to- End Handwritten Paragraph Recognition with MDLSTM Attention",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Castro",
                "B.L.D. Bezerra",
                "M. Valen\u00e7a"
            ],
            "title": "Boosting the Deep Multidimensional Long-Short-TermMemory Network for Handwritten Recognition Systems",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Niagara Falls, NY, Aug. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "P. Krishnan",
                "K. Dutta",
                "C.V. Jawahar"
            ],
            "title": "Word Spotting and Recognition using Deep Embedding",
            "venue": "IAPR Intl. Workshop on Document Analysis Systems (DAS), Vienna, Austria, 2018, pp. 1\u20136.",
            "year": 2018
        },
        {
            "authors": [
                "L. Kang",
                "P. Riba",
                "M. Rusinol",
                "A. Fornes",
                "M. Villegas"
            ],
            "title": "Pay Attention to What You Read: Non-recurrent Handwritten Text-Line Recognition",
            "venue": "Pattern Recognition, vol. 129, Sep. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "V. Pham",
                "T. Bluche",
                "C. Kermorvant",
                "J. Louradour"
            ],
            "title": "Dropout Improves Recurrent Neural Networks for Handwriting Recognition",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Hersonissos, Greece, Sep. 2014, pp. 285\u2013290.",
            "year": 2014
        },
        {
            "authors": [
                "A. Graves",
                "M. Liwicki",
                "S. Fern\u00e1ndez",
                "R. Bertolami",
                "H. Bunke",
                "J. Schmidhuber"
            ],
            "title": "A Novel Connectionist System for Unconstrained Handwriting Recognition",
            "venue": "Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 31(5), May 2009, pp. 855\u2013868.",
            "year": 2009
        },
        {
            "authors": [
                "J. Puigcerver"
            ],
            "title": "AreMultidimensional Recurrent Layers Really Necessary for Handwritten Text Recognition?\u2019",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan,",
            "year": 2017
        },
        {
            "authors": [
                "T. Bluche",
                "R. Messina"
            ],
            "title": "Gated Convolutional Recurrent Neural Networks for Multilingual Handwriting Recognition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017, pp. 646\u2013651.",
            "year": 2017
        },
        {
            "authors": [
                "D. Liang",
                "W. Xu",
                "Y. Zhao"
            ],
            "title": "Combining Word-Level and Character- Level Representations for Relation Classification of Informal Text (RepL4NLP)",
            "venue": "Workshop on Representation Learning for NLP (RepL4NLP), Vancouver, Canada, Aug. 2017, p. 43\u201347.",
            "year": 2017
        },
        {
            "authors": [
                "S. Sudholt",
                "G.A. Fink"
            ],
            "title": "Attribute CNNs for Word Spotting in Handwritten Documents",
            "venue": "Intl. Journal on Document Analysis and Recognition (IJDAR), vol. 21, Feb. 2018, pp. 199\u2013218.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Xiao",
                "K. Cho"
            ],
            "title": "Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers",
            "venue": "arXiv preprint arXiv:1602.00367, Feb. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "V. Carbune",
                "P. Gonnet",
                "T. Deselaers",
                "H.A. Rowley",
                "A. Daryin",
                "M. Calvo",
                "L.-L. Wang",
                "D. Keysers",
                "S. Feuz",
                "P. Gervais"
            ],
            "title": "Fast Multi-language LSTM-based Online Handwriting Recognition",
            "venue": "Intl. Journal on Document Analysis and Recognition (IJDAR), vol. 23, Feb. 2020, pp. 89\u2013102.",
            "year": 2020
        },
        {
            "authors": [
                "B. Tian",
                "Y. Zhang",
                "J. Wang",
                "C. Xing"
            ],
            "title": "Hierarchical Inter-Attention Network for Document Classification with Multi-Task Learning",
            "venue": "Intl. Joint Conf. on Artificial Intelligence (IJCAI), Aug. 2019, pp. 3569\u20133575.",
            "year": 2019
        },
        {
            "authors": [
                "M. Yousef",
                "K.F. Hussain",
                "U.S. Mohammed"
            ],
            "title": "Accurate, Data- Efficient, Unconstrained Text Recognition with Convolutional Neural Networks",
            "venue": "Pattern Recognition, vol. 108, Dec. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Graves"
            ],
            "title": "Generating Sequences with Recurrent Neural Networks",
            "venue": "arXiv preprint arXiv:1308.0850, Jun. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "B. Ji",
                "T. Chen"
            ],
            "title": "Generative Adversarial Network for Handwritten Text",
            "venue": "arXiv preprint arXiv:1907.11845, Feb. 2020.",
            "year": 1907
        },
        {
            "authors": [
                "E. Aksan",
                "F. Pece",
                "O. Hilliges"
            ],
            "title": "DeepWriting: Making Digital Ink Editable via DeepGenerativeModeling",
            "venue": "inCHIConf. onHuman Factors in Computing Systems, Apr. 2018, pp. 1\u201314.",
            "year": 2018
        },
        {
            "authors": [
                "T.S.F. Haines",
                "O.M. Aodha",
                "G.J. Brostow"
            ],
            "title": "My Text in Your Handwriting",
            "venue": "ACM Trans. on Graphics, vol. 35(3), May 2016, pp. 1\u201318.",
            "year": 2016
        },
        {
            "authors": [
                "E. Alonso",
                "B. Moysset",
                "R. Messina"
            ],
            "title": "Adversarial Generation of Handwritten Text Images Conditioned on Sequences",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Sydney, Australia, Sep. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Zhang",
                "L. Chen",
                "Y. Zhang",
                "R. Hu",
                "C. He",
                "Y. Tan",
                "J. Zhang"
            ],
            "title": "A Wearable Real-Time Character Recognition System Based on Edge Computing-Enabled Deep Learning for Air-Writing",
            "venue": "Journal of Sensors, vol. 2022, May 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Bu",
                "L. Xie",
                "Y. Yin",
                "C. Wang",
                "J. Ning",
                "J. Cao",
                "S. Lu"
            ],
            "title": "Handwriting- Assistant: Reconstructing Continuous Strokes with Millimeter-level Accuracy via Attachable Inertial Sensors",
            "venue": "Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), vol. 5(4), article 146, Dec. 2021, pp. 1\u201325.",
            "year": 2021
        },
        {
            "authors": [
                "G. He",
                "Z. Wu",
                "Y. Wu",
                "P. Lin",
                "H. Huangfu"
            ],
            "title": "Online Handwriting Recognition Based on Microphone and IMU",
            "venue": "Intl. Conf. on Electronics Technology (ICET), Chengdu, China, May 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.K. Singh",
                "A. Chaturvedi"
            ],
            "title": "Leveraging Deep Feature Learning for Wearable Sensors Based Handwritten Character Recognition",
            "venue": "Biomedical Signal Processing and Control, vol. 80(1), Feb. 2023. 22 VOLUME 11, 2023 This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3310819 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
            "year": 2023
        },
        {
            "authors": [
                "Q. He",
                "Z. Feng",
                "X. Wang",
                "Y. Wu",
                "J. Wang"
            ],
            "title": "A Smart Pen Based on Triboelectric Effects for Handwriting Pattern Tracking and Biometric Identification",
            "venue": "ACS Appl. Mater. Interfaces, vol. 14(43), Oct. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "T.T. Alemayoh",
                "M. Shintani",
                "J.H. Lee",
                "S. Okamoto"
            ],
            "title": "Deep- Learning-Based Character Recognition from Handwriting Motion Data Captured Using IMU and Force Sensors",
            "venue": "inMDPI Sensors, vol. 22(20), Oct. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Kre\u00df",
                "A. Serdyuk",
                "T. Hotfilter",
                "J. Hoefer",
                "T. Harbaum",
                "J. Becker",
                "T. Hamann"
            ],
            "title": "Hardware-aware Workload Distribution for AI-based Online Handwriting Recognition in a Sensor Pen",
            "venue": "Mediterranean Conference on Embedded Computing (MECO), Budva, Montenegro, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "F. Ott",
                "D. R\u00fcgamer",
                "L. Heublein",
                "B. Bischl",
                "C. Mutschler"
            ],
            "title": "Representation learning for tablet and paper domain adaptation in favor of online handwriting recognition",
            "venue": "IAPR Intl. Workshop on Multimodal Pattern Recognition of Social Signals in Human Computer Interaction (MPRSS), Montreal, Canada, Aug. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "L. Wegmeth",
                "A. Hoelzemann",
                "K.V. Laerhoven"
            ],
            "title": "Detecting Handwritten Mathematical Terms with Sensor Based Data",
            "venue": "arXiv preprint arXiv:2109.05594, Sep. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Bronkhorst"
            ],
            "title": "A Pen is All You Need",
            "venue": "Twente Student Conf. on IT, Enschede, The Netherlands, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S.S. Singh",
                "S. Karayev"
            ],
            "title": "Full Page Handwriting Recognition via Image to Sequence Extraction",
            "venue": "IAPR Intl. Conf. onDocument Analysis and Recognition (ICDAR), Lausanne, Switzerland, Mar. 2021, pp. 55\u201369.",
            "year": 2021
        },
        {
            "authors": [
                "H. Azimi",
                "S. Chang",
                "J. Gold",
                "K. Karabina"
            ],
            "title": "Improving Accuracy and Explainability of Online Handwriting Recognition",
            "venue": "arXiv preprint arXiv:2209.09102, Sep. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "N. Rasiwasia",
                "J.C. Pereira",
                "E. Coviello",
                "G. Doyle",
                "G.R. Lanckriet",
                "R. Levy",
                "N. Vasconcelos"
            ],
            "title": "A New Approach to Cross-Modal Multimedia Retrieval",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2010, pp. 251\u2013260.",
            "year": 2010
        },
        {
            "authors": [
                "S. Deldari",
                "H. Xue",
                "A. Saeed",
                "J. He",
                "D.V. Smith",
                "F.D. Salim"
            ],
            "title": "Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data",
            "venue": "arXiv preprint arXiv:2206.02353, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "X. Gu",
                "L. Ou",
                "D. Ong",
                "Y. Wang"
            ],
            "title": "MM-ALT: A Multimodal Automatic Lyric Transcription System",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 3328\u20133337.",
            "year": 2022
        },
        {
            "authors": [
                "X. Zeng",
                "D. Xiang",
                "L. Peng",
                "C. Liu",
                "X. Ding"
            ],
            "title": "Local Discriminant Training and Global Optimization for Convolutional Neural Network Based Handwritten Chinese Character Recognition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "W. Liu",
                "Y. Wen",
                "Z. Yu",
                "M. Li",
                "B. Raj",
                "L. Song"
            ],
            "title": "SphereFace: Deep Hypersphere Embedding for Face Recognition",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, Jul. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "S. Chopra",
                "R. Hadsell",
                "Y. LeCun"
            ],
            "title": "Learning a Similarity Metric Disriminatively, with Application to Face Verification",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), San Diego, CA, Jun. 2005.",
            "year": 2005
        },
        {
            "authors": [
                "T. Yoshida",
                "I. Takeuchi",
                "M. Karasuyama"
            ],
            "title": "Safe Triplet Screening for Distance Metric Learning",
            "venue": "Neural Computation, vol. 31(12), Oct. 2019, pp. 2432\u20132491.",
            "year": 2019
        },
        {
            "authors": [
                "B. Harwood",
                "V.K.B.G.",
                "G. Carneiro",
                "I. Reid",
                "T. Drummond"
            ],
            "title": "Smart Mining for DeepMetric Learning",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Venice, Italy, Oct. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "D. Semedo",
                "J. Magalh\u00e3es"
            ],
            "title": "Adaptive Temporal Triplet-loss for Crossmodal Embedding Learning",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2020, pp. 1152\u20131161.",
            "year": 2020
        },
        {
            "authors": [
                "D. Guo",
                "S. Tang",
                "M. Wang"
            ],
            "title": "Connectionist Temporal Modeling of Video and Language: A Joint Model for Translation and Sign Labeling",
            "venue": "Intl. Joint Conf. on Artificial Intelligence (IJCAI), 2019, pp. 751\u2013757.",
            "year": 2019
        },
        {
            "authors": [
                "D. Zeng",
                "Y. Yu",
                "K. Oyama"
            ],
            "title": "Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual Cross-Modal Retrieval",
            "venue": "ACM Trans. on Multimedia Computing, Communications, and Applications, vol. 16(3), Aug. 2020, pp. 1\u201323.",
            "year": 2020
        },
        {
            "authors": [
                "U. Chaudhuri",
                "B. Banerjee",
                "A. Bhattacharya",
                "andM. Datcu"
            ],
            "title": "CrossATNet - A Novel Cross-Attention based Framework for Sketch-based Image Retrieval",
            "venue": "Image and Vision Computing, vol. 104, Dec. 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Gordo",
                "D. Larlus"
            ],
            "title": "Beyond Instance-Level Image Retrieval: Leveraging Captions to Learn a Global Visual Representation for Semantic Retrieval",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, Jul. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Deng",
                "Z. Chen",
                "X. Liu",
                "X. Gao",
                "D. Tao"
            ],
            "title": "Triplet-based Deep Hashing Network for Cross-Modal Retrieval",
            "venue": "IEEE Trans. on Image Processing, vol. 27(8), Apr. 2018, pp. 32 893\u20133903.",
            "year": 2018
        },
        {
            "authors": [
                "J. Zhang",
                "Y. Kalantidis",
                "M. Rohrbach",
                "A. Elgammal",
                "M. Elhoseiny"
            ],
            "title": "Large-Scale Visual Relationship Understanding",
            "venue": "Association for the Advancement of Artificial Intelligence (AAAI), vol. 33(1), Jul. 2019, pp. 9185\u20139194.",
            "year": 2019
        },
        {
            "authors": [
                "H. Bredin"
            ],
            "title": "TristouNet: Triplet Loss for Speaker Turn Embedding",
            "venue": "IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2017, pp. 5430\u20135434.",
            "year": 2017
        },
        {
            "authors": [
                "J.Wang",
                "T. Gong",
                "Z. Zeng",
                "C. Sun",
                "Y. Yan"
            ],
            "title": "C3CMR: Cross-Modality Cross-Instance Contrastive Learning for Cross-Media Retrieval",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 4300\u2013 4308.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ohishi",
                "M. Delcroix",
                "T. Ochiai",
                "S. Araki",
                "D. Takeuchi",
                "D. Niizumi",
                "A. Kimura",
                "N. Harada",
                "K. Kashino"
            ],
            "title": "ConceptBeam: Concept Driven Target Speech Extraction",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 4252\u20134260.",
            "year": 2022
        },
        {
            "authors": [
                "K. Musgrave",
                "S. Belongie",
                "S.-N. Lim"
            ],
            "title": "A Metric Learning Reality Check",
            "venue": "Euorp. Conf. on Computer Vision (ECCV), 2020, pp. 681\u2013 699.",
            "year": 2020
        },
        {
            "authors": [
                "H. Rantzsch",
                "H. Yang",
                "C. Meinel"
            ],
            "title": "Signature Embedding: Writer Independent Offline Signature Verification with Deep Metric Learning",
            "venue": "Advances in Visual Computing (ISVC), Dec. 2016, pp. 616\u2013625.",
            "year": 2016
        },
        {
            "authors": [
                "A. Hermans",
                "L. Beyer",
                "B. Leibe"
            ],
            "title": "In Defence of the Triplet Loss for Person Re-Identification",
            "venue": "arXiv preprint arXiv:1703.07737, Mar. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "K.Q. Weinberger",
                "J. Blitzer",
                "L.K. Saul"
            ],
            "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification",
            "venue": "Advances in Neural Information Processing Systems (NIPS), Dec. 2005, pp. 1473\u20131480.",
            "year": 2005
        },
        {
            "authors": [
                "S. Liu",
                "E. Johns",
                "A.J. Davison"
            ],
            "title": "End-to-End Multi-Task Learning with Attention",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019, pp. 1871\u20131880.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Wang",
                "T. Oates"
            ],
            "title": "Imaging Time-Series to Improve Classification and Imputation",
            "venue": "Intl. Joint. Conf. on Artificial Intelligence (IJCAI), Buenos Aires, Argentinia, Jul. 2015, pp. 3939\u20133945.",
            "year": 2015
        },
        {
            "authors": [
                "M. Liwicki",
                "H. Bunke"
            ],
            "title": "IAM-OnDB - an On-Line English Sentence Database Acquired from Handwritten Text on a Whiteboard",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Seoul, Korea, Aug. 2005, pp. 956\u2013961.",
            "year": 2005
        },
        {
            "authors": [
                "B. Shi",
                "X. Bai",
                "C. Yao"
            ],
            "title": "An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition",
            "venue": "Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 39(11), Nov. 2017, pp. 2298\u20132304.",
            "year": 2017
        },
        {
            "authors": [
                "A. Brock",
                "J. Donahue",
                "K. Simonyan"
            ],
            "title": "Large Scale GAN Training for High Fidelity Natural Image Synthesis",
            "venue": "Intl. Conf. on Learning Representations (ICLR), 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J.H. Lim",
                "J.C. Ye"
            ],
            "title": "Geometric GAN",
            "venue": "arXiv preprint arXiv:1705.02894, May 2017.",
            "year": 2017
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going Deeper with Convolutions",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, MA, Jun. 2015, pp. 1\u20139.",
            "year": 2015
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, Jun. 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. Avidan",
                "A. Shamir"
            ],
            "title": "Seam Carving for Content-Aware Image Resizing",
            "venue": "ACM Trans. on Graphics (SIGGRAPH), vol. 26(3), Jul. 2007, p. 10.",
            "year": 2007
        },
        {
            "authors": [
                "C. Wigington",
                "S. Stewart",
                "B. Davis",
                "B. Barrett",
                "B. Price",
                "S. Cohen"
            ],
            "title": "Data Augmentation for Recognition of Handwritten Words and Lines Using a CNN-LSTM Network",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017.",
            "year": 2017
        },
        {
            "authors": [
                "L. van der Maaten",
                "G. Hinton"
            ],
            "title": "Visualizing Data using t-SNE",
            "venue": "Journal of Machine Learning Research (JMLR), vol. 9(86), Nov. 2008, pp. 2579\u20132605.",
            "year": 2008
        },
        {
            "authors": [
                "H.I. Fawaz",
                "B. Lucas",
                "G. Forestier",
                "C. Pelletier",
                "D.F. Schmidt",
                "J. Weberf",
                "G.I. Webb",
                "L. Idoumghar",
                "P.-A. Muller",
                "F. Petitjean"
            ],
            "title": "InceptionTime: Finding AlexNet for Time Series Classification",
            "venue": "inWIREs Data Mining and Knowledge Discovery, vol. 34(6), Sep. 2019, pp. 1936\u20131962.",
            "year": 2019
        },
        {
            "authors": [
                "A. Mattick",
                "M. Mayr",
                "M. Seuret",
                "A. Maier",
                "V. Christlein"
            ],
            "title": "Smart- Patch: Improving Handwritten Word Imitation with Patch Discrimina- VOLUME 11, 2023 23 This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3310819 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ tors",
            "venue": "Proc. of the IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Lausanne, Switzerland, Sep. 2021, pp. 268\u2013283.",
            "year": 2021
        },
        {
            "authors": [
                "L. Kang",
                "P. Riba",
                "M. Rusinol",
                "A. Forn\u00e9s",
                "M. Villegas"
            ],
            "title": "Content and Style Aware Generation of Text-line Images for Handwriting Recognition",
            "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 44(12), Oct. 2021, pp. 8846\u20138860.",
            "year": 2021
        },
        {
            "authors": [
                "C. Luo",
                "Y. Zhu",
                "L. Jin",
                "Z. Li",
                "D. Peng"
            ],
            "title": "SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text",
            "venue": "IEEE Trans. on Neural Networks and Learning Systems (TNNLS), Feb. 2022, pp. 1\u201313.",
            "year": 2022
        },
        {
            "authors": [
                "J. Gan",
                "W.Wang",
                "J. Leng",
                "X. Gao"
            ],
            "title": "HiGAN+: Handwriting Imitation GAN with Disentangled Representations",
            "venue": "ACM Trans. on Graphics (TOG), Feb. 2023, pp. 1\u201317.",
            "year": 2023
        },
        {
            "authors": [
                "C. Chen",
                "Z. Fu",
                "Z. Chen",
                "S. Jin",
                "Z. Cheng",
                "X. Jin",
                "X. sheng Hua"
            ],
            "title": "HoMM: Higher-Order Moment Matching for Unsupervised Domain Adaptation",
            "venue": "Proc. of the AAAI Conf. on Artificial Intelligence (AAAI), vol. 34(4), Apr. 2020, pp. 3422\u20133429.",
            "year": 2020
        },
        {
            "authors": [
                "E. Grosicki",
                "H. El-Abed"
            ],
            "title": "ICDAR 2011 - French Handwriting Recognition Competition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Beijing, China, Sep. 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M. Kozielski",
                "P. Doetsch",
                "andH. Ney"
            ],
            "title": "Improvements in RWTH\u2019s System for Off-Line Handwriting Recognition",
            "venue": "IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Washington, DC, Aug. 2013.",
            "year": 2013
        },
        {
            "authors": [
                "P. Doetsch",
                "M. Kozielski",
                "H. Ney"
            ],
            "title": "Fast and Robust Training of Recurrent Neural Networks for OfflineHandwriting Recognition",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Hersonissos, Greece, Sep. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "F. Menasri",
                "J. Louradour",
                "A.-L. Bianne-Bernard",
                "C. Kermorvant"
            ],
            "title": "The A2iA French Handwriting Recognition System at the RIMES- ICDAR2011 Competition",
            "venue": "Proc. of the Intl. Society for Optical Engineering (SPIE), vol. 8297(51), Jan. 2012.",
            "year": 2012
        },
        {
            "authors": [
                "P. Voigtlaender",
                "P. Doetsch",
                "S. Wiesler",
                "R. Schl\u00fcter",
                "H. Ney"
            ],
            "title": "Sequence-Discriminative Training of Recurrent Neural Networks",
            "venue": "Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, QLD, Apr. 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Bluche"
            ],
            "title": "Deep Neural Networks for Large Vocabulary Handwritten Text Recognition",
            "venue": "Th\u00e8se de Doctorat, Universit\u00e9 Paris-Sud, May 2015.",
            "year": 2015
        },
        {
            "authors": [
                "C.Wigington",
                "C. Tensmeyer",
                "B. Davis",
                "W. Barrett",
                "B. Price",
                "S. Cohen"
            ],
            "title": "Start, Follow, Read: End-to-End Full-Page Handwriting Recognition",
            "venue": "Europ. Conf. on Computer Vision (ECCV), vol. 11210, Oct. 2018, pp. 372\u2013388.",
            "year": 2018
        },
        {
            "authors": [
                "K. Dutta",
                "P. Krishnan",
                "M. Mathew",
                "C.V. Jawahar"
            ],
            "title": "Improving CNN- RNN Hybrid Networks for Handwriting Recognition",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Niagara Falls, NY, 2018, pp. 80\u201385.",
            "year": 2018
        },
        {
            "authors": [
                "J. Chung",
                "T. Delteil"
            ],
            "title": "A Computationally Efficient Pipeline Approach to Full PageOfflineHandwritten Text Recognition",
            "venue": "Intl. Conf. onDocument Analysis and Recognition Workshops (ICDARW), Sydney, NSW, Sep. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "M. Carbonell",
                "J. Mas",
                "M. Villegas",
                "A. Forn\u00e9s",
                "J. Llad\u00f3s"
            ],
            "title": "End-to- End Handwritten Text Detection and Transcription in Full Pages",
            "venue": "Intl. Conf. on Document Analysis and RecognitionWorkshops (ICDARW), Sydney, NSW, Sep. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A.K. Bhunia",
                "A. Das",
                "A.K. Bhunia",
                "P.S.R. Kishore",
                "P.P. Roy"
            ],
            "title": "Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R.Messina",
                "C. Kermorvant"
            ],
            "title": "Over-Generative Finite State Transducer N-Gram for Out-of-Vocabulary Word Recognition",
            "venue": "IAPR Intl. Workshop on Document Analysis Systems, Tours, France, Apr. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "G. Bastas",
                "K. Kritsis",
                "V. Katsouros"
            ],
            "title": "Air-Writing Recognition using Deep Convolutional and Recurrent Neural Network Architectures",
            "venue": "Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Dortmund, Germany, Sep. 2020, pp. 7\u201312.",
            "year": 2020
        },
        {
            "authors": [
                "A. Frome",
                "G.S. Corrado",
                "J. Shlens",
                "S. Bengio",
                "J. Dean",
                "M.A. Ranzato",
                "T. Mikolov"
            ],
            "title": "DeViSE: A Deep Visual-Semantic Embedding Model",
            "venue": "Advances in Neural Information Processing Systems (NIPS), 2013.",
            "year": 2013
        },
        {
            "authors": [
                "R. Kiros",
                "R. Salakhutdinov",
                "R.S. Zemel"
            ],
            "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
            "venue": "arXiv preprint arXiv:1411.2539, Nov. 2014.",
            "year": 2014
        },
        {
            "authors": [
                "P. Young",
                "A. Lai",
                "M. Hodosh",
                "J. Hockenmaier"
            ],
            "title": "From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Interference Over Event Descriptions",
            "venue": "Trans. of the Association for Computational Linguistics (ACL), vol. 2, Cambridge, MA, 2014, pp. 67\u2013 78.",
            "year": 2014
        },
        {
            "authors": [
                "Y. Huang",
                "Q. Wu",
                "C. Song",
                "L. Wang"
            ],
            "title": "Learning Semantic Concepts and Order for Image and Sentence Matching",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, Jun. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "J. Gu",
                "J. Cai",
                "S. Joty",
                "L. Niu",
                "G. Wang"
            ],
            "title": "Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, Jun. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K.-H. Lee",
                "X. Chen",
                "G. Hua",
                "H. Hu",
                "X. He"
            ],
            "title": "Stacked Cross Attention for Image-Text Matching",
            "venue": "Europ. Conf. on Computer Vision (ECCV), vol. 11208, Oct. 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Li",
                "Y. Zhang",
                "K. Li",
                "Y. Li",
                "Y. Fu"
            ],
            "title": "Visual Semantic Reasoning for Image-Text Matching",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Seoul, Korea, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Song",
                "M. Soleymani"
            ],
            "title": "Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. Wehrmann",
                "M.A. Lopes",
                "D. Souza",
                "R. Barros"
            ],
            "title": "Language- Agnostic Visual-Semantic Embeddings",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Seould, Korea, Oct. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "H. Wu",
                "J. Mao",
                "Y. Zhang",
                "Y. Jiang",
                "L. Li",
                "W. Sun",
                "W.-Y. Ma"
            ],
            "title": "Unified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019.",
            "year": 2019
        },
        {
            "authors": [
                "L. Zhen",
                "P. Hu",
                "X. Wang",
                "D. Peng"
            ],
            "title": "Deep Supervised Cross-Modal Retrieval",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019, pp. 10 394\u201310 403.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Peng",
                "J. Qi"
            ],
            "title": "CM-GANs: Cross-Modal Generative Adversarial Networks for Common Representation Learning",
            "venue": "ACM Trans. on Multimedia Computing, Communications, and Applications, vol. 15(1), Feb. 2019, pp. 1\u201324.",
            "year": 2019
        },
        {
            "authors": [
                "A. van den Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation Learning with Contrastive Predictive Coding",
            "venue": "arXiv preprint arXiv:1807.03748, Jan. 2019.",
            "year": 1807
        },
        {
            "authors": [
                "H. Wang",
                "Y. Zhang",
                "Z. Ji",
                "Y. Pang",
                "L. Ma"
            ],
            "title": "Consensus-Aware Visual- Semantic Embedding for Image-Text Matching",
            "venue": "Euorp. Conf. on Computer Vision (ECCV), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "T. Chen",
                "J. Deng",
                "J. Luo"
            ],
            "title": "Adaptive Offline Quintuplet Loss for Image-Text Matching",
            "venue": "Euorp. Conf. on Computer Vision (ECCV), 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Chun",
                "S.J. Oh",
                "R.S. de Rezende",
                "Y. Kalantidis",
                "D. Larlus"
            ],
            "title": "Probabilistic Embeddings for Cross-Modal Retrieval",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, Jun. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "B. Chen",
                "A. Rouditchenko",
                "K. Duarte",
                "H. Kuehne",
                "S. Thomas",
                "A. Boggust",
                "R. Panda",
                "B. Kingsbury",
                "R. Feris",
                "D. Harwath",
                "J. Glass",
                "M. Picheny",
                "S.-F. Chang"
            ],
            "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Montreal, QC, Oct. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "H. Akbari",
                "L. Yuan",
                "R. Qian",
                "W.-H. Chuang",
                "S.-F. Chang",
                "Y. Cui",
                "B. Gong"
            ],
            "title": "VATT: Transformers forMultimodal Self-Supervised Learning from Raw Video, Audio and Text",
            "venue": "Advances in Neural Information Processing Systems (NIPS), May 2021.",
            "year": 2021
        },
        {
            "authors": [
                "L. Wang",
                "P. Luc",
                "A. Recasens",
                "J.-B. Alayrac",
                "A. van den Oord"
            ],
            "title": "Multimodal Self-Supervised Learning of General Audio Representations",
            "venue": "arXiv preprint arXiv:2104.12807, Apr. 2021.",
            "year": 2021
        },
        {
            "authors": [
                "A. Guzhov",
                "F. Raue",
                "J. Hees",
                "andA. Dengel"
            ],
            "title": "AudioCLIP: Extending Clip to Image, Text and Audio",
            "venue": "IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, May 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A. Baevski",
                "W.-N. Hsu",
                "Q. Xu",
                "A. Babu",
                "J. Gu",
                "andM. Auli"
            ],
            "title": "data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language",
            "venue": "Intl. Conf. on Machine Learning (ICML), vol. 162, 2022, pp. 1298\u20131312.",
            "year": 2022
        },
        {
            "authors": [
                "A. Piergiovanni",
                "A. Angelova",
                "M.S. Ryoo"
            ],
            "title": "Evolving Losses for Unsupervised Video Representation Learning",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2022.",
            "year": 2022
        },
        {
            "authors": [
                "H. Lin",
                "Z. Ma",
                "X. Hong",
                "Y. Wang",
                "Z. Su"
            ],
            "title": "Semi-supervised Crowd Counting via Density Agency",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 1416\u20131426. 24 VOLUME 11, 2023 This article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3310819 This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
            "year": 2022
        },
        {
            "authors": [
                "A. Singh",
                "R. Hu",
                "V. Goswami",
                "G. Couairon",
                "W. Galuba",
                "M. Rohrbach",
                "D. Kiela"
            ],
            "title": "FLAVA: A Foundational Language and Vision Alignment Model",
            "venue": "IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, Jun. 2022, pp. 15 638\u201315 650.",
            "year": 2022
        },
        {
            "authors": [
                "A. Falcon",
                "G. Serra",
                "O. Lanz"
            ],
            "title": "A Feature-space Multimodal Data Augmentation Technique for Text-Video Retrieval",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 4385\u20134394.",
            "year": 2022
        },
        {
            "authors": [
                "D. Chen",
                "M. Wang",
                "H. Chen",
                "L. Wu",
                "J. Qin",
                "W. Peng"
            ],
            "title": "Cross-Modal Retrieval with Heterogeneous Graph Embedding",
            "venue": "Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 3291\u20133300.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "INDEX TERMS contrastive learning, cross-modal retrieval, online handwriting recognition, optical character recognition, representation learning, sensor-enhanced pen, sequence-based learning, triplet learning\nI. INTRODUCTION\nCross-modal retrieval (CMR) such as cross-modal representation learning [1] for learning across two or more modalities (i.e., image, audio, text and 3D data) has recently garnered substantial interest from the machine learning community. CMR can be applied in a wide range of applications, such as multimediamanagement [2] and identification [3]. Extracting information from several modalities and adapting the domain with cross-modal learning allows using the information in all domains [4]. Cross-modal representation learning, however, remains challenging due to the heterogeneity gap (i.e., inconsistent representation forms of different modalities) [5].\nA limitation of cross-modal representation learning is that\nmany approaches require the availability of all modalities at inference time. Image-to-caption CMRmethods solve this via a separate encoder [6], [7]. However, in many applications, certain data sources are only available during training by means of elaborate laboratory setups [8]. For instance, consider a human pose estimation task that uses inertial sensors together with color videos during training, where a camera setup might not be available at inference time due to bad lighting conditions or other application-specific restrictions. Here, a model that allows inference on only themainmodality is required, while auxiliary modalities may only be used to improve the training process (as they are not available at inference time) [9]. Learning using privileged information [10]\nVOLUME 11, 2023 1\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nis one approach in the literature that describes and tackles this problem. During training, in addition to X , it is assumed that additional privileged information X\u2217 is available. However, this privileged information is not present in the inference stage [11].\nFor cross-modal representation learning, we need a deep metric learning technique that aims to transform training samples into feature embeddings that are close for samples that belong to the same class and far apart for samples from different classes [12]. As deep metric learning requires no model update (simply fine-tuning for training samples of new classes), deep metric learning is an often applied approach for continual learning [13]. Typical deepmetric learningmethods use not only simple distances (e.g., Euclidean distance), but also highly complex distances (e.g., canonical correlation analysis [4] and maximum mean discrepancy [14]). While cross-modal representation learning learns representations from all modalities, single-modal learning commonly uses pair-wise learning. The triplet loss [15] selects a positive and negative triplet pair for a corresponding anchor and forces the positive pair distance to be smaller than the negative pair distance. While research of triplet selection for single-modal classification is very advanced [9], [13], [16]\u2013[22], pair-wise selection for cross-modal representation learning has mainly been investigated for specific applications [2], [23], [24], i.e., visual semantic embeddings [7], [25]\u2013[27].\nOne exemplary application for cross-modal learning is handwriting recognition (HWR), which can be categorized into offline and online HWR. Offline HWR \u2013 such as optical character recognition (OCR) \u2013 concerns only analysis of the visual representation of handwriting and cannot be applied for real-time recognition applications [28]. In contrast, online HWRworks on different types of spatio-temporal signals and can make use of temporal information, such as writing speed and direction [29]. As an established real-world application of online HWR, many recording systems make use of a stylus pen together with a touch screen surface [30]. There also exist prototypical systems for online HWR when writing on paper [31]\u2013[34], but these are not yet suitable for real-world applications. However, a novel sensor-enhanced pen based on inertial measurement units (IMUs) may enable new online\nHWR applications for writing on normal paper. This pen has previously been used for single character [16], [35]\u2013[37] and sequence [38] classification. However, the accuracy of previous online HWRmethods is limited, due to the following reasons: (1) The size of datasets is limited, as recording larger amounts of data is time-consuming. (2) Extracting important spatio-temporal features is important. (3) Training a writer-independent classifier is challenging, as different writers can have notably different writing styles. (4) Evaluation performance drops for under-represented groups, i.e., left-handed writers. (5) The model overfits to seen words that can be addressed with generated models. A possible solution is to combine datasets of different modalities using crossmodal representation learning to increase generalizability. In this work, we combine offline HWR from generated images (i.e., OCR) and online HWR from sensor-enhanced pens by learning a common representation between both modalities. The aim is to integrate information on OCR \u2013 i.e., typeface, cursive or printed writing, and font thickness \u2013 into the online HWR task \u2013 i.e., writing speed and direction [39].\nOurContribution.Models that use rich data (e.g., images) usually outperform those that use a less rich modality (e.g., time-series). We therefore propose to train a shared representation using the triplet loss between pairs of image and timeseries data to learn a cross-modal representation between both modality embeddings (cf. Figure 1). This allows for improving the accuracy of single-modal inference in the main task. Cross-modal learning between images and time-series data is rare. Furthermore, we propose a novel dynamic margin for the triplet loss based on the Edit distance. We prove the efficacy of our metric learning-based triplet loss for cross-modal representation learning both with simulated data and in a real-world application.More specifically, our proposed crossmodal representation learning technique 1) improves the multivariate time-series classification accuracy and convergence, 2) results in a small time-series-only network independent from the image modality while allowing for fast inference, and 3) has better generalizability and adaptability [5]. Our approach shows that the recent methods ScrabbleGAN [40] and OrigamiNet [41] are applicable in the real-world setup of offline HWR to enhance the online HWR task. We provide"
        },
        {
            "heading": "2 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nan extensive overview and technical comparison of related methods. Code and datasets are available upon publication.1\nThe paper is organized as follows. Section II discusses related work followed by the mathematical foundation of our method in Section III. The methodology is described in Section IV and the results are discussed in Section V.\nII. RELATED WORK In this section, we discuss related work \u2013 particularly, methods of offline HWR (in Section II-A) and online HWR (in Section II-B). We summarize approaches for learning a cross-modal representation from different modalities (in Section II-C), pairwise and triplet learning (in Section II-D), and deep metric learning (in Section II-E) to minimize the distance between feature embeddings.\nA. OFFLINE HANDWRITING RECOGNITION In the following, we give a brief overview of offline HWR methods to select a suitable lexicon and language modelfree method. For an overview of offline and online HWR datasets, see [29], [42]. For a more detailed overview, see Table 7 in the Appendix B. Methods for offline HWR range from hidden Markov models (HMMs) \u2013 such as [43]\u2013[47] \u2013 to deep learning techniques that became predominant in 2014, such as convolutional neural networks (CNNs) as the methods by [48], [49]. Furthermore, temporal convolutional networks (TCNs) employ the temporal context of the handwriting \u2013 such as the methods [50], [51]. More prominent became recurrent neural networks (RNNs) including long short-term memories (LSTMs), bidirectional LSTMs (BiLSTMs) [52]\u2013[55], and multidimensional RNNs [56]\u2013[62]. These sequential architectures are perfect to fit text lines, due to the probability distributions over sequences of characters, and due to the inherent temporal aspect of text [63]. Pham et al. [64] showed that the performance of LSTMs can be greatly improved using dropout. The authors in [65] introduced the BiLSTM layer in combination with the connectionist temporal classification (CTC) loss. CTC adds up over the probability of possible alignments of the input to the target sequences, producing a loss value which is differentiable with respect to each input node. Additionally, th work by [66] proposed a CNN+BiLSTM architecture that uses the CTC loss. GCRNN [67] combines a convolutional encoder (aiming for generic and multilingual features) and a BiLSTM decoder predicting character sequences. Further methods that combine CNNs with RNNs are [68]\u2013[70], while BiLSTMs are utilized in [71], [72].\nThe most recent method based on CNNs is the gated text recognizer [73] that aims to automate the feature extraction from raw input signals with a minimum required domain knowledge. The fully convolutional network without recurrent connections is trained with the CTC loss. Thus, the gated text recognizer module can handle arbitrary input sizes and\n1Code and datasets: https://www.iis.fraunhofer.de/de/ff/lv/dataanalytics/ anwproj/schreibtrainer/onhw-dataset.html\ncan recognize strings with arbitrary lengths. This module has been used for OrigamiNet [41] which is a segmentation-free multi-line or full-page recognition system. OrigamiNet yields state-of-the-art results on the IAM-OffDB dataset, and shows improved performance of gated text recognizer over VGG and ResNet26. Hence, we use the gated text recognizer module as our visual feature encoder for offline HWR. Recent methods are generative adversarial networks (GANs) and Transformers. The first approach by [74] was a method to synthesize online data based on RNNs. The technique HWGAN by [75] extends this method by adding a discriminator D. DeepWriting [76] is a GAN that is capable of disentangling style from content and thus making digital ink editable. The authors in [77] proposed a method to generate handwriting based on a specific author with learned parameters for spacing, pressure, and line thickness. Alonso et al. [78] used a BiLSTM to obtain an embedding of the word to be rendered and added an auxiliary network as a recognizerR. The model is trained with a combination of an adversarial loss and the CTC loss. ScrabbleGAN by [40] is a semi-supervised approach that can arbitrarily generate many images of words with arbitrary length from a generator G to augment handwriting data and uses a discriminator D and recognizer R. The paper proposes results for original data with random affine augmentation using synthetic images and refinement."
        },
        {
            "heading": "B. ONLINE HANDWRITING RECOGNITION",
            "text": "Motion-based handwriting [31] and air-writing [79] from sensor-enhanced devices have been extensively investigated. While such motions are spacious, the hand and pen motions for writing on paper are comparatively small-scale [80]. Research for classifying text from sensor-enhanced pens has recently attracted substantial interest. He et al. [81] use acceleration and audio data of handwritten actions for character recognition. Furthermore, recent publications came up with similar developments that are only prototypical, for example, the works proposed by [82]\u2013[84]. Hence, there is already a lot of interest and future technical advancements will further boost the classification performance of online HWRmethods. The novel sensor-enhanced pen based on IMUs [35] enables new applications for writing on paper. Note that this pen is a finished product and is commercially available. Data collection and processing is straightforward and allows applications to be easy to implement in real-world. Ott et al. [35] published the OnHW-chars dataset containing single characters. Kla\u00df et al. [37] evaluated the aleatoric and epistemic uncertainty to show the domain shift between right- and left-handed writers. [16] reduced this domain shift by adapting feature embeddings based on transformations from optimal transport techniques. In [85], the authors presented an approach for distributing the computational workload between a sensor pen and a mobile device (i.e., smartphone or tablet) for handwriting recognition, as interference on mobile devices leads to high system requirements. Ott et al. [36] reconstructed the trajectory of the pen tip for single characters written on\nVOLUME 11, 2023 3\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ntablets from IMU data and cameras pointing at the pen tip [86]. A more challenging task than single-character classification is the classification of sequences (i.e., words or equations). The authors in [38] proposed several sequence-based datasets and a large benchmark of convolutional, recurrent, and Transformer-based architectures, loss functions, and augmentation techniques. While [87] combined a binary random forest to classify the writing activity and a CNN for windows of single-label predictions, [88] highlighted the effectiveness of Transformers for classifying equations. Methods such as the one proposed by [89] cannot be applied to this online task, as these methods are designed for image-based (offline) HWR, and traditional methods such as [71] for online HWR are based on online trajectories written on tablets. Recently, Azimi et al. [90] evaluated further machine and deep learning models as well as deep ensembles on the single OnHW-chars dataset.\nC. CROSS-MODAL REPRESENTATION LEARNING For traditional methods that learn a cross-modal representation, a cross-modal similarity for the retrieval can be calculated with linear projections [91]. However, cross-modal correlation is highly complex, and hence, recent methods are based on a modal-sharing network to jointly transfer nonlinear knowledge from a single modality to all modalities [12]. Huang et al. [5] use a cross-modal network between different modalities (image to video, text, audio and 3D models) and a single-modal network (shared features between images of source and target domains). They use two convolutional layers (similar to our proposed architecture) that allow the model to adapt by using more trainable parameters. However, while their auxiliary network uses the samemodality, the auxiliary network of the proposed method in this paper is based on another modality. The work by [2] learns a cross-modal embedding between video frames and audio signals with graph clusters, but both modalities must be available at inference. Sarafianos et al. [3] proposed an image-text modality adversarial matching approach that learns modality-invariant feature representations, but their projection loss is only used for learning discriminative image-text embeddings. The authors in [9] propose a model for single-modal inference. However, they use image and depth modalities for person reidentification without a time-series component, which makes the problem considerably different. Lim et al. [8] handled multi-sensorymodalities for 3Dmodels only. For an overview of CMR, see [92]. An overview of relevant CMR methods is given in Table 8 in the Appendix C. With respect to the kind of the modality, the work by [16], [93] is closest, while the applications in [16], [20], [94] of handwriting recognition are relevant.\nD. PAIRWISE AND TRIPLET LEARNING Networks trained for a classification task can produce useful feature embeddings with efficient runtime complexity O(NC) per epoch, whereN is the number of training samples and C is the number of classes. However, the classical cross-\nentropy (CE) loss is only partly useful for deep metric learning, as it ignores how close each point is to its class centroid (or how far apart each point is from other class centroids). CE variations (e.g., for face recognition) that learn angularly discriminative features have also been developed [95]. The pairwise contrastive loss [96]minimizes the distance between feature embedding pairs of the same class and maximizes the distance between feature embedding pairs of different classes depending on a margin parameter. The drawback is that the optimization of positive pairs is independent of negative pairs, but the optimization should force the distance between positive pairs to be smaller than negative pairs [13]. The triplet loss [97] addresses this by defining an anchor and a positive point as well as a negative point and forces the positive pair distance to be smaller than the negative pair distance by a certain margin. The runtime complexity of the triplet loss is O(N 3/C) and can be computationally challenging for large training sets. Hence, several approaches exist to reduce this complexity, such as hard or semi-hard triplet mining [15] and smart triplet mining [98]. Often, data evolve over time, and hence, [99] proposed a formulation of the triplet loss where the traditional static margin is superseded by a temporally adaptive maximum margin function. While the research by [21], [94] combines the triplet loss with the CE loss, Guo et al. [100] use a triplet selection with L2-normalization for language modeling, but considered all negative pairs for triplet selection with fixed similarity intensity parameter. The proposed method uses a triplet loss with a dynamic margin together with a novel word-level triplet selection. The TNN-C-CCA [101] also uses the triplet loss on embeddings between an anchor from audio data and positive and negative samples from visual data and the cosine similarity for the final representation comparison. In imageto-caption CMR tasks, the most common design is separated encoders that allow the separated inference without the other modality [6], [7]. We choose a similar separate cross-modal encoder for single-modal inference. CrossATNet [102], another triplet loss-based method that uses single class labels, defines class sketch instances as the anchor, the same class image instance as the positive sample, and a different class image instance as the negative sample. While the previous methods are based on a triplet selection method using singlelabel classification, related work exists for using the triplet loss for sequence-based classification (i.e., from texts) [103]\u2013 [106]. To the best of our knowledge, no approach so far has used triplet-based cross-modal learning based on the Edit distance between words. Most relevant are the works by [7], [9], [102], [107], [108] that use the triplet loss, but without a dynamic margin."
        },
        {
            "heading": "E. DEEP METRIC LEARNING",
            "text": "As deep metric learning is a very broad and advanced field, only the most related work is described here. For an overview of deep metric learning, see Musgrave et al. [109]. Most of the related work uses the Euclidean metric as distance loss, although the triplet loss can be defined based on any other"
        },
        {
            "heading": "4 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n(sub-)differentiable distance metric. Wan et al. [20] proposed a method for offline signature verification based on a dual triplet loss that uses the Euclidean space to project an input image to an embedding function. While Rantzsch et al. [110] use the Euclidean metric to learn the distance between feature embeddings, the authors in [94] use the Cosine similarity. Hermans et al. [111] state that using the non-squared Euclidean distance is more stable, while the squared distance made the optimization more prone to collapsing. Recent methods extend the canonical correlation analysis (CCA) [4] that learns linear projection matrices by maximizing pairwise correlation of cross-modal data. To share information between the samemodality (i.e., images), the maximummean discrepancy (MMD) [14] is typically minimized.\nIII. METHODOLOGICAL BACKGROUND We define the problem of cross-mdoal representation learning and present deep metric learning loss functions in Section III-A. In Section III-B, we propose the triplet loss for cross-modal learning.\nA. CROSS-MODAL RETRIEVAL FOR TIME-SERIES AND IMAGE CLASSIFICATION A multivariate time-series U = {u1, . . . ,um} \u2208 Rm\u00d7l is an ordered sequence of l \u2208 N streams with ui = (ui,1, . . . , ui,l), i \u2208 {1, . . . ,m}, where m \u2208 N is the length of the time-series. The multivariate time-series training set is a subset of the array U = {U1, . . . ,UnU } \u2208 RnU\u00d7m\u00d7l , where nU is the number of time-series. Let X \u2208 Rh\u00d7w with entries xi,j \u2208 [0, 255] represent an image from the image training set. The image training set is a subset of the array X = {X1, . . . ,XnX } \u2208 RnX\u00d7h\u00d7w, where nX is the number of time-series. The aim of joint multivariate time-series and image classification tasks is to predict an unknown class label y \u2208 \u2126 for single class prediction or y \u2208 \u2126 for sequence prediction for a given multivariate time-series or image (see also Section IV-B). The time-series samples denote the main training data, while the image samples represent the privileged information that is not used for inference. In addition to good prediction performance, the goal is to learn representative embeddings fc(U) and fc(X) \u2208 Rq\u00d7t to map multivariate time-series and image data into a feature spaceRq\u00d7t , where fc is the output of the convolutional layer(s) c \u2208 N of the latent representation and q\u00d7 t is the dimension of the layer output.\nWe force the embedding to live on the q \u00d7 t-dimensional hypersphere by using softmax \u2013 i.e., ||fc(U)||2 = 1 and ||fc(X)||2 = 1 \u2200c (see [112]). In order to obtain a small distance between the embeddings fc(U) and fc(X), we minimize deep metric learning functions LDML(fc(X), fc(U)). Wellknown deep learning metric are the distance-based mean squared error (MSE) LMSE, the spatio-temporal cosine similarity (CS) LCS, the Pearson correlation (PC) LPC, and the distribution-based Kullback-Leibler (KL) divergence LKL. In our experiments, we additionally evaluate the kernalized maximum mean discrepancy (kMMD) LkMMD, Bray Curtis (BC) LBC, and Poisson LPO losses. We study their perfor-\nmance in Section V. A combination of classification and cross-modal representation learning losses can be realized by dynamic weight averaging [113] as a multi-task learning approach that performs dynamic task weighting over time (see Appendix D)."
        },
        {
            "heading": "B. CONTRASTIVE LEARNING AND TRIPLET LOSS",
            "text": "While the training with the previous loss functions uses inputs where the image and multivariate time-series have the same label, pairs with similar but different labels can improve the training process. This can be achieved using the triplet loss [15], which enforces a margin between pairs of image and multivariate time-series data with the same identity to all other different identities. As a consequence, the convolutional output for one and the same label lives on a manifold, while still enforcing the distance \u2013 and thus, discriminability \u2013 to other identities. Therefore, we seek to ensure that the embedding of the multivariate time-series Uai (anchor) of a specific label is closer to the embedding of the image Xpi (positive) of the same label than it is to the embedding of any image Xni (negative) of another label (see Figure 2). Thus, we want the following inequality to hold for all training samples( fc(Uai ), fc(X p i ), fc(X n i ) ) \u2208 \u03a6:\nLDML ( fc(Uai ), fc(X p i ) ) + \u03b1 < LDML ( fc(Uai ), fc(X n i ) ) , (1)\nwhereLDML ( fc(X), fc(U) ) is a deepmetric learning loss,\u03b1 is a margin between positive and negative pairs, and \u03a6 is the set of all possible triplets in the training set. The contrastive loss minimizes the distance of the anchor to the positive sample and separately maximizes the distance to the negative sample. Instead, based on (1), we can formulate a differentiable loss function - the triplet loss - that we can use for optimization:\nLtrpl,c(Ua,Xp,Xn) = N\u2211 i=1 max [ LDML ( fc(Uai ), fc(X p i ) ) \u2212\nLDML ( fc(Uai ), fc(X n i ) ) + \u03b1, 0 ] ,\n(2) where c \u2208 N.2 Selecting negative samples that are too close\n2To have a larger number of trainable parameters in the latent representation with a greater depth, we evaluate one and two stacked convolutional layers, each trained with a shared loss Ltrpl,c.\nVOLUME 11, 2023 5\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n0 10 20 30 40 50\n1.0\n0.5\n0.0\n0.5\n1.0\n(a) Signal data.\n0 50 100 150 200\n0 50 100 150 200\n0 50 100 150 200\n0 50 100 150 200\n(b) Without noise.\n0 50 100 150 200\n0 50 100 150 200\n0 50 100 150 200\n0 50 100 150 200\n(c) With high noise.\nFIGURE 3: Synthetic signal data (a) for 10 classes, and image data (b-c) for classes 0 (left) and 6 (right).\nto the anchor (in relation to the positive sample) can cause slow training convergence. Hence, triplet selection must be handled carefully and with consideration for each specific application [13]. We choose negative samples based on the class distance (single labels) and on the Edit distance (sequence labels) (see Section IV-B).\nIV. METHOD We now demonstrate the efficacy of our proposal. In Section IV-A, we generate sinusoidal time-series with introduced noise (main task) and compute the corresponding Gramian angular summation fieldwith different noise parameters (auxiliary task) (see Figure 1). In Section IV-B, we combine online (inertial sensor signals, main task) and offline data (visual representations, auxiliary task) for HWR with sensorenhanced pens. This task is particularly challenging, due to different data representations based on images and multivariate time-series data. For both applications, our approach allows to only use the main modality (i.e., multivariate timeseries) for inference. We further analyze and evaluate different deep metric learning functions to minimize the distance between the learned embeddings.\nA. CROSS-MODAL LEARNING ON SYNTHETIC DATA We first investigate the influence of the triplet loss for crossmodal learning between synthetic time-series and imagebased data as a sanity check. For this, we generate signal data of 1,000 timesteps with different frequencies for 10 classes (see Figure 3a) and add noise from a continuous uniform distribution U(a, b) for a = 0 and b = 0.3. We use a recurrent CNN with the CE loss to classify these signals. From each signal without noise, we generate a Gramian angular summation field [114]. For classes with high frequencies, this results in a fine-grained pattern, and for low frequencies in a coarse-grained pattern. We generate Gramian angular summation fields with different added noise between b = 0 (Figure 3b) and b = 1.95 (Figure 3c). A small CNN classifies these images with the CE loss. To combine both networks, we train each signal-image pair with the triplet loss. As the frequency of the sinusoidal signal is closer for more similar class labels, the distance in the manifold embedding should also be closer. For each batch, we select negative sample pairs for samples with the class label CL = 1+ \u230amaxe \u2212e\u2212125 \u230b as the lower bound for the current epoch e and the maximum epoch maxe. We set the margin \u03b1 in the triplet loss separately for each batch such that \u03b1 = \u03b2 \u00b7 (CLp \u2212 CLn) depends on the positive CLp and negative CLn class labels of the batch and\nis in the range [1, 5] with \u03b2 = 0.1. The batch size is 100 and maxe = 100. Appendix E provides further details. This combination of the CE loss with the triplet loss can lead to a mutual improvement of the utilization of the classification task and embedding learning."
        },
        {
            "heading": "B. CROSS-MODAL LEARNING FOR HWR",
            "text": "a: Method Overview Figure 4 gives a method overview. The main task is online HWR to classify words written with a sensor-enhanced pen and represented by multivariate time-series of the different pen sensors. To improve the classification task with a better generalizability, the auxiliary network performs offline HWR based on an image input. We pre-train ScrabbleGAN [40] on the IAM-OffDB [115] dataset. For all time-series word labels, we then generate the corresponding image as the positive time-series-image pair. Each multivariate time-series and each image is associated with y \u2013 a sequence of L class labels from a pre-defined label set \u2126 with K classes. For our classification task, y \u2208 \u2126L describes words. The multivariate time-series training set is a subset of the array U with labels YU = {y1, . . . ,ynU } \u2208 \u2126nU\u00d7L . The image training set is a subset of the array X , and the corresponding labels are YX = {y1, . . . ,ynX } \u2208 \u2126nX\u00d7L . Offline HWR techniques are based on Inception, ResNet34, or gated text recognizer [73] modules. The architecture of the online HWR method consists of an IMU encoder with three 1D convolutional layers of size 400, a convolutional layer of size 200, a max pooling and batch normalization, and a dropout of 20%. The online method is improved by sharing layers with a common representation by minimizing the distance of the feature embedding of the convolutional layers c \u2208 {1, 2} (integrated in both networks) with a shared loss Lshared,c. We set the embedding sizeRq\u00d7t to 400\u00d7200. Both networks are trained with the connectionist temporal classification (CTC) [65] loss LCTC to avoid pre-segmentation of the training samples by transforming the network outputs into a conditional probability distribution over label sequences.\nb: Datasets for Online HWR We make use of two word datasets proposed in [38]. These datasets are recorded with a sensor-enhanced pen that uses two accelerometers (3 axes each), one gyroscope (3 axes), one magnetometer (3 axes), and one force sensor at 100Hz [35], [36]. One sample of sizem\u00d7l represents an multivariate time-series of a written word of m timesteps from l = 13 sensor channels. One word is a sequence of small or capital characters (52 classes) or with mutated vowels (59 classes). TheOnHW-words500 dataset contains 25,218 samples where each of the 53 writers contributed the same 500 words. The OnHW-wordsRandom dataset contains 14,641 randomly selected words from 54 writers. For both datasets, 80/20 train/validation splits are available for writer-(in)dependent (WD/WI) tasks. We transform (zero padding, interpolation) all samples to 800 timesteps. For more information on the datasets, see [38]."
        },
        {
            "heading": "6 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nre a l/ fa k e\nIMU Train\nData \ud835\udc14 \u03f5 \u211d\ud835\udc5a\u00d713\nIMU Encoder\n\ud835\udcdbCTC\nImage Data \ud835\udc17 \u03f5 \u211d\ud835\udc5c\u00d7\ud835\udc5d\nImage Encoder\nWord Labels\nRepresentation:\nOutput of\nConv. Layer\nWord\nPrediction\n\ud835\udc63 \u03f5 \ud835\udefa\ud835\udc3f\n\ud835\udcdbCTC\nWord\nPrediction\n\ud835\udc63 \u03f5 \ud835\udefa\ud835\udc3f\nScrabbleGAN\nIAM-OffDB\n\ud835\udca2 \u201cC o n c e rt \u201c? C o n\nc e rt\nz \u00d7 [\ud835\udc58\n\ud835\udc5a ,\ud835\udc58\n\ud835\udc52 ,\ud835\udc58\n\ud835\udc52 ,\ud835\udc58\n\ud835\udc61 ]\n\u2026\nCross-modal/Shared\nEncoder\n\ud835\udcdbshared,1\nFour GTR Blocks\n\ud835\udc9f\n\u211b\nSensor-enhanced Pen\nfor Data Collection\n\ud835\udcdbCTC\nWord\nPrediction\n\ud835\udc63 \u03f5 \ud835\udefa\ud835\udc3f\nIMU Test\nData\nAuxiliary Network\nMain Network\n(training & inference)\nF I N E \u2013 T U N I N G\ud835\udcdbshared,2\nLoading weights Fine-tuning weights\nForce sensor\nMagnetometer\nFront accel., gyr.\nPre-training\nLegend: Conv1D (400) Conv1D (200) MaxPool BNorm Dropout (0.2) BiLSTM (60) Dense (100)\nPre-Training\nPost-Training\nRear\naccelerometer\nInference\nFIGURE 4: Detailed method overview: The middle pipeline consists of data recording with a sensor-enhanced pen, feature extraction of inertial multivariate time-series data, and word classification with CTC. We generate image data with the pretrained ScrabbleGAN for corresponding word labels. The top pipeline (four gated text recognizer blocks) extracts features from images. The distances of the embeddings are minimized with the triplet loss and deep metric learning functions. The classification network with two BiLSTM layers are fine-tuned for the OnHW task for a cross-modal representation.\nc: Image Generation for Offline HWR\nIn order to couple the online time-series data with offline image data, we use a generative adversarial network (GAN) to arbitrarily generate many images. ScrabbleGAN [40] is a state-of-the-art semi-supervised approach that consists of a generator G that generates images of words with arbitrary length from an input word label, a discriminator D, and a recognizer R that promotes style and data fidelity. While D promotes realistic-looking handwriting styles, R encourages the result to be readable. ScrabbleGAN minimizes a joint\nloss term L = LD + \u03bbLR where LD and LR are the loss terms of D and R, respectively, and the balance factor is \u03bb. The generator G is designed such that each character is generated individually, using the property of the convolutions of overlapping receptive fields to account for the influence of nearby letters. Four character filters (km, ke, ke and kt ) are concatenated, multiplied by a noise vector z, and fed into a class-conditioned generator (see Figure 5). This allows for adjacent characters to interact and creates a smooth transition, e.g., enabling cursive text. The style of the image is controlled by a noise vector z given as the input to the network (being consistent for all characters of a word). The recognizer R discriminates between real and gibberish text by comparing the output of R to the one that was given as input to G. R is trained only on real and labeled samples. R is inspired by CRNN [116] and uses the CTC [65] loss. The architecture of the discriminator D is inspired by BigGAN [117] consisting of four residual blocks and a linear layer with one output.D is fully convolutional, predicts the average of the patches, and is trained with a hinge loss [118]. We train ScrabbleGAN with the IAM-OffDB [115] dataset and generate three different datasets. Exemplary images are shown in Figure 6. First, we generate 2 million images randomly selected from a large lexicon (OffHW-German), and pre-train the offline HWR architectures. Second, we generate 100,000 images based on the same word labels for each of the OnHW-words500 and OnHW-wordsRandom datasets (OffHW-words500, OffHWwordsRandom]) and fine-tune the offline HWR architectures.\nVOLUME 11, 2023 7\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n(a) Metropolis. (b) Citizen. (c) Concerts. (d) Starnberg.\nFIGURE 6: Overview of four generated words with ScrabbleGAN [40] with various text styles.\nd: Methods for Offline HWR OrigamiNet [41] is a state-of-the-art multi-line recognition method using only unsegmented image and text pairs. Similar to OrigamiNet, our offline method is based on different encoder architectures with one or two additional 1D convolutional layers (each with filter size 200, softmax activation [94]) with 20% dropout for the latent representation, and a cross-modal representation decoder with BiLSTMs. For the encoder, we make use of Inception modules from GoogLeNet [119] and the ResNet34 [120] architectures, and we re-implement the newly proposed gated, fully-convolutional method termed the gated text recognizer [73]. See Appendix F for detailed information on the architectures. We train the networks on the generated OffHWGerman dataset for 10 epochs and fine-tune on the OffHW[500, wordsRandom] datasets for 15 epochs. For comparison with state-of-the-art techniques, we train OrigamiNet and compare with IAM-OffDB. For OrigamiNet, we apply interline spacing reduction via seam carving [121], resizing the images to 50% height, and random projective (rotating and resizing lines) and random elastic transform [122]. We augment the OffHW-German dataset with random width resizing and apply no augmentation for the OffHW-[words500, wordsRandom] datasets for fine-tuning.\ne: Offline/Online Cross-Modal Representation Learning Our architecture for online HWR is based on [38]. The encoder extracts features of the inertial data and consists of three convolutional layers (each with filter size 400, ReLU activation) and one convolutional layer (filter size 200, ReLU activation), a max pooling, batch normalization and a 20% dropout layer. As for the offline architecture, the network then learns a latent representation with one or two convolutional layers (each with filter size 200, softmax activation) with 20% dropout and the same cross-modal representation decoder. The output of the convolutional layers of the latent representation are minimized with the Lshared,c loss. The layers of the common representation are fine-tuned based on the pre-trained weights of the offline technique. Here, two BiLSTM layers with 60 units each and ReLU activation extract the temporal context of the feature embedding. As for the baseline classifier, we train for 1,000 epochs. For evaluation, the main time-series network is independent of the image auxiliary network by using only the weights of the main network.\nf: Triplet Selection\nTo ensure (fast) convergence, it is crucial to select triplets that violate the constraint from Equation 1. Typically, it is infeasible to compute the loss for all triplet pairs, or this leads to poor training performance (as poorly chosen pairs dominate hard ones). This requires an elaborate triplet selection [13]. We use the Edit distance to define the identity and select triplets. The Edit distance is the minimum number of substitutions S, insertions I , and deletions D required to change the sequences d = (d1, . . . , dr) into g = (g1, . . . , gz) with length r and z, respectively. We define two sequences with an Edit distance of 0 as the positive pair, and with an Edit distance larger than 0 as the negative pair. Based on preliminary experiments, we use only substitutions for triplet selection that lead to a higher accuracy compared to additional insertions and deletions (whereas these would also change the length difference of image and time-series pairs). We constrain p \u2212 m/2 (the difference in pixels p of the images and half the number of timesteps of the time-series) to be maximally \u00b120. The goal is to achieve a small distance for positive pairs and a large distance for negative pairs that increases with a larger Edit distance (between 1 and 10). Furthermore, despite a limited number of word labels, there still exist a large number of image-time-series pairs per word label for every possible Edit distance (see Figure 7). For each batch, we search in a dictionary of negative sample pairs for samples with Edit_distance = 1 + \u230amaxe \u2212e\u22121100 \u230b as the lower bound for the current epoch e and maximal epochs maxe. For every label, we randomly pick one image. We let the margin \u03b1 in the triplet loss vary for each batch such that \u03b1 = \u03b2 \u00b7 Edit_distance depends on the mean Edit distance of the batch and is in the range [1, 11] with \u03b2 = 10\u22123 for MSE, \u03b2 = 0.1 for CS and PC, and \u03b2 = 1 for KL. The batch size is 100 andmaxe = 1, 000."
        },
        {
            "heading": "V. EXPERIMENTAL RESULTS",
            "text": "a: Hardware and Training Setup.\nFor all experiments, we use Nvidia Tesla V100-SXM2 GPUs with 32 GB VRAM equipped with Core Xeon CPUs and 192 GB RAM.We use the vanilla Adam optimizer with a learning rate of 10\u22124."
        },
        {
            "heading": "8 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA. EVALUATION OF SYNTHETIC DATA We train the time-series (TS) model 18 times with noise b = 0.3 and the combined model with the triplet loss for all 40 noise combinations ( b \u2208 {0, . . . , 1.95} ) with different deep metric learning functions. Figure 8 shows the validation accuracy averaged over all trainings as well as the combined cases separately for noise b < 0.2 and noise 0.2 \u2264 b < 2.0 (for the LCS loss). Table 1 summarizes the final classification results of all cases. The accuracy of the models that use only images and in combination with time-series during inference reach an accuracy of 99.7% (which can be seen as an unreachable upper bound for the TS-only models). The triplet loss improves the final TS baseline accuracy from 92.5% to 95.36% (averaged over all combinations), while combining TS and image data leads to a faster convergence. Conceptually similar to [14], we use the LkMMD loss, which yields 95.83% accuracy. The LPC (96.03%), LKL (96.22%), LMSE (96.25%), LBC (96.62%), and LPO (96.76%) loss functions can further improve the accuracy. We conclude that the triplet loss can be successfully used for cross-modal learning by utilizing negative identities.\nB. EVALUATION OF HANDWRITING RECOGNITION a: Evaluation Metrics. A metric for sequence evaluation is the character error rate (CER), defined as CER = Sc+Ic+DcNc , i.e., the Edit distance (the sum of character substitutions Sc, insertions Ic and deletions Dc) divided by the total number of characters in the set Nc. Similarly, the word error rate (WER) is defined as WER = Sw+Iw+Dw\nNw , which is computedwith the sum ofword operations Sw, Iw and Dw, divided by the number of words in the set Nw.\nb: Evaluation of Offline HWR Methods. Table 2 shows offline HWR results on our generated OffHWGerman dataset and on the IAM-OffDB [115] dataset. ScrabbleGAN [40] yields a WER of 23.61% on the IAM-OffDB dataset, while OrigamiNet [41] achieves a CER of 4.70% with 12 gated text recognizer modules. While OrigamiNet is trained for themulti-line classification, which is an easier task (as the image of the paragraph does not have to be segmented\ninto lines), we trained OrigamiNet on single-lines with zero padding, which is closer to the OffHW-German dataset. While the images for the multi-line task are of approximately similar lengths, the image lengths of the single-line task varies strongly, and hence, zero padding has a high influence on the model performance, resulting in a CER of 15.67%. While [41] did not propose WER results, OrigamiNet yields only a WER of 90.40%. This problem does not appear for the OffHW-German dataset, as the dataset contains only single words with similar lengths. With our own implementation of four gated text recognizer modules and one convolutional layer for the common representation, our model achieves similar results. As the training takes more than one day for one epoch on the large OffHW-German dataset, we train OrigamiNet with four gated text recognizer modules, and achieve 0.11% CER on the generated dataset and 15.67% on the IAM-OffDB dataset. All our models yield low error rates on the generated OffHW-German dataset. Our approach with gated text recognizer blocks outperforms (0.24% to 0.44% CER) the models with Inception [119] (1.17% CER) and ResNet [120] (1.24% CER). OrigamiNet achieves the lowest error rates of 1.50% WER and 0.11% CER. Four gated text recognizer blocks yield the best results at a significantly lower training time compared to six or eight blocks. We fine-tune the model with four gated text recognizer blocks for one and two convolutional layers and achieve notably low error rates between 0.22% to 0.76% CER, and between 0.85% to 2.95% WER on the OffHW-[words500, wordsRandom] datasets (see Table 3). While results for OffHW-wordsRandom are similar for writer-dependent (WD) and writer-independent (WI) tasks, WI results of the OffHW-words500 dataset are lower than WD results, as words with the same label appear in the training and test dataset.We use the weights of the fine-tuning as initial weights of the image model for the cross-modal representation learning.\nc: Evaluation of Representation Learning Feature Embeddings. Table 4 shows the feature embeddings for image f2(Xi) and time-series data f2(Ui) of the positive sample Export and the two negative samples Expert (Edit_distance = 1) and Import (Edit_distance = 2) based on four deep metric\nVOLUME 11, 2023 9\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nlearning loss functions. The pattern of characters are similar, as the words differ only in the fourth letter. In contrast, Import has a different feature embedding, as the replacement of E with I and x with m leads to a higher feature distance in the embedding hypersphere. Note that image and time-series data can vary in length for Edit_distance > 0. Figure 9 shows the feature embeddings of the output of the convolutional layers (c = 1) processed with t-SNE [123]. Figure 9a visualizes the multivariate time-series embeddings\nf1(Ui) of the single modal network. The learned representation generalizes well, but misclassifications (e.g., of small and capital letters at the beginning of a word, which happen quite often) also introduce errors in the latent representation. Figure 9b visualizes the multivariate time-series and image embeddings ( f1(Ui) and f1(Xi), respectively ) in a cross-modal setup. While the embedding of the single modal network is unstructured, the embeddings of the cross-modal network are structured (distance of samples visualizes the"
        },
        {
            "heading": "10 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 5: Evaluation results (WER and CER in %) averaged over five splits of the baseline time-series-only technique and our cross-modal learning technique for the inertial-based OnHW datasets [38] with and without mutated vowels (MV) for one convolutional layer c = 1. Best results are bold, and second best results are underlined. Arrows indicate improvements (\u2191) and degradation (\u2193) of baseline results (w/o MV).\nOnHW-words500 OnHW-wordsRandom WD WI WD WI\nMethod WER CER WER CER WER CER WER CER\nMain Task InceptionTime, LCTC, w/ MV 37.12 12.96 62.09 26.36 42.88 7.19 84.14 32.35 IT+BiLSTM, LCTC, w/ MV 43.22 13.07 61.62 26.08 39.14 6.39 85.42 33.31 CNN+BiLSTM, LCTC, w/ MV 42.81 13.04 60.47 28.30 37.13 6.75 83.28 35.90 CNN+BiLSTM, LCTC, w/o MV 42.77 13.44 59.82 28.54 38.02 7.81 83.54 36.51\nBaseline\nLMSE 40.76 \u2191 12.71 \u2191 55.54 \u2191 25.97 \u2191 37.31 \u2191 7.01 \u2191 82.25 \u2191 33.85 \u2191 LCS 38.62 \u2191 11.55 \u2191 56.37 \u2191 25.90 \u2191 38.85 \u2193 7.35 \u2191 82.48 \u2191 35.67 \u2191 LPC 39.09 \u2191 11.69 \u2191 57.90 \u2191 27.23 \u2191 38.46 \u2193 7.15 \u2191 82.71 \u2191 35.13 \u2191 LKL 38.36 \u2191 11.28 \u2191 60.23 \u2193 27.99 \u2191 38.76 \u2193 7.49 \u2191 81.07 \u2191 33.96 \u2191\nContrastive Lcontr,1(LMSE) 38.34 \u2191 11.57 \u2191 56.81 \u2191 25.98 \u2191 38.25 \u2193 7.31 \u2191 82.09 \u2191 34.03 \u2191 Loss Lcontr,1(LCS) 39.68 \u2191 11.73 \u2191 58.03 \u2191 27.13 \u2191 35.96 \u2191 6.67 \u2191 81.22 \u2191 33.11 \u2191Lcontr,1(LPC) 37.82 \u2191 11.34 \u2191 57.45 \u2191 26.18 \u2191 39.22 \u2193 7.39 \u2191 82.45 \u2191 34.21 \u2191\nLcontr,1(LKL) 36.70 \u2191 10.84 \u2191 61.72 \u2193 29.16 \u2193 38.92 \u2193 7.51 \u2191 83.54 35.52 \u2191\nTriplet Ltrpl,1(LMSE) 42.95 \u2193 14.13 \u2193 56.48 \u2191 26.66 \u2191 37.66 \u2191 7.04 \u2191 81.64 \u2191 34.39 \u2191 Loss Ltrpl,1(LCS) 38.01 \u2191 11.29 \u2191 58.50 \u2191 27.10 \u2191 37.12 \u2191 6.98 \u2191 82.71 \u2191 33.09 \u2191Ltrpl,1(LPC) 40.43 \u2191 12.41 \u2191 58.20 \u2191 27.48 \u2191 37.40 \u2191 7.01 \u2191 81.90 \u2191 33.89 \u2191\nLtrpl,1(LKL) 37.55 \u2191 11.21 \u2191 63.52 \u2193 30.52 \u2193 38.39 \u2193 7.36 \u2191 83.18 \u2191 35.21 \u2191\nEmbedding Time-series (single modality)\n(a) Feature embedding of IMU samples for the single modality network.\nEmbedding cross-modal\nTime-series Image\n(b) Feature embeddings of IMU and image samples for the crossmodal network.\nFIGURE 9: Comparison of the naive method (left) and our proposed approach (right), where our method shows a much better behaved embedding space compared to the naive approach by learning a joint representation. Plot of 400 \u00d7 200 feature embeddings of image and IMU modalities with tSNE.\nEdit distance between words) with the embeddings of the time-series modality being close to the embeddings of the image modality, and hence, more distinctive clusters with better separation.\nd: Evaluation of Cross-Modal Representation Learning. Table 5 gives an overview of cross-modal representation learning (for c = 1). The first row shows baseline results by [38]: 13.04% CER on OnHW-words500 (WD) and 6.75% CER on OnHW-wordsRandom (WD) with mutated vowels. Compared to various time-series classification techniques, their benchmark results showed superior performance of CNN+BiLSTMs on these OnHW recognition tasks. Only InceptionTime [124] (a large time-series encoder\nnetwork with depth = 11 and nf = 96) \u2013 with BiLSTM layers \u2013 yields partly better results or is on par with the CNN+BiLSTM model for sequence-based classification, while the CNN+BiLSTM model outperforms state-of-theart techniques on single character-based classification tasks. Due to the faster training of the CNN+BiLSTM, we chose this network for the cross-modal task. In general, the word error rate (WER) can vary for a similar character error rate (CER). The reason is that a change of one character of a correctly classified word leads to a large change in the WER, while the change of the CER is marginal. We define the results trained without mutated vowels as baseline results, as ScrabbleGAN is pretrained on IAM-OffDB, which does not contain mutated vowels, and hence, such words cannot be generated. Nevertheless, the main model can be trained and is applicable to samples with mutated vowels.\nFor a fair comparison, we compare our results to the results of the models trained without mutated vowels. Here, the error rates are slightly higher for both datasets. As expected, crossmodal learning improves the baseline results up to 11.28% CER on the OnHW-words500 WD dataset and up to 7.01% CER on the OnHW-wordsRandom WD dataset. The contrastive loss shows the best results on the OnHW-words500 (WD) dataset with the Kullback-Leibler metric and on the OnHW-wordsRandom dataset (WD) with the cosine similaritymetric.With the triplet loss,LCS outperforms othermetrics on the OnHW-wordsRandom dataset but is inconsistent on the OnHW-words500 dataset. The importance of the triplet loss is more significant for one convolutional layer (c = 1) than for two convolutional layers (c = 2) (see Appendix G). Furthermore, training with kMMD (implemented as in [14]) does not yield reasonable results. We assume that this metric\nVOLUME 11, 2023 11\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ncannot make use of the important time component in the HWRapplication.We proposed our approach as learningwith privileged information by exploiting a visual modality as an auxiliary task and improve the main task based on an inertial modality. The cross-modal learning would also work for the visual modality as the main task and a generated dataset for the inertial modality as an auxiliary task. However, the error rates are already low for the image-based classification task, as methods for offline HWR are very advanced and the image dataset is very large. Hence, we assume that finetuning the image encoder with inertial data would result in a minor improvement. Prior work [38] evaluated data augmentation techniques for multivariate time-series data (i.e., time warping, scaling, jittering, magnitude warping, and shifting). This approach was rather limited with only 2-3% points of improvement compared with augmentation with the auxiliary image-based task.\ne: Transfer Learning on Left-Handed Writers.\nTo adapt the model to left-handed writers (who are typically under-represented and hence marginalized in the realworld), we make use of the left-handed datasets OnHWwords500-L and OnHW-wordsRandom-L proposed by [38]. These datasets contain recordings of two writers who provided 1,000 and 996 samples. As a baseline, we pre-train the time-series-only model on the right-handed datasets and posttrain the left-handed datasets for 500 epochs (see the second and third rows of Table 6). As these datsets are rather small, the models can overfit on these specific writers and achieve a very low CER of 3.33% on the OnHW-words500-L datasets and 5.26%CER on the OnHW-wordsRandom-L dataset without mutated vowels for the writer-dependent tasks. However, the models cannot generalize on the writer-independent tasks,\nas evidenced by 62.07% CER on the OnHW-words500-L dataset and 81.15% CER on the OnHW-wordsRandom-L dataset. Hence, we focus on the WD tasks. For comparison, we use the state-of-the-art time-series classification technique InceptionTime [124] with depth = 11 and nf = 96 (without pre-training). As shown, our CNN+BiLSTM outperforms InceptionTime by a considerable margin. We use the weights of the pre-training with the offline handwriting datasets and again post-train on the left-handed datasets with c = 1 and c = 2. Using the weights of the cross-modal learning without the triplet loss can decrease the error rates up to 2.57% CER with LKL and 4.47% CER with LPC. Using the triplet lossLtrpl,2(LMSE) can further significantly decrease the WI OnHW-words500-L error rates. In conclusion, due to the use of the weights of the cross-modal setup, the model can adapt faster to new writers and generalize better to unseen words due to the triplet loss."
        },
        {
            "heading": "VI. CONCLUSION & FUTURE RESEARCH",
            "text": "We evaluated metric learning-based triplet loss functions for cross-modal representation learning between image and timeseries modalities with class label-specific triplet selection. We perform experiments on synthetic data for learning a common representation between images and time-series data for single class prediction. The label-specific triplet selection in combination with a deep metric learning loss leads to an accuracy improvement from 92.5% to 96.76% by being more robust against noise present in the data. Furthermore, we propose an extensive evaluation on handwriting datasets. We learn a common representation between offline handwriting data (image-based) and online handwriting data from sensor-enhanced pens (time-series-based). We generated two million images by employing ScrabbleGAN to imitate arbitrarily many writing styles. Our cross-modal"
        },
        {
            "heading": "12 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ntriplet loss with dynamic triplet selection based on the Edit distance further yields a faster training convergence with better generalization on the main task. The representation of the feature embeddings between both modalities is more structured and the model is more robust against different writing styles. This yields a notable accuracy improvement for the main time-series classification task (e.g., from 13.44% 10.84% CER for the OnHW-words500 dataset) that can be decoupled from the auxiliary image classification task at inference time. Our proposed method leads to a better adaptability to different writers, such as a better transfer learning from right-handed writers to the under-represented lefthanded writers.\nFor future work, the influence of the generative model to augment offline handwriting data can be elaborated. Recent models include the approaches presented in [125]\u2013[128]. The generator proposed by Kang et al. [126] conditions on both visual appearance and textual content, and it can produce text-line samples with diverse handwriting styles that visually outperform ScrabbleGAN. On the other hand, HiGAN+ [128] introduces a contextual loss to enhance style consistency and achieves better calligraphic style transfer. Furthermore, domain adaptation techniques such as higher-order moment matching (HoMM) by Chen et al. [129] can further improve the adaptability to left-handed writers.\nAPPENDIX. APPENDICES We provide more information about the broader impact, limitations, ethical concerns, and a comparison to writing on touch sceen surfaces in Section A. While Section B gives an overview of methods for offline handwriting recognition, Section C summarizes cross-modal retrieval methods, the corresponding modalities, pairwise learning, and deep metric learning. We present the multi-task learning technique in Section D, and show more details on learning with the triplet loss on synthetically generated signal and image data in Section E. We propose more details of our architectures in Section F. Section G presents results of representation learning for online HWR.\nA. STATEMENTS a: Broader Impact Statement While research for offline handwriting recognition (HWR) is well-established, research for online HWR from sensorenhanced pens only emerged in 2019. Hence, the methodological research for online HWR currently does not meet the requirements for real-world applications. Handwriting is still important in different fields, in particular graphomotoricity as a fine motor skill. The visual feedback provided by the pen helps young students to learn a new language. A wellknown bottleneck for many machine learning algorithms is their requirement for large amounts of datasets, while data recording of handwriting data is time-consuming. This paper extends the online HWR dataset with generated images from offline handwriting and closes the gap between offline and online HWR by using offline HWR as an auxiliary task\nby learning with privileged information. One downside of training the offline architecture (consisting of gated text recognizer blocks) is its long training time. However, as this model is not required at inference time, processing the timeseries is still fast. The cross-modal representation between both modalities (image and time-series) is achieved by using the triplet loss and a sample selection depending on the Edit distance. This approach is important in many applications of sequence-based classification, i.e., the triplet loss evolved recently for language processing applications such as visual semantic clustering, while pairwise learning is typically applied in fields such as image recognition.\nb: Limitations The limitation of the method is the requirement of multiple image-based datasets in the same language. As the OnHWwords and OnHW-wordsRandom datasets are written in German and contain word labels with mutated vowels, a similar image-based German dataset is required, which does not currently exist. The available dataset most similar to the OnHW dataset is the IAM-OffDB dataset, which does not contain mutated vowels. Hence, the OCR method cannot be pretrained on words with mutated vowels. In conclusion, the method is not limited by ScrabbleGAN, but by the imagebased dataset required for pre-training. The gated text recognizer could also be directly pre-trained on the IAM-OffDB dataset, but we assume less generalized results than for our generated dataset.\nc: Statement on Ethical Concerns Machine learning models face various challenges when classifying text with this sensor-enhanced pen. These challenges can appear if there is a domain shift between training and test datasets, e.g., specific writers have a unique writing style and accelerations, or they hold the pen differently. Also, some writers might have a unique writing environment (different writing surfaces such as a unique table or paper which leads to different magnetic fields). Another difficulty can appear through an under-represented group such as left-handed writers or a disabled person for which the model is not trained on. A well-generalized model trained on all possible pen movements is very challenging and requires a lot of training data. One solution is to record data for a unique writer and adapt the model, or augment the data for a better representation, e.g., as proposed with our method on left-handed writers. Hence, unique writers are not excluded and the task for classifying writing from under-represented groups is addressed in our paper, but domain shifts still remain a challenging problem. Ethical statement about collection consent and personal information: For data recording, the consent of all participants was collected. The datasets only contain the raw data from the sensor-enhanced pen and \u2013 for statistics \u2013 the age, gender, and handedness of the participants. The datasets are fully pseudonymized by assigning an ID to every participant. The datasets do not contain any personal identifying information. The approach proposed in this paper \u2013 in particular, when\nVOLUME 11, 2023 13\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nused for the application of online handwriting recognition from sensor-enhanced pens \u2013 does not (1) facilitate injury to living beings, (2) raise safety or security concerns (due to the anonymity of the data), (3) raise human rights concerns, (4) have a detrimental effect on people\u2019s livelihood, (5) develop harmful forms of surveillance (as the data is pseudonymized), (6) damage the environment, and (7) deceive people in ways that cause harm.\nd: Comparison to Writing on Touch Screen Surfaces Methods for writing on surfaces such as the iPad OS system and others require a tablet with a touch screen surface and stylus pens with integrated magnetometers or pressure sensitivity. These methods can easily reconstruct the trajectory of the pen tip through the magnetometer on the surface, and hence, can classify the written text. This is more challenging when using sensor-enhanced pens, as the classification task is performed directly on the sensor data. One drawback of methods used in the iPad OS is the requirement for writing on specific surfaces, which in turn can influence the writing style. Also, certain applications require writing on normal paper, or the availability of a touch screen surface is not always given, e.g., when writing a short list, but notes need to be digitized afterwards.\nB. OFFLINE HANDWRITING RECOGNITION In the following, we give a detailed overview of offline HWR methods to select a suitable lexicon and language model-free method. To our knowledge, there is no recent paper summarizing published work for offline HWR. For an overview of offline and online HWR datasets, see [29], [42]. Table 7 presents related work. Methods for offline HWR range from hidden Markov models (HMMs) to deep learning techniques that became predominant in 2014, such as convolutional neural networks (CNNs), temporal convolutional networks (TCNs), and recurrent neural networks (RNNs). RNN techniques are well explored, including long short-termmemories (LSTMs), bidirectional LSTMs (BiLSTMs), andmultidimensional RNNs (MDRNN, MDLSTM). Recent methods are generative adversarial networks (GANs) and Transformers. In Table 7, we refer to the use of a language model as LMwith k and identify the data level on which the method works \u2013 i.e., paragraph or full-text level (P), line level (L), and word level (W). We present evaluation results for the IAM-OffDB [115] and RIMES [130] datasets. We show the character error rate (CER) \u2013 the percentage of characters that were incorrectly predicted (the lower, the better) \u2013 and the word error rate (WER) \u2013 a common performancemetric onword level instead of the phoneme level (the lower, the better).\na: LSTMs and BiLSTMs. RNNs for HWR marked an important milestone in achieving impressive recognition accuracies. Sequential architectures are perfect to fit text lines, due to the probability distributions over sequences of characters, and due to the inherent temporal aspect of text [63]. [65] introduced the BiLSTM\nlayer in combination with the CTC loss. [64] showed that the performance of LSTMs can be greatly improved using dropout. [134] investigated sequence-discriminative training of LSTMs using the maximum mutual information (MMI) criterion. While [135] utilized an RNN with an HMM and a language model, [133] combined an RNN with a sliding window Gaussian HMM. GCRNN [67] combines a convolutional encoder (aiming for generic and multilingual features) and a BiLSTM decoder predicting character sequences. Additionally, [66] proposed a CNN+BiLSTM architecture (CNN-1DLSTM-CTC) that uses the CTC loss. The start, follow, read (SFR) [136] model jointly learns text detection and segmentation. [137] used synthetic data for pre-training and image normalization for slant correction. The methods by [52]\u2013[55] also make use of BiLSTMs. While [139] uses a feature pyramid network (FPN), the adversarial feature deformation module (AFDM) [140] learns ways to elastically warp extracted features in a scalable manner. Further methods that combine CNNs with RNNs are [68]\u2013[70], while BiLSTMs are utilized in [71], [72].\nb: TCNs. TCNs use dilated causal convolutions and have been applied to air-writing recognition by [142]. As RNNs are slow to train, [50] presented a faster system that is based on text line images and TCNs with the CTC loss. This method achieves 9.6% CER on the IAM-OffDB dataset. [51] combined 2D convolutions with 1D dilated non-causal convolutions that offer high parallelism with a smaller number of parameters. They analyzed re-scaling factors and data augmentation and achieved comparable results for the IAM-OffDB and RIMES datasets.\nc: CNNs. [48] utilized a CNN with multiple fully connected branches to estimate its n-gram frequency profile (set of n-grams contained in the word). With canonical correlation analysis (CCA), the estimated profile can be matched to the true profiles of all words in a large dictionary. As most attention methods suffer from an alignment problem, [49] proposed a decoupled attention network (DAN) that has a convolutional alignment module that decouples the alignment operation from using historical decoding results based on visual features. The gated text recognizer [73] aims to automate the feature extraction from raw input signals with a minimum required domain knowledge. The fully convolutional network without recurrent connections is trained with the CTC loss. Thus, the gated text recognizer module can handle arbitrary input sizes and can recognize strings with arbitrary lengths. This module has been used for OrigamiNet [41] which is a segmentation-free multi-line or full-page recognition system. OrigamiNet yields state-of-the-art results on the IAM-OffDB dataset, and shows improved performance of gated text recognizer over VGG and ResNet26. Hence, we use the gated text recognizer module as our visual feature encoder for offline HWR (see Section F)."
        },
        {
            "heading": "14 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nd: GANs. Handwriting text generation is a relatively new field. The first approach by [74] was a method to synthesize online data based on RNNs. The technique HWGAN by [75] extends this method by adding a discriminator D. DeepWriting [76] is a GAN that is capable of disentangling style from content and thus making digital ink editable. [77] proposed a method to generate handwriting based on a specific author with learned parameters for spacing, pressure, and line thickness. [78] used a BiLSTM to obtain an embedding of the word to be rendered and added an auxiliary network as a recognizer R. Themodel is trained with a combination of an adversarial loss and the CTC loss. ScrabbleGAN by [40] is a semi-supervised approach that can arbitrarily generate many images of words\nwith arbitrary length from a generator G to augment handwriting data and uses a discriminator D and recognizer R. The paper proposes results for original data with random affine augmentation using synthetic images and refinement.\ne: Transformers. RNNs prevent parallelization, due to their sequential pipelines. [63] introduced a non-recurrent model by the use of Transformer models with multi-head self-attention layers at the textual and visual stages. Their method works for any predefined vocabulary. For the feature encoder, they used modified ResNet50 models. The full page HTR (FPHR) method by [89] uses a CNN as an encoder and a Transformer as a decoder with positional encoding."
        },
        {
            "heading": "16 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nC. OVERVIEW OF CROSS-MODAL RETRIEVAL METHODS We provide a summary of methods for cross-modal learning in Table 8 and Table 9. Typical modalities are video, image, audio, text, sensors (such as inertial sensors used for our method), and haptic modalities. We classify each method with the technique used for pairwise learning that utilizes an objective for deep metric learning. The overview contains a wide range of applications, while visual semantic embedding is a common field for cross-modal retrieval.\nD. MULTI-TASK LEARNING We simultaneously train the LCTC loss for sequence classification combined with one or two shared losses Lshared,1 and Lshared,2 for cross-modal representation learning. As both losses are in different ranges, the naive weighting\nLtotal = |T |\u2211 i=1 \u03c9iLi, (3)\nwith pre-specified constant weights \u03c9i = 1,\u2200i \u2208 {1, . . . , |T |} can harm the training process. Hence, we apply dynamic weight average (DWA) [113] as a multi-task learning approach that performs dynamic task weighting over time (i.e., after each batch).\nE. TRAINING SYNTHETIC DATA WITH THE TRIPLET LOSS\na: Signal and Image Generation. We combine the networks for both signal and image classification to improve the classification accuracy over each singlemodal network. The aim is to show that the triplet loss can be used for such a cross-modal setting in the field of cross-modal representation learning. Hence, we generate synthetic data in which the image data contains information of the signal data. We generate signal data x with xi,k = sin ( 0.05 \u00b7 tik ) for all ti \u2208 {1, . . . , 1, 000} where ti is the timestep of the signal. The frequency of the signal is dependent on the class label k . We generate signal data for 10 classes (see Figure 10a). We add noise from a continuous uniform distribution U(a, b) for a = 0 and b = 0.3 (see Figure 10b) and add time and magnitude warping (see Figure 10c). We generate a signal-image pair such that the image is based on the signal data. We make use of the Gramian angular field that transforms time-series into images. The time-series is defined as x = (x1, . . . , xn) for n = 1, 000. The Gramian angular field creates a matrix of temporal correlations for each (xi, xj) by rescaling the timeseries in the range [p, q] with \u22121 \u2264 p < q \u2264 1 by\nx\u0302i = p+ (q\u2212 p) \u00b7 xi \u2212min(x)\nmax(x)\u2212min(x) ,\u2200i \u2208 {1, . . . , n}, (4)\nand computes the cosine of the sum of the angles for the Gramian angular summation field [114] by\nGASFi,j = cos (\u03d5i + \u03d5j),\u2200i, j \u2208 1, . . . , n, (5)\nVOLUME 11, 2023 17\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nwith \u03d5i = arccos (x\u0302i),\u2200i \u2208 {1, . . . , n} being the polar coordinates. We generate image datasets based on signal data with different noise parameters (b \u2208 {0.0, . . . , 1.95}) to show the influence of the image data on the classification accuracy. As an example, Figure 11 shows the Gramian angular summation field plots for the noise parameters b = [0, 0.5, 1.0, 1.5, 1.95]. We present the Gramian angular summation field for the classes 0, 5, and 9 to show the dependency of the frequency of the signal data on the Gramian angular summation field.\nb: Models.\nWe use the following models for classification. Our encoder for time-series classification consists of a 1D convolutional layer (filter size 50, kernel 4), a max pooling layer (pool size 4), batch normalization, and a dropout layer (20%). The image encoder consists of a layer normalization and 2D convolutional layer (filter size 200), and batch normalization\nwith ELU activation. After that, we add a 1D convolutional layer (filter size 200, kernel 4), max pooling (pool size 2), batch normalization, and 20% dropout. For both models, after the dropout layer follows a cross-modal representation \u2013 i.e., an LSTM with 10 units, a Dense layer with 20 units, a batch normalization layer, and a Dense layer of 10 units (for 10 sinusoidal classes). These layers are shared between both models."
        },
        {
            "heading": "F. DETAILS ON ARCHITECTURES FOR OFFLINE HWR",
            "text": "In this section, we provide details about the integration of Inception [119], ResNet [120] and gated text recognizer [73] modules into the offline HWR system. All three architectures are based on publicly available implementations, but we changed or adapted the first layer for the image input and the last layer for a proper input for our latent representation module."
        },
        {
            "heading": "18 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIn p u\nt im\na g e (\ud835\udc3b\n\u00d7 \ud835\udc4a )\nC o n\nv 2 D (6 4 ,1 \u00d7 1 )\nC o n\nv 2 D (9 6 ,1 \u00d7 1 )\nC o n\nv 2 D (1 6 ,1 \u00d7 1 )\nM a x P o o l (3 \u00d7 3 )\nC o n\nv 2 D (1 2 8 ,3 \u00d7 3 )\nC o n\nv 2 D (3 2 ,5 \u00d7 5 )\nC o n\nv 2 D (3 2 ,1 \u00d7 1 )\nC o n\nc a te\nn a te\nC o n\nv 2 D (1 2 8 ,1 \u00d7 1 )\nC o n\nv 2 D (1 2 8 ,1 \u00d7 1 )\nC o n\nv 2 D (3 2 ,1 \u00d7 1 )\nM a x P o o l (3 \u00d7 3 )\nC o n\nv 2 D (1 9 2 ,3 \u00d7 3 )\nC o n\nv 2 D (9 6 ,5 \u00d7 5 )\nC o n\nv 2 D (6 4 ,1 \u00d7 1 )\nC o n\nc a te\nn a te\nC o n\nv 2 D (1 9 2 ,1 \u00d7 1 )\nC o n\nv 2 D (9 6 ,1 \u00d7 1 )\nC o n\nv 2 D (1 6 ,1 \u00d7 1 )\nM a x P o o l (3 \u00d7 3 )\nC o n\nv 2 D (2 0 8 ,3 \u00d7 3 )\nC o n\nv 2 D (4 8 ,5 \u00d7 5 )\nC o n\nv 2 D (6 4 ,1 \u00d7 1 )\nC o n\nc a te\nn a te\nC o n\nv 1 D\n( 1 3 , 1 0 )\nL a te\nn t\nR e p re\ns e n\nta ti\no n\n\u2026\nC o n\nv 1 D\n( 6 4 , 1 0 )\nC o n\nv 1 D\n( 2 5 6 , 1 0 )\nM a x P o o l (3 \u00d7 3 )\nR e s h\na p e\n(- 1 , 5 1 2 )\nFIGURE 12: Offline HWR method based on Inception modules [119].\nIn p u\nt im\na g e (\ud835\udc3b\n\u00d7 \ud835\udc4a )\nC o n\nv 2 D\n( 6 4 , 7 )\nB a tc\nh N\no rm\n., R\ne L U\nZ e ro\nP a d d in\n2 D\n( 3 \u00d7 3 )\nZ e ro\nP a d d in\n2 D\n( 1 \u00d7 1 )\nM a x P o o li n\ng 2 D\n( 3 )\nC o n\nv 2 D\n( 6 4 , 7 \u00d7 7 ,\n/ 2 )\nC o n\nv 2 D\n( 6 4 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 6 4 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 6 4 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 6 4 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 6 4 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 6 4 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 ,\n/ 2 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 2 5 6 , 3 \u00d7 3 ,\n/ 2 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 2 5 6 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 2 5 6 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 2 5 6 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 2 5 6 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 2 5 6 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 1 2 8 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 5 1 2 , 3 \u00d7 3 ,\n/ 2 )\nC o n\nv 2 D\n( 5 1 2 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 5 1 2 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 5 1 2 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 5 1 2 , 3 \u00d7 3 )\nC o n\nv 2 D\n( 5 1 2 , 3 \u00d7 3 )\nR e s h\na p e\n(2 0 8 , 1 2 8 )\nC o n\nv 1 D\n( 2 0 0 , 5 )\nR e s h\na p e\n(4 0 0 , 1 0 4 )\nL a te\nn t\nR e p re\ns e n\nta ti\no n\n\u2026\nFIGURE 13: Offline HWR method based on the ResNet34 architecture [120].\na: Inception.\nFigure 12 gives an overview of the integration of the Inception module. The Inception module is part of the well-known GoogLeNet architecture. The main idea is to consider how an optimal local sparse structure can be approximated by readily available dense components. As the merging of pooling layer outputs with convolutional layer outputs would lead to an inevitable increase in the number of output and would lead to a high computational increase, we apply the Inceptionmodule with dimensionality reduction to our offline HWR approach [119]. The input image is of size H \u00d7W . What follows is the Inception (3a), Inception (3b), a max pooling layer (3 \u00d7 3) and Inception (4a). We add three 1D convolutional layers to obtain an output dimensionality of 400\u00d7 200 as the input for the latent representation.\nb: ResNet34.\nFigure 13 provides an overview of the integration of the ResNet34 architecture. Instead of learning unreferenced functions, [120] reformulated the layers as learning residual functions with reference to the layer inputs. This residual network is easier to optimize and can gain accuracy from considerably increased depth. The ResNet block allows the layers to fit a residual mapping denoted asH(x)with identity x and fits the mapping F(x) := H(x)\u2212 x. The original mapping is recast into F(x) + x. We reshape the output of ResNet34, add a 1D convolutional layer, and reshape the output for the latent representation.\nc: Gated Text Recognizer.\nFigure 14 gives an overview of the integration of the gated text recognizer [73] module \u2013 a fully convolutional network that uses batch normalization and layer normalization to reg-\nVOLUME 11, 2023 19\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nularize the training process and increase convergence speed. Themodule uses batch renormalization [169] on all batch normalization layers. Depthwise separable convolutions reduce the number of parameters at the same/better classification performance. The gated text recognizer uses spatial dropout instead of regular unstructured dropout for better regularization. After the input image of size H \u00d7W that is normalized follows a convolutional layer with Softmax normlization, a 13 \u00d7 13 filter, and dropout (40%). After the dropout layer, a stack of 2, 4, 6 or 8 gate blocks follows that models the input sequence. Similar to [73], we add a dropout of 20% after the last gated text recognizer block. Lastly, we add a 2D convolutional layer of 200, a batch normalization layer\nand a layer normalization layer that is the input for our latent representation."
        },
        {
            "heading": "G. DETAILED ONLINE HWR EVALUATION",
            "text": "Table 10 gives an overview of cross-modal representation learning results based on two convolutional layers (c = 2) for the cross-modal representation. Our CNN+BiLSTM contains three additional convolutional layers and outperforms the smaller CNN+BiLSTM by [38] on the WD classification tasks. Without triplet loss, LPC yields the best results on the OnHW-wordsRandom dataset. The triplet loss partly decreases results and partly improves results on the OnHWwords500 dataset. In conclusion, two convolutional layers for the cross-modal representation has a negative impact, while"
        },
        {
            "heading": "20 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nhere the triplet loss has no impact.\nREFERENCES [1] Y. Peng, X. Huang, and Y. Zhao, \u2018\u2018AnOverview of Cross-media Retrieval:\nConcepts, Methodologies, Benchmarks and Challenges,\u2019\u2019 in Trans. on Circuits and Systems for Video Technology, vol. 28(9), Apr. 2017, pp. 2372\u20132385. [2] H. Lee, J. Lee, J. Y.-H. Ng, and P. Natsev, \u2018\u2018Large Scale Video Representation Learning via Relational Graph Clustering,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2020. [3] N. Sarafianos, X. Xu, and I. A. Kakadiaris, \u2018\u2018Adversarial Representation Learning for Text-to-Image Matching,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Seoul, Korea, 2019, pp. 5814\u20135824. [4] V. Ranjan, N. Rasiwasia, and C. V. Jawahar, \u2018\u2018Multi-Label Cross-Modal Retrieval,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Santiago de Chile, Chile, Dec. 2015. [5] X. Huang, Y. Peng, and M. Yuan, \u2018\u2018MHTN: Modal-Adversarial Hybrid Transfer Network for Cross-Modal Retrieval,\u2019\u2019 in Trans. on Cybernetics, vol. 50(3), 2020, pp. 1047\u20131059. [6] F. Faghri, D. J. Fleet, H. R. Kiros, and S. Fidler, \u2018\u2018VSE++: Improving Visual-Semantic Embeddings with Hard Negatives,\u2019\u2019 in British Machine Vision Conf. (BMVC), 2018. [7] J. Chen, H. Hu, H.Wu, Y. Jiang, and C.Wang, \u2018\u2018Learning the Best Pooling Strategy for Visual Semantic Embedding,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, Jun. 2022. [8] J. H. Lim, P. O. O. Pinheiro, N. Rostamzadeh, C. Pal, and S. Ahn, \u2018\u2018Neural Multisensory Scene Inference,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), vol. 32(807), 2019, pp. 8996\u20139006. [9] F. M. Hafner, A. Bhuyian, J. F. P. Kooij, and E. Granger, \u2018\u2018Cross-Modal Distillation for RGB-Depth Person Re-Identification,\u2019\u2019 in Computer Vision and Image Understanding (CVIU), vol. 103352, Jan. 2022. [10] V. Vapnik and R. Izmailov, \u2018\u2018Learning Using Privileged Information: Similarity Control and Knowledge Transfer,\u2019\u2019 in Journal of Machine Learning Research (JMLR), Sep. 2015, pp. 2023\u20132049. [11] A. Momeni and K. Tatwawadi, \u2018\u2018Understanding LUPI (Learning Using Privileged Information),\u2019\u2019 2018. [Online]. Available: https://web.stanford. edu/~kedart/files/lupi.pdf [12] Y. Wei, Y. Zhao, C. Lu, S. Wei, L. Liu, Z. Zhu, and S. Yan, \u2018\u2018CrossModal Retrieval with CNN Visual Features: A New Baseline,\u2019\u2019 in Trans. on Cybernetics, vol. 47(2), Mar. 2016, pp. 449\u2013460. [13] T.-T. Do, T. Tran, I. Reid, V. Kumar, T. Hoang, and G. Carneiro, \u2018\u2018A Theoretically Sound Upper Bound on the Triplet Loss for Improving the Efficiency of Deep Distance Metric Learning,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019, pp. 10 404\u201310 413. [14] M. Long, Y. Cao, L. Wang, and M. I. Jordan, \u2018\u2018Learning Transferable Features with Deep Adaptation Networks,\u2019\u2019 in Intl. Conf. on Machine Learning (ICML), vol. 37, Jul. 2015, pp. 97\u2013105. [15] F. Schroff, D. Kalenichenko, and J. Philbin, \u2018\u2018FaceNet: A Unified Embedding for Face Recognition and Clustering,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, MA, Jun. 2015. [16] F. Ott, D. R\u00fcgamer, L. Heublein, B. Bischl, and C. Mutschler, \u2018\u2018Domain Adaptation for Time-Series Classification to Mitigate Covariate Shift,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Lisboa, Portugal, Oct. 2022, pp. 5934\u20135943. [17] S. Deldari, H. Xue, A. Saeed, D. V. Smith, and F. D. Salim, \u2018\u2018COCOA: Cross Modality Contrastive Learning for Sensor Data,\u2019\u2019 in Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), vol. 6(3), article 108, Sep. 2022, pp. 1\u201328. [18] Y. Jain, C. I. Tang, C. Min, F. Kawsar, and A. Mathur, \u2018\u2018ColloSSL: Collaborative Self-Supervised Learning for Human Activity Recognition,\u2019\u2019 in Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), Feb. 2022. [19] S. Venkataramanan, E. Kijak, L. Amsaleg, and Y. Avrithis, \u2018\u2018AlignMix: Improving Representations by Interpolating Aligned Fetaures,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2022, pp. 19 174\u201319 183. [20] Q. Wan and Q. Zou, \u2018\u2018Learning Metric Features for Writer-Independent Signature Verification using Dual Triplet Loss,\u2019\u2019 in Intl. Conf. on Pattern Recognition (ICPR), Milan, Italy, Jan. 2021, pp. 3853\u20133859.\n[21] W. Li, X. Yang, M. Kong, L. Wang, J. Huo, Y. Gao, and J. Luo, \u2018\u2018Triplet is All You Need with Random Mappings for Unsupervised Visual Representation Learning,\u2019\u2019 in arXiv preprint arXiv:2107.10419, Jul. 2021. [22] S. Kim, D. Kim, M. Cho, and S. Kwak, \u2018\u2018Proxy Anchor Loss for Deep Metric Learning,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2020, pp. 3238\u20133247. [23] Y. Zhen, P. Rai, H. Zha, and L. Carin, \u2018\u2018Cross-Modal Similarity Learning via Pairs, Preferences, and Active Supervision,\u2019\u2019 in Association for the Advancement of Artificial Intelligence (AAAI), Jan. 2015, pp. 3203\u20133209. [24] D. Zhang and Z. Zheng, \u2018\u2018Joint Representation Learning with Deep Quadruplet Network for Real-Time Visual Tracking,\u2019\u2019 in IEEE Intl. Joint Conf. on Neural Networks (IJCNN), Glasgow, UK, Jul. 2020. [25] A. F. Biten, A. Mafla, L. G\u00f3mez, and D. Karatzas, \u2018\u2018Is an Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching,\u2019\u2019 in IEEE/CVF Winter Conf. on Applications of Computer Vision (WACV), Waikoloa, HI, Jan. 2022. [26] H. Diao, Y. Zhang, L. Ma, and H. Lu, \u2018\u2018Similarity Reasoning and Filtration for Image-Text Matching,\u2019\u2019 in Association for the Advancement of Artificial Intelligence (AAAI), vol. 35(2), 2021, pp. 1218\u20131226. [27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \u2018\u2018Learning Transferable Visual Models From Natural Language Supervision,\u2019\u2019 in Intl. Conf. on Machine Learning (ICML), vol. 139, 2021, pp. 8748\u20138763. [28] M. M. M. Fahmy, \u2018\u2018Online Signature Verification and Handwriting Classification,\u2019\u2019 in Journal on Ain Shams Engineering (ASEJ), vol. 1(1), Sep. 2010, pp. 59\u201370. [29] R. Plamondon and S. N. Srihari, \u2018\u2018On-line and Off-line Handwriting Recognition: A Comprehensive Survey,\u2019\u2019 in Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 22(1), Jan. 2000, pp. 63\u201384. [30] F. Alimoglu and E. Alpaydin, \u2018\u2018Combining Multiple Representations and Classifiers for Pen-based Handwritten Digit Recognition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), vol. 2, Ulm, Germany, Aug. 1997. [31] J. K. Chen, W. Xie, and Y. K. He, \u2018\u2018Motion-based Handwriting Recognition,\u2019\u2019 in arXiv preprint arXiv:2101.06022, Jan. 2021. [32] M. Schrapel, M.-L. Stadler, andM. Rohs, \u2018\u2018Pentelligence: Combining Pen Tip Motion and Writing Sounds for Handwritten Digit Recognition,\u2019\u2019 in Proc. of the CHI Conf. on Human Factors in Computing Systems, Apr. 2018, pp. 1\u201311. [33] J.-S. Wang, Y.-L. Hsu, and C.-L. Chu, \u2018\u2018Online Handwriting Recognition Using an Accelerometer-Based Pen Device,\u2019\u2019 in Intl. Conf. on Computer Science and Engineering (CSE), Jul. 2013. [34] T. Deselaers, D. Keysers, J. Hosang, and H. A. Rowley, \u2018\u2018GyroPen: Gyroscopes for Pen-Input with Mobile Phones,\u2019\u2019 in Trans. on HumanMachine Systems, vol. 45(2), Apr. 2015, pp. 263\u2013271. [35] F. Ott, M. Wehbi, T. Hamann, J. Barth, B. Eskofier, and C. Mutschler, \u2018\u2018The OnHW Dataset: Online Handwriting Recognition from IMUEnhanced Ballpoint Pens with Machine Learning,\u2019\u2019 in Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), vol. 4(3), article 92, Canc\u00fan, Mexico, Sep. 2020. [36] F. Ott, D. R\u00fcgamer, L. Heublein, B. Bischl, and C.Mutschler, \u2018\u2018Joint Classification and Trajectory Regression of OnlineHandwriting using aMultiTask Learning Approach,\u2019\u2019 in IEEE/CVFWinter Conf. for Applications on Computer Vision (WACV), Waikoloa, HI, Jan. 2022, pp. 266\u2013276. [37] A. Kla\u00df, S. M. Lorenz, M. W. Lauer-Schmaltz, D. R\u00fcgamer, B. Bischl, C. Mutschler, and F. Ott, \u2018\u2018Uncertainty-aware Evaluation of Time-Series Classification for Online Handwriting Recognition with Domain Shift,\u2019\u2019 in IJCAI-ECAI Intl. Workshop on Spatio-Temporal Reasoning and Learning (STRL), vol. 3190, Vienna, Austria, Jul. 2022. [38] F. Ott, D. R\u00fcgamer, L. Heublein, T. Hamann, J. Barth, B. Bischl, and C. Mutschler, \u2018\u2018Benchmarking Online Sequence-to-Sequence and Character-based Handwriting Recognition from IMU-Enhanced Pens,\u2019\u2019 in International Journal on Document Analysis and Recognition (IJDAR), vol. 25(12), Sep. 2022, p. 385\u2013414. [39] A. Vinciarelli and M. P. Perrone, \u2018\u2018Combining Online and Offline Handwriting Recognition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Edinburgh, UK, Aug. 2003, pp. 844\u2013848. [40] S. Fogel, H. Averbuch-Elor, S. Cohen, S. Mazor, and R. Litman, \u2018\u2018ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2020, pp. 4324\u20134333. [41] M. Yousef and T. E. Bishop, \u2018\u2018OrigamiNet: Weakly-Supervised, Segmentation-Free, One-Step, Full Page Text Recognition by Learning\nVOLUME 11, 2023 21\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nto Unfold,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2020, pp. 14 710\u201314 719. [42] R. Hussain, A. Raza, I. Siddiqi, K. Khurshid, and C. Djeddi, \u2018\u2018A Comprehensive Survey of Handwritten Document Benchmarks: Structure, Usage and Evaluation,\u2019\u2019 in EURASIP Journal on Image and Video Processing, vol. 46, Dec. 2015. [43] R. Bertolami and H. Bunke, \u2018\u2018Hidden Markov Model-based Ensemble Methods for Offline Handwritten Text Line Recognition,\u2019\u2019 in Pattern Recognition, vol. 41(11), Nov. 2018, pp. 3452\u20133460. [44] P. Dreuw, P. Doetsch, C. Plahl, and H. Ney, \u2018\u2018Hierarchical Hybrid MLP/HMM or Rather MLP Features for a Discriminatively Trained Gaussian HMM: A Comparison for Offline Handwriting Recognition,\u2019\u2019 in IEEE Intl. Conf. on Image Processing (ICIP), Brussels, Belgium, Sep. 2011. [45] N. Li, J. Chen, H. Cao, B. Zhang, and P. Natarajan, \u2018\u2018Applications of Recurrent Neural Network Language Model in Offline Handwriting Recognition and Word Spotting,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Hersonissos, Greece, Sep. 2014. [46] J. Pastor-Pellicer, S. E. na Boquera, M. J. Castro-Bleda, and F. ZamoraMart\u00ednez, \u2018\u2018A Combined Convolutional Neural Network and Dynamic ProgrammingApproach for Text LineNormalization,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Tunis, Tunisia, Aug. 2015, pp. 341\u2013345. [47] S. Espa\u00f1a-Boquera, M. Castro-Bleda, J. Gorbe-Moya, and F. ZamoraMartinez, \u2018\u2018Improving Offline Handwritten Text Recognition with Hybrid HMM/ANNModels,\u2019\u2019 in Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 33(4), Apr. 2011, pp. 767\u2013779. [48] A. Poznanski and L.Wolf, \u2018\u2018CNN-N-Gram for HandwritingWord Recognition,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, Jun. 2016, pp. 2306\u20132314. [49] T. Wang, Y. Zhu, L. Jin, C. Luo, X. Chen, Y. Wu, Q. Wang, and M. Cai, \u2018\u2018Decoupled Attention Network for Text Recognition,\u2019\u2019 in Association for the Advancement of Artificial Intelligence (AAAI), vol. 34(7), Apr. 2020, pp. 12 216\u201312 224. [50] A. Sharma, R. Ambati, and D. B. Jayagopi, \u2018\u2018Towards Faster Offline Handwriting Recognition using Temporal Convolutional Networks,\u2019\u2019 in NCVPRIPG 2019 Communications in Computer and Information Science (CCIS), Springer, Singapore, vol. 1249, Nov. 2020, pp. 344\u2013354. [51] A. Sharma and D. B. Jayagopi, \u2018\u2018Towards Efficient Unconstrained Handwriting Recognition using Dilated Temporal Convolutional Network,\u2019\u2019 in Expert Systems with Applications, vol. 164, Feb. 2021. [52] A. Chowdhury and L. Vig, \u2018\u2018An Efficient End-to-End Neural Model for Handwritten Text Recognition,\u2019\u2019 in British Machine Vision Conference (BMVC), 2018. [53] J. Sueiras, V. Ruiz, A. Sanchez, and J. F. Velez, \u2018\u2018Offline Continuous Handwriting Recognition Using Sequence-to-Sequence Neural Networks,\u2019\u2019 in Neurocomputing, vol. 289(C), May 2018, pp. 119\u2013128. [54] R. R. Ingle, Y. Fujii, T. Deselaers, J. Baccash, and A. C. Popat, \u2018\u2018A Scalable Handwritten Text Recognition System,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Sydney, Australia, Sep. 2019. [55] J. Michael, R. Labahn, T. Gr\u00fcning, and J. Z\u00f6llner, \u2018\u2018Evaluating Sequenceto-Sequence Models for Handwritten Text Recognition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Sydney, NSW, Sep. 2019. [56] A. Graves and J. Schmidhuber, \u2018\u2018Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), Dec. 2008, pp. 545\u2013552. [57] T. Bluche, \u2018\u2018Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), Dec. 2016, pp. 838\u2013846. [58] P. Voigtlaender, P. Doetsch, and H. Ney, \u2018\u2018Handwriting Recognition with Large Multidimensional Long Short-Term Memory Recurrent Neural Networks,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Shenzhen, China, Oct. 2016, pp. 228\u2013233. [59] Z. Chen, Y.Wu, F. Yin, and C.-L. Liu, \u2018\u2018Simultaneous Script Identification and Handwriting Recognition via Multi-Task Learning of Recurrent Neural Networks,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017, pp. 525\u2013530. [60] T. Bluche, J. Louradour, andR.Messina, \u2018\u2018Scan, Attend andRead: End-toEnd Handwritten Paragraph Recognition with MDLSTM Attention,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017.\n[61] D. Castro, B. L. D. Bezerra, and M. Valen\u00e7a, \u2018\u2018Boosting the Deep Multidimensional Long-Short-TermMemory Network for Handwritten Recognition Systems,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Niagara Falls, NY, Aug. 2018. [62] P. Krishnan, K. Dutta, and C. V. Jawahar, \u2018\u2018Word Spotting and Recognition using Deep Embedding,\u2019\u2019 in IAPR Intl. Workshop on Document Analysis Systems (DAS), Vienna, Austria, 2018, pp. 1\u20136. [63] L. Kang, P. Riba, M. Rusinol, A. Fornes, and M. Villegas, \u2018\u2018Pay Attention to What You Read: Non-recurrent Handwritten Text-Line Recognition,\u2019\u2019 in Pattern Recognition, vol. 129, Sep. 2022. [64] V. Pham, T. Bluche, C. Kermorvant, and J. Louradour, \u2018\u2018Dropout Improves Recurrent Neural Networks for Handwriting Recognition,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Hersonissos, Greece, Sep. 2014, pp. 285\u2013290. [65] A. Graves, M. Liwicki, S. Fern\u00e1ndez, R. Bertolami, H. Bunke, and J. Schmidhuber, \u2018\u2018A Novel Connectionist System for Unconstrained Handwriting Recognition,\u2019\u2019 in Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 31(5), May 2009, pp. 855\u2013868. [66] J. Puigcerver, \u2018\u2018AreMultidimensional Recurrent Layers Really Necessary for Handwritten Text Recognition?\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, 2017, pp. 67\u201372. [67] T. Bluche and R. Messina, \u2018\u2018Gated Convolutional Recurrent Neural Networks for Multilingual Handwriting Recognition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017, pp. 646\u2013651. [68] D. Liang, W. Xu, and Y. Zhao, \u2018\u2018Combining Word-Level and CharacterLevel Representations for Relation Classification of Informal Text (RepL4NLP),\u2019\u2019 in Workshop on Representation Learning for NLP (RepL4NLP), Vancouver, Canada, Aug. 2017, p. 43\u201347. [69] S. Sudholt and G. A. Fink, \u2018\u2018Attribute CNNs for Word Spotting in Handwritten Documents,\u2019\u2019 in Intl. Journal on Document Analysis and Recognition (IJDAR), vol. 21, Feb. 2018, pp. 199\u2013218. [70] Y. Xiao and K. Cho, \u2018\u2018Efficient Character-level Document Classification by Combining Convolution and Recurrent Layers,\u2019\u2019 in arXiv preprint arXiv:1602.00367, Feb. 2016. [71] V. Carbune, P. Gonnet, T. Deselaers, H. A. Rowley, A. Daryin, M. Calvo, L.-L. Wang, D. Keysers, S. Feuz, and P. Gervais, \u2018\u2018Fast Multi-language LSTM-based Online Handwriting Recognition,\u2019\u2019 in Intl. Journal on Document Analysis and Recognition (IJDAR), vol. 23, Feb. 2020, pp. 89\u2013102. [72] B. Tian, Y. Zhang, J. Wang, and C. Xing, \u2018\u2018Hierarchical Inter-Attention Network for Document Classification with Multi-Task Learning,\u2019\u2019 in Intl. Joint Conf. on Artificial Intelligence (IJCAI), Aug. 2019, pp. 3569\u20133575. [73] M. Yousef, K. F. Hussain, and U. S. Mohammed, \u2018\u2018Accurate, DataEfficient, Unconstrained Text Recognition with Convolutional Neural Networks,\u2019\u2019 in Pattern Recognition, vol. 108, Dec. 2020. [74] A. Graves, \u2018\u2018Generating Sequences with Recurrent Neural Networks,\u2019\u2019 in arXiv preprint arXiv:1308.0850, Jun. 2014. [75] B. Ji and T. Chen, \u2018\u2018Generative Adversarial Network for Handwritten Text,\u2019\u2019 in arXiv preprint arXiv:1907.11845, Feb. 2020. [76] E. Aksan, F. Pece, and O. Hilliges, \u2018\u2018DeepWriting: Making Digital Ink Editable via DeepGenerativeModeling,\u2019\u2019 inCHIConf. onHuman Factors in Computing Systems, Apr. 2018, pp. 1\u201314. [77] T. S. F. Haines, O. M. Aodha, and G. J. Brostow, \u2018\u2018My Text in Your Handwriting,\u2019\u2019 in ACM Trans. on Graphics, vol. 35(3), May 2016, pp. 1\u201318. [78] E. Alonso, B. Moysset, and R. Messina, \u2018\u2018Adversarial Generation of Handwritten Text Images Conditioned on Sequences,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Sydney, Australia, Sep. 2019. [79] H. Zhang, L. Chen, Y. Zhang, R. Hu, C. He, Y. Tan, and J. Zhang, \u2018\u2018A Wearable Real-Time Character Recognition System Based on Edge Computing-Enabled Deep Learning for Air-Writing,\u2019\u2019 in Journal of Sensors, vol. 2022, May 2022. [80] Y. Bu, L. Xie, Y. Yin, C. Wang, J. Ning, J. Cao, and S. Lu, \u2018\u2018HandwritingAssistant: Reconstructing Continuous Strokes with Millimeter-level Accuracy via Attachable Inertial Sensors,\u2019\u2019 in Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), vol. 5(4), article 146, Dec. 2021, pp. 1\u201325. [81] G. He, Z. Wu, Y. Wu, P. Lin, and H. Huangfu, \u2018\u2018Online Handwriting Recognition Based on Microphone and IMU,\u2019\u2019 in Intl. Conf. on Electronics Technology (ICET), Chengdu, China, May 2022. [82] S. K. Singh and A. Chaturvedi, \u2018\u2018Leveraging Deep Feature Learning for Wearable Sensors Based Handwritten Character Recognition,\u2019\u2019 in Biomedical Signal Processing and Control, vol. 80(1), Feb. 2023."
        },
        {
            "heading": "22 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[83] Q. He, Z. Feng, X. Wang, Y. Wu, and J. Wang, \u2018\u2018A Smart Pen Based on Triboelectric Effects for Handwriting Pattern Tracking and Biometric Identification,\u2019\u2019 in ACS Appl. Mater. Interfaces, vol. 14(43), Oct. 2022. [84] T. T. Alemayoh, M. Shintani, J. H. Lee, and S. Okamoto, \u2018\u2018DeepLearning-Based Character Recognition from Handwriting Motion Data Captured Using IMU and Force Sensors,\u2019\u2019 inMDPI Sensors, vol. 22(20), Oct. 2022. [85] F. Kre\u00df, A. Serdyuk, T. Hotfilter, J. Hoefer, T. Harbaum, J. Becker, and T. Hamann, \u2018\u2018Hardware-aware Workload Distribution for AI-based Online Handwriting Recognition in a Sensor Pen,\u2019\u2019 in Mediterranean Conference on Embedded Computing (MECO), Budva, Montenegro, Jun. 2022. [86] F. Ott, D. R\u00fcgamer, L. Heublein, B. Bischl, and C. Mutschler, \u2018\u2018Representation learning for tablet and paper domain adaptation in favor of online handwriting recognition,\u2019\u2019 in IAPR Intl. Workshop on Multimodal Pattern Recognition of Social Signals in Human Computer Interaction (MPRSS), Montreal, Canada, Aug. 2022. [87] L. Wegmeth, A. Hoelzemann, and K. V. Laerhoven, \u2018\u2018Detecting Handwritten Mathematical Terms with Sensor Based Data,\u2019\u2019 in arXiv preprint arXiv:2109.05594, Sep. 2021. [88] M. Bronkhorst, \u2018\u2018A Pen is All You Need,\u2019\u2019 in Twente Student Conf. on IT, Enschede, The Netherlands, 2021. [89] S. S. Singh and S. Karayev, \u2018\u2018Full Page Handwriting Recognition via Image to Sequence Extraction,\u2019\u2019 in IAPR Intl. Conf. onDocument Analysis and Recognition (ICDAR), Lausanne, Switzerland, Mar. 2021, pp. 55\u201369. [90] H. Azimi, S. Chang, J. Gold, and K. Karabina, \u2018\u2018Improving Accuracy and Explainability of Online Handwriting Recognition,\u2019\u2019 in arXiv preprint arXiv:2209.09102, Sep. 2022. [91] N. Rasiwasia, J. C. Pereira, E. Coviello, G. Doyle, G. R. Lanckriet, R. Levy, and N. Vasconcelos, \u2018\u2018A New Approach to Cross-Modal Multimedia Retrieval,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2010, pp. 251\u2013260. [92] S. Deldari, H. Xue, A. Saeed, J. He, D. V. Smith, and F. D. Salim, \u2018\u2018Beyond Just Vision: A Review on Self-Supervised Representation Learning on Multimodal and Temporal Data,\u2019\u2019 in arXiv preprint arXiv:2206.02353, Jun. 2022. [93] X. Gu, L. Ou, D. Ong, and Y. Wang, \u2018\u2018MM-ALT: A Multimodal Automatic Lyric Transcription System,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 3328\u20133337. [94] X. Zeng, D. Xiang, L. Peng, C. Liu, and X. Ding, \u2018\u2018Local Discriminant Training and Global Optimization for Convolutional Neural Network Based Handwritten Chinese Character Recognition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017. [95] W. Liu, Y. Wen, Z. Yu, M. Li, B. Raj, and L. Song, \u2018\u2018SphereFace: Deep Hypersphere Embedding for Face Recognition,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, Jul. 2017. [96] S. Chopra, R. Hadsell, and Y. LeCun, \u2018\u2018Learning a Similarity Metric Disriminatively, with Application to Face Verification,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), San Diego, CA, Jun. 2005. [97] T. Yoshida, I. Takeuchi, and M. Karasuyama, \u2018\u2018Safe Triplet Screening for Distance Metric Learning,\u2019\u2019 in Neural Computation, vol. 31(12), Oct. 2019, pp. 2432\u20132491. [98] B. Harwood, V. K. B.G., G. Carneiro, I. Reid, and T. Drummond, \u2018\u2018Smart Mining for DeepMetric Learning,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Venice, Italy, Oct. 2017. [99] D. Semedo and J. Magalh\u00e3es, \u2018\u2018Adaptive Temporal Triplet-loss for Crossmodal Embedding Learning,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2020, pp. 1152\u20131161. [100] D. Guo, S. Tang, and M. Wang, \u2018\u2018Connectionist Temporal Modeling of Video and Language: A Joint Model for Translation and Sign Labeling,\u2019\u2019 in Intl. Joint Conf. on Artificial Intelligence (IJCAI), 2019, pp. 751\u2013757. [101] D. Zeng, Y. Yu, and K. Oyama, \u2018\u2018Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual Cross-Modal Retrieval,\u2019\u2019 in ACM Trans. on Multimedia Computing, Communications, and Applications, vol. 16(3), Aug. 2020, pp. 1\u201323. [102] U. Chaudhuri, B. Banerjee, A. Bhattacharya, andM. Datcu, \u2018\u2018CrossATNet - A Novel Cross-Attention based Framework for Sketch-based Image Retrieval,\u2019\u2019 in Image and Vision Computing, vol. 104, Dec. 2020. [103] A. Gordo and D. Larlus, \u2018\u2018Beyond Instance-Level Image Retrieval: Leveraging Captions to Learn a Global Visual Representation for Semantic\nRetrieval,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, Jul. 2017.\n[104] C. Deng, Z. Chen, X. Liu, X. Gao, and D. Tao, \u2018\u2018Triplet-based Deep Hashing Network for Cross-Modal Retrieval,\u2019\u2019 in IEEE Trans. on Image Processing, vol. 27(8), Apr. 2018, pp. 32 893\u20133903. [105] J. Zhang, Y. Kalantidis, M. Rohrbach, A. Elgammal, and M. Elhoseiny, \u2018\u2018Large-Scale Visual Relationship Understanding,\u2019\u2019 in Association for the Advancement of Artificial Intelligence (AAAI), vol. 33(1), Jul. 2019, pp. 9185\u20139194. [106] H. Bredin, \u2018\u2018TristouNet: Triplet Loss for Speaker Turn Embedding,\u2019\u2019 in IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2017, pp. 5430\u20135434. [107] J.Wang, T. Gong, Z. Zeng, C. Sun, and Y. Yan, \u2018\u2018C3CMR: Cross-Modality Cross-Instance Contrastive Learning for Cross-Media Retrieval,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 4300\u2013 4308. [108] Y. Ohishi, M. Delcroix, T. Ochiai, S. Araki, D. Takeuchi, D. Niizumi, A. Kimura, N. Harada, and K. Kashino, \u2018\u2018ConceptBeam: Concept Driven Target Speech Extraction,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 4252\u20134260. [109] K. Musgrave, S. Belongie, and S.-N. Lim, \u2018\u2018A Metric Learning Reality Check,\u2019\u2019 in Euorp. Conf. on Computer Vision (ECCV), 2020, pp. 681\u2013 699. [110] H. Rantzsch, H. Yang, and C. Meinel, \u2018\u2018Signature Embedding: Writer Independent Offline Signature Verification with Deep Metric Learning,\u2019\u2019 in Advances in Visual Computing (ISVC), Dec. 2016, pp. 616\u2013625. [111] A. Hermans, L. Beyer, and B. Leibe, \u2018\u2018In Defence of the Triplet Loss for Person Re-Identification,\u2019\u2019 in arXiv preprint arXiv:1703.07737, Mar. 2017. [112] K. Q. Weinberger, J. Blitzer, and L. K. Saul, \u2018\u2018Distance Metric Learning for Large Margin Nearest Neighbor Classification,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), Dec. 2005, pp. 1473\u20131480. [113] S. Liu, E. Johns, and A. J. Davison, \u2018\u2018End-to-End Multi-Task Learning with Attention,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019, pp. 1871\u20131880. [114] Z. Wang and T. Oates, \u2018\u2018Imaging Time-Series to Improve Classification and Imputation,\u2019\u2019 in Intl. Joint. Conf. on Artificial Intelligence (IJCAI), Buenos Aires, Argentinia, Jul. 2015, pp. 3939\u20133945. [115] M. Liwicki and H. Bunke, \u2018\u2018IAM-OnDB - an On-Line English Sentence Database Acquired from Handwritten Text on a Whiteboard,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Seoul, Korea, Aug. 2005, pp. 956\u2013961. [116] B. Shi, X. Bai, and C. Yao, \u2018\u2018An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition,\u2019\u2019 in Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 39(11), Nov. 2017, pp. 2298\u20132304. [117] A. Brock, J. Donahue, and K. Simonyan, \u2018\u2018Large Scale GAN Training for High Fidelity Natural Image Synthesis,\u2019\u2019 in Intl. Conf. on Learning Representations (ICLR), 2019. [118] J. H. Lim and J. C. Ye, \u2018\u2018Geometric GAN,\u2019\u2019 in arXiv preprint arXiv:1705.02894, May 2017. [119] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \u2018\u2018Going Deeper with Convolutions,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Boston, MA, Jun. 2015, pp. 1\u20139. [120] K. He, X. Zhang, S. Ren, and J. Sun, \u2018\u2018Deep Residual Learning for Image Recognition,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, Jun. 2016. [121] S. Avidan and A. Shamir, \u2018\u2018Seam Carving for Content-Aware Image Resizing,\u2019\u2019 in ACM Trans. on Graphics (SIGGRAPH), vol. 26(3), Jul. 2007, p. 10. [122] C. Wigington, S. Stewart, B. Davis, B. Barrett, B. Price, and S. Cohen, \u2018\u2018Data Augmentation for Recognition of Handwritten Words and Lines Using a CNN-LSTM Network,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Kyoto, Japan, Nov. 2017. [123] L. van der Maaten and G. Hinton, \u2018\u2018Visualizing Data using t-SNE,\u2019\u2019 in Journal of Machine Learning Research (JMLR), vol. 9(86), Nov. 2008, pp. 2579\u20132605. [124] H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weberf, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean, \u2018\u2018InceptionTime: Finding AlexNet for Time Series Classification,\u2019\u2019 inWIREs Data Mining and Knowledge Discovery, vol. 34(6), Sep. 2019, pp. 1936\u20131962. [125] A. Mattick, M. Mayr, M. Seuret, A. Maier, and V. Christlein, \u2018\u2018SmartPatch: Improving Handwritten Word Imitation with Patch Discrimina-\nVOLUME 11, 2023 23\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ntors,\u2019\u2019 in Proc. of the IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Lausanne, Switzerland, Sep. 2021, pp. 268\u2013283.\n[126] L. Kang, P. Riba, M. Rusinol, A. Forn\u00e9s, and M. Villegas, \u2018\u2018Content and Style Aware Generation of Text-line Images for Handwriting Recognition,\u2019\u2019 in IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), vol. 44(12), Oct. 2021, pp. 8846\u20138860. [127] C. Luo, Y. Zhu, L. Jin, Z. Li, and D. Peng, \u2018\u2018SLOGAN: Handwriting Style Synthesis for Arbitrary-Length and Out-of-Vocabulary Text,\u2019\u2019 in IEEE Trans. on Neural Networks and Learning Systems (TNNLS), Feb. 2022, pp. 1\u201313. [128] J. Gan, W.Wang, J. Leng, and X. Gao, \u2018\u2018HiGAN+: Handwriting Imitation GAN with Disentangled Representations,\u2019\u2019 in ACM Trans. on Graphics (TOG), Feb. 2023, pp. 1\u201317. [129] C. Chen, Z. Fu, Z. Chen, S. Jin, Z. Cheng, X. Jin, and X. sheng Hua, \u2018\u2018HoMM: Higher-Order Moment Matching for Unsupervised Domain Adaptation,\u2019\u2019 in Proc. of the AAAI Conf. on Artificial Intelligence (AAAI), vol. 34(4), Apr. 2020, pp. 3422\u20133429. [130] E. Grosicki and H. El-Abed, \u2018\u2018ICDAR 2011 - French Handwriting Recognition Competition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Beijing, China, Sep. 2011. [131] M. Kozielski, P. Doetsch, andH. Ney, \u2018\u2018Improvements in RWTH\u2019s System for Off-Line Handwriting Recognition,\u2019\u2019 in IAPR Intl. Conf. on Document Analysis and Recognition (ICDAR), Washington, DC, Aug. 2013. [132] P. Doetsch, M. Kozielski, and H. Ney, \u2018\u2018Fast and Robust Training of Recurrent Neural Networks for OfflineHandwriting Recognition,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Hersonissos, Greece, Sep. 2014. [133] F. Menasri, J. Louradour, A.-L. Bianne-Bernard, and C. Kermorvant, \u2018\u2018The A2iA French Handwriting Recognition System at the RIMESICDAR2011 Competition,\u2019\u2019 in Proc. of the Intl. Society for Optical Engineering (SPIE), vol. 8297(51), Jan. 2012. [134] P. Voigtlaender, P. Doetsch, S. Wiesler, R. Schl\u00fcter, and H. Ney, \u2018\u2018Sequence-Discriminative Training of Recurrent Neural Networks,\u2019\u2019 in Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Brisbane, QLD, Apr. 2015. [135] T. Bluche, \u2018\u2018Deep Neural Networks for Large Vocabulary Handwritten Text Recognition,\u2019\u2019 in Th\u00e8se de Doctorat, Universit\u00e9 Paris-Sud, May 2015. [136] C.Wigington, C. Tensmeyer, B. Davis,W. Barrett, B. Price, and S. Cohen, \u2018\u2018Start, Follow, Read: End-to-End Full-Page Handwriting Recognition,\u2019\u2019 in Europ. Conf. on Computer Vision (ECCV), vol. 11210, Oct. 2018, pp. 372\u2013388. [137] K. Dutta, P. Krishnan, M. Mathew, and C. V. Jawahar, \u2018\u2018Improving CNNRNN Hybrid Networks for Handwriting Recognition,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Niagara Falls, NY, 2018, pp. 80\u201385. [138] J. Chung and T. Delteil, \u2018\u2018A Computationally Efficient Pipeline Approach to Full PageOfflineHandwritten Text Recognition,\u2019\u2019 in Intl. Conf. onDocument Analysis and Recognition Workshops (ICDARW), Sydney, NSW, Sep. 2019. [139] M. Carbonell, J. Mas, M. Villegas, A. Forn\u00e9s, and J. Llad\u00f3s, \u2018\u2018End-toEnd Handwritten Text Detection and Transcription in Full Pages,\u2019\u2019 in Intl. Conf. on Document Analysis and RecognitionWorkshops (ICDARW), Sydney, NSW, Sep. 2019. [140] A. K. Bhunia, A. Das, A. K. Bhunia, P. S. R. Kishore, and P. P. Roy, \u2018\u2018Handwriting Recognition in Low-Resource Scripts Using Adversarial Learning,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019. [141] R.Messina and C. Kermorvant, \u2018\u2018Over-Generative Finite State Transducer N-Gram for Out-of-Vocabulary Word Recognition,\u2019\u2019 in IAPR Intl. Workshop on Document Analysis Systems, Tours, France, Apr. 2014. [142] G. Bastas, K. Kritsis, and V. Katsouros, \u2018\u2018Air-Writing Recognition using Deep Convolutional and Recurrent Neural Network Architectures,\u2019\u2019 in Intl. Conf. on Frontiers in Handwriting Recognition (ICFHR), Dortmund, Germany, Sep. 2020, pp. 7\u201312. [143] A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, M. A. Ranzato, and T. Mikolov, \u2018\u2018DeViSE: A Deep Visual-Semantic Embedding Model,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), 2013. [144] R. Kiros, R. Salakhutdinov, and R. S. Zemel, \u2018\u2018Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models,\u2019\u2019 in arXiv preprint arXiv:1411.2539, Nov. 2014. [145] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, \u2018\u2018From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Interference Over Event Descriptions,\u2019\u2019 in Trans. of the Association for\nComputational Linguistics (ACL), vol. 2, Cambridge, MA, 2014, pp. 67\u2013 78.\n[146] Y. Huang, Q. Wu, C. Song, and L. Wang, \u2018\u2018Learning Semantic Concepts and Order for Image and Sentence Matching,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, Jun. 2018. [147] J. Gu, J. Cai, S. Joty, L. Niu, and G. Wang, \u2018\u2018Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Salt Lake City, UT, Jun. 2018. [148] K.-H. Lee, X. Chen, G. Hua, H. Hu, and X. He, \u2018\u2018Stacked Cross Attention for Image-Text Matching,\u2019\u2019 in Europ. Conf. on Computer Vision (ECCV), vol. 11208, Oct. 2018. [149] K. Li, Y. Zhang, K. Li, Y. Li, and Y. Fu, \u2018\u2018Visual Semantic Reasoning for Image-Text Matching,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Seoul, Korea, Oct. 2019. [150] Y. Song and M. Soleymani, \u2018\u2018Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019. [151] J. Wehrmann, M. A. Lopes, D. Souza, and R. Barros, \u2018\u2018LanguageAgnostic Visual-Semantic Embeddings,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Seould, Korea, Oct. 2019. [152] H. Wu, J. Mao, Y. Zhang, Y. Jiang, L. Li, W. Sun, and W.-Y. Ma, \u2018\u2018Unified Visual-Semantic Embeddings: Bridging Vision and Language With Structured Meaning Representations,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019. [153] L. Zhen, P. Hu, X. Wang, and D. Peng, \u2018\u2018Deep Supervised Cross-Modal Retrieval,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, Jun. 2019, pp. 10 394\u201310 403. [154] Y. Peng and J. Qi, \u2018\u2018CM-GANs: Cross-Modal Generative Adversarial Networks for Common Representation Learning,\u2019\u2019 in ACM Trans. on Multimedia Computing, Communications, and Applications, vol. 15(1), Feb. 2019, pp. 1\u201324. [155] A. van den Oord, Y. Li, and O. Vinyals, \u2018\u2018Representation Learning with Contrastive Predictive Coding,\u2019\u2019 in arXiv preprint arXiv:1807.03748, Jan. 2019. [156] H. Wang, Y. Zhang, Z. Ji, Y. Pang, and L. Ma, \u2018\u2018Consensus-Aware VisualSemantic Embedding for Image-Text Matching,\u2019\u2019 in Euorp. Conf. on Computer Vision (ECCV), 2020. [157] T. Chen, J. Deng, and J. Luo, \u2018\u2018Adaptive Offline Quintuplet Loss for Image-Text Matching,\u2019\u2019 in Euorp. Conf. on Computer Vision (ECCV), 2020. [158] S. Chun, S. J. Oh, R. S. de Rezende, Y. Kalantidis, and D. Larlus, \u2018\u2018Probabilistic Embeddings for Cross-Modal Retrieval,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, Jun. 2021. [159] B. Chen, A. Rouditchenko, K. Duarte, H. Kuehne, S. Thomas, A. Boggust, R. Panda, B. Kingsbury, R. Feris, D. Harwath, J. Glass, M. Picheny, and S.-F. Chang, \u2018\u2018Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision (ICCV), Montreal, QC, Oct. 2021. [160] H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui, and B. Gong, \u2018\u2018VATT: Transformers forMultimodal Self-Supervised Learning from Raw Video, Audio and Text,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), May 2021. [161] L. Wang, P. Luc, A. Recasens, J.-B. Alayrac, and A. van den Oord, \u2018\u2018Multimodal Self-Supervised Learning of General Audio Representations,\u2019\u2019 in arXiv preprint arXiv:2104.12807, Apr. 2021. [162] A. Guzhov, F. Raue, J. Hees, andA. Dengel, \u2018\u2018AudioCLIP: Extending Clip to Image, Text and Audio,\u2019\u2019 in IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Singapore, Singapore, May 2022. [163] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, andM. Auli, \u2018\u2018data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language,\u2019\u2019 in Intl. Conf. on Machine Learning (ICML), vol. 162, 2022, pp. 1298\u20131312. [164] A. Piergiovanni, A. Angelova, and M. S. Ryoo, \u2018\u2018Evolving Losses for Unsupervised Video Representation Learning,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, Jun. 2022. [165] H. Lin, Z. Ma, X. Hong, Y. Wang, and Z. Su, \u2018\u2018Semi-supervised Crowd Counting via Density Agency,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 1416\u20131426."
        },
        {
            "heading": "24 VOLUME 11, 2023",
            "text": "This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[166] A. Singh, R. Hu, V. Goswami, G. Couairon, W. Galuba, M. Rohrbach, and D. Kiela, \u2018\u2018FLAVA: A Foundational Language and Vision Alignment Model,\u2019\u2019 in IEEE/CVF Intl. Conf. on Computer Vision and Pattern Recognition (CVPR), New Orleans, LA, Jun. 2022, pp. 15 638\u201315 650. [167] A. Falcon, G. Serra, and O. Lanz, \u2018\u2018A Feature-space Multimodal Data Augmentation Technique for Text-Video Retrieval,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 4385\u20134394. [168] D. Chen, M. Wang, H. Chen, L. Wu, J. Qin, and W. Peng, \u2018\u2018Cross-Modal Retrieval with Heterogeneous Graph Embedding,\u2019\u2019 in Proc. of the ACM Intl. Conf. on Multimedia (ACMMM), Oct. 2022, pp. 3291\u20133300. [169] S. Ioffe, \u2018\u2018Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models,\u2019\u2019 in Advances in Neural Information Processing Systems (NIPS), 2017, pp. 1942\u20131950.\nACKNOWLEDGMENT This work was supported by the Federal Ministry of Education and Research (BMBF) of Germany by Grant No. 01IS18036A (David R\u00fcgamer) and by the research program Human-Computer-Interaction through the project \u2018\u2018Schreibtrainer\u2019\u2019, Grant No. 16SV8228, as well as by the Bavarian Ministry for Economic Affairs, Infrastructure, Transport and Technology through the Center for Analytics-DataApplications (ADA-Center) within the framework of \u2018\u2018BAYERN DIGITAL II\u2019\u2019.\nFELIX OTT (M\u201994) received his M.Sc. degree in Computational Engineering at the FriedrichAlexander-Universit\u00e4t (FAU) Erlangen-N\u00fcrnberg, Germany, in 2019. He joined the Hybrid Positioning & Information Fusion group in the Locating and Communication Systems department at Fraunhofer IIS. In 2020, he started his Ph.D. at the Ludwig-Maximilians-Universit\u00e4t (LMU) in Munich in the Probabilistic Machine and Deep Learning group. His research covers multimodal\ninformation fusion for self-localization.\nDAVID R\u00dcGAMER is an interim professor for Computational Statistics at the TU Dortmund. Before he was an interim professor at the RWTH Aachen and interim professor for Data Science at the LMUMunich, where he also received his Ph.D. in 2018. His research is concerned with scalability of statistical modeling as well as machine learning for functional and multimodal data.\nLUCAS HEUBLEIN received his M.Sc. degree in Integrated Life Science at the FAU ErlangenN\u00fcrnberg. In 2020, he started his Computer Science degree at the FAU. He joined the Hybrid Positioning & Information Fusion group at the Fraunhofer IIS in 2020 as a student assistant.\nBERND BISCHL is a full professor for statistical learning and data science at the LMU Munich and a director of the Munich Center of Machine Learning. His research focuses amongst other things on AutoML, interpretable machine learning and machine learning benchmarking.\nCHRISTOPHER MUTSCHLER leads the precise positioning and analytics department at Fraunhofer IIS. Prior to that, Christopher headed the Machine Learning & Information Fusion group. He gives lectures on machine learning at the FAU ErlangenN\u00fcrnberg, from which he also received both his Diploma and Ph.D. in 2010 and 2014 respectively. Christopher\u2019s research combines machine learning with radio-based localization.\nVOLUME 11, 2023 25\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
        }
    ],
    "title": "Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition",
    "year": 2023
}