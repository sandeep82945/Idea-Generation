{
    "abstractText": "We present a non-convex optimization algorithm metaheuristic, based on the training of a deep generative network, which enables effective searching within continuous, ultra-high dimensional landscapes. During network training, populations of sampled local gradients are utilized within a customized loss function to evolve the network output distribution function towards one peaked at high performing optima. The deep network architecture is tailored to support progressive growth over the course of training, which allows the algorithm to manage the curse of dimensionality characteristic of high dimensional landscapes. We apply our concept to a range of standard optimization problems with dimensions as high as one thousand and show that our method performs better with fewer functional evaluations compared to state-of-the-art algorithm benchmarks. We also discuss the role of deep network over-parameterization, loss function engineering, and proper network architecture selection in optimization, and why the required batch size of sampled local gradients is independent of problem dimension. These concepts form the foundation for a new class of algorithms that utilize customizable and expressive deep generative networks to solve nonconvex optimization problems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiaqi Jiang"
        },
        {
            "affiliations": [],
            "name": "Jonathan A. Fan"
        }
    ],
    "id": "SP:1f706b070f737269f4f3051f49886d672c870711",
    "references": [
        {
            "authors": [
                "S. Arora",
                "N. Cohen",
                "E. Hazan"
            ],
            "title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
            "venue": "International Conference on Machine Learning, 244\u2013253. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "E. Balsa-Canto",
                "J.R. Banga",
                "J.A. Egea",
                "A. FernandezVillaverde",
                "G. de Hijas-Liste"
            ],
            "title": "Global optimization in systems biology: stochastic methods and their applications",
            "venue": "In Advances in Systems Biology,",
            "year": 2012
        },
        {
            "authors": [
                "Z. Chen",
                "W. Jia",
                "X. Jiang",
                "S.-S. Li",
                "L.-W. Wang"
            ],
            "title": "SGO: A fast engine for ab initio atomic structure global optimization by differential evolution",
            "venue": "Computer Physics Communications, 219: 35\u201344.",
            "year": 2017
        },
        {
            "authors": [
                "A. Choromanska",
                "M. Henaff",
                "M. Mathieu",
                "G.B. Arous",
                "Y. LeCun"
            ],
            "title": "The loss surfaces of multilayer networks",
            "venue": "Artificial intelligence and statistics, 192\u2013204. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "F. Draxler",
                "K. Veschgini",
                "M. Salmhofer",
                "F. Hamprecht"
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "International conference on machine learning, 1309\u20131318. PMLR.",
            "year": 2018
        },
        {
            "authors": [
                "S. Held",
                "B. Korte",
                "D. Rautenbach",
                "J. Vygen"
            ],
            "title": "Combinatorial optimization in VLSI design",
            "venue": "Combinatorial Optimization, 33\u201396.",
            "year": 2011
        },
        {
            "authors": [
                "J. Jiang",
                "J.A. Fan"
            ],
            "title": "Global optimization of dielectric metasurfaces using a physics-driven neural network",
            "venue": "Nano letters, 19(8): 5366\u20135372.",
            "year": 2019
        },
        {
            "authors": [
                "J. Jiang",
                "J.A. Fan"
            ],
            "title": "Simulator-based training of generative neural networks for the inverse design of metasurfaces",
            "venue": "Nanophotonics, 9(5): 1059\u20131069.",
            "year": 2020
        },
        {
            "authors": [
                "M.S. J\u00f8rgensen",
                "U.F. Larsen",
                "K.W. Jacobsen",
                "B. Hammer"
            ],
            "title": "Exploration versus exploitation in global atomistic structure optimization",
            "venue": "The Journal of Physical Chemistry A, 122(5): 1504\u20131509.",
            "year": 2018
        },
        {
            "authors": [
                "T. Karras",
                "T. Aila",
                "S. Laine",
                "J. Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "arXiv preprint arXiv:1710.10196.",
            "year": 2017
        },
        {
            "authors": [
                "L. Li",
                "W. Fang",
                "Q. Wang",
                "J. Sun"
            ],
            "title": "Differential grouping with spectral clustering for large scale global optimization",
            "venue": "2019 IEEE Congress on Evolutionary Computation (CEC), 334\u2013341. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "X. Li",
                "K. Tang",
                "M.N. Omidvar",
                "Z. Yang",
                "K. Qin",
                "H. China"
            ],
            "title": "Benchmark functions for the CEC 2013 special session and competition on large-scale global optimization",
            "venue": "gene, 7(33): 8.",
            "year": 2013
        },
        {
            "authors": [
                "S. Mohamed",
                "M. Rosca",
                "M. Figurnov",
                "A. Mnih"
            ],
            "title": "Monte Carlo Gradient Estimation in Machine Learning",
            "venue": "J. Mach. Learn. Res., 21(132): 1\u201362.",
            "year": 2020
        },
        {
            "authors": [
                "M. Pelikan",
                "D.E. Goldberg",
                "E Cant\u00fa-Paz"
            ],
            "title": "BOA: The Bayesian optimization algorithm",
            "venue": "In Proceedings of the genetic and evolutionary computation conference GECCO-99,",
            "year": 1999
        },
        {
            "authors": [
                "D. Sell",
                "J. Yang",
                "S. Doshay",
                "R. Yang",
                "J.A. Fan"
            ],
            "title": "Large-angle, multifunctional metagratings based on freeform multimode geometries",
            "venue": "Nano letters, 17(6): 3752\u2013 3757.",
            "year": 2017
        },
        {
            "authors": [
                "S. Sieniutycz",
                "J. Jezowski"
            ],
            "title": "Energy optimization in process systems",
            "venue": "Elsevier.",
            "year": 2009
        },
        {
            "authors": [
                "J. Snoek",
                "O. Rippel",
                "K. Swersky",
                "R. Kiros",
                "N. Satish",
                "N. Sundaram",
                "M. Patwary",
                "M. Prabhat",
                "R. Adams"
            ],
            "title": "Scalable bayesian optimization using deep neural networks",
            "venue": "International conference on machine learning, 2171\u20132180. PMLR.",
            "year": 2015
        },
        {
            "authors": [
                "Y. Sun",
                "X. Li",
                "A. Ernst",
                "M.N. Omidvar"
            ],
            "title": "Decomposition for large-scale optimization problems with overlapping components",
            "venue": "2019 IEEE congress on evolutionary computation (CEC), 326\u2013333. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "M. Xu",
                "M. Quiroz",
                "R. Kohn",
                "S.A. Sisson"
            ],
            "title": "Variance reduction properties of the reparameterization trick",
            "venue": "The 22nd International Conference on Artificial Intelligence and Statistics, 2711\u20132720. PMLR.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "High dimensional, non-convex optimization problems are pervasive in many scientific and engineering domains, including computational materials science (Chen et al. 2017; J\u00f8rgensen et al. 2018), electromagnetics (Piggott et al. 2015; Sell et al. 2017), circuits design (Held et al. 2011), process engineering (Sieniutycz and Jezowski 2009), and systems biology (Balsa-Canto et al. 2012). These problems are known to be very difficult to solve because they are NP-hard, and algorithms aiming to definitively search for the global optimum, such as branch and bound methods, cannot practically scale to high dimensional systems. As such, various algorithm heuristics have been developed, ranging from evolutionary metaheuristics to Bayesian optimization (Pelikan et al. 1999; Snoek et al. 2015), which use judicious sampling of the landscape to identify high performing optima. In all cases, it remains challenging to apply these algorithms to ultra-high dimensional spaces with dimensions of hundreds to thousands due to the curse of dimensionality.\nThe explosion of interest and research in deep neural networks over the last decade has presented new opportunities\nin optimization, as the process of training a deep network involves solving a high dimensional optimization problem. To this end, gradient-based optimization metaheuristics termed global topology optimization networks (GLOnets) (Jiang and Fan 2020, 2019) were recently proposed that use the training of a deep generative network to perform non-convex optimization. The concept applies to optimization problems where x is a d-dimensional variable and the goal is to maximize the smoothly varying, non-convex objective function f(x). To run the metaheuristic, the generative network is first initialized so that it outputs a distribution of x values that spans the full optimization landscape. Over the course of network training, this distribution is sampled, f(x) and local gradients are computed for these sampled points, and these values are incorporated into a customized loss function and backpropagated to evolve and narrow the distribution around high performing optima. Initial demonstrations indicate that GLOnets can perform better than standard gradient-based optimizers and global search heuristics for various non-convex optimization problems. However it is unable to extend to high dimensional problems in its current form, and the lack of interpretability with this black box algorithm has made it difficult to understand if and how it can to adapt to more general problems, including high dimensional problems.\nIn this Article, we introduce the progressive growing GLOnet (PG-GLOnet) in which optimization within an ultra-high dimensional non-convex landscape is mediated through the training of a progressive growing deep generative network. Our tailoring of the network architecture for this optimization task serves to incorporate knowledge and assumptions about the optimization landscape into the metaheuristic, which is a requirement for tractably navigating ultra-high dimensional landscapes. We also explain how the algorithm works to smoothen the design landscape, how evaluation of the loss function serves as a gradient estimation calculation, and why the number of required functional evaluations is independent of problem dimension. With standard benchmarking test functions, we show that our concept performs better than state-of-the-art algorithms with fewer functional evaluations for one thousand dimensional problems. We anticipate that the customization of network architectures within the GLOnets framework will seed new connections between deep learning and optimization. ar X iv :2\n30 7.\n04 06\n5v 1\n[ cs\n.L G\n] 9\nJ ul\n2 02\n3"
        },
        {
            "heading": "Progressive Growing GLOnets Algorithm and Benchmarking",
            "text": "The PG-GLOnet concept builds on the foundation of the original GLOnet algorithm, which we briefly review here. The optimization problem to be solved with GLOnets can be written in the following form:\nmax x f(x) (1)\nwhere f(x) is a non-convex, continuous objective function with feasible gradients. With GLOnets, this optimization problem is indirectly solved through the training of a general neural network (Figure 1a), where the input is a ddimensional random variable z with a standard normal distribution and the output is a distribution of x\u2019s. The generator therefore serves to map z onto x = G(z;\u03d5) with a distribution P (x;\u03d5), where \u03d5 denotes the trainable neural network parameters. The optimization objective for the generator is defined as:\nL = max \u03d5 E x\u223cP (x;\u03d5) exp\n[ f(x)\nT\n] (2)\nThe distribution that maximizes this expected value is a delta function centered at the global optimum, and as such, an ideally trained generator will produce a narrow distribution centered at the global optimum, thereby solving the original optimization problem. The use of the exponential function and the hyperparameter T in the optimization objective further enhance the valuation of the global optimum, and more generally high performing optima, in the design space.\nGenerator training is consistent with conventional deep learning training methods: gradients of the objective function with respect to network parameters, \u2207\u03d5Ef , are calculated through backpropagation, and they are used to iteratively optimize \u03d5 using standard gradient-based methods. In practice, the objective function is approximated by a batch of M samples. P (x;\u03d5), on the other hand, is typically implicit and cannot be directly sampled. To circumvent this issue, we draw M samples {z(m)}Mm=1 from the standard normal distribution, transform them to {x(m)}Mm=1, and then approximate L and its gradient \u2207\u03d5L with respect to network parameters \u03d5:\nL \u2248 1 M M\u2211 m=1 exp [ f(x(m)) T ] (3)\n\u2207\u03d5L \u2248 1\nM M\u2211 m=1 1 T exp [ f(x(m)) T ] \u2207xf \u00b7D\u03d5x(m) (4)\n\u2207xf = [ \u2202f\u2202x1 , \u2202f \u2202x2 , . . . , \u2202f\u2202xd ] are the gradients of f(x) and D\u03d5x = \u2202(x1,x2,... ) \u2202(\u03d51,\u03d52,...)\nis the Jacobian matrix. Evaluation of f(x) is usually performed by a numerical simulator and the gradient of f(x) can be calculated explicitly or by autodifferentiation for analytic expressions, or by the adjoint variables method (AVM).\nIn the initial conception of GLOnet, which we term FCGLOnet, the generative network was a fully connected deep\nnetwork and was capable of effectively addressing optimization problems with a modest number of dimensions. However, it was found to be ineffective at optimizing within very high dimensional landscapes due to the curse of dimensionality, which makes a direct search for the global optimum within a full, high dimensional landscape an intractable proposition. We therefore propose the PG-GLOnet, which utilizes a generative network that outputs a distribution that gradually grows from a coarse, low dimensional space to a fine, high dimensional space. By tailoring the network architecture in this way, we regularize the optimization process to take place over differing degrees of optimization landscape smoothing, enabling our search process to be computationally efficient and tractable.\nThe PG-GLOnet generator architecture is shown in Figure 1b. The progressive growth concept is inspired by progressively growing GANs (Karras et al. 2017) that have been developed in the computer vision community to process images with increasing spatial resolution during network training. The input to the network is a D-dimensional random vector x0, and its dimension is much smaller than that of x. With L growing blocks, the network simultaneously transforms and increases the dimensionality of the input vector, and its output is a 2LD dimensional vector xL that matches the dimensionality of x.\nIn each growing block, the input vector dimension is doubled in two ways, by direct upsampling and by a linear transform. The resulting outputs are combined together and further transformed using a non-linear activation function:\nxout2d\u00d71 = q\n( (1\u2212 \u03b1) ( xind\u00d71 xind\u00d71 ) + \u03b1 A2d\u00d7d \u00b7 xind\u00d71 ) (5)\nA2d\u00d7d are trainable parameters in the linear transformation branch, q(\u00b7) is a non-linear activation function, and \u03b1 is a hyperparameter that is manually tuned over the course of optimization.\nInitially, \u03b1\u2019s for all of the growing blocks in the network are set to 0, such that the vector outputted by each block has the same effective dimensionality as its input vector. The network output xL therefore has an effective dimensionality that matches the dimensionality of the input x0. As \u03b1 is increased for a particular growing block, its output vector becomes dominated by its linear transformation branch, as opposed to its upsampling branch, and it has an effective dimensionality that exceeds and eventually doubles that of the growing block input vector. The effective dimensionality of xL therefore arises from the aggregation of effective dimensionality increases from all growing blocks. To control the effective dimensionality of xL over the course of PG-GLOnet training, \u03b1 is manually changed from 0 to 1 sequentially from the left to right blocks (bottom of Figure 1b). At the end of PG-GLOnet training, \u03b1 is 1 for all growing blocks and the effective dimensionality of xL matches that of x.\nTo evaluate the efficacy of PG-GLOnet in solving high dimensional non-convex optimization problems, we perform a series of benchmark numerical experiments where we optimize a set of standard test functions with PG-GLOnet and other established algorithms. In the first set of experiments,\nwe consider a testing function that can be tuned from a convex to non-convex function and compare PG-GLOnet with ADAM, a well known momentum-based gradient descent algorithm that is typically more effective than gradient descent. ADAM is a local optimization algorithm and performs well on convex objective functions but can get trapped within local optima for non-convex functions. Our test function is a modified Rastrigin function defined as follows:\nf(x; \u03c1) = \u03c1d+ d\u2211 i=1 [x2i \u2212 \u03c1 cos(2\u03c0xi)] (6)\n\u03c1 is a hyperparameter that specifies the amplitude of the sinusoidal modulation within the function. When \u03c1 = 0, f(x; \u03c1) = \u2211d i=1 x 2 i and is a convex function. As \u03c1 increases, more local optima emerge and these optima become separated by larger magnitude barriers.\nWe first consider the computational cost required by ADAM and PG-GLOnet to find the global optimum of a two dimensional modified Rastrigin function as a function of \u03c1. For ADAM, we run 10000 optimizations for 200 iterations with random starting points, and for PG-GLOnet, we run the algorithm 10 times with a batch size of 20 for 200 total iterations. In both cases, the algorithms terminate early when they output results within 10\u22123 of the global optimum, and computational cost is quantified as the average number of function evaluations required to find the global optimum. The results are summarized in Figure 2a and indicate that for convex or nearly convex optimization landscapes, ADAM is more efficient at finding the global opti-\nmum. This efficiency arises because ADAM is a specially tailored local optimizer that is well suited for these types of problems, while PG-GLOnet always requires relatively large batch sizes and more iterations to converge. As \u03c1 increases, orders-of-magnitude more ADAM evaluations are required to search for the global optimum due to trapping within local optima in the design landscape. The computational cost for PG-GLOnet, on the other hand, does not increase nearly as rapidly due to its ability to navigate non-convex landscapes and is ten times more efficient than ADAM for \u03c1 greater than 3.\nWe also perform benchmarks between ADAM and PGGLOnet for a ten dimensional problem. Due to the inability for ADAM to converge to the global optimum in non-convex, high dimensional landscapes, we perform this benchmark differently and compare the best optimal value found by ADAM and PG-GLOnet given the same amount of computational resources. Here, we run ADAM for 200 iterations with 20 random starting points and PG-GLOnet for 200 iterations with a batch size of 20. We run these benchmark experiments ten times and average the best values from each experiment, and the results are reported in Figure 2b. We find that the PG-GLOnet is able to consistently find solutions at or near the global optimum for all values of \u03c1, but the local optimizer gets progressively worse as \u03c1 increases.\nIn our next set of benchmark experiments, we compare PG-GLOnet with the covariance matrix adaptation evolution strategy (CMA-ES), which is an established evolutionary algorithm used to perform population-based global searching of an optimization landscape. Compared to ADAM, it is\nmore suitable for performing non-convex optimization. We consider two standard non-convex testing functions with lots of local optima, the Rastrigin and Schwefel functions (defined in the Appendix).\nPlots in Figures 2c and 2d show the average number of function evaluations required to find the global optimum as a function of problem dimension d. The computational cost of CMA-ES increases exponentially as the problem dimension becomes larger, indicating the intractability of applying this algorithm to ultra-high dimensional problems. For the Schwefel function, we limited our CMA-ES benchmarking experiments to a problem dimension of 20 due to this scaling trend. PG-GLOnet, on the other hand, has a relatively small computational cost that is not sensitive to the dimension. In fact, the same neural network architecture and batch size is used for all problems. A more detailed discussion as to the origins of problem dimension and batch size decoupling is provided in the Discussion section.\nFinally, we benchmark PG-GLOnet with state-of-art algorithms on testing functions proposed by the CEC\u20192013 Special Session and Competition on Large-Scale Global Optimization (LSGO) (Li et al. 2013). We consider the six nonconvex benchmark functions from the competition, which involve variations and combinations of the Rastrigin and Ackely functions and are defined in the Appendix. These benchmark functions were designed to incorporate a number of challenging features for optimization, including:\n1. High dimensions. The design space of a optimization problem grows exponentially as the dimension of design variables increases. These benchmark functions utilize one thousand dimensional landscapes.\n2. Functions with non-separable subcomponents. The whole design variable is decomposed into several subcomponents and dimensions within each subcomponent are strongly coupled together.\n3. Imbalance in the contribution of subcomponents. The contribution of a subcomponent is magnified or dampened by a coefficient.\n4. Non-linear transformations to the base functions.\nThree transformations are applied to break the symmetry and introduce some irregularity on the landscape: (1) Ill-conditioning (2) Irregularities (3) Symmetry breaking.\nTo globally search these landscapes for the global optimum, we perform a two step optimization procedure. First, we run PG-GLOnet for each benchmark function for 200 iterations and a batch size of 100, from which our generative network outputs a narrow distribution of x\u2019s in promising regions of the optimization landscape. We then sample this distribution 100 times and perform local gradient descent on each of these design variables for an additional 200 iterations. The best function values found by PG-GLOnet plus local gradient descent are reported in Table 1, together with results produced from FC-GLOnet plus local gradient descent, local conjugate gradient descent, and two state-of-art non-convex optimization algorithms that were the best performing algorithms in the most recent LSGO contest: CCRDG3, which is a divide-and-conquer method (Sun et al. 2019), and DGSC, which is a differential group method utilizing spectral clustering (Li et al. 2019). We observe that PG-GLOnet with local gradient descent refinement is able to significantly outperform the other algorithms for the majority of test functions. In addition, the total computational cost of the two step optimization procedure is only 4\u00d7 104 function evaluations, while CC-RDG3 and DGSC require 3\u00d7106 function evaluations."
        },
        {
            "heading": "Discussion",
            "text": "We discuss the origins of the efficiency and efficacy of PG-GLOnet in solving ultra-high dimensional non-convex optimization problems. First, we examine how the generic GLOnet algorithm operates and why it is able to effectively utilize a gradient-based strategy to solve non-convex optimization problems. Second, we examine the role of the progressive growing generative network architecture in PGGLOnet in solving ultra-high dimensional problems. By understanding the relationship between network architecture and optimization procedure, we elucidate built-in assumptions used by PG-GLOnet in its search for the global opti-\nmum. With the generic GLOnet algorithm, the original optimization problem cited in Equation 1 is reframed as a related problem (Equation 2) that addresses a transformed, smoothened optimization landscape. The key concepts that produce this landscape transformation and enable effective gradient-based optimization are outlined in Figure 3a and are: 1) distribution optimization, where the original problem involving the optimization of x is transformed to a problem involving the optimization of parameters within a simple distribution P (x); 2) exponential transformation, where the objective function is exponentially weighted; 3) overparametrization, where the distribution P (x) is now parameterized by a neural network with hundreds to thousands of weights; and 4) gradient estimation, where gradients that specify the evolution of the continuous distribution P (x) are accurately computed through discrete samplings of z.\nDistribution optimization. With the concept of distribution optimization, the original problem of searching for an optimal x is recast as a population-based search in which parameters within a distribution function are optimized, thereby enabling a search for the global optimum in a smoother and higher dimensional optimization landscape. This concept is shared by other population-based optimization algorithms, such as CMA-ES. To visualize the concept, we consider a non-convex one-dimensional function f(x) plotted as a blue line in the leftmost figure in Figure 3a. The objective is to maximize f(x), and the function contains multiple local maxima separated by deep valleys. It is easy for optimization algorithms, particularly gradient-based algorithms, to get trapped in the local optima. For example, if gradient descent optimization is used and is initialized at the yellow dot position, the algorithm will converge to the local optimum delineated by the red dot. With this approach, multiple independent gradient descent optimizations with random starting points are needed to increase the possibility of finding the global optimum. For these problems, gradientfree optimization heuristics are often employed, which can reduce the chances of trapping within suboptimal maxima but which introduce a more stochastic nature to the search process.\nHowever, if we consider the optimization of a distribution function that interacts with the global optimization landscape, local information at different parts of the landscape can be aggregated and collectively utilized to evolve this distribution in a manner that reduces issues of trapping within\nsuboptimal maxima. Formally, we transform the optimization variable x to parameters within the distribution P (x), and the globally optimal distribution is one that is narrowly peaked around the global optimum. Distribution functions can be explicitly parameterized in many ways. As a simple illustrative example that builds on our discussion of the onedimensional f(x), we consider the one-dimensional Gaussian distribution denoted as P (x;\u00b5, \u03c3), shown as the red curve in the leftmost figure in Figure 3a. \u00b5 and \u03c3 refer to mean and standard deviation, respectively.\nWith a Gaussian distribution function, the objective function now becomes transformed to the expected value of f(x) as a function of (\u00b5, \u03c3): Ex\u223cP (x;\u00b5,\u03c3) f(x). As this new optimization landscape is a function of two distribution parameters, \u00b5 and \u03c3, it is two dimensional. We can directly visualize this new landscape by evaluating \u222b f(x)P (x;\u00b5, \u03c3)dx for all values of (\u00b5, \u03c3), and the result is summarized in the second figure from the left in Figure 3a. The horizontal line section at the bottom of the contour plot, where \u03c3 equals zero, is the original one-dimensional f(x) with multiple optima. As \u03c3 increases to finite values above zero, the landscape becomes smoother. Mathematically, horizontal line sections for finite sigma are calculated by convolving f(x) with the Gaussian function, producing a Gaussian blur that leads to smoothening. This smoothened landscape facilitates gradient-based optimization of (\u00b5, \u03c3) when the distribution is initialized to large \u03c3 values, and the final optimized distributions converge to the original f(x) space at the bottom of the plot. However, while this two-dimensional landscape is smoother than the original f(x), there remain multiple distribution parameter initializations for which the gradient-based optimizer converges to suboptimal maxima.\nExponential transformation. To further smoothen the optimization landscape and enhance the presence of the global optimum, we perform an exponential transformation of the objective function. Mathematically, the objective function for the distribution optimization problem becomes:\nEx\u223cP (x;\u00b5,\u03c3) exp [ f(x) T ] . The temperature term T modulates the impact of the global optimum on the optimization landscape such that low T produces strong landscape modulation by the global optimum. For our one-dimensional f(x) example, the exponentially transformed landscape is plotted in the second figure from the left in Figure 3a and shows that the local optima has faded out, such that gradient-based optimization within this landscape is more likely to converge\nto the global optimum. The choice of T depends on the scale of f(x). Consider f(x) that is linearly normalized to span (0, 1). Such normalization can be typically achieved based on prior knowledge about the upper and lower bound of f(x). If we want to amplify f(x) for f(x) > fd and minimize f(x) for f(x) < fd, where fd is a division point between 0 and 1, the temperature is chosen to be T = fd/ log(1 + fd). For example, if fd is chosen to be the golden ratio, then the temperature is roughly T = 1.3. In practice, the selection of fd is problem specific, and T can be treated as a hyperparameter that can be manually tuned around 1 for tailoring to a particular problem.\nOver-parameterization. To further enhance the ability for GLOnet to efficiently and reliably converge to the global optimum, we next consider the concept of overparameterization in which the distribution P (x) is now a neural network parameterized by weights \u03d5. The objective function then becomes: Ex\u223cP (x;\u03d5) exp [ f(x) T ] . Our use of a neural network is inspired by the fact that deep network training involves the solving of an extremely high dimensional non-convex optimization problem, that the convergence of the neural network is typically insensitive to initialization, and that good neural network parameters can be found using backpropagation.\nThe underlying mathematical principles outlining why\ngradient descent is so effective for deep network training have been revealed to some extent by computer scientists in recent years. (Choromanska et al. 2015; Arora, Cohen, and Hazan 2018; Draxler et al. 2018) First, the parameter space of deep networks is a high-dimensional manifold, such that most local optima are equivalently good and the probability of converging to a bad optimum during training decreases quickly with network size. Second, these equivalently high performing local optima originate from neural network over-parameterization, which builds in redundancy in the optimization landscape that speeds up and stabilizes the gradient-based optimization process.\nTo understand how this applies to GLOnet, we revisit our one-dimensional f(x) landscape in which local optima are separated by deep barriers. When the optimization landscape is transformed using P (x, \u03d5), it frames the optimization problem in a very high dimensional landscape, as the dimensionality of \u03d5 is much higher than x. Solutions to the optimization problem therefore reside in a highdimensional manifold, such that many different \u03d5\u2019s serve as high performing local optima. Additionally, local optima in f(x) are no longer separated by deep barriers but are instead connected by pathways with low to no barriers in our transformed high dimensional landscape, mitigating trapping within these local optima during gradient-based optimization. The high dimensional landscape representing the transformed f(x) is visualized as a two-dimensional pro-\njection in the rightmost plot in Figure 3a. The global optimum is now a connected band in the optimization landscape, as opposed to a single point in f(x), and there are fewer energy barriers preventing gradients from converging to the global optimum, enabling gradient descent optimization to be more robust and faster. We note that neural network depth and expressivity play a large role in determining the practical impact of over-parameterization on optimization, and as a demonstration, we compare the performance of GLOnets based on linear and deep non-linear networks in the Appendix.\nGradient estimation. A critical feature to maximizing the performance of GLOnet is ensuring that gradients used to evolve P (x), which are approximated using a finite batch of samples, are sufficiently accurate. There are two methods for gradient estimation that can be used for GLOnets. The first is to use a score function gradient estimator, which utilizes the evaluated derivatives of the probability distribution P (x;\u03d5) and f(x). This method for estimation requires explicit evaluation of derivatives to P (x;\u03d5) but only an implicit evaluation of \u2207xf . The second is to use a pathwise gradient estimator, which relies on knowing the explicit derivatives of f(x) but for which the probability distribution P (x;\u03d5) can be implicit. Empirically, we find for GLOnet that the pathwise gradient estimator more consistently produces smaller gradient error compared with the score function gradient estimator, and we therefore implement the pathwise gradient estimator in Equation 4. (Xu et al. 2019; Mohamed et al. 2020)\nThe pathwise gradient estimator is based on the principle of Monte Carlo estimation, such that the estimation error decreases with the inverse square root of batch size. Importantly, this estimation error is independent of dimension. As a result, GLOnet and specifically PG-GLOnet are able to operate for batch sizes that are independent of problem dimension, as demonstrated in Figures 2c and 2d. This scaling of problem dimension without a required scaling in the number of functional evaluations allows PG-GLOnet to readily scale and address the 1000-dimensional problems in Table 1 with modest computational resources.\nProgressive growth. Direct searching within a high dimensional, non-convex landscape is an intractable problem. In the case of FC-GLOnet, which utilizes all of the features above, including distribution optimization and overparameterization, the algorithm is still not effective in directly searching high dimensional landscapes (Table 1). With PG-GLOnet, the progressive growing architecture regularizes the optimization procedure to search first within a relatively coarse, low dimensional representation of the optimization landscape, followed by relatively local searching within increasingly higher dimensional landscape representations. This hierarchical increase of landscape dimensionality directly corresponds to the serial toggling of \u03b1 within the series of growing blocks in the generator. As such, the optimization landscape is evolved over the course of PGGLOnet training in a manner that maintains the tractability of the optimization problem.\nTo further visualize the relationship between generative network architecture and optimization search procedure, we\nconsider a non-convex two-dimensional landscape shown in Figure 3b. The generative network contains a single growing block, and the toggling of \u03b1 from zero to one modulates the effective dimensionality of the generator output from one to two. Initially, \u03b1 is zero and the vector outputted by the generator has the same effective dimensionality as its input vector and is one. The optimization landscape being searched is therefore a diagonal line within the two-dimensional landscape (Figure 3b, left-most plot), and with optimal solutions near the center of the line, the outputted generator distribution (red coloring in plot) narrows towards this region. As \u03b1 is increased, the generator output vector becomes dominated by its linear transformation branch, as opposed to its upsampling branch, and it has an effective dimensionality that increases and eventually doubles. In our PG-GLOnet visualization, this increase in effective dimensionality corresponds to a broadening of the optimization landscape being searched, and the outputted generator distribution widens relative to the diagonal line. Upon the completion of network growth, the PG-GLOnet distribution converges to the global optimum.\nThe success of PG-GLOnet is therefore predicated on the ability for the outputted distribution of the generative network to be narrowed down to smaller but more promising regions of a coarse optimization landscape, prior to increasing the landscape dimensionality and adding more degrees of freedom to the problem. This concept therefore works particularly well for problems where optima within a low dimensional analogue of the optimization landscape help to inform of the presence and position of optima within the high dimensional landscape. This regularization of the optimization procedure also indicates that for problems where optima within coarse variants of the optimization landscape do not inform the position of the global optimum, PG-GLOnet will not work well.\nIn summary, we present a general global optimization algorithm metaheuristic based on progressive growing deep generative neural networks termed PG-GLOnet. Unlike other population-based algorithms, PG-GLOnet uses gradient-based optimization to evolve an expressive, complex distribution in the optimization landscape to one centered around promising optima. This complex distribution, parameterized using the deep network framework, utilizes loss function engineering and over-parameterization to facilitate effective gradient-based searching. PG-GLOnet is particularly well suited to address ultra-high dimensional problems because the required batch size is independent of problem dimension and the progressively growing network architecture facilitates a hierarchical search process within a landscape with progressively growing effective dimensionality. This use of a hierarchical search strategy also provides bounds as to the types of problems and landscapes that are suited for PG-GLOnet optimization. We anticipate that further research in the tailoring of application-specific generative network architectures to particular optimization landscapes will enable the GLOnet platform to extend and adapt to an even wider range of non-convex, high dimensional optimization problems."
        }
    ],
    "title": "Large scale global optimization of ultra-high dimensional non-convex landscapes based on generative neural networks",
    "year": 2023
}