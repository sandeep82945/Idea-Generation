{
    "abstractText": "In this work we explore the combination of metaheuristics and learned neural network solvers for combinatorial optimization. We do this in the context of the transit network design problem, a uniquely challenging combinatorial optimization problem with real-world importance. We train a neural network policy to perform single-shot planning of individual transit routes, and then incorporate it as one of several sub-heuristics in a modified Bee Colony Optimization (BCO) metaheuristic algorithm. Our experimental results demonstrate that this hybrid algorithm outperforms the learned policy alone by up to 20% and the original BCO algorithm by up to 53% on realistic problem instances. We perform a set of ablations to study the impact of each component of the modified algorithm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Andrew Holliday"
        }
    ],
    "id": "SP:7c17d5a9bd7ef1fceedbd23ace7e35ba758f4a94",
    "references": [
        {
            "authors": [
                "David Applegate",
                "Robert E. Bixby",
                "Va\u0161ek Chv\u00e1tal",
                "William J. Cook"
            ],
            "title": "Concorde tsp solver",
            "venue": "timetable. Applied Soft Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Shaked Brody",
                "Uri Alon",
                "Eran Yahav"
            ],
            "title": "How attentive are graph attention networks?, 2021",
            "year": 2021
        },
        {
            "authors": [
                "2017. Ahmed Darwish",
                "Momen Khalil",
                "Karim Badawi"
            ],
            "title": "Optimising public bus transit networks",
            "year": 2017
        },
        {
            "authors": [
                "Val\u00e9rie Guihaire",
                "Jin-Kao Hao"
            ],
            "title": "Transit network design and scheduling: A global review",
            "year": 2006
        },
        {
            "authors": [
                "Ranhee Jeong",
                "R Rilett"
            ],
            "title": "Bus arrival time prediction using artificial neural network model",
            "venue": "In Proceedings. The 7th international IEEE conference on intelligent transportation systems (IEEE Cat. No. 04TH8749),",
            "year": 2004
        },
        {
            "authors": [
                "Zhibin Jiang",
                "Wei Fan",
                "Wei Liu",
                "Bingqin Zhu",
                "Jinjing Gu"
            ],
            "title": "Reinforcement learning approach for coordinated passenger inflow control of urban rail transit in peak hours",
            "venue": "Transportation Research Part C: Emerging Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew P. John",
                "Christine L. Mumford",
                "Rhyd Lewis"
            ],
            "title": "An improved multi-objective algorithm for the urban transit routing problem",
            "venue": "Evolutionary Computation in Combinatorial Optimisation,",
            "year": 2014
        },
        {
            "authors": [
                "Konstantinos Kepaptsoglou",
                "Matthew Karlaftis"
            ],
            "title": "Transit route network design problem: Review",
            "venue": "Journal of Transportation Engineering,",
            "year": 2009
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "W. Kool",
                "H.V. Hoof",
                "M. Welling"
            ],
            "title": "Attention, learn to solve routing problems",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Fatih K\u0131l\u0131\u00e7",
                "Mustafa G\u00f6k"
            ],
            "title": "A demand based route generation algorithm for public transit network design",
            "venue": "Computers & Operations Research,",
            "year": 2014
        },
        {
            "authors": [
                "Can Li",
                "Lei Bai",
                "Wei Liu",
                "Lina Yao",
                "S Travis Waller"
            ],
            "title": "Graph neural network for robust public transit demand prediction",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Lu",
                "Xingwen Zhang",
                "Shuang Yang"
            ],
            "title": "A learning-based iterative method for solving vehicle routing problems",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Christoph E Mandl"
            ],
            "title": "Evaluation and optimization of urban public transportation networks",
            "venue": "European Journal of Operational Research,",
            "year": 1980
        },
        {
            "authors": [
                "Azalia Mirhoseini",
                "Anna Goldie",
                "Mustafa Yazgan",
                "Joe Wenjie Jiang",
                "Ebrahim Songhori",
                "Shen Wang",
                "Young-Joon Lee",
                "Eric Johnson",
                "Omkar Pathak",
                "Azade Nazi"
            ],
            "title": "A graph placement methodology for fast chip",
            "venue": "design. Nature,",
            "year": 2021
        },
        {
            "authors": [
                "Christine L Mumford"
            ],
            "title": "New heuristic and evolutionary operators for the multi-objective urban transit routing problem",
            "venue": "IEEE congress on evolutionary computation,",
            "year": 2013
        },
        {
            "authors": [
                "Milo\u0161 Nikoli\u0107",
                "Du\u0161an"
            ],
            "title": "Teodorovi\u0107. Transit network design by bee colony optimization",
            "venue": "Expert Systems with Applications,",
            "year": 2013
        },
        {
            "authors": [
                "CB Quak"
            ],
            "title": "Bus line planning. A passenger-oriented approach of the construction of a global line network and an efficient timetable",
            "year": 2003
        },
        {
            "authors": [
                "Jean-Paul Rodrigue"
            ],
            "title": "Parallel modelling and neural networks: An overview for transportation/land use systems",
            "venue": "Transportation Research Part C: Emerging Technologies,",
            "year": 1997
        },
        {
            "authors": [
                "Kenneth S\u00f6rensen",
                "Marc Sevaux",
                "Fred Glover"
            ],
            "title": "A history of metaheuristics",
            "venue": "In Handbook of heuristics,",
            "year": 2018
        },
        {
            "authors": [
                "Quinlan Sykora",
                "Mengye Ren",
                "Raquel Urtasun"
            ],
            "title": "Multi-agent routing value iteration network",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Rob van Nes"
            ],
            "title": "Multiuser-class urban transit network design",
            "venue": "Transportation Research Record,",
            "year": 2003
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ronald J Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "Yihua Xiong",
                "Jerry B Schneider"
            ],
            "title": "Transportation network design using a cumulative genetic algorithm and neural network",
            "venue": "Transportation Research Record,",
            "year": 1992
        },
        {
            "authors": [
                "Haoyang Yan",
                "Zhiyong Cui",
                "Xinqiang Chen",
                "Xiaolei Ma"
            ],
            "title": "Distributed multiagent deep reinforcement learning for multiline dynamic bus timetable optimization",
            "venue": "IEEE Transactions on Industrial Informatics,",
            "year": 2023
        },
        {
            "authors": [
                "Rex Ying",
                "Ruining He",
                "Kaifeng Chen",
                "Pong Eksombatchai",
                "William L. Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graph convolutional neural networks for web-scale recommender systems",
            "venue": "CoRR, abs/1806.01973,",
            "year": 2018
        },
        {
            "authors": [
                "Sunhyung Yoo",
                "Jinwoo Brian Lee",
                "Hoon Han"
            ],
            "title": "A reinforcement learning approach for bus network design and frequency setting optimisation",
            "venue": "Public Transport,",
            "year": 2023
        },
        {
            "authors": [
                "Liang Zou",
                "Jian-min Xu",
                "Ling-xiang Zhu"
            ],
            "title": "Light rail intelligent dispatching system based on reinforcement learning",
            "venue": "In 2006 International Conference on Machine Learning and Cybernetics,",
            "year": 2006
        },
        {
            "authors": [
                "Muhammed Yasin \u00c7odur",
                "Ahmet Tortum"
            ],
            "title": "An artificial intelligent approach to traffic accident estimation",
            "venue": "Model development and application. Transport,",
            "year": 2009
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n30 6.\n00 72\n0v 1\n[ cs\n.N E\n] 1\n8 M\nIn this work we explore the combination of metaheuristics and learned neural network solvers for combinatorial optimization. We do this in the context of the transit network design problem, a uniquely challenging combinatorial optimization problem with real-world importance. We train a neural network policy to perform single-shot planning of individual transit routes, and then incorporate it as one of several sub-heuristics in a modified Bee Colony Optimization (BCO) metaheuristic algorithm. Our experimental results demonstrate that this hybrid algorithm outperforms the learned policy alone by up to 20% and the original BCO algorithm by up to 53% on realistic problem instances. We perform a set of ablations to study the impact of each component of the modified algorithm."
        },
        {
            "heading": "1 Introduction",
            "text": "The design of urban transit networks is an important real-world problem, but is computationally very challenging. It has some similarities with other combinatorial optimization (CO) problems such as the Travelling Salesman problem (TSP) and Vehicle Routing Problem (VRP), but due to its manyto-many nature, combined with the fact that demand can be satisfied by transfers between transit lines, the problem is much more complex than those well-studied problems. The most successful approaches to the Transit Network Design Problem (NDP) to-date have been metaheuristic algorithms. Metaheuristics are high-level approximate strategies for problem-solving that are agnostic to the kind of problem. Many are inspired by natural phenomena, such as Simulated Annealing (SA), Genetic Algorithm (GA), and Bee Colony Optimization (BCO).\nMetaheuristic algorithms have proven useful and remain the state-of-the-art in several very complex optimization problems [Ahmed et al., 2019]. But little cross-over exists between the literature on this problem and that of machine learning with neural networks. In this work, we use a neural network system to learn low-level heuristics for the NDP, and use these learned heuristics in a metaheuristic algorithm. We show that this synthesis of a machine learning approach and meta-heuristic approach outperforms either of them alone.\nWe first develop a novel Graph Neural Network (GNN) policy model and train it in an Reinforcement Learning (RL) context to output transit networks that minimize an established cost function. We compare the performance of the trained GNN model to that of Nikolic\u0301 and Teodorovic\u0301 [2013]\u2019s BCO approach on a standard benchmark of NDP instances [Mumford, 2013a], characterizing them over a range of different cost functions. We then integrate this model into a metaheuristic algorithm called BCO, as one of the heuristics that the algorithm can employ as it performs a stochastic search of the solution space. We compare this approach to the GNN model and the unmodified BCO algorithm, and we find that on realistically-sized problem instances, the combination outperforms\nPreprint. Under review.\nthe GNN by up to 20% and BCO by up to 53%. Lastly, we perform several ablations to understand the importance of different components of the proposed system to its performance."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Graph Networks and Reinforcement Learning for Optimization Problems",
            "text": "Graph Neural Networks (GNNs) are neural network models that are designed to operate on graphstructured data [Bruna et al., 2013, Kipf and Welling, 2016, Defferrard et al., 2016, Duvenaud et al., 2015]. They were inspired by the success of convolutional neural nets on computer vision tasks and have been applied in many domains, including analyzing large web graphs [Ying et al., 2018], designing printed circuit boards [Mirhoseini et al., 2021], and predicting chemical properties of molecules [Duvenaud et al., 2015, Gilmer et al., 2017]. An overview of GNNs is provided by Battaglia et al. [2018].\nThere has recently been growing interest in the application of machine learning techniques to solve CO problems such as the TSP and VRP [Bengio et al., 2021]. As many such problems have natural interpretations as graphs, a popular approach has been to use GNNs to solve them. A prominent early example is the work of Vinyals et al. [2015], who propose Pointer Networks and train them via supervised learning to solve TSP instances.\nIn CO problems generally, it is difficult to find a globally optimal solution but easier to compute a scalar quality metric for any given solution. As noted by Bengio et al. [2021], this makes RL, in which a system learns to maximize a scalar reward, a natural fit. Recent work [Dai et al., 2017, Kool et al., 2019, Lu et al., 2019, Sykora et al., 2020] has used RL to train GNN models and have attained impressive performance on the TSP, the VRP, and related problems.\nThe solutions from some neural methods come close to the quality of those from specialized TSP algorithms such as Concorde [Applegate et al., 2001], while requiring much less run-time to compute [Kool et al., 2019]. However, these methods all learn heuristics for constructing a single solution to a single problem instance. By the nature of NP-hard problems such heuristics will always be limited in the quality of their results; In this work, we show that a metaheuristic algorithm that searches over multiple solutions from the learned heuristic can offer better quality."
        },
        {
            "heading": "2.2 Optimization of Public Transit",
            "text": "The transportation optimization literature has extensively studied the Transit Network Design Problem. This problem is NP-complete [Quak, 2003], making it impractical to find optimal solutions for most cases. While analytical optimization and mathematical programming methods have been successful on small instances [van Nes, 2003, Guan et al., 2006], they struggle to realistically represent the problem [Guihaire and Hao, 2008, Kepaptsoglou and Karlaftis, 2009], and so metaheuristic approaches (as defined by S\u00f6rensen et al. [2018]) have been more widely applied. Historically, GAs, SA, and ant-colony optimization have been most popular, along with hybrids of these methods [Guihaire and Hao, 2008, Kepaptsoglou and Karlaftis, 2009]. But more recent work has adapted other metaheuristic algorithms such as BCO [Nikolic\u0301 and Teodorovic\u0301, 2013] and sequence-based selection hyper-heuristics [Ahmed et al., 2019], demonstrating that they outperform approaches based on GAs and SA.\nOn the other hand, while much work has used neural networks for predictive problems in urban mobility [Xiong and Schneider, 1992, Rodrigue, 1997, Chien et al., 2002, Jeong and Rilett, 2004, \u00c7odur and Tortum, 2009, Li et al., 2020] and for other transit optimization problems such as scheduling and passenger flow control [Zou et al., 2006, Ai et al., 2022, Yan et al., 2023, Jiang et al., 2018], relatively little work has applied RL or neural networks (NNs) to the NDP. Darwish et al. [2020] and Yoo et al. [2023] both use RL to design a network and schedule for the Mandl benchmark [Mandl, 1980], a single small graph with just 15 nodes. Darwish et al. [2020] use a GNN approach inspired by Kool et al. [2019], while Yoo et al. [2023] uses tabular RL. Tabular RL approaches tend to scale poorly; meanwhile, in our own work we experimented with a nearly identical approach to Darwish et al. [2020], but found it did not scale beyond very small instances. Both these approaches also require a new model to be trained on each problem instance. The technique developed here, by contrast, is able to find good solutions for realistically-sized NDP instances of more than 100 nodes, and can be applied to problem instances unseen during training."
        },
        {
            "heading": "3 The Transit Network Design Problem",
            "text": "In the NDP, one is given an augmented city graph C = (N , Es, D), comprised of a set of n nodes N representing candidate stop locations; a set of street edges (i, j, \u03c4ij) connecting the nodes, with weights \u03c4ij indicating drive times on those streets; and an n \u00d7 n Origin-Destination (OD) matrix D giving the travel demand (in number of trips) between every pair of nodes in N . The goal is to propose a set of routesR, where each route r is a sequence of nodes in N , so as to minimize a cost function C : C,R\u2192 R+. R is also subject to the following constraints:\n1. The route network R must be connected, allowing every node in N to be reached from every other node via transit.\n2. The route network must contain exactly S routes, that is, |R| = S, where S is a parameter set by the user.\n3. Every route r \u2208 R must be within stop limits MIN \u2264 |r| \u2264 MAX , where MIN and MAX are parameters set by the user.\n4. No route r \u2208 R may contain cycles; that is, it must include each node i at most once.\nWe here deal with the symmetric NDP, that is: D = D\u22a4, (i, j, \u03c4ij) \u2208 Es iff. (j, i, \u03c4ij) \u2208 Es, and all routes are traversed both forwards and backwards by vehicles on them."
        },
        {
            "heading": "3.1 Markov Decision Process Formulation",
            "text": "A Markov Decision Process (MDP) is a formalism for describing a step-by-step problem-solving process, commonly used to define problems in RL. In an MDP, an agent interacts with an environment over a series of time steps. At each time step t, the environment is in some state st \u2208 S; the agent takes some action at which belongs to the set At of available actions in state st. This causes a transition to a new state st+1 \u2208 S according to the state transition distribution P (s\u2032|s, a), and also gives the agent a numerical reward Rt \u2208 R according to the reward distribution P (R|s, a, s\u2032). The agent acts according to a policy \u03c0(a|s), which is a probability distribution over the available actions in each state. In RL, the goal is typically to learn a policy \u03c0 through repeated interactions with the environment, such that \u03c0 maximizes some measure of reward over time.\nWe here describe the MDP we use to represent the Transit Network Design Problem. At a high level, the MDP alternates at every step t between two modes: extend, where the agent selects an extension to the route rt that it is currently planning; and halt, where the agent chooses whether to continue extending rt or stop, adding it as-is to the transit network and beginning the planning of a new route. This alternation is captured by the state variable extendt \u2208 {True, False}, a boolean which changes its value after every step:\nextendt =\n{\n\u00acextendt\u22121 if t > 0 False otherwise (1)\nMore completely, the state st is composed of the city graph C, the set of routes Rt planned so far, the state of the in-progress route rt, and the mode variable extendt. As C does not change with t, we represent the st as in eqn. 2.\nst = (Rt, rt, extendt) (2)\nThe starting state is s0 = (R0 = {}, r0 = [], extend0 = True).\nWhen the expression (extendt = True), the available actions are drawn from SP, the set of shortest paths between all node pairs. If rt = [], then At = {a|a \u2208 SP, |a| \u2264 MAX}. Otherwise, At is comprised of paths a \u2208 SP satisfying all of the following conditions:\n\u2022 (i, j, \u03c4ij) \u2208 Es, where i is the first node of a and j is the last node of rt, or vice-versa\n\u2022 a and rt have no nodes in common\n\u2022 |a| \u2264MAX \u2212 |rt|\nOnce a path at \u2208 At is chosen, rt+1 is formed by appending at to the beginning or end of rt as appropriate: rt+1 = combine(rt, at).\nWhen (extendt = False), the action space is given by eqn. 3.\nAt =\n\n\n\n{continue} if|r| < MIN\n{halt} if|r| = MAX\n{continue, halt} otherwise\n(3)\nIf at = halt, rt is added to Rt to get Rt+1, and rt+1 = [] is a new empty route; if at = continue, thenRt+1 and rt+1 are unchanged from step t.\nThus, the full state transition distribution is deterministic, and is described by eqn. 4.\nst =\n\n\n\n(Rt = Rt\u22121, rt = combine(rt\u22121, at\u22121), False) if extendt\u22121 (Rt = Rt\u22121 \u222a {rt\u22121}, rt = [],True) if \u00acextendt\u22121 and at\u22121 = halt (Rt = Rt\u22121, rt = rt\u22121,True) if \u00acextendt\u22121 and at\u22121 = continue (4)\nWhen |Rt| = S, the MDP terminates, giving the final reward Rt = \u2212C(C,Rt). The reward Rt = 0 at all prior steps.\nThis MDP formalization imposes some helpful biases on the solution space. First, it requires all transit routes to follow the street graph (N , Es); any route connecting i and j must also stop at all nodes along some path between i and j, thus biasing planned routes towards covering more nodes. Second, it biases routes towards being direct and efficient by forcing them to be composed of shortest paths; though in the limiting case a policy may construct arbitrarily indirect routes by choosing paths with length 2 at every step, this is unlikely as the majority of paths in SP are longer than two edges in realistic street graphs. Thirdly and finally, the alternation between deciding to whether to continue a route and deciding to how to extend it means that the probability of halting does not depend on how many different extensions are possible."
        },
        {
            "heading": "3.2 Cost Function",
            "text": "We can define the cost function in general as being composed of three components. The cost to riders is the average time of all passenger trips over the network:\nCp(C,R) =\n\u2211\ni,j Dij\u03c4Rij \u2211\ni,j Dij (5)\nWhere \u03c4Rij is the time of the shortest transit trip from i to j given R, including a time penalty pT for each transfer. The operating cost is the total driving time of the routes:\nCo(C,R) = \u2211\nr\u2208R\n\u03c4r (6)\nWhere \u03c4r is the time needed to completely traverse a route r in both directions.\nTo enforce the constraints on R, we also add a term Cc, which is the fraction of node pairs that are not connected byR plus a measure of how much |r| > MAX or |r| < MIN across all routes. The cost function is then:\nC(C,R) = \u03b1wpCp(C,R) + (1 \u2212 \u03b1)woCo(C,R) + \u03b2Cc(C,R) (7)\nThe weight \u03b1 \u2208 [0, 1] controls the trade-off between passenger and operator costs. wp and wo are rescaling constants chosen so that wpCp and woCo both vary roughly over the range [0, 1) for different C and R; this is done so that \u03b1 will properly balance the two, and to stabilize training of the neural network policy. The values used are wp = (maxi,j Tij) \u22121 and wo = (3Smaxi,j Tij) \u22121, where T is an n\u00d7 n matrix of shortest-path driving times between every node pair."
        },
        {
            "heading": "4 Learned Planner",
            "text": "We propose to learn a policy \u03c0\u03b8(a|s) with parameters \u03b8 with the objective of maximizing G = \u2211\nt Rt on the MDP described in section 3.1. Since reward is only given at the final timestep, we have:\nG = \u2212C(C,Rfinal) (8)\nBy rolling out this policy on the MDP with some city C, we can obtain a transit networkR for that city. We denote this algorithm the Learned Planner (LP), or LP(C, \u03b1,R0 = {}).\nThe policy \u03c0\u03b8 is a neural network model parameterized by \u03b8. Its \u201cbackbone\u201d is a graph attention network [Brody et al., 2021] which treats the city as a fully-connected graph on the nodesN , where each edge has an associated feature vector \u2309ij containing information about demand, existing transit connections, and the street edge (if one exists) between i and j. We note that a graph attention network operating on a fully-connected graph has close parallels to a Transformer model [Vaswani et al., 2017], but unlike Transformers this architecture enables the use of edge features that describe known relationships between elements.\nThe backbone GNN outputs node embeddings Y , which are operated on by one of two policy \u201cheads\u201d, depending on the state st: NNext for choosing among extensions when extendt = True, and NNhalt for deciding whether to halt when extendt = False. The details of the network architecture are provided in Appendix B in the supplementary material."
        },
        {
            "heading": "4.1 Training",
            "text": "Following the work of Kool et al. [2019], we train the policy network using the policy gradient method REINFORCE with baseline [Williams, 1992] and set \u03b3 = 1. Since the reward Rt for the last step is the negative cost and at all other steps Rt = 0, by setting the discount rate \u03b3 = 1, the return Gt to each action at is simply Gt = \u2211 t\u2032 \u03b3 t\u2032\u2212tRt = \u2212C(C,R). The learning signal for each action at is Gt\u2212baseline(C, \u03b1), where the baseline function baseline(C, \u03b1) is a separate Multi-Layer Perceptron (MLP) trained to predict the final reward obtained by the current policy for a given cost weight \u03b1 and city C.\nThe model is trained on a variety of synthetic cities and over a range of values of \u03b1 \u2208 [0, 1]. S, n,MIN, and MAX are held constant during training. For each batch, a full rollout of the MDP episode is performed, the cost is computed, and back-propagation and weight updates are applied to both the policy network and the baseline network.\nEach synthetic city begins construction by generating its nodes and street network using one of these processes chosen at random:\n\u2022 Incoming 4-nn: Sample n random 2D points uniformly in a square to give N . Add street edges to each node i from it\u2019s four nearest neighbours.\n\u2022 Outgoing 4-nn: The same as the above, but add edges in the opposite direction.\n\u2022 Voronoi: Sample m random 2D points, and compute their Voronoi diagram [Fortune, 1995]. Take the shared vertices and edges of the resulting Voronoi cells asN and Es. m is chosen so |N | = n.\n\u2022 4-grid: Place n nodes in a rectangular grid as close to square as possible. Add edges from each node to its horizontal and vertical neighbours.\n\u2022 8-grid: The same as the above, but also add edges between diagonal neighbours.\nFor all models except Voronoi, each edge is then deleted with user-defined probability \u03c1. If the resulting street graph is not strongly connected (that is, all nodes are reachable from all other nodes), it is discarded and the process is repeated. Nodes are sampled in a 30km \u00d7 30km square, and a fixed vehicle speed of v = 15m/s is assumed to compute street edge weights \u03c4ij = ||(xi, yi) \u2212 (xj , yj)||2/v. Finally, we generate the OD matrix D by setting diagonal demands Dii = 0 and uniformly sampling off-diagonal elements Dij \u223c [60, 800].\nAll neural network inputs are normalized so as to have unit variance and zero mean across the entire dataset during training. The scaling and shifting normalization parameters are saved as part of the model and applied to new data presented at test time."
        },
        {
            "heading": "5 Bee Colony Optimization",
            "text": "BCO is an algorithm inspired by how bees in a hive cooperate to search for nectar. At a high level, it works as follows. Given an initial problem solution R0 and a cost function C, a fixed number B of \u201cbee\u201d processes are initialized with Rb = R0 \u2200 b \u2208 [0, B]. Each bee makes a fixed number NC\nof random modifications to Rb, discarding the modification if it increases cost C(Rb). Then each bee is randomly designated a \u201crecruiter\u201d or \u201cfollower\u201d, where P (b = follower) \u221d C(Rb). Each follower bee bf copies the solution of a random recruiter bee br, with probability inversely related to C(Rbr ). These alternating steps of exploration and recruitment are repeated until some termination condition is met, and the lowest-cost solutionRbest found over the process is returned.\nIn Nikolic\u0301 and Teodorovic\u0301 [2013], BCO is adapted to the NDP by dividing the worker bees into two types, which apply different random modification processes. Given a network Rb with S routes for city C for each bee b, each bee selects a route r \u2208 Rb with probability inversely related to the amount of demand r directly satisfies, and then selects a random terminal (first or last node) on r. Type-1 bees replace the chosen terminal with a random other terminal node inN , and make the new route the shortest path between the new terminals. Meanwhile, type-2 bees choose with probability 0.2 to delete the chosen terminal from the route, and with probability 0.8 to add a random node neighbouring the chosen terminal to the route (at the start or end, depending on the terminal), making that node the new terminal. The overall best solution is updated after every NP modification-andrecruitment steps (making one \u201citeration\u201d), and the algorithm performs I iterations before halting. Henceforth, \u201cBCO\u201d refers specifically to this NDP algorithm.\nWe propose a modification of this algorithm, called Neural BCO (\u201cNBCO\u201d henceforth), in which the type-1 bees are replaced by \u201cneural bees\u201d. A neural bee selects a route r \u2208 R for modification in the same manner as the type-1 and type-2 bees, but instead of selecting a terminal on r, it rolls out our learned policy \u03c0\u03b8 to replace r with a new route r\n\u2032 \u2190 LP(C, \u03b1,R \\ {r}). We replace the type-1 bee because its action space (replacing one route by a shortest path) is a subset of the action space of the neural bee (replacing one route by a new route composed of shortest paths), while the type-2 bee\u2019s action space is quite different. The algorithm is otherwise unchanged; for the full details, we refer the reader to Nikolic\u0301 and Teodorovic\u0301 [2013]."
        },
        {
            "heading": "6 Experiments",
            "text": "In all experiments, the policies \u03c0\u03b8 used are trained on a dataset of 2 15 = 32, 768 synthetic cities with n = 20. A 90:10 training:validation split of this dataset is used; after each epoch of training, the model is evaluated on the validation set, and at the end of training, the model weights from the epoch with the best validation-set performance are returned. Data augmentation is applied each time a city is used for training. This consists of multiplying the node positions (xi, yi) and travel times \u03c4ij by a random factor cs \u223c [0.4, 1.6], rotating the node positions about their centroid by a random angle \u03c6 \u223c [0\u25e6, 360\u25e6), and multiplying D by a random factor cd \u2208 [0.8, 1.2]. During training and evaluation, constant values S = 10,MIN = 2,MAX = 15 are used. Training proceeds for 5 epochs, with a batch size of 64 cities. When training with different random seeds, the dataset is held constant across seeds but the data augmentation is not.\nAll evaluations are performed on the Mandl [Mandl, 1980] and Mumford [Mumford, 2013a] city datasets, two popular benchmarks for evaluating NDP algorithms [Mumford, 2013b, John et al., 2014, K\u0131l\u0131\u00e7 and G\u00f6k, 2014, Ahmed et al., 2019]. The Mandl dataset is one small synthetic city, while the Mumford dataset consists of four synthetic cities, labelled Mumford0 through Mumford3, that range in size from n = 30 to n = 127, and gives values of S,MIN, and MAX to use when benchmarking on each city. The values n, S,MIN, and MAX for Mumford1, Mumford2, and Mumford3 are taken from three different real-world cities and their existing transit networks, giving the dataset a degree of realism. Details of these benchmarks are given in Table 1.\nFor both BCO and NBCO, we set all algorithmic parameters to the values used in the experiments of Nikolic\u0301 and Teodorovic\u0301 [2013]: B = 10, NC = 2, NP = 5, I = 400. We also ran BCO for up to I = 2, 000 on several cities, but found this did not yield any improvement over I = 400. We run NBCO with equal numbers of neural bees and type-2 bees, just as BCO uses equal numbers of type1 and type-2 bees. Hyperparameter settings of the policy\u2019s model architecture and training process were arrived at by a limited manual search; for their values, we direct the reader to the configuration files contained in our code release. We set the constraint penalty weight \u03b2 = 5 in all experiments."
        },
        {
            "heading": "6.1 Results",
            "text": "We compare LP, BCO, and NBCO on Mandl and the four Mumford cities. To evaluate LP, we perform 100 rollouts and choose the lowest-cost R from among them (denoted LP-100). Each algorithm is run across a range of 10 random seeds, with a separate policy network trained with that seed. We report results averaged over all of the seeds. Our main results are summarized in Table 2, which shows results at three different \u03b1 values, 0.0, 1.0, and 0.5, which optimize for the operators\u2019 perspective, the passengers\u2019 perspective, and a balance of the two. This table also contains results for two ablation experiments: one in which LP was rolled out 40, 000 times instead of 100 (denoted LP-40k), and one in which we ran a variant of NBCO with only neural bees, no type-2 bees.\nThe results show that while BCO performs best on the two smallest cities in most cases, its relative performance worsens considerably when n = 70 or more. On Mumford1, 2, and 3, for each \u03b1, LP matches or outperforms BCO. Meanwhile, NBCO with a mixture of bee types performs best overall on these three cities. It is better than LP-100 in every instance, improving on its cost by about 6% in most cases at \u03b1 = 1.0 and 0.5, and by up to 20% at \u03b1 = 0.0; and it improves on BCO by 33% to 53% on Mumford3 depending on \u03b1.\nNBCO does fail to obey route length limits on 1 out of 10 seeds when \u03b1 = 0.0. This may be due to \u03b1 = 0.0 causing the benefits from under-length routes overwhelm the cost penalty due to a few routes being too long. This could likely be resolved by simply increasing \u03b2 or adjusting the specific form of Cc."
        },
        {
            "heading": "6.2 Ablations",
            "text": "We first observe that under the parameter settings used here, BCO and NBCO both consider a total of B \u00d7 NC \u00d7 NP \u00d7 I = 40, 000 different networks over a single run. To see whether NBCO\u2019s improvement over LP-100 is simply due to its exploring more solutions, we performed the LP-40k experiments, taking 40, 000 samples from LP instead of 100. The results for LP-40k in Table 2 show that it while it improves on NBCO on Mandl, for all larger cities it is only slightly better than LP-100. The gap between LP-40k and NBCO is at least 54% larger than the gap between LP-40k and LP-100 on each Mumford city for each \u03b1 value, and in 10 of the 12 cases it is more than twice as large. This indicates that the main factor in NBCO\u2019s improvement over LP is the metaheuristic algorithm that guides the search through solution space.\nTo examine the importance of the type-2 bee to NBCO, we run another set of experiments with a variant of NBCO with no type-2 bees, only neural bees, denoted No-2-NB. Again the results are displayed in Table 2. We observe that with the exception of Mumford0 with \u03b1 = 0.0 and Mandl with \u03b1 = 0.5, its performance is worse than with both types of bees: the very different action space of the type-2 bees is a useful complement to the neural bees. However, this variant still outperforms BCO and both LP variants for most cities and \u03b1 values: it is the guidance of the learned heuristic by the bee-colony metaheuristic that is responsible for most of NBCO\u2019s superior performance."
        },
        {
            "heading": "6.3 Trade-offs Between Passenger and Operator Costs",
            "text": "There is a necessary trade-off between minimizing the passenger cost Cp and the operator cost Co: making transit routes longer increases Co, but allows more and faster direct connections between stops, and so may decrease Cp. The weight \u03b1 can be set by the user to indicate how much they care about Cp versus Co, and each algorithm output will change accordingly. Figure 1 illustrates the trade-offs made by the different methods, as we vary \u03b1 over its full range [0, 1] in steps of 0.1, except for LP-40k, for which we only plot values for \u03b1 = 0.0, 0.5, and 1.0. For the two smallest cities (sub-figures 1a and 1b), BCO offers a superior trade-off for most \u03b1, but for the larger cities Mumford1, 2, and 3, NBCO\u2019s solutions not only dominate those of BCO, they achieve a much wider range of Cp and Co than either of BCO or LP, which will be more satisfactory if the user cares only about one or the other component.\nBoth LP and BCO have more narrow ranges of Cp and Co on the three larger cities, but the ranges are mostly non-overlapping. Some of NBCO\u2019s greater range seems to be due to combining the non-overlapping ranges of the constituent parts, but NBCO\u2019s range is greater than the union of LP\u2019s and BCO\u2019s ranges. This implies that the larger action space of the neural bee versus the type-1 bee allows NBCO to explore a much wider range of solutions by taking wider \u201csteps\u201d in solution space. Meanwhile, LP-40k has about the same range as LP-100, implying that these wide steps must be guided by the metaheuristic to eventually reach a wider space of solutions."
        },
        {
            "heading": "7 Discussion",
            "text": ""
        },
        {
            "heading": "7.1 Limitations",
            "text": "Bee Colony Optimization is just one instance of the broad class of metaheuristic algorithms, and while the results we present are promising, it remains to be seen whether incorporating learned heuristics into metaheuristic algorithms is a sound algorithmic strategy in general. Furthermore, in this work we consider such a combination in light of only one CO problem, the NDP. This is a very interesting and impactful problem, but evaluating this method on a wider variety of CO problems such as the TSP and VRP would more broadly establish the usefulness of this strategy.\nWe also note that, while the Mumford dataset is a widely used benchmark, it is still synthetic data. Establishing whether our method would be useful for transit planning in real-world cities will require evaluating on a real-world dataset."
        },
        {
            "heading": "7.2 Conclusions",
            "text": "Ultimately, it is doubtful whether a single-pass generation heuristic like that implemented by the GNN will be capable of outperforming search based methods like metaheuristics on combinatorial\noptimization problems like the NDP. By these problems\u2019 nature, there is no one-step algorithm for finding optimal solutions, and any fast-to-compute heuristic will necessarily be approximate. Consequently, methods for exploring the solution space on a given instance have a general advantage over such heuristics. But we have shown that the choice of heuristics can have a significant impact on the quality of the solutions found by a metaheuristic, and learned heuristics in particular can significantly benefit metaheuristic algorithms when used as some of their sub-heuristics.\nIn terms of the applicability of these methods in real cities, we note that both LP and Neural BCO outperform BCO on all three cities - Mumford1, 2, and 3 - that were designed to match a specific real-world city in scale. Furthermore, the gap between BCO and the other methods grows with the size of the city. This suggests that Neural BCO may scale better to much larger problem sizes - which is significant, as some real-world cities have hundreds or even thousands of bus stop locations [Soci\u00e9t\u00e9 de transport de Montr\u00e9al, 2013].\nWe note that better results could likely be achieved by training a policy directly in a metaheuristic context, rather than training it in isolation and then applying it in a metaheuristic as was done here. It would also be interesting to use multiple separately-trained models as different heuristics within a metaheuristic algorithm, as opposed to the single model used in our experiments. This could be seen as a form of ensemble method, with the metaheuristic intelligently combining the strengths of the different learned models to get the best use out of each.\nWe would also like to explore the training of a further Machine Learning (ML) component to act as the higher-level metaheuristic, creating an entirely learned method for searching the solution space for particular problem instances. Recent work on few-shot adaptation in RL [Behbahani et al., 2023] may provide a promising starting point.\nBeyond studying the relative performance of machine learning and metaheuristic approaches and their combination, we hope by this work to draw the attention of the machine community to the NDP. It is a uniquely challenging combinatorial optimization problem with real-world impact, with much potential for novel and useful study by our discipline."
        }
    ],
    "title": "Neural Bee Colony Optimization: A Case Study in Public Transit Network Design",
    "year": 2023
}