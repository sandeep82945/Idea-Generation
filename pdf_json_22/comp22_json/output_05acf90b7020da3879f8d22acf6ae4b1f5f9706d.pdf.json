{
    "abstractText": "Department of Electronics and Communication Engineering, Mallareddy Institute of Technology and Science, Secunderabad 500100, Telangana, India Department of Computer Science and Engineering, Mohamed Sathak A. J College of Engineering, Sipcot IT Park, Siruseri, Chennai 603103, Tamilnadu, India Department of Electronics and Communication Engineering, Sona College of Technology, Salem 636005, Tamilnadu, India Department of Electronics and Communication Engineering, Kuppam Engineering College, Kuppam 517425, Andhra Pradesh, India Department of Electronics and Communication Engineering, SRM Institute of Science & Technology, Ramapuram, Chennai 603203, Tamilnadu, India Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, \u0085alavapalayam, Karur 639113, Tamilnadu, India Department of Electronics and Communication Engineering, Sri Venkateswara College of Engineering, Nellore, Andhra Pradesh-52316, India Department of Chemical Engineering, College of Biological and Chemical Engineering, Addis Ababa Science and Technology University, Addis Ababa, Ethiopia",
    "authors": [
        {
            "affiliations": [],
            "name": "S. Kannan"
        },
        {
            "affiliations": [],
            "name": "G. Premalatha"
        },
        {
            "affiliations": [],
            "name": "M. Jamuna Rani"
        },
        {
            "affiliations": [],
            "name": "D. Jayakumar"
        },
        {
            "affiliations": [],
            "name": "P. Senthil"
        },
        {
            "affiliations": [],
            "name": "S. Palanivelrajan"
        },
        {
            "affiliations": [],
            "name": "S. Devi"
        },
        {
            "affiliations": [],
            "name": "Kibebe Sahile"
        }
    ],
    "id": "SP:0b6f40beb05652c82ae16c870cc8beba3702ae59",
    "references": [
        {
            "authors": [
                "M. Bandarabadi",
                "C.A. Teixeira",
                "J. Rasekhi",
                "A. Dourado"
            ],
            "title": "Epileptic seizure prediction using relative spectral power features",
            "venue": "Clinical Neurophysiology, vol. 126, no. 2, pp. 237\u2013248, 2015.",
            "year": 2015
        },
        {
            "authors": [
                "T. Netoff",
                "Y. Park",
                "K. Parhi"
            ],
            "title": "Seizure prediction using cost-sensitive support vector machine",
            "venue": "Proceedings of the 2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, pp. 3322\u20133325, MN, USA, September 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Y. Park",
                "L. Luo",
                "K.K. Parhi",
                "T. Netoff"
            ],
            "title": "Seizure prediction with spectral power of EEG using cost-sensitive support vector machines",
            "venue": "Epilepsia, vol. 52, no. 10, pp. 1761\u20131770, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "K. Gadhoumi",
                "J.-M. Lina",
                "J. Gotman"
            ],
            "title": "Discriminating preictal and interictal states in patients with temporal lobe epilepsy using wavelet analysis of intracerebral EEG",
            "venue": "Clinical Neurophysiology, vol. 123, no. 10, pp. 1906\u20131916, 2012. 8 Computational Intelligence and Neuroscience",
            "year": 1906
        },
        {
            "authors": [
                "L. Wang",
                "C. Fu",
                "F. Yu"
            ],
            "title": "Temporal lobe seizure prediction based on a complex Gaussian wavelet",
            "venue": "Clinical Neurophysiology, vol. 122, no. 4, pp. 656\u2013663, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "A. Shahidi Zandi",
                "R. Tafreshi",
                "M. Javidan",
                "G.A. Dumont"
            ],
            "title": "Predicting epileptic seizures in scalp EEG based on a variational Bayesian Gaussian mixture model of zero-crossing intervals",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 60, no. 5, pp. 1401\u20131413, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "J.R. Williamson",
                "D.W. Bliss",
                "D.W. Browne",
                "J.T. Narayanan"
            ],
            "title": "Seizure prediction using EEG spatiotemporal correlation structure",
            "venue": "Epilepsy and Behavior, vol. 25, no. 2, pp. 230\u2013238, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "S. Min",
                "B. Lee",
                "S. Yoon"
            ],
            "title": "Deep learning in bioinformatics",
            "venue": "Briefings in Bioinformatics, vol. 18, no. 5, pp. 851\u2013869, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "Y. Lecun",
                "L. Bottou",
                "Y. Bengio",
                "P. Haffner"
            ],
            "title": "Gradientbased learning applied to document recognition",
            "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "N.D. Truong",
                "A.D. Nguyen",
                "L. Kuhlmann",
                "M.R. Bonyadi",
                "J. Yang",
                "O. Kavehei"
            ],
            "title": "A Generalised Seizure Prediction with Convolutional Neural Networks for Intracranial and Scalp Electroencephalogram Data Analysis",
            "venue": "2017, https:// arxiv.org/abs/1707.01976.",
            "year": 2017
        },
        {
            "authors": [
                "H. Khan",
                "L. Marcuse",
                "M. Fields",
                "K. Swann",
                "B. Yener"
            ],
            "title": "Focal onset seizure prediction using convolutional networks",
            "venue": "IEEE Transactions on Biomedical Engineering, vol. 65, no. 9, pp. 2109\u20132118, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Gadhoumi",
                "J.-M. Lina",
                "F. Mormann",
                "J. Gotman"
            ],
            "title": "Seizure prediction for therapeutic devices: a review",
            "venue": "Journal of Neuroscience Methods, vol. 260, pp. 270\u2013282, 2016.",
            "year": 2016
        },
        {
            "authors": [
                "S. +ome-Souza",
                "S. Jackson",
                "M. Kadish"
            ],
            "title": "Seizure detection, seizure prediction, and closed-loop warning systems in epilepsy",
            "venue": "Epilepsy and Behavior, vol. 37, pp. 291\u2013307, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "K.M. Tsiouris",
                "V.C. Pezoulas",
                "D.D. Koutsouris",
                "M. Zervakis",
                "D.I. Fotiadis"
            ],
            "title": "Discrimination of preictal and interictal brain states from long-term EEG data",
            "venue": "Proceedings of the 2017 IEEE 30th International Symposium on Computer-Based Medical Systems, pp. 318\u2013323, CBMS), +essaloniki, Greece, June 2017.",
            "year": 2017
        },
        {
            "authors": [
                "V.C. Pezoulas",
                "M. Zervakis",
                "S. Konitsiotis",
                "D.D. Koutsouris",
                "D.I. Fotiadis"
            ],
            "title": "A Long Short-Term Memory deep learning network for the prediction of epileptic seizures using EEG signals",
            "venue": "Computers in Biology and Medicine, vol. 99, pp. 24\u201337, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T.N. Alotaiby",
                "S.A. Alshebeili",
                "T. Alshawi",
                "I. Ahmad",
                "F.E. Abd El-Samie"
            ],
            "title": "EEG seizure detection and prediction algorithms: a survey",
            "venue": "EURASIP Journal on Applied Signal Processing, vol. 2014, no. 1, p. 183, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "F. Kreuz",
                "T. Rieke",
                "C. Andrzejak"
            ],
            "title": "On the predictability of epileptic seizures",
            "venue": "Clinical Neurophysiology, vol. 116, no. 3, pp. 569\u2013587, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "S.G. Mallat"
            ],
            "title": "A theory for multiresolution signal decomposition: the wavelet representation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 11, no. 7, pp. 674\u2013693, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "S. Hochreiter"
            ],
            "title": "+e vanishing gradient problem during learning recurrent neural nets and problem solutions",
            "venue": "International Journal of Uncertainty, Fuzziness and Knowledge- Based Systems, vol. 6, no. 2, 1998.",
            "year": 1998
        },
        {
            "authors": [
                "S. Schmidhuber",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "R.G. Hefron",
                "B.J. Borghetti",
                "J.C. Christensen",
                "C.M.S. Kabban"
            ],
            "title": "Deep long short-term memory structures model temporal dependencies improving cognitive workload estimation",
            "venue": "Pattern Recognition Letters, vol. 94, pp. 96\u2013104, 2017/07/15/2017.",
            "year": 2017
        },
        {
            "authors": [
                "X. Glorot",
                "A. Bordes",
                "Y. Bengio"
            ],
            "title": "Deep sparse rectifier neural networks",
            "venue": "Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, pp. 315\u2013323, FL, USA, April 2011.",
            "year": 2011
        },
        {
            "authors": [
                "P. Ramachandran",
                "B. Zoph",
                "Q.V. Le"
            ],
            "title": "Searching for Activation Functions",
            "venue": "2018, https://arxiv.org/abs/1710. 05941.",
            "year": 2018
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: a method for stochastic optimization",
            "venue": "2014, https://arxiv.org/abs/1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "M. Abadi"
            ],
            "title": "Tensorflow: large-scale machine learning on heterogeneous distributed systems",
            "venue": "2016, https://arxiv.org/ abs/1603.04467.",
            "year": 2016
        },
        {
            "authors": [
                "I. Ullah",
                "M. Hussain",
                "E.-u.-H. Qazi",
                "H. Aboalsamh"
            ],
            "title": "An Automated System for Epilepsy Detection Using EEG Brain Signals Based on Deep Learning Approach",
            "venue": "2018, https:// arxiv.org/ftp/arxiv/papers/1801/1801.05412.pdf.",
            "year": 2018
        },
        {
            "authors": [
                "S. Tripathi",
                "S. Acharya",
                "R.D. Sharma",
                "S. Mittal",
                "S. Bhattacharya"
            ],
            "title": "Using deep and convolutional neural networks for accurate emotion classification on DEAP data",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, vol. 31, no. 2, pp. 4746\u20134752, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M.M. Hasib",
                "T. Nayak",
                "Y. Huang"
            ],
            "title": "A Hierarchical LSTM Model with Attention for Modeling EEG Non-stationarity for Human Decision Prediction",
            "venue": "Proceedings of the 2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), pp. 104\u2013107, NV, USA, March 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T.N. Alotaiby",
                "S.A. Alshebeili",
                "F.M. Alotaibi",
                "S.R. Alrshoud"
            ],
            "title": "Epileptic seizure prediction using CSP and LDA for scalp EEG signals",
            "venue": "Computational Intelligence and Neuroscience, vol. 2017, pp. 1\u201311, Article ID 1240323, 2017. Computational Intelligence and Neuroscience 9",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "Research Article",
            "text": ""
        },
        {
            "heading": "Effective Evaluation of Medical Images Using Artificial",
            "text": ""
        },
        {
            "heading": "Intelligence Techniques",
            "text": "S. Kannan ,1 G. Premalatha,2 M. Jamuna Rani,3 D. Jayakumar,4 P. Senthil,5 S. Palanivelrajan ,6 S. Devi,7 and Kibebe Sahile 8\n1Department of Electronics and Communication Engineering, Mallareddy Institute of Technology and Science, Secunderabad 500100, Telangana, India 2Department of Computer Science and Engineering, Mohamed Sathak A. J College of Engineering, Sipcot IT Park, Siruseri, Chennai 603103, Tamilnadu, India 3Department of Electronics and Communication Engineering, Sona College of Technology, Salem 636005, Tamilnadu, India 4Department of Electronics and Communication Engineering, Kuppam Engineering College, Kuppam 517425, Andhra Pradesh, India 5Department of Electronics and Communication Engineering, SRM Institute of Science & Technology, Ramapuram, Chennai 603203, Tamilnadu, India 6Department of Electronics and Communication Engineering, M. Kumarasamy College of Engineering, alavapalayam, Karur 639113, Tamilnadu, India 7Department of Electronics and Communication Engineering, Sri Venkateswara College of Engineering, Nellore, Andhra Pradesh-52316, India 8Department of Chemical Engineering, College of Biological and Chemical Engineering, Addis Ababa Science and Technology University, Addis Ababa, Ethiopia\nCorrespondence should be addressed to S. Kannan; drkannanprof19@gmail.com and Kibebe Sahile; kibebe.sahele@aastu.edu.et\nReceived 28 March 2022; Accepted 1 June 2022; Published 10 August 2022\nAcademic Editor: Arpit Bhardwaj\nCopyright \u00a9 2022 S. Kannan et al. \u00a0is is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\u00a0is work is implemented for the management of patients with epilepsy, and methods based on electroencephalography (EEG) analysis have been proposed for the timely prediction of its occurrence. \u00a0e proposed system is used for crisis detection and prediction system; it is useful for both patients and medical sta to know their status easily and more accurately. In the treatment of Parkinson\u2019s disease, the a ected patients with Parkinson\u2019s disease can assess the prognostic risk factors, and the symptoms are evaluated to predict rapid progression in the early stages after diagnosis. \u00a0e presented seizure prediction system introduces deep learning algorithms into EEG score analysis. \u00a0is proposed work long short-term memory (LSTM) network model is mainly implemented for the identi cation and classi cation of qualitative patterns in the EEG of patients. While compared with other techniques like deep learningmodels such as convolutional neural networks (CNNs) and traditional machine learning algorithms, the proposed LSTM model plays a signi cant role in predicting impending crises over 4 di erent qualifying intervals from 10 minutes to 1.5 hours with very few wrong predictions."
        },
        {
            "heading": "1. Introduction",
            "text": "Seizure prediction methodologies are based entirely on continuous EEG recordings. Unlike seizure detection, the sample\u2019s weight now shifts to epileptic waves at EEG intervals before seizure onset. Quali ed EEG analysis for seizure prediction has two main approaches. \u00a0e rst is\nbased on the analysis of the characteristics of export signals to track the temporal change in their prices, which leads to the onset of a crisis. When prices exceed a certain threshold, the system is activated to warn of an impending crisis. From the time the alert is issued, a window of time is given during which a crisis is expected to eventually occur. If the crisis occurs within the forecast window, it is considered to have\nHindawi Computational Intelligence and Neuroscience Volume 2022, Article ID 8419308, 9 pages https://doi.org/10.1155/2022/8419308\nbeen successfully forecast; otherwise, it is characterized as an incorrect forecast. +e second approach is based on the use of machine learning to identify subcritical and intra-critical areas of the EEG of patients. In this case, the scoring window and the corresponding EEG segments from the start point of each attack to the window definition are determined before grouping them in the same class as the markers. All other sections of the EEG preceding the scoring window and all sections after the end of the seizure are intercritical. After exporting two classes, the classification algorithm learns to separate them. Each time, a part of the EEG is deemed qualified; a corresponding warning of an impending crisis is generated. +e qualifying window in both approaches is randomly chosen by investigators in each study and has been shown to last from minutes to hours before a seizure."
        },
        {
            "heading": "2. Literature Review",
            "text": "In recent years, machine learning algorithms have been widely used to predict crises. In said system, a model was developed and trained for the classification of the rating and average critical departments using support vector machines (SVMs) using the characteristics of spectrum extraction for each EEG channel and energy distribution in different frequency ranges [1]. Similar analysis methods have shown that the EEG frequency composition changes significantly over the qualifying period. In their study, Netoff and his colleagues used EEG energy distribution in 9 frequency bands (0.5 to 4Hz, 4 to 8Hz, 8 to 13Hz, 13 to 30Hz, 30 to 50Hz, 50 to 70Hz, 70\u201390Hz, and > 90Hz) and an SVM classifier to separate the qualifying and mesocritical segments.+e evaluation was performed on the EEG recordings of 9 patients at the Freiburg base (45 seizures in 219 hours) with a mean sensitivity of 77.8% without prediction errors [2]. +e qualifying window was much shorter at 5 minutes. In a similar study evaluating a large sample of 18 patients from Freiburg (80 seizures in 433 hours), the researchers focused on extracting features from the higher frequency spectrum and the methodology achieved a significantly higher average sensitivity of 97.5%, but also 0.27 predictions per hour [3]. And in this study, the classification was performed using SVM. In addition to the Fourier transform and the corresponding wavelet transform, it has also been shown to be a very efficient method of calculating the energy distribution of the signal to separate the defining parts of the EEG both intracranial [4] and on the surface recordings [5] with similar results. In another approach, the number of null transitions (sign change in the EEG waveform) was used to determine qualifying sites and predict seizures. In the proposed methodology, the differences in the rate of zero transitions between qualified and intercritical units were studied using Gaussian mixed models (GMMs) to predict 40-minute depth crises [6]. To improve neural networks, models have been proposed that are capable of synthesizing more efficient complex networks that can be better adapted to train data and learn more complex representations and hidden dependencies. In recent years, deep learning algorithms have become increasingly popular in medical image and signal analysis due to the increase in available\ncomputing power and the collection of large amounts of data [7]. One of the best-known deep learning models is convolutional neural networks (CNNs), a type of network consisting of repeated layers of convolution and pooling that is very efficient for analyzing data representing a lattice topology, such as medical images [8]. However, some studies have also been suggested, presented to CNN on EEG analysis for the purpose of predicting seizures. One of these methodologies used a CNN network with 3 successive hidden convergence levels to estimate the spectrum of EEG signals extracted using the short-term Fourier transform. +e spectrum was analyzed as an image to find differences between qualifying and mesocritical segments [9]. Training of the deeper CNN with 6 hidden layers and use of the wavelet transform to extract frequency information from EEG [10]."
        },
        {
            "heading": "3. Crisis Prediction Model with Deep Machine Learning Algorithms",
            "text": "+is section presents a proposed methodology for predicting seizures from EEG data.+e proposed methodology is based on the separation of the qualified EEG regions from the corresponding mesocritics using traditional classification algorithms [11], as well as deep learning models [12]. +e process of EEG signal analysis and feature extraction for classifier training is carried out separately for each patient. +e main stages of the technique are shown in Figure 1. In the first stage of data preprocessing, an initial evaluation of the available EEG recordings is performed to determine the number of channels available and the recording model when receiving signals.+is is necessary because both the channels and the recording schema contained in the patient\u2019s charts may change during a long continuous EEG recording. By collecting information relevant to each patient\u2019s record file, channels that are not available in the entire record are discarded in subsequent analysis to ensure that the system consistently obtains the same amount of information from the patient\u2019s EEG, regardless of the log point parsed at any time.\nApart from checking the homogeneity of the EEG channels, no other preprocessing is applied to the data (e.g., filtering to exclude static noise or possible spurious endogenous or extracerebral parameters). +e next step consists of dividing the EEG signals into segments of shorter duration, which will be analyzed separately to extract characteristics from which the final classification will be made. +e duration of these sections is set to 5 seconds, and consecutive sections are separated without overlapping. +e most well-known EEG processing and analysis methods are used to extract features.+e set of exported features contains values from the EEG analysis in the time field and in the frequency field, from the calculation of the correlation between its different channels and from the trace, in an attempt to create a space of features that contains the complete information as possible. +e advantage of using a feature extraction step is that it is an efficient method for revealing hidden and more complex correlations that may be hidden in the EEG signals. Next, the effect of feature extraction on the efficiency of the final classification is estimated in\ncomparison with the direct use of the signals of each EEG channel in the form of a time series.\n\u00a0e last stage consists of training the classi er, which is in charge of dividing the sections into classi catory and mesocritical, using the values of the exported characteristics. \u00a0e accuracy of crisis forecasting depends largely on the classi cation algorithm; therefore, in the following blocks di erent methods are evaluated, in the eld of both classical machine learning (RIPPER algorithm, decision trees, SVM) and deep learning (LSTM model). Initially, each 5-second section is assigned to the appropriate class to which it belongs (critical, intercritical, or qualifying) based on the length of the qualifying window and the start and end points of each crisis, as speci ed in the database of data scores. File judgment segments are automatically discarded as they have no useful value for prediction, leading to a binary classi - cation problem with two classes (mesocritic quali er). \u00a0e length of the qualifying period is an arbitrary choice for any study, as it has not yet been proven that there is a strictly de ned period of time before the onset of each crisis. Several studies have even shown that changes in EEG activity can occur several hours before the onset of the disease [13\u201315]. For the sake of completeness, this article estimates 4 qualifying windows ranging from 15 minutes to 2 hours before the onset of crises."
        },
        {
            "heading": "4. Export Features",
            "text": "Features are extracted separately for each part of the EEG by analyzing the signal values of all available channels. \u00a0e number of samples in each 5-second segment depends on the sample rate selected when the data were written (e.g., 1280 samples for 256Hz). Exported features are among the most widely used in EEG analysis, and previous studies have also shown them to be very useful for seizure prediction, as their values change signi cantly over the scoring period.\n4.1. Features in the Field of Time. \u00a0is category includes features that can be calculated directly from recorded EEG samples at the time they are acquired. \u00a0ese features include average price (mean), variance, standard deviation, skewness, kurtosis, number of zero crossings, signal width, signalto-peak (V-peak), and signal area with a trapezoid rule. All\nfunctions are exported separately for each EEG channel. Although the abovemeasurements are relatively simple, they have great potential for detecting qualitative changes in patients\u2019 EEGs. For example, variability has been shown to decrease signi cantly during the rating period, while curvature increases as the onset of crisis approaches [16]. Also, as mentioned above, the number of spikes and zero crossings varies considerably over the rating period.\n4.2.Characteristics in the FrequencyDomain. EEG frequency analysis is one of the most useful methods, since the distribution of the signal energy hides a lot of information about the state of the brain.\u00a0erefore, the distribution of the signal energy in the main EEG frequencies is calculated: \u03b4 (1\u20133Hz), \u03b8 (4\u20137Hz), \u03b1 (8\u201313Hz), \u03b2 (14\u201330Hz), c1 (31\u201355Hz), and c2 (65\u2013110Hz). \u00a0e EEG spectrum for energy distribution is exported as a periodic table (periodogram, P) based on discrete Fourier transform (DFT). Also, in addition to the power distribution over the above 6 speeds, the total signal power of each channel is used.\nA discrete wavelet transform (DWT) is then applied to the EEG signals of each section to calculate the energy distribution using the pyramidal algorithm proposed by [17], with the key feature of low computational cost. \u00a0e transformation occurs in successive steps in an iterative process that each time separates the information contained in the high-frequency signal values by applying high-pass and low-pass composite lters to the original wavelet. In the present analysis, the fourth Daubechies wavelet was chosen as the initial wavelet. At each level, the signal samples are halved due to ltering. \u00a0e procedure is tentatively illustrated in Figure 2 for 3 levels at a sample rate of 256Hz, which is the CHB-MIT base sample rate that will be used to evaluate the methodology. \u00a0e coe\u00a8cients Di are called detail coe\u00a8cients, while the approximation coe\u00a8cients Ai are used to determine the outputs of the high-pass and lowpass lters, respectively. With the help of DWT, it is possible to estimate the energy distribution of the signal at individual sub-frequencies 64\u2013128Hz, 32\u201364Hz, 16\u201332Hz, 8\u201316Hz, 4\u20138Hz, 2\u20134Hz, 1-2Hz, and <1Hz with low computational cost.\n\u00a0e use of 7 levels allows to calculate the energy distribution in the spectra very close to the base frequencies,\nadequately extracting the \u03b4 frequency at 1\u20133Hz and discarding frequencies <1Hz, which usually contain strong spurious potentials.\n4.3. Channel Correlation and Self-Correlation. Calculating the correlation between di erent EEG channels (crosscorrelation) can provide information on the simultaneous activation of di erent brain regions, since it has been shown that both synchronization and de-synchronization between them can indicate an impending crisis. \u00a0e correlation is tested in pairs between all the possible combinations of available channels and is calculated based on the act of convergence (\u2217 ) from the following equation, taking into account the temporal o set n between the signals (delay time or delay):\n\u03c1 cicj[m+n] ci \u2217 cj( )[m+n( )/\u03c3ci\u03c3cj( ), n\u2208[\u2212m,+m] . (1)\nTo calculate the correlation coe\u00a8cient, the signals must have the same duration. \u00a0e nal correlation values are normalized to the interval [\u22121, 1]. \u00a0e greater the value to one, the better the correlation between the pair of channels considered, and negative values indicate the presence of phase di erence correlation. In addition to the correlation between di erent channels, the decorrelation time is also calculated. \u00a0e decoupling time indicates the time interval before the rst transition from zero of the autocorrelation signal of each channel. \u00a0e point at which the autocorrelation rst resets to zero is the requested latency and is calculated for each available channel separately."
        },
        {
            "heading": "5. Long Short-Term Memory (LSTM) Crisis Prediction",
            "text": "\u00a0e architecture of the LSTM model is an evolution of recurrent neural networks (RNNs) and has been proposed to improve adaptability to sequential training data. RNNs were the rst class of neural networks designed speci cally for the analysis of sequential data and time series, and have found\napplications in the analysis of medical signals such as EEG. \u00a0e peculiarity of this datum is that each value is a natural continuation of the previous one and depends on it. Patterns of time change also often appear, which in order to discover a network must be able to respond to current input given information that preceded it even long ago. RNNs are designed to manage such serial connections by allowing previous input values to in\u00aauence the current response of the network, essentially implementing a type of memory as shown in Figure 3.\u00a0ree tables with weights U,W, and V are used and applied for the input data for this purpose to the shared data in the hidden layer of the next state and to the network output, respectively. \u00a0e weights are distributed among the di erent states based on the depth of the grid.\nWhile theoretically long-range RNNs can be adapted to signals such as EEG, in practice they have been shown to be unable to detect long-range dependencies e ectively. \u00a0e reason is that the weight values in the U,W, and V tables are common to all time states supported by the depth of the grid, so it is very likely that some parameter will become unstable, causing the slope of the cost function rise or fall abruptly, which leads to a violation of the learning process and the impossibility of successfully adapting the network to the data due to the high complexity of the load calculation functions. \u00a0is problem is commonly known as the gradient problem and is mainly related to back propagation in time [18].\n5.1. Operation of LSTM Networks. \u00a0e previous di\u00a8culty of the internal architecture in an RNN was overcome with the long short-term memory model (LSTM) [19]. Although memory cells are wired in the same way as RNNs (Figure 3), LSTMs have a more complex architecture within them, allowing them to better manage information over long periods of time without requiring much e ort to train them. LSTM is a chain-like structure constructed by replicating blocks of NN. \u00a0e memory cell stores the information and runs the chain. In addition, the gates control whether information can eventually be added to or blocked in the memory cell. It is necessary to rst create a vector for the cell state using the tanh function, then sort the information from gt\u22121 to xt, and multiply by the previously created vector to obtain the new output. LSTM functions are de ned as in equations (2)\u2013(7) as shown in Figure 4.\nit\u2212g \u03d5 \u03c9i gt\u22121, xt[ ]( ) + ai, (2)\nft\u2212g \u03d5 \u03c9f gt\u22121, xt[ ]( ) + af, (3)\not\u2212g \u03d5 \u03c9o[g., x.]( ) + a0, (4)\nyt tanh \u03c9c gt\u22121, xt[ ]( ) + af, (5)\nyt fg \u00b7 yt\u22121 + ig \u00b7 yt, (6)\ngt og \u00b7 tanh yt( ). (7)\nLogistic sigmoid function is represented by \u03d5, hyperbolic tangent function is represented by tanh, and\u00b7 denotes\nmultiplication function. At time t, ig denotes input gate, fg denotes forget gate, og denotes output gate, and gt denotes hidden state. \u03c9i,\u03c9f and\u03c9o denote the weights of input, forget, and output gate, respectively, while ai, af and ao denote their respective biases.\n\u00a0erefore, by being able to independently install and con gure three gateways for each memory cell, the LSTM network may be much more suitable for analyzing timebased data such as EEG recordings. Furthermore, an LSTM network can be composed of several hidden layers, in which the cache state output of one layer of memory cells is used as input for the next layer of memory cells, forming deeper and more complex architectures. However, the complexity of such a network grows rapidly, resulting in millions of training parameters for a network with multiple layers and a large number of memory cells in each layer. With networks this deep, there are two problems to solve:\n(1) Computational costs, which are enormous, and even arrays of computing units are required to train such networks.\n(2) Even if funds are available to cover the computational cost, a very large set of big data must be available for training to be e\u00a8cient and not to over t on the training data (over tting).\nFor this reason, although advances in computers now provide a lot of computing power at a relatively low cost, the design of an LSTM network must be done very carefully to achieve the desired result with a minimum number of\nparameters possible, and the models should be generalizable and useful.\n5.2. LSTM Network Architecture. \u00a0erefore, this section provides a preliminary analysis that considers three di erent LSTM network architectures, consisting of a simple circuit with multiple memory cells per layer and more complex networks with greater depth and number of elements. \u00a0ese networks will be tested with a small subset of EEG records from three randomly selected patients in the CHM-MIT database to assess classi cation accuracy between subcritical and intra-critical EEG segments and select the optimal architecture. \u00a0e rst architecture, LSTM_1, is the simplest approach with a grid consisting of a hidden level with 32 memory cells. In the second architecture, LSTM_2, the number of network memory cells is increased to 128 while maintaining a single layer design. In the third project, LSTM_3, the depth of the network is increased, one more hidden level is introduced, but the number of memory cells in each level remains constant: 128. \u00a0ree architectures are shown in Figure 5. Figure 5 also shows intermediate levels of neuronal deletion (dropout). \u00a0e use of dropout rates has been proposed as a method to solve the problem of training data over tting in order to make LSTM networks more robust and generalizable to new samples. In practice, its purpose is to randomly reset part of each level\u2019s output in such a way as to remove a percentage of network information and make it di\u00a8cult to learn very speci c patterns that would be useless when evaluated with other data. \u00a0e usefulness of this level in the present study of crisis forecasting is assessed below, since the size of LSTM networks is relatively small, and their contribution can be very limited. In all three cases, the output of the last layer of LSTM networks is complemented by two additional layers consisting of fully interconnected neural networks. \u00a0e rst takes the input of the LSTM network output and creates a 30-component output using the activation function of the recti ed linear unit, ReLU [21]. \u00a0e ReLU function is described by the equation:\nf(x) max(0, x), (8)\nwhere x is the input. In practice, the function returns a value of zero for each negative input value, while for positive values, triggering occurs on a 45\u00b0 dial ramp.\n\u00a0e ReLU function is preferred because it has been shown to perform better on deep machine learning network training problems and for this reason it is recognized as the most popular activation function [22, 23]. Finally, a second fully interconnected network produces a binary classi cation e ect by dividing the EEG segments into qualitative or intercritical segments using the softmax function. \u00a0e softmax function returns normalized values in the range [0, 1] for each network class. \u00a0e class with the highest value is considered probably the most correct and is chosen to classify the corresponding EEG segment.\n\u00a0e cost function to train the algorithm uses a logarithmic cross-entropy function, and Adam\u2019s algorithm (adaptive momentum estimation) [24] was chosen as the optimization algorithm, using standard values of internal parameters (i.e., learning rate 0.001, beta_1 0.9, beta_2 0.999, epsilon 1e \u2013 08, and decay 0). As shown in Figure 6, the advantages of Adam\u2019s algorithm are lower computational costs and relatively faster convergence, which is a key factor for deep learning applications where network parameters can be very large and can be trained on large data sets. For this reason, although Adam\u2019s algorithm is a relatively recent implementation, it is almost installed on all deep learning network design platforms (e.g., TensorFlow, Keras, Torch, Ca e) as the recommended optimization algorithm.\nFinally, due to the complexity of networks, the training process is carried out in smaller subsets of the total number of training samples, called batches, to limit the memory requirements of the system. In addition, in this way a smoother convergence is achieved when training the LSTM network. In each batch, pseudorandom training data subsets are selected based on a parameter value (e.g., for batch = 10, 10 samples) through an iterative process until all data samples have been used of training available. \u00a0e process is then repeated for a predetermined number of iterations, called epochs. For the present study, both parameters (batch and epoch) are initialized to 10. \u00a0e basic parameters of the network are shown in Table 1.\nAll LSTM network models for the needs of this work were implemented using libraries from the Keras package (version 2.0.9) [25] together with the TensorFlow environment [26]. Programming was done in Python 3.6."
        },
        {
            "heading": "6. Evaluation Results\u2014CHB-MIT Database",
            "text": "\u00a0e proposed seizure prediction methodology is evaluated using records from the CHB-MIT database and four different scoring windows at 15, 30, 60, and 120minutes before\nTa bl\ne 2:\nC om\npa ri so n of\nth e pr op\nos ed\ncr isi s fo re ca st in g m et ho\ndo lo gy\nw ith\npr ev io us\nst ud\nie s.\nSt ud\ny D at ab as e\nN o. of pa tie nt s\nN um\nbe r of\nse iz ur es\nTo ta ld\nur at io n\nof EE\nG (h ou\nrs )\nEx po\nrt ed\nfe at ur es\nC la ss ifi er\nSe ns\n(% )\nSp ec\n(% )\nFP R (h\n\u2212 1)\nQ ua lif yi ng tim e (m in )\n[2 4]\nC H BM IT\n21 60\n10 .8 0\nPh as e sy nc hr on\niz at io n\nSV M\n82 .4 7\u2217\n82 .7 5\n\u2014 5\n[2 5]\nC H BM IT\n17 75\n64 5\nA bs ol ut e/ re la tiv\ne en er gy\ndi st ri bu\ntio n\nSV M\n98 .6 6\n\u2014 0. 04 5\n55\n[2 6]\nC H BM IT\n10 30\n60 N um\nbe r of\nze ro\npa ss ag es ,s im\nila ri ty /\ndi ss im\nila ri ty\nco effi\nci en t\n\u2014 77 .0 5\n\u2014 0. 16\n55\n[2 7]\nC H BM IT\n3 15\n27 2\nN um\nbe r of\nze ro\npa ss ag es ,s im\nila ri ty /\ndi ss im\nila ri ty\nco effi\nci en t\n\u2014 83 .8 6\n\u2014 0. 16 4\n35\nPr iv at e da ta\n17 65\n28 5\nN um\nbe r of\nze ro\npa ss ag es ,s im\nila ri ty /\ndi ss im\nila ri ty\nco effi\nci en t\n\u2014 91 .2 5\n\u2014 0. 06\n35\n[2 8]\nC H BM IT\n13 12 0\n43 4. 4\nFo ur ie r tr an sf or m\n\u2014 83 .3 3\n\u2014 0. 39 3 85 Pr iv at e da ta 3 15 14 8. 1 Fo ur ie r tr an sf or m \u2014 77 .7 4 \u2014 0. 48 3 85 [2 9] C H BM IT 13 65 31 1. 1 Sp ec tr um im ag es w ith ST FT tr an sf or m C N N 81 .2 0 \u2014 0. 14 5 [3 0] C H BM IT 15 15 70 .4 D isc re te tr an sf or m at io n w ith w av el et s C N N 83 .3 6 \u2014 0. 14 6 15 Pr iv at e da ta 12 14 24 .2 0 D isc re te tr an sf or m at io n w ith w av el et s C N N 93 .3 3 \u2014 0. 12 7 15\n[3 1]\nC H BM IT\n24 17 4\n98 2. 7\nEi ge nv\nal ue s of\nco va ri at e ta bl es\nLD A\n81 .0 7\n60 .0 5\n0. 46 65 87 .0 6 50 .0 3 0. 3 95 89 .0 7 36 .0 1 0. 37 12 5\nC N N -L ST\nM C H BM IT\n24 18 0\n97 8. 2\nSt at ist ic al\nva lu es ,n\num be r of\nze ro\npa ss ag es ,\ndi st in ct\ntr an sf or m .W\nith w av el et ,p\now er\ndi st ri bu\ntio n,\nan d co rr el at io n be tw ee n ch an ne ls\nLS TM\n10 0/ 99 .2 9\u2217\n99 .2 3\n0. 12 10 10 0/ 99 .3 7\u2217 99 .6 7 0. 07 25 10 0/ 99 .6 2\u2217 99 .7 7 0. 04 55 10 0/ 99 .8 1\u2217 99 .8 5 0. 03 11 5\nSV M\n\ufffd su pp\nor tv\nec to rm\nac hi ne ,C\nN N\n\ufffd co nv\nol ut io na ln\neu ra ln\net w or k, LD\nA \ufffd lin\nea rd\nisc ri m in an ta na ly sis\n,S en s\ufffd\nse ns iti vi ty ,S pe c\ufffd\nsp ec ifi ci ty ,F PR\n\ufffd fa lse\npr ed ic tio\nn ra te pe rr ec or di ng\ntim e. \u2217 Ba\nse d on\nsp ec ifi ed\nas se ss m en tfi\nel d.\nseizure onset. +e LSTM network is separately trained and evaluated for crisis prediction on each case from the CHBMIT database. As in the preliminary analysis, the impact of choosing a different length of the LSTM network input sequence on the prediction accuracy is evaluated. +e imbalance between the available sections in the two classes is resolved by dividing the intermediate sections into smaller subgroups of the same size as the classified class, and the results presented show the average of the measurements across all subgroups [27\u201330].\nFor a more complete evaluation, results are presented both for the case of segment-based evaluation and for the ability to predict events as events (event-based evaluation). To calculate the model performance, the following sampling parameters are defined as follows:\n(i) True Positives (TP): +e number of eligible partitions that are correctly classified as eligible.\n(ii) True Negatives (TN): +e number of mesocritical regions that are correctly classified as mesocritical.\n(iii) False Positives (FP): +e number of relevant sections misclassified as mesocritical.\n(iv) False Negatives (FN): +e number of intermediate classes that are incorrectly classified as qualifiers.\nFor an evaluation based on the EEG components, from the above values, the sensitivity and specificity of the model are calculated as follows:\nSensitivity \ufffd TP\nTP + FN ,\nSpecificity \ufffd TN\nTN + FP .\n(9)\nTo assess evidence-based prognosis, each seizure is considered an independent event and sensitivity is defined as the percentage of successfully predicted seizures relative to the total number of seizures for each of 24 CHB-MIT-based cases. For a statement to be considered successful, at least one of its qualifying sections must be scored by the assessor as qualifying. Fact-based scoring also uses the false prediction rate (FPR), which indicates the number of false predictions per EEG.\n6.1. Comparative Analysis. Table 1 presents a comparison of the proposed methodology with the international literature. +e comparison focuses on studies that use the same database (e.g., the CHB-MIT Scalp EEG database). +e proposed LSTM model provided better crisis prediction results than all previous methodologies that were previously evaluated with the same data set and using a similar qualification period. +e exported features and the classification model used in each study are also presented in Table 2. Studies that did not use the classification algorithm but applied seizure prediction rules are marked with a \u201c\u2212.\u201d With the exception of the graph-theoretic features, the other features extracted in the present analysis have previously been used successfully to predict seizures [31]. However, if previous studies\ndid not use a large number of functions, according to the results of this evaluation, their combination provides a significant advantage, since the exported function space contains more and more essential information."
        },
        {
            "heading": "7. Conclusion",
            "text": "+is work is dedicated to the development of artificial intelligence methods to improve the treatment of patients with epilepsy or Parkinson\u2019s disease, the two most common neurological conditions. In the treatment of Parkinson\u2019s disease, the affected patients with Parkinson\u2019s disease can assess the prognostic risk factors, and the symptoms are evaluated to predict rapid progression in the early stages after diagnosis. EEG seizure prediction, the superiority of LSTMs over CNNs has recently been reported in several applications related to EEG analysis. +e presented seizure prediction system introduces deep learning algorithms into EEG score analysis. +is proposed work long short-term memory (LSTM) network model is mainly implemented for the identification and classification of qualitative patterns in the EEG of patients. Compared to simpler classification models, as well as rule-based methodologies that rely on dynamic EEG changes, the proposed LSTM network demonstrates significantly higher overall seizure prediction accuracy.\nOur future work, to enhance the work with optimization scheme based on artificial intelligence methods for accurate detection of patients with epilepsy or Parkinson\u2019s disease with less time consumption."
        },
        {
            "heading": "Data Availability",
            "text": "+e data sets used and/or analyzed during the current study are available from the corresponding author on reasonable request."
        },
        {
            "heading": "Conflicts of Interest",
            "text": "+e authors declare that they have no conflicts of interest."
        }
    ],
    "title": "Effective Evaluation of Medical Images Using Artificial Intelligence Techniques",
    "year": 2022
}