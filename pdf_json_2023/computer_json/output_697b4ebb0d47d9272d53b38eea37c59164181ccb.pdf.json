{
    "abstractText": "An arc-search interior-point method is a type of interior-point methods that approximates the central path by an ellipsoidal arc, and it can often reduce the number of iterations. In this work, to further reduce the number of iterations and computation time for solving linear programming problems, we propose two arc-search interior-point methods using Nesterov\u2019s restarting strategy that is wellknown method to accelerate the gradient method with a momentum term. The first one generates a sequence of iterations in the neighborhood, and we prove that the convergence of the generated sequence to an optimal solution and the computation complexity is polynomial time. The second one incorporates the concept of the Mehrotra type interior-point method to improve numerical performance. The numerical experiments demonstrate that the second one reduced the number of iterations and computational time. In particular, the average number of iterations was reduced compared to existing interior-point methods due to the momentum term.",
    "authors": [
        {
            "affiliations": [],
            "name": "Einosuke Iida"
        },
        {
            "affiliations": [],
            "name": "Makoto Yamashita"
        }
    ],
    "id": "SP:9333527bb36055c89b1a741d3e8ccb77814689ae",
    "references": [
        {
            "authors": [
                "A. Altman",
                "J. Gondzio"
            ],
            "title": "Regularized symmetric indefinite systems in interior point methods for linear and quadratic optimization",
            "venue": "Optimization Methods and Software,",
            "year": 1999
        },
        {
            "authors": [
                "S. Browne",
                "J. Dongarra",
                "E. Grosse",
                "T. Rowan"
            ],
            "title": "The Netlib mathematical software repository",
            "venue": "D-lib Magazine,",
            "year": 1995
        },
        {
            "authors": [
                "M. Colombo",
                "J. Gondzio"
            ],
            "title": "Further development of multiple centrality correctors for interior point methods",
            "venue": "Computational Optimization and Applications,",
            "year": 2008
        },
        {
            "authors": [
                "T. Dozat"
            ],
            "title": "Incorporating Nesterov momentum into adam",
            "venue": "Natural Hazards,",
            "year": 2016
        },
        {
            "authors": [
                "T.A. Espaas",
                "V.S. Vassiliadis"
            ],
            "title": "An interior point framework employing higherorder derivatives of central path-like trajectories: Application to convex quadratic programming",
            "venue": "Computers & Chemical Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "J. Gondzio"
            ],
            "title": "Multiple centrality corrections in a primal-dual method for linear programming",
            "venue": "Computational Optimization and Applications,",
            "year": 1996
        },
        {
            "authors": [
                "N. Gould",
                "J. Scott"
            ],
            "title": "A note on performance profiles for benchmarking software",
            "venue": "ACM Transactions on Mathematical Software (TOMS),",
            "year": 2016
        },
        {
            "authors": [
                "N. Karmarkar"
            ],
            "title": "A new polynomial-time algorithm for linear programming",
            "venue": "Proceedings of the sixteenth annual ACM symposium on Theory of computing,",
            "year": 1984
        },
        {
            "authors": [
                "B. Kheirfam",
                "M. Chitsaz"
            ],
            "title": "Polynomial convergence of two higher order interiorpoint methods for p\u2217(\u03ba)-lcp in a wide neighborhood of the central path",
            "venue": "Periodica Mathematica Hungarica,",
            "year": 2018
        },
        {
            "authors": [
                "M. Kojima"
            ],
            "title": "Basic lemmas in polynomial-time infeasible-interiorpoint methods for linear programs",
            "venue": "Annals of Operations Research,",
            "year": 1996
        },
        {
            "authors": [
                "M. Kojima",
                "S. Mizuno",
                "A. Yoshise"
            ],
            "title": "A primal-dual interior point algorithm for linear programming",
            "venue": "In Progress in Mathematical Programming,",
            "year": 1989
        },
        {
            "authors": [
                "I.J. Lustig",
                "R.E. Marsten",
                "D.F. Shanno"
            ],
            "title": "On implementing mehrotra\u2019s predictor-corrector interior-point method for linear programming",
            "venue": "SIAM journal on Optimization,",
            "year": 1992
        },
        {
            "authors": [
                "S. Mehrotra"
            ],
            "title": "On the implementation of a primal-dual interior point method",
            "venue": "SIAM Journal on Optimization,",
            "year": 1992
        },
        {
            "authors": [
                "J. Miao"
            ],
            "title": "Two infeasible interior-point predictor-corrector algorithms for linear programming",
            "venue": "SIAM Journal on Optimization,",
            "year": 1996
        },
        {
            "authors": [
                "R.D. Monteiro",
                "I. Adler",
                "M.G. Resende"
            ],
            "title": "A polynomial-time primal-dual affine scaling algorithm for linear and convex quadratic programming and its power series extension",
            "venue": "Mathematics of Operations Research,",
            "year": 1990
        },
        {
            "authors": [
                "M.S. Morshed",
                "M. Noor-E-Alam"
            ],
            "title": "Generalized affine scaling algorithms for linear programming problems",
            "venue": "Computers and Operations Research, 114,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Nesterov"
            ],
            "title": "A method of solving a convex programming problem with convergence rate o(1/k2)",
            "venue": "Soviet Math. Doklady,",
            "year": 1983
        },
        {
            "authors": [
                "D. Orban",
                "contributors"
            ],
            "title": "BenchmarkProfiles.jl: A simple Julia package to plot performance and data profiles",
            "venue": "https://github.com/JuliaSmoothOptimizers/ BenchmarkProfiles.jl,",
            "year": 2019
        },
        {
            "authors": [
                "A. Tits",
                "Y. Yang"
            ],
            "title": "Globally convergent algorithms for robust pole assignment by state feedback",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 1996
        },
        {
            "authors": [
                "S.J. Wright"
            ],
            "title": "Primal-dual interior-point methods",
            "venue": "SIAM, PA,",
            "year": 1997
        },
        {
            "authors": [
                "M. Yamashita",
                "K. Fujisawa",
                "M. Fukuda",
                "K. Kobayashi",
                "K. Nakata",
                "M. Nakata"
            ],
            "title": "Latest developments in the SDPA family for solving large-scale SDPs. In Handbook on semidefinite, conic and polynomial optimization, pages 687\u2013713",
            "year": 2012
        },
        {
            "authors": [
                "M. Yamashita",
                "K. Fujisawa",
                "M. Kojima"
            ],
            "title": "Implementation and evaluation of SDPA 6.0 (semidefinite programming algorithm 6.0)",
            "venue": "Optimization Methods and Software,",
            "year": 2003
        },
        {
            "authors": [
                "M. Yamashita",
                "E. Iida",
                "Y. Yang"
            ],
            "title": "An infeasible interior-point arc-search algorithm for nonlinear constrained optimization",
            "venue": "Numerical Algorithms,",
            "year": 2021
        },
        {
            "authors": [
                "X. Yang",
                "H. Liu",
                "Y. Zhang"
            ],
            "title": "An arc-search infeasible-interior-point method for symmetric optimization in a wide neighborhood of the central path",
            "venue": "Optimization Letters,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yang"
            ],
            "title": "A polynomial arc-search interior-point algorithm for convex quadratic programming",
            "venue": "European Journal of Operational Research,",
            "year": 2011
        },
        {
            "authors": [
                "Y. Yang"
            ],
            "title": "Constrained lqr design using interior-point arc-search method for convex quadratic programming with box constraints",
            "venue": "arXiv preprint arXiv:1304.4685,",
            "year": 2013
        },
        {
            "authors": [
                "Y. Yang"
            ],
            "title": "CurveLP-A MATLAB implementation of an infeasible interior-point algorithm for linear programming",
            "venue": "Numerical Algorithms, 74:967\u2013996,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Yang"
            ],
            "title": "Arc-search techniques for interior-point methods",
            "venue": "CRC Press,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yang",
                "M. Yamashita"
            ],
            "title": "An arc-search O(nL) infeasible-interior-point algorithm for linear programming",
            "venue": "Optimization Letters,",
            "year": 2018
        },
        {
            "authors": [
                "B. Yuan",
                "M. Zhang",
                "Z. Huang"
            ],
            "title": "A wide neighborhood primal-dual interiorpoint algorithm with arc-search for linear complementarity problems",
            "venue": "Journal of Nonlinear Functional Analysis, Article ID,",
            "year": 2018
        },
        {
            "authors": [
                "M. Zhang",
                "B. Yuan",
                "Y. Zhou",
                "X. Luo",
                "Z. Huang"
            ],
            "title": "A primal-dual interiorpoint algorithm with arc-search for semidefinite programming",
            "venue": "Optimization Letters,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Keywords: interior-point method, arc-search, Nesterov\u2019s restarting method, linear programming."
        },
        {
            "heading": "1 Introduction",
            "text": "Numerical methods for solving linear programming (LP) problems have been studied well in the literature, since LP problems have wide-range applications including practical ones and theoretical ones. In particular, the simplex method, the gradient method and the interior-point method are well-known methods. The interior-point method was originally proposed by Karmarkar [8] in 1984, and it has been improved by, for example, the primaldual interior-point method [11] and Mehrotra type predictor-corrector method [13]. For more details, see Wright [20] and references therein.\n\u2217Department of Mathematical and Computing Science,Tokyo Institute of Technology \u2020Department of Mathematical and Computing Science,Tokyo Institute of Technology. The research\nof Makoto Yamashita was partially supported by JSPS KAKENHI (Grant Number: 21K11767)\nar X\niv :2\n30 3.\n01 66\n6v 2\n[ m\nat h.\nO C\n] 2\n8 Se\nThe computational efficiency of higher-order algorithms [1,5,6,9,13,15] using secondorder or higher derivatives in framework of interior-point methods has been getting a lot of attention. Mehrotra type predictor-corrector method [13] can be considered as one of the high-order algorithms. Lustig et al. [12] reported that Mehrotra type predictorcorrector method was effective in reducing the computation time. The method has also been implemented in software packages [21, 22]. However, the higher-order algorithms sometimes had a worse polynomial bound, or the analysis of computational complexity was not simple.\nThe arc-search interior-point method [25] proposed for linear programming also employs higher derivatives, and Yang and Yamashita [29] proved that the number of iterations in the arc-search infeasible interior-point method can be bounded by O(nL), which is equal to the best known polynomial bound for the infeasible interior-point algorithms. The arc-search interior-point method has been extended for quadratic programming with box constraints [26], second-order cone programming [24], semidefinite programming [31], complementarity problem [30], and general nonlinear programming [23].\nA main idea of the arc-search interior-point method is to approximate the central path with an ellipsoidal arc and this leads to a reduction of the number of iterations. The central path is a smooth curve that converges to an optimal solution, but standard interior-point methods use a linear approximation to trace the central path and apply a line search. Since the central path is a curve, it can be expected that an ellipsoidal arc can approximate the central path better than the line search with a fixed search direction. In fact, Yang and Yamashita [29] showed through numerical experiments that the arc-search interior-point method reduces the number of iterations for solving LP test instances.\nHowever, the computational time for each iteration in the arc-search interior-point method tends to increase compared to line-search methods due to calculations of the higher-order derivatives. Therefore, reducing the number of iterations further without calculating the higher-order derivatives is one of important issues to attain better computational time.\nOn the other hand, Nesterov\u2019s restarting strategy [17] is widely employed to improve the computation time of first-order methods. Nesterov proved that this technique reduces the worst-case convergence rate of the gradient methods for minimizing smooth convex functions from O(1/k) to O(1/k2). Dozat [4] utilized this technique in a framework of stochastic gradient descent methods in Deep Learning.\nFor LP problems, Morshed and Noor-E-Alam [16] examined a method that combines Nesterov\u2019s restarting strategy with the affine scaling method and proved that their method is polynomial time. In addition, their numerical results showed that their method reduces the computation time compared to a conventional affine scaling method. To the best of the author\u2019s knowledge, however, Nesterov\u2019s restarting strategy has not been introduced in the arc-search interior-point methods.\nIn this paper, we propose a new arc-search method for LP problems combined with Nesterov\u2019s restarting strategy so that the arc-search interior-point method can further reduce the number of iterations. We establish the convergence of a generated sequence of\nthe proposed method to an optimal solution, and we also show that the proposed method is a polynomial-time algorithm. Furthermore, to reduce actual computation time, we modify the proposed method by introducing a concept of the Merhotra type interiorpoint method. From numerical experiments on Netlib test instances [2], we observed that this modified method achieved better performance than existing methods in terms of both the number of iterations and computation time. In particular, the number of iterations is successfully reduced by 5.4% compared to an existing arc-search method.\nThis paper is organized as follows. Section 2 introduces the standard form of LP problems in this paper and gives a brief summary of arc-search interior-point methods. We describe the proposed method in Section 3, and we discuss its convergence properties in Section 4. In Section 5, we modify the proposed method to improve performance in numerical experiments and compare it with existing methods. Finally, we give a conclusion of this paper and discuss future directions in Section 6."
        },
        {
            "heading": "1.1 Notations",
            "text": "We denote i-th element of a vector x by xi and the Hadamard product (the elementwise product) of two vectors x and s by x \u25e6 s. We use \u2225x\u2225 and \u2225x\u2225\u221e for the Euclidean norm and the infinity norm of a vector x, respectively. The identity matrix and the vector of all ones of an appropriate dimension are denoted by I and e, respectively. We use a superscript \u22a4 to denote the transpose of a vector or a matrix. The block column vector [ x\u22a4, s\u22a4 ]\u22a4 is shortly expressed by (x, s). For x \u2208 Rn, we will use the capital character X \u2208 Rn\u00d7n for the diagonal matrix that puts the components of x at its diagonal positions."
        },
        {
            "heading": "2 Preliminaries",
            "text": "We consider a linear programming (LP) problem in the following standard form:\nmin x\u2208Rn\nc\u22a4x, subject to Ax = b, x \u2265 0, (1)\nwhere A \u2208 Rm\u00d7n, b \u2208 Rm, and c \u2208 Rn. The corresponding dual problem for (1) is\nmax \u03bb\u2208Rm, s\u2208Rn\nb\u22a4\u03bb, subject to A\u22a4\u03bb+ s = c, s \u2265 0, (2)\nwhere \u03bb is the dual variable vector, and s is the dual slack vector. Let S denote the set of the optimal solutions of (1) and (2). It is well known that (x\u2217, \u03bb\u2217, s\u2217) \u2208 S if (x\u2217, \u03bb\u2217, s\u2217) satisfies the following KKT conditions:\nAx\u2217 = b (3a)\nA\u22a4\u03bb\u2217 + s\u2217 = c (3b)\n(x\u2217, s\u2217) \u2265 0 (3c) x\u2217i s \u2217 i = 0, i = 1, . . . , n. (3d)\nWe denote the residuals of the equality constraints in (1) and (2) by\nrb(x) = Ax\u2212 b (4a) rc(\u03bb, s) = A \u22a4\u03bb+ s\u2212 c, (4b)\nand the duality measure by\n\u00b5 = xT s\nn .\nIn this paper, we make the following assumption for (1) and (2). This assumption is a mild one and is used in many papers\u2014for example, see [20,28].\nAssumption 2.1.\n(A1) Each of the primal problem (1) and the dual problem (2) has interior feasible points.\n(A2) The row vectors in A are linear independent.\nAssumption 2.1 guarantees the existence of optimal solutions and the boundedness of S\u2014see [28]."
        },
        {
            "heading": "2.1 Arc-search interior-point method",
            "text": "Interior-point methods are iterative methods, and we denote the kth iteration by (xk, \u03bbk, sk) \u2208 Rn \u00d7 Rm \u00d7 Rn; in particular, the initial point is (x0, \u03bb0, s0).\nGiven a strictly positive iteration (xk, \u03bbk, sk) such that (xk, sk) > 0, an infeasible predictor-corrector method [28] traces a smooth curve called an approximate center path:\nC = {(x(t), \u03bb(t), s(t)) | t \u2208 (0, 1]} , (5)\nwhere (x(t), \u03bb(t), s(t)) is the unique solution of the following system\nAx(t)\u2212 b = t rb(xk) (6a) A\u22a4\u03bb(t) + s(t)\u2212 c = t rc(\u03bbk, sk) (6b) x(t) \u25e6 s(t) = t (xk \u25e6 sk) (6c) (x(t), s(t)) > 0. (6d)\nAs t approaches 0, (x(t), \u03bb(t), s(t)) converges to an optimal solution in S. Though the existence of the curve C is guaranteed by the implicit function theorem, it does not admit an analytical form.\nThe key idea in the arc-search type interior-point algorithms [27] is to approximate the curve C with an ellipsoidal arc in the 2n + m dimensional space. An ellipsoidal\napproximation of (x(t), \u03bb(t), s(t)) at (xk, \u03bbk, sk) for an angle \u03b1 \u2208 (0, \u03c0) is explicitly given by (x(\u03b1), \u03bb(\u03b1), s(\u03b1)) with\nx(\u03b1) = x\u2212 x\u0307 sin(\u03b1) + x\u0308(1\u2212 cos(\u03b1)), (7a) \u03bb(\u03b1) = \u03bb\u2212 \u03bb\u0307 sin(\u03b1) + \u03bb\u0308(1\u2212 cos(\u03b1)), (7b) s(\u03b1) = s\u2212 s\u0307 sin(\u03b1) + s\u0308(1\u2212 cos(\u03b1)). (7c)\nHere, the first derivative (x\u0307, \u03bb\u0307, s\u0307) and the second derivative (x\u0308, \u03bb\u0308, s\u0308) are the solutions of the following systems: A 0 00 A\u22a4 I\nSk 0 Xk  x\u0307\u03bb\u0307 s\u0307  =  rb(xk)rc(\u03bbk, sk) xk \u25e6 sk  (8) A 0 00 A\u22a4 I Sk 0 Xk  x\u0308\u03bb\u0308 s\u0308  =  00 \u22122x\u0307 \u25e6 s\u0307\n . (9) In the remainder of this section, we introduce definitions that are necessary to discuss the convergence. In the same way as Yang [28], we introduce the neighborhood of the central path by\nN (\u03b8) := {(x, \u03bb, s) | (x, s) > 0, \u2225x \u25e6 s\u2212 \u00b5e\u2225 \u2264 \u03b8\u00b5} . (10)\nBy generating a sequence in this neighborhoodN (\u03b8), Yang [28] proved that the algorithm converges in polynomial time. Similarly to Miao [14] and Kojima [10], we also choose a sufficiently large parameter \u03c1 \u2265 1 and an initial point ( x0, \u03bb0, s0 ) such that(\nx0, \u03bb0, s0 ) \u2208 N (\u03b8), x\u2217 \u2264 \u03c1x0, s\u2217 \u2264 \u03c1s0 (11)\nfor some (x\u2217, \u03bb\u2217, s\u2217) \u2208 S. Let \u03c9f and \u03c9o be the distances of feasibility and optimality from the initial point as follows:\n\u03c9f = min x,\u03bb,s max {\u2225\u2225\u2225(X0)\u22121 (x\u2212 x0)\u2225\u2225\u2225 \u221e , \u2225\u2225\u2225(S0)\u22121 (s\u2212 s0)\u2225\u2225\u2225 \u221e } | Ax = b, A\u22a4\u03bb+ s = c, (x, s) \u2265 0  (12) \u03c9o = min\nx\u2217,\u03bb\u2217,s\u2217\n{ max { (x\u2217)\u22a4s0\n(x0)\u22a4s0 , (s\u2217)\u22a4x0 (x0)\u22a4s0 , 1\n} | (x\u2217, \u03bb\u2217, s\u2217) \u2208 S } . (13)\nFrom (12), there exists a feasible solution (x\u0304, \u03bb\u0304, s\u0304) satisfying\u2223\u2223x0i \u2212 x\u0304i\u2223\u2223 \u2264 \u03c9fx0i , \u2223\u2223s0i \u2212 s\u0304i\u2223\u2223 \u2264 \u03c9fs0i . (14) In addition, from (13), there is an optimal solution (x\u2217, \u03bb\u2217, s\u2217) \u2208 S satisfying\n(x0)\u22a4s\u2217, (x\u2217)\u22a4s0 \u2264 \u03c9o(x0)\u22a4s0. (15)\nWe will use these definitions to discuss the convergence of the proposed method."
        },
        {
            "heading": "3 The proposed method",
            "text": "We introduce Nesterov\u2019s restarting strategy [17] for the arc-search interior-point method. Nesterov\u2019s restarting strategy has been described in the literature to accelerate convergence in gradient descent algorithms, and Morshed and Noor-E-Alam [16] applied Nesterov\u2019s restarting strategy to an affine scaling method.\nTo combine Nesterov\u2019s restarting strategy into the arc-search interior-point method, we employ the momentum term\n\u03b4(xk) = xk \u2212 xk\u22121 (16)\nwith a strictly positive point x0 > 0, and we construct zk by\nzk =\n{ xk + \u03b2k\u03b4(x k) k > 0 and (xk + \u03b2k\u03b4(x k), \u03bbk, sk) \u2208 N (\u03b8)\nxk otherwise, (17)\nwhere \u03b2k \u2265 0 is a weight of the momentum term which will be calculated by\n\u03b2k = min  \u03b2\u2225\u2225X\u22121k \u03b4(xk)\u2225\u2225\u221e ,min  \u2223\u2223\u2223\u2223 rb(xk)jrb(xk)j \u2212 rb(xk\u22121)j \u2223\u2223\u2223\u2223 for j \u2208 { rb(x k)j \u2212 rb(xk\u22121)j \u0338= 0 }   . (18)\nHere, \u03b2 \u2208 (0, 1) is a parameter that is given before starting algorithms. In (17), zk with the momentum \u03b4(xk) is adopted if it is in the neighborhood N (\u03b8). We use this zk instead of xk to compute the arc in each iteration. The first and the second derivatives at (zk, \u03bbk, sk) with respect to t are given byA 0 00 A\u22a4 I\nSk 0 Zk  z\u0307\u03bb\u0307 s\u0307  =  rb(zk)rc(\u03bbk, sk) zk \u25e6 sk  (19) A 0 00 A\u22a4 I Sk 0 Zk  z\u0308\u03bb\u0308 s\u0308  =  00 \u22122z\u0307 \u25e6 s\u0307,\n , (20) and we denote the duality measure at (zk, \u03bbk, sk) by\n\u00b5zk := (zk)\u22a4sk\nn . (21)\nA framework of the proposed method is given as Algorithm 1.\nAlgorithm 1 Arc-search interior-point method with Nesterov\u2019s restarting strategy Input: a neighborhood parameter \u03b8 \u2208 (0, 1/(2 + \u221a 2)), an initial point (x0, \u03bb0, s0) \u2208\nN (\u03b8), restarting parameter \u03b2 \u2208 (0, 1), and a stopping threshold \u03f5 \u2208 (0, 1). 1: If a stopping criterion is satisfied, then output (xk, \u03bbk, sk) as an optimal solution\n(x\u2217, \u03bb\u2217, s\u2217) and stop. 2: Set \u03b2k by (18). 3: Set zk by (17) and \u00b5zk by (21). 4: Compute (z\u0307, \u03bb\u0307, s\u0307) and (z\u0308, \u03bb\u0308, s\u0308) using (19) and (20), respectively. 5: Find the largest \u03b1k \u2208 (0, \u03c0/2] such that for all \u03b1 \u2208 (0, \u03b1k], the following inequalities\nhold:\nx(\u03b1) = zk \u2212 z\u0307 sin(\u03b1) + z\u0308(1\u2212 cos(\u03b1)) > 0 (22a) s(\u03b1) = sk \u2212 s\u0307 sin(\u03b1) + s\u0308(1\u2212 cos(\u03b1)) > 0 (22b) \u2225(x(\u03b1) \u25e6 s(\u03b1))\u2212 (1\u2212 sin(\u03b1))\u00b5zke\u2225 \u2264 2\u03b8(1\u2212 sin(\u03b1))\u00b5zk (22c)\n6: Set\nx(\u03b1k) = z k \u2212 z\u0307 sin (\u03b1k) + z\u0308 (1\u2212 cos (\u03b1k)) (23a) \u03bb(\u03b1k) = \u03bb k \u2212 \u03bb\u0307 sin (\u03b1k) + \u03bb\u0308 (1\u2212 cos (\u03b1k)) (23b) s(\u03b1k) = s k \u2212 s\u0307 sin (\u03b1k) + s\u0308 (1\u2212 cos (\u03b1k)) (23c)\n7: Calculate (\u2206x,\u2206\u03bb,\u2206s) by solving A 0 00 A\u22a4 I S (\u03b1k) 0 X (\u03b1k)  \u2206x\u2206\u03bb \u2206s  =  00 (1\u2212 sin (\u03b1k))\u00b5ke\u2212 x (\u03b1k) \u25e6 s (\u03b1k)  . (24) 8: Update ( xk+1, \u03bbk+1, sk+1 ) = (x(\u03b1k), y(\u03b1k), s(\u03b1k)) + (\u2206x,\u2206\u03bb,\u2206s) (25)\nand \u00b5k+1 = (x k+1)\u22a4sk+1/n.\n9: Set k \u2190 k + 1. Go back to Step 1."
        },
        {
            "heading": "4 Theoretical proof",
            "text": "We discuss theoretical aspects of Algorithm 1, in particular, we focus on the convergence of the generated sequence and the number of iterations. In this section, let {(xk, \u03bbk, sk)} be the sequence generated by Algorithm 1.\nTo compute the first and the second derivatives at (zk, \u03bbk, sk) uniquely by (19) and\n(20), the following matrix must be nonsingular for all k:A 0 00 A\u22a4 I Sk 0 Zk  . (26) Since A is full rank matrix from Assumption 2.1 and Sk is a positive diagonal matrix due to Proposition 4.1 below, the nonsingularity of (26) is ensured.\nProposition 4.1. There exists L0 > 0 such that s k i \u2265 L0\u00b5k for each i = 1, . . . , n.\nThe proof of Proposition 4.1 will be given later. The following lemma indicates that zk is a positive vector.\nLemma 4.1. Suppose xk and zk are obtained at the kth iteration of Algorithm 1. Then, for all i \u2208 {1, 2, . . . , n},\n(1\u2212 \u03b2)xki \u2264 zki \u2264 (1 + \u03b2)xki . (27)\nProof. If zk = xk, it is clear. When zk = xk + \u03b2k\u03b4(x k), we have\n\u2223\u2223\u03b4(xk)i\u2223\u2223\u2225\u2225X\u22121k \u03b4(xk)\u2225\u2225\u221e = \u2223\u2223\u2223xki \u2212 xk\u22121i \u2223\u2223\u2223\nmaxj \u2223\u2223\u2223\u2223xkj\u2212xk\u22121jxkj \u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223xki \u2212 xk\u22121i \u2223\u2223\u2223 |xki \u2212xk\u22121i | |xki | = \u2223\u2223\u2223xki \u2223\u2223\u2223 = xki ,\nfor all i, where the last equality holds from xk > 0. The definition of \u03b2k (18) indicates\u2223\u2223\u2223\u03b2k\u03b4(xk)i\u2223\u2223\u2223 \u2264 \u03b2 \u2223\u2223\u03b4(xk)i\u2223\u2223\u2225\u2225X\u22121k \u03b4(xk)\u2225\u2225\u221e \u2264 \u03b2xki , so we can get (27) since \u03b2 \u2208 (0, 1).\nFrom (27), we obtain zk \u2265 (1 \u2212 \u03b2)xk > 0. Furthermore, from sk > 0, we can also derive the following corollary.\nCorollary 4.1. Suppose xk, sk and zk are obtained at the kth iteration of Algorithm 1. Then,\n(1\u2212 \u03b2)xki ski \u2264 zki ski \u2264 (1 + \u03b2)xki ski . (28)"
        },
        {
            "heading": "4.1 Convergence of Algorithm 1",
            "text": "To discuss the convergence of the generated sequence by Algorithm 1, we first evaluate the residuals of the equality constraints. Some proofs in this section can be done in similar ways to the reference [28], but will be described in this paper for the sake of completeness, since we are using zk instead of xk.\nTo prove the monotonic decrement of the residuals in the constraints (4), we first show that the signs in the primal constraint remain same.\nLemma 4.2. Let xk and zk be generated by Algorithm 1. Then,\n(i) rb(x k)j \u2265 rb(zk)j \u2265 0 if rb(x0)j \u2265 0,\n(ii) rb(x k)j \u2264 rb(zk)j \u2264 0 if rb(x0)j \u2264 0.\nProof. From (4a), (25), (23a), (20), (24) and (19), it holds that\nrb(x k+1)\u2212 rb(zk) = A ( xk+1 \u2212 zk ) = A ( x(\u03b1k) + \u2206x\u2212 zk ) = A ( zk \u2212 z\u0307 sin (\u03b1k) + z\u0308 (1\u2212 cos (\u03b1k)) + \u2206x\u2212 zk\n) = \u2212Az\u0307 sin(\u03b1k) = \u2212rb(zk) sin(\u03b1k),\ntherefore, we can get rb(x k+1) = rb(z k) (1\u2212 sin(\u03b1k)) . (29)\nWe give the proof by induction on the iteration number k. If k = 0 or zk = xk in (17) is satisfied, thus both (i) and (ii) hold. We assume \u2223\u2223rb(zk)j\u2223\u2223 \u2264 \u2223\u2223rb(xk)j\u2223\u2223 with k = t, and discuss the case of k = t+ 1 and zk = xk + \u03b2k\u03b4(x k). From (4a) and (16), we derive\nrb(z t+1)\u2212 rb(xt+1) = Azt+1 \u2212Axt+1 = \u03b2t+1A\u03b4(xt+1)\n= \u03b2t+1A(x t+1 \u2212 xt) = \u03b2t+1(rb(xt+1)\u2212 rb(xt)). (30)\nWe consider the case of (i). From (29) and rb(x t)j \u2265 rb(zt)j , we know\nrb(x t+1)j \u2212 rb(xt)j = rb(zt)j (1\u2212 sin(\u03b1t))\u2212 rb(xt)j \u2264 \u2212rb(xt)j sin(\u03b1t) \u2264 0,\nthus, we get rb(x t+1)j \u2265 rb(zt+1)j from (30). In addition, we can get rb(xt+1)j \u2265 0 from (29) and rb(z t)j \u2265 0, and\n\u03b2t+1 \u2264 \u2223\u2223rb(xt+1)j\u2223\u2223\n|rb(xt+1)j \u2212 rb(xt)j | from (18), so we obtain\nrb(z t+1)j = rb(x t+1)j + \u03b2t+1(rb(x t+1)j \u2212 rb(xt)j) \u2265 rb(xt+1)j + \u2223\u2223rb(xt+1)j\u2223\u2223\n|rb(xt+1)j \u2212 rb(xt)j | (rb(x\nt+1)j \u2212 rb(xt)j)\n= rb(x t+1)j +\nrb(x t+1)j\nrb(xt)j \u2212 rb(xt+1)j (rb(x\nt+1)j \u2212 rb(xt)j)\n= 0,\ntherefore, we can get rb(x t+1)j \u2265 rb(zt+1)j \u2265 0 and this shows the case of (i). The proof for (ii) can be given similarly.\nNext, we prove the convergence of the residuals of constraints.\nTheorem 4.1. The residuals (4) shrink at a rate of at least (1 \u2212 sin(\u03b1k)) in each iteration, more precisely,\u2223\u2223\u2223rb(xk+1)j\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223rb(xk)j\u2223\u2223\u2223 (1\u2212 sin(\u03b1k)) \u2200j \u2208 {1, . . . ,m}\nrc(\u03bb k+1, sk+1) = rc(\u03bb k, sk) (1\u2212 sin(\u03b1k)) .\nProof. From Lemma 4.2, \u2223\u2223rb(zt)j\u2223\u2223 \u2264 \u2223\u2223rb(xt)j\u2223\u2223 holds regardless of the sign of rb(x0)j .\nHence, from (29), we obtain \u2223\u2223rb(xt+1)j\u2223\u2223 = \u2223\u2223rb(zt)j\u2223\u2223 (1\u2212 sin(\u03b1t)) \u2264 \u2223\u2223rb(xt)j\u2223\u2223 (1\u2212 sin(\u03b1t)) for each j. Lastly, we prove the second part. From (4b), (25), (23), (20), (24) and (19), we know\nrc(\u03bb k+1, sk+1)\u2212 rc(\u03bbk, sk) =A\u22a4 ( \u03bb(\u03b1k) + \u2206\u03bb\u2212 \u03bbk ) + ( s(\u03b1k) + \u2206s\u2212 sk ) =\u2212 ( A\u22a4\u03bb\u0307+ s\u0307 ) sin(\u03b1k) + ( A\u22a4\u03bb\u0308+ s\u0308 ) (1\u2212 cos(\u03b1k))\n+ ( A\u22a4\u2206\u03bb+\u2206s ) =\u2212 rc(\u03bbk, sk) sin(\u03b1k).\nOwing to the momentum term, the residual of every primal constraint can shrink at the ratio of (1\u2212 sin(\u03b1k)) or faster. To evaluate the primal residual rb(zk) more precisely, we introduce the following lemma.\nLemma 4.3. For each k, there exists \u03c4k such that 0 \u2264 \u03c4k \u2264 1 and\nrb(x k+1) = (1\u2212 sin(\u03b1k))\u03c4krb(xk). (31)\nProof. We give a proof by the induction on the iteration number k. When k = 0, rb(x\n1) = (1\u2212 sin(\u03b10))rb(x0), then (31) is satisfied with \u03c40 = 1. For a positive integer t, we assume that (31) holds for any k such that k \u2264 t. The sign of each component of rb(x\nt) does not change due to Lemma 4.2, thus we know \u03c4k \u2265 0 for k \u2264 t. If 1\u2212 sin(\u03b1t) = 0 or \u03c4t = 0 hold, Theorem 4.1 implies rb(xt \u2032 ) = 0 for all t\u2032 \u2265 t. Then, we can choose arbitrary \u03c4t\u2032 from the range 0 \u2264 \u03c4t\u2032 \u2264 1, and we can satisfy (31). From here, therefore, we focus on the case at k = t+1 when 1\u2212 sin(\u03b1k\u2032) > 0 and \u03c4k\u2032 > 0 hold for all k\u2032 \u2264 t.\nFrom (29), we have rb(x t+2) = (1\u2212 sin(\u03b1t+1))rb(zt+1). Due to (17) and (29),\nrb(z t+1) = rb(x t+1) + \u03b2t+1(rb(x t+1)\u2212 rb(xt))\n= rb(x t+1) + \u03b2t+1 ( 1\u2212 1\n(1\u2212 sin(\u03b1t))\u03c4t\n) rb(x t+1).\nFrom 0 < 1\u2212 sin(\u03b1t) \u2264 1, 0 < \u03c4t \u2264 1, and (18), i.e., \u03b2t+1 \u2265 0, we can get\n\u03b2t+1\n( 1\u2212 1\n(1\u2212 sin(\u03b1t))\u03c4t\n) \u2264 \u03b2t+1(1\u2212 1) = 0.\nTherefore, \u03c4t+1 = ( 1 + \u03b2t+1 \u2212 \u03b2t+1(1\u2212sin(\u03b1t))\u03c4t ) satisfies \u03c4t+1 \u2264 1 and\nrb(x t+2) = (1\u2212 sin(\u03b1t+1))rb(zt+1)\n= (1\u2212 sin(\u03b1t+1))\u03c4t+1rb(xt+1).\nOn the other hand, from Lemma 4.2, we know \u03c4t+1 \u2265 0. Hence, there exists \u03c4k such that 0 \u2264 \u03c4k \u2264 1 and (31) when k = t+ 1.\nThe next lemma shows the reduction speed of the duality measure.\nLemma 4.4. Let the sequence {(xk, \u03bbk, sk)} be generated by Algorithm 1. Then, it holds that\n\u00b5k+1 = \u00b5k (1\u2212 sin (\u03b1k)) .\nLemma 4.4 can be proved in the same way as Yang [29, Lemma 3.1], because this lemma does not depend on using zk, therefore we omit the proof.\nFor the following discussions, let\n\u03bdk = k\u22121\u220f i=0 (1\u2212 sin(\u03b1i)), (32a)\n\u03bd\u03c4k = k\u22121\u220f i=0 (1\u2212 sin(\u03b1i))\u03c4i. (32b)\nFrom (31), Theorem 4.1 and Lemma 4.4, we can get\nrb(z k) = \u03c4k\u03bd \u03c4 krb(x 0), (33a)\nrc(\u03bb k, sk) = \u03bdkrc(\u03bb 0, s0), (33b)\n\u00b5k = \u03bdk\u00b50. (33c)\nNext, Lemmas 4.5 and 4.6 show that Algorithm 1 is well-defined, i.e., it continues the iterations with positive step sizes.\nLemma 4.5. Assume (zk, \u03bbk, sk) \u2208 N (\u03b8). Then, there is a \u03b1\u0304k \u2208 (0, \u03c0/2) such that (x(\u03b1), s(\u03b1)) > 0 and (22c) hold for any \u03b1 \u2208 (0, \u03b1\u0304k].\nProof. We can find sk \u25e6 z\u0307 + zk \u25e6 s\u0307 = zk \u25e6 sk and sk \u25e6 z\u0308 + zk \u25e6 s\u0308 = \u22122z\u0307 \u25e6 s\u0307 in the last rows of (19) and (20), respectively. Using sin2(\u03b1)\u2212 2(1\u2212 cos(\u03b1)) = \u2212(1\u2212 cos(\u03b1))2, we have\nx(\u03b1) \u25e6 s(\u03b1) =zk \u25e6 sk \u2212 (sk \u25e6 z\u0307 + zk \u25e6 s\u0307) sin(\u03b1) + (sk \u25e6 z\u0308 + zk \u25e6 s\u0308)(1\u2212 cos(\u03b1)) + z\u0307 \u25e6 s\u0307 sin2(\u03b1)\u2212 (z\u0307 \u25e6 s\u0308+ z\u0308 \u25e6 s\u0307) sin(\u03b1)(1\u2212 cos(\u03b1)) + z\u0308 \u25e6 s\u0308(1\u2212 cos(\u03b1))2\n=zk \u25e6 sk(1\u2212 sin(\u03b1)) + z\u0307 \u25e6 s\u0307(sin2(\u03b1)\u2212 2(1\u2212 cos(\u03b1))) \u2212 (z\u0307 \u25e6 s\u0308+ z\u0308 \u25e6 s\u0307) sin(\u03b1)(1\u2212 cos(\u03b1)) + z\u0308 \u25e6 s\u0308(1\u2212 cos(\u03b1))2\n=zk \u25e6 sk(1\u2212 sin(\u03b1)) + (z\u0308 \u25e6 s\u0308\u2212 z\u0307 \u25e6 s\u0307)(1\u2212 cos(\u03b1))2\n\u2212 (z\u0308 \u25e6 s\u0307+ z\u0307 \u25e6 s\u0308) sin(\u03b1)(1\u2212 cos(\u03b1)).\nTherefore, it holds that\n\u2225(x(\u03b1) \u25e6 s(\u03b1))\u2212 (1\u2212 sin(\u03b1))\u00b5zke\u2225 =\u2225zk \u25e6 sk(1\u2212 sin(\u03b1)) + (z\u0308 \u25e6 s\u0308\u2212 z\u0307 \u25e6 s\u0307)(1\u2212 cos(\u03b1))2\n\u2212 (z\u0308 \u25e6 s\u0307+ z\u0307 \u25e6 s\u0308) sin(\u03b1)(1\u2212 cos(\u03b1))\u2212 (1\u2212 sin(\u03b1))\u00b5zke\u2225 =\u2225(zk \u25e6 sk \u2212 \u00b5zke)(1\u2212 sin(\u03b1)) + (z\u0308 \u25e6 s\u0308\u2212 z\u0307 \u25e6 s\u0307)(1\u2212 cos(\u03b1))2\n\u2212 (z\u0308 \u25e6 s\u0307+ z\u0307 \u25e6 s\u0308) sin(\u03b1)(1\u2212 cos(\u03b1))\u2225 \u2264\u03b8\u00b5zk(1\u2212 sin(\u03b1)) + (\u2225z\u0308 \u25e6 s\u0308\u2225+ \u2225z\u0307 \u25e6 s\u0307\u2225) sin4(\u03b1) + (\u2225z\u0308 \u25e6 s\u0307\u2225+ \u2225z\u0307 \u25e6 s\u0308\u2225) sin3(\u03b1),\nwhere the last inequality was derived by (zk, \u03bbk, sk) \u2208 N (\u03b8) from (17) and 0 \u2264 1 \u2212 cos(\u03b1) \u2264 sin2(\u03b1) for \u03b1 \u2208 (0, \u03c0/2). Clearly, if\nq(\u03b1) := \u2212\u03b8\u00b5zk(1\u2212sin(\u03b1))+(\u2225z\u0308 \u25e6 s\u0308\u2225+\u2225z\u0307 \u25e6 s\u0307\u2225) sin4(\u03b1)+(\u2225z\u0308 \u25e6 s\u0307\u2225+\u2225z\u0307 \u25e6 s\u0308\u2225) sin3(\u03b1) \u2264 0 (34)\nthen (22c) holds. In fact, due to q(0) = \u2212\u00b5zk\u03b8 < 0, q(\u03c0/2) > 0 and continuity, there exists a \u03b1\u0304k \u2208 (0, \u03c0/2] such that (34) is satisfied at any \u03b1 \u2208 (0, \u03b1\u0304k], This guarantees (22c), thus we can derive that\nxi(\u03b1)si(\u03b1) \u2265 (1\u2212 2\u03b8)(1\u2212 sin(\u03b1))\u00b5zk > 0, \u2200\u03b8 \u2208 [0, 0.5), \u2200\u03b1 \u2208 (0, \u03b1\u0304k].\nSince x(\u03b1) and s(\u03b1) are continuous with respect to \u03b1 and (x(0), s(0)) = (xk, sk) > 0, we can get (x(\u03b1), s(\u03b1)) > 0 for any \u03b1 \u2208 (0, \u03b1\u0304k]. Lemma 4.6. Assume (zk, \u03bbk, sk) \u2208 N (\u03b8). If \u03b8 \u2264 1/(2+ \u221a 2), ( xk+1, \u03bbk+1, sk+1 ) \u2208 N (\u03b8) and xk+!, sk+1 > 0.\nWe can prove this lemma in the same way as Yang [29, Theorem 3.1] by applying Lemmas 4.4 and 4.5.\nNext, we prepare several lemmas to prove in Lemma 4.12 that there exists a strictly positive constant \u03b1\u0304 > 0 such that \u03b1k > \u03b1\u0304 through all iterations until the algorithm termination. In Lemmas 4.10 and 4.11 below, we will derive upper bounds for z\u0307, s\u0307, z\u0308, s\u0308. For these lemmas, we first prove the upper bound of \u03c4k\u03bd \u03c4 k (s k)\u22a4x0+ \u03bdk(z k)\u22a4s0 by letting\nx\u0302 = \u03c4k\u03bd \u03c4 kx 0 + (1\u2212 \u03c4k\u03bd\u03c4k )x\u2217 (35a) (\u03bb\u0302, s\u0302) = \u03bdk(\u03bb 0, s0) + (1\u2212 \u03bdk)(\u03bb\u2217, s\u2217), (35b)\nwhere (x\u2217, \u03bb\u2217, s\u2217) \u2208 S is a solution that attains the minimum of (13).\nLemma 4.7. If ( x0, s0 ) is defined by (11), then\n\u03c4k\u03bd \u03c4 k (x 0)\u22a4sk + \u03bdk(s 0)\u22a4zk \u2264\n( 2 \u03c9o\n1\u2212 \u03b2 + 1\n) (zk)\u22a4sk (36)\nfor all k \u2265 0.\nThe proof of Lemma 4.7 can be done with similar steps in [28, Lemma 6.4]. However, since the coefficients of (36) are slightly different from those in [28], we include the proof here.\nProof. Let (x\u0302, \u03bb\u0302, s\u0302) be defined by (35), then from (33a),\nA(x\u0302\u2212 zk) = \u03c4k\u03bd\u03c4krb(x0) + (1\u2212 \u03c4k\u03bd\u03c4k )rb(x\u2217)\u2212 rb(zk) = \u03c4k\u03bd \u03c4 krb(x 0)\u2212 \u03c4k\u03bd\u03c4krb(x0) = 0\ns\u0302\u2212 sk = c\u2212A\u22a4\u03bb\u0302\u2212 (c\u2212A\u22a4\u03bbk) = \u2212A\u22a4 ( \u03bb\u0302\u2212 \u03bbk ) ,\nso we can get ( x\u0302\u2212 zk )\u22a4 ( s\u0302\u2212 sk ) = \u2212 ( A(x\u0302\u2212 zk) )\u22a4 ( \u03bb\u0302\u2212 \u03bbk ) = 0. (37)\nTherefore, we can derive( \u03c4k\u03bd \u03c4 kx 0 + (1\u2212 \u03c4k\u03bd\u03c4k )x\u2217 )\u22a4 sk + ( \u03bdks 0 + (1\u2212 \u03bdk) s\u2217 )\u22a4 zk\n= ( \u03c4k\u03bd \u03c4 kx 0 + (1\u2212 \u03c4k\u03bd\u03c4k )x\u2217 )\u22a4 ( \u03bdks 0 + (1\u2212 \u03bdk) s\u2217 ) + (zk)\u22a4sk. (38)\nFrom \u03bdk \u2208 [0, 1] and (x\u2217, s\u2217) \u2265 0, we can get\n\u03c4k\u03bd \u03c4 k (x 0)\u22a4sk + \u03bdk(s 0)\u22a4zk \u2264 ( \u03c4k\u03bd \u03c4 kx 0 + (1\u2212 \u03c4k\u03bd\u03c4k )x\u2217 )\u22a4 sk + ( \u03bdks 0 + (1\u2212 \u03bdk) s\u2217 )\u22a4 zk\n[\u2235 (38)) = ( \u03c4k\u03bd \u03c4 kx 0 + (1\u2212 \u03c4k\u03bd\u03c4k )x\u2217 )\u22a4 ( \u03bdks 0 + (1\u2212 \u03bdk) s\u2217 ) + (zk)\u22a4sk[\n\u2235 (x\u2217)\u22a4s\u2217 = 0 ]\n= \u03c4k\u03bd \u03c4 k\u03bdk(x 0)\u22a4s0 + \u03c4k\u03bd \u03c4 k (1\u2212 \u03bdk)(x0)\u22a4s\u2217 + (1\u2212 \u03c4k\u03bd\u03c4k )\u03bdk(x\u2217)\u22a4s0\n+(zk)\u22a4sk\n[\u2235 (15)] \u2264 \u03c4k\u03bd\u03c4k\u03bdk(x0)\u22a4s0 + (\u03c4k\u03bd\u03c4k (1\u2212 \u03bdk) + (1\u2212 \u03c4k\u03bd\u03c4k )\u03bdk)\u03c9o(x0)\u22a4s0 +(zk)\u22a4sk [\u2235 \u03c9o \u2265 1] \u2264 (\u03c4k\u03bd\u03c4k\u03bdk + \u03c4k\u03bd\u03c4k (1\u2212 \u03bdk) + (1\u2212 \u03c4k\u03bd\u03c4k )\u03bdk)\u03c9o(x0)\u22a4s0 + (zk)\u22a4sk [\u2235 0 \u2264 \u03c4k \u2264 1 and (32)] \u2264 2\u03bdk\u03c9o(x0)\u22a4s0 + (zk)\u22a4sk [\u2235 (33c)] = 2\u03c9o(xk)\u22a4sk + (zk)\u22a4sk\n[\u2235 (28)] \u2264 ( 2 \u03c9 o 1\u2212\u03b2 + 1 ) (zk)\u22a4sk.\nThis completes the proof.\nFor the latter discussions, letDk = ( Zk ) 1 2 ( Sk )\u2212 1 2 . We introduce the following lemma\nto prove that \u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225 and \u2225\u2225(Dk) s\u0307\u2225\u2225 have upper bounds. Lemma 4.8. For i = 1, 2, 3, let ( \u03b4zi, \u03b4\u03bbi, \u03b4si\n) be the solution of A 0 00 A\u22a4 I\nSk 0 Zk  \u03b4zi\u03b4\u03bbi \u03b4si  =  00 ri  (39)\nwhere r1 = zk \u25e6 sk, r2 = \u2212\u03c4k\u03bd\u03c4ksk \u25e6 (x0 \u2212 x\u0304) and r3 = \u2212\u03bdkzk \u25e6 (s0 \u2212 s\u0304). Then,\u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z1\u2225\u2225\u2225\u2225 ,\u2225\u2225\u2225(Dk) \u03b4s1\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z1 + (Dk) \u03b4s1\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225(Zksk) 12\u2225\u2225\u2225 = \u221an\u00b5zk, (40a)\u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z2\u2225\u2225\u2225\u2225 ,\u2225\u2225\u2225(Dk) \u03b4s2\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z2 + (Dk) \u03b4s2\u2225\u2225\u2225\u2225 = \u03c4k\u03bd\u03c4k \u2225\u2225\u2225\u2225(Dk)\u22121 (x0 \u2212 x\u0304)\u2225\u2225\u2225\u2225 , (40b)\u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225\u2225 ,\u2225\u2225\u2225(Dk) \u03b4s3\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z3 + (Dk) \u03b4s3\u2225\u2225\u2225\u2225 = \u03bdk \u2225\u2225\u2225(Dk) (s0 \u2212 s\u0304)\u2225\u2225\u2225 . (40c)\nProof. From the first and second rows of (39),(( Dk )\u22121 \u03b4zi )\u22a4 (( Dk ) \u03b4si ) = 0\nfor i = 1, 2, 3, therefore,\u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4zi\u2225\u2225\u2225\u22252 ,\u2225\u2225\u2225(Dk) \u03b4si\u2225\u2225\u22252 \u2264 \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4zi\u2225\u2225\u2225\u22252 + \u2225\u2225\u2225(Dk) \u03b4si\u2225\u2225\u22252 \u2264\n\u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4zi + (Dk) \u03b4si\u2225\u2225\u2225\u22252 . (41) Applying (ZkSk)\u22121/2(Sk\u03b4zi+Zk\u03b4si) = (ZkSk)\u22121/2ri to (41) for i = 1, 2, 3, respectively, we obtain (40).\nIn Lemmas 4.9 and 4.10 below, upper bounds of \u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225 and \u2225\u2225(Dk) s\u0307\u2225\u2225 can be\nderived. This allows us to prove the lower bound of sin(\u03b1k) in the same way as in [29].\nLemma 4.9. Let (z\u0307, s\u0307) be defined by (19). Then,\nmax {\u2225\u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225(Dk) s\u0307\u2225\u2225\u2225} \u2264\u221an\u00b5kz + \u03c9f (2 \u03c9o1\u2212 \u03b2 + 1 ) ( zk )\u22a4 sk\nmini \u221a zki s k i . (42)\nProof. Let (x\u0304, \u03bb\u0304, s\u0304) be the feasible solution of (1) and (2) that attains the minimum of (12). From (33a),\nAz\u0307 = rb(z k) = \u03c4k\u03bd \u03c4 krb(x 0),\nwe have A ( z\u0307 \u2212 \u03c4k\u03bd\u03c4k (x0 \u2212 x\u0304) ) = 0. (43)\nSimilarly, since\nA\u22a4\u03bb\u0307+ s\u0307 = rc(\u03bb k, sk) = \u03bdk ( A\u22a4\u03bb0 + s0 \u2212 c ) = \u03bdk ( A\u22a4 ( \u03bb0 \u2212 \u03bb\u0304 ) + ( s0 \u2212 s\u0304 )) ,\nit holds that\nA\u22a4 ( \u03bb\u0307\u2212 \u03bdk ( \u03bb0 \u2212 \u03bb\u0304 )) + ( s\u0307\u2212 \u03bdk ( s0 \u2212 s\u0304 )) = 0. (44)\nFrom the third row of (19), we also have sk \u25e6 ( z\u0307 \u2212 \u03c4k\u03bd\u03c4k ( x0 \u2212 x\u0304 )) + zk \u25e6 ( s\u0307\u2212 \u03bdk ( s0 \u2212 s\u0304 )) = zk \u25e6 sk \u2212 \u03c4k\u03bd\u03c4ksk \u25e6 ( x0 \u2212 x\u0304 ) \u2212 \u03bdkzk \u25e6 ( s0 \u2212 s\u0304 ) . (45)\nBy combining (43), (44) and (45) into a matrix form, we have A 0 00 A\u22a4 I Sk 0 Zk  z\u0307 \u2212 \u03c4k\u03bd\u03c4k ( x0 \u2212 x\u0304 ) \u03bb\u0307\u2212 \u03bdk ( \u03bb0 \u2212 \u03bb\u0304 ) s\u0307\u2212 \u03bdk ( s0 \u2212 s\u0304 )  =  00 zk \u25e6 sk \u2212 \u03c4k\u03bd\u03c4ksk \u25e6 ( x0 \u2212 x\u0304 ) \u2212 \u03bdkzk \u25e6 ( s0 \u2212 s\u0304 )  .\nPutting\n\u03b4z1 + \u03b4z2 + \u03b4z3 = z\u0307 \u2212 \u03c4k\u03bd\u03c4k ( x0 \u2212 x\u0304 ) \u03b4\u03bb1 + \u03b4\u03bb2 + \u03b4\u03bb3 = \u03bb\u0307\u2212 \u03bdk ( \u03bb0 \u2212 \u03bb\u0304\n) \u03b4s1 + \u03b4s2 + \u03b4s3 = s\u0307\u2212 \u03bdk ( s0 \u2212 s\u0304\n) (r1, r2, r3) = ( zk \u25e6 sk,\u2212\u03c4k\u03bd\u03c4ksk \u25e6 (x0 \u2212 x\u0304),\u2212\u03bdkzk \u25e6 (s0 \u2212 s\u0304) ) ,\ninto (39), we apply Lemma 4.8 to obtain the upper bounds of\u2225\u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225(Dk)\u22121 [\u03b4z1 + \u03b4z2 + \u03b4z3 + \u03c4k\u03bd\u03c4k (x0 \u2212 x\u0304)]\u2225\u2225\u2225\u2225 , (46)\u2225\u2225\u2225(Dk)s\u0307\u2225\u2225\u2225 = \u2225\u2225\u2225(Dk) [\u03b4s1 + \u03b4s2 + \u03b4s3 + \u03bdk (s0 \u2212 s\u0304)]\u2225\u2225\u2225 . (47) Considering (39) with i = 2, we can get\nSk\u03b4z2 + Zk\u03b4s2 = r2 = \u2212\u03c4k\u03bd\u03c4kSk ( x0 \u2212 x\u0304 ) ,\nand this is equal to\n\u03b4z2 = \u2212\u03c4k\u03bd\u03c4k ( x0 \u2212 x\u0304 ) \u2212 ( Dk )2 \u03b4s2. (48)\nThus, from (46) and (48), we have\u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225 = \u2225\u2225\u2225(Dk)\u22121 \u03b4z1 \u2212 (Dk) \u03b4s2 + (Dk)\u22121 \u03b4z3\u2225\u2225\u2225 \u2264\n\u2225\u2225\u2225(Dk)\u22121 \u03b4z1\u2225\u2225\u2225+ \u2225\u2225(Dk) \u03b4s2\u2225\u2225+ \u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225 . (49) Similarly, for i = 3, we have\nS\u03b4z3 + Z\u03b4s3 = r3 = \u2212\u03bdkZk ( s0 \u2212 s\u0304 ) ,\nwhich is equivalent to \u03b4s3 = \u2212\u03bdk ( s0 \u2212 s\u0304 ) \u2212 (Dk)\u22122\u03b4z3. (50)\nThus, from (47) and (50), we have\u2225\u2225(Dk)s\u0307\u2225\u2225 = \u2225\u2225\u2225(Dk) \u03b4s1 + (Dk) \u03b4s2 \u2212 (Dk)\u22121 \u03b4z3\u2225\u2225\u2225 \u2264\n\u2225\u2225(Dk) \u03b4s1\u2225\u2225+ \u2225\u2225(Dk) \u03b4s2\u2225\u2225+ \u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225 . (51) From (40a), \u2225\u2225(Dk)\u03b4s1\u2225\u2225 ,\u2225\u2225\u2225(Dk)\u22121 \u03b4z1\u2225\u2225\u2225 \u2264\u221an\u00b5kz . (52) Using (40b), it holds that\u2225\u2225\u2225(Dk) \u03b4s2\u2225\u2225\u2225 \u2264 \u03c4k\u03bd\u03c4k \u2225\u2225\u2225\u2225(Dk)\u22121 (x0 \u2212 x\u0304)\u2225\u2225\u2225\u2225\n\u2264 \u03c4k\u03bd\n\u03c4 k min \u221a zki s k i \u2225\u2225\u2225Sk (x0 \u2212 x\u0304)\u2225\u2225\u2225 [\u2235 (14)] \u2264 \u03c4k\u03bd \u03c4 k\u03c9 f\nmin \u221a zki s k i \u2225\u2225\u2225Skx0\u2225\u2225\u2225 [\u2235 (\u2225\u00b7\u22252 \u2264 \u2225\u00b7\u22251)] \u2264 \u03c4k\u03bd \u03c4 k\u03c9 f\nmin \u221a zki s k i\n( (sk)\u22a4x0 ) . (53)\nUsing (40c), we have \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225\u2225 \u2264 \u03bdk \u2225\u2225\u2225(Dk) (s0 \u2212 s\u0304)\u2225\u2225\u2225 \u2264 \u03bdk\nmin \u221a zki s k i \u2225\u2225\u2225Zk (s0 \u2212 s\u0304)\u2225\u2225\u2225 [\u2235 (14)] \u2264 \u03bdk\u03c9 f\nmin \u221a zki s k i \u2225\u2225\u2225Zks0\u2225\u2225\u2225 [\u2235 (\u2225\u00b7\u22252 \u2264 \u2225\u00b7\u22251)] \u2264 \u03bdk\u03c9 f\nmin \u221a zki s k i\n( (zk)\u22a4s0 ) . (54)\nTherefore, from (53), (54) and (36),\u2225\u2225\u2225(Dk) \u03b4s2\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225\u2225 \u2264 \u03c9f mini \u221a zki s k i ( \u03c4k\u03bd \u03c4 k (s k)\u22a4x0 + \u03bdk(z k)\u22a4s0 )\n\u2264 \u03c9 f\nmini \u221a zki s k i\n( 2 \u03c9o\n1\u2212 \u03b2 + 1\n) (zk)\u22a4sk. (55)\nTherefore, from (49) and (51),\nmax {\u2225\u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225\u2225 ,\u2225\u2225\u2225(Dk) s\u0307\u2225\u2225\u2225} \u2264max{\u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z1\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225(Dk) \u03b4s1\u2225\u2225\u2225} + \u2225\u2225\u2225(Dk) \u03b4s2\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225\u2225\n[\u2235 (52)] \u2264 \u221a n\u00b5kz + \u2225\u2225\u2225(Dk) \u03b4s2\u2225\u2225\u2225+ \u2225\u2225\u2225\u2225(Dk)\u22121 \u03b4z3\u2225\u2225\u2225\u2225 [\u2235 (55)] \u2264 \u221a n\u00b5kz + \u03c9 f ( 2 \u03c9o\n1\u2212 \u03b2 + 1\n) ( zk )\u22a4 sk\nmini \u221a zki s k i .\nThe upper bounds derived in Lemma 4.7 and 4.9 depend on the value of \u03b2. If \u03b2 is very close to 1, the upper bounds would be very large. Conversely, if it is close to 0, the momentum term has only a small effect. For the proof, we use \u03b2 \u2208 (0, 1), and we will discuss the effect of \u03b2 numerically through numerical results in Section 5.4.1 and 5.4.3.\nLemma 4.9 leads to the following lemma.\nLemma 4.10. Let the sequence {(xk, \u03bbk, sk)} be generated by Algorithm 1 and let (z\u0307, \u03bb\u0307, s\u0307) be obtained from (19). Then, there is a positive constant C0 satisfying\nmax {\u2225\u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225\u2225 , \u2225\u2225\u2225(Dk) s\u0307\u2225\u2225\u2225} \u2264 C0\u221an (zk)\u22a4 sk, (56) and independent of n.\nLemma 4.10 can be proved by replacing xk and x\u0307 in [28, Lemma 6.7] by zk and z\u0307. In this case, C0 is taken such that\nC0 \u2265 1 + \u03c9f\n( 2 \u03c9 o 1\u2212\u03b2 + 1 )\n\u221a (1\u2212 \u03b8) .\nFrom Lemma 4.10, we can give an upper bound on the norm of the search directions.\nLemma 4.11. Let (z\u0307, \u03bb\u0307, s\u0307) and (z\u0308, \u03bb\u0308, s\u0308) be calculated by (19) and (20). Then, there are positive constants C1, C2, C3, and C4 satisfying\n\u2225z\u0307 \u25e6 s\u0307\u2225 \u2264 C1n2\u00b5zk, (57) \u2225z\u0308 \u25e6 s\u0308\u2225 \u2264 C2n4\u00b5zk, (58)\nmax {\u2225\u2225\u2225\u2225(Dk)\u22121 z\u0308\u2225\u2225\u2225\u2225 ,\u2225\u2225\u2225(Dk) s\u0308\u2225\u2225\u2225} \u2264 C3n2\u221a\u00b5zk, (59) max{\u2225z\u0308 \u25e6 s\u0307\u2225 , \u2225z\u0307 \u25e6 s\u0308\u2225} \u2264 C4n3\u00b5zk. (60)\nIn addition, C1, C2, C3, and C4 are independent of n.\nWe can apply the same discussion as Yang [29, Lemma 4.4] to prove the above lemma, by replacing x and \u00b5k with z and \u00b5 z k, respectively. The above lemma allows us to make an estimation regarding the lower bound of sin (\u03b1k).\nLemma 4.12. Let the sequence {(xk, \u03bbk, sk)} be generated by Algorithm 1. Then, \u03b1k satisfies the following inequality:\nsin (\u03b1k) \u2265 \u03b8\n2Cn ,\nwhere C = max { 1, C 1 3 4 , (C1 + C2) 1 4 } .\nThe proof of Lemma 4.12 can be done in a similar way to [29, Lemma 4.5] with a replacement \u00b5k by \u00b5 z k.\nLemma 4.12 enables to establish polynomial-time complexity from the same argument as in [29]. We consider the following stopping criterion at Step 1 of Algorithm 1:\n\u00b5k \u2264 \u03f5, \u2225\u2225\u2225rb(xk)\u2225\u2225\u2225 \u2264 \u2225\u2225rb(x0)\u2225\u2225\n\u00b50 \u03f5, \u2225\u2225\u2225rc(\u03bbk, sk)\u2225\u2225\u2225 \u2264 \u2225\u2225rc(\u03bb0, s0)\u2225\u2225 \u00b50 \u03f5. (61)\nTheorem 4.2. Let {(xk, \u03bbk, sk)} be generated by Algorithm 1 with an initial point given by (11) and L = ln(\u00b50/\u03f5) for a given \u03f5 > 0. Then, the algorithm will terminate with (xk, \u03bbk, sk) satisfying (61) in at most O(nL) iterations.\nThe proof of Theorem 4.2 is the essentially same as Wright [20, Theorem 3.2] as well as [29, Theorem 4.1]. Therefore, Algorithm 1 reaches a solution up to the stopping threshold \u03f5."
        },
        {
            "heading": "4.2 Proof for Proposition 4.1",
            "text": "Lastly, we give the proof of Proposition 4.1 for nonsingularity of (26). For this purpose, since (xk, \u03bbk, sk) is in the neighborhood N (\u03b8), we prove that there is a lower bound that is greater than 0 on sk by finding an upper bound for xk.\nLemma 4.13. There is a positive constant \u03ba such that\u2225\u2225\u2225xk\u2225\u2225\u2225 1 \u2264 \u03ba\nfor each k.\nThe proof of Lemma 4.13 is given similarly to [20, Lemma 6.3].\nProof. Similarly to (37), we obtain\n(\u03bd\u03c4kx 0 + (1\u2212 \u03bd\u03c4k )x\u2217 \u2212 xk)\u22a4(s\u0302\u2212 sk) = 0,\nwhere s\u0302 is defined by (35b). Therefore,\n\u03bdk(x k)\u22a4s0 + \u03bd\u03c4k (x 0)\u22a4sk = \u03bd\u03c4k\u03bdk(x 0)\u22a4s0 + \u03bd\u03c4k (1\u2212 \u03bdk)(x0)\u22a4s\u2217 + \u03bdk(1\u2212 \u03bd\u03c4k )(x\u2217)\u22a4s0 + (xk)\u22a4sk\n\u2212 (1\u2212 \u03bd\u03c4k )(x\u2217)\u22a4sk \u2212 (1\u2212 \u03bdk)(xk)\u22a4s\u2217 + (1\u2212 \u03bdk)(1\u2212 \u03bd\u03c4k )(x\u2217)\u22a4s\u2217[ \u2235 (x\u2217)\u22a4s\u2217 = 0 ] = \u03bd\u03c4k\u03bdk(x 0)\u22a4s0 + \u03bd\u03c4k (1\u2212 \u03bdk)(x0)\u22a4s\u2217 + \u03bdk(1\u2212 \u03bd\u03c4k )(x\u2217)\u22a4s0\n+ (xk)\u22a4sk \u2212 (1\u2212 \u03bd\u03c4k )(x\u2217)\u22a4sk \u2212 (1\u2212 \u03bdk)(xk)\u22a4s\u2217\n[\u2235 xk, sk, x\u2217, s\u2217 \u2265 0] \u2264 \u03bd\u03c4k\u03bdk(x0)\u22a4s0 + \u03bd\u03c4k (1\u2212 \u03bdk)(x0)\u22a4s\u2217 + \u03bdk(1\u2212 \u03bd\u03c4k )(x\u2217)\u22a4s0 + (xk)\u22a4sk.\nLet us now define the constant \u03be by\n\u03be = min i\ns0i .\nBecause s0 > 0, we have \u03be > 0, thus, \u03be \u2225\u2225\u2225xk\u2225\u2225\u2225\n1 \u2264 min i (s0i ) \u2225\u2225\u2225xk\u2225\u2225\u2225 1 \u2264 (xk)\u22a4s0.\nFrom \u03bd\u03c4k \u2265 0, x0, sk \u2265 0, we have\n\u03bdk(x k)\u22a4s0 + \u03bd\u03c4k (x 0)\u22a4sk \u2265 \u03bdk(xk)\u22a4s0 \u2265 \u03bdk\u03be \u2225\u2225\u2225xk\u2225\u2225\u2225\n1 ,\nso it holds that \u03bdk\u03be \u2225\u2225\u2225xk\u2225\u2225\u2225\n1 \u2264 \u03bd\u03c4k\u03bdkn\u00b50 + \u03bd\u03c4k (1\u2212 \u03bdk) \u2225\u2225x0\u2225\u2225\u221e \u2225s\u2217\u22251 + \u03bdk(1\u2212 \u03bd\u03c4k ) \u2225x\u2217\u22251 \u2225\u2225s0\u2225\u2225\u221e + n\u00b5k. Furthermore, (33c) and \u03bd\u03c4k \u2264 \u03bdk \u2264 1 lead to\n\u03be \u2225\u2225\u2225xk\u2225\u2225\u2225\n1 \u2264 \u03bd\u03c4kn\u00b50 + \u03bd\u03c4k \u03bdk (1\u2212 \u03bdk) \u2225\u2225x0\u2225\u2225\u221e \u2225s\u2217\u22251 + (1\u2212 \u03bd\u03c4k ) \u2225x\u2217\u22251 \u2225\u2225s0\u2225\u2225\u221e + n\u00b50\n\u2264 2n\u00b50 + (1\u2212 \u03bdk) \u2225\u2225x0\u2225\u2225\u221e \u2225s\u2217\u22251 + (1\u2212 \u03bd\u03c4k ) \u2225x\u2217\u22251 \u2225\u2225s0\u2225\u2225\u221e\n\u2264 2n\u00b50 + \u2225\u2225x0\u2225\u2225\u221e \u2225s\u2217\u22251 + \u2225x\u2217\u22251 \u2225\u2225s0\u2225\u2225\u221e .\nThe proof is completed by setting \u03ba = \u03be\u22121 ( 2n\u00b50 + \u2225\u2225x0\u2225\u2225\u221e \u2225s\u2217\u22251 + \u2225x\u2217\u22251 \u2225\u2225s0\u2225\u2225\u221e) . Based on the above, we prove the Proposition 4.1.\nProof of Proposition 4.1. From (xk, \u03bbk, sk) \u2208 N (\u03b8), we obtain\nxki s k i \u2265 (1\u2212 \u03b8)\u00b5k.\nSince there is the constant \u03ba such that xki \u2264 \u03ba from Lemma 4.13 and xk > 0, we have\nski \u2265 (1\u2212 \u03b8)\u00b5k 1\nxki \u2265 (1\u2212 \u03b8)\u00b5k\n1 \u03ba > 0. (62)\nTherefore, Proposition 4.1 holds by L0 = (1\u2212 \u03b8)/\u03ba."
        },
        {
            "heading": "5 Numerical experiments",
            "text": "We conducted numerical experiments to compare the proposed method with the existing methods, i.e., arc-search method and line-search method. In addition, we also investigate the effect of the restarting parameter \u03b2. The numerical experiments were executed on a Linux server with Opteron 4386 (3.10GHz), 16 cores and 128 GB RAM. We implemented the methods with Python 3.10.9.\nIn the following, an existing arc-search interior-point method [27] is called \u201cArcsearch\u201d and an existing Mehortra type line-search interior-point method [13] \u201cLinesearch\u201d. Both of these implementations are built upon the algorithms described in the referenced papers."
        },
        {
            "heading": "5.1 Test problems",
            "text": "For the test problems, we used the Netlib test repository [2]. We chose 85 problems in the experiment 1.\nThe Netlib instances are given in the following format:\nmin xb\nc\u22a4b xb\nsubject to AExb = bE , AGxb \u2265 bG, ALxb \u2264 bL, xb \u2265 bLO, xb \u2264 bUP .\n(There exist cases where bLO = bUP .) To transform it into the form of (1), we employ the following setting:\nx =  xb \u2212 bLO\nsG sL sB\n , A = \nAE 0 0 0 AG \u2212I 0 0 AL 0 I 0 IB 0 0 I\n , b = \nbE \u2212AEbLO bG \u2212AGbLO bL \u2212ALbLO bUP \u2212 bLO\n , c = \ncb 0 0 0  where sG, sL, and sB are slack variables for inequality constraints such that\nAGxb \u2212 sG = bG, ALxb + sL = bL, xb + sB = bUP .\nThough another setting like xb \u2212 sLO = bLO can be considered, the above setting has fewer constraints.\nIn addition, we apply the preprocessing described in [27, Section 4.2] to each problem. Therefore, the size (n,m) of each problem in this paper is not always the same as the size reported at the Netlib webpage [2]. The variable size n ranges from 51 to 9,253, and the number of constraints m from 27 to 4,523.\n1We excluded the following problems, since they were too large or some infeasibility was detected at a preprocessing stage: 80BAU3B, BLEND, CRE-B, CRE-D, DEGEN2, DFL001, E226, FIT2D, FIT2P, FORPLAN, GFRD-PNC, GROW7, GROW15, GROW22, KEN-11, KEN-13, KEN-18, NESM, OSA-07, OSA-14, OSA-30, OSA-60, PDS-06, PDS-10, PDS-20, QAP15, SCORPION, SIERRA, STOCFOR3."
        },
        {
            "heading": "5.2 The modified algorithm",
            "text": "To improve practical performance of Algorithm 1, we modify it into Algorithm 2 with the Mehrotra type implementation [13] similarly to [27, Algorithm 3.1]. The comparison of numerical experiments in Section 5.4.1 will show that Algorithm 2 is numerically superior to Algorithm 1. In the following, we discuss the main differences.\nBased on the concept of Mehrotra type implementation, we slightly perturb the second derivative (20) to prevent the step size from diminishing to zero:A 0 00 A\u22a4 I\nSk 0 Zk  z\u0308\u03bb\u0308 s\u0308  =  00\n\u03c3k\u00b5 z ke\u2212 2z\u0307 \u25e6 s\u0307  . (63) Here \u03c3k \u2208 (0, 0.5] is called the centering parameter, and is selected in the same way as Mehrotra [13].\nAs for the setting of \u03b2k, we use two formulas of computation: the formula in (18) and its simplified formula:\n\u03b2k = \u03b2\u2225\u2225X\u22121k \u03b4(xk)\u2225\u2225\u221e . (64)\nAlthough the term \u2223\u2223\u2223 rb(xk)jrb(xk)j\u2212rb(xk\u22121)j \u2223\u2223\u2223 was used in the proof, we mainly use (64) for setting \u03b2k because (64) was more efficient in preliminary numerical experiments. A comparison of the difference in \u03b2k will be reported in Section 5.4.2.\nIn addition, Algorithm 2 does not use the neighborhood N (\u03b8), thus zk is evaluated as follows:\nzk = xk + \u03b2k\u03b4(x k). (65)\nIn other words, the momentum term is employed in all the iterations.\nAlgorithm 2Mehrotra type arc-search interior-point method with Nesterov\u2019s restarting Input: an initial point (x0 > 0, \u03bb0, s0 > 0), restarting parameter \u03b2 \u2208 (0, 1). 1: If (xk, \u03bbk, sk) satisfies a stopping criterion, output (xk, \u03bbk, sk) as an optimal solution\nand stop. 2: Set \u03b2k. 3: Set zk using (65) and \u00b5zk using (21). 4: Calculate (z\u0307, \u03bb\u0307, s\u0307) using (19), and set\n\u03b1za := argmax{\u03b1 \u2208 [0, 1] | zk \u2212 \u03b1z\u0307 \u2265 0} \u03b1sa := argmax{\u03b1 \u2208 [0, 1] | sk \u2212 \u03b1s\u0307 \u2265 0}.\n5: Calculate \u00b5a = ( zk \u2212 \u03b1zaz\u0307 )\u22a4 ( sk \u2212 \u03b1sas\u0307 ) /n and the centering parameter \u03c3k =\n(\u00b5a/\u00b5 z k) 3. 6: Calculate (z\u0308, \u03bb\u0308, s\u0308) using (63) and set\n\u03b1zmax = argmax { \u03b1 \u2208 [ 0, \u03c0\n2\n] | zk \u2212 z\u0307 sin(\u03b1) + z\u0308(1\u2212 cos(\u03b1)) \u2265 0 } (66a)\n\u03b1smax = argmax { \u03b1 \u2208 [ 0, \u03c0\n2\n] | sk \u2212 s\u0307 sin(\u03b1) + s\u0308(1\u2212 cos(\u03b1)) \u2265 0 } . (66b)\n7: If\nxk+1max = z k \u2212 z\u0307 sin (\u03b1zmax) + z\u0308 (1\u2212 cos (\u03b1zmax)) \u2265 0 \u03bbk+1max = \u03bb k \u2212 \u03bb\u0307 sin (\u03b1smax) + \u03bb\u0308 (1\u2212 cos (\u03b1smax)) sk+1max = s k \u2212 s\u0307 sin (\u03b1smax) + s\u0308 (1\u2212 cos (\u03b1smax)) \u2265 0\nsatisfies the stopping criterion, output (xk+1max, \u03bb k+1 max, s k+1 max) as an optimal solution and\nstop. 8: Set step scaling factor \u03b3k \u2208 (0, 1), scale step size \u03b1zk = \u03b3k\u03b1zmax, \u03b1sk = \u03b3k\u03b1smax and\nupdate\nxk+1 = zk \u2212 z\u0307 sin (\u03b1zk) + z\u0308 (1\u2212 cos (\u03b1zk)) > 0 (68a) \u03bbk+1 = \u03bbk \u2212 \u03bb\u0307 sin (\u03b1sk) + \u03bb\u0308 (1\u2212 cos (\u03b1sk)) (68b) sk+1 = sk \u2212 s\u0307 sin (\u03b1sk) + s\u0308 (1\u2212 cos (\u03b1sk)) > 0. (68c)\n9: Set \u00b5k+1 = (x k+1)\u22a4sk+1/n. k \u2190 k + 1. Go back to Step 1."
        },
        {
            "heading": "5.3 Implementation details",
            "text": "In the following, we discuss more details on implementation. In Algorithm 1, we set \u03b8 = 0.25."
        },
        {
            "heading": "5.3.1 Initial points",
            "text": "In Algorithm 1, the components of x0 and s0 are all set to 100 and those of \u03bb0 is all set to 0 to ensure that the initial point (x0, \u03bb0, s0) is in neighborhood N (\u03b8). In contrast, the initial point is set the same as Yang [27, Section 4.1] in Algorithm 2, Arc-search and Line-search."
        },
        {
            "heading": "5.3.2 Step size",
            "text": "In Algorithm 1, since it is difficult to obtain the solution of (22) analytically, Armijo\u2019s rule [20] is employed to determine an actual step size \u03b1k that satisfies (22). In Algorithm 2, we decide \u03b1zmax and \u03b1 s max by (66) on the same strategy as Yang [27, Section 4.8]. The step scaling factor \u03b3k in Algorithm 2 is calculated as \u03b3k = 0.9 for guaranteeing the positiveness of xk and sk. Although Yang [28, Section 7.3.9] proposed the scaling factor of \u03b3k = 1 \u2212 e\u2212(k+2), it is not robust for numerical errors in our implementation because it converges to 1 too quickly."
        },
        {
            "heading": "5.3.3 Stopping criteria",
            "text": "In Theorem 4.2, we used (61) as the stopping criterion for Algorithm 1. However, the part \u00b5k \u2264 \u03f5 does not take the magnitude of the data into consideration, thus, it is not practical especially when the magnitude of the optimal values is relatively large. In addition, (61) depends on the initial point, though we set different initial points for Algorithms 1 and 2 as described in Section 5.3.1. Therefore, we employed the following stopping criterion in the numerical experiment:\nmax { \u2225\u2225rb(xk)\u2225\u2225 max{1, \u2225b\u2225} , \u2225\u2225rc(\u03bbk, sk)\u2225\u2225 max{1, \u2225c\u2225} , \u00b5k max {1, \u2225c\u22a4xk\u2225 , \u2225b\u22a4\u03bbk\u2225} } < \u03f5, (69)\nwhere we set the threshold \u03f5 = 10\u22127. In addition, we stopped the iteration immaturely when one of three conditions was detected; (i) the iteration count k reached 100, (ii) the step size \u03b1k was too small like \u03b1k < 10\n\u22127, or (iii) the linear systems (19), (20) or (63) could not be solved accurately due to numerical errors."
        },
        {
            "heading": "5.4 Numerical results",
            "text": "We report numerical results as follows. In Section 5.4.1, we compare Algorithms 1 and 2, and show that Algorithm 2 is superior to Algorithm 1 in the numerical experiments. Therefore, we use Algorithm 2 in Section 5.4.2 and thereafter. Section 5.4.2 compares the settings (18) or (64) for \u03b2k in Algorithm 2, and Section 5.4.3 evaluates the effect of the value of \u03b2. Lastly, Section 5.4.4 compares the numerical performance of Algorithm 2 with the existing methods, using the formula (64) chosen from Section 5.4.2 and the best \u03b2 from Section 5.4.3."
        },
        {
            "heading": "5.4.1 Comparison between Algorithm 1 and Algorithm 2",
            "text": "Table 1 reports the number of Netlib problems solved by Algorithms 1 and 2 with different settings of \u03b2. The value of \u03b2k is set by (64) except a case of \u03b2 = 0.5 in Algorithm 2 that investigates both (18) and (64). The table also shows the number of unsolved problems due to (i) the iteration limit, (ii) the diminishing step length or (iii) numerical errors\u2014see Section 5.3.3.\nFrom Table 1, we can see that Algorithm 2 improves the number of solvable problems from Algorithm 1. When \u03b2 = 0.5 and the setting of \u03b2k was (18), Algorithms 1 and 2 could not solve ten and six problems, respectively, among the 85 problems due to the iteration limit. As we will see soon in Figure 2, Algorithm 2 can reduce the number of iterations compared to Algorithm 1, therefore, Algorithm 2 can reach more optimal solutions before the iteration limit as shown in Table 1.\nIn addition, Table 1 indicates Algorithm 1 with setting \u03b2 = 1 does not affect the number of solvable problems remarkably, despite the upper bounds on max {\u2225\u2225\u2225(Dk)\u22121 z\u0307\u2225\u2225\u2225 ,\u2225\u2225(Dk) s\u0307\u2225\u2225}\nin Lemma 4.9 diverges when we take \u03b2 \u2192 1. If \u03b2 = 1 in Algorithm 1, xki + \u03b2k\u03b4(xk)i = 0 may happen at some index i, thus (xk +\u03b2k\u03b4(x\nk), \u03bbk, sk) /\u2208 N (\u03b8) holds and zk = xk is selected in (17). As can be inferred from this, the closer \u03b2 is to 1, the more likely zk = xk, and such an iteration is the same as the case of \u03b2 = 0. Therefore, the divergence of C0 is unlikely to occur in actual computation up to numerical errors.\nThe performance profile [7, 19] on the number of iterations in Algorithm 1 with different \u03b2 is shown in Figure 1. In the performance profiling, the horizontal axis is a\nscaled value \u03c4 and the vertical axis P (r \u2264 \u03c4) is the proportion of test problems. For example, for the number of iterations, P (r \u2264 \u03c4) corresponds the percentage of test problems that were solved by less than a \u03c4 times the number of iterations of the best algorithm. Simply speaking, a good algorithm has a larger value P (r \u2264 \u03c4) from smaller \u03c4 . To output the performance profile, we used a Julia package [18]. In Figure 1, the numbers of iterations with \u03b2 = 0.999 are the exactly same as those with \u03b2 = 1, thus the plot for \u03b2 = 0.999 is hidden by that for \u03b2 = 1 in the figure.\nFigure 1 shows that the momentum term improves the number of iterations when \u03b2 is 0.001. In contrast, when \u03b2 is close to 1, xk+\u03b2k\u03b4(x\nk) tends to be out of the neighborhood N (\u03b8), therefore, selecting zk = xk by (17) in many iterations makes the effect of the momentum term small.\nFigure 2 compares Algorithm 1 and 2 under the setting of \u03b2 = 0.001. Figure 2 indicates that the result of Algorithm 2 is significantly better than that of Algorithm 1, since Algorithm 2 uses the momentum term through all the iterations.\nFrom the above result, we use the results of Algorithm 2 in the following numerical experiments instead of Algorithm 1."
        },
        {
            "heading": "5.4.2 Effect of the choice of the weight of the momentum term",
            "text": "Theorem 4.1 indicates that \u03b2k by (18) can guarantee the decreasing of rb(x k), thus the number of solvable problems with (18) should be larger than (64). In contrast, the numerical result in Table 1 shows that Algorithm 2 with \u03b2 = 0.5 and the formula (18) is 79, while the algorithm with \u03b2 = 0.5 and the formula (64) is 78, therefore, the difference in solvable problems between (18) and (64) is only one.\nIn Figure 3, we plot the residuals and the duality measure on a test instance KB2 solved with \u03b2k in (18) and (64). When we use (18), the primal residual \u2225\u2225rb(xk)\u2225\u2225\u221e decreases monotonically in the first six iterations to a level of 10\u22128. In contrast, when we use (64),\n\u2225\u2225rb(xk)\u2225\u2225\u221e slightly increases at the fifth iteration. However, both (18) and (64) decrease\n\u2225\u2225rb(xk)\u2225\u2225\u221e sufficiently in 25 iterations. Figure 4 shows the performance profile on the number of iterations, and we can see\nthat (64) is better."
        },
        {
            "heading": "5.4.3 Numerical sensitivity of the restarting parameter",
            "text": "Here, we investigate influence of the restarting parameter \u03b2. Figure 5 shows the performance profiling on the number of iterations of Algorithm 2 with different \u03b2 from 0.001 to 1. We can observe there that larger \u03b2 tends to lead to a slightly less number of iterations, and this implies that a momentum term is effective to reduce the iterations.\nIn the following, we fix \u03b2 = 0.9 in Algorithm 2 which solves more problems in Table 1, and we compare Algorithm 2 with the existing methods."
        },
        {
            "heading": "5.4.4 Comparison with existing methods",
            "text": "Figure 6 shows a performance profile that compares Algorithm 2 and the two existing methods, Arc-search [27] and Line-search [13]. The results in Figure 6 indicates that Algorithm 2 performs better than the two existing methods in terms of the number of iterations.\nFigure 3: Trajectories for\n\u2225\u2225rb(xk)\u2225\u2225\u221e. The left is the result with the setting of (18), and\nthe right is with (64).\nThe detailed numerical results of the three methods are reported in Table 2 of Appendix. Among 76 problems in which all three methods found optimal solutions, Algorithm 2 reduces the number of iterations in 45 problems compared to Arc-search and Line-search (or finishes the iterations within at most the same numbers of iterations in 67 problems).\nWe also discuss the computational time. Since the results of small problems are too short, we calculate average computational times on problems for which all the three methods spent 30 seconds or longer. The average computational times are 252.51 in Algorithm 2, 272.73 in Arc-search, and 264.04 in Line-search, therefore, Algorithm 2 reduce the average time by about 5.4%."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this work, we proposed an iterative method for LP problems by integrating Nesterov\u2019s restarting strategy into the arc-search interior-point method. In particular, we approximated the central path with an ellipsoidal arc based on the point zk that is computed from xk with the momentum term \u03b4(xk). By keeping all the iterations in the neighborhood N (\u03b8), we established the convergence of a generated sequence of the proposed method to an optimal solution and showed the proposed method achieves the polynomial-time computational complexity. In order to improve numerical performance, we also proposed the modified method with a concept of the Mehrotra type interior-point method. From the numerical experiments with the Netlib benchmark problems, we observed that the modified method achieved better performance than existing arc-search and line-search interior-point methods.\nAs a future direction, we consider that Nesterov\u2019s restarting strategy in arc-search interior-point methods can be extended to more general types of problems, such as\nsecond-order cone programming, semidefinite programming, linear constrained quadratic programming, and more general conic programming. In particular, there is room for further discussions on how the restarting strategy behaves in nonlinear-constrained cases.\nIn addition, the multiple centrality correction discussed in [3,6] may also be combined to improve the numerical performance, since the step size can be larger when the iteration points are closer to the central path.\nAppendix\nDetails on numerical results\nTable 2 reports the numerical results in Section 5.4.4. The first column of the table is the problem name, while the second and the third are the variable size n and the number of constraints m, respectively. The fourth to ninth columns reports the numbers of iterations and the computation time of the methods. A mark \u2018-\u2019 indicates the unsolved problem due to (i), (ii) or (iii) in Section 5.3.3. or that the step size \u03b1k is too small. The results in bold face indicate the best results among the three methods.\nAGG3 755 514 23 11.39 23 9.81 28 13.56 BANDM 371 216 15 4.76 17 3.89 19 5.43 BEACONFD 167 77 7 0.07 8 0.08 10 0.07 BNL1 1496 611 44 32.82 55 37.29 43 31.19 BNL2 4335 2209 28 184.42 31 202.27 33 217.67 BOEING1 856 490 20 10.92 22 10.95 28 12.72 BOEING2 333 194 21 4.81 22 7.35 26 8.08 BORE3D 128 71 19 0.16 18 0.14 19 0.12 BRANDY 227 122 19 4.75 20 3.52 22 2.78 CAPRI 480 275 23 7.96 23 6.18 24 6.1 CRE-A 6997 3299 25 569.99 25 557.27 26 576.95 CRE-C 5684 2647 26 324.25 29 361.66 28 342.03 CYCLE 3123 1763 24 71.98 24 73.74 26 77.14 CZPROB 2786 678 29 45.91 34 55.68 32 50.73 D2Q06C 5807 2147 29 325.99 30 343.07 34 388.62 D6CUBE 6183 403 20 162.82 20 158.59 20 156.83 DEGEN3 2604 1503 15 30.84 17 33.85 18 36.45 ETAMACRO 771 436 24 9.46 24 10.27 23 9.31 FFFFF800 990 486 - - - - 59 31.06 FINNIS 976 465 17 7.87 18 7.99 21 7.87 FIT1D 2075 1050 27 30.89 25 30.11 30 36.29 FIT1P 2076 1026 17 20.05 22 26.22 31 35.83 GANGES 1753 1356 14 16.1 17 20.52 20 22.77 GREENBEA 4536 2187 - - - - 53 378.63 GREENBEB 4524 2182 31 218.74 36 253.29 37 259.54 ISRAEL 309 167 22 5.77 21 4.62 24 4.69 KB2 77 52 24 0.15 24 0.14 24 0.11 LOTFI 357 144 15 3.46 15 3.34 17 3.07 MAROS 1510 713 29 22.03 31 22.53 32 25.23 MAROS-R7 7448 2156 12 237.6 13 256.78 16 314.89 MODSZK1 1621 685 22 19.02 24 18.93 29 24.68 PDS-02 9253 4523 20 1031.95 - - 22 1112.86 PEROLD 1650 764 34 28.35 41 32.11 34 28.93 PILOT 5348 2173 56 538.86 72 694.07 52 502.17 PILOT-WE 3145 951 38 81.64 - - - - PILOT4 1393 628 94 70.9 - - - - PILOT87 7776 3365 39 1111.14 44 1214.68 42 1150.4 PILOTNOV 2315 1040 19 27.18 20 27.63 20 27.59 RECIPELP 203 117 9 2.3 9 1.94 11 0.35 SC105 162 104 9 2.85 10 2.86 11 0.49 SC205 315 203 11 3.38 13 3.94 13 1.05 SC50A 77 49 8 0.05 8 0.04 9 0.04 SC50B 76 48 7 0.05 7 0.04 8 0.04\nSCAGR25 572 372 16 6.72 17 6.56 19 6.7 SCAGR7 158 102 12 0.74 12 2.99 15 2.55 SCFXM1 567 304 17 6.36 18 6.63 20 4.64 SCFXM2 1134 608 20 13.79 21 12.64 22 15.85 SCFXM3 1701 912 20 18.69 21 18.53 22 19.72 SCRS8 1202 424 19 10.22 20 12.23 21 11.19 SCSD1 760 77 8 0.42 9 0.46 9 0.5 SCSD6 1350 147 10 5.21 10 5.28 12 5.05 SCSD8 2750 397 9 11.75 10 22.83 12 15.18 SCTAP1 660 300 16 6.43 18 5.71 17 4.26 SCTAP2 2500 1090 13 20.46 13 21.9 14 24.09 SCTAP3 3340 1480 13 42.16 14 44.53 14 43.32 SEBA 1345 890 17 13.09 17 13.17 19 14.95 SHARE1B 243 107 26 6.63 25 6.8 26 1.19 SHARE2B 161 95 12 0.12 14 0.15 15 0.12 SHELL 1520 529 20 14.62 20 15.01 21 15.82 SHIP04L 1963 325 12 11.44 12 11.08 15 13.6 SHIP04S 1323 241 12 6.41 13 6.34 15 6.68 SHIP08L 3225 526 14 26.66 15 28.56 17 33.0 SHIP08S 1688 322 12 9.31 14 10.78 16 11.26 SHIP12L 4274 664 15 54.67 17 60.97 20 72.1 SHIP12S 2038 390 14 12.91 16 14.57 19 16.11 STAIR 387 205 17 4.49 22 7.96 20 5.74 STANDATA 1273 395 12 7.49 12 9.54 14 8.31 STANDGUB 1273 395 12 7.23 12 7.66 14 9.29 STANDMPS 1273 503 14 8.59 16 10.32 18 12.87 STOCFOR1 139 91 17 0.16 17 0.16 24 0.18 STOCFOR2 2868 1980 28 83.62 28 85.24 28 83.08 TRUSS 8806 1000 16 375.51 17 385.02 19 430.57 TUFF 619 307 24 7.42 27 11.82 27 8.43 VTP-BASE 155 103 20 3.71 23 2.26 22 7.82 WOOD1P 1802 171 36 29.3 35 25.88 30 20.18 WOODW 5368 712 55 355.49 53 333.81 40 246.85"
        }
    ],
    "title": "An Infeasible Interior-Point Arc-search Method with Nesterov\u2019s Restarting Strategy for Linear Programming Problems",
    "year": 2023
}