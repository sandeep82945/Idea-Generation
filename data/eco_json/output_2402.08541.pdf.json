{
    "abstractText": "Tullock contests model real-life scenarios that range from competition among proof-of-work blockchain miners to rent-seeking and lobbying activities. We show that continuous-time bestresponse dynamics in Tullock contests with convex costs converges to the unique equilibrium using Lyapunov-style arguments. We then use this result to provide an algorithm for computing an approximate equilibrium. We also establish convergence of related discrete-time dynamics, e.g., when the agents best-respond to the empirical average action of other agents. These results indicate that the equilibrium is a reliable predictor of the agents\u2019 behavior in these games.",
    "authors": [
        {
            "affiliations": [],
            "name": "Edith Elkind"
        },
        {
            "affiliations": [],
            "name": "Abheek Ghosh"
        },
        {
            "affiliations": [],
            "name": "Paul W. Goldberg"
        }
    ],
    "id": "SP:062e14c58a0fdaecc3d86ee2c2ddc77f24c5d36d",
    "references": [
        {
            "authors": [
                "X. Chen",
                "C. Papadimitriou",
                "T. Roughgarden"
            ],
            "title": "An axiomatic approach to block rewards",
            "venue": "In Proceedings of the 1st ACM Conference on Advances in Financial Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Y.K. Cheung",
                "S. Leonardos",
                "G. Piliouras"
            ],
            "title": "Learning in markets: Greed leads to chaos but following the price is right",
            "venue": "In Proceedings of the 30th International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "A. Dasgupta",
                "K.O. Nti"
            ],
            "title": "Designing an optimal contest",
            "venue": "European Journal of Political Economy,",
            "year": 1998
        },
        {
            "authors": [
                "P. Dasgupta"
            ],
            "title": "The theory of technological competition. In New Developments in the Analysis of Market Structure",
            "venue": "Proceedings of a Conference Held by the International Economic Association in Ottawa,",
            "year": 1986
        },
        {
            "authors": [
                "R. Deschamps"
            ],
            "title": "An algorithm of game theory applied to the duopoly problem",
            "venue": "European Economic Review,",
            "year": 1975
        },
        {
            "authors": [
                "Dixit",
                "A. K"
            ],
            "title": "Strategic behavior in contests",
            "venue": "American Economic Review,",
            "year": 1987
        },
        {
            "authors": [
                "D. Dragone",
                "L. Lambertini",
                "A. Palestini"
            ],
            "title": "Static and dynamic best-response potential functions for the nonlinear cournot",
            "venue": "game. Optimization,",
            "year": 2012
        },
        {
            "authors": [
                "P. Dubey",
                "O. Haimanko",
                "A. Zapechelnyuk"
            ],
            "title": "Strategic complements and substitutes, and potential games",
            "venue": "Games and Economic Behavior,",
            "year": 2006
        },
        {
            "authors": [
                "C. Ewerhart"
            ],
            "title": "The lottery contest is a best-response potential game",
            "venue": "Economics Letters,",
            "year": 2017
        },
        {
            "authors": [
                "C. Ewerhart"
            ],
            "title": "Ordinal potentials in smooth games",
            "venue": "Economic Theory,",
            "year": 2020
        },
        {
            "authors": [
                "C. Ewerhart",
                "F. Quartieri"
            ],
            "title": "Unique equilibrium in contests with incomplete information",
            "venue": "Economic theory,",
            "year": 2020
        },
        {
            "authors": [
                "C. Ewerhart",
                "K. Valkanova"
            ],
            "title": "Fictitious play in networks",
            "venue": "Games and Economic Behavior,",
            "year": 2020
        },
        {
            "authors": [
                "D. Fudenberg",
                "D.K. Levine"
            ],
            "title": "The theory of learning in games, volume 2",
            "year": 1998
        },
        {
            "authors": [
                "A. Ghosh"
            ],
            "title": "Best-response dynamics in tullock contests with convex costs",
            "venue": "In Proceedings of the Web and Internet Economics (WINE),",
            "year": 2023
        },
        {
            "authors": [
                "A. Ghosh",
                "P. Goldberg"
            ],
            "title": "Best-response dynamics in lottery contests",
            "venue": "In Proceedings of the ACM Conference on Economics and Computation (EC),",
            "year": 2023
        },
        {
            "authors": [
                "J. Hofbauer",
                "W.H. Sandholm"
            ],
            "title": "On the global convergence of stochastic fictitious play",
            "year": 2002
        },
        {
            "authors": [
                "Z. Huang"
            ],
            "title": "Fictitious play for games with a continuum of strategies",
            "venue": "State University of New York at Stony Brook, Stony Brook, NY, USA,",
            "year": 2002
        },
        {
            "authors": [
                "M.K. Jensen"
            ],
            "title": "Aggregative games and best-reply potentials",
            "venue": "Economic theory,",
            "year": 2010
        },
        {
            "authors": [
                "K.A. Konrad"
            ],
            "title": "Strategy and Dynamics in Contests",
            "year": 2009
        },
        {
            "authors": [
                "N.S. Kukushkin"
            ],
            "title": "Best response dynamics in finite games with additive aggregation",
            "venue": "Games and Economic Behavior,",
            "year": 2004
        },
        {
            "authors": [
                "J.D. Leshno",
                "P. Strack"
            ],
            "title": "Bitcoin: An axiomatic approach and an impossibility theorem",
            "venue": "American Economic Review: Insights,",
            "year": 2020
        },
        {
            "authors": [
                "P. Milgrom",
                "I. Segal"
            ],
            "title": "Envelope theorems for arbitrary choice",
            "venue": "sets. Econometrica,",
            "year": 2002
        },
        {
            "authors": [
                "H. Moulin",
                "J.P. Vial"
            ],
            "title": "Strategically zero-sum games: the class of games whose completely mixed equilibria cannot be improved upon",
            "venue": "International Journal of Game Theory,",
            "year": 1978
        },
        {
            "authors": [
                "T. Puu"
            ],
            "title": "Chaos in duopoly pricing",
            "venue": "Chaos, Solitons & Fractals,",
            "year": 1991
        },
        {
            "authors": [
                "J.S. Shamma",
                "G. Arslan"
            ],
            "title": "Unified convergence proofs of continuous-time fictitious play",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2004
        },
        {
            "authors": [
                "M. Slade"
            ],
            "title": "What does an oligopoly maximize",
            "venue": "Journal of Industrial Economics,",
            "year": 1994
        },
        {
            "authors": [
                "L. Thorlund-Petersen"
            ],
            "title": "Iterative computation of cournot equilibrium",
            "venue": "Games and Economic Behavior,",
            "year": 1990
        },
        {
            "authors": [
                "G. Tullock"
            ],
            "title": "Efficient rent-seeking",
            "venue": "Texas A & M University,",
            "year": 1980
        },
        {
            "authors": [
                "M. Vojnovi\u0107"
            ],
            "title": "Contest Theory: Incentive Mechanisms and Ranking Methods",
            "year": 2016
        },
        {
            "authors": [
                "M. Voorneveld"
            ],
            "title": "Best-response potential games",
            "venue": "Economics letters,",
            "year": 2000
        },
        {
            "authors": [
                "K. W\u00e4rneryd"
            ],
            "title": "Chaotic dynamics in contests",
            "venue": "Economic Inquiry,",
            "year": 2018
        },
        {
            "authors": [
                "S.J. Wright"
            ],
            "title": "Coordinate descent algorithms",
            "venue": "Mathematical programming,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Contests are games where agents compete by making costly investments to win valuable prizes. The model of Tullock (1980) is one of the most widely used for studying these environments, and has been applied to scenarios that originate from economics, political science, and computer science (see, e.g., Konrad (2009), Vojnovic\u0301 (2016)). As a concrete real-life application, consider the game among miners in proof-of-work cryptocurrencies such as Bitcoin (Chen et al., 2019; Leshno & Strack, 2020). The probability that a Bitcoin miner adds the next block (and collects the corresponding reward) is proportional to her costly computational effort. A Tullock contest captures the game among these miners: a set of agents compete to win a valuable prize by investing costly effort, and an agent gets a portion of the prize proportional to his effort.\nGiven an environment with strategic agents, such as a contest, it is desirable to be able to reliably predict the agents\u2019 behavior, so as to reason about possible outcomes. Also, when applicable, one may want to design the environment to elicit certain behaviors. Nash equilibrium strategies serve as an initial approximation to this goal, but the existence\n1Department of Computer Science, University of Oxford, Oxfordshire, United Kingdom. Correspondence to: Abheek Ghosh <abheek.ghosh@cs.ox.ac.uk>.\nof an equilibrium is not always a good predictor of the agents\u2019 behavior. Indeed, the traditional explanation of equilibrium is that it results from analysis and introspection by the agents in a situation where the rules of the game, the rationality of the agents, and the agents\u2019 payoff functions are all common knowledge. Both conceptually and empirically, these assumptions are not always satisfied in real-life scenarios (Fudenberg & Levine, 1998). A model provides a more robust prediction of the outcome of a game if it explains how the outcome can be attained in a decentralized manner, ideally via a process that involves agents responding to incentives provided by their environment. Variations of best-response (BR) dynamics are arguably the simplest and the most intuitive of these models. In BR dynamics, agents sequentially change their current strategy towards one that best-responds to that of the other agents. This framework is especially well-suited to settings such as Tullock contests with convex costs, where pure-strategy equilibria are guaranteed to exist.\nRecent papers (Ewerhart, 2017; Ghosh & Goldberg, 2023; Ghosh, 2023) have explored discrete-time BR dynamics in Tullock contests with convex costs. In this model, at each time step, an agent updates their current action to their BR action. These papers show that this dynamics converges for homogeneous agents, i.e., when all agents have the same cost function. On the other hand, when the agents are nonhomogeneous, the dynamics does not converge, and this nonconvergence result holds for most instances.1 Intuitively, these results imply that we can expect simple and myopic agents to reach the equilibrium if they have similar cost functions, but not if they have different cost functions.\nIn real-life situations, agents may not be able to instantly change their current actions to their BR actions. They are more likely to slowly move from their current actions to actions with better utility. For example, in the game among Bitcoin miners, which is almost exactly modeled by a Tullock contest, a miner may gradually buy more equipment and thereby increase their computational power if they see an opportunity to improve their utility by doing so. The\n1The set of instances (cost functions, starting state of the dynamics) on which the dynamics does not converge has a positive measure with respect to the set of all instances.\nar X\niv :2\n40 2.\n08 54\n1v 1\n[ cs\n.G T\n] 1\n3 Fe\nb 20\n24\nchange may be more rapid if the discrepancy is substantial, i.e., if the distance between the current action and the optimal BR action is larger, but we may still expect the change to be smooth enough that other agents can react before a given agent moves from their current action to their BR action. This observation also holds for other applications of Tullock contests: e.g., in a competition among firms for research and development of drugs and vaccines (Dasgupta, 1986), a firm may generally be able to only slowly change their output by hiring/firing researchers or increasing/decreasing their investment.\nThe classic model for studying such smoothly changing actions is the continuous-time BR dynamics (Fudenberg & Levine, 1998), which is the focus of this paper. We show that continuous-time BR dynamics converges to the unique equilibrium when agents have arbitrary, possibly non-homogeneous, weakly convex cost functions.2 Our rate-of-convergence bound is tight: the dynamics converges to an \u03f5-approximate equilibrium in \u0398(log(1/\u03f5)) time. We prove this result using a Lyapunov potential function that measures the total regret, as perceived by the agents, for playing their current action instead of their BR action. We then extend this analysis to show convergence in certain classes of discrete-time dynamics, e.g., when the agents take small steps towards the BR (where the step size is not necessarily limiting to 0 as assumed for continuous time) or when the agents best-respond to the empirical average action of other agents. These results also lend to a simple algorithm to compute an \u03f5-approximate Nash equilibrium in time polynomial in 1/\u03f5 and parameters of the model."
        },
        {
            "heading": "2. Related Work",
            "text": "The works by Ewerhart (2017), Ghosh & Goldberg (2023), and Ghosh (2023) study learning dynamics in Tullock contests and are directly related to ours. Ewerhart (2017) and Ghosh & Goldberg (2023) study a special case of Tullock contests where the cost functions are linear, also known as lottery contests. Ewerhart (2017) shows that a lottery contest with homogeneous agents is a BR potential game (Voorneveld, 2000; Kukushkin, 2004); the class of BR potential games is a strict generalization of the better known classes of ordinal and exact potential games (Monderer & Shapley, 1996). Lottery contests are not exact potential games even with homogeneous agents (Ewerhart, 2017), and they are not\n2The primary focus in the literature has been on convex cost functions, because they model increasing marginal cost per unit utility, or, equivalently, decreasing marginal utility per unit cost, which is a feature of most economic environments. Also, Tullock contests with non-convex cost functions generally do not have a pure-strategy Nash equilibrium (PNE), and BR dynamics are not stable by definition when there does not exist a PNE; hence, such models need to be studied using alternative equilibrium concepts and learning dynamics.\nordinal potential games with non-homogeneous agents (Ewerhart, 2020).\nGhosh & Goldberg (2023) use the BR potential function of Ewerhart (2017) to derive bounds on the rate of convergence of BR dynamics in lottery contests with homogeneous agents assuming exactly one agent moves at a time. They show that the potential function of Ewerhart (2017) is strongly convex and smooth in a region around the equilibrium point, and the BR dynamics frequently visits this region. Outside this region, the BR potential function never increases, and inside this region they show rapid improvement in the potential using techniques related to coordinate descent (Wright, 2015).\nGhosh (2023) extends the convergence results for lottery contests mentioned above to Tullock contests with convex costs (and homogeneous agents). Tullock contests with general convex cost functions do not have a closed-form formula for the BR of an agent. To circumvent this, Ghosh (2023) introduces a dynamics called the discounted-sum dynamics and uses it in his analysis, instead of using a BR potential function or techniques from convex optimization as done by Ghosh & Goldberg (2023).\nIn contrast to the papers above, we study continuous-time BR dynamics and show convergence for arbitrary nonhomogeneous convex cost functions. In our analysis, we use a natural potential function that measures the total regret as perceived by the agents for playing their current action instead of their BR action, and use Lyapunov arguments to prove convergence. This potential function has been used before in a different context of proving convergence of stochastic fictitious play in two-player zero-sum games (Hofbauer & Sandholm, 2002; Shamma & Arslan, 2004); however, these results do not extend to zero-sum games with three or more players.3 Our analysis works because of the special structure exhibited by Tullock contests.\nMoulin & Vial (1978) implicitly show strategic equivalence between contests and zero-sum games. This directly implies convergence of fictitious play dynamics for two agents (Ewerhart & Valkanova, 2020), but no such result has been proven for three or more agents. A Tullock contest corresponds to a Cournot game with isoelastic inverse demand and constant marginal costs. There are convergence results for learning dynamics that apply to specific types of Cournot games, such as Cournot oligopoly with strictly declining BR functions (Deschamps, 1975; Thorlund-Petersen, 1990), Cournot game with linear demand (Slade, 1994), aggregative games that allow monotone BR selections (Huang, 2002; Dubey et al., 2006; Jensen, 2010), and others (Dragone et al., 2012; Bourle\u0300s et al., 2017). However, all these\n3Any n-player game can be written as an (n+ 1)-player zerosum game.\nmethods do not apply to the Tullock contest whose BR function is not monotone (Dixit et al., 1987). A different line of research has studied the convergence (or chaotic behavior) of learning dynamics in other types of contests (such as all-pay auctions) and Cournot games (e.g., Puu (1991); Wa\u0308rneryd (2018); Cheung et al. (2021)), but these techniques and results also do not apply to Tullock contests."
        },
        {
            "heading": "3. Preliminaries",
            "text": "In Tullock\u2019s model, n agents participate in a contest with unit prize (normalized). Let [n] = {1, 2, . . . , n}. The agents simultaneously produce non-negative output; we denote the output of agent i by xi \u2208 R\u22650 and the output profile by x = (x1, . . . , xn) \u2208 Rn\u22650. Let x\u2212i = (x1, . . . , xi\u22121, xi+1, . . . , xn) and s\u2212i = \u2211 j \u0338=i xj . Agent i incurs a cost of ci(xi) for producing the output xi and receives a fraction of the prize proportional to xi if at least one agent produces a strictly positive output, and 1/n otherwise.4 The utility of agent i, ui(x), is\nui(x) = xi\u2211 j xj \u2212 ci(xi) = xi xi + s\u2212i \u2212 ci(xi). (1)\nNotice that the utility of agent i depends upon her output xi and the total output of other agents s\u2212i, but not upon the distribution of s\u2212i across the n\u2212 1 agents. To indicate this, we will write ui(x) as ui(xi, s\u2212i).\nWe make the following assumptions on the cost functions: for every agent i, the cost function ci is (a) twice differentiable; (b) zero cost for non-participation, ci(0) = 0; (c) increasing, c\u2032i(z) > 0 for all z > 0; (d) weakly convex, c\u2032\u2032i (z) \u2265 0 for all z \u2265 0. These assumptions are standard in the literature, and without them, a pure-strategy Nash equilibrium may not exist (see, e.g., Vojnovic\u0301 (2016, Chapter 4), Ewerhart & Quartieri (2020)). Remark 3.1. Our model is equivalent to contests of general logit form with concave success functions and convex costs (Vojnovic\u0301 (2016, Chapter 4)). In logit contests, the utility of agent i is given by u\u0302i(x\u0302) =\nf\u0302i(x\u0302i)\u2211 j f\u0302j(x\u0302j) \u2212 c\u0302i(x\u0302i), where\nx\u0302j is the output of agent j. For each i, if f\u0302i is concave and c\u0302i is convex (and both are non-negative and strictly increasing), then we can do the change of variables xi = f\u0302i(x\u0302i) and set ci(xi) = c\u0302i(f\u0302i \u22121\n(xi)) to write the utility function as given in equation (1). Similarly, a utility function of the form Vixi\u2211\nj xj \u2212 ci(xi), where Vi is the value of the prize for agent i, can be converted to the form given in (1) by scaling down the utility by Vi, which does not affect the strategy of the agent. Also note that Tullock contests with utility functions of the form u\u0302i(x\u0302) = x\u0302i\nr\u2211 j x\u0302j r \u2212 c\u0302i(x\u0302i) for some r \u2208 (0, 1]\n4Some papers in the literature, e.g., Dasgupta & Nti (1998), assume that if s = 0, all agents get a prize of 0. Our analysis and results remain the same under this alternate assumption as well.\nare special cases of logit contests where f\u0302i(x\u0302i) = x\u0302i r."
        },
        {
            "heading": "3.1. Best-Response",
            "text": "Given the total output s\u2212i = \u2211\nj \u0338=i xj of all agents except i, the best-response (BR) of agent i is an action xi such that\nxi \u2208 argmax z\u22650 ui(z, s\u2212i) = argmax z\u22650\n( z\nz + s\u2212i \u2212 ci(z)\n) .\nFirst, note that an agent has no BR if the output of every other agent is 0.5 To circumvent this technical issue, we make the exogenous assumption that agent i plays some small positive action if s\u2212i = 0. We allow this action to be arbitrary (and can possibly change over time) as long as it is positive and is at most 12maxi c\u2032i(0) .\n6 On the other hand, agent i has a unique BR if s\u2212i > 0, i.e., if the output produced by at least one other agent j is non-zero. This unique BR can be computed by taking a derivative of ui(z, s\u2212i) with respect to z. If s\u2212i > 0, then\n\u2202ui(z, s\u2212i)\n\u2202z = s\u2212i (z + s\u2212i)2 \u2212 c\u2032i(z), (2)\n\u22022ui(z, s\u2212i)\n\u2202z2 = \u22122s\u2212i (z + s\u2212i)3 \u2212 c\u2032\u2032i (z) < 0, (3)\nwhere the last inequality holds because z \u2265 0, c\u2032\u2032i (z) \u2265 0, and s\u2212i > 0. So, ui(z, s\u2212i) is strictly concave w.r.t. z, i.e., \u2202ui(z,s\u2212i)\u2202z is strictly decreasing w.r.t. z. Let BRi(s\u2212i) denote the BR of agent i given that the total output of other agents is s\u2212i. The first-order conditions for the BR are\nBRi(s\u2212i) > 0 and \u2202ui(z, s\u2212i)\n\u2202z\n\u2223\u2223\u2223\u2223 z=BRi(s\u2212i) = 0,\nif \u2202ui(z, s\u2212i)\n\u2202z\n\u2223\u2223\u2223\u2223 z=0 > 0; (4)\nBRi(s\u2212i) = 0 and \u2202ui(z, s\u2212i)\n\u2202z\n\u2223\u2223\u2223\u2223 z=BRi(s\u2212i) \u2264 0,\nif \u2202ui(z, s\u2212i)\n\u2202z\n\u2223\u2223\u2223\u2223 z=0 \u2264 0. (5)\n5Indeed, if s\u2212i = 0, then by producing an output of \u03f5 > 0, agent i gets a utility of ui(\u03f5, 0) = 1\u2212ci(\u03f5). Since ci is continuous and ci(0) = 0, for sufficiently small \u03f5 we have ci(\u03f5) < 1\u2212 1/n and hence ui(\u03f5, 0) > ui(0, 0) = 1/n. Thus, xi = 0 cannot be a BR. Further, no \u03f5 > 0 can be a BR, because ci is increasing and thus ui(\u03f5/2, 0) = 1\u2212 ci(\u03f5/2) > 1\u2212 ci(\u03f5) = ui(\u03f5, 0).\n6This issue of not having any BR to 0 can also be resolved by an alternate technical assumption: the prize is given to agent i with probability xi b+ \u2211 j xj and agent i\u2019s expected utility is\nxi b+ \u2211 j xj\n\u2212 ci(xi), where b is a small positive constant. Under this alternate assumption the prize may remain unallocated with a positive probability of b b+ \u2211 j xj , unlike in our model. We expect all results in this paper to hold for this alternate model as well."
        },
        {
            "heading": "3.2. Continuous-Time BR Dynamics",
            "text": "Let x(t) = (xi(t))i\u2208[n] denote the action profile of the agents at time t in the BR dynamics. Similarly, let s(t) =\u2211 j xj(t) and s\u2212i(t) = \u2211\nj \u0338=i xj(t). The continuous-time (or simply, continuous) BR dynamics starts from an initial profile x(0) = (xi(0))i\u2208[n] \u2208 R\u22650. At time t \u2265 0, each agent i \u2208 [n] continuously updates their action as\ndxi(t)\ndt = BRi(s\u2212i(t))\u2212 xi(t). (6)\nThe continuous BR dynamics is a limiting case of the discrete BR dynamics when the step size goes to 0. In discrete BR, for a step size of \u2206t = 1, xi(t + 1) = BRi(s\u2212i(t)) \u21d0\u21d2 xi(t + 1) \u2212 xi(t) = BRi(s\u2212i(t)) \u2212 xi(t) for any given agent i. For arbitrary \u2206t \u2265 0, this dynamics can be generalized to xi(t + \u2206t) \u2212 xi(t) = \u2206t(BRi(s\u2212i(t)) \u2212 xi(t)) \u21d0\u21d2 xi(t+\u2206t)\u2212xi(t)\u2206t = BRi(s\u2212i(t))\u2212 xi(t). The continuous dynamics takes the limit \u2206t \u2192 0."
        },
        {
            "heading": "3.3. Equilibrium",
            "text": "A Tullock contest with weakly convex cost functions always has a pure-strategy Nash equilibrium (which is also the unique equilibrium, including mixed-strategy Nash equilibria, see, e.g., Ewerhart & Quartieri (2020)). So, we exclusively focus on pure equilibria in this paper. Definition 3.2 (Pure-Strategy Nash Equilibrium). An action profile x\u2217 = (x\u22171, . . . , x \u2217 n) is a pure-strategy Nash equilibrium if it satisfies\nui(x \u2217 i ,x \u2217 \u2212i) \u2265 ui(x\u2032i,x\u2217\u2212i),\nfor every agent i and every action x\u2032i for agent i.\nIn general, the BR dynamics in a Tullock contest may never exactly reach the equilibrium in finite time, rather it may converge to the equilibrium. The dynamics converges to an equilibrium if it reaches an \u03f5-approximate equilibrium in finite time for any \u03f5 > 0. Definition 3.3 (Approximate Pure-Strategy Nash Equilibrium). An action profile x = (x1, . . . , xn) is an \u03f5approximate pure-strategy Nash equilibrium, for \u03f5 > 0, if it satisfies\nui(xi,x\u2212i) \u2265 ui(x\u2032i,x\u2212i)\u2212 \u03f5,\nfor every agent i and every action x\u2032i for agent i."
        },
        {
            "heading": "4. Convergence of Continuous BR Dynamics",
            "text": "In this section, we prove that the continuous BR dynamics given in equation (6) rapidly converges to the unique purestrategy Nash equilibrium, i.e., x(t) \u2192 x\u2217 as t \u2192 \u221e, where x\u2217 is the equilibrium.\nTheorem 4.1. The continuous best-response dynamics x(t) in Tullock contests with weakly convex cost functions converges to an \u03f5-approximate pure-strategy Nash equilibrium in O(log(1/\u03f5)) time. Further, there are instances where reaching an \u03f5-approximate equilibrium takes \u2126(log(1/\u03f5)) time.\nNote that linear dynamical systems converge in \u0398(log(1/\u03f5)) time. In our dynamics, see equation (6), the \u2212xi(t) term is linear but the BRi(s\u2212i) term is non-linear, so a \u0398(log(1/\u03f5)) convergence can be expected but is not obvious. We present the proof for the lower and upper bounds given in Theorem 4.1 separately. Let us start with the upper bound.\nProof of Theorem 4.1 (Upper Bound). To prove this convergence result, we use the potential function given in (7). This potential has been used previously to prove the convergence of stochastic fictitious play for two-player zero-sum games with finite action space; see Section 2 for further discussion. For an action profile x, let the potential function V (x) be defined as:\nV (x) = \u2211 i\u2208[n] Vi(x), (7)\nwhere Vi(x) = max z ui(z, s\u2212i)\u2212 ui(xi, s\u2212i)\n= ui(BRi(s\u2212i), s\u2212i)\u2212 ui(xi, s\u2212i).\nNotice that maxz ui(z, s\u2212i) is not well-defined if s\u2212i = 0. In this case, as discussed in Section 3, we assume that BRi(0) \u2208 (0, 12maxi c\u2032i(0) ], and Vi(x) = ui(BRi(0), 0) \u2212 ui(xi, 0). We will prove that such profiles can only occur during a short initial phase of the BR dynamics.\nVi(x) measures agent i\u2019s regret for playing xi instead of the best possible action given s\u2212i, i.e., it is the amount of utility that agent i can increase by playing the BR instead of xi. Notice that, by definition,\n1. V (x) \u2265 0. Because ui(BRi(s\u2212i), s\u2212i) \u2265 ui(xi, s\u2212i) \u21d0\u21d2 Vi(x) \u2265 0 for every agent i and profile x, which implies V (x) \u2265 0.\n2. V (x) = 0 at the equilibrium. V (x) = 0 \u21d0\u21d2 x = x\u2217 because V (x) = 0 \u21d0\u21d2 Vi(x) = 0,\u2200i \u2208 [n] \u21d0\u21d2 ui(BRi(s\u2212i), s\u2212i) = ui(xi, s\u2212i),\u2200i \u2208 [n].\nGiven the profile x(t), we can write the potential at time t as V (x(t)) = \u2211 i Vi(x(t)). For conciseness, we denote Vi(x(t)) by Vi(t), or simply Vi, where the dependency on x(t) will be clear from the context; similarly, V (x(t)) by V (t) or V . Given the dynamics followed by x(t), equation (6), we can write the dynamics that V (t) follows as\ndV dt = \u2211 i\u2208[n] \u2202V \u2202xi dxi dt . (8)\nWe next bound the time it takes to reach a state with two positive outputs, and we show that this property always holds thereafter.\nWarm-Up Phase First, notice that if xi(\u03c4) > 0 at some time point \u03c4 , then xi(t) > 0 for all t \u2265 \u03c4 because dxi(t)dt = BR(s\u2212i(t)) \u2212 xi(t) \u2265 \u2212xi(t) =\u21d2 dxi(t)xi(t) \u2265 \u2212dt =\u21d2 xi(t) \u2265 xi(\u03c4)e\u2212(t\u2212\u03c4) > 0. So, once we reach a state with two agents i and j \u0338= i with positive output, then these two agents will always have a positive output thereafter.\nSay we start from a profile x(0) = 0, i.e., all agents have 0 output initially. By our technical assumption discussed in Section 3, the action of an agent i is some small constant in (0, 12maxi c\u2032i(0) ], say \u03b7i. So, dxi dt = \u03b7i > 0 at time t = 0 for all i, which implies that at time dt > 0 with dt \u2192 0, we have xi(dt) > 0, as required.\nLet us now consider the case when there is only one agent i with xi(0) = \u03b1 > 0, and all other agents j \u0338= i have xj(0) = 0. Now, if xi(0) = \u03b1 < 1c\u2032j(0) for some j \u0338= i, then BRj(s\u2212j(0)) = BRj(\u03b1) > 0 because by the first-order condition, equation (2), for z = 0, we have\n\u2202ui(z, \u03b1)\n\u2202z =\n\u03b1\n(z + \u03b1)2 \u2212 c\u2032j(z) =\n1 \u03b1 \u2212 c\u2032j(0) > 0.\nSo, at time 0, we have dxjdt = BRj(s\u2212j(0)) \u2212 xj(0) = BRj(\u03b1) > 0, which implies that at time dt > 0 with dt \u2192 0, we have xj(dt) > 0.\nNow, let us consider the case when xi(0) = \u03b1 \u2265 maxj \u0338=i\n1 c\u2032j(0) . Let \u03b2 = maxj \u0338=i 1c\u2032j(0) for conciseness. Let us bound the time\u2014denoted by T\u2014it takes to reach xi(T ) < \u03b2. As xi(t) \u2265 \u03b2 for all t < T , we have dxi(t) dt = BRi(s\u2212i(t)) \u2212 xi(t) = BRi(0) \u2212 xi(t) = \u03b7i \u2212 xi(t) \u2264 \u03b22 \u2212 xi(t), where the last inequality holds because \u03b7i \u2264 maxj 12c\u2032j(0) \u2264 maxj \u0338=i 1 2c\u2032j(0)\n= \u03b22 . Using this, we get\ndxi(t) dt \u2264 \u03b2 2 \u2212 xi(t) =\u21d2 dxi(t)\nxi(t)\u2212 \u03b22 \u2264 \u2212dt\n=\u21d2 ln ( xi(t)\u2212 \u03b22 \u03b1\u2212 \u03b22 ) \u2264 \u2212t,\nwhich implies that to reach xi(T ) < B, it is sufficient to have T > ln (\n\u03b1\u2212 \u03b22 \u03b2\u2212 \u03b22\n) = ln ( 2\u03b1 \u03b2 \u2212 1 ) . Further notice that\nV (x(0)) = Vi(x(0)) = (1 \u2212 ci(\u03b7i)) \u2212 (1 \u2212 ci(xi(0))) = xi(0) \u2212 \u03b7i \u2265 \u03b1 \u2212 \u03b22 . So, within T = O(log(V (0))) time, we get at least two agents with positive output, and this property holds thereafter.\nMain Phase Given our analysis of the warm-up phase above, from here on we assume that there are always two\nagents with positive output. We next prove the following lemma about V (t).\nLemma 4.2. The potential V (t) = V (x(t)) at any time t satisfies the following differential inequality\ndV\ndt + V \u2264 \u2212 \u2211 i yi\u2211 j yj ( 1\u2212 \u2211 j yj yi + s\u2212i )2 \u2264 0,\nwhere the dependency on t is suppressed, where yi(t) = BRi(s\u2212i(t)), and assuming that there are at least two agents with positive output in the profile x(t).\nProof. Let us suppress the dependency on t, e.g., let us write x(t) as x and V (t) as V . Let yi = BRi(s\u2212i) for conciseness.\nNote that for general convex cost function ci, we do not have a closed-form formula for BRi. But from the first-order conditions, equation (4), we have\nyi > 0 & \u2202ui(z, s\u2212i)\n\u2202z\n\u2223\u2223\u2223\u2223 z=yi = 0, if \u2202ui(z, s\u2212i) \u2202z \u2223\u2223\u2223\u2223 z=0 > 0;\nyi = 0 & \u2202ui(z, s\u2212i)\n\u2202z\n\u2223\u2223\u2223\u2223 z=yi \u2264 0, if \u2202ui(z, s\u2212i) \u2202z \u2223\u2223\u2223\u2223 z=0 \u2264 0.\nNow, \u2202ui(z,s\u2212i)\u2202z = s\u2212i (z+s\u2212i)2 \u2212 c\u2032i(z); plugging in z = 0 we get \u2202ui(z,s\u2212i)\u2202z \u2223\u2223\u2223 z=0 = s\u2212i(0+s\u2212i)2 \u2212 c \u2032 i(0) = 1 s\u2212i \u2212 c\u2032i(0).\nThe condition \u2202ui(z,s\u2212i)\u2202z \u2223\u2223\u2223 z=0 > 0 corresponds to 1s\u2212i \u2212 c\u2032i(0) > 0 \u21d0\u21d2 s\u2212ic\u2032i(0) < 1. Similarly, \u2202ui(z,s\u2212i)\n\u2202z\n\u2223\u2223\u2223 z=0\n\u2264 0 corresponds to s\u2212ic\u2032i(0) \u2265 1. Using these, the first-order conditions at yi = BRi(s\u2212i) can be rewritten as\ns\u2212i (yi + s\u2212i)2 = c\u2032i(yi), if s\u2212ic \u2032 i(0) < 1, (9)\nyi = 0, if s\u2212ic\u2032i(0) \u2265 1. (10)\nWe can write V as V = \u2211 i Vi = \u2211 i (ui(yi, s\u2212i)\u2212 ui(xi, s\u2212i))\n= \u2211 i ( yi yi + s\u2212i \u2212 ci(yi)\u2212 xi xi + s\u2212i + ci(xi) ) = \u2211 i yi yi + s\u2212i \u2212 \u2211 i ci(yi) + \u2211 i ci(xi)\u2212 1. (11)\nThe time derivative of V w.r.t. t is dVdt = \u2211 k \u2202V \u2202xk dxk dt , where dxk dt = yk \u2212 xk. To write \u2202V \u2202xk\n, we need to know \u2202yi\u2202xk for all i and k. Note that \u2202yi\u2202xk = 0 for k = i and \u2202yi \u2202xk\n= dyids\u2212i for k \u0338= i. Due to the constraint that yi (the best-response) is always non-negative, there is a point of non-differentiability at yi = 0. In particular, if c\u2032i(0) > 0, then:\n\u2022 If s\u2212i > 1c\u2032i(0) , then in the small neighborhood around s\u2212i, say [s\u2212i \u2212 \u03b4, s\u2212i + \u03b4] for small \u03b4 > 0, we will have the corresponding yi = 0. So, dyids\u2212i = 0.\n\u2022 If s\u2212i < 1c\u2032i(0) , then yi > 0 and is governed by equation (9). We can differentiate this equation w.r.t. to s\u2212i to get\n1 (yi + s\u2212i)2 \u2212 2s\u2212i (yi + s\u2212i)3\n= ( c\u2032\u2032i (yi) +\n2s\u2212i (yi + s\u2212i)3 ) dyi ds\u2212i\n=\u21d2 dyi ds\u2212i = yi \u2212 s\u2212i 2s\u2212i + (yi + s\u2212i)3c\u2032\u2032i (yi) . (12)\nIf we take the limit s\u2212i \u2191 1c\u2032i(0) , which implies that yi \u2193 0, we get\ndyi ds\u2212i = \u22121 2 + c\u2032\u2032i (0)/(c \u2032 i(0)) 2 < 0,\nwhere the last inequality is true because c\u2032i(0) > 0 and c\u2032\u2032i (0) \u2265 0. On the other hand, we know that the magnitude is bounded dyids\u2212i = \u22121 2+c\u2032\u2032i (0)/(c \u2032 i(0)) 2 \u2265 \u221212 .\nThe above two cases tell us that dyids\u2212i at s\u2212i = 1 c\u2032i(0) has a left limit strictly less than 0 but a right limit equal to 0, so we have non-differentiability at s\u2212i = 1c\u2032i(0) . But as dyi ds\u2212i is bounded near s\u2212i = 1c\u2032i(0) , we can define it to be equal to some finite value in [ \u221212+c\u2032\u2032i (0)/(c\u2032i(0))2 , 0] at s\u2212i = 1 c\u2032i(0) , which is sufficient for our analysis. An alternate analysis can be done using the envelop theorem of Milgrom & Segal (2002, Theorem 2) to arrive at the same result. Taking partial derivative of V w.r.t. xk we get \u2202V\u2202xk equal to\n= \u2202\n\u2202xk (\u2211 i yi yi + s\u2212i \u2212 \u2211 i ci(yi) + \u2211 i ci(xi)\u2212 1 )\n= \u2211 i ( s\u2212i (yi + s\u2212i)2 \u2212 c\u2032i(yi) ) \u2202yi \u2202xk\n\u2212 \u2211 i \u0338=k yi (yi + s\u2212i)2 + c\u2032k(xk).\nFrom the discussion above, we know that either \u2202yi\u2202xk = 0 or s\u2212i\n(yi+s\u2212i)2 = c\u2032i(yi) and \u2202yi \u2202xk is bounded. In either case, we have (\ns\u2212i (yi+s\u2212i)2 \u2212 c\u2032i(yi) ) \u2202yi \u2202xk = 0, so\n\u2202V \u2202xk = c\u2032k(xk)\u2212 \u2211 i \u0338=k yi (yi + s\u2212i)2 . (13)\nPutting together, we can write dVdt as\ndV dt = \u2211 k \u2202V \u2202xk dxk dt\n= \u2211 k (yk \u2212 xk)c\u2032k(xk)\u2212 \u2211 k (yk \u2212 xk) \u2211 i \u0338=k yi (yi + s\u2212i)2\n= \u2211 i (yi \u2212 xi)c\u2032i(xi)\u2212 \u2211 i yi(\u03c3 \u2212 yi \u2212 s\u2212i) (yi + s\u2212i)2 ,\nwhere \u03c3 = \u2211\nk yk. Adding V and dV dt together, we get\nV + dV\ndt = \u2211 i yi yi + s\u2212i \u2212 \u2211 i ci(yi) + \u2211 i ci(xi)\u2212 1\n+ \u2211 i (yi \u2212 xi)c\u2032i(xi)\u2212 \u2211 i yi(\u03c3 \u2212 yi \u2212 s\u2212i) (yi + s\u2212i)2\n= \u22121 + \u2211 i (\u2212ci(yi) + ci(xi) + (yi \u2212 xi)c\u2032i(xi)\ufe38 \ufe37\ufe37 \ufe38 \u22640 as ci is convex )\n+ \u2211 i 2yi yi + s\u2212i \u2212 \u2211 i\nyi\u03c3\n(yi + s\u2212i)2 .\nNow, let pi = yi\u2211 j yi = yi\u03c3 and qi = s\u2212i \u03c3 . Notice that\u2211\ni pi = 1 and, for all i, pi \u2265 0 and qi \u2265 0. Plugging this into the inequality above, we get\nV + dV\ndt \u2264 \u22121 + \u2211 i 2yi yi + s\u2212i \u2212 \u2211 i\nyi\u03c3\n(yi + s\u2212i)2 = \u22121 + \u2211 i 2pi pi + qi \u2212 \u2211 i pi (pi + qi)2\n= \u2211 i pi ( \u22121 + 2 pi + qi \u2212 1 (pi + qi)2 )\n= \u2212 \u2211 i pi ( 1\u2212 1 pi + qi )2 \u2264 0.\nLet us now use Lemma 4.2 to get the desired rate of convergence upper bound. We use standard Lyapunov arguments:\ndV (t) dt + V (t) \u2264 0 =\u21d2 dV (t) V (t) \u2264 \u2212dt\n=\u21d2 V (t) \u2264 V (0)e\u2212t.\nFor any t \u2265 ln ( 1 \u03f5 ) + ln(V (0)), we get V (t) \u2264 \u03f5, which implies that for every agent i we have Vi(t) = Vi(x(t)) \u2264 \u03f5 \u21d0\u21d2 ui(x(t)) \u2265 ui(BRi(s\u2212i(t)), s\u2212i(t)) \u2212 \u03f5. So, we are at an \u03f5-approximate equilibrium. This completes the proof for the upper bound.\nProof of Theorem 4.1 (Lower Bound). We provide an example where it takes \u2126(log ( 1 \u03f5 ) + log(V (0))) time to converge to an \u03f5-approximate equilibrium.\nLet there be n = 2 homogeneous agents with linear cost function c1(y) = c2(y) = y/4 for any y \u2265 0. It can be easily derived that the unique equilibrium is x\u2217 = (1, 1) and that there is a closed-form formula for the best-response BRi(s\u2212i) = 2 \u221a s\u2212i \u2212 s\u2212i (see, e.g., Vojnovic\u0301 (2016)).\nLet x(0) = (y(0), y(0)), where we assume that y(0) is sufficiently large and far away from the equilibrium value 1. As the two players are homogeneous and start from the same action, they will maintain the same action x(t) = (y(t), y(t)), for some y(t), for all t \u2265 0. We suppress the dependency on t to avoid clutter. Let us track the evolution of y. From equation (6), we have\ndy dt = dxi dt = BRi(s\u2212i)\u2212 xi = BRi(y)\u2212 y\n= (2 \u221a y \u2212 y)\u2212 y = 2(\u221ay \u2212 y).\nLet us now compute the potential function V . We have V = \u2211 i (ui(BRi(s\u2212i), s\u2212i)\u2212 ui(xi, s\u2212i))\n= 2 (u1(2 \u221a y \u2212 y, y)\u2212 u1(y, y))\n= 2\n( 2 \u221a y \u2212 y\n2 \u221a y \u2212 y + y \u2212 2 \u221a y \u2212 y 4\n\u2212 ( y\ny + y \u2212 y 4 )) = 1 + y \u2212 2\u221ay = (\u221ay \u2212 1)2.\nFurther, we can find the rate of change of V using the rate of change of y as\ndV dy = d dy ( \u221a y \u2212 1)2 = 2( \u221a y \u2212 1) 2 \u221a y = \u221a y \u2212 1 \u221a y ,\ndV\ndt =\ndV\ndy\ndy dt = \u221a y \u2212 1 \u221a y 2( \u221a y \u2212 y) = \u22122(\u221ay \u2212 1)2\n=\u21d2 dV dt = \u22122V =\u21d2 V (t) = V (0)e\u22122t.\nBy the definition of V and the symmetry of the two agents, at an \u03f5-approximate equilibrium, V (t) = 2\u03f5. So, it takes exactly t = 12 ln( 1 2\u03f5 )+ 1 2 ln(V (0)) = \u2126(log ( 1 \u03f5 ) +log(V (0))) time to reach the \u03f5-approximate equilibrium."
        },
        {
            "heading": "5. Discrete-Time; Equilibrium Computation",
            "text": "In this section, we consider discrete-time BR dynamics. We also provide an algorithm for computing an approximate equilibrium based on such dynamics. Proofs are given in Appendix A.\nLet us consider a modification to the original Tullock contest model we have studied till now. We assume that each agent i must always play an action xi \u2265 xmin instead of xi \u2265 0, for some xmin \u2265 0. Notice that xmin = 0 corresponds to the original model, while a xmin > 0 says that any participant in the contest must have a positive minimum\noutput. An assumption of xmin > 0 may be plausible in practical scenarios where there is a positive cost of participation (showing up for the game). We also normalize the cost functions and assume that mini ci(1) = 1 for all i; this ensures that any rational agent will always play an action \u2264 1. We also assume that the second derivative and the ratio of the first derivatives of the cost functions are bounded: maxi,z\u2208[xmin,1] c \u2032 i(z) mini,z\u2208[xmin,1] c \u2032 i(z) = B1 and maxi,z\u2208[xmin,1] c \u2032\u2032 i (z) = B2.\nThe first-order conditions for the case when xmin > 0 are similar to the ones given in (4) except that the critical point above which the first-order condition is satisfied with equality is xmin instead of 0. The analysis for the continuous BR dynamics for this model is also analogous to the analysis for Theorem 4.1.\nLet us now consider BR dynamics in this model with the step-size, say \u2206t, small but not necessarily going to 0. In particular,\nxi(t+\u2206t) = xi(t) + \u2206t \u00b7 (BRi(s\u2212i(t))\u2212 xi(t)). (14)\nThe continuous-time BR dynamics corresponds to equation (14) with \u2206t \u2192 0. We aim to find bounds on \u2206t that ensure convergence.\nLemma 5.1. For a profile x, let H(x) be defined as\nH(x) =\nB2 2 \u2211 i(yi \u2212 xi)2 + \u2211 i\u2208E\n(\u03c3\u2212yi\u2212s\u2212i)2 s2\u2212i\u2211\ni yi(\u03c3\u2212yi\u2212s\u2212i)2 \u03c3(yi+s\u2212i)2\n,\nwhere yi = BRi(s\u2212i) and \u03c3 = \u2211\ni yi. If the step-size at time t is bounded above by 1/max(2, H(x(t)), then the BR dynamics converges to the unique equilibrium. In particular, for 0 < \u03b1t \u2264 1/max(2, H(x(t)), we have V (t+ \u03b1t) \u2264 (1\u2212 \u03b1t)V (t).\nIf xmin is assumed to be strictly positive, then we can upper bound H(x) as a function of xmin.\nLemma 5.2. If xmin > 0, then H(x) = O (\nn(1+B2) x3min ) for all x, which implies that the dynamics reaches an \u03f5approximate equilibrium in O ( 1 \u03b1 log ( V (0) \u03f5 )) steps with a\nsuitable step-size \u03b1 = \u0398 (\nx3min n(1+B2)\n) .\nNotice that the bound in Lemma 5.2 depends upon the number of agents n. This is essential, as highlighted by Lemma 5.3 below.\nLemma 5.3. If the step-size is not O(1/n), then there are instances with linear and homogeneous cost functions where the dynamics does not converge.\nNote that although the bound in Lemma 5.2 does not depend upon B1 (the ratio of the first-derivatives of the cost functions of the agents, which measures the relative skills of\nthe agents), B1 may be implicit in xmin. If xmin is not sufficiently small, e.g., if xmin = \u03c9(1/B21), then the equilibrium when the agents are restricted to play xi \u2265 xmin may be different from the equilibrium when the agents can play less than xmin. For example, for two agents with linear cost functions c1(x1) = x1 and c2(x2) = \u03b2x2, where \u03b2 \u2265 1, the unique equilibrium is x\u2217 = ( \u03b2\n(1+\u03b2)2 , 1 (1+\u03b2)2\n) if the agents\ncan play any xi \u2265 0. Note that B1 = \u03b2, so the equilibrium output of agent-2 is \u0398(1/B21). On the other hand, if the agents are restricted to play xi \u2265 xmin = \u03c9(1/B21), then the equilibrium will be forced to be different from x\u2217. Given this observation, it would be natural to assume that xmin is small enough; in particular, xmin = O(1/B21). Moreover, the dependency on B1 is unavoidable, as formalized by Lemma 5.4 below.\nLemma 5.4. If the step-size is not O(1/B1), then there are instances with two agents and linear cost functions where the dynamics does not converge.\nIf xmin = 0, then our results do not provide a lower bound on the step-size that is independent of the action profile x(t). In particular, Lemma 5.1 does not directly imply such a bound because H(x) may be unbounded for some profiles x, then the step-size recommended by the lemma at x to ensure convergence goes to 0. Indeed, such a lower bound might not exist. On the other hand, even in the case of xmin = 0, we can simulate with a pseudo x\u0302min = \u0398(\u03f5) to compute an equilibrium in poly(1/\u03f5, n,B1, B2) steps as shown below.\nAlgorithm Let us construct a modified game with a pseudo lower bound on the outputs of the agents: x\u0302min = \u03f5/(4B1). We simulate the BR dynamics in this game with a step-size of \u03b1 = \u0398 ( x\u03023min\nn(1+B2)\n) , as recommended by\nLemma 5.2, to compute an (\u03f5/2)-approximate equilibrium of this modified game in O ( 1 \u03b1 log ( V (0) \u03f5 )) steps. Let this approximate equilibrium be x\u0302. At x\u0302, all agents have a regret of at most \u03f5/2 assuming that they can only play above x\u0302min. By playing below x\u0302min, they can further increase their utility by at most ci(x\u0302min) \u2212 ci(0) \u2264 B1x\u0302min/(1 \u2212 x\u0302min) \u2264 2B1x\u0302min \u2264 \u03f5/2. So, at x\u0302, the total regret of any agent in the original game is at most \u03f5, as required.\nBest-Response to Empirical Average Let us consider a discrete-time dynamics with a step-size of \u2206t = 1, but where the agents best-respond to the empirical average action of the other agents. Let xi(t) = 1t \u2211t \u03c4=1 xi(t) and\ns\u2212i(t) = \u2211\nj \u0338=i xj(t). Formally, the dynamics is defined as follows: the action of agent i at time t+ 1 is\nxi(t+ 1) = BRi(s\u2212i(t)), \u2200i \u2208 [n], t \u2208 Z\u22650 (15)\nGiven this, we can write the updated empirical average at time t+ 1 as\nxi(t+ 1) = xi(t) + 1\nt+ 1 (BRi(s\u2212i(t)\u2212 xi(t)).\nNotice that xi(t) tracks a BR dynamics with a sequence of decreasing step-sizes that correspond to the harmonic sequence ( 1t )t\u2208Z\u22651 . The harmonic sequence satisfies the following crucial properties: as t \u2192 \u221e, the sequence 1t \u2192 0 but the series \u2211t k=1 1 k \u2192 \u221e. These two properties ensure that the dynamics converges for the case xmin > 0 using Lemma 5.1. In fact, we can generalize this dynamics to a weighted average, where the step-size at time t is \u03b7t, and xi(t) follows\nxi(t+ 1) = xi(t) + \u03b7t(BRi(s\u2212i(t)\u2212 xi(t)). (16)\nLemma 5.5. If xmin > 0, then a dynamics that evolves according to (16) converges if the sequence of step-sizes (\u03b7t)t\u2208Z\u22651 satisfies: as t \u2192 \u221e, the sequence \u03b7t \u2192 0 but the series \u2211t k=1 \u03b7k \u2192 \u221e.\nOther examples of step-size sequences that lead to convergence are \u03b7t = 1/tr for r \u2208 (0, 1] and \u03b7t = 1/ log(1 + t). Note that convergence of x also implies convergence of x."
        },
        {
            "heading": "6. Conclusion and Future Research",
            "text": "We showed that the continuous BR dynamics, which is motivated by the observation that in certain applications the agents change their actions slowly compared to the feedback they receive from others, converges to the unique equilibrium in Tullock contests with convex costs. We then extended these convergence results to related discrete dynamics with small step sizes. These results indicate that we can expect Tullock contests with convex costs to reach equilibrium in a decentralized manner.\nOne open problem is to show convergence (or nonconvergence) of the discrete dynamics with small step sizes when xmin = 0; see Section 5. A different direction is to study the case when the agents move at rates different from continuous BR dynamics. If the relative rates of the agents can change arbitrarily over time, we may have nonconvergence. On the other hand, if the rates of the agents are similar to that of the continuous BR dynamics, we can show convergence. It would be valuable to analyze scenarios between these two extreme cases. This direction is discussed further in Appendix B. Another unexplored direction is to study learning in Tullock contests when the agents get only probabilistic feedback. In many practical applications, the proportional allocation function corresponds to the probability of allocating an indivisible item (and not the fraction of the item allocated to the agent). Here, the agent may only know his own actions and whether or not he won the indivisible item but may not know the actions of others."
        },
        {
            "heading": "A. Proofs from Section 5",
            "text": "Proof of Lemma 5.1. We shall suppress the dependency on t for conciseness, e.g., write x(t) as x and xi(t) as xi. Let yi = BRi(s\u2212i). Given the minimum output level xmin \u2265 0, yi satisfies the first-order conditions given below, which can be derived following steps similar to the derivation of conditions (9) and (10).\ns\u2212i (yi + s\u2212i)2 = c\u2032i(yi), if s\u2212ic \u2032 i(xmin) < 1, (17)\nyi = xmin, if s\u2212ic\u2032i(xmin) \u2265 1. (18)\nFrom equation (11), we know that the potential V (t) = V (x(t)) is given by\nV (t) = \u2211 i yi yi + s\u2212i \u2212 \u2211 i ci(yi) + \u2211 i ci(xi)\u2212 1.\nIn Lemma 4.2, we showed that dVdt < 0, i.e., lim\u2206t\u21920 V (t+\u2206t)\u2212V (t) \u2206t < 0, which implies lim\u2206t\u21930(V (t+ \u2206t) \u2212 V (t)) < 0. We shall now consider small positive values for \u2206t, not necessarily limiting to 0, such that we can still guarantee that V (t+\u2206t)\u2212 V (t) < 0.\nLet \u2206t = \u03b1 be the step-size. Given the current profile x and the best-response profile y = (BRi(s\u2212i))i\u2208[n], the profile after the \u03b1-step is \u03b1(y \u2212 x) + x. We want to show that V (t + \u03b1) = V (\u03b1(y \u2212 x) + x) < V (t) = V (x). The multivariate Taylor\u2019s expansion of V (\u03b1(y \u2212 x) + x) w.r.t. V (x) with a second-order error term is given by\nV (\u03b1(y \u2212 x) + x) = V (x) + \u03b1(y \u2212 x)\u22ba\u2207V (x)\n+ \u03b12\n2 (y \u2212 x)\u22ba\u22072V (x\u0302)(y \u2212 x), (19)\nwhere x\u0302 = x + \u03b2(y \u2212 x) for some value \u03b2 \u2208 [0, \u03b1], and where \u2207V (z) = (\u2202V (z)\u2202zi )i\u2208[n] and \u2207 2V (z) = (\u2202 2V (z)\n\u2202zi\u2202zj )i,j\u2208[n] for any given profile z. Let us first compute \u22072V (x).7 As s\u2212i = \u2211\nj \u0338=i xj , so ds\u2212i dxi\n= 0 and ds\u2212idxj = 1 for all j \u0338= i. From equation (12), we have\n\u2202yi \u2202xj = yi \u2212 s\u2212i 2s\u2212i + (yi + s\u2212i)3c\u2032\u2032i (yi) ,\n7As discussed in Lemma 4.2, all points except when s\u2212i = 1 c\u2032i(xmin) are continuously differentiable (and can be shown to be twice continuously differentiable by extending the same argument). When s\u2212i = 1c\u2032i(xmin)\n, the derivative of yi w.r.t. s\u2212i is not welldefined. However, the derivative is well-defined and bounded at all points in its neighborhood. This allowed us to extend the analysis for the differentiable points to this non-differentiable point. A similar but more detailed analysis can be done for computing \u22072V (x) to get the same results, but here let us restrict our focus on the generic case of s\u2212i \u0338= 1c\u2032i(xmin) .\nfor j \u0338= i and yi > xmin, and 0 otherwise. As derived in equation (13), we know that\n\u2202V \u2202xi = c\u2032i(xi)\u2212 \u2211 k \u0338=i yk (yk + s\u2212k)2 .\nLet E = {k \u2208 [n] | yk > xmin}. Differentiating again w.r.t. xj , if j = i, we have\n\u22022V \u2202x2i = c\u2032\u2032i (xi) + \u2211 k\u2208E,k \u0338=i ( 2yk (yk + s\u2212k)3\n+\n( 2yk\n(yk + s\u2212k)3 \u2212 1 (yk + s\u2212k)2 ) \u2202yi \u2202xk ) = c\u2032\u2032i (xi) +\n\u2211 k\u2208E,k \u0338=i 2yk (yk + s\u2212k)3\n+ \u2211\nk\u2208E,k \u0338=i\n\u2202yi \u2202xk yk \u2212 s\u2212k (yk + s\u2212k)3 .\nPlugging in the value of \u2202yi\u2202xk for k \u2208 E, we get\n\u22022V \u2202x2i = c\u2032\u2032i (xi) + \u2211 k\u2208E,k \u0338=i 2yk (yk + s\u2212k)3\n+ \u2211\nk\u2208E,k \u0338=i\n(yk \u2212 s\u2212k)2\n(yk + s\u2212k)3(2s\u2212k + (yk + s\u2212k)3c\u2032\u2032k(yk)) .\nLet \u03b7k = (yk + s\u2212k)3c\u2032\u2032k(yk). The above equation can be rewritten as\n\u22022V \u2202x2i = c\u2032\u2032i (xi) + \u2211 k\u2208E,k \u0338=i 2yk (yk + s\u2212k)3\n+ \u2211\nk\u2208E,k \u0338=i\n(yk \u2212 s\u2212k)2\n(yk + s\u2212k)3(2s\u2212k + \u03b7k)\n= c\u2032\u2032i (xi) + \u2211\nk\u2208E,k \u0338=i\n(yk + s\u2212k) 2 + 2yk\u03b7k\n(yk + s\u2212k)3(2s\u2212k + \u03b7k) .\nLet ai = c\u2032\u2032i (xi). Let bi = (yi+s\u2212i) 2+2yi\u03b7i (yi+s\u2212i)3(2s\u2212i+\u03b7i)\nif i \u2208 E and bi = 0 if i /\u2208 E. Notice that ai \u2265 0 and bi \u2265 0 for all i \u2208 [n]. We have\n\u22022V \u2202x2i = ai + ( \u2211 i b)\u2212 bi.\nFollowing a similar sequence of steps, we can compute \u22022V \u2202xi\u2202xj for j \u0338= i as\n\u22022V \u2202xi\u2202xj = ( \u2211 i b)\u2212 bi \u2212 bj .\nLet a = (ai)i\u2208[n] and b = (bi)i\u2208[n]. Let e = (1, 1, . . . , 1) denote the n-dimensional vector of all 1s. Let A denote the\nn\u00d7 n diagonal matrix with Aii = ai + bi and Aij = 0 for j \u0338= i. Putting everything together, we can write \u22072V (x) as\n\u22072V (x) = ( \u2211 i b)ee\u22ba \u2212 be\u22ba \u2212 eb\u22ba +A. (20)\nLet us now consider a vector w \u2208 Rn. We can compute w\u22ba\u22072V (x)w as w\u22ba\u22072V (x)w = w\u22ba(( \u2211 i b)ee\u22ba \u2212 be\u22ba \u2212 eb\u22ba +A)w\n= ( \u2211 i bi)( \u2211 i wi) 2 \u2212 ( \u2211 i biwi)( \u2211 i wi)\n\u2212 ( \u2211 i wi)( \u2211 i biwi) + \u2211 i (ai + bi)w 2 i\n= \u2211 i bi (\u2211 j wj) 2 \u2212 2wi( \u2211 j wj) + w 2 i +\u2211 i aiw 2 i\n= \u2211 i bi( \u2211 j \u0338=i wj) 2 + \u2211 i aiw 2 i .\nNow, we know that bi = (yi+s\u2212i) 2+2yi\u03b7i (yi+s\u2212i)3(2s\u2212i+\u03b7i)\nfor i \u2208 E. Let us find a suitable upper bound on bi. Let the function g : R\u22650 \u2192 R\u22650 be defined as\ng(z) = (yi + s\u2212i)\n2 + 2yiz\n(yi + s\u2212i)3(2s\u2212i + z) .\nNotice that bi = g(\u03b7i). Differentiating g w.r.t. z, we get\ndg dz = 2yi (yi + s\u2212i)3(2s\u2212i + z) \u2212 (yi + s\u2212i) 2 + 2yiz (yi + s\u2212i)3(2s\u2212i + z)2\n= 2yi(2s\u2212i + z)\u2212 (yi + s\u2212i)2 \u2212 2yiz\n(yi + s\u2212i)3(2s\u2212i + z)2\n= \u2212(yi \u2212 s\u2212i)2\n(yi + s\u2212i)3(2s\u2212i + z)2 .\nSo, dgdz is less than or equal to 0 for all yi \u2265 0, s\u2212i \u2265 0, and z \u2265 0, which are satisfied by the definition of yi, s\u2212i, and z. So, g(z) is maximized at z = 0, which implies that\nbi = g(\u03b7i) \u2264 max z\u22650 g(z) = g(0) \u2264 1 2(yi + s\u2212i)s\u2212i .\nPlugging back this bound on bi and the value of ai to w\u22ba\u22072V (x)w, we get\nw\u22ba\u22072V (x)w \u2264 \u2211 i\u2208E\n( \u2211\nj \u0338=i wj) 2 2(yi + s\u2212i)s\u2212i + \u2211 i c\u2032\u2032i (xi)w 2 i .\nIn the Taylor\u2019s expansion in equation (19), \u22072V is evaluated at x\u0302. Let s\u0302\u2212i = \u2211 j \u0338=i x\u0302j and y\u0302i = BRi(s\u0302\u2212i). Setting the\nvector w = y \u2212 x, we get\n(y \u2212 x)\u22ba\u22072V (x\u0302)(y \u2212 x)\n\u2264 \u2211 i\u2208E\n( \u2211\nj \u0338=i(yi \u2212 xi))2 2(y\u0302i + s\u0302\u2212i)s\u0302\u2212i + \u2211 i c\u2032\u2032i (x\u0302i)(yi \u2212 xi)2.\nNotice that s\u0302\u2212i = \u03b1 \u2211\nj \u0338=i yj + (1\u2212 \u03b1)s\u2212i \u2265 (1\u2212 \u03b1)s\u2212i. So, (y\u0302i + s\u0302\u2212i)s\u0302\u2212i \u2265 (1\u2212\u03b1)2s\u2212i. As \u03b1 is a small constant, (1 \u2212 \u03b1)2 is bounded away from 0, e.g., if \u03b1 \u2264 1/2, then (1\u2212 \u03b1)2 \u2265 1/4. Let \u03c3 = \u2211 i yi. For \u03b1 \u2264 1/2, we get\n(y \u2212 x)\u22ba\u22072V (x\u0302)(y \u2212 x)\n\u2264 \u2211 i 2(\u03c3 \u2212 yi \u2212 s\u2212i))2 s2\u2212i + \u2211 i B2(yi \u2212 xi)2,\nas c\u2032\u2032i (z) \u2264 B2 for any z. Let \u03b1 be equal to the smaller among 1/2 and\n\u03b1 \u2264 \u2211 i yi(\u03c3\u2212yi\u2212s\u2212i)2 \u03c3(yi+s\u2212i)2\nB2 2 \u2211 i(yi \u2212 xi)2 + \u2211 i\u2208E (\u03c3\u2212yi\u2212s\u2212i)2 s2\u2212i .\nNotice that \u2211\ni yi(\u03c3\u2212yi\u2212s\u2212i)2 \u03c3(yi+s\u2212i)2 \u2264 \u2212((y \u2212 x)\u22ba\u2207V (x) + V (x)) from Lemma 4.2. So, we get\n\u03b1 \u2264 \u22122(y \u2212 x) \u22ba\u2207V (x)\u2212 2V (x)\n(y \u2212 x)\u22ba\u22072V (x\u0302)(y \u2212 x) =\u21d2\n(y \u2212 x)\u22ba\u2207V (x) + \u03b1(y \u2212 x) \u22ba\u22072V (x\u0302)(y \u2212 x)\n2 \u2264 \u2212V (x).\nPlugging this into the Taylor\u2019s expansion in equation (19), we get\nV (\u03b1(y \u2212 x) + x) \u2264 (1\u2212 \u03b1)V (x),\nas required.\nProof of Lemma 5.2. Note that for any z \u2208 Rn and n \u2265 2, we have \u2211 i z 2 i \u2264 \u2211 i( \u2211\nj \u0338=i zj) because\u2211 i ( \u2211 j \u0338=i zj) 2 = \u2211 i (( \u2211 j zj)\u2212 zi)2\n= n( \u2211 j zj) 2 + \u2211 i z2i \u2212 2( \u2211 j zj)( \u2211 i zi)\n= (n\u2212 2)( \u2211 j zj) 2 + \u2211 i z2i .\nUsing this, we have \u2211 i(yi \u2212 xi)2 \u2264 \u2211 i(\u03c3 \u2212 yi \u2212 s\u2212i)2.\nAs xi, yi \u2208 [xmin, 1] for all i, we have \u03c3(yi+s\u2212i) 2\nyi \u2264 n\n3\nxmin\nand 1 s2\u2212i \u2264 1 (n\u22121)2x2min\n. So, H(x) \u2264 B2n 3\n2xmin + n\n3\n(n\u22121)2x3min =\nO( (1+B2)n x3min ) assuming xmin \u2264 1n .\nFor any step-size of \u03b1 > 0 satisfying \u03b1 \u2264 1H(x) for all x, from Lemma 5.1, we know that V (t+ \u03b1) \u2264 (1\u2212 \u03b1)V (t). After k steps, we have\nV (k\u03b1) \u2264 (1\u2212 \u03b1)kV (0) \u2264 e\u2212k\u03b1V (0).\nIf we take k = 1\u03b1 log( V (0) \u03f5 ), then V (k\u03b1) \u2264 \u03f5, which implies that we have reached an \u03f5-approximate equilibrium, as required.\nProof of Lemma 5.3. We construct a simple example with n homogeneous agents where the BR dynamics cycles. Each agent has a cost function ci(xi) = n\u22121n2 xi. It is not hard to derive that the unique equilibrium is at (1, 1, . . . , 1) (see, e.g., Vojnovic\u0301 (2016)). We will construct a simple two-step cycle where the agents play (x, x, . . . , x) and (y, y, . . . , y) repeatedly, where x < 1 and y > 1. This implies that if the agents enter this cycle, then they will never reach an \u03f5-approximate equilibrium for small enough \u03f5.\nLet us try to construct this cycle, i.e., we try to compute the values of x and y given \u2206t, which will then tell us the required bound on \u2206t to have such a cycle. From the first-order conditions, for a linear cost function, we have a closed-form formula for the BR as\n\u2202ui(z, s\u2212i) \u2202z = 0 =\u21d2 s\u2212i (z + s\u2212i)2 = n\u2212 1 n2\n=\u21d2 BRi(s\u2212i) = n \u221a\ns\u2212i n\u2212 1 \u2212 s\u2212i.\nAs x is the next action of each agent if everyone is playing y, so\nx = y +\u2206t \u00b7 (BR((n\u2212 1)y)\u2212 y) = y+\u2206t \u00b7 (n\u221ay\u2212 (n\u2212 1)y\u2212 y) = y+n\u2206t(\u221ay\u2212 y).\nLet \u03b2 = n\u2206t, we get\nx = \u03b2 \u221a y + (1\u2212 \u03b2)y.\nBy symmetry, as y is the next action for an agent if everyone is playing x, we get\ny = \u03b2 \u221a x+ (1\u2212 \u03b2)x \u21d0\u21d2 (y \u2212 (1\u2212 \u03b2)x)2 = \u03b22x.\nCombining the two equations in x and y, in particular, replacing x to get an equation in a single variable y, we get\n(y \u2212 (1\u2212 \u03b2)(\u03b2\u221ay + (1\u2212 \u03b2)y)))2\n= \u03b22(\u03b2 \u221a y + (1\u2212 \u03b2)y)\n\u21d0\u21d2 (\u03b2(2\u2212 \u03b2)y \u2212 \u03b2(1\u2212 \u03b2)\u221ay)2\n= \u03b22(\u03b2 \u221a y + (1\u2212 \u03b2)y)\n\u21d0\u21d2 (2\u2212 \u03b2)2y2 + (1\u2212 \u03b2)2y \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2)y\u221ay = \u03b2 \u221a y + (1\u2212 \u03b2)y \u21d0\u21d2 \u221ay((2\u2212 \u03b2)2y\u221ay \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2)y \u2212 \u03b2(1\u2212 \u03b2)\u221ay \u2212 \u03b2) = 0.\nFrom the above equation, we get our first root as \u221a y = 0 =\u21d2 y = 0. This is the degenerate solution that corresponds to everyone playing 0, and therefore, the BR also being 0 (see Section 3 for a discussion on BR to 0). Let us focus on the other roots. For simplicity, let z = \u221a y. We have\n(2\u2212 \u03b2)2z3 \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2)z2 \u2212 \u03b2(1\u2212 \u03b2)z \u2212 \u03b2 = 0.\nNotice that z = 1 is a root because\n(2\u2212 \u03b2)2 \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2)\u2212 \u03b2(1\u2212 \u03b2)\u2212 \u03b2 = (2\u2212 \u03b2)((2\u2212 \u03b2)\u2212 2(1\u2212 \u03b2))\u2212 \u03b2 + \u03b22 \u2212 \u03b2 = (2\u2212 \u03b2)\u03b2 \u2212 2\u03b2 + \u03b22 = 0.\nThis root \u221a y = z = 1 =\u21d2 y = 1 corresponds to the equilibrium (1, 1, . . . , 1). Let us now derive the two other roots, which are of our primary interest as they give us the cycle. First, let us factor out the root z = 1 to convert the cubic equation to a quadratic equation. Let z\u0302 = z \u2212 1 \u21d0\u21d2 z = z\u0302 + 1. Replacing z by z\u0302 + 1, we get\n(2\u2212 \u03b2)2(z\u0302 + 1)3 \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2)(z\u0302 + 1)2\n\u2212 \u03b2(1\u2212 \u03b2)(z\u0302 + 1)\u2212 \u03b2 = 0 \u21d0\u21d2 z\u03023(2\u2212 \u03b2)2 + z\u03022(3(2\u2212 \u03b2)2 \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2))\n+ z\u0302(3(2\u2212 \u03b2)2 \u2212 4(2\u2212 \u03b2)(1\u2212 \u03b2)\u2212 \u03b2(1\u2212 \u03b2)) + (2\u2212 \u03b2)2 \u2212 2(2\u2212 \u03b2)(1\u2212 \u03b2)\u2212 \u03b2(1\u2212 \u03b2)\u2212 \u03b2 = 0\n\u21d0\u21d2 z\u0302(z\u03022(2\u2212 \u03b2)2 + (2\u2212 \u03b2)(4\u2212 \u03b2)z\u0302 + (4\u2212 \u03b2)) = 0.\nSolving the above equation, in addition to the root z\u0302 = 0 that we have already considered, we get the following two roots\nz\u0302 = \u03b2 \u2212 4\u00b1\n\u221a \u03b2(\u03b2 \u2212 4)\n2(2\u2212 \u03b2)\n\u21d0\u21d2 z = z\u0302 + 1 = \u03b2 \u00b1\n\u221a \u03b2(\u03b2 \u2212 4)\n2(\u03b2 \u2212 2)\n\u21d0\u21d2 y = z2 = \u03b2(\u03b2 \u2212 2)\u00b1 \u03b2\n\u221a \u03b2(\u03b2 \u2212 4)\n2(\u03b2 \u2212 2)2 .\nIn particular, x = \u03b2(\u03b2\u22122)\u2212\u03b2 \u221a\n\u03b2(\u03b2\u22124) 2(\u03b2\u22122)2 < 1 and y =\n\u03b2(\u03b2\u22122)\u2212\u03b2 \u221a\n\u03b2(\u03b2\u22124) 2(\u03b2\u22122)2 > 1 are the two roots, assuming \u03b2 \u2265 4.\nIn particular, if we take \u03b2 = 6, we get x = 3(2\u2212 \u221a 3)\n8 =\n0.1005 and y = 3(2+ \u221a 3)\n8 = 1.3995. As \u03b2 = \u2206t \u00b7n = 6 implies that \u2206t = 6n , we get a two step cycle for \u2206t = 6 n . We can create such cycles for all \u2206t > 4n , so \u2206t \u2264 4 n = O( 1 n ) is necessary for convergence.\nProof of Lemma 5.4. We consider examples with two agents. Let the first agent have a cost function c1(z) = z and the second agent have a cost function c2(z) = z/d for\nall z \u2265 0 and some d \u2265 1. Notice that agent 2 has a lower cost for the same output than agent 1, and c\u20321(z) = 1 and c\u20322(z) = 1/d. So, mini,z c \u2032 i(z)\nmaxi,z c\u2032i(z) = 1d . Let \u03b1 = 1/\u2206t. We next\nshow non-convergence for \u2206t = \u2126(1/d) \u21d0\u21d2 \u03b1 = O(d), as required.\nAs the cost functions of the agents is linear, using the firstorder conditions given in equation 2, the BR of agent 1 is given by the explicit formula BR1(s\u22121) = BR1(x2) =\u221a x2 \u2212 x2. Similarly, the BR of agent 2 is BR2(s\u22122) =\nBR2(x1) = \u221a dx1 \u2212 x1. The unique equilibrium can also be explicitly computed and is equal to (\nd (1+d)2 ,\nd2\n(1+d)2\n) .\nThe non-linearity of the BR dynamics makes it tedious to analytically compute the cycles for non-homogeneous agents, especially when \u2206t is strictly less than 1 . For example, if d = 16 and \u2206t = 0.5, starting from the state (1/10, 1/10), the dynamics ends up in the cycle in Table 1 where x(t) = x(t + 6). So, we compute the cycles using numerical simulations.\nWe simulate the BR dynamics starting from the profile (1/10, 1/10). Given a value of d, we find the critical value of \u03b1, denoted by \u03b1\u2217(d), such that the dynamics converges if \u2206t < 1\u03b1\u2217(d) but goes into a cycle if \u2206t \u2265 1 \u03b1\u2217(d) . We do this by a simple binary search over the values of \u2206t = 1/\u03b1. We notice that \u03b1\u2217(d) is an exact linear function of d, as required; plotted in Figure 1. The code is included in the supplementary material.\nProof of Lemma 5.5. From Lemma 5.1 and 5.2, we know that at time t, if the step-size \u03b7t \u2264 \u03b1, where \u03b1 = \u0398 (\nx3min n(1+B2)\n) , then the potential V decreases by a factor\nof (1 \u2212 \u03b7t). As \u03b7t \u2192 0, there exists a \u03c4 such that \u03b7t \u2264 \u03b1 and V (t+ 1) \u2264 (1\u2212 \u03b7t)V (t) for all t \u2265 \u03c4 . So, we get for t \u2265 \u03c4 ,\nV (t+ 1) \u2264 ( t\u220f\nk=\u03c4\n(1\u2212 \u03b7k))V (\u03c4) \u2264 ( t\u220f\nk=\u03c4\ne\u2212\u03b7k)V (\u03c4)\n= e\u2212 \u2211t k=\u03c4 \u03b7kV (\u03c4).\nAs \u2211t k=\u03c4 \u03b7k \u2192 \u221e, so e\u2212 \u2211t k=\u03c4 \u03b7k \u2192 0 and V (t) \u2192 0."
        },
        {
            "heading": "B. Agents Move at Different Rates",
            "text": "When the agents move at arbitrary rates that can depend upon time, it is easy to see that the continuous-time dynamics may not converge because we can simulate a discretetime dynamics using the continuous-time dynamics by adjusting the rates. In particular, consider the example for two agents presented in Lemma 5.4 where a discrete-time dynamics with a step-size of 1/2 goes into a cycle. We can simulate this dynamics by slowing down the first agent and allowing the second agent to move, and then slowing down the second agent and allowing the first agent to move, and repeating the process. In a similar manner, the cycles presented in Ghosh & Goldberg (2023) and Ghosh (2023), where only one agent moves at a time, can also be simulated.\nOn the other hand, simple modifications to the continuous BR dynamics converge. For example, if the dynamics followed by the agents is\ndxi dt = f(x, t)(BRi(s\u2212i)\u2212 xi),\nwhere f(x, t) is some positive scaling factor that may depend upon the profile x and the time t but is the same for all agents. This dynamics follows exactly the same path as the original continuous BR dynamics (albeit at a different speed), and thus converges. Let us consider an additional modification to the dynamics\ndxi dt = \u03b7if(x, t)(BRi(s\u2212i)\u2212 xi),\nwhere \u03b7i > 0 is some positive constant specific to agent i. This dynamics speeds up certain agents relative to others.\nOur intuition is that this dynamics also converges, as we are essentially stretching or squeezing the vector field along different dimensions of the phase space, but a formal analysis remains open. Studying similar variations to the dynamics and showing their convergence (or non-convergence) is an important direction for future work."
        }
    ],
    "title": "Continuous-Time Best-Response and Related Dynamics in  Tullock Contests with Convex Costs",
    "year": 2024
}