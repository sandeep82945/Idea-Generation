{
    "abstractText": "We present DreamAvatar, a text-and-shape guided framework for generating high-quality 3D human avatars with controllable poses. While encouraging results have been reported by recent methods on text-guided 3D common object generation, generating high-quality human avatars remains an open challenge due to the complexity of the human body\u2019s shape, pose, and appearance. We propose DreamAvatar to tackle this challenge, which utilizes a trainable NeRF for predicting density and color for 3D points and pretrained text-to-image diffusion models for providing 2D self-supervision. Specifically, we leverage the SMPL model to provide shape and pose guidance for the generation. We introduce a dual-observation-space design that involves the joint optimization of a canonical space and a posed space that are related by a learnable deformation field. This facilitates the generation of more complete textures and geometry faithful to the target pose. We also jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the common multi-face \u201cJanus\u201d problem and improve facial details in the generated avatars. Extensive evaluations demonstrate that DreamAvatar significantly outperforms existing methods, establishing a new state-of-the-art for text-and-shape guided 3D human avatar generation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yukang Cao"
        },
        {
            "affiliations": [],
            "name": "Yan-Pei Cao"
        },
        {
            "affiliations": [],
            "name": "Kai Han"
        },
        {
            "affiliations": [],
            "name": "Ying Shan"
        },
        {
            "affiliations": [],
            "name": "Kwan-Yee K. Wong"
        }
    ],
    "id": "SP:3ec44b3fba8c348037e4c8d70218dda8bcf48caa",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Olga Diamanti",
                "Ioannis Mitliagkas",
                "Leonidas Guibas"
            ],
            "title": "Learning representations and generative models for 3d point clouds",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Seungjun Nah",
                "Xun Huang",
                "Arash Vahdat",
                "Jiaming Song",
                "Karsten Kreis",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine",
                "Bryan Catanzaro"
            ],
            "title": "ediffi: Text-to-image diffusion models with an ensemble of expert denoisers",
            "venue": "arXiv preprint arXiv:2211.01324,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander W. Bergman",
                "Petr Kellnhofer",
                "Wang Yifan",
                "Eric R. Chan",
                "David B. Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Generative neural articulated radiance fields",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Federica Bogo",
                "Angjoo Kanazawa",
                "Christoph Lassner",
                "Peter Gehler",
                "Javier Romero",
                "Michael J Black"
            ],
            "title": "Keep it smpl: Automatic estimation of 3d human pose and shape from a single image",
            "venue": "In European Conference on Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Yukang Cao",
                "Guanying Chen",
                "Kai Han",
                "Wenqi Yang",
                "Kwan-Yee K Wong"
            ],
            "title": "Jiff: Jointly-aligned implicit face function for high quality single view clothed human reconstruction",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yukang Cao",
                "Yan-Pei Cao",
                "Kai Han",
                "Ying Shan",
                "Kwan- Yee K Wong"
            ],
            "title": "Guide3d: Create 3d avatars from text and image guidance",
            "venue": "arXiv preprint arXiv:2308.09705,",
            "year": 2023
        },
        {
            "authors": [
                "Eric R. Chan",
                "Connor Z. Lin",
                "Matthew A. Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis",
                "Tero Karras",
                "Gordon Wetzstein"
            ],
            "title": "Efficient geometry-aware 3D generative adversarial networks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Rui Chen",
                "Yongwei Chen",
                "Ningxin Jiao",
                "Kui Jia"
            ],
            "title": "Fantasia3d: Disentangling geometry and appearance for highquality text-to-3d content creation",
            "year": 2023
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Hao Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Alvaro Collet",
                "Ming Chuang",
                "Pat Sweeney",
                "Don Gillett",
                "Dennis Evseev",
                "David Calabrese",
                "Hugues Hoppe",
                "Adam G. Kirk",
                "Steve Sullivan"
            ],
            "title": "High-quality streamable free-viewpoint video",
            "venue": "ACM Transactions on Graphics,",
            "year": 2015
        },
        {
            "authors": [
                "Massimiliano Favalli",
                "Alessandro Fornaciai",
                "Ilaria Isola",
                "Simone Tarquini",
                "Luca Nannipieri"
            ],
            "title": "Multiview 3d reconstruction in geosciences",
            "venue": "Computers Geosciences,",
            "year": 2012
        },
        {
            "authors": [
                "Jianglin Fu",
                "Shikai Li",
                "Yuming Jiang",
                "Kwan-Yee Lin",
                "Chen Qian",
                "Chen Change Loy",
                "Wayne Wu",
                "Ziwei Liu"
            ],
            "title": "Stylegan-human: A data-centric odyssey of human generation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Matheus Gadelha",
                "Subhransu Maji",
                "Rui Wang"
            ],
            "title": "3d shape induction from 2d views of multiple objects",
            "venue": "In International Conference on 3D Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "year": 2023
        },
        {
            "authors": [
                "Xiangjun Gao",
                "Jiaolong Yang",
                "Jongyoo Kim",
                "Sida Peng",
                "Liu Zicheng",
                "Xin Tong"
            ],
            "title": "Mps-nerf: Generalizable 3d human rendering from multiview images",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Yuan-Chen Guo",
                "Ying-Tian Liu",
                "Ruizhi Shao",
                "Christian Laforte",
                "Vikram Voleti",
                "Guan Luo",
                "Chia-Hao Chen",
                "Zi- Xin Zou",
                "Chen Wang",
                "Yan-Pei Cao",
                "Song-Hai Zhang"
            ],
            "title": "threestudio: A unified framework for 3d content generation",
            "venue": "https://github.com/threestudio-project/ threestudio,",
            "year": 2023
        },
        {
            "authors": [
                "Xiao Han",
                "Yukang Cao",
                "Kai Han",
                "Xiatian Zhu",
                "Jiankang Deng",
                "Yi-Zhe Song",
                "Tao Xiang",
                "Kwan-Yee K Wong"
            ],
            "title": "Headsculpt: Crafting 3d head avatars with text",
            "venue": "arXiv preprint arXiv:2306.03038,",
            "year": 2023
        },
        {
            "authors": [
                "Philipp Henzler",
                "Niloy J Mitra",
                "Tobias Ritschel"
            ],
            "title": "Escaping plato\u2019s cave: 3d shape from adversarial rendering",
            "venue": "In International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Avatarclip: Zero-shot textdriven generation and animation of 3d avatars",
            "venue": "ACM Transactions on Graphics,",
            "year": 2022
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Zhaoxi Chen",
                "Yushi LAN",
                "Liang Pan",
                "Ziwei Liu"
            ],
            "title": "EVA3d: Compositional 3d human generation from 2d image collections",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen- Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Yukun Huang",
                "Jianan Wang",
                "Ailing Zeng",
                "He Cao",
                "Xianbiao Qi",
                "Yukai Shi",
                "Zheng-Jun Zha",
                "Lei Zhang"
            ],
            "title": "Dreamwaltz: Make a scene with complex 3d animatable avatars",
            "year": 2023
        },
        {
            "authors": [
                "Ajay Jain",
                "Ben Mildenhall",
                "Jonathan T. Barron",
                "Pieter Abbeel",
                "Ben Poole"
            ],
            "title": "Zero-shot text-guided object generation with dream fields",
            "year": 2022
        },
        {
            "authors": [
                "Suyi Jiang",
                "Haoran Jiang",
                "Ziyu Wang",
                "Haimin Luo",
                "Wenzheng Chen",
                "Lan Xu"
            ],
            "title": "Humangen: Generating human radiance fields with explicit priors",
            "venue": "arXiv preprint arXiv:2212.05321,",
            "year": 2022
        },
        {
            "authors": [
                "Heewoo Jun",
                "Alex Nichol"
            ],
            "title": "Shap-e: Generating conditional 3d implicit functions",
            "venue": "arXiv preprint arXiv:2305.02463,",
            "year": 2023
        },
        {
            "authors": [
                "Nasir Mohammad Khalid",
                "Tianhao Xie",
                "Eugene Belilovsky",
                "Popa Tiberiu"
            ],
            "title": "Clip-mesh: Generating textured meshes from text using pretrained image-text models. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Tingting Liao",
                "Hongwei Yi",
                "Yuliang Xiu",
                "Jiaxaing Tang",
                "Yangyi Huang",
                "Justus Thies",
                "Michael J Black"
            ],
            "title": "Tada! text to animatable digital avatars",
            "venue": "arXiv preprint arXiv:2308.10899,",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: Highresolution text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2211.10440,",
            "year": 2022
        },
        {
            "authors": [
                "Lingjie Liu",
                "Marc Habermann",
                "Viktor Rudnev",
                "Kripasindhu Sarkar",
                "Jiatao Gu",
                "Christian Theobalt"
            ],
            "title": "Neural actor: Neural free-view synthesis of human actors with pose control",
            "venue": "ACM SIGGRAPH Asia,",
            "year": 2021
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Rundi Wu",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Sergey Zakharov",
                "Carl Vondrick"
            ],
            "title": "Zero-1-to-3: Zero-shot one image to 3d object",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Loper",
                "Naureen Mahmood",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "SMPL: A skinned multiperson linear model",
            "venue": "TOG,",
            "year": 2015
        },
        {
            "authors": [
                "Sebastian Lunz",
                "Yingzhen Li",
                "Andrew Fitzgibbon",
                "Nate Kushman"
            ],
            "title": "Inverse graphics gan: Learning to generate 3d shapes from unstructured 2d data",
            "venue": "arXiv preprint arXiv:2002.12674,",
            "year": 2020
        },
        {
            "authors": [
                "Shitong Luo",
                "Wei Hu"
            ],
            "title": "Diffusion probabilistic models for 3d point cloud generation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Gal Metzer",
                "Elad Richardson",
                "Or Patashnik",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Latent-nerf for shape-guided generation of 3d shapes and textures",
            "venue": "arXiv preprint arXiv:2211.07600,",
            "year": 2022
        },
        {
            "authors": [
                "Oscar Michel",
                "Roi Bar-On",
                "Richard Liu",
                "Sagie Benaim",
                "Rana Hanocka"
            ],
            "title": "Text2mesh: Text-driven neural stylization for meshes",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kaichun Mo",
                "Paul Guerrero",
                "Li Yi",
                "Hao Su",
                "Peter Wonka",
                "Niloy Mitra",
                "Leonidas Guibas"
            ],
            "title": "Structurenet: Hierarchical graph networks for 3d shape generation",
            "venue": "ACM Transactions on Graphics,",
            "year": 2019
        },
        {
            "authors": [
                "Chong Mou",
                "Xintao Wang",
                "Liangbin Xie",
                "Jian Zhang",
                "Zhongang Qi",
                "Ying Shan",
                "Xiaohu Qie"
            ],
            "title": "T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.08453,",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Munkberg",
                "Jon Hasselgren",
                "Tianchang Shen",
                "Jun Gao",
                "Wenzheng Chen",
                "Alex Evans",
                "Thomas M\u00fcller",
                "Sanja Fidler"
            ],
            "title": "Extracting triangular 3d models, materials, and lighting from images. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Alex Nichol",
                "Heewoo Jun",
                "Prafulla Dhariwal",
                "Pamela Mishkin",
                "Mark Chen"
            ],
            "title": "Point-e: A system for generating 3d point clouds from complex prompts",
            "venue": "arXiv preprint arXiv:2212.08751,",
            "year": 2022
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "title": "Neural articulated radiance field",
            "venue": "In International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Atsuhiro Noguchi",
                "Xiao Sun",
                "Stephen Lin",
                "Tatsuya Harada"
            ],
            "title": "Unsupervised learning of efficient geometry-aware neural articulated representations",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Vasileios Choutas",
                "Nima Ghorbani",
                "Timo Bolkart",
                "Ahmed AA Osman",
                "Dimitrios Tzionas",
                "Michael J Black"
            ],
            "title": "Expressive body capture: 3d hands, face, and body from a single image",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Sida Peng",
                "Yuanqing Zhang",
                "Yinghao Xu",
                "Qianqian Wang",
                "Qing Shuai",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "arXiv preprint arXiv:2209.14988,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Elad Richardson",
                "Gal Metzer",
                "Yuval Alaluf",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Texture: Text-guided texturing of 3d shapes",
            "venue": "arXiv preprint arXiv:2302.01721,",
            "year": 2023
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "year": 2022
        },
        {
            "authors": [
                "Simo Ryu"
            ],
            "title": "Low-rank adaptation for fast text-to-image diffusion fine-tuning",
            "venue": "cloneofsimo/lora",
            "year": 2023
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Shunsuke Saito",
                "Zeng Huang",
                "Ryota Natsume",
                "Shigeo Morishima",
                "Angjoo Kanazawa",
                "Hao Li"
            ],
            "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization",
            "venue": "In International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Sanghi",
                "Hang Chu",
                "Joseph G Lambourne",
                "Ye Wang",
                "Chin-Yi Cheng",
                "Marco Fumero",
                "Kamal Rahimi"
            ],
            "title": "Malekshan. Clip-forge: Towards zero-shot text-to-shape generation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tianchang Shen",
                "Jun Gao",
                "Kangxue Yin",
                "Ming-Yu Liu",
                "Sanja Fidler"
            ],
            "title": "Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyi Wang",
                "Cheng Lu",
                "Yikai Wang",
                "Fan Bao",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Prolificdreamer: High-fidelity 10 and diverse text-to-3d generation with variational score distillation",
            "venue": "arXiv preprint arXiv:2305.16213,",
            "year": 2023
        },
        {
            "authors": [
                "Chung-Yi Weng",
                "Brian Curless",
                "Pratul P Srinivasan",
                "Jonathan T Barron",
                "Ira Kemelmacher-Shlizerman"
            ],
            "title": "Humannerf: Free-viewpoint rendering of moving people from monocular video",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiajun Wu",
                "Chengkai Zhang",
                "Tianfan Xue",
                "Bill Freeman",
                "Josh Tenenbaum"
            ],
            "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Yuliang Xiu",
                "Jinlong Yang",
                "Xu Cao",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "ECON: Explicit Clothed humans Obtained from Normals",
            "venue": "In IEEELearning Transferable Visual Models From Natural Language Supervision Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jiale Xu",
                "Xintao Wang",
                "Weihao Cheng",
                "Yan-Pei Cao",
                "Ying Shan",
                "Xiaohu Qie",
                "Shenghua Gao"
            ],
            "title": "Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2212.14704,",
            "year": 2022
        },
        {
            "authors": [
                "Guandao Yang",
                "Xun Huang",
                "Zekun Hao",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Bharath Hariharan"
            ],
            "title": "Pointflow: 3d point cloud generation with continuous normalizing flows",
            "venue": "In International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaohui Zeng",
                "Arash Vahdat",
                "Francis Williams",
                "Zan Gojcic",
                "Or Litany",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "Lion: Latent point diffusion models for 3d shape generation",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yifei Zeng",
                "Yuanxun Lu",
                "Xinya Ji",
                "Yao Yao",
                "Hao Zhu",
                "Xun Cao"
            ],
            "title": "Avatarbooth: High-quality and customizable 3d human avatar generation",
            "year": 2023
        },
        {
            "authors": [
                "Huichao Zhang",
                "Bowen Chen",
                "Hao Yang",
                "Liao Qu",
                "Xu Wang",
                "Li Chen",
                "Chao Long",
                "Feida Zhu",
                "Kang Du",
                "Min Zheng"
            ],
            "title": "Avatarverse: High-quality & stable 3d avatar creation from text and pose, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        },
        {
            "authors": [
                "Yuxuan Zhang",
                "Wenzheng Chen",
                "Huan Ling",
                "Jun Gao",
                "Yinan Zhang",
                "Antonio Torralba",
                "Sanja Fidler"
            ],
            "title": "Image gans meet differentiable rendering for inverse graphics and interpretable 3d neural rendering",
            "year": 2021
        },
        {
            "authors": [
                "Linqi Zhou",
                "Yilun Du",
                "Jiajun Wu"
            ],
            "title": "3d shape generation and completion through point-voxel diffusion",
            "venue": "In International Conference on Computer Vision,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The creation of 3D graphical human models has received great attention in recent years due to its wide-ranging applications in fields such as film-making, video games, AR/VR, and human-robotic interaction. Traditional methods for building such complex 3D models require thousands of man-hours of trained artists and engineers [10, 12], making the process both time-consuming and highly expertdependent. With the development of deep learning methods, we have witnessed the emergence of promising meth-\n*Equal contributions \u2020 Corresponding authors \u2021 Webpage: https://yukangcao.github.io/DreamAvatar/\nods [5, 54, 61] which can reconstruct 3D human models from monocular images. These techniques, however, still face challenges in fully recovering details from the input images and rely heavily on the training dataset. To tackle these challenges and simplify the modeling process, adopting generative models for 3D human avatar modeling has recently received increasing attention from the research community. This approach has the potential to alleviate the need for large 3D datasets and facilitate easier and more accessible 3D human avatar modeling.\nTo leverage the potential of 2D generative image models for 3D content generation, recent methods [8, 29, 31, 35, 46] have utilized pretrained text-guided image diffusion models to optimize 3D implicit representations (e.g., NeRFs [37] and DMTet [40, 56]). DreamFusion [46] introduces a novel Score Distillation Sampling (SDS) strategy to selfsupervise the optimization process and achieves promising results. However, human bodies, which are the primary focus of this paper, exhibit a complex articulated structure, with head, arms, hands, trunk, legs, feet, etc., each capable of posing in various ways. As a result, while DreamFusion [46] and subsequent methods (e.g., Magic3D [29], ProlificDreamer [58], Fantasia3D [8]) produce impressive results, they lack the proper constraints to enforce consistent 3D human structure and often struggle to generate detailed textures for 3D human avatars. Latent-NeRF [35] introduces a sketch-shape loss based on the 3D shape guidance, but it still faces challenges in generating reasonable results for human bodies.\nIn this paper, we present DreamAvatar, a novel framework for generating high-quality 3D human avatars from text prompts and shape priors. Inspired by previous works [29, 46], DreamAvatar employs a trainable NeRF as the base representation for predicting density and color features for each 3D point. Coupled with pretrained textto-image diffusion models [50, 68], DreamAvatar can be trained to generate 3D avatars using 2D self-supervision. The key innovation of DreamAvatar lies in three main aspects. Firstly, we leverage the SMPL model [32] to provide a shape prior, which yields robust shape and pose guidance for the generation process. Secondly, we introduce a dual-\nar X\niv :2\n30 4.\n00 91\n6v 3\n[ cs\n.C V\n] 3\n0 N\nov 2\n02 3\nTrack and field athlete\nUchiha Sasuke\nCrystal Maiden as in Dota2\nI am Groot\nBody builder wearing a tanktop\nLink from Zelda\nJoker\nSpiderman\nBuddhist Monk\nAlien\nHatake Kakashi\nFlash from DC\nA woman in a hippie outfit\nLuffy from One Piece\nWoody in The Toy Story\nWonder Woman\nHipster man\nBuff Chewbacca\nC-3PO from Star Wars\nElectro from Marvel\nFigure 1. Results of DreamAvatar. DreamAvatar can generate high-quality geometry and texture for any type of human avatar.\nobservation-space (DOS) design consisting of a canonical space and a posed space that are related by a learnable deformation field and are jointly optimized. This facilitates the generation of more complete textures and geometry faithful to the target pose. Thirdly, we propose to jointly optimize the losses computed from the full body and from the zoomed-in 3D head to alleviate the multi-face \u201cJanus\u201d problem and improve facial details in the generated avatars.\nWe extensively evaluate DreamAvatar on generating movie/anime/video game characters, as well as general people, and compare it with previous methods. Experimental results show that our method significantly outperforms existing methods and can generate high-quality 3D human avatars with text-consistent geometry and geometryconsistent texture. We will make our code publicly available after publication."
        },
        {
            "heading": "2. Related Work",
            "text": "Text-guided 2D image generation. Recently, the CLIP model [47] (Contrastive Language-Image Pre-training) was proposed with the aim of classifying images and text by mapping them to a shared feature space. However, this model is not consistent with the way human perceives language, and it may not fully capture the intended meanings. With the improvements in neural networks and text-image datasets, the diffusion model has been introduced to handle more complex semantic concepts [2, 35, 48, 53]. Follow-up methods are designed to improve computational efficiency, for instance, through utilizing a cascade of super-resolution models [2, 53] or sampling from a low-resolution latent space and decoding the latent features into high-resolution images [35]. DreamBooth [51] fine-tunes the diffusion model for certain subjects, while ControlNet [67] and T2IAdapter [39] propose controlling the pretrained diffusion\nmodels with additional information. However, text-to-3D generation remains a challenge due to the lack of text-3D paired datasets and the associated high training cost .\nText-guided 3D content generation. Text-guided 3D content generation methods have emerged based on the success of text-guided 2D image generation. Earlier works, such as CLIP-forge [55], generate objects by learning a normalizing flow model from textual descriptions, but these methods are computationally expensive. DreamField [24], CLIP-mesh [27], AvatarCLIP [20], Text2Mesh [36], and Dream3D [62] rely on a pretrained image-text model [47] to optimize the underlying 3D representation (e.g., NeRF or mesh).\nRecently, DreamFusion [46] proposes score distillation sampling based on the pretrained diffusion model [53] to enable text-guided 3D generation. Magic3D [29] improves it by introducing a coarse-to-fine pipeline to generate fine-\ngrained 3D textured meshes. Point-E [41] and Shap-E [26] optimize the point cloud based on the diffusion model. Latent-NeRF [35] improves training efficiency by directly optimizing the latent features. TEXTure [49] applies a depth-diffusion model [50] to generate texture maps for a given 3D mesh. Fantasia3D [8] proposes a disentangled training strategy for geometry and texture generation. Guide3D [6] proposes to transfer multi-view generated images to 3D avatars. ProlificDreamer [58] introduces Variational Score Distillation (VSD) for better diversity and quality. Despite their promising performance, these methods still struggle to generate text-guided 3D human avatars due to the inherent challenges of this task.\n3D human generative models. 3D generative methods based on 3D voxel grids [14, 19, 33, 60], point clouds [1, 34, 38, 63, 64, 70], and meshes [69] often require expensive and limited 3D datasets. In recent years, various meth-\nods [9, 30, 42, 45, 59] have been proposed to utilize NeRF and train on 2D human videos for novel view synthesis. Following these works, EG3D [7] and GNARF [3] propose a tri-plane representation and use GANs for 3D generation from latent codes. ENARF-GAN [43] extends NARF [42] to human representation. Meanwhile, EVA3D [21] and HumanGen [25] propose to generate human radiance fields directly from the 2D StyleGAN-Human [13] dataset. Although these methods have produced convincing results, they do not have the ability to \u201cdream\u201d or generate new subjects that have not been seen during training."
        },
        {
            "heading": "3. Methodology",
            "text": "Here, we introduce our text-and-shape guided generative network, DreamAvatar, which utilizes a trainable NeRF and pretrained diffusion models [50, 68] to generate 3D human avatars under controllable poses. DreamAvatar incorporates two observation spaces, namely a canonical space and a posed space, which are related by a learnable deformation field and are jointly optimized through a shared trainable NeRF (see Fig. 3). We jointly optimize the losses computed from the full-body and from the zoom-in 3D head to alleviate the multi-face \u201cJanus\u201d problem and improve the facial details in the generated avatars. In the following subsections, we first provide the preliminaries that underpin our method in Sec. 3.1. Next, we delve into the details of our method and discuss: (1) the density field derived from the SMPL model for evolving the geometry, (2) the dual observation spaces related by a learnable deformation field, and (3) the joint optimization of the losses computed from the full body and from the zoom-in 3D head in Sec. 3.2."
        },
        {
            "heading": "3.1. Preliminaries",
            "text": "Text-guided 3D generation methods Recent text-guided 3D generation models [46, 58] showcase promising results by incorporating three fundamental components:\n(1) NeRF that represents a 3D scene via an implicit function, formulated as\nF\u03b8(\u03b3(x)) 7\u2192 (\u03c3, c), (1)\nwhere x is a 3D point that is processed by the grid frequency encoder \u03b3(\u00b7) [37], and \u03c3 and c denote its density value and color respectively. Typically, the implicit function F\u03b8(\u00b7) is implemented as an MLP with trainable parameters \u03b8.\n(2) Volume Rendering technique that effectively renders a 3D scene onto a 2D image. For each image pixel, the rendering is done by casting a ray r from the pixel location into the 3D scene and sampling 3D points \u00b5i along r. The density and color of the sampled points are predicted by F\u03b8. The RGB color C of each image pixel is then given by\nC(r) = \u2211 i Wici, Wi = \u03b1i \u220f j<i (1\u2212 \u03b1j) (2)\nwhere \u03b1i = 1\u2212 e(\u2212\u03c3i||\u00b5i\u2212\u00b5i+1||). (3) Variational Score Distillation (VSD) derived on textguided diffusion models \u03d5 [50, 53]. We employ a pretrained diffusion model [50, 68] with a learned denoising function \u03f5pre(xt; y, t). Here xt denotes the noisy image at timestep t, and y is the text embedding. Given an image g(\u03b8, c) rendered from the NeRF with camera parameters c, we add random noise \u03f5 to obtain a noisy image xt = \u03b1tg(\u03b8, c) + \u03c3t\u03f5 (\u03b1t and \u03c3t are hyperparameters). We then parameterize \u03f5 using LoRA (Low-Rank Adaptation [22, 52]) of the pretrained model \u03f5pre(xt; y, t) to obtain \u03f5\u03d5, and add camera parameters c to the condition embeddings in the network. The gradient for updating the NeRF is then given by \u2207\u03b8LVSD(\u03b8) \u225c Et,\u03f5,c [ \u03c9(t) ( \u03f5pre(xt, t, y)\u2212\u03f5\u03d5(xt, t, c, y) ) \u2202g(\u03b8,c)\n\u2202\u03b8\n] ,\n(3) where w(t) is a weighting function that depends on the timestep t.\nSMPL [4, 44] 3D parametric human model It builds a 3D human shape using 6,890 body vertices. Formally, by assembling pose parameters \u03be and shape parameters \u03b2, we can obtain the 3D SMPL human model by:\nTP (\u03b2, \u03be) = T\u0304 +BS(\u03b2;S) +BP (\u03be;P), (4) M(\u03b2, \u03be) = LBS(TP (\u03b2, \u03be), J(\u03b2), \u03be,W), (5)\nwhere TP (\u00b7) represents the non-rigid deformation from the canonical model T\u0304 using the shape blend shape function BS and pose blend shape function BP . S and P are the principal components of vertex displacements. LBS(\u00b7) denotes the linear blend skinning function, corresponding to articulated deformation. It poses TP (\u00b7) based on the pose parameters \u03be and joint locations J(\u03b2), using the blend weights W , individually for each body vertex:\nvp = G \u00b7 vc, G = K\u2211\nk=1\nwkGk(\u03be, jk), (6)\nwhere vc is an SMPL vertex under the canonical pose, vp denotes the corresponding vertex under the given pose, wk is the skinning weight, Gk(\u03be, jk) is the affine deformation that transforms the k-th joint jk from the canonical space to posed space, and K is the number of neighboring joints.\nUnlike the original SMPL which defines \u201cT-pose\u201d as the canonical model, here, we adopt \u201cA-pose\u201d as the canonical model which is a more natural human rest pose for the diffusion models to understand (see Fig. 3)."
        },
        {
            "heading": "3.2. DreamAvatar",
            "text": "As illustrated in Fig. 3, our proposed framework takes as input a text prompt and SMPL parameters, defining the target shape and pose in the posed space. DreamAvatar conducts Variational Score Distillation (VSD)-based optimization [58] in both the canonical space and posed space simultaneously and learns a deformation field relating the two\nspaces. To represent the canonical space and posed space, we utilize an extended neural radiance field where the density, RGB color value, and normal direction of each sample point can be queried and optimized. We utilize the input SMPL parameters to handle different body parts separately and derive good initial density values in each space. To alleviate the multi-face \u201cJanus\u201d problem and improve facial details in the generated avatars, we optimize, in addition to the loss computed from the full body, the loss computed from the zoomed-in 3D head using a landmark-based ControlNet [67] and a learned special <back-view> token [15].\nSMPL-derived density fields We propose to make our NeRF evolve from the density field derived from the input SMPL model. Specifically, given a 3D point xc in the canonical space, we first calculate its signed distance dc to the SMPL surface in the canonical pose and convert it to a density value \u03c3\u0304c using\n\u03c3\u0304c = max(0, softplus \u22121(\n1 a sigmoid(\u2212dc/a))),\nwhere sigmoid(x) = 1/(1 + e\u2212x), softplus\u22121(x) = log(ex \u2212 1), and a is a predefined hyperparameter [62] which is set to 0.001 in our experiments. Similarly, given a point xp in the posed space (the upper branch in Fig. 3), we compute its density value \u03c3\u0304p from its signed distance dp to the SMPL surface in the target pose.\nDeformation field Inspired by HumanNeRF [59], we employ a deformation field to map points xp from the posed space to their corresponding points x\u0302c in the canonical space. The deformation field is composed of two parts, namely (1) articulated deformation that applies the inverse transformation of SMPL linear blend skinning LBS(\u00b7)\n(Sec. 3.1), and (2) non-rigid motion modelled by an MLP to learn the corrective offset:\nx\u0302c = G\u22121 \u00b7 xp + MLP\u03b8NR(\u03b3(G \u22121 \u00b7 xp)), (7)\nwhere G is obtained from posed SMPL vertex closest to xp.\nDual observation spaces (DOS) Given a 3D point xc in the canonical space and xp in the posed space, we compute their density values \u03c3c, \u03c3p and latent color features cc, cp by\nF (xc, \u03c3\u0304c) = F\u03b8(\u03b3(xc)) + (\u03c3\u0304c,0) 7\u2192 (\u03c3c, cc), (8)\nF (xp, \u03c3\u0304p) = F\u03b8(\u03b3(x\u0302c)) + (\u03c3\u0304p,0) 7\u2192 (\u03c3p, cp), (9)\nwhere x\u0302c denotes the corresponding point of xp in the canonical space, and \u03c3\u0304c and \u03c3\u0304p are the SMPL-derived density values in the canonical space and posed space respectively. Our DOS design serves two main purposes. Firstly, the avatar in rest pose in the canonical space exhibits minimum self-occlusion. Observations in the canonical space therefore facilitate the generation of more complete textures. Observations in the posed space, on the other hand, facilitate the generation of geometry faithful to the target pose. They also provide extra supervision in optimizing the NeRF. Our DOS design also differentiates itself from MPSNeRF [16] and TADA! [28] by prioritizing joint optimization and mutual distillation between the canonical space and posed space. In contrast, existing methods only utilize observations in the posed space without fully exploiting information in the canonical space.\nZoomed-in head Inspired by our previous work [5] which enhances human reconstruction by learning to recover details in the zoomed-in face, we propose to optimize, in addition to the loss computed from the full body, the loss\ncomputed from the zoomed-in 3D head to improve the facial details in the generated 3D avatars. Specifically, we render a zoomed-in head image at each iteration for computing the VSD loss on head. To alleviate the common multiface \u201cJanus\u201d problem, we follow our previous work [18] to employ a landmark-based ControlNet C [67] and a learned special <back-view> token [15] to compute the head VSD loss. The gradient for updating the NeRF now becomes\n\u2207\u03b8LheadVSD(\u03b8) \u225c Et,\u03f5,c [ \u03c9(t) (\u03f5pre(\u00b7)\u2212 \u03f5\u03d5(\u00b7)) \u2202g(\u03b8, c)\n\u2202\u03b8\n] , (10)\n\u03f5pre(\u00b7) = \u03f5pre(xt, t, y, C(P\u03c0)),\u03f5\u03d5(\u00b7) = \u03f5\u03d5(xt, t, c, y, C(P\u03c0)),\nwhere P\u03c0 denotes the facial landmark map obtained from the projection of the SMPL head model."
        },
        {
            "heading": "4. Experiments",
            "text": "We now validate the effectiveness and capability of our proposed framework on a variety of text prompts and provide comparisons with existing text-guided 3D generation methods using the same text prompts.\nImplementation details We follow threestudio [17] to implement the NeRF [37] and Variational Score Distillation in our method. We utilize Ver-2.1 of Stable Diffusion [57] and Ver-1.1 of ControlNet [11, 68] in our implementation. Typically, for each text prompt, we train our network for 10, 000 iterations on one single NVIDIA A40 GPU.\nBaseline methods We compare our method with DreamFusion [46], Latent-NeRF [35], Fantasia3D [8], Magic3D [29], ProlificDreamer [58], DreamWaltz [23], AvatarCLIP [20], and TADA! [28]. We are not able to compare with AvatarBooth [65] and AvatarVerse [66] as their codes are not publicly available."
        },
        {
            "heading": "4.1. Qualitative Evaluations",
            "text": "Avatar generation with different styles In Fig. 1, we provide a diverse set of 3D human avatars, e.g., real-world human beings, movie, anime, and video game characters, generated by our DreamAvatar. We can consistently observe high-quality geometry and texture from all these examples. Avatar generation under different poses In Fig. 2, we validate the effectiveness of our method for generating 3D human avatars in various poses, which is not achievable by other existing methods due to the absence of the shape prior. Thanks to our DOS design, DreamAvatar can maintain high-quality texture and geometry for extreme poses, e.g., complex poses with severe self-occlusion. Comparison with SOTA methods We provide qualitative comparisons with existing SOTA methods in Fig. 4 and Fig. 5. We can observe that our method consistently achieves topologically and structurally correct geometry and texture compared to baseline methods, and outperforms the avatar-specified generative methods with much better and higher-resolution texture and geometry. See supplementary for more comparisons. Text manipulation on avatar generation We explore the capabilities of DreamAvatar by editing the text prompt for\ncontrolled generation (see Fig. 6). Our method can generate faithful avatars that accurately embody the provided text, incorporating additional descriptive information and capturing the unique characteristics of the main subject.\nShape modification via SMPL shape parameters We further demonstrate the possibility of our method to generate different sizes of 3D human avatars, e.g., thin, short, tall, fat, by editing the SMPL shape parameters in Fig. 7.\nAttributes integration: Zoomed-in head and full body Benefiting from the joint modeling of the zoomed-in 3D head and full body, our method seamlessly integrates the unique attributes derived from both head and body characters. See visualizations in Fig. 8. In contrast to the conventional approach of separately modeling head and body parts, DreamAvatar harmoniously combines these elements, resulting in a cohesive model that faithfully captures the essence of both subjects."
        },
        {
            "heading": "4.2. User Studies",
            "text": "We conduct user studies to compare with SOTA methods [8, 20, 23, 29, 35, 46, 58]. 25 volunteers are presented with 20 examples to rate these methods in terms of (1) geometry quality, (2) texture quality, and (3) consistency with the text from 1 (worst) to 8 (best). The final rates in Fig. 9 clearly show that our method achieves the best rankings in all three aspects."
        },
        {
            "heading": "4.3. Further Analysis",
            "text": "Effectiveness of SMPL-derived density We optimize the NeRF without using the SMPL-derived densities \u03c3\u0304c, \u03c3\u0304p\nas the basis for density prediction. We find that without using \u03c3\u0304c and \u03c3\u0304p, (1) the generated avatars exhibit low-quality geometry and texture with strong outliers, and (2) the generated shapes are not constrained to reasonable human bodies and are not view-consistent (see \u201cw/o SMPL-density field\u201d in Fig. 10).\nEffectiveness of incorporating head VSD loss In order to assess its impact, we conduct ablation studies by disabling the head VSD loss. The results are presented in Fig. 11. Notably, we observe a significant drop in the quality of the generated head region. Moreover, the multi-face \u201dJanus\u201d problem becomes more pronounced in the generated content.\nEffectiveness of DOS design To validate our design, we experiment with two degenerated versions of our framework: (1) only the canonical space, and (2) only the posed space without deformation field. The results, showed in Fig. 10, clearly demonstrate that neither of these degenerated designs can match the performance achieved by our DOS design."
        },
        {
            "heading": "5. Conclusions",
            "text": "In this paper, we have introduced DreamAvatar, an effective framework for text-and-shape guided 3D human avatar generation. In DreamAvatar, we propose to leverage the parametric SMPL model to provide shape prior, guiding the generation with a rough shape and pose. We also propose a dual-observation-space design, facilitating the generation\nof more complete textures and geometry faithful to the target pose. Additionally, we propose to jointly optimize the loss computed from the full body and from the zoomed-in 3D head, effectively alleviating the multi-face \u201cJanus\u201d problem and improving facial details in the generated avatars. Extensive experiments show that our method has achieved state-of-the-art 3D human avatar generation. Limitations Despite establishing a new state-of-the-art, DreamAvatar encounters limitations: (1) Animation was not considered in our current implementation of DreamAvatar; (2) The model inherits biases from the pretrained diffusion model due to the text-image data distribution, such that performance on more frequently appeared subjects in the pretraining data may be better than the others. Societal impact Advancements in 3D avatar generation can streamline metaverse development. However, dangers exist regarding the nefarious use of this technology to generate plausible renderings of individuals. We encourage that research and usage to be conducted in an open and transparent manner."
        }
    ],
    "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models",
    "year": 2023
}