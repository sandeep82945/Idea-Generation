{
    "abstractText": "Given the scale of user-generated content online, the use of artificial intelligence (AI) to flag problematic posts is inevitable, but users do not trust such automated moderation of content. We explore if (a) involving human moderators in the curation process and (b) affording \u201cinteractive transparency,\u201d wherein users participate in curation, can promote appropriate reliance on AI. We test this through a 3 (Source: AI, Human, Both) 3 (Transparency: No Transparency, Transparency-Only, Interactive Transparency) 2 (Classification Decision: Flagged, Not Flagged) betweensubjects online experiment (N1\u20444676) involving classification of hate speech and suicidal ideation. We discovered that users trust AI for the moderation of content just as much as humans, but it depends on the heuristic that is triggered when they are told AI is the source of moderation. We also found that allowing users to provide feedback to the algorithm enhances trust by increasing user agency. Lay Summary As more users post in online forums, there has been a rise in harmful content such as hate speech and thoughts about committing suicide. Most online sites use humans to monitor such content. But, there is so much of this content each day that platforms have started using artificial intelligence (AI) to automatically flag and stop its spread. AI can be better than human moderators. It uses the same consistent criteria for classification, and it is faster. But, the problem is that people do not trust AI with such an important responsibility. One way to increase their trust is to involve humans in the moderation task. Another is to allow users to provide feedback on the classification. We conducted an experiment to test these ideas. Participants were told that the content was classified either by an AI, or by humans, or by both working together. Also, some participants were provided with a list of rules used for classification. Others were allowed to provide feedback about the rules. A third group did not receive any rules. We discovered that letting users provide feedback increased trust. Trust in AI also depends on the perceptions that users have about AI for moderating content.",
    "authors": [
        {
            "affiliations": [],
            "name": "Maria D. Molina"
        },
        {
            "affiliations": [],
            "name": "Shyam Sundar"
        },
        {
            "affiliations": [],
            "name": "Donald P. Bellisario"
        }
    ],
    "id": "SP:cee16922ea87ee27e75e0990adb56c09b240addf",
    "references": [
        {
            "authors": [
                "H.A. Abbass"
            ],
            "title": "Social integration of artificial intelligence: Functions, automation allocation logic and human-autonomy trust",
            "venue": "Cognitive Computation,",
            "year": 2019
        },
        {
            "authors": [
                "M. Ananny",
                "K. Crawford"
            ],
            "title": "Seeing without knowing: Limitations of the transparency ideal and its application to algorithmic accountability",
            "venue": "New Media & Society,",
            "year": 2018
        },
        {
            "authors": [
                "R. Binns",
                "M. Van Kleek",
                "M. Veale",
                "U. Lyngs",
                "J. Zhao",
                "N. Shadbolt"
            ],
            "title": "It\u2019s reducing a human being to a percentage\u201d: Perceptions of justice in algorithmic decisions",
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "T.W. Chen",
                "S.S. Sundar"
            ],
            "title": "This app would like to use your current location to better serve you: Importance of user assent and system transparency in personalized mobile services",
            "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
            "year": 2018
        },
        {
            "authors": [
                "D. Cortiz",
                "A. Zubiaga"
            ],
            "title": "Ethical and technical challenges of AI in tackling hate speech",
            "venue": "The International Review of Information Ethics,",
            "year": 2021
        },
        {
            "authors": [
                "M.A. DeVito",
                "D. Gergle",
                "J. Birnholtz"
            ],
            "title": "Algorithms ruin everything\u201d: #RIPTwitter, folk theories, and resistance to algorithmic change in social media",
            "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "M. Eslami",
                "K. Vaccaro",
                "M.K. Lee",
                "A. Elazari Bar On",
                "E. Gilbert",
                "K. Karahalios"
            ],
            "title": "User attitudes towards algorithmic opacity and transparency in online reviewing platforms",
            "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "G. Gauchat"
            ],
            "title": "Politicization of science in the public sphere: A study of public trust in the United States",
            "venue": "American Sociological Review",
            "year": 2012
        },
        {
            "authors": [
                "D. Gunning",
                "D.W. Aha"
            ],
            "title": "DARPA\u2019s explainable artificial intelligence program",
            "venue": "AI Magazine,",
            "year": 2019
        },
        {
            "authors": [
                "A.F. Hayes"
            ],
            "title": "Introduction to mediation, moderation, and conditional process analysis: A regression-based approach (2nd ed.)",
            "year": 2018
        },
        {
            "authors": [
                "C.J. Hoofnagle",
                "J. King"
            ],
            "title": "What Californians understand about privacy online",
            "venue": "Social Science Research Network",
            "year": 2008
        },
        {
            "authors": [
                "A. Kittur",
                "L. Yu",
                "T. Hope",
                "J. Chan",
                "H. Lifshitz-Assaf",
                "K. Gilon",
                "F. Ng",
                "R.E. Kraut",
                "D. Shahaf"
            ],
            "title": "Scaling up analogical innovation with crowds and AI",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2019
        },
        {
            "authors": [
                "K. Langvardt"
            ],
            "title": "Regulating online content moderation",
            "venue": "Georgetown Law Journal,",
            "year": 2018
        },
        {
            "authors": [
                "J.D. Lee",
                "K.A. See"
            ],
            "title": "Trust in automation: Designing for appropriate reliance",
            "venue": "Human Factors,",
            "year": 2004
        },
        {
            "authors": [
                "M.K. Lee"
            ],
            "title": "Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management",
            "venue": "Big Data & Society,",
            "year": 2018
        },
        {
            "authors": [
                "E. e",
                "A. Pentland",
                "P. Vinck"
            ],
            "title": "Fair, transparent, and accountable algorithmic decision-making processes: The premise, the proposed solutions, and the open challenges",
            "venue": "Philosophy & Technology,",
            "year": 2018
        },
        {
            "authors": [
                "J. Li",
                "J.S. Huang"
            ],
            "title": "Dimensions of artificial intelligence anxiety based on the integrated fear acquisition theory",
            "venue": "Technology in Society,",
            "year": 2020
        },
        {
            "authors": [
                "G. Meikle"
            ],
            "title": "Social media: Communication, sharing and visibility",
            "year": 2016
        },
        {
            "authors": [
                "J. Oh",
                "S.S. Sundar"
            ],
            "title": "How does interactivity persuade? An experimental test of interactivity on cognitive absorption, elaboration, and attitudes",
            "venue": "Journal of Communication,",
            "year": 2015
        },
        {
            "authors": [
                "D. Peters",
                "R.A. Calvo",
                "R.M. Ryan"
            ],
            "title": "Designing for motiva",
            "year": 2018
        },
        {
            "authors": [
                "L.N. Reid",
                "K.W. King"
            ],
            "title": "Measuring trust in advertis",
            "year": 2009
        },
        {
            "authors": [
                "S.S. 3367380206 Sundar"
            ],
            "title": "The MAIN model: A heuristic approach",
            "year": 2008
        },
        {
            "authors": [
                "S.S. Press. Sundar"
            ],
            "title": "Rise of machine agency: A framework for studying",
            "venue": "(pp. 72\u2013100)",
            "year": 2020
        },
        {
            "authors": [
                "G. Bansal"
            ],
            "title": "The challenge of crafting intelligible",
            "year": 2019
        },
        {
            "authors": [
                "S.M. West"
            ],
            "title": "Censored, suspended, shadowbanned: User inter",
            "year": 2018
        },
        {
            "authors": [
                "X.J. Zhang",
                "Y. Wu",
                "Y. Li"
            ],
            "title": "The tendency of trust",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "Lay Summary",
            "text": "As more users post in online forums, there has been a rise in harmful content such as hate speech and thoughts about committing suicide. Most online sites use humans to monitor such content. But, there is so much of this content each day that platforms have started using artificial intelligence (AI) to automatically flag and stop its spread. AI can be better than human moderators. It uses the same consistent criteria for classification, and it is faster. But, the problem is that people do not trust AI with such an important responsibility. One way to increase their trust is to involve humans in the moderation task. Another is to allow users to provide feedback on the classification. We conducted an experiment to test these ideas. Participants were told that the content was classified either by an AI, or by humans, or by both working together. Also, some participants were provided with a list of rules used for classification. Others were allowed to provide feedback about the rules. A third group did not receive any rules. We discovered that letting users provide feedback increased trust. Trust in AI also depends on the perceptions that users have about AI for moderating content. Keywords: human-AI collaboration, content classification, source cues, interactivity, HAII-TIME model\nPeople share a wide variety of content online, from information about important life events (e.g., birthdays and weddings) to breaking news and political opinion. Just as the control afforded by new media technologies has enabled them to share these experiences, it has also resulted in the posting of daring challenges (e.g., Tide Pod Challenge or Blue Whale Challenge), dangerous selfies, misinformation, hate speech, and suicidal ideation.\nAlthough harmful content existed long before the Internet, the \u201cease, speed, and anonymity [of the Internet] all drive up the volume and substantive virulence\u201d of these types of content (Langvardt, 2018, p. 1359), prompting the need for timely action to counteract their rapid dissemination. Given the scale of information being generated online, timely action can only be achieved with the assistance of artificial intelligence (AI) systems that can moderate content automatically. However, users may not be ready to let AI assume this important filtering function, especially in sensitive content domains, partly because it takes away the control that users expect from social media sites (DeVito et al., 2017) and partly because AI is not currently equipped to handle such a task, given several ethical and technical challenges elaborated in the literature (Cortiz & Zubiaga, 2021). This poses a socio-technical dilemma: Automation of content classification is necessary if\nwe want to address the scourge of inappropriate usergenerated content, but users may be unwilling to entrust AI with this task.\nOne solution is to augment AI systems by adding human collaboration. The HAII-TIME model of Human\u2013AI interaction (Sundar, 2020), based on the Theory of Interactive Media Effects (Sundar et al., 2015) suggests two possibilities\u2014by adding a human source in addition to the AI source (cue route) and by allowing user input in the process of curation (action route). According to TIME (Sundar et al., 2015), there are two routes through which affordances, or \u201caction possibilities,\u201d can play a role in user decision-making. The first is the cue route, where affordances serve as visual cues that the user notices but does not necessarily act upon. The second route is the action route, where the user actually acts on a specific affordance. We detail both possibilities in the sections that follow.\nAugmenting AI through human collaboration\nEven though the goal of content classification is to improve user experience by preventing the spread of content that may harm users, it has met with considerable pushback. The idea of content classification and moderation suggests censorship,\nAssociate Editor: Eun-Ju Lee Received: 21 October 2021. Revised: 9 April 2022. Accepted: 16 May 2022 VC The Author(s) 2022. Published by Oxford University Press on behalf of International Communication Association. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.\nnloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\nwhich runs counter to the ethos of social media, premised on freely expressing and connecting with others (Meikle, 2016). When content is flagged or taken down, classification systems make users feel frustrated (West, 2018) because it takes away the control and ability to self-express (DeVito et al., 2017). In other words, it creates reactance due to perceived violation of their freedom of expression.\nThis reactance is exacerbated by the lack of transparency of many content classification systems with many unanswered questions about its operation (West, 2018). For starters, most systems do not make it clear whether AI algorithms or humans oversee moderation (source transparency), and the extent of their involvement at different stages of the process. Some systems do not even disclose the existence of an algorithm in operation, which can be seen as deception, negatively influencing user trust in the system (Eslami et al. 2019). As if to compensate for this lack of source transparency, users tend to rely on their experience and develop folk theories about the functioning of platforms (West, 2018). For example, when the source of moderation is unknown, some users attribute content classification to human intervention, especially when this occurs after a heated discussion about politics, religion, or other controversial topics (West, 2018). Other folk theories attribute moderation to automated systems, with limited opportunity to obtain information about the reasons behind moderation. Furthermore, automated systems rarely disclose the rationales behind the decision-making process of the AI (process transparency); and when they do, they may not be meaningful for users because the rules used for content classification are often complex and difficult to explain (Lepri et al., 2018). In the absence of transparency (whether source transparency or process transparency), users \u201cstrive to make sense of content-moderation processes by drawing connections between related phenomena, developing nonauthoritative conceptions of why and how their content was removed\u201d (West, 2018, p. 4372).\nInstances like these, where users\u2019 trust1\u2014defined by their willingness to rely on the system (Soh et al., 2009)\u2014does not match the system\u2019s actual attributes, suggest inappropriate reliance (Lee & See, 2004) based on incorrect understanding, whether that is over-trusting or under-trusting the system. Such inappropriate reliance may also lead to users agreeing or disagreeing with the system without a correct understanding of its operation.\nOne solution to this problem of incorrect folk theories and inappropriate reliance is to explicitly reveal the source of classification to users. Research suggests that incorporating AI as a source cue can trigger a variety of heuristics, which can in turn influence trust toward the content being presented (Sundar, 2008; Wang 2021). In the context of content moderation, disclosing AI as a source could trigger positive or negative stereotypes of the machine, influencing users\u2019 reliance on AI (Sundar, 2020). Including human sources could help calibrate such heuristics toward an accurate reliance on such systems. We expand on this in the following section.\nHuman collaboration by disclosing source One way to increase users\u2019 trust and incentivize appropriate reliance on AI-based technologies is by including humans in the decision-making process and explicitly disclosing the source of classification to users. Source signals such as these trigger cognitive heuristics, or \u201crules of thumb,\u201d that are then employed by users to evaluate content and make decisions\nonline (Sherman & Corty, 1984; Sundar, 2008). The presence of such source cues on media interfaces, and the cognitive heuristics they trigger when evaluating content online, are part of the cue route to persuasion proposed by TIME (Sundar et al., 2015). More specifically, the cue route posits that interface features serve as visual cues that the user notices but does not necessarily act upon. These visual cues guide online evaluations and decision-making by triggering such cognitive heuristics as the \u201cauthority heuristic,\u201d often processed peripherally, i.e., without much cognitive effort (Sundar, 2008). In the current study context, when users are explicitly told that AI is the source of moderation, their trust and agreement with the system will depend on the heuristic they hold about AI (Sundar, 2020).\nIn the context of content moderation, users might hold different, and often opposing, heuristics about AI. When invoked by the AI signaling cue, those heuristics will influence users\u2019 trust toward the system. On the one hand, it is possible that AI as a source of classification triggers what Sundar (2008) refers to as the positive machine heuristic or the belief that machines are more objective and error-free compared to humans. In Binns et al. (2018), participants felt that AI could be more accurate than humans because decisions are always based on the same rules, yielding results that at least are \u201cstatistically fair\u201d (p. 9). In this case, being told that the content classification is performed by AI elicits the rule of thumb that \u201cif the classification was done by machine, then it is objective and accurate\u201d (Sundar, 2008). Wang (2021) found initial evidence for the operation of the positive machine heuristic in the content moderation domain\u2014participants perceived a news story as less biased when an uncivil comment was moderated by a machine (compared to human), in turn increasing perceived credibility of the story. These effects were stronger for users with strong beliefs in the positive machine heuristic.\nHowever, the positive machine heuristic might not be the only heuristic that governs AI perception in the content moderation domain. Machines might be viewed as incapable of nuanced subjective judgments, and thus users might prefer moderation by a human entity. The fear is that AI\u2019s classification of content may lead to a high volume of false positives because of AI\u2019s inability to detect semantic and other linguistic nuances (Gollatz et al., 2018). In this case, the rule of thumb triggered by machine as the source of moderation would be \u2018if the classification was done by machine, then it lacks subjective judgement\u2019 (Lee, 2018). To our knowledge, the invocation of the negative machine heuristic has not been empirically tested. However, its invocation can be inferred by the fact that despite evidence of preference of AI over human (e.g., Wang, 2021), there is not a wider acceptance of AI for content moderation. In other words, it is possible that depending on the valence of the machine heuristic invoked, users would respond more or less favorably to AI. Thus, we pose:\nH1: The effect of AI (vs. human) as the source of classification on trust and agreement toward a system will be mediated a) positively by the positive machine heuristic and b) negatively by the negative machine heuristic.\nAside from machines lacking human touch and subjective judgement, they tend to threaten human agency by usurping control that historically belonged to humans. As Sundar\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\n(2020) notes, several AI-based media technologies offering personalization have created a tension between human agency and machine agency. Thus, users might dislike AI systems that have full control over content classification because they may be seen as undermining users\u2019 agency. Thus, we pose:\nH2: The effect of AI (vs. human) as the source of classification will be mediated, in parallel, by perceived a) increase in AI agency and b) decrease in human agency, thereby negatively influencing overall trust and agreement toward the system.\nNonetheless, when it comes to the integration of AI into society, scholars argue that there will always be a need for humans to participate in decision-making because both sources have strengths that can be maximized for effective content classification (Abbass, 2019; Sundar, 2020). In this hybrid model, AI can augment humans by allowing them to think faster and perform operations that the human brain cannot solve (Abbass, 2019). Likewise, humans can augment AI by providing the sensitive judgment that AI is not equipped to perform (Sundar, 2020). For example, in the context of content moderation, the AI could be in charge of classifying large volumes of data and leave those \u201cgray areas\u201d to human moderators who can bring \u201cexpertise, empathy, and contextual knowledge to judge\u201d complex cases (Gollatz et al., 2018, p. 12). As such, the involvement of humans in the moderation task could decrease the operation of the negative machine heuristic because humans could provide the subjective judgment deemed necessary for classifying online content, while AI will accentuate the positive machine heuristic due to its ability to classify content faster and more objectively. We also suggest that such human\u2014AI collaboration will help restore the human agency that is lost when AI classifies content in isolation (Sundar 2020). As such, when agency is shared between AI and humans, perceived AI agency and human agency will increase trust and agreement with the system. Thus, we propose:\nH3: When AI and humans provide a classification together, a) trust and b) agreement with the classification will be higher compared to AI-only and human-only classifications.\nH4: This relationship will be mediated in parallel positively by the operation of the a) positive machine heuristic b) perceived human agency, and c) perceived AI agency, and negatively by the operation of the d) negative machine heuristic.\nHuman collaboration through user interaction Although highlighting human involvement in moderation decisions might be one strategy toward greater trust and appropriate reliance on content moderation systems powered by AI, it may not be enough. Even if moderation is performed in collaboration with a human, content classification still takes away the user agency that is expected from social media, and may even be considered a form of censorship (West, 2018). Moreover, human moderators also have their own biases and viewpoints and may lack \u201ccontextual knowledge and cultural sensitivity\u201d (Suzor et al., 2019, p. 1530). All this suggests that the limitations of AI for content moderation might be due to users\u2019 reservations based on their lack of understanding of AI.\nWhile both humans and AI fall short of perfect accuracy, AI\u2019s decisions are at least based on consistently applied criteria (Binns et al., 2018; Langvardt, 2018). These criteria, however, are rarely displayed to end-users, even though they are reliably applied by the system. In the absence of explanations, users rely on folk theories to derive rationales for why a post was flagged, thus inadvertently over-trusting a system that may be flawed or under-trusting one that is not (Lee & See, 2004; West, 2018).\nSo, how do we generate better understanding of the functioning of the classification system while also ensuring user agency? One possibility is to provide users with the rationales for a system\u2019s decision-making process (e.g., Suzor et al., 2019). However, incorporating such transparency in interfaces does not mean that users will actually understand it (e.g., Hoofnagle & King, 2008). One solution is to provide users with greater involvement with the classification system. Researchers exploring human\u2013AI interaction acknowledge the benefits associated with interactivity and have proposed utilizing it to provide transparency in a more efficient manner (Weld & Bansal, 2019). In this study, we conceptualize and test interactive transparency by affording users the option to provide feedback to the classification system by suggesting characteristics to include or exclude when making a classification decision. We compare interactive transparency with two other conditions: Transparency only\u2014wherein the system discloses the characteristics used by the system for classification (with no option to suggest characteristics to include or exclude) and complete absence of transparency.2\nOne of the advantages of interactive transparency is that it could enhance users\u2019 understanding of the system beyond simply disclosing the specific rules utilized for classification (transparency only). Content classification systems are often complex and difficult to explain, resulting in transparency statements that might induce more uncertainty rather than understandability (Lepri et al., 2018). Explainable AI (XAI) acknowledges these limitations and suggests that the field move one step further into explainability where we \u201cprovide users with explanations that enable them to understand the system\u2019s overall strengths and weaknesses, convey an understanding of how it will behave in future or different situations, and perhaps permit users to correct the system\u2019s mistakes\u201d (Gunning & Aha, 2019, p. 45). The action route in TIME (Sundar et al., 2015) suggests the goals of XAI could be accomplished through user involvement with the system because interactivity increases systematic processing of information, meaning greater elaboration and conscious thinking regarding the functioning of the system (Petty & Cacioppo, 1986). Furthermore, as discussed previously, fear of automation is largely driven by the loss of human agency that has occurred with the advancement of AI systems (Sundar, 2020). Abbass (2019) maps the risk of human\u2013AI relationship as a function of the extent of human control and the role played by AI and humans through the decision-making process. Even when humans have some control over the classification system, machine agency is perceived as a threat. Chen and Sundar (2018) reveal similar results. When a recommendation system utilized covert personalization, users trusted the system less than when the system utilized overt personalization. Covert personalization provides recommendations without seeking human assent or input, meaning that the algorithm has the upper hand in the interaction, whereas overt personalization delivers services after first consulting the user. Interactive\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\ntransparency mirrors the latter by allowing users to suggest characteristics for inclusion and exclusion, and thereby imbuing users with the sense of agency necessary to increase trust of the system. The resulting user agency could also help in the sense-making that is necessary to comprehend the classification rules (Ananny & Crawford, 2018). The suggestions made by users could then be assessed to identify potential flaws and biases in the system, and to further understand the needs and concerns of its users (Eslami et al., 2019). We propose that while any form of transparency can increase trust and understandability3 by increasing perceived disclosure, interactive transparency will do so by affording user agency. More formally:\nH5: The effects of transparency (interactive or not interactive) vs. no transparency on a) trust and b) understanding of the system will be positively mediated by perceived disclosure.\nH6: The effects of interactive transparency (vs. transparency only) on a) trust and b) understanding of the system will be positively mediated by an increase in user agency.\nPrevious literature provides sufficient evidence to formulate specific hypotheses about the role of interactive transparency in increasing trust and understanding toward a classification system. It is less clear, however, on the effect of interactive transparency on agreement with the classification decision. Interactivity could make users more critical about the information presented, due to increased systematic processing. The basis of the decision may not withstand such heightened scrutiny, resulting in users disagreeing, rather than agreeing, with it. Given the lack of empirical evidence to pose a directional hypothesis, we pose the following research questions:\nRQ1: What is the relationship between type of transparency and level of agreement with the classification decision?\nRQ2: Do a) perceived disclosure and b) user agency mediate this relationship?\nEffects of interactive transparency as a function of source of classification\nWhile interactive transparency is hypothesized to have an overall positive effect (vs. transparency-only and control condition), its relative strength and psychological explanatory mechanisms are likely to differ across sources of classification. Thus far, AI agency and user agency have been treated as separate constructs, such that in the absence of interactivity, the system has full control over content classification, diminishing user agency as a consequence. Nevertheless, when users can interact with the system, user agency is restored. As discussed, from an AI development standpoint, collaboration between human (whether the user or the human moderator) and AI could enhance creativity and scale the innovation process (Kittur et al., 2019). Similarly, from a user psychology perspective, human\u2013AI collaboration is a negotiation between machine agency and user agency, wherein the user could take advantage of the benefits of AI while simultaneously exerting their influence over the classification system. This influence, in turn, will benefit the AI system by providing more data for training and improving the underlying machine learning\nalgorithm. This is what Sundar (2020) calls \u201cmutual augmentation\u201d: When the AI system and the user collaborate in the decision-making process, both user agency and AI agency together will lead to better outcomes not only for the classification, but also for human perception of the interaction context, because interactive transparency is a way to restore the human agency that is lost when AI performs a classification in isolation. Thus, we can expect that in the absence of interactivity, AI as the source of classification will increase AI agency but decrease user agency, thereby negatively influencing trust, agreement, and understanding of the AI. However, when interactive transparency is provided, trust toward the system and the perceived understanding of the functioning of the system will increase, partly because it provides greater user agency and partly because it highlights the desirable facets of AI agency which promise collaboration with humans. More formally:\nH7: When AI is the source of classification (compared to human), the relationship between interactive transparency and a) trust toward the system b) agreement with the system and c) understanding of the system will be positively mediated, in parallel, by perceived user agency and perceived AI agency.\nSimilarly, it is possible that when humans and AI are moderators in tandem, the control over the interaction that is afforded by interactive transparency will increase trust toward the system, agreement, and perceived understanding of the system, because of the operation of both user agency and AI agency. Nevertheless, it is also possible that interactive transparency (vs. transparency-only) will not necessarily increase trust or understanding of the system when humans and AI are the source of classification in tandem because the human input or agency required for a human\u2013AI synergistic relationship does not have to come from the user him/herself but could be provided by any other human entity (Sundar, 2020). When AI and humans both serve as moderators of content, the human moderator could provide the human agency necessary for mutual augmentation by contributing the subjective judgment expected from human intervention in the classification system, eschewing the need for user agency over and beyond the human agency imbued by the involvement of human moderators. To test these possibilities, the following research question is posed:\nRQ3: When humans and AI together serve as moderators of content (vs. AI only or human only), what is the relationship between interactive transparency (vs. transparency vs. no transparency) and a) trust b) agreement c) understanding of the system and d) perceived user agency, human agency, and AI agency?"
        },
        {
            "heading": "Method",
            "text": "To address the research questions and hypotheses, we conducted a 3 (Source of Classification: AI, Human, Combination) 3 (Type of Transparency: Interactive Transparency vs. Transparency-Only vs. No Transparency) 2 (Classification Decision: Flagged vs. Not Flagged) betweensubjects online experiment.\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024"
        },
        {
            "heading": "Participants",
            "text": "A total of 750 participants from the United States were recruited from Amazon Mechanical Turk (M-Turk) to complete this study.4 The final sample, after deleting participants who did not pass three attention checks or provided incomplete responses, consisted of 676 participants (52.5% male, 46.6% female, 0.9% other/prefer not to say). Participants\u2019 ages ranged from 18 to 79 (M\u00bc38.05, SD \u00bc 11.84, Mdn \u00bc 35) and were mostly Caucasian (78.4%). Most participants were highly educated with 60.8% reporting having a bachelor\u2019s degree or higher."
        },
        {
            "heading": "Procedure",
            "text": "Upon consenting to take part in this study, participants were told that we are studying user perception of a classification system under development, and that we will ask them to interact with it and answer some questions afterward. Participants were randomly assigned to one of 18 conditions varying in the source of moderation, type of transparency, and classification decision. Classification decision was added as a factor for this study because previous research reveals a positivity bias, such that positive ratings or classifications are better received by users compared to negative reviews or comments (e.g., Wang et al., 2020), regardless of the source of moderation. Flagging of user-generated content is also associated with censorship and potential violation of free speech. Thus, including classification decision as a factor allows us to control for any differential effects due to a post being flagged or not flagged by the system, and ensure that our findings are not confounded by the outcome of the classification.\nAdditionally, for stimulus sampling purposes, participants were either assigned to a classification system that identified instances of hate speech or suicidal ideation. Two different posts were created for each context of classification, for further stimulus sampling. Each participant received just one of the four messages (thus, it was a four-category between-subjects factor), and we treated the posts as a control variable in our analyses. After interacting with the platform and seeing an example of the system in action classifying a post from another social media user, participants were directed to a questionnaire."
        },
        {
            "heading": "Stimulus",
            "text": "A total of 72 webpages were created for this study (18 for each of the four user-generated posts selected for classification). Upon accessing the webpage, users were guided through an initial interaction where they were provided with information about the webpage and its objectives, as well as a definition of hate speech or suicidal ideation, depending on the\nassigned context. The latter information was included to convey the difficulty associated with classifying these types of content. After this initial interaction, participants were guided to a new page that showed them one of four user-generated posts (see Table A1, Online Supplementary Material) and were told that the post came directly from social media. The posts were selected based on a pretest to ensure that the post would be somewhat ambiguous in terms of reflecting hate speech (or suicidal ideation), meaning that classification decision is not cut and dry, but prone to subjectivity and discussion (see Appendix B, Online Supplementary Material).\nAfter reading the post and clicking a \u201csee classification\u201d button, participants were provided with a classification decision (hate speech/suicidal or not hate speech/not suicidal) and the source of moderation of their assigned condition (human, AI, or both). Then, participants in the no transparency condition were prompted to continue with the survey. Participants in the transparency-only and interactive transparency conditions were provided with a list of words that the classification system utilized for classification. Those in the interactive transparency condition could interact with the list of words and suggest words for inclusion and exclusion."
        },
        {
            "heading": "Source of moderation manipulation",
            "text": "Source of moderation was operationalized in two ways. First, during the initial interaction, participants were told that the system employs human content moderators, AI moderators, or both. Second, when receiving the classification decision, users saw a visual cue of the source of moderation next to the classification (see Figure 1)."
        },
        {
            "heading": "Classification decision manipulation",
            "text": "We included classification decision as a factor in our design given previous research suggesting a general aversion to content classification. For half the participants, the post was determined to be an instance of hate speech (or suicidal ideation), while for the other half, the post was determined to not to be an instance of hate speech (or suicidal ideation) (see Figure 1). The first scenario is referred to as \u201cflagged\u201d decision, while the latter as \u201cnot flagged.\u201d"
        },
        {
            "heading": "Transparency manipulation",
            "text": "Transparency was operationalized by providing users with detail about the words that were used by the classification system to decide on a classification (transparency-only) versus providing the same list of words with the ability for users to suggest words for inclusion and exclusion (interactive transparency), versus providing no transparency at all (no transparency). In the no transparency condition, participants\u2019\nFigure 1. Manipulation of Source of Classification and Classification Decision. Note: AI source of classification and flagged classification decision [left]. Human source of classification and not flagged classification decision [right]. The condition where AI and human provide a decision together had both visual avatars together.\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\ninteraction ended after seeing the classification decision (see Figure 2). For the transparency-only and interactive transparency conditions, participants were prompted with a button that read \u201clearn more about this classification\u201d (see Figure 2). Upon clicking, participants were redirected to a page with two lists of words. One list contained words from the post that were used to provide a classification (independent of whether the post was flagged or not). The other list contained words that were not used as indicators or rules to determine classification (see Figure 3). In the interactive transparency condition, participants were additionally told that they could help improve the accuracy of classification by moving words from one list to the other (see Figure C1, Online Supplementary Material). After they had time to tinker with the list, they were provided a \u201cSubmit\u201d button."
        },
        {
            "heading": "Measures",
            "text": "All questions used 7-point response scales. The dependent variables of this study were measured as follows: Trust was conceptualized via two dimensions, attitudinal trust (i.e., honest, accurate, dependable; M\u00bc 4.52, SD\u00bc 1.39, a\u00bc.95) and behavioral trust (i.e., how likely would you be to rely on this\nsystem to classify user-generated content posted by other users;M\u00bc4.01, SD \u00bc 1.84, a \u00bc .94), and was measured via Soh et al.\u2019s (2009) scale, adapted to the context of this study. Rating agreement was assessed by asking participants the degree to which they perceive the post to be hate speech or suicidal, depending on the context to which participants were assigned (M\u00bc 4.49, SD\u00bc 1.9). Scores of participants assigned to the classification decision where the post was not flagged (not hate speech and not suicidal ideation) were reverse-coded to reflect user agreement with the classification decision. Perceived understanding was measured by participants\u2019 selfreport of their understanding of the functioning of the classification system (M\u00bc 4.86, SD \u00bc 1.43) (see Table A2 of Online Supplementary Material for details).\nThe mediating variables were: The invocation of the machine heuristic assessed through two dimensions (the positive and negative machine heuristic) conceptualized by Sundar (2020). Items measuring the positive machine heuristic included \u201chas machine-like precision,\u201d and \u201cis error free,\u201d (M\u00bc 4.37, SD \u00bc 1.31, a \u00bc .79). Items measuring the negative machine heuristic, included: \u201cis able to detect human emotion\u201d (RC), and \u201chas human-like subjective judgements\u201d\nFigure 3. Transparency-Only Manipulation. Note: Upon clicking \u201cnext,\u201d participants were prompted to continue with the survey.\nnloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\n(RC) (M\u00bc 4.19, SD \u00bc 1.00, a \u00bc .66). A confirmatory factor analysis was conducted to assess if both levels of the machine heuristic were part of a unidimensional scale or represent two distinct factors. A v2 difference test revealed the two-factor model to be better than a one-factor model, v2 difference\u00bc 270, p < .001. Perceived disclosure was measured via three items (i.e., the system: explains its decisions, is transparent, openly shares all relevant information; M\u00bc 4.52, SD\u00bc 1.68, a \u00bc .90) from Pirson and Malhotra (2011). User agency was measured through the autonomy items from Peters et al. (2018) adapted to the context of this study (M\u00bc4.39, SD\u00bc 1.20, a \u00bc .63) with items like \u201cI can get the classification system to do the things I want it to\u201d and \u201cI feel pressured by the classification system (RC).\u201d Perceived AI and human agency were measured via two scales created by the authors (i.e., \u201cthe classification was controlled by the system/human\u201d and \u201crules for classification were composed by the system/human\u201d (MAI \u00bc 4.15, SDAI \u00bc 1.45, aAI \u00bc .74; MHuman \u00bc 4.96, SDHuman \u00bc 1.36, aHuman \u00bc .74). (see Table A3 of Online Supplementary Material for details)."
        },
        {
            "heading": "Control variables",
            "text": "This study controlled for power use, fear of AI, and dispositional trust. Previous studies suggest that preference for interactivity features can be contingent upon the user being a power user or not (Sundar & Marathe, 2010). Likewise, AI aversion (or fear of AI) and individual differences in dispositional trust are known to influence the acceptance of technologies, particularly AI (Li & Huang, 2020; Zhang et al., 2019). We also controlled for political orientation due to the negative relationship between conservatism and trust in science found in previous research (Gauchat, 2012). In our original analyses, we also tested the mediating role of elaboration, uncertainty, perceived accuracy, and perceived clarity in the relationship between transparency and the dependent variables of interest. We did not include these variables in the current study due to parsimony, but control for them statistically (see Table A4 for measurement details)."
        },
        {
            "heading": "Results",
            "text": "A manipulation check was administered to assess if participants recalled the source of classification of their assigned condition. While 88.1% of participants assigned to the AI condition recalled the source of classification correctly, 60.2% of participants recalled the source correctly for the human condition, and 69.6% for the combined condition. A 3 (Assigned Source: AI vs. Human vs. both) 2 (Manipulation Check: Pass vs. Fail) chi-square test revealed this difference to be statistically significant, v2 (2, N\u00bc676) \u00bc 46.79, p < .001, V* \u00bc .26. It is possible that users in the human and hybrid conditions failed to recognize the source of classification because the interaction was too similar to their previous experiences with AI-based technologies or that the system was too mechanistic. Despite these differences, hypotheses testing were conducted with the complete sample because the source manipulation is hypothesized to be a heuristic cue, likely processed peripherally. Individuals may be unaware of the cue that triggers the heuristic and its operation but may still be influenced by its presence (Sundar, 2008). Results remained similar when running the analyses with only participants who passed the manipulation check. We note minor differences in Appendix D of the Online Supplementary Materials.\nMain effects and interaction effects To test the hypotheses proposing main and interaction effects, a 3 (Source: AI vs. Human vs. Combination) 3 (Transparency Type: No Transparency vs. TransparencyOnly vs. Interactive Transparency) 2 (Classification Decision: Flagged vs. Not Flagged) MANCOVA was conducted by entering attitudinal trust and behavioral trust as dependent variables, followed by separate ANCOVAs for agreement and understanding. Political orientation, fear of AI, dispositional trust, power use, and stimulus sampling were always entered as covariates.\nThe MANCOVA with attitudinal trust and behavioral trust as dependent variables revealed a significant main effect for classification decision, Wilks\u2019 K \u00bc .99, F (2, 650) \u00bc 3.40, p \u00bc .034, partial g2 \u00bc .015 The univariate analysis for both variables revealed a significant main effect such that users trusted the classification system more when a post was not flagged (Mtrust \u00bc 4.67, SEtrust \u00bc .07; MBehavioral Trust \u00bc 4.14, SEBehavioral Trust \u00bc .09) compared to when it was flagged (MTrust \u00bc 4.42, SETrust \u00bc .07; MBehavioral Trust \u00bc 3.89, SEBehavioral Trust \u00bc .09), FTrust (1, 651) \u00bc 6.80, p \u00bc .009, partial g2 \u00bc .01; FBehavioral Trust (1, 651)\u00bc 3.75, p\u00bc .053, partial g2\u00bc .01.\nAs a next step, two separate ANCOVAs were conducted, one for agreement and the other for understanding. No significant main effects or interaction effects were found when running the ANCOVA with agreement as the dependent variable6. When running the ANCOVA for users\u2019 perceived understanding of the classification system, a significant main effect of transparency type was found, with participants in the transparency-only condition reporting more understanding (M\u00bc 5.02, SE \u00bc .09), followed by participants in the interactive transparency condition (M\u00bc 4.96, SE \u00bc .09), and participants in the no transparency condition (M\u00bc 4.62, SE \u00bc .09), F (2, 651) \u00bc 5.74, p \u00bc .003, partial g2 \u00bc .02. Least Significant Difference post-hoc analyses reveal that only the difference between no transparency and either transparency conditions were significant, but not the difference between the interactive transparency and transparency-only conditions.7"
        },
        {
            "heading": "Mediation analyses for cue effects",
            "text": "To test the mediating role of the negative machine heuristic, positive machine heuristic, perceived AI agency, and perceived human agency on the relationship between source and the dependent variables of interest (attitudinal trust, behavioral trust, and agreement), a series of parallel mediation analyses using PROCESS macro\u2019s Model 4 (Hayes, 2018) was conducted, (95% bias-corrected confidence interval based on 5,000 bootstrap samples) one for each dependent variable. Because source is a three-level variable it was treated as a multicategorical variable using indicator coding. Power use, dispositional trust, fear of AI, political orientation, and stimulus sampling were used as covariates.\nFirst, attitudinal trust was entered as the dependent variable. For the comparison between human (0) and AI (1) (see Figure 4), the indirect effects through positive machine (Indirect Effect \u00bc .31, CI: .17, .47) and negative machine heuristic (Indirect Effect\u00bc .40, CI: .52, .29), holding the other mediators constant, were significant. The indirect effects via perceived AI agency (Indirect Effect\u00bc .02, CI: .07, .02) and perceived human agency (Indirect Effect\u00bc .02, CI: .09, .03) were not significant. Similarly, for the comparison between human (0) and both (1) (see Figure 4), the indirect\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\neffects through positive machine heuristic (Indirect Effect \u00bc .15, CI: .004, .30) and negative machine heuristic (Indirect Effect\u00bc .16, CI: .27, .06), holding the other mediators constant, were significant. The indirect effects via perceived AI agency (Indirect Effect\u00bc .02, CI: .06, .02) and perceived human agency (Indirect Effect\u00bc .01, CI: .05, .02), were not significant. Similar patterns emerge when entering behavioral trust as the dependent variable (see Appendix E).\nWhen entering perceived agreement (see Appendix F) as dependent variables, the same patterns emerge for the positive and negative machine heuristic. However, there is also a significant mediating effect of perceived AI agency, for both comparisons (Human vs. AI: .23, CI: .36, .12; Human vs. both: .18, CI: .29, -.08). Patterns reveal that while AI, or the presence of AI, led to higher perceived AI agency, this reduced agreement with the classification decision.\nMediation analyses for effects of user interaction To test the mediating role of disclosure and user agency on the relationship between transparency type and the dependent variables of interest (attitudinal trust, behavioral trust, understanding, and agreement), another set of parallel mediation analysis was conducted, also using PROCESS macro\u2019s Model 4 (Hayes, 2018) with a 95% bias-corrected confidence interval based on 5,000 bootstrap samples. For these analyses, given the ordinal nature of our multi-categorical independent variable, Helmert coding (which compares each category to the aggregate of all higher levels) was used, as recommended by Hayes (2018), because it allows for two comparisons of interest\u2014one between no transparency and transparency (whether interactive or not) and the other between transparency-only and interactive transparency. Power use, fear of AI, dispositional trust, political orientation, elaboration, uncertainty, accuracy, clarity, and stimulus sampling were used as control variables.\nWhen comparing no transparency (0) with transparency (1) (see Figure 5) and entering attitudinal trust as the dependent variable the indirect effects through disclosure (Indirect Effect \u00bc .12, CI: .06, .18) and user agency (Indirect Effect \u00bc .04, CI: .01, .07) were significant. The same patterns emerged when entering behavioral trust as the dependent variable (see Appendix G). When entering perceived understanding as the dependent variable, only perceived disclosure was significant (see Appendix H). When entering agreement (see Appendix I) as the dependent variable, both user agency and disclosure were significant. However, the direction of the effect of disclosure was different (Indirect Effect\u00bc .19, CI: .33, .06), such that transparency increased perceived disclosure, but the higher the perceived disclosure, the lesser the agreement.\nOn the other hand, when comparing the transparency-only condition (0) with the interactive transparency condition (1) (see Figure 5) and entering attitudinal trust as the dependent variable, only user agency served as a significant mediator (Indirect Effect \u00bc .04, CI: .01, .07).8 Disclosure (Indirect Effect\u00bc .01, CI: .03, .02) was not a significant mediator. The same pattern emerged when entering behavioral trust and agreement as the dependent variable, but not for perceived understanding.\nModerated mediation for user interaction as a function of source cue and transparency Finally, to test if source of classification influences the relationships between type of transparency and the dependent variables of interest (trust, behavioral trust, understanding, and agreement), via (human agency, AI agency, and user agency), a series of moderated mediation analyses were conducted using PROCESS macros, Model 7. Transparency type was coded as a multicategorical variable using Helmert coding, while source was coded with indicator coding. Results of these analyses revealed no significant differences for any of the comparisons.\nFigure 4. Mediation Analysis: Source of Classification and Attitudinal Trust. Note: Mediating effects of positive machine heuristic (PM heuristic), negative machine heuristic (NM heuristic), AI agency, human agency. Human vs. AI comparison [left], Human vs. Both comparison [right]. All effects are unstandardized coefficients (b). ***p < .001, ** p < .01. c\u2019 represent the direct effects of source on trust; c represents the total effect of source on trust.\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024"
        },
        {
            "heading": "Summary of results",
            "text": "In answering our hypotheses and research questions (see Table A9 of the Online Supplementary material for details), we found that when AI is the source of classification, the invocation of the positive machine heuristic positively influences trust and agreement, while the invocation of the negative machine heuristic negatively influences the dependent variables, thereby supporting H1. However, we did not find support for the influence of AI agency or human agency (H2). We also found partial support for H3 and H4\u2014the combined (human and AI) condition increased trust and agreement toward the system, when the positive machine heuristic was invoked and it decreased trust and agreement when the negative machine heuristic was invoked. However, there was no mediation of human or AI agency. Furthermore, we found that transparency (vs. no transparency) increased trust and understanding of the system due to an increase in perceived disclosure (supporting H5). However, the increase in perceived disclosure decreased overall agreement (RQ1 and RQ2). We also found partial support for H6\u2014interactive transparency increased trust due to an increase in user agency, but it did not increase understanding. Finally, we also found that the effects of interactive transparency are equally powerful regardless of the source of moderation (answering RQ3 and showing no support for H7)."
        },
        {
            "heading": "Discussion",
            "text": "Overall, our data reveal that user trust in AI for content moderation is enhanced when the system (a) triggers positive machine heuristics in the minds of users and (b) provides users an opportunity to participate in content classification. More specifically, users trust AI as content moderators just as much as humans, but it depends on the heuristic that is triggered when users are told that AI is the source of classification. On the one hand, when AI is the source of classification or works in tandem with humans, the positive machine heuristic can be invoked, in which case users trust the AI and agree with it more compared to human classifiers because they believe AI is\nmore accurate and objective. However, AI as a source of classification may also elicit the negative machine heuristic, or the perception that AI is rigid and unable to discern human emotion, in which case users will be less likely to trust and agree with AI\u2019s decision.\nAlthough having both human and AI as sources of classification did not enhance users\u2019 trust in the classification system (as expected), providing agency to users did. Regardless of the source of classification, providing users the opportunity to suggest words to include or exclude from the classification system increased their trust and agreement with the system. This occurred because the resulting interactivity heightened sense of control and provided a greater sense of agency to individual users. It is possible that users have equal concerns about the classification of user-generated content, regardless of who conducts the classification (West, 2018). This means that the concerns do not stem only from previous perceptions and schemas users hold about AI, but also because they view classification of user-generated content as inherently problematic, as evidenced by the main effect of classification decision found in this study, which revealed that users trust the classification system more when the post is not flagged (compared to when it is).\nFindings also reveal that providing transparency, regardless of whether it is interactive or not, increased perceived disclosure en route to higher agreement, trust (attitudinal and behavioral), and understanding of the system. Interestingly, providing transparency-only also heightened perceived user agency, which increased agreement and trust toward the system. This means that the mere disclosure of the keywords utilized for classification provided the sense of control necessary for the user to feel more agentic, perhaps because knowledge about the rules of classification allows users to make a more informed decision about their perceptions of the system and how the classification is performed. This finding is consistent with Suzor et al.\u2019s (2019) analysis of survey responses submitted to the advocacy project onlinecensorship.org. Among users\u2019 main concerns was that platforms rarely provided reasons for removing content or suspending users. In light of the lack of information, users formulate their own explanations\nTrustc' = -.12 c =.04\nDisclosure\nUser Agency\na1 = 1.08***\na2 = .24**\nb1 = .11***\nb2 = .15***\nTransparency Type: No T. (0) T. (1)\nTrustc' = .04 c = .11"
        },
        {
            "heading": "Disclosure",
            "text": "User Agency\na1 = -.06\na2 = .43***\nb1 = .11***\nb2 = .15***\nTransparency Type: T. Only (0) Int. T. (1)\nFigure 5. Mediation Analysis: Transparency Type and Attitudinal Trust. Note: Mediating effects of disclosure and user agency. No T. refers to the no transparency condition, T refers to transparency (whether interactive or no), T Only refer to transparency only, and Int. T. refers to interactive transparency. The independent variable represents the comparison between no transparency and transparency [left], and transparency only and interactive transparency [right]. All effects are unstandardized coefficients (b). ***p < .001, ** p < .01, *p < .05. c\u2019 represents the direct effect of transparency type on trust; c represents the total effect of transparency type on trust.\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\nof what might have occurred. Our study extends this finding by revealing that disclosing decision criteria not only results in more understanding of the system, but also enhances trust and agreement with its decision.\nThat said, user agency was heightened in the interactive transparency condition compared to the transparency-only condition. This finding is consistent with the propositions of the action route of TIME (Sundar et al., 2015). The sense of control resulting from interactivity improved overall user experience, but it was not sufficient to induce higher understanding of the system. It is possible that the prediction of TIME holds true in so far as interactivity can increase user agency en route to better user experience of the classification system."
        },
        {
            "heading": "Theoretical and practical implications",
            "text": "This study holds several theoretical and practical implications. Theoretically, it tested the predictions of the HAIITIME model (Sundar, 2020) by examining if human collaboration in the classification process via source signaling (cue route) and via user interaction (action route) influenced users\u2019 trust and understanding of classification systems. As predicted by theory, AI as source of classification (whether alone or in tandem with human) invoked the machine heuristic, indirectly influencing user perception of the system. When users invoked the positive machine heuristic, the AI (alone or in tandem) improved users\u2019 perception of the system. However, the opposite occurred when the negative machine heuristic was invoked. Past research exploring the machine heuristic has tended to focus on the positive machine heuristic, or the cognitive rule of thumb that if a machine is the source of a task, then it is objective and accurate. Nevertheless, as Lee (2018) suggests, the rule of thumb that users have of machines is not always positive, but may reflect machines\u2019 inability to detect human emotions. Results of this study reveal the importance of considering both positive and negative perceptions when analyzing human\u2013AI interaction.\nThis study also adds to our understanding of the action route of TIME in the AI context. As hypothesized by Sundar (2020), providing users the ability to tinker with the classification system helped enhance users\u2019 sense of agency. User agency, in turn, increased agreement and trust, regardless of the source of moderation. Results of this study also highlight user agency as a mechanism for enhancing process transparency. When a system discloses to the user the rules of classification, it is being respectful of users\u2019 needs and feelings, which imbue in users a greater sense of agency compared to not disclosing the rules. If the system additionally seeks user input, it affords even more user agency. We see evidence of this effect in our results.\nPractically, this study provides ideas for enhancing the design of AI to improve trust and appropriate reliance of AIdriven technologies for societal benefits. First, providing source transparency, especially when using AI-based classification systems, not only aligns with the premises of explainable AI research, but the invoked heuristics have important implications for user experience. In evaluating an AI-driven classification system, AI as source invokes both positive and negative stereotypes of machines. Thus, in designing the AI cue, designers should focus on highlighting those facets of AI that reflect its objectivity and accuracy. This could be achieved by including the accuracy score and precision of the classification decision, and accurately conveying the strengths\nand capabilities of the system. The development of such cues should adhere to ethical design practices, with the paramount goal of providing source transparency in a way that accurately conveys the real capabilities of the system.\nOur findings also provide ideas for operationalizing algorithmic transparency. We show that displaying the keywords used for classification enhances overall user experience and perceived understanding of the system. More importantly, we demonstrate that incorporation of interactivity in transparency serves to enhance user agency and improve user trust toward the technology. In this study, interactivity was provided by allowing users to click on words to suggest their inclusion or exclusion for classification. For more complex scenarios, designers can consider alternatives such as including a \u201cshould be included\u201d and \u201cshould not be included\u201d button next to the rules used by the algorithm (similar to a voting system), plus a commenting function so that users can type suggestions, or allow users to provide their input on the relative weight or importance that a rule should have for each classification decision. Of course, it is important to conduct a cost-benefit analysis of the incorporation of interactivity. Although interactivity heightened user agency more than providing transparency alone, transparency alone also increased user agency (compared to no transparency) and was equally good at heightening perceived disclosure. Thus, the effort and resources necessary to build an interactive interface would be feasible if imbuing user agency is a major design goal over and beyond mere disclosure. For example, user interaction may serve as feedback that can help to identify \u201cbias bugs\u201d and other possible issues with the AI system (Eslami et al., 2019, p. 13). Hence, incorporating interactive transparency can be beneficial from a user-experience perspective and for fostering ethical design practices."
        },
        {
            "heading": "Limitations and future research",
            "text": "The aforementioned findings should be interpreted in light of the study\u2019s limitations. First, many participants did not pass the manipulation check. More participants in the AI condition were able to recall the source of classification correctly, compared to the human and the combined conditions. It is possible that this occurred because classification of user-generated content (regardless of source) was performed based on previously defined rules. Using rigid rules might have been perceived as a very mechanistic task, violating the expectation that users hold about humans in content moderation roles, such as the human ability to contextualize and empathize. Similarly, it is possible that despite being told that the moderator was human, the overall interaction may have resembled users\u2019 previous experiences with AI-based systems, influencing their perceptions of the \u201creal\u201d source of classification. Additionally, care was taken to create an intuitive interactive transparency condition where users clearly understood their task and the purpose behind the interactivity they were provided. Instructions explaining how to operate the system were included before users were given the opportunity to tinker with the rules of classification. This additional explanation, unique to the interactive transparency condition, might have induced higher levels of systematic processing than what users would have experienced without such an explanation. Likewise, in this study the system was tasked with classifying other users\u2019 posts. Participants\u2019 reactions and perceptions may be different if the system classified their own posts or those of their own network of friends and family.\nD ow nloaded from https://academ ic.oup.com /jcm c/article/27/4/zm ac010/6648459 by Indian Institute of Technology Patna user on 17 January 2024\nThese limitations, along with findings of this study, stimulate several avenues for future study. For starters, although we found that interactive transparency increased user experience, it was not able to improve user understanding of the system. Though it is possible that interactivity serves as a tool to enhance affective and behavioral outcomes but not cognitive ones, it is also possible that the type of interactivity we used, whereby users clicked to change words from one list to the other, is not the most effective at inducing cognitive elaboration. It is worth testing if other types of interactivity, such as message-interactivity (where messages and actions are contingent on each other) (Oh & Sundar, 2015), are better able to achieve this goal. Future research would do well to expand the range and scope of interactive transparency, not only conceptually but also operationally by tracking specific user actions and examining if they are related to outcomes of interest. Furthermore, in this research we were interested in understanding the heuristics that govern user\u2014AI interactions, thus we tested the positive and negative machine heuristics. However, there are other heuristics that could be triggered by the presence of a human source (e.g., social presence heuristic; Sundar, 2008) that we did not consider. Likewise, in the human condition, we assumed a single moderator and therefore did not consider the potential for group dynamics (of multiple moderators, as in crowdsourced moderation) to trigger heuristics and social norms that differ from that of a single moderator. Future research should analyze this dynamic as we achieve a better understanding of cognitive heuristics that govern our interactions with content moderation systems (whether human or AI-based)."
        },
        {
            "heading": "Conclusion",
            "text": "Given the rampant proliferation of problematic usergenerated content on social media, the use of AI tools for content moderation is promising, if not inevitable. Findings of this study reveal that user acceptance of AI as content moderator depends on the cognitive heuristic that is triggered when users are told AI is the source of moderation. Additionally, affording users agency over the classification system by allowing them to provide input into the classification criteria improves user experience and trust in the system, regardless of the source of moderation. It is hoped that these findings can help motivate design innovations in classification systems and promote appropriate reliance on AI-based technologies, so that we can capitalize on their vast potential for societal benefits in a number of domains."
        },
        {
            "heading": "Data availability",
            "text": "The data underlying this article will be shared on reasonable request to the corresponding author."
        },
        {
            "heading": "Supplementary material",
            "text": "Supplementary material is available online at Journal of Computer-Mediated Communication."
        },
        {
            "heading": "Notes",
            "text": "1. We analyze attitudinal trust and behavioral trust. Attitudinal\ntrust refers to users\u2019 expectations and feelings regarding the classification system. Behavioral trust refers to user\u2019s behavioral intentions (Lee & See, 2004). 2. There is a push for higher transparency in real-life AI-based tech-\nnologies (e.g., U.K.\u2019s algorithm transparency standard). Current systems provide transparency through different mechanisms and to different degrees. In the context of moderation of usergenerated content, these statements are often static without user input (e.g., https://help.twitter.com/en/rules-and-policies/hatefulconduct-policy). 3. For our hypothesis regarding transparency, we do not hypothesize\nan effect on agreement, but on perceived understanding. Theoretically, source signaling cues (cue route of TIME, Sundar et al. 2015) influence agreement with the system because of heuristic processing of information (i.e., if the machine is accurate and objective, then the decision must be true). Understanding of the system, on the other hand, requires systematic processing. Interactivity (action route of TIME, Sundar et al., 2015) is shown to improve systematic processing, thus interactive transparency could enhance understanding of the system. However, higher systematic processing does not necessarily mean users will agree with the classification. Thus, we pose RQ1 and RQ2 to test the effects of transparency on agreement. 4. An a priori power analysis using G* Power revealed that to\nachieve a medium effect size (f \u00bc .25), at a power of .80 and an error lower than .05, approximately 330 participants are\nrequired. 5. There was no effect for source (p \u00bc .85), transparency (p \u00bc .20),\nor their interaction (psource*interactivity \u00bc .78, psource*classification decision \u00bc .77, pinteractivity*classification decision \u00bc .33, psource*interactivity*classification decision \u00bc .38). 6. Source: p \u00bc .41; transparency: p \u00bc .12; classification decision; p\u00bc .98, source*transparency: p \u00bc .38, source*classification decision: p \u00bc .37, transparency*classification decision: p \u00bc .13, source*transparency*classification decision: p \u00bc .84. 7. There was no significant effect of source (p \u00bc .11), classification decision (p \u00bc .97), or their interactions with transparency (psource*transparency \u00bc .39; psource*classification decision\u00bc .82; pclassification decision*transparency \u00bc .50; psource*transparency*classification decision\u00bc .31). 8. To rule out the possibility that the effect of interactive transpar-\nency is contingent on users actually engaging with the system (vs. taking no action), we ran a series of regression analysis, testing if the time spent, in seconds, engaging with the interface influenced user agency and trust, for those in the interactive transparency condition. We found no statistically significant effects; user agency (B \u00bc 2.27, p \u00bc.93), user trust (B \u00bc 17.77, p \u00bc.55). For these analyses, the time variable was positively skewed, thus we used its reciprocal."
        }
    ],
    "title": "When AI moderates online content: effects of human collaboration and interactive transparency on user trust",
    "year": 2022
}