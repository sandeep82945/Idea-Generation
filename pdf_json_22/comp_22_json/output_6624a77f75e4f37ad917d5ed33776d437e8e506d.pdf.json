{
    "abstractText": "Heterogeneous graph neural network has unleashed great potential on graph representation learning and shown superior performance on downstream tasks such as node classification and clustering. Existing heterogeneous graph learning networks are primarily designed to either rely on pre-defined meta-paths or use attention mechanisms for type-specific attentive message propagation on different nodes/edges, incurring many customization efforts and computational costs. To this end, we design a relation-centered Pooling and Convolution for Heterogeneous Graph learning Network, namely PC-HGN, to enable relation-specific sampling and cross-relation convolutions, from which the structural heterogeneity of the graph can be better encoded into the embedding space through the adaptive training process. We evaluate the performance of the proposed model by comparing with stateof-the-art graph learning models on three different real-world datasets, and the results show that PC-HGN consistently outperforms all the baseline and improves the performance maximumly up by 17.8%.",
    "authors": [
        {
            "affiliations": [],
            "name": "Tiehua Zhang"
        },
        {
            "affiliations": [],
            "name": "Yuze Liu"
        },
        {
            "affiliations": [],
            "name": "Yao Yao"
        },
        {
            "affiliations": [],
            "name": "Youhua Xia"
        },
        {
            "affiliations": [],
            "name": "Xin Chen"
        },
        {
            "affiliations": [],
            "name": "Xiaowei Huang"
        },
        {
            "affiliations": [],
            "name": "Jiong Jin"
        }
    ],
    "id": "SP:84fcb1fe133554a8668911d0d0a0013b7f972e9e",
    "references": [
        {
            "authors": [
                "H. Abdi",
                "L.J. Williams"
            ],
            "title": "Principal component analysis",
            "venue": "Wiley Interdisciplinary Reviews: Computational",
            "year": 2010
        },
        {
            "authors": [
                "D. Bahdanau",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "Neural machine translation by jointly learning to align and translate",
            "venue": "arXiv preprint arXiv:1409.0473.",
            "year": 2014
        },
        {
            "authors": [
                "J. Chen",
                "T. Ma",
                "C. Xiao"
            ],
            "title": "FastGCN: Fast learning with graph convolutional networks via importance sampling",
            "venue": "arXiv preprint arXiv:1801.10247.",
            "year": 2018
        },
        {
            "authors": [
                "M. Defferrard",
                "X. Bresson",
                "P. Vandergheynst"
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "International Conference on Neural Information Processing Systems, 3837\u20133845.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Dong",
                "N.V. Chawla",
                "A. Swami"
            ],
            "title": "metapath2vec: Scalable representation learning for heterogeneous networks",
            "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 135\u2013144.",
            "year": 2017
        },
        {
            "authors": [
                "X. Glorot",
                "Y. Bengio"
            ],
            "title": "Understanding the difficulty of training deep feedforward neural networks",
            "venue": "International Conference on Artificial Intelligence and Statistics, 249\u2013256.",
            "year": 2010
        },
        {
            "authors": [
                "W.L. Hamilton",
                "R. Ying",
                "J. Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "International Conference on Neural Information Processing Systems, 1025\u20131035.",
            "year": 2017
        },
        {
            "authors": [
                "H. Han",
                "T. Zhao",
                "C. Yang",
                "H. Zhang",
                "Y. Liu",
                "X. Wang",
                "C. Shi"
            ],
            "title": "OpenHGNN: An Open Source Toolkit for Heterogeneous Graph Neural Network",
            "venue": "ACM International Conference on Information and Knowledge Management, 3993\u20133997.",
            "year": 2022
        },
        {
            "authors": [
                "H. Hong",
                "H. Guo",
                "Y. Lin",
                "X. Yang",
                "Z. Li",
                "J. Ye"
            ],
            "title": "An attention-based graph neural network for heterogeneous structural learning",
            "venue": "AAAI Conference on Artificial Intelligence, volume 34, 4132\u20134139.",
            "year": 2020
        },
        {
            "authors": [
                "Z. Hu",
                "Y. Dong",
                "K. Wang",
                "Y. Sun"
            ],
            "title": "Heterogeneous graph transformer",
            "venue": "Web Conference, 2704\u20132710.",
            "year": 2020
        },
        {
            "authors": [
                "W. Huang",
                "T. Zhang",
                "Y. Rong",
                "J. Huang"
            ],
            "title": "Adaptive sampling towards fast graph representation learning",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "B. Jin",
                "C. Gao",
                "X. He",
                "D. Jin",
                "Y. Li"
            ],
            "title": "Multibehavior recommendation with graph convolutional networks",
            "venue": "International ACM SIGIR Conference on Research and Development in Information Retrieval, 659\u2013668.",
            "year": 2020
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "T.N. Kipf",
                "M. Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "International Conference on Learning Representations, 1\u201314.",
            "year": 2017
        },
        {
            "authors": [
                "J. Lee",
                "I. Lee",
                "J. Kang"
            ],
            "title": "Self-attention graph pooling",
            "venue": "International Conference on Machine Learning, 3734\u20133743.",
            "year": 2019
        },
        {
            "authors": [
                "M.E. Newman"
            ],
            "title": "Modularity and community structure in networks",
            "venue": "Proceedings of the National Academy of Sciences, 103(23): 8577\u20138582.",
            "year": 2006
        },
        {
            "authors": [
                "Y. Rong",
                "Y. Bian",
                "T. Xu",
                "W. Xie",
                "Y. Wei",
                "W. Huang",
                "J. Huang"
            ],
            "title": "Self-supervised graph transformer on largescale molecular data",
            "venue": "arXiv preprint arXiv:2007.02835.",
            "year": 2020
        },
        {
            "authors": [
                "M. Schlichtkrull",
                "T.N. Kipf",
                "P. Bloem",
                "R. v. d. Berg",
                "I. Titov",
                "M. Welling"
            ],
            "title": "Modeling relational data with graph convolutional networks",
            "venue": "European Semantic Web Conference, 593\u2013607.",
            "year": 2018
        },
        {
            "authors": [
                "C. Shi",
                "B. Hu",
                "W.X. Zhao",
                "S.Y. Philip"
            ],
            "title": "Heterogeneous information network embedding for recommendation",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 31(2): 357\u2013370.",
            "year": 2018
        },
        {
            "authors": [
                "C. Shi",
                "Y. Li",
                "J. Zhang",
                "Y. Sun",
                "S.Y. Philip"
            ],
            "title": "A survey of heterogeneous information network analysis",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, 29(1): 17\u201337.",
            "year": 2016
        },
        {
            "authors": [
                "Y. Sun",
                "J. Han"
            ],
            "title": "Mining heterogeneous information networks: a structural analysis approach",
            "venue": "ACM SIGKDD Explorations Newsletter, 14(2): 20\u201328.",
            "year": 2013
        },
        {
            "authors": [
                "P. Veli\u010dkovi\u0107",
                "G. Cucurull",
                "A. Casanova",
                "A. Romero",
                "P. Lio",
                "Y. Bengio"
            ],
            "title": "semi-ttention networks",
            "venue": "International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "X. Wang",
                "D. Bo",
                "C. Shi",
                "S. Fan",
                "Y. Ye",
                "S.Y. Philip"
            ],
            "title": "A survey on heterogeneous graph embedding: methods, techniques, applications and sources",
            "venue": "IEEE Transactions on Big Data.",
            "year": 2022
        },
        {
            "authors": [
                "X. Wang",
                "D. Bo",
                "C. Shi",
                "S. Fan",
                "Y. Ye",
                "P.S. Yu"
            ],
            "title": "A survey on heterogeneous graph embedding: methods",
            "venue": "Techniques, Applications and Sources.",
            "year": 2020
        },
        {
            "authors": [
                "B. Weisfeiler",
                "A. Leman"
            ],
            "title": "reduction of a graph to canonical form and the algebra which appears therein",
            "venue": "NTI, Series, 2(9): 12\u201316.",
            "year": 1968
        },
        {
            "authors": [
                "T. Yang",
                "L. Hu",
                "C. Shi",
                "H. Ji",
                "X. Li",
                "L. Nie"
            ],
            "title": "HGAT: Heterogeneous graph attention networks for semisupervised short text classification",
            "venue": "ACM Transactions on Information Systems (TOIS), 39(3): 1\u201329.",
            "year": 2021
        },
        {
            "authors": [
                "Z. Ying",
                "J. You",
                "C. Morris",
                "X. Ren",
                "W. Hamilton",
                "J. Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "L. Yu",
                "L. Sun",
                "B. Du",
                "C. Liu",
                "W. Lv",
                "H. Xiong"
            ],
            "title": "Heterogeneous graph representation learning with relation awareness",
            "venue": "IEEE Transactions on Knowledge and Data Engineering.",
            "year": 2022
        },
        {
            "authors": [
                "H. Zeng",
                "H. Zhou",
                "A. Srivastava",
                "R. Kannan",
                "V. Prasanna"
            ],
            "title": "GraphSAINT: Graph sampling based inductive learning method",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "C. Zhang",
                "D. Song",
                "C. Huang",
                "A. Swami",
                "N.V. Chawla"
            ],
            "title": "Heterogeneous graph neural network",
            "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 793\u2013803.",
            "year": 2019
        },
        {
            "authors": [
                "T. Zhang",
                "Y. Liu",
                "X. Chen",
                "X. Huang",
                "F. Zhu",
                "X. Zheng"
            ],
            "title": "GPS: A policy-driven sampling approach for graph representation learning",
            "venue": "arXiv preprint arXiv:2112.14482.",
            "year": 2021
        },
        {
            "authors": [
                "T. Zhang",
                "Y. Liu",
                "Z. Shen",
                "R. Xu",
                "X. Chen",
                "X. Huang",
                "X. Zheng"
            ],
            "title": "FedRel: An adaptive federated relevance framework for spatial temporal graph learning",
            "venue": "arXiv preprint arXiv:2206.03420.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "Introduction",
            "text": "There has been increasing research on modelling graph data as such data becomes more ubiquitous in real-world scenarios. These well-organized graph data contain rich yet complex semantic and structural information of various realworld systems such as recommendation (Shi et al. 2018), medical diagnosis (Rong et al. 2020; Zhang et al. 2022) and text analysis (Bahdanau, Cho, and Bengio 2014). To better leverage such information, graph representation learning, which aims to learn a function that maps the input space into the low-dimensional embedding space while preserving the attributes of the graph, aroused to the spotlight to serve the purpose of node/graph classification and clustering tasks. Conventionally, matrix factorization (e.g., adjacency matrix) methods (Newman 2006; Weisfeiler and Leman 1968) have been used to learn the latent embedding vectors of a graph, yet it suffers from both the computational cost on large-scale graph data and statistical performance drawback (Shi et al. 2016).\n*co-first author (corresponding author: Tiehua Zhang). Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nGraph Neural Networks (GNN) have recently become the most prevalent technique to enhance graph learning performance. However, most GNN models, such as GCN, GraphSAGE, GAT and GPS, are initially designed for homogeneous graphs (Wang et al. 2022), which only involve one type of node and edge. These models thus cannot be directly applicable to graphs with different types of entities (i.e., heterogeneous graphs with various node/edge types). Specifically, heterogeneous graphs present a more complex semantic-dependent topology, meaning that the local structure of one node varies in different types of relations. Apart from that, in heterogeneous graphs, there are a number of types of nodes and edges, each of which represents a particular type of attribute in the graph and thus requires different feature spaces. The heterogeneity attributes need to be taken into account when designing heterogeneous embedding methods, as it is essential to ponder on the significant roles of nodes and edges altogether. Currently, much research focuses on designing the specialized GNNs to learn representations in heterogeneous graphs. For instance, both HetGNN (Zhang et al. 2019) and HAN (Yang et al. 2021) rely on pre-defined meta-paths to perform graph learning. Other existing works either emphasize on capturing the characteristics of nodes (e.g., HetSANN (Hong et al. 2020), HGT (Hu et al. 2020)) or multi-typed edge properties (e.g., RGCN (Schlichtkrull et al. 2018), MBGCN (Jin et al. 2020)). While the most recent work R-HGNN (Yu et al. 2022) starts to learn both the semantic meaning of edge and node features in a collaborative manner, it incurs a high computational cost due to the heavy use of attentions at both node and edge level.\nIn light of these limitations, we propose an adaptive relation-centric heterogeneous graph learning method, aiming to better incorporate the information of multi-relation types without concerning pre-defined meta-paths in heterogeneous graphs. Specifically, we first design the relationspecific pooling to enable the importance-based sampling, aiming to only select a subset of neighbor nodes to improve the learning efficiency. Based on the sampled neighbors of each node, a cross-relation convolution is implemented to learn the heterogeneity of the graph through different types of relations. After that, both the pooling and convolution can be optimized end-to-end. The contributions are summarized\nar X\niv :2\n21 0.\n17 14\n2v 1\n[ cs\n.L G\n] 3\n1 O\nct 2\n02 2\nas follows:\n\u2022 We propose a novel graph learning model to enable both relation-specific pooling and cross-relation convolutions on heterogeneous graphs. The model can be directly applied to any heterogeneous graphs without concerning the pre-defined meta-paths.\n\u2022 We project relation-specific neighbors into the same latent space regardless of the neighbor types and design an adaptive importance-based pooling to enable sampling for each relation type. Benefiting from the sampled neighbors set, an effective cross-relation convolution is implemented to incorporate the heterogeneity attributes of the graph into the node embeddings.\n\u2022 We conduct extensive experiments to evaluate the performance of the proposed model. The results show that PC-HGN excels in all three real-world datasets compared with state-of-the-art homogeneous and heterogeneous graph learning models. We also analyze the impact of filters and hidden node dimensions to interpret the effectiveness of the proposed model."
        },
        {
            "heading": "Related Work",
            "text": "There has been a rich line of research on graph representation learning in recent years. The graph convolution network (GCN) (Kipf and Welling 2017) is the first of its kind on learning the node embedding through graph Laplacian methods. Following that, graph attention network (GAT) (Velic\u030ckovic\u0301 et al. 2018) introduces multi-head attention mechanisms on each edge to enable the attentive message aggregation when updating the node embeddings. As opposed to aggregating the message from all connected nodes like GCN and GAT, researchers start to focus on designing more efficient graph learning algorithms. Several sampling-based graph learning methods, including GraphSAGE (Hamilton, Ying, and Leskovec 2017), ASGCN (Huang et al. 2018), FastGCN (Chen, Ma, and Xiao 2018), GPS (Zhang et al. 2021), were developed for fast representation learning on graphs. To be more specific, both GraphSAGE, and GPS adopt the node-based sampling, and it considers only part of the connected neighbor nodes when aggregating the neighbor messages. AS-GCN and FastGCN, on the other hand, design different layer-based sampling strategies. While FastGCN constructs each layer from sampled nodes independently, AS-GCN samples all neighborhoods nodes altogether and allow the neighborhood sharing, in which the lower layer is sampled conditionally on the top one to preserve the between-layer connections. GraphSAINT (Zeng et al. 2020) proposes a graph-level sampling approach, in which a full graph is downsampled into subgraphs based on designated node, edge and random walk samplers. Similarly, some research explore the pooling operations on graph. For instance, (Defferrard, Bresson, and Vandergheynst 2016) uses a binary tree indexing for graph coarsening, in which 1-D pooling operations are applied on the fixed node indices in the tree. (Ying et al. 2018) achieves the pooling with the help of an assignment matrix, and nodes can be assigned to different clusters of the next layer. Graph U-Nets designs a encoder-decoder pooling architecture to\nspeed up the large-scale graph learning. The gPool encoding layer adaptively selects nodes according to the scalar projection value on the trainable projection vector to form a small subgraph, and the inverse gUnpool decoding layer is used to restore the original structure of the graph by using the position information of the selected nodes.\nAs pointed out in (Wang et al. 2020), the sampling on heterogeneous graph is still in its infancy stage and worth exploring owing to the rich structure and semantic information across different node/edge types. Two scalable representation learning models, namely metapath2vec and metapath2vec++, are developed (Dong, Chawla, and Swami 2017). Specifically, metapath2vec builds heterogeneous neighborhoods of nodes based on random walks on the pre-defined metapaths, and uses heterogeneous skipgram model to perform node embedding. Metapath2vec++ further explores synchronous modeling of structural and semantic correlations across different edges in heterogeneous graphs. Recently, many efforts aim to develop GNN-related models for heterogeneous graph learning. HAN (Yang et al. 2021) aims to learn the importance between a node and its meta-path based neighbors, and semantic-level attention is designed to learning the hand-designed mete-paths. Following that, type-specific parameters are considered in HGT (Hu et al. 2020) to learn the characteristics of different nodes and relations. To make the training on web-scale data more efficient, it designs a HGSampling algorithm to sample an equal number of nodes per node type during training process. R-HGNN (Yu et al. 2022) proposes a relationaware method to learn the semantic representation of relations while discerning the node representations with respect to different relation types."
        },
        {
            "heading": "Methodology",
            "text": "We present the proposed PC-HGN in this section. We first describe the notations that denote the heterogeneous graphs and then elaborate on relation-specific pooling and crossrelation convolutions, respectively."
        },
        {
            "heading": "Heterogeneous Graph Definition",
            "text": "We formally define the heterogeneous graph in this part. Given a heterogeneous graph G = (V, E , Tv, Te), it encompasses a number of nodes V with multiple node types Tv and edges E with multiple edge types Te. We define the node type mapping function \u03c6 : V \u2192 Tv and the edge type mapping function \u03c8 : E \u2192 Te, respectively (Sun and Han 2013). Each heterogeneous graph presents multiple edge types, meaning |Te| > 1. We use N to denote the number of nodes in the graph |V| = N , and the neighbor of each node v is represented as N (v). Simply, euv denotes the edge connection from u to v. We use X \u2208 RN\u00d7D to represent the initial node feature matrix, and node v \u2019s initial feature vector is xv \u2208 RD."
        },
        {
            "heading": "Relation-specific Pooling",
            "text": "As pointed out in (Lee, Lee, and Kang 2019), pooling operation is considered effective to improve both training efficiency and generalization ability of the model. To enable\nthe pooling on semantic-dependent heterogeneous graphs, we propose the relation-specific pooling operations, which enables an adaptive sampling on neighbor nodes for each relation type.\nAs illustrated in Fig. 1 (a), we first split the graph into different subgraphs based on different relations. Given a target node v and its neighbor nodesN (v), we define a relation as \u3008\u03c6 (u) , \u03c8 (euv) , \u03c6 (v)\u3009 between node v and node u \u2208 N (v), indicating there exists an edge euv from neighbor node u to target node v with edge type \u03c8 (euv). Then, We conduct relation-specific pooling operation on each subgraph to get sampled neighbor nodes under that relation. Specifically, for any node v, its neighbor node set can be represented as N tv = { u \u2223\u2223u \u2208 N (v) , \u03c8 (euv) = t, t \u2208 Te, v \u2208 V}, in which all nodes belongs to only one relation. The initial node feature matrix is then denoted as XtNv . We formulate the relation-specific pooling of t \u2208 Te in the following:\nstNv = X t Nv \u00b7 vt \u2016vt\u2016 ,\nidxtNv = rank ( stNv , k ) ,\ns\u0302tNv = \u03c3 ( stNv [ idxtNv ]) ,\nX\u0302 t Nv = X t Nv [ idxtNv , : ] , X\u0303 t\nNv = X\u0302 t Nv ( s\u0302tNv \u00b7 11\u00d7D\n) (1)\nFor each relation, we project the neighbors under that relation to a latent space via a trainable vector vt, from which the respective important scores are calculated as stNv \u2208 R\n|N tv|. In other word, it measures how much information of neighbor nodes can be retained when projected onto the direction of vt for each relation t. rank ( stNv , k ) is the importancebased node ranking function, which returns indices of the k-largest values in stNv . idx t Nv , returned by the ranking function, contains the indices of neighbors selected for target node v. stNv [ idxtNv ] extracts values with indices, followed by a non-linear function \u03c3 to get weighted score vector s\u0302tNv . Similarly, XtNv [ idxtNv , : ] perform the row extraction to form the feature matrix of selected neighbors, whose result is denoted as X\u0302 t\nNv . 11\u00d7D \u2208 R 1\u00d7D is a row vector with all ele-\nments being 1, and denotes the element-wise matrix multiplication. It is worth noting that in case of lack of neighbor nodes in a specific relation, we apply mean operation on X\u0302 t Nv for padding purpose, guaranteeing X\u0303 t\nNv \u2208 R k\u00d7D."
        },
        {
            "heading": "Cross-relation Convolutions",
            "text": "The process of cross-relation convolution aims to consider both the heterogeneity attributes and node feature embedding simultaneously, which is shown in Fig 1 (c). The output of relation-specific pooling is represented as XNv = [ X\u0303 t1 Nv , X\u0303 t2 Nv , ..., X\u0303 t|Te| Nv ] , which contains sampled neighbor node features X\u0303 t\nNv \u2208 R k\u00d7D for each relation t.\nTherefore, the sampled feature matrices with different relations are stacked into the three-dimensional tensor XNv \u2208 R|Te|\u00d7k\u00d7D. Note that for computation convenience, we apply zero-padding along the relation dimension if such relation does not exist for node v. When it comes to convolution, we use P trainable filters, each of which encompasses different kernels stacked together along the relation dimension Kp = [ Kt1p ,K t2 p , ...,K t|Te| p ] \u2208 R|Te|\u00d7s\u00d7D, p \u2208 [1, ..., P ]. Each kernel Ktp \u2208 Rs\u00d7D in p-th filter is used to generate the feature map of relation t, where s denotes the number of search window. The process of cross-relation convolution of filter p is formulated as:\nhpv = XNv \u25e6 Kp (2)\nwhere hpv \u2208 R(k\u2212s+1) represents the convoluted results. Concretely, the m-th element of hpv is calculated as:\n(XNv \u25e6 Kp)(m) = |Te|\u2211 i=1 s\u2211 j=1 X\u0303 ti Nv [m+ j \u2212 1, :] \u00b7 ( Ktip [j, :] )T (3)\nwhere X\u0303 ti Nv [m+ j \u2212 1, :] and K ti p [j, :] indicates the index dependent row extractions. Following that, the convoluted results of each filter are stacked to generate the input of a shallow multi-layer perceptions (MLP), represented as:\nh\u0302v = MLP  P\u2225\u2225\u2225 p=1 hpv  (4) Herein, h\u0302v represents the learnt hidden embedding of\nnode v."
        },
        {
            "heading": "Experiment",
            "text": "In this section, we conduct extensive experiments, aiming to answer the following research questions:\n\u2022 RQ1: how does our method perform compared with other state-of-the-art homogeneous/heterogeneous baselines for representation learning.\n\u2022 RQ2: what is the impact of cross-relation convolutions on the model.\n\u2022 RQ3: how much the hyper-parameters (e.g., embedding dimension and the number of sampled heterogeneous neighbors) affect the performance of the model."
        },
        {
            "heading": "Dataset",
            "text": "We adopt the the following three real-world datasets to verify the performance of the proposed model, including two citation networks DBLP and ACM, and a moive dataset ACM.\n\u2022 DBLP1 contains three different types of nodes, including Paper(P), Author(A), and Conference(C). Also, edges are presented as P-A, A-P, P-C and C-P, each representing a respective edge type. Four different categories of papers are defined as labels of this dataset. The initial node features are calculated using bag-of-words.\n\u2022 ACM2 shares a similar data characterises with DBLP. It contains three types of nodes (Paper(P), Author(A) and Subject(S)) and four types of edges (P-A, A-P, P-S and S-P). The papers are labelled according to three different categories. It also uses bag-of-words to construct the initial node features.\n\u2022 IMDB3 contains movie(M), actors(A) and directors(D) as node types. The genres of movies are used as different labels for each node. Node features are also initialised using bag-of-words."
        },
        {
            "heading": "Baselines",
            "text": "To verify the effectiveness of the proposed model, we compare it with several state-of-the-art models designed for homogeneous and heterogeneous graph learning, respectively.\n\u2022 GCN (Kipf and Welling 2017) is a graph convolutional network designed specifically for homogeneous graph learning.\n\u2022 GAT (Velic\u030ckovic\u0301 et al. 2018) is the first work that introduces attention mechanism in homogeneous graph learning. It enables the weighted message aggregations from neighbor nodes.\n\u2022 GraphSAGE (Hamilton, Ying, and Leskovec 2017) designs a sampling approach when aggregating messages from neighbor nodes. It also supports different aggregation functions.\n\u2022 GraphSAINT (Zeng et al. 2020) splits nodes and edges from a bigger graph into a number of subgraphs, on which the GCN is applied for node representation learning.\n1https://s3.cn-north-1.amazonaws.com.cn/dgldata/dataset/openhgnn/dblp4GTN.zip\n2https://s3.cn-north-1.amazonaws.com.cn/dgldata/dataset/openhgnn/acm4GTN.zip\n3https://s3.cn-north-1.amazonaws.com.cn/dgldata/dataset/openhgnn/imdb4GTN.zip\n\u2022 HAN (Yang et al. 2021) uses attention techniques on heterogeneous graph learning, in which the node embeddings are updated through manually designed metapaths.\n\u2022 HGT (Hu et al. 2020) designs type-specific attention layers, assigning different trainable parameters to each node and edge type.\n\u2022 R-HGNN (Yu et al. 2022) proposes a relation-aware method to learn the semantic representation of edges while discerning the node representations with respect to different relation types."
        },
        {
            "heading": "Experiment Setup",
            "text": "We describe the experiment setup for both baseline models and the proposed PC-HGN in this part. For baseline models dependent on attention mechanisms (i.e., GAT, HAN, and HGT), we set the number of attention heads to 8 uniformly. The sample window of GraphSAGE is set to 10 (Han et al. 2022). We adopt the node sampling strategy for GraphSAINT and use 8000 as the node budget and 25 as the number of subgraphs (default from the released code). We use 256 as the hidden dimension of node representation throughout all baseline models for fair comparisons, and the layer depth of all models is set to 3. For the proposed PC-HGN, we set the number of kernels to 64 and pooling size to 2. We randomly split all three datasets into train/val/test with the ratio of 0.2/0.1/0.7, respectively. All models are trained with a fixed 200 epochs, using an early stopping strategy when the performance on the validation set is not improved for 50 consecutive epochs. All trainable parameters of the neural network are initialized through Xavier (Glorot and Bengio 2010) and optimized using Adam (Kingma and Ba 2014) with the learning rate 15e-4. We use both f1micro and f1-macro as the evaluation metrics.The code of baseline models are available at https://github.com/BUPTGAMMA/OpenHGNN."
        },
        {
            "heading": "Performance Comparison and Analysis",
            "text": "Effectiveness of representation learning. To answer Q1, we evaluate the performance of our proposed model comparing with other baselines and record the experimental results in Table 1. It can be clearly observed that our model achieves the best result on all datasets against both homogeneous and heterogeneous representation learning methods. For traditional homogeneous graph learning models, GraphSAGE generally achieves the best result even when learning the node representations on heterogeneous graphs. Interestingly, other homogeneous graph learning models also obtain promising results compared with heterogeneous graph learning models such as HAN and HGT. Regarding the state-ofthe-art heterogeneous graph learning models, R-HGNN reports the second-highest score for both F1-micro and F1macro, implying the significance of integrating semantic information on different edge types. Furthermore, our proposed model shows a better performance than attentionbased R-HGNN, which attributes to incorporating the structural heterogeneity of the graph by combing both relationspecific pooling and cross-relation convolution from neigh-\nbor nodes simultaneously. Apart from that, our model obtains a significant improvement on both metrics compared with the meta-paths dependent model HAN. We also visualize the embedding result of PC-HGN for a more intuitive view. Specifically, we use Principal Component Analysis (PCA) (Abdi and Williams 2010) to project the learnt node embeddings of paper of ACM into a 2-dimensional space. As shown in Fig. 2, we can see a clear division among different paper classes, implying the paper nodes has been represented properly.\nAnalysis of cross-relation convolutions. We investigate the contribution of cross-relation convolutions in this part (RQ2). Specifically, we conduct experiments to remove the filter-based convolutions and replace them with two steps: 1)\nmean operation on the sampled neighbors embeddings X\u0303 t Nv to output the hidden embedding under relation t; 2) stacking hidden embeddings from different relations as the input to the MLP. Table 2 provides the comparison results of PCHGN and its varient. It can be seen a significant improvement on performance when adopting cross-relation convolutions, leading to a margin by 11.4% on DBLP, 11.6% on ACM and 7.1% on IMDB, respectively.\nImpact of node embedding size. To answer RQ3, we test the impact of different node embedding sizes on the performance. As shown in Fig. 3, it can be observed clearly that the performance of the model increases steadily at first and then starts to drop, reporting the best result with the embedding size 256. Intuitively, larger dimension sizes bring in extra redundancies in the kernel space, thus causing unexpected performance drops after convolutions."
        },
        {
            "heading": "Conclusion",
            "text": "In this paper, we propose a novel relation-centric heterogeneous graph learning method called PC-HGN, aiming to better incorporate the information of multi-relation types without concerning pre-defined meta-paths in heterogeneous graphs. The proposed PC-HGN designs an adaptive importance-based pooling to enable sampling for each relation type, followed by the cross-relation convolutions to incorporate the structural heterogeneity of graphs into the embedding space. The experiment results show the effectiveness of PC-HGN comparing with other baselines. Essentially, this research sheds light on the way to heterogeneous graph learning in addition to meta-path or attention-driven models."
        }
    ],
    "title": "Towards Relation-centered Pooling and Convolution for Heterogeneous Graph Learning Networks",
    "year": 2022
}