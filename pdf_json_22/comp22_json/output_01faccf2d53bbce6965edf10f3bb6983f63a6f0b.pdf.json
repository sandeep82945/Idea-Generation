{
    "abstractText": "In this paper, we propose a data-driven approach to derive explicit predictive control laws, without requiring any intermediate identification step. The keystone of the presented strategy is the exploitation of available priors on the control law, coming from model-based analysis. Specifically, by leveraging on the knowledge that the optimal predictive controller is expressed as a piecewise affine (PWA) law, we directly optimize the parameters of such an analytical controller from data, instead of running an on-line optimization problem. As the proposed method allows us to automatically retrieve also a model of the closed-loop system, we show that we can apply model-based techniques to perform a stability check prior to the controller deployment. The effectiveness of the proposed strategy is assessed on two benchmark simulation examples, through which we also discuss the use of regularization and its combination with averaging techniques to handle the presence of noise.",
    "authors": [
        {
            "affiliations": [],
            "name": "Valentina Breschi"
        },
        {
            "affiliations": [],
            "name": "Andrea Sassella"
        },
        {
            "affiliations": [],
            "name": "Simone Formentin"
        }
    ],
    "id": "SP:46a59a6571b1d775abafe6c4930a36d71fc6e45c",
    "references": [
        {
            "authors": [
                "A. Bemporad"
            ],
            "title": "Hybrid Toolbox ",
            "venue": "User\u2019s Guide,",
            "year": 2004
        },
        {
            "authors": [
                "A. Bemporad",
                "K. Fukuda",
                "F.D. Torrisi"
            ],
            "title": "Convexity recognition of the union of polyhedra",
            "venue": "Computational Geometry,",
            "year": 2001
        },
        {
            "authors": [
                "A. Bemporad",
                "M. Morari",
                "V. Dua",
                "E.N. Pistikopoulos"
            ],
            "title": "The explicit linear quadratic regulator for constrained systems",
            "year": 2002
        },
        {
            "authors": [
                "J. Berberich",
                "J. K\u00f6hler",
                "M.A. M\u00fcller",
                "F. Allg\u00f6wer"
            ],
            "title": "Data-driven tracking MPC for changing setpoints",
            "venue": "IFAC- PapersOnLine,",
            "year": 2020
        },
        {
            "authors": [
                "J. Berberich",
                "J. K\u00f6hler",
                "M.A. M\u00fcller",
                "F. Allg\u00f6wer"
            ],
            "title": "Datadriven model predictive control with stability and robustness guarantees",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2021
        },
        {
            "authors": [
                "J. Berberich",
                "J. K\u00f6hler",
                "M.A. M\u00fcller",
                "F. Allg\u00f6wer"
            ],
            "title": "Linear tracking MPC for nonlinear systems part II: The data-driven case",
            "year": 2021
        },
        {
            "authors": [
                "F. Borrelli",
                "M. Baoti\u0107",
                "J. Pekar",
                "G. Stewart"
            ],
            "title": "On the complexity of explicit mpc laws",
            "venue": "European Control Conference (ECC),",
            "year": 2009
        },
        {
            "authors": [
                "V. Breschi",
                "A. Sassella",
                "S. Formentin"
            ],
            "title": "On the design of regularized explicit predictive controllers from input-output data",
            "venue": "In arXiv/2110.11808,",
            "year": 2021
        },
        {
            "authors": [
                "A. Chiuso",
                "G. Pillonetto"
            ],
            "title": "System identification: A machine learning perspective",
            "venue": "Annual Review of Control, Robotics, and Autonomous Systems,",
            "year": 2019
        },
        {
            "authors": [
                "J. Coulson",
                "J. Lygeros",
                "F. D\u00f6rfler"
            ],
            "title": "Data-enabled predictive control: In the shallows of the DeePC",
            "venue": "In 2019 18th European Control Conference (ECC),",
            "year": 2019
        },
        {
            "authors": [
                "J. Coulson",
                "J. Lygeros",
                "F. D\u00f6rfler"
            ],
            "title": "Regularized and distributionally robust data-enabled predictive control",
            "venue": "IEEE 58th Conference on Decision and Control (CDC),",
            "year": 2019
        },
        {
            "authors": [
                "C. De Persis",
                "P. Tesi"
            ],
            "title": "Formulas for data-driven control: Stabilization, optimality, and robustness",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2019
        },
        {
            "authors": [
                "F. D\u00f6rfler",
                "J. Coulson",
                "I. Markovsky"
            ],
            "title": "Bridging direct & indirect data-driven control formulations via regularizations and relaxations",
            "venue": "In arXiv/2101.01273,",
            "year": 2021
        },
        {
            "authors": [
                "T. Faulwasser",
                "M.A. M\u00fcller",
                "K. Worthmann"
            ],
            "title": "Recent advances in model predictive control: Theory, algorithms, and applications",
            "year": 2021
        },
        {
            "authors": [
                "S. Formentin",
                "M. Lovera"
            ],
            "title": "Flatness-based control of a quadrotor helicopter via feedforward linearization",
            "venue": "In 2011 50th IEEE Conference on Decision and Control and European Control Conference,",
            "year": 2011
        },
        {
            "authors": [
                "M. Grant",
                "S. Boyd"
            ],
            "title": "Graph implementations for nonsmooth convex programs",
            "venue": "Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences,",
            "year": 2008
        },
        {
            "authors": [
                "M. Grant",
                "S. Boyd"
            ],
            "title": "CVX: Matlab software for disciplined convex programming, version 2.1",
            "year": 2014
        },
        {
            "authors": [
                "T. Hastie",
                "R. Tibshirani",
                "J.H. Friedman"
            ],
            "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
            "year": 2009
        },
        {
            "authors": [
                "D. Hrovat",
                "S. Di Cairano",
                "H.E. Tseng",
                "I.V. Kolmanovsky"
            ],
            "title": "The development of model predictive control in automotive industry: A survey",
            "venue": "IEEE International Conference on Control Applications,",
            "year": 2012
        },
        {
            "authors": [
                "L. Ljung"
            ],
            "title": "Perspectives on system identification",
            "venue": "Annual Reviews in Control,",
            "year": 2010
        },
        {
            "authors": [
                "D.Q. Mayne"
            ],
            "title": "Model predictive control: Recent developments and future",
            "venue": "promise. Automatica,",
            "year": 2014
        },
        {
            "authors": [
                "D. Mignone",
                "G. Ferrari-Trecate",
                "M. Morari"
            ],
            "title": "Stability and stabilization of piecewise affine and hybrid systems: an lmi approach",
            "venue": "In Proceedings of the 39th IEEE Conference on Decision and Control (Cat. No.00CH37187),",
            "year": 2000
        },
        {
            "authors": [
                "T.A.S.J. Qin"
            ],
            "title": "Badgwell. A survey of industrial model predictive control technology",
            "venue": "Control Engineering Practice,",
            "year": 2003
        },
        {
            "authors": [
                "J.B. Rawlings"
            ],
            "title": "Tutorial overview of model predictive control",
            "venue": "IEEE control systems magazine,",
            "year": 2000
        },
        {
            "authors": [
                "A. Sassella",
                "V. Breschi",
                "S. Formentin"
            ],
            "title": "Learning explicit predictive controllers: theory and applications",
            "venue": "In arXiv/2108.08412,",
            "year": 2021
        },
        {
            "authors": [
                "J.C. Willems",
                "P. Rapisarda",
                "I. Markovsky",
                "B.L.M. De Moor"
            ],
            "title": "A note on persistency of excitation",
            "venue": "Systems & Control Letters,",
            "year": 2005
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 7.\n01 14\n8v 1\n[ ee\nss .S\nY ]\n4 J\nul 2\nIn this paper, we propose a data-driven approach to derive explicit predictive control laws, without requiring any intermediate identification step. The keystone of the presented strategy is the exploitation of available priors on the control law, coming from model-based analysis. Specifically, by leveraging on the knowledge that the optimal predictive controller is expressed as a piecewise affine (PWA) law, we directly optimize the parameters of such an analytical controller from data, instead of running an on-line optimization problem. As the proposed method allows us to automatically retrieve also a model of the closed-loop system, we show that we can apply model-based techniques to perform a stability check prior to the controller deployment. The effectiveness of the proposed strategy is assessed on two benchmark simulation examples, through which we also discuss the use of regularization and its combination with averaging techniques to handle the presence of noise.\nKey words: Data-driven control; learning-based control; predictive control; explicit predictive control"
        },
        {
            "heading": "1 Introduction",
            "text": "For its capability of handling constraints, Model predictive control (MPC) is a widely employed technique for advanced control applications (see, e.g., [14, 19, 21, 23, 24]). Due to the increasing complexity of the systems to be controlled, the model required by MPC is often no longer parameterized from the physics but learned from data in a black-box fashion. However, this identification step generally takes the lion\u2019s share of the time and effort required for control design. In the last years, research has thus benched into two main direction. On the one side, efforts has been focused on improving and easing learning procedures [9, 20]. On the other side, many approaches have been proposed to directly employ data for the design of predictive controllers, while bypassing any model identification step. Among existing works in this direction, we recall the foundational contributions of [5, 10], which have been extended to handle tracking problems [4], to deal with nonlinear sys-\n\u22c6 This project was partially supported by the Italian Ministry of University and Research under the PRIN\u201917 project \u201cData-driven learning of constrained control systems\u201d, contract no. 2017J89ARP.\nEmail addresses: valentina.breschi@polimi.it (Valentina Breschi), andrea.sassella@polimi.it (Andrea Sassella), simone.formentin@polimi.it (Simone Formentin).\ntems [6], and to improve the performance in the presence of noise [11, 13], just to mention a few. For both traditional and data-driven predictive controls, the computational effort required to solve the related constrained optimization problem is known to be a potential limit for its application, especially for fast-sampling systems. Nonetheless, when the MPC problem is relatively small, i.e., one has to control a low order system and/or the prediction horizon is relatively short, this limitation can be overcome by explicitly deriving its solution [3]. In fact, when the cost of the optimization problem is quadratic and the constraints are linear, the explicit solution of MPC is known to be a Piece-Wise Affine (PWA) state feedback law.\nIn this work, we propose an approach to directly learn explicit predictive control laws from data. More specifically, we initially build on the foundational results in [12, 26] and on model-based priors (namely, the aforementioned fact that the optimal solution of linearMPC is PWA [3]), to construct a data-driven, multi-step closed-loop predictor. The latter is exploited to construct a fully databased optimization problem, yielding the estimation of the parameters of the optimal predictive control law. As a by-product, we obtain a data-driven description of the closed-loop behavior, which can be used in combination with existing model-based techniques (see, e.g., [22]) to check the stability of the system controlled with the data-driven explicit controller, prior to its deployment.\nPreprint submitted to Automatica 5 July 2022\nAs far as we are aware, this is the first time a data-driven predictive control strategy is provided together with a preliminary assessment of its closed-loop performance.\nWe should mention here that an early attempt to obtain a data-driven counterpart of explicit MPC was already carried out in [25]. However, the strategy proposed therein relies on an implicit open-loop identification step. Another method was presented in [8], by relying on the behavioral predictor used in [5, 11]. Even though that method involves no open-loop identification phase and it does not require the state to be fully measurable, the latter does not leverage on priors coming from model-based analysis and design. Nonetheless, by leveraging on priors, we are here able to retrieve a data-based characterization of the closed-loop and, thus, practically assess its stability in a data-driven fashion prior to its deployment. This is instead not possible in [8], where stability cannot be directly assessed nor it is theoretically guaranteed in presence of noisy data.\nThe remainder of the paper is organized as follows. The targeted problem is formalized in Section 2, while all the steps required to obtain its data-driven formulation from priors are introduced in Section 3. The explicit control law is derived in Section 4, where we additional discuss practical aspects for its implementation, certified deployment and noise handling. The effectiveness of the proposed strategy is assessed on two benchmark examples in Section 5. Section 6 concludes the work and indicates some directions for future research.\nNotation We denote with N0 the set of natural numbers, that includes zero. Let R, Rn and Rn\u00d7m be the set of real numbers, column vectors of dimension n and n \u00d7 m dimensional real matrices, respectively. Given B \u2208 Rm\u00d7n, its transpose is B\u22a4, its Moore-Penrose inverse is B\u2020 and, when m = n its inverse is indicated as B\u22121. Given a vector v \u2208 Rn, [v]i:j indicates its rows from i to j, with i \u2264 j \u2264 n. For a matrix B \u2208 Rn\u00d7m, [B]1:i,1:j denotes a sub-matrix comprising the first i rows and j columns of B, with i \u2264 n and j \u2264 m. Identity matrices are denotes as I, while zero matrices and vectors will be denoted as 0. If a matrix Q \u2208 Rn\u00d7n is positive definite (positive semi-definite), this is denoted as Q \u227b 0 (Q 0). Given a vector x \u2208 Rn, the quadratic form x\u2032Qx is compactly indicated as \u2016x\u20162Q. Given a signal {\u03bdt \u2208 R m}t\u2208N0 and 0 < L < T , we denote with N0,L,T\u22121 \u2208 R mL\u00d7T\u2212L+1 the associated Hankel matrix\nN0,L,T\u22121 =\n\n     \n\u03bd(0) \u03bd(1) \u00b7 \u00b7 \u00b7 \u03bd(T \u2212 L)\n\u03bd(1) \u03bd(2) \u00b7 \u00b7 \u00b7 \u03bd(T \u2212 L+ 1) ... ... . . . ...\n\u03bd(L) \u03bd(L + 1) \u00b7 \u00b7 \u00b7 \u03bd(T \u2212 1)\n\n      , (1)\nwhile, for i, j \u2208 N0, we introduce\nNi,T\u2212j = [ \u03bd(i) \u03bd(i+ 1) \u00b7 \u00b7 \u00b7 \u03bd(T \u2212 j) ] , i < T \u2212 j. (2)"
        },
        {
            "heading": "2 Problem formulation",
            "text": "Consider the class S of discrete-time linear, time invariant (LTI), controllable systems with fully measurable state, i.e.,\nS :\n{ x(t + 1) = Ax(t) +Bu(t),\nyo(t) = x(t), (3)\nwhere x(t) \u2208 Rn denotes the state of S at time t \u2208 N0, u(t) \u2208 R m is an exogenous input and yo(t) \u2208 Rn is the associated noiseless output. Let us consider the following model predictive control (MPC) problem:\nminimize {u\u0303(k)}L\u22121\nk=0\nL\u22121\u2211\nk=0\n[ \u2016x\u0303(k)\u20162Q+\u2016u\u0303(k)\u2016 2 R ] +\u2016x\u0303(L)\u20162P (4a)\ns.t. x\u0303(k+1)=Ax\u0303(k)+Bu\u0303(k), k=0, . . . , L\u22121, (4b)\nCxx\u0303(k) + Cuu\u0303(k) \u2264 d, k=0, . . . , L\u22121, (4c)\nx\u0303(0) = x(t). (4d)\nThe objective of (4) is to steer both the (predicted) state x\u0303(k) and the input u\u0303(k) to zero, over a prediction horizon of prefixed length L > 0. Optimality is indeed dictated by: (i) the distance of the predicted state from zero, penalized with Q 0 over the whole horizon except for the terminal state (weighted via P 0), and (ii) the control effort, penalized via R \u227b 0. Meanwhile, a set of nc polyhedral constraints dictated by (4c) has to be satisfied, with Cx \u2208 R nc\u00d7n, Cu \u2208 R nc\u00d7m and d \u2208 Rnc , while relying on the latest information on the system (see (4d)). Assume also that the matrices A \u2208 Rn\u00d7n and B \u2208 Rn\u00d7m characterizing the dynamics of S are unknown, and that we can access a set of input/output data pairs DT = {UT ,YT }, where UT and YT denote the available input and output sequences, respectively, satisfying the following assumptions.\nAssumption 1 (Persistently exciting inputs) The input sequence UT = {u(t)} T t=0 is persistently exciting of order n+ L.\nAssumption 2 (Noisy outputs) The output sequence YT = {y(t)} T t=0 is corrupted by noise, namely\ny(t) = yo(t) + w(t), (5)\nwhere v(t) is the realization of a zero mean white noise with covariance \u2126 \u2208 Rn\u00d7n.\nAssumption 3 (Sufficiently long dataset) The length T of the dataset DT satisfies the following:\nT \u2265 (m+ 1)(n+ L)\u2212 1.\nThe goal of this work is to directly exploit the available data to find an explicit solution to (4), under Assumptions 1-3, while bypassing any identification step."
        },
        {
            "heading": "3 Exploiting priors for explicit DDPC",
            "text": "To attain our goal, it is fundamental to replace the model in (4b) with an expression that directly depends on the available data, by also not forgetting what we have learned from model-based predictive control. To start with, we thus recall the following lemma, explicitly stating the form of the optimal explicit predictive controller in our setting [3].\nLemma 1 (On the solution of (4)) Let U denote the vector stacking the inputs over the prediction horizon, i.e.,\nU = U(x(t)) =\n\n     \nu\u0303(0)\nu\u0303(1) ...\nu\u0303(L\u2212 1)\n\n      \u2208 RmL. (6)\nThe optimal control sequence U\u22c6 = U\u22c6(x(t)) solving (4) is a continuous piecewise affine (PWA) function of x(t).\nThen, we generalize the one-step ahead predictor introduced in [12] to the multi-step case. To this end, it is important to recall that, thanks to Assumptions 1, 3 and the Fundamental Lemma [26], the following rank condition holds:\nrank\n\n\n\n X0,T\u2212L\u22121\nU0,L,T\u2212L\u22121\n\n\n\n = n+mL. (7)\nWe can now derive the multi-step data-based predictor as follows.\nTheorem 1 (Data-based multi-step predictor) Let Assumptions 1 and 3 hold. Given x(t), the sequence\nof predicted states X\u0303 = X\u0303(x(t)) admits the following data-based representation:\nX\u0303 =\n\n    x\u0303(1) ...\nx\u0303(L)\n\n    = X1,L,T\u2212L\n\n X0,T\u2212L\u22121\nU0,L,T\u2212L\u22121\n\n\n\u2020 [\nx(t)\nU\n]\n. (8)\nProof: The proof follows the steps of the one in [12, Appendix B]. Specifically, let v \u2208 Rn+mL and S \u2208 Rn+mL\u00d7T be respectively defined as:\nv :=\n[\nx(t)\nU\n]\n, S :=\n\n X0,T\u2212L\u22121\nU0,L,T\u2212L\u22121\n\n ,\nwith S being full row rank, i.e., rank(S) = n+mL. By the Rouche\u0301-Capelli theorem, for any given v, the equality\nv = S\u03b1,\nadmits infinite solutions \u03b1 of the form\n\u03b1 = S\u2020v +\u03a0\u22a5Sw, \u2200w \u2208 R T , (9)\nwith \u03a0\u22a5S = (I \u2212 S \u2020S) being the orthogonal projector onto the kernel of S. Meanwhile, based on the model in (4b), the predicted state sequence can be defined as a function of x(t) and U as:\nX\u0303 =\n\n   \nA\n...\nAL\u22121\n\n   \n\ufe38 \ufe37\ufe37 \ufe38\n\u03be\nx(t) +\n\n     \nB 0 \u00b7 \u00b7 \u00b7 0\nAB B \u00b7 \u00b7 \u00b7 0 ... ... . . . ...\nAL\u22121B AL\u22122B \u00b7 \u00b7 \u00b7 B\n\n     \n\ufe38 \ufe37\ufe37 \ufe38\n\u0393\nU. (10)\nIn turn, such a sequence can be recast as a function of \u03b1, i.e.,\nX\u0303 = [ \u03be \u0393 ] S\u03b1 = X1,L,T\u2212L\u03b1.\nwhere the second equality straightforwardly follows from (10) and the definition of S. By replacing \u03b1 with (9), we then obtain\nX\u0303 = X1,L,T\u2212L ( S\u2020v +\u03a0\u22a5Sw ) = X1,L,T\u2212LS \u2020v,\nas X1,L,T\u2212L\u03a0 \u22a5 S =\n[ \u03be \u0393 ] S\u03a0\u22a5S = 0, based on the defini-\ntion of the projector.\nThis preliminary result allows us to exploit priors on the solution of (4) for the definition of the DDPC problem. In fact, according to Lemma 1, we can parameterize the control sequence U as:\nU=\n \n\nK1x(t)+f1, if H1x(t) \u2264 \u21131, ...\nKMx(t)+fM , if HMx(t) \u2264 \u2113M ,\n(11)\nwhere Ki \u2208 R mL\u00d7n and fi \u2208 R mL are the (unknown) feedback and affine gains characterizing the control law, {Hi, \u2113i} M i=1 dictates the associated polyhedral partition,\nfor i = 1, . . . ,M , and the amounts of modes M is dictated by the number of possible combinations of active constraints. Therefore, for a given state x(t), the input sequence is the affine function\nU(x(t)) = Kx(t) + f, (12)\nwith K = Ks(x(t)) and f = fs(x(t)) denoting the gains associated to the active control law, and with\ns(x(t)) = i \u21d0\u21d2 Hix(t) \u2264 \u2113i, i \u2208 {1, . . . ,M}.\nBased on this parameterization, we can compute a databased closed-loop characterization of the predictor in (4b), as outlined in the following theorem.\nTheorem 2 (Closed-loop multi-step predictor) Let Assumptions 1 and 3 hold. Given x(t), the sequence\nof predicted states X\u0303 = X\u0303(x(t)) can be equivalently expressed as:\nX\u0303 = X1,L,T\u2212L (GKx(t) +Gf ) , (13a)\nwith GK \u2208 R T\u2212L\u00d7n and Gf \u2208 R T\u2212L satisfying:\n[\nI\nK\n]\n=\n\n X0,T\u2212L\u22121\nU0,L,T\u2212L\u22121\n\nGK (13b)\n[\n0\nf\n]\n=\n\n X0,T\u2212L\u22121\nU0,L,T\u2212L\u22121\n\nGf (13c)\nwhere K and f characterize the local control law (12). Accordingly, the input sequence is given by:\nU = U0,L,T\u2212L\u22121(GKx(t) +Gf ). (14)\nProof: For the Rouche\u0301-Capelli theorem, there exists an T \u00d7 n matrix Gk and a T -dimensional vector Gf such that (13b)-(13c) hold.Meanwhile, replacing (12) into the open-loop predictor in (8), the predicted state sequence can be represented as:\nX\u0303 = X1,L,T\u2212L\n\n X0,T\u2212L\u22121\nU0,L,T\u2212L\u22121\n\n\n\u2020([\nI\nK\n]\nx(t) +\n[\n0\nf\n])\n.\nBy combining these two results, the closed-loop representation in (13) and the equivalent definition of the input sequence in (14) straightforwardly follow.\nBy leveraging on (13)-(14), we can now equivalently recast the predictive control task in (4) as an optimization\nproblem with the closed-loop matricesGK , Gf being the decision variables, as follows 1 :\nminimize GK ,Gf\nX\u0303\u22a4QX\u0303+U\u22a4RU (15a)\ns.t. X\u0303 = X1,L,T\u2212L (GKx(t) +Gf ) (15b)\nU = U0,L,T\u2212L\u22121(GKx(t) +Gf ), (15c) [\nCx 0\n0 Cx\n][\nx(t)\nX\u0303\n]\n+ CuU \u2264 D, (15d)\nX0,T\u2212L\u22121GKx(t) = x(t), (15e)\nX0,T\u2212L\u22121Gf = 0, (15f)\nIn (15), Q=diag ([Q, \u00b7 \u00b7 \u00b7 , Q, P ]), R=diag ([R, \u00b7 \u00b7 \u00b7 , R]), Cu=diag ([Cu, \u00b7 \u00b7 \u00b7 , Cu]) and\nCx=\n\n     \nCx 0 \u00b7 \u00b7 \u00b7 0 0\n0 Cx \u00b7 \u00b7 \u00b7 0 0 ... ... . . . ... ...\n0 0 \u00b7 \u00b7 \u00b7 Cx 0\n\n      , D =\n\n   \nd\n...\nd\n\n    .\nNote that, the last constraints (see (15e)-(15f)) are introduced for the problem to be consistent with the closedloop representation in (13).\nThis shift from an open-loop predictor to its closed-loop counterpart allows us to directly learn the control law from data, and avoid any system identification step.\nRemark 1 Both the equivalences in (8) and (13) exactly hold in a noiseless setting only. As such, problem (15) is equivalent to (4) only when the available batch of data is noise-free."
        },
        {
            "heading": "4 Learning Explicit DDPC",
            "text": "To derive its explicit solution, the problem in (15) is manipulated to obtain a multi-parametric Quadratic Program (mp-QP). As a preliminary step, we condense the unknowns of (15) into a single variable:\nG(t) =\n[\nGKx(t)\nGf\n]\n\u2208 R2(T\u2212L). (16)\n1 Notice that the term in the cost that depends only on x(t) has been neglected. This can be done without loss of generality, as the optimal solution does not change.\nAccordingly, we can recast (15) as the following mp-QP\nminimize G(t)\n(G(t))\u22a4HdG(t) (17a)\ns.t. \u039edG(t) + \u03a8x(t) \u2264 D, (17b)\n\u0398dG(t) =\n[\nx(t)\n0\n]\n, (17c)\nwhere\nHd = X \u22a4QX + V\u22a4RV , (18a)\n\u039ed =\n[\nCx 0\n0 Cx\n] [\n0\nX\n]\n+ CuV , \u03a8=\n[\nCx 0\n0 Cx\n][\nI\n0\n]\n, (18b)\n\u0398d=diag ([X0,T\u2212L\u22121, X0,T\u2212L\u22121]) , , (18c)\nand\nX = [\nX1,L,T\u2212L X1,L,T\u2212L\n]\n,\nV= [\nU0,L,T\u2212L\u22121 U0,L,T\u2212L\u22121\n]\n.\nBy focusing on Hd in (17a), it can be proven that this weighting matrix satisfies the following lemma.\nLemma 2 (Features of Hd) Under Assumptions 1 and 3, Hd is positive semi-definite.\nProof: This is a direct consequence of Assumptions 1 and 3, for which V\u22a4 \u2208 R2(T\u2212L)\u00d7mL is not full row rank.\nAs the cost should be strictly convex for a unique explicit solution to be retrieved, this feature of Hd prevents us from deriving the explicit law. To overcome this limitation, we introduce a regularization term in the cost of (17), thus replacing the weight Hd with:\nH\u03b3d = 1\n2 (Hd + \u03b3I) , (19)\nwhere \u03b3 > 0 is an hyper-parameter to be tuned 2 . The data-driven control problem then corresponds to the regularized mp-QP:\nminimize G(t)\n(G(t))\u22a4H\u03b3dG(t) (20a)\ns.t. \u039edG(t) + \u03a8x(t) \u2264 D, (20b)\n\u0398dG(t) =\n[\nx(t)\n0\n]\n. (20c)\n2 The cost has been normalized to ease the subsequent derivations."
        },
        {
            "heading": "4.1 Derivation of the explicit DDPC law",
            "text": "The introduction of the regularizer in (20) allows us to derive the explicit DDPC law through the manipulation of the Karush-Kuhn-Tucker (KKT) conditions associated with the new DDPC problem. To ease the computations, let us consider the following further assumption.\nAssumption 4 (Non-degenerate constraints) The active constraints of (20) are linearly independent.\nBased on this assumption, we now follow the same steps used to derive the explicit model-based predictive control law in [3].\nThe KKT conditions for the regularized DDPC problem in (20) are:\nH\u03b3dG(t) + \u039e \u22a4 d \u03bb+\u0398 \u22a4 d \u00b5 = 0, (21a) \u03bb\u22a4(\u039edG(t) + \u03a8x(t)\u2212D) = 0, (21b)\n\u03bb \u2265 0, (21c)\n\u039edG(t) + \u03a8x(t)\u2212D \u2264 0, (21d)\n\u0398dG(t)\u2212\n[\nx(t)\n0\n]\n= 0, (21e)\nwhere \u03bb and \u00b5 are the Lagrange multipliers associated with inequality and equality constraints in (20b) and (20c), respectively. Let us focus on the i-th set of active constraints only, distinguishing between the Lagrange multipliers associated with a given active and inactive inequality constraints. We respectively denote them as \u03bb\u0303i and \u03bb\u0304i. It is straightforward to notice that the combination of (21b) and (21c) leads to the following condition on \u03bb\u0304i:\n\u03bb\u0304i = 0.\nBy merging (21b) and (21e) for the i-th set of active constraints, it is also straightforward to show that the optimal solution Gi(t) satisfies\n\u03a6d,iGi(t)\u2212 S\u0303ix(t) \u2212 W\u0303i = 0, (22)\nwhere\n\u03a6d,i =\n[\n\u039e\u0303d,i\n\u0398d\n]\n, S\u0303i =\n\n  \n\u2212\u03a8\u0303i\nI\n0\n\n   , W\u0303i =\n[\nD\u0303i\n0\n]\nand \u039e\u0303d,i, \u03a8\u0303i and D\u0303i are the rows of \u039ed, \u03a8 and D coupled with the considered set active constraints. By leveraging on (21a), we can now express our optimization variable Gi(t) as a function of the Lagrange multipliers, i.e.,\nGi(t) = \u2212(H \u03b3 d)\n\u22121 [\n\u039e\u0303\u22a4d,i \u0398 \u22a4 d\n]\n\ufe38 \ufe37\ufe37 \ufe38\n\u03a6\u22a4 d,i\n\u039b\u0303i, (23)\nwhere\n\u039b\u0303i =\n[\n\u03bb\u0303i\n\u00b5\n]\n.\nWe can now replace the latter into (22) to obtain an explicit expression of the Lagrangemultipliers as functions of the matrices characterizing (20):\n\u039b\u0303i = \u2212 [ \u03a6d,i (H \u03b3 d) \u22121 \u03a6\u22a4d,i ]\u22121\n\ufe38 \ufe37\ufe37 \ufe38\n\u03a5d,i\n(\nS\u0303ix(t)+W\u0303i\n)\n. (24)\nIn turn, this allows us to explicitly retrieve Gi(t) as:\nGi(t)=(H \u03b3 d) \u22121 \u03a6\u22a4d,i\u03a5d,i\n(\nS\u0303ix(t)+W\u0303i\n)\n, (25)\nand the associated optimal input sequence as\nUi(x(t))=V (H \u03b3 d) \u22121 \u03a6\u22a4d,i\u03a5d,i\n(\nS\u0303ix(t)+W\u0303i\n)\n. (26)\nThus, the input to be fed to the system when the i-th set of constraints is active is defined as:\nui(x(t))=[Ui(x(t))]1:m. (27)\nThrough (21c) and (21d), we can finally define the polyhedral region associated with the considered combination of active constraints, which is dictated by the following inequalities:\n\u03a5d,i\n(\nS\u0303ix(t)+W\u0303i\n)\n\u2264 0, (28a)\n\u039ed(H \u03b3 d) \u22121 \u03a6\u22a4d,i\u03a5d,i\n(\nS\u0303ix(t)+W\u0303i\n)\n+\u03a8x(t)\u2212D\u2264 0. (28b)\nThe complete data-driven expression for (11) is then straightforwardly obtained by following the above steps for all possible combinations of the active constraints. This operation ultimately yields an optimal input sequence U(x(t)) of the form:\nU(x(t)) =\n \n\nU1(x(t)), if Fd,1x(t) \u2264 Ed,1, ...\nUM (x(t)), if Fd,Mx(t) \u2264 Ed,M ,\n(29)\nwhereM is given by the number of possible combinations of active constraints, Ui corresponds to (26), for all i \u2208 {1, . . . ,M}, while {Fd,i, Ed,i} M i=1 can be easily obtained from (28). Consequently, the input to be fed to S starting from x(t) can be retrieved by evaluating the PWA law\nu(x(t)) =\n \n\nu1(x(t)), if Fd,1x(t) \u2264 Ed,1, ...\nuM (x(t)), if Fd,Mx(t) \u2264 Ed,M ,\n(30)\nwith ui(x(t)) given by (27), for i = 1, . . . ,M .\nAlgorithm 1 Offline construction of the explicit law\nInput: Dataset DT ; penalties Q,P 0; R \u227b 0; horizon N >0; constraint matrices Cx, Cu, d; regularization parameter \u03b3 > 0.\n1. Construct the data-based matrices X1,L,T\u2212L, U0,L,T\u2212L\u22121, X0,T\u2212L\u22121. 2. Build H\u03b3d , \u039ed, \u03a8, \u0398d in (18a) based on the cost and constraints of the DDPC problem. 3. Find all possible combinations of active constraints. 4. For each combination, isolate the matrices \u039e\u0303d, W\u0303 and S\u0303 characterizing (22)\n5. If not all rows of \u039e\u0303d are linearly independent, handle the degeneracy, e.g., as in [3]. 6. Find the PWA explicit law by retrieving (26)-(28) for all possible combinations of active constraints. 7. Merge polyhedral regions as in [2]. 8. Extract the first component of the optimal input\nsequence U(x(t)).\nOutput: Optimal input u(x(t)).\nRemark 2 (On Assumption 4) Although introduced to ease computations, we remark that Assumption ?? is not restrictive. Indeed, degenerate cases can be straightforwardly handled via existing approaches, e.g., see [3].\nRemark 3 (Data-driven and model-based) Within a noiseless setting, the results in Theorem 2 and the one-to-one correspondence between the chosen parameterization of the control law in (11) and its model-based counterpart guarantee the equivalence between (4) and (15). Therefore, when there is no noise, the data-driven explicit controller coincides with the E-MPC law as \u03b3 \u2192 0."
        },
        {
            "heading": "4.2 Implementing Explicit DDPC",
            "text": "Based on the available batch of data and the features of the considered predictive control problem, the explicit DDPC law can be completely retrieved offline from the available measurements, as summarized in Algorithm 1.\nGiven the data, one has to initially construct the Hankel matrices needed to build the DDPC problem (see steps 1-2). Once all the possible combinations of active constraints have been detected at step 3 and degenerate scenarios have been handled (see step 5), at step 6 the local controllers and the associated polyhedral regions are retrieved according to (26)-(28). Lastly, at step 7, the optimal control sequence is simplified, by merging polyhedral regions whenever possible. After this step, the explicit optimal input can simply be retrieved by extracting the first element of the input sequence U(x(t)) (see step 8).\nOnce the explicit DDPC law has been retrieved, the computation of the optimal input at each time instant simply\nconsists of a function evaluation. Specifically, one has to (i) search for the polyhedral region the current state x(t) belongs to, and (ii) apply the corresponding parametric law. We stress that this computational advantage is retained for simple control problems only (i.e., for short prediction horizon and small systems). Indeed, the complexity of the PWA law is known to rapidly increase [7] with the one of the DDPC problem to be solved, analogously to the model-based case."
        },
        {
            "heading": "4.3 Explicit data-driven predictive control and closedloop stability",
            "text": "When designing a controller in a data-driven setting, it is crucial to check the stability of the resulting closed-loop system before the controller deployment. Towards this objective, we now show how the peculiar features of the explicit data-driven predictive control can be leveraged in combination with existing techniques to devise an offline, data-driven stability test. To this end, let us assume that the i-th set of constraints is active and consider the following multi-step ahead closed-loop model:\nX\u0302(t)=X (H\u03b3d ) \u22121\u03a6\u22a4d,i\u03a5d,iS\u0303ix(t)+X (H \u03b3 d) \u22121\u03a6\u22a4d,i\u03a5d,iW\u0303i, (31) obtained by combining (13) with the result of our explicit\nderivation in (25), where X\u0302 stacks the state predicted by the learned closed-loop model, i.e.,\nX\u0302(t) =\n\n    x\u0302(t+ 1) ...\nx\u0302(t+ L)\n\n    .\nFrom (31), we can then isolate the learned one-step ahead closed-loop model, namely\nx\u0302(t+ 1) = Acld,ix(t) + f cl d,i, (32a)\nwhere\nAcld,i = [X (H \u03b3 d ) \u22121\u03a6\u22a4d,i\u03a5d,iS\u0303i]1:n,1:n, (32b) f cld,i = [X (H \u03b3 d) \u22121\u03a6\u22a4d,i\u03a5d,iW\u0303i]1:n. (32c)\nWhen performed for each mode i \u2208 {1, . . . ,M}, these manipulations allow us to retrieve the datadriven closed-loop transition matrix Acld,i for each i \u2208 {1, . . . ,M}. Retrieving these matrices ultimately enables us to apply model-based techniques, e.g., the ones presented in [22], to shed a light on the features of the final closed-loop. As an example, one can search for a matrix P \u2208 Rn\u00d7n satisfying the following sufficient conditions for asymptotic stability:\nP > 0 (33a)\n(Acld,i) \u22a4PAcld,i \u2212 P < 0, i = 1, . . . ,M, (33b)\nor, alternatively, look for the set of matrices {Pi \u2208 R\nn\u00d7n}Mi=1 verifying the following LMIs:\nPi > 0, i = 1, . . . ,M, (34a) (Acld,i) \u22a4PjA cl d,i \u2212 P < 0, i, j = 1, . . . ,M, (34b)\nwhich are also sufficient conditions for asymptotic stability.\nRemark 4 (On the tuning of P in (4a)) When S is known to be open-loop stable, the terminal weight P 0 in (4a) is generally selected as the solution of the Lyapunov equation\nP = A\u22a4PA+Q.\nThis equation can be directly translated into its datadriven counterpart by exploiting [12] as follows:\nP = AdPAd +Q, (35)\nwith\nAd = X1,T\n\n X0,T\u22121\nU0,1,T\u22121\n\n\n\u2020[\nI\n0\n]\n,\nthus providing a data-based approach for the selection of this parameter.\nRemark 5 (Hyper-parameter tuning) The possibility of performing an off-line data-based stability check on the data-driven explicit law can be useful to preliminarily assess the effects of different choices of the tuning parameters Q, R and P in (4a) and \u03b3 in (19), allowing one to discard the ones resulting in a failure of the data-driven stability tests."
        },
        {
            "heading": "4.4 Regularization and noise handling",
            "text": "As highlighted in Remark 1, all the equivalences we rely on to derive the explicit predictive control law are verified when DT is noiseless. However, in practice, \u2126 in Assumption 2 is generally a non-zero matrix.\nTo cope with noisy data, we follow the footsteps of [5,13] and propose to leverage on the regularization term introduced in (19). Indeed, as in standard ridge regression [18], this additional element of the cost steers all the components of G(t) towards small values. In turn, this potentially limits the impact of noise on the constraints in (20b)-(20c) and, thus, on the final explicit law. This shrinkage effect is modulated by the regularization parameter \u03b3, with the reduction in the magnitude of G(t) being stronger whenever large values of \u03b3 are considered. At the same time, \u03b3 implicitly changes the balance between the penalties in the original datadriven control problem in (15), with excessively high values of \u03b3 potentially driving the explicit data-driven controller far away from its model-based counterpart. The\nchoice of this hyper-parameters thus becomes an important tuning-knob of the approach, requiring one to tradeoff between handling noise and keeping explicit DDPC as close as possible to the implicit DDPC problem.\nAlthough the stability checks in (33)-(34) can be used to have a preliminary assessment on the effect of different choices of \u03b3, at the moment this balance can only be attained through closed-loop trials for several values of \u03b3. Such a procedure allows one to ultimately select the hyper-parameter that best fits one\u2019s needs, at the price of requiring closed-loop experiments that can be rather safety-critical in practice, especially when a simulator of the system is not available.\nWhenever multiple experiments can be performed by feeding the plant with the same input sequence UT , the burden associated to the choice of \u03b3 can be alleviated by exploiting the features of Assumption 2 itself. In this scenario, one can indeed replace DT with the averaged dataset D\u0304T = {UT , Y\u0304T }, where Y\u0304T = {y\u0304t} T t=0 and\ny\u0304t = 1\nN\nN\u2211\ni=1\ny (i) t , (36)\nwith y (i) t denoting the output of the i-th experiment. Since the noise is assumed to be zero mean, the law of large numbers asymptotically yields\ny\u0304t \u2212\u2192 N\u2192\u221e\nyot . (37)\nAs such, when the number N of experiments increases, the role of \u03b3 in handling noise is progressively less dominant. In this case, \u03b3 should then be used only to make the DDPC problem well-defined. Any small \u03b3 > 0 is acceptable for this purpose."
        },
        {
            "heading": "5 Numerical examples",
            "text": "The performance of the explicit predictive controller are now assessed on two benchmark examples: (i) the regulation to zero of the stable open-loop system of [3], for the case when the state is fully measurable; and (ii) the altitude control of a quadcopter. Since the last example features an open-loop unstable linearized plant, data are collected in closed-loop, by assuming that the drone is stabilized by an (unknown) pre-existing controller. In both the examples, the level of noise acting on the measured states is assessed through the averaged Signal-toNoise-Ratio (SNR):\nSNR= 1\nn\nn\u2211\ni=1\n10 log10\n(\u2211T t=0(xi(t)\u2212 wi(t)) 2\n\u2211T t=0(wi(t)) 2\n)\n, [dB]\n(38)\nwhere xi(t) and wi(t) denote the i-th components of the state and the measurement noise, respectively. All computations are carried out on an Intel Core i7-7700HQ processor, running MATLAB 2019b."
        },
        {
            "heading": "5.1 Open-loop stable benchmark system",
            "text": "Let us consider the benchmark system described by:\nS : x(t+1)=\n[\n0.7326 \u22120.0861\n0.1722 0.9909\n]\nx(t)+\n[\n0.0609\n0.0064\n]\nu(t).\n(39) Our goal is to regulate both components of the state to zero, while enforcing the following box-constraint on the\ninput: \u2212 2 \u2264 u(k) \u2264 2. (40)\nTowards this goal, we collect a set DT of T = 100 input/state pairs, by feeding S with an input sequence uniformly distributed within the interval [\u22125, 5]. According to Assumption 2, the measured states are corrupted by an additive zero-mean white noise sequence, with variance yielding SNR = 20 dB. The parameters characterizing the DDPC problem to be solved are selected as in [3], namely L = 2, Q = I, R = 0.01 and P is chosen as the solution of the data-driven Lyapunov in (35). By setting \u03b3 = 10, the partition associated with the explicit data-driven predictive controller 3 is the one shown in Figure 1, which approximately correspond to that reported in [3] 4 . Prior to the controller deployment, we have performed the data-based closed-loop stability check in (33), resulting in 5\nP =\n[\n24.8695 10.5595\n10.5595 43.2657\n]\n\u227b 0.\nThis indicates that the explicit law preserves the stability of the open-loop system. Figure 2 report the trajectories of the state and the optimal input obtained over a noise-free closed-loop tests with the explicit data-driven law, which confirm its effectiveness and validate the result of the data-driven stability check.\nWe now assess the sensitivity of the explicit controller to the choice of \u03b3 over 20 Monte-Carlo realizations of the batch datasets DT , for different noise levels. This evaluation is performed by looking at the cost of the controller over the same noiseless closed-loop test of length Tv = 50 considered previously, i.e.,\nJ =\nTv\u2211\nt=0\n\u2016x(t)\u20162Q + \u2016u(t)\u2016 2 R. (41)\nAs shown in Figure 3, the value of \u03b3 that leads to the minimum closed-loop cost tends to decrease when the noise level increases, supporting our considerations in Section 4.4. At the same time, by properly selecting \u03b3 we attain a cost J = 16.37\u00b1 0.58, which is generally close to the oracle J O = 15.77, which is the one achieved by the oracle law, i.e., the model-based predictive controller obtained by exploiting the true model of S. These results additionally show that, for increasing levels of noise, the choice of \u03b3 becomes more challenging, since the range of values leading to the minimum J progressively shrinks. Note that, when \u03b3 is excessively small the optimal input is always zero and S evolves freely 6 ."
        },
        {
            "heading": "3 The partition is plotted thanks to the Hybrid Toolbox [1].",
            "text": ""
        },
        {
            "heading": "4 The negligible differences with respect to the model-based partition are due to the noise on the batch data.",
            "text": ""
        },
        {
            "heading": "5 The LMIs in (33) are solved with CVX [16,17].",
            "text": "6 This behavior is also observed for \u03b3\u226410\u22126, \u03b3\u226410\u22124 and\nWe additionally evaluate the effect of averaging, by looking at the performance index J in (41) over 30 MonteCarlo data-collections for an increasing number N of repeated experiments of length T = 100. The measurements are affected by noise, yielding SNR = 20 dB. Figure 4 shows that the use of the averaged dataset D\u0304T has a similar effect to a reduction of the noise level. Indeed, for increasingN the optimal \u03b3 slowly shifts towards smaller values, thus further implying the gradual reduction in the impact of \u03b3 on noise handling."
        },
        {
            "heading": "5.2 Altitude control of a quadcopter",
            "text": "As a final case study, we consider a nonlinear system, namely the problem of controlling the altitude of a quadcopter, to perform landing or take-offmaneuvers. To this end, we exploit the same simulator used in [15] to collect the data and to carry out the closed-loop experiments with the learned explicit law. Let z(t) [m] be the altitude of the quadcopter, vz(t) [m/s] be its vertical velocity and (\u03b8(t), \u03c6(t), \u03c8(t)) [deg] its roll, pitch and yaw angles at time t. Both the the altitude z(t) [m] and the vertical velocity vz(t) are assumed to be measured, with the measurement being corrupted by a zero-mean white noise, resulting in SNR = 30 dB over these two outputs. As this system is open-loop unstable, the data collection phase is carried out in closed-loop for 20 [s] at a sampling rate of 40 [Hz], by using the four proportional derivative (PD) controllers introduced in [15]. The altitude set point used at this stage is generated uniformly at random in the interval [0, 4] [m]. The set points for all the attitude angles are instead selected as slowly variable random signals within the interval [\u22120.2, 0.2] [rad]. These choices yield a datasetDT of length T = 800, that satisfies Assumption 1 and allows us to retain information on possible non-zero angular configurations.\nThe three attitude controllers introduced in [15] are further retained in testing to keep the attitude angles at zero and to decouple the altitude dynamics from that of the other state variables. Within this setting, the explicit data-driven law is designed by imposing L = 5, Q = P = diag([1, 0.1]), R = 10\u22125 and \u03b3 = 1. To mitigate the effect of the gravitational force, the design and closed-loop deployment of the designed explicit controller are carried out by pre-compensating it. As a result, the input to be optimized is\nu(t) = uz(t)\nm \u2212 g, (42)\nwhere m = 0.5 [kg] is the mass of the quadcopter, g = 9.81 [m/s] is the gravitational acceleration and uz(t) is the input prior to the compensation. A similar approach is adopted for the control problem to fit our framework\n\u03b3\u226410\u22123 when SNR=40 dB, SNR=20 dB and SNR=10 dB, respectively.\nin both landing and take-off scenarios. We thus consider\nthe reduced state\nx(t) =\n[\nz(t)\u2212 z\u0304\nvz(t)\n]\n, (43)\nwhere z\u0304 [m] is the altitude set point. To avoid potential crashes of the quadcopter, in designing the explicit law we impose the following constraint on the state of the system:\nx1(t) \u2265 \u2212z\u0304, (44)\nwhich, in turn, guarantees the altitude to be always nonnegative. Meanwhile, the pre-compensated input is constrained to the interval:\n\u2212 9.81 \u2264 u(t) \u2264 9.564, (45)\nwhere the lower bound corresponds to a null input and the upper limit is dictated by the maximum power of the motors 7 .\nThe performance of the learned explicit law attained in take-off and landing are reported in Figure 5. Here we consider closed-loop tests in which the altitude and the"
        },
        {
            "heading": "7 The reader is referred to [15] for additional details on the",
            "text": "system.\nvertical velocity are noisy, with the noise acting on the closed-loop measurements sharing the features of that corrupting the batch ones. Despite the noise acting on the initial condition at each step, both maneuvers are successfully performed, thus showing the effectiveness of the retrieved explicit data-driven laws."
        },
        {
            "heading": "6 Conclusions",
            "text": "By leveraging on the known PWA nature of the explicit MPC law within linear quadratic predictive control, in this paper we propose an approach to derive such an explicit controller from data only, without undertaking a full modeling/identification step. Thanks to the formalization of the problem, well-known model-based techniques can be straightforwardly adapted to check the stability of the closed-loop system before deploying the controller. Future research will be devoted to extend these preliminary results to cases in which the state is not fully measurable, to exploit priors to guarantee practical closedloop stability by design. Future work will also be devoted to formalize the connections between the explicit solution proposed in this paper and the one introduced in [8], consequently providing a comparative analysis of the two approaches."
        }
    ],
    "year": 2022
}