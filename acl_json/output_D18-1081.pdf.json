{
    "abstractText": "BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences.",
    "authors": [
        {
            "affiliations": [],
            "name": "Elior Sulem"
        },
        {
            "affiliations": [],
            "name": "Omri Abend"
        },
        {
            "affiliations": [],
            "name": "Ari Rappoport"
        }
    ],
    "id": "SP:35833bf9afac09d36974165ec71ea8d6dbc1c603",
    "references": [
        {
            "authors": [
                "Roee Aharoni",
                "Yoav Goldberg."
            ],
            "title": "Split and rephrase: Better evaluation and a stronger baseline",
            "venue": "Proc. of ACL\u201918, Short papers, pages 719\u2013 724. http://aclweb.org/anthology/ P18-2114.",
            "year": 2018
        },
        {
            "authors": [
                "Chris Callison-Burch",
                "Miles Osborne",
                "Philipp Koehn."
            ],
            "title": "Re-evaluating the role of BLEU in machine translation",
            "venue": "Proc. of EACL\u201906, pages 249\u2013256. http://www.aclweb.org/ anthology/E06-1032.",
            "year": 2006
        },
        {
            "authors": [
                "Raman Chandrasekar",
                "Christine Doran",
                "Bangalore Srinivas."
            ],
            "title": "Motivations and methods for sentence simplification",
            "venue": "Proc. of COLING\u201996, pages 1041\u20131044. http://aclweb. org/anthology/C/C96/C96-2183.pdf.",
            "year": 1996
        },
        {
            "authors": [
                "Jacob Cohen."
            ],
            "title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
            "venue": "Psychological bulletin, 70(4):213.",
            "year": 1968
        },
        {
            "authors": [
                "Yvette Graham."
            ],
            "title": "Re-evaluating automatic summarization with BLEU and 192 shades of ROUGE",
            "venue": "Proc. of EMNLP\u201915, pages 128\u2013137. http:// www.aclweb.org/anthology/D15-1013.",
            "year": 2015
        },
        {
            "authors": [
                "J. Peter Kincaid",
                "Robert P. Fishburne Jr.",
                "Richard L. Rogers",
                "Brad S. Chissom."
            ],
            "title": "Derivation of new readability formulas (automated readability index, fog count and Flesch reading ease formula) for Navy enlisted personnel",
            "venue": "Technical report, Defense",
            "year": 1975
        },
        {
            "authors": [
                "Philipp Koehn",
                "Christof Monz."
            ],
            "title": "Manual and automatic evaluation of machine translation between European languages",
            "venue": "Proc. of the Workshop on Statistical Machine Translation. http://www. aclweb.org/anthology/W06-3114.",
            "year": 2006
        },
        {
            "authors": [
                "Junyi Jessi Li",
                "Ani Nenkova."
            ],
            "title": "Detecting content-heavy sentences: A cross-language case study",
            "venue": "Proc. of EMNLP\u201915, pages 1271\u20131281. http://www.aclweb.org/anthology/ D15-1148.",
            "year": 2015
        },
        {
            "authors": [
                "Edward Loper",
                "Steven Bird."
            ],
            "title": "NLTK: the natural language toolkit",
            "venue": "Proc. of EMNLP\u201902, pages 63\u201370. http://www.aclweb.org/ anthology/W/W02/W02-0109.pdf.",
            "year": 2002
        },
        {
            "authors": [
                "Shuming Ma",
                "Xu Sun"
            ],
            "title": "A semantic relevance based neural network for text summarization",
            "year": 2017
        },
        {
            "authors": [
                "Jana M. Mason",
                "Janet R. Kendall."
            ],
            "title": "Facilitating reading comprehension through text structure manipulation",
            "venue": "Alberta Journal of Medical Psychology, 24:68\u201376.",
            "year": 1979
        },
        {
            "authors": [
                "Kshitij Mishra",
                "Ankush Soni",
                "Rahul Sharma",
                "Dipti Misra Sharma."
            ],
            "title": "Exploring the effects of sentence simplification on Hindi to English Machine Translation systems",
            "venue": "Proc. of the Workshop on Automatic Text Simplification: Methods and",
            "year": 2014
        },
        {
            "authors": [
                "Shashi Narayan",
                "Claire Gardent."
            ],
            "title": "Hybrid simplification using deep semantics and machine translation",
            "venue": "Proc. of ACL14, pages 435\u2013 445. http://aclweb.org/anthology/P/ P14/P14-1041.pdf.",
            "year": 2014
        },
        {
            "authors": [
                "Shashi Narayan",
                "Claire Gardent."
            ],
            "title": "Unsupervised sentence simplification using deep semantics",
            "venue": "Proc. of INLG\u201916, pages 111\u2013 120. http://aclweb.org/anthology/W/ W16/W16-6620.pdf.",
            "year": 2016
        },
        {
            "authors": [
                "Shashi Narayan",
                "Claire Gardent",
                "Shay B. Cohen",
                "Anastasia Shimorina."
            ],
            "title": "Split and rephrase",
            "venue": "Proc. of EMNLP\u201917, pages 617\u2013627. http:// aclweb.org/anthology/D17-1064.",
            "year": 2017
        },
        {
            "authors": [
                "Sergiu Nisioi",
                "Sanja \u0160tajner",
                "Simone Paolo Ponzetto",
                "Liviu P. Dinu."
            ],
            "title": "Exploring neural text simplification models",
            "venue": "Proc. of ACL\u201917 (Short paper), pages 85\u201391. http://www.aclweb. org/anthology/P17-2014.",
            "year": 2017
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu."
            ],
            "title": "BLEU: a method for automatic evaluation of machine translation",
            "venue": "Proc. of ACL\u201902, pages 311\u2013318. http://aclweb. org/anthology/P/P02/P02-1040.pdf.",
            "year": 2002
        },
        {
            "authors": [
                "Y. Albert Park",
                "Roger Levy."
            ],
            "title": "Automated whole sentence grammar correction using a noisy channel model",
            "venue": "Proc.of ACL-HLT\u201911, pages 934\u2013 944. http://aclweb.org/anthology/P/ P11/P11-1094.pdf.",
            "year": 2011
        },
        {
            "authors": [
                "Ellie Pavlick",
                "Joel Tetreault."
            ],
            "title": "An empirical analysis of formality in online communication",
            "venue": "TACL, 4:61\u201374. http://www.aclweb.org/ anthology/Q16-1005.",
            "year": 2016
        },
        {
            "authors": [
                "Advaith Siddharthan."
            ],
            "title": "Syntactic simplification and text cohesion",
            "venue": "Research on Language and Computation, 4:77\u2013109.",
            "year": 2006
        },
        {
            "authors": [
                "Advaith Siddharthan",
                "M.A. Angrosh"
            ],
            "title": "Hybrid text simplification using synchronous dependency",
            "year": 2014
        },
        {
            "authors": [
                "Sanja \u0160tajner",
                "Hannah Bechara",
                "Horacio Saggion."
            ],
            "title": "A deeper exploration of the standard PBSMT approach to text simplification and its evaluation",
            "venue": "Proc. of ACL\u201915, Short papers, pages 823\u2013 828. http://aclweb.org/anthology/P/",
            "year": 2015
        },
        {
            "authors": [
                "Sanja \u0160tajner",
                "Ruslan Mitkov",
                "Horacio Saggion."
            ],
            "title": "One step closer to automatic evaluation of text simplification systems",
            "venue": "Proc. of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 1\u201310. http://",
            "year": 2014
        },
        {
            "authors": [
                "Elior Sulem",
                "Omri Abend",
                "Ari Rappoport."
            ],
            "title": "Semantic structural evaluation for text simplification",
            "venue": "Proc. of NAACL\u201918, pages 685\u2013 696. http://aclweb.org/anthology/ N18-1063.",
            "year": 2018
        },
        {
            "authors": [
                "Elior Sulem",
                "Omri Abend",
                "Ari Rappoport."
            ],
            "title": "Simple and effective text simplification using semantic and neural methods",
            "venue": "Proc. of ACL\u201918, pages 162\u2013173. http://aclweb. org/anthology/P18-1016.",
            "year": 2018
        },
        {
            "authors": [
                "Hong Sun",
                "Ming Zhou."
            ],
            "title": "Joint learning of a dual SMT system for paraphrase generation",
            "venue": "Proc. of ACL\u201912, pages 38\u2013",
            "year": 2012
        },
        {
            "authors": [
                "Sandra Williams",
                "Ehud Reiter",
                "Liesl Osman."
            ],
            "title": "Experiments with discourse-level choices and readability",
            "venue": "Proc. of the European Natural Language Workshop (ENLG).",
            "year": 2003
        },
        {
            "authors": [
                "Kristian Woodsend",
                "Mirella Lapata."
            ],
            "title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming",
            "venue": "Proc. of EMNLP\u201911, pages 409\u2013420. http://aclweb. org/anthology/D/D11/D11-1038.pdf.",
            "year": 2011
        },
        {
            "authors": [
                "Sander Wubben",
                "Antal van den Bosch",
                "Emiel Krahmer."
            ],
            "title": "Sentence simplification by monolingual machine translation",
            "venue": "Proc. of ACL\u201912, pages 1015\u20131024. http://aclweb.org/ anthology/P/P12/P12-1107.pdf.",
            "year": 2012
        },
        {
            "authors": [
                "Wei Xu",
                "Courtney Napoles",
                "Ellie Pavlick",
                "Quanze Chen",
                "Chris Callison-Burch."
            ],
            "title": "Optimizing statistical machine translation for text simplification",
            "venue": "TACL, 4:401\u2013415. http://aclweb.org/ anthology/Q/Q16/Q16-1029.pdf.",
            "year": 2016
        },
        {
            "authors": [
                "Xingxing Zhang",
                "Mirella Lapata."
            ],
            "title": "Sentence simplification with deep reinforcement learning",
            "venue": "Proc. of EMNLP\u201917, pages 595\u2013605. http:// aclweb.org/anthology/D17-1062.",
            "year": 2017
        },
        {
            "authors": [
                "Yaoyuan Zhang",
                "Zhenxu Ye",
                "Dongyan Zhao",
                "Rui Yan."
            ],
            "title": "A constrained sequence-tosequence neural model for sentence simplification",
            "venue": "ArXiv:1704.02312 [cs.CL]. https:// arxiv.org/pdf/1704.02312.pdf.",
            "year": 2017
        },
        {
            "authors": [
                "Zhemin Zhu",
                "Delphine Bernhard",
                "Iryna Gurevych."
            ],
            "title": "A monolingual tree-based translation model for sentence simplification",
            "venue": "Proc. of COLING\u201910, pages 1353\u20131361. http://aclweb. org/anthology/C/C10/C10-1152.pdf.",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738\u2013744 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n738"
        },
        {
            "heading": "1 Introduction",
            "text": "BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; S\u030ctajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences.\nAlong with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see \u00a72). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity,\n1The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus\nbut obtained a higher correlation with grammaticality and meaning preservation, relative to the SARI metric they proposed.\nIn this paper, we further explore the applicability of BLEU for TS evaluation, examining BLEU\u2019s informativeness where sentence splitting is involved. Sentence splitting, namely the rewriting of a single sentence as multiple sentences while preserving its meaning, is the main structural simplification operation. It has been shown useful for MT preprocessing (Chandrasekar et al., 1996; Mishra et al., 2014; Li and Nenkova, 2015) and human comprehension (Mason and Kendall, 1979; Williams et al., 2003), independently from other lexical and structural simplification operations. Sentence splitting is performed by many TS systems (Zhu et al., 2010; Woodsend and Lapata, 2011; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014, 2016). For example, 63% and 80% of the test sentences are split by the systems of Woodsend and Lapata (2011) and Zhu et al. (2010), respectively (Narayan and Gardent, 2016). Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018), in which the automatic metric used is BLEU.\nFor exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus \u2013 HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by Xu et al. (2016), evaluating a variety of system outputs, as well as HSplit. The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion.2 Second, we experiment with HSplit as\n2Nevertheless, they are also used in contexts where struc-\nthe reference set, evaluating systems that focus on sentence splitting. The first setting allows assessing whether BLEU with the standard reference set is a reliable metric on systems that perform splitting. The second allows assessing whether BLEU can be adapted to evaluate splitting, given a reference set so oriented.\nWe find that BLEU is often negatively correlated with simplicity, even when evaluating outputs without splitting, and that when evaluating outputs with splitting, it is less reliable than a simple measure of similarity to the source (\u00a74.2). Moreover, we show that BLEU cannot be adapted to assess sentence splitting, even where the reference set focuses on this operation (\u00a74.3). We conclude that BLEU is not informative and is often misleading for TS evaluation and for the related Split and Rephrase task."
        },
        {
            "heading": "2 Related Work",
            "text": "The BLEU Metric. BLEU (Papineni et al., 2002) is reference-based, where the use of multiple references is used to address cross-reference variation. To address changes in word order, BLEU uses n-gram precision, modified to eliminate repetitions across the references. A brevity term penalizes overly short sentences. Formally:\nBLEU = BP\u00d7 exp( N\u2211\nn=1\nwnlog(pn))\nwhere BP is the brevity penalty term, pn are the modified precisions, and wn are the corresponding weights, which are usually uniform in practice.\nThe experiments of Papineni et al. (2002) showed that BLEU correlates with human judgments in the ranking of five English-to-Chinese MT systems and that it can distinguish human and machine translations. Although BLEU is widely used in MT, several works have pointed out its shortcomings (e.g., Koehn and Monz, 2006). In particular, Callison-Burch et al. (2006) showed that BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words.\ntural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b).\nBLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adequacy. T-BLEU (S\u030ctajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality. Correlation with simplicity was not considered in this experiment. Xu et al. (2016) focused on lexical simplification, finding that BLEU obtains reasonable correlation for grammaticality and meaning preservation but fails to capture simplicity, even when multiple references are used. To our knowledge, no previous work has examined the behavior of BLEU on sentence splitting, which we investigate here using a manually compiled gold standard."
        },
        {
            "heading": "3 Gold-Standard Splitting Corpus",
            "text": "In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of Xu et al. (2016).3\nWhile Narayan et al. (2017) recently proposed the semi-automatically compiled WEBSPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by Xu et al. (2016) for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017).\nWe use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence\u2019s gram-\n3https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences.\nmaticality, fluency and meaning. The guidelines include two sentence splitting examples.4 In Set 2, annotators are encouraged to split only in cases where it simplifies the original sentence. That is, simplicity is implicit in Set 1 and explicit in Set 2. In both sets, the annotators are instructed to leave the source unchanged if splitting violates grammaticality, fluency or meaning preservation.5\nEach set of guidelines is used by two annotators, with native or native-like proficiency in English. The obtained corpora are denoted by HSplit1, HSplit2 (for Set 1), and HSplit3 and HSplit4 (for Set 2), each containing 359 sentences. Table 1 presents statistics for the corpora. Both in terms of the number of splits per sentence (# Sents) and in terms of the proportion of input sentences that have been split (SplitSents), we observe that the average difference within each set is significantly greater than the average difference between the sets.6 This suggests that the number of splits is less affected by the explicit mention of simplicity than by the inter-annotator variability."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "Metrics. In addition to BLEU,7 we also experiment with (1) iBLEU (Sun and Zhou, 2012) which was recently used for TS (Xu et al., 2016; Zhang et al., 2017) and which takes into account the BLEU scores of the output against the input and against the references; (2) the Flesch-Kincaid Grade Level (FK; Kincaid et al., 1975), computed at the system level, which estimates the readability of the text with a lower value indicating higher\n4Examples are taken from Siddharthan (2006). 5Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material.\n6Wilicoxon\u2019s signed rank test, p = 1.6 \u00b7 10\u22125 for #Sents and p = 0.002 for SplitSents.\n7System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002).\nreadability;8 (3) SARI (Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LDSC), which serves as a measure of conservatism.9\nWe explore two settings. In one (\u201cStandard Reference Setting\u201d, \u00a74.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by Xu et al. (2016) (yielding BLEU-8ref, iBLEU-8ref and SARI-8ref). In the other (\u201cHSplit as Reference Setting\u201d, \u00a74.3), we use HSplit as the reference set.\nSystems. For \u201cStandard Reference Setting\u201d, we consider both a case where evaluated systems do not perform any splittings on the test set (\u201cSystems/Corpora without Splits\u201d), and one where we evaluate these systems, along with the HSplit corpus, used in the role of system outputs (\u201cAll Systems/Corpora\u201d). Systems include six MT-based simplification systems, including outputs of the state-of-the-art neural TS system of Nisioi et al. (2017), in four variants: either default settings or initialization by word2vec, for each both the highest and the fourth ranked hypotheses in the beam are considered.10 We further include Moses (Koehn et al., 2007) and SBMT-SARI (Xu et al., 2016), a syntax-based MT system tuned against SARI, and the identity function (outputs are same as inputs). The case which evaluates outputs with sentence splitting additionally includes the four HSplit corpora and the HSplit average scores.\nFor \u201cHSplit as Reference Setting\u201d, we consider the outputs of six simplification systems whose main simplification operation is sentence splitting: DSS, DSSm, SEMoses, SEMosesm, SEMosesLM and SEMosesmLM , taken from (Sulem et al., 2018b).\nHuman Evaluation. We use the evaluation benchmark provided by Sulem et al. (2018b),11 including system outputs and human evaluation scores corresponding to the first 70 sentences of\n8We thus computed the correlation in \u00a74.2 for -FK. 9LDSC is computed using NLTK.\n10Taking the fourth hypothesis rather than the first has been found to yield considerably less conservative TS systems.\n11https://github.com/eliorsulem/ simplification-acl2018\nthe test corpus of Xu et al. (2016), and extend it to apply to HSplit as well.\nThe evaluation of HSplit is carried out by 3 in-house native English annotators, who rated the different input-output pairs for the different systems according to 4 parameters: Grammaticality (G), Meaning preservation (M), Simplicity (S) and Structural Simplicity (StS). G and M are measured using a 1 to 5 scale. A -2 to +2 scale is used for measuring simplicity and structural simplicity. For computing the inter-annotator agreement of the whole benchmark (including the system outputs and the HSplit corpora), we follow Pavlick and Tetreault (2016) and randomly select, for each sentence, one annotator\u2019s rating to be the rating of Annotator 1 and the rounded average rating of the two other annotators to be the rating of Annotator 2. We then compute weighted quadratic \u03ba (Cohen, 1968) between Annotator 1 and 2. Repeating this process 1000 times, the obtained medians and 95% confidence intervals are 0.42 \u00b1 0.002 for G, 0.77 \u00b1 0.001 for M and 0.59 \u00b1 0.002 for S and StS."
        },
        {
            "heading": "4.2 Results with Standard Reference Setting",
            "text": "Description of the Human Evaluation Scores. The human evaluation scores for each parameter are obtained by averaging over the 3 annotators. The scores at the system level are obtained by averaging over the 70 sentences. In the \u201dAll systems/corpora\u201d case of the \u201dStandard Reference Setting\u201d, where 12 systems/corpora are considered, the range of the average G scores at the system level is from 3.71 to 4.80 (\u03c3 = 0.29). For M, this max-min difference between the systems is 1.23 (\u03c3=0.40). For S and StS, the differences are 0.53 (\u03c3 = 0.17) and 0.65 (\u03c3 = 0.20). At the sentence level, considering 840 sentences (70 for\neach of the system/corpora), the G and M scores vary from 1 to 5 (\u03c3 equals 0.69 and 0.85 respectively), and the S and StS scores from -1 to 2 (\u03c3 equals 0.53 and 0.50).\nIn the \u201dSystems/corpora without Splits\u201d case of the \u201dStandard Reference Setting\u201d, where 7 systems/corpora are considered, the max-min difference at the system level are again 1.09 (\u03c3 = 0.36) and 1.23 (\u03c3 = 0.47) for G and M respectively. For S and StS, the differences are 0.45 and 0.49 (\u03c3 = 0.18). At the sentence level, considering 490 sentences (70 for each of the system/corpora), the G and M scores vary from 1 to 5 (\u03c3 equals 0.78 and 1.01 respectively), and the S and StS scores from -1 to 2 (\u03c3 equals 0.51 and 0.46).\nComparing HSplit to Identity. Comparing the BLEU score on the input (the identity function) and on the HSplit corpora, we observe that the former yields much higher BLEU scores. Indeed, BLEU-1ref obtains 59.85 for the input and 43.90 for the HSplit corpora (averaged over the 4 HSplit corpora). BLEU-8ref obtains 94.63 for the input and 73.03 for HSplit.12 The high scores obtained for Identity, also observed by Xu et al. (2016), indicate that BLEU is a not a good predictor for relative simplicity to the input. The drop in the BLEU scores for HSplit is not reflected by the human evaluation scores for grammaticality (4.43 for AvgHSplit vs. 4.80 for Identity) and meaning preservation (4.70 vs. 5.00), where the decrease between Identity and HSplit is much more limited. For examining these tendencies in more detail, we compute the correlations between the au-\n12These scores concern the first 70 sentences of the corpus. A similar phenomenon is observed on the whole corpus (359 sentences). BLEU-1ref obtains 59.23 for the input and 45.68 for HSplit. BLEU-8ref obtains 94.93 for the input and 75.68 for HSplit.\ntomatic metrics and the human evaluation scores. They are described in the following paragraph.\nCorrelation with Human Evaluation. The system-level Spearman correlations between the rankings of the automatic metrics and the human judgments (see \u00a74.1) are presented in Table 2. We find that in all cases BLEU and iBLEU negatively correlate with S and StS, indicating that they fail to capture simplicity and structural simplicity. Where gold standard splits are evaluated as well, BLEU\u2019s and iBLEU\u2019s failure to capture StS is even more pronounced. Moreover, BLEU\u2019s correlation with G and M in this case disappears. In fact, BLEU\u2019s correlation with M in this case is considerably lower than that of -LDSC and its correlation with G is comparable, suggesting BLEU is inadequate even as a measure of G and M if splitting is involved.\nWe examine the possibility that BLEU mostly acts as a measure of conservatism, and compute the Spearman correlation between -LDSC and BLEU. The high correlations we obtain between the metrics indicate that this may be the case. Specifically, BLEU-1ref obtains correlations of 0.86 (p = 7 \u00d7 10\u22123) without splits and of 0.52 (p = 0.04) where splitting is involved. BLEU-8ref obtains 0.82 (p = 0.01) and 0.55 (p = 0.03).\nSARI obtains positive correlations with S, of 0.52 (without splits) and 0.26 (all systems/corpora), but correlates with StS in neither setting. This may stem from SARI\u2019s focus on lexical, rather than structural TS.\nSimilar trends are observed in the sentencelevel correlation for S, StS and M, whereas G sometimes benefits in the sentence level from including HSplit in the evaluation. For G and M, the correlation with BLEU is lower than its correlation with -LDSC in both cases."
        },
        {
            "heading": "4.3 Results with HSplit as Reference Setting",
            "text": "We turn to examining whether BLEU may be adapted to address sentence splitting, if provided with references that include splittings.\nDescription of the Human Evaluation Scores. In the \u201dHSplit as Reference Reference Setting\u201d, where 6 systems are considered, the max-min difference at the system level is 0.16 (\u03c3 = 0.06) for G, 0.37 for M (\u03c3 = 0.15), and 0.41 for S and StS (\u03c3 equals 0.20 and 0.19 respectively). At the sentence level, considering 420 sentences (70 for each of the systems), the G and M scores vary from 1 to 5 (\u03c3 equals 0.99 and 0.88 respectively), and the S and StS scores from -2 to 2 (\u03c3 equals 0.63).\nCorrelation with Human Evaluation. On the system-level Spearman correlation between BLEU and human judgments, we find that while correlation with G is high (0.57, p = 0.1), it is low for M (0.11, p = 0.4), and negative for S (-0.70, p = 0.06) and StS (-0.60, p = 0.1). Sentence-level correlations of BLEU and iBLEU are positive, but they are lower than those obtained by LDSC . See Table 3.\nTo recap, results in this section demonstrate that even when evaluated against references that focus on sentence splitting, BLEU fails to capture the simplicity and structural simplicity of the output."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper we argued that BLEU is not suitable for TS evaluation, showing that (1) BLEU negatively correlates with simplicity, and that (2) even as a measure of grammaticality or meaning preservation it is comparable to, or worse than -LDSC , which requires no references. Our findings suggest that BLEU should not be used for the evaluation of TS in general and sentence splitting in particular, and motivate the development of alternative methods for structural TS evaluation, such as (Sulem et al., 2018a)."
        },
        {
            "heading": "Acknowledgments",
            "text": "We would like to thank the annotators for participating in our generation and evaluation experiments. We also thank the anonymous reviewers for their helpful advices. This work was partially supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and by the Israel Science Foundation (grant No. 929/17), as well as by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister\u2019s Office."
        }
    ],
    "title": "BLEU is Not Suitable for the Evaluation of Text Simplification",
    "year": 2018
}