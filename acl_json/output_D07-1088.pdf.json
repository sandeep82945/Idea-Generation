{
    "abstractText": "In this paper, we address the problem of extracting data records and their attributes from unstructured biomedical full text. There has been little effort reported on this in the research community. We argue that semantics is important for record extraction or finer-grained language processing tasks. We derive a data record template including semantic language models from unstructured text and represent them with a discourse level Conditional Random Fields (CRF) model. We evaluate the approach from the perspective of Information Extraction and achieve significant improvements on system performance compared with other baseline systems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Donghui Feng"
        },
        {
            "affiliations": [],
            "name": "Gully Burns"
        },
        {
            "affiliations": [],
            "name": "Eduard Hovy"
        }
    ],
    "id": "SP:214ed15ab946836e2c03c68ee73498b38a1da95d",
    "references": [
        {
            "authors": [
                "A. Arasu",
                "H. Garcia-Molina"
            ],
            "title": "Extracting structured data from web pages",
            "venue": "Proc. of SIMOD2003.",
            "year": 2003
        },
        {
            "authors": [
                "D. Beeferman",
                "A. Berger",
                "J. Lafferty"
            ],
            "title": "Text segmentation using exponential models",
            "venue": "Proc. of EMNLP-1997.",
            "year": 1997
        },
        {
            "authors": [
                "P. Blunsom",
                "T. Cohn"
            ],
            "title": "Discriminative word alignment with conditional random fields",
            "venue": "Proc. of ACL-2006.",
            "year": 2006
        },
        {
            "authors": [
                "A Brazma"
            ],
            "title": "Minimum information about a microarray experiment (MIAME)-toward standards for microarray data",
            "venue": "Nat Genet,",
            "year": 2001
        },
        {
            "authors": [
                "G.A. Burns",
                "Cheng",
                "W.-C."
            ],
            "title": "Tools for knowledge acquisition within the NeuroScholar system and their application to anatomical tract-tracing data",
            "venue": "Journal of Biomedical Discovery and Collaboration.",
            "year": 2006
        },
        {
            "authors": [
                "G. Burns",
                "D. Feng",
                "E.H. Hovy"
            ],
            "title": "Intelligent Approaches to Mining the Primary Research Literature: Techniques, Systems, and Examples",
            "venue": "Book Chapter in Computational Intelligence in Bioinformatics, Springer-Verlag, Germany.",
            "year": 2007
        },
        {
            "authors": [
                "F.Y.Y. Choi"
            ],
            "title": "Advances in domain independent linear text segmentation",
            "venue": "Proc. of NAACL-2000.",
            "year": 2000
        },
        {
            "authors": [
                "U. Hahn",
                "M. Romacher",
                "S. Schulz"
            ],
            "title": "Creating knowledge repositories from biomedical reports the MEDSYNDIKATE text mining system",
            "venue": "Proc. of PSB-2002.",
            "year": 2002
        },
        {
            "authors": [
                "M. Hearst"
            ],
            "title": "Multi-paragraph segmentation of expository text",
            "venue": "Proc. of ACL-1994.",
            "year": 1994
        },
        {
            "authors": [
                "F. Jiao",
                "S. Wang",
                "C. Lee",
                "R. Greiner",
                "D. Schuurmans"
            ],
            "title": "Semi-supervised conditional random fields for improved sequence segmentation and labeling",
            "venue": "Proc. of ACL-2006.",
            "year": 2006
        },
        {
            "authors": [
                "J. Lafferty",
                "A. McCallum",
                "F. Pereira"
            ],
            "title": "Conditional Random Fields: probabilistic models for segmenting and labeling Sequence Data",
            "venue": "In Proc. of ICML-2001",
            "year": 2001
        },
        {
            "authors": [
                "D. Lin"
            ],
            "title": "Dependency-based evaluation of MINIPAR",
            "venue": "Proc. of Workshop on the Evaluation of Parsing Systems.",
            "year": 1998
        },
        {
            "authors": [
                "B. Liu",
                "R. Grossman",
                "Y. Zhai"
            ],
            "title": "Mining data records in web pages",
            "venue": "Proc. of SIGKDD-2003.",
            "year": 2003
        },
        {
            "authors": [
                "I. Malioutov",
                "R. Barzilay"
            ],
            "title": "Minimum cut model for spoken lecture segmentation",
            "venue": "Proc. of ACL-2006.",
            "year": 2006
        },
        {
            "authors": [
                "A.K. McCallum"
            ],
            "title": "MALLET: A Machine Learning for Language Toolkit",
            "venue": "http://mallet.cs.umass.edu.",
            "year": 2002
        },
        {
            "authors": [
                "I. Muslea",
                "S. Minton",
                "C.A. Knoblock"
            ],
            "title": "Hierarchical wrapper induction for semistructured information sources",
            "venue": "Autonomous Agents and MultiAgent Systems 4:93-114.",
            "year": 2001
        },
        {
            "authors": [
                "D. Okanohara",
                "Y. Miyao",
                "Y. Tsuruoka",
                "J. Tsujii"
            ],
            "title": "Improving the scalability of semi-markov conditional random fields for named entity recognition",
            "venue": "Proc. of ACL-2006.",
            "year": 2006
        },
        {
            "authors": [
                "F. Peng",
                "A. McCallum"
            ],
            "title": "Accurate information extraction from research papers using conditional random fields",
            "venue": "Proc. of HLT-NAACL-2004.",
            "year": 2004
        },
        {
            "authors": [
                "L. Pevzner",
                "M. Hearst"
            ],
            "title": "A Critique and Improvement of an Evaluation Metric for Text Segmentation",
            "venue": "Computational Linguistics.",
            "year": 2002
        },
        {
            "authors": [
                "D. Pinto",
                "A. McCallum",
                "X. Wei",
                "W.B. Croft."
            ],
            "title": "Table Extraction Using Conditional Random Fields",
            "venue": "Proc. of SIGIR-2003.",
            "year": 2003
        },
        {
            "authors": [
                "K.E. Stephan"
            ],
            "title": "Advanced database methodology for the Collation of Connectivity data on the Macaque brain (CoCoMac)",
            "venue": "Philos Trans R Soc Lond B Biol Sci,",
            "year": 2001
        },
        {
            "authors": [
                "L.W. Swanson"
            ],
            "title": "Brain Maps: Structure of the Rat Brain",
            "venue": "3rd edition, Elsevier Academic Press.",
            "year": 2004
        },
        {
            "authors": [
                "M. Wick",
                "A. Culotta",
                "A. McCallum"
            ],
            "title": "Learning field compatibilities to extract database records from unstructured text",
            "venue": "Proc. of EMNLP-2006.",
            "year": 2006
        },
        {
            "authors": [
                "Y. Yang",
                "J. Pedersen"
            ],
            "title": "A comparative study on feature selection in text categorization",
            "venue": "Proc. of ICML-1997, pp. 412-420.",
            "year": 1997
        },
        {
            "authors": [
                "J. Zhu",
                "Z. Nie",
                "J. Wen",
                "B. Zhang",
                "W. Ma"
            ],
            "title": "Simultaneous record detection and attribute labeling in web data extraction",
            "venue": "Proc. of KDD-2006.",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 837\u2013846, Prague, June 2007. c\u00a92007 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "The discovery and extraction of specific types of information, and its (re)structuring and storage into databases, are critical tasks for data mining, knowledge acquisition, and information integration from large corpora or heterogeneous resources (e.g., Muslea et al., 2001; Arasu and GarciaMolina, 2003). For example, webpages of products on Amazon may contain a list of data records such as books, watches, and electronics. Automatic extraction of individual records will facilitate the access and management of data resources.\nMost current approaches address this problem for structured or semi-structured text, for instance, from XML format files or lists and/or tabular data records on webpages (e.g., Liu et al., 2003; Zhu et al., 2006). The techniques applied rely strongly on the analysis of document structure derived from\nthe webpage\u2019s html tags (e.g., the DOM tree model).\nRegarding unstructured text, most Information Extraction (IE) work has focused on named entities (people, organizations, places, etc.). Such IE treats each extracted element as a separate record. Much less work has focused on the case where several related pieces of information have to be extracted to jointly comprise a single data record. In this work, it is usually assumed that there is only one record for each document (e.g., Kristjannson et al., 2004). Almost no work tries to extract multiple data records from a single document. Multiple data records can be scattered across the narrative in free text. The problem becomes much harder as there are no explicit boundaries between data records and no heavily indicative format features (like html tags) to utilize.\nWith the exponential increase of unstructured text resources (e.g., digitalized publications, papers and/or technical reports), knowledge needs have made it a necessity to explore this problem. For example, biomedical papers contain numerous experiments and findings. But the large volume and rate of publication have made it infeasible to read through the articles and manually identify data records and attributes.\nWe present a study to extract data records and attributes from the biomedical research literature. This is part of an effort to develop a Knowledge Base Management System to benefit neuroscience research. Specifically we are interested in knowledge of various aspects (attributes) of Tract-tracing Experiments (TTE) (data records) in neuroscience. The goal of TTE experiments is to chart the interconnectivity of the brain by injecting tracer chemicals into a region of the brain and identifying corresponding labeled regions where the tracer is\n837\ntaken up and transported to (Burns et al., 2007). To extract data records from the research literature, we need to solve two sub-problems: discovering individual attributes of records and grouping them into one or more individual records, each record representing one TTE experiment. Each attribute may contain a list of words or phrases and each record may contain a list of attributes.\nListing each sentence from top to bottom, we call the first problem the Horizontal Problem (HP) and the second the Vertical Problem (VP). Figure 1 provides an example of a TTE research article with colored fragments representing attributes and dashed frames representing data records. For instance, the third dashed frame represents one experiment record having three attributes with corresponding biological interpretations: \u201cno labeled cells\u201d, \u201cthe DCN\u201d, and \u201cthe contralateral AVCN\u201d.\nWe view the HP and VP problems as two sequential labeling problems and describe our approach using two-level Conditional Random Fields (CRF) (Lafferty et al., 2001) models to extract data records and their attributes.\nThe HP problem (finding individual attribute values) is solved using a sentence-level CRF labeling model that integrates a rich set of linguistic features. For the VP problem, we apply a discourse-level CRF model to identify individual experiments (data records). This model utilizes deep\nsemantic knowledge from the HP results (attribute labels within sentences) together with semantic language models and achieves significant improvements over baseline systems.\nThis paper mainly focuses on the VP problem, since linguistic features for the HP problem is the general IE topic of much past research (e.g., Peng and McCallum, 2004). We apply various feature combinations to learn the most suitable and indicative linguistic features.\nThe remainder of this paper is organized as follows: in the next section we discuss related work. Following that, we present the approach to extract data records in Section 3. We give extensive experimental evaluations in Section 4 and conclude in Section 5."
        },
        {
            "heading": "2 Related Work",
            "text": "As mentioned, data record extraction has been extensively studied for structured and semistructured resources (e.g., Muslea et al., 2001; Arasu and Garcia-Molina, 2003; Liu et al., 2003; Zhu et al., 2006). Most of those approaches rely on the analysis of document structure (reflected in, for example, html tags), from which record templates are derived. However, this approach does not apply to unstructured text. The reason lies in the difficulty of representing a data record template in free text without formatting tags and integrating it\ninto a learning system. We show how to address this problem by deriving data record templates through language analysis and representing them with a discourse level CRF model.\nGiven the problem of identifying one or more records in free text, it is natural to turn toward text segmentation. The Natural Language Processing (NLP) community has come up with various solutions towards topic-based text segmentation (e.g., Hearst, 1994; Choi, 2000; Malioutov and Barzilay, 2006). Most unsupervised text segmentation approaches work under optimization criteria to maximize the intra-segment similarity and minimize the inter-segment similarity based on word distribution statistics. However, this approach cannot be applied directly to data record extraction. A careful study of our corpus shows that data records share many words and phrases and are not distinguishable based on word similairties. In other words, different experiments (records) always belong to the same topic and there is no way to segment them using standard topic segmentation techniques (even if one views the problem as a finer-level segmentation than traditional text segmentation). In addition, most text segmentation approaches require a prespecified number of segments, which in our domain cannot be provided.\n(Wick et al., 2006) report extracting database records by learning record field compatibility. However, in our case, the field compatibility is hard to distinguish even by a human expert. Cluster-based or pairwise field similarity measures do not apply to our corpora without complex knowledge reasoning. Most of Wick et al.\u2019s data (faculty and student\u2019s homepages) contains one record.\nIn addition, as explained below, we have found that surface word statistics alone are not sufficient to derive data record templates for extraction. Some (limited) form of semantic understanding of text is necessary. We therefore first perform some sentence level extraction (following the HP problem) and then integrate semantic labels and semantic language model features into a discourse level CRF model to represent the template for extracting data records in the future.\nRecently an increasing number of research efforts on text mining and IE have used CRF models (e.g., Peng and McCallum, 2004). The CRF model provides a compact way to integrate different types of features when sequential labeling is important.\nRecent work includes improved model variants (e.g., Jiao et al., 2006; Okanohara et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), and word alignment (Blunsom and Cohn, 2006). But none of them have used CRFs for discourse level data record extraction.\nWe use a CRF model to represent a data record template and integrate various knowledge as CRF features. Instead of traditional work on the sentence level, our focus here is on the discourse level. As this has not been carefully explored, we experiment with various selected features.\nFor the biomedical domain, our work will facilitate biomedical research by supporting the construction of Knowledge Base Management Systems (e.g., Stephan et al., 2001; Hahn et al., 2002; Burns and Cheng, 2006). Unlike the well-studied problem of relation extraction from biomedical text, our work focuses on grouping extracted attributes across sentences into meaningful data records. TTE experiment is only one of many experimental types in biology. Our work can be generalized to many different types of data records to facilitate biology research.\nIn the next section, we present our approach to extracting data records."
        },
        {
            "heading": "3 Extracting Data Records",
            "text": "Inspired by the idea of Noun Phrase (NP) chunking in a single sentence, we view the data records extraction problem as discourse chunking from a sequence of sentences using a sequential labeling CRF model."
        },
        {
            "heading": "3.1 Sequential Labeling Model: CRF",
            "text": "The CRF model addresses the problem of labeling sequential tokens while relaxing the strong independence assumptions of Hidden Markov Models (HMMs) and avoiding the presence of label bias from having few successor states. For each current state, we obtain the conditional probability of its output states given previously assigned values of input states. For most language processing tasks, this model is simply a linearchain Markov Random Fields model.\nIn typical labeling processes using CRFs each token is viewed as a labeling unit. For our problem, we process each input document\n),...,,( 21 nsssD = as a sequence of individual sen-\ntences, with a corresponding labeling sequence of labels, ),...,,( 21 nlllL = , so that each sentence corresponds to only one label. In our problem, each data record corresponds to a distinct TTE experiment. Similar to NP chunking, we define three labels for sentences, \u201cB_REC\u201d (beginning of record), \u201cI_REC\u201d (inside record), and \u201cO\u201d (other). The default label \u201cO\u201d indicates that this sentence is beyond our concern.\nThe CRF model is trained to maximize the probability of )|( DLP , that is, given an input document D, we find the most probable labeling sequence L. The decision rule for this procedure is:\n)|(maxarg\u02c6 DLPL L = (1)\nA CRF model of the two sequences is characterized by a set of feature functions kf and their corresponding weights k\u03bb . As in Markov fields, the conditional probability )|( DLP can be computed using Equation 2.\n\u239f \u23a0 \u239e\u239c \u239d \u239b\u2211\u2211= = \u2212\nT\nt k ttkk\nS\ntDllf Z DLP 1\n1 ),,,(*exp 1)|( \u03bb (2)\nwhere ),,,( 1 tDllf ttk \u2212 is a feature function, representing either the state transition feature ),,( 1 Dllf ttk \u2212 or the feature of output state ),( Dlf tk given the input sequence. All these feature functions are userdefined boolean functions.\nCRF works under the framework of supervised learning, which requires a pre-labeled training set to learn and optimize system parameters to maximize the probability or its log format. Equipped with this model, we investigate how to apply it and prepare features accordingly."
        },
        {
            "heading": "3.2 Feature Preparation",
            "text": "The CRF model provides a compact, unified framework to integrate features. However, unlike sentence-level processing, where features are very intuitive and circumscribed, it is not obvious what features are most indicative for our problem. We therefore explore three categories of features for discourse level chunking."
        },
        {
            "heading": "3.2.1 Semantic Attribute Labels",
            "text": "Most text segmentation approaches compute surface word similarity scores in given corpora without semantic analysis. However, in our case, data records have very similar characteristics and\nshare most of the words. They are not distinguishable just from an analysis of surface word statistics. We have to understand the semantics before we can make decisions about data record extraction.\nIn our case, we care about the four types of attributes of each data record (one TTE experiment). Table 1 gives the definitions of the four attributes for each data record.\nindividual sentences (the HP problem), we first apply another sentence-level CRF model to label each sentence. We consider five categories of features based on language analysis. Table 2 shows the features for each category.\na. Lexicon knowledge. We used names of brain structures taken from brain atlases (Swanson, 2004), standard terms to denote neuroanatomical topographical relationships (e.g., \u201crostral\u201d), the name or abbreviation of the tracer chemical used (e.g., \u201cPHAL\u201d), and commonsense descriptions for descriptions of the labeling (e.g., \u201cdense\u201d, \u201clight\u201d). b. Surface and window word. The current word and the words around are important indicators of the most probable label. c. Context window. The TTE is a description of the inject-label-findings process. Whenever a word having a root form of \u201cinjection\u201d or \u201cdeposit\u201d appears, we generate a context window and all the words falling into this window are assigned a feature of \u201cCONTINJ\u201d. d. Dependency features. We apply a dependency parser MiniPar (Lin, 1998) to parse each sentence, and then derive four types of features from the parsing result. These features are (a) root form of every word, (b) the subject within the sentence, (c) the object within the sentence, and (d) the governing verbs.\nThe labeling system assigns a label for every token in each sentence. We achieved the best performance with an F-score of 0.79 (based on a precision of 0.80 and a recall of 0.78). This is not the focus of this paper. Please refer to our previous work (Burns et al., 2007) for details.\n3.2.2 Semantic Language Model\nSince text narratives might adhere to logical ways of expressing facts, language models for each sentence will also provide good features to extract data records. However, in biomedical research articles many of the technical words/phrases used in the narrative are repeated across experiments, making the surface word language model of little use in deriving generalized data record templates. Considering this, we replace in each sentence the labeled fragments with their attribute labels and then derive semantic language models from that format. By \u2018semantic language model\u2019 we therefore mean a combination of semantic labels and surface words.\nFor example, in the sentence shown in Figure 2, we have the semantic language model trigrams location-of-<tracerChemical>, sites-in<injectionLocation>, and <labelingDescription>followed-the. In addition, we also query WordNet for the root form of each word to generalize the semantic language models. This for example produces the semantic language model trigrams sitein-<injectionLocation> and <labelingDescription>follow-the.\nWe believe the collected semantic language models represent an inherent structure of unstructured data records. By integrating them as features with a CRF model, we expect to represent data record templates and use the learned model to extract new data records.\nHowever, it is not clear what semantic language models are most indicative and useful. A bag-ofwords (language models) approach may bring much noise in. We show below a comparison of regular language models and semantic language models in evaluations."
        },
        {
            "heading": "3.2.3 Layout and Word Heuristics",
            "text": "The previous two categories of features come from the discovery of semantic components of sentences and their narrative form word analysis. When interviewing the neuroscience expert annotator, we learned that some layout and word level heuristics may also help to delineate individual data records. Table 3 gives the two types of heuristic features.\nWhen a sentence contains heuristic words, it will be assigned to a word heuristic feature. If the sentence is at the boundary of a paragraph, it will be assigned a layout heuristic feature, namely the first or the last sentence in the paragraph."
        },
        {
            "heading": "4 Empirical Evaluation",
            "text": "To evaluate the effectiveness and performance of our technique, we conducted extensive experiments to measure the data record extraction approach."
        },
        {
            "heading": "4.1 Experimental Setup",
            "text": "We used the machine learning package MALLET (McCallum, 2002) to conduct the CRF model training and labeling.\nWe have obtained the digital publications of 9474 Journal of Comparative Neurology (JCN)1 articles from 1982 to 2005. We have converted the PDF format into plain text, maintaining paragraph breaks (some errors still occur though). A simple heuristic based approach identifies semantic sections of the paper (e.g, Introduction, Results, Discussion). As most experimental descriptions appear in the Results section, we only process the Results section. A neuroscience expert manually annotated the data records in the Results section of 58 research articles. The total number of sentences in the Results section of the 58 files is 6630 (averaging 114.3 sentences per article).\nWe randomly divided this material into training and testing sets under a 2:1 ratio, giving 39 documents in the training set and 19 in the testing set.\n1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248\nTable 4 gives the numbers of documents and data records in the training and the testing set."
        },
        {
            "heading": "4.2 Evaluation Metrics",
            "text": "To evaluate data record extraction, we notice it is not fair to strictly evaluate the boundaries of data records because this does not penalize the nearmiss and false positive of data records in a reasonable way; sentences near a boundary that contain no relevant record information can be included or omitted without affecting the results. Hence the standard Pk (Beeferman et al., 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task.\nAs we are concerned with the usefulness of knowledge in extracted data records, we instead evaluate from the perspective of IE. We measure system performance on the quality of the extracted data records. For each extracted data record, it will be aligned to one of the data records in the gold standard using the \u201cdominance rule\u201d (if the data record can be aligned to multiple records in the gold standard, it will be aligned to the one with highest overlap). Then we evaluate the precision, recall, and F1 scores of extracted units of the data record. The units are the attributes in data records.\nsystem by the units extracted theof # unitscorrect # of precision = (3)\nstandard gold in the units theof # unitscorrect # ofrecall = (4)\necallrprecision recall*precisionF + = *21 (5)\nThese measures provide an indication of the completeness and correctness of each extracted record (experiment). We also measure the number of distinct records extracted, compared with the gold standard as appearing in the document."
        },
        {
            "heading": "4.3 Experiment Results",
            "text": "To fully compare the effectiveness of our semantic analysis functionality, we evaluated system performance for all the following systems:\nTextTiling (TT): To compare with text segmentation techniques, we use TextTiling (Hearst, 1994) with default parameters as the first baseline system.\nRandom Guess (RG): In order to demonstrate the data balance of all the possible labels in the testing set, we also use another baseline system with random decisions for each sentence.\nDomain Heuristics (DH): In a regular TTE experiment, only one tracer chemical will typically be used. Given this heuristic, we assume each data record contains one tracer chemical. In this system, we first locate sentences with identified trace chemicals, and then we greedily expand backward and forward until another new tracer chemical appears or no other attribute is included.\nSurface Text (ST): To measure the effectiveness of the semantic analysis (attribute labels and semantic language models), the ST system utilizes only standard surface word language models and heuristic features.\nSemantic Analysis (SEM): The SEM system uses all the semantic features available (including identified attributes and semantic language models) and two heuristic features.\nTable 5 shows the final performance of these different systems. The second column provides the numbers of extracted data records. In this task, a larger number does not necessarily mean a better system, as a system might produce too many false positives. The remaining three columns represent the precision, recall, and F1 scores, averaged over all data records. With our approach, the system performance is significantly improved compared with other systems. System TT fails in this task as it only outputs the full document as one single record.\nTo investigate how plain text language models and semantic language models affect system performance, we also experimented with all the language models. Table 6 shows comparisons of three types of language models. Systems with semantic analysis always work better than those with only surface text analysis. Without semantic analysis, unigram features work better than bigram and trigram features. This matches our intuition: without generalizing to semantic language models, higher order language models will be relatively sparse and contain much noise. However, when taking into account the semantic features, we found that bigram and trigram semantic language model fea-\ntures outperformed unigrams. They are especially important in boosting the recall scores as they capture more generalized information when derived.\nEach data record is measured with precision, recall, and F1 scores. Figure 3 depicts the distribution of extracted data records according to these measures in the best system.\nrepresents the value interval for precision, recall, and F1, and Axis Y represents the number of extracted records with their corresponding values. For example, 57 records have recall scores falling into [0.9, 1.0].\nFigure 4 gives an example alignment between system result and the gold standard. Each record is represented by a range of sentences. The numbers following each record in the system result are individual data record\u2019s precision and recall scores."
        },
        {
            "heading": "4.4 Error Analysis and Discussion",
            "text": "When we investigated the errors, we found that sometimes the extracted data records combined two or more smaller gold standard records, or vice versa. As shown in Figure 4, extracted records R4 and R7 are both combinations of records in the gold standard. This is partially due to the granularity definition problem. Authors may mention several approaches/symptoms to one type of experiment for a single purpose. In this case, it is almost infeasible to have annotators strictly agree on granularity and thus to teach the system to acquire this knowledge. For example, in the gold standard, the annotator annotated three successive sentences as three separate records but the system output\nthose as only one data record. In this extreme case, it is too hard to expect the system to perform well.\nIn our approach, the semantic attribute labels and semantic language models require the result of the initial sentence-level labeling, which has an Fscore of 0.79. The error may propagate into the data record extraction procedure and lower overall system performance.\nIn our current experiments, we also assume all the attributes within one segment belong to one record. However, the situation of embedded data records will make this problem harder. For example, authors sometimes compare the current experiment with other approaches in referenced papers. In this case, those attributes should be excluded from the records. We need to invent rules or constraints to filter them out. When such reference occurs at experiment boundaries, it brings higher risk for correct results.\nIt is a very hard problem to extract from unstructured text neat structured records. The annotators sometimes employ background knowledge or reasoning when performing manual extraction; such knowledge cannot today be easily modeled and integrated into learning systems.\nIn our study, we also compared some feature selection approaches. Similar to (Yang and Pedersen, 1997), we tried Feature Instance Frequency, Mutual Information, Information Gain, and CHIsquare test. But we eventually found that the system including all the features worked best, and with all the other configurations unchanged, feature instance frequency worked at almost the same level as other complex measures such as mutual information and information gain."
        },
        {
            "heading": "5 Conclusion and Future Work",
            "text": "In this paper, we explored the problem of extracting data records from unstructured text. The lack of structure makes it difficult to derive meaningful objects and their values without resorting to deeper language analysis techniques. We derived indicative linguistic features to represent data record templates in free text, using a two-pass approach in which the second pass used the IE labels derived from the first to compose attributes into coherent data records. We evaluated the results from an IE perspective and reported potential problems of error generation.\nFor the future, we plan to explore additional feature types and feature selection strategies to determine what is \u201cgood\u201d for unstructured record templates to improve our results. More effort will also be put into the sentence-level analysis to reduce error propagations. In addition, ontology based knowledge inference strategies might be useful to validate attributes in single record and in turn help data record extraction. The last thing under our direction is to explore new models if applicable.\nWe hope this thought-provoking problem will attract more attention from the community. In the future, we plan to make our corpus available to the community. The solution to this problem will highly affect the access of knowledge in large scale unstructured text corpora."
        },
        {
            "heading": "Acknowledgements",
            "text": "The work was supported in part by an ISI seed funding, and in part by a grant from the National Library of Medicine (RO1 LM07061). The authors want to thank Feng Pan for his helpful suggestions with the manuscript. We would also like to thank the anonymous reviewers for their valuable comments."
        }
    ],
    "title": "Extracting Data Records from Unstructured Biomedical Full Text",
    "year": 2007
}