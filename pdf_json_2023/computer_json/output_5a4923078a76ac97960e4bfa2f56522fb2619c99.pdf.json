{
    "abstractText": "This paper proposes a novel Attention-based Encoder-Decoder network for End-to-End Neural speaker Diarization (AEDEEND). In AED-EEND system, we incorporate the target speaker enrollment information used in target speaker voice activity detection (TS-VAD) to calculate the attractor, which can mitigate the speaker permutation problem and facilitate easier model convergence. In the training process, we propose a teacher-forcing strategy to obtain the enrollment information using the ground-truth label. Furthermore, we propose three heuristic decoding methods to identify the enrollment area for each speaker during the evaluation process. Additionally, we enhance the attractor calculation network LSTM used in the end-to-end encoder-decoder based attractor calculation (EENDEDA) system by incorporating an attention-based model. By utilizing such an attention-based attractor decoder, our proposed AED-EEND system outperforms both the EEND-EDA and TSVAD systems with only 0.5s of enrollment data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhengyang Chen"
        },
        {
            "affiliations": [],
            "name": "Bing Han"
        },
        {
            "affiliations": [],
            "name": "Shuai Wang"
        },
        {
            "affiliations": [],
            "name": "Yanmin Qian"
        }
    ],
    "id": "SP:f597260520cefb86cca29c1a42bd922899469b64",
    "references": [
        {
            "authors": [
                "S.H. Shum",
                "N. Dehak",
                "R. Dehak",
                "J.R. Glass"
            ],
            "title": "Unsupervised methods for speaker diarization: An integrated and iterative approach",
            "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2015\u20132028, 2013.",
            "year": 2015
        },
        {
            "authors": [
                "G. Sell",
                "D. Garcia-Romero"
            ],
            "title": "Speaker diarization with plda ivector scoring and unsupervised calibration",
            "venue": "2014 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 413\u2013417.",
            "year": 2014
        },
        {
            "authors": [
                "G. Sell",
                "D. Snyder",
                "A. McCree",
                "D. Garcia-Romero",
                "J. Villalba",
                "M. Maciejewski",
                "V. Manohar",
                "N. Dehak",
                "D. Povey",
                "S. Watanabe"
            ],
            "title": "Diarization is hard: Some experiences and lessons learned for the jhu team in the inaugural dihard challenge.",
            "venue": "in Interspeech,",
            "year": 2018
        },
        {
            "authors": [
                "Q. Wang",
                "C. Downey",
                "L. Wan",
                "P.A. Mansfield",
                "I.L. Moreno"
            ],
            "title": "Speaker diarization with lstm",
            "venue": "2018 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5239\u20135243.",
            "year": 2018
        },
        {
            "authors": [
                "Q. Lin",
                "R. Yin",
                "M. Li",
                "H. Bredin",
                "C. Barras"
            ],
            "title": "Lstm based similarity measurement with spectral clustering for speaker diarization",
            "venue": "arXiv preprint arXiv:1907.10393, 2019.",
            "year": 1907
        },
        {
            "authors": [
                "G. Sell",
                "D. Garcia-Romero"
            ],
            "title": "Diarization resegmentation in the factor analysis subspace",
            "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4794\u20134798.",
            "year": 2015
        },
        {
            "authors": [
                "M. Diez",
                "L. Burget",
                "P. Matejka"
            ],
            "title": "Speaker diarization based on bayesian hmm with eigenvoice priors.",
            "venue": "in Odyssey,",
            "year": 2018
        },
        {
            "authors": [
                "Y. Fujita",
                "N. Kanda",
                "S. Horiguchi",
                "K. Nagamatsu",
                "S. Watanabe"
            ],
            "title": "End-to-end neural speaker diarization with permutation-free objectives",
            "venue": "arXiv preprint arXiv:1909.05952, 2019.",
            "year": 1909
        },
        {
            "authors": [
                "Y. Fujita",
                "N. Kanda",
                "S. Horiguchi",
                "Y. Xue",
                "K. Nagamatsu",
                "S. Watanabe"
            ],
            "title": "End-to-end neural speaker diarization with selfattention",
            "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 296\u2013303.",
            "year": 2019
        },
        {
            "authors": [
                "S. Horiguchi",
                "Y. Fujita",
                "S. Watanabe",
                "Y. Xue",
                "K. Nagamatsu"
            ],
            "title": "End-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors",
            "venue": "arXiv preprint arXiv:2005.09921, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "Y. Fujita",
                "S. Watanabe",
                "S. Horiguchi",
                "Y. Xue",
                "J. Shi",
                "K. Nagamatsu"
            ],
            "title": "Neural speaker diarization with speaker-wise chain rule",
            "venue": "arXiv preprint arXiv:2006.01796, 2020.",
            "year": 2006
        },
        {
            "authors": [
                "Y. Takashima",
                "Y. Fujita",
                "S. Watanabe",
                "S. Horiguchi",
                "P. Garc\u0131\u0301a",
                "K. Nagamatsu"
            ],
            "title": "End-to-end speaker diarization conditioned on speech activity and overlap detection",
            "venue": "2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 849\u2013 856.",
            "year": 2021
        },
        {
            "authors": [
                "J.R. Hershey",
                "Z. Chen",
                "J. Le Roux",
                "S. Watanabe"
            ],
            "title": "Deep clustering: Discriminative embeddings for segmentation and separation",
            "venue": "2016 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2016, pp. 31\u201335.",
            "year": 2016
        },
        {
            "authors": [
                "I. Medennikov",
                "M. Korenevsky",
                "T. Prisyach",
                "Y. Khokhlov",
                "M. Korenevskaya",
                "I. Sorokin",
                "T. Timofeeva",
                "A. Mitrofanov",
                "A. Andrusenko",
                "I. Podluzhny"
            ],
            "title": "Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario",
            "venue": "arXiv preprint arXiv:2005.07272, 2020.",
            "year": 2005
        },
        {
            "authors": [
                "\u2014\u2014"
            ],
            "title": "The stc system for the chime-6 challenge",
            "venue": "CHiME 2020 Workshop on Speech Processing in Everyday Environments, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "W. Wang",
                "X. Qin",
                "M. Cheng",
                "Y. Zhang",
                "K. Wang",
                "M. Li"
            ],
            "title": "The dku-smiip diarization system for the voxceleb speaker recognition challenge 2022",
            "venue": "Voxsrc Workshop, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "C.-Y. Cheng",
                "H.-S. Lee",
                "Y. Tsao",
                "H.-M. Wang"
            ],
            "title": "Multi-target extractor and detector for unknown-number speaker diarization",
            "venue": "arXiv preprint arXiv:2203.16007, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "D. Wang",
                "X. Xiao",
                "N. Kanda",
                "T. Yoshioka",
                "J. Wu"
            ],
            "title": "Target speaker voice activity detection with transformers and its integration with end-to-end neural diarization",
            "venue": "arXiv preprint arXiv:2208.13085, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "M. Cheng",
                "W. Wang",
                "Y. Zhang",
                "X. Qin",
                "M. Li"
            ],
            "title": "Targetspeaker voice activity detection via sequence-to-sequence prediction",
            "venue": "arXiv preprint arXiv:2210.16127, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "Y. Jiang",
                "R. Tao",
                "Z. Pan",
                "H. Li"
            ],
            "title": "Target active speaker detection with audio-visual cues",
            "venue": "arXiv preprint arXiv:2305.12831, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, vol. 30, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "A. Woodward",
                "C. Bonn\u0131\u0301n",
                "I. Masuda",
                "D. Varas",
                "E. Bou-Balust",
                "J.C. Riveiro"
            ],
            "title": "Confidence measures in encoder-decoder models for speech recognition.",
            "venue": "INTERSPEECH,",
            "year": 2020
        },
        {
            "authors": [
                "S. Horiguchi",
                "Y. Fujita",
                "S. Watanabe",
                "Y. Xue",
                "P. Garcia"
            ],
            "title": "Encoder-decoder based attractors for end-to-end neural diarization",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 1493\u20131507, 2022.",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "The speaker diarization task is defined to address the \u201cWho spoke when?\u201d problem. Traditional speaker diarization systems [1, 2] usually use a stage-wise paradigm: 1) A speaker embedding extractor is adopted to get the speaker embedding for each segment. 2) A clustering algorithm [3, 4, 5] merges segments based on the speaker embedding similarity and assigns the speaker label to each segment. 3) Optionally, some compensation algorithm like Variational-Bayesian refinement [6, 3, 7] is used to calibrate the clustering results. However, the stagewise system can not deal with the overlap problem due to the frame-wise one-to-one assignment of the clustering algorithm.\nTo address this issue, many end-to-end methods have been proposed in recent years. For example, EEND [8, 9] models the diarization task as a multi-class classification problem, allowing frames to be classified into multiple speakers. However, a limitation of EEND is its fixed number of output heads, which makes it difficult to handle varying numbers of speakers. To address this limitation, Horiguchi et al. [10] proposed EEND-EDA, which uses an LSTM encoder-decoder to predict the attractor for each speaker. The number of attractors can be flexible, allowing the system to generalize to sessions where the number of speakers varies. Fujita et al. [11] and Takashima et al. [12] have also proposed methods that sequentially output diarization results for each speaker using a chain rule. However, all of these methods suffer from the speaker permutation\n\u2020Corresponding Author\nproblem [13], making system training challenging. The target-speaker voice activity detection (TS-VAD) [14, 15] system has attracted significant attention due to its outstanding performance in speaker diarization challenges [15, 16]. This has led to the development of various TS-VAD variants [17, 18, 19, 20] by different researchers. In the TS-VAD system, a separate diarization system first predicts the single-speaker speaking area for each speaker. Subsequently, the speaker embeddings extracted for each speaker are concatenated with the acoustic features and fed into the diarization system. Because the TS-VAD system uses pre-acquired speaker embeddings, it does not suffer from the speaker permutation problem.\nIn this paper, we propose an advanced attention-based encoder-decoder network for end-to-end neural speaker diarization (AED-EEND) by integrating the target speaker enrollment information used in the TS-VAD system into the original EEND-EDA system. Our system overcomes the limitations of both TS-VAD and EEND-EDA while leveraging their advantages. Unlike the TS-VAD system, which requires enrollment information from an external diarization system and speaker embedding extractor, our system obtains the enrollment information directly from the AED-EEND system, thereby making our system fully end-to-end. Additionally, we replace the LSTM-based model used in EEND-EDA to generate attractors with an attention-based decoder, which has shown to be more effective. More importantly, our AED-EEND system only uses an attention-based encoder-decoder network, which is much simpler than the EEND-EDA and TS-VAD systems.\nBesides, we propose a teacher forcing strategy for the training process that uses ground-truth labels to acquire enrollment information. For evaluation, we propose three heuristic decoding methods to output the diarization results for each speaker iteratively. Our proposed attention-based attractor decoder is particularly noteworthy as it produces robust results with only 0.5 seconds of enrollment speech, which significantly outperforms both the EEND-EDA and TS-VAD systems on the CALLHOME evaluation set."
        },
        {
            "heading": "2. Method",
            "text": ""
        },
        {
            "heading": "2.1. Attention-based Encoder-Decoder Network for EEND",
            "text": "In this section, we introduce the architecture of our proposed AED-EEND system. The overall architecture of our system is shown in the upper part of Figure 1. The AED-EEND system contains two parts: an embedding encoder and an attractor decoder. The encoder-decoder architecture is very similar to the original transformer model proposed in [21] except that we neglect the positional encoding in our system. The embedding encoder accepts the acoustic feature sequence X = [x1,x2, ...,xT ] \u2208 RT\u00d7F as input, and outputs the frame\nar X\niv :2\n30 5.\n10 70\n4v 3\n[ cs\n.S D\n] 1\n5 A\nug 2\n02 3\nlevel speaker embedding sequence E = [e1, e2, ..., eT ] \u2208 RT\u00d7D . The attractor decoder takes the enrollment sequence Eenroll = [enon, esgl, eovl, espk1 , ..., espkS ] \u2208 R\n(S+3)\u00d7D as input, where enon, esgl and eovl correspond to the enrollment embeddings for non-speech, single-speaker speaking speech and overlap speech, respectively, and espki corresponds to the enrollment embedding for i-th speaker in the recording. The enrollment embedding sequence Eenroll is fed into the attractor decoder to get the corresponding attractor sequence A = [anon,asgl,aovl,aspk1 , ...,aspkS ] \u2208 R (S+3)\u00d7D .\nIn contrast to the embedding encoder that solely incorporates the self-attention (SA) module, the attractor decoder is equipped with both the self-attention (SA) module and the cross-attention (CA) module. The SA module in the attractor decoder enables the enrollment inputs to attend to one another, whereas the CA module facilitates incorporating additional relevant information from the embedding sequence into the enrollment embedding. Specifically, the output from the SA module serves as the query, while the speaker embedding sequence output from the embedding encoder is treated as the key and value. This mechanism is expected to enhance the quality of the enrollment embedding.\nUnlike the EEND-EDA architecture [10], which only generates attractors for the speakers present in the recording, our proposed AED-EEND system also outputs attractors for three distinct speech types in the recording. These speech types include the single-speaker, speaker-overlap, and non-speech regions. The resulting attractors are then used to compute framelevel posterior probabilities for each speaker and speech type\nby performing a dot product operation with the corresponding embedding sequence.\nY\u0302 = \u03c3 ( AE\u22a4 ) \u2208 (0, 1)(S+3)\u00d7T (1)\nwhere \u03c3(\u00b7) is the element-wise sigmoid function. Here, we denote the posterior probabilities at each frame as y\u0302t = [y\u0302nont , y\u0302 sgl t , y\u0302 ovl t , y\u0302 1 t , ..., y\u0302 S t ] \u2208 (0, 1)(S+3), and denote the ground truth label at each frame for speaker and speech activity as yt = [y non t , y sgl t , y ovl t , y 1 t , ..., y S t ] \u2208 {0, 1}(S+3). The label 1 corresponds to the specific speaker or specific kind of speech existing in the corresponding frame, and vice versa. We calculate the loss by averaging the cross entropy between the prediction and ground truth for each speaker or speech type at each frame:\nL = 1 T (S + 3) T\u2211 t=1 \u2211 s\u2208S [\u2212yst log y\u0302st \u2212 (1\u2212 yst ) log (1\u2212 y\u0302st )] (2) where S = {non, sgl, ovl, 1, .., S}."
        },
        {
            "heading": "2.2. System Training with Teacher Forcing Enrollment",
            "text": "Similar to the TS-VAD method [14], our proposed AED-EEND system also requires enrollment embedding for each speaker, but we obtain it directly from the embedding sequence E, eliminating the need for a pre-trained speaker embedding extraction system. As depicted in the lower part of Figure 1, given the single-speaker speaking area of each speaker in the recording, we can obtain the corresponding embeddings and average them to obtain the enrollment embedding for each speaker.\nIn the training of automatic speech recognition (ASR) systems, the teacher forcing strategy [22] is a widely adopted technique that leverages ground-truth input sequences to predict subsequent outputs during the training process. This strategy is known to improve training stability and efficiency. In the training process of our proposed AED-EEND, we similarly employ the teacher-forcing strategy. Specifically, we obtain the singlespeaker speaking area for each speaker from the ground-truth label and randomly select a consecutive area with an enrollment length Lenroll as the enrollment area.\nIn contrast to speaker enrollment embeddings, which are obtained from the embedding sequence E, we directly set the enrollment embeddings for non-speech, single-speaker speech, and overlapping speech as three learnable embeddings."
        },
        {
            "heading": "2.3. System Inference by Iterative Decoding",
            "text": "In this section, we propose an iterative decoding method for AED-EEND in the evaluation process, and the pipeline is shown in algorithm 1. The first step is to predict the results for single-speaker, overlap, and non-speech with learnable embeddings. Then, the prediction for each speaker is obtained iteratively. At each iteration, we first find the un-predicted singlespeaker speaking area I and then split the area into some timecontinuous segments. If the longest segment is shorter than a pre-defined threshold Lstop, we will stop the decoding. To get the prediction for each speaker in one iteration, we propose three heuristic strategies to enlarge the probability that there is only one speaker in the enrollment area: \u2022 Init-Decode: Use the initial Lenroll frames of the first time-\ncontinuous segments in I as the enrollment area. \u2022 Rand-Decode: Randomly select a time-continuous segment I \u2032 from I , and then randomly select Lenroll length consecutive frames from I \u2032 as the enrollment area.\nAlgorithm 1: Iterative Decoding Pipeline Data: I = [t1, t2, ...]: frame indexes list for embedding in E\nC = [I1, I2, ..., IK ]: K time-continuous segments from I , and the indexes in Ik are sorted in order\n// get the active frame indexes list for three kinds of speeches\n1 Inon, Isgl, Iovl = AED-EEND(enon, esgl, eovl) // active frame indexes list for speakers; enroll embedding set 2 Ispk = []; E = {} 3 while True do\n// get the un-predicted single-speaker area\n4 I = Isgl \u2212 Isgl \u2229 Ispk // get segments with length \u2265 LEnroll 5 C\u2032 = [I\u20321, I \u2032 2, ..., I \u2032 K\u2032 ] = filter segs(C,LEnroll)\n// get the segment with the longest length\n6 Ilongest = get longest seg(C) 7 C\u2032.add(Ilongest) 8 LEnroll tmp = min(length(Ilongest), LEnroll) 9 if Lstop > length(Ilongest) then\n10 break;\n11 case Init-Decode do 12 Ienroll = [t1, t2, ..., tLEnroll tmp ] \u2208 I \u2032 1\n13 case Rand-Decode do // randomly select a segment from C\u2032 14 I\u2032k = random select seg(C \u2032)\n// randomly select a consecutive sub-segment with length\nLEnroll tmp\n15 Ienroll = random select sub-seg(I\u2032k, LEnroll tmp) 16 case SC-Decode do // The index in I is clustered based on corresponding\nembedding\n17 [Icls1 , I cls 2 , ...] = spectral cluster(I) 18 Iclsk = get longest seg([I cls 1 , I cls 2 , ...]) 19 Ienroll = random select sub-seg(Iclsk , LEnroll tmp) // average the embeddings with indexes in Ienroll 20 e = average(EIenroll ) 21 E .add(e) 22 Ispk = (AED-EEND(E))\nOutput: Ispk\n\u2022 SC-Decode: Cluster the embeddings in I using spectral clustering algorithm [4, 5], and then randomly select Lenroll length consecutive frames from the biggest cluster as the enrollment area.\nBesides, for comparison, we also follow the strategy in the training process to get the enrollment area and denote this strategy as GT-Decode."
        },
        {
            "heading": "3. Experimental Setup",
            "text": "To ensure comparability with previous studies, we adopted most of the configurations used in EEND-EDA [10] except for the model architecture. In [10], the authors utilized an LSTM encoder-decoder to output the attractors. As outlined in section 2.1, we replaced the LSTM with a four-layer transformer model. The attention unit number is set to 256 in our experiment. Besides, our experiment also utilized the 345-dimensional acoustic features in [10] as input to the embedding encoder.\nRegarding training data, we followed the pipeline in [10] to simulate 1-, 2-, 3-, and 4-speaker datasets. However, unlike [10], we only created the simulation evaluation set with a similar overlap ratio to the training set. We used the CALLHOME [23] dataset to evaluate our system on real recordings and split the dataset into two parts - part1 for adaptation and part2 for evaluation, in accordance with the Kaldi recipe 1.\n1https://github.com/kaldi-asr/kaldi/tree/\nFollowing [10], we evaluated our systems under two conditions: the fixed number of speakers condition and the flexible number of speakers condition. For the fixed number of speakers condition, we trained the systems on 2-speaker or 3- speaker datasets for 100 epochs. To evaluate the systems on real recordings, we further fine-tuned the models on the corresponding number of speakers adaptation set from CALLHOME for another 100 epochs. For the flexible number of speakers condition, we first fine-tuned the pre-trained model on the 2- speaker simulation set on the concatenation of 1-, 2-, 3-, and 4-speaker datasets for another 25 epochs. To evaluate the model on CALLHOME, we further fine-tuned the model on the entire CALLHOME adaptation set for another 100 epochs.\nDuring our training process, we set the enrollment length Lenroll, as introduced in section 2, to 10-30 frames, corresponding to 1s-3s. During the evaluation process, we set the enrollment length Lenroll to 5 frames (0.5s) and the stop decoding length Lstop to 10 frames (1s). Additionally, we randomly set some enrollment speaker embeddings to zero vectors during the training process to ensure the model works correctly even when not all the enrollment embeddings are available.\nWe evaluated each system\u2019s performance using the diarization error rate (DER) with a 0.25s collar tolerance to mitigate potential labeling errors."
        },
        {
            "heading": "4. Results and Analysis",
            "text": ""
        },
        {
            "heading": "4.1. Comparison Among Different Decoding Methods",
            "text": "This section presents the evaluation results of different decoding methods proposed in section 2.3. The results are summarized in Table 1. In the experiments, we either use the oracle speaker number or estimate the speaker number using the stop decoding criterion proposed in section 2.3. Surprisingly, the performance gap between estimated speaker number and oracle speaker number is not significant, which demonstrates the effectiveness of the stop decoding criterion in accurately predicting the speaker number. Furthermore, the SC-Decode method exhibits the best performance on most evaluation conditions, even surpassing the GT-Decode method. Therefore, we adopt the SC-Decode method in the subsequent experiments."
        },
        {
            "heading": "4.2. Results on Fixed Number of Speakers",
            "text": "This section evaluates our system when the speaker number is fixed and known in advance. The corresponding results are shown in Table 2. Apart from the 2-spk simulation dataset evaluation, our proposed method outperforms all the other methods. It is worth noting that even though our proposed AED-EEND method performs worse than the EEND-EDA on the 2-spk simulation dataset, our method outperforms the EEND-EDA on the\nmaster/egs/callhome_diarization/v2\n2-spk CALLHOME dataset, which demonstrates the better generalizability of our proposed method."
        },
        {
            "heading": "4.3. Results on Flexible Number of Speakers",
            "text": ""
        },
        {
            "heading": "4.3.1. Results on simulation dataset",
            "text": "In this section, we evaluate our systems when the speaker number is flexible. We first do the experiment on the simulation dataset and show the result in Table 3. From the results, we find our method outperforms both the stage-wise x-vector clustering method and the EEND-EDA method on any number of speakers condition.\n4.3.2. Results on CALLHOME dataset\nThen, we evaluate our system on the CALLHOME dataset and list the results in Table 4. The overall performance of our proposed method is better than both x-vector clustering and EEND-EDA, and this improvement mainly comes from the small speaker number condition. Similar to the conclusion on the simulation dataset, whether or not to use an oracle speaker number in the decoding process has little effect on the results of CALLHOME dataset. Besides, similar to the results of EENDEDA, our system performs worse than the x-vector clustering in the 5-speaker and 6-speaker conditions. In the follow-up work\n[24] of EEND-EDA, the authors improve these results by simulating the data with more speakers in each recording. We plan to try this approach in our future work."
        },
        {
            "heading": "4.4. Analysis of Non-Speech, Single-speaker Speech and Overlap Speech Prediction",
            "text": "As mentioned in section 2.3, we first predict the nonspeech, single-speaker speech, and overlap speech area for an input utterance and then use the single-speaker speech area as the prior knowledge to do the iterative decoding. This section presents an evaluation of our proposed system on these three types of predictions, as shown in Table 5. Here, we will treat each type of prediction independently, without considering the potential conflicts between different types of predictions. The simulation dataset yields accurate area predictions for all three types of speech, with false alarm and miss rates below 10%. However, the system performs worse on some evaluation metrics for the CALLHOME dataset. We believe that the reason for this outcome is the significant mismatch in single-speaker and overlap proportions between the simulation dataset and the real CALLHOME dataset. The statistics in [24] indicate that in the simulation dataset, the proportions of different speech types are relatively balanced. However, in the real CALLHOME dataset, single-speaker speech accounts for the majority of the audio, with low proportions of both overlap and single-speaker speech."
        },
        {
            "heading": "5. Conclusion",
            "text": "In this paper, we propose the AED-EEND system, which effectively integrates enrollment information into the EENDEDA system. To enhance the model\u2019s training, we introduce a teacher forcing technique that leverages ground-truth labels to obtain enrollment embeddings during training. We present three heuristic decoding approaches to identify the enrollment information for each speaker during the evaluation process. Furthermore, we replace the LSTM architecture for attractor calculation in EEND-EDA with an attention-based model. Our approach achieves remarkable performance with very short enrollment data. AED-EEND introduces a novel paradigm for diarization tasks. However, due to space limitations, our evaluation on real datasets was limited. In future research, we will conduct a more thorough analysis of our proposed AED-EEND system and evaluate it in a broader range of real-world datasets."
        },
        {
            "heading": "6. Acknowledgements",
            "text": "This work was supported in part by China STI 2030-Major Projects under Grant No. 2021ZD0201500, in part by China NSFC projects under Grants 62122050 and 62071288, and in part by Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0102. The author Zhengyang Chen is supported by Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University."
        },
        {
            "heading": "7. References",
            "text": "[1] S. H. Shum, N. Dehak, R. Dehak, and J. R. Glass, \u201cUnsuper-\nvised methods for speaker diarization: An integrated and iterative approach,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 10, pp. 2015\u20132028, 2013.\n[2] G. Sell and D. Garcia-Romero, \u201cSpeaker diarization with plda ivector scoring and unsupervised calibration,\u201d in 2014 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 413\u2013417.\n[3] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watanabe et al., \u201cDiarization is hard: Some experiences and lessons learned for the jhu team in the inaugural dihard challenge.\u201d in Interspeech, 2018, pp. 2808\u20132812.\n[4] Q. Wang, C. Downey, L. Wan, P. A. Mansfield, and I. L. Moreno, \u201cSpeaker diarization with lstm,\u201d in 2018 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 5239\u20135243.\n[5] Q. Lin, R. Yin, M. Li, H. Bredin, and C. Barras, \u201cLstm based similarity measurement with spectral clustering for speaker diarization,\u201d arXiv preprint arXiv:1907.10393, 2019.\n[6] G. Sell and D. Garcia-Romero, \u201cDiarization resegmentation in the factor analysis subspace,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4794\u20134798.\n[7] M. Diez, L. Burget, and P. Matejka, \u201cSpeaker diarization based on bayesian hmm with eigenvoice priors.\u201d in Odyssey, 2018, pp. 147\u2013154.\n[8] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, \u201cEnd-to-end neural speaker diarization with permutation-free objectives,\u201d arXiv preprint arXiv:1909.05952, 2019.\n[9] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, \u201cEnd-to-end neural speaker diarization with selfattention,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 296\u2013303.\n[10] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and K. Nagamatsu, \u201cEnd-to-end speaker diarization for an unknown number of speakers with encoder-decoder based attractors,\u201d arXiv preprint arXiv:2005.09921, 2020.\n[11] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, and K. Nagamatsu, \u201cNeural speaker diarization with speaker-wise chain rule,\u201d arXiv preprint arXiv:2006.01796, 2020.\n[12] Y. Takashima, Y. Fujita, S. Watanabe, S. Horiguchi, P. Garc\u0131\u0301a, and K. Nagamatsu, \u201cEnd-to-end speaker diarization conditioned on speech activity and overlap detection,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 849\u2013 856.\n[13] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, \u201cDeep clustering: Discriminative embeddings for segmentation and separation,\u201d in 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2016, pp. 31\u201335.\n[14] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny et al., \u201cTarget-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,\u201d arXiv preprint arXiv:2005.07272, 2020.\n[15] \u2014\u2014, \u201cThe stc system for the chime-6 challenge,\u201d in CHiME 2020 Workshop on Speech Processing in Everyday Environments, 2020.\n[16] W. Wang, X. Qin, M. Cheng, Y. Zhang, K. Wang, and M. Li, \u201cThe dku-smiip diarization system for the voxceleb speaker recognition challenge 2022,\u201d in Voxsrc Workshop, 2022.\n[17] C.-Y. Cheng, H.-S. Lee, Y. Tsao, and H.-M. Wang, \u201cMulti-target extractor and detector for unknown-number speaker diarization,\u201d arXiv preprint arXiv:2203.16007, 2022.\n[18] D. Wang, X. Xiao, N. Kanda, T. Yoshioka, and J. Wu, \u201cTarget speaker voice activity detection with transformers and its integration with end-to-end neural diarization,\u201d arXiv preprint arXiv:2208.13085, 2022.\n[19] M. Cheng, W. Wang, Y. Zhang, X. Qin, and M. Li, \u201cTargetspeaker voice activity detection via sequence-to-sequence prediction,\u201d arXiv preprint arXiv:2210.16127, 2022.\n[20] Y. Jiang, R. Tao, Z. Pan, and H. Li, \u201cTarget active speaker detection with audio-visual cues,\u201d arXiv preprint arXiv:2305.12831, 2023.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.\n[22] A. Woodward, C. Bonn\u0131\u0301n, I. Masuda, D. Varas, E. Bou-Balust, and J. C. Riveiro, \u201cConfidence measures in encoder-decoder models for speech recognition.\u201d in INTERSPEECH, 2020, pp. 611\u2013 615.\n[23] \u201c2000 nist speaker recognition evaluation,\u201d 2000. [Online]. Available: https://catalog.ldc.upenn.edu/LDC2001S97\n[24] S. Horiguchi, Y. Fujita, S. Watanabe, Y. Xue, and P. Garcia, \u201cEncoder-decoder based attractors for end-to-end neural diarization,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 1493\u20131507, 2022."
        }
    ],
    "title": "Attention-based Encoder-Decoder Network for End-to-End Neural Speaker Diarization with Target Speaker Attractor",
    "year": 2023
}