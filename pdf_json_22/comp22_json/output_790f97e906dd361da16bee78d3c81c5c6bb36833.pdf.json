{
    "abstractText": "With the rising industrial attention to 3D virtual modeling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and manipulation. Experimental results show that Diffusion-SDF generates both higher quality and more diversified 3D shapes that conform well to given text descriptions when compared to previous approaches. Code is available at: https: //github.com/ttlmh/Diffusion-SDF. \u2020Corresponding author.",
    "authors": [
        {
            "affiliations": [],
            "name": "Muheng Li"
        },
        {
            "affiliations": [],
            "name": "Yueqi Duan"
        },
        {
            "affiliations": [],
            "name": "Jie Zhou"
        },
        {
            "affiliations": [],
            "name": "Jiwen Lu"
        }
    ],
    "id": "SP:82376e06a2a0c5b6c36e65c368b54936a373f60e",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Olga Diamanti",
                "Ioannis Mitliagkas",
                "Leonidas J. Guibas"
            ],
            "title": "Learning representations and generative models for 3D point clouds",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Panos Achlioptas",
                "Leonidas J. Guibas",
                "Noah D. Goodman",
                "Judy Fan",
                "Robert X.D. Hawkins"
            ],
            "title": "Shapeglot: Learning language for shape differentiation",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Rohan Chabra",
                "Jan Eric Lenssen",
                "Eddy Ilg",
                "Tanner Schmidt",
                "Julian Straub",
                "Steven Lovegrove",
                "Richard A. Newcombe"
            ],
            "title": "Deep local shapes: Learning local SDF priors for detailed 3D reconstruction",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Paramanand Chandramouli",
                "Kanchana Vaishnavi Gandikota"
            ],
            "title": "Ldedit: Towards generalized text guided image manipulation via latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Angel X. Chang",
                "Thomas A. Funkhouser",
                "Leonidas J. Guibas",
                "Pat Hanrahan",
                "Qi-Xing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su",
                "Jianxiong Xiao",
                "Li Yi",
                "Fisher Yu"
            ],
            "title": "ShapeNet: An information-rich 3D model repository",
            "year": 2015
        },
        {
            "authors": [
                "Kevin Chen",
                "Christopher B. Choy",
                "Manolis Savva",
                "Angel X. Chang",
                "Thomas A. Funkhouser",
                "Silvio Savarese"
            ],
            "title": "Text2Shape: Generating shapes from natural language by learning joint embeddings",
            "venue": "In ACCV,",
            "year": 2018
        },
        {
            "authors": [
                "Nanxin Chen",
                "Yu Zhang",
                "Heiga Zen",
                "Ron J. Weiss",
                "Mohammad Norouzi",
                "William Chan"
            ],
            "title": "Wavegrad: Estimating gradients for waveform generation",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Hao Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Christopher B. Choy",
                "Danfei Xu",
                "JunYoung Gwak",
                "Kevin Chen",
                "Silvio Savarese"
            ],
            "title": "3D-R2N2: A unified approach for single and multi-view 3D object reconstruction",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "\u00d6zg\u00fcn \u00c7i\u00e7ek",
                "Ahmed Abdulkadir",
                "Soeren S. Lienkamp",
                "Thomas Brox",
                "Olaf Ronneberger"
            ],
            "title": "3D U-Net: Learning dense volumetric segmentation from sparse annotation",
            "venue": "In MICCAI,",
            "year": 2016
        },
        {
            "authors": [
                "Florinel-Alin Croitoru",
                "Vlad Hondru",
                "Radu Tudor Ionescu",
                "Mubarak Shah"
            ],
            "title": "Diffusion models in vision: A survey",
            "year": 2022
        },
        {
            "authors": [
                "Angela Dai",
                "Charles Ruizhongtai Qi",
                "Matthias Nie\u00dfner"
            ],
            "title": "Shape completion using 3D-encoder-predictor CNNs and shape synthesis",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Quinn Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yueqi Duan",
                "Haidong Zhu",
                "He Wang",
                "Li Yi",
                "Ram Nevatia",
                "Leonidas J. Guibas"
            ],
            "title": "Curriculum deepsdf",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Gao",
                "Tianchang Shen",
                "Zian Wang",
                "Wenzheng Chen",
                "Kangxue Yin",
                "Daiqing Li",
                "Or Litany",
                "Zan Gojcic",
                "Sanja Fidler"
            ],
            "title": "GET3D: A generative model of high quality 3D textured shapes learned from images. 2022",
            "year": 2022
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron C. Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "Commun. ACM,",
            "year": 2020
        },
        {
            "authors": [
                "Thibault Groueix",
                "Matthew Fisher",
                "Vladimir G. Kim",
                "Bryan C. Russell",
                "Mathieu Aubry"
            ],
            "title": "A papier-m\u00e2ch\u00e9 approach to learning 3D surface generation",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Moritz Ibing",
                "Isaak Lim",
                "Leif Kobbelt"
            ],
            "title": "3D shape generation with grid-based implicit functions",
            "venue": "In CVPR, pages 13559\u201313568,",
            "year": 2021
        },
        {
            "authors": [
                "Chiyu \u201dMax\u201d Jiang",
                "Avneesh Sud",
                "Ameesh Makadia",
                "Jingwei Huang",
                "Matthias Nie\u00dfner",
                "Thomas A. Funkhouser"
            ],
            "title": "Local implicit grid representations for 3D scenes",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Yue Jiang",
                "Dantong Ji",
                "Zhizhong Han",
                "Matthias Zwicker"
            ],
            "title": "SDFDiff: Differentiable rendering of signed distance fields for 3D shape optimization",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Hiroharu Kato",
                "Yoshitaka Ushiku",
                "Tatsuya Harada"
            ],
            "title": "Neural 3D mesh renderer",
            "venue": "In CVPR, pages 3907\u20133916,",
            "year": 2018
        },
        {
            "authors": [
                "Gwanghyun Kim",
                "Taesung Kwon",
                "Jong Chul Ye"
            ],
            "title": "Diffusionclip: Text-guided diffusion models for robust image manipulation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Ke Li",
                "Jitendra Malik"
            ],
            "title": "Implicit maximum likelihood estimation. 2018",
            "year": 2018
        },
        {
            "authors": [
                "Zhengzhe Liu",
                "Yi Wang",
                "Xiaojuan Qi",
                "Chi-Wing Fu"
            ],
            "title": "Towards implicit text-guided 3D shape generation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andr\u00e9s Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting 9 using denoising diffusion probabilistic models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Shitong Luo",
                "Wei Hu"
            ],
            "title": "Diffusion probabilistic models for 3D point cloud generation",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Lars M. Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger"
            ],
            "title": "Occupancy networks: Learning 3D reconstruction in function space",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Oscar Michel",
                "Roi Bar-On",
                "Richard Liu",
                "Sagie Benaim",
                "Rana Hanocka"
            ],
            "title": "Text2mesh: Text-driven neural stylization for meshes",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Paritosh Mittal",
                "Yen-Chi Cheng",
                "Maneesh Singh",
                "Shubham Tulsiani"
            ],
            "title": "Autosdf: Shape priors for 3D completion, reconstruction and generation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Charlie Nash",
                "Yaroslav Ganin",
                "S.M. Ali Eslami",
                "Peter W. Battaglia"
            ],
            "title": "Polygen: An autoregressive generative model of 3D meshes",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard A. Newcombe",
                "Steven Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3D classification and segmentation",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Li Yi",
                "Hao Su",
                "Leonidas J. Guibas"
            ],
            "title": "Pointnet++: Deep hierarchical feature learning on point sets in a metric space",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical text-conditional image generation with CLIP latents",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed",
                "Daan Wierstra"
            ],
            "title": "Stochastic backpropagation and approximate inference in deep generative models",
            "venue": "In ICML,",
            "year": 2014
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U- Net: Convolutional networks for biomedical image segmentation",
            "venue": "In MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S. Sara Mahdavi",
                "Rapha Gontijo Lopes",
                "Tim Salimans",
                "Jonathan Ho",
                "David J. Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Sanghi",
                "Hang Chu",
                "Joseph G. Lambourne",
                "Ye Wang",
                "Chin-Yi Cheng",
                "Marco Fumero",
                "Kamal Rahimi"
            ],
            "title": "Malekshan. Clip-forge: Towards zero-shot text-to-shape generation",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric A. Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jiajun Wu",
                "Chengkai Zhang",
                "Tianfan Xue",
                "Bill Freeman",
                "Josh Tenenbaum"
            ],
            "title": "Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Zhirong Wu",
                "Shuran Song",
                "Aditya Khosla",
                "Fisher Yu",
                "Linguang Zhang",
                "Xiaoou Tang",
                "Jianxiong Xiao"
            ],
            "title": "3D ShapeNets: A deep representation for volumetric shapes",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Qiangeng Xu",
                "Weiyue Wang",
                "Duygu Ceylan",
                "Radom\u0131\u0301r Mech",
                "Ulrich Neumann"
            ],
            "title": "DISN: deep implicit surface network for high-quality single-view 3D reconstruction",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Xinchen Yan",
                "Jimei Yang",
                "Ersin Yumer",
                "Yijie Guo",
                "Honglak Lee"
            ],
            "title": "Perspective transformer nets: Learning singleview 3D object reconstruction without 3D supervision",
            "venue": "In NeurIPS,",
            "year": 2016
        },
        {
            "authors": [
                "Guandao Yang",
                "Xun Huang",
                "Zekun Hao",
                "Ming-Yu Liu",
                "Serge J. Belongie",
                "Bharath Hariharan"
            ],
            "title": "Pointflow: 3D point cloud generation with continuous normalizing flows",
            "venue": "In ICCV,",
            "year": 2019
        },
        {
            "authors": [
                "Xumin Yu",
                "Yongming Rao",
                "Ziyi Wang",
                "Zuyan Liu",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Pointr: Diverse point cloud completion with geometry-aware transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohui Zeng",
                "Arash Vahdat",
                "Francis Williams",
                "Zan Gojcic",
                "Or Litany",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "LION: latent point diffusion models for 3D shape generation",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "\u2020Corresponding author."
        },
        {
            "heading": "1. Introduction",
            "text": "Exploring data representations for 3D shapes has been a fundamental and critical issue in 3D computer vision. Explicit 3D representations including point clouds [38, 39], polygon meshes [17, 24] and occupancy voxel grids [9, 53] have been widely applied in various 3D downstream applications [1, 35, 56]. While explicit 3D representations achieve encouraging performance, there are some primary limitations including not being suitable for generating watertight surfaces (e.g. point clouds), or being subject to topological constraints (e.g. meshes). On the other hand, implicit 3D representations have been widely studied more recently [3, 14, 37], with representative works including DeepSDF [37], Occupancy Network [32] and IM-Net [8]. In general, implicit functions encode the shapes by the isosurface of the function, which is a continuous field and can be evaluated at arbitrary resolution.\nIn recent years, numerous explorations have been conducted for implicit 3D generative models, which show promising performance on several downstream applications such as single/multi-view 3D reconstruction [23, 54] and shape completion [12, 34]. Besides, several studies have also explored the feasibility of directly generating novel 3D shapes based on implicit representations [15, 21]. However, these approaches are incapable of generating specified 3D shapes that match a given condition, e.g. a short text describing the shape characteristics as shown in Figure 1.\nar X\niv :2\n21 2.\n03 29\n3v 2\nText-based visual content synthesis has the advantages of the flexibility and generality [41, 42]. Users may generate rich and diverse 3D shapes based on easily obtained natural language descriptors. In addition to generating 3D shapes directly based on text descriptions, manipulating 3D data with text guidance can be further utilized for iterative 3D synthesis and fine-grained 3D editing, which can be beneficial for non-expert users to create 3D visual content.\nIn the literature, there have been few attempts on the challenging task of text-to-shape generation based on implicit 3D representations [29, 34, 48]. For example, AutoSDF [34] introduced a vector quantized SDF autoencoder together with an autoregressive generator for shape generation. While encouraging progress has been made, the quality and diversity of generated shapes still requires improvement. The current approaches struggle to generate highly diversified 3D shapes that both guarantee generation quality and conform to the semantics of the input text. Motivated by the success of denoising diffusion models in 2D image [13, 19, 36] and even explicit 3D point cloud [31, 58, 59] generation, we find that DMs achieve high-quality and highly diversified generation while being robust to model training. To this end, we aim to design an implicit 3D representation-based generative diffusion process for text-to-shape synthesis that can achieve better generation flexibility and generalization performance.\nIn this paper, we propose the Diffusion-SDF framework for text-to-shape synthesis based on truncated signed distance fields (TSDFs). Considering that 3D shapes share structural similarities at local scales, and the cubic data volume of 3D voxels may lead to slow sampling speed for diffusion models, we propose a two-stage separated generation pipeline. First, we introduce a patch-based SDF autoencoder that map the original signed distance fields into patchindependent local Gaussian latent representations. Second, we introduce the Voxelized Diffusion model that captures the intra-patch information along with both patch-topatch and patch-to-global relations. Specifically, we design a novel UinU-Net architecture to replace the standard U-Net [46] for the noise estimator in the reverse process. UinU-Net implants a local-focused inner network inside the outer U-Net backbone, which takes into account the patch-independent prior of SDF representations to better reconstruct local patch features from noise. Our work digs deeper into the further potential of diffusion model-based approaches towards text-conditioned 3D shape synthesis based on voxelized TSDFs. Experiments on the largest existing text-shape dataset [6] show that our Diffusion-SDF approach achieves promising generation performance on text-to-shape tasks compared to existing state-of-the-art approaches, in both qualitative and quantitative evaluations.\nStrictly speaking, our approach employs a combined explicit-implicit representation in the form of voxelized signed distance fields."
        },
        {
            "heading": "2. Related Work",
            "text": "Text-conditioned generative 3D models. In the field of generative 3D modeling, a variety of works have focused on synthesizing 3D visual content unconditionally [1, 32, 52, 56] or conditioned on single/multi-view images [9, 34, 54, 55]. Besides, there has also been a series of works concentrating on the challenging task of text-to-shape generation [6]. Some of them adopted purely explicit 3D data representation-based methods to generate 3D shapes conditioned on input text [6, 33, 58]. In contrast, our focus lies on synthesizing 3D shapes based on implicit 3D data representations. So far, there have also been few existing works focusing on the task of implicit text-to-shape generation [29, 34, 48]. All of these approaches have yielded impressive generation results, but there are still some remained issues to be addressed. Sanghi et al. [48] proposed a normalizing flow [43]-based approach to generate shape voxels using implicit Occupancy Networks [32], and Liu et al. [29] introduced a shape-IMLE [28] generator using implicit IMNET [8] decoder, while these approaches neither employ the more flexible implicit SDFs as data representation nor take into account the constraints for local shape structures. More similar to our work, AutoSDF [34] introduced an autoregressive prior for 3D shape generation based on a discretized SDF autoencoder. The autoregressive model adopts a relatively unnatural way to predict patched 3D tokens in a sequential manner that loses the 3D-specific inductive bias and is also relatively inefficient.\nDiffusion Probabilistic Models. Diffusion Probabilistic Models (DPMs) [19, 49], also known as diffusion models, have currently arisen as a powerful family of generative models. Compared to previous state-of-the-art generative models, including Generative Adversarial Network (GAN) [16], Variational Autoencoder (VAE) [26], and flowbased generative models [43], the diffusion model demonstrates its superiority in both training stability and generative diversity [11]. Diffusion models have achieved promising performance on image [13,19,36,45] and speech [7,27] synthesis. Especially, fairly impressive results have been achieved with diffusion model-based approaches on the task of text-to-image synthesis [41, 45, 47]. In the field of 3D computer vision, several studies have adopted diffusion models for generative 3D modeling [31, 58, 59]. PVD [59] employed diffusion models to generate 3D shapes based on point-voxel 3D representation. Luo et al. [31] treated points in point clouds as particles in a thermodynamic system with a heat bath. LION [58] introduced a VAE framework with hierarchical diffusion models in latent space. All these approaches have focused on the diffusion process of explicit 3D data representations for shape generation. On the contrary, our work tries to explore the feasibility of diffusion models on implicit 3D data representations."
        },
        {
            "heading": "3. Method",
            "text": "In this section, we introduce the methodology design for Diffusion-SDF. In general, we propose a two-stage pipeline as illustrated in Figure 2. Detailed information of each stage will be included in the following sections."
        },
        {
            "heading": "3.1. Autoencoding Signed Distance Fields",
            "text": "Signed distance fields (SDF) belong to a type of implicit 3D data representation which assigns the scalar signed distance value to the shape surface for each point p in the 3D space R3. Our objective is to generate the signed distance fields for the target 3D shapes that match the given text conditions. Specifically, the 3D data are represented by truncated signed distance functions (TSDF) in a regularly-spaced voxel grid as [3, 22]. Generating voxelized SDFs directly from denoising diffusion models is both cost-expensive and time-consuming. In addition, the local structural information of 3D shapes is not wellemphasized through direct voxel-based generation. To address this, we propose a patch-wise autoencoder to learn the latent representations for voxelized signed distance fields.\nGiven a 3D shape, we first sample its truncated signed distance field x as a voxel grid of size D3. Before the shape x is encoded, it is first split into N local patches of size P 3, producing a sequence of shape patches Xp = [xp1, ..., xpN ], where N = (D/P )3 is the resulting number of patches. Then, the local shape encoder Eloc encodes each shape patches into latent representations Zp = [zp1, ..., zpN ], where zpn = Eloc(xpn) \u2208 Rc, and c is the number of latent channels. Here, since each patch is encoded independently, the local structural information can be\nexplored well through the local shape encoder. Meanwhile, the input data scale can also be downsampled by the factor P . However, when reconstructing the shape from local patches, it is also necessary to consider the spatial location of each patch in the global shape, and the interrelationship between adjacent patches. To preserve both patch-to-global and patch-to-patch information while decoding the latent embeddings, the patch embeddings are first rearranged into a voxel grid embedding z, and then sent into the patch-joint decoder D that reconstructs the SDF field from the latent patches, giving x\u0303 = D(z).\nIn detail, we adopt a VAE [26, 44]-like autoencoder, which encodes each shape patch into a normal distribution. We utilize a combination of both L1 reconstruction loss together with the KL-regularization loss at the training stage for the SDF autoencoder. The latter one forces a mild KL-penalty towards a standard normal distribution on each learned patch latent. The final SDF representations will be in the form of patch-independent Gaussian distributions."
        },
        {
            "heading": "3.2. Voxelized Diffusion Models (VDMs)",
            "text": "Diffusion models (DMs) [19, 49] are a class of probabilistic generative models that learn to fit a certain distribution by gradually denoising a Gaussian variable through a fixed Markov Chain of length T . Given a data sample x0 \u223c q(x0), DMs describe two different processes in the opposite direction: a forward process q (x0:T ) that gradually transform a data sample into pure Gaussian noise, and a reverse process p\u03b8 (x0:T ) that gradually denoise a pure\nGaussian sample into real data,\nq (x0:T ) = q (x0)\u03a0 T t=1q (xt|xt\u22121) ,\np\u03b8 (x0:T ) = p (xT )\u03a0 T t=1p\u03b8 (xt\u22121|xt) ,\n(1)\nwhere q (xt|xt\u22121) and p\u03b8 (xt\u22121|xt) are both Gaussian transition probabilities in the forms of\nq (xt|xt\u22121) = N ( xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI ) ,\np\u03b8 (xt\u22121|xt) = N (xt\u22121;\u00b5\u03b8 (xt, t) , \u03b2tI) . (2)\nBased on [19], the mean variable \u00b5\u03b8 (xt, t) for the reverse transition p\u03b8 (xt\u22121|xt) can be expressed in the form of\n\u00b5\u03b8 (xt, t) = 1\n\u221a \u03b1t\n( xt \u2212\n\u03b2t\u221a 1\u2212 \u03b1\u0304t \u03f5\u03b8 (xt, t)\n) , (3)\nwhere \u03b1t = 1 \u2212 \u03b2t, \u03b1\u0304t = \u03a0ti=1\u03b1i, and \u03b2t decreases to 0 gradually as t approaches 0. The evidence lower bound (ELBO) is maximized at the training stage of DMs, eventually leading to the loss function as the form of\nLDM = Ex,t,\u03f5\u223cN (0,1) [ \u2225\u03f5\u2212 \u03f5\u03b8 (xt, t)\u22252 ] , (4)\nwhere xt = \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, \u03f5 is a noise variable, and t is uniformly sampled from {1, ..., T}. The neural network-based score estimator \u03f5\u03b8 (xt, t) is the core function of denoising diffusion models, which is implemented as a timestep-conditioned denoising autoencoder in [19]. UinU-Net Architecture. In our scenario, we are designing a novel diffusion-based architecture to generate latent SDF representations with regard to the target text conditions. The corresponding TSDF fields can be then reconstructed through the patch-joint decoder which is pre-trained in the first stage. As we mentioned in the above section, the key part of generative diffusion models is to estimate the function approximator \u03f5\u03b8 from the data distribution of the training set. Since our model is designed to generate latent samples, the score estimator can be expressed as \u03f5\u03b8 (zt, t), as the loss function becomes\nLDiffusion\u2212SDF = Ez,t,\u03f5\u223cN (0,1) [ \u2225\u03f5\u2212 \u03f5\u03b8 (zt, t)\u22252 ] .\n(5) To incorporate the 3D positional prior of different latent shape patches, we adopt a 3D U-Net [10]-based autoencoder as the neural function approximator \u03f5\u03b8 that directly learns to denoise the voxel grid embedding zt that is obtained from the pre-trained local shape encoder Eloc. Besides, another crucial prior for our design is the patch-based encoding process, through which all the shape patches are encoded into independent Gaussian distributions. Thus, when generating the local shape embeddings, an important point is to recover the independent distribution for\neach shape patch as well. To address this, we propose a novel UinU-Net architecture for the implementation of the autoencoder-based neural score estimator as shown in Figure 3. The standard 3D U-net [10] adopts a series of 3 \u00d7 3 \u00d7 3 convolutional layers to construct a downsampling-upsampling network architecture with the information shared through skip connections. This design takes into account the patch-to-patch and patch-to-global features via hierarchical receptive fields. Given that our input data are compressed 3D latent representations, and each patch is encoded independently, we propose to implant another inner network inside the outer U-Net architecture where the embedded resolution is equal to the original latent resolution D/P . Specifically, we adopt a 1 \u00d7 1 \u00d7 1 convolution-based ResNet [18] structure to learn independent patch-focused information. We also introduce the spatial Transformer [51] network that accepts positional embedded patch representations as input tokens, following self-attention layers to capture the relational information between independent local patch embeddings. The inner synthesis paths are skip-connected to the outer ones, ensuring information transmission from inner to outer networks. This architecture is designed to capture the intra-patch information along with both patch-to-patch and patch-toglobal relations. Text-Guided Shape Generation So far, we have discussed the generative process without text conditions. To synthe-\nsize latent SDF representations based on given conditions, we introduce a conditioning mechanism based on classifierfree guidance diffusion [20]. We adopt a text-conditioned score estimator. In detail, to fit the target conditions into the 3D denoising autoencoder, we utilize the cross-attention mechanism as proposed in [45]. The input caption is first encoded as text embeddings through a pre-trained text encoder \u03c4 . Thus, the conditioned score can be formed as \u03f5\u03b8 (zt|\u03c4(c)), where c is the input caption. Meanwhile, we also obtain an unconditioned score \u03f5\u03b8 (zt|\u2205) with an empty sequence as input caption. Within the guided diffusion sampling process, the output of the model is extrapolated further in the direction of \u03f5\u03b8(zt|\u03c4(c)) and away from \u03f5\u03b8(zt|\u2205) as:\n\u03f5\u0302\u03b8 (zt|\u03c4(c)) = \u03f5\u03b8 (zt|\u2205) + s \u00b7 (\u03f5\u03b8 (zt|\u03c4(c))\u2212 \u03f5\u03b8 (zt|\u2205)) , (6) where s \u2265 1 refers to the guidance scale. Text-Guided Shape Completion Current shape completion approaches mainly focused on recovering the full shapes from partial input shapes or single view images [1, 57, 59]. Besides, we have explored the feasibility of textconditioned shape completion. According to the patchbased design of our SDF encoder, information for the given shape patches can be encoded even if some of the patches are missing. Motivated by the success of diffusion modelbased image inpainting [30, 45], we introduce a maskdiffusion strategy to generate the missing shape based on a pre-trained VDM. The core idea is to incorporate known information into each denoising step of the reverse process. For a full shape representation, we transform it into a partial shape representation given the mask mz . For each estimation step zt\u22121, we combine the estimated result through the reverse process z\u0303t\u22121 with the forward sampling result of the unmasked patches z\u0302t\u22121,\nzt\u22121 = (1\u2212m)\u2299 z\u0303t\u22121 +m\u2299 z\u0302t\u22121, (7)\nwhere \u2299 refers to element-wise product, and\nz\u0303t\u22121 \u223c N (\u00b5\u03b8 (zt, t, c) , \u03b2tI) z\u0302t\u22121 = \u221a \u03b1\u0304tz0 + \u221a 1\u2212 \u03b1\u0304t\u03f5.\n(8)\nBy such a strategy, the unknown region can be recovered based on both the given shape and text conditions. Text-Guided Shape Manipulation Visual content editing is one of the most valuable applications for vision-language generation. We propose a diffusion-based text-guided shape manipulation approach. For a given shape representation zinit, our goal is to transform it into another shape representation zgoal based on text c. Inspired by image manipulation techniques [4, 25], we use a cycle-sampling strategy based on a pre-trained VDM. Firstly, we forward-sample the given shape zinit for tmid steps, where 0 < tmid < T . This operation will lead to an intermediate output zmid\nzmid = \u221a \u03b1\u0304tmidzinit + \u221a 1\u2212 \u03b1\u0304tmid\u03f5. (9)\nThen, the reverse process will start from zmid, and another tmid denoising steps will be conducted conditioned on c. Such a design will make the generated shapes correspond to the target text descriptions while maintaining the original shape characteristics."
        },
        {
            "heading": "4. Experiments",
            "text": "We extensively experimented with our proposed Diffusion-SDF on various text-conditioned 3D shape synthesis tasks, such as text-to-shape generation, textconditioned shape completion, and text-guided shape manipulation. The following subsections describe details of experimental settings, results, and analyses. Dataset. We mainly trained and evaluated our approach on the current largest text-shape dataset Text2Shape (T2S) [6]. T2S gathered data in the form of shape-text pairs based on two object classes (chairs, tables) in ShapeNet [5]. T2S contains about 75K shape-text pairs in total (\u223c30K for chairs, \u223c40K for tables), with an average of \u223c16 words per description. T2S was originally designed to contain both color and shape information within the text descriptions, while we mainly focus on text-shape correspondence. Implementation details. For the training stage of the SDF autoencoder, to improve the model\u2019s generalization ability, we trained the autoencoder across the 13 categories of ShapeNet [5] dataset. For the input data, we sampled the voxelized SDF from original shapes with 64\u00d7 64\u00d7 64 grid points, which is also the data size adopted for follow-up operations. For the diffusion stage, to facilitate the generative sampling speed, we adopt the DDIM [50] sampler to reduce the original DDPM sampling steps from T = 1000 to 50. For the conditioning mechanism, we adopt a freeze CLIP [40] text embedder as text encoder \u03c4 ."
        },
        {
            "heading": "4.1. Text-Conditioned Shape Generation",
            "text": "The most immediate application of our approach involves generating novel shapes that are conditioned on textual descriptions. To showcase the efficacy of our approach, we conduct a comparative evaluation of our generation results against two state-of-the-art supervised text-to-shape synthesis methods that are based on implicit 3D representations [29, 34]. Besides, there are some other works regarding text-to-shape generation, which have different settings compared to our approach. For instance, [48] proposed a zero-shot shape generation approach conditioned on categorical texts, and [15, 33] proposed to generate textured shapes based on provided/pre-generated meshes. As a result, these prior works are not directly compared against our proposed approach. Quantitative comparison. To compare the generation performance of our approach to the previous methods quantitatively, we propose to use several metrics to evaluate the generated results:\n- IoU (Intersection over Union) measures the occupancy similarity between the generated shape to the ground truth. To compute the IoU score, we downsample all the generated SDFs to occupancy voxel grids of size 323. IoU metric is used to measure the conformity of the generated samples with the ground truth shapes.\n- Acc (Classification Accuracy) introduces a voxel-based classifier, which is pre-trained to classify the 13 categories in ShapeNet [5] according to [48]. Acc metric is used to measure the semantic authenticity of the generated samples.\n- CLIP-S (CLIP Similarity Score) introduces the pretrained vision-language model CLIP [40] for further evaluation. The CLIP can be used to measure the correspondence between given images and texts. We render 5 view images for each generated shape, and compute the cosine similarity score between rendered images and the given text. The highest score is reported for each text query. CLIP-S metric is used to measure the semantic conformance between the generated samples and input texts.\n- TMD (Total Mutual Difference). For each given text description, we generate k = 10 different samples. Then, we compute the average IoU score for each generated shape to other k \u2212 1 shapes. The average IoU score for all text queries is reported. TMD assesses the generation diversity for each given text query.\nTo keep consistency with previous approaches, we limit the comparisons to the chair category in Text2Shape [6] dataset. Since the officially released AutoSDF [34] model was trained on another dataset [2], we re-trained the model following the original settings on the training set of\nText2Shape for the fair comparison. The results are shown in Table 1. From the results, it can be seen that our approach is able to achieve relatively high generation quality with high IoU and Acc scores, and robust text-shape conformance with good CLIP-S performance, while achieving much better generation diversity with a much lower TMD performance. Qualitative results. The quantitative experiment shows the generation performance of our approach on the limited given subset from Text2Shape dataset. Due to the generalization of natural language, our approach is capable of generating different forms of shapes based on various text descriptions. Figure 4 shows the generated results of Diffusion-SDF from different text inputs. The results show that our method can generate eligible results from distinguished conditions. Besides, our method is capable of synthesizing various shape outputs from the same input text description. Figure 5 compares our approach with [29, 34] based on same text queries. The results have shown that our method has a great advantage in the diversity of generated samples, while the previous approaches are unable to generate highly-diversified shapes."
        },
        {
            "heading": "4.2. Ablation Studies",
            "text": "We have conducted extensive ablation studies to demonstrate the effectiveness of our special architecture design for Diffusion-SDF.\nEffectiveness of the UinU-Net architecture. In order to validate the benefit of our proposed modifications to the conventional U-Net architecture, we additionally compare our proposed UinU-Net architecture to the simpler U-Net architecture used in [45]. The quantitative results for textto-shape generation are shown in Table 2. Qualitative comparisons are shown in Figure 6. From the results, it can be found that our proposed UinU-Net architecture for Voxelized Diffusion models generally improves the quality of generated shapes. With the UinU-Net architecture, the noise for each step is estimated both patch-mutually and patch-independently. Since the shape representations are repositioned based on separate patch embeddings extracted by the SDF encoders, the key to recovering the authentic shape SDFs from the joint decoder is to ensure that the generated SDF representations are also distributed independently in patches. From the qualitative results, it can be inferred that the inner network is conducive to recovering the independently distributed patch representations.\nInner network architecture. As for the UinU-Net architecture, we have conducted ablation experiments on several variants for the inner network to validate the effectiveness of the final design. The results have shown in Table 3. According to the results, it can be inferred that the inner outer concatenation mechanism and spatial attention layer can generally improve the generation quality. Without the inner-outer\nconcatenation mechanism, the accuracy score and CLIP-S have significant decreases, indicating that the information transmission mechanism has affected the semantic authenticity and conformance of the generated shapes. Accuracy score has dropped without the spatial attention module, indicating that the introduction of patch-to-patch information helps preserve the full shape semantics while recovering the patch-wise representations."
        },
        {
            "heading": "4.3. Text-Guided Shape Completion",
            "text": "In this section, we validate the effectiveness of our approach on the text-guided shape completion task. Specifically, given a partial input shape, our goal is to generate the missing part conditioned on the input text descriptions. For\ninstance, we can generate the body of a chair based on the given chair legs or generate the missing chair legs based on the chair body. The qualitative results of our approach are displayed in Figure 7. The results show that our approach is capable of generating the missing part of a shape while being well-blended with the given shape, as evident in the generated examples such as revolving throne, revolving toilet, and revolving sofa. Moreover, our completion performance is not limited by the shape geometry in the training set. The core limitation of our approach is the cut region of patch grids, which can be addressed by increasing resolution and decreasing patch size."
        },
        {
            "heading": "4.4. Text-Guided Shape Manipulation",
            "text": "We demonstrate our approach\u2019s effectiveness for textguided shape manipulation. Our approach can modify a given shape to match an instructive text description. Figure 8 gives several qualitative illustrations for our proposed approach. From the results, it can be summarized that our approach can handle the shape manipulation of both local shape structures (e.g. the existence/style of arms or legs) or the overall shape characteristics (e.g. sofa or straight chair). In addition, one key issue for our designed approach is the choice for tmid. Since the shape characteristics usually form in the early stage of the reverse process, thus we set tmid to relatively larger step numbers (600\u223c800 step before DDIM\u2019s down-sampling). In experiments, we observe that excessively small or large values for tmid result in unchanged or destructed original shape features, respectively."
        },
        {
            "heading": "5. Limitations and Conclusion",
            "text": "Limitations. This paper demonstrates the superiority of our approach on the Text2Shape [6] dataset, which is limited to only two categories from ShapeNet, restricting our approach\u2019s generalization to other categories. Moreover, the lack of current shape-text datasets prevents us from validating our approach on additional benchmarks. To address this, we propose introducing more datasets and exploring zero-shot text-to-shape generation by leveraging knowledge from 2D vision-language models. While some studies have attempted to tackle this challenge [48], achieving flexible text-to-shape generation remains a significant obstacle. Our future work will seek a possible solution to this problem.\nConclusion. Our paper presents a framework for textto-shape synthesis called Diffusion-SDF, which utilizes a diffusion model to generate voxelized SDFs conditioned on texts. The approach comprises a two-stage pipeline: a patch-wise autoencoder for generating Gaussian SDF representations, followed by a Voxelized Diffusion model with UinU-Net denoisers for generating patch-independent SDF representations. We evaluate our approach on various textto-shape synthesis tasks, and the results demonstrate that it can generate highly diverse and high-quality 3D shapes. Acknowledgments This work was supported in part by the National Key Research and Development Program of China under Grant 2017YFA0700802, in part by the National Natural Science Foundation of China under Grant 62206147 and Grant 62125603, and in part by a grant from the Beijing Academy of Artificial Intelligence (BAAI)."
        },
        {
            "heading": "A. Qualitative Illustrations",
            "text": "In this section, we provide more qualitative illustrations for the generation results based on our proposed DiffusionSDF approach. Figure 9 shows some extra diversified generated samples for text-conditioned shape generation. Figure 10 displays further text-conditioned shape completion results based on different input shapes. Figure 11 illustrates several text-conditioned shape manipulation results from diverse initial shapes. The extra qualitative illustrations are displayed at the end of this paper."
        }
    ],
    "title": "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion",
    "year": 2023
}