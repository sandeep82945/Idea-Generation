{
    "abstractText": "nonlinear phase-field dynamics Elham Kiyani1,2, Steven Silber2,3, Mahdi Kooshkbaghi4, Mikko Karttunen2,3,5 1Department of Mathematics, The University of Western Ontario, 1151 Richmond Street, London, ON N6A 5B7 Canada 2The Centre for Advanced Materials and Biomaterials (CAMBR), The University of Western Ontario, 1151 Richmond Street, London, ON N6A 5B7 Canada 3Department of Physics and Astronomy, The University of Western Ontario, 1151 Richmond Street, London, ON N6A 3K7 Canada 4Simons Center for Quantitative Biology, Cold Spring Harbor Laboratory, Cold Spring Harbor, NY, USA 5Department of Chemistry, The University of Western Ontario, 1151 Richmond Street, London, ON N6A 5B7 Canada April 1, 2022",
    "authors": [
        {
            "affiliations": [],
            "name": "Elham Kiyani"
        },
        {
            "affiliations": [],
            "name": "Steven Silber"
        },
        {
            "affiliations": [],
            "name": "Mahdi Kooshkbaghi"
        },
        {
            "affiliations": [],
            "name": "Mikko Karttunen"
        }
    ],
    "id": "SP:b8ba783051dc901e34810c3eddfadaf4145e7ac1",
    "references": [
        {
            "authors": [
                "J. Han",
                "A. Jentzen"
            ],
            "title": "W",
            "venue": "E, Solving high-dimensional partial differential equations using deep learning, Proc. Natl. Acad. Sci. USA 115 ",
            "year": 2018
        },
        {
            "authors": [
                "I. Lagaris",
                "A. Likas",
                "D. Fotiadis"
            ],
            "title": "Artificial neural networks for solving ordinary and partial differential equations",
            "venue": "IEEE Trans. Neural Netw. 9 ",
            "year": 1998
        },
        {
            "authors": [
                "J. Sirignano",
                "K. Spiliopoulos"
            ],
            "title": "DGM: A deep learning algorithm for solving partial differential equations",
            "venue": "J. Comput. Phys. 375 ",
            "year": 2018
        },
        {
            "authors": [
                "C. Hur\u00e9",
                "H. Pham",
                "X. Warin"
            ],
            "title": "Some machine learning schemes for high-dimensional nonlinear PDEs",
            "venue": "Math. Comput. 89 ",
            "year": 2020
        },
        {
            "authors": [
                "W. E",
                "J. Han",
                "A. Jentzen"
            ],
            "title": "Algorithms for solving high dimensional PDEs: From nonlinear Monte Carlo to machine learning, Nonlinearity",
            "year": 2021
        },
        {
            "authors": [
                "R. Ranade",
                "C. Hill",
                "J. Pathak"
            ],
            "title": "DiscretizationNet: A machine-learning based solver for Navier\u2013Stokes equations using finite volume discretization",
            "venue": "Comput. Methods Appl. Mech. Eng. 378 ",
            "year": 2021
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "J. Comput. Phys. 378 ",
            "year": 2019
        },
        {
            "authors": [
                "G.E. Karniadakis",
                "I.G. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nat. Rev. Phys. 3 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Lu",
                "P. Jin",
                "G. Pang",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators",
            "venue": "Nat. Mach. Intell. 3 ",
            "year": 2021
        },
        {
            "authors": [
                "L. Lu",
                "X. Meng",
                "Z. Mao",
                "G.E. Karniadakis"
            ],
            "title": "DeepXDE: A deep learning library for solving differential equations",
            "venue": "SIAM Rev. 63 ",
            "year": 2021
        },
        {
            "authors": [
                "S.L. Brunton",
                "J.L. Proctor",
                "J.N. Kutz"
            ],
            "title": "Discovering governing equations from data by sparse identification of nonlinear dynamical systems",
            "venue": "Proc. Natl. Acad. Sci. 113 ",
            "year": 2016
        },
        {
            "authors": [
                "H. Schaeffer"
            ],
            "title": "Learning partial differential equations via data discovery and sparse optimization",
            "venue": "Proc. R. Soc. Math. Phys. Eng. Sci. 473 ",
            "year": 2017
        },
        {
            "authors": [
                "S. Lee",
                "M. Kooshkbaghi",
                "K. Spiliotis",
                "C.I. Siettos",
                "I.G. Kevrekidis"
            ],
            "title": "Coarse-scale PDEs from fine-scale observations via machine learning",
            "venue": "Chaos Interdiscip. J. Nonlinear Sci. 30 ",
            "year": 2020
        },
        {
            "authors": [
                "P.R. Vlachas",
                "W. Byeon",
                "Z.Y. Wan",
                "T.P. Sapsis",
                "P. Koumoutsakos"
            ],
            "title": "Data-driven forecasting of high-dimensional chaotic systems with long short-term memory networks",
            "venue": "Proc. R. Soc. Math. Phys. Eng. Sci. 474 ",
            "year": 2018
        },
        {
            "authors": [
                "F.A. Gers",
                "D. Eck",
                "J. Schmidhuber"
            ],
            "title": "Applying LSTM to time series predictable through timewindow approaches",
            "venue": "in: Neural Nets WIRN Vietri-01, Springer, Vienna, Austria",
            "year": 2002
        },
        {
            "authors": [
                "J. del \u00c1guila Ferrandis",
                "M.S. Triantafyllou",
                "C. Chryssostomidis",
                "G.E. Karniadakis"
            ],
            "title": "Learning functionals via LSTM neural networks for predicting vessel dynamics in extreme sea states",
            "venue": "Proc. R. Soc. A",
            "year": 2021
        },
        {
            "authors": [
                "Y. Bar-Sinai",
                "S. Hoyer",
                "J. Hickey",
                "M.P. Brenner"
            ],
            "title": "Learning data-driven discretizations for partial differential equations",
            "venue": "Proc. Natl. Acad. Sci. 116 ",
            "year": 2019
        },
        {
            "authors": [
                "C. Theodoropoulos",
                "E. Luna-Ortiz"
            ],
            "title": "A reduced input/output dynamic optimisation method for macroscopic and microscopic systems",
            "venue": "in: Model reduction and coarse-graining approaches for multiscale phenomena, Springer, Berlin/Heidelberg, Germany",
            "year": 2006
        },
        {
            "authors": [
                "T.N. Thiem",
                "M. Kooshkbaghi",
                "T. Bertalan",
                "C.R. Laing",
                "I.G. Kevrekidis"
            ],
            "title": "Emergent spaces for coupled oscillators",
            "venue": "Front. Comput. Neurosci. 14 ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Kim"
            ],
            "title": "Convolutional neural networks for sentence classification",
            "venue": "in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language, Association for Computational Linguistics, Doha, Qatar",
            "year": 2014
        },
        {
            "authors": [
                "D. Qin",
                "J. Yu",
                "G. Zou",
                "R. Yong",
                "Q. Zhao"
            ],
            "title": "B",
            "venue": "Zhang, A novel combined prediction scheme based on CNN and LSTM for urban PM 2.5 concentration, IEEE Access 7 ",
            "year": 2019
        },
        {
            "authors": [
                "S.M. Allen",
                "J.W. Cahn"
            ],
            "title": "Ground state structures in ordered binary alloys with second neighbor interactions",
            "venue": "Acta Metall. 20 ",
            "year": 1016
        },
        {
            "authors": [
                "J.W. Cahn",
                "J.E. Hilliard"
            ],
            "title": "Free energy of a nonuniform system",
            "venue": "I. Interfacial free energy, J. Chem. Phys. 28 ",
            "year": 1958
        },
        {
            "authors": [
                "K.R. Elder",
                "M. Katakowski",
                "M. Haataja",
                "M. Grant"
            ],
            "title": "Modeling elasticity in crystal growth",
            "venue": "Phys. Rev. Lett. 88 ",
            "year": 2002
        },
        {
            "authors": [
                "K.R. Elder",
                "M. Grant"
            ],
            "title": "Modeling elastic and plastic deformations in nonequilibrium processing using phase field crystals",
            "venue": "Phys. Rev. E 70 ",
            "year": 2004
        },
        {
            "authors": [
                "S.A. Silber",
                "M. Karttunen"
            ],
            "title": "SymPhas\u2013General purpose software for phase-field",
            "venue": "phase-field crystal, and reaction-diffusion simulations, Adv. Theory Simul. 5 ",
            "year": 2022
        },
        {
            "authors": [
                "M. Abadi",
                "A. Agarwal",
                "P. Barham",
                "E. Brevdo",
                "Z. Chen",
                "C. Citro",
                "G.S. Corrado",
                "A. Davis",
                "J. Dean",
                "M. Devin",
                "S. Ghemawat",
                "I.J. Goodfellow",
                "A. Harp",
                "G. Irving",
                "M. Isard",
                "Y. Jia",
                "R. J\u00f3zefowicz",
                "L. Kaiser",
                "M. Kudlur",
                "J. Levenberg",
                "D. Man\u00e9",
                "R. Monga",
                "S. Moore",
                "D.G. Murray",
                "C. Olah",
                "M. Schuster",
                "J. Shlens",
                "B. Steiner",
                "I. Sutskever",
                "K. Talwar",
                "P.A. Tucker",
                "V. Vanhoucke",
                "V. Vasudevan",
                "F.B. Vi\u00e9gas",
                "O. Vinyals",
                "P. Warden",
                "M. Wattenberg",
                "M. Wicke",
                "Y. Yu",
                "X. Zheng"
            ],
            "title": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems",
            "venue": "ArXiv ",
            "year": 2016
        },
        {
            "authors": [
                "N. Provatas",
                "K. Elder"
            ],
            "title": "Phase-Field Methods in Materials Science and Engineering",
            "venue": "John Wiley & Sons, Inc., Weiheim, Germany",
            "year": 2010
        },
        {
            "authors": [
                "A.M. Turing"
            ],
            "title": "The chemical basis of morphogenesis",
            "venue": "Bull. Math. Biol. 52 ",
            "year": 1990
        },
        {
            "authors": [
                "T. Lepp\u00e4nen",
                "M. Karttunen",
                "K. Kaski",
                "R.A. Barrio",
                "L. Zhang"
            ],
            "title": "A new dimension to Turing patterns",
            "venue": "Phys. D: Nonlinear Phenom. 168\u2013169 ",
            "year": 2789
        },
        {
            "authors": [
                "P. Gray",
                "S. Scott"
            ],
            "title": "Sustained oscillations and other exotic patterns of behavior in isothermal reactions",
            "venue": "J. Phys. Chem. 89 ",
            "year": 1985
        },
        {
            "authors": [
                "B. Grossmann",
                "K.R. Elder",
                "M. Grant",
                "J.M. Kosterlitz"
            ],
            "title": "Directional solidification in two and three dimensions",
            "venue": "Phys. Rev. Lett. 71 ",
            "year": 1993
        },
        {
            "authors": [
                "W.J. Boettinger",
                "J.A. Warren",
                "C. Beckermann",
                "A. Karma"
            ],
            "title": "Phase-field simulation of solidification",
            "venue": "Annu. Rev. Mater. Sci. 32 ",
            "year": 2002
        },
        {
            "authors": [
                "B. Nestler",
                "H. Garcke",
                "B. Stinner"
            ],
            "title": "Multicomponent alloy solidification: Phase-field modeling and simulations",
            "venue": "Phys. Rev. E 71 ",
            "year": 2005
        },
        {
            "authors": [
                "V. Heinonen",
                "C. Achim",
                "J. Kosterlitz",
                "S.-C. Ying",
                "J. Lowengrub",
                "T. Ala-Nissila"
            ],
            "title": "Consistent hydrodynamics for phase field crystals",
            "venue": "Phys. Rev. Lett. 116 ",
            "year": 2016
        },
        {
            "authors": [
                "E. Alster",
                "K. Elder",
                "P.W. Voorhees"
            ],
            "title": "Displacive phase-field crystal model",
            "venue": "Phys. Rev. Mater. 4 ",
            "year": 2020
        },
        {
            "authors": [
                "N. Provatas",
                "J. Dantzig",
                "B. Athreya",
                "P. Chan",
                "P. Stefanovic",
                "N. Goldenfeld",
                "K. Elder"
            ],
            "title": "Using the phase-field crystal method in the multi-scale modeling of microstructure evolution",
            "venue": "JOM 59 ",
            "year": 2007
        },
        {
            "authors": [
                "N. Faghihi",
                "S. Mkhonta",
                "K. Elder",
                "M. Grant"
            ],
            "title": "Phase-field crystal for an antiferromagnet with elastic interactions",
            "venue": "Phys. Rev. E 100 ",
            "year": 2019
        },
        {
            "authors": [
                "I. Aranson",
                "V. Kalatsky",
                "V. Vinokur"
            ],
            "title": "Continuum field description of crack propagation",
            "venue": "Phys. Rev. Lett. 85 ",
            "year": 2000
        },
        {
            "authors": [
                "R. Spatschek",
                "E. Brener",
                "A. Karma"
            ],
            "title": "Phase field modeling of crack propagation",
            "venue": "Philos. Mag. 91 ",
            "year": 2011
        },
        {
            "authors": [
                "P.C. Hohenberg",
                "B.I. Halperin"
            ],
            "title": "Theory of dynamic critical phenomena",
            "venue": "Rev. Mod. Phys. 49 ",
            "year": 1016
        },
        {
            "authors": [
                "L.Q. Chen",
                "J. Shen"
            ],
            "title": "Applications of semi-implicit Fourier-spectral method to phase field equations",
            "venue": "Comput. Phys. Commun. 108 ",
            "year": 1016
        },
        {
            "authors": [
                "T. Barlow"
            ],
            "title": "Feed-forward neural networks for secondary structure prediction",
            "venue": "J. Mol. Graph. 13 ",
            "year": 1016
        },
        {
            "authors": [
                "K. Diederik"
            ],
            "title": "B",
            "venue": "Jimmy, et al., Adam: A method for stochastic optimization, ArXiv 1412.6980 ",
            "year": 2014
        },
        {
            "authors": [
                "R. Arora",
                "A. Basu",
                "P. Mianjy",
                "A. Mukherjee"
            ],
            "title": "Understanding deep neural networks with rectified linear units",
            "venue": "ArXiv 1611.01491 ",
            "year": 2016
        },
        {
            "authors": [
                "W.L. Ziegler"
            ],
            "title": "Computational method to compute the derivative and antiderivative; with concern for terminating a converging iterative process and considering accuracy",
            "venue": "roundoff error, approximation, and extrapolation, Math. Model. 8 ",
            "year": 1016
        },
        {
            "authors": [
                "W.H. Press",
                "S.A. Teukolsky"
            ],
            "title": "Numerical calculation of derivatives",
            "venue": "Comput. Phys. 5 ",
            "year": 1016
        },
        {
            "authors": [
                "H. Arbabi",
                "I.G. Kevrekidis"
            ],
            "title": "Particles to partial differential equations parsimoniously",
            "venue": "Chaos Interdiscip. J. Nonlinear Sci. 31 ",
            "year": 2021
        },
        {
            "authors": [
                "H. Arbabi",
                "J.E. Bunder",
                "G. Samaey",
                "A.J. Roberts",
                "I.G. Kevrekidis"
            ],
            "title": "Linking machine learning with multiscale numerics: Data-driven discovery of homogenized equations",
            "venue": "JOM 72 ",
            "year": 2020
        },
        {
            "authors": [
                "C. Szegedy",
                "W. Liu",
                "Y. Jia",
                "P. Sermanet",
                "S. Reed",
                "D. Anguelov",
                "D. Erhan",
                "V. Vanhoucke",
                "A. Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA , USA",
            "year": 2015
        },
        {
            "authors": [
                "J.-F. Cai",
                "B. Dong",
                "S. Osher",
                "Z. Shen"
            ],
            "title": "Image restoration: Total variation",
            "venue": "wavelet frames, and beyond, J. Am. Math. Soc. 25 ",
            "year": 2012
        },
        {
            "authors": [
                "B. Dong",
                "Q. Jiang",
                "Z. Shen"
            ],
            "title": "Image restoration: Wavelet frame shrinkage",
            "venue": "nonlinear evolution pdes, and beyond, Multiscale Model. Simul. 15 ",
            "year": 2017
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "in: International Conference on Medical Image Computing and Computer- Assisted Intervention, Springer, Cham, Munich, Germany",
            "year": 2015
        },
        {
            "authors": [
                "C.J. Lapeyre",
                "A. Misdariis",
                "N. Cazard",
                "D. Veynante",
                "T. Poinsot"
            ],
            "title": "Training convolutional neural networks to estimate turbulent sub-grid scale reaction rates",
            "venue": "Combust. Flame 203 ",
            "year": 2019
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Comput. 9 ",
            "year": 1997
        },
        {
            "authors": [
                "A. Graves"
            ],
            "title": "Long short-term memory",
            "venue": "in: Supervised Sequence Labelling with Recurrent Neural Networks, Springer, Berlin/Heidelberg, Germany",
            "year": 2012
        },
        {
            "authors": [
                "L. Prechelt"
            ],
            "title": "Early stopping-but when",
            "venue": "in: Neural Networks: Tricks of the Trade, Springer, Berlin/Heidelberg, Germany",
            "year": 1998
        },
        {
            "authors": [
                "W. Rawat",
                "Z. Wang"
            ],
            "title": "Deep convolutional neural networks for image classification: A comprehensive review",
            "venue": "Neural Comput. 29 ",
            "year": 2017
        },
        {
            "authors": [
                "P. Virtanen",
                "R. Gommers",
                "T.E. Oliphant",
                "M. Haberland",
                "T. Reddy",
                "D. Cournapeau",
                "E. Burovski",
                "P. Peterson",
                "W. Weckesser",
                "J. Bright"
            ],
            "title": "S",
            "venue": "J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, \u0130. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, SciPy 1.0 Contributors, SciPy 1.0: Fundamental algorithms for scientific computing in python, Nat. Methods 17 ",
            "year": 2020
        },
        {
            "authors": [
                "A.C. Hindmarsh"
            ],
            "title": "ODEPACK",
            "venue": "a systematized collection of ODE solvers, Sci. Comput. 1 ",
            "year": 1983
        }
    ],
    "sections": [
        {
            "text": "Machine learning based data-driven discovery ofnonlinear phase-field dynamics Elham Kiyani1,2, Steven Silber2,3, Mahdi Kooshkbaghi4, Mikko Karttunen2,3,5\n1Department of Mathematics, The University of Western Ontario, 1151Richmond Street, London, ON N6A 5B7 Canada2The Centre for Advanced Materials and Biomaterials (CAMBR), The Universityof Western Ontario, 1151 Richmond Street, London, ON N6A 5B7 Canada3Department of Physics and Astronomy, The University of Western Ontario,1151 Richmond Street, London, ON N6A 3K7 Canada4Simons Center for Quantitative Biology, Cold Spring Harbor Laboratory, ColdSpring Harbor, NY, USA5Department of Chemistry, The University of Western Ontario, 1151 RichmondStreet, London, ON N6A 5B7 Canada April 1, 2022\nAbstractOne of the main questions regarding complex systems at large scales concerns the effectiveinteractions and driving forces that emerge from the detailed microscopic properties. Coarse-grained models aim to describe complex systems in terms of coarse-scale equations with areduced number of degrees of freedom. Recent developments in machine learning (ML) algo-rithms have significantly empowered the discovery process of the governing equations directlyfrom data. However, it remains difficult to discover partial differential equations (PDEs) withhigh-order derivatives. In this paper, we present new data-driven architectures based on multi-layer perceptron (MLP), convolutional neural network (CNN), and a combination of CNN andlong short-term memory (CNN-LSTM) structures for discovering the non-linear equations ofmotion for phase-field models with non-conserved and conserved order parameters. The well-known Allen\u2013Cahn, Cahn\u2013Hilliard, and the phase-field crystal (PFC) models were used as thetest cases. Two conceptually different types of implementations were used: (a) guided by physi-cal intuition (such as local dependence of the derivatives) and (b) in the absence of any physicalassumptions (black-box model). We show that not only can we effectively learn the time deriva-tives of the field in both scenarios, but we can also use the data-driven PDEs to propagate thefield in time and achieve results in good agreement with the original PDEs."
        },
        {
            "heading": "1 Introduction",
            "text": "PDEs are widely used in modeling of complex physical, chemical and biological systems includingfluid dynamics, chemical kinetics, population dynamics and phase transitions. The study of PDEsin the context of ML, broadly speaking, falls into two categories: 1.) solving PDEs and 2.) predict-ing unknown PDEs from data [1, 2, 3, 4, 5, 6, 7]. In simulations of phase-field and reaction-diffusionmodels, commonly used numerical techniques are based on time and space discretization, suchas finite difference and finite element methods. In recent years, a third approach based on MLhas emerged with promising results for solving and even discovering unknown PDEs from data,see for example Ref. [8] and references therein.The core idea for using ML algorithms to solve PDEs is representing the residuals of PDEsas a loss function of a neural network (NN) where the loss function is minimized; a loss functionmeasures how far the predicted values are from their true values. This approach does not require\nar X\niv :2\n20 3.\n16 69\n2v 1\n[ ph\nys ic\ns. co\nm p-\nph ]\n3 0\nM ar\n2 02\n2\ndiscretization or meshing, which is beneficial when dealing with problems of high dimensionsand/or complex geometries [1, 2, 9]. Since most deep learning frameworks are based on automaticdifferentiation, these methods are known as mesh-free approaches [10].In the case of discovering unknown PDEs from data, the key idea of ML-based approachesis to estimate the time derivative of the desired (dependent) quantity. These approaches can bebroadly categorized as follows:1. An ensemble of macroscopic observations is available and there is knowledge about thephysics of the governing coarse PDE(s). The typical knowledge is that the time evolution ofthe field of interest depends on the field and its derivatives (e.g. Navier-Stokes equations).One can design the ML algorithm to find that dependency. This relation can be formulatedbased any of the following methods: (i) Linear dependence of the field evolution using adictionary of spatial derivatives with unknown coefficients [11, 12]; (ii) Nonlinear dependencewith black-box inference [7]; or (iii) Nonlinear dependence using a selective dictionary ofspatial derivatives which were found by other data-driven approaches [13]; (iv) Nonlineardependence where spatial derivatives are informed by the memory (history) of the systemusing a feedback loop, e.g. Recurrent neural network (RNN) together with long short-termmemory (LSTM) and gated recurrent unit (GRU) [14, 15, 16].2. An ensemble of microscopic observations is available and the macroscopic field of interest isknown. For example, the microscopic solutions of the Boltzmann equation are available andone is looking for the time evolution of coarse fields such as density, velocity or temperature.Again, one can assume that the time evolution of the field depends on the spatial derivativesusing physical intuition [17, 13].3. An ensemble of microscopic observations is available but the macroscopic field is unknown.Therefore, the first step is to discover the coarse-grained field, which is generally formu-lated as a model reduction problem [18]. The second step is to find the PDE(s) for thecoarse variable(s). For example, Thiem et al. determined an order parameter for coupledoscillators using diffusion maps and the corresponding governing PDE using a Runge-Kuttanetwork [19].In this paper, we explore ML-based approaches which fall under the first category mentionedabove. We assess two scenarios:(i) There is an unknown relation between field evolution spatial derivatives.(ii) The spatial derivatives, their orders and combinations are unknown (there is no spatialderivative dictionary).Afterwards, we also solve the predicted PDEs in time and space.For the first scenario, a flexible framework that can deal with large data sets and extract theunavailable PDE(s) from coarse-scale variables implicitly is developed. Two different approachesare presented for learning coarse-scale PDEs: 1) an MLP architecture and 2) a CNN-LSTM. SinceLSTM only passes time information to its layers and misses the spatial features of previous timesteps, CNN can be used to learn and detect the spatial features of the inputs [20, 21]. For thesecond scenario, a convolution operator is used to implicitly learn the dependence of the timederivative of the field on the spatial derivative(s) of unknown orders. The learned PDE is thenmarched in time with a time-integrator.We demonstrate the capability of the above algorithms to learn PDEs using data obtained fromphase-field simulations of the well-known Allen\u2013Cahn [22], Cahn\u2013Hilliard [23], and the phase-fieldcrystal (PFC) [24, 25] models using the open source software SymPhas [26].The rest of this article is organized as follows. A brief summary of the phase-field approachand data preparation is presented in Section 2. In Section 3, an overview of MLP and CNN-LSTMnetworks as two data-driven approaches which learn PDEs with a spatial derivatives dictionaryis presented. Finally, in Section 4, we introduce a CNN network that learns PDEs without anyassumption regarding spatial derivatives. Then, the solutions of the original and data-driven PDEsare compared. The tensorflow2 framework [27] was used to implement and train our networks."
        },
        {
            "heading": "2 Phase-Field Modeling",
            "text": "2.1 Phase-Field Modeling in a NutshellPhase-field modeling provides a theoretical and computational approach for simulating non-equilibrium processes in materials, typically with the objective of studying the dynamics andstructural changes. For an excellent overview of phase-field modelling, see the book by Provatasand Elder [28]. In its essence, phase-field modeling is a coarse-grained approach that uses con-tinuum fields to describe slow variables such as concentrations instead of atoms. The continuumfields are given by order parameters which may be conserved or non-conserved, and the dynam-ics of the order parameter(s) is (are) described by a Langevin equation, or equations when severalorder parameters are present. In this work, we consider only systems described by a single orderparameter U \u2261 U(~x, t). In all of the descriptions below, we use the conventional dimensionlessunits [28].The Langevin equations, or equations of motion, for the non-conserved and conserved orderparameters are (see Ref. [28] for a more detailed discussion) given as \u2202U \u2202t = \u2212\u0393 \u03b4F\u03b4U (non-conserved) (1) \u2202U \u2202t = \u0393\u22072 \u03b4F\u03b4U (conserved), (2)where F is a free energy functional and \u03b4/\u03b4U is a functional derivative. We have neglected thermalnoise from the above equations. The parameter \u0393 is a generalized mobility and is assumed tobe constant. It is also noteworthy that when no free energy functional is available, the equationsof motion are often postulated. This is the case with reaction-diffusion systems, for example,the well-known Turing [29, 30] and Gray\u2013Scott models [31, 30]. Phase-field models have beenwidely applied to various types of systems and phenomena, including dendritic and directionalsolidification [32, 33, 34], crystal growth [24, 35, 36, 37] and magnetism [38] as well as for phenomenasuch as fracture propagation [39, 40] to mention some examples.\n2.2 Phase-Field Models used in the Current WorkWe employed three different well-studied single order parameter phase-field models: 1) TheAllen\u2013Cahn model [22] for the case of a non-conserved order parameter, 2) the Cahn\u2013Hilliardmodel [23] for the conserved order parameter, and 3) the phase-field crystal (PFC) model [24] thathas a conserved order parameter and generates a modulated field that describes atomistic lengthscales and diffusive times. In addition to being well-studied, these models were chosen becausethey contain differing orders of spatial derivatives: the Allen\u2013Cahn model is described using a2nd order derivative, Cahn\u2013Hilliard using 4th, and PFC using 6th. Moreover, they each exhibitvarious spatial patterns that evolve according to different time scales. 2.2.1 The Allen\u2013Cahn ModelThe Allen\u2013Cahn model, a dynamical model for solidification originally developed by Allen andCahn, has a single non-conserved order parameter corresponding to Equation (1), and is definedusing a free energy density with a double well potential. The equation of motion has the form \u2202U \u2202t = \u2212M (\u22072U + a2U + a4U3) , (3)where M = \u0393 is related to chemical mobility. It represents a physical system that evolves purelydue to a chemical potential. It is also called Model A according to the Hohenberg and Halperinclassification of phase-field models [41].\n2.2.2 The Cahn\u2013Hilliard ModelThe Cahn\u2013Hilliard model, formulated by Cahn and Hilliard, represents spinodal decomposition.It is a conserved order parameter model corresponding to Equation (2). Like the Allen\u2013Cahn\n\u2202t = D\u22072 (\u22072U + a2U + a4U3) , (4)where D = \u0393 represents the diffusion constant. It is also known as Model B in the Hohenbergand Halperin classification [41].\n2.2.3 The Phase-Field Crystal ModelA free energy density to describe a crystal lattice at an atomistic scale incorporating elastic effectsinto a phase-field model was originally developed by Elder et al. [24, 25]. The free energy isminimized by a hexagonal periodic lattice, and is given by f (U) =\u22072(U33 + U44 +U ((q0 +\u22072)2 \u2212 \u03b5) U2 ) , (5) where q0 and \u03b5 are constants. The equation of motion with a conserved order parameter canthen be written as \u2202U \u2202t =\u22072 (U2 +U3 + ((q0 +\u22072)2 \u2212 \u03b5)U) . (6)The order parameter, U , represents the mass density. The PFC model can used to describeelastic and plastic deformations in isotropic materials, i.e., crystal structures. The lattice structurecan assume any orientation (based on the initial conditions) and interactions of different grains(individual crystal structures) with each other can lead to defects and dislocations.\n2.3 Simulation of Phase-Field ModelsThe open-source SymPhas [26] software package was used to numerically simulate the abovethree systems. SymPhas allows the user to define phase-field models directly from their PDEformulations, and control simulation parameters from a single configuration file. All simulationswere done using dimensionless units [28]. In terms of the numerical solution, SymPhas hasthe ability to simulate models using either explicit finite difference methods or the semi-implicitFourier spectral method. The latter was chosen. By virtue of its excellent error properties,the Fourier semi-implicit spectral method typically allows for larger time stepping than a finitedifference solver [42]. For each of the models, five independent simulations with 100 frames ofthe field U were saved at equally spaced intervals. Parameters for the numerical simulationsare summarized in Table 1. Each model uses a different time step or final time, and these werequalitatively chosen based on how long each model requires to reach a characteristic pattern.To illustrate the different dynamics of each model, snapshots at the end of each simulation areprovided in Figure 1."
        },
        {
            "heading": "3 Data-Driven PDEs with Spatial Derivatives Dictionary",
            "text": "As already discussed above, we consider two distinct types of methods for discovering PDEs fromdata, assuming the network is informed by spatial derivatives either explicitly or implicitly. In thefirst method, an MLP network is used to learn a function FMLP that can be formulated as\nUt(t, x, y) = FMLP (U(t, x, y), Ux(t, x, y), Uxx(t, x, y), Uy(t, x, y), Uyy(t, x, y), ...) , (7)\n3.1 Multi-layer Perceptron Network Architecture and PerformanceAn MLP is an example of a typical feedforward artificial neural network, consisting of a series oflayers. Each layer calculates the weighted sum of its inputs and then applies an activation functionto get a signal that is transferred to the next neuron [43].In our MLP network, the number of layers, neurons, and the type of activation functions foreach phase field models were found by trial and error. We approximate spatial derivatives of thecoarse variable U by finite differences, and along with U itself, feed this to the MLP network tolearn the function FMLP in Equation (7).The MLP architecture for learning the Allen\u2013Cahn model (Equation (3)) is shown in Fig-ure 2a) as an example. The input layer consisting of five neurons U(t, x, y), Ux(t, x, y), Uxx(t, x, y), Uy(t, x, y) and Uyy(t, x, y), and is connected to the hidden layers which have 128, 64, 16, and 8neurons each. In the output layer, we use a dense layer with a single neuron to predict Ut . Thenetwork is trained for 2, 000 epochs using the ADAM optimizer [44] and rectified linear unit (ReLU)activation function [45]. For the Cahn\u2013Hilliard (Equation (4)) and PFC (Equation (6)) models, weused spatial derivatives up to fourth and sixth order for the input layers, respectively.The performance of the MLP network on learning the models is shown in Figure 3. The rootmean squared error (rMSE) is the square root of the mean squared error (MSE) calculated as\nMSE = 1nx \u00d7 ny nx\u00d7ny\u2211 i=1 (U it \u2212 U\u0302 it )2. (9)\nAs shown in Figure 3, the rMSE values are small (\u223c10\u22122) indicating that the target time derivatives(U\u0302t) learned by the proposed MLP network are close to the true ones for all three models.\nConvolution Layer Pooling Layer\nFlatten Layer\n5 nodes\n10 nodes\nCoarse-Scale\n(I) Finite Difference\n(a) MLP Architecture\n(b) CNN-LSTM Architecture\nInput\nInput\nLSTM sub-model\n5 nodes\n10 nodes 64 nodes\n128 nodes\nFully connected layers\nOutput\nOutput\nCNN sub-model\nFigure 2: Schematic of the general steps in discovery of PDEs with spatial derivatives dictionary. Learningof PDEs from spatial derivatives and local values of coarse variables using two different approaches, (a) MLPand (b) CNN-LSTM. Coarse-scale variables are collected as snapshots from phase-field simulations. (I) Finitedifference methods are used to approximate the spatial derivatives which are fed into (a) the MLP networkaccording to Equation (7). The network connecting the input layer consists of a list of input features (thefield U and its spatial derivatives) to the output layer of a single neuron (time derivative Ut ). (II) The valuesof the macroscopic field U evaluated around each grid point are fed through (b) the CNN-LSTM network tolearn the PDEs of the form given in Equation (8). CNN-LSTM network connecting the input layer consistsof a list of input features (local variables U(t, xi\u22121, yi), U(t, xi, yi), U(t, xi+1, yi), U(t, xi, yi\u22121), U(t, xi, yi+1)) to theoutput layer of a single neuron (time derivative).\n0 100 200\nx\n0\n50\n100\n150\n200\ny\nUt test\n0 100 200\nx\n0\n50\n100\n150\n200\ny\nU\u0302t MLP\n0 100 200\nx\n0\n50\n100\n150\n200\ny\nrMSE=5.486e-02\n\u22120.36 \u22120.18 0.00\n0.18\n0.36\n0.54\n0.72\n\u22120.48 \u22120.32 \u22120.16 0.00\n0.16\n0.32\n0.48\n0.64\n0.80\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n(a) Allen\u2013Cahn model\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\nrMSE=7.300e-02\n\u22120.96 \u22120.80 \u22120.64 \u22120.48 \u22120.32 \u22120.16 0.00\n0.16\n0.32\n0.48\n\u22120.80 \u22120.64 \u22120.48 \u22120.32 \u22120.16 0.00\n0.16\n0.32\n0.48\n0.00\n0.04\n0.08\n0.12\n0.16\n0.20\n0.24\n0.28\n0.32\n(b) Cahn\u2013Hilliard model\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\nrMSE=5.552e-02\n0.00\n0.06\n0.12\n0.18\n0.24\n0.30\n0.36\n0.00\n0.06\n0.12\n0.18\n0.24\n0.30\n0.36\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18\n(c) PFC model Figure 3: Performance of the MLP network on the prediction of the phase-fields given in Equations (3),(4), and (6). In each contour, x = nx , y = ny . The first contour shows Ut , the time derivative computedfrom the numerical solution generated by SymPhas [26], and the center contour shows U\u0302t , the learned timederivative. The right panel shows the difference between Ut and U\u0302t , as well as the corresponding rMSEvalue for each phase-field model. Our data is randomly split 60:20:20 into training, validation, and test sets.The network is trained for 2, 000 epochs using the ADAM optimizer and ReLU activation function.\n3.2 Convolution and Long Short-Term Memory (CNN-LSTM) Network Architecture and PerformanceOne of the main challenges in approximating coarse-scale PDEs is the estimation of spatial deriva-tives. While in previous studies PDEs have been successfully identified by learning time derivativesas a function of the estimated spatial derivatives, approximating derivatives remains challeng-ing [46, 47]. Generally, the choice of the time step is one of the most important considerations innumerical differentiation. While large step sizes can increase simulation speed, too large steps cancreate instabilities. On the other hand, if the steps are too small, numerical errors can dominateand the derivatives are of no use. Accordingly, the question that arises in discovering PDEs is theaccuracy of numerical differentiation that has been used for training.Unlike an MLP, CNN-LSTM is capable of automatically learning time derivatives from coarse-scale values. Using a combination of convolutional layers with other network structures for data-driven differential equations is an active field of research (see, for example, Refs. [48, 49]). CNNsare widely used for image classification and there have been several breakthroughs in imagerecognition with performance close to that of humans [50]. The CNN architecture can progres-sively extract higher level representations (color, shape, topology, etc.) of an input feature (image)and learn the dependency of the output (mostly a single class label) to those representations. Theconvolution operation sweeps a filter across the entire input field and extracts the global featuresand local (pixel-to-pixel) variations. The convolutional layer in the networks can be consideredas an efficient implementation of the convolution operator, hence, this layer represents approxi-mations of (potentially of high order) derivatives of a scalar field. The convolution-differentiationconnection and derivatives-order of filters relationships have been discussed in detail by Cai andDong [51, 52]. For particular applications where the desired outputs include localization (a classlabel is assigned to each pixel), a specific CNN architecture called \u201cU-net\u201d has been proposed [53].Since in most engineering and physics applications the time evolution of the scalar field dependson the local spatial derivatives, the U-net architecture is a reasonable candidate for such a learningtask. The U-net-inspired network has also been successfully used in subgrid flame surface densityestimation for premixed turbulent combustion modeling [54].A schematic diagram of the proposed CNN-LSTM architecture is shown in Figure 2b). The ar-chitecture consists of two sub-networks: (i) A CNN sub-network, including one-dimensional convo-lution and maxpooling layers for feature extraction from input data and, (ii) a LSTM sub-networkincluding sequential layers followed by one LSTM layer and two dense layers with ReLU activa-tion. We feed the CNN-LSTM network with the local coarse-scale variables, i.e., (U(ti, xi\u22121, yi),\nU(ti, xi, yi), U(ti, xi+1, yi), U(ti, xi, yi\u22121), U(ti, xi, yi+1)) tuple. These coarse-scale variables at eachgrid point are fed into the CNN sub-network and the output of the convolutional layer passesthrough the LSTM layer followed by a dense layer to provide the output. The output of networkis the single neuron approximating Ut(t, x, y) at each grid point.The LSTM network consists of a cell state which is the core concept of LSTM networksand memory blocks. Each block is composed of gates that can make decisions about whichinformation passes through the cell state and which information can be removed. There arethree kinds of gates: 1) input, 2) output, and 3) forget gate. Each memory block in an LSTMarchitecture has an input and an output gate which control information coming into the memorycell and information going out to the rest of the network, respectively. In addition, an LSTMarchitecture has a forget gate which contains an activation function and allows the LSTM tokeep or forget information. Information from the previous hidden state and information fromthe current input is passed through the activation function. The output of each gate is a valuebetween 0 (block the information) and 1 (pass the information) [55, 56].The performance of the trained CNN-LSTM network is shown in Figure 4. The networkconsists of a convolutional layer with 64 filters before pooling layer, kernel size 3 followed byan LSTM layer with 80 neurons. There are two dense layers (fully connected) with 10 and 5neurons each. In the left two panels, contours of Ut and U\u0302t for the test sets and the correspondingpredictions by CNN-LSTM are compared. The difference between the original and the predictionsare shown in the right panels, where rMSE is also reported for each phase-field model. Theprediction errors from CNN-LSTM remain unchanged and an rMSE \u223c 10\u22122 is obtained for allthree models.\n0 100 200\nx\n0\n50\n100\n150\n200\n250\ny\nUt test\n0 100 200\nx\n0\n50\n100\n150\n200\n250\ny\nU\u0302t CNN-LSTM\n0 100 200\nx\n0\n50\n100\n150\n200\n250\ny\nrMSE=1.987e-02\n\u22120.24 \u22120.16 \u22120.08 0.00\n0.08\n0.16\n0.24\n0.32\n0.40\n0.48\n\u22120.24 \u22120.16 \u22120.08 0.00\n0.08\n0.16\n0.24\n0.32\n0.40\n0.48\n0.000\n0.075\n0.150\n0.225\n0.300\n0.375\n0.450\n0.525\n(a) Allen\u2013Cahn model\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\nrMSE=9.596e-02\n\u22120.96 \u22120.80 \u22120.64 \u22120.48 \u22120.32 \u22120.16 0.00\n0.16\n0.32\n0.48\n\u22120.80 \u22120.64 \u22120.48 \u22120.32 \u22120.16 0.00\n0.16\n0.32\n0.48\n0.000\n0.075\n0.150\n0.225\n0.300\n0.375\n0.450\n0.525\n(b) Cahn\u2013Hilliard model\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\n0 50 100\nx\n0\n20\n40\n60\n80\n100\ny\nrMSE=6.714e-02\n0.00\n0.06\n0.12\n0.18\n0.24\n0.30\n0.36\n\u22120.09 0.00\n0.09\n0.18\n0.27\n0.36\n0.45\n0.000\n0.045\n0.090\n0.135\n0.180\n0.225\n0.270\n(c) PFC model Figure 4: CNN-LSTM predictions for (a) the Allen\u2013Cahn Equation (3), (b) the Cahn\u2013Hilliard Equation (4),and (c) the PFC Equation (6). Actual and learned time derivatives Ut and U\u0302t are shown in the left two panels.The difference between the predicted and the actual time derivatives as well as rMSE are presented in theright panel. Our data is randomly split 60:20:20 into training, validation and test sets, and the network istrained for 2,000 epochs using the ADAM optimizer and ReLU activation function.\n3.3 Hyper-parameter studyA comparison of regression results over the selected prediction period obtained by MLP andCNN-LSTM is shown in Figure 5. One can clearly see the ability of both MLP and CNN-LSTMto accurately reproduce the original data and make predictions of the phase-field models.We used the coefficient of determination, R2 to compare the performance of the networks, R2 = 1\u2212 \u2211nx\u00d7nyi=1 (U it \u2212 U\u0302 it )2\u2211nx\u00d7ny i=1 (U it \u2212Ut)2 , (10)where Ut is the mean value of the time derivative for a single snapshot. Root mean squares and\nR2 scores can be affected by different hyper-parameters such as learning rate, number of trainingepochs, and network depth and width. Here, we study the effect of adding/removing MLP andconvolutional layers while all the other parameters are fixed.Figure 6a) shows the effect of adding layers to our MLP architecture. An MLP network with64 hidden neurons is expanded to 128/64 and 256/128/64 hidden neurons. It can be seen thatadding hidden layers reduces the rMSE and increases the performance of prediction. Figure 6b)presents the effect of adding CNN and LSTM layers to the CNN-LSTM. Here, we use a singleLSTM layer with two configurations for CNN layers: 1) single CNN layer with output filters of size64, 2) two CNN layers with 128/64 output shape as well as two LSTM layers with 128/64 neuronsfollowed by two CNN layers with 128/64 output sizes. Adding convolutional layers increases theperformance. However, MLP networks are more sensitive to the choice of architecture than theCNN-LSTM networks. Moreover, the computational cost of training a multi-layer CNN-LSTM ishuge compared to a single layer and should be taken into account for large-scale data. It canbe roughly concluded that the optimal number of LSTM and CNN layers is 1 in our CNN-LSTMnetwork. Conversely, the R2 values show less sensitivity to the structural changes in our proposedneural networks, particularly in the CNN-LSTM network (see Figure 6c)).To further study the dynamics of the optimization process (training models), the MSE andmean absolute error (MAE) as a function of epochs are given in Figure 7. The MAE is thedifference between the original and predicted values. This is calculated by averaging the absolutedifference over the data set and is expressed as MAE = 1nx \u00d7 ny nx\u00d7ny\u2211 i=1 |U i t \u2212 U\u0302 it |. (11) In order to achieve sufficiently small error, we trained networks for 2,000 epochs with a batchsize of 64. However, using approximately 500 epochs (e.g. early-stopping [57]) seems adequatefor achieving optimal results, particularly for the Allen\u2013Cahn and the Cahn\u2013Hilliard models.Since training CNN-LSTM networks is computationally expensive, using smart early-stoppingapproaches can help in cases of large data PDE learning tasks."
        },
        {
            "heading": "4 Data-Driven PDEs without Spatial Derivatives Dictionary",
            "text": "In this section, we reformulate the problem of learning PDEs as a black-box supervised learn-ing task using convolutional neural network architecture where there is no selection of spatialderivatives and the field U is the only input to our deep learning model. The mathematicalrepresentation of data-driven PDE learning task with CNN is Ut(t, x, y) = FCNN (U(t, x, y)) FCNN : Rnx\u00d7ny \u2192 Rnx\u00d7ny , (12)where nx and ny are the number of grid points in the x- and y-directions, respectively. We use\nU from our phase-field model simulations to train the CNN. After successful training of the CNNnetworks, arbitrary initial conditions were chosen for the field U and it was evolved in time bysolving Ut = FCNN(U) numerically at each grid point. 4.1 Convolutional Neural Network (CNN) ArchitectureThe CNN network architecture is illustrated in Figure 8. The full details of the mathematicaloperations and functionality of each layer are beyond the scope of this paper and can be foundin reviews on CNNs such as the one by Rawar and Wang [58]. The CNN structure proposedhere, similar to the U-net [53, 54], resembles the encoding-decoding (auto-encoding) networks.The scalar field which is discretized on nx \u00d7 ny grid points was fed as the input to the network.In the contracting path, two convolutional layers (conv1, conv2 in Figure 8) with 32 filters eachfollowed by ReLU and batch normalization (bn1, bn2) were applied. The kernel size was 3 \u00d7 3for all the convolutional layers. After the bn2 layer, the 2D max pooling operation (mp1) withzero stride (for dimensionality reduction purposes) was applied. The pool size for all the maxpooling layers was 2\u00d7 2. The same sub-structure is repeated with 64 filters (conv3, bn3, conv4, bn4) up to the bottleneck unit (output of mp2). The expansion path consists of two convolutionallayers (conv5,conv6) with ReLU units, each followed by an upsampling layer (up1, up2) with theexpansion factor of (2, 2). Finally, at the last convolutional layer (conv7), a linear activation functionwas used with a filter of size one resulting in an output of shape nx \u00d7ny . The stochastic gradientdecent optimization was applied to find the parameters of the network, where the cost function isthe mean absolute error between the network output and Ut from the training set. In total, ourCNN network consists of 121,057 trainable parameters. 4.2 CNN performance for learning PDEsThe phase-field models presented in Section 2 were used to evaluate the performance of theCNN network. For each model, the total of nt two-dimensional U and Ut fields were used andrandomly split 80:10:10 into training, validation and test sets, respectively. The U and Ut fieldsfrom training sets were provided as an input and output to the CNN. All models were trainedfor 20,000 epochs and the performance of the network to recover the Ut (learning the RHS of a\n4.3 Simulation of Data-Driven PDEsIn this section, the potential of the proposed method to predict the field U in time and space basedon a given initial condition U0 is presented. For all three phase-field models (Section 2), we areinterested in solving a set of PDEs of the form\n\u2202U(t, x, y) \u2202t = FCNN (U(t, x, y))\nU(0, x, y) = U0; initial condition, (13)where the right hand side is the output (prediction) of the trained CNN networks.In the following, we used the U fields at t = 2 (simulation time) as the initial condition (U0) for allthe three models. The U field had nx\u00d7ny = 128\u00d7128 real values for the Cahn\u2013Hilliard (Equation (4))and the PFC (Equation (6)) models, and 256\u00d7256 for the Allen\u2013Cahn model (Equation (3)). Thedifferent sizes were used to test if there is any size dependence. At each time (t) the Ut valuesfor every grid point were determined from our trained CNN models and nx \u00d7 ny ODEs weresolved using the (stiff) integrator. We used the scipy Adams/BDF method with automatic stiffnessdetection and switching for time integration [59, 60]. Those ODEs were solved up to t = 6 in ourbenchmark dataset.Figure 10 shows the solutions of the original and the data-driven PDEs. The contours for Uare given for qualitative comparison as well as the U values along the centerline y=ny/2 for twosnapshots at times t = 2.2 and t = 6. The results in Figure 10 showed that the data-driven PDEslearned by CNN approximate the original dynamics in both quantitative and qualitative manner.Finally, we would like to emphasize the following points: 1) The explicit forms of the data-driven PDEs are not known and there is no obvious relation between the functional form ofthe original and the learned PDEs. Therefore, unlike with the phase-field models, there is noguarantee for existence and uniqueness for the learned PDEs. 2) There are some isolated points inwhich the Ut predicted values are different from the original models. This discrepancy propagatesin time and space, and can lead to finite time blow-up in simulations. This is a known issue in\n(a) Allen\u2013Cahn model\n(b) Cahn\u2013Hilliard model\n(c) PFC model Figure 10: Time integration results of the PDEs learned by CNN for (a) the Allen\u2013Cahn (Equation (3)), (b)the Cahn\u2013Hilliard (Equation (4)) and (c) PFC (Equation (6)) models at t = 2.2 and t = 6. Left panels: U fieldfor original data. Middle panels: U field from simulations of the learned PDEs. Right panel: U values alongthe centerline y = ny/2 for the original PDEs (solid-lines) and from simulations of the learned PDEs (dashedlines).\n(almost all) machine learning algorithms for time series forecasting where there is no periodicityin time. In the case of no underlying periodicity, it may occur that the system trajectories donot span the phase space properly, that is, in such a case the observations are not representative.Such a situation may limit the applicability of the approach to short simulation times."
        },
        {
            "heading": "5 Conclusion",
            "text": "We have presented a data-driven methodology for discovering PDEs from phase-field dynamics.The well-known Allen\u2013Cahn, Cahn\u2013Hilliard and phase-field crystal models were used as the testcases to predict the underlying equations of motion.First, we provide an MLP architecture to learn the PDEs where the spatial derivatives areexplicitly or implicitly given. Second, CNN-LSTMs were employed to learn the governing PDEsfrom coarse-scale local values. Third, we proposed a special CNN architecture for cases wherethere is no information about spatial dependence. In addition, using numerical integration, weshowed how the learned PDEs can be used to predict coarse-scale variables as a function of timeand space, starting from given initial conditions. The evolution of the learned and original PDEsshowed excellent agreement. We emphasize that all of the above algorithms yield a black-box-typediscovery of PDEs with no obvious connection to the functional form of the physical models.In general, MLP networks are extremely flexible with data, and PDEs can be learned fromvarious types of data using these networks. More specifically, they can be used to learn a mappingfrom a coarse field and its spatial derivatives as the inputs. However, the performance of an MLPnetwork is greatly affected by the choice of architecture, and also the training of such networksrequires the use of spatial derivatives. Along with approximating derivatives, we need to knowthe derivatives\u2019 orders as that is required to train an MLP network.In CNN networks, however, spatial derivatives are not required, and thus a CNN can bethought of as a finite-difference method capable of estimating derivatives in its first convolutionlayer. Moreover, one major advantage in using CNNs is their capability to extract spatial featuresfrom inputs. Since LSTMs pass only time information to the layers and keep the missing spatialinformation from the previous steps, a combination of CNNs and LSTMs can be applied moregenerally on data with spatial relationships, and, in the current case, to learn phase-field models.In spite of these advantages, CNN networks are memory intensive and require a large amountof data and several iterations in order to be trained effectively and LSTMs are computationallyexpensive. Despite the above limitations, we believe that the techniques introduced here offerapproaches that are both general and systematic, and provide a basis for future developments."
        },
        {
            "heading": "Acknowledgments",
            "text": "Mahdi Kooshkbaghi partially supported by NIH Grant GM133777. Mikko Karttunen thanks theNatural Sciences and Engineering Research Council of Canada (NSERC) and the Canada Re-search Chairs Program. Computing facilities were provided by SHARCNET (www.sharcnet.ca)and Compute Canada (www.computecanada.ca)."
        }
    ],
    "title": "Machine learning based data-driven discovery of nonlinear phase-field dynamics",
    "year": 2022
}