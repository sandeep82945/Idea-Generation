{
    "abstractText": "In reinforcement learning, reward-driven feature learning directly from high-dimensional images faces two challenges: sample-efficiency for solving control tasks and generalization to unseen observations. In prior works, these issues have been addressed through learning representation from pixel inputs. However, their representation faced the limitations of being vulnerable to the high diversity inherent in environments or not taking the characteristics for solving control tasks. To attenuate these phenomena, we propose the novel contrastive representation method, Action-Driven Auxiliary Task (ADAT), which forces a representation to concentrate on essential features for deciding actions and ignore control-irrelevant details. In the augmented state-action dictionary of ADAT, the agent learns representation to maximize agreement between observations sharing the same actions. The proposed method significantly outperforms model-free and model-based algorithms in the Atari and OpenAI ProcGen, widely used benchmarks for sample-efficiency and generalization.",
    "authors": [
        {
            "affiliations": [],
            "name": "Minbeom KimID"
        },
        {
            "affiliations": [],
            "name": "Kyeongha Rho"
        },
        {
            "affiliations": [],
            "name": "Yong-duk Kim"
        },
        {
            "affiliations": [],
            "name": "Kyomin Jung"
        }
    ],
    "id": "SP:8b09a7f79a1f6d111022c76e0e716680267ab3bc",
    "references": [
        {
            "authors": [
                "F Such",
                "V Madhavan",
                "R Liu",
                "R Wang",
                "P Castro",
                "Y Li"
            ],
            "title": "An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents",
            "venue": "In: International Joint Conference on Artificial Intelligence;",
            "year": 2019
        },
        {
            "authors": [
                "A Anand",
                "E Racah",
                "S Ozair",
                "Y Bengio",
                "MA C\u00f4t\u00e9",
                "RD. Hjelm"
            ],
            "title": "Unsupervised state representation learning in atari. In: Advances in neural information processing",
            "year": 2019
        },
        {
            "authors": [
                "D Yarats",
                "A Zhang",
                "I Kostrikov",
                "B Amos",
                "J Pineau",
                "R. Fergus"
            ],
            "title": "Improving sample efficiency in model-free reinforcement learning from images",
            "year": 1910
        },
        {
            "authors": [
                "K Gregor",
                "F. Besse"
            ],
            "title": "Temporal Difference Variational Auto-Encoder",
            "venue": "ArXiv. 2019;abs/1806.03107",
            "year": 2019
        },
        {
            "authors": [
                "I Higgins",
                "A Pal",
                "AA Rusu",
                "L Matthey",
                "CP Burgess",
                "A Pritzel"
            ],
            "title": "Darla: Improving zero-shot transfer in reinforcement learning",
            "venue": "arXiv preprint",
            "year": 2017
        },
        {
            "authors": [
                "J Ng",
                "R. Petrick"
            ],
            "title": "Incremental Learning of Planning Actions in Model-Based Reinforcement Learning",
            "venue": "In: International Joint Conference on Artificial Intelligence;",
            "year": 2019
        },
        {
            "authors": [
                "DP Kingma",
                "M. Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:13126114",
            "year": 2013
        },
        {
            "authors": [
                "D Hafner",
                "T Lillicrap",
                "J Ba",
                "M. Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "arXiv preprint arXiv:191201603",
            "year": 2019
        },
        {
            "authors": [
                "L Kaiser",
                "M Babaeizadeh",
                "P Milos",
                "B Osinski",
                "RH Campbell",
                "K Czechowski"
            ],
            "title": "Model-based reinforcement learning for atari",
            "year": 1903
        },
        {
            "authors": [
                "M Laskin",
                "A Srinivas",
                "Curl Abbeel P"
            ],
            "title": "Contrastive unsupervised representations for reinforcement learning",
            "venue": "In: International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "A Stooke",
                "K Lee",
                "P Abbeel",
                "M. Laskin"
            ],
            "title": "Decoupling representation learning from reinforcement learning",
            "venue": "In: International Conference on Machine Learning",
            "year": 2021
        },
        {
            "authors": [
                "G Liu",
                "C Zhang",
                "L Zhao",
                "T Qin",
                "J Zhu",
                "J Li"
            ],
            "title": "Return-based contrastive representation learning for reinforcement learning",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "A Zhang",
                "R McAllister",
                "R Calandra",
                "Y Gal",
                "S. Levine"
            ],
            "title": "Learning invariant representations for reinforcement learning without reconstruction",
            "year": 2006
        },
        {
            "authors": [
                "S Greydanus",
                "A Koul",
                "J Dodge",
                "A. Fern"
            ],
            "title": "Visualizing and understanding atari agents",
            "venue": "In: International Conference on Machine Learning",
            "year": 2018
        },
        {
            "authors": [
                "H Wang",
                "G Pang",
                "C Shen",
                "C. Ma"
            ],
            "title": "Unsupervised Representation Learning by Predicting Random Distances",
            "venue": "In: International Joint Conference on Artificial Intelligence;",
            "year": 2020
        },
        {
            "authors": [
                "T Chen",
                "S Kornblith",
                "M Norouzi",
                "G. Hinton"
            ],
            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
            "venue": "Proceedings of the 37th International Conference on Machine Learning",
            "year": 2020
        },
        {
            "authors": [
                "M Hessel",
                "J Modayil",
                "H Van Hasselt",
                "T Schaul",
                "G Ostrovski",
                "W Dabney"
            ],
            "title": "Rainbow: Combining improvements in deep reinforcement learning",
            "venue": "arXiv preprint",
            "year": 2017
        },
        {
            "authors": [
                "T Haarnoja",
                "A Zhou",
                "P Abbeel",
                "S. Levine"
            ],
            "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
            "venue": "CoRR. 2018;abs/1801.01290",
            "year": 2018
        },
        {
            "authors": [
                "K He",
                "H Fan",
                "Y Wu",
                "S Xie",
                "R. Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition;",
            "year": 2020
        },
        {
            "authors": [
                "Avd Oord",
                "Y Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint",
            "year": 2018
        },
        {
            "authors": [
                "P Khosla",
                "P Teterwak",
                "C Wang",
                "A Sarna",
                "Y Tian",
                "P Isola"
            ],
            "title": "Supervised contrastive learning",
            "venue": "arXiv preprint arXiv:200411362",
            "year": 2020
        },
        {
            "authors": [
                "M Laskin",
                "K Lee",
                "A Stooke",
                "L Pinto",
                "P Abbeel",
                "A. Srinivas"
            ],
            "title": "Reinforcement Learning with Augmented Data",
            "year": 2020
        },
        {
            "authors": [
                "J Schulman",
                "F Wolski",
                "P Dhariwal",
                "A Radford",
                "O. Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint",
            "year": 2017
        },
        {
            "authors": [
                "K Cobbe",
                "C Hesse",
                "J Hilton",
                "J. Schulman"
            ],
            "title": "Leveraging procedural generation to benchmark reinforcement learning",
            "venue": "In: International conference on machine learning",
            "year": 2020
        },
        {
            "authors": [
                "K. Kielak"
            ],
            "title": "Do recent advancements in model-based deep reinforcement learning really improve data efficiency",
            "venue": "arXiv preprint arXiv:200310181",
            "year": 2020
        },
        {
            "authors": [
                "HP van H Hasselt",
                "M Hessel",
                "J. Aslanides"
            ],
            "title": "When to use parametric models in reinforcement learning? In: Advances in Neural Information Processing Systems; 2019",
            "venue": "PLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March",
            "year": 1433
        }
    ],
    "sections": [
        {
            "text": "In reinforcement learning, reward-driven feature learning directly from high-dimensional\nimages faces two challenges: sample-efficiency for solving control tasks and generalization\nto unseen observations. In prior works, these issues have been addressed through learning\nrepresentation from pixel inputs. However, their representation faced the limitations of being\nvulnerable to the high diversity inherent in environments or not taking the characteristics for\nsolving control tasks. To attenuate these phenomena, we propose the novel contrastive\nrepresentation method, Action-Driven Auxiliary Task (ADAT), which forces a representation\nto concentrate on essential features for deciding actions and ignore control-irrelevant\ndetails. In the augmented state-action dictionary of ADAT, the agent learns representation\nto maximize agreement between observations sharing the same actions. The proposed\nmethod significantly outperforms model-free and model-based algorithms in the Atari and\nOpenAI ProcGen, widely used benchmarks for sample-efficiency and generalization."
        },
        {
            "heading": "Introduction",
            "text": "Reinforcement learning (RL) has achieved state-of-the-art performance on a variety of sequential decision tasks [1]. With a lot of trials-and-errors, agents obtain competent policies achieving human-level control in complex tasks. Despite the successes in simulation games, however, RL has faced the limitation that numerous trials-and-errors are essential for learning. In real world, collecting such an enormous amount of trials is time-consuming and requires large amounts of resources. Furthermore, the unexpected factors in new environments can yield test-performance decay. Therefore, sample-efficiency and generalization capability in RL are emerged as challenging tasks.\nIn general, state representation from raw pixel inputs contributes to efficient exploration and robustness to zero-shot observations. This intuition has been proven experimentally in various environments through comparisons between state-based exploration and learning in high dimensional observations [2, 3]. Therefore, learning representation is considered as crucial apparatus for sample-efficiency and generalization. To address learning representation in RL, various approaches have been proposed in the literature. Broadly, there are three\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 1 / 14\na1111111111 a1111111111 a1111111111\nOPEN ACCESS\nCitation: Kim M, Rho K, Kim Y-d, Jung K (2022) Action-driven contrastive representation for reinforcement learning. PLoS ONE 17(3): e0265456. https://doi.org/10.1371/journal. pone.0265456\nEditor: Sathishkumar V E, Hanyang University, KOREA, REPUBLIC OF\nReceived: November 17, 2021\nAccepted: March 2, 2022\nPublished: March 18, 2022\nCopyright: \u00a9 2022 Kim et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.\nData Availability Statement: We note that data and code used in the paper is open and processed data has been provided via Github repo with link given in the paper: https://github.com/openai/ataripy and https://github.com/openai/procgen.\nFunding: This project was jointly supported by the Swarm AI Learning Framework Development Project of Agency For Defense Development and National Research Foundation of Korea (NRF) grant funded by the Korea government (No.2021R1A2C2008855). The funders provided support in the form of salaries for authors, but did\nmainstreams on Auxiliary tasks: (1) reconstructing pixel-inputs, (2) World Model building predictive models of environments, and (3) contrastive representation learning.\nMethodologies with reconstruction errors [4, 5] and World Model [6] yielded break-\nthroughs in representation mechanism, gaining various advantages such as sample-efficiency, the efficacy of exploration, and domain transfer [7\u20139]. Yet, they might suffer from difficulties when facing complex environments or environments with a lot of control-irrelevant visual information, as shown in Fig 1. To overcome the limitation of these methods, Contrastive Unsupervised Representations for Reinforcement Learning (CURL) proposed an auxiliary task maximizing accordance between different augmented versions of the same images [10]. CURL gains significant sample-efficiency, outperforming existing model-free and modelbased methodologies. However, only with the auxiliary task of CURL, the agent considers only image augmentation-invariant features. It does not suffice to distinguish whether visual information is control-relevant or not. Under those circumstances, [11, 12] extend auxiliary tasks of CURL to consider accordance between temporally consecutive observations or similar returns. [13] points out the same problem as our research and represents control-relevant objects through a learned world model. However, looking at previous researches, their auxiliary tasks too focus on environment-specific accordance. For various downstream tasks, agents should extract intrinsic representation relevant to overall control problems. To learn intrinsic representation, the auxiliary task needs to rethink \u2018actions decided by the observations\u2019. The rich history of \u2018state-action pairs\u2019 contains the intuitions obtained by the agent from numerous interactions with the environment. Therefore, auxiliary task should be reformed to leverage state-action pairs as self-made labels.\nhttps://doi.org/10.1371/journal.pone.0265456.g001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 2 / 14\nnot have any additional role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript. The specific roles of these authors are articulated in the \u2018author contributions\u2019 section.\nCompeting interests: The authors have declared that no competing interests exist.\nThis paper proposes a novel auxiliary task Action-Driven Auxiliary Task (ADAT). The proposed method adopts pseudo-supervised contrastive learning through instance discrimination about the states sharing the same actions. Our hypothesis is straightforward: representation can capture the motivational key-features for deciding an action and ignore the control-irrelevant information through contrastively aggregated states labeled by actions. This intuition is understood visually with our experiment of saliency map [14]. Also, we suggest Unbiased Sampling module for attaching ADAT to existing RL algorithms. Consequently, Action-Driven Contrastive Representation for Reinforcement Learning is designed based on the proposed methodologies. The effect of improving performance in sample-efficiency and robustness compared to existing baselines is verified by conducting experiments. In Atari Games, widely used benchmarks for measuring sample-efficiency, our method achieves state-of-the-art in 15 out of 26 games and outperforms human performance on five games. Moreover, the agents in ProcGen Games show dramatically improved generalization ability to unseen observations. Finally, the saliency map experiment is conducted to visually understand what ADAT agents concentrate on.\nThe contributions of this paper are summarized as: (1) ADAT, the novel auxiliary task, ded-\nicated to solving control tasks; (2) Unbiased Sampling for ADAT to be compatible with offpolicy algorithms; (3) empirical demonstration of superior sample-efficiency and generalization in Atari Games and OpenAI ProcGen, and visual understanding of action-supervision\u2019s efficacy with saliency map."
        },
        {
            "heading": "Background",
            "text": "Simply supervision-driven or reward-driven features have struggled with real-world problems for downstream tasks [15]. For enriching features without external supervision, contrastive learning defines an instance discrimination task [16], where the positive key should be distinguished from the negatives according to the given query. CURL [10] learns representation in this manner. An input image goes through \u2018the augmentation of random crop\u2019 and results in a query q and the positive key k+. Denote K = {k+}[{k1, k2, . . ., kN\u22121}, the negative keys sampled from the CURL\u2019s replay buffer. The dictionary is looked-up as D \u00bc ffk\u00f0T \u00f0k\u00de\u00dejk 2 Kg, where T is the augmentation and fk is the key encoder. Through the query encoder fq, the representation of the query and keys are measured as a pairwise similarity. The encoded query is used for policy optimization, such as DQN Rainbow [17] and Soft Actor-Critic [18]. In the process of maximizing an agreement between the query and the positive key, fq is updated by contrastive loss and policy loss of interactions with environments. On the other hand, fk is only trained by momentum contrast update [19] as below:\nyfk \u00bc m\ufffd yfk \u00fe \u00f01 m\u00de \ufffd yfq : \u00f01\u00de\nTo respect the relative similarity of embeddings, CURL measures pairwise similarity as con-\ntrastive predictive coding [20]. This method was proposed for an encoder to measure the underlying shared structure contrastively rather than trivial information by inserting a bilinear product into measuring the pairwise similarity of a query and keys:\nf \u00f0q; k\u00de \u00bc exp\u00f0qTWk\u00de; \u00f02\u00de\nRepresentation features predict far in the contrastive measuring, extracting meaningful\nagreement. W can be the linear transformation or non-linear neural networks for the bilinear product of query q and key k. For calculating gradients of dictionary {k1, k2, k3, . . ., kn} for each query q, InfoNCE loss was proposed for segregating one positive keys k+ and N \u2212 1\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 3 / 14\nnegative keys as:\nLq \u00bc log exp\u00f0qTWk\u00fe\u00de\nexp\u00f0qTWk\u00fe\u00de \u00fe PN 1 i\u00bc1 exp\u00f0qTWki\u00de : \u00f03\u00de\nTherefore, Contrastive Unsupervised Representations for Reinforcement Learning adopted\nInfoNCE loss, discriminating one positive key for each query, a different image-augmented observation of query image. Through an end-to-end contrastive representation learning mechanism, CURL has shown state-of-the-art performance in representation learning across continuous and discrete control benchmark tasks. Recently, InfoNCE loss has been reformed to reinterpret self-supervised learning for a variety of purposes [21]."
        },
        {
            "heading": "Method",
            "text": ""
        },
        {
            "heading": "Action-driven auxiliary task",
            "text": "We propose the novel auxiliary task for learning intrinsic representation about solving control tasks. To represent the intrinsic features that determine the action, an auxiliary task needs to utilize the knowledge of the agent who has accumulated intuition about the environment through a number of interactions. The rich history of interactions will be of great help in identifying only crucial relevance between visual information and actions. Our auxiliary task, Action-Driven Auxiliary Task (ADAT), forces learned representation to become invariant about control-irrelevant pixels and sensitive to essential pixels for control tasks. From the history of interactions such as the replay buffer or a minibatch rolled out by runners, ADAT samples state-action pairs {x, a} to build a dictionary consisting of the query {xq, aq} and keys ffxk1 ; ak1g; fxk2 ; ak2g; fxk3 ; ak3g; . . . ; fxkn ; akngg as shown in Fig 2. Randomly augmented states T \u00f0xk\u00de (e.g., T : translate [22]) are encoded into representation fk\u00f0T \u00f0xk\u00de\u00de and become positive keys of the dictionary if sharing the same actions with the query. The projected pairwise similarity is measured as a bilinear product with contrastive predictive coding [20] which can help capture meaningful structures other than irrelevant minors. In each minibatch, this dictionary is looked up as pseudo-supervised contrastive learning to maximize agreement between the\nhttps://doi.org/10.1371/journal.pone.0265456.g002\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 4 / 14\nquery and its positive keys. Unlike CURL, there are multiple positive keys per query because there is not only one state that the agent made the same choice in history. Therefore, multipositive keys should be matched with each query in the dictionary. We employ the log-summation loss with binary cross-entropy of sigmoid classifier other than InfoNCE, interpreted as cross-entropy of softmax classifier.\nLADAT \u00bc X\ni2I\nX\nj2J\nLi;j \u00bc X\ni2I\nX\nj2J\n\u2018\u00f0zTi zj; labeli;j\u00de;\n\u2018\u00f0zTi zj; labeli;j\u00de \u00bc\nlog exp\u00f0zTi zj\u00de\nexp\u00f0zTi zj\u00de \u00fe 1 labeli;j \u00bc 1;\nlog 1\nexp\u00f0zTi zj\u00de \u00fe 1 labeli;j \u00bc 0;\n8 > > ><\n> > >:\n\u00f04\u00de\nwhere zi \u00bc Proj\u00f0f \u00f0T \u00f0xi\u00de\u00de\u00de is the linear projection of the augmented pixel-input\u2019s representations. I and J are the sets of indices for all the elements augmented differently in the dictionary. \u2113 is log-loss, which measures agreement between two representations. labeli,j is a pseudo-label that means if the agent chose the same action in both states, it becomes a positive label. Product of linear projections zTi zj can be reformed as a bilinear product of query and key q TWk, contrastive predictive coding for capturing meaningful shared structures. The sigmoid function activates pairwise similarity zTi zj and L ADAT\naggregates \u2113, binary cross-entropy loss of all sigmoid pairwise similarities in a dictionary.\nAlgorithm 1 ADAT\u2019s main learning algorithm Input: batch size N, momentum m, \u03b8query of query encoder fq, \u03b8key of key encoder fk, linear projection g, set of random augmentations T for sampled batch fxk; akg N k\u00bc1\nfor all k 2 {1, . . ., N} do draw random augmentation t \ufffd T , t0 \ufffd T # query inference ~x2k 1 \u00bc t\u00f0xk\u00de h2k 1 \u00bc fq\u00f0~x2k 1\u00de \u22b3 representation of query z2k 1 \u00bc g\u00f0~h2k 1\u00de \u22b3 projection a2k\u22121 = ak # key inference ~x2k 1 \u00bc t0\u00f0xk\u00de h2k \u00bc fk\u00f0~x2k 1\u00de \u22b3 representation of key z2k \u00bc g\u00f0~h2k\u00de \u22b3 projection a2k = ak end for for all i 2 {1, 3, 5, . . ., 2N \u2212 1} and j 2 {2, 4, 6, . . ., 2N} do\nsi;j \u00bc 1=\u00f01\u00fe e z T i zj\u00de \u22b3 sigmoid pairwise similarity mi;j \u00bc 0 ai 6\u00bc aj;\n1 ai \u00bc aj;\n(\n\u22b3 pseudo labeling\nLi;j \u00bc mi;j \ufffd log si;j \u00f01 mi;j\u00de \ufffd log\u00f01 si;j) \u22b3 binary cross entropy loss end for update fq and g to minimize P Li;j\n\u03b8key = m \u00d7 \u03b8key + (1 \u2212 m) \u00d7 \u03b8query \u22b3 MoCo update end for return \u03b8query \u22b3 for policy training phase\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 5 / 14\nWith this novel auxiliary task, the contrastive learner gives more attention to crucial fea-\ntures to decide the action. ADAT is the attachable learning representation module which can be plugged into both off-policy and on-policy algorithm. In our experiments, we attach ADAT to Rainbow DQN [17] and Proximal Policy Optimization(PPO) [23] for proving each performance improvement on off-policy and on-policy version."
        },
        {
            "heading": "Implementation on existing baselines",
            "text": "ADAT can be compatible with both on and off-policy algorithms. The Off-policy baseline with ADAT adopts DQN Rainbow [17] as a framework for policy optimization and Momentum Contrast Update [19] as a mechanism of learning representation with the replay buffer. ADAT Rainbow builds upon the successful approach by CURL Rainbow. There are two major differences between ADAT Rainbow and CURL Rainbow. The first is a change in \u2018action-driven\u2019 contrastive representation, and the other one is Unbiased Sampling. For a dictionary of ADAT, actions are self-motivated supervision that the learning agents answered. Therefore, low-quality foolish answers are in the front part of the queue in the replay buffer. When sampling pseudo-labels from a uniform distribution in the ADAT dictionary, the earlier sample determined by the naive actor would be used more than the one labeled by the smarter actor as shown in Fig 3, even with prioritized experience replay, hard-converged foolish answers from less trained agents would be before consistent pairs.\nTherefore, we propose Unbiased Sampling. This is a straightforward module that does not\ncost computationally in the total algorithm. It only takes twice as many samples as the planned minibatch size from a uniform distribution and just selects the most recent samples by the batch size from the queue. As shown in Fig 3, most elements in the replay buffer have been uniformly leveraged from unbiased sampling during the whole training time. Unbiased Sampling follows very cheap time complexity (worst case quadratic of ADAT\u2019s batch size), so there is a little time delay in the whole training time. On our Atari Setting with ADAT Rainbow, low-quality pseudo-labels are used up to 12 times more than without unbiased sampling. It is expected that ADAT with unbiased sampling gains performance improvement by leveraging high-quality self-made labels more and raising the efficacy of sampling.\nhttps://doi.org/10.1371/journal.pone.0265456.g003\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 6 / 14\nFor the on-policy version with ADAT, we adopted PPO as a policy learning algorithm.\nUnlike off-policy algorithms using a replay buffer to get the training data, on-policy RL algorithms like PPO get the samples rolled out by the current policies. Therefore, ADAT is free from sample imbalance problems. Since the training batch size of PPO is much larger than that of ADAT in our experiments, we randomly sampled data from the training minibatches of PPO for contrastive learning. We compared our method with only reward-driven PPO to validate the robustness on unseen observations."
        },
        {
            "heading": "Results and discussion",
            "text": ""
        },
        {
            "heading": "Evaluation and implementation details",
            "text": "If the agent can discern the essentials for control between a lot of visual information, the significant improvement can be guaranteed on sample-efficiency for optimizing policy and generalization about robustness for zero-shot observation with similar structures. The purpose of this experiment is to measure them and to understand the proposed intuition visually. As a benchmark for sample-efficient gameplay, ADAT was employed broadly in various simulations in Atari Games. For measuring generalization capability, ProcGen Games [24] were adopted for estimating the contribution of ADAT. Then, we captured a saliency map of policies to understand which pixel information the agent focused on throughout the whole input image. By measuring how much the policies fluctuate depending on the perturbation for each pixel, we validated that our auxiliary task helped agents to concentrate more on essential pixels for decisions. Tables 1 and 2 and Fig 4 gives detailed hyperparameters for reproducing results."
        },
        {
            "heading": "Baselines",
            "text": "The existing baselines that are adopted for comparison with our methods are as follows,\n\u2022 Rainbow DQN [17] is an enhanced DQN that aggregates various techniques for stabilizing\nRL networks into a single learner.\n\u2022 SimPLe [9] trains the world model by self-supervised representation learning with observa-\ntions collected from real environments. Then, the world model learns policy in the RL phase and gains sample-efficiency.\n\u2022 OTRainbow [25] trains rainbow DQNs taking extra updates for sample-efficiency with\nrepetitive samples in replay buffer, which is an advantage of DQNs\n\u2022 EFF.Rainbow [26] suggests novel hyperparameters tuning methods for rainbow DQN\u2019s\ndata-efficient learning.\n\u2022 PPO [23] is a widely used benchmark for years, suggesting novel clipped surrogate objective\nloss for monotonous improvements by bounded policy updates.\n\u2022 CURL [10] leverages contrastive representation learning for sample-efficiency. Its auxiliary\ntask learns to match different augmented versions of the same images. This self-supervision gains improved sample-efficiency, and this paper enhances it as \u2018self-supervision with actions history\u2019, outperforming existing representation methodology."
        },
        {
            "heading": "Sample-efficiency",
            "text": "Atari Games were benchmarked at 100k interactions (Atari100k), which frequently have appeared as the benchmark for sample-efficiency. Rainbow DQN [17], SimPLe [9], OTRainbow [25], Efficient Rainbow [26], CURL and human scores have been baselines to show how\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 7 / 14\nsample-efficient our algorithm is compared to widely used representation methods. Performance of each algorithm is evaluated after 100k timesteps (400K frames, frameskip of 4) of interactions between agents and the 26 Atari Games, equivalent to two hours of gameplay. As shown in Table 3, we can empirically define the pure contribution of our novel auxiliary task \u2018ADAT\u2019 as score improvement of ADAT Rainbow over CURL Rainbow. ADAT Rainbow has shown 1.1x mean human-normalized score (HNS) gains in 100k interactions over CURL. Furthermore, ADAT+, which means ADAT with unbiased sampling gained a 1.24x higher HNS score than CURL. Not only compared to CURL, but it has also been state-of-the-art in 15 out\nAcross every 26 games, this setting was used the same. For Atari100K, we wanted to show that the performance is improved only by the action-driven auxiliary task. Therefore, the same experiment was conducted by importing the CURL Rainbow official code [10] as it is and modifying the contrastive learning phase.\nhttps://doi.org/10.1371/journal.pone.0265456.t001\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 8 / 14\nof 26 Atari Games, surpassing human performance on five games Jamesbond (2.18HNS), Krull (1.78HNS), Road_Runner (1.60HNS), Assualt (1.01HNS) and Freeway (1.00HNS). ADAT+ Rainbow achieved the mean HNS of 47.2%, while 22.2%, 28.5%, 40.4%, 28.5% and 38.1% for Rainbow, OTRainbow, SimPLe, Efficient Rainbow and CURL. Therefore, the experimental result proves that action-driven supervision contributes to the improvement of existing contrastive representation learning methodologies. In addition, the dramatic performance improvement seen in ADAT+ confirms that the unbiased sampling module reliably solves the low-quality action labels issue.\nFor ProcGen experiments, we set the hyperparameters almost same with suggested in [24], except using data augmentation(translate) and framestack. Additionally, we adopt the residual convolutional neural network architecture used in IMPALA as the encoder with some modifications. For instance, we set the latent space dimension as 50 and add a single MLP layer with 256 hidden units to calculate policy and value logits. Then, the encoder is trained using the PPO and ADAT with this hyperparameter setting.\nhttps://doi.org/10.1371/journal.pone.0265456.t002\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 9 / 14"
        },
        {
            "heading": "Generalization",
            "text": "Generalization is the capability of coping with unseen observation with a similar structure. OpenAI ProcGen is a benchmark with tremendous diversities, with 100,000 levels per environment [24]. Therefore, the generalization capability of agents needs to be measured by training on limited observations and performing gameplay in unfamiliar situations using ProcGen. To observe the effect of robustness contributed by ADAT, the performance trends of naive PPO and PPO supported by ADAT were compared during 40M timesteps. Bigfish and plunder Games, which possess a lot of visual information irrelevant to gameplay, and have few objects essential to control, were selected as the experimental environments. Both agents were trained at 200 levels and evaluated at 100,000 levels. Fig 5 can summarize contributions of the proposed auxiliary task in Bigfish and Plunder as:\n\u2022 With the help of an action-driven auxiliary task, the performance of the PPO agent in the\nnewly encountered environment has been dramatically improved. Furthermore, in Plunder Game, while vanilla PPO agent started to be saturated after 20M interactions, the agent with intrinsic representation progressively explored better policies.\n\u2022 In the gameplay of Bigfish, a degradation of accomplishments in unseen levels game stood\nout clearly. It can be interpreted as the Bigfish Games demand hard generalization. Whereas the PPO agent underperformed apparently in few-shot levels, the agent aided by our novel auxiliary task coped well with unknown diversity inherent in the environment.\nTranslate 8\nhttps://doi.org/10.1371/journal.pone.0265456.g004\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 10 / 14\nVisual understanding with saliency map\nIn the above experiment, it was empirically shown that the agent gained sample-efficiency and generalization through ADAT. In this subsection, the saliency map experiment with a variety of detailed visual information was conducted to obtain insights into which pixels of image inputs the agent is focusing on. We measured the pixel-interest of the agent as to how much will the policy change if pixel-information is removed from the area around the location (i, j) [14].\nSVp\u00f0s; i; j\u00de \u00bc 1\n2 k Vp\u00f0s\u00de Vp\u00f0F\u00f0s; i; j\u00de\u00de k 2: \u00f05\u00de\nSaliency metric is the squared difference between the value estimate of the original sequence\nand the perturbed one. F(s, i, j) means the perturbation on image S at pixel coordinates (i,j). It removes pixel information by masking out a 5x5 size black patch around the (i,j) coordinate.\nTo demonstrate if the agent captures essential pixels for the control by performing our aux-\niliary task, we took a saliency map of PPO and ADAT PPO agents. Similar to the method in\nTable 3. Comparison of sample efficiency."
        },
        {
            "heading": "GAME Human Rainbow SimPLe OTRainbow EFF.Rainbow CURL ADAT ADAT+",
            "text": "ALIEN 7127.7 318.7 616.9 824.7 739.9 558.2 953.8 1029.7 AMIDAR 1719.5 32.5 88.0 82.8 188.6 142.1 146.1 147.3 ASSAULT 742.0 231 527.2 351.9 431.2 600.6 689.5 749.4 ASTERIX 8503.3 243.6 1128.3 628.5 470.8 734.5 808 864 BANK HEIST 753.1 15.55 34.2 182.1 51.0 131.6 128.5 164 BATTLE ZONE 37187.5 2360.0 5184.4 4060.6 10124.6 14870.0 17160 21240 BOXING 12.1 -24.8 9.1 2.5 0.2 1.2 0.6 0.4 BREAKOUT 30.5 1.2 16.4 9.84 1.9 4.9 5.2 4.5 CHOPPER COMMAND 7387.8 120.0 1246.9 1033.33 861.8 1058.5 1151 1106 CRAZY CLIMBER 35829.4 2254.5 62583.6 21327.8 16185.3 12146.5 18022 21240 DEMON ATTACK 1971.0 163.6 208.1 711.8 508.0 817.6 609.8 851.9 FREEWAY 29.6 0.0 20.3 25.0 27.9 26.7 29.3 29.7 FROSTBITE 4334.7 60.2 254.7 231.6 866.8 1181.3 1838.4 1943.2 GOPHER 2412.5 431.2 771.0 778.0 349.5 669.3 634 601.2 HERO 30826.4 487 2656.6 6458.8 6857.0 6279.3 6114.2 7259.2 JAMESBOND 302.8 47.4 125.3 112.3 301.6 471.0 491 635.7 KANGAROO 3035.0 0.0 323.1 605.4 779.3 872.5 1120 956.9 KRULL 2665.5 1468 4539.9 3277.9 2851.5 4229.6 3675.9 3502.9 KUNG FU MASTER 22736.3 0.0 17257.2 5722.2 14346.1 14307.8 13767 19146 MS PACMAN 6951.6 67 1480.0 941.9 1204.1 1465.5 1144.8 1075 PONG 14.6 -20.6 12.8 1.3 -19.3 -16.5 -15.9 -15.1 PRIVATE EYE 69571.3 0 58.3 100.0 97.8 218.4 250 388 QBERT 13455.0 123.46 1288.8 509.3 1152.9 1042.4 1303.6 1578 ROAD RUNNER 7845.0 1588.46 5640.6 2696.7 9600.0 5661.0 9711 12508 SEAQUEST 42054.7 131.69 683.3 286.92 354.1 384.5 370.2 251.6 UP N DOWN 11693.2 504.6 3350.3 2847.6 2877.4 2955.2 3286 3597.8\nMean HNS 100.0% 22.2% 40.4% 26.4% 28.5% 38.1% 41.0% 47.2%\nAtari scores of ADAT Rainbow and other baselines of 26 Atari Games benchmark achieved after 100K interactions. The average score over five random seeds. Improvements over baselines are measured as relative Human Normalized Score(HNS).\nhttps://doi.org/10.1371/journal.pone.0265456.t003\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 11 / 14\n[14], pixels of the image were added to the R-value of RGB according to the normalized saliency score of the corresponding coordinates. As shown in Fig 6, the saliency map indicates an apparent difference between the PPO agent and the ADAT PPO agent. The Bigfish environment comprises a few crucial pixels for gameplay, so these pixels need to be focused. In the PPO agent map, the saliency score is evenly spread throughout on the whole input images. Whereas in ADAT, red cloud points are clustered around objects that the player fish should consider immediately for gameplay. These results visually validate that the pixels considered for inference were different between two agents through ADAT. Without the support of ADAT, the PPO agent became dependent to control-irrelevant details. This phenomenon made the agent vulnerable to the unnecessary characteristics of the environment. On the other hand, the ADAT PPO agent gained intrinsic representation capturing which pixels are crucial for addressing reinforcement learning problems and became independent to irrelevant details."
        },
        {
            "heading": "Conclusion",
            "text": "In this work, we proposed \u2018Action-Driven Auxiliary task,\u2019 novel instance discrimination in a self-supervised manner, for representation to capture intrinsic features directly related to deciding actions and become insensitive to irrelevant details. Learning the shared structure between aggregated observations by contrastive representation, the agent distinguished control-irrelevant pixels and gained both sample-efficiency and generalization capabilities. These improvements are enhanced through proposed Unbiased Sampling. Our experiments on Atari and ProcGen demonstrated the efficacy of the ADAT and Unbiased Sampling module, visually confirming these intuitions. ADAT is a simple module attachable to various existing RL\nhttps://doi.org/10.1371/journal.pone.0265456.g005\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 12 / 14\nalgorithms, both off-policy and on-policy. It is worthwhile investigating how to label continuous actions for pseudo-supervision as a future topic. In addition, in an environment where there are many tiny objects for control, such as Starpilot and Bossfight in OpenAI ProcGen, both existing and our representation methodologies have adversely affected the performance. Therefore, future work will be needed to make learning representation effective in these particular cases. d ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque."
        },
        {
            "heading": "Author Contributions",
            "text": "Conceptualization: Minbeom Kim.\nFunding acquisition: Yong-duk Kim, Kyomin Jung.\nInvestigation: Minbeom Kim, Kyeongha Rho.\nMethodology: Minbeom Kim.\nProject administration: Minbeom Kim, Yong-duk Kim, Kyomin Jung.\nSoftware: Minbeom Kim, Kyeongha Rho.\nSupervision: Kyomin Jung.\nVisualization: Kyeongha Rho.\nWriting \u2013 original draft: Minbeom Kim.\nWriting \u2013 review & editing: Kyeongha Rho, Yong-duk Kim, Kyomin Jung.\nhttps://doi.org/10.1371/journal.pone.0265456.g006\nPLOS ONE | https://doi.org/10.1371/journal.pone.0265456 March 18, 2022 13 / 14"
        }
    ],
    "title": "Action-driven contrastive representation for reinforcement learning",
    "year": 2022
}