{
    "abstractText": "The world consists of objects: distinct entities possessing independent properties and dynamics. For agents to interact with the world intelligently, they must translate sensory inputs into the bound-together features that describe each object. These object-based representations form a natural basis for planning behavior. Active inference (AIF) is an influential unifying account of perception and action, but existing AIF models have not leveraged this important inductive bias. To remedy this, we introduce \u2018object-based active inference\u2019 (OBAI), marrying AIF with recent deep object-based neural networks. OBAI represents distinct objects with separate variational beliefs, and uses selective attention to route inputs to their corresponding object slots. Object representations are endowed with independent action-based dynamics. The dynamics and generative model are learned from experience with a simple environment (active multi-dSprites). We show that OBAI learns to correctly segment the action-perturbed objects from video input, and to manipulate these objects towards arbitrary goals.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ruben S. van Bergen"
        },
        {
            "affiliations": [],
            "name": "Pablo L. Lanillos"
        }
    ],
    "id": "SP:735fa3e58947c04864e3be381313a1b794f6bf00",
    "references": [
        {
            "authors": [
                "P. Lanillos",
                "E. Dean-Leon",
                "G. Cheng"
            ],
            "title": "Yielding self-perception in robots through sensorimotor contingencies",
            "venue": "IEEE Transactions on Cognitive and Developmental Systems 9(2), 100\u2013112",
            "year": 2016
        },
        {
            "authors": [
                "Z. Kourtzi",
                "C.E. Connor"
            ],
            "title": "Neural representations for object perception: structure, category, and adaptive coding",
            "venue": "Annual review of neuroscience 34, 45\u201367",
            "year": 2011
        },
        {
            "authors": [
                "B. Peters",
                "N. Kriegeskorte"
            ],
            "title": "Capturing the objects of vision with neural networks",
            "venue": "Nature Human Behaviour 5, 1127\u20131144",
            "year": 2021
        },
        {
            "authors": [
                "K. Greff",
                "R.L. Kaufman",
                "R. Kabra",
                "N. Watters",
                "C. Burgess",
                "D. Zoran",
                "L. Matthey",
                "M. Botvinick",
                "A. Lerchner"
            ],
            "title": "Multi-object representation learning with iterative variational inference",
            "venue": "International Conference on Machine Learning. pp. 2424\u2013 2433. PMLR",
            "year": 2019
        },
        {
            "authors": [
                "F. Locatello",
                "D. Weissenborn",
                "T. Unterthiner",
                "A. Mahendran",
                "G. Heigold",
                "J. Uszkoreit",
                "A. Dosovitskiy",
                "T. Kipf"
            ],
            "title": "Object-centric learning with slot attention",
            "venue": "Advances in Neural Information Processing Systems 33, 11525\u201311538",
            "year": 2020
        },
        {
            "authors": [
                "V. Mnih",
                "K. Kavukcuoglu",
                "D. Silver",
                "A.A. Rusu",
                "J. Veness",
                "M.G. Bellemare",
                "A. Graves",
                "M. Riedmiller",
                "A.K. Fidjeland",
                "G Ostrovski"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature 518(7540), 529\u2013533",
            "year": 2015
        },
        {
            "authors": [
                "R. Veerapaneni",
                "J.D. Co-Reyes",
                "M. Chang",
                "M. Janner",
                "C. Finn",
                "J. Wu",
                "J.B. Tenenbaum",
                "S. Levine"
            ],
            "title": "Entity abstraction in visual model-based reinforcement learning",
            "year": 1910
        },
        {
            "authors": [
                "N. Watters",
                "L. Matthey",
                "M. Bosnjak",
                "C.P. Burgess",
                "A. Lerchner"
            ],
            "title": "Cobra: Dataefficient model-based rl through unsupervised object discovery and curiosity-driven exploration",
            "year": 1905
        },
        {
            "authors": [
                "T. Parr",
                "G. Pezzulo",
                "K.J. Friston"
            ],
            "title": "Active inference: the free energy principle in mind, brain, and behavior",
            "venue": "MIT Press",
            "year": 2022
        },
        {
            "authors": [
                "P. Lanillos",
                "C. Meo",
                "C. Pezzato",
                "A.A. Meera",
                "M. Baioumy",
                "W. Ohata",
                "A. Tschantz",
                "B. Millidge",
                "M. Wisse",
                "Buckley",
                "C.L"
            ],
            "title": "Active inference in robotics and artificial agents: Survey and challenges",
            "venue": "arXiv preprint arXiv:2112.01871",
            "year": 2021
        },
        {
            "authors": [
                "P. Zablotskaia",
                "E.A. Dominici",
                "L. Sigal",
                "A.M. Lehrmann"
            ],
            "title": "Unsupervised video decomposition using spatio-temporal iterative inference",
            "year": 2006
        },
        {
            "authors": [
                "J. Marino",
                "Y. Yue",
                "S. Mandt"
            ],
            "title": "Iterative amortized inference",
            "venue": "35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "L. Matthey",
                "I. Higgins",
                "D. Hassabis",
                "A. Lerchner"
            ],
            "title": "dsprites: Disentanglement testing sprites dataset",
            "venue": "https://github.com/deepmind/dsprites-dataset/",
            "year": 2017
        },
        {
            "authors": [
                "N. Sajid",
                "P.J. Ball",
                "T. Parr",
                "K.J. Friston"
            ],
            "title": "Active inference: Demystified and compared",
            "venue": "Neural Computation 33, 674\u2013712",
            "year": 2021
        },
        {
            "authors": [
                "K. Friston"
            ],
            "title": "A free energy principle for a particular physics",
            "year": 1906
        },
        {
            "authors": [
                "L. Da Costa",
                "T. Parr",
                "N. Sajid",
                "S. Veselic",
                "V. Neacsu",
                "K. Friston"
            ],
            "title": "Active inference on discrete state-spaces: a synthesis",
            "venue": "Journal of Mathematical Psychology 99, 102447",
            "year": 2020
        },
        {
            "authors": [
                "B. Millidge",
                "A. Tschantz",
                "C.L. Buckley"
            ],
            "title": "Whence the expected free energy",
            "year": 2004
        },
        {
            "authors": [
                "E. Jang",
                "S. Gu",
                "B. Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Keywords: Multi-object representation learning \u00b7 Active inference"
        },
        {
            "heading": "1 Introduction",
            "text": "Intelligent agents are not passive entities that observe the world and learn its causality. They learn the relationship of action and effect by interacting with the world, in order to fulfil their goals [1]. In higher-order intelligence, such as exhibited by primates, these interactions very often take place at the level of objects [2,3]. Whether picking a ripe fruit from a tree branch, kicking a football, or taking a drink from a glass of water; all require reasoning and planning in terms of objects. Objects, thus, are natural building blocks for representing the world and planning interactions with it.\nWhile there have been recent advances in unsupervised multi-object representation learning and inference [4,5], to the best of the authors knowledge, no existing work has addressed how to leverage the resulting representations for generating actions. In addition, object perception itself could benefit from being placed in an active loop, as carefully selected actions could resolve ambiguity about object properties (including their segmentations - i.e., which inputs belong to which objects). Meanwhile, state-of-the-art behavior-based learning\nar X\niv :2\n20 9.\n01 25\n8v 1\n[ cs\n.A I]\n2 S\nep 2\n02 2\n(control), such as model-free reinforcement learning [6] uses complex encoding of high-dimensional pixel inputs without taking advantage of objects as an inductive bias (though see [7,8].\nTo bridge the gap between these different lines of work, we here introduce \u2018object-based active inference\u2019 (OBAI, pronounced /@\u2019beI/), a new framework that combines deep, object-based neural networks [4] and active inference [9,10]. Our proposed neural architecture functions like a Bayesian filter that iteratively refines perceptual representations. Through selective attention, sensory inputs are routed to high-level object modules (or slots [5]) that encode each object as a separated probability distribution, whose evolution over time is constrained by an internal model of action-dependent object dynamics. These object representations are highly compact and abstract, thus enabling efficient unrolling of possible futures in order to select optimal actions in a tractable manner. Furthermore, we introduce a closed-form procedure to learn preferences or goals in the network\u2019s latent space.\nAs a proof-of-concept, we evaluate our proposed framework on an active version of the multi-dSprites dataset, developed for this work (See Fig. 1a). Our preliminary results show that OBAI is able to: i) learn to segment and represent objects ii) learn the action-dependent, object-based dynamics of the environment; and iii) plan in the latent space \u2013 obviating the need to imagine detailed pixel-level outcomes in order to generate behavior. This work is a first step towards building more complex object-based active inference systems that can perform more cognitively challenging tasks on naturalistic input."
        },
        {
            "heading": "2 Methods",
            "text": ""
        },
        {
            "heading": "2.1 Object-structured generative model",
            "text": "We extend the IODINE architecture proposed in [4] for object representation learning, to incorporate dynamics. Zablotskaia et al. [11] previously developed a similar extension to IODINE, in which object dynamics were modeled implicitly, through LSTM units operating one level below the latent-space representation. Here, we instead implement the dynamics directly in the latent space, and allow these dynamics to be influenced by actions on the part of the agent.\nLike IODINE, our framework relies on iterative amortized inference [12] (IAI) on an object-structured generative model. This model describes images of up to K objects with a Normal mixture density (illustrated in Fig. 1):\np(oi|{s(k)}k\u22081:K ,mi) = \u2211 k [mi = k]N ( gi(s (k)), \u03c32o ) (1)\nwhere oi is the value of the i-th image pixel, s(k) is the state of the k-th object, gi(\u2022) is a decoder function (implemented as a deep neural network (DNN)) that translates an object state to a predicted mean value at pixel i, \u03c32o is the variability of pixels around their mean values and, crucially,mi is a categorical variable that\nindicates which object (out of a possible K choices) pixel i belongs to1. Note that the same decoder function is shared between objects. The pixel assignments themselves also depend on the object states:\np(mi|{s(k)}k\u22081:K) = Cat ( Softmax ( {\u03c0i(s(k))}k\u22081:K )) (2)\nwhere \u03c0i(\u2022) is another DNN that maps an object state to a log-probability at pixel i, which (up to a constant of addition) defines the probability that the pixel belongs to that object. Marginalized over the assignment probabilities, the pixel likelihoods are given by:\np(oi|{s(k)}k\u22081:K) = \u2211 k m\u0302ikN ( gi(s (k)), \u03c32o ) (3)\nm\u0302ik = p(mi = k|{s(k)}k\u22081:K) (4)\nDuring inference, the soft pixel assignments {m\u0302ik} introduce dynamics akin to selective attention, as each object slot is increasingly able to focus on those pixels that are relevant to that object."
        },
        {
            "heading": "2.2 Incorporating action-dependent dynamics",
            "text": "So far, this formulation is identical to the generative model in IODINE. We now extend this with an action-based dynamics model. We want to endow objects with (approximately) linear dynamics, and to allow actions that accelerate the objects. First, we redefine the state of an object at time point t in generalized\ncoordinates, i.e. s\u2020t = [ st s\u2032t ] , where s\u2032 refers to the first-order derivative of the\n1 Note the use of Iverson-bracket notation; the bracket term is binary and evaluates to 1 iff the expression inside the brackets is true.\nstate. The action-dependent state dynamics are then given by:\ns \u2032(k) t = s \u2032(k) t\u22121 + Da (k) t\u22121 + \u03c3s 1 (5)\ns (k) t = s (k) t\u22121 + s \u2032(k) t + \u03c3s 2 (6)\nwhere a(k)t is the action on object k at time t. This action is a 2-D vector that specifies the acceleration on the object in pixel coordinates. Multiplication by D (which is learned during training) transforms the pixel-space acceleration to its effect in the latent space2. Equations 5-6 thus define the object dynamics model p(s \u2020(k) t |s \u2020(k) t\u22121 ,a (k) t\u22121).\nWe established that a(k)t is the action on object k in the model at a given time. However, note that the correspondence between objects represented by the model, and the true objects in the (simulated) environment, is unknown.3 To solve this correspondence problem, we introduce the idea of action fields. An action field \u03a8 = [\u03c81, ...,\u03c8M ]T is an [M\u00d72] matrix (withM the number of pixels in an image or video frame), such that the i-th row in this matrix (\u03c8i) specifies the (x,y)-acceleration applied at pixel i. In principle, a different acceleration can be applied at each pixel coordinate (in practice, we apply accelerations sparsely). These pixel-wise accelerations affect objects through the rule that each object receives the sum of all accelerations that occur at its visible pixels:\na (k) t = \u2211 i [mi = k]\u03c8i + \u03c3\u03c8 3 (7)\nwhere we include a small amount of Normally distributed noise in order to make this relationship amenable to variational inference. This definition of actions in pixel-space is unambiguous and allows the model to interact with the environment."
        },
        {
            "heading": "2.3 Inference",
            "text": "On the generative model laid out in the previous section, we perform iterative amortized inference (IAI). IAI generalizes variational autoencoders (VAEs), which perform inference in a single feedforward pass, to architectures which use several iterations (implemented in a recurrent network) to minimize the Evidence Lower Bound (ELBO). As in VAEs, the final result is a set of variational beliefs in the latent space of the network. In our case, this amounts to inferring\n2 Since the network will be trained unsupervised, we do not know in advance the nature of the latent space representation that will emerge. In particular, we do not know in what format (or even if) the network will come to represent the positions of the objects. 3 In particular, since the representation across objects slots in the network is permutationinvariant, their order is arbitrary \u2013 just as the order in the memory arrays that specify the environment is also arbitrary. Thus, the object-actions, as represented in the model, cannot be unambiguously mapped to objects in the environment that the agent interacts with. This problem is exacerbated if the network has not inferred the object properties and segmentations with perfect accuracy or certainty, and thus cannot accurately or unambiguously refer to a true object in the environment.\nq({s\u2020(k),a(k)}k\u22081:K). We choose these beliefs to be independent Normal distributions. Inference and learning both minimize the following ELBO loss:\nL = \u2212 T\u2211 t=0 [ H ( q ( {s\u2020(k)t ,a (k) t } )) + E q({s(k)t }) [log p(ot|{s(k)t })]\n+ \u2211 k E q(a (k) t ) [log p(a (k) t |\u03a8t)] + \u2211 k E q ( s \u2020(k) t ,s \u2020(k) t\u22121 ,a (k) t\u22121 )[log p(s\u2020(k)t |s\u2020(k)t\u22121 ,a(k)t\u22121)] ] (8)\nfor some time horizon T . Note that for t = 0, we define p(s\u2020(k)t |s \u2020(k) t\u22121 ,a (k) t\u22121) = p(s\u2020(k)) = \u220f jkN (s \u2020(k) j ; 0, 1), i.e. a fixed standard-Normal prior. To compute\nE q(a\n(k) t )\n[log p(a (k) t |\u03a8t)], we employ a sampling procedure, described in Appendix D.\nThe IAI architecture consists of a decoder module that implements the generative model, and a refinement module which outputs updates to the parameters \u03bb of the variational beliefs. Mirroring the decoder, the refinement module consists of K copies of the same network (sharing the same parameters), such that refinement network k outputs updates to \u03bb(k). Network architectures for the decoder and refinement modules are detailed in Appendix B. To perform inference across multiple video frames, we simply copy the refinement and decoder networks across frames as well as object slots. Importantly, as in [4], each refinement network instance also receives as input a stochastic estimate of the current gradient \u2207\n\u03bb (k) t L. Since the ELBO loss includes a temporal dependence\nterm between time points, the inference dynamics in the network are automatically coupled between video frames, constraining the variational beliefs to be consistent with the dynamics model. To infer q({a(k)}k\u22081:K), we employ a separate (small) refinement network (again copied across objects slots; details in Appendix B.2)."
        },
        {
            "heading": "2.4 Task & training",
            "text": "We apply OBAI to a simple synthetic environment, developed for this work, which we term active-dSprites. This environment was created by re-engineering the original dSprites shapes [13], to allow these objects to be translated at will and by non-integer pixel offsets. The environment simulates these shapes moving along linear trajectories that can be perturbed through the action fields we introduced above. In the current work, OBAI was trained on pre-generated experience with this environment, with action fields sampled arbitrarily (i.e. not based on any intent on the part of the agent).\nSpecifically, we generated video sequences (4 frames, 64\u00d764 pixels) from the active-dSprites environment, with 3 objects per video, in which we applied an action field only at a single time point (in the 2nd frame). Action fields were sparsely sampled such that (whenever possible) every object received exactly one non-zero acceleration at one of its pixels. Exactly one background pixel also received a non-zero acceleration, to encourage the model to learn not to assign background pixels to the segmentation masks of foreground objects. In practice,\nthe appearance of the background was unaffected by these actions (conceptually, the background can be thought of as an infinitely large plane extending outside the image frame, and thus shifting it by any amount will not change its visual appearance in the image).\nOBAI was trained on 50,000 pre-generated video sequences, to minimize the ELBO loss from equation 8. This loss was augmented to include the losses at intermediate iterations of the inference procedure, and this composite loss was backpropagated through time to compute parameter gradients. More details about the environment and training procedure can be found in Appendices A & C."
        },
        {
            "heading": "2.4.1 Learning goals in the latent space",
            "text": "The active-dSprites environment was conceived to support cognitive tasks that require object-based reasoning. A natural task objective in active-dSprites is to move a certain object to a designated location. This type of objective is simple in and of itself, but the rules that determine which object must be moved where can be arbitrarily complex. For now, we restrict ourselves to the simple objective of moving all objects in a scene to a fixed location, and focus on how to encode this objective. We follow previous Active Inference work (e.g. [14,15]) in conceptualizing goals as a preference distribution p\u0303. However, rather than defining this preference to be over observations, as is common (though see [16,17,18]), we instead opt to define it over latent states, i.e. p\u0303({s\u2020(k)}), which simplifies action selection (a full discussion of the merits of this choice is outside the scope of this paper).\nAssuming that we can define a preference over the true state of the environment, strue (e.g. the ground-truth object positions), the preference distribution in latent space can be obtained through the following importance-sampling procedure:\np\u0303(s) \u221d \u2211 j p(s|o\u2217j )uj \u2248 \u2211 j q(s|o\u2217j )uj (9)\no\u2217j \u223c p(o|s\u2217truej ), uj = p\u0303(s \u2217 truej ), s \u2217 truej \u223c p(strue) \u221d Constant (10)\nThis allows the latent-space preference to be estimated in closed form from a set of training examples, constructed by sampling true states uniformly from the environment and rendering videos from these states. Inference is performed on the resulting videos, and the latent-state preference is computed as the importanceweighted average of the inferred state-beliefs (alternatively, we can sample states directly from p\u0303(s\u2217true), and let the importance weights drop out of the equation). In particular, if p\u0303(s\u2217true) is Normal, then the preference in the latent space is also Normal:\nq(s|o\u2217j ) = N ( \u00b5(o\u2217j ),\u03c3(o \u22172 j ) ) , p\u0303(s) = N (\u00b5\u0303, \u03c3\u03032) (11)\n\u00b5\u0303 = 1\u2211 j uj \u2211 j uj\u00b5(o \u2217 j ), \u03c3\u0303 = \u221a 1\u2211 j uj \u2211 j uj ( (\u00b5\u0303\u2212 \u00b5(o\u2217j ))2 + \u03c3(o\u2217j )2 ) (12)"
        },
        {
            "heading": "2.4.2 Planning actions",
            "text": "OBAI can plan actions aimed at bringing the environment more closely in line with its learned preferences. Specifically, we choose actions that minimize the Free Energy of the Expected Future (FEEF) [18]. When preferences are defined with respect to latent states, the FEEF of a policy (action sequence) \u03c0 = { [a\n(k) 1 ,a (k) 2 , . . . ,a (k) T ] } k\u22081:K is given by:\nG(\u03c0) = T\u2211 \u03c4=1 \u2211 k DKL ( q(s(k)\u03c4 |\u03c0)||p\u0303(s) ) (13)\nwhere q(s(k)\u03c4 |\u03c0) is the policy-conditioned variational prior, obtained by propagating the most recent state beliefs through the dynamics model by the requisite number of time steps.\nGiven this objective, the optimal policy can be calculated in closed form for arbitrary planning horizons. In this work, as a first proof-of-principle, we only consider greedy, one-step-ahead planning. In this case, the optimal \"policy\" (single action per object) is given by:\na\u0302(k) = (DTLD)\u22121DTL(\u00b5\u0303\u2212 \u00b5(k)s ) (14)\nwhere L = diag(\u03c3\u0303\u22122), and \u00b5(k)s is the mean of the current state belief for object k."
        },
        {
            "heading": "3 Results",
            "text": ""
        },
        {
            "heading": "3.1 Object segmentation and reconstruction in dynamic scenes",
            "text": "We first evaluated OBAI on its inference and reconstruction capabilties, when presented with novel videos of moving objects (not seen during training). To evaluate this, we examined the quality of its object segmentations, and of the video frame reconstructions (Fig. 2). Segmentation quality was computed using the Adjusted Rand Index (ARI), as well as a modified version of this index that only considers (ground-truth) foreground pixels (FARI). Across a test set of 10,000 4-frame video sequences of 3 randomly sampled moving objects each, OBAI achieved an average ARI and FARI of 0.948 and 0.939, respectively (where 1 means perfect accuracy and 0 equals chance-level performance), and a MSE of 9.51\u00d7 10\u22124 (note that pixel values were in the range of [0, 1]). For comparison, a re-implementation of IODINE, trained on 50,000 static images of 3 dSprite objects, achieved an ARI of 0.081, FARI of 0.856 and MSE of 1.63 \u00d7 10\u22123 (on a test set of identically sampled static images). The very low ARI score reflects the fact that IODINE has no built-in incentive to assign background pixels to their own object slot. OBAI, on the other hand, has to account for the effects of actions being applied to the background, which must not affect the dynamics of the foreground objects. Thus, for OBAI to accurately model the dynamics in the training data, it must learn not to assign background pixels to the segmentation masks of foreground objects, lest and action might be placed on one of these spurious pixels."
        },
        {
            "heading": "3.2 Predicting the future state of objects",
            "text": "An advantage of our approach is that the network can predict future states of the world at the level of objects using the learned state dynamics. Figure 3 shows three examples of the network predicting future video frames. The first 4 video frames are used by the network to infer the state. Afterwards, we extrapolate the inferred state and dynamics of the last observed video frame into the future, and decode the thus-predicted latent states into predicted video frames. These predictions are highly accurate, with a MSE of 4\u00d7 10\u22123."
        },
        {
            "heading": "3.3 Goal-directed action planning",
            "text": "Can OBAI learn and accomplish behavioral objectives? As a first foray into this question, we asked OBAI to learn fixed preference distributions defined in the true state-space of the environment, using the method described in section 2.4.1. Specifically, we placed a Gaussian preference distribution on the location of the object and had the network learn the corresponding preference in its latent space from a set of 10,000 videos (annotated with the requisite importance weights). We then presented the network with static images of dSprite objects in random locations, and asked it to \"imagine\" the action that would bring the state of the environment into alignment with the learned preference. Finally, we applied this\nimagined action to the latent state, and decoded the image that would result from this. As illustrated in Figure 4, the network is reliably able to imagine actions that would accomplish the goals we wanted it to learn."
        },
        {
            "heading": "4 Conclusion",
            "text": "This work seeks to bridge an important gap in the field. On the one hand, computer vision research has developed object-based models, but these only perform passive inference. On the other hand, there is a wealth of research on behavioral learning (e.g. reinforcement learning and active inference), which has not generally leveraged objects as an inductive bias, built into the network architecture (cf. [7,8]). OBAI reconciles these two lines of research, by extending object-based visual inference models with action-based dynamics. We showed that OBAI can accurately track and predict the dynamics (and other properties) of simple objects whose movements are perturbed by actions \u2013 an important prerequisite for an agent to plan its own actions. In addition, we presented an efficient method for internalizing goals as a preference distribution over latent states, and showed that the agent can infer the actions necessary to accomplish these goals, at the\nsame abstract level of reasoning. While our results are preliminary, they are an important proof-of-concept, establishing the potential of our approach. In future work, we aim to scale OBAI to more naturalistic environments, and more cognitively demanding tasks."
        },
        {
            "heading": "5 Acknowledgements",
            "text": "RSvB is supported by Human Brain Project Specific Grant Agreement 3 grant ID 643945539: \"SPIKEFERENCE\"."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Active-dSprites",
            "text": "Active-dSprites can be thought of as an \"activated\" version of the various multidSprites datasets that have been used in previous work on object-based visual inference (e.g. [4,5]). Not only does it include dynamics, but these dynamics can can be acted on by an agent. Thus, active-dSprites is an interactive environment, rather than a dataset.\nObjects in the active-dSprites environment are 2.5-D shapes (squares, ellipses and hearts): they have no depth dimension of their own, but can occlude each other within the depth dimension of the image. When an active-dSprites instance is intialized, object shapes, positions, sizes and colors are all sampled Uniformly at random. Initial velocities are drawn from a Normal distribution with mean 0 and standard deviation 4 (in units of pixels). Shape colors are sampled at discrete intervals spanning the full range of RGB-colors. Shapes are presented in random depth order against a solid background with a random grayscale color. Accelerations in action fields (at those locations that have been selected to incur a non-zero acceleration) are drawn from a Normal distribution with mean 0 and s.d. of 4."
        },
        {
            "heading": "B Network architectures",
            "text": "The OBAI architecture discussed in this paper consists of two separate IAI modules, each of which in turn contains a refinement and a decoder module. The first IAI module concerns the inference of the state beliefs q({s\u2020(k)}) \u2013 we term this the state inference module. The second IAI module infers the object action beliefs q({a(k)}), and we refer to this as the action inference module.\nWe ran inference for a total of F \u00d7 4 iterations, where F is the number of frames in the input. Inference initially concerns just the first video frame, and beliefs for this frame are initialized to \u03bb0, which is learned during training. After every 4 iterations, an additional frame is added to the inference window, and the beliefs for this new frame are initialized predictively, by extrapolating the object dynamics inferred up to that point. This procedure minimizes the risk that object representations are swapped between slots across frames, which can constitute a local minimum for the ELBO loss and leads to poor inference. We trained all IAI modules with K=4 object slots."
        },
        {
            "heading": "B.1 State inference module",
            "text": "This module used a latent dimension of 16. Note that, in the output of the refinement network, this number is doubled once as each latent belief is encoded by a mean and variance, and then doubled again as we represent (and infer) both the states and their first-order derivatives. In the decoder, the latent dimension is doubled only once, as the state derivatives do not enter into the reconstruction\nof a video frame. As in IODINE [4], we use a spatial broadcast decoder, meaning that the latent beliefs are copied along a spatial grid with the same dimensions as a video frame, and each latent vector is concatenated with the (x, y) coordinate of its grid location, before passing through a stack of transposed convolution layers. Decoder and refinement network architectures are summarized in the tables below. The refinement network takes in 16 image-sized inputs, which are identical to those used in IODONE [4], except that we omit the leave-one-out likelihoods. Vector-sized inputs join the network after the convolutional stage (which processes only the image-sized inputs), and consist of the variational parameters and (stochastic estimates of) their gradients."
        },
        {
            "heading": "Decoder",
            "text": "Type Size/#Chan. Act. func. Comment\nInput (\u03bb) 32 Broadcast 34 Appends coordinate channels ConvT 5\u00d7 5 32 ELU ConvT 5\u00d7 5 32 ELU ConvT 5\u00d7 5 32 ELU ConvT 5\u00d7 5 32 ELU ConvT 5\u00d7 5 4 Outputs RGB + mask"
        },
        {
            "heading": "Refinement network",
            "text": "Type Size/#Chan. Act. func. Comment\nLinear 64 LSTM 128 tanh Concat [...,\u03bb,\u2207\u03bbL] 256 Appends vector-sized inputs Linear 128 ELU Flatten 800 Conv 5\u00d7 5 32 ELU Conv 5\u00d7 5 32 ELU Conv 5\u00d7 5 32 ELU Inputs 16"
        },
        {
            "heading": "B.2 Action inference module",
            "text": "The action inference module does not incorporate a decoder network, as the quality of the action beliefs is computed by evaluating equation 7 and plugging this into the ELBO loss from equation 8. While this requires some additional tricks (see Appendix D), no neural network is required for this. This module does include a (shallow) refinement network, which is summarized in the table below.\nThis network takes as input the current variational parameters \u03bba(k) (2 means and 2 variances), their gradients, and the \u2018expected object action\u2019, \u2211 i m\u0302ik\u03c8i."
        },
        {
            "heading": "Refinement network",
            "text": "Type Size/#Chan. Act. func. Comment\nLinear 4 LSTM 32 tanh Inputs 10"
        },
        {
            "heading": "C Training procedure",
            "text": "The above network architecture was trained on pre-generated experience with the active-dSprites environment, as described in the main text. The training set comprised 50,000 videos of 4 frames each. An additional validation set of 10,000 videos was constructed using the same environment parameters as the training set, but using a different random seed. Training was performed using the ADAM optimizer [REF] with default parameters and an initial learning rate of 3\u00d710\u22124. This learning rate was reduced automatically by a factor 3 whenever the validation loss had not decreased in the last 10 training epochs, down to a minimum learning rate of 3\u00d7 10\u22125. Training was performed with a batch size of 64 (16 \u00d7 4 GPUs), and was deemed to have converged after 245 epochs."
        },
        {
            "heading": "C.1 Modified ELBO loss",
            "text": "OBAI optimizes an ELBO loss for both learning and inference. The basic form of this loss is given by equation 8. In practice, we modify this loss in two ways (similar to previous work, e.g. [4]). First, we re-weight the reconstruction term in the ELBO loss as follows:\nL\u03b2 = \u2212 T\u2211 t=0 [ H ( q ( {s\u2020(k)t ,a (k) t } )) + \u03b2E q({s(k)t }) [log p(ot|{s(k)t })]\n+ \u2211 k E q(a (k) t ) [log p(a (k) t |\u03a8t)] + \u2211 k E q ( s \u2020(k) t ,s \u2020(k) t\u22121 ,a (k) t\u22121 )[log p(s\u2020(k)t |s\u2020(k)t\u22121 ,a(k)t\u22121)] ] (15)\nSecond, we train the network to minimize not just the loss at the end of the inference iterations through the network, but a composite loss that also includes the loss after earlier iterations. Let L(n)\u03b2 be the loss after n inference iterations, then the composite loss is given by:\nLcomp = Niter\u2211 n=1 n Niter L(n)\u03b2 (16)"
        },
        {
            "heading": "C.2 Hyperparameters",
            "text": "OBAI includes a total of 4 hyperparameters: (1) the loss-reweighting coefficient \u03b2 (see above); (2) the variance of the pixels around their predicted values, \u03c32o ; (3) the variance of the noise in the latent space dynamics, \u03c32s ; and (4) the variance of the noise in the object actions, \u03c32\u03c8. The results described in the current work were achieved with the following settings:\nParam. Value\n\u03b2 5.0 \u03c3o 0.3 \u03c3s 0.1 \u03c3\u03c8 0.3\nD Computing Eq(a(k))[log p(a(k)|\u03a8)]\nThe expectation under q(a(k)) of log p(a(k)|\u03a8), which appears in the ELBO loss (eq. 8), cannot be computed in closed form, because the latter log probability requires us to marginalize over all possible configurations of the pixel-to-object assignments, and to do so inside of the logarithm. That is:\nlog p(a(k)|\u03a8) = \u2211 m log ( p(a(k)|\u03a8,m)p(m|{s(k)}) ) (17)\n= log ( Ep(m|{s(k)})[p(a (k)|\u03a8,m)] )\n(18)\nHowever, note that within the ELBO loss, we want to maximize the expected value of this quantity (as its negative appears in the ELBO, which we want to minimize). From Jensen\u2019s inequality, we have:\nEp(m|{s(k)})[log p(a (k)|\u03a8,m)] \u2264 log ( Ep(m|{s(k)})[p(a (k)|\u03a8,m)] )\n(19)\nTherefore, the l.h.s. of this equation provides a lower bound on the quantity we want to maximize. Thus, we can approximate our goal by maximizing this lower bound instead. This is convenient, because this lower bound, and its expectation under q(a(k)) can be approximated through sampling:\nEq(a(k)) [ Ep(m|{s(k)})[log p(a (k)|\u03a8,m)] ] \u2248 1 Nsamples \u2211 j log p(a (k)\u2217 j |\u03a8,m \u2217 j ) (20)\n= 1\nNsamples \u2211 j logN ( a (k)\u2217 j ; \u2211 i m\u0302 \u2217(i) jk \u03c8i, \u03c3 2 \u03c8I ) (21)\nm\u0302 \u2217(i) j \u223c p(mi|{s (k)}), a(k)\u2217j \u223c q(a (k)), s (k)\u2217 j \u223c q(s (k)) (22)\nwhere we slightly abuse notation in the sampling of the pixel assignments, as a vector is sampled from a distribution over a categorical variable. The reason\nthis results in a vector is because this sampling step uses the Gumbel-Softmax trick [19], which is a differentiable method for sampling categorical variables as \"approximately one-hot\" vectors. Thus, for every pixel i, we sample a vector m\u0302 \u2217(i) j , such that the k-th entry of this vector, m\u0302 \u2217(i) jk , denotes the \"soft-binary\" condition of whether pixel i belongs to object k. In practice, we use Nsamples = 1, based on the intuition that this will still yield a good approximation over many training instances, and that we rely on the refinement network to learn to infer good beliefs. The Gumbel-Softmax sampling method depends on a temperature \u03c4 , which we gradually reduce across training epochs, so that the samples gradually better approximate the ideal one-hot vectors.\nIt is worth noting that, as the entropy of p(m|{s(k)}) decreases (i.e. as object slots \"become more certain\" about which pixels are theirs), the bound in equation 19 becomes tighter. In the limit as the entropy becomes 0, the network is perfectly certain about the pixel assignments, and so the distribution collapses to a point mass. The expectation then becomes trivial, and so the two sides of eq. 19 become equal. Sampling the pixel assignments is equally trivial in this case, as the distribution has collapsed to permit only a single value for each assignment. In short, at this extreme point, the procedure becomes entirely deterministic. In our data, we typically observe very low entropy for p(m|{s(k)}), and so we likely operate in a regime close to the deterministic one, where the approximation is very accurate."
        }
    ],
    "title": "Object-based active inference",
    "year": 2022
}