{
    "abstractText": "Conventional sensor-based localization relies on high-precision maps, which are generally built using specialized mapping techniques involving high labor and computational costs. In the architectural, engineering and construction industry, Building Information Models (BIM) are available and can provide informative descriptions of environments. This paper explores an effective way to localize a mobile 3D LiDAR sensor on BIM-generated maps considering both geometric and semantic properties. First, original BIM elements are converted to semantically augmented point cloud maps using categories and locations. After that, a coarse-to-fine semantic localization is performed to align laser points to the map based on iterative closest point registration. The experimental results show that the semantic localization can track the pose successfully with only one LiDAR sensor, thus demonstrating the feasibility of the proposed mapping-free localization framework. The results also show that using semantic information can help reduce localization errors on BIM-generated maps.",
    "authors": [
        {
            "affiliations": [],
            "name": "Huan Yina"
        },
        {
            "affiliations": [],
            "name": "Zhiyi Linc"
        },
        {
            "affiliations": [],
            "name": "Justin K.W. Yeohb"
        }
    ],
    "id": "SP:af3dca655c792f6abfce00dcfbd8e9576cb2cc4a",
    "references": [
        {
            "authors": [
                "H. Peel",
                "S. Luo",
                "A. Cohn",
                "R. Fuentes"
            ],
            "title": "Localisation of a mobile robot for bridge bearing inspection",
            "venue": "Automation in Construction 94 ",
            "year": 2018
        },
        {
            "authors": [
                "P. Kim",
                "J. Chen",
                "Y.K. Cho"
            ],
            "title": "Slam-driven robotic mapping and registration of 3d point clouds",
            "venue": "Automation in Construction 89 ",
            "year": 2018
        },
        {
            "authors": [
                "N. Kayhani",
                "W. Zhao",
                "B. McCabe",
                "A.P. Schoellig"
            ],
            "title": "Tag-based visualinertial localization of unmanned aerial vehicles in indoor construction environments using an on-manifold extended kalman filter",
            "venue": "Automation in Construction 135 ",
            "year": 2022
        },
        {
            "authors": [
                "A. Prorok",
                "A. Martinoli"
            ],
            "title": "Accurate indoor localization with ultrawideband using spatial models and collaboration",
            "venue": "The International Journal of Robotics Research 33 ",
            "year": 2014
        },
        {
            "authors": [
                "J. Zhang",
                "S. Singh"
            ],
            "title": "Loam: Lidar odometry and mapping in real-time",
            "venue": "in: Robotics: Science and Systems",
            "year": 2014
        },
        {
            "authors": [
                "T. Qin",
                "P. Li",
                "S. Shen"
            ],
            "title": "Vins-mono: A robust and versatile monocular visual-inertial state estimator",
            "venue": "IEEE Transactions on Robotics 34 ",
            "year": 2018
        },
        {
            "authors": [
                "H. Yin",
                "L. Tang",
                "X. Ding",
                "Y. Wang",
                "R. Xiong"
            ],
            "title": "Locnet: Global localization in 3d point clouds for mobile vehicles",
            "venue": "in: 2018 IEEE Intelligent Vehicles Symposium, IEEE",
            "year": 2018
        },
        {
            "authors": [
                "M. B\u00fcrki",
                "M. Dymczyk",
                "I. Gilitschenski",
                "C. Cadena",
                "R. Siegwart",
                "J. Nieto"
            ],
            "title": "Map management for efficient long-term visual localization in outdoor environments",
            "venue": "in: 2018 IEEE Intelligent Vehicles Symposium, IEEE",
            "year": 2018
        },
        {
            "authors": [
                "X. Liu",
                "C. Yuan",
                "F. Zhang"
            ],
            "title": "Targetless extrinsic calibration of multiple small fov lidars and cameras using adaptive voxelization",
            "venue": "IEEE Transactions on Instrumentation and Measurement 71 ",
            "year": 2022
        },
        {
            "authors": [
                "P. Kr\u00fcsi",
                "B. B\u00fccheler",
                "F. Pomerleau",
                "U. Schwesinger",
                "R. Siegwart",
                "P. Furgale"
            ],
            "title": "Lighting-invariant adaptive route following using iterative closest point matching",
            "venue": "Journal of Field Robotics 32 ",
            "year": 2015
        },
        {
            "authors": [
                "X. Ding",
                "Y. Wang",
                "R. Xiong",
                "D. Li",
                "L. Tang",
                "H. Yin",
                "L. Zhao"
            ],
            "title": "Persistent stereo visual localization on cross-modal invariant map",
            "venue": "IEEE Transactions on Intelligent Transportation Systems 21 ",
            "year": 2019
        },
        {
            "authors": [
                "C. Kim",
                "H. Son",
                "C. Kim"
            ],
            "title": "Fully automated registration of 3d data to a 3d cad model for project progress monitoring",
            "venue": "Automation in Construction 35 ",
            "year": 2013
        },
        {
            "authors": [
                "K. Asadi",
                "H. Ramshankar",
                "M. Noghabaei",
                "K. Han"
            ],
            "title": "Real-time image localization and registration with bim using perspective alignment for indoor monitoring of construction",
            "venue": "Journal of Computing in civil Engineering 33 ",
            "year": 1061
        },
        {
            "authors": [
                "H. Blum",
                "J. Stiefel",
                "C. Cadena",
                "R. Siegwart",
                "A. Gawel"
            ],
            "title": "Precise robot localization in architectural 3d plans",
            "venue": "in: 38th International Symposium on Automation and Robotics in Construction, International Association for Automation and Robotics in Construction",
            "year": 2021
        },
        {
            "authors": [
                "J. Chen",
                "S. Li",
                "W. Lu"
            ],
            "title": "Align to locate: Registering photogrammetric point clouds to bim for robust indoor localization",
            "venue": "Building and Environment 209 ",
            "year": 2022
        },
        {
            "authors": [
                "X. Chen",
                "A. Milioto",
                "E. Palazzolo",
                "P. Giguere",
                "J. Behley",
                "C. Stachniss"
            ],
            "title": "Suma++: Efficient lidar-based semantic slam",
            "venue": "in: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "Y. Wei",
                "B. Akinci"
            ],
            "title": "A vision and learning-based indoor localization and semantic mapping framework for facility operations and management",
            "venue": "Automation in Construction 107 ",
            "year": 2019
        },
        {
            "authors": [
                "D. Acharya",
                "R. Tennakoon",
                "S. Muthu",
                "K. Khoshelham",
                "R. Hoseinnezhad",
                "A. Bab-Hadiashar"
            ],
            "title": "Single-image localisation using 3d models: Combining hierarchical edge maps and semantic segmentation for domain adaptation",
            "venue": "Automation in Construction 136 ",
            "year": 2022
        },
        {
            "authors": [
                "F. Boniardi",
                "T. Caselitz",
                "R. K\u00fcmmerle",
                "W. Burgard"
            ],
            "title": "Robust lidar-based localization in architectural floor plans",
            "venue": "in: 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2017
        },
        {
            "authors": [
                "F. Boniardi",
                "T. Caselitz",
                "R. K\u00fcmmerle",
                "W. Burgard"
            ],
            "title": "A pose graph-based localization system for long-term navigation in cad floor plans",
            "venue": "Robotics and Autonomous Systems 112 ",
            "year": 2019
        },
        {
            "authors": [
                "X. Wang",
                "R.J. Marcotte",
                "E. Olson"
            ],
            "title": "Glfp: Global localization from a floor plan",
            "venue": "in: 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2019
        },
        {
            "authors": [
                "L. Gao",
                "L. Kneip"
            ],
            "title": "Fp-loc: Lightweight and drift-free floor planassisted lidar localization",
            "venue": "in: 2022 International Conference on Robotics and Automation",
            "year": 2022
        },
        {
            "authors": [
                "C. Follini",
                "V. Magnago",
                "K. Freitag",
                "M. Terzer",
                "C. Marcher",
                "M. Riedl",
                "A. Giusti",
                "D.T. Matt"
            ],
            "title": "Bim-integrated collaborative robotics for application in building construction and maintenance",
            "venue": "Robotics 10 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Kim",
                "M. Peavy",
                "P.-C. Huang",
                "K. Kim"
            ],
            "title": "Development of bim-integrated construction robot task planning and simulation system",
            "venue": "Automation in Construction 127 ",
            "year": 2021
        },
        {
            "authors": [
                "K. Kim",
                "M. Peavy"
            ],
            "title": "Bim-based semantic building world modeling for robot task planning and execution in built environments",
            "venue": "Automation in Construction 138 ",
            "year": 2022
        },
        {
            "authors": [
                "R. Hendrikx",
                "P. Pauwels",
                "E. Torta",
                "H.P. Bruyninckx"
            ],
            "title": "M",
            "venue": "van de Molengraft, Connecting semantic building information models and robotics: An application to 2d lidar-based localization, in: 2021 IEEE International Conference on Robotics and Automation, IEEE",
            "year": 2021
        },
        {
            "authors": [
                "C. Chen",
                "B. Wang",
                "C.X. Lu",
                "N. Trigoni",
                "A. Markham"
            ],
            "title": "A survey on deep learning for localization and mapping: Towards the age of spatial machine intelligence",
            "venue": "arXiv preprint arXiv:2006.12567 ",
            "year": 2020
        },
        {
            "authors": [
                "S. Lowry",
                "N. S\u00fcnderhauf",
                "P. Newman",
                "J.J. Leonard",
                "D. Cox",
                "P. Corke",
                "M.J. Milford"
            ],
            "title": "Visual place recognition: A survey",
            "venue": "IEEE Transactions on Robotics 32 ",
            "year": 2015
        },
        {
            "authors": [
                "M. Elhousni",
                "X. Huang"
            ],
            "title": "A survey on 3d lidar localization for autonomous vehicles",
            "venue": "in: 2020 IEEE Intelligent Vehicles Symposium, IEEE",
            "year": 2020
        },
        {
            "authors": [
                "H. Yin",
                "R. Chen",
                "Y. Wang",
                "R. Xiong"
            ],
            "title": "Rall: end-to-end radar localization on lidar map using differentiable measurement model",
            "venue": "IEEE Transactions on Intelligent Transportation Systems 23 ",
            "year": 2022
        },
        {
            "authors": [
                "Z. Li",
                "M.H. Ang",
                "D. Rus"
            ],
            "title": "Online localization with imprecise floor space maps using stochastic gradient descent",
            "venue": "in: 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2020
        },
        {
            "authors": [
                "M. Dreher",
                "H. Blum",
                "R. Siegwart",
                "A. Gawel"
            ],
            "title": "Global localization in meshes",
            "venue": "in: 38th International Symposium on Automation and Robotics in Construction, International Association for Automation and Robotics in Construction",
            "year": 2021
        },
        {
            "authors": [
                "S. Ercan",
                "H. Blum",
                "A. Gawel",
                "R. Siegwart",
                "F. Gramazio",
                "M. Kohler"
            ],
            "title": "Online synchronization of building model for on-site mobile robotic construction",
            "venue": "in: 37th International Symposium on Automation and Robotics in Construction, International Association for Automation and Robotics in Construction",
            "year": 2020
        },
        {
            "authors": [
                "C. Toft",
                "E. Stenborg",
                "L. Hammarstrand",
                "L. Brynte",
                "M. Pollefeys",
                "T. Sattler",
                "F. Kahl"
            ],
            "title": "Semantic match consistency for long-term visual localization",
            "venue": "in: European Conference on Computer Vision",
            "year": 2018
        },
        {
            "authors": [
                "S.A. Parkison",
                "L. Gan",
                "M.G. Jadidi",
                "R.M. Eustice"
            ],
            "title": "Semantic iterative closest point through expectation-maximization",
            "venue": "in: Proceedings of the British Machine Vision Conference",
            "year": 2018
        },
        {
            "authors": [
                "S.W. Chen",
                "G.V. Nardari",
                "E.S. Lee",
                "C. Qu",
                "X. Liu",
                "R.A.F. Romero",
                "V. Kumar"
            ],
            "title": "Sloam: Semantic lidar odometry and mapping for forest inventory",
            "venue": "IEEE Robotics and Automation Letters 5 ",
            "year": 2020
        },
        {
            "authors": [
                "Y. Tan",
                "S. Li",
                "H. Liu",
                "P. Chen",
                "Z. Zhou"
            ],
            "title": "Automatic inspection data collection of building surface based on bim and uav",
            "venue": "Automation in Construction 131 ",
            "year": 2021
        },
        {
            "authors": [
                "Z. Ma",
                "S. Cai",
                "N. Mao",
                "Q. Yang",
                "J. Feng",
                "P. Wang"
            ],
            "title": "Construction quality management based on a collaborative system using bim and indoor positioning",
            "venue": "Automation in Construction 92 ",
            "year": 2018
        },
        {
            "authors": [
                "P. Cignoni",
                "C. Rocchini",
                "R. Scopigno"
            ],
            "title": "Metro: measuring error on simplified surfaces",
            "venue": "in: Computer graphics forum, volume 17, Wiley Online Library",
            "year": 1998
        },
        {
            "authors": [
                "P.J. Besl",
                "N.D. McKay"
            ],
            "title": "A method for registration of 3-d shapes",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 14 ",
            "year": 1992
        },
        {
            "authors": [
                "F. Pomerleau",
                "F. Colas",
                "R. Siegwart"
            ],
            "title": "A review of point cloud registration algorithms for mobile robotics",
            "venue": "Foundations and Trends in Robotics 4 ",
            "year": 2015
        },
        {
            "authors": [
                "P.J. Huber"
            ],
            "title": "Robust estimation of a location parameter",
            "venue": "in: Breakthroughs in statistics, Springer",
            "year": 1992
        },
        {
            "authors": [
                "A.W. Fitzgibbon"
            ],
            "title": "Robust registration of 2d and 3d point sets",
            "venue": "Image and vision computing 21 ",
            "year": 2003
        },
        {
            "authors": [
                "F. Pomerleau",
                "F. Colas",
                "R. Siegwart",
                "S. Magnenat"
            ],
            "title": "Comparing icp variants on real-world data sets",
            "venue": "Autonomous Robots 34 ",
            "year": 2013
        },
        {
            "authors": [
                "Z. Zhang",
                "D. Scaramuzza"
            ],
            "title": "A tutorial on quantitative trajectory evaluation for visual (-inertial) odometry",
            "venue": "in: 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE",
            "year": 2018
        },
        {
            "authors": [
                "F. Nie",
                "W. Zhang",
                "Y. Wang",
                "Y. Shi",
                "Q. Huang"
            ],
            "title": "A forest 3-d lidar slam system for rubber-tapping robot based on trunk center atlas",
            "venue": "IEEE/ASME Transactions on Mechatronics ",
            "year": 2021
        },
        {
            "authors": [
                "W. Hess",
                "D. Kohler",
                "H. Rapp",
                "D. Andor"
            ],
            "title": "Real-time loop closure in 2d lidar slam",
            "venue": "in: 2016 IEEE international conference on robotics and automation, IEEE",
            "year": 2016
        },
        {
            "authors": [
                "Q. Zou",
                "Q. Sun",
                "L. Chen",
                "B. Nie",
                "Q. Li"
            ],
            "title": "A comparative analysis of lidar slam-based indoor navigation for autonomous vehicles",
            "venue": "IEEE Transactions on Intelligent Transportation Systems 23 ",
            "year": 2022
        },
        {
            "authors": [
                "K. Chen"
            ],
            "title": "B",
            "venue": "T. Lopez, A.-a. Agha-mohammadi, A. Mehta, Direct lidar odometry: Fast localization with dense point clouds, IEEE Robotics and Automation Letters 7 ",
            "year": 2022
        },
        {
            "authors": [
                "E. Jelavic",
                "J. Nubert",
                "M. Hutter"
            ],
            "title": "Open3d slam: Point cloud based mapping and localization for education",
            "venue": "in: Robotic Perception and Mapping: Emerging Techniques, ICRA 2022 Workshop, ETH Zurich, Robotic Systems Lab",
            "year": 2022
        },
        {
            "authors": [
                "H. Yin",
                "Y. Wang",
                "X. Ding",
                "L. Tang",
                "S. Huang",
                "R. Xiong"
            ],
            "title": "3d lidar-based global localization using siamese neural network",
            "venue": "IEEE Transactions on Intelligent Transportation Systems 21 ",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "Conventional sensor-based localization relies on high-precision maps, which are generally built using specialized mapping techniques involving high labor and computational costs. In the architectural, engineering and construction industry, Building Information Models (BIM) are available and can provide informative descriptions of environments. This paper explores an effective way to localize a mobile 3D LiDAR sensor on BIM-generated maps considering both geometric and semantic properties. First, original BIM elements are converted to semantically augmented point cloud maps using categories and locations. After that, a coarse-to-fine semantic localization is performed to align laser points to the map based on iterative closest point registration. The experimental results show that the semantic localization can track the pose successfully with only one LiDAR sensor, thus demonstrating the feasibility of the proposed mapping-free localization framework. The results also show that using semantic information can help reduce localization errors on BIM-generated maps.\nKeywords: building information modeling, LiDAR, localization, semantic, iterative closest point"
        },
        {
            "heading": "1. Introduction",
            "text": "Localization is an essential capability for robot navigation that estimates the position and orientation of a robot. Almost all construction robots, whether tele-operated or autonomous require the estimated poses from the localization module to achieve safe human operation or self-navigation [1, 2].\nWith the development of sensor technologies, indoor localization can be achieved by deploying AprilTag [3], ultrawideband [4] or other signal emitters in buildings. Such methods rely on the distribution of sensors and inherently lack flexibility within large built-up environments. Instead of deploying sensors in such environments, a more popular approach is to utilize the perception capabilities of onboard sensors, such as laser scanners and cameras, which can improve the generalizability of the localization module in large scenes.\nIn robotics, a general localization approach is the Simultaneous Localization and Mapping (SLAM) system [5, 6], which achieves mapping and localization simultaneously using onboard sensors. However, for some long-term applications that operate under stable conditions, i.e., a quadruped robot working daily on building inspection, the mapping process of SLAM is redundant because the generated map is almost invariant in each run of SLAM. Besides that, a complete SLAM system requires high computing resources and multiple additional modules to guarantee both efficiency and accuracy, such as online loop closing [7], map management [8] and sensor calibration [9], leading to high costs for long-term operations.\nA two-stage approach is widely used to address this problem: first mapping and then metric localization within the known\n\u2217Corresponding author\nmap [10, 11]. In this approach, map building is required only once and after that localization on the map is able to handle the pose tracking for long-term operations, thereby reducing the complexity of repetitive SLAM processes. In the Architectural, Engineering and Construction (AEC) industry, some models or representations are directly available, such as Computeraided Design (CAD) or Building Information Models (BIM). These map-like representations contain informative measurements that are human readable. We propose the idea that highcost pre-mapping may not always be necessary in known built environments, and mapping-free localization could be an alternative choice.\nOn the other hand, architectural CAD and BIM are designed for construction and building management so they are not localization-oriented. To bridge the gap between architectural models and pose estimation, a number of research works proposed to align laser points or visual images to the as-designed models [12, 13, 14, 15]. However, almost all alignment approaches were performed using only geometric properties of observed points and models. In recent years, with the popularity of BIM, semantically rich models provide high-level semantic information for building construction and management. This semantic information is helpful for scene understanding and is easy to obtain compared to traditional CAD models. Thus, we hypothesize that the semantic property of BIM could help improve the performance of robot localization.\nDeep learning techniques have been widely used to build a semantic localization method for feature extraction and data association [16, 17, 18]. Large amounts of labeled data is required to train neural networks and these existing works can not guarantee the generalization ability in unseen environments.\nPreprint submitted to Elsevier November 30, 2022\nIn addition, learning-based localization methods are generally computationally expensive with high time costs in the inference stage, especially for real-time 3D LiDAR points, leading to inefficient applications when using resource-constrained devices.\nIt is concluded that a desirable localization in BIM requires both effectiveness and efficiency for applications in the real world. In this paper, a novel learning-free framework is proposed to achieve localization on BIM-generated maps with only one 3D mobile LiDAR sensor, as shown in Figure 1. Specifically, the entire framework consists of two pipelines: offline BIM-to-Map conversion and online coarse-to-fine localization. The offline pipeline can convert the as-designed BIM to semantically augmented point cloud maps. After that, these semantic maps are utilized to filter input laser points, and pose tracking is achieved by performing Iterative Closest Point (ICP) on the filtered points. The entire framework requires no deep learning for feature extraction or pose regression, making it totally interpretable. Finally, extensive experiments are conducted using our self-collected multi-session dataset in a real-world university building.\nOur major contributions are summarized as follows:\n\u2022 A pipeline is built to effectively convert BIM to semantic point cloud maps, which can bridge the gap between digital representations and localization-oriented maps. The pipeline does not require manually labeled data.\n\u2022 The semantic information of BIM is utilized to filter laser points and weight data associations, thus building a semantic-aided LiDAR localization on BIM-generated maps.\n\u2022 The proposed method is validated in a real-world building via multi-session tests. Experimental results validate the feasibility and effectiveness of semantic localization on BIM-generated maps.\nThe rest of this paper is organized as follows: the related work is presented in Section 2. The proposed semantic localization framework is introduced in Section 3. Section 4 reports the experimental set-up and results on our self-collected datasets. Section 5 presents conclusions and future studies."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Robot localization on CAD or BIM-based maps",
            "text": "Many research publications have reviewed pose estimation topics from different perspectives, including deep learningbased [28], sensor-based [29, 30, 31], etc. These research papers mainly focused on robot localization on visual or lidar maps, which require SLAM or data collection for pre-mapping. In this study, we propose to achieve mapping-free localization, and the related works mainly focus on CAD or BIM-based localization in this subsection.\nFloor plans or point clouds can be generated from CAD models for LiDAR localization [32, 19, 20, 21, 14, 22]. Researchers in [32] proposed to localize a 2D laser scanner on floor plans and hand-drawn maps using stochastic gradient descent. At\nthe back-end, pose graphs were built in [19, 20] to increase the localization robustness on floor plan-based maps. As for localization in 3D space, ICP-based alignment is considered an effective method to track the robot pose [14]. Other than the point-based ICP method, meshes were also used for robot global localization without the need of an initial guess in [33]. Recently, researchers in [34] proposed a novel interface to connect building construction and map representation, which could also detect deviations between as-designed and as-built models via localization results.\nCompared to traditional CAD models, BIM is more interoperable in the construction industry and contains more semantic information that may be suitable for robot navigation. For single-frame-based localization, photogrammetric point clouds can be aligned to BIM [15] for camera pose estimation from scratch. As for pose tracking, visual-based pose tracking was also demonstrated to be effective [13], in which camera poses were estimated by aligning images to BIM models. In [17], learning-based visual localization was proposed for facility operations and management. Generally, deep learning-based methods rely on pre-trained neural networks for feature extraction or pose regression, bringing difficulties for debugging and deployment in the real world. Researchers in [27] extracted semantic features without learning and also performed robot localization in BIM using 2D laser scans. Some recent studies [23, 25, 26] also used BIMs as maps in Gazebo for robot planning tasks, in which Adaptive Monte Carlo Localization (AMCL) [24] was used to track the mobile robot pose. These recent studies inspire us that it is feasible to integrate BIM into robotic systems as maps.\nTable 1 presents several representative studies with brief descriptions. Most of these works in the table used some other sensors as support, such as wheel odometry [21, 27] and IMU [22, 23], which will increase the robustness of the localization module. But on the other hand, these sensors bring higher hardware costs and potential calibration problems. Besides that, many works utilized state estimators at the back-end, i.e., graph optimization-based [20, 21] and particle filter-based [23, 25], which are mainly designed for multiple sensor fusion. These methods are more computational expensive compared to those that only use scan matching at the front-end [14]. In this study, only one LiDAR scanner is used for pose estimation, and the method can be deployed on a resource-constrained laptop device.\nFurthermore, most studies only used the geometric information for localization. In [14] and [27], semantic information was integrated into LiDAR localization system. Specifically, in [14], semantic information was generated from learningbased image segmentation, and the researchers validated the method using a stationary robot. In [27], semantic features were extracted from laser scans and then matched to BIM-based database. The features were sparse compared to laser points, and a factor graph was also built to achieve pose estimation in [27]. Overall, our method is inspired by these existing studies, and we propose to build a semantic-aided LiDAR-only localization on BIM-generated point cloud maps."
        },
        {
            "heading": "2.2. Semantic mapping and localization",
            "text": "Semantic mapping and localization is a popular topic in the robotics community. Compared to geometric-only localization, semantic localization is able to closely mimic human understanding of the real world.\nSemantic information is easy to extract from visual images. A typical semantic-based visual localization is retrieving query images from database, namely visual place recognition or global localization [29]. Semantic information is also helpful for metric pose estimation [35]. Almost all semantic-based visual localization require deep neural networks for feature extraction at the front-end.\nAs for point cloud-based localization, researchers also proposed to use semantics to enhance the data matching. A semantic ICP-based registration was proposed and validated in RGBD dataset [36]. Similarly, semantic ICP was also used in [16] to localize a vehicle on the road. In [37], semantic-based LiDAR SLAM was tested in challenging forest environments, where tree trunks can be segmented by neural networks. Overall, semantic information was obtained by manually labeled data and trained networks in [36, 16, 37]. In this study, the input laser points are labeled using the BIM-generated maps, which could make the localization module more efficient."
        },
        {
            "heading": "3. Methodology",
            "text": ""
        },
        {
            "heading": "3.1. Overview",
            "text": "Given a mobile LiDAR scanner and a BIM file, we denote the input LiDAR data as Pk at timestamp k and the global point cloud map asM. The timestamp index is omitted for simplified representation of a single time instance in this paper. The main problem of metric localization is how to alignP to the reference M by estimating a transformation T = [R, t] \u2208 SE(3),R4\u00d74, where R and t are estimated rotation and translation respectively. The alignment must be precise and efficient to guarantee the estimation of Tk=1,2,\u00b7\u00b7\u00b7with sequential inputs Pk=1,2,\u00b7\u00b7\u00b7, or namely pose tracking. In the context of this paper, pose tracking and localization are deemed to have the same meaning.\nAs shown in Figure 1, the proposed semantic localization framework consists of two pipelines: offline BIM-to-Map conversion and online semantic localization with a mobile LiDAR scanner. The offline pipeline converts original BIM file to a localization-oriented point cloud map M, and also labels the map points with categories from BIM. The semantic localization pipeline is designed to track the mobile LiDAR scanner based on the referenceM and inputs Pk=1,2,\u00b7\u00b7\u00b7 ."
        },
        {
            "heading": "3.2. From BIM to semantic maps",
            "text": "Within the AEC industry, BIMs can be created by many software tools and has been used to support various construction\nprocesses, such as building inspection [38] and quality management [39]. To achieve robot or sensor localization in Euclidean space, precise metric maps are required instead of modeled information. In this study, the first challenge is how to generate localization-oriented point cloud maps from BIM files.\nThe BIM-to-Map conversion consists of three steps, shown in the upper part of Figure 1. The whole BIM of one building is first split into several separate BIMs according to different storeys. After that, the digital BIM files are converted to obj files using IfcOpenShell [40]. Finally, 3D point clouds are sampled from triangular meshes of obj files with a defined density [41]. There are several other sampling strategies in some software [42, 43], such as Monte-Carlo Sampling. Considering that density value is easily understood and defined by most users, we decide to use this strategy for point cloud generation in this paper. The final point cloud maps can be regarded as sub-maps of each floor in the building.\nOur experience has shown that it is better not to change the sequence of this conversion. In other words, if geometries are extracted from the whole BIM first without separating into storeys, the storey information of BIM is not captured. It then becomes more challenging to split a large geometric model or\npoint cloud map into storey-based sub-maps.\nUntil this step, the point cloud maps are generated using the geometric and storey information of BIM. However, BIM also contains rich semantic information compared to a typical geometry model and these semantic properties can be updated manually and dynamically, which may bring potential advantages for robot navigation. Thus we also propose to integrate the semantics into the offline BIM-to-Map process, thus maximizing the utilization of information from BIM for metric localization in this paper.\nTo achieve this, an automated approach for map labeling is used, which is simple but effective compared to the manually labeling process [37]. Let mi be a map point of M. To label the map point mi, Dynamo [44] is used to extract the category labels C and bounding boxes D of all elements in BIM. The minimum and maximum location points of one bounding box d j are notated as dmin = [ xmin, ymin, zmin\n]> and dmax =[ xmax, ymax, zmax\n]>, which can represent the coverage of d j in 3D space. With the extracted bounding boxes and labels, we then retrieve all the boxes and classify whether mi = [ xi, yi, zi\n]> is in a specific box. The classification criteria is as follows:\nmi is in d j =  xmin \u2264 xi \u2264 xmax 1 , ymin \u2264 yi \u2264 ymax zmin \u2264 zi \u2264 zmax\n0 , else.\n(1)\nSpecifically, to accelerate the semantic labeling process, a K-Dimensional (k-d) tree is built based on the center points of D. In summary, the proposed semantic labeling process is presented in detail in Algorithm 1. Note that the labels of C are not unique, which means some map points are in different boxes but with the same category label c(\u00b7), e.g., columns are with the same category label \u201cColumn\u201d. An example is presented in Figure 2 to help better understand the semantic mapping process, in which different colors represent different categories.\nOne might argue that the bounding box extraction in Dynamo is not so accurate and some map points could exist in multiple boxes, i.e, points may lie on the boundary of columns and floors, leading to the ambiguity of semantic map building. These are termed as mixed labeled points. In reality, there are relatively few of such ambiguously labeled points on the boundary of multiple elements.\nAnother problem is that the bounding box is not oriented from Dynamo in this study, and it might be so large that could\nAlgorithm 1 Semantic Labeling of Map Points Using BIM Input:\nA point cloud map from BIM:M = {mi} Bounding boxes from BIM:D = {d j} Category labels of each box: C = {c j}\nOutput: Build a k-d tree onD for mi inM do Dnn \u2190 nearest neighbor search of mi in the tree for d j inDnn do\nif Creteria (1) then c (mi) = c j break\nend if end for\nend for\ncover other elements. For example, a thin wall is from (0,0,0) to (10,10,10), but the size of its box is 10\u00d710\u00d710, which will make some points incorrectly labeled in this large box. Figure 8 presents two cases in the following experimental section. Overall, these mixed labeled or incorrectly labeled points have impact on the localization performance, but will not cause localization failures. This will be validated in the experimental section."
        },
        {
            "heading": "3.3. Semantic localization on BIM-generated maps",
            "text": "With the generated semantic map M, the online semantic localization pipeline aims to estimate transformations Tk=1,2,\u00b7\u00b7\u00b7 with inputs Pk=1,2,\u00b7\u00b7\u00b7. At timestamp k, the kernel of the localization problem is to align a LiDAR scan P to M, which can be achieved by minimizing the error function e (\u00b7) between two point clouds, stated as follows:\nT = arg min T\u2208SE(3) (e (M,TP)) (2)\nThen, data association is required to build the error function. We denote the data association as A = {(p,m) ; p \u2208 TP,m \u2208 M}, where (p,m) is a match between the transformed input LiDAR scan and the reference map. The error function is formulated as follows:\nT = arg min T\u2208SE(3)\n A\u2211 e (p,m)  (3)\nFurthermore, to build a robust data association, some relations can be used to build weights W = {w (p,m) \u2208 [0, 1] ; (p,m) \u2208 A}. W = 1 means all point matches are used without weights in error minimization. Consequently, the error function is as follows:\nT = arg min T\u2208SE(3)\n A\u2211 w (p,m) e (p,m)  (4)\nVarious point cloud registration methods have been proposed to minimize the error function in Equation (4). Generally,\namong these methods, ICP is one of the most widely used methods in the robotics community [45, 46]. Equation (4) is solved via ICP iteratively and the matches A and weightsW are updated in each iteration.\nICP has many variants in different robotic or computer vision applications. For the error term e (\u00b7), we estimate the normal vectors n of each map point m and use point-to-plane ICP for pose estimation. Thus the error metric term in Equation (4) can be expressed as:\ne (p,m) = \u2016(Rp + t \u2212m) \u00b7 n\u20162 (5) where R and t are rotation and translation of T respectively.\nAs for the weight term w (\u00b7) in Equation (4), we intend to use semantic associations to weight the data associations in this paper. Generally, the semantic associations are built from semantically labeled maps and sensor readings, as presented in previous studies [36, 16]. In this study, semantic mapsM can be built from BIM, but the raw input scan P are not labeled, leading to a difficulty in building semantic associations. Thus, the challenge is how to label the input laser points effectively and efficiently on BIM-generated maps. Then the labeled laser points can be utilized for a semantic-aided localization on semantic maps.\nTo address this challenge, a coarse-to-fine localization is proposed and it consists of three steps: original ICP, semantic filtering, semantic ICP. Original ICP can be regarded as the coarse step to achieve a preliminary result. Then semantic filtering step can achieve laser points labeling and selection based on the result of the first step. Finally, semantic ICP is used to refine the pose estimation. Semantic filtering and semantic ICP can be regarded as the fine step in the pipeline. The whole pipeline is illustrated in Figure 3 and Algorithm 2.\nFirstly, an original ICP is performed to minimize Equation (3). The data association of the last iteration can be recorded, denoted asAo, as follows:\nAlgorithm 2 Three-step Semantic Localization Input:\nSemantic map from BIM using Algorithm 1: M = {mi} Input LiDAR scan at timestamp t: P = {ps} Estimated transformation at k \u2212 1: Tk\u22121\nOutput: // Set initial guess Tinit = Tk\u22121 // Filter dense LiDAR scan datafilter(P) // Original point-to-plane ICP To,Ao \u2190 arg minT\u2208SE(3) ( e ( M,TinitP\n)) // Semantic labeling by Criteria (7) and point selection P\u2032 \u2190 fselect ( flabel (P,Ao)) // Weighted point-to-plane ICP with Equation (9,10,11) Tk \u2190 arg minT\u2208SE(3) (w (M,ToP\u2032) e (M,ToP\u2032))\nAo =  ( p1,m1,1 ) ( p2,m2,1 ) \u00b7 \u00b7 \u00b7 (pS ,mS ,1) ... ... . . .\n...( p1,m1,K ) ( p2,m2,K ) \u00b7 \u00b7 \u00b7 (pS ,mS ,K)  (6)\nwhere S is the number of points in P and K is the number of nearest neighbor search of each point in P. Each column of Ao represents the matched results of one point ps to its nearest neighbors after ICP alignment.\nIn the second step, we check each column inAo and label ps if the matched map points satisfy the consistency criteria: all the matched map points should be in the same category, formulated as follows:\nc (ps) = { c ( ms,1 ) , if c ( ms,1 ) = \u00b7 \u00b7 \u00b7 = c (ms,K)\nNot labeled , otherwise (7)\nThereafter, some points in P are \u201clabeled\u201d and some are not. Only labeled points are considered in the following process. However, not all the labeled points are informative, e.g., points matched as \u201cWindows\u201d may not return any meaningful measurements using LiDAR sensors. Besides that, we consider the category selection is flexible, and can be decided by users in different working environments and conditions, e.g., \u201cFurniture\u201d could be helpful for the localization when a robot is traveling in a room filled with static furniture, but might be harmful when there are many semi-dynamic office chairs.\nThus, we only select those labeled points of certain specific types, so the second step can be formulated as two steps: first label and then select, as follows:\nP\u2032 \u2190 fselect ( flabel (P,Ao)) (8) where flabel (\u00b7) and fselect (\u00b7) represent the labeling and selection process, respectively; P\u2032 is the filtered point cloud that will be used in the following estimation.\nIn the third step, semantic ICP is designed to minimize Equa-\ntion (4) based on the coarse result from the first step. A semantic-aided weight function wc (\u00b7) is formulated that incorporates the labels of LiDAR readings P\u2032 and the mapM:\nwc = { \u00b5 , c (ps) = c ( ms,k ) 1 \u2212 \u00b5 , c (ps) , c ( ms,k\n) (9) in which \u00b5 \u2208 [0.5, 1] is a variable that determines the importance of semantic association. If \u00b5 = 1, only few matched lasers are kept in challenging scenes. In the experimental section of this paper, we set \u00b5 = 0.8 as a constant value, which means a data association is with higher weight when the laser point is in the same category as the matched map point.\nBesides the semantic-aided weight function, a Huber function [47, 48] is also utilized to weight the data association, as follows:\nw\u03c1 = { 1 , e (\u00b7) < \u03b4 \u03b4\ne(\u00b7) , else (10)\nwhere \u03b4 is a point-to-plane distance threshold. Finally, the overall weight function of for each matched (p,m) is computed as the combination of the semantic and geometric relation:\nw = wcw\u03c1 (11)\nThe proposed three-step semantic localization is shown in Figure 3 and Algorithm 2. A toy example is also presented to illustrate the proposed coarse-to-fine localization pipeline in Figure 4, in which the number of nearest neighbor K is set as 3 and \u201cCurtain Panels\u201d is not selected in the fine step.\nTo guarantee the efficiency for real-time application, we randomly sample the raw LiDAR scan and sub-sample input points in high-density regions. The initial guess of the transformation is also critical to build an efficient and robust scan matching. However, in this study, there is no Inertial Measurement Unit (IMU) or other odometry to estimate the transformation between k \u2212 1 and k, which is different from other CAD or BIM-based localization methods [22, 23, 27]. At each timestamp k, we set the previous estimated Tk\u22121 as the initial guess to estimate Tk, as shown in Figure 3 and 4. This means we only test the robustness and accuracy via scan-by-scan matching at the front-end, and there is no customized back-end estimator in\nour proposed localization pipeline."
        },
        {
            "heading": "4. Experiments",
            "text": "In order to validate the effectiveness of the proposed framework, several experiments are conducted in the real world, including the offline BIM-to-Map conversion and online semantic localization."
        },
        {
            "heading": "4.1. Set-up",
            "text": "Ten sequences are collected using a Velodyne VLP-16 sensor. The data collection devices are shown in Figure 5. All the data sessions are collected in the building of School of Design and Environment 4 (SDE4) at NUS, which is a six-storey university building. For an extensive experiment, the localization performance is tested from the second to the fifth storey, covering different environments including corridors and lounges, as shown in Table 2. Approximated traveled distances of sequences are also presented. The total traveled distance is over 340 meters.\nThe semantic maps are generated using Dynamo, CloudCompare and MATLAB. The density of map points is set as 30 points/m3. The semantic localization is implemented using a C++ package libpointmatcher [49] on Robot Operating System (ROS). All the online localization experiments are performed using a low power laptop with Intel I5-8265U and 16G RAM.\nA visualization of trajectories and semantic maps are shown in Figure 6. Two sequences are with loop closings (Sequence 3- 3 and 4-2). Furthermore, we consider that the localization tasks on the 2nd and 4th Storey are more challenging than those on the 3rd and 5th. The 2nd Storey is connected to the building entrance and the street, where a few dynamics (mostly pedestrians) exist in the collected LiDAR data. There also exist mixed and incorrectly labeled map points close to the start positions of Sequence 2-1 and 2-2, shown in Figure 6 and 8. The 4th\nStorey contains a long narrow corridor, which will degenerate the accuracy of pose tracking."
        },
        {
            "heading": "4.2. BIM-generated semantic maps",
            "text": "First of all, the original BIM model and its generated maps are presented in Figure 7(a). Sub-maps of individual storeys are also presented in Figure 7(b) and several categories are visualized with different colors. In SDE4 building, there are 13 categories extracted from Dynamo software: \u201cCeilings (CE)\u201d, \u201cColumns (CO)\u201d, \u201cCurtain Panels (CP)\u201d, \u201cCurtain Wall Mullions (CWM)\u201d, \u201cFloors (FL)\u201d, \u201cFurniture (FU)\u201d, \u201cGeneric Models (GM)\u201d, \u201cPlanting (PL)\u201d, \u201cRoofs (RO)\u201d, \u201cStructural Columns (SC)\u201d, \u201cStructural Framing (SF)\u201d, \u201cWalls (WA)\u201d and \u201cWindows (WI)\u201d. Furthermore, we make a statistic on the num-\nber of each category and the distributions are shown in Figure 7(c),7(d),7(e) and 7(f).\nAs shown in Figure 7(b), the mapping pipeline can generate semantically augmented point cloud maps. With regards to the distribution of points, it was found that most points are associated to Walls (\u2248 40%), Floors (\u2248 20%) and Curtain Panels (\u2248 20%). We find that there are some incorrectly or mixed labeled map points on the 2nd storey, shown in Figure 8. The main reasons for this problem have been analyzed in Section 3.2. On the other hand, most of the map points are labeled with correct categories. In the next subsection, the proposed semantic localization pipeline will be evaluated in the NUS SDE4 building quantitatively."
        },
        {
            "heading": "4.3. Localization evaluation",
            "text": "Ground truth poses are required to evaluate the continuous localization. But compared to outdoor autonomous vehicles equipped with GPS/INS, it is challenging to collect ground truth poses in indoor scenes, since motion capture systems, such as\nVicon, are difficult to be deployed across all halls and corridors in a large building.\nIn recent research work [37, 51], state-of-the-art SLAM methods are often used as a proxy for ground truth during evaluation. Cartographer [52] is a well-designed SLAM system with a loop closing module. In [53], it showed a superior performance over other SLAM systems in indoor scenes, and we adopt this as ground truth. Specifically, in this paper, Cartographer is run with our fine-tuned parameters and also with lowspeed rosbag to generate ground truth poses. Cartographer and some other SLAM methods typically require IMU sensors to achieve accurate 3D pose estimation, which is infeasible with only a mobile LiDAR sensor in this study. Thus we set Cartographer with 2D configurations and evaluate our method in 2D space (x,y and yaw).\nThe generated ground truth poses are not aligned to map reference or BIM model. To obtain localization errors, we set sensor timestamps as indexes in all the trajectories, and utilize the open source tool [50] to achieve trajectory alignment and error calculation. The Root Mean Square Error (RMSE) is calculated\nSeq.\nas follows:\nin which \u2206t and \u2206R are the translation error (x and y) and rotation error (only yaw angle) between one estimated pose and ground truth pose respectively.\nOur proposed semantic localization pipeline consists of three steps: original ICP, semantic filtering and semantic-aided ICP with wcw\u03c1. We test several combinations of these steps to validate the effectiveness gradually. All the tested methods are listed as follows:\n\u2022 ICP (ORG) [45, 49], which is actually the first step in Algorithm 2. A common configuration in libpointmatcher [54] includes two geometric-based outlier filters: TrimmedDistOutlierFilter and SurfaceNormalOutlierFilter. These outlier filters are essentially weight functions. We keep this original configuration as a purely geometricbased competitive method.\n\u2022 Sem (wc), which is an updated version of Sem (ORG). In the second half of ICP, two geometric-based outlier filters are replaced by the semantic-aided weight function wc. We set \u00b5 = 0.8 as a constant value in the test.\n\u2022 Sem (w\u03c1): we also test the weight function w\u03c1 under the semantic filtering scheme.\n\u2022 Sem (wcw\u03c1), which is the complete version of Algorithm 2 with the three steps: ICP (ORG), semantic filtering and semantic ICP.\nTo achieve a fair comparison, all the methods above share the same data pre-processing and filtering. Maximum number of iteration (MaxIt) is critical for ICP-based localization. For ICP (ORG), we set MaxIt as 40. As for semantic localization, MaxIt\nof ICP (ORG) and semantic ICP is fixed as 20 respectively, so there are also 40 iterations for a fair comparison.\nBefore tests on all sequences, one important configuration is to decide which elements should be used in the semantic fil-\ntering. We test the Sem (ORG) with several combinations on Sequence 3-1 in the SDE4 building, and present localization results in Table 3, in which ALL CPNT means semantic filtering is not used and all components are integrated into ICP-based lo-\ncalization. As observed from the table, the errors increase when windows and curtain panels are selected for localization, and decrease gradually when walls or columns are integrated into the localization. The results indicate that walls and columns could be more helpful and informative for the proposed semantic localization in the SDE4 building.\nTherefore, floors, walls and columns are selected for semantic filtering based on the results and analyses above. Then, all the methods are tested on ten sequences, and errors are presented in Table 4. We summarize the conclusions as follows by analyzing the results from ICP (ORG) (Column 1) to Sem (wcw\u03c1) (Column 6):\n\u2022 ICP (ORG) and ICP (w\u03c1) results in an acceptable accuracy. This indicates that it is feasible to achieve LiDAR localization on BIM-generated maps with only one LiDAR sensor.\n\u2022 Sem (ORG) performs better than ICP (ORG) on most sequences, indicating that the proposed semantic filtering can help improve LiDAR localization. There is an overall improvement of 18% on the translation error.\n\u2022 Compared to Sem (ORG), Sem (wc) achieves higher accuracy on translation estimation but lower accuracy on heading estimation. This indicates that the semantic-aided\nweight function wc almost has the same performance as the two geometric-based outlier functions, but it could not improve the original method significantly.\n\u2022 Compared to Sem (ORG) and Sem (wc), Sem (w\u03c1) performs the best under the semantic filtering scheme. We consider it is because the Huber function w\u03c1 can better overcomes the deviations.\n\u2022 The complete version Sem (wcw\u03c1) could not improve the overall performance compared to Sem (w\u03c1), but it can handle a challenging sequence 4-2 in the datasets. Finally, Sem (wcw\u03c1) can achieve an overall improvement of 34% on translation estimation compared to the original version ICP (ORG).\nMore specifically, we can find that the localization errors of the 2nd and 4th Storey are higher than that of the 3rd and 5th. This indicates that localization difficulty is related to the accuracy of maps and environments, as analyzed in Section 4.1.\nThe translation errors are also presented in Figure 9 using boxplots. The localization performance can be visualized from the median error and the error variance in the boxplots. The localization errors decrease when the semantic filtering is applied on ICP (ORG), thus verifying the hypothesis that using seman-\ntic properties can improve localization. The evaluation package [50] also provides variations on errors with respect to the traveled distance. The estimated localization trajectories and errors are shown in Figure 10, 11 and 12. As observed from the trajectories and errors, Sem (wcw\u03c1) results in a smooth trajectory close to the ground truth, thus verifying the effectiveness of the proposed coarse-to-fine localization pipeline.\nIn addition to the numerical analyses, we present several case studies of localized LiDAR scan on BIM-generated semantic maps, as shown in Figure 13. The four cases show the localization process of Sequence 2-2, 3-3, 4-1 and 5-1 in different storeys of the SDE4 building. We also present the number of points in each step of these four cases, shown in Table 5. After the random sampling and semantic filter, only hundreds of laser points (\u22483% of raw data) are selected for the final semantic ICP step. As for efficiency, the mean-time cost of semantic localization (Algorithm 2) is 108ms, 79ms, 112ms and 114ms in these four sequences. The real-time method is able to track the LiDAR scanner operating at 10Hz with only a resourceconstrained embedded device. We also release a video demonstration online 1."
        },
        {
            "heading": "4.4. Compared to LiDAR-only SLAM systems",
            "text": "SLAM is a widely-used mapping system that aims to achieve localization and mapping simultaneously. In this subsection, we also compare the proposed BIM-based localization with the SLAM systems. Three open-sourced SLAM systems are performed on our self-collected data: Lidar Odometry and Mapping (LOAM) [5] 2, Direct Lidar Odometry (DLO) [55] 3 and Open3D SLAM [56] 4. Specifically, LOAM and DLO are LiDAR-based odometry methods (reduced LiDAR SLAM systems), while Open3D SLAM is a complete LiDAR SLAM system, including loop closing and graph optimization. For a fair comparison, there is no IMU or other information as assistance used in this experiment.\nThe SLAM systems are first evaluated by comparing them to the ground truth poses in 2D space, and the translation and orientation errors can be obtained accordingly. These errors are eventually calculated on 2D x-y plane using Equation 12\n1The video is available at this link 2https://github.com/HKUST-Aerial-Robotics/A-LOAM 3https://github.com/vectr-ucla/direct lidar odometry 4https://github.com/leggedrobotics/open3d slam\nand 13. In addition, we also propose to evaluate the drifts on the Z-axis of SLAM systems and our proposed BIM-based localization. The drift errors are calculated under the criteria: \u2206Z = Zlast \u2212 Zinit (m), in which Zinit and Zlast are the average height of 50 poses at the beginning and the end of the trajectory. The data collection process is conducted with a planar motion. Thus \u2206Z can be regarded as a measurement metric to evaluate the drift on Z-axis.\nThe quantitative results on four sequences are presented in Table 6. Compared with the LiDAR-only SLAM systems, the proposed BIM-based lidar localization does not show better performance on 2D pose estimation. Still, it shows competitive results by matching scans on BIM-generated maps. On the other hand, LiDAR-only SLAM systems show more significant height drifts in Sequence 2-3, 4-2 and 5-2, because these trajectories contain few revisted places for loop closing, as shown in Figure 6. The drift on Z-axis is a common degeneracy problem for LiDAR-only SLAM applications, especially in a long straight travel. We also present the SLAM-generated maps in Figure 14. While in the BIM-generated maps, the floor is almost perfectly flat in one storey, which means all floor points are with the same height, thus providing certain constraints for BIM-based localization."
        },
        {
            "heading": "4.5. Discussion",
            "text": "In Figure 13(a), it is interestingly found that there are notable differences between pre-builtM and observed P, which are essentially the differences between as-designed and the as-built. The two columns in green boxes are observed in the LiDAR scan but there are no columns on the map respectively, making the LiDAR points match to walls (colored with pink) due to the nearest neighbor search strategy of ICP. Another observed column in the blue box is not matched to any element since there is a considerable distance between the nearest column on the map. Actually, the mismatch problem occurs on every storey in the NUS SDE4 building because of the deviations between asbuilt and as-designed. There are other factors that cause errors in this study, such as dynamics and sensor noises. On the other hand, the localization pipeline is designed with powerful weight functions, so it can still track the pose successfully under these challenges.\nWe also notice that there is a large error when traveling in the long, challenging corridor on the 4th Storey. This results in a discontinuity in the estimated trajectory, shown in the red box of Figure 13(c). Specifically, the large drift is not eliminated in a short time. There are mainly two reasons for this. Generally, a long corridor is a challenging scene for localization that will degenerate the localization performance. Besides, once a pose is with a large error, the considerable error may be conducted into the following pose estimation.\nOverall, there are still some requirements and challenges when applying the proposed localization method on BIMgenerated maps. We first summarize the requirements for applying our proposed framework:\n(a) The proposed method is applicable in static built environments. In a dynamic environment, like an ever-changing\nconstruction site, the BIM model should be reviewed and updated by the user, which will involve human labor and be time-consuming in application.\n(b) The BIM content should contain the basic geometric sizes and category labels of main structures in a building. These two pieces of information are necessary requirements to generate semantic point cloud maps in this study.\n(c) It is unavoidable that there exist deviations between asdesigned and as-built. The deviations should not be too large in the application for localization success.\n(d) The proposed method is more appropriate to use in environments that have certain diversity. This diversity includes the categories and spatial distribution of BIM elements. For example, a typical scene is a long straight corridor which consists of only walls and floors, which is lack of diversity and is challenging for LiDAR-only localization.\nWe also list the challenges and limitations of the proposed method as follows:\n(a) The biggest challenge is the deviations between asdesigned and as-built. The deviations can cause incorrect data associations and ambiguous scans, which could cause localization failure in challenging scenes.\n(b) There exist inaccurate semantic maps using the proposed BIM-to-Map conversion, as shown in Figure 9. These incorrectly or mixed labeled map points may reduce the diversity of the semantics, leading to a degeneration of localization accuracy.\n(c) The pose estimation of Tk relies on the result of Tk\u22121, which means the significant error in Tk\u22121 may also result in Tk, or even cause a localization failure. In addition, we manually set the floor and Tk=0 at the first stamp of each sequence. To build a more automatic localization system, we need to estimate the initial pose in a whole building with a global localization module [57, 33]."
        },
        {
            "heading": "5. Conclusions",
            "text": "This paper proposes a mapping-free and learning-free semantic localization framework. A BIM-to-Map conversion is proposed by using spatial locations and category labels of elements in BIM. This paper also proposes a coarse-to-fine localization method to track a 3D LiDAR sensor based on semantic maps, in which both geometric and semantic information are considered in data associations. The tests on real-world datasets demonstrate that the proposed framework can achieve effective and efficient localization using only one BIM file and one mobile LiDAR sensor.\nWe consider there remain research directions based on the experimental results and discussion on limitations in Section 4.5. We list some of them as follows:\n\u2022 The accuracy of semantic map building and data labeling can be improved by integrating the geometrics of local point clouds, e.g., the points with similar surface normals might be in the same category.\n\u2022 Another promising study is that we can first filter certain elements in BIM first [23], and then generate semantic maps. It is also worth studying how to filter the BIM to guarantee the localization performance.\n\u2022 Multiple sensors can help improve the robustness and accuracy of localization, e.g., IMU as an assistance and support to overcome the Limitation (c) and (d).\n\u2022 To address the Limitation (e), global registration or localization is critical for applications, which can localize a robot from scratch without initial guess.\n\u2022 Besides, the map management is also important in large indoor scenes. A concise and interactive map form is desired for robot navigation, such as topological maps."
        },
        {
            "heading": "6. Acknowledgement",
            "text": "This research is supported by Building Construction Authority (BCA) and National Robotics Programme (NRP) under its Built Environment Robotics R&D programme (Grant Award Ref No. W2122d0154). Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the BCA and NRP.\nThis project is also supported by the Hong Kong Center for Construction Robotics (InnoHK center supported by Hong Kong ITC).\nWe would also like to thank colleagues in NUS ARC for kindly sharing the experimental devices with us."
        }
    ],
    "title": "Semantic localization on BIM-generated maps using a 3D LiDAR sensor",
    "year": 2022
}