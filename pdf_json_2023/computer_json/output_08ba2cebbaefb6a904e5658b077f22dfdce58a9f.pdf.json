{
    "abstractText": "7 A publicly available human genome serves as both a valuable resource for researchers and a 8 potential risk to the individual who provided the genome. Many actors with selfish intentions could 9 exploit it to extract information about the donor's health or that of their relatives. Recent efforts have 10 employed artificial intelligence models to simulate genomic data, aiming to create synthetic datasets 11 with scientific merit while preserving patient anonymity. However, a major challenge arises in 12 dealing with the vast amount of data that constitutes a complete human genome and the resources 13 required to process it. 14 We have developed a dimension reduction method that combines artificial intelligence with 15 our knowledge of in vivo mutation association mechanisms. This approach enables the processing of 16 large amounts of data without significant computational resources. Our genome segmentation follows 17 chromosomal recombination hotspots, closely resembling mutation transmission mechanisms. 18 Training data is sourced from the 1000 Genomes Project, which catalogues over 2500 genomes from 19 diverse ethnic groups. Variational autoencoders, utilising neural networks, serve as an extension to 20 the generative model. Wasserstein Generative Adversarial Networks (WGAN) are a benchmark 21 among generation methods for various data types. 22 After optimisation of our data simulation strategy our pipeline allows the generation of a 23 simulated population meeting several essential criteria. It demonstrates good diversity, closely 24 resembling that found in the reference dataset. It is plausible, as newly generated combinations of 25 mutations do not disrupt the linkage disequilibria found in humans. It also preserves donor anonymity 26 by synthesising combinations of reference genomes that are distant from reference samples. 27 28 . CC-BY-NC-ND 4.0 International license perpetuity. It is made available under a preprint (which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in The copyright holder for this this version posted December 9, 2023. ; https://doi.org/10.1101/2023.12.08.570767 doi: bioRxiv preprint",
    "authors": [
        {
            "affiliations": [],
            "name": "Callum Burnard"
        },
        {
            "affiliations": [],
            "name": "Alban Mancheron"
        },
        {
            "affiliations": [],
            "name": "William Ritchie"
        }
    ],
    "id": "SP:6c132f1ae8f2d207c4c4122d5b30361a70e23110",
    "references": [
        {
            "authors": [
                "403 Arjovsky",
                "Martin",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein GAN",
            "year": 2017
        },
        {
            "authors": [
                "Carvajal-Rodr\u00edguez",
                "Antonio"
            ],
            "title": "GENOMEPOP: A Program to Simulate Genomes",
            "year": 2008
        },
        {
            "authors": [
                "R.W. Hamming"
            ],
            "title": "Error Detecting and Error Correcting Codes",
            "venue": "Bell System Technical",
            "year": 1950
        },
        {
            "authors": [
                "415 Joly",
                "Yann",
                "Ida Ngueng Feze",
                "Jacques Simard"
            ],
            "title": "Genetic Discrimination and Life",
            "year": 2013
        },
        {
            "authors": [
                "Kim",
                "Miran",
                "Kristin Lauter"
            ],
            "title": "Private Genome Analysis through Homomorphic",
            "year": 2015
        },
        {
            "authors": [
                "Kingma",
                "Diederik P",
                "Max Welling"
            ],
            "title": "Auto-Encoding Variational Bayes",
            "year": 2013
        },
        {
            "authors": [
                "M.A. Kramer"
            ],
            "title": "Autoassociative Neural Networks",
            "venue": "Computers & Chemical Engineering",
            "year": 1992
        },
        {
            "authors": [
                "S. Kullback",
                "R.A. Leibler"
            ],
            "title": "On Information and Sufficiency",
            "year": 1951
        },
        {
            "authors": [
                "431 Kuo",
                "Tsung-Ting",
                "Anh Pham"
            ],
            "title": "Detecting Model Misconducts in Decentralized",
            "year": 2021
        },
        {
            "authors": [
                "Paul Flicek"
            ],
            "title": "Variant Calling on the GRCh38 Assembly with the Data from Phase",
            "year": 2019
        },
        {
            "authors": [
                "Abyzov",
                "Wing H. Wong",
                "Hugo Y.K. Lam"
            ],
            "title": "VarSim: A High-Fidelity Simulation",
            "year": 2015
        },
        {
            "authors": [
                "446 Peng",
                "Bo",
                "Marek Kimmel"
            ],
            "title": "simuPOP: A Forward-Time Population Genetics Simulation",
            "year": 2005
        },
        {
            "authors": [
                "Price",
                "W. Nicholson",
                "I. Glenn Cohen"
            ],
            "title": "Privacy in the Age of Medical Big Data",
            "venue": "Nature",
            "year": 2019
        },
        {
            "authors": [
                "451 Prince",
                "Anya E.R"
            ],
            "title": "Insurance Risk Classification in an Era of Genomics: Is a Rational",
            "year": 2017
        },
        {
            "authors": [
                "453 Ramachandran",
                "Prajit",
                "Barret Zoph",
                "Quoc V. Le"
            ],
            "title": "Searching for Activation Functions",
            "year": 2017
        },
        {
            "authors": [
                "A. Kitts",
                "Terence D. Murphy"
            ],
            "title": "Evaluation of GRCh38 and de Novo Haploid",
            "year": 2017
        },
        {
            "authors": [
                "459 Shabani",
                "Mahsa",
                "Luca Marelli"
            ],
            "title": "Re-Identifiability of Genomic Data and the GDPR",
            "year": 2019
        },
        {
            "authors": [
                "461 Tahmasbi",
                "Rasool",
                "Matthew Keller"
            ],
            "title": "GeneEvolve: A Fast and Memory Efficient",
            "year": 2016
        },
        {
            "authors": [
                "Tange",
                "Ole"
            ],
            "title": "GNU Parallel: The Command-Line Power Tool",
            "year": 2011
        },
        {
            "authors": [
                "Montinaro",
                "Cyril Furtlehner",
                "Luca Pagani",
                "Flora Jay"
            ],
            "title": "Creating Artificial Human",
            "year": 2021
        },
        {
            "authors": [
                "472 Yue",
                "Jia-Xing",
                "Gianni Liti"
            ],
            "title": "simuG: A General-Purpose Genome Simulator",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "A publicly available human genome serves as both a valuable resource for researchers and a 8 potential risk to the individual who provided the genome. Many actors with selfish intentions could 9 exploit it to extract information about the donor's health or that of their relatives. Recent efforts have 10 employed artificial intelligence models to simulate genomic data, aiming to create synthetic datasets 11 with scientific merit while preserving patient anonymity. However, a major challenge arises in 12 dealing with the vast amount of data that constitutes a complete human genome and the resources 13 required to process it. 14 We have developed a dimension reduction method that combines artificial intelligence with 15 our knowledge of in vivo mutation association mechanisms. This approach enables the processing of 16 large amounts of data without significant computational resources. Our genome segmentation follows 17 chromosomal recombination hotspots, closely resembling mutation transmission mechanisms. 18 Training data is sourced from the 1000 Genomes Project, which catalogues over 2500 genomes from 19 diverse ethnic groups. Variational autoencoders, utilising neural networks, serve as an extension to 20 the generative model. Wasserstein Generative Adversarial Networks (WGAN) are a benchmark 21 among generation methods for various data types. 22 After optimisation of our data simulation strategy our pipeline allows the generation of a 23 simulated population meeting several essential criteria. It demonstrates good diversity, closely 24 resembling that found in the reference dataset. It is plausible, as newly generated combinations of 25 mutations do not disrupt the linkage disequilibria found in humans. It also preserves donor anonymity 26 by synthesising combinations of reference genomes that are distant from reference samples. 27\n28\nIntroduction 29\nThe sequence of nucleotides in the DNA contained in our cells constitutes the biological 30 information necessary to characterise us as individuals. This information covers aspects as innocuous 31 as the colour of our hair, eyes and skin, as well as functional elements of our bodies like our capability 32 to digest certain foods, and can go so far as to cause diseases. The progresses in sequencing 33 technologies over the past two decades allow us to express these characteristics as variations from a 34 reference genome, established by studying large swathes of the population (Schneider et al. 2017). 35 We are all uniquely identified by these aspects, and it is becoming increasingly easy for 36 healthcare services, research teams and private companies to identify the genomic mutations 37 underlying the phenotypes we observe. This leads to a tradeoff, where this data can be used to better 38 treat diseases, in particular when dealing with personalised medicine, but at the same time could have 39 negative repercussions on the everyday lives of the bearers of these variations (Joly, Ngueng Feze, 40 and Simard 2013; Prince 2017; Price and Cohen 2019). Great care is therefore needed when dealing 41 with this data, in particular when sharing it with other actors. 42 A large number of regulations are active across the world regarding the storage and sharing 43 of medical data, especially in the European Union, where the authors are based (Shabani and Marelli 44 2019). However, it is much more complex to protect this data when it is in use. Methods such as 45 homomorphic encryption (Kim and Lauter 2015) or federated learning (Kuo and Pham 2021) could 46 potentially alleviate privacy issues, but they are either very costly to implement or reduce the 47 information that can be extracted from sequencing data. 48 A different approach to the privacy issue is to synthesise novel genomic data not directly 49 derived from an individual\u2019s sample. In the past, these methods have often focused on the simulation 50 of an entire population and the coalescence of alleles within it (Balloux 2001; Tahmasbi and Keller 51 2016; Carvajal-Rodr\u00edguez 2008; Peng and Kimmel 2005). In recent years, teams have sought to use 52 available data describing the frequency of mutations in populations and create novel samples by 53 assigning these mutations in novel combinations (Juan et al. 2020; Yue and Liti 2019; Mu et al. 2015). 54 One of the most recent projects of this type uses artificial intelligence methods, specifically 55 generative approaches to determine appropriate combinations of mutations for sample generation 56 (Yelmen et al. 2021). This approach produces diverse, realistic and novel populations of genomes. 57 The main limitation however of contemporary AI methods is the size of both the training data 58 and generated genomes. A classical artificial neural network is densely connected, meaning each 59 neuron in a given layer is connected to each neuron of the following and previous layers. Increasing 60\nthe size of the sample used for training the model therefore leads to exponential increases in 61 calculation cost. 62\nGraphical abstract 63\n64 Pipeline for increased scale of synthetic genome generation. Genomic data is segmented into subsections, 65 then compressed using deep autoencoders. Finally a generative neural network learns from this compressed 66 genomic data to create novel encoded samples, which are then decoded back into full samples. 67 68 We present here the Haplotypic Human Genome Generator (H2G2), a method to generate 69 human genomic data on an increased scale. We first segment the data based on available biological 70 knowledge and create a compressed representation of these subsections of data using dimension 71 reduction methods. Then, these genomic identities are processed by a generative neural network to 72 simulate novel samples, while remaining coherent with the source dataset. 73 We use Human genomic data from the 1000 Genomes Project (The 1000 Genomes Project 74 Consortium 2015), realigned onto HG38 (Lowy-Gallego et al. 2019), for its availability, high global 75 diversity and standardised methodologies. Mutation data for chromosome 1 was segmented based on 76 hotspots of recombination likelihood as determined by a high resolution map of recombination events 77 established across thousands of detailed samples (Halldorsson et al. 2019). These recombination 78 events dictate which mutations tend to be inherited together across generations, and therefore 79\norchestrate the biological linkage between genomic variations at a large scale. Following this logic, 80 mutations within a subsection share higher correlation than mutations contained in two different 81 subsections, and allow for a more accurate compression of the data by dimension reduction methods. 82 We chose autoencoders to compress the data. These are a family of unsupervised machine 83 learning methods using artificial neural networks to create latent representations of samples by 84 compressing them more and more at each layer in the network (Kramer 1992; Kingma and Welling 85 2013). The models also implement a decoder to restore the compressed data into its full state, as 86 faithful to the original samples as possible over the course of the training process. Their strengths in 87 dealing with any type of data, as well as handling nonlinear datasets with variable multimodal 88 distributions, are particularly relevant for this project. 89 The generation method we chose was the Wasserstein GANs (Arjovsky, Chintala, and Bottou 90 2017). Their robust training process, by avoiding mode collapse and being less sensitive to 91 hyperparameters when compared to other similar models, is particularly relevant for a project that 92 seeks to increase the scale of data to be simulated. 93 Additionally, we validated the realism of our simulated population by applying 94 Chromopainter to reference and synthetic genomes (Lawson et al. 2012). Meta-characteristics of these 95 two groups were calculated based on its output to compare them and highlight their similarities. 96 97\n98\n99 100\n101\nResults 102\nGenome segmentation 103\nThe first step of our approach is to segment the genome into portions that are amenable to 104 compression using autoencoders. To this end we compared two segmentation approaches which were 105 a linear segmentation of the genome into equal sized bins and a segmentation based on crossing-over 106 hotspot regions. Recombination hotspots were used to segment mutation data for HG38 chromosome 107 1 (supp. figure 1a), setting minimum and maximum values for number of mutations per subsection 108 (supp. figure 1b). This method was compared to using a naive method of simply separating mutations 109 into bins of equal size. Weight pruning and latent space reduction were also applied to these models 110 to maximise encoding efficiency (supp. figure 1c) 111 According to our model (figure 1a), some mutations can become separated from others that 112 share the most biological significance with them by chance when using a naive approach, whereas 113 the recombination hotspots-based method preserves these links. Analysing bins of mutations 114 containing a strong hotspot (figure 1b) confirms this, as these sections that are split by a 115 recombination hotspot show significantly lower accuracy after autoencoding when compared to 116 sections that do not contain a strong recombination hotspot. 117 We selected using hotspot-based segregation of mutations for the rest of this study, allowing 118 for a minimum of 500 and a maximum of 5000 mutations per subsection. 119 120\n122 Genome segmentation. Recombination hotspots provide higher accuracy 123 segmentation. 124 a) Our model explaining linear separations splitting up biological links between mutations. Mutations that 125 share more biological meaning with their neighbours on one side than on the other are sometimes 126 attributed to the wrong group of mutations when not using recombination hotspot based methods, 127 leading to less accurate reconstruction by AE. 128 b) Boxplot showing the accuracy of autoencoder models applied to contiguous bins of mutations. For 129 each size of bins, they were separated into bins containing a high intensity hotspot (light colour) and 130 those that don\u2019t (dark colour). Statistical significance of the differences were tested using Mann-131 Whitney U\u2019s test. 132\nDimension reduction by autoencoders 133\nSegments generated from the previous section were then compressed using autoencoders (see 134 methods section for full details). We first investigated the frequency of each mutation before and after 135 compression via the autoencoders. We discovered that mutations with a low frequency in the 136 reference dataset tend to be missing from the decoded dataset (figure 2a, upper left plot, highlighted 137 by red box). This can be due to the tradeoff between higher compression rates and encoding rare 138 variations. The autoencoder is not learning the rare occurrences in which the mutation should be 139\npresent as a reasonable tradeoff for a gain in compression The poor representation of rare mutations 140 could also be due to neurons in the model \u201cdying\u201d, which can happen when using the ReLU activation 141 function (Ramachandran, Zoph, and Le 2017). We therefore investigated multiple alternative 142 activation functions, as well as a Variational AutoEncoder (VAE) (Kingma and Welling 2013). 143 Results in figure 2a show that different activation functions do indeed affect the number of 144 disappearing mutations. Interestingly, the TanH activation function yields the fewest of these 145 disappearing mutations, but also systematically tends to overrepresent the frequency of mutations in 146 the dataset (figure 2a, bottom row, highlighted in red). 147 Overall, using a VAE with either sigmoid or TanH activation function show the lowest 148 number of disappearing mutations as well as a low frequency drift (supp. figure 2a). Sigmoid 149 activation function was chosen, so as to avoid the visible bias of highly overrepresented mutations 150 that occur when using TanH, both in VAE and classical AE. 151 PCA projections show that these VAE models are very good at reconstructing the population 152 after dimension reduction (figure 2b and supp. figure 2b). Some do exhibit mode search, where the 153 model simplifies the information it learns down to a set number of modes which, once decoded, map 154 to the main modes of the reference distribution. 155\n157 Dimension reduction by autoencoders. Model and hyperparameter optimisation for 158 accurate reconstruction. 159 a) Scatter plots comparing mutation frequencies in the reference dataset to the frequency of that 160 mutation in the dataset after encoding and decoding by various autoencoder models. 161 Occurrences where frequencies are equal align with red diagonal. Classical autoencoders and 162 variational autoencoders were tested, each with four different activation functions. For each 163\nplot, the colour of the dot indicated the frequency drift, which is simply the absolute difference 164 of the two frequencies. Particular drift patterns observed across different setups are 165 highlighted manually. 166 b) Scatter plots of Principal Component Analyses (PCA) of various datasets of genomic 167 subsections. Each column is a different genomic subsection. In each plot, the reference dataset 168 is shown as grey dots, and a dataset obtained after encoding then decoding by a Variational 169 AutoEncoder (VAE) is shown in colour. Top row shows data obtained by decoding the mean 170 of the projection of each individual in its latent space. Bottom row shows data obtained from 171 the same VAE, but using a sampling around the mean of the projection of each individual in 172 its latent space. 173 174\nGenome generation 175\nFollowing the segmentation and compression steps described above, we trained a Generative 176 Adversarial Network using Wasserstein loss (WGAN) on encoded subsections of genomic data 177 spanning over 15000 mutations, equivalent to 1 megabase of DNA. It produces simulations of 178 encoded genomic data. After decoding this data and applying the same PCA projection, we can see 179 that the samples generated by this model remain realistic, as their coverage of the reference data is 180 similar to that of the decoded data in figure 2b, and explore some space which the VAE did not utilise 181 completely (figure 3a and supp. figure 3). 182 To analyse the validity of our simulated genomes we used a population based approach where 183 we estimate how well the ancestry composition of the simulations match that of the real genomes. To 184 this end we used a tool called Chromopainter which reconstitutes the genomic ancestry of a query 185 population based on the mutation profile of a donor population. Chromopainter was applied to both 186 78 reference genomes (three per sub-population listed in (Lowy-Gallego et al. 2019)) and 100 187 simulated genomes. From the raw results obtained, we established metaprofiles of ancestry switches, 188 based on all reference or simulated genomes analysed (figure 3b). These metaprofiles show strong 189 correlation between the reference and simulated datasets. This means that our simulated population 190 could realistically be obtained through population admixture in vivo. 191 One other characteristic to analyse for this new population is privacy loss - that is, how close 192 are these simulated genomes to the ones found in the reference dataset? To this end we use the 193 Hamming distance to measure the number of substitutions necessary to transform one sequence into 194 another of same length (Hamming 1950). It was used here as a part of the edit score, which is the 195 ratio between the Hamming distance from a simulated genome to its nearest neighbour in the 196\nreference dataset and the total length of the sequence. This score therefore ranges from 0 (identical 197 sequences) to 1 (no common elements in sequence). Applying it to the simulated section of 198 chromosome 1 (figure 3c) shows that none of our simulated genomes are exact copies of any reference 199 samples. The minimum score is 0.01, which is equivalent to just over 150 mutations difference. 200\n202 Genome generation. Realistic and novel genomes are simulated using a multi-step 203 pipeline. 204\na) Scatter plots of Principal Component Analyses (PCA) of data generated by Generative 205 Adversarial Network using Wasserstein loss (WGAN) in red, versus reference data in grey 206 for three genomic subsections. 207 b) Chromopainter metaprofiles calculated over all samples for three genomic subsections, and 208 three corresponding Chromopainter illustrated reconstructions from the datasets used to 209 construct the metaprofiles. Chromopainter gives as output a series of reconstructions for each 210 sample indicating the most probable ancestry for each mutation in the sample. Each plot is a 211 different sample, each row in each plot is a reconstruction by Chromopainter. The horizontal 212 axis is each mutation in the genomic subsection. Metaprofiles show how frequently this 213 ancestry changes within samples. In dark blue, ancestry change from one individual to 214 another, in light grey, ancestry change from one population to another. 215 c) Boxplot of edit score (based on Hamming distance) for each generated sample against the 216 reference dataset, measured across all genomic subsections combined. 217\nDiscussion 218\nIn this study we propose a novel approach for generating human genomic data at a large scale 219 and to a high degree of detail. This pipeline combines a novel dimension reduction approach, 220 compression and a GAN structure. These allow a high level of parallelisation and a reduction of 221 computational resources required to generate genomes. Specifically, we show that our models can 222 generate realistic and novel mutation data at a larger scale and at an increased level of detail than 223 previous AI based methods (Yelmen et al. 2021). One of the main applications we envision for this 224 sample generation pipeline is to create synthetic cohorts for specific sets of patients. A WGAN model 225 that has been trained to process general genomes could be specialised on a smaller dataset, obtained 226 for example by a healthcare establishment sequencing all its patients bearing a disease, or exhibiting 227 similar traits. This generator would then create novel samples that follow the same implicit rules as 228 the original patients in the cohort. This would allow the sharing of the cohort\u2019s general information, 229 which could be an important resource for research teams, without revealing the identity of any one 230 person in it. 231 There are many other potential use cases that one could imagine for realistic synthetic 232 genomes generated by AI. Creating a digital twin of an individual or a population could be a useful 233 asset for sharing or experimenting on. Augmenting already available datasets of genomes might lend 234 more robustness to certain research projects. Finally, the autoencoding of genomic data and its 235 accurate reconstruction is a potential method of processing these samples in a secure manner, through 236 federated learning or homomorphic encryption. 237 238 However, combining sequencing data that was obtained using different techniques will 239 require caution as to the standardisation methods used. Additionally, we have limited ourselves to 240 short variations, whereas many large scale, structural, and/or copy number variations also have large 241 impacts on the phenotype of the organisms bearing them. 242 The haplotypes used here as samples are based on available phasing data, which is not 243 guaranteed to be completely accurate, especially at long range. It would be beneficial to projects like 244 these to resolve long range links between mutations, so as to present high accuracy samples to our 245 ML models. Haplotypes are also not consistent between chromosomes, as it is complex to establish 246 biological linkage between mutations across chromosomes, but future applications may look to tie 247 data from different chromosomes together. A project that is able to provide correlation data across 248 chromosomes for certain populations would enable this scale to be explored. 249\nThe recombination loci used to segment the genome here is based off of a study focused on 250 an icelandic population. This represents a very small subset of the Human population, and therefore 251 the hotspots identified are not accurate on a global scale. They still constitute a significant gain of 252 reconstruction accuracy for this methodology and a good starting point, but a similar study on a larger 253 scale would be very beneficial in establishing more widespread recombination hotspots. 254 Regarding the machine learning models implemented in this project, a great deal of 255 exploration remains possible, and our methods are by no means set in stone. VAEs using sigmoid 256 activation function were shown to be the best dimension reduction model amongst those we tested, 257 but there are many more architectures that could be tested in search of even greater efficiency and 258 accuracy. As for generative models, these past years have shown remarkable progress in stable 259 diffusion-based models that allow for detailed illustrations based on user specifications. Although 260 these models are based on convolution neural networks, which are particularly well suited for 261 processing images, which we do not use, it would be interesting to study their application to genomic 262 datasets. 263 Evaluating the realism of a synthetic genome is a difficult task, in part because no standardised 264 tools or methods exist for this purpose, and even more so for a synthetic population. The criteria 265 proposed here - PCA tiling overlap score, Chromopainter metaprofile correlation and edit score - all 266 highlight various aspects of a simulated population that are desirable. To go further, it could be 267 interesting to investigate the biological relevance of mutations contained in these simulated genomes 268 and seek to measure degrees of similarity and difference using variations with known biological 269 meaning. 270\n271\nMethods 272\nPython environment 273\nUnless specified otherwise, all scripts were run in python 3.8.10, using numpy 1.24.3, pandas 274 1.2.4 and keras 2.12.0 with tensorflow backend. Scripts run on the Genotoul cluster were launched 275 using SLURM 22 and parallel release 20180122 (Tange 2011). 276\nCode availability 277\nThe main scripts used during this project for data preprocessing, building ML models and the 278 custom training loop for the WGAN model, are available at this github repository: 279 https://github.com/callum-b/H2G2/ 280\nData 281\nMutation data for 2504 individuals as well as donor information comes from the 1000 282 Genomes Project (phase 3), realigned onto HG38 in (Lowy-Gallego et al. 2019). It was downloaded 283 here: 284 http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/2285 0190312_biallelic_SNV_and_INDEL/ 286 Each donor genome was separated into haplotypes. Mutations on chromosome 1 were filtered 287 to only include those present in at least three of these haplotypes, so as to remove data with little 288 detectable correlation with the rest of the dataset. Contiguous bins of a set number of mutations were 289 then created from this list of variants. 290 Crossing over data was obtained from a high definition map of recombination rates throughout 291 the human genome. Ranges for chromosome 1 were extracted, and those with a recombination rate 292 of <=5 cM per Mb were removed to eliminate noise. The remaining ranges were reduced to their 293 centres and sorted by recombination rate. Any low-intensity ranges within 5kb of a higher intensity 294 range were removed. This resulted in approximately 2500 hotspots across all of chromosome 1. 295 Splitting mutation data into sections bounded by hotspots use these delimiters. Starting at one 296 end of chromosome 1, mutations were added to a section until a hotspot is encountered, at which 297 point the data collected thus far is saved to a file and a new section begins. If fewer than a minimum 298 threshold of mutations are encountered, instead these mutations are appended to the previous section. 299 If more than a maximum threshold of mutations are encountered, an \u201cartificial hotspot\u201d is created, 300 and data is saved to disk for this range. 301\nAutoencoders 302\nClassical autoencoders are composed of an encoder with three hidden layers, then a 303 bottleneck, and finally a mirrored encoder setup for the decoder. Each layer uses ReLU (unless 304 otherwise specified) for output, and there is a 20% dropout rate in between the hidden layers in both 305 encoder and decoder. Bottleneck size is defined as input size divided by 100, rounded up, and the size 306 of the hidden layers is calculated using geometric spacing. It uses classical reconstruction loss, 307 measuring binary cross entropy between the input data and the reconstruction data. 308 Variational autoencoders are composed of an encoder with three hidden layers, then a 309 bottleneck and finally a mirrored encoder setup for the decoder. Each layer is connected by sigmoid 310 (for the final models used for compressing data), except for the bottleneck, which uses linear 311 activation, and the output layer, which uses ReLU with a maximum value of 1. There is a 40% dropout 312 rate in between each hidden layer, for the encoder and the decoder. Bottleneck size is defined as input 313 size divided by 100, rounded up, and the size of the hidden layers is calculated using geometric 314 spacing. Its loss is measured as the sum of classical reconstruction loss and the Kullback-Leibler 315 divergence (Kullback and Leibler 1951), to ensure a locally continuous latent space. 316 The training process for both these model architectures uses a training set and a validation set, 317 with an 80/20 split. The models are trained while monitoring loss on the validation dataset, and when 318 this value increases (with a patience of 10 epochs, or 30 when using sigmoid and tanH activation due 319 to slower convergence), training is halted and the weights of the model are restored to its best 320 performing iteration. The accuracy of this saved model on the validation dataset is used as its 321 performance metric in the figures presented here. 322\nWGAN 323\nThe Generative Adversarial Network used here is composed of two networks: a critic, that 324 assigns a realness score to each sample it analyses, and a generator, that creates novel samples that 325 attempt to fool the critic. It implements the Wasserstein (earth mover) loss function, as well as a 326 custom training loop so as to train the critic more than the generator, as per the usage of Wasserstien 327 loss recommends. This loop also allows checkpoints to be saved during training. 328 The latent vector used contains 1000 random variables drawn from a Gaussian distribution 329 with mean 0 and variance 1. The generator is composed of three hidden layers, using ReLU activation, 330 their sizes determined by the total number of latent dimensions to predict, depending on the genomic 331 sections processed. The first hidden layer is 10 times smaller, the second hidden layer is 5 times 332\nsmaller, and the third hidden layer is 2 times smaller than that total. The output layer comes after this, 333 using the same size and activation function as the combined latent spaces of the autoencoders. 334 The critic is also composed of three hidden layers and an output, using ReLU activation, and 335 their sizes also depend on the total number of latent dimensions considered. The first hidden layer is 336 10 times smaller, the second is 50 times smaller, and the third is 100 times smaller. The output layer 337 is a single neuron with linear output. 338 The models, both the WGAN as a whole and the critic individually, are trained using 339 RMSProp optimiser with a learning rate of 0.00005. In each training loop, the critic is trained 5 times 340 more than the generator. We are using Wasserstein distance, so the aim of the critic is to maximise 341 the distance between the real samples and the simulated ones, and the aim of the generator is to 342 minimise this distance. To implement this during training, the loss of the critic is its loss on simulated 343 samples minus its loss on real samples, and the loss of the generator is equal to -1 times the loss 344 calculated by the critic on the samples created by the generator for this training loop. 345 Multiple checkpoints of the model were sampled during training, and the data they produced 346 were evaluated to find the optimal stopping point. Checkpoints were saved every 10,000 training 347 epochs, and the results presented here are taken from the model trained for 30,000 epochs. 348\nOverlap score 349\nOverlap is calculated based on a tiling of the PCA projection space. For each sample in one 350 dataset, if its tile also contains a sample from the other dataset, it is considered to overlap. This process 351 is applied to both datasets then averaged. 352\nChromopainter 353\nWe estimated the ancestral diversity of our simulated genomes using Chromopainter, which 354 attempts to assign sections of donor genomes to query genomes. We use this as a verisimilitude 355 measure, as our simulated genomes should present approximately the same profile of crossing overs 356 as the reference genomes. 357 We applied this tool to a reference query dataset, composed of three haplogenomes from each 358 subpopulation listed in the 1000 Genomes Project phase 3 supplementary information, and to 100 359 randomly chosen simulated haplogenomes, for three genomic subsections. The donor dataset is 360 composed of the 1000 Genomes Project individuals, without the members of the reference query 361 dataset. 362\nEdit score 363\nThe edit score used here to compare simulated populations to reference populations is based 364 on the Hamming distance metric. We measure this distance for each simulated individual to each 365 reference individual and retain the lowest of these values, which is then divided by the length of the 366 sequence. This creates an \u201cedit rate\u201d, which allows comparison between sections of different sizes. 367 368\nSupplemental figures 369\n370\nSupplemental fig 1 371\na) Scatter plot illustrating the distribution of recombination hotspots along Human chromosome 372 1. Vertical axis is the recombination rate at that hotspot. Some individual hotspots or 373 categories of hotspots are highlighted manually. 374\nb) Boxplot of the size of genomic subsections depending on the segmentation method used. 375 Interestingly, the centromere contains few listed mutations and therefore appears as one very 376 large section. 377 c) Boxplot showing the accuracy of autoencoder models depending on the segmentation method 378 used (order is the same as figure 1a). For each pair of boxes, the richly coloured box is the 379 original model with pruning implemented, and the lighter coloured box is a separate decoder 380 trained based on a reduced latent space. 381\n382\n383\nSupplemental fig 2 384\na) Boxplot of drift of each mutation for different autoencoder models. Average drift for each 385 model is also included. 386 b) Scatter plots of PCAs for different datasets as described for figure 2b, with each row 387 illustrating two different sections. All genomic subsections studied in detail that were not 388 shown in fig 2b are shown here. 389\n390\n391\nSupplemental fig 3 392\nScatter plots of PCAs for different datasets as described for figure 3a, with each row illustrating two 393 different sections. All genomic subsections studied in detail that were not shown in fig 3a are shown 394 here. 395\nAcknowledgments 396\nC.B. wishes to thank the Ligue Contre le Cancer for providing financing for the doctoral thesis 397 behind this project. 398 The authors thank Genotoul for providing data processing and storage resources for the 399 machine learning models listed here. 400\nConflicts of interest 401\nNone declared 402\nReferences 403\nArjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. 2017. \u2018Wasserstein GAN\u2019. arXiv. 404 http://arxiv.org/abs/1701.07875. 405 Balloux, F. 2001. \u2018EASYPOP (Version 1.7): A Computer Program for Population Genetics 406 Simulations\u2019. Journal of Heredity 92 (3): 301\u20132. https://doi.org/10.1093/jhered/92.3.301. 407 Carvajal-Rodr\u00edguez, Antonio. 2008. \u2018GENOMEPOP: A Program to Simulate Genomes in 408 Populations\u2019. BMC Bioinformatics 9 (1): 223. https://doi.org/10.1186/1471-2105-9-223. 409 Halldorsson, Bjarni V., Gunnar Palsson, Olafur A. Stefansson, Hakon Jonsson, Marteinn T. 410 Hardarson, Hannes P. Eggertsson, Bjarni Gunnarsson, et al. 2019. \u2018Characterizing 411 Mutagenic Effects of Recombination through a Sequence-Level Genetic Map\u2019. Science 363 412 (6425): eaau1043. https://doi.org/10.1126/science.aau1043. 413 Hamming, R. W. 1950. \u2018Error Detecting and Error Correcting Codes\u2019. Bell System Technical 414 Journal 29 (2): 147\u201360. https://doi.org/10.1002/j.1538-7305.1950.tb00463.x. 415 Joly, Yann, Ida Ngueng Feze, and Jacques Simard. 2013. \u2018Genetic Discrimination and Life 416 Insurance: A Systematic Review of the Evidence\u2019. BMC Medicine 11 (January): 25. 417 https://doi.org/10.1186/1741-7015-11-25. 418 Juan, Liran, Yongtian Wang, Jingyi Jiang, Qi Yang, Qinghua Jiang, and Yadong Wang. 2020. 419 \u2018PGsim: A Comprehensive and Highly Customizable Personal Genome Simulator\u2019. 420 Frontiers in Bioengineering and Biotechnology 8. 421 https://www.frontiersin.org/articles/10.3389/fbioe.2020.00028. 422 Kim, Miran, and Kristin Lauter. 2015. \u2018Private Genome Analysis through Homomorphic 423 Encryption\u2019. BMC Medical Informatics and Decision Making 15 (Suppl 5): S3. 424 https://doi.org/10.1186/1472-6947-15-S5-S3. 425 Kingma, Diederik P., and Max Welling. 2013. \u2018Auto-Encoding Variational Bayes\u2019. arXiv. 426 http://arxiv.org/abs/1312.6114. 427 Kramer, M.A. 1992. \u2018Autoassociative Neural Networks\u2019. Computers & Chemical Engineering 16 428 (4): 313\u201328. https://doi.org/10.1016/0098-1354(92)80051-A. 429 Kullback, S., and R. A. Leibler. 1951. \u2018On Information and Sufficiency\u2019. The Annals of 430 Mathematical Statistics 22 (1): 79\u201386. https://doi.org/10.1214/aoms/1177729694. 431 Kuo, Tsung-Ting, and Anh Pham. 2021. \u2018Detecting Model Misconducts in Decentralized 432 Healthcare Federated Learning\u2019. International Journal of Medical Informatics 158 433 (December): 104658. https://doi.org/10.1016/j.ijmedinf.2021.104658. 434 Lawson, Daniel John, Garrett Hellenthal, Simon Myers, and Daniel Falush. 2012. \u2018Inference of 435 Population Structure Using Dense Haplotype Data\u2019. PLOS Genetics 8 (1): e1002453. 436 https://doi.org/10.1371/journal.pgen.1002453. 437\nLowy-Gallego, Ernesto, Susan Fairley, Xiangqun Zheng-Bradley, Magali Ruffier, Laura Clarke, 438 and Paul Flicek. 2019. \u2018Variant Calling on the GRCh38 Assembly with the Data from Phase 439 Three of the 1000 Genomes Project\u2019. Wellcome Open Research 4 (December): 50. 440 https://doi.org/10.12688/wellcomeopenres.15126.2. 441 Mu, John C., Marghoob Mohiyuddin, Jian Li, Narges Bani Asadi, Mark B. Gerstein, Alexej 442 Abyzov, Wing H. Wong, and Hugo Y.K. Lam. 2015. \u2018VarSim: A High-Fidelity Simulation 443 and Validation Framework for High-Throughput Genome Sequencing with Cancer 444 Applications\u2019. Bioinformatics 31 (9): 1469\u201371. 445 https://doi.org/10.1093/bioinformatics/btu828. 446 Peng, Bo, and Marek Kimmel. 2005. \u2018simuPOP: A Forward-Time Population Genetics Simulation 447 Environment\u2019. Bioinformatics 21 (18): 3686\u201387. 448 https://doi.org/10.1093/bioinformatics/bti584. 449 Price, W. Nicholson, and I. Glenn Cohen. 2019. \u2018Privacy in the Age of Medical Big Data\u2019. Nature 450 Medicine 25 (1): 37\u201343. https://doi.org/10.1038/s41591-018-0272-7. 451 Prince, Anya E.R. 2017. \u2018Insurance Risk Classification in an Era of Genomics: Is a Rational 452 Discrimination Policy Rational?\u2019 Nebraska Law Review 96 (3): 624\u201387. 453 Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. 2017. \u2018Searching for Activation Functions\u2019. 454 arXiv. http://arxiv.org/abs/1710.05941. 455 Schneider, Valerie A., Tina Graves-Lindsay, Kerstin Howe, Nathan Bouk, Hsiu-Chuan Chen, Paul 456 A. Kitts, Terence D. Murphy, et al. 2017. \u2018Evaluation of GRCh38 and de Novo Haploid 457 Genome Assemblies Demonstrates the Enduring Quality of the Reference Assembly\u2019. 458 Genome Research 27 (5): 849\u201364. https://doi.org/10.1101/gr.213611.116. 459 Shabani, Mahsa, and Luca Marelli. 2019. \u2018Re-Identifiability of Genomic Data and the GDPR\u2019. 460 EMBO Reports 20 (6): e48316. https://doi.org/10.15252/embr.201948316. 461 Tahmasbi, Rasool, and Matthew Keller. 2016. \u2018GeneEvolve: A Fast and Memory Efficient 462 Forward-Time Simulator of Realistic Whole-Genome Sequence and SNP Data\u2019. 463 Bioinformatics (Oxford, England) 33 (September). 464 https://doi.org/10.1093/bioinformatics/btw606. 465 Tange, Ole. 2011. \u2018GNU Parallel: The Command-Line Power Tool\u2019. Login. 466 The 1000 Genomes Project Consortium. 2015. \u2018A Global Reference for Human Genetic Variation\u2019. 467 Nature 526 (7571): 68\u201374. https://doi.org/10.1038/nature15393. 468 Yelmen, Burak, Aur\u00e9lien Decelle, Linda Ongaro, Davide Marnetto, Corentin Tallec, Francesco 469 Montinaro, Cyril Furtlehner, Luca Pagani, and Flora Jay. 2021. \u2018Creating Artificial Human 470 Genomes Using Generative Neural Networks\u2019. PLOS Genetics 17 (2): e1009303. 471 https://doi.org/10.1371/journal.pgen.1009303. 472 Yue, Jia-Xing, and Gianni Liti. 2019. \u2018simuG: A General-Purpose Genome Simulator\u2019. 473 Bioinformatics 35 (21): 4442\u201344. https://doi.org/10.1093/bioinformatics/btz424. 474 475"
        }
    ],
    "title": "Generating realistic artificial Human genomes using adversarial autoencoders",
    "year": 2023
}