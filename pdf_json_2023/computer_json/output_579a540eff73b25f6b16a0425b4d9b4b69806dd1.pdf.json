{
    "abstractText": "In this work, we analyze the relation between reparametrizations of gradient flow and the induced implicit bias on general linear models, which encompass various basic classification and regression tasks. In particular, we aim at understanding the influence of the model parameters \u2014 reparametrization, loss, and link function \u2014 on the convergence behavior of gradient flow. Our results provide user-friendly conditions under which the implicit bias can be well-described and convergence of the flow is guaranteed. We furthermore show how to use these insights for designing reparametrization functions that lead to specific implicit biases like lpor trigonometric regularizers.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hung-Hsu Chou"
        },
        {
            "affiliations": [],
            "name": "Johannes Maly"
        },
        {
            "affiliations": [],
            "name": "Dominik St\u00f6ger"
        }
    ],
    "id": "SP:ac42abc8c57a46b410180294e52d0351cfb411a0",
    "references": [
        {
            "authors": [
                "E. Amid",
                "M.K. Warmuth"
            ],
            "title": "Reparameterizing mirror descent as gradient descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "E. Amid",
                "M.K. Warmuth"
            ],
            "title": "Winnowing with gradient descent",
            "venue": "Proceedings of Thirty Third Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "M. Andriushchenko",
                "A.V. Varre",
                "L. Pillaud-Vivien"
            ],
            "title": "Flammarion. SGD with large step sizes learns sparse features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "S. Arora",
                "N. Cohen",
                "E. Hazan"
            ],
            "title": "On the optimization of deep networks: Implicit acceleration by overparameterization",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "S. Arora",
                "N. Cohen",
                "W. Hu",
                "Y. Luo"
            ],
            "title": "Implicit regularization in deep matrix factorization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "P. Awasthi",
                "A. Das",
                "W. Kong",
                "R. Sen"
            ],
            "title": "Trimmed maximum likelihood estimation for robust generalized linear model",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "S. Azulay",
                "E. Moroshko",
                "M.S. Nacson",
                "B.E. Woodworth",
                "N. Srebro",
                "A. Globerson",
                "D. Soudry"
            ],
            "title": "On the implicit bias of initialization shape: Beyond infinitesimal mirror descent",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "F. Bach"
            ],
            "title": "Active learning for misspecified generalized linear models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2006
        },
        {
            "authors": [
                "E.J. Cand\u00e8s",
                "J. Romberg",
                "T. Tao"
            ],
            "title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2006
        },
        {
            "authors": [
                "H. Chou",
                "C. Gieshoff",
                "J. Maly",
                "H. Rauhut"
            ],
            "title": "Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank",
            "year": 2011
        },
        {
            "authors": [
                "H.-H. Chou",
                "J. Maly",
                "H. Rauhut"
            ],
            "title": "More is less: Inducing sparsity via overparameterization",
            "venue": "Information and Inference: A Journal of the IMA, 12(3):iaad012,",
            "year": 2023
        },
        {
            "authors": [
                "H.-H. Chou",
                "J. Maly",
                "C.M. Verdun"
            ],
            "title": "Non-negative least squares via overparametrization",
            "venue": "arXiv preprint:2207.08437,",
            "year": 2022
        },
        {
            "authors": [
                "H.-H. Chou",
                "H. Rauhut",
                "R. Ward"
            ],
            "title": "Robust implicit regularization via weight normalization",
            "venue": "arXiv preprint:2305.05448,",
            "year": 2023
        },
        {
            "authors": [
                "L. Ding",
                "Z. Qin",
                "L. Jiang",
                "J. Zhou",
                "Z. Zhu"
            ],
            "title": "A validation approach to over-parameterized matrix and image recovery",
            "venue": "arXiv preprint:2209.10675,",
            "year": 2022
        },
        {
            "authors": [
                "A.J. Dobson",
                "A.G. Barnett"
            ],
            "title": "An introduction to generalized linear models",
            "year": 2018
        },
        {
            "authors": [
                "D.L. Donoho"
            ],
            "title": "Compressed sensing",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2006
        },
        {
            "authors": [
                "M. Even",
                "S. Pesme",
                "S. Gunasekar"
            ],
            "title": "Flammarion. (S)GD over diagonal linear networks: Implicit regularisation, large stepsizes and edge of stability",
            "venue": "arXiv preprint:2302.08982,",
            "year": 2023
        },
        {
            "authors": [
                "S. Foucart",
                "H. Rauhut"
            ],
            "title": "A Mathematical Introduction to Compressive Sensing",
            "year": 2013
        },
        {
            "authors": [
                "D. Gissin",
                "S. Shalev-Shwartz",
                "A. Daniely"
            ],
            "title": "The implicit bias of depth: How incremental learning drives generalization",
            "venue": "International Conference on Learning Representations (ICLR).,",
            "year": 2020
        },
        {
            "authors": [
                "S. Gunasekar",
                "J. Lee",
                "D. Soudry",
                "N. Srebro"
            ],
            "title": "Characterizing implicit bias in terms of optimization geometry",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "S. Gunasekar",
                "B. Woodworth",
                "N. Srebro"
            ],
            "title": "Mirrorless mirror descent: A natural derivation of mirror descent",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "S. Gunasekar",
                "B.E. Woodworth",
                "S. Bhojanapalli",
                "B. Neyshabur",
                "N. Srebro"
            ],
            "title": "Implicit regularization in matrix factorization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "J.W. Hardin",
                "J.M. Hilbe",
                "J. Hilbe"
            ],
            "title": "Generalized linear models and extensions",
            "venue": "Stata press,",
            "year": 2007
        },
        {
            "authors": [
                "M. Hardt",
                "T. Ma"
            ],
            "title": "Identity matters in deep learning",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "T.J. Hastie"
            ],
            "title": "Generalized additive models. In Statistical models in S, pages 249\u2013307",
            "year": 2017
        },
        {
            "authors": [
                "P.D. Hoff"
            ],
            "title": "Lasso, fractional norm and structured sparse estimation using a hadamard product parametrization",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2017
        },
        {
            "authors": [
                "P.J. Huber"
            ],
            "title": "Robust Estimation of a Location Parameter",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1964
        },
        {
            "authors": [
                "M. Huh",
                "H. Mobahi",
                "R. Zhang",
                "B. Cheung",
                "P. Agrawal",
                "P. Isola"
            ],
            "title": "The low-rank simplicity bias in deep networks",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Z. Ji",
                "M. Telgarsky"
            ],
            "title": "The implicit bias of gradient descent on nonseparable data",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Z. Ji",
                "M. Telgarsky"
            ],
            "title": "Characterizing the implicit bias via a primal-dual analysis",
            "venue": "In Algorithmic Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "J. Jin",
                "Z. Li",
                "K. Lyu",
                "S.S. Du",
                "J.D. Lee"
            ],
            "title": "Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "C. Kolb",
                "C.L. M\u00fcller",
                "B. Bischl",
                "D. R\u00fcgamer"
            ],
            "title": "Smoothing the edges: A general framework for smooth optimization in sparse regularization using hadamard overparametrization",
            "venue": "arXiv preprint:2307.03571,",
            "year": 2023
        },
        {
            "authors": [
                "J. Li",
                "T.V. Nguyen",
                "C. Hegde",
                "K.W. Wong"
            ],
            "title": "Implicit sparse regularization: The impact of depth and early stopping",
            "venue": "In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Y. Li",
                "T. Ma",
                "H. Zhang"
            ],
            "title": "Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Z. Li",
                "Y. Luo",
                "K. Lyu"
            ],
            "title": "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Z. Li",
                "T. Wang",
                "J.D. Lee",
                "S. Arora"
            ],
            "title": "Implicit bias of gradient descent on reparametrized models: On equivalence to mirror",
            "year": 2022
        },
        {
            "authors": [
                "P. McCullagh",
                "J.A. Nelder"
            ],
            "title": "Generalized linear models",
            "year": 2019
        },
        {
            "authors": [
                "E. Moroshko",
                "B.E. Woodworth",
                "S. Gunasekar",
                "J.D. Lee",
                "N. Srebro",
                "D. Soudry"
            ],
            "title": "Implicit bias in deep linear classification: Initialization scale vs training accuracy",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "M.S. Nacson",
                "J. Lee",
                "S. Gunasekar",
                "P.H.P. Savarese",
                "N. Srebro",
                "D. Soudry"
            ],
            "title": "Convergence of gradient descent on separable data",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "J.A. Nelder",
                "R.W.M. Wedderburn"
            ],
            "title": "Generalized linear models",
            "venue": "Journal of the Royal Statistical Society. Series A,",
            "year": 1972
        },
        {
            "authors": [
                "B. Neyshabur",
                "R. Tomioka",
                "R. Salakhutdinov",
                "N. Srebro"
            ],
            "title": "Geometry of optimization and implicit regularization in deep learning",
            "venue": "arXiv preprint:",
            "year": 2017
        },
        {
            "authors": [
                "B. Neyshabur",
                "R. Tomioka",
                "N. Srebro"
            ],
            "title": "In search of the real inductive bias: On the role of implicit regularization in deep learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "S. Pesme",
                "L. Pillaud-Vivien",
                "N. Flammarion"
            ],
            "title": "Implicit bias of SGD for diagonal linear networks: a provable benefit of stochasticity",
            "year": 2021
        },
        {
            "authors": [
                "B.T. Polyak"
            ],
            "title": "Gradient methods for the minimisation of functionals",
            "venue": "USSR Computational Mathematics and Mathematical Physics,",
            "year": 1963
        },
        {
            "authors": [
                "N. Razin",
                "N. Cohen"
            ],
            "title": "Implicit regularization in deep learning may not be explainable by norms",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "N. Razin",
                "A. Maman",
                "N. Cohen"
            ],
            "title": "Implicit regularization in tensor factorization",
            "venue": "arXiv preprint:",
            "year": 2021
        },
        {
            "authors": [
                "R.T. Rockafellar"
            ],
            "title": "Convex analysis, volume 18 of Princeton Mathematical Series, No. 28",
            "year": 1970
        },
        {
            "authors": [
                "M. Soltanolkotabi",
                "D. St\u00f6ger",
                "C. Xie"
            ],
            "title": "Implicit balancing and regularization: Generalization and convergence guarantees for overparameterized asymmetric matrix sensing",
            "venue": "arXiv preprint:2303.14244,",
            "year": 2023
        },
        {
            "authors": [
                "D. Soudry",
                "E. Hoffer",
                "M.S. Nacson",
                "S. Gunasekar",
                "N. Srebro"
            ],
            "title": "The implicit bias of gradient descent on separable data",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "D. St\u00f6ger",
                "M. Soltanolkotabi"
            ],
            "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "B. Tzen",
                "A. Raj",
                "M. Raginsky",
                "F. Bach"
            ],
            "title": "Variational principles for mirror descent and mirror langevin dynamics",
            "venue": "IEEE Control Systems Letters,",
            "year": 2023
        },
        {
            "authors": [
                "T. Vaskevicius",
                "V. Kanade",
                "P. Rebeschini"
            ],
            "title": "Implicit regularization for optimal sparse recovery",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "L.P. Vivien",
                "J. Reygner",
                "N. Flammarion"
            ],
            "title": "Label noise (stochastic) gradient descent implicitly solves the LASSO for quadratic parametrisation",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "B. Woodworth",
                "S. Gunasekar",
                "J.D. Lee",
                "E. Moroshko",
                "P. Savarese",
                "I. Golan",
                "D. Soudry",
                "N. Srebro"
            ],
            "title": "Kernel and rich regimes in overparametrized models",
            "venue": "In Proceedings of Thirty Third Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "F. Wu",
                "P. Rebeschini"
            ],
            "title": "A continuous-time mirror descent approach to sparse phase retrieval",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "F. Wu",
                "P. Rebeschini"
            ],
            "title": "Implicit regularization in matrix sensing via mirror descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "F. Wu",
                "P. Rebeschini"
            ],
            "title": "Nearly minimax-optimal rates for noisy sparse phase retrieval via early-stopped mirror descent",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2023
        },
        {
            "authors": [
                "X. Xu",
                "Y. Shen",
                "Y. Chi",
                "C. Ma"
            ],
            "title": "The power of preconditioning in overparameterized low-rank matrix sensing",
            "venue": "arXiv preprint:2302.01186,",
            "year": 2023
        },
        {
            "authors": [
                "C. Yun",
                "S. Krishnan",
                "H. Mobahi"
            ],
            "title": "A unifying view on implicit bias in training linear neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "C. Zhang",
                "S. Bengio",
                "M. Hardt",
                "B. Recht",
                "O. Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Keywords \u2014 Gradient flow, implicit bias, generalized linear models"
        },
        {
            "heading": "1 Introduction",
            "text": "Initiated by the surprising success of massively overparameterized models in deep learning, recent research has been focusing more and more on understanding the first-order descent methods that are used in training. The term training refers hereby to the optimization of a designed loss function on given training data over the parameters of the network. Despite the existence of multiple minimizers, (stochastic) gradient descent tends to pick parameter configurations that make the networks generalize well to unseen data. Moreover, the performance of computed networks seemingly increases with the overparametrization [41, 42, 60]. This tendency of (stochastic) gradient descent to prefer certain minimizers over others is commonly called implicit bias/regularization and appears to be a key to understanding the reliability of deep learning approaches.\nDue to the complicated structure of general neural networks, a rigorous analysis is difficult to achieve given the current mathematical tools we have. Thus in the past few years many works have focused on simplified settings like underdetermined linear systems and matrix factorization/sensing, cf. Section 1.2. Solving such problems via overparametrized gradient descent shows a resemblance to the training of linear neural networks while allowing more rigorous theoretical analysis. Interestingly, all of these works demonstrate an implicit bias of vanilla gradient flow/descent towards solutions of simple structure like sparsity or low rank.\nLet us make this precise in the case of solving underdetermined linear systems. The problem is about recovering an unknown vector w\u22c6 \u2208 RN from few linear observations\ny = Aw\u22c6 \u2208 RM , (1)\nar X\niv :2\n30 8.\n04 92\n1v 1\n[ m\nat h.\nO C\n] 9\nA ug\nwhere A \u2208 RM\u00d7N . A basic approach to solve (1) via gradient descent is to compute solutions to\nargmin z\u2208RN\n\u2225Az\u2212 y\u222522. (2)\nIt is straight-forward to verify that the gradient descent trajectory w(t) initialized at w0 = 0 converges to the least-square fit w\u221e := limt\u2192\u221e w(t) = A\n\u2020y, which minimizes the \u21132-norm among all solutions of (2). Here the matrixA\u2020 \u2208 RM\u00d7N denotes the Moore-Penrose pseudoinverse ofA. On the one hand, we know that in general w\u221e \u0338= w\u22c6 if M \u226a N . On the other hand, if w\u22c6 is s-sparse1 and A behaves sufficiently well with M \u2273 s log(N/s), signal processing theory [9, 16, 18] shows that w\u22c6 can be uniquely identified from (1) by solving the constrained \u21131-minimization problem\nargmin z\u2208RN\n\u2225z\u22251, s. t. Az = y, (3)\nwhere the \u21131-norm serves as an explicit regularizer for sparsity. Recent works [52, 54, 11, 33] proved that instead of solving (3), which is convex but non-differentiable due to the \u21131-norm, one can recover w\u22c6 by solving the overparametrized but unregularized least-squares problem\nargmin z(1),...,z(L)\u2208RN \u2225\u2225\u2225A(z(1) \u2299 \u00b7 \u00b7 \u00b7 \u2299 z(L))\u2212 y\u2225\u2225\u22252 2\n(4)\nvia vanilla gradient flow/descent, where \u2299 denotes the (entry-wise) Hadamard product. Indeed, several works [11, 33, 54] guarantee that if all Hadamard factors are initialized close to the origin, then gradient flow/descent (w(1)(t), . . . ,w(L)(t)) converges to (w (1) \u221e , . . . ,w (L) \u221e ) with w\u0303\u221e := w (1) \u221e \u2299 \u00b7 \u00b7 \u00b7\u2299w(L)\u221e being an approximate \u21131-minimizer among all possible solutions of (1), and thus w\u0303\u221e \u2248 w\u22c6. Such phenomena led to the conclusion that the (artificially) introduced overparametrization allows gradient descent to implicitly identify the underlying structure of the problem, i.e., sparsity of w\u22c6. Interestingly, the formulation in (4) is equivalent to the training of deep linear networks with diagonal weight matrices [54] by viewing z(\u2113) as the diagonal of the \u2113-th layer weight matrix.\nIt is important to note that several works [54, 19, 11] make use of a simple observation in their analysis: if all z(k) are initialized with the same vector w0 and gradient flow is applied to (4), then the iterates w(k) stay identical over time and (4) is equivalent to solving\nargmin z\u2208RN\n\u2225Az\u2299L \u2212 y\u222522 (5)\nvia gradient flow with initialization w0, where z \u2299L = z\u2299 \u00b7 \u00b7 \u00b7 \u2299 z denotes the L-th Hadamard power of z, see e.g. [11, Lemma 12]. In other words, the implicit bias of overparametrized gradient flow in (4) is linked to the implicit bias of reparametrized gradient flow in (5). The only difference between (2) and (5) lies in a reparametrization of the input space via the univariate non-linear map z 7\u2192 zL that is applied entry-wise. A related reduction technique appears in the analysis of deep linear networks under the name balanced initialization [24, 4]. Therefore, to analyze the implicit bias of gradient descent in general neural network training, it seems necessary to fully understand the relation between over- and reparametrization of the input space in the simplified settings first.\nBut it is not only about deep learning. As we have just seen, the smooth reparametrization in (5) allows to solve (3) via vanilla gradient descent while bypassing the lacking differentiability of the \u21131-regularizer. It only seems natural to leverage this concept in other explicitly regularized regression problems as well in order to provide novel and smooth alternatives to existing solvers.\n1A vector z \u2208 RN is called s-sparse if at most s of its entries are non-zero."
        },
        {
            "heading": "1.1 Contribution",
            "text": "In this work, we restrict ourselves to the setting of linear systems as outlined above and focus on two central questions. Which implicit regularization other than \u21131-norm minimization can be induced via reparametrized gradient flow/descent? How does the choice of the squared loss influence gradient flow? We approach these questions on the class of generalized linear models [15, 23, 37, 25]. Generalized linear models are linear regression models in which the underlying linear model may be related to the response variables by a (non-linear) link function. They have been introduced in [40] and encompass various basic classification and regression tasks [8, 6]. We only consider link functions that act entry-wise on vectors. Under this restriction the training of generalized linear models corresponds to the training of single neurons without bias in general feedforward neural networks.\nDefinition 1.1. Let N,M \u2208 N, A \u2208 RM\u00d7N and y \u2208 RM . Let \u03c1link : R \u2192 R be a link function acting entry-wise on vectors and let L : RM \u00d7 RM \u2192 R be a general loss function with minimum value 0. The goal of the generalized linear model (GLM) is to fit \u03c1link(Az) to y by solving\nmin z\u2208RN L(\u03c1link(Az),y) (6)\nFor any reparametrization function \u03c1rp : R \u2192 R acting entry-wise on vectors, the reparametrized gradient flow w : [0,\u221e) \u2192 RN on (6) is defined by the dynamics\nw\u2032(t) = \u2212\u2207L(w(t)), w(0) = w0, (7)\nwhere w0 \u2208 RN and L : RN \u2192 R is defined as\nL(z) := L ( \u03c1link ( A\u03c1rp(z) ) ,y ) . (8)\nNote that (8) reduces to (5) if we set \u03c1link(z) = z, \u03c1rp(z) = z L, and L(z, z\u2032) = \u2225z\u2212 z\u2032\u222522.\nOur contribution: In a nutshell, our results show that the implicit bias of gradient flow in (7) is governed by the choice of the reparametrization \u03c1rp and the initialization w0, whereas the convergence behavior depends in addition on the regularity of the link function \u03c1link and the loss L. In light of existing work, our contribution can be summarized as follows:\n\u2022 Theorem 2.3 (i): We provide sufficient conditions on \u03c1link, \u03c1rp, and L that are simple to evaluate and guarantee a well-behaved trajectory of w(t) for all times t \u2265 0. Existing works implicitly assume this via existence of a limit, e.g. [22, 54, 1], or restrict themselves to particular choices of \u03c1link, \u03c1rp, and L, e.g. [52, 55, 11].\n\u2022 Theorem 2.3 (ii): For a wide class of reparametrizations \u03c1rp, we explicitly characterize the implicit bias of w(t) in the limit in terms of a Bregman divergence D(\u00b7,w0) depending on \u03c1rp and w0. In contrast to previous work [20, 1, 56], which already described a connection between the implicit bias of gradient flow and particular Bregman divergences, our assumptions on \u03c1rp allow us to easily calculate D from \u03c1rp. We provide a two-way relation between \u03c1rp and D that facilitates to design a suitable \u03c1rp for obtaining a specific implicit bias of w(t). An analogous relation appeared recently in [36]. Due to the more general setting considered therein, the results however require more restrictive assumptions. Whereas we assume \u03c1rp to be once differentiable, the authors of [36] assume twice continuous differentiability.\n\u2022 Theorem 2.6: By setting \u03c1link = Id, we furthermore guarantee convergence ofw(t) and describe the convergence rates in dependence on the properties of L. Many existing works on implicit regularization explicitly assume convergence of w(t), cf. Section 1.2. Note that the proof techniques we use in this part are conceptually close to the ones used in [51] which appeared recently and analyzes mirror descent. We however emphasize that the authors of [51] consider a simpler setting in which the global minimizer is unique. Moreover, to the best of our knowledge the sufficient condition for boundedness of the gradient flow trajectory in Theorem 2.6 (iii) has not appeared previously.\nWith this work we aim to unify various existing (partial) results on implicit bias of gradient flow on regression models (resp. diagonal linear networks) into a coherent theme and make them easily accessible. Especially the latter goal motivates the title of our paper. Apart from deepening the understanding of the influence of over-/reparametrization on gradient flow, we aim to provide a user-friendly approach to design reparametrizations in regression models that implicitly encode specific regularizers into (vanilla) gradient flow/descent. Before diving into the technical details, we would like to showcase two special instances of such implicit biases, which can be easily designed from our results. They shall serve as a blueprint for further adaptions. To keep the presentation concise for casual readers, we restrict ourselves to \u03c1link = Id, only consider vanishing initialization, and do not discuss convergence rates in this section. The more interested reader finds those details in the general statements in Section 2. Let us summarize the assumptions that we use in both showcase theorems.\nAssumption 1.2. For the rest of this section, we assume the following simplified setting:\n\u2022 \u03c1link = Id.\n\u2022 L is continuously differentiable and satisfies minL = 0 with L(z, z\u2032) = minz,z\u2032\u2208RM L(z, z\u2032) if and only if z = z\u2032.\nOur first showcase result illustrates how to induce \u2113p-regularization.\nTheorem 1.3. Under Assumption 1.2, let \u03c1rp(z) = z 2 p , for p \u2208 (0, 2), and let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N and y \u2208 RM . Assume that there exists z0 \u2208 RN>0 with Az0 = y and set \u03c1rp(w0) = \u03b11, for \u03b1 > 0.\nThen, for any \u03b1 sufficiently small, w\u0303\u221e,\u03b1 := limt\u2192\u221e \u03c1rp(w(t)) \u2208 RN\u22650 exists and\nlim \u03b1\u21920 gp (w\u0303\u221e,\u03b1) = min z\u2208RN\u22650,L(z)=0 gp (\u03c1rp(z)) , (9)\nwhere\ngp(z) := { \u2225z\u22251 if p \u2264 1 \u2225z\u2225pp if p > 1,\n(10)\ni.e., for small \u03b1, the reparametrized flow \u03c1rp(w(t)) approximately minimizes gp among all (reparametrized) global minimizers of L.\nTheorem 1.3 combines Corollaries 2.7 and 2.8 below. It interpolates between established results for implicit \u21131-minimization [52, 54, 11] and the implicit \u21132-minimization in (2) which corresponds to the case p = 2. Note that Theorem 1.3 covers the reparametrization in (5) by setting p = 2L , for L \u2265 2, such that w\u0303\u221e,\u03b1 = limt\u2192\u221e w(t)\u2299L, and recovers the implicit bias characterization in [1,\nTheorem 4], which needed to explicitly assume convergence of the flow. Finally, note that Theorem 1.3 differs from existing \u21131/\u21132-interpolation results like [54, 56] which change the initialization scale or a mirror map parameter and only work in the limit cases of vanishing (\u21131-norm minimization) and exploding parameter (\u21132-norm minimization). Remark 1.4. The restriction of Theorem 1.3 to the positive orthant RN\u22650 is due to the fact that w(t) cannot cross orthant boundaries by the choice of \u03c1rp, a fact that has previously been observed (and also exploited) in [52, 33, 11, 12]. For \u2113p-regularization on the whole space RN one can always enlarge the parameter space by considering the modified loss function\nL\u00b1(w\u00b1) := L ( \u03c1link(A \u00b1\u03c1rp(w \u00b1)),y ) ,\nfor A\u00b1 := [ A \u2212A ] \u2208 RM\u00d72N and w\u00b1 := [ wT+ w T \u2212 ]T \u2208 R2N and initialize both w+,w\u2212 \u2208 RN in the positive orthant. While each of the two is restricted to RN\u22650, their reparametrized difference w\u0303 := \u03c1rp(w+) \u2212 \u03c1rp(w\u2212) is free to move all over RN . This strategy has been applied in many preceding works to extend implicit bias statements like (9) to RN .\nOur second showcase result applies our insights to trigonometric reparametrizations of space.\nTheorem 1.5. Under Assumption 1.2, let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N and y \u2208 RM . Assume that there exists z0 \u2208 RN with Az0 = y and set w0 = 0. Then, the following statements hold:\n1. Let \u03c1rp(z) = sinh(z). Then, w\u0303\u221e := limt\u2192\u221e \u03c1rp(w(t)) \u2208 RN exists and\ngsinh (w\u0303\u221e) = min z\u2208RN ,L(z)=0 gsinh (\u03c1rp(z)) , (11)\nwhere\ngsinh(z) = \u27e8z, arctan(z)\u27e9 \u2212 \u27e81, 12 log(1+ z \u22992)\u27e9. (12)\n2. Let \u03c1rp(z) = tanh(z). Then, w\u0303\u221e := limt\u2192\u221e \u03c1rp(w(t)) \u2208 [\u22121, 1]N exists and\ngtanh (w\u0303\u221e) = min z\u2208RN ,L(z)=0 gtanh (z) , (13)\nwhere\ngtanh(z) = \u27e8z, artanh(z)\u27e9. (14)\nTheorem 1.5 combines Corollaries 2.9 and 2.10. Whereas in Theorem 1.3 we could not set w0 to zero due to the saddle-point structure of the loss function, Theorem 1.5 allows to replace the small initialization limit by w0 = 0. Observing furthermore that arctan(z) \u2248 \u03c02 sign(z) for |z| \u226b 1 and arctan (z) \u2248 z for |z| \u226a 1, we note that\n(\u27e8z, arctan(z)\u27e9)i \u2248 { \u03c0 2 |zi| if |zi| \u226b 1 |zi|2 if |zi| \u226a 1\n(15)\nwhich has the nice property that it behaves globally like the \u21131-norm but is smooth around 0. Since the additional log-term in gsinh is almost neglectable in comparison, the regularizer gsinh can be viewed as an alternative to the Huber loss without additional hyper-parameter, cf. Figure 1.\nRemark 1.6. The theory we present in the following is based on Definition (1.1) which strongly restricts the choice of \u03c1rp and \u03c1link to univariate functions. Let us mention that all results can be generalized in a straight-forward way to cover any pair of entry-wise defined link and reparametrization functions, i.e., all functions \u03c1link : RM \u2192 RM and \u03c1rp : RN \u2192 RN of the form\n\u03c1link(z) = (\u03c1 (1) link(z1), . . . , \u03c1 (M) link (zM )) T ,\n\u03c1rp(z) = (\u03c1 (1) rp (z1), . . . , \u03c1 (N) rp (zN )) T ,\nwhere \u03c1 (1) link, . . . , \u03c1 (M) link , \u03c1 (1) rp , . . . , \u03c1 (N) rp : R \u2192 R are real-valued functions. For the sake of conciseness, we however refrain from presenting the results in full generality. Also note that the recent work [36] allows more general choices of \u03c1rp but requires twice continuous differentiability."
        },
        {
            "heading": "1.2 Related work",
            "text": "Before turning to the mathematical details of our work, let us review the recent progress in the field. Since modern machine learning models such as neural networks are typically highly overparameterized [60], it is important to understand to which minimizer gradient descent converges. Algorithmic or implicit regularization refers to the phenomenon that gradient descent tends to pick minima with low intrinsic complexity even when no regularization is enforced explicitly. Indeed, empirically it has been observed that the weights of trained (deep) neural networks are often approximately lowrank [28]. However, a precise theoretical explanation does not seem to be within reach of current techniques.\nNonetheless, the deep learning theory community has made progress in understanding the implicit regularization phenomenon by studying simplified models. For example, implicit regularization has been observed in classification tasks, see e.g., [49, 39, 38, 29, 30], where gradient-based methods favor max-margin solutions.\nFor regression tasks, the implicit regularization on reparameterized loss functions has been studied in [52, 54, 1, 2, 59, 7, 33, 11]. A key insight of this line of research is that gradient flow on the\nreparameterized function can be interpreted as mirror flow on the original function and that mirror flow converges to the solution which is closest to the initialization with respect to the Bregman divergence induced by the mirror map. Recent work aims to characterize whether gradient flow for a particular reparameterization can be written as an equivalent mirror flow [21, 36]. In a more general setting but under stronger assumptions on the reparametrization map (including higher order differentiability), the authors of [36] derive convergence guarantees and a relation between reparametrization and implicit bias that is analogous to (17) below.\nBuilding on these insights in linear regression, the authors of [12] use the reparametrization in (5) to encode convex constraints into the optimization landscape. In [32], the authors discuss how to use Hadamard product reparametrization to replace non-smooth \u2113p,q-regularized problems by smooth \u21132-regularized counterparts. In the context of sparse phase retrieval, mirror flow with hypentropy mirror map and the closely related quadratically reparameterized Wirtinger flow have been studied in [55, 57].\nMoving beyond vector-valued problems, implicit regularization has also been studied in overparameterized low-rank matrix recovery. In the influential work [22] it has been observed that factorized gradient descent converges to a solution with low-rank when the initialization is chosen sufficently small. A number of works investigated both the symmetric setting [34, 35, 50, 58, 14, 31] and the asymmetric setting [48]. In [5, 10] the authors studied the scenario with more than two factors, which sometimes is referred to as deep matrix factorization. In [56], it has been shown that mirror descent with sufficiently small initialization converges to the solution with lowest nuclear norm. In [45] an example was presentend, in which factorized gradient descent with small initialization converges to a low-rank solution which is not the nuclear norm minimizer. In the setting of low-rank tensor recovery, [46] investigated the bias of gradient descent towards low-rank tensors.\nThis paper, along with most of the aforementioned works, focuses on how implicit regularization is influenced by a combination of reparametrization and scale of initialization. It is worth to note that there are other mechanisms which induce algorithmic regularization such as weight normalization [13] or label noise [43, 53]. In [3, 17] an intriguing connection regarding implicit regularization induced by large step sizes coupled with SGD noise has been discussed."
        },
        {
            "heading": "1.3 Outline and Notation",
            "text": "The remaining paper is structured as follows. In Section 2, we will present our results in full generality and discuss how to derive Theorems 1.3 and 1.5 from them. Parts of the proofs are deferred to Section 3.\nAt this point, let us briefly introduce the notation we use in the rest of the work. For N \u2208 N, we denote [N ] = {1, 2, . . . , N}. Boldface lower-case letters such as z represent vectors with entries zn, while boldface upper-case letters such as A represent matrices with entries Amn. We denote by \u03c3min(A) the smallest non-zero singular value ofA. For vectors u,v \u2208 RN , u \u2265 v means that un \u2265 vn for all n \u2208 [N ]. In addition, u \u2265 0 means that un \u2265 0 for all n \u2208 [N ]. The operator diag can be applied to matrices and vectors. In the former case, it extracts the diagonal of the input as a vector; in the latter case, it embeds the input as diagonal into a square matrix. We will use 0 and 1 to denote the all-zero and all-ones vector. The dimensions of both will always be clear from the context.\nReparametrized quantities: Since our work considers reparametrizations of the input space RN , we will use tilde-symbols to distinguish a plain vector z \u2208 RN from its reparametrization z\u0303 = \u03c1rp(z), which lives in the same space but is usually only compared to other reparametrized vectors. In a similar way, the reparametrized version of the gradient flow w(t) with domain DN \u2282 RN will be abbreviated by w\u0303(t) = \u03c1rp(w(t)) with (reparametrized) domain DNrp \u2282 RN .\nHadamard calculus: We use \u2299 to denote Hadamard products and powers, e.g., (z\u2299y)n = znyn and (z\u2299L)n = z L n . We denote the closed positive orthant by RN\u22650. For any S \u2282 RN , int(S) is the interior of S and \u2202S the boundary of S. If h : R \u2192 R is a real-valued function, it acts entry-wise on vectors, i.e., h(z) \u2208 RN with [h(z)]n = h(zn). If h : RN \u2192 RN is a decoupled function, i.e.\nh(z) = (h1(z1), . . . , hN (zN ))\nfor some hn : R \u2192 R and n \u2208 [N ], we write its entry-wise derivative as h\u2032 : RN \u2192 RN where\nh\u2032(z) = (h\u20321(z1), . . . , h \u2032 N (zN ))."
        },
        {
            "heading": "2 Reparametrizing gradient flow on GLMs",
            "text": "In this section, we present our results in full detail and explain how to derive Theorems 1.3 and 1.5 from them. All omitted proofs can be found in Section 3 below.\nAssumption 2.1. We will use the following set of assumptions.\n(A1) \u03c1link : R \u2192 R is continuously differentiable and injective with y \u2208 \u03c1link(RM )\n(A2) \u03c1rp : R \u2192 R is continuously differentiable and there exist closed and convex intervals D,Drp \u2282 R such that \u03c1rp : D \u2192 Drp is invertible with \u03c1\u2032rp(z) = 0 if and only if z \u2208 \u2202D.\n(A3) L : RM \u00d7 RM \u2192 R is continuously differentiable and satisfies minL = 0 with L(z, z\u2032) = minz,z\u2032\u2208RM L(z, z \u2032) if and only if z = z\u2032.\nParts of the results will require the strengthened assumptions that\n(B1) \u03c1link = Id.\n(B2) there exists z\u03030 \u2208 int(DNrp) with Az\u03030 = y.2\nAssuming convergence of (7) to global optimality in (8), the first theorem characterizes to which minimizer the flow converges. In particular, if \u03c1rp is injective on DN and has a vanishing first derivative on the boundary, then among all global minimizers of L the limit of gradient flow minimizes the distance to the initialization w0 measured in terms of a Bregman divergence that only depends on the choice of \u03c1rp. In view of existing work [36], the theorem only requires a once continuously differentiable reparametrization \u03c1rp. Moreover, its set of assumptions is easy to evaluate.\nDefinition 2.2 (Bregman Divergence). Let F : \u2126 \u2192 R be a continuously-differentiable, strictly convex function defined on a closed convex set \u2126. The Bregman divergence associated with F for points p \u2208 \u2126 and q \u2208 int (\u2126) is defined as\nDF (p, q) = F (p)\u2212 F (q)\u2212 \u27e8\u2207F (q), p\u2212 q\u27e9. (16)\nTheorem 2.3 (Implicit Bias). Let w(t) \u2208 RN be the gradient flow trajectory defined by (7) in Definition 1.1, for A \u2208 RM\u00d7N , y \u2208 RM , \u03c1rp, \u03c1link : R \u2192 R, and L : RM \u00d7 RM \u2192 R. Define the reparametrized flow and loss-function\nw\u0303(t) := \u03c1rp(w(t)) and L\u0303(z\u0303) = L ( \u03c1link(Az\u0303),y ) for all z\u0303 \u2208 RN , and suppose that Assumptions (A1), (A2), and (A3) hold.\nThen, for w\u03030 \u2208 int(DNrp) the following statements hold: 2Note that this assumption together with Assumptions (A2) and (A3) implies that L achieves its minimum at 0\n(i) {w\u0303(t) : t \u2265 0} \u2282 int(DNrp)\n(ii) Define H : int(Drp) \u2192 R as an antiderivative of h : int(Drp) \u2192 R, which in turn is defined as an antiderivative of\nh\u2032(z\u0303) := ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 . (17)\nThis is well-defined and continuous on int(Drp) due to Assumption (A2). If the limit w\u221e := limt\u2192\u221e w(t) exists, L(w\u221e) = 0, and\n\u2022 w\u0303\u221e \u2208 int(DNrp) or \u2022 the continuous extensions of h and H to Drp are finite-valued,\nthen w\u0303\u221e \u2208 argmin\nz\u0303\u2208DNrp ,L\u0303(z\u0303)=0 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9. (18)\nMinimizing the objective in (18) is furthermore equivalent to minimizing the Bregman divergence DF (z\u0303, w\u03030) defined on int(DNrp) with respect to F (z\u0303) := \u27e81, H(z\u0303)\u27e9 and extended to DNrp by continuity (including +\u221e as admissible objective value)3.\nThe proof of Theorem 2.3 is provided in Section 3.1. Note that apart from providing a characterization of the implicit bias in (18), Theorem 2.3 also controls the trajectory of w\u0303(t). The specific shape of \u03c1rp assumed in (A2) is crucial for both restricting the trajectory of w\u0303(t) to a certain domain of interest and for allowing a concise representation of H \u2032\u2032 = h\u2032.4 Let us highlight some more points before moving on:\n(i) Dependence on data (A,y): Note that Claim (ii) in Theorem 2.3 makes an implicit assumption on A and y. Indeed, in light of (A3) the demanded existence of w\u221e with L(w\u221e) = 0 implicitly implies existence of a solution z of A\u03c1rp(z) = \u03c1 \u22121 link(y).\n(ii) Dependence on initialization w\u03030: The implicit regularization in (18) not only depends on the choice of \u03c1rp but also on the initialization w\u03030. Theorems 1.3 and 1.5 assumed that w\u03030 is close to the origin and in deep learning it is common to choose a small, random initialization w\u03030 [22, 5, 50]. Note, however, that this does not need to be the case in general [54].\n(iii) Model design based on a desirable implicit bias/regularization: To create specific regularizers as in Theorems 1.3 and 1.5, one has to reverse-engineer Theorem 2.3 and compute a suitable \u03c1rp for given F . To be precise, suppose we would like to minimize a function which can be expressed (or approximated) by a Bregman divergence with generating function F of the form F (z\u0303) := \u27e81, H(z\u0303)\u27e9, for some H : RN \u2192 RN . Then the corresponding \u03c1rp can be explicitly calculated from Theorem 2.3 as soon as H is twice differentiable.\n(iv) More general loss functions: Although one can usually assume the minimal value of L to be zero for losses that are bounded from below, there might be cases in which the assumption minL = 0 does not hold. Under Assumption (B1), i.e., the linear regression setting, the assumption L(w\u221e) = 0 can be relaxed to L(w\u221e) = minL. The required adaptions of the proof are straight-forward. Extending this to general \u03c1link is an open problem.\n3 It is easy to verify in Definition 2.2 that DF is invariant under adding affine functions to F and thus independent of the particular choices of h and H.\n4Relaxing Assumption (A2) by not assuming a vanishing derivative of \u03c1rp on \u2202D, Theorem 2.3 (ii) remains to hold as long as 2.3 (i) is assumed to be true. Without boundary conditions on \u03c1rp, however, the latter can hardly be expected to hold in general.\n(v) More general reparametrizations: To cover the more general setting described in Remark 1.6, in which \u03c1rp : RN \u2192 RN may vary on different entries of input vectors, one only has to replace DN and DNrp in Theorem 2.3 with DN = D1\u00d7\u00b7 \u00b7 \u00b7\u00d7DN and DNrp = (Drp)1\u00d7\u00b7 \u00b7 \u00b7\u00d7(Drp)N , for D1, . . . ,DN , (Drp)1, . . . , (Drp)N \u2282 R, require that Assumption (A2) holds for each marginal of \u03c1rp, and define h : int(DNrp) \u2192 RN accordingly. For the sake of conciseness, we leave this adaption to the reader.\nWhereas \u03c1rp heavily influences the characterization of w\u0303\u221e in (18) via F , the choice of \u03c1link and L plays a minor role, i.e., the implicit bias of gradient flow on (8) is solely determined by \u03c1rp and w0. The following result shows that \u03c1link and L strongly influence the convergence behavior of gradient flow. Indeed, for \u03c1link = Id and convex loss functions L, the trajectory of gradient flow either converges to a global optimum or is unbounded. Consequently, the convergence assumption of Theorem 2.3 is automatically fulfilled as long as the trajectory stays bounded over time. The loss L hereby determines the decay rate in objective value. In particular, if L satisfies the so-called Polyak-Lojasiewicz inequality we derive a linear rate.\nDefinition 2.4. [44, Condition C] A differentiable function f : RN \u2192 R with minimum f\u22c6 satisfies the Polyak-Lojasiewicz inequality with \u00b5 > 0 if, for any z \u2208 RN , it holds that\n\u2225\u2207f(z)\u222522 \u2265 2\u00b5(f(z)\u2212 f\u22c6).\nRemark 2.5. Let us recall two useful observations:\n(i) Any \u00b5-strongly convex function f , i.e., for any u,v \u2208 RN ,\nf(v) \u2265 f(u) + \u27e8\u2207f(u),v \u2212 u\u27e9+ \u00b5 2 \u2225u\u2212 v\u222522,\nsatisfies the Polyak-Lojasiewicz inequality with \u00b5.\n(ii) If f(z) = g(Az), for some matrix A \u2208 RM\u00d7N and g : RM \u2192 R satisfying the PolyakLojasiewicz inequality with \u00b5, then f satisfies the Polyak-Lojasiewicz inequality with \u03c32min(A) \u00b5, where \u03c3min (A) denotes the smallest non-zero singular value of A.\nTheorem 2.6 (Convergence). Let w(t) \u2208 RN be the gradient flow trajectory defined by (7) in Definition 1.1, for A \u2208 RM\u00d7N , y \u2208 RM , \u03c1rp, \u03c1link : R \u2192 R, and L : RM \u00d7 RM \u2192 R. Define the reparametrized flow and loss-function as\nw\u0303(t) := \u03c1rp(w(t)) and L\u0303(z\u0303) = L ( \u03c1link(Az\u0303),y ) for all z\u0303 \u2208 RN . Assume that Assumptions (B1), (A2), (A3), and (B2) hold and recall from Theorem 2.3 the function h defined in (17) and the Bregman divergence DF .\nThen, the following claims hold for all w\u03030 \u2208 int(DNrp):\n(i) General convergence rate: For all t \u2265 0, it holds that\nL\u0303(w\u0303(t)) \u2264 DF (z\u03030, w\u03030) t .\nIn particular, one has that DF (z\u03030, w\u03030) < \u221e and limt\u2192\u221e Aw\u0303(t) = y.\n(ii) Linear convergence rate: If, in addition, L(\u00b7,y) satisfies the Polyak-Lojasiewicz inequality for \u00b5 > 0 and (h\u2032)\u2299\u22121(w\u0303(t)) \u2265 r, for some r > 0 and all t \u2265 0, then\nL\u0303(w\u0303(t)) \u2264 L\u0303(w\u03030)e\u22122r\u00b5\u03c3 2 min(A)t,\nwhere \u03c3min(A) is the smallest non-zero singular value of A.\n(iii) Convergence of trajectory: If sup t\u22650\n\u2225w\u0303(t)\u22252 < +\u221e, then\nw\u0303(t) \u2192 w\u0303\u221e \u2208 DNrp with L\u0303(w\u0303\u221e) = 0. (19)\nMoreover, if Drp is unbounded, i.e., Dlimrp := {inf Drp, supDrp} \u2229 {\u2212\u221e,\u221e} \u0338= \u2205, and the univariate function z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) is in the function class\nFDrp = { f \u2208 Lc1(Drp) : \u2223\u2223\u2223 \u222b b z f(x)dx \u2223\u2223\u2223 = \u221e, for any z \u2208 int(Drp) and b \u2208 Dlimrp }, (20)\nwhere Lc1(Drp) = {f \u2208 L1(C) for any compact C \u2282 Drp}, then sup t\u22650 \u2225w\u0303(t)\u22252 < +\u221e and (19) holds.\nThe proof of Theorem 2.6 can be found in Section 3.2. The theorem sheds some light on the influence that the different components of our model in (8) have on the expected convergence behaviour of gradient flow. First, if \u03c1link = Id, the objective value of L\u0303 decays along the trajectory with rate O( 1t ) independent of the convergence behaviour of w\u0303(t). If L behaves well, this rate is improved to O(e\u2212ct). Second, although \u03c1rp has no influence on the rate of convergence in a qualitative sense \u2014 the choice of \u03c1rp only influences the constants C and C\n\u2032 in Theorem 2.6 (i) and (ii) \u2014, it determines whether [z 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303)] \u2208 FDrp and thus whether one can expect boundedness/convergence of the trajectory w\u0303(t), for t \u2192 \u221e. Whereas analogs to Claims (i) and (ii) already appeared in special settings [56, 51], the sufficient condition (20) for boundedness of the trajectory w\u0303(t) is novel to the best of our knowledge."
        },
        {
            "heading": "2.1 How to induce specific regularization",
            "text": "With Theorem 2.3 and Theorem 2.6 in hand, we now discuss how the specific instances of implicit regularization in Theorem 1.3 and Theorem 1.5 can be derived from them. We restrict ourselves to the case that w\u03030 = \u03b11, for \u03b1 > 0 being small, and omit technical details by only discussing asymptotic statements for \u03b1 \u2192 0 here. The interested reader can find the formal arguments (including the non-asymptotic dependence on \u03b1) in Section 3.3. Let us first focus on the setting of Theorem 1.3 and apply Theorem 2.3.\nCorollary 2.7 (Polynomial Regularization). Let \u03c1rp(z) = |z| 2 p , for p \u2208 (0, 2), and let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N , y \u2208 RM , and \u03c1link and L satisfying Assumptions (A1) and (A3). For p \u2264 1, assume in addition that (B1) holds.\nDefine w\u03030 = \u03c1rp(w0) = \u03b11, for \u03b1 > 0, and assume that w\u0303\u221e,\u03b1 := limt\u2192\u221e \u03c1rp(w(t)) exists for any \u03b1 sufficiently small. Then,\nlim \u03b1\u21920 gp (w\u0303\u221e,\u03b1) = min z\u0303\u2208RN\u22650,L\u0303(z\u0303)=0 gp (z\u0303) , (21)\nwhere\ngp(z\u0303) := { \u2225z\u0303\u22251 if p \u2264 1 \u2225z\u0303\u2225pp if p > 1.\n(22)\nCorollary 2.7 follows from its non-asymptotic version Corollary 3.3 below. For the sake of conciseness, we provide here only a sketch of the argument.\nProof sketch: To use Theorem 2.3, we have to verify Assumption (A2) for DN = DNrp = RN\u22650. It is easy to see that, for p \u2208 (0, 2), the function \u03c1rp(z) is continuously differentiable and invertible on D = [0,\u221e), and that the derivative \u03c1\u2032rp(z) = 2pz 2 p\u22121 vanishes on \u2202D = {0}. Applying Theorem 2.3 (and arguing why at least one of the additional conditions in Theorem 2.3 (ii) is satisfied) yields now that\nw\u0303\u221e,\u03b1 \u2208 argmin z\u0303\u2208DNrp,L\u0303(z\u0303)=0 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9, (23)\nwhere H and h have been defined in Theorem 2.3. By case distinction, one can show that\n\u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 =\n{ 1 4 ( \u27e8z\u0303, log(z\u0303)\u27e9 \u2212 \u27e8z\u0303,1\u27e9 \u2212 \u27e8z\u0303, log(w\u03030)\u27e9 ) if p = 1,\np 4(p\u22121) ( \u27e81, z\u0303\u2299p\u27e9 \u2212 p\u27e81, z\u0303\u2299 w\u0303\u2299(p\u22121)0 \u27e9 ) if p \u0338= 1.\n(24)\nLet us now informally take the limit \u03b1 \u2192 0 for w\u03030 = \u03b11. For p = 1, the third term dominates and (23) becomes equivalent to minimizing \u27e81, z\u0303\u27e9 which is \u2225z\u0303\u22251 on DNrp = RN\u22650. For 0 < p < 1, the factor p p\u22121 is negative and the term p\u27e81, z\u0303\u27e9 dominates, while for p > 1, the factor p p\u22121 is positive and the term \u27e81, z\u0303\u2299p\u27e9 dominates. Hence, (23) becomes equivalent to\nargmin z\u0303\u2208DNrp, L\u0303(z\u0303)=0\n{ \u27e81, z\u0303\u27e9 if 0 < p < 1 \u27e81, z\u0303p\u27e9 if p > 1\n= argmin z\u0303\u2208RN\u22650, L\u0303(z\u0303)=0 { \u2225z\u0303\u22251 if 0 < p < 1 \u2225z\u0303\u2225pp if p > 1,\nwhich is the claim.\nIf we choose p = 2L for L \u2265 2 in Corollary 2.7, which means setting \u03c1rp(z) = z \u2299L, we recover the results in [11]. As previously mentioned, we focus on the case w\u03030 \u2192 0 only for simplicity. Other choices of initialization can be considered as well in Theorem 2.3 and result in completely different regularization. For example, if we choose w\u03030 to be large, then for L > 2 (and p = 2 L ) the term \u2212\u27e81, z\u0303\u2299 2L \u27e9 in (24) dominates, and (23) becomes equivalent to maximizing the \u2113 2 L -norm. This has been previously observed in [26].\nTo obtain Theorem 1.3 in full, we still need to apply Theorem 2.6 to \u03c1rp(z) = |z| 2 p .\nCorollary 2.8 (Polynomial Regularization \u2014 Convergence). Let \u03c1rp(z) = |z| 2 p , for p \u2208 (0, 2), and let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N and y \u2208 RM satisfying Assumption (B2), and \u03c1link and L satisfying Assumptions (B1) and (A3) of Theorem 2.6.\nThen, the following statements hold:\n(i) For any w\u03030 > 0, the trajectory w\u0303(t) = \u03c1rp(w(t)) converges to a limit w\u0303\u221e \u2208 RN\u22650 with L\u0303(w\u0303\u221e) = 0 with the rate characterized in Theorem 2.6 (i).\n(ii) If p \u2208 (0, 1] and L(\u00b7,y) satisfies the Polyak-Lojasiewicz inequality with \u00b5 > 0, there exists a constant c > 0 (only depending on p, \u00b5, w\u03030, and A) such that\nL\u0303(w\u0303(t)) \u2264 L\u0303(w\u03030)e\u2212ct.\nProof. To apply Theorem 2.6, we only need to verify Assumption (A2) for DN = DNrp = RN\u22650. This has already been done in the proof sketch of Corollary 2.7. Claim (i) now follows from Theorem 2.6 (i) and (iii) if the function z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) lies in the function class FDrp , where\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 = p2\n4 z\u0303p\u22122\nand FDrp was defined in (20). It is easy to verify that by continuity z\u0303 7\u2192 z\u0303 \u00b7h\u2032(z\u0303) = p2 4 z\u0303 p\u22121 \u2208 Lc1(Drp). Moreover, for p \u2208 (0, 2) and any z\u0303 \u2208 (0,\u221e),\u222b \u221e z\u0303 p2 4 \u03c4p\u22121d\u03c4 = \u221e\nsuch that z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) \u2208 FDrp . To see Claim (ii), note that if we assume in addition that L(\u00b7,y) satisfies the Polyak-Lojasiewicz inequality with \u00b5 > 0, all that remains is to show that h\u2032(w\u0303(t))\u2299\u22121 = c\u22121p w\u0303(t) \u2299(2\u2212p) \u2265 r, for some r > 0 depending only on p and w\u03030, and all t \u2265 0. Theorem 2.6 (ii) then yields the claimed linear convergence. For p \u2264 1, this lower bound on w\u0303(t) can be deduced from Lemma 3.2 by refining the argument for w\u0303\u221e \u2208 RN>0 appearing in the proof of Corollary 3.3. The resulting r > 0 will depend on p and w\u03030. We leave this refinement to the interested reader.\nAfter having derived Corollaries 2.7 and 2.8, which induce Theorem 1.3, let us now turn to the analogous results for trigonometric reparametrizations as considered in Theorem 1.5. In contrast to Corollary 2.7, we set the initialization magnitude \u03b1 directly to zero in order to simplify the statement. In the polynomial case this was not possible since the origin is a saddle-point of the respective loss function.\nCorollary 2.9 (Trigonometric Regularization). Let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N , y \u2208 RM , and \u03c1link and L satisfying Assumptions (A1) and (A3) of Theorem 2.3. Then, the following statements hold:\n(i) Let \u03c1rp(z) = sinh(z) and w\u03030 = \u03c1rp(w0) = 0. If w\u0303\u221e := limt\u2192\u221e \u03c1rp(w(t)) exists, then\ngsinh (w\u0303\u221e) = min z\u0303\u2208RN ,L\u0303(z\u0303) gsinh (z\u0303) , (25)\nwhere\ngsinh(z\u0303) = \u27e8z\u0303, arctan(z\u0303)\u27e9 \u2212 \u27e81, 12 log(1+ z\u0303 \u22992)\u27e9. (26)\n(ii) Let \u03c1rp(z) = tanh(z) and w\u03030 = \u03c1rp(w0) = 0. If \u03c1link = Id and w\u0303\u221e := limt\u2192\u221e \u03c1rp(w(t)) exists, then\ngtanh (w\u0303\u221e) = min z\u0303\u2208[\u22121,1]N ,L\u0303(z\u0303) gtanh (z\u0303) , (27)\nwhere\ngtanh(z\u0303) = \u27e8z\u0303, artanh(z\u0303)\u27e9. (28)\nCorollary 2.9 is a special case of Corollary 3.4 below. We only provide here a sketch of the argument.\nProof sketch: The proof idea of Corollary 2.9 is very similar to the proof sketch of Corollary 2.7. The main difference is that we can pick \u03b1 = 0 and have\n\u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 = gsinh(z\u0303)\u2212 arctan(\u03b1)\u27e81, z\u0303\u27e9,\nfor \u03c1rp(z) = sinh(z) with DN = DNrp = RN , and\n\u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 = 1\n2\n( gtanh(z\u0303)\u2212 (artanh(\u03b1) + \u03b11\u2212\u03b12 )\u27e81, z\u0303\u27e9 ) ,\nfor \u03c1rp(z) = tanh(z) with DN = RN and DNrp = [\u22121, 1]N , where H and h have been defined in Theorem 2.3. The claim follows by inserting \u03b1 = 0 into the above equations.\nJust as in the polynomial case, we can use Theorem 2.6 to analyze the convergence behaviour of w\u0303(t) for \u03c1rp(z) = sinh(z) and \u03c1rp(z) = tanh(z).\nCorollary 2.10 (Trigonometric Regularization \u2014 Convergence). Let \u03c1rp(z) = sinh(z) or \u03c1rp(z) = tanh(z). Let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N and y \u2208 RM satisfying Assumption (B2), and \u03c1link and L satisfying Assumptions (B1) and (A3) of Theorem 2.6. Then, the following statements hold:\n(i) For any w\u03030 > 0, the trajectory w\u0303(t) = \u03c1rp(w(t)) converges to a limit w\u0303\u221e \u2208 DNrp with L\u0303(w\u0303\u221e) = 0 with the rate characterized in Theorem 2.6 (i). Here, DNrp = RN for \u03c1rp = sinh and DNrp = [\u22121, 1]N for \u03c1rp = tanh.\n(ii) If L(\u00b7,y) satisfies the Polyak-Lojasiewicz inequality with \u00b5 > 0, there exists a constant c > 0 (only depending on \u00b5, w\u03030, and A) such that\nL\u0303(w\u0303(t)) \u2264 L\u0303(w\u03030)e\u2212ct.\nProof. Since DN = RN in both cases, it is easy to check that (A2) holds. Recall the definition of h from Theorem 2.3. Let us split the cases:\nCase \u03c1rp(z) = sinh(z): If \u03c1rp(z) = sinh(z), one can compute that\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 = (1 + z\u03032)\u22121,\nfor z\u0303 = \u03c1rp(z). Claim (i) now follows from Theorem 2.6 (i) and (iii) if the function z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) lies in the function class FDrp , where FDrp was defined in (20). By continuity z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) \u2208 Lc1(RN ). Moreover, for any z\u0303 \u2208 R, \u2223\u2223\u2223 \u222b \u00b1\u221e\nz\u0303\n\u03c4 1 + \u03c42 d\u03c4 \u2223\u2223\u2223 = \u221e\nsuch that z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) \u2208 FDrp . To see Claim (ii), note that h\u2032(w\u0303(t))\u2299\u22121 = 1 + w\u0303(t)\u22992 \u2265 1. If we assume in addition that L(\u00b7,y) satisfies the Polyak-Lojasiewicz inequality with \u00b5 > 0 we thus get\nthe claim from Theorem 2.6 (ii).\nCase \u03c1rp(z) = tanh(z): If \u03c1rp(z) = tanh(z), one can compute that\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 = (1\u2212 z\u03032)\u22122,\nfor z\u0303 = \u03c1rp(z). Claim (i) follows from Theorem 2.6 (i) and (iii) since DNrp = [\u22121, 1]N is bounded. Claim (ii) again holds if h\u2032(w\u0303(t))\u2299\u22121 = (1 \u2212 w\u0303(t)\u22992)\u22992 \u2265 r, for some r > 0 which in turn holds as long as 1 \u2212 |w\u0303(t)| > r\u2032, for some r\u2032 > 0. By refining the argument we used to show that w\u0303\u221e \u2208 int(DNrp) in the proof of Corollary 3.4, see paragraph below (45), the latter can be deduced via Lemma 3.2. The resulting r > 0 will depend on w\u03030."
        },
        {
            "heading": "3 Proofs",
            "text": "In this section, we provide the proofs of our main results."
        },
        {
            "heading": "3.1 Proof of Theorem 2.3",
            "text": "The proof of Theorem 2.3 is based on the following technical observation. Recall that the derivatives of entry-wise acting functions are defined entry-wise.\nLemma 3.1. Let A \u2208 RM\u00d7N , \u03c5 \u2208 RM , and a closed, convex interval Drp \u2282 R be given. Suppose that w\u0303 : [0,\u221e) \u2192 RN is continuously differentiable and that there exists a continuous g : DNrp \u2192 RN and a continuously differentiable h : int(Drp) \u2192 R (acting entry-wise on vectors) such that\nw\u0303\u2032(t) = h\u2032(w\u0303(t))\u2299\u22121 \u2299 g(w\u0303(t)), (29)\nAssume furthermore that\n1. w\u0303(t) \u2208 int(DNrp), for all t \u2265 0,\n2. the limit w\u0303\u221e := limt\u2192\u221e w\u0303(t) exists,\n3. ATAw\u0303\u221e = A T\u03c5,\n4. h\u2032(z\u0303) > 0 for all z\u0303 \u2208 int(Drp),\n5. g (z\u0303) \u2208 ker(A)\u22a5 for z\u0303 \u2208 int(DNrp).\nLet H be defined by H \u2032 = h on int(Drp) and extend both h and H to Drp by continuity to obtain functions h : Drp \u2192 R\u222a {\u00b1\u221e} and H : Drp \u2192 R\u222a {+\u221e}.5 If any of the following two conditions is satisfied\n(i) |h| < \u221e and H < \u221e\n(ii) w\u0303\u221e \u2208 int(DNrp),\nthen the limit of w\u0303(t) satisfies the equation\nw\u0303\u221e \u2208 argmin z\u0303\u2208DNrp :ATAz\u0303=AT\u03c5 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9. (30)\n5By Assumption 4, H is convex and can thus only diverge to +\u221e on the boundary of Drp.\nProof. The proof is based on validating the KKT-conditions of (30) for the limit w\u0303\u221e. We only need to consider the case that h and H are finite valued. Indeed, if h or H attain \u221e on the boundary of Drp, we will have by assumption that w\u0303\u221e \u2208 int(DNrp) such that we can replace Drp by a shrinked and closed domain D\u25e6 \u228a Drp on which Assumptions 1-5 are still valid, both h and H are finite valued, and for any z\u0303\u25e6 \u2208 DN\u25e6 and z\u0303 \u2208 DNrp \\ DN\u25e6\n\u27e81, H(z\u0303\u25e6)\u2212 z\u0303\u25e6 \u2299 h(w\u03030)\u27e9 \u2264 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9.\nProving the claim on DN\u25e6 thus automatically extends to DNrp. So assume w.l.o.g. that |h| < \u221e and H < \u221e on Drp. By Assumptions 1 and 4, we can write g(w\u0303(t)) = w\u0303\u2032(t)\u2299 h\u2032(w\u0303(t)) such that for any t \u2265 0\u222b t 0 g(w\u0303(t))dt = \u222b t 0 w\u0303\u2032(t)\u2299 h\u2032(w\u0303(t))dt = h(w\u0303 (t))\u2212 h(w\u03030). (31)\nUsing Assumptions 1 and 5 we obtain for any t \u2265 0 that\nh(w\u0303 (t))\u2212 h(w\u03030) \u2208 ker(A)\u22a5.\nUsing Assumption 2 together with continuity of h, we can take the limit t \u2192 \u221e to obtain that\nh(w\u0303\u221e)\u2212 h(w\u03030) \u2208 ker(A)\u22a5.\nTogether with Assumptions 1 and 3, we have that w\u0303\u221e satisfies the following conditions\nw\u0303\u221e \u2208 DNrp, (32) 0 = ATAw\u0303\u221e \u2212AT\u03c5, (33)\nh(w\u0303\u221e)\u2212 h(w\u03030) \u2208 ker(A)\u22a5. (34)\nWe now examine the expression (30) and derive its KKT conditions. Since DNrp is a convex polytope, there exist B \u2208 RJ\u00d7N , d \u2208 RJ , for J \u2208 N, such that DNrp = {z\u0303 \u2208 RN : Bz\u0303 \u2212 d \u2264 0}. Consequently, the Lagrangian of (30) is given by\nL(z\u0303,\u03bb,\u00b5) = \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9+ \u27e8\u03bb,ATAz\u0303\u2212AT\u03c5\u27e9+ \u27e8\u00b5,Bz\u0303\u2212 d\u27e9,\nwhere \u03bb \u2208 RN and \u00b5 \u2208 RJ . The KKT conditions are given by\n0 = h(z\u0303)\u2212 h(w\u03030) +ATA\u03bb+BT\u00b5 (stationarity) 0 = ATAz\u0303\u2212AT\u03c5 (primal feasibility) 0 \u2265 Bz\u0303\u2212 d 0 \u2264 \u00b5 (dual feasibility) 0 = \u27e8\u00b5,Bz\u0303\u2212 d\u27e9 (complementary slackness).\nSince the constraints of (30) are affine functions, the KKT-conditions are necessarily fulfilled by solutions of the problem. Furthermore, since the objective in (30) is convex (due to Assumption 4), any (z\u0303,\u03bb,\u00b5) satisfying the KKT-conditions provides a solution z\u0303 of (30), see, e.g., [47, Theorem 28.3]. Hence, all that remains is to show that there exist \u03bb and \u00b5 such that (w\u0303\u221e,\u03bb,\u00b5) satisfies the KKT-conditions.\nBy (32) and (33), we easily see that w\u0303\u221e obeys the primal feasibility. Moreover, by choosing \u00b5 = 0, we automatically satisfy dual feasibility and complementary slackness. Now note that, in this case, stationarity reduces to\n0 = h(w\u0303\u221e)\u2212 h(w\u03030) +ATA\u03bb.\nSince h(w\u0303\u221e)\u2212h(w\u03030) \u2208 ker (A)\u22a5 = range(AT ) by (34) we can find a \u03bb such that the above equation holds. Just note that AT is invertible if restricted to range(A).\nWith Lemma 3.1 at hand, we can now prove Theorem 2.3.\nProof of Theorem 2.3. By definition of w in (7) and (8), the derivative of w is w\u2032(t) = \u2212\u2207L(w) = \u2212 [ AT\u2207vL ( \u03c1link(v),y )\u2223\u2223 v=A\u03c1rp(w) ] \u2299 \u03c1\u2032rp(w),\nwhere we used the chain rule in the second equality. Consequently,\nw\u0303\u2032(t) = \u03c1\u2032rp(w)\u2299w\u2032(t) = \u03c1\u2032rp(w)\u2299 [\u2212\u2207L(w)]\n= [\u03c1\u2032rp(w) \u22992]\u2299 [ \u2212AT \u00b7 \u2207vL ( \u03c1link(v),y )\u2223\u2223 v=Aw\u0303 ] .\n(35)\nProof of (i): Since the right-hand side of (35) is a continuous function of w\u0303 on RN by Assumptions (A1)-(A3), it is locally Lipschitz continuous at any point. Hence, by Picard-Lindelo\u0308f there exists for any t\u22c6 > 0 some \u03b5 > 0 such that the trajectory w\u0303(t) is unique on (t\u22c6 \u2212 \u03b5, t\u22c6 + \u03b5). Recall that w\u03030 \u2208 int(DNrp) and assume that there exists t\u22c6 > 0 such that w\u0303(t) \u2208 int(DNrp), for t < t\u22c6, and w\u0303(t\u22c6) \u2208 \u2202DNrp, which implies that w\u0303i(t) \u2208 int(Drp), for t < t\u22c6, and w\u0303i(t\u22c6) \u2208 \u2202Drp, for some i \u2208 [N ]. By Assumption (A2), we have \u03c1\u2032rp(w\u0303i(t\u22c6)) = 0 such that w\u0303 \u2032 i(t\u22c6) = 0. This clearly contradicts the local uniqueness of the trajectory w\u0303 since on (t\u22c6 \u2212 \u03b5, t\u22c6) both the original trajectory \u03b31(t) = w\u0303i(t) and the constant trajectory \u03b32(t) = w\u0303i(t\u22c6) are solutions for (35). Hence, (i) holds.\nFrom (i) we know that the trajectories w(t) and w\u0303(t) stay in int(DN ) and int(DNrp), respectively, such that we can write\nw\u0303\u2032(t) = [\u03c1\u2032rp(w) \u22992]\ufe38 \ufe37\ufe37 \ufe38\n=h\u2032(w\u0303)\u2299\u22121\n\u2299 [ \u2212AT \u00b7 \u2207vL ( \u03c1link(v),y )\u2223\u2223 v=Aw\u0303 ] \ufe38 \ufe37\ufe37 \ufe38\n=:g(w\u0303)\n, (36)\nwhere g : RN \u2192 RN and h\u2032 : int(Drp) \u2192 R has been defined in (17). Just note that for z\u0303 = \u03c1rp(z) with z \u2208 D\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(\u03c1rp(z)) )2 = \u03c1\u2032rp(z) \u22122\nby the inverse function rule.\nProof of (ii): Recall that we assume now in addition that w\u221e := limt\u2192\u221e w(t) with L(w\u221e) = 0 exists and that w\u0303\u221e := \u03c1rp(w\u221e) \u2208 int(DNrp) or the functions h and H are finite valued. Set \u03c5 = \u03c1\u22121link(y), which is well-defined by Assumption (A1). We wish to apply Lemma 3.1 to w\u0303 = \u03c1rp(w). First, note that by (36) w\u0303 is of the form (29) since g is continuous by Assumptions (A1) and (A3), and h is continuously differentiable on int(DNrp) by Assumption (A2). In particular, this means that\nw\u0303 is continuously differentiable. Let us now check Assumptions 1-5 of Lemma 3.1. Using (i) and the additional assumptions we make in (ii), it is straightforward to see that w\u0303 satisfies Assumptions 1 and 2 of Lemma 3.1. To validate Assumption 3 as well, recall Assumptions (A1) and (A3) which yield that L(w\u221e) = 0 if and only if \u03c1link(Aw\u0303\u221e) = y which in turn is equivalent to Aw\u0303\u221e = \u03c5 by injectivity of \u03c1link. By the shape of (17), the function h\n\u2032 is nonnegative and by Assumption (A2) it is strictly positive on int(DNrp) which yields Assumption 4 of Lemma 3.1. Finally, Assumption 5 holds since g(w\u0303) \u2208 range(AT ) = ker(A)\u22a5.\nApplying Lemma 3.1 and using that {z\u0303 \u2208 RN : Az\u0303 = \u03c5} \u2282 {z\u0303 \u2208 RN : ATAz\u0303 = AT\u03c5}, we obtain\nw\u0303\u221e \u2208 argmin z\u0303\u2208DNrp,Az\u0303=\u03c5 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9\n= argmin z\u0303\u2208DNrp, L\u0303(z\u0303)=0\n\u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9,\nwhere we recalled from above that L\u0303(z\u0303) = 0 if and only if Az\u0303 = \u03c5. This yields (18). To conclude, we only need to subtract from the right-hand side the quantity (\u27e81, H(w\u03030)\u27e9 + \u27e8w\u03030, h(w\u03030)\u27e9), which is independent of z\u0303. For F (z\u0303) = \u27e81, H(z\u0303)\u27e9, we then have\nargmin z\u0303\u2208DNrp, L\u0303(z\u0303)=0\n\u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9\n= argmin z\u0303\u2208DNrp, L\u0303(z\u0303)=0\n\u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 \u2212 \u27e81, H(w\u03030)\u27e9+ \u27e8w\u03030, h(w\u03030)\u27e9\n= argmin z\u0303\u2208DNrp, L\u0303(z\u0303)=0\n\u27e81, H(z\u0303)\u2212H(w\u03030)\u27e9 \u2212 \u27e8h(w\u03030), z\u0303\u2212 w\u03030\u27e9\n= argmin z\u0303\u2208DNrp, L\u0303(z\u0303)=0 DF (z\u0303, w\u03030).\n(37)"
        },
        {
            "heading": "3.2 Proof of Theorem 2.6",
            "text": "Let us begin by collecting some useful observations: First, note that due to Assumptions (B1), (A2), and (A3) we know from Theorem 2.3 (i) that w\u0303(t) \u2208 int(DNrp), for all t \u2265 0. Second, the initial considerations in the proof of Theorem 2.3 are valid for \u03c1link = Id such that (36) yields\nw\u0303\u2032(t) = [\u03c1\u2032rp(w) \u22992]\u2299 [ \u2212AT \u00b7 \u2207vL ( v,y )\u2223\u2223 v=Aw\u0303 ] =: h\u2032(w\u0303)\u2299\u22121 \u2299 g(w\u0303). (38)\nThird, by Assumption (B2), there exists z\u03030 \u2208 int(DNrp) with Az\u03030 = y such that Assumption (A3) implies L\u0303(z\u03030) = 0. Finally, let DF be the Bregman divergence associated with F (z) = \u27e81, H(z)\u27e9, where H is an antiderivative of h (recall the remark in Footnote 3 at this point). Then, the function t 7\u2192 DF (z\u03030, w\u0303(t)) is decreasing and DF (z\u03030, w\u0303(t)) converges for t \u2192 \u221e. Since this observation will be of further use to us below, we formalize it as a lemma.\nLemma 3.2. Under assumptions (B1), (A2), and (A3), let F (z) = \u27e81, H(z)\u27e9, where H is an antiderivative of h as defined in Theorem 2.3. Then, for any z\u03030 \u2208 int(DNrp)\n\u2202tDF (z\u03030, w\u0303(t)) \u2264 \u2212L\u0303(w\u0303(t)).\nIn particular, this implies that DF (z\u03030, w\u0303(t)) \u2265 0 converges for t \u2192 \u221e.\nProof. Applying the chain rule, we compute the time derivative of t 7\u2192 DF (z\u03030, w\u0303(t)) as\n\u2202tDF (z\u03030, w\u0303(t)) = \u2212\u27e8\u2207F (w\u0303(t)), w\u0303\u2032(t)\u27e9 \u2212 \u27e8\u2202t\u2207F (w\u0303(t)), z\u03030 \u2212 w\u0303(t)\u27e9+ \u27e8\u2207F (w\u0303(t)), w\u0303\u2032(t)\u27e9 = \u2212\u27e8h\u2032(w\u0303(t))\u2299 w\u0303\u2032(t), z\u03030 \u2212 w\u0303(t)\u27e9 = \u2329 AT \u00b7 \u2207vL ( v,y )\u2223\u2223 v=Aw\u0303(t) , z\u03030 \u2212 w\u0303(t) \u232a\n= \u2329 \u2207vL ( v,y )\u2223\u2223 v=Aw\u0303(t) ,Az\u03030 \u2212Aw\u0303(t) \u232a\n\u2264 L ( Az\u03030,y ) \u2212 L ( Aw\u0303(t),y ) = \u2212L\u0303(w\u0303(t)),\n(39)\nwhere we used in the second line (38), in the fourth line the convexity of L, and in the last line that L\u0303(z\u03030) = 0. Since z\u03030, w\u03030 \u2208 int(DNrp), we know that DF (z\u03030, w\u03030) < \u221e due to H being the antiderivative of a continuously differentiable function defined on int(DNrp). This and (39) immediately imply that DF (z\u03030, w\u0303(t)) converges for t \u2192 \u221e since \u2202tDF (z\u03030, w\u0303(t)) \u2264 0 by Assumption (A3) and DF (z\u03030, w\u0303(t)) \u2265 0 by strict convexity of F . The latter follows from \u22072F (z) = diag(h\u2032(z)) and the fact that h\u2032 is strictly positive by (17) and Assumption (A2).\nWe can now proceed with proving Claims (i)-(iii) of Theorem 2.6.\nProof of (i): We use Lemma 3.2, which is a generalized version of [11, Lemma 8], to show that (i) holds. Because DF is non-negative and t 7\u2192 DF (z\u03030, w\u0303(t)) is continuously differentiable with DF (z\u03030, w\u03030) < \u221e (as argued in the proof of Lemma 3.2), we get for any T \u2265 0 that\nDF (z\u03030, w\u03030) \u2265 DF (z\u03030, w\u0303(0))\u2212DF (z\u03030, w\u0303(T )) = \u2212 \u222b T 0 \u2202tDF (z\u03030, w\u0303(t))dt\n\u2265 \u222b T 0 L\u0303(w\u0303(t))dt \u2265 T L\u0303(w\u0303(T )),\nwhere the last inequality comes from the fact that t 7\u2192 L\u0303(w\u0303(t)) is non-increasing by the definition of w in (7). Rearranging the inequality yields the claim. We furthermore see that limt\u2192\u221e L\u0303(w\u0303(t)) = 0 since DF (z\u03030, w\u0303(0)) < \u221e. Consequently, Assumption (A3) yields that limt\u2192\u221e Aw\u0303(t) = y.\nProof of (ii): The just observed sublinear convergence rate can be improved if we assume in addition that (h\u2032)\u2299\u22121(w\u0303(t)) \u2265 r, for some r > 0 and all t \u2265 0, and that L(\u00b7,y) satisfies the Polyak-Lojasiewicz inequality with \u00b5 > 0. In this case, Equation (38) yields\n\u2202tL\u0303(w\u0303(t)) = \u2202tL(Aw\u0303(t),y) = \u2329 \u2207w\u0303L ( Aw\u0303(t),y ) , w\u0303\u2032(t) \u232a = \u2212 \u2329 \u2207w\u0303L ( Aw\u0303(t),y ) , h\u2032(w\u0303(t))\u2299\u22121 \u2299\u2207w\u0303L ( Aw\u0303(t),y\n)\u232a \u2264 \u2212r \u2225\u2225\u2225\u2207w\u0303L(Aw\u0303(t),y)\u2225\u2225\u22252 2\n\u2264 \u22122\u00b5\u03c3min(A)2r ( L(Aw\u0303(t),y)\u2212 L(Az\u03030,y) ) = \u22122\u00b5\u03c3min(A)2rL\u0303(w\u0303(t)),\nfor any t \u2265 0, where the penultimate step follows from the Polyak-Lojasiewicz inequality together with Remark 2.5 (ii), and the last equality uses that Az\u03030 = y. Applying Gro\u0308nwall\u2019s inequality, we obtain that\nL\u0303(w\u0303(t)) \u2264 L\u0303(w\u03030)e\u22122r\u00b5\u03c3min(A) 2t\nand thus the linear convergence. Proof of (iii): We first show that if supt\u22650 \u2225w\u0303(t)\u22252 < \u221e, then w\u0303(t) \u2192 w\u0303\u221e with L\u0303(w\u0303\u221e) = 0. The additional claim that w\u0303\u221e \u2208 DNrp then automatically follows from Theorem 2.3 (i). The argument is along the lines of [11] and is repeated here in a simplified form for the reader\u2019s convenience. Let us assume w\u0303 is bounded, i.e., lim supt\u2192\u221e \u2225w\u0303(t)\u22252 < \u221e. By Assumption (B2), there exists a sufficiently large compact ball B around the origin such that B \u2229 {z\u0303 \u2208 RN : Az\u0303 = y} =\u0338 \u2205 and w\u0303(t) \u2208 B, for all t \u2265 0. Since w\u0303(t) \u2282 B, there exists a sequence of times t0 < t1 < . . . such that w\u0303(ti) \u2192 z\u0303, for some z\u0303 \u2208 B. By continuity of DF , we thus get that limi\u2192\u221e DF (z\u0303, w\u0303(ti)) = 0. From (i) we furthermore know that limt\u2192\u221e Aw\u0303(t) = y which implies that Az\u0303 = y. Replacing z\u03030 with z\u0303 in Lemma 3.2 yields that the function t 7\u2192 DF (z\u0303, w\u0303(t)) is non-increasing. Since DF (z\u0303, w\u0303(t)) \u2265 0, we obtain that limt\u2192\u221e DF (z\u0303, w\u0303(t)) = 0. Let now t0 < t1 < . . . be any sequence of times such that w\u0303(tk) converges to a limit x\u0303. Continuity of DF implies that DF (z\u0303, x\u0303) = 0 which yields by strict convexity of F that x\u0303 = z\u0303. Since z\u0303 is the only accumulation point of w\u0303(t), we have w\u0303\u221e = limt\u2192\u221e w\u0303(t) = z\u0303 and L\u0303(w\u0303\u221e) = 0. Let us conclude by showing the following: If Drp is unbounded, i.e., Dlimrp = {inf Drp, supDrp} \u2229 {\u2212\u221e,\u221e} =\u0338 \u2205, and if the univariate function z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) is in the function class\nFDrp = { f \u2208 Lc1(Drp) : \u2223\u2223\u2223 \u222b b z f(x)dx \u2223\u2223\u2223 = \u221e, for any z \u2208 int(Drp) and b \u2208 Dlimrp },\nwhere Lc1(Drp) = {f \u2208 L1(C) for any compact C \u2282 Drp}, then sup t\u22650 \u2225w\u0303(t)\u22252 < \u221e such that the first part of (iii) implies convergence of the trajectory w\u0303(t). To this end, we will use a z\u03030 \u2208 int(DNrp) with Az\u03030 = y, which exists according to Assumption (B2), and argue by contradiction that if [z\u0303 7\u2192 z\u0303\u00b7h\u2032(z\u0303)] \u2208 FDrp , the unboundedness of \u2225w\u0303(t)\u22252 would imply the unboundedness ofDF (z\u03030, w\u0303(t)) which contradicts the fact that 0 \u2264 DF (z\u03030, w\u03030) < \u221e and that t 7\u2192 DF (z\u03030, w\u0303(t)) is non-increasing according to Lemma 3.2. First note that, due to F (z\u03030) = \u27e81, H(z\u03030)\u27e9, we have DF (z\u03030, w\u0303(t)) = DH((z\u03030)1, w\u03031(t)) + \u00b7 \u00b7 \u00b7 + DH((z\u03030)N , w\u0303N (t)). Since DH is lower bounded by 0 (recall the convexity of H), the unboundedness of DF (z\u03030, w\u0303(t)) is equivalent to at least one summand being unbounded. Let us assume that \u2225w\u0303(t)\u22252 is unbounded, i.e., there exists i \u2208 [N ] and a sequence of times t1, t2, . . . such that w\u0303i(tk) \u2192 \u221e or w\u0303i(tk) \u2192 \u2212\u221e. W.l.o.g. we assume that \u221e \u2208 Dlimrp and consider the first case. Now, we have that\nDH((z\u03030)i, w\u0303i(tk)) = DH((z\u03030)i, w\u0303i(t0)) + \u222b w\u0303i(tk) w\u0303i(t0) ( d d\u03be DH((z\u03030)i, \u03be) ) d\u03be\n= DH((z\u03030)i, w\u0303i(t0)) + \u222b w\u0303i(tk) w\u0303i(t0) h\u2032(\u03be)(\u03be \u2212 (z\u03030)i)d\u03be. (40)\nSince h \u2032(\u03be)(\u03be\u2212(z\u03030)i)\nh\u2032(\u03be)\u03be \u2192 1, for \u03be \u2192 \u221e, and the terms h \u2032(\u03be)(\u03be \u2212 (z\u03030)i) and h\u2032(\u03be)\u03be are both positive for\nsufficiently large \u03be, there exists \u03be0 such that 1 2h \u2032(\u03be)\u03be \u2264 h\u2032(\u03be)(\u03be\u2212(z\u03030)i) \u2264 2h\u2032(\u03be)\u03be, for \u03be \u2265 \u03be0. W.l.o.g. we assume that \u03be0 > w\u0303i(t0) such that \u222b \u03be0 w\u0303i(t0)\nh\u2032(\u03be)(\u03be \u2212 (z\u03030)i)d\u03be is finite by (40). Since z\u0303 7\u2192 z\u0303 \u00b7 h\u2032(z\u0303) is in FDrp , we know that \u222b\u221e \u03be0 h\u2032(\u03be)\u03bed\u03be is equal to +\u221e or \u2212\u221e. In the first case,\n\u222b w\u0303i(tk) w\u0303i(t0) h\u2032(\u03be)(\u03be \u2212 (z\u03030)i)d\u03be \u2265 \u222b \u03be0 w\u0303i(t0) h\u2032(\u03be)(\u03be \u2212 (z\u03030)i)d\u03be + 1 2 \u222b w\u0303i(tk) \u03be0 h\u2032(\u03be)\u03bed\u03be \u2192 \u221e,\nwhich in turn implies that DH((z\u03030)i, w\u0303i(tk)) \u2192 \u221e, for k \u2192 \u221e, and hence yields the claim. In the second case, a similar argument leads to DH((z\u03030)i, w\u0303i(tk)) \u2192 \u2212\u221e contradicting the fact that DH((z\u03030)i, w\u0303i(tk)) \u2265 0."
        },
        {
            "heading": "3.3 Non-asymptotic variants of Corollaries 2.7 and 2.9",
            "text": "To show Corollaries 2.7 and 2.9, we provide and proof in this section the more general non-asymptotic variants Corollaries 3.3 and 3.4 that apply to small positive resp. non-negative initializations. In contrast to [11], we do not derive optimally scaling dependencies between regularization error \u03b5 and initialization scale \u03b1. The necessary additional technical work would not help to understand how Theorems 2.3 and 2.6 are applied. We invite the interested reader to compare the statement in Corollary 3.3 to [11, Theorem 4].\nCorollary 3.3 (Polynomial Regularization \u2014 Non-asymptotic version). Let \u03c1rp(z) = |z| 2 p , for p \u2208 (0, 2) and let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N , y \u2208 RM , and \u03c1link and L satisfying, for p \u2264 1, \u03c1link = Id and Assumption (A3) of Theorem 2.3 and, for p > 1, Assumptions (A1) and (A3) of Theorem 2.3. Define w\u03030 = \u03c1rp(w0) = \u03b11, for \u03b1 > 0, and\nQp := min z\u0303\u2208RN\u22650,L\u0303(z\u0303)=0 gp(z\u0303),\nwhere gp has been introduced in Corollary 2.7. Then, the following statement holds for any \u03b5 > 0: If w\u0303\u221e = limt\u2192\u221e w\u0303(t) exists and\n\u03b1 \u2264  min ( 1 e , exp ( \u2212Q 2+N/e+\u03b5 \u03b5 )) if p = 1 min ( p 1 1\u2212p , ( p\u03b5 Q+N+\u03b5 ) 1 1\u2212p ) if p < 1 min ( p 1 1\u2212p , ( \u03b5\np(Q+N+\u03b5)\n) 1 p\u22121 )\nif p > 1\n,\nthen\ngp(w\u0303\u221e) \u2264 ( min\nz\u0303\u2208RN\u22650,L\u0303(z\u0303)=0 gp(z\u0303)\n) + \u03b5,\ni.e., w\u0303\u221e minimizes gp up to \u03b5.\nProof. We will apply Theorem 2.3 with DN = DNrp = RN\u22650. In fact, we only need to check Assumption (A2) and to verify that either w\u0303\u221e \u2208 RN>0 or that h and H are finite valued on R\u22650. It is easy to see that, for p \u2208 (0, 2), the function \u03c1rp(z) is continuously differentiable and invertible on D = [0,\u221e), and that the derivative \u03c1\u2032rp(z) = 2 pz 2 p\u22121 vanishes on \u2202D = {0}.\nGiven one of the additional conditions on w\u0303\u221e respectively h andH in Theorem 2.3 (ii) is satisfied, the theorem yields that (18) holds, i.e.,\nw\u0303\u221e \u2208 argmin z\u0303\u2208DNrp,L\u0303(z\u0303)=0 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9, (41)\nwhere h and H have been defined in Theorem 2.3 as antiderivatives of\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 = p2\n4 z\u0303p\u22122.\nLet us calculate the explicit form of (41) by distinguishing the cases p = 1 and p \u0338= 1. Since the functions H and h are acting entry-wise, we can restrict our derivation to single entries, i.e., \u03c1rp(z) = z 2 p , p \u2208 (0, 2), and w0 = \u03b1 > 0.\nCase p = 1: Since h(z\u0303) = 14 log(z\u0303) in this case, we get that H(z\u0303) = 1 4 (z\u0303 log(z\u0303)\u2212 z\u0303) and\nH(z\u0303)\u2212 z\u0303h(w\u03030) = 1\n4 (z\u0303 log(z\u0303)\u2212 z\u0303 \u2212 z\u0303 log(w\u03030)).\nHence, \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 = 14 \u27e8z\u0303, log(z\u0303)\u2212 (1+ log(w\u03030))\u27e9 in this case. Before applying Theorem 2.3, we will argue that w\u0303\u221e \u2208 int(DNrp) = RN>0. Note that, for any z\u0303, u\u0303k \u2208 RN\u22650 with (u\u0303k)i \u2192 0 for some i \u2208 [N ], we have for F (z\u0303) = \u27e81, H(z\u0303)\u27e9 that DF (z\u0303, u\u0303k) \u2192 +\u221e by the shape of H. Since we know at the same time from Lemma 3.2 that under our assumptions DF (z\u0303, w\u0303(t)) converges, we deduce that w\u0303\u221e \u2208 int(DNrp). By (41), we have\n\u27e8w\u0303\u221e, log(w\u0303\u221e)\u2212 (1 + log\u03b1)1\u27e9 \u2264 \u27e8z\u0303, log(z\u0303)\u2212 (1 + log\u03b1)1\u27e9.\nSince \u03b1 < 1e , we have that 1 + log\u03b1 < 0. Rearranging the terms and using the fact that w\u0303\u221e, z\u0303 \u2265 0, we obtain\n\u2225w\u0303\u221e\u22251 \u2212 \u2225z\u0303\u22251 \u2264 \u22121\n1 + log\u03b1 (\u27e8z\u0303, log(z\u0303)\u27e9 \u2212 \u27e8w\u0303\u221e, log(w\u0303\u221e)\u27e9) .\nSince \u03be2 \u2265 \u03be log(\u03be) \u2265 \u22121/e, we can further deduce that\n\u2225w\u0303\u221e\u22251 \u2212 \u2225z\u0303\u22251 \u2264 \u22121\n1 + log\u03b1\n( \u2225z\u0303\u222522 + N\ne\n) \u2264 \u22121\n1 + log\u03b1\n( \u2225z\u0303\u222521 + N\ne\n) .\nBy taking the minimum over all z\u0303 \u2208 DNrp and using our assumption on \u03b1, we arrive at our conclusion.\nCase p \u0338= 1: Similarly, we compute for p \u0338= 1 that h(z\u0303) = p 2 4(p\u22121) z\u0303 p\u22121 and H(z\u0303) = p4(p\u22121) z\u0303 p. Consequently,\nH(z\u0303)\u2212 z\u0303h(w\u03030) = p 4(p\u2212 1) (z\u0303p \u2212 pz\u0303w\u0303p\u221210 ). (42)\nHence, \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 = p4(p\u22121) \u27e81, z\u0303 \u2299p \u2212 pz\u0303\u2299 w\u0303\u2299(p\u22121)0 \u27e9 in this case. Here we have to further divide the case into p > 1 and p < 1. Consider the case p < 1. Again, we will argue that w\u0303\u221e \u2208 int(DNrp) = RN>0 before applying Theorem 2.3. As above, for any z\u0303, u\u0303k \u2208 RN\u22650 with (u\u0303k)i \u2192 0 for some i \u2208 [N ], we have for F (z\u0303) = \u27e81, H(z\u0303)\u27e9 that DF (z\u0303, u\u0303k) \u2192 +\u221e by the shape of H. Since we know at the same time from Lemma 3.2 that under our assumptions DF (z\u0303, w\u0303(t)) converges, we deduce that w\u0303\u221e \u2208 int(DNrp). By (41), we thus have \u27e81, w\u0303\u2299p\u221e \u2212 p\u03b1p\u22121w\u0303\u221e\u27e9 \u2265 \u27e81, z\u0303\u2299p \u2212 p\u03b1p\u22121z\u0303\u27e9. Rearranging the terms we have\n\u2225w\u0303\u221e\u22251 \u2212 \u2225z\u0303\u22251 \u2264 \u03b11\u2212p\np (\u2225w\u0303\u221e\u2225pp \u2212 \u2225z\u0303\u2225pp) \u2264\n\u03b11\u2212p\np (\u2225w\u0303\u221e\u22251 +N)\nwhere the last inequality comes from \u03bep \u2264 \u03be + 1 for \u03be > 0 and p \u2208 (0, 1). Since \u03b11\u2212p < p by assumption, we obtain\n\u2225w\u0303\u221e\u22251 \u2264 p\np\u2212 \u03b11\u2212p \u2225z\u0303\u22251 +\n\u03b11\u2212pN\np\u2212 \u03b11\u2212p\nand hence\n\u2225w\u0303\u221e\u22251 \u2212 \u2225z\u0303\u22251 \u2264 \u03b11\u2212p\np\u2212 \u03b11\u2212p (\u2225z\u0303\u22251 +N).\nNow consider p > 1. Since h and H are finite valued on R\u22650 in this case, we can directly use (41), to get \u27e81, w\u0303\u2299p\u221e \u2212 p\u03b1p\u22121w\u0303\u221e\u27e9 \u2264 \u27e81, z\u0303\u2299p \u2212 p\u03b1p\u22121z\u0303\u27e9. Rearranging the terms we have\n\u2225w\u0303\u221e\u2225pp \u2212 \u2225z\u0303\u2225pp \u2264 p\u03b1p\u22121(\u2225w\u0303\u221e\u22251 \u2212 \u2225z\u0303\u22251) \u2264 p\u03b1p\u22121(\u2225w\u0303\u221e\u2225pp +N)\nwhere the last inequality comes from \u03be \u2264 \u03bep+1 for \u03be > 0 and p > 1. Since p\u03b1p\u22121 < 1 by assumption, we obtain\n\u2225w\u0303\u221e\u2225pp \u2264 1\n1\u2212 p\u03b1p\u22121 \u2225z\u0303\u2225pp +\np\u03b1p\u22121N\n1\u2212 p\u03b1p\u22121\nand hence\n\u2225w\u0303\u221e\u2225pp \u2212 \u2225z\u0303\u2225pp \u2264 p\u03b1p\u22121\n1\u2212 p\u03b1p\u22121 (\u2225z\u0303\u2225pp +N).\nFor both cases we simply take the minimum over all z\u0303 \u2208 DNrp and use our assumption on \u03b1 to conclude.\nLet us now consider the trigonometric case.\nCorollary 3.4 (Trigonometric Regularization \u2014 Non-asymptotic version). Let w(t) be the flow defined in (7) with A \u2208 RM\u00d7N , y \u2208 RM , and \u03c1link and L satisfying Assumptions (A1) and (A3) of Theorem 2.3. Then, the following holds:\n(i) Let \u03c1rp(z) = sinh(z), DN = DNrp = RN , and w\u03030 = \u03c1rp(w0) = \u03b11, for 0 \u2264 \u03b1 < 12 . Recall the function gsinh from Corollary 2.9, define Q := min\nz\u0303\u2208RN ,L\u0303(z\u0303)=0 gsinh(z\u0303), and set \u03b5 > 0. If\n\u03b1 \u2264 \u03b5 2(Q+ 2N + 2\u03b5) ,\nand w\u0303\u221e := limt\u2192\u221e \u03c1rp(w(t)) exists, then\ngsinh(w\u0303\u221e) \u2264 ( min\nz\u0303\u2208RN ,L\u0303(z\u0303)=0 gsinh(z\u0303)\n) + \u03b5,\ni.e., w\u0303\u221e minimizes gsinh up to \u03b5.\n(ii) Let \u03c1link = Id, \u03c1rp(w) = tanh(w), DN = RN , DNrp = [\u22121, 1]N , and w\u03030 = \u03c1rp(w0) = \u03b11, for 0 \u2264 \u03b1 < 12 . Recall the function gtanh from Corollary 2.9 and set \u03b5 > 0. If\n\u03b1 \u2264 \u03b5 8N ,\nand w\u0303\u221e := limt\u2192\u221e \u03c1rp(w(t)) exists, then\ngtanh(w\u0303\u221e) \u2264 ( min\nz\u2208[\u22121,1]N ,L\u0303(z)=0 gtanh(z)\n) + \u03b5,\ni.e., w\u0303\u221e minimizes gtanh up to \u03b5.\nProof of Corollary 3.4. As in the proof of Corollary 3.3, we will apply Theorem 2.3. Again, we only need to verify Assumption (A2). In both cases \u03c1rp = sinh and \u03c1rp = tanh this is fulfilled since \u03c1\u2032rp \u0338= 0 and \u2202D = \u2205. If w\u0303\u221e \u2208 int(DNrp), which trivially holds for \u03c1rp = sinh but needs to additionally be verified for \u03c1rp = tanh, Theorem 2.3 now yields that (18) holds, i.e.,\nw\u0303\u221e \u2208 argmin z\u0303\u2208DNrp,L\u0303(z\u0303)=0 \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9, (43)\nwhere h and H have been defined in Theorem 2.3 as antiderivatives of h\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 .\nProof of (i): We first consider \u03c1rp(z) = sinh(z) with DN = DNrp = RN . In this case, w\u0303\u221e \u2208 int(DNrp) clearly holds. Since the function \u03c1rp is defined entry-wise, both H and h will be defined entry-wise such that we can restrict our derivation to single entries. For z\u0303 = \u03c1rp(z) = sinh(z) and DN = RN , we get that\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 = \u03c1\u2032rp(z) \u22122 = cosh(z)\u22122 = (1 + sinh(z)2)\u22121 = (1 + z\u03032)\u22121 (44)\nsuch that h(z\u0303) = arctan(z\u0303), H(z\u0303) = z\u0303 arctan(z\u0303)\u2212 12 log(1 + z\u0303 2), and\nH(z\u0303)\u2212 z\u0303h(w\u03030) = z\u0303(arctan(z\u0303)\u2212 arctan(w\u03030))\u2212 1\n2 log(1 + z\u03032).\nHence, \u27e81, H(z\u0303)\u2212 z\u0303\u2299h(\u03b11)\u27e9 = gsinh(z\u0303)\u2212arctan(\u03b1)\u27e81, z\u0303\u27e9 in this case. Consequently, (43) yields that\ngsinh(w\u0303\u221e)\u2212 arctan(\u03b1)\u27e8w\u0303\u221e,1\u27e9 \u2264 gsinh(z\u0303)\u2212 arctan(\u03b1)\u27e8z\u0303,1\u27e9,\nfor any z\u0303 \u2208 DN with L\u0303(z\u0303) = 0, which further leads to\ngsinh(w\u0303\u221e) \u2264 gsinh(z\u0303) + arctan(\u03b1)\u27e8w\u0303\u221e \u2212 z\u0303,1\u27e9 \u2264 gsinh(z\u0303) + arctan(\u03b1) (\u2225w\u0303\u221e\u22251 + \u2225z\u0303\u22251) \u2264 gsinh(z\u0303) + arctan(\u03b1) (gsinh(w\u0303\u221e) + gsinh(z\u0303) + 2N) ,\nwhere we used in the last line that |x| \u2264 (x arctan(x) \u2212 12 log(1 + x 2)) + 1, for any x \u2208 R. Since arctan(\u03b1) \u2208 [0, 2\u03b1], for 0 < \u03b1 < 12 , the first claim follows by rearranging the estimate as\ngsinh(w\u0303\u221e) \u2264 gsinh(z\u0303) + 2\u03b1\n1\u2212 2\u03b1 (gsinh(z\u0303) + 2N)\nand minimizing the right hand side.\nProof of (ii): We now consider \u03c1rp(z) = tanh(z) with DN = RN and DNrp = [\u22121, 1]N , and assume in addition that \u03c1link = Id. As before, we can analyze \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(w\u03030)\u27e9 entry-wise. First note that, for z\u0303 = \u03c1rp(z) = tanh(z),\nh\u2032(z\u0303) = ( [\u03c1\u22121rp ] \u2032(z\u0303) )2 = \u03c1\u2032rp(z) \u22122 = (1\u2212 tanh(z)2)\u22122 = (1\u2212 z\u03032)\u22122.\nBy partial fraction decomposition, we obtain that\nh(z\u0303) = 1\n4\n( log(1 + z\u0303)\u2212 log(1\u2212 z\u0303) + 2z\u0303\n1\u2212 z\u03032 ) = 1\n2\n( artanh(z\u0303) + z\u0303\n1\u2212 z\u03032\n)\nand\nH(z\u0303) = 1\n2\n( z\u0303 artanh(z\u0303) + 1\n2 log(1\u2212 z\u03032)\u2212 1 2 log(1\u2212 z\u03032) ) = 1\n2 z\u0303 artanh(z\u0303)\nConsequently,\nH(z\u0303)\u2212 z\u0303h(w\u03030) = z\u0303\n2\n( artanh(z\u0303)\u2212 artanh(w\u03030)\u2212\nw\u03030 1\u2212 w\u030320\n) , (45)\nwhich shows that \u27e81, H(z\u0303)\u2212 z\u0303\u2299 h(\u03b11)\u27e9 = 12 ( gtanh(z\u0303)\u2212 (artanh(\u03b1) + \u03b11\u2212\u03b12 )\u27e81, z\u0303\u27e9 ) in this case. Before applying (43), we however have to verify that w\u0303\u221e \u2208 int(DNrp) = (\u22121, 1)N . Note that, for any z\u0303, u\u0303k \u2208 (\u22121, 1)N with (u\u0303k)i \u2192 \u00b11 for any i \u2208 [N ], we have for F (z\u0303) = \u27e81, H(z\u0303)\u27e9 that DF (z\u0303, u\u0303k) \u2192 +\u221e by the shape of H. Since we know at the same time from Lemma 3.2 that under our assumptions DF (z\u0303, w\u0303(t)) converges, we deduce that w\u0303\u221e \u2208 int(DNrp). Hence, (43) yields that\ngtanh(w\u0303\u221e)\u2212 (artanh(\u03b1) + \u03b11\u2212\u03b12 )\u27e81, w\u0303\u221e\u27e9 \u2264 gtanh(z\u0303)\u2212 (artanh(\u03b1) + \u03b1 1\u2212\u03b12 )\u27e81, z\u0303\u27e9,\nfor any z\u0303 \u2208 DNrp with L\u0303(z\u0303) = 0. Since DN = [\u22121, 1]N and, for 0 < \u03b1 \u2264 12 , artanh(\u03b1) \u2208 [0, 2\u03b1] and \u03b1 1\u2212\u03b12 \u2208 [0, 2\u03b1], we get that\ngtanh(w\u0303\u221e) \u2264 gtanh(z\u0303) + 4\u03b1 \u00b7 |\u27e8w\u0303\u221e \u2212 z\u0303,1\u27e9| \u2264 gtanh(z\u0303) + 8\u03b1N,\nfor 0 < \u03b1 \u2264 12 . The claim easily follows by minimizing the right-hand side."
        }
    ],
    "title": "How to induce regularization in generalized linear models: A guide to reparametrizing gradient flow",
    "year": 2023
}