{
    "abstractText": "Deep learning models develop successive representations of their input in sequential layers, the last of which maps the final representation to the output. Here we investigate the informational content of these representations by observing the ability of convolutional image classification models to autoencode the model\u2019s input using embeddings existing in various layers. We find that the deeper the layer, the less accurate that layer\u2019s representation of the input is before training. Inaccurate representation results from non-uniqueness in which various distinct inputs give approximately the same embedding. Non-unique representation is a consequence of both exact and approximate non-invertibility of transformations present in the forward pass. Learning to classify natural images leads to an increase in representation clarity for early but not late layers, which instead form abstract images. Rather than simply selecting for features present in the input necessary for classification, deep layer representations are found to transform the input so that it matches representations of the training data such that arbitrary inputs are mapped to manifolds learned during training.",
    "authors": [
        {
            "affiliations": [],
            "name": "Benjamin L. Badger"
        }
    ],
    "id": "SP:fef21e7bb74d950863d87598d91638d1664e8158",
    "references": [
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Maxime Oquab",
                "Leon Bottou",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Learning and transferring mid-level image representations using convolutional neural networks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2014
        },
        {
            "authors": [
                "Carl Vondrick",
                "Aditya Khosla",
                "Tomasz Malisiewicz",
                "Antonio Torralba"
            ],
            "title": "Hoggles: Visualizing object detection features",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2013
        },
        {
            "authors": [
                "David G Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "International journal of computer vision,",
            "year": 2004
        },
        {
            "authors": [
                "Aravindh Mahendran",
                "Andrea Vedaldi"
            ],
            "title": "Understanding deep image representations by inverting them",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Leon A Gatys",
                "Alexander S Ecker",
                "Matthias Bethge"
            ],
            "title": "Image style transfer using convolutional neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey E Hinton"
            ],
            "title": "To recognize shapes, first learn to generate images",
            "venue": "Progress in brain research,",
            "year": 2007
        },
        {
            "authors": [
                "A. \u00d8ygard"
            ],
            "title": "Visualizing GoogLeNet Classes. https://www.auduno.com/2015/07/29/visualizing-googlenet-classes",
            "year": 2015
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Lawrence Cayton"
            ],
            "title": "Algorithms for manifold learning",
            "venue": "Univ. of California at San Diego Tech. Rep,",
            "year": 2005
        },
        {
            "authors": [
                "Rajiv Mehrotra",
                "Kameswara Rao Namuduri",
                "Nagarajan Ranganathan"
            ],
            "title": "Gabor filter-based edge detection",
            "venue": "Pattern recognition,",
            "year": 1992
        },
        {
            "authors": [
                "Timothy Gowers",
                "June Barrow-Green",
                "Imre Leader"
            ],
            "title": "The Princeton companion to mathematics",
            "year": 2008
        },
        {
            "authors": [
                "Maithra Raghu",
                "Thomas Unterthiner",
                "Simon Kornblith",
                "Chiyuan Zhang",
                "Alexey Dosovitskiy"
            ],
            "title": "Do vision transformers see like convolutional neural networks",
            "year": 2021
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning (still) requires rethinking generalization",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Sotiris Anagnostidis",
                "Gregor Bachmann",
                "Lorenzo Noci",
                "Thomas Hofmann"
            ],
            "title": "The curious case of benign memorization, 2022",
            "venue": "URL https://arxiv.org/abs/2210.14019",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Keywords Deep Learning \u00b7 Regularization \u00b7 Representation"
        },
        {
            "heading": "1 Introduction",
            "text": "Deep learning models are termed deep because they are composed of sequential layers which pass input information to each other in succession to form an output. The theory of deep learning postulates that each layer forms a representation of the input such that certain features of the input are selected, and these representations become more abstract as model depth increases (Zeiler and Fergus, 2014). The last layer provides a mapping from these abstract features to the output, and the goal of forming representations of the input in successive layers is to make that mapping process effective as possible. Presumably this is done by passing only the information necessary to allow the final layer\u2019s mapping to approximate the desired output.\nThere is abundant evidence to support this theory of deep learning representations (Goodfellow et al., 2016; Oquab et al., 2014), which is in some sense somewhat counter-intuitive given the architectures of commonly used vision models. Focusing on convolutional neural networks used for image classification, it is unclear that increasing layer depth would necessarily limit the information that is passed from input to output given that many models currently in use contain more elements in each layer than there are elements in the input. Embedding theory postulates that a model typically requires a reduction in the number of nodes in a hidden layer relative to the input in order to perform a proper (non-trivial) embedding such that the input is not merely copied.\nPerhaps the most direct way to understand embedding informational content is to observe the ability of embeddings (which are equivalent to a layer outputs) of various hidden layers of vision models to copy the model\u2019s input. In this work the information present in a layer embedding is observed via performing gradient descent on an initially random input in order to give an output from that layer that approximates the output of that same layer given some target image input. This procedure may be thought of as performing an autoencoding of the input, with the similarity (via a distance metric or simply a visual measure) between the input generated and the true input being a measure of how accurate that\n\u2217The author would like to thank Guidehouse for support during the research and writing of this paper. Code for this work may be found on https://github.com/blbadger/depth-representation.\nar X\niv :2\n21 1.\n06 49\n6v 3\n[ cs\n.C V\n] 2\n5 Fe\nb 20\nautoencoding is. Assuming that the gradient descent procedure is sufficiently powerful to generate arbitrary images, a better autoencoding signifies more information exists in the layer\u2019s embedding with respect to the input whereas a worse autoencoding signifies the opposite.\nThe question of whether input information is restricted in successive layer representations is addressed first, and then a theory behind the answer to this question is developed, as well as some insight into how the training process informs layer representations."
        },
        {
            "heading": "1.1 Related Work",
            "text": "Elsewhere, shallow histograms of oriented gradient representation visualizations were used to understand instances of mis-identification (Vondrick et al., 2013) which drew from earlier work on visualization of image matching features in the context of layered difference-of-Gaussian images (Lowe, 2004). More recently, work which used gradient-based updates on initially random inputs to visualize convolutional neural network hidden layer representations (Mahendran and Vedaldi, 2015) focused on invariances between representation visualizations using various regularizers in the gradient update. It was also shown that deep learning image representations are capable of separating image style from content (or equivalently abstract features from specific ones), allowing for the gradient-based generation of an image that has the content of one target while applying the artistic style of another target image (Gatys et al., 2016)."
        },
        {
            "heading": "1.2 Our Contribution",
            "text": "The point of departure for this work is that rather than attempting to make the most accurate visualization for some deep layer representation, or aiming to manipulate an image using representations, we instead focus on the limits of representations and specifically focus on their informational content. Investigating this question we observe both trained and untrained vision model representations and the specific changes that occur upon training. We find that representations become more and more inaccurate as model depth increases, and provide a theoretical basis for how this occurs due to greater non-uniqueness with increased depth. We provide an alternative rationale for why deep vision models tend to learn Gabor filters in their early layers of these models, and find experimental support for the idea that later layers impose their expectations of what the input should be on their representations, rather than simply attempting to select for certain features of that input. We conclude that vision models trained for classification tend to generate their own interpretations of any given input even if they are not specifically trained to do so, supporting the hypothesis that image recognition and generation are required for one another (Hinton, 2007)."
        },
        {
            "heading": "2 Representation Accuracy Decreases with Depth",
            "text": "In an attempt to understand the informational content in various layers and whether this information is restricted the deeper the layer in question, we take the direct approach of visualizing the embedding that exists in that layer. This visualization may be accomplished in a variety of ways, but here we choose a method that is particularly germane to the learning process: gradient descent.\nGiven some input a, a model designated by parameters \u03b8 and a number of model layers composed as the function Ol, the output at the layer of interest is denoted in Equation (1).\ny\u0302 = Ol(a, \u03b8) (1)\nThis y\u0302 vector is the representation in layer l of input a and contains the information that layer has with respect to the input. Visualizing this representation allows for the understanding of the information it contains, so what we want to find is some input an that gives an output at the same layer that approximates the target output y\u0302 as shown in (2).\ny = Ol(an, \u03b8) (2)\nWe can use any number of objective functions to compare the similarity of y\u0302 and the output y for some other input an, but here we choose the L1 metric. The gradient of this metric with respect to the input is given in (3) for elements indexed by i in output y.\ng = \u2207an \u2211 i |y\u0302i \u2212 yi| (3)\nThe choice of initial input a0 is somewhat arbitrary, and for this work we assign pixels randomly on a scaled Gaussian distribution such that a0 = N (a; \u00b5 = 7/10, \u03c3 = 1/20). The gradient in (3) may be used to update the input at iteration n directly as shown in Equation (4), or applied with a R3x3 Gaussian convolutionNc at each iteration as in (5), both of which may be implemented with an that decreases as n increases or else stays constant. If Gaussian convolution is applied, the distribution\u2019s standard deviation \u03c3 typically is assigned to start at \u03c3 = 2.4 (pixels) before linearly decreasing to \u03c3 = 0.4 as n\u2192 f where f is the number of iterations specified.\nan+1 = an \u2212 \u2217 g (4)\nAfter n = f iterations of (4) or (5), the generated input is termed ag .\nan+1 = Nc(an \u2212 \u2217 g) (5)\nThis visualization technique is a variation on those have been used extensively for output class visualization (\u00d8ygard, A.), feature visualization (Olah et al., 2017), and deep dream (Mordvintsev et al.).\nFirst we will explore representations of an image of a dalmatian, one of the 1000 classes of the ImageNet 1K dataset (hereafter referred to as \u2019ImageNet\u2019), a commonly used excerpt from the full ImageNet dataset (Deng et al., 2009). This image is chosen because it contains multiple levels of recognizable detail: the general outline of the canine as well as the specific pattern of spots on its coat. We focus on the ResNet class of models (He et al., 2016), reasoning that the residual connections in these models would make gradient descent on the input easier to tune, ie that there would be a wider range of possible values that yield a sufficiently small |y\u0302 \u2212 y|. Applying (5) to a random Gaussian input, the representation visualizations obtained for certain layers of ResNet50 after a fixed number of iterations n are shown in Figure 1. We follow the naming configuration in the original ResNet publication (He et al., 2016) rather than the PyTorch implementation which renames layers Conv2 as Layer 1, Conv3 as Layer 2 etc.\nIt is clear that the representation becomes more abstract the deeper the layer in question, with a near-perfect copy of the input image in the early layers giving way to a cartoon-like abstraction in the later layers. Of note, the general outline of some canine features (head, nostrils) are observed in all layers but finer details like the exact spot pattern of the input are only found from layers Conv1 to Conv3, whereas the representations from Conv4 and Conv5 contain a noticeably different spot pattern than the original input.\nEach representation may be viewed as a combination of an exact copy of the input (a \u2018trivial\u2019 representation) and what could be thought of as a true representation, one that is the result of the learning procedure. To investigate the relative contributions of each type of representation to each layer, we applied (5) using an untrained ResNet50 model with the reasoning that an untrained model\u2019s layer representations would have no contributions from the \u2018true\u2019 representation category.\nFrom the results shown in Figure 2, it is clear that the unlearned representations provide less and less information as layer depth increases. Notably for this particular input, the spot pattern on the Dalmatian\u2019s coat disappears in the same layer (Conv4) of the untrained model that the new spot pattern appears in the trained model\u2019s representation (Figure 1). The same observations are made when (4) is used, meaning that this decrease in input information present in the later layers does not result from Gaussian convolution during the representation visualization process (Figure S3)."
        },
        {
            "heading": "3 Imperfect Representations result from Non-Uniqueness",
            "text": ""
        },
        {
            "heading": "3.1 Visually poor input representations despite accurate embedding approximation",
            "text": "The inability of untrained image recognition models to form an accurate representations of an input is unexpected given the immense size of these models. For example, layer Conv4 in ResNet50 has over 369, 000 output elements, whereas the input it fails to accurately approximate (Figure 2) has only 3 \u2217 299 \u2217 299 = 268, 203 elements. Understanding why these deep learning models are incapable of making accurate trivial representations despite having more than enough parameters to do so is investigated next in depth.\nThe process of representation visualization, ie the transformations necessary to send a0 \u2192 ag, may be thought of as many iterations of three separable components: first a forward pass is performed to obtain some output via (1) followed by comparison to the target output obtained in (2) to obtain the gradient (3), and finally this gradient is used to update the input via gradient descent with (4) or without (5) Gaussian convolution. To understand how the representation fails to approximate an input for deeper layers, we consider each component of the visualization process.\nFirst we consider whether or not the gradient acquisition and update components are the cause of the poor representations. To do this, we test whether the output of ag approximates the output of a sufficiently. Rather than use the L1 metric which is minimized during gradient descent, we employ an L2 metric to avoid bias. Specifically the L2 norm of the difference between the target embedding and generated image embedding, mg , is computed (6) which is analogous to an n-dimensional version of the Frobenius norm of the difference between output tensors. This is identical to converting the output tensors into vectors by reshaping them into Rix1 matricies, where i signifies the number of elements in each\noutput tensor (both of which must be the same shape), and then finding the L2 norm on the difference between these vectors.\nmg = ||O(ag, \u03b8)\u2212O(a, \u03b8)||2 = \u221a\u2211\ni\n(O(ag, \u03b8)i \u2212O(a, \u03b8)i)2 (6)\nThe value of m without some reference is not very illuminating given the abstract nature of hidden layer architectures. This reference may be supplied by choosing a slightly shifted value a\u2032 by adding random normally distributed values to a as shows in (7).\na\u2032 = a+N (a;\u00b5 = 0, \u03c3 = 1/20) (7)\nOne may establish a measure of the distance mr between the outputs given the shifted input and original as shown Equation (8), which is compared to mg with the assumption that mg < mr signifies a \u2018good\u2019 approximation if a\u2032 is sufficiently close to a.\nms = ||O(a\u2032, \u03b8)\u2212O(a, \u03b8)||2 (8)\nFor two layers of ResNet50, we see that this is indeed the case: in Figure 3 each representation visualization exhibits an mg < ms regardless of visual quality, and the same is true for the same layers ResNet152 and for ResNet18. Observing the difference in Conv5 representation visual quality between ResNet50 and ResNet152, it is clear that an increase in depth further reduces the input representation visualization quality regardless of layer width (Figure 3)."
        },
        {
            "heading": "3.2 Analytically poor input representations",
            "text": "Thus it appears that the gradient descent procedure is capable of at least one kind of accurate approximation. It may be wondered whether this approximation to within a value of m of O(a, \u03b8) using an L2 metric is sufficient to say that our gradient descent procedure is actually effective. Instead of comparing mg = ||O(ag, \u03b8)\u2212O(a, \u03b8)||2 to some reference value, we can instead we can try to understand if our approximation of y\u0302 = O(a, \u03b8) is sufficient by comparing the limit of the output (embedding) distance represented by (8) to the limit of the input distance represented by Equation (9) as the gradient descent procedure is carried out for an increasing number of iterations n.\nThe hypothesis is that if the gradient descent procedure is capable of approximating y\u0302, then (9) holds.\nan+1 = an \u2212 g \u2217 , n\u2192\u221e =\u21d2 ||O(a, \u03b8)\u2212O(ag, \u03b8)||2 \u2192 0 (9)\nFor a more precise measure of the generated input\u2019s closeness to the target input a, we defined mi as the L2 norm of the difference between generated and target inputs in Equation 12, analogously to what was done to measure output tensor differences.\nmi = ||a\u2212 ag||2 (10)\nWe investigate this first in layer Conv2 with an untrained ResNet50, iterating the non-Gaussian convolved version of gradient descent (4). The results are shown in Figure 4, where it is observed that an exponential decay in the output distance between generated and original embeddings, ||O(a, \u03b8)\u2212 O(ag, \u03b8)||2, occurs even as there is an increase in input distance mi.\nThis decoupling of a decrease in mg with a decrease in mi is often more pronounced for deeper layers: for ResNet50 layer Conv5 plotted in Figure 5, there is an inverse correlation between the values of mg and mi as n increases.\nThe finding that an approximation of an output vector mg can become better as the input space mi approximation becomes worse suggests non-uniqueness of the output embeddingO, as having many possible inputs give approximately the same output would yield an inverse relation between mg and mi if ag 6= a. In the next section we explore the theoretical basis behind non-uniqueness and why depth contributes to this phenomenon.\nAfter training has completed, it may be wondered if the phenomenon of non-uniqueness remains. Testing this question for ResNet50 layer Conv5, we find in Figure S1 that there is evidence that training does not remove non-uniqueness of layer representations."
        },
        {
            "heading": "4 Representations worsen with depth due to non-invertibility",
            "text": "From an informational perspective, it at first seems counter-intuitive that depth would lead to poorer representations even if all layers of the model in question may be considered to be overcomplete being that they have more nodes than the input has elements. Another possible explanation for poor representation is that the transformation O(a, \u03b8) may be expressed as a linear operator of poor conditioning, meaning that even though this transformation may be invertible in theory it is not invertible in practice due to large differences between the smallest and largest eigenvalues of that transformation. Therefore we examine both exact as well as approximate non-invertibility as potential causes of poor hidden layer representation. It should be noted that we did not observe arithmetic precision to play a significant role in representation quality at least for the experiments documented here, as for example changing from 32-bit to 64-bit floating precision did not affect representations for convolutional or fully connected models.\nFirst we observe that nearly all deep learning vision models studied today contain non-invertible transformations between successive layers. To see why this is the case, take a typical deep learning vision model such as ResNet. Between layers we have various operations: convolutions, max and average pooling, with the somewhat special operation (to the ResNet model family) of element-wise tensor addition. In the case of convolution and pooling operations, observe that any layer with fewer elements than the previous layer cannot uniquely define that previous layer. For the case of a fully connected neural network with no nonlinearity applied, any layer that has fewer elements than the previous layer is equivalent to a transformation by a non-rectangular matrix (followed by addition of a bias term) and non-rectangular matricies are in the general case non-invertible.\nApproximate non-invertibility may be restated as the case where an approximation of some embedding tensor (the representation) may not be a good approximation of the input that gave that embedding. Say that one were to train a fully connected model with no biases or nonlinearities, where model is invertible such that any given output gave a unique input. The model may be represented by a single transformation ABCDx = Ex. Now suppose that the transformation E contracted or expanded the basis vectors of x at widely different scales, which is a case of poor conditioning of the transformation E. This means that points initially either close to or far from some target input a are transformed to lie in the same region E(a+ \u03b4), depending on the direction of a to those points. If this is the case, it may be necessary to approximate E(a) extremely well in order to make ag approximate a."
        },
        {
            "heading": "4.1 Arbitrarily accurate input representation is achievable for MLPs with each layer equivalent to an overcomplete transformation",
            "text": "To investigate the effect of invertibility on input representation accuracy, we focused on input representation without Gaussian convolutions (using Equation 4) applied to simple fully connected neural networks, which for simplicity we call \u2018MLP\u2019 architectures. Unless otherwise denoted, the MLPs used in this work are of typical design (all connections between nodes having a weight parameter, and all nodes except the input containing a bias parameter) but notably no nonlinear transformation is applied to each layer output. The case of models with ReLU nonlinearity applied to hidden layers is examined in the next section.\nFirst we examine the case of input representation accuracy for a simple three-layer MLP where the number of nodes of the first layer varies. We use a 3x29x29 down-sampled version of the image of the Dalmatian applied as an input elsewhere in the paper in order to prevent model memory blow-up. Layer weights are initialized to a Kaiming uniform distribution and the model is untrained.\nConsidering the case of exact non-invertibility, the hypothesis is that an arbitrarily \u2018good\u2019 input representation is possible if and only if each transformation between successive layers is overcomplete (or in other words if there are at least as many output elements as input elements). For a fixed number of iterations of (4), it is evident that the generated input ag using the output of the final layer as the target embedding becomes more similar to the target input a as the number of elements in the first hidden layer surpasses the number of elements in the input (Figure 6).\nThe possibility remains that an increase in the number of iterations of (4) would allow undercomplete models (with layer width x < 2523) to approximate a arbitrarily well despite non-invertibility if there were only one input near the starting point a0 that yields an accurate approximation of the desired embedding (even if there are many inputs that equivalently give the same representations, but these may be very far from a0 such that they would not be found via gradient descent). This idea can be easily tested, however, and in Figure 7 is observed to not be supported for x = 2000 and x = 1000: as seen for ResNets, increasing iterations of gradient descent (4) yield better and better output approximations, i.e. ||O(a, \u03b8) \u2212 O(ag, \u03b8)||2 \u2192 0, but the input representation distance ||a \u2212 ag|| becomes slightly larger and certainly does not decay towards the origin.\nTo conclude this section, we obtain experimental evidence for the idea that non-invertibility of a multi-layer perceptron results in necessarily imperfect input representation as a result of non-uniqueness. It should be noted that non-\ninvertibility does not necessarily lead to the poor representation phenomenon observed in the last section, however: take for example the case where of all possible inputs that yield an identical output only one is near a0."
        },
        {
            "heading": "4.2 Approximate non-uniqueness contributes to poor representation clarity but does not account for asymptotically poor representations",
            "text": "The inability to accurately represent an input given non-invertible layer transformations does not preclude the idea that deep layers have poor representations of their inputs in part because of approximate non-uniqueness. To investigate this possibility, an experiment was designed in which an MLP model was constructed to be identical to the case for Figure 6 except that the number of layers initialized were changed, rather than the width of the first hidden layer.\nAs shown in Figure 8, an increase in the number of layers between the input and output leads to less accurate representation visualizations of the output layer for a fixed number of gradient descent iterations. But if the number of gradient descent iterations grows, deeper models form representations that are capable of approximating the input well (Figure 8).\nIt may be wondered exactly how poorly conditioned (or equivalently how approximately non-invertible) an invertible fully connected model is. Rather than investigate this directly by solving for a model\u2019s eigenvalues, a perhaps more intuitive way of approaching this problem is to observe what possible changes can be made in the input for some small change in the output.\nFor simplicity, we consider a model where A,B,C are matricies corresponding to the layer weights, and all layers have a width equal to the number of input elements, 2523. Given this model, we can invert the last layer as follows: given the output vector o and layer input vector x, we can solve for x in terms of o and weight matrix C and bias vector b as given in Equation (11).\no = Cx+ b =\u21d2 x = C\u22121(o\u2212 b)\n(11)\nApplying (11) iteratively to layers C,B,A we can recover the input given any output vector. Given input a, it can be experimentally verified that a fully connected three-deep model may be inverted accurately using Torch library matrix inversion, but notably only if double (64-bit floating) precision is used. If we add a small random normal shift to the output we can then recover the input a\u2032\u2032, which is defined as O(a\u2032\u2032, \u03b8) = O(a, \u03b8) +N (O(a, \u03b8);\u00b5 = 0, \u03c3 = 1/1000). The distance ||a \u2212 a\u2032\u2032|| can then be compared to our standard shifted input distance ||a \u2212 a\u2032||, being that the output distance ||O(a\u2032, \u03b8)\u2212O(a, \u03b8)) > ||O(a\u2032\u2032, \u03b8)\u2212O(a\u2032, \u03b8)|| by design.\nFrom Figure S2 it can be appreciated that nearly all possible inputs a\u2032\u2032 are much farther (more than one million times to be specific) from a than a\u2032, meaning that even an untrained three-hidden-layer MLP is typically quite poorly conditioned."
        },
        {
            "heading": "4.3 ReLU nonlinearities make overcomplete architectures require more elements per layer than the previous layer",
            "text": "All of the MLP experiments in this work so far have employed typical architectural choices (weights and biases) but contain no nonlinear activation function. While experimenting with various MLP architectures, it became clear that including ReLU activations, and to a lesser extent other nonlinear activations, after each layer leads to a substantial drop in representation visualization accuracy, or more precisely ||a\u2212 ag|| increased for a given output distance. ReLU is given in Equation 12 for convenience.\ny = f(x) = { 0, if x \u2264 0 x, if x > 0\n(12)\nConsidering the nature of what is required for invertibility between layers, it is apparent that adding ReLU to a layer\u2019s output diminishes that layer\u2019s ability to represent the layer\u2019s input simply because some number of the elements that were possibly unique before ReLU transformation are now equivalently zero, such that no information on the exact inputs of these elements may be obtained. More precisely, all elements of the set of layer outputs y0, y1, y2, ... less than zero are now identically zero, f(yi : yi < 0) = 0. If we assume that there is some probability p such that p(y) = P (y : y < 0) and assume further that this probability does not change for different layers, it is clear that the number of output elements required in order to solve for the input elements is 1/p which is evident from the definition of the expectation of p(y). Extending this to the case of multiple layers, we find that for a model with ReLU nonlinearities to be overcomplete this model must have an increasing number of neurons per layer, and the precise number of neurons required for layer n is m(1/p)n where m denotes the size of a. This is summarized in Figure 9 for clarity.\nAt the start of training, it may be assumed that half of each layer\u2019s neurons are zeroed out if ReLU is applied. If this is the case, each successive layer must double in width in order to be capable of preserving all the information necessary\nto represent the input exactly. It should be noted that for trained model it is rarely seen that there is some constant probability of ReLU transforming an activation to 0 for different layers, and that a model may be chosen that can approximate perfect representation with only a constant number of neurons per layer regardless of whether or not any nonlinear activation used if all activations are greater than 0.\nThis observation provides a theoretical basis for the standard autoencoder architecture of connecting the latent space hidden layer to the output via layers each larger than the last: given that such models are typically trained with ReLU activation applied to hidden layers, increasing the widths of layers connecting the latent space to the output is required in order to prevent information loss."
        },
        {
            "heading": "4.4 Why representation accuracy decreases with depth",
            "text": "Returning to ResNet, it appears that the non-uniqueness due to absolute non-invertibility as well as approximate non-invertibility both contribute to poor representation accuracy in deep layers. Evidence for absolute non-invertibility is the observation that the limit of the distance between the approximation of O(a, \u03b8) and the target output itself tends towards the origin even as the distance ||a\u2212 ag||2 increases. Poor conditioning would make O(a, \u03b8) difficult to\napproximate but would not expected to lead to consistently larger ||a\u2212 ag||2 distances upon better approximations of the output, whereas exact non-uniqueness certainly would. On the other hand, it typically takes a very large number of iterations at very small learning rates for this distance to approach the origin, which is what would be expected given approximate non-invertibility.\nA lack of invertibility between layers means that a potentially infinite number of linear combinations of the input can give one output. As realistic image inputs are bounded in some way (for the inputs used in this work, every input element is between 0 and 1, ie ai \u2208 [0, 1]), we can expect a finite number of inputs to yield some output when given to the model. Say for example that a model has 5 layers and each layer has 100 possible inputs for a given output. For example, if 100 distinct inputs for layer 1 yield a given output, the output of the second layer there are 1002 possibilities and for layer n we have in general 100n possibilities.\nFurthermore, exact representation is further hampered if a model contains distributed representations because subsets of the input may be replaced with linear combinations of subsets of the input without changing the output.\nIn conclusion, most deep learning architectures applied to image recognition can be shown to exhibit non-unique representations. This non-uniqueness results in representations that lose input information such that they are unable to exactly copy the input. Once non-uniqueness is removed, arbitrary approximation of an embedding yields arbitrary approximation of the target input. For representations with a fixed number of iterations, however, approximate non-uniqueness does limit representation accuracy."
        },
        {
            "heading": "5 Hidden layers transform arbitrary inputs into expected ones.",
            "text": "Comparing Figures 1 and 2, it is evident that training leads to an increase in representation clarity in early layers for a target input that is a member of a class in the training dataset, regardless of whether Gaussian convolution was used in the representation visualization process (Figure S3). It may be wondered how invariant this increase in clarity is: does it also apply to images that are not members of the training dataset in question, and does any training protocol yield greater clarity in early layer representations? The first question is addressed using an image of a Tesla coil, which is not a member of the ImageNet dataset and would not be observed during training. We see in Figure 10 that indeed there is still a marked increase in clarity in the ImageNet-trained model\u2019s representation of this input compared to the untrained model.\nIt may be hypothesized that recognizing any image would lead to a clearer representation after training, but we find that this is not the case. Training ResNet50 to classify 10k images of random Gaussian noise N (a; \u00b5 = 1/2, \u03c3 = 1/3) randomly labeled as one of 100 classes, we find that layers Conv3 and Conv4 are noticeably less clear than for the same layers either untrained or trained on ImageNet (Figure 10). Comparing the kernal weights learned by training on ImageNet versus training on random noise, it is observed that the latter do not contain the the wavelet transformations that are typical of the former (Figure S5).\nIt is interesting to note that the input representation visualizations for this random-trained model 10 do not give very accurate outputs, or more specifically ||O(ag, \u03b8)\u2212O(a, \u03b8)|| does not become arbitrarily small as the number of iterations increases. This is not the case if Gaussian convolution is omitted, although the representation visualizations are qualitatively very similar (Figure S4). When one considers that the training data for the model in question had no inputs that are well-fit by a convolved version of themselves, this finding is not surprising.\nThus a model trained to recognize images of random noise (and recognize them successfully) does not noticeably sharpen the input representation in early layers, and actively transforms natural image representations into those nearly indistinguishable from the pattern of noise present in the training set. This provides evidence for the notion that rather than simply selecting for certain features in an input, deep learning hidden layers map inputs to representations of inputs that the model has learned to expect from the training data. If deep learning models are capable of approximating the low-dimensional manifold defined by the set of inputs (Cayton, 2005), deep hidden layers are here observed to map arbitrary inputs to the manifold learned even if the actual input is very different from that manifold. This idea is a modification of the standard manifold learning hypothesis in which inputs are assumed to exist on a low-dimensional manifold embedded in high-dimensional space such that the model learns to perform mappings on this manifold. Instead, we find that arbitrary inputs (which are not necessarily on the manifold) are mapped to the manifold or manifolds learned by hidden layers, regardless of whether the inputs are actually on the manifold or far from it."
        },
        {
            "heading": "6 Implications for representation learning",
            "text": "The convolutional kernals that are learned by early layers during image recognition training resemble kernals handdesigned for edge detection, and it can be appreciated kernals of ResNet layers Conv1 (after max pooling), Conv2, and\nConv3 appear to be effective at sharpening the representation relative to what it was at the start of training. It has been appreciated for some time that convolutional models often learn kernals in their early layers that resemble the Gabor functions that are the products of trigonometric wavelets and Gaussian functions (Mehrotra et al., 1992; Goodfellow et al., 2016). Indeed, ResNet50\u2019s layer Conv1 kernal weights are observed to learn Gabor and other wavelet functions after training on ImageNet (Figure S5). Wavelets provide a basis for locally efficient encodings of sharp edges (Gowers et al., 2008) and as such may be capable of preventing some of the blurring that is observed to occur for non-unique transformations typical of pooling or strided convolutional layers. We postulate that wavelet functions are learned precisely because they create an output that is more unique with respect to certain aspects of the input than the naive output, resulting in representations from these layers to become visually sharper after training on ImageNet.\nWe also offer an explanation for the recently observed difference between early and late layer convolutional filter correlation (Raghu et al., 2021). Early layers attempt to restrict the possible feasible (ie am,n \u2208 [0, 1]) inputs that yield some given embedding, but late layers do not and instead attempt to map the early layer representations to the manifold learned during training.\nIt has been observed that image recognition models such as ResNets are capable of memorizing pure noise (Zhang et al., 2021), which is somewhat counter-intuitive given their resistance to overfitting natural images. In light of the results provided in this work, it seems likely that the process of learning to recognize pure noise is very different than the process of learning to recognize natural images given the difference in ability to represent the input after training, even if both lead to training accuracy at unity. Other work has also suggested a similar distinction (Anagnostidis et al., 2022).\nFinally, our work provides an explanation for the often-observed requirement for specific types of regularizations such as Gaussian convolution and position invariance when an output class is used to represent an input. It is clear that many possible images may represent an output (to any degree of accuracy specified) due to non-uniqueness, and there is no guarantee that these images resemble natural ones that are examples of the class in question. Mandating the generated images to have some statistical similarity to natural images is required to prevent non-uniqueness from leading to unrecognizable image generation."
        },
        {
            "heading": "7 Conclusion and Future Perspectives",
            "text": "In this work we have explored the ability of various layers of deep learning vision models to represent some input. Information on the input is lost during the forward pass, replaced in later layers with the model\u2019s expectation of what the input resembles from information gained during training.\nInexact representation might be expected to be one reason why deep learning models generalize so well even when they have far more nodes than are required for memorization. If this were true, one would expect for an \u2018inverted\u2019 architecture that is capable of exact image representation (for example, a fully connected architecture with an increasing number of nodes per layer) to be more prone to overfitting relative to the standard architectures used for image classification where the number of nodes decreases with increasing depth.\nBut this is not observed to be the case: for both CIFAR10 and CIFAR100, MLP-stype architectures that with inverted architectures overfit no more readily than standard architectures. This is also true for convolutional models applied to the same datasets, suggesting that some other regularizer may be present to prevent overfitting even for a model that is capable of precise input representation in all hidden layers. But as the gradient flows from the loss of the last layer to the criterion, it is also possible that precise input representation in late hidden layers is not relevant for classification model overfitting given that the output will always be limited in how it can represent the input, assuming fewer classes than input elements.\nThe results presented in this work appear to be general to other convolutional image classification models (GoogleNet etc.) but it remains to be seen whether they are also applicable to vision transformers, mixers, or other non-convolutional models."
        },
        {
            "heading": "8 Appendix",
            "text": "Experiments for this work were performed in Colab. Code and target images used in this paper may be found in https://github.com/blbadger/depth-representation. Models were modified from PyTorch implementations, with default ImageNet 1k -trained weights from the Torchvision hub used for the \u2019trained\u2019 models. \u2019Untrained\u2019 models were Kaiming uniform-initialized by default (as they were constructed with PyTorch).\nThe representation visualization procedures (4) and especially (5) were found to be sensitive to changes in the learning rate employed. Typically shallow layer visualization were obtained with larger learning rates than deep layer visualization, with ranging from approximately 1/2 for Conv1 to 1/1000 for layer Conv5. Learning rates used for the representation visualizations ag in this work were obtained by random line search, selecting for that approximately minimized ||O(a, \u03b8)\u2212O(ag, \u03b8)||. For analytic studies on representation visualization with (4), we note that the value of chosen has less of an effect: assuming that it is small enough, there was no observed change in the behavior (ie exponential decay) of the representation accuracy upon increasing or decreasing the learning rate.\nFigure S1: Trained ResNet50 input representations are non-unique.\nFigure S2: Inverting a slightly shifted output yields a very large input shift for an untrained MLP.\nFigure S3: ResNet50 Input Representations before and after training on ImageNet with or without Gaussian convolution in the generation process.\nFigure S4: Tesla coil input representation of ResNet50 trained on fixed random samples, without Gaussian convolution. All representations have a closer embedding distance than the standard shifted input a\u2032 = a+N (a;\u00b5 = 0.7, \u03c3 = 1/18)\nFigure S5: ResNet50 Conv1 convolutional kernal weights form Gabor function wavelets after training on natural images but not random Gaussian images."
        }
    ],
    "year": 2023
}