{
    "abstractText": "Deep learning models have demonstrated impressive performance in various domains. However, the prolonged training time of these models remains a critical problem. Manually designed parallel training strategies could enhance efficiency but require considerable time and deliver little flexibility. Hence, automatic parallelism is proposed to automate the parallel strategy searching process. Even so, existing approaches suffer from sub-optimal strategy space because they treat automatic parallelism as two independent stages, namely interand intra-layer parallelism. To address this issue, we propose UniAP, which utilizes mixed integer quadratic programming to unify interand intra-layer automatic parallelism. To the best of our knowledge, UniAP is the first work to unify these two categories to search for a globally optimal strategy. The experimental results show that UniAP outperforms state-of-the-art methods by up to 1.70\u00d7 in throughput and reduces strategy searching time by up to 16\u00d7 across four Transformer-like models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hao Lin"
        },
        {
            "affiliations": [],
            "name": "Ke Wu"
        },
        {
            "affiliations": [],
            "name": "Jun Li"
        },
        {
            "affiliations": [],
            "name": "Wu-Jun Li"
        }
    ],
    "id": "SP:7123331bb8e79c29dd8480b09fd0a314843e03eb",
    "references": [
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
            "venue": "In The North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2019
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh",
                "Daniel M. Ziegler",
                "Jeffrey Wu",
                "Clemens Winter",
                "Christopher Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "In International Conference on Learning Representations",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "venue": "In International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Deepak Narayanan",
                "Mohammad Shoeybi",
                "Jared Casper",
                "Patrick LeGresley",
                "Mostofa Patwary",
                "Vijay Korthikanti",
                "Dmitri Vainbrand",
                "Prethvi Kashinkunti",
                "Julie Bernauer",
                "Bryan Catanzaro",
                "Amar Phanishayee",
                "Matei Zaharia"
            ],
            "title": "Efficient Large-scale Language Model Training on GPU Clusters Using Megatron-LM",
            "venue": "In International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Noam Shazeer",
                "Youlong Cheng",
                "Niki Parmar",
                "Dustin Tran",
                "Ashish Vaswani",
                "Penporn Koanantakool",
                "Peter Hawkins",
                "HyoukJoong Lee",
                "Mingsheng Hong",
                "Cliff Young",
                "Ryan Sepassi",
                "Blake A. Hechtman"
            ],
            "title": "Mesh-TensorFlow: Deep Learning for Supercomputers",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Yuanzhong Xu",
                "HyoukJoong Lee",
                "Dehao Chen",
                "Blake A. Hechtman",
                "Yanping Huang",
                "Rahul Joshi",
                "Maxim Krikun",
                "Dmitry Lepikhin",
                "Andy Ly",
                "Marcello Maggioni",
                "Ruoming Pang",
                "Noam Shazeer",
                "Shibo Wang",
                "Tao Wang",
                "Yonghui Wu",
                "Zhifeng Chen"
            ],
            "title": "GSPMD: General and Scalable Parallelization for ML",
            "venue": "Computation Graphs. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Michael J. Flynn"
            ],
            "title": "Very High-speed Computing Systems",
            "venue": "Proceedings of the IEEE,",
            "year": 1901
        },
        {
            "authors": [
                "Michael J. Flynn"
            ],
            "title": "Some Computer Organizations and Their Effectiveness",
            "venue": "IEEE Transactions on Computers,",
            "year": 1972
        },
        {
            "authors": [
                "Yanping Huang",
                "Youlong Cheng",
                "Ankur Bapna",
                "Orhan Firat",
                "Dehao Chen",
                "Mia Xu Chen",
                "HyoukJoong Lee",
                "Jiquan Ngiam",
                "Quoc V. Le",
                "Yonghui Wu",
                "Zhifeng Chen"
            ],
            "title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Deepak Narayanan",
                "Aaron Harlap",
                "Amar Phanishayee",
                "Vivek Seshadri",
                "Nikhil R. Devanur",
                "Gregory R. Ganger",
                "Phillip B. Gibbons",
                "Matei Zaharia"
            ],
            "title": "PipeDream: Generalized Pipeline Parallelism for DNN Training",
            "venue": "In Symposium on Operating Systems Principles,",
            "year": 2019
        },
        {
            "authors": [
                "Deepak Narayanan",
                "Amar Phanishayee",
                "Kaiyu Shi",
                "Xie Chen",
                "Matei Zaharia"
            ],
            "title": "Memory- Efficient Pipeline-Parallel DNN Training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Shiqing Fan",
                "Yi Rong",
                "Chen Meng",
                "Zongyan Cao",
                "Siyu Wang",
                "Zhen Zheng",
                "Chuan Wu",
                "Guoping Long",
                "Jun Yang",
                "Lixue Xia",
                "Lansong Diao",
                "Xiaoyong Liu",
                "Wei Lin"
            ],
            "title": "DAPPLE: A Pipelined Data Parallel Approach for Training Large Models",
            "venue": "In Symposium on Principles and Practice of Parallel Programming,",
            "year": 2021
        },
        {
            "authors": [
                "Shigang Li",
                "Torsten Hoefler"
            ],
            "title": "Chimera: Efficiently Training Large-Scale Neural Networks with Bidirectional Pipelines",
            "venue": "In International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Dmitry Lepikhin",
                "HyoukJoong Lee",
                "Yuanzhong Xu",
                "Dehao Chen",
                "Orhan Firat",
                "Yanping Huang",
                "Maxim Krikun",
                "Noam Shazeer",
                "Zhifeng Chen"
            ],
            "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Nan Du",
                "Yanping Huang",
                "Andrew M. Dai",
                "Simon Tong",
                "Dmitry Lepikhin",
                "Yuanzhong Xu",
                "Maxim Krikun",
                "Yanqi Zhou",
                "Adams Wei Yu",
                "Orhan Firat",
                "Barret Zoph",
                "Liam Fedus",
                "Maarten P. Bosma",
                "Zongwei Zhou",
                "Tao Wang",
                "Yu Emma Wang",
                "Kellie Webster",
                "Marie Pellat",
                "Kevin Robinson",
                "Kathleen S. Meier-Hellstern",
                "Toju Duke",
                "Lucas Dixon",
                "Kun Zhang",
                "Quoc V. Le",
                "Yonghui Wu",
                "Zhifeng Chen",
                "Claire Cui"
            ],
            "title": "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Shen Li",
                "Yanli Zhao",
                "Rohan Varma",
                "Omkar Salpekar",
                "Pieter Noordhuis",
                "Teng Li",
                "Adam Paszke",
                "Jeff Smith",
                "Brian Vaughan",
                "Pritam Damania",
                "Soumith Chintala"
            ],
            "title": "PyTorch Distributed: Experiences on Accelerating Data Parallel Training",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2020
        },
        {
            "authors": [
                "Jeff Rasley",
                "Samyam Rajbhandari",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters",
            "venue": "In The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Shixiong Zhao",
                "Fanxin Li",
                "Xusheng Chen",
                "Xiuxian Guan",
                "Jianyu Jiang",
                "Dong Huang",
                "Yuhao Qing",
                "Sen Wang",
                "Peng Wang",
                "Gong Zhang",
                "Cheng Li",
                "Ping Luo",
                "Heming Cui"
            ],
            "title": "vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training",
            "venue": "IEEE Transactions on Parallel and Distributed Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhihao Jia",
                "Sina Lin",
                "Charles R. Qi",
                "Alex Aiken"
            ],
            "title": "Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Zhenkun Cai",
                "Xiao Yan",
                "Kaihao Ma",
                "Yidi Wu",
                "Yuzhen Huang",
                "James Cheng",
                "Teng Su",
                "Fan Yu"
            ],
            "title": "TensorOpt: Exploring the Tradeoffs in Distributed DNN Training With Auto-Parallelism",
            "venue": "IEEE Transactions on Parallel and Distributed Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Minjie Wang",
                "Chien-Chin Huang",
                "Jinyang Li"
            ],
            "title": "Supporting Very Large Models using Automatic Dataflow Graph Partitioning",
            "venue": "In Proceedings of the Fourteenth EuroSys Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Zhihao Jia",
                "Matei Zaharia",
                "Alex Aiken"
            ],
            "title": "Beyond Data and Model Parallelism for Deep Neural Networks",
            "venue": "In Machine Learning and Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Schaarschmidt",
                "Dominik Grewe",
                "Dimitrios Vytiniotis",
                "Adam Paszke",
                "Georg Stefan Schmid",
                "Tamara Norman",
                "James Molloy",
                "Jonathan Godwin",
                "Norman Alexander Rink",
                "Vinod Nair",
                "Dan Belov"
            ],
            "title": "Automap: Towards Ergonomic Automated Parallelism for ML",
            "venue": "Models. CoRR,",
            "year": 2021
        },
        {
            "authors": [
                "Yuliang Liu",
                "Shenggui Li",
                "Jiarui Fang",
                "Yanjun Shao",
                "Boyuan Yao",
                "Yang You"
            ],
            "title": "Colossal- Auto: Unified Automation of Parallelization and Activation Checkpoint for Large-scale Models",
            "year": 2023
        },
        {
            "authors": [
                "Chaoyang He",
                "Shen Li",
                "Mahdi Soltanolkotabi",
                "Salman Avestimehr"
            ],
            "title": "PipeTransformer: Automated Elastic Pipelining for Distributed Training of Large-scale Models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jakub Tarnawski",
                "Amar Phanishayee",
                "Nikhil R. Devanur",
                "Divya Mahajan",
                "Fanny Nina Paravecino"
            ],
            "title": "Efficient Algorithms for Device Placement of DNN Graph Operators",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Jakub Tarnawski",
                "Deepak Narayanan",
                "Amar Phanishayee"
            ],
            "title": "Piper: Multidimensional Planner for DNN Parallelization",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Zhuohan Li",
                "Hao Zhang",
                "Yonghao Zhuang",
                "Zhifeng Chen",
                "Yanping Huang",
                "Yida Wang",
                "Yuanzhong Xu",
                "Danyang Zhuo",
                "Eric P. Xing",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning",
            "venue": "In USENIX Symposium on Operating Systems Design and Implementation,",
            "year": 2022
        },
        {
            "authors": [
                "Rafael Lazimy"
            ],
            "title": "Mixed-integer Quadratic Programming",
            "venue": "Math. Program.,",
            "year": 1982
        },
        {
            "authors": [
                "Siyu Wang",
                "Yi Rong",
                "Shiqing Fan",
                "Zhen Zheng",
                "Lansong Diao",
                "Guoping Long",
                "Jun Yang",
                "Xiaoyong Liu",
                "Wei Lin"
            ],
            "title": "Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for DNN Workloads",
            "year": 2007
        },
        {
            "authors": [
                "Xupeng Miao",
                "Yujie Wang",
                "Youhe Jiang",
                "Chunan Shi",
                "Xiaonan Nie",
                "Hailin Zhang",
                "Bin Cui"
            ],
            "title": "Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2022
        },
        {
            "authors": [
                "Saeed Rashidi",
                "Matthew Denton",
                "Srinivas Sridharan",
                "Sudarshan Srinivasan",
                "Amoghavarsha Suresh",
                "Jade Nie",
                "Tushar Krishna"
            ],
            "title": "Enabling Compute-Communication Overlap in Distributed Deep Learning Training Platforms",
            "venue": "In International Symposium on Computer Architecture,",
            "year": 2021
        },
        {
            "authors": [
                "Samyam Rajbhandari",
                "Jeff Rasley",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
            "venue": "In International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Deep learning models have been widely used in many applications. For example, BERT [1], GPT-3 [2], and T5 [3] achieved state-of-the-art (SOTA) results on different natural language processing (NLP) tasks. For computer vision (CV), Transformer-like models such as ViT [4] and Swin Transformer [5] deliver excellent accuracy performance upon multiple tasks.\nAt the same time, training deep learning models has been a critical problem troubling the community due to the long training time, especially for those large models with billions of parameters [2]. In order to enhance the training efficiency, researchers propose some manually designed parallel training strategies [6\u20138]. However, selecting, tuning, and combining these strategies require extensive domain knowledge in deep learning models and hardware environments. With the increasing diversity of modern hardware architectures [9, 10] and the rapid development of deep learning models, these manually designed approaches are bringing heavier burdens to developers. Hence, automatic parallelism is introduced to automate the parallel strategy searching for training models.\nThere are two main categories of parallelism in deep learning models: inter-layer parallelism [11\u201318] and intra-layer parallelism [6, 19\u201321]. Inter-layer parallelism partitions the model into disjoint sets on different devices without slicing tensors. Alternatively, intra-layer parallelism partitions tensors in a layer along one or more axes and distributes them across different devices.\nCurrent automatic parallelism techniques focus on optimizing strategies within these two categories. However, they treat these two categories separately. Some methods [22\u201328] overlook potential\n*Equal contribution. \u2020Corresponding author.\nPreprint. Under review.\nar X\niv :2\n30 7.\n16 37\n5v 1\n[ cs\n.L G\n] 3\n1 Ju\nl 2 02\nopportunities for inter- or intra-layer parallelism, the others optimize inter- and intra-layer parallelism hierarchically and sequentially [12, 14, 29\u201332]. As a result, current automatic parallelism techniques often fail to achieve the global optima and instead become trapped in local optima. Therefore, a unified inter- and intra-layer approach is needed to enhance the effectiveness of automatic parallelism.\nThis paper aims to find the optimal parallelism strategy while simultaneously considering inter- and intra-layer parallelism. It enables us to search in a more extensive strategy space where the globally optimal solution lurk. However, unifying inter- and intra-layer parallelism in automatic parallelism brings us two challenges. Firstly, to adopt a unified perspective on the inter- and intra-layer automatic parallelism, we should not formalize them with separate formulations as prior works. Therefore, how can we express these parallelism strategies in a unified formulation? Secondly, previous methods take a long time to obtain the solution with a limited strategy space. Therefore, how can we ensure that the best solution can be obtained in a reasonable time while expanding the strategy space?\nTo solve the above challenges, we propose UniAP. For the first challenge, UniAP adopts the mixed integer quadratic programming (MIQP) [33] to search for the globally optimal parallel strategy automatically. It unifies the inter- and intra-layer automatic parallelism in a single MIQP formulation. For the second challenge, our complexity analysis and experimental results show that UniAP can obtain the globally optimal solution in a significantly shorter time.\nThe contributions of this paper are summarized as follows:\n\u2022 We propose UniAP, the first framework to unify inter- and intra-layer automatic parallelism in model training.\n\u2022 The optimal parallel strategies discovered by UniAP exhibit scalability on training throughput and strategy searching time.\n\u2022 The experimental results show that UniAP speeds up model training on four Transformerlike models by up to 1.70\u00d7 and reduces the strategy searching time by up to 16\u00d7, compared with the SOTA method."
        },
        {
            "heading": "2 Background",
            "text": ""
        },
        {
            "heading": "2.1 Inter- and intra-layer parallelism",
            "text": "In general, there exist two main categories of parallelism strategies for deep learning models: interand intra-layer parallelism. If we want to divide them further, inter-layer parallelism mainly includes pipeline parallelism (PP) in our context. Meanwhile, intra-layer parallelism mainly includes data parallelism (DP), tensor parallelism (TP), and fully sharded data parallelism (FSDP). Most manual and automatic parallelism approaches search for the optimal strategy within these dimensions."
        },
        {
            "heading": "2.2 Manual parallelism",
            "text": "Manual parallelism refers to parallel computing strategies designed and optimized by human experts. Representative methods include Megatron-LM [6], Mesh-TensorFlow [7], and GSPMD [8]. MegatronLM is a high-performance computing library for parallel Transformer training. It exhibits superior efficiency in both computing and scaling on clusters. Mesh-TensorFlow and GSPMD require users to annotate the desired intra-layer parallel computing mode. Such methods rely on expert design and manual tuning, challenging their automatic application to other models."
        },
        {
            "heading": "2.3 Automatic parallelism",
            "text": "Inter- or intra-layer-only automatic parallelism For inter-layer-only automatic parallelism, GPipe [11] and vPipe [22] employ a balanced partition algorithm and a dynamic layer partitioning middleware to partition pipelines, respectively. The parallel strategies they generate could be more optimal because both algorithms are greedy. For intra-layer-only automatic parallelism, OptCNN [23], TensorOpt [24], and Tofu [25] employ dynamic programming methods to solve DP and TP together. Meanwhile, FlexFlow [26] and Automap [27] use a Monte Carlo approach to find the parallel execution plan. Colossal-Auto [28] utilizes integer programming techniques to generate strategies for intra-layer parallelism. These methods explore a more limited strategy space for automatic parallelism and do not produce a globally optimal solution.\nInter- and intra-layer automatic parallelism Auto-MAP [34] presents a Deep Q-Network (DQN) for DP-only, TP-only, and PP-only strategy searching, which requires relatively high model training time. PipeDream [12], DAPPLE [14], and PipeTransformer [29] use pure dynamic programming to determine optimal strategies for both DP and PP. DNN-partitioning [30] adopts integer and dynamic programming to explore DP and PP strategies. All of these approaches neglect potential optimization opportunities in TP. Piper [31] and Alpa [32] adopt a hierarchical approach to automatic parallelism, considering DP, TP, and PP. The main difference is that Piper searches for strategies in layer granularity, while Alpa searches for operator granularity. This perspective produces locally nearoptimal solutions rather than globally optimal ones. Galvatron [35] uses pure dynamic programming to determine DP, TP, and FSDP strategies on a single pipeline stage. As for PP, it partitions stages and determines micro-batch size using naive greedy algorithms. Compared with them, UniAP holds the most extensive search space for PP, DP, TP, and FSDP."
        },
        {
            "heading": "3 Method",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "As shown in Figure 1, UniAP first profiles multiple kinds of runtime information, including communication efficiency, forward computation time, memory consumption, and computation-communication overlap coefficient [35, 36] in a simulated training process.\nSecond, UniAP estimates inter- and intra-layer costs given the deep learning model and profiling results with its cost models. Third, UniAP transforms the estimated costs and the computation graph into a MIQP problem. The objective is to maximize the training throughput, or in other words, to minimize the time-per-iteration (TPI). During its optimization process, the off-the-shelf MIQP solver will guarantee optimality. By repeatedly applying the cost model and MIQP solver with varying pipeline stages and chunks, UniAP will obtain the globally minimal TPI and its corresponding parallel strategies. We name this process the unified optimization process (UOP). Finally, UniAP interprets the solution into the execution plan for the whole model."
        },
        {
            "heading": "3.2 Strategy space",
            "text": "Pipeline parallelism In PP, each worker holds a disjoint set of model layers. Adjacent layers on different workers need to transfer activations in the forward propagation (FP) step and gradients in the backward propagation (BP) step. UniAP focuses on synchronous PP, which performs weight updating in each stage at the end of each iteration.\nData parallelism In DP, each worker holds a replica of the model and uniformly partitioned training samples. In each iteration during training, each worker computes its local parameter updates (i.e., gradients) and synchronizes them with the other workers using an all-reduce communication primitive. All workers will observe the same model parameters after the synchronization step.\nTensor parallelism In TP, each worker holds a partition of the model and a replica of training samples. In each iteration, each worker computes its local output in FP and its local gradients in BP.\n\ud835\udc46\ud835\udc46\nFigure 3: A contiguous set S.\nIf the tensor is sliced uniformly, all workers will perform the same collective communication (CC), such as all-reduce in FP and BP steps.\nFully sharded data parallelism The FSDP approach involves partitioning optimizer states, parameters, and gradients of the model into separate workers. During the FP and BP step of each iteration, FSDP performs an all-gather CC to obtain the complete parameters for the relevant layer, respectively. Following the computation of gradients, FSDP conducts a reduce-scatter CC to distribute the global gradients among the workers."
        },
        {
            "heading": "3.3 Cost model",
            "text": "UniAP employs two primary cost models: the time cost model and the memory cost model. To model the computation time, UniAP first multiplies the batch size and the forward computation time per sample obtained from profiling to estimate the forward computation time. For Transformerlike models that mainly consist of the MatMul operator, the computation time in the BP stages is roughly twice that of the FP stages. Additionally, UniAP estimates the communication time by dividing the size of transmitting tensors by the interconnect bandwidth. To account for computation and communication overlapping, UniAP employs the profiled computation-communication overlap coefficient [35, 36]. Notably, UniAP does not average possible P2P communication costs by the number of layers and adds them to each layer\u2019s intra-layer cost, as in the previous work [35]. Instead, UniAP independently models P2P communication between consecutive computation stages as the cross-stage cost.\nIn addition to the time cost model, UniAP estimates the memory consumption in GPUs by multiplying the tensor\u2019s shape and data type for the memory cost model. Furthermore, the memory cost model considers the context memory and the activation memory. Overall, the cost models employed by UniAP strike a balance between complexity and accuracy."
        },
        {
            "heading": "3.4 Mixed integer quadratic programming",
            "text": "This section describes our MIQP expression in terms of a formulation-oriented approach.\nFirst, we need to model the objective function. In UniAP, we choose the GPipe-style pipeline for simplicity without losing generality. Therefore, the objective function of the MIQP problem is minimizing TPI in GPipe, which is equivalent to maximizing the training throughput. Figure 2 depicts a typical GPipe scheduling process that incurs a non-negligible communication overhead.\nWe denote the cost for computation stages as p1, . . . , pps and the cost for communication stages as o1, . . . , oos. Here ps represents the number of computation stages in the pipeline, while os represents the number of communication stages. We refer to these two pipeline stages collectively as the virtual pipeline stage. fpi and bpi means forward and backward computation time for computation stage i, respectively. Meanwhile, foj and boj means forward and backward communication time for communication stage j, respectively. Hence, we have pi = fpi + bpi and oj = foj + boj .\nIn a GPipe-style pipeline, we denote c as the number of chunks. As visualized in Figure 2, a minibatch is uniformly split into three chunks, and the total minimum TPI is determined by the latency of all virtual pipeline stages and the slowest stage. Given that 1) a stage with a higher FP computation cost leads to a higher BP computation cost with high probability; 2) time cost in computation stages\nis usually higher than that in communication stages in modern hardware, we could summarize the TPI of GPipe-style pipeline as follows:\ntpi = ps\u2211 i=1 pi + os\u2211 j=1 oj +max{p1, . . . , pps} \u2217 (c\u2212 1). (1)\nSubsequently, we contemplate which aspects should be considered in the constraints of the MIQP expression. We list our main thoughts below:\n1. In order to determine the total overhead for a single computation stage i, it is necessary to aggregate all computation and communication costs associated with that stage and assign them to pi;\n2. To calculate the total overhead for a single communication stage j, we should sum the P2P costs incurred between consecutive stages and assign them to oj ;\n3. We should guarantee that no GPUs will encounter out-of-memory (OOM) exceptions; 4. The computation graph of the model must be partitioned into contiguous subgraphs to\nprevent disordered assignment to different pipeline stages.\nAmong them, the last point might be the most ambiguous one. We further explain it here. Typically, we can represent a deep learning model as a directed acyclic graph (DAG), namely G(V,E). Here, V represents all layers in the model, while E represents all edge connections between these layers. We borrow the definition of contiguity from [30, 31]. Definition 1. A set S \u2286 V is contiguous if there do not exist nodes u \u2208 S, v \u2208 V \\ S, and w \u2208 S such that v is reachable from u and w is reachable from v.\nAs Figure 3 illustrates, we cannot find any reachable node pairs \u27e8u, v\u27e9 and \u27e8v, w\u27e9 where u,w \u2208 S and v \u2208 V \\ S. Therefore, the set S is considered contiguous. In our scenario, our model will not be assigned to different pipeline stages in a disordered fashion if we make sure that all subgraphs on each computation stage are contiguous.\nBased on the above considerations, the MIQP for the unified automatic parallelism can be formulated as follows:\nmin tpi = ps\u2211 i=1 pi + os\u2211 j=1 oj +max{p1, . . . , pps}(c\u2212 1), (MIQP)\ns.t. \u2211 u\u2208V PuiS T uAu + \u2211 \u27e8u,v\u27e9\u2208E\nPuiPvi(S T uRuvSv) = pi, \u2200i \u2208 {1, . . . , ps}, (2)\u2211\n\u27e8u,v\u27e9\u2208E\nPujPv(j+1)(S T uR \u2032 uvSv) = oj , \u2200j \u2208 {1, . . . , os}, (3)\u2211 u\u2208V PuiS T uMu \u2a7d m, \u2200i \u2208 {1, . . . , ps}, (4)\nVi = {\u2200u \u2208 V : Pui = 1} is contiguous, \u2200i \u2208 {1, . . . , ps}, (5) ps\u2211 i=1\nPui = 1, \u2200u \u2208 V, (6)\u2211 u\u2208V Pui \u2a7e 1, \u2200i \u2208 {1, . . . , ps}, (7) |gu|\u2211 k=1 Suk = 1, \u2200u \u2208 V, (8)\nPui \u2208 {0, 1}, \u2200u \u2208 V, i \u2208 {1, . . . , ps}, (9) Suk \u2208 {0, 1}, \u2200u \u2208 V, k \u2208 {1, . . . , |gu|}. (10)\nFor a given layer u \u2208 V , we utilize the following notations: gu represents its set of intra-layer parallel strategies, Auk denotes the k-th intra-layer execution cost obtained from our time cost model, and\nMuk denotes the k-th intra-layer memory cost on a single device obtained from our memory cost model. Additionally, we use Suk as a 0-1 variable indicating whether the k-th parallel strategy is selected for the layer u, and Pui as a 0-1 variable indicating whether layer u is to be placed on the i-th computation stage. It is important to note that since os = ps\u2212 1, the expression PujPv(j+1) will not result in an out-of-bounds exception. Each edge \u27e8u, v\u27e9 \u2208 E is assigned a resharding cost denoted by Ruv if the vertices are located within the same pipeline stage. Alternatively, if the vertices are located across consecutive stages, the resharding cost between them is denoted by R\u2032uv. These two resharding costs are constant matrices derived from our time cost model.\nWe explain the constraints as follows:\n\u2022 Constraint (2) encodes the summation of intra-stage computation and communication costs as pi. The first term of the polynomial represents the cost of choosing some particular intra-layer strategies for layers placed in stage i. The second term represents total resharding costs in stage i. Thus, this constraint term formalizes the first point of our thoughts.\n\u2022 Constraint (3) encodes the inter-stage communication cost between consecutive computation stages as oj . This term formalizes the second point of our thoughts.\n\u2022 Constraint (4) formalizes the third point of our thoughts with a memory limit of m for each device. Assuming that all computing devices are homogeneous, the value of m is a scalar that remains constant across them.\n\u2022 Constraint (5) represents the last point of our thoughts. It is worth noting that we can formulate this constraint as a set of linear constraints as follows. Intuitively, Zvi = 1 if there exists a node w \u2208 S reachable from v. Otherwise, Zvi = 0. Please refer to Appendix A in the supplementary material for formal proofs.\nZvi \u2a7e Pvi, \u2200v \u2208 V, \u2200i \u2208 {1, 2, . . . , ps}, (11) Zvi \u2a7d Zui, \u2200u, v \u2208 V, \u2200\u27e8u, v\u27e9 \u2208 E, \u2200i \u2208 {1, 2, . . . , ps}, (12) Zvi \u2a7d Pvi \u2212 Pui + 1, \u2200u, v \u2208 V, \u2200\u27e8u, v\u27e9 \u2208 E, \u2200i \u2208 {1, 2, . . . , ps}. (13)\n\u2022 Constraints (6), (7) and (9) represent that all layers should be placed on exactly one pipeline stage and at least one layer should be placed on each pipeline stages.\n\u2022 Constraints (8) and (10) represent that each layer should choose exactly one strategy.\nUniAP gets the minimum TPI and all its corresponding parallel strategies by solving the above MIQP expression using an off-the-shelf solver."
        },
        {
            "heading": "3.5 Unified optimization process",
            "text": "In this section, we propose our design for UOP in UniAP. In short, UOP is mainly responsible for invoking the cost model and MIQP algorithms based on the profiling results and the computation graph. It eventually returns the globally optimal strategy and the corresponding TPI.\nFirst, UOP enumerates the pipeline degree deg for the PP strategy. Without losing generality, we assume the number of devices is a power of 2, and these devices are homogeneous. Hence, the enumerated numbers for PP are selected from 1 to the total number of devices exponentially to guarantee load balancing. For each PP degree, UOP determines whether it is equivalent to one. If this is the case, UOP enters a phase that only considers intra-layer parallelism. Several works [28, 32] have adopted quadratic integer programming (QIP) to solve it and achieved promising results. UniAP provides a QIP formulation for intra-layer-only parallelism in Appendix B in the supplementary material.\nIf this is not the case, UOP enumerates the chunk size of the pipeline from 2 to mini-batch size one by one. Such enumeration is for a more straightforward expression of the MIQP problem while keeping the size of the strategy space the same. In this process, UOP uses those chunk sizes divisible by the mini-batch size. This setting is for load balancing among different micro-batches. After that, UOP constructs the cost and waits for the MIQP solver to return the optimal cost and parallelism strategy under the current configuration. Eventually, UOP will return the minimum cost costmin and its corresponding pipeline degree, chunk size, layer placement, and intra-layer strategies.\nAlgorithm 1 Unified Optimization Process Input: Profiling results PR, strategy dictionary SD, mini-batch size B, computation graph G, and the number of GPUs n. Initialization: Optimal cost costmin = \u221e, pipeline degree degmin = 1, chunk size cmin = B, layer placement Pmin = None, and intra-layer strategy Smin = None. Output: Optimal cost costmin, pipeline degree degmin, chunk size cmin, layer placement Pmin, and intra-layer strategy Smin for deg in {1, 2, 4, . . . , n} do\nif deg == 1 then A, R, _, M = ConstructCost(PR, SD[1], G, B); costmin, Pmin, Smin = QIP(A, R, M ); else for c = 2 to B do\nif c \u2224 B then continue; end if Micro-batch size b = B/c; A, R, R\u2032, M = ConstructCost(PR, SD[deg], G, b); cost, P , S = MIQP(A, R, R\u2032, M , deg, c); if cost < costmin then costmin, degmin, cmin, Pmin, Smin = cost, deg, c, P , S;\nend if end for\nend if end for\nAlgorithm 1 summarizes this process. In the UOP algorithm, we denote intra-layer cost as A, interlayer cost as R, cross-stage cost as R\u2032, and memory cost as M . ConstructCost() process in the algorithm returns these four costs according to the cost model in Section 3.3."
        },
        {
            "heading": "3.6 Complexity analysis",
            "text": "Let |V |, |g|, and n denote the number of layers, parallel strategies, and the number of GPUs, respectively. As illustrated in Alg. 1, UniAP searches all possible pipeline stages exponentially until n is reached. Given a hyperparameter of mini-batch size B, UniAP invokes ConstructCost() to model each layer\u2019s costs for each parallel strategy. Additionally, the optimization time limit of the MIQP solver can be set as a constant hyperparameter when UniAP calls it. Therefore, the overall computational complexity of UniAP is O(|V ||g| log(n))."
        },
        {
            "heading": "4 Experiment",
            "text": "UniAP utilizes the Gurobi Optimizer 10.1 [37] to solve the MIQP problem. We conduct experiments on three kinds of hardware environments. EnvA refers to a node with 1 Xeon(R) Gold 6248 CPU, 8 V100-SXM2 32GB GPUs, and 472GB main memory. EnvB refers to two nodes interconnected with 10Gbps networks, where each node has 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB main memory. EnvC has four nodes, each with the same configuration as that in EnvB. We evaluate UniAP with four Transformer-like models, BERT-Huge [1], T5-Large [3], ViT-Huge [4], and Swin-Huge [5] with different mini-batch sizes. Overall, we follow the common practice of training these Transformer-like models. Our experimental evaluation focuses on two primary metrics: training throughput and strategy searching time. The former metric is computed by averaging throughput from the 10th to the 60th iteration of training, while the latter is determined by measuring the time of the UOP. Further elaborations on these experimental settings are available in Appendix C in the supplementary material."
        },
        {
            "heading": "4.1 Training throughput and strategy searching time",
            "text": "We compare the throughput of the optimal parallel strategy and UniAP\u2019s strategy searching time with the baseline approach. For experiments conducted on EnvA, we select 32, 16, 128, and 128 as the mini-batch size for BERT, T5, ViT, and Swin, respectively. As for experiments conducted on EnvB, we set the mini-batch size as 16, 8, 64, and 32 for these four models. We choose Galvatron [35] as our baseline because it has been identified as the SOTA method. In particular, Galvatron has outperformed existing methods, including PyTorch DDP [19], Megatron-LM [6], FSDP [21, 38], GPipe [11], and DeepSpeed 3D [39], in terms of training throughput, as reported in the original publication [35].\nTable 1 and Table 2 show our results on EnvA and EnvB, respectively. The number immediately following the model\u2019s name in each table represents the number of hidden layers in the corresponding model. For example, ViT-Huge-32 represents a ViT-Huge model with 32 hidden layers. Besides, OOM denotes out-of-memory exceptions during our training process.\nOn EnvA, UniAP generates the same optimal strategy as Galvatron on BERT-Huge-32, T5-Large-48, and ViT-Huge-32 with a significantly shorter time. On Swin-Huge-48, UniAP finds a solution while Galvatron encounters OOM. Specifically, UniAP achieves a search speed that is 16\u00d7 faster than Galvatron on BERT-Huge-32. On EnvB, UniAP uses about half of the search time to find the optimal parallel strategies with an average speedup of 1.47\u00d7 and a maximum speedup of 1.70\u00d7. Overall, UniAP gains significant advantages from broadening its strategy space by unifying interand intra-layer automatic parallelism and using MIQP.\nFurthermore, we can observe significant speedup differences between these two environments. For instance, UniAP cannot identify a parallel strategy with higher throughput on EnvA but is able to do so on EnvB. This phenomenon may be due to the higher interconnect bandwidth on EnvA than EnvB. Given the observation [31, 32, 35] that the communication volume for inter-layer parallelism is much smaller than that for intra-layer, both frameworks present a pure intra-layer parallelism strategy on EnvA to maximize the bandwidth utilization ratio. However, tests on EnvB always generate an interand intra-layer or an inter-layer-only parallelism strategy. UniAP, in this case, will identify a better parallel strategy because of its larger strategy space.\nMeanwhile, UniAP obtains a higher search time speedup on EnvA than EnvB. The main reasons for this phenomenon are as follows: 1) Galvatron takes a longer time to enumerate larger memory size in its dynamic programming algorithm on EnvA; 2) UniAP may take longer to confirm that the parallel strategies will fit into the tighter memory bounds on EnvB. We can easily find evidence supporting the first reason in Galvatron [35]. The second reason can be verified by the strategy searching time of BERT-Huge-32, ViT-Huge-32, and Swin-Huge-48. More specifically, although a smaller batch size leads to a more limited strategy space, UniAP takes longer to find the optima on EnvB than on EnvA."
        },
        {
            "heading": "4.2 Scalability",
            "text": "In this section, we conduct a scalability study on EnvC for UniAP. As Figure 4 shows, the training throughput of the optimal strategy and its strategy searching time on EnvC exhibits near-linearity in a real-world system as the number of nodes and mini-batch size increase exponentially. This phenomenon verifies the computational complexity analysis in Section 3.6."
        },
        {
            "heading": "4.3 Ablation study",
            "text": "In this section, we study the importance of strategy space on the optimality of parallel strategies. Specifically, we reduce the strategy space to inter-layer-only and intra-layer-only strategies in UniAP and evaluate the training throughput of the resulting optimal strategy on EnvB. We set the mini-batch size as 16, 12, 64, and 32, respectively. Table 3 shows that constraining the strategy space can compromise the optimality of parallel strategies or provide strategies that encounter OOM across different models. Therefore, holding a unified view of the automatic parallelism problem is essential."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose UniAP, the first framework to conduct a unified strategy searching in interand intra-layer parallelism strategy space. Our experimental results show that UniAP speeds up model training on four Transformer-like models by up to 1.70\u00d7 and reduces the strategy searching time by up to 16\u00d7. Moreover, the optimal parallel strategies discovered by UniAP exhibit scalability on training throughput and strategy searching time."
        },
        {
            "heading": "A Proof of the linear form for the contiguous set",
            "text": "To facilitate our discussion, we adopt the linear form of the contiguous constraint as presented in the main paper. We denote Pui as a 0-1 variable indicating whether layer u is to be placed on the i-th computation stage, ps as the number of computation stages in the pipeline. Besides, G(V,E) represents the computation graph for the model. Then, we formalize the theorem as follows: Theorem 1. A subgraph Vi = {\u2200u \u2208 V : Pui = 1} is contiguous if and only if there exists Zvi such that constraints (11), (12), and (13) are satisfied.\nPrevious work [30] has proven this theorem. Our proof draws on the process of this work. The details of the proof are as follows: Proof 1. \"If\": Assume that there exists nodes u,w \u2208 Vi and v /\u2208 Vi such that v and w are reachable from u and v, respectively. Hence, Pui = 1, Pwi = 1, and Pvi = 0. Without losing generality, we assume \u27e8u, v\u27e9 \u2208 E. Thus, according to constraint (13), we have Zvi \u2a7d Pvi \u2212 Pui + 1 = 0. By applying constraint (12) repeatedly following the path from v to w, we have Zwi \u2a7d Zvi. Thus, Zwi \u2a7d 0. However, we also have Zwi \u2a7e Pwi = 1 according to constraint (11). A contradiction.\n\"Only if\": First, we define Zvi = 1 if a node w \u2208 S is reachable from v. Otherwise, Zvi = 0. Thus, constraints (11) and (12) are satisfied according to this kind of definition. For constraint (13), if Pvi = 1, the constraint will hold true regardless of whether Pui is 1 or 0. If Pvi = 0 and Pui = 0, Zvi \u2a7d Pvi \u2212 Pui + 1 = 1 will also hold true because Zvi could be either 0 or 1. Finally, if Pvi = 0 and Pui = 1, Zvi = 0 will hold true because Vi is a contiguous set and we couldn\u2019t find any w \u2208 Vi, such that w is reachable from v."
        },
        {
            "heading": "B QIP formulation for intra-layer-only parallelism",
            "text": "Here we present the QIP formulation for intra-layer-only parallelism with explanations.\nmin tpi = p1, (QIP) s.t. \u2211 u\u2208V STuAu + \u2211 \u27e8u,v\u27e9\u2208E\nSTuRuvSv = p1, (14)\u2211 u\u2208V STuMu \u2a7d m, (15) |gu|\u2211 k=1 Suk = 1, \u2200u \u2208 V, (16)\nSuk \u2208 {0, 1}, \u2200u \u2208 V, k \u2208 {1, . . . , |gu|}. (17)\nThe objective function (QIP) tends to minimize the TPI, thereby maximizing training throughput. This function solely takes the value of p1 into account, as there is only one computation stage involved in the intra-layer-only parallelism. Subsequently, we proceed to explain the constraints of this formulation:\n\u2022 Constraint (14) encodes the intra-layer-only computation and communication costs as p1. The first summation term for any u \u2208 V represents the cost of choosing intra-layer strategies for all layers. The second term represents the summation of resharding costs on all edges.\n\u2022 Constraint (15) encodes that the memory consumption on a single device should not exceed its device memory bound m. It is worth noting that m should be an identical constant across multiple devices because these devices are homogeneous.\n\u2022 Constraints (16) and (17) indicate that each layer should select exactly one strategy."
        },
        {
            "heading": "C Experiment detail",
            "text": "Gurobi configuration When tackling the MIQP problem, UniAP employs several configurations for the Gurobi Optimizer [37]. In particular, we set TimeLimit to 60 seconds, MIPFocus to 1, NumericFocus to 1, and remain other configurations to default. Furthermore, we have implemented\nan early stopping mechanism to terminate the optimization process as early as possible. There are two conditions that can activate the mechanism. Firstly, if the current runtime exceeds 15 seconds and the relative MIP optimality gap is less than 4%, we will terminate the optimization. Secondly, if the current runtime exceeds 5 seconds and the best objective bound is worse than the optimal solution obtained in the previous optimization process, we will terminate the optimization.\nModel detail Table 4 presents the details of four Transformer-like models selected for our evaluations. Two of these models, namely BERT-Huge [1] and T5-Large [3], belong to the domain of natural language processing (NLP). At the same time, the remaining two, ViT-Huge [4] and Swin-Huge [5], are associated with computer vision (CV). It is noteworthy that BERT-Huge and ViT-Huge share the same hidden layer type, whereas T5-Large and Swin-Huge have multiple layer types. Numbers separated by slashes represent the statistical information for different layer types. For instance, Swin-Huge comprises four types of layers, each with 2, 2, 42, and 2 layers, respectively.\nTraining detail UniAP is based on the PyTorch framework and integrates models from HuggingFace Transformers. It employs various types of parallelism, including Pipeline Parallelism (PP), Data Parallelism (DP), Tensor Parallelism (TP), and Fully Sharded Data Parallelism (FSDP), utilizing GPipe [11], PyTorch DDP [19], Megatron-LM [6], and FairScale [21], respectively. For NLP models, we use the English Wikipedia dataset [40], while the ImageNet-1K dataset [41] is used for CV models. We train these models using the Adam optimizer [42] and precision of FP32. We omit hyperparameters here such as learning rate and weight decay as these have minimal impact on training throughput. The model parameters in the HuggingFace Transformer are configured to align with the specifications of each individual model. For instance, we set hidden_size to 1280, num_hidden_layers to 32, num_attention_heads to 16, and seq_length to 512 for BERT-Huge. Regarding other hyperparameters in the HuggingFace configurations, we set hidden_dropout_prob and attention_probs_dropout_prob to 0.0 for ViT-Huge. For Swin-Huge, we set drop_path_rate to 0.2. We remain other configurations to default. It should be noted that the training batch sizes for each experiment are outlined in the main paper."
        },
        {
            "heading": "D Case study: BERT-Huge",
            "text": "In this section, we present a visualization of the optimal parallelism strategy discovered by UniAP. As represented in Figure 5, the strategy pertains to training BERT-Huge with 32 hidden layers in a 2-node environment EnvB with a mini-batch size of 16. Each node was equipped with 2 Xeon E5-2620 v4 CPUs, 4 TITAN Xp 12GB GPUs, and 125GB of primary memory. These nodes are interconnected via a 10Gbps network. It should be noted that we only showcase the parallelism strategy for the hidden layers here for simplicity but without losing generality.\nHere, we provide further topological information for a node within EnvB. As illustrated in Figure 6, we categorize the GPUs numbered 0 and 1 in each node and refer to them collectively as GPUGroup0. Similarly, we label the GPUs numbered 2 and 3 as GPUGroup1. In EnvB, the interconnects within each GPU group (i.e., PCIe) have superior bandwidth than that between different groups (i.e., QPI). We collectively designate these two connection bandwidths as intra-node bandwidth, which is higher than inter-node bandwidth.\nIn this example, UniAP has identified a parallelism strategy for inter-layer parallelism that involves a two-stage pipeline. This strategy utilizes parallelism in a manner that is both efficient and effective. Specifically, the communication cost of point-to-point (P2P) between two nodes is less than that of all-reduce. Additionally, the inter-node bandwidth is lower than that of the intra-node. These factors make the two-stage PP approach a reasonable choice. Moreover, the pipeline has been designed such\nthat each stage comprises an equal number of layers. This design leverages the homogeneity of the nodes and ensures load balancing across the cluster.\nUniAP employs an intra-layer parallelism strategy within each PP stage. It utilizes a 2-way DP for the initial 12 hidden layers in each stage between GPUGroup0 and GPUGroup1. For the remaining four hidden layers, a 2-way FSDP is utilized between GPUGroup0 and GPUGroup1 to reduce memory footprint and meet memory constraints. Within each GPU group, UniAP employs a 2-way TP for each layer. In general, TP incurs more significant communication volumes than DP and FSDP. In order to achieve maximum training throughput on EnvB, it is necessary to implement parallelism strategies that prioritize higher communication volumes within each group and lower volumes between groups. Therefore, the strategy for BERT-Huge with 32 hidden layers combines the best elements of PP, DP, TP, and FSDP to maximize training throughput."
        }
    ],
    "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
    "year": 2023
}