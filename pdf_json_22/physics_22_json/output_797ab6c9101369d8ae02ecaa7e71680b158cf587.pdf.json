{
    "abstractText": "Performance of model\u2013based feedforward controllers is typically limited by the accuracy of the inverse system dynamics model. Physics\u2013guided neural networks (PGNN), where a known physical model cooperates in parallel with a neural network, were recently proposed as a method to achieve high accuracy of the identified inverse dynamics. However, the flexible nature of neural networks can create overparameterization when employed in parallel with a physical model, which results in a parameter drift during training. This drift may result in parameters of the physical model not corresponding to their physical values, which increases vulnerability of the PGNN to operating conditions not present in the training data. To address this problem, this paper proposes a regularization method via identified physical parameters, in combination with an optimized training initialization that improves training convergence. The regularized PGNN framework is validated on a real\u2013life industrial linear motor, where it delivers better tracking accuracy and extrapolation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Max Bolderman"
        },
        {
            "affiliations": [],
            "name": "Mircea Lazar"
        },
        {
            "affiliations": [],
            "name": "Hans Butler"
        }
    ],
    "id": "SP:f47241e0737f762bc9e997923982681bb8e04990",
    "references": [
        {
            "authors": [
                "M.L.G. Boerlage",
                "M. Steinbuch",
                "P.F. Lambrechts",
                "M.M.J. van de Wal"
            ],
            "title": "Model\u2013based feedforward for motion systems",
            "venue": "IEEE International Conference on Control Applications, vol. 2, pp. 1158\u2013 1163, 2003.",
            "year": 2003
        },
        {
            "authors": [
                "J.A. Butterworth",
                "L.Y. Pao",
                "D.Y. Abramovitch"
            ],
            "title": "Analysis and comparison of three discrete\u2013time feedforward model\u2013inverse control techniques for nonminimum\u2013phase systems",
            "venue": "Mechatronics\u201d, vol. 22, no. 5, pp. 577\u2013587, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "Y.H. Yuen",
                "M. Lazar",
                "H. Butler"
            ],
            "title": "Data-driven neural feedforward controller design for industrial linear motors",
            "venue": "23rd International Conference on System Theory, Control and Computing, pp. 461\u2013467, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "J. van Zundert",
                "T. Oomen"
            ],
            "title": "On inversion-based approaches for feedforward and ILC",
            "venue": "Mechatronics, vol. 50, pp. 282\u2013291, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "K. Igarashi",
                "R. Igarashi",
                "T. Atsumi",
                "S. Nakadai"
            ],
            "title": "Feedforward control for track\u2013seeking control in hard disk drive with sampled\u2013data polynomial based on first\u2013order hold",
            "venue": "IEEE International Conference on Mechatronics, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Steinbuch",
                "R.M.J.G. van de Molengraft"
            ],
            "title": "Iterative learning control of industrial motion systems",
            "venue": "IFAC Proceedings Volumes, vol. 33, no. 26, pp. 899\u2013904, 2000.",
            "year": 2000
        },
        {
            "authors": [
                "S. Devasia"
            ],
            "title": "Should model\u2013based inverse inputs be used as feedforward under plant uncertainty?",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2002
        },
        {
            "authors": [
                "O. S\u00f8rensen"
            ],
            "title": "Additive feedforward control with neural networks",
            "venue": "IFAC Proceedings Volumes, vol. 32, no. 2, pp. 1378\u20131383, 1999.",
            "year": 1999
        },
        {
            "authors": [
                "K. Hornik",
                "M.B. Stinchcombe",
                "H. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks, vol. 2, no. 5, pp. 359\u2013366, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of Control, Signals and Systems, vol. 2, pp. 303\u2013 314, 1989.",
            "year": 1989
        },
        {
            "authors": [
                "G. Otten",
                "T.J.A. de Vries",
                "J. van Amerongen",
                "A.M. Rankers",
                "E.W. Gaal"
            ],
            "title": "Linear motor motion control using a learning feedforward controller",
            "venue": "IEEE Transactions on Mechatronics, vol. 2, no. 3, pp. 179\u2013 187, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "X. Ren",
                "Y.L. Chow",
                "V. Venkataramanan",
                "F.L. Lewis"
            ],
            "title": "Feedforward control based on neural networks for disturbance rejection in hard disk drives",
            "venue": "IET Control Theory & Applications, vol. 3, no. 4, pp. 411\u2013418, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "G. Li",
                "J. Na",
                "D.P. Stoten",
                "X. Ren"
            ],
            "title": "Adaptive neural network feedforward control for dynamically substructured systems",
            "venue": "IEEE Transactions on Control Systems Technology, vol. 22, no. 3, pp. 944\u2013 954, 2014.",
            "year": 2014
        },
        {
            "authors": [
                "P.J. Haley",
                "D. Soloway"
            ],
            "title": "Extrapolation limitations of multilayer feedforward neural networks",
            "venue": "Proceedings of Internation Joint Conference on Neural Networks, vol. 4, pp. 25\u201330, 1992.",
            "year": 1992
        },
        {
            "authors": [
                "M. Bolderman",
                "M. Lazar",
                "H. Butler"
            ],
            "title": "Physics\u2013guided neural networks for inversion\u2013based feedforward control applied to linear motors",
            "venue": "IEEE Conference on Control Technology and Applications, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "O. Nelles"
            ],
            "title": "Nonlinear System Identification",
            "year": 2001
        },
        {
            "authors": [
                "T. Yasui",
                "A. Moran",
                "M. Hayase"
            ],
            "title": "Integration of linear systems and neural networks for identification and control of nonlinear systems",
            "venue": "IFAC Proceedings Volumes, vol. 30, no. 11, pp. 783\u2013788, 1997.",
            "year": 1997
        },
        {
            "authors": [
                "H. Zabiri",
                "M. Ramasamy",
                "T.D. Lemma",
                "A. Maulud"
            ],
            "title": "Nonlinear system identification using integrated linear\u2013nn models: serives vs. parallel structures",
            "venue": "International Conference on Modeling, Simulation, and Control, vol. 10, pp. 33\u201337, 2011.",
            "year": 2011
        },
        {
            "authors": [
                "M. Schoukens",
                "R. T\u00f3th"
            ],
            "title": "On the initialization of nonlinear LFR model identification with the best linear approximation",
            "venue": "IFAC PapersOnline, vol. 53, no. 2, pp. 310\u2013315, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "A. Karpatne",
                "W. Watkins",
                "J. Read",
                "V. Kumar"
            ],
            "title": "Physics-guided neural networks (PGNN): An application in lake temperature modeling",
            "venue": "arXiv preprint arXiv:1710.11431, 2017.",
            "year": 2017
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G.E. Karniadakis"
            ],
            "title": "Physics\u2013informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "Journal of Computation Physics, vol. 378, pp. 686\u2013707, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "R. Pintelon",
                "J. Schoukens"
            ],
            "title": "System identification: A frequency domain approach",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Index Terms\u2014 Neural networks, Feedforward control, Nonlinear system identification, Motion control, Linear motors.\nI. INTRODUCTION Inversion\u2013based feedforward is a control method that improves reference tracking of dynamical systems by generating a control input based on a model of the inverse system dynamics, see, for example, [1]\u2013[5]. Typically, linear or linear\u2013in\u2013the parameters physical models are used to parametrize the inverse system model [3], [6]. These types of model sets, however, are not able to approximate parasitic nonlinear forces that are always present in mechatronics, which significantly limits the achievable performance [7].\nWith the aim to approximate and compensate for nonlinear effects, neural networks (NN) have been originally proposed in [8] as inverse system parametrizations. NNs are known as universal function approximators [9], [10], and are therefore considered capable of identifying the complete inverse dynamics under suitable assumptions. Although, there exists some examples of successful implementation of NNs in feedforward control [11]\u2013[13], the lack of physical interpretability and poor extrapolation capabilities [14] hinders adoption of NN\u2013based controllers in industry.\nTo combine the benefits of physical models and neural networks, [15] introduced physics\u2013guided neural networks\n*This work is part of the research programme 9654 with project number 17973, which is (partly) financed by the Dutch Research Council (NWO).\n1Control Systems Group, department of Electrical Engineering, Eindhoven University of Technology, The Netherlands.\n2ASML, Veldhoven, The Netherlands. Email: m.bolderman@tue.nl, m.lazar@tue.nl, and\nh.butler@asml.nl\n(PGNN) for feedforward control. The PGNN in [15] employs a parallel model structure that combines the output of a known, not necessarily linear, physics\u2013guided part, with the output of an unknown, possibly nonlinear part that is parameterized by a black\u2013box NN. Both the physics\u2013guided and the NN part are embedded in a single model structure, which allows the PGNN to be identified as a single model. The considered model structure is in line with earlier parallel model structures utilized in nonlinear system identification, see, e.g., [16]\u2013[18], which typically combine a nonlinear/NN model with a linear model. In [16], two standard approaches for identifying the parameters of such hybrid parallel models are distinguished:\n1) Simultaneous identification of all parameters; 2) Sequential (or consecutive) identification: first inden-\ntify the parameters of the physics\u2013guided layer and then train the NN using the residuals of the physcs\u2013 guided layer.\nIn principle, the first approach yields the best data fit and it was also used in [15] to train a feedforward controller. However, this approach can suffer from overaparameterization caused by the flexible nature of the NN that is capable of identifying also parts of the parallel physical model. This creates a parameter drift during training that may yield physically inconsistent parameter values for the physics\u2013 guided layer and poor extrapolation capabilities caused by the large NN outputs [14]. The second approach, used for example in [17], [18], preserves the physical model parameters in the physics\u2013guided layer, which can be beneficial for extrapolation outside the training data set. However, approximation accuracy can be reduced because the training has less free parameters to optimize simultaneously. The sequential identification was shown in [16] to act as a regularization.\nIn this paper we propose the following contributions to the state of the art described above. First, we define a regularized cost function which penalizes, besides the standard data fit, the deviation of the parameters of the physics\u2013guided layer with respect to some known/identified physical parameters. The known physical parameters can be obtained based on physical insights or via identification using a linear\u2013in\u2013 the\u2013parameters model (LIP). Second, we develop a novel initialization procedure for training of parallel PGNNs and we prove that it guarantees an improvement in accuracy of the data fitting with respect to the original LIP model. This is in line with recent initialization procedures for nonlinear system identification based on the best linear approximator,\nar X\niv :2\n20 1.\n12 08\n8v 1\n[ cs\n.L G\n] 2\n8 Ja\nn 20\n22\nsee [19] and the references therein. Experimental results on a coreless linear motor used in lithography industry confirm that the developed regularized training cost function and initialization procedure lead to a high performance and improved robustness to non\u2013training data.\nRemark 1.1: An alternative regularization method has been proposed in the physics\u2013informed neural network (PINN) literature, see, e.g., [20]\u2013[22]. These PINNs employ a black\u2013box NN to identify an underlying function based on a cost function that penalizes, besides data fit, compliance of the NN output with the output of an available physical model. The penalty of deviating from the physical model requires the NN to optimize between either data fit or model fit, which results in a biased estimate when the underlying system dynamics deviates from the available physical model. Note that our PGNN design [15] differs by having the physical model as an intrinsic part of the PGNN, whereas the PINNs use the output of physical model only for regularization, i.e. the physical model itself is not embedded in the resulting PINN.\nThe remainder of this paper is organized as follows: Section II introduces the considered system dynamics and PGNN identification setup. The problem statement is formalized in Section III. Section IV describes the regularization\u2013based PGNN cost function, and Section V explains the PGNN training initialization. Afterwards, the developed methodology is tested on an industrial coreless linear motor in Section VI, followed by conclusions in Section VII."
        },
        {
            "heading": "II. PRELIMINARIES",
            "text": "The symbols Z and R denote the set of integers and real numbers, respectively. The set Z+ := {i \u2208 Z | i > 0} denotes the positive integers, and the same notation is used for positive real numbers, i.e., R+ := {i \u2208 R | i > 0}.\nA. Inverse system dynamics and modelling\nConsider the following discrete\u2013time inverse dynamical system\nu(t) = \u03b8T0 Tphy ( \u03c6(t) ) + g ( \u03c6(t) ) , (1)\nwhich describes the relation between the input u(t) and the output y(t), that corresponds to a nonlinear ARX model with a particular structure that is clarified as follows. In (1), \u03c6(t) = [y(t+na), . . . , y(t\u2212nb), u(t\u22121), . . . , u(t\u2212nb)]T is the regressor, with na, nb, nc \u2208 Z+ the orders of the system, and \u03b80 \u2208 Rn\u03b80 is the parameter vector corresponding to the known physical model. Furthermore, Tphy : Rna+nb+nc+1 \u2192 Rn\u03b80 comprises of a set of functions that model the known part of the dynamics, e.g., based on physical insights, and g : Rna+nb+1 \u2192 R is an unknown nonlinear function that contains the unknown dynamics.\nWithin this paper, the following model structures are considered for identification of the inverse system dynamics (1).\nDefinition 1: A linear\u2013in\u2013the\u2013parameters (LIP) model is defined as\nu\u0302 ( \u03b8LIP, \u03c6(t) ) = \u03b8TLIPTphy ( \u03c6(t) ) , (2)\nwhere u\u0302 ( \u03b8LIP, \u03c6(t) ) is the predicted output of the model, and \u03b8LIP \u2208 Rn\u03b80 are the parameters to be identified. Definition 2: A physics\u2013guided neural network (PGNN) is defined as\nu\u0302 ( \u03b8PGNN, \u03c6(t) ) = fNN ( \u03b8NN, \u03c6(t) ) + \u03b8TphyTphy ( \u03c6(t) ) , (3)\nwhere \u03b8PGNN = {\u03b8NN, \u03b8phy} are the PGNN parameters, with \u03b8NN = {W1, B1, . . . ,Wl+1, Bl+1} the neural network weights and biases for each of the l \u2208 Z+ hidden layers, and \u03b8phy \u2208 Rn\u03b80 are the parameters corresponding to the physics\u2013guided part of the PGNN.\nRemark 2.1: The NN output is recursively computed as fNN ( \u03b8NN, \u03c6(t) ) = Wl+1\u03b1l ( . . . \u03b11 ( W1\u03c6(t) +B1 )) +Bl+1, (4) where \u03b1i contains the activation functions that are applied element\u2013wise.\nThe purpose of the PGNN model (3) is to exploit the NN part to identify the inverse dynamics (1) more accurately compared to the LIP model that is derived from physical knowledge (2). A visualization of the PGNN (3), see also [15], is shown in Figure 1.\nB. Inverse system identification using the LIP model\nConsider that an input\u2013output data set\nZN = {u(0), y(0), . . . , u(N \u2212 1), y(N \u2212 1)}, (5)\nis available that is generated by the actual system, i.e., ZN satisfies the inverse system dynamics (1). Definition 3: Two variables xa(t) and xb(t) are uncorrelated if and only if\n1\nN \u2211 t\u2208ZN xa(t)xb(t) = 0. (6)\nLet \u03b8 be a general vector of parameters, e.g., \u03b8LIP or \u03b8PGNN. Then, identification is performed by choosing \u03b8\u0302 as the minimizing argument of a cost function, i.e.,\n\u03b8\u0302 = arg min \u03b8 V ( u\u0302 ( \u03b8, \u03c6(t) ) , ZN ) . (7)\nTypically, the mean\u2013squared error (MSE)\nV ( u\u0302 ( \u03b8, \u03c6(t) ) , ZN ) = 1\nN \u2211 t\u2208ZN ( u(t)\u2212 u\u0302 ( \u03b8, \u03c6(t) ))2 (8)\nis chosen as a cost function. After identification of the parameters \u03b8\u0302, the corresponding feedforward controller is obtained according to the following definition.\nDefinition 4: An inversion based feedforward controller is given as uff(t) = u\u0302 ( \u03b8\u0302, \u03c6ff(t) ) , (9)\nwith uff(t) the feedforward signal, \u03c6ff := [r(t+na), . . . , r(t\u2212 nb), uff(t\u2212 1), . . . , uff(t\u2212 nc)]T , and r(t) the reference.\nSubstitution of the LIP (2) in the identification criterion (7) with MSE cost function (8), gives the identified parameters\n\u03b8\u0302LIP =M \u22121\n[ 1\nN \u2211 t\u2208ZN Tphy ( \u03c6(t) ) u(t)\n] . (10)\nwhere M := 1N \u2211 t\u2208ZN Tphy ( \u03c6(t) ) Tphy ( \u03c6(t) )T . The solution (10) is unique if and only if M is non\u2013singular. Remark 2.2: In the situation that Tphy ( \u03c6(t) ) = \u03c6(t), the identified parameters (10) becomes the best linear approximator (BLA) [19], [23].\nSubstitution of the LIP model (2) with identified parameters (10) into the inverse dynamics (1), gives\nu(t) = \u03b8\u0302TLIPTphy ( \u03c6(t) ) + f ( \u03c6(t) ) , (11)\nwhere f ( \u03c6(t) ) := g ( \u03c6(t) ) + (\u03b80\u2212 \u03b8\u0302LIP)TTphy ( \u03c6(t) ) denotes the unmodelled dynamics. Lemma 2.1: The unknown dynamics g ( \u03c6(t) ) are identi-\ncal to the unmodelled dynamics f ( \u03c6(t) ) , if and only if\nTphy ( \u03c6(t) ) and g ( \u03c6(t) ) are uncorrelated.\nProof: The unknown and unmodelled dynamics are identical if and only if \u03b80 \u2212 \u03b8\u0302LIP = 0. Using the solution \u03b8\u0302LIP (10) and the inverse dynamics (1) gives\n\u03b80 \u2212 \u03b8\u0302LIP = M\u22121M\u03b80 \u2212M\u22121 [ 1\nN \u2211 t\u2208ZN Tphy ( \u03c6(t) ) u(t)\n]\n= M\u22121\n[ 1\nN \u2211 t\u2208ZN Tphy ( \u03c6(t) ) g ( \u03c6(t)\n)] ,\n(12)\nwhich proves Lemma 2.1, since M is non\u2013singular."
        },
        {
            "heading": "III. PROBLEM STATEMENT",
            "text": "A. Inverse system identification using the PGNN model\nIdentification of the inverse system dynamics (1) using the LIP model (2) leaves room for improvement, as there is still the unmodelled dynamics f ( \u03c6(t) ) to be identified. Therefore, it was proposed in [15] to use the PGNN model structure (3) for identification of the inverse system dynamics. Due to the parallel layer structure of the PGNN, if the cost function (8) is used for identification, there is no guarantee that the resulting parameters for the physical layer correspond to the parameters of the identified LIP model (2) of the original system (1). In fact, the NN part of the PGNN (3) can start to compete with the physical model.\nMinimizing the cost (8) solely aims to optimize the data fit of the PGNN and does not attribute specific dynamics to the neural layer versus the physics\u2013guided layer. It would\nbe desirable to steer the training of the PGNN such that the physics\u2013guided layer identifies the known physical dynamics and the NN layer identifies the remaining unmodelled dynamics. This is a well known problem in nonlinear system identification when models with a parallel structure are used, see [16], Chapter 21. Therein, a solution is proposed which is based on first identifying the known physical dynamics, and then training the NN using the residuals. This preserves the known dynamics in the physical part of the PGNN (3), but also limits achievable data fitting accuracy, as the whole set of parameters for the PGNN are not identified simultaneously.\nB. Illustrative example of layer competition\nIn order to illustrate the competition between the NN and physics\u2013guided layer in the PGNN (3), we consider the coreless linear motor (CLM) also used in [15]. A reasonably accurate LIP model can be obtained via first\u2013principle modelling using Newton\u2019s second law, which gives\nu(t) = [ m fv fc fk ]  \u03b42y(t) \u03b4y(t) sign ( \u03b4y(t)\n) y(t) + g(\u03c6(t)), (13) where \u03b4 = 1\u2212q \u22121\nTs is the backward Euler differentiation, with\nq\u22121 the backwards\u2013shift operator, and Ts the sampling time. Also, \u03c6(t) = [y(t), y(t\u2212 1), y(t\u2212 2)]T , and m, fv, fc, fk \u2208 R+ are the mass, viscous friction coefficient, Coulomb friction coefficient, and stiffness coefficient, respectively.\nIdentification using only the known, LIP model in (13) gives \u03b8\u0302LIP = [18.8, 172, 7.21, 1.36 \u00b7 10\u22128]T , whereas the corresponding parameters in the PGNN (13) converged to \u03b8\u0302phy = [18.2, 301,\u22124.99, 1.18 \u00b7 103]T . The results are obtained using a single hidden layer NN with 16 neurons that have tanh(\u00b7) activation functions. This parameter drift during training has two main disadvantages.\nFirst of all, the parameter drift results in an inconsistent physics\u2013guided layer in the PGNN (3). This complicates validation of the PGNN and thereby hinders applicability to safety critical systems. Secondly, extrapolation capabilities of the PGNN to operating conditions not present in the training data are severely limited. This is caused by the large output of the NN layer, and uninterpretable physics\u2013guided layer, as is shown in Figure 2. Although the NN has universal approximation capabilities to identify the dynamics in the domain for which training data is generated, it is well known that its extrapolation capabilites are limited [14]. Therefore, it is desired to have the NN layer output small, while the parameters of the physics\u2013guided layer of the PGNN remain close to the physical parameters."
        },
        {
            "heading": "C. Problem formulation",
            "text": "In order to solve the aforementioned problems, in this paper we propose a regularization\u2013based approach for simultaneously training the PGNN (3). Under the condition that PGNN training converges to a global optimum, the regularized training allow for the following:\n1) When the NN layer can identify the unmodelled dynamics, the physics\u2013guided layer must become the LIP, and the NN must identify the unmodelled dynamics. 2) When the NN cannot identify the unmodelled dynamics, the physics\u2013guided layer parameters can still be partially used to improve data\u2013fit.\nSince training of the PGNN (3) is a non\u2013convex optimization, it is possible that the optimization scheme ends up in a local minimum. Therefore, the second problem considered in this paper is defined as the design of a PGNN parameter initialization method that returns a smaller cost function compared to the LIP model. This property must also be satisfied for the regularized PGNN cost function."
        },
        {
            "heading": "IV. REGULARIZED PGNN TRAINING",
            "text": "In order to have the physical parameters \u03b8phy of the PGNN (3) remain close to the LIP parameters \u03b8\u0302LIP (10), the regularized PGNN cost function is defined.\nDefinition 5: The regularized PGNN training cost function is defined as\nV ( u\u0302 ( \u03b8PGNN, \u03c6(t) ) , ZN ) = 1\nN \u2211 t\u2208ZN ( u(t)\u2212 u\u0302 ( \u03b8PGNN, \u03c6(t) ))2 + (\u03b8phy \u2212 \u03b8\u0302LIP)T\u039b(\u03b8phy \u2212 \u03b8\u0302LIP),\n(14)\nwhere \u039b is a symmetric positive definite matrix that defines the relative weights for penalizing deviation from the LIP.\nRemark 4.1: The regularized PGNN training cost function (14) is fundamentally different from the PINNs regularization\nV ( u\u0302 ( \u03b8NN, \u03c6(t) ) , ZN ) = 1\nN \u2211 t\u2208ZN ( u(t)\u2212 u\u0302 ( \u03b8NN, \u03c6(t) ))2 + \u03bb 1\nN \u2211 t\u2208ZN ( u\u0302 ( \u03b8NN, \u03c6(t) ) \u2212 \u03b8TphyTphy ( \u03c6(t) ))2 ,\n(15)\nwith \u03bb \u2208 R+ the relative weight of the regularization. Training of the PINNs can be done either by optimizing only \u03b8NN and fixing \u03b8phy = \u03b8\u0302LIP, or by simultaneously training {\u03b8NN, \u03b8phy}. Clearly, it is not possibly to have both contributions in (15) become zero when there is unknown dynamics, see g ( \u03c6(t) ) in (1). This implies that the PINNs approach [20]\u2013[22] must optimize between either data fit or physical model compliance. Straightforwardly, this creates a bias in the estimation of \u03b8\u0302NN.\nIt can be proven that the PGNN regularization (14) does not create a bias under the following assumptions.\nAssumption 4.1: The NN in the PGNN (3) is chosen sufficiently rich to identify the unmodelled dynamics, i.e., there exists a \u03b8\u2217NN such that\nfNN ( \u03b8\u2217NN, \u03c6(t) ) = f ( \u03c6(t) ) . (16)\nThis \u03b8\u2217NN is unique up until the level of interchanging neurons in the same layer.\nAssumption 4.2: The data set ZN (5) is persistently exciting with respect to the NN part of the PGNN (3), such that 1\nN \u2211 t\u2208ZN ( fNN ( \u03b8NN, \u03c6(t) ) \u2212 f ( \u03c6(t) ))2 = 0 \u21d2 \u03b8NN = \u03b8\u2217NN.\n(17) Proposition 4.1: Consider the data generating system (1), the PGNN (3), and the identification criterion (7) with regularized PGNN cost function (14). Suppose that Assumptions 4.1 and 4.2 hold, and let \u03b8\u0302LIP be calculated as in (10). Assume that the minimization of the cost function (14) converges to a global optimum. Then, the corresponding optimal parameters satisfy \u03b8\u0302PGNN = {\u03b8\u0302LIP, \u03b8\u2217NN}.\nProof: Writing out the regularized cost function (14) and substituting the PGNN (3) gives\nV ( u\u0302 ( \u03b8PGNN, \u03c6(t) ) , ZN ) =\n1\nN \u2211 t\u2208ZN ( u(t)\u2212 \u03b8TphyTphy ( \u03c6(t) ) \u2212 fNN ( \u03b8NN, \u03c6(t) ))2 +\n(\u03b8Tphy \u2212 \u03b8\u0302LIP)T\u039b(\u03b8phy \u2212 \u03b8\u0302LIP) \u2265 0. (18)\nBoth terms in (18) are non\u2013negative. Correspondingly, the global optimum is attained if \u03b8\u0302phy = \u03b8\u0302LIP (regularization term), and \u03b8\u0302NN = \u03b8\u2217NN (data\u2013fit term, substitute u(t) from (1)).\nRemark 4.2: It is in general difficult to know the required NN dimensions (number of hidden layers, number of neurons per hidden layer) for Assumption 4.1 to be satisfied. In the\nsituation that Assumption 4.1 is violated, the training of the PGNN (3) with regularized cost function (14) allows the physics\u2013guided layer parameters to slightly deviate from the LIP in order to better fit the data."
        },
        {
            "heading": "V. PGNN TRAINING INITIALIZATION",
            "text": "Proposition 4.1 holds under the assumption that training of the PGNN (3) converges to the global minimum of the cost function (14). The non\u2013convexity of the optimization however, requires the use of nonlinear optimization schemes that tend to get stuck in local minima, which strongly depends on the initialized parameters [24]. For this reason, there is a strong interest in finding initial parameter values that already perform well on the cost function (14).\nIn order to illustrate the proposed PGNN parameter initialization, we denote \u03b8(k)PGNN as the value of \u03b8PGNN at the k\u2019th iteration in training, and rewrite the PGNN output (3) as\nu\u0302 ( \u03b8PGNN, \u03c6(t) ) =WTl+1fHL ( \u03b8HL, \u03c6(t) ) +Bl+1 + \u03b8 T phyTphy ( \u03c6(t) ) ,\n(19)\nwhere \u03b8HL := {W1, B1, . . . ,Wl, Bl}, such that \u03b8NN = {\u03b8HL,Wl+1, Bl+1}. Also, fHL : Rna+nb+nc+1 \u2192 Rnl denotes the output of the last hidden layer l \u2208 Z+ that contains nl \u2208 Z+ neurons. In contrary to the majority of NN literature that initializes all the weights and biases \u03b8(0)NN randomly, we only do so for the hidden layer weights and biases \u03b8(0)HL , and define \u03c6(0)OL (t) := [fHL ( \u03b8 (0) HL , \u03c6(t) )T , 1, Tphy ( \u03c6(t) )T ]T . Then, under the following assumption, we prove that the PGNN can be initialized at a lower regularized cost function (14) compared to the LIP model (2).\nAssumption 5.1: The data set ZN (5) is persistently exciting for the output weights and biases of the PGNN (3), i.e.,\nMR := 1\nN \u2211 t\u2208ZN \u03c6 (0) OL (t)\u03c6 (0) OL (t) T + 00 I \u039b 00 I T (20) is non\u2013singular.\nLemma 5.1: Consider the PGNN (3) with parameters \u03b8 (0) HL initialized randomly, and the regularized cost function (14). Suppose that Assumption 5.1 holds. Define \u03b8OL := [WTl+1, Bl+1, \u03b8 T phy] T and initialize as\n\u03b8 (0) OL = M \u22121 R  1 N \u2211 t\u2208ZN u(t)\u03c6 (0) OL (t) + 00 I \u039b\u03b8\u0302LIP  . (21)\nThen, it holds that V ( u\u0302 ( \u03b8 (0) PGNN, \u03c6(t) ) , ZN ) \u2264 V ( u\u0302 ( \u03b8\u0302LIP, \u03c6(t) ) , ZN ) , (22)\nwith strict inequality if and only if\nMR  00 \u03b8\u0302LIP \u2212  1 N \u2211 t\u2208ZN u(t)\u03c6 (0) OL (t) + \u03b8\u0302 T LIP\u039b 00 I T  6= 0.\n(23)\nProof: Rewriting the regularized cost function (14) gives V ( \u03b8TOL\u03c6 (0) OL , Z N ) = 1\nN \u2211 t\u2208ZN u(t)2 + \u03b8\u0302TLIP\u039b\u03b8\u0302LIP + \u03b8 T OLMR\u03b8OL\n\u2212 2\n( 1\nN \u2211 t\u2208ZN u(t)\u03c6 (0) OL T + \u03b8\u0302TLIP\u039b [ 0 0 I\n]) \u03b8OL,\n(24)\nthat has the global optimum \u03b8(0)OL in (21), which is unique due to Assumption 5.1. Correspondingly, we can lowerbound the regularized cost function (14) according to\nV ( u\u0302 ( \u03b8 (0) PGNN, \u03c6(t) ) , ZN ) = V ( \u03b8 (0) OL T \u03c6 (0) OL (t), Z N ) \u2264 V ([ 0 0 \u03b8\u0302LIP ]T \u03c6 (0) OL (t), Z N )\n= V ( u\u0302 ( \u03b8\u0302LIP, \u03c6(t) ) , ZN ) .\n(25)\nIn (25), the inequality holds with equality if and only if \u2202V ( \u03b8TOL\u03c6 (0) OL (t),Z N )\n\u2202\u03b8OL\n\u2223\u2223 \u03b8OL=[0,0,\u03b8\u0302TLIP]\nT = 0. If (23) holds, this equality is not satisfied and we have (22) with strict inequality.\nAfter initialization of the parameters \u03b8(0)PGNN = {\u03b8 (0) OL , \u03b8 (0) HL } as in Lemma 5.1, training \u03b8PGNN is performed using a nonlinear optimization scheme that returns {\u03b8(0)PGNN, . . . , \u03b8 (k) PGNN}, with k the number of iterations for the solver to converge or stop. Then, choosing \u03b8\u0302PGNN as the iteration with the smallest cost function, concludes that\nV ( u\u0302 ( \u03b8\u0302PGNN, \u03c6(t) ) , ZN ) \u2264 V ( u\u0302 ( \u03b8 (0) PGNN, \u03c6(t) ) , ZN ) < V ( u\u0302 ( \u03b8\u0302LIP, \u03c6(t) ) , ZN ) , (26)\nif condition (23) holds. Remark 5.1: Beneficial of the proposed PGNN (3) is that Lemma 5.1 does not depend on the dimensions of the NN part of the PGNN. This is in contrast to the PINNs approach, where there are no clear guidelines for the design of the NN."
        },
        {
            "heading": "VI. EXPERIMENTAL VALIDATION",
            "text": "To illustrate the effectiveness of the regularized PGNN (3) for feedforward control as defined in (9), we consider the problem of position control for a real\u2013life coreless linear motor (CLM) shown in Figure 3. The system is controlled in closed\u2013loop by a feedback controller proposed in [15]. Training data is generated while operating the CLM in closed\u2013 loop at a frequency of 10 kHz, with a third order nominal reference r1(t) := {r(0), . . . , r(NR \u2212 1)} that moves back\u2013 and\u2013forth in r1(t) \u2208 {\u22120.1, 0.1} m, with maximum velocity max(|r\u03071(t)|) = 0.05 ms , acceleration max(|r\u03081(t)|) = 4 m s2 , and jerk max(|...r 1(t)|) = 1000 ms3 . Additionally, the input to the system u(t) is dithered with a normally distributed white noise \u2206u(t) \u223c N (0, 502).\nFollowing the previously introduced CLM dynamics (13), the PGNN is defined as\nu\u0302 ( \u03b8PGNN, \u03c6CLM(t) ) = \u03b8Tphy\u03c6CLM(t) + fNN ( \u03b8NN, \u03c6CLM(t) ) ,\n(27) with \u03c6CLM := [\u03b42y(t), \u03b4y(t), sign ( \u03b4y(t) ) , y(t)]T . The transformed inputs enter the NN, as this was shown to enhance convergence of the PGNN training for high sampling rates in [15]. Moreover, the NN part of the PGNNs (27) have a single hidden layer with nl = 16 neurons that have tanh(\u00b7) activation function.\nFigure 4 shows the effect of the regularization using different values for \u039b = \u03bbI on the training convergence for both the data fit and the LIP fit (14). There is a trend observable where a smaller \u03bb allows the physics\u2013guided parameters \u03b8phy to deviate more from the \u03b8\u0302LIP in order to improve the data fit.\nFigure 5 shows the tracking error e(t) := r(t) \u2212 y(t) resulting from the nominal reference r1(t) for the PGNNs (3) that are trained either sequentially, or with regularized cost function (14) using \u039b = \u03bbI with \u03bb = 0 or \u03bb = 0.01. There is no major difference between the different PGNNs in terms\nof tracking performance, as is also confirmed by the data in Table I that lists the mean\u2013absolute error\nMAE ( e(t) ) := 1\nNR \u2211 t\u2208ZR |r(t)\u2212 y(t)| . (28)\nFigure 6 shows the tracking error resulting from a reference r2(t) that oscillates between r2(t) \u2208 {0, 0.17} m, using the same bounds on velocity, acceleration, and jerk as r1(t). The regularization helps to keep performance loss limited when operating the PGNN feedforwards on conditions not present in the training data. This is also confirmed by the MAE shown in Table I. On the other hand, it is mainly due to the feedback controller that the MAE for \u03bb = 0 does not become larger on this experiment.\nAdditionally, Table I shows the results for nl = 8 hidden layer neurons. For this situation, in which the NN part of the PGNN has limited approximation capabilities, it is clear that the regularized PGNN significantly outperforms the sequentially (consecutively) trained PGNN. Currently, what limits the measurable tracking performance is the 0.5 \u00b7 10\u22125 m resolution of the linear encoder with which position measurements y(t) are taken."
        },
        {
            "heading": "VII. CONCLUSION",
            "text": "In this paper, a regularization\u2013based PGNN feedforward control framework was introduced for enabling a trade\u2013 off between high data fitting accuracy of NNs and the good extrapolation properties of physical models. An optimized initialization method was developed for the considered PGNN with a parellel structure, which has improved training convergence compared to a corresponding linear\u2013in\u2013 the\u2013parameter model. Experimental validation showed that developed regularization of the training cost is capable of optimizing the trade\u2013off between data fit and physics\u2013model based extrapolation."
        }
    ],
    "title": "On feedforward control using physics\u2013guided neural networks: Training cost regularization and optimized initialization",
    "year": 2022
}