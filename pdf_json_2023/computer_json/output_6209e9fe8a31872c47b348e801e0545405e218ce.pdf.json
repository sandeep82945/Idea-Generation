{
    "abstractText": "Video retrieval (VR) involves retrieving the ground truth video from the video database given a text caption or viceversa. The two important components of compositionality: objects & attributes and actions are joined using correct semantics to form a proper text query. These components (objects & attributes, actions and semantics) each play an important role to help distinguish among videos and retrieve the correct ground truth video. However, it is unclear what is the effect of these components on the video retrieval performance. We therefore, conduct a systematic study to evaluate the compositional and semantic understanding of video retrieval models on standard benchmarks such as MSRVTT, MSVD and DIDEMO. The study is performed on two categories of video retrieval models: (i) which are pre-trained on video-text pairs and fine-tuned on downstream video retrieval datasets (Eg. Frozen-in-Time, Violet, MCQ etc.) (ii) which adapt pre-trained image-text representations like CLIP for video retrieval (Eg. CLIP4Clip, XCLIP, CLIP2Video etc.). Our experiments reveal that actions and semantics play a minor role compared to objects & attributes in video understanding. Moreover, video retrieval models that use pre-trained image-text representations (CLIP) have better semantic and compositional understanding as compared to models pre-trained on videotext data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Avinash Madasu"
        },
        {
            "affiliations": [],
            "name": "Vasudev Lal"
        }
    ],
    "id": "SP:a4d64f3941cc67c6bdff9a6a29318a1746eabb89",
    "references": [
        {
            "authors": [
                "Mostafa Abdou",
                "Vinit Ravishankar",
                "Artur Kulmizev",
                "Anders S\u00f8gaard"
            ],
            "title": "Word order does matter and shuffled language models know it. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Oliver Wang",
                "Eli Shechtman",
                "Josef Sivic",
                "Trevor Darrell",
                "Bryan Russell"
            ],
            "title": "Localizing moments in video with natural language",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Max Bain",
                "Arsha Nagrani",
                "G\u00fcl Varol",
                "Andrew Zisserman"
            ],
            "title": "Frozen in time: A joint video and image encoder for end-to-end retrieval",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Shyamal Buch",
                "Crist\u00f3bal Eyzaguirre",
                "Adrien Gaidon",
                "Jiajun Wu",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Revisiting the",
            "venue": "video\u201d in video-language understanding",
            "year": 2022
        },
        {
            "authors": [
                "David Chen",
                "William B Dolan"
            ],
            "title": "Collecting highly parallel data for paraphrase evaluation",
            "year": 2011
        },
        {
            "authors": [
                "Zhenfang Chen",
                "Peng Wang",
                "Lin Ma",
                "Kwan-Yee K Wong",
                "Qi Wu"
            ],
            "title": "Cops-ref: A new dataset and task on compositional referring expression comprehension",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Chenyou Fan",
                "Xiaofan Zhang",
                "Shu Zhang",
                "Wensheng Wang",
                "Chi Zhang",
                "Heng Huang"
            ],
            "title": "Heterogeneous memory enhanced multimodal attention model for video question answering",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Han Fang",
                "Pengfei Xiong",
                "Luhui Xu",
                "Yu Chen"
            ],
            "title": "Clip2video: Mastering video-text retrieval via image clip",
            "venue": "arXiv preprint arXiv:2106.11097,",
            "year": 2021
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Linjie Li",
                "Zhe Gan",
                "Kevin Lin",
                "William Yang Wang",
                "Lijuan Wang",
                "Zicheng Liu"
            ],
            "title": "Violet: End-to-end video-language transformers with masked visual-token modeling",
            "year": 2021
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "Linjie Li",
                "Zhe Gan",
                "Kevin Lin",
                "William Yang Wang",
                "Lijuan Wang",
                "Zicheng Liu"
            ],
            "title": "An empirical study of end-to-end video-language transformers with masked visual modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Mona Gandhi",
                "Mustafa Omer Gul",
                "Eva Prakash",
                "Madeleine Grunde-McLaughlin",
                "Ranjay Krishna",
                "Maneesh Agrawala"
            ],
            "title": "Measuring compositional consistency for video question answering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Aina Gar\u0131\u0301 Soler",
                "Marianna Apidianaki"
            ],
            "title": "Let\u2019s play monopoly: Bert can reveal words\u2019 polysemy level and partitionability into senses",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Yuying Ge",
                "Yixiao Ge",
                "Xihui Liu",
                "Dian Li",
                "Ying Shan",
                "Xiaohu Qie",
                "Ping Luo"
            ],
            "title": "Bridging video-text retrieval with multiple choice questions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuying Ge",
                "Yixiao Ge",
                "Xihui Liu",
                "Jinpeng Wang",
                "Jianping Wu",
                "Ying Shan",
                "Xiaohu Qie",
                "Ping Luo"
            ],
            "title": "Miles: visual bert pre-training with injected language semantics for videotext retrieval",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Satya Krishna Gorti",
                "No\u00ebl Vouitsis",
                "Junwei Ma",
                "Keyvan Golestan",
                "Maksims Volkovs",
                "Animesh Garg",
                "Guangwei Yu"
            ],
            "title": "X-pool: Cross-modal language-video attention for textvideo retrieval",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Madeleine Grunde-McLaughlin",
                "Ranjay Krishna",
                "Maneesh Agrawala"
            ],
            "title": "Agqa: A benchmark for compositional spatio-temporal reasoning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Aida Nematzadeh"
            ],
            "title": "Probing imagelanguage transformers for verb understanding. In Find- 9 ings of the Association for Computational Linguistics: ACL",
            "venue": "IJCNLP",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Alexandra Schofield"
            ],
            "title": "How effective is bert without word ordering? implications for language understanding and data privacy",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),",
            "year": 2021
        },
        {
            "authors": [
                "De-An Huang",
                "Vignesh Ramanathan",
                "Dhruv Mahajan",
                "Lorenzo Torresani",
                "Manohar Paluri",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "What makes a video a video: Analyzing temporal information in video understanding models and datasets",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Drew A Hudson",
                "Christopher D Manning"
            ],
            "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jingwei Ji",
                "Ranjay Krishna",
                "Li Fei-Fei",
                "Juan Carlos Niebles"
            ],
            "title": "Action genome: Actions as compositions of spatiotemporal scene graphs",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Peng Jin",
                "Jinfa Huang",
                "Fenglin Liu",
                "Xian Wu",
                "Shen Ge",
                "Guoli Song",
                "David Clifton",
                "Jie Chen"
            ],
            "title": "Expectationmaximization contrastive learning for compact video-andlanguage representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Peng Jin",
                "Hao Li",
                "Zesen Cheng",
                "Jinfa Huang",
                "Zhennan Wang",
                "Li Yuan",
                "Chang Liu",
                "Jie Chen"
            ],
            "title": "Text-video retrieval with disentangled conceptualization and set-to-set alignment",
            "venue": "arXiv preprint arXiv:2305.12218,",
            "year": 2023
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens Van Der Maaten",
                "Li Fei-Fei",
                "C Lawrence Zitnick",
                "Ross Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Junnan Li",
                "Ramprasaath Selvaraju",
                "Akhilesh Gotmare",
                "Shafiq Joty",
                "Caiming Xiong",
                "Steven Chu Hong Hoi"
            ],
            "title": "Align before fuse: Vision and language representation learning with momentum distillation",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Juncheng Li",
                "Junlin Xie",
                "Long Qian",
                "Linchao Zhu",
                "Siliang Tang",
                "Fei Wu",
                "Yi Yang",
                "Yueting Zhuang",
                "Xin Eric Wang"
            ],
            "title": "Compositional temporal grounding with structured variational cross-graph correspondence learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ivan Lillo",
                "Alvaro Soto",
                "Juan Carlos Niebles"
            ],
            "title": "Discriminative hierarchical modeling of spatio-temporally composable human activities",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Yuqi Liu",
                "Pengfei Xiong",
                "Luhui Xu",
                "Shengming Cao",
                "Qin Jin"
            ],
            "title": "Ts2-net: Token shift and selection transformer for text-video retrieval",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Huaishao Luo",
                "Lei Ji",
                "Ming Zhong",
                "Yang Chen",
                "Wen Lei",
                "Nan Duan",
                "Tianrui Li"
            ],
            "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Ma",
                "Guohai Xu",
                "Xiaoshuai Sun",
                "Ming Yan",
                "Ji Zhang",
                "Rongrong Ji"
            ],
            "title": "X-clip: End-to-end multi-grained contrastive learning for video-text retrieval",
            "venue": "In Proceedings of the 30th ACM International Conference on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Avinash Madasu",
                "Estelle Aflalo",
                "Gabriel Ben Melech Stan",
                "Shao-Yen Tseng",
                "Gedas Bertasius",
                "Vasudev Lal"
            ],
            "title": "Improving video retrieval using multilingual knowledge transfer",
            "venue": "arXiv preprint arXiv:2208.11553,",
            "year": 2022
        },
        {
            "authors": [
                "Avinash Madasu",
                "Shashank Srivastava"
            ],
            "title": "What do large language models learn beyond language",
            "venue": "arXiv preprint arXiv:2210.12302,",
            "year": 2022
        },
        {
            "authors": [
                "Avinash Madasu",
                "Shashank Srivastava"
            ],
            "title": "What do large language models learn beyond language? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6940\u20136953",
            "venue": "Abu Dhabi, United Arab Emirates,",
            "year": 2022
        },
        {
            "authors": [
                "Antoine Miech",
                "Dimitri Zhukov",
                "Jean-Baptiste Alayrac",
                "Makarand Tapaswi",
                "Ivan Laptev",
                "Josef Sivic"
            ],
            "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Anssi Moisio",
                "Mathias Creutz",
                "Mikko Kurimo"
            ],
            "title": "Evaluating morphological generalisation in machine translation by distribution-based compositionality assessment",
            "venue": "In The 24rd Nordic Conference on Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "Letitia Parcalabescu",
                "Michele Cafagna",
                "Lilitta Muradjan",
                "Anette Frank",
                "Iacer Calixto",
                "Albert Gatt"
            ],
            "title": "Valse: A taskindependent benchmark for vision and language models centered on linguistic phenomena",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Ellie Pavlick"
            ],
            "title": "Semantic structure in deep learning",
            "venue": "Annual Review of Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ishaan Singh Rawal",
                "Shantanu Jaiswal",
                "Basura Fernando",
                "Cheston Tan"
            ],
            "title": "Revealing the illusion of joint multi- 10 modal understanding in videoqa models",
            "venue": "arXiv preprint arXiv:2306.08889,",
            "year": 2023
        },
        {
            "authors": [
                "Anna Rogers",
                "Olga Kovaleva",
                "Anna Rumshisky"
            ],
            "title": "A primer in bertology: What we know about how bert works",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Ravi Shekhar",
                "Sandro Pezzelle",
                "Yauhen Klimovich",
                "Aur\u00e9lie Herbelot",
                "Moin Nabi",
                "Enver Sangineto",
                "Raffaella Bernardi"
            ],
            "title": "Foil it! find one mismatch between image and language caption",
            "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2017
        },
        {
            "authors": [
                "Koustuv Sinha",
                "Robin Jia",
                "Dieuwke Hupkes",
                "Joelle Pineau",
                "Adina Williams",
                "Douwe Kiela"
            ],
            "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Alane Suhr",
                "Stephanie Zhou",
                "Ally Zhang",
                "Iris Zhang",
                "Huajun Bai",
                "Yoav Artzi"
            ],
            "title": "A corpus for reasoning about natural language grounded in photographs",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Tristan Thrush",
                "Ryan Jiang",
                "Max Bartolo",
                "Amanpreet Singh",
                "Adina Williams",
                "Douwe Kiela",
                "Candace Ross"
            ],
            "title": "Winoground: Probing vision and language models for visiolinguistic compositionality",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ivan Vuli\u0107",
                "Edoardo Maria Ponti",
                "Robert Litschko",
                "Goran Glava\u0161",
                "Anna Korhonen"
            ],
            "title": "Probing pretrained language models for lexical semantics",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "Superglue: A stickier benchmark for generalpurpose language understanding systems",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R Bowman"
            ],
            "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Jun Xu",
                "Tao Mei",
                "Ting Yao",
                "Yong Rui"
            ],
            "title": "Msr-vtt: A large video description dataset for bridging video and language",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Yiming Yang",
                "Jaime Carbonell",
                "Russ R Salakhutdinov",
                "Quoc V Le"
            ],
            "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ximing Lu",
                "Jack Hessel",
                "Youngjae Yu",
                "Jae Sung Park",
                "Jize Cao",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Merlot: Multimodal neural script knowledge models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Yuwei Wu",
                "Hai Zhao",
                "Zuchao Li",
                "Shuailiang Zhang",
                "Xi Zhou",
                "Xiang Zhou"
            ],
            "title": "Semantics-aware bert for language understanding",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "Video-retrieval (VR) is the task of retrieving videos for a given text caption or given a video, retrieve the corresponding text caption. This involves understanding important details such as objects & attributes (Eg: two women and a man, red shirt guy), actions (Eg. playing, standing, talking etc.) in the text caption and the video. In vision it is referred to as compositional reasoning [7, 19, 26, 31, 32], i.e. representing the image or video requires the understanding of primitive concepts that make them. In the recent years,\nnew benchmarks [6,25,29,41] have been proposed to measure the compositional capabilities of foundational image models. The compositionality in these models is measured by creating new text captions from the original text captions using word ordering [51], word substitutions [42], negative pairs [22], image-text mismatch [48].\nWhen compared to images, measuring compositionality is a lot harder in videos. There are multiple reasons to this: First, videos are made-up of time-series image frames with multiple objects & attributes and actions unlike images. Therefore, methods like creating negative pairs, mismatching pairs etc. used for evaluating compositionality in image-language models have very limited scope. Second, even though tasks based on video question answering (VQA) [6, 25] have been proposed to measure the compositionality, recent studies [4,10,14,24,45] have shown that these datasets exhibit single frame bias.\nMost of the previous works [22, 42, 51] focus on understanding the compositionality of image-text models. It mainly involves experimenting with objects & attributes in the text captions and retrieving the images. However, actions play a crucial role in when retrieving videos using text captions. Another important aspect which is often overlooked in the previous studies is the semantics. For example consider the query \u201ca guy wearing a red shirt drives a car while talking, the objects & attributes are guy, red shirt and car, the actions are wearing, driving and talking and rest of the words (a, while) form the semantics of the text captions. The video retrieval models can comprehend such queries because of the accurate semantics and compositionality (objects & attributes and actions).\nNow consider the following scenarios of the text captions in which (i) objects & attributes are missing (\u201ca wearing a drives a while talking) (ii) actions are missing (\u201ca guy a red shirt a car while) and (iii) semantics are missing (\u201cguy wearing red shirt drives car talking). This begs an important question: What is the effect of each of these scenarios on the video retrieval performance? To address this question, we propose a detailed study to evaluate the semantic and compositional understanding of video retrieval models.\nar X\niv :2\n30 6.\n16 53\n3v 1\n[ cs\n.C V\n] 2\n8 Ju\nn 20\nFor this study we create a comprehensive test bed to evaluate the state-of-the-art video retrieval models for compositonality and semantics. We base this investigation along three axes: Objects & attributes, actions and semantics. We propose a set of 10 tasks for these categories: four tasks to evaluate the knowledge of VR models for objects & attributes (\u00a73.1.1), three tasks for testing action understanding (\u00a73.1.2) and finally, three tasks for semantic capabilities (\u00a73.2). Table 1 describes these tasks with an example. We perform a comprehensive evaluation on 12 state-of-the-art video retrieval models belonging to two categories (\u00a74.1): The first category of models such as Frozen-in-Time (FiT) [3], MCQ [16] etc. are pre-trained on large scale video datasets and fine-tuned for video retrieval. The second category uses pretrained image features like CLIP for video retrieval namely CLIP4Clip [35], CLIP2Video [11] etc. These models are tested on three standard video retrieval benchmarks (\u00a74.2) such as MSRVTT [55], MSVD [5] and DiDeMo [2].\nOur experiments (\u00a75.1) reveal that objects & attributes are the most crucial to video retrieval followed by actions and semantics. Among video retrieval models, CLIP based models have a better compositional and semantic understanding when compared with pretrained video models. We further perform detailed studies to fully judge how retrieval models perceive each of the components. We find that (\u00a75.2) video retrieval models have a poor understanding of relationship between objects and its attributes. However, they are extremely sensitive to incorrect object references in the captions. Our studies on action understanding (\u00a75.3) disclose that models have poor sense of action negation and replacing them with incorrect actions lead to slight decrease in video retrieval performance. Finally, we discover (\u00a75.4) that models perform significantly better even without the right semantic word order. Our contributions are as follows:\n\u2022 Ours is the first work to comprehensively investigate the compositional and semantic understanding of video retrieval models and we propose a set of 10 tasks for this study.\n\u2022 We perform this analysis on a broad range of 12 stateof-the-art models and generalize the findings to the video retrieval task.\n\u2022 We establish that video retrieval models exhibit distinct and contrasting behaviours for interpreting various elements in the text captions."
        },
        {
            "heading": "2. Related Work",
            "text": ""
        },
        {
            "heading": "2.1. Video retrieval",
            "text": "In the recent years, there has been a tremendous improvement on the task of video-retrieval. This is mainly due\nto two reasons (i) with the adaption of transformer based models to vision tasks like image classification [9, 20, 34] (ii) with the availabilty of large scale video-text datasets like HowTo100M [40], WebVid-2M [3] and YT180M [57]. Frozen-in-Time [3] is a dual-stream transformer model pre-trained on WebVid-2M and Conceptual captions-3M [47] datasets and fine-tuned for downstream video retrieval. A prompt based novel pre-training task [30] is proposed to effectively align visual and text features during large scale video-text pre-training. A new pre-training approach Masked Visual-token Modeling (MVM) [12] is presented to better model the temporal dependencies among videos for video-retrieval. To incorporate the rich semantic features of the videos, a novel pretext task Multiple Choice Questions (MCQ) is put forward in which the model is trained to answer questions about the video.\nIn a parallel direction, image features pre-trained on large amounts of image-text data have been adopted for the task of video retrieval. CLIP4Clip [35] is an end-to-end trainable video retrieval model based on CLIP [44] architecture in which frame features are extracted using clip image encoder and the temporal modelling is performed using a transformer encoder. A two-stage framework CLIP2Video [11] is proposed to enhance interaction among video features and video-text features for video-retrieval. Madasu et at. [37] used off-the-shelf multi-lingual data to enhance the performance of video-retrieval. All these videoretrieval models previously haven\u2019t been tested for semantic and compositional understanding. To the best of our knowledge, ours is the first work to comprehensively explore semantic and compositional understanding of video retrieval models."
        },
        {
            "heading": "2.2. Semantics",
            "text": "Transformer based language models [8, 21, 56] have achieved state-of-the-art results on most natural language understanding tasks [53,54]. Hence, there has been a growing interest to explore the semantic capabilities of these models [15, 43, 46, 52, 58]. More recently, a body of works [1,23,38,49] investigated the effect of word order and its effect on the semantic understanding in pre-trained language models. Since all the video retrieval models use pre-trained language models for encoding text captions, we build upon those works and investigate their semantic understanding."
        },
        {
            "heading": "2.3. Compositionality",
            "text": "Although vision-language models pretrained on large amounts of data achieved state-of-the-results there has been a growing interest to understand the working of these models [6, 25, 25, 29, 41, 50]. These works mainly focus on compositional knowledge these models by proposing new benchmarks. Winoground [51] dataset was introduced in which a pair of text captions contain the same set of\nwords but pertain to different images. The models are then tested for image and caption match. Another benchmark CREPE [14] was put forward to evaluate two aspects of compositionality: systematicity and productivity. This benchmark contains unseen compounds and atoms in the test split to evaluate the models\u2019 generalization. Parcalabescu et al. [42] proposed VALSE dataset to measure visio-linguistic capabilities of pretrained vision and language models. AGQA-Decomp [14] is a new benchmark to measure compostional consistency for the task of Video Question Answering. All these works proposed new benchmarks for compostional reasoning in image-language models. Contrary to these, our work focuses on measuring compositionality of video retrieval models using the standard datasets and doesn\u2019t require a new benchmark. Moreover, our experiments are evaluated on 13 models which are significantly higher than the models in these works."
        },
        {
            "heading": "3. Compositional and Semantic Understanding",
            "text": "In this section, we first define semantics and compositionality and subsequently establish the evaluation protocol for semantic and compositional understanding in video retrieval models. For this evaluation, we augment the existing text captions and create new datasets that assess their semantic and compositional understanding. We explain this protocol using an example test caption Q \u201da guy wearing a red shirt drives a car while talking\u201c from the MSRVTT dataset. Table 1 summarizes different augmentation methods used for the proposed study."
        },
        {
            "heading": "3.1. Compositionality in videos",
            "text": "A video is composed of multiple objects & attributes interacting with each other in a similar or different fashion. To retrieve a video, corresponding text caption is passed as an input to the video retrieval model. This text caption typically consists of objects & attributes and interactions (actions) unique to that particular video. The video retrieval model parses the input caption and computes the matching scores with all the videos. Finally, the video with the highest matching score is the predicted ground truth video. Therefore a video retrieval model should be able to understand each of the objects & attributes and actions present in the caption. This is called compositionality in the visual world. To evaluate the compositional understanding in video retrieval models, we mainly focus on their ability to parse objects & attributes and actions. Next we discuss the evaluation protocol to measure compositionality in VR models."
        },
        {
            "heading": "3.1.1 Object & Attribute knowledge",
            "text": "Object & Attribute removal (Qobjattrrem): In this setup, we remove all the objects & attributes in the\noriginal caption Q and the resulting caption is \u201dwearing a drives a while talking\u201c. Here guy, red shirt and car are the objects & attributes. Object shift (Qobjshift): To test the VR models ability to relate objects with their attributes, we shift the places of objects in the captions. The modified caption is \u201da shirt wearing a red car drives a guy while talking\u201c. Object replacement (Qobjrep): We evaluate the VR models sensitivity to objects by randomly replacing the objects with an entirely different objects. The replaced caption is \u201da surf wearing a red mars drives a guy channel while talking\u201c. Object partial (Qobjpartial): In this setup, the VR models are given access to just 50% of the objects in the caption. This is to understand if the models perform any shortcuts while retrieving videos. Eg: \u201da wearing a red drives a car while talking\u201c. Next, we introduce the tasks for evaluating action knowledge in VR models."
        },
        {
            "heading": "3.1.2 Action knowledge",
            "text": "Another important compositional component in the captions is action. To test the action knowledge of video retrieval models, we propose the following caption augmentations: Action removal (Qactrrem): The actions present in the original captions are eliminated. The modified caption is \u201da red shirt a car while\u201c as the actions wearing, drives are removed. This is to understand the influence of actions on the video retrieval performance. Action negation (Qactneg): A negation is added to all the actions in the captions resulting in the new caption \u201da guy not wearing a red shirt not drives a car while not talking\u201c This tests the VR models ability to comprehend negation in the captions. Action replacement (Qactrep): In this setup, the actions are randomly replaced with a different set of actions. The replaced actions are neither antonyms nor synonyms. It checks if the models truly recognize the meaning of the action words. Next we present the evaluation protocol for semantic understanding of VR models."
        },
        {
            "heading": "3.2. Semantic understanding",
            "text": "In the previous section we elucidated the components for compositional reasoning in videos namely objects & attributes and actions. These components are bind together by semantics there by forming a meaningful caption. Let\u2019s consider a part of the example described previously \u201da guy wearing a red shirt drives a car\u201d, if the word \u201dcar\u201c and \u201dguy\u201c are interchanged the resulting caption will be \u201da car wearing a red shirt drives a guy\u201d which is not meaningful. Conse-\nquently, semantics also play a crucial role in video retrieval performance along with the compositonality. Subsequently, we put forward the evaluation protocol to measure semantic understanding in video retrieval models. Semantics removal (Qsemrem): Our first experiment focuses on the effect of semantics on VR models. We modify the caption by keeping just the objects & attributes, actions and eliminate any semantic words among them. The resulting caption is \u201dguy wearing red shirt drives car talking\u201c. Word order shuffle (Qshuf ): In this setup, all the words are shuffled in the caption. This destroys the order of compositionality and semantics. This tests the order sensitivity of VR models. Word order reverse (Qrev): In this setup, we preserve the word order except that in the reverse order. It evaluates the positional knowledge of video retrieval models. Next, we present the experiment set up for quantifying the compositional and semantic understanding."
        },
        {
            "heading": "4. Experiments",
            "text": "In this section, we explain the video retrieval models and datasets used for the proposed analysis."
        },
        {
            "heading": "4.1. Models",
            "text": "We experiment with two categories of video retrieval models. The first category of models are pretrained on large scale video-text datasets like WebVid-2.5M [3] and YTTemporal-180M [57] and fine-tuned for downstream video retrieval datasets. These include Frozen-in-Time (FiT) [3], MCQ [16], MILES [17], VIOLET [12] and MVM [13]. The second category involves models that adapt pretrained image-text features such as CLIP [44] for the task of video retrieval. This category comprise of seven architectures namely TS2NET [33], CLIP4CLip [35], CLIP2Video [11], XCLIP [36], XPOOL [18], EMCL [27] and DiCoSA [28]."
        },
        {
            "heading": "4.2. Datasets",
            "text": "We perform the evaluation on three video retrieval datasets: MSRVTT [55], MSVD [5] and DiDeMo [2]. MSRVTT has 10000 videos and each video has multiple captions totalling 200K. We report the results on MSRVTT9k split (9000 for training and 1000 for testing). MSVD consists of 1970 videos and 80K captions. The training split has 1300 videos and the test split has 670 videos. The captions in these datasets are single sentence. DiDeMo is made up of 10K videos and 40K descriptions. Following [35], we concatenate all the sentences and evaluate as paragraphto-video retrieval."
        },
        {
            "heading": "4.3. Implementation",
            "text": "We use spacy1 to identify parts-of-speech for all the words in the caption. We consider nouns, adverbs and adjectives as objects & attributes, verbs as actions and rest of the parts-of-speech as semantics. We use the exact set up used by the state-of-the-art video retrieval models and measure the performance on all the augmented datasets."
        },
        {
            "heading": "5. Results and Discussion",
            "text": ""
        },
        {
            "heading": "5.1. Objects & Attributes vs Actions vs Semantics:",
            "text": "Do all of them matter?\nOur aim is to analyze the importance of three components: objects & attributes, actions and semantics that make up a text query for retrieving videos. Hence, we test the video retrieval models with text captions that have missing objects & attributes (Qobjattrrem), actions (Qactrem) and semantics (Qsemrem). Tables 2, 3 and 4 show the results on MSRVTT, MSVD and DiDeMo datasets respectively.\nIt is evident from the table that there is a drop in video retrieval performance when tested with text captions that\n1https://spacy.io/usage/linguistic-features\ndon\u2019t have actions (Qactrem). The drop is more pronounced among CLIP based models than pretrained video models. This shows that actions play a role for retrieving correct videos. However, we see that the performance drop is not as expected. Videos are time-series image frames which can have same attributes. In those scenarios, actions help in differentiating those videos. We see this effect when the R@1 is lower among pretrained video models and higher among CLIP based models. When the performance is lower, actions do not play a significant role in video retrieval and hence the videos can be retrieved without them in the text caption. On the contrary if R@1 score is higher, we see a notable decline. This is due to the robust video representations of CLIP based models as compared to pretrained video models. CLIP based models accurately encode video\nrepresentations but, when the differentiating factor among videos i.e actions are missing in text captions leads to retrieval of incorrect videos. In short caption length datasets like MSRVTT and MSVD, we notice a significant drop in performance as compared with DiDeMo which is a paragraph (> 1 sentences) dataset. This is because text captions in DiDeMo contains detailed description of the videos and hence, missing actions didn\u2019t lead to drop in performance as compared to MSRVTT and MSVD. It demonstrates that actions are not essential in paragraph-video retrieval.\nNext, we analyze the performance of video retrieval models tested with text captions without semantics (Qsemrem). From the table, it is clear that there is a reduction in R@1 without the semantics in the text captions. It validates that semantics are necessary for retrieving correct\n.\nground truth videos. For MSRVTT, we observe that models tested without semantics under-perform compared to actions in the text captions and the average difference in performance is 2%. The reverse is true for MSVD and DiDeMo datasets where there is a huge difference of 9%. In addition, we also notice that CLIP based models are more sensitive to semantics than pretrained video models. Finally, we evaluate the video retrieval models on text captions in the absence of objects & attributes (Qobjattrrem). As seen from the results, these models perform poorly (a drop in 20%)\nwhich underscores the significance of objects & attributes. We also notice that Qattrrem trails Qactrem and Qsemrem by a huge margin. This difference is more striking among CLIP based models as opposed to pretrained video models."
        },
        {
            "heading": "5.2. What role do Objects & Attributes play in video",
            "text": "retrieval?\nIn the previous sections (\u00a75.1), findings from our experimental results suggested that objects & attributes are the most important component in text captions while retrieving\nvideos. To investigate further, we perform additional detailed studies on their importance. In captions there can be multiple objects & attributes and every pair of object & attribute is distinct from the other. Any slight modification in the pairs can totally change their correspondence and\nthereby the ground truth video and hence, video retrieval models should be able to account for these changes. We perform a test in which we interchange the places of objects while keeping rest of the caption same Qobjshift. In the second study, we randomly replace objects in the cap-\ntion Qobjrep and evaluate the models on the modified ones. The final ablation involves keeping just half the objects in the captions (Qobjpartial). This is to assess if VR models adapt any shortcuts and still retrieve correct videos without the critical information. Figure 1 demonstrates the results for these studies. As shown in the figure, there is a slight deterioration of video retrieval performance when there is a object shift in the caption. The drop is a meagre 5.5% for MSRVTT and 3.6% in case of MSVD dataset. It demonstrates that VR models do not quite fully understand the relationship between object and its attribute. On the other hand if the objects are randomly replaced (Qobjrep) with different unrelated objects, there is a massive degradation in R@1 score. In fact, the performance is quite similar to the models performance tested on captions without objects & attributes. These results prove that video retrieval models are extremely sensitive to alteration of objects.\nFigure 1 shows that there is a noticeable fall in performance when the VR models have access to just 50% of the object data in the captions. The R@1 score lags by 30% in MSRVTT and 22% in MSVD datasets. It reinforces the aforesaid extreme sensitivity nature of the retrieval models towards objects. Furthermore, random object replacement performs far worse than partial objects in the captions. This highlights that factual object description even though 50% is much more crucial than access to the entire caption albeit with incorrect objects."
        },
        {
            "heading": "5.3. Do VR models pay attention to actions?",
            "text": "We demonstrated in the section 5.1 that actions play a role in video retrieval. Now, this raises an important question How much attention do VR pay to actions in the captions? To investigate this we perform certain ablation studies on the action understanding of VR models in the text captions. We replace the action word with the negation of it (Qactneg) and test the performance of VR models on the newly formed captions. In parallel, the actions in the captions are randomly replaced with different actions (Qactrep) and VR models are evaluated on the altered captions. In Figure 2, we provide the results of VR models tested on captions with negated (Qactneg) and replaced (Qactrep) actions. From the figure, it is evident that action negation (Qactneg) achieves comparable results to Q and there is a slight drop in performance in case of action replacement (Qactrep). Most of the actions are expressed in positive sense in these datasets and this is not always the case. For a fine-grained description of videos, the actions of the static objects can be communicated in a negation form. So, naturally video retrieval models are expected to understand the negation in captions. However, we notice that action negation has similar performance as original captions which demonstrates that VR models lack the capability of action negation sense.\nNext, we randomly replace the actions with a different action and test the attention of VR models. In an ideal scenario, the performance of these models should drop drastically as the replaced actions do not correspond to that of the ones in ground truth videos. Nevertheless, we see that the R@1 score of action replacement (Qactrep) is only slightly less than original caption Q. In fact the average drop in R@1 is only 6.8% in MSRVTT and 7.5% in MSVD. Hence even though the actions are important in video retrieval, VR models use other influential information such as objects & attributes to retrieve ground truth videos."
        },
        {
            "heading": "5.4. Does word order of text captions matter?",
            "text": "In the figures 3a and 3b, we present the findings on the word order evaluation. First we observe that models tested on datasets without word order perform worse than the original dataset. The R@1 score is reduced on average by 6.3% and 9.1% on shuffled (Qs) and reversed (Qr) MSRVTT captions respectively. On a similar note the performance drops 5.5% on shuffled and 5% on reversed DiDeMo dataset. Additionally, the R@1 decrease is more pronounced on reversed captions than shuffled. This is surprising as the object-action order is preserved in reversed captions in contrast with shuffled. This shows that models adapt bag-ofwords approach for semantic understanding of captions and positioning of object-action order doesn\u2019t matter. A possible explanation for this behaviour is: all the video retrieval models use pretrained language models as their text encoder. Recent studies have shown that [39, 49] distributional information is preserved even though the syntactic word order is disturbed and hence, LMs leverage it for hierarchical text understanding. Surprisingly, video retrieval models manifest the same behaviour in caption understanding."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this work, we proposed a comprehensive investigation of compositional and semantic understanding of video retrieval models. For this study we put forward 10 different tasks to evaluate models reasoning of objects & attributes, actions and semantics for retrieving videos. We experiment with a wide range of 12 state-of-the-art video retrieval models and 3 standard benchmarks. We show that video retrieval performance is heavily impacted by objects & attributes and lightly by semantics. Furthermore, our results also reveal that word order matter less for video retrieval models. These results shed an important light on the inner workings of video retrieval models. We believe the future works can utilize these findings to design compositional aware video retrieval models."
        }
    ],
    "title": "ICSVR: Investigating Compositional and Semantic Understanding in Video Retrieval Models",
    "year": 2023
}