 Here are 5 potential future research ideas after reading this paper:

1. Explore other methods for non-exact repetition detection besides SentenceBERT, such as using different sentence embedding techniques or even non-embedding based approaches. This could lead to more robust and generalized repetition suppression. 

2. Investigate combining the token-level and sequence-level unlikelihood training objectives into one joint loss function. This unified approach may achieve better performance than using the objectives separately.

3. Expand the content moderation capabilities beyond a static blocklist, such as dynamically generating blocklists per genre/domain or incorporating human feedback. This could allow for more customizable and adaptive content control.

4. Evaluate the methods on other large language models besides BART, such as GPT-3, to test wider applicability across model architectures. Comparisons could reveal insights into how repetition manifests in different models.

5. Conduct user studies to measure real-world effectiveness of the techniques for writing assistance applications. Metrics could include user satisfaction, task completion time, and appropriateness of generated text. This would provide valuable perspective beyond offline metrics.