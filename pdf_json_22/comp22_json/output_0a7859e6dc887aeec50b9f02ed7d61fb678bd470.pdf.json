{
    "abstractText": "The presence of these indicators undermines our confidence in the integrity of the article\u2019s content and we cannot, therefore, vouch for its reliability. Please note that this notice is intended solely to alert readers that the content of this article is unreliable. We have not investigated whether authors were aware of or involved in the systematic manipulation of the publication process. Wiley and Hindawi regrets that the usual quality checks did not identify these issues before publication and have since put additional measures in place to safeguard research integrity. We wish to credit our own Research Integrity and Research Publishing teams and anonymous and named external researchers and research integrity experts for contributing to this investigation. The corresponding author, as the representative of all authors, has been given the opportunity to register their agreement or disagreement to this retraction. We have kept a record of any response received.",
    "authors": [
        {
            "affiliations": [],
            "name": "R. Kalpana"
        },
        {
            "affiliations": [],
            "name": "Abdul Wahab Rahmani"
        }
    ],
    "id": "SP:e5ba75abd3bd6ac9fce21aead4568972a1269ce4",
    "references": [
        {
            "authors": [
                "A.N. Rehman",
                "M.A. Khan",
                "T. Saba"
            ],
            "title": "Microscopic brain tumor detection and classification using3D CNNand feature selection architecture",
            "venue": "Research and Techniques in Microscopy, vol. 84, no. 1, pp. 133\u2013149, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Rammurthy",
                "P.K. Mahesh"
            ],
            "title": "Whale Harris Hawks optimization based deep learning classifier for brain tumor detection using MRI images",
            "venue": "Journal of King Saud University - Computer and Information Sciences, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "S.N. Shivhare",
                "N. Kumar"
            ],
            "title": "Tumor bagging: a novel framework for brain tumor segmentation using metaheuristic optimization algorithms",
            "venue": "Multimedia Tools and Applications, vol. 80, no. 17, pp. 26969\u201326995, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "S. Kumar",
                "D.P. Mankame"
            ],
            "title": "Optimization driven deep convolution neural network for brain tumor classification",
            "venue": "Biocybernetics and Biomedical Engineering., vol. 40, no. 3, pp. 1190\u20131204, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Yin",
                "C. Wang",
                "F. Abza"
            ],
            "title": "New brain tumor classification method based on an improved version of whale optimization algorithm",
            "venue": "Biomedical Signal Processing and Control, vol. 56, p. 101728, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "M. Maha",
                "Y. Abdel-Raheem",
                "K. Salah",
                "E. Mohamed"
            ],
            "title": "Lightning attachment procedure optimization algorithm for nonlinear non-convex short-term hydrothermal generation scheduling",
            "venue": "Soft Computing, vol. 24, no. 21, pp. 16225\u2013 16248, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "B. Vilas",
                "A. Narayan",
                "M. Akshatha",
                "P. Tunga",
                "D.V. Singh"
            ],
            "title": "Automatic brain tumor segmentation using Dense-Net",
            "venue": "International Research Journal of Engineering and Technology (IRJET), vol. 7, no. 6, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Alagarsamy",
                "Y.D. Zhang",
                "V. Govindaraj",
                "M.P. Rajasekaran",
                "S. Sankaran"
            ],
            "title": "Smart identification of topographically variant anomalies in brain magnetic resonance imaging using a fish school-based fuzzy clustering approach",
            "venue": "IEEE Transactions on Fuzzy Systems, vol. 29, no. 10, pp. 3165\u20133177, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "D. Deb",
                "S. Roy"
            ],
            "title": "Brain tumor detection based on hybrid deep neural network in MRI by adaptive squirrel search optimization",
            "venue": "Multimedia Tools and Applications, vol. 80, pp. 2621\u20132645, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Sharif",
                "J. Amin",
                "M. Raza",
                "M. Yasmin",
                "S.C. Satapathy"
            ],
            "title": "An integrated design of particle swarm optimization (PSO) 14 BioMed Research International RE TR AC TE D with fusion of features for detection of brain tumor",
            "venue": "Pattern Recognition Letters, vol. 129, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "D.R. Cristin",
                "D.K.S. Kumar",
                "D.P. Anbhazhagan"
            ],
            "title": "Severity level classification of brain Tumor based on MRI images using fractional-chicken swarm optimization algorithm",
            "venue": "The Computer Journal, vol. 64, no. 10, pp. 1514\u20131530, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M.I. Sharif",
                "M.A. Khan",
                "M. Alhussein",
                "K. Aurangzeb",
                "M. Raza"
            ],
            "title": "A decision support system for multimodal brain tumor classification using deep learning",
            "venue": "Complex Intell. Syst., 2021.",
            "year": 2021
        },
        {
            "authors": [
                "M. Ali",
                "S.O. Gilani",
                "A. Waris",
                "K. Zafar",
                "M. Jamil"
            ],
            "title": "Brain tumour image segmentation using deep networks",
            "venue": "IEEE Access, vol. 8, pp. 153589\u2013153598, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "P. Liu",
                "Q. Dou",
                "Q. Wang",
                "P. Heng"
            ],
            "title": "An encoder-decoder neural network with 3D squeeze-and-excitation and deep supervision for brain tumor segmentation",
            "venue": "IEEE Access, vol. 8, pp. 34029\u201334037, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Gandhi",
                "M. Shabaz"
            ],
            "title": "Evichain: evaluating and scrutinizing crime using block chain",
            "venue": "International Journal of Recent Technology and Engineering, vol. 8, no. 3, pp. 3992\u20133994, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "A. Gupta"
            ],
            "title": "Securing the future of the Indian IT industry: a case for educational innovation in higher technical education challenges and the road ahead",
            "venue": "Industry and Higher Education, vol. 19, no. 6, pp. 423\u2013431, 2005.",
            "year": 2005
        },
        {
            "authors": [
                "G.K. Saini",
                "H. Chouhan",
                "S. Kori"
            ],
            "title": "Recognition of human sentiment from image using machine learning",
            "venue": "Annals of the Romanian Society for Cell Biology, vol. 25, no. 5, pp. 1802\u2013 1808, 2021.",
            "year": 1802
        },
        {
            "authors": [
                "S. Shridhar",
                "M. Lakhanpuria",
                "A. Charak",
                "A. Gupta",
                "S. Shridhar"
            ],
            "title": "SNAIR: a framework for personalised recommendations based on social network analysis",
            "venue": "Proceedings of The 5th International Workshop on Location-Based Social Networks - LBSN \u201812. The 5th International Workshop, pp. 55\u201361, ACM Press, 2012.",
            "year": 2012
        },
        {
            "authors": [
                "C. Li",
                "H. Niu",
                "M. Shabaz",
                "K. Kajal"
            ],
            "title": "Design and implementation of intelligent monitoring system for platform security gate based on wireless communication technology using ML",
            "venue": "International Journal of Systems Assurance Engineering and Management, vol. 13, no. S1, pp. 298\u2013304, 2022.",
            "year": 2022
        },
        {
            "authors": [
                "A.R. Raju",
                "P. Suresh",
                "R.R. Rao"
            ],
            "title": "Bayesian HCS-based multi-SVNN: a classification approach for brain tumor segmentation and classification using Bayesian fuzzy clustering",
            "venue": "Biocybern Biomed Eng, vol. 38, no. 3, pp. 646\u2013660, 2018. 15 BioMed Research International",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Retraction Retracted: Metaheuristic Optimization-Driven Novel Deep Learning Approach for Brain Tumor Segmentation\nBioMed Research International\nReceived 8 January 2024; Accepted 8 January 2024; Published 9 January 2024\nCopyright \u00a9 2024 BioMed Research International. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.\nThis article has been retracted by Hindawi following an investigation undertaken by the publisher [1]. This investigation has uncovered evidence of one or more of the following indicators of systematic manipulation of the publication process:\n(1) Discrepancies in scope\n(2) Discrepancies in the description of the research reported\n(3) Discrepancies between the availability of data and the research described\n(4) Inappropriate citations\n(5) Incoherent, meaningless and/or irrelevant content included in the article\n(6) Manipulated or compromised peer review\nThe presence of these indicators undermines our confidence in the integrity of the article\u2019s content and we cannot, therefore, vouch for its reliability. Please note that this notice is intended solely to alert readers that the content of this article is unreliable. We have not investigated whether authors were aware of or involved in the systematic manipulation of the publication process.\nWiley and Hindawi regrets that the usual quality checks did not identify these issues before publication and have since put additional measures in place to safeguard research integrity.\nWe wish to credit our own Research Integrity and Research Publishing teams and anonymous and named external researchers and research integrity experts for contributing to this investigation.\nThe corresponding author, as the representative of all authors, has been given the opportunity to register their agreement or disagreement to this retraction. We have kept a record of any response received."
        },
        {
            "heading": "1. Introduction",
            "text": "Brain tumors are the foremost dreadful cancer among the various kinds of them. One of the most unpleasant types of cancer is a tumour, which has resulted in a massive population die-off [1]. Brain tumor needs precise analysis by the doctor that may categorize the tumor exactly [2]. Solely concerning, some kinds of brain tumors are cancerous, i.e., malignant. The tumor will impair the function of the brain either benign or malignant. It compresses the nerve and blood vessels and also causes many symptoms such as headaches perhaps severe, temperament changes, confusion, balance issues, nausea, difficulty in focusing, coordination, and concentration, numbness, weakness, and complication in sensory like hearing, vision, or speaking and seizures, and\nuncommon temporary state, amnesia, stumbling with thinking, speaking, and understanding languages [3].\nThere are two sorts of tumours: benign and malignant. A tumour is occasionally treatable and is not considered malignant; however, a malignancy is harmful if it is not detected early. Tumors are among the various types of development that can be fatal and affect important brain components such as nerve tissue, white matter (WM), substantia grisea (GM), and liquid body material (CSF). This additionally damages the central nervous system [4]. Primary tumors will develop in the brain extent, and the tumors which develop apart from the brain extent are known as secondary tumors [3]. The tumor cells are also called neoplastic cells where they grow rapidly and divide in multiples more than the usual, or instead, they will not die [5]. One of the foremost\nHindawi BioMed Research International Volume 2022, Article ID 2980691, 15 pages https://doi.org/10.1155/2022/2980691\nRE TR AC TE D\ncommon kinds of brain tumor is glioma. These gliomas emerge by the surrounding and nurture cells of glial cells, which also contain astrocytes, which are found in the brain, oligodendrocytes, and ependymoma cells [6]. Themost common CNS tumor is low-grade glioma (LGG) and is categorized as grade-I and grade-II glial tumors, namely, oligodendodrogrioma and gangriomas pleomorphic xanthoastrocytoma. These were commonmostly in pediatrics than adults. The least malignant and most common LGG is a pilocytic astrocytoma; by gross total research, overall survival can be >90-95% for about 5 years. Figure 1(a) represents the epidemiology of LGG. Many patients have multiple progressions and recurrences depending on location and ability to resect. Then, the most aggressive andmalignant glial tumor is high-grade glioma (HGG) which is classified as grade-III and grade-IV, and these were common mostly in adults than in pediatrics. Figure 1(b) represents the epidemiology of HGG. HGG has a penurious survival outcome and is more resistant to therapy compared to LGG. The outcomes of HGG is universally poor showing 5-year overall survival is 15-20%. Nevertheless, recent analysis helps doctors move to victimization growth genetic science to higher classify gliomas. Individuals with a 5-year survival rate with a cancerous brain or system growth is three hundred and sixty-five days. The 10-year survival rate is concerning thirty-first. The 5-year survival rate for individuals younger than age fifteen is over seventy-fifth. For individuals aged fifteen to thirty-nine, the 5-year survival rate is over seventytwo. The 5-year survival rate for individuals aged forty and over is over twenty-first [7].\nThe use of magnetic resonance imaging (MRI) in medical imaging allows for a good view of the body\u2019s soft tissues [5]. The positioning and size of these tumours in a brain magnetic resonance imaging (MRI) picture must be determined for diagnosis and therapy [8]. The most common types of MRI sequences are called T1-weighted and T2weighted scans. When the TE and TR timings are kept relatively short, the resulting images are T1-weighted. The contrast and brightness of the image are mostly attributable to the characteristics of the T1 tissue. On the other hand, T2weighted images are produced by employing TE and TR times that are significantly longer. The T2 properties of the tissue are primarily responsible for determining the contrast and brightness of these pictures. In this study, we present procedure for lightning attachment (PLA), a novel optimization approach inspired by lightning occurrences in which large quantities of electrical charges build up within the cloud. Lightning is created when the number of charges within a cloud increase, resulting in an increase in electrical intensity. Lightning can strike at any time, and it will erupt from a variety of locations [9].\nThis paper uses Dense Net Model as the basic structural unit feature extraction of the tumor and classifies the abnormal brain tumors in the LGG and HGG. When it comes to deep learning, CNNs (convolutional neural networks) are a type of deep neural network that is used to analyse graphical pictures. The dense convolutional network (Dense-Net), introduced in is a convolutional network where the layers are linked to all the further layers in the network. The Dense-Net is used for accurate feature extraction of the\nimage. In our proposed model, Dense net 169 is used as a feature extractor. The Dense-Net architecture has been proposed in recent years, and work on standard datasets has shown it to be substantially deeper, more accurate, and efficient than most architectures. Its dense interconnections between layers are proposed to encourage feature reuse [10]."
        },
        {
            "heading": "1.1. Contribution of the Paper",
            "text": "(i) We use BraTS 2016, 2017, and BraTS 2018 datasets to be used in the training and validation phases\n(ii) The brain tumor features were extracted and classified by utilizing DenseNet-169 for obtaining deeper and more generic features\n(iii) We propose a novel method named lightning attachment procedure algorithm for feature selection\n(iv) The performance measures are evaluated with optimization performance\nThe remainder of the paper is structured as follows: Section 2 is dedicated to related work. The preprocessing method and the phase of scheme are explained in Section 3. The experimental results of the advanced model is explain in Sections 4 and 5. Finally, in Sections 6, the findings and future work are discussed."
        },
        {
            "heading": "2. Related Works",
            "text": "Brain tumor optimization is embraced for tumor detection due to high mortality, and many researchers are floundering to diagnose brain tumors at early stages using several machine learning architectures. The very first step for brain tumor optimization is preprocessing Kumar (2020) [5] developed a deep learning algorithm that has been tuned called Dolphin-SCA correlated with D-CNN for effective classification and segmentation of brain tumor. After preprocessing, the segmentation is passed out by a vague deformable blending model with dolphin-SCA. And on power LDP and statistical characteristics, the feature extraction method is carried out. These features are used in D-CNN for the sorting of brain tumor with DSCA and were compared with the BraTS and Sim BraTS datasets. However, for effective treatments, the accuracy of the current model should be improved. Brain tumour segmentation approaches based on classical image processing and machine learning are not optimal enough among the currently suggested brain segmentation methods. After preprocessing feature extraction is the vital procedure which is done by Yin et al. (2020) [8], a novel metaheuristic-based technique for brain tumour early detection background removal, feature extraction, and classification using a multilayer perceptron neural network are the three key aspects of the proposed technique. The best selection of features and classification stages is achieved using an updated model of the whale optimization method based on chaos theory and logistic mapping approach. Furthermore, Alagarsamy et al. (2021) [11] used\nRE TR AC\na spatially constrained fish school optimization method (SCFSO) and an interval type-II vague logic system to address brain tumour abnormalities. SCFSO and IT2FLS can intervene and investigate large datasets and complicated cancers. The suggested approach provides a distinct separation of the tumour and nontumor regions, allowing for treatment preplanning. Huge database requirements and high computational time still pose a problem for deep learning. So, a lot of work is done for feature extraction in order to forecast the occurrence of a brain tumour. Deb and Roy (2021) [12] recommended a system to identify picture normalcy and abnormality; we used an adaptive fuzzy deep neural network with frog leap optimization. Classification is done by AFNN, and segmentation is done using adaptive flying squirrel algorithms. The accuracy gained by the proposed system is 99.6%. Additionally, various authors have proposed different feature extraction models to classify brain tumors. For the identification of brain tumours, Sharif et al. (2019) [13] designed a swarm optimization with a blending of characteristics which was used. In the initial stage, the head is taken out by the BSE technique. Then, the image is fed to PSO for segmentation. For feature selection, a genetic algorithm is used to extract LBP and deep features from segmented pictures. At last, the classification of tumor kinds is done using ANN and is compared with the RIDER and BraTS datasets.\nBecause of the time necessary for the training process, this suggested system has certain disadvantages, including a long processing time and decreased accuracy. Later, Cristin et al. (2021) [14] created an excellent tumour classification algorithm called fractional-chicken swarm optimization (fractional-CSO). To improve the accuracy, chicken swarm is combined with a derivative factor. The MR pictures have been preprocessed, and the features have been retrieved effi-\nciently. The tumour classification security level is achieved utilising deep recurrent neural networks that are trained using the suggested fractional CSO technique and have an accuracy of roughly 93.35 percent. The conventional methods lack accuracy in segmentation due to the complex spatial variation of tumors. Furthermore, Shivhare and Kumar (2021) [4] proposed the MLP (multilayer perceptron) to improve the accuracy of segmentation of brain tumours, to improve the accuracy of segmentation of brain tumours, and to improve the accuracy of segmentation of brain tumours. For this, three metaheuristic optimization algorithms GWO, AEFO, and SMO have been used. Grounded on the voting, a majority three models have been combined. The three brain tumor regions are segmented by different magnetic resonance modalities. The proposed system uses the BraTS dataset and can achieve 92% of DSC. Rammurthy and Mahesh (2020) [2] experimented with an automatic tumor classification model. This paper an optimized Whale Harris Hawks Optimization technique is used. The dissection process is done using cellular automata and rough set theory. Some features like size, LOOP, mean, variance, and kurtosis are extracted from segments. Detection done using D-CNN, wherein the training is done using the proposed model, is designed by WOH and HHO Algorithm. Different strategies for brain tumour classification have been created in the literature and, however, owed to inaccuracy, then inadequate result making the prevailing techniques have failed to give enhanced classification. Besides, Vilas et al. (2020) [10] proposed spontaneous brain tumor dissection using Dense Net. In this project, we recommend a Dense Net architecture for automatic segmentation based on CNN. The performance of Dense Net architecture against that of U-net is utilized, and the drawn analysis is compared with the BraTS dataset.\nRE\nTR\nAC TE"
        },
        {
            "heading": "3. Methodology Proposed",
            "text": "Skull scripting is adopted as a preprocessing in our proposed model, followed by Dense Net-169 for feature extraction and classification. Procedure for lightning attachment has been used to execute a feature selection step prior to categorization (PLA). The above procedures are used in the BraTS datasets from 2016, 2017, and 2018. The architecture for the suggested model is shown in Figure 2.\n3.1. Dataset Description. For the purpose of training and validation of the architecture that has been proposed, three datasets were used in this study. BraTS 2016, BraTS 2017, and BraTS 2018, respectively, were chosen as the datasets. Each database contains ground-truth photos for four distinct classes, including Flair, T1CE, T1, and T2. The datasets are divided into two distinct modes: LGG and HGG, with each distinct mode containing four stage tumours (T1-weighted, T1CE, T2-weighted, and flair) [15]. To resolve disparities, the data is removed from the skull, aligned to suit an anatomical template, and resampled at 1mm3 resolution. A volume (dimension) of 240 \u2217 240 \u2217 155 is assigned to each sequence [16]. All photos have anisotropic resolutions that are resampled to become isotropic. The 60% flair and T1CE photos, as well by using this picture, ground truth images are utilised to train a CNN model for the disunion technique. The remaining 40% of photos, from both classes, and 100% of T1 and T2 are used in the testing phase [1]. The BraTS datasets\u2019 training validation is shown in Table 1. Figure 3 shows a selection of photos from the BraTS collection.\n3.2. Skull Stripping as Preprocessing. Skull striping is the process of removing nonbrain structures and undesired picture sections from a scanned image in order to obtain the image\nnecessary for tumour identification. The brain, scalp, skull, and dura are all visible in the photograph. With the use of a cerebrospinal fluid rim, you can separate the undesired components (CSF). Intensity thresholding and morphological operations can be used to remove the skull and acquire the requisite brain region for tumour identification. Allow the input picture to be epitomised as a collection of pixels with intensity values at relevant locations in the image.\nLet Ip = Ip1, Ip2, Ipn = Ipn = Ipn = Ipn = Ipn = Ipn = Ipn = Ipn: \u00f01\u00de\nIn the case of IP1, the intensity levels of pixels 1 to n are represented by the integer Ipn. And np stands for the total number of pixels in a picture. Let us say the intensity threshold is T , and the circumstance for removing pixels extracted from the picture is that the intensity of those pixels is less\nTable 1: Training and validation datasets for the BraTS system.\nDataset Description\nBraTS-2016 [13] Each has 220 aberrant higher-grade glioma (HGG) photos and\n54 normal lower-grade glioma (LGG) images.\nBraTS 2017 [1] There have been a total of 431 examples in total, 285 training cases (210 HGG and 75 LGG)\nand 146 testing cases (both HGG and LGG) were included in the study.\nBraTS 2018 [1] There has been a total of 476 examples in total, 285 training cases (210 HGG and 75 LGG)\nand 191 testing cases for both HGG and LGG were included in the study.\nFLAIR\nT1\nT2\nT1c\nFigure 3: A selection of pictures from the Multimodal Brain Dataset (BraTS) [17].\nRE TR AC TE D\nthan T . Pixels that match this requirement are normally used to denote thin connections. The technique meets two conditions. The first is that nonbrain structures should be only weakly connected to the brain. The mask formed by intensity thresholding should also preserve as much brain as possible undamaged. Setting the proper threshold number is critical in this case, since setting it too low might result in the inclusion of garbage, which is undesired. Threshold levels that are too high can assist distinguish between brain and nonbrain structures, but they come at the expense of brain function.\nWe now have the requisite brain picture, which must be improved to make it suitable for the tumour identification procedure. We employ morphological procedures to do this. It also aids in the elimination of tight connections [18].\n3.3. Feature Extraction. By directly integrating all layers with the same feature sizes, DenseNet is able to solve the problem of gradient vanishing, which occurs frequently in deep CNN. The multilayer architecture of the DenseNet-169 is seen in Figure 3. The most compelling justification for utilising DenseNet as a feature extractor is due to the fact that as you delve further into the network, you will become aware of an increasing number of general features. The method for the extraction of features was carried out with the assistance of a densely connected convolutional neural network (DenseNet-169) that had been trained in advance. The variant that was used in this work was trained with the use of the ImageNet dataset, which is a large dataset that is accessible to the public.\nIn order to create the DenseNet169 architecture, one layer of complexity and amalgamation is placed at the beginning, followed by three conversion layers and four dense blocks. The classification layer comes after these previous stages have been completed. The first convolutional layer performs 7 \u2217 7 intricacies when stride 2 is used, and this is followed by a maximum pooling of 3 \u2217 3 when stride 2 is used. After then, there is a dense block in the network that is surveyed by three sets. Within each set is a conversion layer, and then, there is a dense block. Conversion layers are the names given to the layers that can be found between thick blocks. A batch normalisation layer and a 1 \u2217 1 convolutional layer come first in each of the network\u2019s conversion layers. Next comes a 2 \u2217 2 average pooling layer with a stride of 2 and finally comes a stride of 2.\nAs previously stated, there are four dense blocks, each of which has two intricacy layers, the first of which is of 1 \u2217 1, and the second of which is of size 3 \u2217 3. The four dense blocks of the DenseNet169 design pretrained on ImageNet are 6, 12, 32, and 32 pixels in size. Following this is the sorting layer, which does overall average pooling of 77% and finally followed by fully connected layer that uses \u201csoftmax\u201d as the activation [19].\n3.4. Classifier. In recent years, CNN has made significant improvement in a number of areas, including picture categorization. This is due to the fact that CNN networks are one of the most accurate technologies currently available for detecting characteristics in input pictures.\nIntricacial layers, initiation layers, set normalisation layer, and amalgamating layers are used for feature extraction in Dense Net-169, while dropout layers are utilised for classification.\n(i) Compressed layers, also known as completely linked layers, comprise numerous neurons or units, with the last thick layer has the same number of neurons as the number of categories. The activation layer is placed after each dense layer. The activation function used to the output of the final dense layer differs significantly from the sigmoid or softmax function used in the previous dense layers. In multiclass classification tasks, each category is assigned decimal probabilities using the Softmax function, with the target category having the greatest probability. The following formula is used to calculate the softmax of ith output unit\ny\u0302i = exi\n\u2211Ni exi , For i = 1, 2, 3,\u22ef,N , \u00f02\u00de\nwhere xi is the output of the ith dimension, N is the no. of dimensions, and y\u0302i is the probability associates with the ith category.\nDuring forecasting, a sample is assigned to the category with the highest likelihood, as indicated below.\ny\u0302i = maxi\u2208 1,N\u00bd y\u0302i: \u00f03\u00de\nIn binary classification problems, the sigmoid function is employed. It accepts any real number between 0 and 1 and returns a result that falls inside that range. The following equation is used to compute it numerically:\nSigmoid X\u00f0 \u00de = 11 + e\u2212x : \u00f04\u00de\n(ii) Dropout layers are a regularisation method used only during network training to prevent overfitting by temporarily removing a subsection of the inputted neurons and their connections from the thick layer before them. Except the last layer, which yields category-specific probabilities, the dense layers are normally surveyed by a loafer layer"
        },
        {
            "heading": "3.5. Feature Selection: Procedure for Lightning Attachment",
            "text": "(PLA). The suggested algorithm and its source of inspiration are shown in this section. Lightning is caused by an increase the total number of charges stored in the cloud, which results in a surge in electrical intensity. Precipitous will airstrike, and it may hit several times.\nThe four phases of attachment to precipitous technique are as follows: (1) breakup of air at the cloud\u2019s surface, (2) descendant motion of the lightning channel, (3) ascendant leader expansion, and (4) ultimate hit location [9].\nRE TR AC\nTE D\n3.5.1. Creativity. The four processes of lightning attachment are as follows: (1) air disintegration on the cloud surface, (2) descending exodus of the precipitous channel, (3) the formation and spread of rising leaders from the ground earthed objects, and (4) final leap. These steps are detailed in this article. (1) Cloud Edge Air Breakdown. As seen in Figure 4, the cloud\u2019s charge may be broken down into three pieces: a large amount of negative charge in the cloud\u2019s lower half, a large amount of affirmative charge in the cloud\u2019s superior part, and tiny amount of affirmative charge in cloud\u2019s bottom portion. The potential between the charge centres grows as the number of these charges grows, and it is possible for the negative charges to separate from the large positive charge section or the little positive charge component. As a result of this breakdown, power gradient near the cloud\u2019s edge rises, lightning forms, and a massive amount of electrical energy (mainly negative charge) flows toward the earth. Lightning may originate from several points, as evidenced by high-speed images of genuine lightning strikes [20].\n(2) Effort of the Downhill Leader Headed for the Earth. The precipitous approaches the ground in an ongoing motion as the air breakdown occurs near the cloud\u2019s edge. The precipitous comes to a halt after each stride, then continues in one or more different directions towards the earth. To comprehend this technique, envision a hemisphere underneath the leader tip with the midpoint of the leader tip and the ambit of the next step length after each step. On the surface of this hemisphere, there are several potential jump places to choose from. The next jump point is chosen at random, however, a place, with a greater electrical field value; it is more likely that the line connecting the leader tip and the matching point will be picked.\n(3) Fading Branches. The charge of the upper most division is allocated into innovative divisions if there are more than\none point for the following lightning jump. The same technique is followed for all new branches, resulting in the formation of new branches. When the charge on a branch falls below critical value (IC), there is no decomposition of air and so no additional movement. As a result, this branch would vanish.\n(4) Propagation of Upward Leaders. Clouds indicate that there is a massive negative charge above the ground. Positive charges clump together on the earth surface or on an earthed item underneath the cloud as a result of this. The intense electric field produces air breakup in the sharp points; thus, the upward leader begins there and spreads across the air. These upper leads accelerate their approach to the descending leader as it reaches the earth. The ascending leaders likewise go through the branching and branch fading process.\n(5) Final Leap (3.5.1.5). (Striking Point Determination). The ultimate jump happens whenever an ascending leader reaches a descending leader, and the striking point is the place where the upward leader began. All other branches vanish in this condition, and the cloud\u2019s charge is absorbed through this route [21].\n(6) The PLA Algorithm.\nStep 1. Trial spots. The trial locations indicate the downward leaders\u2019 beginning points, which is obtained as follows:\nXits = X i min + X i max \u2212 X i min \u2217 rand: \u00f05\u00de\nThe initial trial locations are denoted by Xits. The control variable\u2019s lowest value is Ximin, and its maximum value is Ximin. rand is a random number between 0 and 1. For the first places, the fitness function is determined as follows:\nDownward leader\nUpward leader\nFigure 4: The formation of charges in the cloud [9].\nRE TR AC TE\nD\nFits = obj X i ts : \u00f06\u00de Step 2. Determination of the next leap. The fitness values are calculated by averaging all of the original points: Xavr = mean Xts\u00f0 \u00de, Favr = obj Xavr\u00f0 \u00de: \u00f07\u00de The average point is denoted by Xavr, and the neutral purpose of the average point is denoted by Favr. As previously stated, the lightning has multiple paths where it jumps to the next highest optional point. A random solution j (potential point) is chosen to update the point I so i \u2260 j. The acquired answer is then compared to the possible solution. As a result, the following formula may be used to determine the next jump: Xits new = X i ts + rand \u2217 Xavr + X j PS IFFj < Favr, Xits new = X i ts \u2212 rand \u2217 Xavr + X j PS IFF j > Favr: \u00f08\u00de Step 3. Fading of sections. If the critical value is smaller than the electric field of the\nnew test point, the branch will stay continuous; otherwise, it will fade, as shown in the following diagram.\nXits = X i ts newIFF i ts new < F i ts: \u00f09\u00de\nIn this procedure, test points are run, and the first stage\u2019s leftover points are moved down.\nStep 4. Rising march of the leader. The ascending leader, which is spread throughout the canal significantly, moves the points up in this operation.\nAs a result, an exponent operator looks like this:\nS = 1 \u2212 t\ntmax\n\u2217 exp \u2212 t tmax : \u00f010\u00de\nIf t is the number of iterations, and tmax is the maximum number of iterations, and next leap is determined by the channel\u2019s charge, the next point is as follows:\nXits new = X i ts new + rand \u2217 s \u2217 X i best \u2212 X i worst , \u00f011\u00de\nwhere Xibest and X i worst are the best and the worst solutions among the populations.\nStep 5. No returns of lightning. When the down and up leaders get together, and the striking spot is assigned, the lightning operation comes to a halt [9]."
        },
        {
            "heading": "4. Performance Assessments",
            "text": "To assess the recommended ML model with optimization performance, performance metrics including accuracy, error rate, sensitivity, specificity, and F1-measure are employed.\nTrue positives (TP) are instances in which the ground truth image\u2019s tumour (1) data point is accurately identified as the segmented image\u2019s tumour (1) data point.\nTrue negatives (TN) occur when a ground truth image\u2019s nontumour (0) data point is accurately tagged as a segmented image\u2019s nontumour (0) data point.\nFalse positives (FP) happen when a ground truth image\u2019s nontumour (0) data point is incorrectly identified as a segmented image\u2019s tumour (1) data point.\nFalse negatives (FN) are when the ground truth image\u2019s tumour (1) data point is accurately tagged as the segmented image\u2019s nontumour (0) data point.\nThe number of positive and negative data points divided by the total number of data points is known as accuracy\nAccuracy = TP + TN\u00f0 \u00de TP + TN + FP + FN\u00f0 \u00de \u2217 100: \u00f012\u00de\nThe ratio of genuine positives to positive calls is known as precision or specificity. Positive predictive rate (PPR):\nSpecificity = TP\nTP + FP : \u00f013\u00de\nTable 4: Discussion of the BraTS 2018 dataset in comparison.\nTechniques Acc Spec Sens\nDolphin SCA+ FNB 95.3 95.3 97.7\nDWT+DBN 93.20 96 95\nBayesian HSC multi-SVNN 92.56 95.09 94.29\nFine-tuned CNN 92.42 95.04 94.05\nFractional CSO 93.35 96 95\nPLA + Dense Net-169 96.8 97 98\nRE TR\nThe chance of a positive test if the patient has a tumour is known as recall or sensitivity. It is also known as the truepositive rate: Sensitivity = TP TP + FN : \u00f014\u00de"
        },
        {
            "heading": "5. Result and Discussion",
            "text": "5.1. Contrast. The proposed PLA-Deep CNN mechanism is associated to four current techniques: (i) Dolphin Echolocation-based Sine Cosine Algorithm +fuzzy-based Naive Bayes (Dolphin-SCA+FNB) [5], (ii) DWT-deep belief network (DBN) (DWT+DBN) [22], (iii) Bayesian HCSmulti-SVNN [23], and (iv) Fractional\u2014the comparison study for BraTS 2016, 2017, and 2018 data is shown in Tables 2\u20134."
        },
        {
            "heading": "5.1.1. Using the BraTS 2016 Information, a Comparative Study Was Conducted",
            "text": "(i) Variation of data analysis for training\nUsing the BraTS 2016 database with different training data percentages, the recommended PLA-Deep CNN is\ncompared to current approaches like Dolphin-SCA + FNB, DWT + DBN, Bayesian HCS-multi-SVNN, and Fractional CSO + DRNN in relations of specificity, compassion, and precision. The specificity analysis findings for various training data percentages are shown in Figure 5(a). Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, and Fractional CSO +DRNN all have specificity scores of 93.57 percent, 94 percent, 93.89 percent, and 94 percent, respectively, with 90 percent training data. The proposed PLA-based Deep CNN technique has a high specificity, which means it can recognise negatives more accurately. The findings of the sensitivity parameter analysis utilising the BraTS 2016 database are shown in Figure 5(b). When the training data percentage is 70, the relevant sensitivity values measured by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and suggested PLA-based Deep CNN are 95.94 percent, 93 percent, 92.59 percent, 93 percent, and 95.9 percent. Figure 5 depicts the precision parameter analysis by the BraTS 2018 record (c). The precision values assessed by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and the suggested PLA-based Deep CNN are 93.35 percent, 91.38 percent, 90.98 percent, 93.75 percent, and 94 percent, respectively, once the exercise data\nRE\nproportion is 50. Among the existing techniques, the recommended PLA-based Deep CNN has the maximum accuracy, showing that it is capable of reliably recognising the tumorous region.\n(ii) Variation of K-Fold analysis\nFigure 5 depicts a qualified analysis plot based on specificity, understanding, and precision metrics by the BraTS 2016 record for different K-Fold values. Figure 6 depicts the results of a specificity examination for K-Fold values vacillating from 2 to 6. (a). For K \u2212 Fold = 2, the specificity standards measured by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multiSVNN, and Fractional CSO +DRNN are 0.767, 0.765, 0.691, 0.702, and 0.863, respectively. Figure 6 depicts the results of the sensitivity parameter analysis using the BraTS database\n(b). Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multiSVNN Fine Tuned CNN Fractional CSO +DRNN, and suggested PLA-based Deep CNN exhibit sensitivity values of 0.895, 0.958, 0.927, 0.965, and 0.981 when K \u2212 Fold = 2. Figure 6 depicts the precision parameter analysis using the BraTS record. (c). When K-Fold is equal to 4, the precision values assessed byDolphin-SCA+ FNB, DWT+DBN, Bayesian HCS-multi-SVNN Fine Tuned CNN Fractional CSO +DRNN, and suggested PLA-based Deep CNN are 0.796, 0.824, 0.795, 0.875, and 0.907.\n(iii) ROC analysis\nOnce FPR is 0.3, the analogous TPR standards restrained by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multiSVNN, Fractional CSO +DRNN, and projected and\nRE\nTR\nAC TE D\nproposed PLA-based Deep CNN are 0.905, 0.742, 0.712, 0.84, and 0.943, respectively. Furthermore, the suggested PLA-based Deep CNN outperforms the FPR = 0:8, 0.9, 1 for identifying tumour and nontumour areas with a TPR of 1."
        },
        {
            "heading": "5.1.2. Using the BraTS 2017 Information, a Comparative Study Was Conducted",
            "text": "(i) Variation of data analysis for training\nThe recommended PLA-based Deep CNN is compared to current approaches such as Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, and Fractional CSO +DRNN in terms of specificity, compassion, and precision using the different training data percentages from the BraTS 2017 database. Figure 7 depicts the findings of a research on specificity for different training data percentages (a). Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-\nSVNN, and Fractional CSO +DRNN have specificity values of 94.9 percent, 95 percent, 94.68 percent, and 95 percent, respectively, whereas suggested PLA-based Deep has a specificity of 95.7 percent with 90% training data. The suggested PLA-based Deep CNN approach has a high specificity, and as a result, it has a better capacity to properly recognise negatives. The sensitivity parameter analysis utilising the BraTS 2017 database is shown in Figure 7(b). The sensitivity values determined by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and suggested PLA-based Deep CNN are 96.8%, 94.5 percent, 94.5 percent, and 96.2 percent, respectively, once the physical activity data percentage is 70%. The accuracy parameter analysis utilising the BraTS 2018 database is shown in Figure 7(c). The precision values assessed by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and suggested PLA-based Deep CNN are 94.5 percent, 92.88 percent, 91.90 percent, 92.67 percent, and 95 percent, respectively, when the training data percentage is 50. The\n90 91 92 93 94 95 96 97 98 99\n50 60 70 80 90\nSp ec\nifi ci\nty (%\n)\nTraining data (%)\nDolphin-SCA + FNB DWT + DBN\nFractional CSO + DRNN PLA+ dense net 169\nBayesian HSC multi SVNN (a) Se\nns iti\nvi ty\n(% )\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n50 60 70 80 90 Training data (%)\nDolphin-SCA + FNB DWT + DBN\nFractional CSO + DRNN PLA+ dense net 169\nBayesian HSC multi SVNN\n(b)\n88\n90\n92\n94\n96\n98\n100\n50 60 70 80 90\nA cc\nur ac\ny (%\n)\nTraining data (%) Dolphin SCA + FNB DWT + DBN Bayesian HSC multi SVNN Fractional CSO + DRNN PLA + Dense Net 169\n(c)\nFigure 7: Analysis utilising the BraTS 2017 database with different training data. (a) Specificity, (b) sensitivity, and (c) accuracy.\nRE\nsuggested PLA-based Deep CNN has all the known techniques and has the highest accuracy, indicating that it is capable of accurately identifying the tumorous portion. (ii) Variation of K-Fold analysis\nFor varied K-Fold values, employing the BraTS 2018 database, Figure 8 depicts a relative analysis plot using specificity, sensitivity, and accuracy measures. Figure 7 shows the results of a specificity study for K-Fold values ranging from 2 to 6 (a). Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, and Fractional CSO +DRNN have specificity values of 0.798, 0.772, 0.708, 0.691, and 0.871, respectively, while the suggested PLA-based Deep CNN has a specificity of 0.842 for K \u2212 Fold = 2. The results of the sensitivity parameter analysis utilising the BraTS database are shown in Figure 8(b). When K \u2212 Fold = 2, the sensitivity\nvalues of Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and suggested PLA-based Deep CNN are 0.901, 0.962, 0.938, 0.942, and 0.971, respectively. Figure 8 shows the results of the accuracy parameter study using the BraTS database (c). When K \u2212 Fold = 4, the accuracy scores of Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and the suggested PLA-based Deep CNN are 0.881, 0.831, 0.801, 0.891, and 0.913, respectively.\n(iii) ROC-based analysis\nTPR values measured by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and proposed PLA-based Deep CNN are 0.912, 0.735, 0.831, 0.85, and 0.951, respectively, when FPR is 0.3. Furthermore, the suggested PLA-based Deep CNN\nRE\noutperforms the FPR = 0:8, 0.9, and 1 in categorising tumour and nonneoplastic areas with a TPR of 1."
        },
        {
            "heading": "5.1.3. Using the BraTS 2018 Information, a Comparative Study Was Conducted",
            "text": "(i) Variation of the training data analysis\nIn Figure 9, the suggested PLA-based Deep CNN is compared to current approaches such as Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, and Fractional CSO+DRNN in terms of specificity, compassion, and precision using the BraTS 2018 database for different training data percentages. Figure 9 shows the specificity analysis for different training data percentages (a). Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, and Fractional CSO +DRNN had specificity values of 95.3 percent,\n96 percent, 95.09 percent, and 96 percent, respectively, and recommended PLA-based Deep 97 percent for 90 percent training data. The suggested PLA-based Deep CNN approach has a high specificity, which means it has a better capacity to properly recognise negatives.\nUsing the BraTS 2018 database, the analysis in terms of the sensitivity parameter is shown in Figure 9(b). When the training data percentage is 70%, the sensitivity values assessed by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, Fractional CSO +DRNN, and proposed PLA-based Deep CNN are 97.7%, 95.29 percent, 95 percent, and 98 percent, respectively. Using the BraTS 2018 database, the accuracy parameter analysis is shown in Figure 9(c). The equivalent accuracy values measured by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN and Fractional CSO +DRNN and suggested PLA-based Deep CNN are 95.3 percent, 93.20 percent, 92.56 percent, 93.35 percent,\nRE\nTR\nAC TE D\nand 96.8 percent when the training data percentage is 50. Among the available approaches, the suggested PLA-based Deep CNN has the best accuracy, indicating that it is capable of accurately identifying the tumorous portion. (ii) Variable K-Fold analysis\nUsing the BraTS 2018 database, Figure 10 shows a relative investigation plan created on specificity, compassion, and precision characteristics for varied K-Fold values. Figure 10 shows the results of a specificity study for a range of K-Fold values from 2 to 6. (a). Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN, and Fractional CSO +DRNN have specificity values of 0.801, 0.795, 0.724, and 0.882, respectively, whereas the suggested PLA-based Deep CNN has a specificity of 0.851 for K \u2212 Fold = 2. Using the BraTS database, the examination in relations of the understanding parameter is shown in Figure 10(b).\nDolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multiSVNN, Fractional CSO +DRNN, and projected PLA-based Deep CNN have sensitivity values of 0.923, 0.971, 0.942, 0.959, and 0.981 when K \u2212 Fold = 2. Figure 10 shows the results of the precision parameter study using the BraTS record (c). When K-Fold is equal to 4, the precision values assessed by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multi-SVNN Fine Tuned CNN Fractional CSO +DRNN, and proposed PLA-based Deep CNN are 0.892, 0.843, 0.825, 0.893, and 0.924, respectively.\n(iii) ROC-based analysis\nAfter FPR is 0.3, the comparable TPR morals assessed by Dolphin-SCA + FNB, DWT +DBN, Bayesian HCS-multiSVNN, Fractional CSO +DRNN, and suggested PLA-based Deep CNN are 0.9276, 0.742, 0.845, 0.86, and 0.964. Furthermore, the suggested PLA-based Deep CNN outperforms the\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n6 7 8 9 10\nSp ec\nifi ci\nty\nK Value\nDolphin-SCA + FNB DWT + DBN\nFractional CSO + DRNN PLA+ dense net 169\nBayesian HSC multi SVNN\n(a)\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n6 7 8 9 10\nSe ns\niti vi\nty\nK Value\nDolphin-SCA + FNB DWT + DBN\nFractional CSO + DRNN PLA+ dense net 169\nBayesian HSC multi SVNN\n(b)\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n6 7 8 9 10\nA cc\nur ac\ny\nK Value\nDolphin-SCA + FNB DWT + DBN\nFractional CSO + DRNN PLA+ dense net 169\nBayesian HSC multi SVNN\n(c)\nFigure 10: Using the BraTS 2018 database, examine the effects of changing K-Fold. (a) Sensitivity, (b) accuracy, and (c) specificity.\nRE TR AC TE D\nFPR = 0:8, 0.9, and 1 in categorising tumour and nonneoplastic areas with a TPR of 1.\nIn this paper, a novel enhanced PLA is suggested as a complete technique for brain tumour classification based on optimum feature selection. For the validation of the proposed technique, two BraTS datasets were employed. The preprocessing technology is intended to aid in the categorization of brain tumours in brain imaging. Cristian et al. (2021) presented fractional-chicken swarm optimization (fractional-CSO) as a useful categorization approach. The cancer categorization is carried out once the brain pictures have been preprocessed and the characteristics retrieved efficiently. Using a simulated BraTS dataset, we achieved accuracy, specificity, and sensitivity of 93.35, 96, and 95 percent. For brain tumour diagnosis utilising MR images, Rammurthy and Mahesh (2020) introduces Whale Harris Hawks Optimization (WHHO), an optimization-driven approach. Maximum accuracy, sensitivity, and specificity for the proposed WHHO-based Deep CNN were 0.816, 0.974, and 0.791, respectively. On a set of benchmark cases, PLA\u2019s experimental findings are compared to those of other common optimizers, and the results are confirmed. According to the comparative results in the tables, employing the suggested technique for improving picture feature selection and ML classification produces good results when compared to existing optimization methods."
        },
        {
            "heading": "6. Conclusion",
            "text": "In this study, the procedure for lightning attachment (PLA) plus support vector machine (SVM) classifier is proposed as a brain tumour classification method for finding cancer locations from MRI data. Both of these classification methods are combined into one. Both the training and the validation of our proposed architecture made use of the datasets that we acquired, which were referred to as BraTS 2017 and BraTS 2018. Skull stripping, also known as the elimination of nonbrain structure and undesirable aspects of an image obtained from a scanned photograph in order to obtain the necessary imaging for the identification of a tumour, is a technique that is used to remove components that are not requested. In this particular scenario, the preprocessor known as \u201cskull stripping\u201d is utilised. The characteristics are extracted by DenseNet-169, which then produces features that are more general for the deeper network. Following that, the procedure for lightning attachment is used to the feature selection process. This population-based strategy got its start because of the physical phenomena that occur during the lightning attachment technique. These phenomena include air interruption, descending leader movement, ascending leader inception and dispersion, and ultimate leap. Our research utilised a classifier known as a sustainment vector machine (SVM), which brings us to the next stage of the process, which is classification. Because it is a binary classifier that is based on supervised learning, it differentiates between two classes by building a hyperplane in high-dimensional feature space. This allows it to process information in a more efficient manner. The reliability of the system may be improved by increasing the amount of\ndata points. The process of accurately classifying things may lead to the discovery of other features that are significant in this regard. This computerised method could also be used to categorise other brain diseases in addition to other medical photographs of various pathological situations, types, and states of disease.\nData Availability\nThe data shall be made available on request.\nConflicts of Interest\nThe authors declare that they have no conflict of interest."
        }
    ],
    "title": "Retraction Retracted: Metaheuristic Optimization-Driven Novel Deep Learning Approach for Brain Tumor Segmentation",
    "year": 2024
}