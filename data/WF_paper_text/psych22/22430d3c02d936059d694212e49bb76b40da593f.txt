The prevalence of autism spectrum disorder (ASD) has risen significantly in the past two decades. Unfortunately, there is a shortage of mental health providers who have specialized training in delivering evidenced-based services to this population. Early intensive behavioral intervention (EIBI) is an evidenced-based treatment recommended for toddlers with ASD, and school psychologists are uniquely positioned to help children with ASD receive it. However, many school psychologists do not receive adequate training in this subspecialty. This paper makes recommendations to school psychology training programs about how to add or improve training in this subspecialty based on the results of an Office of Special Education Programs grant-funded ASD training program which involved collaboration between a NASP-approved and APA-accredited school psychology training program and a community-based early intensive behavioral intervention (EIBI) clinic. The grant supported development of an interdisciplinary didactic and clinical training program to increase the ASD knowledge, skills, and competencies of school psychology graduate students, with the broader goals of developing a replicable training model and increasing the workforce of trained providers for this underserved population. Fifteen graduate students completed the training program. Outcomes related to trainee knowledge, skills, and competencies, trainee satisfaction, and lessons learned over time analyzed within a logic model that guided the project’s development and execution can be informative for other school psychology programs undertaking training in this subspecialty. methods  setting Researchers from the psychology department of a University Center for Excellence in Developmental Disabilities Education, Research, and Service (UCEDD) housed within a public university located in the Midwest and faculty from an APAaccredited and NASP-approved school psychology training program formed a collaboration to develop a “Specialization Track in Toddlers and Preschoolers with Autism Spectrum Disorders.” The program was funded through a Personnel Preparation Grant #H325K140306 awarded by the Office of Special Education Programs. The grant funding supported each student with a stipend and travel expenses for inter-campus commute, along with support for professional development seminars. Training occurred both at the university as a part of the school psychology trainees’ core curriculum and in a nonprofit, community-based EIBI clinic, the Autism Care for Toddlers (ACT) clinic, which was affiliated and staffed by personnel from the UCEDD. At the onset of the grant, the ACT clinic served approximately 10 toddlers/preschoolers with ASD per day, but after 2 years into the grant, the clinic expanded and served approximately 24 toddlers/preschoolers (age range 14 to 54 months) per day. The ACT Clinic used a
1 3
3-tiered delivery of care model in which RBTs delivered ABA procedures under the supervision of master’s-level BCBAs. A doctoral-level BCBA directed the clinic and provided supervision to master’s level BCBAs. For more information about the ACT Clinic and its development, see Mathews et al., (2018). The behavioral programming for the individual preschool/ toddlers was developed by the BCBAs with guidance of the Verbal Behavior -Milestones Achievement Placement Program (VB-MAPP; Sundberg, 2008). The VB-MAPP is an assessment tool, a curriculum guide, and skill tracking system that has been validated as an evidenced based curriculum for early learners with ASD and developmental disabilities (Barnes et al., 2014; Gould et al., 2011; Vietze & Lax, 2020). The curriculum is based on Skinner’s analysis of verbal behaviors, developmental milestones, and tenets of the field of ABA. development and utility of the logic model Development of a logic model was one of the first activities undertaken by the program’s creators. The logic model presented in Fig. 1 describes (a) situational factors relevant at the initiation of the program, (b) goals of the program, (c) inputs into the program provided by all parties, (d) outputs (products that resulted from project activities), and (e) short-term, intermediate, and long-term outcomes. The logic
model served to frame activities as the program was organized and to help the personnel stay on course as the program developed over time. The situation clearly delineates the problem of the significant shortage of personnel trained and qualified to serve and address the needs of toddler and preschoolers with ASD, while the goals explicitly guide the framework of the need to prepare and train school psychology students in delivering early intervention services. The situation and goals remained the impetus and sustaining force which stayed on track throughout the 5-year project. The inputs (what the project will invest) were essential during the planning stages of the project so the grantees could assess what resources (i.e., faculty qualified to teach, access to students, community support, complementary ongoing grants, collaborative partnerships, and relationships) could be leveraged to support success in the programming. The outputs (a direct product of the project activities) described at the onset of the grant entailed the learning activities of the school psychology trainees along with the expected number and types of patients served. The outcomes reflected the change in people and systems and were measured as short term, intermediate, and long term. The short-term outcomes (i.e., 1–2 years) focused on the trainee skill development and the increased number of children receiving access to care. The intermediate outcomes (i.e., 3–5 years) reflect the knowledge imparted to the trainees who will be
1 3
prepared to implement and disseminate evidenced based early intervention services in schools and community clinics. The long-term outcomes (i.e., 5–10 years) describe the process of dissemination of this model program with the expectation that similar programs may be developed. It is the goal that more school psychology programs will incorporate similar training programs to help increase the workforce of qualified personnel and thus improve access to early intensive behavioral intervention services. Ongoing evaluation of the satisfaction and impact of the program through yearly interviews with the trainees occurred resulting in adjustment to the program. The trainee’s feedback along with advisory board meetings were particularly helpful to assure trainee needs would be met. trainees Over the 5-year period of the program, 19 school psychology trainees were recruited, two left for medical reasons, one left for academic reasons, and one received only 1 year of the program because of entering during the last year of the training program. Thus, 15 of the participants completed the entire 2-year ASD specialization track program. The specialization was separated into four cohorts across five academic years. Cohort 1 consisted of five graduate students, cohorts 2 and 3 consisted of four students, and there were two students in cohort 4. Nine of the students would go on to pursue a PhD in school psychology, and six would go on to complete an Educational Specialist degree in school psychology. The training program required approximately 13 h of training activities per week (e.g., didactic seminars, clinical experiences, and assignments) followed by 120 h of clinical externship experiences. faculty and trainers A BCBA predoctoral intern and a master’s-level BCBA served as initial project managers and developed the original didactic curriculum and evaluation tools. Both project managers were BCBA supervisors at the EIBI clinic that offered didactic and clinical training to the students participating in this training program. The project co-directors also presented didactic lectures and student support with the small N study. Additionally, two clinician/faculty from the UCEDD, four EIBI clinic BCBA supervisors, one community resource expert, one speech-language therapists, one local school BCBA consultants, and one parent advocate provided guest lectures in the ASD-specific didactic curriculum. curriculum for school psychology trainees  didactic The BCBA task list (2012) served as the primary source of guidance in development of the curriculum for the school
psychology students. The curriculum consisted of didactic and clinical experiences throughout the duration of the first 2 years. Didactic coursework was delivered via synchronous 90-min seminars bi-weekly for the entirety of the first year in the program and for the first half of the second year. Additionally, two workshops (Conjoint Behavioral Consultation and Cultural Competency) were held every other year and training for the Autism Diagnostic Observation Schedule – 2nd ed was held yearly. Students also were expected to attend at least one additional professional development activity (e.g., conference, workshop), related to ASD during each year of the program. There were several written and oral assignments and projects required of the students which were designed to focus on the toddler/preschooler ASD population. See Table 1 for details on topic seminars, assignments, and workshops (additional information, syllabus, Power Point® presentations, and readings are available upon request). clinical The trainees completed 4 to 6 h per week of clinical observation and/or direct treatment implementation at the EIBI clinic. Initial clinical experience primarily consisted of observation of the therapeutic session, which included structured and unstructured 1:1 therapy and small group activities. As the students acquired knowledge and skills relevant to treatment implementation and with the discretion of the BCBA supervisor, they began to participate in the delivery of 1:1 service with support from the clinic staff. Seven students underwent a 40-h RBT didactic training and passed the examination to become certified as RBTs. The students in the latter cohorts did not have this opportunity because of the stringent demands of maintaining delivery of clinical hours and supervision requirements as an RBT. After completion of the second year of training, students completed a 120-h externship focused on direct care with children with ASD in a setting of their choice, preferably with young children. However, because of the COVID pandemic, four of the students were unable to access these experiences for the full required amount of time, so additional clinical experiences with school-aged children (aged 6–12 years of age) were supplemented in community or school settings. outcomes measures Data were collected continuously for the purpose of assessing the trainees’ competencies and satisfaction (assessment tools available upon request). A project-developed 13-item self-assessment tool was completed by each student at four time-points: prior to program year 1 (pre or baseline), postyear 1, post-year 2, and at the completion of the clinical
1 3
externship (referred to as post-year 3 for simplicity). This self- assessment tool was developed by the program directors and evaluated ASD knowledge topics and skill competencies which included (a) characteristics and diagnostic criteria, (b) prevalence and risk factors, (c) screening and diagnostic tools, (d) early intensive ABA evidenced-based interventions, (e) conducting a literature review, and (f) the Verbal Behavior Milestones Assessment and Placement Program (VB-MAPP). The instrument was divided into three subscales: ASD basic knowledge and skills (7 items), psychology-specific knowledge and skills (3 items), and special education-specific knowledge and skills (3 items). Students were asked to rate on a Likert scale of 1–5 (lowest to highest) their knowledge and practice competency for each item. Scores from this measure resulted in knowledge and competency subscales, and total knowledge scores. An additional 3-item subscale measuring the student’s knowledge in research and intensive early intervention programming (e.g., conducting a literature review and a small N study, and using the VB-MAPP for individualized programming) was piloted at all four time points with cohort 3 and with other students in the other three cohorts resulting in a total of 4 students at timepoint 1 (baseline), 8 students at timepoint 2 (post-year 1), 10 students at timepoint 3 (postyear 2), and 15 students at timepoint 4 (post-externship). The development of this pilot subscale resulted from the project director’s realization that the 13-item instrument was missing these items which were the primary focus of program year 2; however, this realization did not occur until after the first two cohorts had completed program year 2. Thus, cohort 3 received the expanded 16-item instrument at all four timepoints; however, cohort 4 inadvertently received the original 13-item instrument at baseline. The university faculty and clinical staff at the EIBI clinic evaluated the students on a 5-point scale (lowest–highest)
in respect to professionalism behaviors yearly. A 12-item checklist of expected professionalism included characteristics such as appropriate attire, grooming, confidentiality, respect for clients and staff, and promptness. Following participation in the professional development activity, students provided a narrative summary and critique of the activity attended. Faculty members scored this assignment with a rubric from 1.0 (lowest) to 4.0 (highest). The literature review, small N study, and dissemination assignments were graded by the faculty using rubrics from 1.0 (lowest) to 4.0 (highest) which measured criteria specific for each assignment (e.g., purpose of the paper, relevance to ASD, supporting evidence, measuring targeted behaviors, organization, creativity, APA formatting). The quality of the weekly seminars and workshops were evaluated by the students using a 4-point Likert scale rating tool from 1.0 (lowest) to 4.0 (highest). This evaluation tool measured the presenter using criteria of presentation, organization, ability for student participation, and usefulness. The students completed a 13-item survey with a 5-point Likert scale from 1.0 (lowest) to 5.0 (highest) upon completion of the program that queried their overall satisfaction with the program of their knowledge and skills learned in autism and delivery of early intervention services. In addition, annually, students completed an open-ended qualitative reflections questionnaire with six items (i.e., level of support provided in the training program, effectiveness of the program leaders, strengths and weakness of the seminars and the required projects, their comfort level in being a leader in the field, and their overall happiness with the program and recommendations for program improvement) for program years 1 and 2, and an additional item for program year 2 that queried the students on how well adjustments were made in the program based upon their recommendations. See Table 2 for the reflection questionnaire items. Table 1 Curriculum for school psychology trainees
Year 1 seminar topics Year 1 written assignments Year 2 seminar topics Year 2 written assignments Workshops (each cohort attended once)
Introduction to ASD Literature review Evidenced-based early intensive behavior intervention for ASD Small N project Cultural competency
Early signs of ASD Critique of early intervention strategies Building rapport (didactic and role play) Dissemination project Autism diagnostic observation schedule-2nd edition
ASD diagnostic assessment Professional development summary reflection Verbal behavior (didactic and role play) Professional development summary reflection
Conjoint behavioral consultation
Understanding research and evidence Data collection Visual analysis and data-based
decision making Supporting parent advocacy Interdisciplinary collaboration Transition to kindergarten Functional assessment/func-
tional analysis Creating summary Reports
Assessing preference and maintaining motivation (didactic and role play) Concurrent chains Skill Acquisition techniques Prompting techniques (didactic with role
playing) Behavior reduction techniques School-based interventions Caregiver and staff training data analysis  quantitative The data presented were determined to be a program evaluation by the university Institutional Review Board (IRB). The data were entered into Excel for storage and data management, and SPSS version 25 was used for statistical analysis. Descriptive analyses were used for the written assignments, student professionalism, and student satisfaction for the program workshops and the overall program satisfaction. Average self-assessment scores were plotted across the four measurement occasions (baseline and post years 1, 2, and 3). Repeated measures analysis of variance (RM-ANOVA) models were performed to assess significance of change over the course of the program. Huynh–Feldt adjusted degrees of freedom were used to account for any deviations from sphericity, and non-parametric Friedman’s tests were assessed to validate findings. Wilcoxon signed rank tests were used to assess significant changes between adjacent time points. Finally, non-parametric correlations (Spearman’s rho) were used to test relationships between self-assessment scores and overall program satisfaction. A significance level of 0.05 was used for all analyses (see Table 3). qualitative Qualitative methodological approaches include processes that seek to gain an understanding of phenomena being sought (Creswell, 2018). In this training project, qualitative methods were used to gain an understanding of students’ experiences with various programmatic components. This
was primarily conducted through an annual written reflection questionnaire (described in “Outcomes Measures”). The questionnaire was provided in written format to the students within approximately one month following completion of their first and second year. For two items, prompts were provided specifically to solicit both strengths and weaknesses (items 3 and 4), and item 7 included a specific prompt for suggestions for program improvements. A conceptual content analysis (Creswell, 2012) was used to analyze the qualitative results from the annual student reflection questionnaire in order to determine the content and the frequency with which these types of content occurred across students and across the years in which the program was implemented. An external evaluator received the written responses from the students (to maintain anonymity in
Table 3 Descriptive statistics for program assessments
Assessment Mean Standard deviation Range
Literature review 3.65 .41 3.0–4.0 Single-case design study 3.75 .52 3.0–4.0 Dissemination project 3.89 .49 3.7–4.0 Professional development self-reflec-
tion yr 1 3.91 .37 3.5–4.0
Professional development self-reflection yr 2 Student professionalism yr 1 Student professionalism yr 2
3.92 4.86 4.83 .55 .47 .47
3.8–4.0 4.45–5.0 4.4–5.0
Student satisfaction with workshops 3.73 .39 3.0–4.0 Student satisfaction with overall
program 4.62 .49 4.07–5.0
1 3
responses provided). The external evaluator then provided the program instructors with two documents: a complete compilation of responses for each item (literally, using copy and paste) and an abbreviated summary of responses for each item, synthesizing the positive and negative responses. See Table 5 for the quantity and types of exemplar quotes related to the five questionnaire item prompts (items 1–5) that sought to gain understanding of the students’ experience with the program as it related to their perceptions of how the program impacted their competencies in autism in young children. The criteria used for coding written comments as “positive” or “negative” were as follows. Positive statements were those that stated or implied that the participant liked, loved, enjoyed, helpful, satisfied, or emphasized the program or aspects of the program. This also included phrases such as “great job with… or a huge thumb’s up for…” Negative statements were those that stated or implied that the participant disliked, did not enjoy, a weakness, or wished some aspect(s) of the program were different. This included statements that were made about redundancy in training (e.g., a content component that was already included as part of another course in their training program). Comments were coded as “neutral” if they did not fit the criteria to be coded as “positive” or “negative.” This included comments that simply stated that the participant participated in a training seminar. However, it is worth noting that there were very few/no neutral comments identified. The process used for coding participant comments as positive, negative, and neutral follows. First, the project co-evaluator read through the comments and assigned a code of positive, negative, or neutral. Then, the project evaluator engaged in the same process: read through the comments and assigned a code of positive, negative, or neutral. The project co-evaluator compared the two independent coders’ assigned codes for agreement or disagreement. There was 100% agreement. Had there been any disagreements, the two coders would have discussed the disagreements and come to consensus. If consensus would not have been able to be achieved, then a third member of the project team would have coded those comments for which consensus had not been achieved, and the three coders would have discussed and come to consensus. Had there still been no consensus, then a “majority rules” would have determined the final comment code. For data analysis of the comments, we totaled the positive plus negative comments to derive a denominator. Then, we calculated the percentage of positive comments by dividing the positive comments by the total (positive plus negative). We did the same to yield a percentage of negative comments. We did not include neutral comments in the calculation of the denominator. This was, in part, because there were so few/no neutral comments coded. Regardless
of the occurrence or absence of neutral comments, comments coded as neutral would have been omitted from the calculation of the denominator because including them in the count could have distorted the accuracy of percentages of positives and negatives. Further, we were only interested in positive and negative comments because of their value in program improvement: positive comments informed retaining components, and negative comments prompted making improvements to the program. results  quantitative Descriptive statistics for program assessments (e.g., assignments, professionalism, student satisfaction) are depicted in Table 3. The results show the average scores for the written and oral assignments ranged from 3.66 to 3.92 (4-point Likert scale), (1 = unsatisfactory, 4 = exceptional) and 4.83–4.86 (5-point Likert scale) (1 = never/rarely, 5 = always) for faculty-scored professionalism. The student satisfaction scores for the training workshops ranged from 3.69 to 3.78 (4-point Likert scale) (1 = strongly disagree, 4 = strongly agree) and the overall satisfaction with the program was 4.62 (5-point Likert scale) (1 = strongly disagree, 5 = strongly agree). The student self-assessment results over the training years appear in Table 4. Average scores over the course of the training considerably increased over time for total scores and all subscales, with the greatest increases in the first year (see Fig. 2). Correlations indicated that trainee satisfaction at the end of the program was significantly positively correlated with student self-assessments at the end of the program (post year 3) for total assessment scores (r = 0.779, p = 0.001), psychology-specific competencies (r = 0.783, p = 0.001), and special education-related competencies (r = 0.679, p = 0.005). Positive, but nonsignificant relationships were seen between satisfaction and basic autism competencies (r = 0.469, p = 0.078), as well as research-related competencies (r = 0.488, p = 0.065). Further replication with a larger sample size is needed to verify these relationships. Seven of the 17 students were RBT certified, but no significant differences were observed based on RBT certification on subscale or total scores using a Mann–Whitney U test. qualitative The responses to the annual written reflection questionnaire are summarized in Table 5. There were 164 positive comments provided and 56 negative comments provided, translating to 75% positive comments (164 positive comments/220 total comments); thus, even with direct prompts
for “weaknesses,” there were approximately 3:1 positive to negative comments provided. Items 6 and 7 were used for slightly different purposes from items 1–5. Specifically, item 7 was used primarily to solicit feedback from the students regarding recommendations for improving the training program. The program implementation team carefully considered recommendations provided and made changes to the program based on feasible and reasonable suggestions. Item 6 was used to gage the student’s perceptions of how well their feedback from the previous year was reflected in changes to the program the following year. Therefore, items 6 and 7 were excluded from the conceptual content analysis
for this questionnaire and thus were not part of the frequency count reported in Table 3. However, students, especially in the first and second cohorts, did provide many recommendations for program changes. This made sense given that this was a new program. Responses to the part of item 7 that asked, “Overall, are you happy with the training provided by this specialization?” were overwhelmingly positive such that no one said that they were not happy with the training. In addition, responses to item 6 were also overwhelmingly positive across students and across the years of program implementation, indicating that the students noticed and appreciated that the program leaders did use their feedback to make changes. Table 4 Descriptive statistics and analyses of self-assessment
Bold typeface indicates significance at p < .05
Time Wilcoxon signed rank tests
Baseline T2 T3 T4 ANOVA T1-T2 T2-T3 T3-T4
Total score N 17 17 15 15 Mean 1.49 3.43 3.95 4.39 F(3,42) = 148.59, p < .001 < .001 .007 .008 SD 0.41 0.43 0.46 0.33
Basic N 17 17 15 15 Mean 1.67 3.69 4.05 4.50 F(2.8,39.9) = 126.29, p < .001 < .001 .023 .006 SD 0.56 0.48 0.38 0.33
Psych N 17 17 15 15 Mean 1.08 3.06 3.69 4.00 F(2.7,38.3) = 72.85, p < .001 < .001 .045 .154 SD 0.22 0.56 0.79 0.58
SPED N 17 17 15 15 Mean 1.47 3.35 3.98 4.38 F(2.6,36.1) = 91.08, p < .001 < .001 .030 .020 SD 0.41 0.53 0.73 0.50
Research and IEI N 4 8 10 15 Mean 1.59 3.21 4.20 4.56 F(3,9) = 34.4, p < .001 .068 .027 .078 SD 0.79 0.66 0.36 0.37
1 3
The concerns which were identified by the students were addressed, and modifications were implemented. For example, after the first year, students in cohort 1 reported the assignment of reading an ABA specific book and completing a book report was too time consuming, so a change in assignment to critiquing of an evidenced based treatment intervention versus a “pop” culture treatment was assigned for the subsequent cohorts. Positive feedback was reported from the subsequent students regarding the helpfulness of this assignment. During the second year of cohort 1, students reported a lack of communication about expectations and structure in the clinical setting. In response, one of the BCBA supervisors was assigned as a designated training “contact” person who assigned students to the supervisors and assured content learned in the didactic training was then applied in the clinical setting. The students in cohort 2 requested more time to be spent doing clinical work during the first year of the programming, thus, thereafter students were assigned to begin 3 h per week of clinical experiences during the first year of the program. discussion This unique specialization track which trained school psychology students addresses a significant challenge of increasing the workforce of qualified personnel to serve toddlers and preschoolers with ASD using evidenced based interventions. Based upon the expected short-term outcomes addressed in the logic model, the data suggests that students’ knowledge and skills increased because of engagement in the specialization track in ASD. All students reported positive gains in self-assessment in understanding the diagnostic criteria, diagnostic measures and delivering evidenced based early interventions services to young children. Although it is premature to evaluate, it would be expected these students once graduated will be positioned to advocate and direct programming for intensive early interventions services in the preschool setting. In respect to the intermediate goals described in the logic model, dissemination of this program and training model may prompt faculty in school psychology programs to consider an endeavor such as this. Although measuring child outcomes is beyond the scope of this paper,
Table 5 Exemplar quotes related to reflection questionnaire items
Item 1: How would you describe the level of support provided by this training opportunity? a. Do you feel you were provided support prior to and after training began? b. Were you aware of all paperwork to be completed and all required training activities? Positive (32 comments): “Yes, all staff have been very supportive throughout the first year in the program. Prior to starting on our first year in the grant we met with two staff members that explained all the program expectations, requirements, training opportunities, and paperwork.” Negative (16 comments): “In the beginning, it was difficult to understand exactly what was required of us for the year.” “There seemed to be a lot of confusion on what we were capable of doing, and what kind of support we needed.”
Item 2: What is your opinion of the project management leadership team (e.g., directors and project managers)? a. Did they effectively manage the Seminar and Externship experiences? b. Were they effective leaders? Positive (35 comments): “I thought the leadership team was very knowledgeable and dedicated to the field.” “The project managements leadership team was amazing. I appreciated all their help, support and guidance.” “The project leader provided great feedback throughout the year to help us improve our writing skills, presentation and clinic skills.” Negative (10 Comments): “ I would like to see more communication between grant leaders and staff in the clinic.” “BCBAs were not sure what was expected of them.” “Some confusion as to how the externships would be scheduled.”
Item 3: What is your opinion of the seminar series you completed this year? a. What do you consider the strengths of this experience? b. What do you consider the weaknesses of this experience? Positive (34 comments): “Diverse group of speakers – all really good!” “.. was full of interesting and important information.” “Well organized and well connected to the area of ASD.” Negative (9 comments): “More advanced material or more specific material to the ACT clinic.” “Provide more readings between seminars.” “Some repetitiveness and redundancy with their structured course work in the psychology program.”
Item 4: What is your opinion of the required projects you completed this year? a. What do you consider the strengths of these projects? b. What do you consider the weaknesses of these projects? Positive (31 comments): “I liked how the dissemination projects made me critically think about what I had learned and how it could be practically applied in different settings.” “The dissemination project was fun to conceptualize and really made me think critically about gaps in knowledge in the larger community and I was able to create something that may be useful in my professional career to share with educators.” “Small-n project was a great experience.” Negative (12 comments): “I would have liked to be more involved in ADOS administrations.” “More guidance for dissemination project.” “More clear communication about the expectations of the project.”
Item 5: Do you feel that this specialization is providing you with the training you need to be a leader in Toddlers with Autism Spectrum Disorders services in your future career? a. Why do you feel this way? Positive (32 comments): “Yes. I’ve already been able to use what I’ve learned across situations and many of my peers have recognized I have a wide range of knowledge of ASD because of my training.” “I feel as though I have strong knowledge of the population and feel competent sharing information with colleagues, parent training, and providing direct service after one year in the program.” Negative (9 comments): “Future careers would not include being an RBT thus they would prefer more practical experiences with assessment and program planning.” “I feel less prepared to work in a supervisory role, which is the role I most likely work in my future career.”
1 3
anecdotal reports of clinical improvement in skill acquisition among the children is verifiable. Finally, the long-term outcomes of increased school psychology education in the topic areas of early intervention in the ASD population may be realized with policy change at the national and international level through professional organizations such as NASP. The high volume of children requiring services with ASD will only be met when all providers involved in their care are well trained and competent in meeting their unique needs and delivery of evidenced based services. One interesting finding is that the relationship between the student satisfaction and the perceived basic autism and research related competencies was not robust. There are several possible explanations for this, but most likely the sample size of 15 may not yield sufficient power to draw conclusions for this correlation. Additionally, the satisfaction with the training program may not be a strong predictor of perceived competency. Although all students increased in knowledge and skill acquisition, the content and experiences delivered may not have been sufficient for students to have achieved the confidence to meet mastery level, particularly for those students who had previous limited experience with children with ASD and delivery of early intervention services. Finally, for those students who were trained in the 4th cohort, the completion of the small N study and “in person” clinical experiences were reduced because of the COVID pandemic possibly leading to a decreased perceived competency. The specialization track described may serve as a model for other school psychology training programs or subgroups of a cohort interested in specializing in delivering services for the population of young children with ASD. Below are recommendations based on lessons learned through the development and implementation of this program. recommendations  pursue collaboration with applied settings using a compatible service‑delivery model Collaboration is essential to the proper establishment and running of programs such as this and must occur from the very beginning (Sweizy et al., 2008; Gardner et al., 2022). There must be a compatibility between the university program’s training model and the training site’s service-delivery model. This compatibility and the reasonable proximity of the two programs were important factors that brought the collaborators together. The school psychology students were receiving training in applied behavior analysis, consultation, assessment, and single-case experimental design methodology as a part of their core curriculum, which fit well with the ABA services delivered in the clinic. Given the strong evidence supporting ABA services to the ASD population and the increasing prevalence of ABA services,
school psychology programs providing training in this area may find ready collaborators. ABA training may help school psychology students obtain some of the necessary practicum experiences necessary to qualify for the BCBA credential. Obtaining the BCBA credential can help school psychologists develop a specialization in the field and even find jobs. A Burning Glass Technologies (2015) report commissioned by the Behavior Analysis Certification Board found an increase in job postings from 146 to 425 for clinical, counseling, and school psychologists demanding behavior analyst skills. It also found that in lists of jobs requiring behavior analyst credentials between 2012 and 2014, the title of “school psychologist” was consistently in the top ten job titles (third in 2012, ninth in 2013, and fourth in 2014). But for school psychology trainees to benefit from this, a mutual commitment to a training model like this must be made. This commitment is at the core of collaboration and helps all parties to work through challenges and difficulties when they inevitably arise. Close collaboration must also continue on an ongoing basis to effectively coordinate training across sites, evaluate outcomes, and make necessary adjustments over time. Strong collaboration helps to assure continuity of the program over time when personnel changes, problems, and crises like a worldwide pandemic occur (Ortiz & Levine, 2021). develop a logic model to plan your program Logic models provide a simple visual representation of how a program or organization works to accomplish its goals while specifying intended outcomes. This tool provides accountability and allows programs to communicate how they are functioning and how well they are doing (W.W. Kellogg Foundation, 2004). Kekahio et al. (2014) provide a helpful description of the utility of logic models when they state, “Developing a logic model at the beginning of program planning gives you a framework for charting the links between your program’s resources, activities, and outputs and its intended outcomes. It enables you to evaluate your program once it begins. And it helps you communicate to your stakeholders what you want to accomplish, how you intend to reach your goals, and how you will track your progress” (p. 1). Expanding a professional psychology program’s training opportunity is a large undertaking and requires careful planning, including a vision for the purpose of the program, conceptualizing its interconnected features, coordination between different elements of the program, and a conceptualization of the potential impact of the program. The logic model served to frame activities as the program was organized and to help the personnel to stay on course as the program developed over time. Although logic models generally contain the same basic elements, the names for
1 3
those elements and their visual representation can vary from model to model, which gives programs flexibility in how they portray their project. The most important thing is the planning that goes into taking a general idea for a project and conceptualizing how all the pieces will be fit together as a coherent picture. As a partnership is being formed across sites, trainers should propose to collaboratively develop a logic model with their colleagues at the training site (see Kekahio et al., 2014 &W.W. Kellogg Foundation, 2004). This activity will frame the project for both parties and allow them to communicate to their constituents who may wonder whether the program adds value to the existing services. It will also help both parties evaluate whether the program is having its intended impact or whether it needs to be revised or terminated. provide supplemental on‑site training Much of the training that school psychology trainees are receiving should be useful to them in the community based EIBI clinic. In the current program, the core curriculum included courses in ABA, school-based consultation, academic and behavioral assessment, professional ethics, developmental psychopathology, single-case experimental designs, school mental health and behavioral interventions, and a school-based practicum, all of which met the requirements for both NASP and APA accreditation. But because of unique characteristics of the clinic setting, programs should plan to deliver supplemental training, preferably on-site and with professionals who are knowledgeable about the clinic program. supplemental assignments were given to engage the students at a deeper level Specifically, the students completed a literature review on a topic of their choice related to ASD and gave an oral presentation to program faculty and peers during their first year in the program. They completed a single-case experimental design study during their 2nd year concurrently with a universitybased class on single-case experimental designs. The faculty guided the students in development of the research project, supervision during data collection, and dissemination of the results. Examples of study topics were “Using Natural Environment Teaching (NET) to Improve the Generalization of Peer Directed Mands” and “Increasing Functional Play in the Presence of High Level Restricted and Repetitive Behaviors.”
Active learning involves didactic and supplemental assignments which augments students' training by providing greater depth and breadth to areas to which they have not been previously exposed, which has potential to increase their skills and expand the professional settings in which they are qualified to work (Fayombo & Campus, 2014). This
too is an area in which collaboration can be beneficial to the students. Program faculty may already be aware of areas where program training can be strengthened, and clinic personnel can identify areas of needed training based on their observations of the trainees in the clinic setting from a more impartial point of view. The clinic personnel may be able to deliver some of the training, as was the case in this project, or identify qualified individuals to provide such training. start early and provide a continuum of training experiences A significant feature of the program was that students spent 4 to 6 h a week as a part of their 13-h a week training commitment for 2 years at the same clinic, beginning with observations and increasing responsibilities as students gained more training, supervision, and experience. Continuity at the same site over 2 years meant that the students could work on a longer-term basis with the toddlers (watch their development and programming for a longer time frame) and the professional staff, which gave them a longer-term perspective than if they had been in the site for only a year. use the clinic training experience to expand competency assessments Following decades of discussion on identifying professional competencies in psychology, an influential special issue of Training and Education in Professional Psychology devoted to the topic sought to provide a coherent framework for conceptualizing, operationalizing, and assessing professional competencies in graduate training programs (Fouad et al., 2009; Kaslow et al., 2009; Price et al., 2017). The NASP and APA organizations both encourage and require ongoing competency assessment of students as a part of their accreditation requirements. Continuous, systematic evaluation of students at the clinic site can provide supplemental information about individual students’ growth along with data for formative evaluation of the program. An advantage of this approach is that assessments can be conducted by professionals outside of the program, bringing a much-needed external perspective on students’ progress toward independence. In our program, we developed and used assessments such as the professionalism evaluation form and self-assessment form which are similar to rating scales or checklists used by training programs (e.g., practicum supervisor evaluation forms, clinic supervisor evaluation forms, and internship supervisor evaluation forms). Accredited programs may adopt similar assessments to the ones used in this program, use other available assessments (e.g., Price et al., 2017), or they may already have assessments of their own which can be adapted. It is
1 3
important, however, for programs to collaborate with clinic professionals to refine them to (a) match the objectives of the site and (b) reflect priorities that clinic professionals may have in terms of professional skills, behaviors, and attitudes. continuously evaluate outcomes Continuous evaluation is vital to individual student development, program improvement, and accountability to accreditors and constituents (Hernandez, 2012). We would recommend encouraging students to deliver feedback based upon their individual needs and previous experiences and adapt the programming to assure their continual skill acquisition and ability to meet their education goals. Competency assessments provide programs an opportunity for self-examination and when those assessments are carried out in independent training sites, program faculty can learn things about the quality of their program. The current program’s evaluation measures were developed as a part of a grant project. School psychology training programs have more extensive requirements for competency assessment. Fortunately, the APA (https:// www. apa. org/ ed/ gradu ate/ compe tency) and NASP (https:// www. naspo nline. org/) have a number of resources for doing assessments that can span both functions, student competency assessment, and program outcome reporting. limitations There are numerous limitations to be noted. First, it is important to note that this paper is not an empirical study but rather a program evaluation. Thus, the evaluation measures were developed for the purpose of determining the outcome of increase in student knowledge and skill acquisition, and student satisfaction with the programming. Although not formally evaluated, the feasibility of collaborating with a pre-existing well established university school psychology program and a UCEDD center was attempted and successfully achieved. Another limitation was the attrition of several students. Although two students left for medical reasons, the number of total students trained was less than the expected amount. Finally, although written assignments and projects were graded by the faculty, the primary outcome measurements of knowledge and skill acquisition were based upon student perceived self-report. conclusion With the increasing prevalence of young children diagnosed with ASD, the demand for qualified personnel and behavioral health work force to meet the diagnostic and
clinical services needed for toddlers/preschoolers in the school system and community remains a daunting task. To identify early signs, diagnose in a timely manner, and assure early intervention services are delivered, clinicians from a variety of related disciplines need to join forces in identifying focused training tracks within their specialty (Gardner et al., 2022). Although NASP (2015) calls for school psychologists to be prepared to assess and identify young children with ASD, and deliver evidenced based interventions, many school psychologists feel ill equipped and unprepared to meet this need (Harris et al., 2020). The described specialization track provides an exemplar that is feasible to implement within a school psychology program. Although this project was funded through a Personnel Preparation grant, other programs may not have grant funding. Thus, academic leaders may find that developing a similar program may be feasible with the collaboration between institutes and clinical enterprises with a common goal of training and creating a pipeline of workforce for the clinic. There were many lessons that were learned throughout the 5-year program with resulting modifications to the curriculum and clinical experiences. Some examples of modifications were suggested by the trainees which included more time in the clinic setting and assisting with writing behavioral programs. The COVID-19 crisis interrupted the final cohort’s training sequence, requiring the second half of program year 2 learning experiences to be primarily via remote learning. Nonetheless, silver linings for learning opportunities arose. Most of the students completed their final year of the program observing virtually modified diagnostic evaluations, delivery of ABA interventions, and parentmanagement training interventions. The notable successes of this project were that all 15 students showed improvement in the knowledge, skills, and competencies in delivering evidenced based care to children with ASD. The training curriculum was conducive to active learning such that both university-based and clinic-based didactic training occurred concurrently with the applied clinical experiences provided through the community-based facility. Finally, high levels of satisfaction with the program were expressed by the trainees and faculty members. The collaboration between interprofessional and inter-agencies exemplified in this program enhanced the scope, depth, and breadth of the training experience for the graduate students and improved satisfaction of faculty members across both institutions. 1 3
Acknowledgements The authors would like to acknowledge the dedicated staff at the Autism Care for Toddlers clinic who worked so diligently and professionally in assisting the graduate students with the didactic and clinical training. Without your dedicated service, the outcomes of this grant would not have come to fruition. Author Contribution Therese Mathews, Gina Kunz, Ashley Lugo, Paige McArdle, and Katy Menousek contributed to the study conception and design, material preparation, data collection, and implementation of the curriculum and education of the students. The data analysis were performed by Gina Kunz and Kevin Kupzyk. The first draft of the manuscript was written by Therese Mathews, and Ed Daly contributed greatly to revisions and critical review of the manuscript. All authors commented on previous versions of the manuscript. All authors read and approved the final manuscript. All team members contributed to the revisions of the manuscript. Funding This project was supported by the Office of Special Education Personnel Preparation Grant #H325K140306. declarations Ethics Approval This study was granted exemption from the Institutional Review Board of the University of Nebraska Medical Center. It was determined by the IRB that this project was deemed a program evaluation. Consent to Participate Informed consent was obtained from all participants in the project. Consent to Publish All authors consent to publication of this manuscript. Conflict of Interest The authors declare no competing interests.