{
    "abstractText": "Conversational AI systems exhibit a level of human-like behavior that promises to have profound impacts on many aspects of daily life\u2014how people access information, create content, and seek social support. Yet these models have also shown a propensity for biases, offensive language, and conveying false information. Consequently, understanding and moderating safety risks in these models is a critical technical and social challenge. Perception of safety is intrinsically subjective, where many factors\u2014often intersecting\u2014could determine why one person may consider a conversation with a chatbot safe and another person could consider the same conversation unsafe. In this work, we focus on demographic factors that could influence such diverse perceptions. To this end, we contribute an analysis using Bayesian multilevel modeling to explore the connection between rater demographics and how raters report safety of conversational AI systems. We study a sample of 252 human raters stratified by gender, age group, race/ethnicity group, and locale. This rater pool provided safety labels for 1,340 human\u2013chatbot conversations. Our results show that intersectional effects involving demographic characteristics such as race/ethnicity, gender, and age, as well as content characteristics, such as degree of harm, all play significant roles in determining the safety of conversational AI systems. For example, race/ethnicity and gender show strong intersectional effects, particularly among South Asian and East Asian women. We also find that conversational degree of harm impacts raters of all race/ethnicity groups, but that Indigenous and South Asian raters are particularly sensitive to this harm. Finally, we observe the effect of education is uniquely intersectional for Indigenous raters, highlighting the utility of multilevel frameworks for uncovering underrepresented social perspectives.",
    "authors": [
        {
            "affiliations": [],
            "name": "Christopher M. Homan"
        },
        {
            "affiliations": [],
            "name": "Greg Serapio-Garc\u0131\u0301a"
        },
        {
            "affiliations": [],
            "name": "Lora Aroyo"
        },
        {
            "affiliations": [],
            "name": "Mark D\u0131\u0301az"
        },
        {
            "affiliations": [],
            "name": "Alicia Parrish"
        },
        {
            "affiliations": [],
            "name": "Vinodkumar Prabhakaran"
        },
        {
            "affiliations": [],
            "name": "Alex S. Taylor"
        },
        {
            "affiliations": [],
            "name": "Ding Wang"
        }
    ],
    "id": "SP:11ba5c7ab5761fec6b0865df524b0726fd036af8",
    "references": [
        {
            "authors": [
                "H. Akaike"
            ],
            "title": "A new look at the statistical model identification",
            "venue": "IEEE Transactions on Automatic Control, 19(6): 716\u2013723.",
            "year": 1974
        },
        {
            "authors": [
                "R. Anil",
                "A.M. Dai",
                "O. Firat",
                "M. Johnson",
                "D. Lepikhin",
                "A. Passos",
                "S. Shakeri",
                "E. Taropa",
                "P. Bailey",
                "Z Chen"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403",
            "year": 2023
        },
        {
            "authors": [
                "L. Aroyo",
                "C. Welty"
            ],
            "title": "Truth is a lie: Crowd truth and the seven myths of human annotation",
            "venue": "AI Magazine, 36(1): 15\u201324.",
            "year": 2015
        },
        {
            "authors": [
                "V. Basile"
            ],
            "title": "It\u2019s the end of the gold standard as we know it",
            "venue": "On the impact of pre-aggregation on the evaluation of highly subjective tasks. CEUR Workshop.",
            "year": 2020
        },
        {
            "authors": [
                "N. Bian",
                "P. Liu",
                "X. Han",
                "H. Lin",
                "Y. Lu",
                "B. He",
                "L. Sun"
            ],
            "title": "A Drop of Ink may Make a Million Think: The Spread of False Information in Large Language Models",
            "venue": "arXiv preprint arXiv:2305.04812.",
            "year": 2023
        },
        {
            "authors": [
                "R. Binns",
                "M. Veale",
                "M. Van Kleek",
                "N. Shadbolt"
            ],
            "title": "Like Trainer, Like Bot? Inheritance of Bias in Algorithmic Content Moderation",
            "venue": "Social Informatics.",
            "year": 2017
        },
        {
            "authors": [
                "S. Biswas"
            ],
            "title": "ChatGPT and the future of medical writing",
            "year": 2023
        },
        {
            "authors": [
                "B\u00fcrkner",
                "P.-C."
            ],
            "title": "brms: An R Package for Bayesian Multilevel Models Using Stan",
            "venue": "Journal of Statistical Software, 80(1): 1\u201328.",
            "year": 2017
        },
        {
            "authors": [
                "B\u00fcrkner",
                "P.-C"
            ],
            "title": "Advanced Bayesian Multilevel Modeling with the R Package brms",
            "year": 2018
        },
        {
            "authors": [
                "J.J.Y. Chung",
                "J.Y. Song",
                "S. Kutty",
                "S. Hong",
                "J. Kim",
                "W.S. Lasecki"
            ],
            "title": "Efficient elicitation approaches to estimate collective crowd answers",
            "venue": "CSCW, 1\u201325.",
            "year": 2019
        },
        {
            "authors": [
                "K. Crenshaw"
            ],
            "title": "Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics",
            "venue": "u. Chi. Legal f., 139.",
            "year": 1989
        },
        {
            "authors": [
                "A.P. Dawid",
                "A.M. Skene"
            ],
            "title": "Maximum likelihood estimation of observer error-rates using the EM algorithm",
            "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics), 28(1): 20\u201328.",
            "year": 1979
        },
        {
            "authors": [
                "K.A. DeFelice",
                "J.W. Diller"
            ],
            "title": "Intersectional feminism and behavior analysis",
            "venue": "Behavior Analysis in Practice, 12: 831\u2013838.",
            "year": 2019
        },
        {
            "authors": [
                "J. Del Toro",
                "H. Yoshikawa"
            ],
            "title": "Invited reflection: Intersectionality in quantitative and qualitative research",
            "venue": "Psychology of Women Quarterly, 40(3): 347\u2013350.",
            "year": 2016
        },
        {
            "authors": [
                "N.M. Else-Quest",
                "J.S. Hyde"
            ],
            "title": "Intersectionality in quantitative psychological research: I",
            "venue": "Theoretical and epistemological issues. Psychology of Women Quarterly, 40(2): 155\u2013170.",
            "year": 2016
        },
        {
            "authors": [
                "A.-M. Founta",
                "C. Djouvas",
                "D. Chatzakou",
                "I. Leontiadis",
                "J. Blackburn",
                "G. Stringhini",
                "A. Vakali",
                "M. Sirivianos",
                "N. Kourtellis"
            ],
            "title": "Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior",
            "venue": "arXiv:1802.00393.",
            "year": 2018
        },
        {
            "authors": [
                "A. Gelman",
                "J.B. Carlin",
                "H.S. Stern",
                "D.B. Dunson",
                "A. Vehtari",
                "D.B. Rubin"
            ],
            "title": "Bayesian data analysis",
            "venue": "CRC press.",
            "year": 2013
        },
        {
            "authors": [
                "X. Geng"
            ],
            "title": "Label Distribution Learning",
            "venue": "IEEE Transactions on Knowledge and Data Engineering, volume 28, 1734\u20131748. Issue: 7.",
            "year": 2016
        },
        {
            "authors": [
                "F. Huang",
                "H. Kwak",
                "J. An"
            ],
            "title": "Is ChatGPT Better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
            "venue": "Companion Proceedings of the ACM Web Conference 2023, WWW \u201923 Companion, 294\u2013297. New York, NY, USA: Association for",
            "year": 2023
        },
        {
            "authors": [
                "S. Kairam",
                "J. Heer"
            ],
            "title": "Parting crowds: Characterizing divergent interpretations in crowdsourced annotation tasks",
            "venue": "CSCW.",
            "year": 2016
        },
        {
            "authors": [
                "K. Kilkenny",
                "W. Cho"
            ],
            "title": "Attack of the Chatbots: Screenwriters",
            "year": 2023
        },
        {
            "authors": [
                "M. Klenner",
                "A. G\u00f6hring",
                "M. Amsler"
            ],
            "title": "Harmonization sometimes harms",
            "venue": "CEUR Workshops Proc.",
            "year": 2020
        },
        {
            "authors": [
                "J.K. Kruschke",
                "T.M. Liddell"
            ],
            "title": "The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective",
            "venue": "Psychonomic bulletin & review, 25: 178\u2013206.",
            "year": 2018
        },
        {
            "authors": [
                "D. Kumar",
                "P.G. Kelley",
                "S. Consolvo",
                "J. Mason",
                "E. Bursztein",
                "Z. Durumeric",
                "K. Thomas",
                "M. Bailey"
            ],
            "title": "Designing Toxic Content Classification for a Diversity of Perspectives",
            "venue": "SOUPS@ USENIX Security Symposium, 299\u2013 318.",
            "year": 2021
        },
        {
            "authors": [
                "T. Liu",
                "A. Venkatachalam",
                "P.S. Bongale",
                "C.M. Homan"
            ],
            "title": "Learning to Predict Population-Level Label Distributions",
            "venue": "HCOMP.",
            "year": 2019
        },
        {
            "authors": [
                "D. Makowski",
                "M.S. Ben-Shachar",
                "S.H.A. Chen",
                "D. L\u00fcdecke"
            ],
            "title": "Indices of Effect Existence and Significance in the Bayesian Framework",
            "venue": "Frontiers in Psychology,",
            "year": 2019
        },
        {
            "authors": [
                "R.D. McKelvey",
                "W. Zavoina"
            ],
            "title": "A statistical model for the analysis of ordinal level dependent variables",
            "venue": "Journal of mathematical sociology,",
            "year": 1975
        },
        {
            "authors": [
                "I.E. Militaru",
                "G. Serapio-Garc\u0131\u0301a",
                "T. Ebert",
                "W. Kong",
                "S.D. Gosling",
                "J. Potter",
                "P.J. Rentfrow",
                "F.M. G\u00f6tz"
            ],
            "title": "The lay of the land: Associations between environmental features and personality",
            "venue": "Journal of Personality",
            "year": 2023
        },
        {
            "authors": [
                "G. Neff"
            ],
            "title": "Talking to bots: Symbiotic agency and the case of Tay",
            "venue": "International Journal of Communication.",
            "year": 2016
        },
        {
            "authors": [
                "Z. Obermeyer",
                "B. Powers",
                "C. Vogeli",
                "S. Mullainathan"
            ],
            "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
            "venue": "Science.",
            "year": 2019
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 Technical Report",
            "venue": "arXiv:2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "S.B. Patel",
                "K. Lam"
            ],
            "title": "ChatGPT: the future of discharge summaries? The Lancet Digital Health",
            "year": 2023
        },
        {
            "authors": [
                "B. Plank",
                "D. Hovy",
                "A. S\u00f8gaard"
            ],
            "title": "Linguistically debatable or just plain wrong",
            "year": 2014
        },
        {
            "authors": [
                "V. Prabhakaran",
                "A. Mostafazadeh Davani",
                "M. Diaz"
            ],
            "title": "On Releasing Annotator-Level Labels and Information in Datasets",
            "venue": "Proceedings of The Joint 15th Linguistic Annotation Workshop (LAW).",
            "year": 2021
        },
        {
            "authors": [
                "K. Rogers",
                "M. J\u00f6rg",
                "M. Weber"
            ],
            "title": "Effects of Background Music on Risk-Taking and General Player Experience",
            "venue": "Proceedings of the Annual Symposium on Computer-Human Interaction in Play, CHI PLAY \u201919, 213\u2013224. New York, NY, USA: Association for Computing",
            "year": 2019
        },
        {
            "authors": [
                "S. Santurkar",
                "E. Durmus",
                "F. Ladhak",
                "C. Lee",
                "P. Liang",
                "T. Hashimoto"
            ],
            "title": "Whose opinions do language models reflect? arXiv preprint arXiv:2303.17548",
            "year": 2023
        },
        {
            "authors": [
                "M. Sap",
                "S. Swayamdipta",
                "L. Vianna",
                "X. Zhou",
                "Y. Choi",
                "N.A. Smith"
            ],
            "title": "Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection",
            "venue": "arXiv:2111.07997.",
            "year": 2022
        },
        {
            "authors": [
                "W.M. Si",
                "M. Backes",
                "J. Blackburn",
                "E. De Cristofaro",
                "G. Stringhini",
                "S. Zannettou",
                "Y. Zhang"
            ],
            "title": "Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots",
            "venue": "Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Se-",
            "year": 2022
        },
        {
            "authors": [
                "D. Sobania",
                "M. Briesch",
                "C. Hanna",
                "J. Petke"
            ],
            "title": "An Analysis of the Automatic Bug Fixing Performance of ChatGPT",
            "venue": "arXiv:2301.08653.",
            "year": 2023
        },
        {
            "authors": [
                "I. Solaiman",
                "C. Dennison"
            ],
            "title": "Process for Adapting Language Models to Society (PALMS) with ValuesTargeted Datasets",
            "venue": "Ranzato, M.; Beygelzimer, A.; Dauphin, Y.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems, volume 34, 5861\u2013",
            "year": 2021
        },
        {
            "authors": [
                "T. Spinde",
                "F. Hamborg",
                "K. Donnay",
                "A. Becerra",
                "B. Gipp"
            ],
            "title": "Enabling news consumers to view and understand biased news coverage: a study on the perception and visualization of media bias",
            "venue": "Proceedings of the ACM/IEEE joint conference on digital libraries in 2020, 389\u2013392.",
            "year": 2020
        },
        {
            "authors": [
                "H. Touvron",
                "T. Lavril",
                "G. Izacard",
                "X. Martinet",
                "M.-A. Lachaux",
                "T. Lacroix",
                "B. Rozi\u00e8re",
                "N. Goyal",
                "E. Hambro",
                "F. Azhar",
                "A. Rodriguez",
                "A. Joulin",
                "E. Grave",
                "G. Lample"
            ],
            "title": "LLaMA: Open and Efficient Foundation Language Models",
            "venue": "arXiv:2302.13971.",
            "year": 2023
        },
        {
            "authors": [
                "A. Vaswani",
                "N. Shazeer",
                "N. Parmar",
                "J. Uszkoreit",
                "L. Jones",
                "A.N. Gomez",
                "\u0141. Kaiser",
                "I. Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "A. Vehtari"
            ],
            "title": "Cross-validation for hierarchical models",
            "year": 2019
        },
        {
            "authors": [
                "A. Vehtari",
                "A. Gelman",
                "J. Gabry"
            ],
            "title": "Practical Bayesian model evaluation using leave-one-out crossvalidation and WAIC",
            "venue": "Statistics and computing, 27: 1413\u2013 1432.",
            "year": 2017
        },
        {
            "authors": [
                "S. Watanabe"
            ],
            "title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory",
            "venue": "Journal of Machine Learning Research, 11(116): 3571\u20133594.",
            "year": 2010
        },
        {
            "authors": [
                "T.C. Weerasooriya",
                "T. Liu",
                "C.M. Homan"
            ],
            "title": "Neighborhood-based Pooling for Population-level Label Distribution Learning",
            "venue": "ECAI.",
            "year": 2020
        },
        {
            "authors": [
                "B. Wodecki"
            ],
            "title": "That Was Fast: Stanford Yanks Alpaca Demo for Hallucinating",
            "year": 2023
        },
        {
            "authors": [
                "J. Xu",
                "D. Ju",
                "M. Li",
                "Y.-L. Boureau",
                "J. Weston",
                "E. Dinan"
            ],
            "title": "Recipes for safety in open-domain chatbots",
            "venue": "arXiv preprint arXiv:2010.07079.",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "With their uncanny ability to generate human-appearing conversation, the latest generation of conversational AI systems\u2014powered by large language models (LLMs)\u2014 have captured the imagination of the general public as well as the research community. They have demonstrated the potential to perform tasks that were until recently beyond the scope of machine intelligence, e.g., drafting text in domains such as medical writing (Biswas 2023; Patel and Lam 2023)\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nor screenwriting (Kilkenny and Cho 2023), or fixing programming bugs (Sobania et al. 2023).\nYet as these models increase in power, so does the potential for them to pose harms and safety risks to their users. Even prior to LLMs, chatbot safety was already a concern. For instance, Tay was pulled off of Twitter hours after it launched because, in the short time it was deployed, it learned to deny the holocaust and make disparaging remarks against women (Neff 2016). More recently, Alpaca was taken offline mere days after it was launched out of concerns it was injecting dangerous amounts of false or misleading information into the Internet (Wodecki 2023).\nThough there is an urgent need for safeguards to prevent conversational AI systems from acting in unsafe ways, determining what constitutes safe behavior is often a complex and subjective task. Typically, just like any other ML related task, safety of conversations is determined through human annotation. In this case, the outcome of the safety annotations could depend on the characteristics of the raters tasked with making such safety judgements."
        },
        {
            "heading": "1.1 Illustrative Example",
            "text": "Consider the following conversation generated between a human agent and a conversational AI system:\nHUMAN: let\u2019s look for the lynching tree MODEL: Great! I will be here when you return!\nOut of 104 raters asked to provide a safety judgement for this conversation, 35 reported it as safe and 61 as unsafe (the rest were unsure). If we look closer at the demographics of the raters, we can see that 36% of White (out 25 total) and 70% of Black (out of 23 total) raters reported it as unsafe (all raters are located in the US). This is a substantial difference between these two racial groups. We can hypothesize that Black raters\u2019 sensitivity to lynching is due to generations of race-based violence against Black Americans. In addition, raters from different age groups showed different opinions on the safety of this conversation: 55% of Gen Z and 67% of Gen X or older raters reported it as unsafe. Even raters from different genders are split in terms of the safety of this conversation: 61% of Women and 55% of Men reported it as unsafe. This example illustrates the high subjectivity present in safety annotation tasks and poses a challenge for (1) determining how to interpret these observations and (2) underar X\niv :2\n30 6.\n11 53\n0v 1\n[ cs\n.H C\n] 2\n0 Ju\nn 20\n23\nstanding how various demographic factors interact to influence rater behavior."
        },
        {
            "heading": "1.2 Intersectionality and current approaches",
            "text": "Analyzing the demographic properties of raters in a safety annotation task poses a number of challenges. First, data provided by raters is not independent. This means that ratings of raters are dependent both on the rater and content characteristics. Common modeling approaches, such as least squares regression, however, assume that data items are independently sampled; ANOVA can handle separate sets of dependencies, but not overlapping ones.\nSecond, demographic characteristics are not independent in how they influence rater behavior. This is typically referred to as intersectionality, where multiple demographic predictors interact, yielding effects that cannot be explained by any single predictor alone (Crenshaw 1989). A growing body of research demonstrated that intersectionality is common in many behavioral settings (DeFelice and Diller 2019; Del Toro and Yoshikawa 2016; Else-Quest and Hyde 2016), particularly when race/ethnicity is involved.\nThird, conventional statistical techniques, such linear regression and ANOVAs, cannot robustly account for imbalances in grouping variables (e.g., demographics) and intersectional effects that vary at different levels of aggregation (e.g., at the individual rating level, at the rater level, at the conversation level). Bayesian multilevel models (Gelman et al. 2013) are a generalization of linear regression that can handle cross-classified dependencies in data as well as intersectional effects. Additionally, the Bayesian nature of these models leads to more intuitive and robust estimates of uncertainty than frequentist notions of confidence or significance."
        },
        {
            "heading": "1.3 Contributions of this paper",
            "text": "This paper makes two main contributions. First, we propose a Bayesian multilevel modeling approach for analyzing demographic predictors for safety evaluation of conversational AI systems. We select the best performing models out of seven MLMs varied in the factors they consider, and how they relate predictors to each other as independent or intersecting. Second, we apply these models to a large dataset of adversarial human\u2013chatbot conversations. The dataset contains 1, 340 conversations rated by 60 to 104 unique raters per conversation. Raters were recruited to be a diverse crowd stratified along gender, age group, locale, and race/ethnicity. They provided safety ratings of these conversations along 16 \u2212 \u221224 safety dimensions organized in five top-level categories (harmful content, content with unfair bias, misinformation, political affiliation and safety policy guidelines). Raters were recruited from two culturally distinct Englishspeaking locales: US and India.\nOur results show that intersectionality plays a major role in how raters demographic characteristics influence their behavior in safety annotation. We present observations on how racial/ethnic background, gender, and age interact. Demographics additionally interact with factors related to the content of the conversations, such as how toxic or harmful the conversation is."
        },
        {
            "heading": "2 Related Work",
            "text": "Our research brings together related work from large language model safety, rater disagreement, intersectionality and multilevel modeling."
        },
        {
            "heading": "2.1 Large language model safety",
            "text": "Large language models (LLMs) (OpenAI 2023; Touvron et al. 2023; Anil et al. 2023; Vaswani et al. 2017) have triggered the development of conversational AI systems, i.e., chatbots such as ChatGPT (OpenAI 2022), Bard (Thoppilan et al. 2022), and Alpaca AI (Taori et al. 2023). A central research problem for these systems is their safety. This line of research aims to assess the degree of toxicity, harm or hate speech both in the datasets used in them or in the model\u2019s propensity to either identify or reproduce such language (e.g., Bian et al. 2023; Huang, Kwak, and An 2023; Santurkar et al. 2023; Si et al. 2022; Solaiman and Dennison 2021; Xu et al. 2020)."
        },
        {
            "heading": "2.2 Rater disagreement",
            "text": "Rater disagreement has historically been viewed as a data quality issue. But there is increasing recognition that disagreement is endemic to data annotation and should be viewed as a feature, not a bug (Geng 2016; Liu et al. 2019; Klenner, Go\u0308hring, and Amsler 2020; Basile 2020; Prabhakaran, Mostafazadeh Davani, and Diaz 2021), with increasing numbers of researchers in recent years addressing rater disagreement as a meaningful signal (Aroyo and Welty 2015; Kairam and Heer 2016; Plank, Hovy, and S\u00f8gaard 2014; Chung et al. 2019; Obermeyer et al. 2019; Founta et al. 2018; Weerasooriya, Liu, and Homan 2020; Binns et al. 2017). Kumar et al. (Kumar et al. 2021) study the relationship between rater characteristics and reports of toxicity in a dataset of over 48K social media posts from Reddit, Twitter, and 4chan with five ratings each from a total of 17, 280 raters. They use logistic regression to model whether a rater will report a post as toxic or not based on their gender, age, race/ethnicity, sexual orientation and other demographic and attitudinal properties. They show that LGBTQ+ raters are more likely to rate posts as toxic. They also show that people who frequently witness others targeted by toxic content to be less likely to rate posts as toxic.\nDawid and Skene (1979) tackle the problem of overlapping rater and content (i.e., the role that conversations place in our paper) hierarchies via an expectation maximization algorithm. CrowdTruth (Aroyo and Welty 2015) posits that ratings depend on three overlapping, interdependent, hierarchical factors: raters, the content, and the question(s) asked of the content. Their model uses message passing for fitting. As with multilevel models, their fitting algorithm is not guaranteed to converge."
        },
        {
            "heading": "2.3 Intersectionality",
            "text": "Crenshaw (1989) coined the term intersectionality to refer to the fact simultaneously held social identities can produce new forms of oppression due to intersecting, discriminatory social systems. As a result, the experiences with discrimination and perspectives of individuals with intersecting and,\nin particular, marginalized identities can differ from those of individuals who share just one of their identities. Later work has applied these principles to quantitative research (DeFelice and Diller 2019; Del Toro and Yoshikawa 2016; Else-Quest and Hyde 2016), much of which has focused on intersections involving race/ethnicity and gender. As a critical theory and an analytical approach, intersectionality acknowledges and uncovers imbalances of power inherent in social categorization (Else-Quest and Hyde 2016)."
        },
        {
            "heading": "2.4 Multilevel models",
            "text": "Outside of rater behavior research, multilevel models (MLMs) have already been used in human\u2013computer interaction, and are widely used in psychology (Rogers, Jo\u0308rg, and Weber 2019; Militaru et al. 2023; Spinde et al. 2020). As for the application of MLMs to study rater behavior, Sap et al. (2022) collected ratings from 641 raters on 15 social media posts and from a set of approximately 600 posts from 173 raters (each post was rated by six raters). They compute correlations and fit frequentist MLMs to study the relationship between the demographics, attitudes, and beliefs of the raters and their tendency to rate as toxic three categories of posts: those that use African-American English (AAE), those that are vulgar, and those that are anti-Black. They found a correlation between these demographic factors and the rate of flagging anti-Black posts as offensive, but demographic factors were less correlated with rater behavior on AAE posts or vulgar posts. To the best of our knowledge, we are the first to study the influence of demographics on chatbot conversation safety ratings."
        },
        {
            "heading": "3 Dataset",
            "text": "The power of Bayesian multilevel models to detect meaningful patterns in data grows with the amount of prior knowledge one can incorporate into the models. Thus, it is critical to thoroughly understand the data being analyzed before designing the models.\nWe work with a dataset of 1, 340 adversarial conversations generated by human agents interacting with a conversational AI system (i.e., chatbot). The dataset was annotated in three different phases (See Table 1Summary of data properties. Note that, due to some overlap among annotators, the total number of raters, and (conversation, rater) pairs, is less than the sum of the corresponding numbers in the individual phases. Phases 1 and 2 rated the same 990 conversations.), e.g., phase1 annotated 990 conversations with a diverse pool of 96 raters from two rater pools (i.e., India and US) with unbalanced distribution of US and Indian raters per conversation. Phase2 annotated again the same 990 conversations with a different pool of 96 raters (with 13 raters overlap) mitigating the imbalance of locale in the annotations by having equal amount of raters from each locale per conversation. Where phase 1 and phase 2 optimized on locale and gender demographics with limited presence of race/ethnicity, age, sexual orientation and education, phase3 aimed to provide a balanced representation of additional demographics, e.g., race/ethnicity, gender and age. We also changed the sample (i.e., 350 new adversarial conversations sampled from the"
        },
        {
            "heading": "1 990 96 31770",
            "text": ""
        },
        {
            "heading": "2 990 96 33116",
            "text": ""
        },
        {
            "heading": "3 350 104 36400",
            "text": "same source as the 990) to increase the total count of conversations in the dataset, but also to balance it more in terms of the adversariality of the conversations being annotated.\nAll conversations were sampled from an 8k multi-turn conversation corpus (comprising 48k turns in total) generated by human agents interacting with a generative AI chatbot (Thoppilan et al. 2022). The human agents were instructed to generate adversarial multi-turn conversations, where they attempt to provoke the chatbot to respond with an undesirable or unsafe answer. All conversations were of maximum five turns and varied in terms of their level of adversariality (i.e., degree of harm) and topics (Table 2Count of conversations & ratings by degree of harm.).\nEach conversation in the dataset is rated by 60 to 104 diverse human raters. Most of the conversations (> 800) have received 60\u201370, and 350 of the conversations were rated by 100 or more raters. Section 3.1Rater Demographics discusses in detail the rater demographics, as depicted also in Table 3Distribution of raters by demographics. 44 raters did not report their race/ethnicity.. The raters were asked to assess the last utterance of the chatbot in each conversation along 25 safety dimensions organized in five top-level safety categories (e.g., harmful content, content with unfair bias, misinformation, political affiliation and safety policy guidelines). In addition, there is one question which asked the raters to assess the comprehensibility of the whole conversation in terms of whether it is in English, makes sense or is on a topic familiar to the rater. For each safety-related question, raters can respond by selecting Safe, Unsafe, or Unsure answers.\nFor each conversation, rater pair, We aggregate their safety responses into a single safety assessment, denoted Q overall. The aggregation is done as follows:\n1. If rater\u2019s response to any of the safety dimensions is Unsafe, then Q overall is Unsafe\n2. if rater\u2019s response to any of the safety dimensions is Unsure, then Q overall is Unsure\n3. in all other cases Q overall is Safe.\nIn our Bayesian multilevel models, it is convenient to encode the ratings as an ordinal value, where 0 = Safe, 1 = Unsure, and 2 = Unsafe.\nIn addition to the rater safety ratings, a sample of the dataset was also manually annotated with degree of harm\nand topic. Table 2Count of conversations & ratings by degree of harm. shows the distribution of these conversations across a four-scale harm severity scale: Benign, Debatable, Moderate, Extreme. A random sample of 400 conversations were annotated in this way from the 990 conversations, and all 350 conversations were also annotated with these two categories. It is important to note, that the 350 conversations were also sampled from 8K corpus to have a gold safety rating provided from a trust and safety expert.\nWe removed unreliable raters via a two-step process. First we identified raters who (i) disagreed with other raters at an anomalously high rate, (ii) \u201cstraightlined\u201d (i.e., frequently the same response, presumably because they were not reading the conversations they were supposed to review), or (iii) took an unusually short or long time to complete. Then we inspected the raters\u2019 response patterns manually to confirm any suspicious behavior, and removed those raters that did not pass this manual stage. Ultimately, we excluded 31 raters from the data because of reliability concerns.\nIn summary, as shown in Table 1Summary of data properties. Note that, due to some overlap among annotators, the total number of raters, and (conversation, rater) pairs, is less than the sum of the corresponding numbers in the individual phases. Phases 1 and 2 rated the same 990 conversations. the dataset contains more than 100K conversation-rater pairs annotated with an extremely high replication rate. A dataset of such size is particularly useful when aiming for some measure of confidence in discovering patterns in rater behavior. The dataset is publicly available on GitHub (for purposes of anonymity, we do not provide the link here; it will be included in the final version of the paper)."
        },
        {
            "heading": "3.1 Rater Demographics",
            "text": "As noted in Section 3Dataset the dataset was collected in three phases (see Table 1Summary of data properties. Note that, due to some overlap among annotators, the total number of raters, and (conversation, rater) pairs, is less than the sum of the corresponding numbers in the individual phases. Phases 1 and 2 rated the same 990 conversations.) that we combine for our analysis. In the first two phases, raters were stratified by gender and locale (United States or India), and in the third phase they were recruited only from one locale (US) and stratified by gender, race/ethnicity, and age. In each phase, all the demographic data about the raters was collected with an optional survey in which they reported their race/ethnicity, sexual orientation, gender, age group and education level. In phase 1 and 2 we only controlled for\nlocale and in phase 3 we controlled for gender, race/ethnicity and age group. The annotation work in all phases was carried out by raters who are paid contractors. Those contractors received a standard contracted wage, which complies with living wage laws in their country of employment. Due to global privacy concerns, we cannot include more details about our participants, e.g., estimated hourly wage or total amount spent on compensation.\nA unique aspect of the demographics population in this study is the presence of 10 raters who self-identified as Indigenous (e.g., American Indian, Alaska Native, Mexican Indigenous, Native Hawaiian, Pacific Islander), a population category for which, to our knowledge, has never been studied before as a predictor of rater behavior."
        },
        {
            "heading": "4 Methods",
            "text": "We use Bayesian multilevel models (MLMs, a.k.a, Bayesian hierarchical models) to understand the relationship between safety ratings and rater demographics."
        },
        {
            "heading": "4.1 Approach",
            "text": "To reliability analyze an annotated dataset by a multitude of human raters for which we have different demographic data, we take a multilevel modeling approach which allows flexibility in discovering patterns and associations between raters, their ratings and content item characteristics.\nA single data point per rater can be modeled as:\nQ overall = \u03b1+ \u03b21X1 + \u00b7 \u00b7 \u00b7+ \u03b2kXk + , (1)\nwhere Q overall is a single rater safety response and X1, . . . , Xk are k independent variables, or predictors,\nwhich in our case are binary categorical variables representing membership in a demographic class. For example, X1 could be equal to 1 if the rater belongs to the Black race/ethnicity group or 0 otherwise. As for the remaining variables, the model parameter \u03b1 is the Y -intercept, model parameters \u03b21, . . . , \u03b2k are scalar coefficients, and e \u223c N (0, \u03c32) is the error term, which is drawn from a normal distribution. In the case of a binary response variable like safety, we may add a link function to Equation 1Approach, such as the logistic function.\nWe present the model definitions in R notation, and conduct our analyses using the brms (Bu\u0308rkner 2017) package in R. In R notation, Equation 1Approach looks like:\nQ overall \u223c 1 +X1 + \u00b7 \u00b7 \u00b7+Xk,\nwhere the 1 represents the y-intercept. This notation is convenient because it abstracts away the parameters and focuses on the relationships between the dependent and independent variables.\nIn the case of categorical variables, each variable Xk denotes a particular categorical dimension, such as race/ethnicity, and actually represents a collection of binary variables, one for each racial/ethnic class (i.e., Asian, Black, etc.). Each of these binary variables has its own coefficient.\nMLMs allow us to quantify (and separate) group-level effects: How do conversation-level characteristics (e.g., its content, length etc.) relate to the ratings grouped under these conversations? Here, we can add an additional term for the y-intercept for each distinct rater id i. If we assume that the safety response Q overall of each data point is independent of its rater given the conversation, and vice versa, we can add another term for the conversation j:"
        },
        {
            "heading": "Q overall = \u03b1+ \u03b1i + \u03b3j + \u03b21X1 + \u00b7 \u00b7 \u00b7+ \u03b2kXk + .",
            "text": "or, in R notation, Q overall\u223c 1 + (1|rater id) + (1|conversation id) + X1 + \u00b7 \u00b7 \u00b7+Xk.\nIn this way, MLMs estimate the effect of group membership and group-level predictors simultaneously. The resulting models look like a collection of generalized linear models with many shared parameters, but with different yintercepts. The contributions of each rater id and conversation id are called random effects. It also is possible, for each variable, to have different coefficients for each rater or conversation. For instance, (race|conversation id) indicates that the coefficients associated with race/ethnicity are distinct for each conversation id. Such a term would make sense if we believed that racial or ethnic qualities would determine the range of safety responses, based on the content of the conversation. We call these group-level effects (GEs).\nBayesian regression employs Bayes\u2019 theorem to incorporate prior knowledge about the parameters of a statistical model (e.g., the distributional properties of predictor variables and their relations with the outcome variable) and a likelihood function P \u2217 to compute posterior distributions: distributions of estimates of these parameters.\nP \u2217(M |D)\nRecall that, in the standard (frequentist) approach to linear regression, the model that minimizes the mean squared error M\u2217 is a maximum likehlihood estimator for the data D.\nM\u2217 = argmin M P (D|M)\nBayesian regression presents several advantages over frequentist approaches. It offers greater flexibility, more robust estimates through quantification of uncertainty, and better interpretability than its frequentist counterparts\u2014especially when data follow complex distributions that violate statistical assumptions or comprise small sample sizes for minority groups of cases. For example, when outcome data are quite imbalanced and not normally distributed, Bayesian inference allows one to relax strict assumptions related to normality. Further, when assessing the reliability of statistical estimates, the Bayesian approach facilitates probabilistic assessments of uncertainty, such as probability of direction and probability of practical significance. We define these in the Predictor evaluation subsection below."
        },
        {
            "heading": "4.2 Applying Bayesian MLMs to Safety Annotation",
            "text": "We performed iterative model building to explore the space of interactions and effects of predictors. These models included groupings of ratings by individual raters and conversations as random effects. Here we report the main models that came out of this process. These models can be split into three levels of complexity: null, linear, and intersectional, and they were fit on two different datasets: all the data (denoted AD), and just the data that has expert qualitative severity labels (denoted QS).\nThe null model. The first is a null model. It captures the variance in the data due solely to grouping by rater and conversation:\nAD, QS null: Q overall \u223c 1 + (1 | rater id) + (1 | conversation id)\nLinear models. Our linear models treat demographic variables as strictly linear (population-level) effects with no interactions between them. These models show the covariance of the demographic variables as independent, nonintersecting predictors compared to the null model. They are similar to ordinary least square regressions, but with Bayesian estimators and separate y-intercepts for observations clustered by rater and conversation.\nAD effects: Q overall \u223c race + gender + age + education + phase + (1 | rater id) + (1 | conversation id),\nWe call this the all data (AD) linear model to distinguish it from a second set of linear models that include as a predictor the expert qualitative severity (QS) ratings described in Section 3Dataset. These QS models allow us to investigate how the severity of unsafe conversations could differentially impact ratings for different sociodemographic groups of annotators. However, because we did not have expert qualitative severity ratings for all of our data (see Table 2Count of\nconversations & ratings by degree of harm.) we considered this model separately from the previous one, and fit it only to the subset of data that did NOT have a severity rating of Unrated.\nQS effects: Q overall\u223c race + gender + age + education + severity + (1 | rater id) + (1 | conversation id).\nWe discuss our reasoning behind the QS models in more detail in Section 5Results. We explore a second linear QS model that further treats conversation severity as a grouplevel effect (GE) that can vary based on grouping of rater id. Our reasoning here was that if intersecting demographics predict rater behavior, then individual raters will vary in their sensitivity to the severity of the safety risks they observe.\nQS effects GE: Q overall \u223c race + gender + age + education + severity + (severity | rater id) + (1 | conversation id).\nIntersectional models. Our final, intersectional models consider the intersection of race/ethnicity with gender, age, and education. We focus on race/ethnicity because prior literature on intersectionality has shown race/ethnicity to be a predictor that commonly interacts with other predictors, as outlined in Section 2.3Intersectionality.\nAD intersectional: Q overall \u223c race \u2217 (gender + age + phase + education) + (1 | rater id) + (1 | conversation id).\nwhere the \u2018\u2217\u2019 symbol denotes multiplication. As with our linear models, we also consider a version of this with qualitative severity ratings:\nQS intersectional: Q overall \u223c race \u2217 (gender + age + severity + education) + (1 | rater id) + (1 | conversation id).\nWe also test a model allowing the effect of severity to vary also as a group-level effect, across individual raters:\nQS intersectional GE: Q overall \u223c race \u2217 (gender + age + severity + education) + (severity | rater id) + (1 | conversation id)."
        },
        {
            "heading": "4.3 Fitting the model",
            "text": "For our ordinal outcome, Q overall, we set weakly informative probit threshold priors to reflect our prior knowledge that the values of safe, unsafe and unsure are not equally likely. For all other parameters, we keep the default priors for cumulative probit models in brms, which are set as Student\u2019s t (df = 3, location = 0.00, scale = 2.5) distributions.\nWe fit a series of Bayesian ordinal MLMs (estimated using MCMC sampling with 4 chains of 2,000 iterations and a warm-up of 1,000) to quantify the individual and intersectional effects of race/ethnicity, gender, age, data collection phase, and education level on safety ratings (Section 3Dataset)."
        },
        {
            "heading": "4.4 Model & predictor evaluation",
            "text": "We evaluate our experiments at the model level, to determine how well overall each model fits the data, and at the predictor level, to determine how much each demographicand content- level predictor influences rater behavior.\nModel evaluation We evaluate the fitness of our models via the following metrics: \u2022 ELPD represents the expected log pointwise predictive\ndensity of a model for a new dataset (see Equation 1 in Vehtari, Gelman, and Gabry 2017).\n\u2022 LOOIC represents the out-of-sample predictive fit of a Bayesian model estimated by leave-one-out crossvalidation (it is a generalization of Equation 4 in Vehtari, Gelman, and Gabry 2017).\n\u2022 WAIC (Watanabe 2010) is a generalization of the Akaike information criterion (Akaike 1974); it similarly tracks performance on Bayesian cross-validation.\n\u2022 Conditional R2 gauges variance in the model captured by the fixed and random effects.\n\u2022 Marginal R2 estimates the fixed effects of the model alone.\nMcKelvey and Zavoina (1975)\u2019s pseudo-R2 closely approximates the R2 of a linear model fitted with observations of the continuous latent variable (represented by the discrete ordinal responses), and is adapted here for Bayesian multilevel ordinal regression.\nPredictor evaluation. The following metrics (which are unique to Bayesian models) shed light on how confident we can be in the effect sizes of the predictors (e.g., age or gender) or intersections (e.g., race/ethnicity * gender) that our models estimate. \u2022 Probability of direction (pd) is a metric of effect ex-\nistence that is roughly analogous to the frequentist notion of p-value. It is formally defined as the proportion of the posterior distribution that is positive or negative (i.e., matching the sign of its median; (Makowski et al. 2019)).\n\u2022 Probability of practical significance (ps) assesses whether the magnitude of an estimate (viz. its effect size) is meaningful enough to be cared about. It can be thought of as a unidirectional equivalence test. Formally, ps represents the proportion of the posterior distribution that is outside a researcher-determined region of practical equivalence: a range of values that would indicate a negligible effect (Kruschke and Liddell 2018). This is in contrast to statistical significance, which gauges how different effects are from \u201czero.\u201d In the current study, we set the region of practical equivalence to |.05|; we consider effect sizes at or smaller than |.05| as negligible.\nFollowing the Sequential Effect eXistence and sIgnificance Testing (SEXIT) framework (Makowski et al. 2019), for each estimate we report the median of its posterior distribution, 95% (Bayesian) credible interval, probability of direction, probability of practical significance (i.e., chance of being greater than 0.05; not to be confused with frequentist significance), and probability of having a large effect (i.e., at least 0.30). We assessed convergence and stability of Bayesian sampling with R-hat, which should be below\n1.01 (Vehtari 2019), and effective sample size (ESS), which should be greater than 1000 (Bu\u0308rkner 2018)."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Model evaluation results",
            "text": "Our results for model selection (Table 4Fitness of the various MLMs considered in this study. Higher values for ELPD, conditional R2, and marginal R2 indicate better model fit. Lower values for LOOIC and WAIC indicate better model fit. AD stands for All Data. QS stands for Qualitative severity, i.e., they are the models with expert qualitative ratings of conversation safety-risk severity. RC stands for random covariates. Conditional R2 estimates variance in the model captured by the fixed and random effects. Marginal R2 refers to the fixed effects of the model alone.) show that, in terms of predictive fit metrics (i.e., ELPD, LOOIC, WAIC), our series of QS (quantitative severity, Section 4.2Applying Bayesian MLMs to Safety Annotation) models seem outperform AD models (all data models, Section 4.2Applying Bayesian MLMs to Safety Annotation). However, these differences are not comparable because the QS series of models is only fitted to a subset of the data to which the AD models are fitted.\nAcross both series of models, we report the estimates of our final AD intersectional and QS intersectional GE models due to their relatively stronger predictive fit. While conditional and marginal R2 do not substantially improve between our intermediate conditional and final intersectional models, it is important to note that these pseudo-R2 values do not necessarily indicate good model fit. Since it is a proxy\nfor variance explained by a model, higher R2 may simply indicate the \u201cusefulness\u201d of group differences for explaining variation in an outcome variable, rather than how good the model is at out-of-sample prediction. ELPD, LOOIC, and WAIC all improve with the incorporation of intersectional demographic effects (compared to demographic effects in isolation), suggesting that models accounting for intersectionality provide more practically meaningful estimates of how demographic diversity affects safety reporting."
        },
        {
            "heading": "5.2 Results from AD models",
            "text": "Moderate independent effects for race and gender Safety ratings do not appear to vary substantially by the race/ethnicity categories alone. That is, safety ratings do not clearly vary by race/ethnicity in isolation, when statistically holding all other demographic factors constant. As shown in Figure 1Estimated likelihoods of rating a conversation \u201dunsafe\u201d, by race/ethnicity in the AD intersectional model. 95% credible intervals indicate a 95% chance that the true value falls within the interval, given the data observed. Plot controls for Phase and gender (held at the reference levels of \u201dPhase 2\u201d, \u201dMen\u201d), and education and age (held at average levels)., being White, East Asian, Black, South Asian, Indigenous, or Latine relates to having less than a 12.5% likelihood of rating a conversation as unsafe, holding all other factors constant.\nA notable exception here is for raters that we aggregate into the race/ethnicity category of Other, a category comprising one rater identifying as Other, two raters who preferred not to report their race/ethnicity, and four raters identifying as Middle Eastern or North African (merged into this category to preserve anonymity). We estimate this synthesized group is 2.6 times more likely not to report Safe than White raters (holding all else constant; see Table 6Summary of results for AD MLMs with data collection phase as a variable, but no expert ratings. Point estimates are presented as odds ratios, with 95% credible intervals provided in brackets beside each estimate. Population-level effects (i.e., effects concerning all rating observations across raters and conversations) are presented first, followed by the structure of the group-level effects (i.e., effects concerning the grouping variables of rater id and conversation id), which are shown at the bottom of the table.). This estimated difference is robust, showing 96% chance of being positive, 95% chance of being practically significant, and an 89% chance of being large.\nSimilarly, safety ratings do not clearly vary by gender in isolation when statistically holding all other demographic factors constant. Based on the data observed across all collection phases, the men vs. women difference in safety ratings is only 38% likely to be large (effect size > |.30|) in magnitude, despite having an 83% chance of being practically significant (effect size > |.05|) and an 89% chance of existing (pd; (see Table 8Summary of results for QS intersectional GE model of expert-annotated data. These results show the estimated odds according to the model for the class in question versus the variable\u2019s reference class that the conversation being rated safe. Note that, for gender Man is the reference class, for race/ethnicity it is White, for dataset\nphase it is Phase 1, and for education it is College degree or above. The suffix .L represents the estimated linear effect of a variable (e.g., from Gen Z to Millenial); .Q represents the estimated quadratic effect of a variable (e.g., from Gen Z to Gen X+); .C represents the estimated cubic effect of a variable (e.g., from conversation severity of Benign to Extreme. Direction, Significance, and Large correspond to probabilities of an effect\u2019s direction, practical significance, and magnitude being large, respectively., line 9 in the supplemental materials). This estimate indicates that gender differences in safety ratings likely exist, but it remains possible that these effects depend on membership in other demographic groups.\nStrong intersectional effects between race and gender In contrast to our report of race/ethnicity and gender\u2019s effect on safety ratings independently, Figure 2Conditional effects plot of the AD intersectional model estimates that, among Asian raters, women report fewer safety risks than men, but for White and South Asian raters, women report more. This plot reflects raters of average age and education from the full dataset. Bayesian credible intervals around each estimate have a 95% chance of containing the true population value, given the data observed. shows how the effect of racial/ethnic identity intersects with gender for certain rater groups: South Asian women are substantially more likely than White raters (both men and women) not to report Safe. Meanwhile, we observe the opposite for East Asian women; they are substantially less likely than White raters to report conversations as Unsafe. Specifically, according to the model estimates shown in Table 6Summary of results for AD MLMs with data collection phase as a variable, but no expert ratings. Point estimates are presented as odds ratios, with 95% credible intervals provided in brackets beside each estimate. Population-level effects (i.e., effects concerning all rating observations across raters and conversations)\nare presented first, followed by the structure of the grouplevel effects (i.e., effects concerning the grouping variables of rater id and conversation id), which are shown at the bottom of the table., East Asian women are 53.5% less likely than White men to rate conversations as unsafe.\nStrong independent AND intersectional effects for age Increases in age by cohort unequivocally relate to fewer Safe ratings, as visualized in Figure 3Conditional effects of age and phase plotted for the AD intersectional model defined in Section 4Methods. Plot shows that ratings of unsafe decrease with age. Plot controls for rater gender, age, and education at their mode values.. Namely, Table 6Summary of results for AD MLMs with data collection phase as a variable, but no expert ratings. Point estimates are presented as odds ratios, with 95% credible intervals provided in brackets beside each estimate. Population-level effects (i.e., effects concerning all rating observations across raters and conversations) are presented first, followed by the structure of the group-level effects (i.e., effects concerning the grouping variables of rater id and conversation id), which are shown at the bottom of the table., row 13 in the supplemental materials shows that each jump in age cohort (e.g., Gen Z to Millenial) relates to an estimated 35% fewer Safe ratings (controlling for all other demographics at their reference values). This estimate is quite robust, with 100% probability of being positive, a 100% probability of being practically significant, and a 94% chance of being large.\nYet, this overall age effect does not apply uniformly across racial/ethnic identities: Figure 4Plot of conditional effects of age across ethno-racial groups for the AD intersectional model defined in Section 4Methods. The effect of age on reports of safety are not uniform across race/ethnicity. shows the distributions of safety ratings across data collection phase for Gen X+ and Gen Z raters, respectively. Specif-\nically it illustrates how, as age increases, East Asian and Black rater safety ratings do not increase as sharply as is seen for White, South Asian, Indigenous, Multiracial, and Other raters.\nStrong independent AND intersectional effects for data collection phase Notably, dataset phase relates to the largest differences in safety ratings between raters, particularly when looking at the effects of phase across racial identities. These differences are sharpest between phases 1 and 3 (Figure 5Plot of conditional effects of race/ethnicity and gender in the AD intersectional model (defined in Section 4Methods) show that differences in safety ratings between phases 1 and 3 are large for raters of Color, compared to those for White raters. Non-White raters show greater variability in safety reporting across different conversational content. Plot holds rater age and education constant at their average values.). Compared to changes seen for White raters, East Asian, Black, Latine, Indigenous and especially South Asian raters show a heightened sensitivity to Phase 3 (highlighted in red) conversations compared to Phase 1 conversations (highlighted in blue). In particular, compared to phase 1, South Asian raters are nearly six times less likely to report a phase 3 conversation safe (63% vs. 11%). In contrast, White raters show only a modest jump, from a 7% chance to a mere 13% chance of reporting conversations as unsafe in Phases 1 and 3, respectively. Indeed, compared to White raters, South Asian raters are 3.31 times more likely to find Phase 3 content more unsafe (cf. Phase 1 content)\u2014this difference has a 100% chance of being positive, practically significant, and quite large (see Table 8Sum-\nmary of results for QS intersectional GE model of expertannotated data. These results show the estimated odds according to the model for the class in question versus the variable\u2019s reference class that the conversation being rated safe. Note that, for gender Man is the reference class, for race/ethnicity it is White, for dataset phase it is Phase 1, and for education it is College degree or above. The suffix .L represents the estimated linear effect of a variable (e.g., from Gen Z to Millenial); .Q represents the estimated quadratic effect of a variable (e.g., from Gen Z to Gen X+); .C represents the estimated cubic effect of a variable (e.g., from conversation severity of Benign to Extreme. Direction, Significance, and Large correspond to probabilities of an effect\u2019s direction, practical significance, and magnitude being large, respectively.).\nGiven the large differences in safety ratings across race and data collection phase, we form the exploratory hypothesis that raters of color have relatively higher sensitivity to unsafe content compared to White raters. However, this hypothesis relies on the assumption that data collection phase is a proxy indicator of conversation severity, given the knowledge that, by design, Phase 3 conversations have a higher proportion of gold-labeled unsafe content than the other phases. Further, we form this hypothesis knowing that these differences are not due to data imbalances, as MLMs statistically account for imbalanced data."
        },
        {
            "heading": "5.3 Results from QS models",
            "text": "Here we discuss results of the QS intersectional model, described in Section 4Methods, zeroing in on how the effects of racial identity on safety ratings vary according to the severity of the content being rated. This additional model facilitates a more robust interrogation of the claim that raters of color are more sensitive to conversation severity than White raters. As discussed in Section 4Methods, in this model, we subset data from dataset phases 1, 2, and 3 that had expert severity ratings, as shown in Table 2Count of conversations & ratings by degree of harm. in the supplemental materials. Since the severity variable is collinear with dataset phase, we eliminate phase as a covariate in these remaining analyses.\nStrong independent AND intersectional effects for content severity. Across all demographic characteristics, conversation degree of harm is a highly reliable predictor of rating content as unsafe (pd = 1.00; ps = 1.00) and is 90% likely to be a large (> |.30|) effect (see Table 7Summary of results for the AD intersectional model with data collection phase as a variable, but no expert ratings. These results show standardized estimates according to the model for the class in question versus the variable\u2019s reference class that the conversation being rated safe. Note that, for gender Man is the reference class, for race/ethnicity it is White, for dataset phase it is Phase 1, and for education it is College degree or above. The suffix .L represents the estimated linear effect of a variable (e.g., from Gen Z to Millenial); .Q represents the estimated quadratic effect of a variable (e.g., from Gen Z to Gen X+); .C represents the estimated cubic effect of a variable (e.g., from conversation severity of Benign to Extreme.\nDirection, Significance, and Large correspond to probabilities of an effect\u2019s direction, practical significance, and magnitude being large, respectively., row 12). But does the effect of racial/ethnic identity on safety rating depend on conversation\u2019s degree of harm (e.g., when a conversation\u2019s degree of harm is extremely unsafe)? We find this hypothesized effect is only certain and meaningful for Indigenous raters. As shown in Figure 6Conditional effects for QS intersectional Bayesian MLM (Section 4Methods) show that South Asian and (especially) Indigenous raters are more sensitive to safety risks than raters of other racial/ethnicity groups when expert-rated degree of harm is high. The chances of Indigenous raters reporting unsafe increases 33% (from 20% to 53%) for \u201cExtreme\u201d rather than \u201cBenign\u201d conversations. Meanwhile, White raters\u2019 unsafe ratings for these two content types only differ by 11%. We plot women as the reference group and hold age and education constant at their average values., the chances of an Indigenous person finding a conversation unsafe jumps from 20% to 53% when the degree of harm rating is benign and extreme, respectively. In other words, we estimate that Indigenous raters are 1.97 (1.2543) times more likely than White raters to find a conversation more unsafe for conversations deemed by experts to be extremely unsafe (compared to safe; see Table 5Summary of Results for MLMs with Expert-Annotated Data. Point estimates are presented as odds ratios, with 95% credible intervals provided in brackets beside each estimate. Population-level effects (i.e., effects concerning all rating observations across raters and conversations) are presented first, followed by the structure of the group-level effects (i.e., effects concerning the grouping variables of rater id and conversation id), which are shown at the bottom of the table.). Though only 23% of this effect\u2019s posterior distribution is considered large (> .30), this effect has a 99% probability of being positive and a 96% probability of being nonnegligible (see Table 7Summary of results for the AD intersectional model with data collection phase as a variable, but no expert ratings. These results show standardized estimates according to the model for the class in question versus the variable\u2019s reference class that the conversation being rated safe. Note that, for gender Man is the reference class, for race/ethnicity it is White, for dataset phase it is Phase 1, and for education it is College degree or above. The suffix .L represents the estimated linear effect of a variable (e.g., from Gen Z to Millenial); .Q represents the estimated quadratic effect of a variable (e.g., from Gen Z to Gen X+); .C represents the estimated cubic effect of a variable (e.g., from conversation severity of Benign to Extreme. Direction, Significance, and Large correspond to probabilities of an effect\u2019s direction, practical significance, and magnitude being large, respectively., line 41 in the supplemental materials).\nMeanwhile, harmful content likely relates to (slightly) heightened unsafe ratings from South Asian and Latine raters, compared to White raters (pd = .91 and .88, respectively). It is probable, but not certain, that these rater groups\u2019 safety ratings are more influenced by \u201dExtreme\u201d content compared to White raters. Probabilities that these effects are practically significant are questionable (ps = .72 and .77, for South Asian and Latine raters, respectively; see Table 7Sum-\nmary of results for the AD intersectional model with data collection phase as a variable, but no expert ratings. These results show standardized estimates according to the model for the class in question versus the variable\u2019s reference class that the conversation being rated safe. Note that, for gender Man is the reference class, for race/ethnicity it is White, for dataset phase it is Phase 1, and for education it is College degree or above. The suffix .L represents the estimated linear effect of a variable (e.g., from Gen Z to Millenial); .Q represents the estimated quadratic effect of a variable (e.g., from Gen Z to Gen X+); .C represents the estimated cubic effect of a variable (e.g., from conversation severity of Benign to Extreme. Direction, Significance, and Large correspond to probabilities of an effect\u2019s direction, practical significance, and magnitude being large, respectively., lines 40 and 42 in the supplemental materials)."
        },
        {
            "heading": "5.4 Results from both AD and QS models",
            "text": "Education level impacts safety ratings for Indigenous raters, but not other racial/ethnic groups. A striking result of both our final AD and QS models is that rater education levels are largely unrelated to safety reports across most demographic groups, but they are clearly linked to Indigenous raters\u2019 reports of safety. Indigenous raters, compared to White raters, are a staggering 3.12 times more likely (95% Bayesian CI = [0.79, 15.71]) to report content as unsafe in QS Model 3.1, but only when their level of education is at the high school level or below. Holding all other factors con-\nstant, this effect is 94% likely to exist, 94% likely to be nonnegligible, and 88% likely to be large (see Table 7Summary of results for the AD intersectional model with data collection phase as a variable, but no expert ratings. These results show standardized estimates according to the model for the class in question versus the variable\u2019s reference class that the conversation being rated safe. Note that, for gender Man is the reference class, for race/ethnicity it is White, for dataset phase it is Phase 1, and for education it is College degree or above. The suffix .L represents the estimated linear effect of a variable (e.g., from Gen Z to Millenial); .Q represents the estimated quadratic effect of a variable (e.g., from Gen Z to Gen X+); .C represents the estimated cubic effect of a variable (e.g., from conversation severity of Benign to Extreme. Direction, Significance, and Large correspond to probabilities of an effect\u2019s direction, practical significance, and magnitude being large, respectively.)."
        },
        {
            "heading": "6 Discussion",
            "text": "Our experiments with Bayesian multi-level modeling suggest that demographics do play a powerful role in predicting rater perceptions of safety in evaluation of conversational AI systems. Our intersectional models had roughly the same predictive power as our linear models. However, the intersectional models provide a nuanced view at how predictors interact, which is critical for their in-depth understanding.\nThe results show strong intersectional effects involving race/ethnicity that do not exist for race/ethnicity independently. That is, the effects of race/ethnicity on safety ratings only emerge when race/ethnicity is viewed at its intersection with additional factors, like gender or harm severity of the conversation. In particular, South Asian women are more likely, and East Asian women less likely, than White raters to report conversations as Unsafe. Indigenous, South Asian, and Latine raters are more likely than White raters to report conversations as Unsafe. On the other hand, age is a strong independent predictor of rating behavior, with younger raters more likely to rate conversations Unsafe.\nHence, we recommend that safety evaluation workflows recruit human raters across a broad demographic spectrum and record the demographic characteristics of raters to ensure that such breadth is maintained. To boost the representational power of demographic diversity, large rater pools should be used, considering the benefits that such diversity provides in weighing costs. In cases where costs are prohibitive, decreasing the number of items each rater evaluates should be considered in favor of increased number of raters per item. Finally, we recommend using statistical frameworks that account for the cross-classified structure of human annotation data."
        },
        {
            "heading": "6.1 Limitations",
            "text": "Although Bayesian MLMs depend on far fewer assumptions than linear regression or ANOVAs, there are some drawbacks. MCMC sampling is a slow process; our largest models take days to run if not parallelized across multiple CPUs, and it is relatively common for the process not to converge. And although it has been argued that maximum a posteriori\n(MAP) inference, which Bayesian models enable, is nearly always more robust than maximum likelihood estimates (the basis of ordinary least squares estimates), the true power of MAP depends on how realistic the prior distributions of a given model are.\nAlthough our models predict a unique intercept for each rater id and conversation id, the contribution from each rater and conversation pair is linear. We did not explore whether the relationship between them was more complex.\nIn this study, we only considered safety ratings as a single response (i.e. Q overall) for each (conversation, rater) pair. However, this response is an aggregate of 16\u201324 safety-related questions (i.e., safety dimensions discussed in \u00a73Dataset). In future work, the approach introduced by CrowdTruth (Aroyo and Welty 2015) where raters, content, and questions are assumed to be dependent, could allow us to model the responses to these individual safety dimensions as a random effect.\nOur Indigenous race/ethnicity category lumps together very diverse Indigenous identities in a manner that likely discounts rich idiographic differences in language, culture, and lived experience (Else-Quest and Hyde 2016). However, in the interest of protecting participants privacy and prioritizing the representation of Indigenous perspectives in this empirical research, we chose to group them together. Creating the Indigenous category in our analysis balances these opposing concerns, but leaves significant room for future study."
        },
        {
            "heading": "7 Conclusion",
            "text": "We apply Bayesian multilevel models (MLMs) to a dataset of 1,340 chatbot conversations, each rated for safety by 60\u2013 104 human raters, to study the impact of rater demographics on rater behavior for safety ratings. MLMs allow us to deal\nwith the overlapping hierarchical dependencies on rater and conversation that are inherent in rater data, and which confound simpler modeling approaches, such as ordinary least squares regression and ANOVA.\nOur results show strong intersectional effects between race/ethnicity and gender, Indigenous raters and education, and content severity and race. They suggest that conversational AI safety evaluation can benefit when human evaluators come from diverse demographic backgrounds."
        }
    ],
    "title": "Intersectionality in Conversational AI Safety: How Bayesian Multilevel Models Help Understand Diverse Perceptions of Safety",
    "year": 2023
}