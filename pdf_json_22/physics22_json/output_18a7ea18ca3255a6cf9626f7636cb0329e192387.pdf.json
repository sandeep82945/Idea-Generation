{
    "abstractText": "Density estimation is a fundamental task in statistics and machine learning applications. Kernel density estimation is a powerful tool for non-parametric density estimation in low dimensions; however, its performance is poor in higher dimensions. Moreover, its prediction complexity scale linearly with more training data points. This paper presents a method for neural density estimation that can be seen as a type of kernel density estimation, but without the high prediction computational complexity. The method is based on density matrices, a formalism used in quantum mechanics, and adaptive Fourier features. The method can be trained without optimization, but it could be also integrated with deep learning architectures and trained using gradient descent. Thus, it could be seen as a form of neural density estimation method. The method was evaluated in different synthetic and real datasets, and its performance compared against state-of-the-art neural density estimation methods, obtaining competitive results.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joseph A. Gallego"
        }
    ],
    "id": "SP:33e261732e851d558d0a1b95e5814a231e46e1d2",
    "references": [
        {
            "authors": [
                "H. Avron",
                "V. Sindhwani",
                "J. Yang",
                "M.W. Mahoney"
            ],
            "title": "Quasi-monte carlo feature maps for shift-invariant kernels",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "A. Backurs",
                "P. Indyk",
                "T. Wagner"
            ],
            "title": "Space and time efficient kernel density estimation in high dimensions",
            "year": 2019
        },
        {
            "authors": [
                "Y. Bengio",
                "S. Bengio"
            ],
            "title": "Modeling high-dimensional discrete data with multi-layer neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1999
        },
        {
            "authors": [
                "M. Charikar",
                "P. Siminelakis"
            ],
            "title": "Hashing-based-estimators for kernel density in high dimensions. 8 2018",
            "venue": "URL http://arxiv.org/abs/1808.10530",
            "year": 2018
        },
        {
            "authors": [
                "Y.C. Chen"
            ],
            "title": "A tutorial on kernel density estimation and recent advances. Biostatistics and Epidemiology, 1: 161\u2013187, 1 2017",
            "venue": "ISSN 24709379",
            "year": 2017
        },
        {
            "authors": [
                "P.I. Davies",
                "N.J. Higham"
            ],
            "title": "Numerically stable generation of correlation matrices and their factors",
            "venue": "BIT Numerical Mathematics,",
            "year": 2000
        },
        {
            "authors": [
                "L. Dinh",
                "D. Krueger",
                "Y. Bengio"
            ],
            "title": "Nice: Non-linear independent components estimation",
            "venue": "arXiv preprint arXiv:1410.8516,",
            "year": 2014
        },
        {
            "authors": [
                "L. Dinh",
                "J. Sohl-Dickstein",
                "S. Bengio"
            ],
            "title": "Density estimation using real nvp",
            "venue": "URL http://arxiv. org/abs/1605.08803",
            "year": 2016
        },
        {
            "authors": [
                "C. Durkan",
                "A. Bekasov",
                "I. Murray",
                "G. Papamakarios"
            ],
            "title": "Neural spline flows",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "B.J. Frey",
                "J.F. Brendan"
            ],
            "title": "Graphical models for machine learning and digital communication",
            "venue": "MIT press,",
            "year": 1998
        },
        {
            "authors": [
                "M. Germain",
                "K. Gregor",
                "I. Murray",
                "H. Larochelle"
            ],
            "title": "Made: Masked autoencoder for distribution estimation",
            "year": 2015
        },
        {
            "authors": [
                "F.A. Gonz\u00e1lez",
                "V. Vargas-Calder\u00f3n",
                "H. Vinck-Posada"
            ],
            "title": "Supervised Learning with Quantum Measurements",
            "year": 2020
        },
        {
            "authors": [
                "F.A. Gonz\u00e1lez",
                "A. Gallego",
                "S. Toledo-Cort\u00e9s",
                "V. Vargas-Calder\u00f3n"
            ],
            "title": "Learning with density matrices and random features",
            "venue": "arXiv preprint arXiv:2102.04394,",
            "year": 2021
        },
        {
            "authors": [
                "A.G. Gray",
                "A.W. Moore"
            ],
            "title": "Nonparametric density estimation: Toward computational tractability",
            "venue": "In Proceedings of the 2003 SIAM International Conference on Data Mining,",
            "year": 2003
        },
        {
            "authors": [
                "M.A. Hearst",
                "S.T. Dumais",
                "E. Osuna",
                "J. Platt",
                "B. Scholkopf"
            ],
            "title": "Support vector machines",
            "venue": "IEEE Intelligent Systems and their applications,",
            "year": 1998
        },
        {
            "authors": [
                "D.P. Kingma",
                "T. Salimans",
                "R. Jozefowicz",
                "X. Chen",
                "I. Sutskever",
                "M. Welling"
            ],
            "title": "Improved variational inference with inverse autoregressive flow",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "H. Larochelle",
                "I. Murray"
            ],
            "title": "The neural autoregressive distribution estimator",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Y. LeCun",
                "B. Boser",
                "J.S. Denker",
                "D. Henderson",
                "R.E. Howard",
                "W. Hubbard",
                "L.D. Jackel"
            ],
            "title": "Backpropagation applied to handwritten zip code recognition",
            "venue": "Neural computation,",
            "year": 1989
        },
        {
            "authors": [
                "J. Li",
                "A. Barron"
            ],
            "title": "Mixture density estimation",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "Z. Li",
                "J.F. Ton",
                "D. Oglic",
                "D. Sejdinovic"
            ],
            "title": "Towards a unified analysis of random fourier features",
            "venue": "In 36th International Conference on Machine Learning, ICML 2019,",
            "year": 2019
        },
        {
            "authors": [
                "F. Liu",
                "X. Huang",
                "Y. Chen",
                "J. Yang",
                "J. Suykens"
            ],
            "title": "Random fourier features via fast surrogate leverage weighted sampling",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Q. Liu",
                "J. Xu",
                "R. Jiang",
                "W.H. Wong"
            ],
            "title": "Density estimation using deep generative neural networks",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "W.B. March",
                "B. Xiao",
                "G. Biros"
            ],
            "title": "Askit: Approximate skeletonization kernel-independent treecode in high dimensions",
            "venue": "SIAM Journal on Scientific Computing,",
            "year": 2015
        },
        {
            "authors": [
                "G. Papamakarios",
                "T. Pavlakou",
                "I. Murray"
            ],
            "title": "Masked autoregressive flow for density estimation. 5 2017",
            "venue": "URL http://arxiv.org/abs/1705.07057",
            "year": 2017
        },
        {
            "authors": [
                "E. Parzen"
            ],
            "title": "On estimation of a probability density function and mode",
            "venue": "The annals of mathematical statistics,",
            "year": 1962
        },
        {
            "authors": [
                "A. Rahimi",
                "B Recht"
            ],
            "title": "Random features for large-scale kernel machines",
            "venue": "In NIPS,",
            "year": 2007
        },
        {
            "authors": [
                "C.E. Rasmussen"
            ],
            "title": "Gaussian processes in machine learning",
            "venue": "In Summer school on machine learning,",
            "year": 2003
        },
        {
            "authors": [
                "D.J. Rezende",
                "S. Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "year": 2015
        },
        {
            "authors": [
                "M. Rosenblatt"
            ],
            "title": "Remarks on some nonparametric estimates of a density function",
            "venue": "Ann. Math. Statist.,",
            "year": 1956
        },
        {
            "authors": [
                "B. Sch\u00f6lkopf",
                "A. Smola",
                "M\u00fcller",
                "K.-R"
            ],
            "title": "Kernel principal component analysis",
            "venue": "In International conference on artificial neural networks,",
            "year": 1997
        },
        {
            "authors": [
                "P. Siminelakis",
                "K. Rong",
                "P. Bailis",
                "M. Charikar",
                "P. Levis"
            ],
            "title": "Rehashing kernel evaluation in high dimensions. volume 2019-June",
            "year": 2019
        },
        {
            "authors": [
                "A. Sinha",
                "J. Duchi"
            ],
            "title": "Learning kernels with random features",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "Keywords Density estimation \u00b7 kernel methods \u00b7 neural density estimation \u00b7 kernel density estimation \u00b7 density matrices \u00b7 random Fourier features \u00b7 adaptive Fourier features \u00b7 quantum inspired machine learning"
        },
        {
            "heading": "1 Introduction",
            "text": "The estimation of the joint distribution, p(x1, \u00b7 \u00b7 \u00b7 , xn), of a set of random variables is a general important task in machine learning. This estimation of the underlying distribution has a variety of applications, for instance: density estimation, anomaly detection, non-supervised, and supervised learning. Kernel density estimation (KDE) can approximate arbitrary density functions and its performance increases when more data points are available [26, 30]. The drawback of this method is that it requires all the training data points to make a prediction, which makes it a memory-based method [6]. State-of-the-art approximations methods of KDE such as space partitioning [15], random sampling [24], and hashing based estimators [2, 5] try to overcome this issue; nonetheless, their prediction complexity increases with more training data points [32]. The new approach proposed in this paper rely on quantum machine learning and random Fourier features and will be discussed in Section 3. This new method is an approximation of kernel density estimation and its prediction complexity is constant with respect to the number of training points.\nA different approach to density estimation is based on neural networks with deep architecture. This approach is called neural density estimation and one of its main advantages is that it can be integrated with other deep-learning architectures. The most representative approaches to neural density estimation are autoregressive neural models, normalizing flows and generative adversarial models.\nGonz\u00e1lez et al. [14] showed that a combination of random Fourier features and density matrices can be used to perform density estimation. Density matrices are a formalism used in quantum mechanics to represent the state of a quantum system. Its application in machine learning, and in particular in density estimation, has been limited, but that initial exploration suggested that this approach could be a competitive alternative [13]. In this paper we present a neural density estimation method that approximates the kernel function of KDE using adaptive Fourier features combined\n\u2217Citation: Joseph et al., Quantum Adaptive Fourier Features for Neural Density Estimation.\nar X\niv :2\n20 8.\n00 56\n4v 2\n[ cs\n.L G\n] 4\nA ug\nwith density matrices and systematically evaluate it in different benchmark tasks. One of the main advantages of this approach is that it provides an efficient prediction method whose complexity does not depend on the number of training samples, and also its implementation as a computational graph that is differentiable and thus integrable with deep learning architectures that can be trained by gradient descent. An important characteristic of the model is that, for some applications, it is also possible to train it without using optimization, which alleviates the computational burden associated to gradient-based optimization methods. The method is systematically evaluated on different density estimation tasks and compared against state-of-the-art neural density estimation methods.\nThe rest of the paper is organized as follows. Section 2 reviews density estimation and related work. Section 3 presents the novel neural density estimation model based on adaptive Fourier features and density matrices. Section 4 presents the evaluation of the method in different datasets and its comparison against state-of-the-art neural density estimation methods. Finally, Section 5 presents the conclusions and ideas for future work."
        },
        {
            "heading": "2 Density Estimation",
            "text": "Given a random variable X and its associated probability distribution Pr[X], a probability density function is a measurable function f with the property that:\nPr[X \u2208 A] = \u222b A f(x)dx\nThe problem of density estimation consist on estimating f from a set of iid values x1,x2, \u00b7 \u00b7 \u00b7 ,xn sampled from Pr[X]. Density estimation methods can be broadly divided into parametric and non-parametric approaches. The former assume a parametric model q\u03c6(x) that has adjustable parameters \u03c6, and an optimization process is performed to make q\u03c6(x) as close as possible to the underlying density function p(x). For instance, we can assume that the underlying data comes from a Gaussian (Normal) distribution. However, a drawback is that simple parametric models cannot represent arbitrarily complex density functions. More complex ones, such as mixture models, require several components to approximate density functions compared to compact forms such as kernel density estimation."
        },
        {
            "heading": "2.1 Kernel Density Estimation",
            "text": "Kernel density estimation was independently proposed by Emanuel Parzen [26] and Murray Rosenblat [30] in its present form. Let x1,x2, \u00b7 \u00b7 \u00b7 ,xn be iid training data points drawn from a particular, unknown distribution on Rd. Let x be a new sample whose density estimate is desired; we can obtain an estimate of f(x) as the mean of a kernel function evaluated on x and xi, as follows:\nf\u0302(x) = 1\nM\u03b3n n\u2211 i=1 k\u03b3(xi,x) (2.1)\nwhere \u03b3 is the bandwidth parameter, k\u03b3(\u00b7) is a kernel function such as the Gaussian kernel and M\u03b3 is a normalizing constant depending on the bandwith parameter. This method converges to the true distribution function with more training data points, but its linear complexity for estimate the density of a new x data point in terms of the training data points is prohibitive on current large data sets."
        },
        {
            "heading": "2.2 Neural Density Estimation",
            "text": "Three main approaches have been used in state-of-the-art neural density estimation: autoregressive models, normalizing flows and generative adversarial networks. Autoregressive methods have their origin in the restricted Boltzman machine, which is a Markov random field with bipartite substructure, where a connection is established between the weights W and the observations v. One issue of this kind of method is their intractable Z partition function who ensure a valid distribution and sums to 1 [3, 11, 18].\nNormalizing Flow models were proposed in the last decades as an improvement of autoregressive flows models, whose strength is based on the change of variables [8, 29]. This change of variable can be composed in a series of differentiable and invertible transformations of a known density function, for instance, the normal distribution. The change of variables needs to preserve the volume, which imposes a constraint on the availability of base density functions. Nonetheless, these normalizing flow algorithms are really difficult to tune, thus their convergence is not always guaranteed [23].\nIn [23], the authors propose a new algorithm for density estimation using deep generative neural networks. In the case of the discriminators, the z discriminator is used to distinguish the generated latent variable z\u0302 from the real latent variable z. The other discriminator is used to discern the true data x from the generated data x\u0302. This method can use complex deep neural networks as discriminators, e.g., convolutional neural networks or transformers.\nThe model presented in this paper follows a different approach to neural density estimation which is based on density matrices and kernel-approximating Fourier features. One of the main advantages of this approach is its simplicity as well as its good performance in some benchmark tasks as shown by the experimental evaluation in Section 4. In this section different state-of-the-art neural density methods were used as baselines. These methods are described next:\n\u2022 Masked Autoregressive Flow (MAF): use the following recursions for each layer: xi = ui exp\u03b1i + \u00b5i where \u00b5i = f\u00b5i(u1:i\u2212) [25]. MAF is a generalization of RealNVP.\n\u2022 Inverse Autoregressive Flow (IAF) [17]: In [25], the authors show that inverse autorregresive flow is a generalization of RealNVP. Define z0 = (z\u20320 \u2212 \u00b50)/\u03c30 and zi = (z\u2032i \u2212 \u00b5(z\u20321:i\u22121)/\u03c3(z\u20321:i\u22121), then the Jacobian is lower triangular. This implies that the determinant|dz/dz\u2032| can be computed as \u220fD i=1 1/\u03c3i(z1:i\u22121) who is\nnot dependent of z\u2032i.\n\u2022 Planar Flow [29]: this normalized flow uses a family of transformation of the form f(z) = z + uh(wTz + b) where u, w, b are free parameters and h(\u00b7) is a element-wise function. This transformation has a triangular Jacobian.\n\u2022 Real NVP [9]: this method uses coupling layers as follows y1:d = x1:d and yd+1:D = xd+1:D \u2299 exp(s(x1:d)) + t(x1:d) where s and t means scale and translation respectively, and \u2299 is the\nHadamard product or element-wise product. Also, the inverse of such transformation do not involve the computation of the inverse of neither s or t, therefore these functions can be arbitrarily complex and difficult to invert in particular can be multilayer neural networks.\n\u2022 Neural Spline Flow [10]: Neural Splines Flow uses a partition of K *nodes* of the space between (-B,-B) and (B,B). The out-of-range transformation is mapped as the identity. This makes the overall transformation linear out-of-range, so it can take unrestricted inputs. Each knot uses a monotone rational-quadratic function. The authors claim that rational-quadratic functions are easy to derive and, due to their monotonic behavior, are also analytically invertible."
        },
        {
            "heading": "3 Quantum Adaptive Fourier Features for Density Estimation (QAFFDE)",
            "text": "In this section, we present a method for neural density estimation that starts from the same idea as kernel density estimation to build a non-memory-based method that is more efficient in prediction and that can be also trained using gradient descent. This method extends the DMKDE method proposed by [14]. For accomplishing this we will build an\nFigure 2: (left) Comparison between the real Gaussian kernel centered on 2 as k(x, y) = exp\n{\n\u2212\u03b3||x\u2212 y||2 } , the\nrandom Fourier feature, the random Fourier feature squared, the adaptive Fourier feature, and the adaptive Fourier feature squared. (Right) Mean squared error between approximation Fourier features and the real Gaussian kernel.\nexplicit feature map, \u03c6 : X \u2192 F , that approximates the kernel in Equation 2.1, k\u03b3(x, xi) \u2248 \u3008\u03c6(x), \u03c6(xi)\u3009. The main components of the method are shown in Figure 1. Each component is explained in the following subsections."
        },
        {
            "heading": "3.1 Adaptive Fourier feature learning",
            "text": "Kernel methods are the backbone of several machine learning algorithms, such as support vector machines [16], Gaussian processes [28], kernel density estimation [26, 30], kernel principal component analysis [31], among others. A kernel calculates the dot product in an implicit feature space. This feature space is usually high-dimensional or even of infinite dimension, as it is the case for the Gaussian kernel [34]. Random Fourier features (RFF) [27] is a method that given a shift-invariant kernel, k : X \u00d7X \u2192 R, calculates an explicit feature map \u03c6rff : X \u2192 F such that k(x, y) \u2248 \u3008\u03c6rff(x), \u03c6rff(x)\u3009. RFF are based on the Bochner\u2019s theorem [27] and approximates the kernel by estimating an expected value k(x,y) ' Ew[Zw(x)Zw(y)] where Zw(x) = \u221a 2cos(w\u2217x + b), with w \u223c N (0, 1) and b \u223c Uniform[0, 2\u03c0]\nfor the Gaussian kernel. The features correspond to a set \u03c6rff,i}i=1...D with \u03c6rff,i(x) = \u221a\n2cos(w\u2217i x + bi) where wi and bi are sampled from the aforementioned distributions. The higher the number of features, the better the approximation.\nAn interesting characteristic of RFF is that they are data independent. However it is possible to achieve a better approximation of the kernel with the same number of features if we use data to learn the features instead of the data-agnostic sampling procedure of the original RFF method. Some works have proposed data-dependent strategies to obtain better features: leverage score sampling [21, 22], reweighted random features [1, 33], and kernel learning [4, 21].\nIn this work we propose a new method to learn the w and b vectors using gradient descent. We called this approach adaptive Fourier features (AFF). The method uses a siamese neural network which is shown in the first step of Figure 1. The neural network is trained by sampling pairs of samples xi and xj from the data set and minimizing a square error\nloss function L = ( k(xi, xj)\u2212 k\u0302w,b(xi, xj) )2 , as shown in Algorithm 1.\nGonz\u00e1lez et al. [14] propose to use the square of the dot product of samples represented using RFF as a better approximation of the Gaussian kernel. We follow the same approach here. Figure 2 shows the comparison between the real Gaussian kernel, its approximation using RFF, squared RFF, adaptive Fourier features, and squared adaptive Fourier features . The best approximation is obtained by squared AFF, followed by AFF, squared RFF and RFF. As shown by the right plot in Figure 2, squared AFF can reach a good approximation even with a small number of features, this has a positive impact in the efficiency of the density estimation algorithms presented in the next sections."
        },
        {
            "heading": "3.2 Kernel Density Estimation using Adaptive Random Features",
            "text": "If we start from Eq. 2.1, with k\u03b3 representing the Gaussian kernel, we can do the following derivation [14]:\nf\u0302(x) = 1\nM\u03b3N N\u2211 i=1 k\u03b3(x,xi)\n= 1\nM\u03b3N N\u2211 i=1 k2\u03b3/2(x,xi)\n' 1 M\u03b3N N\u2211 i=1 \u3008\u03c6aff(x), \u03c6aff(xi)\u30092\n= 1\nM\u03b3N N\u2211 i=1 \u03c6Taff(x)\u03c6aff(xi)\u03c6 T aff(xi)\u03c6aff(x)\n= 1\nM\u03b3 \u03c6Taff(x)\n( 1\nN N\u2211 i=1 \u03c6aff(xi)\u03c6 T aff(xi)\n) \u03c6aff(x)\n= 1\nM\u03b3 \u03c6(x)Taff \u03c1 \u03c6(x)aff\n(3.1)\nThe matrix \u03c1 is called a density matrix [14] and it can be seem as a summary of the training data set that can be used to estimate the density of a new sample. Density matrices are a formalism used in quantum mechanics to represent the state of a quantum system. The last line in Eq. 3.1 can be seen as an instance of the Born rule that calculates the probability of obtaining a particular state \u03c6(x)aff when measuring a quantum system whose state is described by the density matrix \u03c1 [14].\nUsing directly Eq. 3.1 to do estimation of the probability of a new sample could be very inefficient, since the size of training density matrix is O(D2), where D is the dimension of the AFF. To alleviate this, we can perform a low-rank factorization of \u03c1 as follows:\n\u03c1 \u2248 V T\u039bV (3.2)\nwhere V \u2208 Rr\u00d7D contains as rows the eigenvectors of \u03c1 corresponding to the r largest eigenvalues, and \u039b \u2208 Rr\u00d7r is a diagonal matrix with the r largest eigenvalues. In this way, the estimation in Eq. 3.1 can be calculated as:\nf\u0302\u03b3(x) ' 1\nM\u03b3 \u03c6(x)Taff\u03c1\u03c6(x)aff\n' 1 M\u03b3 ||\u039b1/2V \u03c6(x)aff||2\n(3.3)\nEq. 3.3 is the basis for the neural density estimation model presented in the step 3 of Figure 2. The parameters of this model are the weights W and b of the AFF mapping and the V and \u039b parameters of the density estimation step. The AFF parameters are learned independently by training the neural architecture in step 1 of Figure 2 using Algorithm 1. The parameters V and \u039b can be learned using two different approaches depicted in Figure 2 and detailed in Algorithm 2 and 3.\nThe first approach, Algorithm 2, estimates the density matrix \u03c1 from training data and calculates the factorization components V and \u039b using a spectral decomposition. An important feature of this approach is that it does not requires any optimization, just averaging of the density matrices representing the training samples.\nThe second approach, Algorithm 3, exploits the fact that the prediction model (step 3 in Figure 2) is in fact differentiable neural network that can be trained by backpropagation and gradient descent, as it is the widespread practice for neural models. This process is in general more computational demanding than the optimization-less approach of Algorithm 2. Its main advantage is that it can be integrated with other deep architectures and trained jointly. This approach is explored in Section 4.3 of the experimental evaluation.\nAlgorithm 1: Adaptive Fourier Feature learning Input: Training data set D = {xi}i=1,\u00b7\u00b7\u00b7 ,N , \u03b3 kernel bandwidth Output: w, b AFF parameters Build a set s = {(x\u2032i, x\u2032\u2032i )}i=1,\u00b7\u00b7\u00b7 ,m where x\u2032i and x\u2032\u2032i are randomly sampled from D Apply gradient descent to find w\u2217, b\u2217 = arg minw,b 1 m \u2211 xi,xj\u2208s(k\u03b3(xi, xj)\u2212 k\u0302w,b(xi, xj)) 2 return w\u2217, b\u2217\nAlgorithm 2: Adaptive Fourier density matrix density estimation training (QAFFDE) Input: Training dataset D = {xi}i=1,\u00b7\u00b7\u00b7 ,N , w, b AFF parameters Output: V,\u039b Calculate \u03c1 = 1N \u2211N i=1 \u03c6aff(xi)\u03c6 T aff(xi)\nwhere \u03c6aff(xi) = \u221a\n2cos(wxi + b) Perform a spectral decomposition of \u03c1 \u03c1 \u2248 V T\u039bV return V,\u039b"
        },
        {
            "heading": "4 Experimental Evaluation",
            "text": ""
        },
        {
            "heading": "4.1 Unconditional Density Estimation",
            "text": "In this subsection we proposed an experiment based on synthetically generated data to evaluate the performance of various neural density estimation methods and compare them with QAFFDE and QAFFDE-SGD. It should be noted that the underlying density function is known for each data set used in the current experiment."
        },
        {
            "heading": "4.1.1 Data sets and experimental setup",
            "text": "Nine synthetic data sets were generated using distinct underlying density functions. Table 3 shows the density estimation function of each synthetic where each number represent the following distributions: (1). Multivariate normal distribution, (2). Arc distribution [25], (3). Mixture Gaussian distribution, (4). Star [23], (5). Swiss Roll [23], (6). Potential 1 , (7). Potential 2, (8). Potential 3, and (9). Potential 4 [29]. The symbols used in Table 3 were as follows: . For each dataset we generated 100 000 training data points and 50 000 testing data points. Besides, we assessed four baseline neural flow algorithms: (1). Masked Autoregressive Flow for Density Estimation [25], (2). Inverse Autoregressive Flow [17] and (3). Planar Flow [29].\nAll the algorithms were trained using Adam Optimizer with a polynomial decay. We used a cross validation setup along with random search for hyper parameters selection. For each data set and algorithm we tested 30 different combinations of hyper parameters. For the validation phase, we used 40 000 training data points and 1 000 validation data points. The batch size was set to 64. The initial learning rate for the polynomial decay was searched in the interval [10\u22125, 10\u22121]; the final learning rate was set to 10\u22125. For each neural flow algorithm, the following setup was used: hidden shape was searched between 20 and 1000; the number of layers was searched in the list 2, \u00b7 \u00b7 \u00b7 24. For Neural Splines: the number of bins was searched between three and eleven; the b-interval was searched between three and seven for each dimension. For QAFFDE: AFF dimension was explored between 250 and 2 000, sigma was explored between 2\u221220 and 220, and the training of the AFF was performed using 10000 pairs of different points. In the case of QAFFDE-SGD, we selected the number of eigen-components in the list 0, 0.1, 0.5, 1 where each number represents a percentage of the number of AFF. Moreover, we assessed random initialization and QAFFDE initialization for the density matrix of QAFFDE-SGD.\nWe used two metrics to evaluate the performance of each algorithm. Those metrics are the mean average error (MAE) and the Spearman\u2019s rank coefficient. For each metric, we compute the metrics using the estimate of each algorithm and the real probability. MAE is computed as: 1/n \u00b7 \u2211n i |p(xi)\u2212 p\u0302(xi)|, where n is the number of data points, p(xi) is the real probability density, p\u0302(xi) is the estimated probability density given by the method, and | \u00b7 | is the absolute value function."
        },
        {
            "heading": "4.1.2 Results and discussion",
            "text": "Table 1 shows the results obtained by each of the six neural density estimation algorithms on the 9 synthetic data sets. The results on the ARC, Bimodal and Binomial data sets are similar among the algorithms, excluding Planar Flow\nAlgorithm 3: Adaptive Fourier density matrix density estimation training using gradient descent (QAFFDE-SGD) Input: Training dataset D = {xi}i=1,\u00b7\u00b7\u00b7 ,N , w, b AFF parameters Output: V \u2217,\u039b\u2217 Apply gradient descent to find: V \u2217,\u039b\u2217 = arg minV,\u039b \u2211N i=1 log f\u0302(xi)\nwhere f\u0302(xi) = 1M\u03b3 ||\u039b 1/2V \u03c6aff(x)||2\nand \u03c6aff(xi) = \u221a\n2cos(wxi + b) return V \u2217,\u039b\u2217\nFigure 3: Synthetic data set\nwhose performance is inferior. The performance of QAFFDE and QAFFDE-SGD is better when compared to the other methods at Potentials 1 to 4, where QAFFDE-SGD is better than QAFFDE at potential 4, which is a really difficult distribution function. The performance of QAFFDE in Star Eight and Swiss Roll is superior compared to the other algorithms. Planar Flow is the worst algorithm among them. Neural Splines has consistent satisfactory results.\nTable 2 shows the results of the mean average error between the actual distribution function and the estimate of each density estimation function on the 9 synthetic data sets. Made has the best MAE in arc, bimodal and binomial. QAFFDE is the best in potential 2, start eight and swiss roll. Neural splines is the best in potential 1 and 4. QAFFDE-SGD is similar to QAFFDE in terms of MAE results, and its results are close to the best results in Potential 1 to 4, Start Eight and Swiss Roll.\nQAFFDE is not the best one in terms of MAE. However, it is worth noting that for the majority of real applications such as classification, anomaly detection, and regression, the real value of the density function is not really important. The most important property is being able to differentiate between low density areas and high-density areas. This property is intrinsically captured by Spearman correlation.\nFigure 5 shows the density estimate obtained by applying the six algorithms on the nine two-dimensional synthetic data sets. The first column is QAFFDE. Its results are the best amongst the six algorithms. It was able to regenerate every data set including the most difficult ones such as Potential 3 and Potential 4. The second columns is QAFFDE Sgd. Its results are similar to those of QAFFDE in seven of nine datasets. It obtained a different results in Potential\n4 and Swizz Roll. Figure 6 shows the comparison between the real density and the density estimate obtained on the nine synthetic data sets when applying the six different algorithms. QAFFDE (first column) shows good results and lower scatter in all data sets. In the first and the second data set, it shows slightly more dispersion than Made. In the Swizz Roll it shows the best performance of all the algorithms. The QAFFDE-SGD algorithm (second column) shows a tendency to over estimate points of low density points given presumably by its log likelihood optimization approach."
        },
        {
            "heading": "4.2 Unconditional Random Density Estimation in Higher Dimensions",
            "text": "Classical methods for density estimation, such as Gaussian Mixture Models [20] or Kernel Density Estimation [26, 30], suffer with higher dimensions due to the curse of dimensionality. Several papers claim that normalizing flow methods can solve this problem. Related to QAFFDE, it potentially can inherit the problems from kernel density estimation; however, as shown in the following experiment, it can deal with higher dimensions and obtain good density estimates. In this subsection, we propose an experiment to show the robustness of QAFFDE methods when the number of dimensions varies and its increased."
        },
        {
            "heading": "4.2.1 Data sets and experimental setup",
            "text": "In this experiment, we generate data points from a mixture of random independent Gaussian distributions. For n dimensions, we generated 10 \u00b7 n Gaussian distributions for n \u2208 [1, \u00b7 \u00b7 \u00b7 , 10]. The \u00b5i parameters of the Gaussians were generated from a uniform distribution \u00b5i \u2208 (0, 1)n. The covariance matrices, \u03a3i, were generated with a vector of uniform values as eigenvalues. With this vector, we compute the algorithm proposed by Davies et al. [7] to generate a random correlation matrix. Using both, the centroid and the covariance matrix, we sampled equal number of points from each Gaussian distributions to obtain 40 000 training data points and 10 000 testing data points. Figure 4 shows an example in two dimensions of a the random generated samples."
        },
        {
            "heading": "4.2.2 Results and discussion",
            "text": "Figure 4 shows the results obtained by each algorithm in the random Gaussian mixture model synthetic data set. Planar flow has the worst performance among the six algorithms. Made and Inverse Maf have a similar behavior. They start at nearly 0.92 of Spearman\u2019s correlation and decrease with higher dimensions. In ten dimensions obtain nearly 0.76 of Spearman\u2019s correlation. Neural Splines starts better than Made and Inverse Maf; however, its performance decrease with more dimension. It shows worst performance in ten dimension compared to Made and Inverse Maf obtaining 0.61 of Sperman\u2019s correlation. QAFFDE start with nearly 1.0 Spearman\u2019s correlation and drop slightly with more dimensions. However, its performance is the best among all the neural density estimation methods. QAFFDE-SGD has similar behavior than QAFFDE but with lower Spearman\u2019s correlation."
        },
        {
            "heading": "4.3 Conditional Density Estimation",
            "text": "Density estimation can be used as a conditional density algorithm (conditioned on class values). In this subsection, we present a systematic evaluation of the conditional density estimation obtained by QAFFDE and compared it against state-of-the-art neural flow methods in two frequently used benchmark image data sets MNIST and CIFAR."
        },
        {
            "heading": "4.3.1 Data sets and experimental setup",
            "text": "Two benchmark image data sets were used. The details of these data sets are shown in Table 3. QAFFDE was trained using a conditional Bayesian density estimation strategy and ADAM as the stochastic optimization gradient algorithm. As a baseline we compared QAFFDE algorithms against three state-of-the-art normalizing flow algorithms (MADE [12], RealNVP [8], MAF [25]) and RoundTrip [23], which is a normalizing flow generative adversarial algorithm, for further details see Subsection 2.2. Each algorithm was assessed using the conditional density computed over the test image data set and maximizing the posterior probability conditioning on the class label. Besides, we built a LeNet architecture [19] as a feature extraction method and paired it with QAFFDE-SGD as the density estimation method. The LeNet part had two sequential convolutional layers. Both of them have a kernel size of 5, a same padding and a relu activation function. The first and the second convolutional layer had 20 and 50 filters correspondingly. The third layer is a fully connected layer with a size of 84 neurons. This fully connected layer is joined to the QAFFDE layer. The AFF layer were trained using 1000 adaptive Fourier features. In addition, the AFF layer was only optimized using the algorithm shown in the Section 3, but not in the QAFFDE-SGD optimization steps to make all comparisons fair. Thus, all AFF weights were set as untrainable in the QAFFDE-SGD optimization step of the neural network.\nFor each data set, we performed a hyper parameter optimization using a cross-validation methodology with 30 randomly generated settings. For tunning the \u03b3 parameter of QAFFDE, we computed the mean distance between pair of points in every data set and selected an appropriate value for \u03b3 = 12\u03c32 . The number of adaptive Fourier features was set to 1000 for every QAFFDE algorithm. The learning rate was selected in the interval (0, 0.001]. The number of eigen-components was selected in the list 0, 0.1, 0.5, 1 where each number represents a percentage of the number of AFF. The mean of the accuracy on ten experiments was reported."
        },
        {
            "heading": "4.3.2 Results and discussion",
            "text": "Table 4 shows the results obtained by each algorithm in the density estimation task for MNIST and CIFAR-10 image datasets. It can be seen that without convolutional neural networks QAFFDE-SGD is better than all other neural flow methods except RoundTrip on Mnist, and it is the best on CIFAR-10. When we use convolutional layers with QAFFDE-SGD its performance outperforms the other methods on both image datasets. The method can be used not only as a density estimation algorithm, but also as a conditional density estimation method. QAFFDE was sistem"
        },
        {
            "heading": "5 Conclusion",
            "text": "This paper presented a method for density estimation and its systematic evaluation on different density estimation tasks. The method combines two, in principle, different approaches for density estimation. First, it can be seen as an approximated version of KDE that do not require to store all the training samples; and, second, it can be seen as a form of neural density estimation that can be trained using gradient descent and can be integrated with other deep learning architectures. The method combines two ideas from seemingly unrelated fields, density matrices used in quantum mechanics to represent the state of a quantum system, and random features a method to efficiently approximate kernels in machine learning. QAFFDE was systematically evaluated and compared against state-of-the-art neural density estimation methods, on four different experiments. QAFFDE showed a competitive performance through all the experiments, performing on par and in some cases outperforming state-of-the-art methods."
        },
        {
            "heading": "Acknowledgements",
            "text": ""
        }
    ],
    "year": 2022
}