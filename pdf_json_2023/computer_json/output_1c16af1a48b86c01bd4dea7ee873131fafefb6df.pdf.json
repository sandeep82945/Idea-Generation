{
    "abstractText": "Most existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training an ML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.",
    "authors": [
        {
            "affiliations": [],
            "name": "LYRIC DOSHI"
        },
        {
            "affiliations": [],
            "name": "VINCENT ZHUANG"
        },
        {
            "affiliations": [],
            "name": "HAOYU HUANG"
        },
        {
            "affiliations": [],
            "name": "Vincent Zhuang"
        },
        {
            "affiliations": [],
            "name": "Gaurav Jain"
        },
        {
            "affiliations": [],
            "name": "Ryan Marcus"
        },
        {
            "affiliations": [],
            "name": "Haoyu Huang"
        }
    ],
    "id": "SP:70988846ae5c50aacd41868d5983d1eb62ca397f",
    "references": [
        {
            "authors": [
                "Mert Akdere",
                "Ugur \u00c7etintemel",
                "Matteo Riondato",
                "Eli Upfal",
                "Stanley B Zdonik"
            ],
            "title": "Learning-based query performance modeling and prediction",
            "venue": "IEEE 28th International Conference on Data Engineering",
            "year": 2012
        },
        {
            "authors": [
                "Gunes Alu\u00e7",
                "David E DeHaan",
                "Ivan T Bowman"
            ],
            "title": "Parametric plan caching using density-based clustering",
            "venue": "IEEE 28th International Conference on Data Engineering",
            "year": 2012
        },
        {
            "authors": [
                "Alejandro Correa Bahnsen",
                "Djamia Aouada",
                "Bj\u00f6rn Ottersten"
            ],
            "title": "Example-dependent cost-sensitive logistic regression for credit scoring",
            "venue": "In 2014 13th International conference on machine learning and applications",
            "year": 2014
        },
        {
            "authors": [
                "Surajit Chaudhuri",
                "Hongrae Lee",
                "Vivek R Narasayya"
            ],
            "title": "Variance aware optimization of parameterized queries",
            "venue": "In Proceedings of the 2010 ACM SIGMOD International Conference on Management of data",
            "year": 2010
        },
        {
            "authors": [
                "Surajit Chaudhuri",
                "Vivek Narasayya",
                "Ravi Ramamurthy"
            ],
            "title": "Exact cardinality query optimization for optimizer testing",
            "venue": "Proceedings of the VLDB Endowment",
            "year": 2009
        },
        {
            "authors": [
                "Djellel Eddine Difallah",
                "Andrew Pavlo",
                "Carlo Curino",
                "Philippe Cudr\u00e9-Mauroux"
            ],
            "title": "OLTP-Bench: An Extensible Testbed for Benchmarking Relational Databases",
            "venue": "PVLDB 7,",
            "year": 2013
        },
        {
            "authors": [
                "Bailu Ding",
                "Sudipto Das",
                "Ryan Marcus",
                "Wentao Wu",
                "Surajit Chaudhuri",
                "Vivek R Narasayya"
            ],
            "title": "2019. Ai meets ai: Leveraging query executions to improve index recommendations",
            "venue": "In Proceedings of the 2019 International Conference on Management of Data",
            "year": 2019
        },
        {
            "authors": [
                "Anshuman Dutt",
                "Vivek Narasayya",
                "Surajit Chaudhuri"
            ],
            "title": "Leveraging re-costing for online optimization of parameterized queries with guarantees",
            "venue": "In Proceedings of the 2017 ACM International Conference on Management of Data",
            "year": 2017
        },
        {
            "authors": [
                "Yuxing Han",
                "Ziniu Wu",
                "Peizhi Wu",
                "Rong Zhu",
                "Jingyi Yang",
                "Liang Wei Tan",
                "Kai Zeng",
                "Gao Cong",
                "Yanzhao Qin",
                "Andreas Pfadler"
            ],
            "title": "Cardinality Estimation in DBMS: A Comprehensive Benchmark Evaluation",
            "year": 2021
        },
        {
            "authors": [
                "Naveen Reddy Jayant R Haritsa"
            ],
            "title": "Analyzing plan diagrams of database query optimizers",
            "venue": "In Proceedings of the 31st international conference on Very large data bases. VLDB Endowment",
            "year": 2005
        },
        {
            "authors": [
                "Alexander Hepburn",
                "Ryan McConville",
                "Ra\u00fal Santos-Rodr\u00edguezo",
                "Jes\u00fas Cid-Sueiro",
                "Dario Garc\u00eda-Garc\u00eda"
            ],
            "title": "Proper losses for learning with example-dependent costs",
            "venue": "In Second International Workshop on Learning with Imbalanced Domains: Theory and Applications. PMLR,",
            "year": 2018
        },
        {
            "authors": [
                "Arvind Hulgeri",
                "S Sudarshan"
            ],
            "title": "Parametric query optimization for linear and piecewise linear cost functions",
            "venue": "Proceedings of the 28th International Conference on Very Large Databases",
            "year": 2002
        },
        {
            "authors": [
                "Yannis E Ioannidis",
                "Raymond T Ng",
                "Kyuseok Shim",
                "Timos K Sellis"
            ],
            "title": "Parametric query optimization",
            "venue": "The VLDB Journal 6,",
            "year": 1997
        },
        {
            "authors": [
                "Kyoungmin Kim",
                "Jisung Jung",
                "In Seo",
                "Wook-Shin Han",
                "Kangwoo Choi",
                "Jaehyok Chong"
            ],
            "title": "2022. Learned Cardinality Estimation: An In-depth Study",
            "venue": "In Proceedings of the 2022 International Conference on Management of Data",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Kipf",
                "Thomas Kipf",
                "Bernhard Radke",
                "Viktor Leis",
                "Peter Boncz",
                "Alfons Kemper"
            ],
            "title": "Learned cardinalities: Estimating correlated joins with deep learning",
            "year": 2018
        },
        {
            "authors": [
                "Sanjay Krishnan",
                "Zongheng Yang",
                "Ken Goldberg",
                "Joseph Hellerstein",
                "Ion Stoica"
            ],
            "title": "Learning to optimize join queries with deep reinforcement learning",
            "year": 2018
        },
        {
            "authors": [
                "Viktor Leis",
                "Andrey Gubichev",
                "Atanas Mirchev",
                "Peter Boncz",
                "Alfons Kemper",
                "Thomas Neumann"
            ],
            "title": "How good are query optimizers, really",
            "venue": "Proceedings of the VLDB Endowment 9,",
            "year": 2015
        },
        {
            "authors": [
                "Jeremiah Liu",
                "Zi Lin",
                "Shreyas Padhy",
                "Dustin Tran",
                "Tania Bedrax Weiss",
                "Balaji Lakshminarayanan"
            ],
            "title": "Simple and principled uncertainty estimation with deterministic deep learning via distance awareness",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Yao Lu",
                "Srikanth Kandula",
                "Arnd Christian K\u00f6nig",
                "Surajit Chaudhuri"
            ],
            "title": "Pre-training summarization models of structured datasets for cardinality estimation",
            "venue": "Proceedings of the VLDB Endowment 15,",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Marcus",
                "Parimarjan Negi",
                "Hongzi Mao",
                "Nesime Tatbul",
                "Mohammad Alizadeh",
                "Tim Kraska"
            ],
            "title": "Bao: Making learned query optimization practical",
            "venue": "In Proceedings of the 2021 International Conference on Management of Data",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Marcus",
                "Parimarjan Negi",
                "Hongzi Mao",
                "Chi Zhang",
                "Mohammad Alizadeh",
                "Tim Kraska",
                "Olga Papaemmanouil",
                "Nesime Tatbul"
            ],
            "title": "Neo: A learned query optimizer",
            "year": 2019
        },
        {
            "authors": [
                "Ryan Marcus",
                "Olga Papaemmanouil"
            ],
            "title": "Deep reinforcement learning for join order enumeration",
            "venue": "In Proceedings of the First International Workshop on Exploiting Artificial Intelligence Techniques for Data Management",
            "year": 2018
        },
        {
            "authors": [
                "Ryan Marcus",
                "Olga Papaemmanouil"
            ],
            "title": "Towards a hands-free query optimizer through deep learning",
            "venue": "arXiv preprint arXiv:1809.10212",
            "year": 2018
        },
        {
            "authors": [
                "Parimarjan Negi",
                "Ryan Marcus",
                "Andreas Kipf",
                "Hongzi Mao",
                "Nesime Tatbul",
                "Tim Kraska",
                "Mohammad Alizadeh"
            ],
            "title": "Flow-Loss: learning cardinality estimates that matter",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Richard T Snodgrass",
                "Sabah Currim",
                "Young-Kyoon Suh"
            ],
            "title": "Have query optimizers hit the wall",
            "venue": "The VLDB Journal 31,",
            "year": 2022
        },
        {
            "authors": [
                "Ji Sun",
                "Jintao Zhang",
                "Zhaoyan Sun",
                "Guoliang Li",
                "Nan Tang"
            ],
            "title": "Learned cardinality estimation: A design space exploration and a comparative evaluation",
            "venue": "Proceedings of the VLDB Endowment 15,",
            "year": 2021
        },
        {
            "authors": [
                "Immanuel Trummer"
            ],
            "title": "Exact cardinality query optimization with bounded execution cost",
            "venue": "In proceedings of the 2019 international conference on management of data",
            "year": 2019
        },
        {
            "authors": [
                "Grigorios Tsoumakas",
                "Ioannis Katakis"
            ],
            "title": "Multi-label classification: An overview",
            "venue": "International Journal of Data Warehousing and Mining (IJDWM)",
            "year": 2007
        },
        {
            "authors": [
                "Kapil Vaidya",
                "Anshuman Dutt",
                "Vivek Narasayya",
                "Surajit Chaudhuri"
            ],
            "title": "Leveraging query logs and machine learning for parametric query optimization",
            "venue": "Proceedings of the VLDB Endowment 15,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoying Wang",
                "Changbo Qu",
                "Weiyuan Wu",
                "Jiannan Wang",
                "Qingqing Zhou"
            ],
            "title": "Are we ready for learned cardinality estimation",
            "year": 2020
        },
        {
            "authors": [
                "Wentao Wu",
                "Jeffrey F Naughton",
                "Harneet Singh"
            ],
            "title": "Sampling-based query re-optimization",
            "venue": "In Proceedings of the 2016 International Conference on Management of Data",
            "year": 2016
        },
        {
            "authors": [
                "Zongheng Yang",
                "Wei-Lin Chiang",
                "Sifei Luan",
                "Gautam Mittal",
                "Michael Luo",
                "Ion Stoica"
            ],
            "title": "Balsa: Learning a Query Optimizer Without Expert Demonstrations",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Zongheng Yang",
                "Amog Kamsetty",
                "Sifei Luan",
                "Eric Liang",
                "Yan Duan",
                "Xi Chen",
                "Ion Stoica"
            ],
            "title": "NeuroCard: one cardinality estimator for all tables",
            "year": 2020
        },
        {
            "authors": [
                "Zongheng Yang",
                "Eric Liang",
                "Amog Kamsetty",
                "Chenggang Wu",
                "Yan Duan",
                "Xi Chen",
                "Pieter Abbeel",
                "Joseph M Hellerstein",
                "Sanjay Krishnan",
                "Ion Stoica"
            ],
            "title": "Deep unsupervised cardinality estimation",
            "venue": "arXiv preprint arXiv:1905.04278",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "109\nKepler: Robust Learning for Faster ParametricQuery Optimization LYRIC DOSHI\u2217, Google, USA VINCENT ZHUANG\u2217, Google, USA GAURAV JAIN, Google, USA RYAN MARCUS, University of Pennsylvania, USA HAOYU HUANG, Google, USA DENIZ ALTINB\u00dcKEN, Google, USA EUGENE BREVDO, Google, USA CAMPBELL FRASER, Google, USA\nMost existing parametric query optimization (PQO) techniques rely on traditional query optimizer cost models, which are often inaccurate and result in suboptimal query performance. We propose Kepler, an end-to-end learning-based approach to PQO that demonstrates significant speedups in query latency over a traditional query optimizer. Central to our method is Row Count Evolution (RCE), a novel plan generation algorithm based on perturbations in the sub-plan cardinality space. While previous approaches require accurate cost models, we bypass this requirement by evaluating candidate plans via actual execution data and training an ML model to predict the fastest plan given parameter binding values. Our models leverage recent advances in neural network uncertainty in order to robustly predict faster plans while avoiding regressions in query performance. Experimentally, we show that Kepler achieves significant improvements in query runtime on multiple datasets on PostgreSQL.\nCCS Concepts: \u2022 Information systems\u2192 Query optimization.\nAdditional Key Words and Phrases: databases, query optimization, machine learning\nACM Reference Format: Lyric Doshi, Vincent Zhuang, Gaurav Jain, Ryan Marcus, Haoyu Huang, Deniz Alt\u0131nb\u00fcken, Eugene Brevdo, and Campbell Fraser. 2023. Kepler: Robust Learning for Faster Parametric Query Optimization. Proc. ACM Manag. Data 1, 1, Article 109 (May 2023), 25 pages. https://doi.org/10.1145/3588963"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Parametric query optimization (PQO) aims to optimize parameterized queries, i.e. queries that have identical SQL structure and only differ in the value of bound parameters. Such parameterized queries are ubiquitous in modern database usage and present a significant opportunity for improving query performance because they are executed repeatedly. \u2217Equal contribution.\nAuthors\u2019 addresses: Lyric Doshi, lyric@google.com, Google, Mountain View, CA, USA; Vincent Zhuang, vincentzhuang@ google.com, Google, Mountain View, CA, USA; Gaurav Jain, gaurav@gauravjain.org, Google, Mountain View, CA, USA; Ryan Marcus, rcmarcus@seas.upenn.edu, University of Pennsylvania, Philadelphia, PA, USA; Haoyu Huang, haoyuhuang@ google.com, Google, Mountain View, CA, USA; Deniz Alt\u0131nb\u00fcken, denizalti@google.com, Google, Mountain View, CA, USA; Eugene Brevdo, ebrevdo@google.com, Google, Mountain View, CA, USA; Campbell Fraser, campbellf@google.com, Google, Mountain View, CA, USA.\nThis work is licensed under a Creative Commons Attribution International 4.0 License.\n\u00a9 2023 Copyright held by the owner/author(s). 2836-6573/2023/5-ART109 https://doi.org/10.1145/3588963\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nar X\niv :2\n30 6.\n06 79\n8v 2\n[ cs\n.D B\n] 1\n8 O\nct 2\n02 3\nHowever, PQO has primarily been studied from the perspective of reducing query planning time by avoiding re-optimization when possible [7, 9, 13, 17, 18, 34]. Such approaches are implicitly constrained by the performance of the system\u2019s query optimizer, and therefore inherit all of the well-studied sub-optimalities of traditional query optimizers [22]. Thus, an ideal system for parameterized queries should not only seek to minimize planning time via PQO, but also optimize query execution performance via query optimization (QO). A variety of approaches have attempted to improve query optimization by applying machine learning [20, 25, 29, 38, 39]. Unfortunately, most learned query optimization techniques suffer from at least four drawbacks: (1) they require inference times higher than traditional methods [19, 24], (2) they have inconsistent performance across dataset sizes and distributions [19, 26, 31], and (3) they often have unclear query performance improvements [19]. Worse yet, many of these learned systems lack (4) robustness: regressions in query performance are unacceptable in most production scenarios [12]. This poses an especially large challenge for learning-based approaches, since they typically cannot guarantee that all of their predictions result in improved execution time [35].\nWe propose that restricting the query optimization problem to the parameterized query setting poses a more tractable learning problem and hence can be more robustly solved. To this end, we present Kepler (K-plan Evolution for Parametric Query Optimization: Learned, Empirical, Robust), an end-to-end learning-based approach for parameterized queries. Building on prior work in PQO [34], Kepler leverages a novel plan generation strategy, a training query execution phase, and a robust neural network model design. Combined, we show that these techniques provide significant improvements in both planning time and query execution performance, satisfying both the PQO and QO objectives. Best of all, Kepler\u2019s use of robust neural network techniques drastically reduces the frequency and magnitude of performance regressions. Figure 1 highlights how each of Kepler\u2019s components contribute to a 2.41x geometric mean speedup across the entire Stack benchmark [25]. Kepler follows a decoupled plan generation and learning-based plan prediction architecture similar to the approach of [34] with three key differences. First, Kepler provides the key insight that designing better candidate plan generation algorithms can lead to substantially faster plans than the built-in optimizer\u2019s. We propose Row Count Evolution (RCE), a method that efficiently generates candidate plans by perturbing the optimizer\u2019s cardinality estimates. RCE only requires a simple interface to any standard cost-based optimizer, making it compatible with most database systems. Second, Kepler leverages actual query execution data to build a training dataset for best-plan prediction, avoiding the well-studied mismatch between cost models and execution latency [22].\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nWhile Kepler\u2019s collection of execution data may be costly if the parameterized query is run infrequently, we argue that the additional execution data in our setting is justified by (1) the scale of parameterized queries in production and (2) the query execution speedups afforded by RCE. Third, Kepler uses robust neural network prediction techniques to decrease tail latency and reduce query regressions (i.e. worse performance than the existing query optimizer). Specifically, Kepler uses Spectral-normalized Neural Gaussian Processes (SNGPs) [23] to accurately quantify how confident it is about a prediction, and falls back to the database\u2019s query optimizer when it is uncertain.\nOur contributions.\n\u2022 We identify a novel and practical formulation of query optimization for parameterized query templates in which speedups against a classical query optimizer can be robustly achieved. \u2022 We propose a novel candidate plan generation algorithm, Row Count Evolution (RCE), that produces significant speedup compared to classical query optimizers on real-world and synthetic datasets. \u2022 We demonstrate that incorporating robust ML techniques allows models to capture large portions of the speedups while greatly reducing the risk of regressions. \u2022 We demonstrate that our model inference costs are negligible via an end-to-end PostgreSQL integration for the query path. \u2022 We open-source both our system implementation for PostgreSQL1 as well as our query execution datasets, which we believe is the first dataset tailored towards parameterized query optimization. The datasets collectively represent \u223c14.2 CPU years of query execution time. They serve as a benchmark for further work on best-plan prediction as well as simulating more efficient techniques for training data collection."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": "Parametric query optimization. PQO has been extensively studied in a variety of works [7, 9, 13, 17, 18, 34]. The goal of the standard PQO formulation is to reduce the amount of times the query optimizer is invoked while minimizing the corresponding regression in query latency [7, 13, 34]. Although Kepler also focuses on parametric queries, its primary objective is closer to that of standard query optimization, which seeks to improve query latencies. Kepler also simultaneously improves on the PQO objective by leveraging fast-inference ML models. Prior PQO approaches typically make simplifying assumptions such as heavily relying on the optimizer cost model or using base table selectivities as input features [13, 34]. This may be feasible for some advanced commercial systems; however, this over-reliance on the existing optimizer is particularly dangerous given the well-studied deficiencies of optimizers such as PostgreSQL [22]. Our approach follows a similar structure as [34], which also decouples the populateCache (candidate generation) and getPlan stages (ML-based prediction). However, since they focus on the standard PQO objective of attempting to match the existing optimizer, they require using a bandit algorithm to reduce their training data cost. By contrast, the primary objectives of Kepler are query performance and robustness, leading to a lower emphasis on training query efficiency. Several popular database systems have implemented PQO features, including Oracle Adaptive Cursor Sharing, Aurora Managed Plans, and SQL Server Parameter Sensitivity Plan optimization [1\u2013 3]. These features all heavily rely on their cost models (based on traditional statistics and heuristics), and do not utilize machine learning models.\n1https://github.com/google/kepler\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nQuery plan generation. Several prior works suggest methods for candidate generation, which we divide into four main categories.\n(1) Default optimizer plans. The simplest method combines the optimizer\u2019s selected plan for each query instance. This approach is frequently found in PQO algorithms since they seek to cache the optimizer\u2019s plans [34]. This strategy is also employed in [30] to estimate the empirical suboptimality of existing query optimizers. The quality of the resulting candidate plan set is predicated upon the optimizer\u2019s ability to either generate optimal plans for each query instance or a sufficient variety of good plans across the workload to benefit from plan sharing. However, we empirically observed that the optimizer fails to do so on real-world datasets. (Table 10b). (2) Cost-based plan pruning. populateCache algorithm [34] extends the default optimizer candidate generation method with cost-based \ud835\udc3e-set identification to prune the candidate set to size \ud835\udc3e . However, this pruning method may mistakenly prune good plans if the correlation between the cost estimates and actual execution times are poor. (3) Optimizer configuration parameters. Query optimizers typically expose a variety of configuration parameters that can be used to alter their query planning behavior. In particular, PostgreSQL has configuration parameters that allow one to disable entire classes of join and scan operators from being used in query plans. Bao selectively applies subsets of these parameters in order to generate new query plans [25]. Although simple, disabling operator types is a heavy-handed and indirect approach to generating new plans. (4) Exact cardinalities. Exact cardinality query optimization (ECQO) attempts to construct the optimal plan by computing the plan induced by the exact cardinality values of all possible sub-plans [10]. However, for sufficiently complex queries, evaluating these exponentiallymany sub-plans is prohibitively slow even with optimizations [32]. The selected plans are also not always the fastest, as observed by [30].\nIn summary, these methods are all unsatisfactory for a variety of reasons: failure to generate faster plans (1, 2, 4), ineffectively exploring the plan space (3), or are computationally intractable (4).\nMachine learning for query optimization. A wide range of techniques apply ML on QO, most notably for predicting cardinality estimates (CE) [20, 38, 39]. Recent work show cardinality estimation may be brittle in practice, and that even small Q-errors can lead to noticeably worse plans [22, 35]. In general, these work do not measure the actual end-to-end execution latency of selected plans after integrating their models into an optimizer [24].\nSeveral approaches have demonstrated improved query performance, but typically do not consider the issue of robustness. Neo [26] and Bao [25] leverage tree convolutional neural networks to adaptively optimize plans using reinforcement learning and contextual bandits respectively. These online algorithms offer no guarantees on stability or regression avoidance, and hence cannot reliably be deployed in production. Similarly, techniques applying deep reinforcement learning to QO have not demonstrated consistently better performance and suffer from robustness issues [21, 28, 37]. For example, Figure 9 in [37] indicates a significant amount of regressions both at train and test time."
        },
        {
            "heading": "3 OVERVIEW",
            "text": "In this section, we describe our problem setting (Section 3.1), give an overview of our approach (Section 3.2), and further discuss specific design choices that are made in Kepler (Section 3.3).\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023."
        },
        {
            "heading": "3.1 Problem Setting",
            "text": "As in prior work [34], we consider parameterized queries that are repeatedly invoked with different parameter bindings. Such queries are specified by a template \ud835\udc44 with\ud835\udc5a parameterized predicates2 \ud835\udc650, . . . , \ud835\udc65\ud835\udc5a\u22121 of varying data types. We let \ud835\udc5e denote a specific query instance, i.e. \ud835\udc44 with a fixed set of parameter binding values. A query plan \ud835\udc5d associated with a template \ud835\udc44 specifies how to execute any query instance \ud835\udc5e \u223c \ud835\udc44 . A sub-plan query of \ud835\udc44 is \ud835\udc44 restricted to only a subset of its tables [14], and its output cardinality is referred to as its sub-plan cardinality. We assume a fixed database system with a built-in query optimizer, and denote the default plan \ud835\udc5ddefault (\ud835\udc5e) to be the plan selected by the query optimizer for \ud835\udc5e. Finally, a workload\ud835\udc4a \u2282 W consists of a set of query instances {\ud835\udc5e0, . . . , \ud835\udc5e\ud835\udc5b\u22121} for a single template \ud835\udc44 , whereW denotes the space of all possible query instances."
        },
        {
            "heading": "3.2 Kepler Overview",
            "text": "Our approach at a high level follows that of [34]: we consider a single, isolated query template \ud835\udc44 , and decouple the problems of generating a set of possible plans and deciding which plan to use for each query instance. More formally, these problems can be described as: (1) Candidate generation. Generate a candidate set of \ud835\udc58 plans {\ud835\udc5d0, . . . , \ud835\udc5d\ud835\udc58\u22121} for \ud835\udc44 , out of the\nexponentially-large set of all possible plans P (corresponding to populateCache in [34]). (2) Best-plan prediction. Learn a mapping \ud835\udc40 :W \u2192 \ud835\udc43 that minimizes some objective, e.g.\nsome measure of execution latency over the workload (corresponding to getPlan in [34]). Unlike [34], who attempt tomatch the performance of the built-in optimizer, our goal is to improve upon the built-in optimizer as much as possible. To achieve this, Kepler includes a sophisticated candidate generation algorithm, described in Section 4, that empirically generates better plans than the built-in optimizer. The afforded speedups allow Kepler to avoid relying on potentially-brittle online learning approaches (e.g. contextual bandits) during the training data collection phase.\nObjective. We first define several key metrics and terms in our problem setting. For a given query instance, we denote the optimal plan over some plan set \ud835\udc43 as \ud835\udc5d\ud835\udc43opt = min\ud835\udc5d\u2208\ud835\udc43 \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d, \ud835\udc5e), where \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 refers to the actual execution time. We define \ud835\udc5d\u2217opt (\ud835\udc5e) as the optimal plan over all possible plans, i.e. when \ud835\udc43 = P. Typically, the optimal plan refers to \ud835\udc5d\u2217opt for candidate generation and \ud835\udc5d\ud835\udc43opt for modeling. We also refer to near-optimal plans as plans that have similar execution time to \ud835\udc5d\ud835\udc43opt or \ud835\udc5d \u2217 opt.\nFor some fixed candidate set \ud835\udc43 , we define the (oracle) speedup ratio relative to the default plan as: \ud835\udc46opt (\ud835\udc43,\ud835\udc4a ) = \u2211\n\ud835\udc5e\u2208\ud835\udc4a \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5ddefault, \ud835\udc5e)\u2211 \ud835\udc5e\u2208\ud835\udc4a \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d\ud835\udc43opt, \ud835\udc5e)\n(1)\nThis quantity is the factor by which we can improve the total execution time of the workload if we had oracle access to the optimal plan in \ud835\udc43 for each query instance. We note that this ratio corresponds exactly with the definition of execution cost sub-optimality in [34]; the re-naming to speedup emphasizes the differences in our system objectives. Since we can union \ud835\udc43 with the set of all default plans over\ud835\udc4a , this speedup ratio is always lower bounded by 1.\nSimilarly, for some model\ud835\udc40 :W \u2192 \ud835\udc43 , we define its model speedup as: \ud835\udc46model (\ud835\udc4a ) = \u2211\n\ud835\udc5e\u2208\ud835\udc4a \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5ddefault, \ud835\udc5e)\u2211 \ud835\udc5e\u2208\ud835\udc4a \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5dmodel, \ud835\udc5e)\n(2)\n2The parameters do not necessarily have to be in predicates, e.g. they may appear in a LIMIT clause. However, our experiments only include the parameterized predicate case.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\n109:6 Lyric Doshi et al.\n\u2462 Deploy trained models\n\u2026\nTraining DBMSs\n\u2460 Generate candidate plans\n\u2461 Collect execution statistics SQL queries \u2026 Production DBMSs Kepler ML models Kepler ML models Kepler Trainer\nThis quantity corresponds to how much faster the model is at executing a workload than the default optimizer. Although \ud835\udc46model is by definition upper bounded by \ud835\udc46opt, it is not necessarily lower bounded by 1, i.e. if the model selects plans worse than the default plan.\nAn auxiliary objective of Kepler is reducing workload tail latency. Several work have identified that database optimizers may perform significantly worse in the tail of the query latency distribution, which poses a significant obstacle for use cases that require a more uniform runtime [25].\nKepler architecture. Figure 2 shows the architecture of Kepler, consisting of a Kepler trainer and Kepler client. The trainer ingests query instances from the query logs produced by production DBMSs and aggregates them into query templates. For each query template \ud835\udc44\ud835\udc56 , the Kepler trainer aims to find the near-optimal plans for all its query instances \ud835\udc5e \ud835\udc57 . It uses Row Count Evolution (RCE) to generate candidate plans \ud835\udc5d\ud835\udc58 and executes the queries with these plans to collect execution statistics. Tominimize impact on productionDBMSs, the trainermay optionally request a production DBMS to spawn ephemeral instances to execute these queries. The Kepler trainer trains an ML model to predict the best plan for \ud835\udc5e \ud835\udc57 based on these execution statistics and deploys the trained models into the production DBMSs.\nA Kepler client maintains a mapping from a query template to an ML model. When a production DBMS receives a query instance \ud835\udc5e, the client first checks if an ML model is available for \ud835\udc5e. If available, it performs model inference to predict the best plan hints and provides the hints to the optimizer only if the associated confidence score is higher than a threshold. Otherwise, it falls back to the built-in optimizer to produce a plan.\nChanging environments and workloads. In our current implementation, Kepler assumes a fixed system state, including database configuration, optimizer implementation, and data distribution. If any of these aspects changes relatively slowly or infrequently, Kepler can periodically collect new execution data and retrain purely on data from the new system state. We posit that in the majority of production parameterized query use cases, (1) the database is reconfigured infrequently, and (2) the data distribution drifts slowly, e.g. in scenarios in which a relatively small amount of similarly-distributed data is added each day. Additionally, Kepler is designed to be robust to dynamic workloads in which query parameter binding values change by detecting when inputs are out of its training distribution (see Section 7.4).\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nLimitations. The target usage of Kepler is for parameterized queries that are executed frequently enough to justify the training data collection cost. As discussed in Section 8, the exact training data collection regime in this paper serves the dual purposes of definitively demonstrating the speedups available and enabling further research in efficiency. We anticipate a final production system will use an iteration of this research with leaner training data collection. The cost-benefit analysis of using Kepler is situation-dependent; ultimately the user must weigh the potential query performance gains against the cost. If ephemeral instances are used for training data collection, Kepler assumes they are representative of production query performance."
        },
        {
            "heading": "3.3 Kepler Design Choices",
            "text": "In this section, we further discuss the specific design choices made to ensure that Kepler can be reliably deployed with minimal production overhead.\nUsing actual execution latencies. Since the objective of Kepler is to reduce actual end-to-end query latencies, it necessitates executing queries on a real database to provide ground-truth signal. To minimize the training collection time and avoid load on the production system, the DBMS may spawn ephemeral instances to speed up and isolate the training execution process.\nLimiting reliance on cost models. By collecting actual execution latencies, Kepler eschews explicitly relying on optimizer cost estimates for determining the quality of a plan. Figure 3 shows the estimated vs. exact cardinalities of all joins on a sample of query instances from Stack [25]. In particular, 64% of points have estimated cardinality = 1, likely due to the independence assumption of the PostgreSQL optimizer.\nFalling back to the built-in optimizer. Kepler avoids regressions by falling back to the existing query optimizer when it is not confident in identifying the optimal plan. Given the low overhead of model inference, the overall Kepler inference cost is nearly always lower than that of Opt-Always. For cases where planning time is a concern due to high fallback frequency, one can incorporate an additional model designed to predict a safe plan without re-invoking the optimizer.\nIndependence of query templates. Kepler handles query templates independently, i.e. each query template will generate its own candidate plans, collect its own training data, and train a model specific to that template. Though potentially more expensive than a procedure that generalizes over multiple query templates, this design has the advantages of 1) providing a more tractable\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nML problem, and 2) isolating each query from regressions caused by changes pertaining to other queries as models iterate over time and new query templates are on-boarded. Leveraging shared information between query templates while not increasing the risk of regressions is an interesting direction for future work."
        },
        {
            "heading": "4 ROW COUNT EVOLUTION",
            "text": "The goal of the candidate generation stage is to construct a set of plans \ud835\udc43 such that it contains a near-optimal plan for every query instance \ud835\udc5e in the workload distributionW. Additionally, \ud835\udc43 should be sufficiently small such that it is feasible to execute each plan for training dataset collection. Balancing these two competing objectives is the main challenge for any candidate generation algorithm. In this work, we only consider generating fully-specified plans, i.e. the join order and every join/scan method are defined. Alternatively, a candidate generation algorithm could specify a subset of the plan decisions and allow the query optimizer to determine the remainder.\nWorkload candidate generation. Given an algorithm \ud835\udc34 for generating a candidate plan set over a single query instance \ud835\udc5e, we define the corresponding plan set over a workload\ud835\udc4a as the union of the per-instance plan sets \ud835\udc34(\ud835\udc4a ) := \u22c3\ud835\udc5e\u2208\ud835\udc4a \ud835\udc34(\ud835\udc5e) (see lines 1-5, Algorithm 1). We also define plan sharing to describe the case where \ud835\udc5d\ud835\udc43opt (\ud835\udc5e) is generated from some other query instance \ud835\udc5e\u2032 (i.e. \ud835\udc5d\ud835\udc43opt (\ud835\udc5e) \u2209 \ud835\udc34(\ud835\udc5e), \ud835\udc5d\ud835\udc43opt (\ud835\udc5e) \u2208 \ud835\udc34(\ud835\udc5e\u2032)).\nOur approach. We propose Row Count Evolution (RCE)3, a computationally-efficient algorithm that generates new plans by randomly perturbing the optimizer\u2019s cardinality estimates. RCE is predicated on the idea that cardinality misestimates are the primary underlying reason for optimizer suboptimality. RCE exploits the fact that our candidate generation stage only needs to generate a set of plans that contains a (near-)optimal plan instead of directly identifying a single performant plan. Like Bao [25], RCE leverages the built-in query optimizer to generate candidate plans, but does so in a more fine-grained and efficient way. We instantiate the idea of applying random perturbations as an evolutionary-style algorithm, described in Algorithm 1. RCE maintains a sequence of generations of plans, with the initial generation consisting solely of the query optimizer\u2019s plan. To construct subsequent generations, RCE first uniformly samples parent plans from the previous generation. For each of these base plans, RCE perturbs the join cardinalities of only the sub-plans that appear in the parent plan by multiplicative factors sampled from an exponentially-spaced range (lines 21-28). By repeating this process multiple times and feeding in the resulting perturbations into the query optimizer, RCE generates a set of children plans (lines 14-18). Out of these, only unseen plans (i.e. those that did not appear in any prior generation) are kept for the next generation (lines 17-18). 3The name \"Row Count\" is inspired by the PostgreSQL extension pg_hint_plan\u2019s row count hints, which we use to modify the PostgreSQL optimizer\u2019s cardinality estimates.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nAlgorithm 1 Row Count Evolution. 1: functionWorkloadCandidateGeneration(workload\ud835\udc4a ) 2: \ud835\udc43 \u2190 {} 3: for query instance \ud835\udc5e \u2208\ud835\udc4a do 4: \ud835\udc43 \u2190 \ud835\udc43 \u222a RowCountEvolution(\ud835\udc5e) 5: return \ud835\udc43 6: 7: function RowCountEvolution(query instance \ud835\udc5e) 8: \ud835\udc5d0 = the base plan for \ud835\udc5e 9: \ud835\udc360 = {(\ud835\udc5d0, {}, {\ud835\udc60 \u2192 0\u2200 sub-plans \ud835\udc60})} 10: for generations \ud835\udc54 = 1, 2, . . . \ud835\udc3a do 11: Sample up to \ud835\udc46 base plans \ud835\udc35\ud835\udc54 uniformly from \ud835\udc36\ud835\udc54\u22121 12: \ud835\udc36\ud835\udc54 \u2190 {} 13: for (base plan \ud835\udc5d , row count map \ud835\udc5f ) \u2208 \ud835\udc35\ud835\udc54 do 14: for \ud835\udc56 = 1, 2, . . . , \ud835\udc41 do 15: \ud835\udc5f \u2032 \u2190 SamplePerturbations(\ud835\udc5d , \ud835\udc5f ) 16: \ud835\udc5d\u2032 \u2190 GetOptimizerPlan(\ud835\udc5f \u2032) 17: if \ud835\udc5d\u2032! = \ud835\udc5d then 18: \ud835\udc36\ud835\udc54.add((\ud835\udc5d\u2032, \ud835\udc5f \u2032)) 19: return \ud835\udc360 \u222a\ud835\udc361 \u222a . . . \u222a\ud835\udc36\ud835\udc3a 20: 21: function SamplePerturbations(plan \ud835\udc5d , row count map \ud835\udc5f ) 22: for sub-plan \ud835\udc60 \u2208 \ud835\udc5d do 23: \ud835\udc64 \u2190 \ud835\udc5d .getEstimatedCardinality(\ud835\udc60) 24: \ud835\udc52\ud835\udc59 \u2190 \u2212min(log\ud835\udc4f (\ud835\udc64),\ud835\udc5a) 25: \ud835\udc52\ud835\udc62 \u2190 \ud835\udc52\ud835\udc59 + 2\ud835\udc5a 26: Sample \ud835\udc53 uniformly from [\ud835\udc4f\ud835\udc52\ud835\udc59 , . . . , \ud835\udc4f\ud835\udc52\ud835\udc62 ] 27: \ud835\udc5f [\ud835\udc60] \u2190 \ud835\udc64 \u00b7 \ud835\udc53 28: return \ud835\udc5f\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nExample. Figure 4 shows an example of RCE generating candidate plans for a query instance with two generations. The base plan joins the result of \ud835\udc34 Z \ud835\udc35 and \ud835\udc36 Z \ud835\udc37 with estimated |\ud835\udc34 Z \ud835\udc35 | = 40, |\ud835\udc36 Z \ud835\udc37 | = 17. RCE first constructs a set of candidate row counts for each sub-plan by perturbing their cardinalities by multiplicative factors. These candidate row counts for \ud835\udc34 Z \ud835\udc35 and \ud835\udc36 Z \ud835\udc37 are [4, 40, 400] and [1, 17, 170], respectively, using a base of 10 and a range of 1. RCE then uniformly samples new join cardinalities from these sets; one sample of 400 and 17 influences the optimizer to produce a new plan Plan-1 in generation 1 1 . It repeats the same process N times to produce \ud835\udc361 plans in generation 1. Next, RCE samples \ud835\udc46 from a deduplicated set of plans from generation 1 2 and randomly perturbs the row counts on each sampled plan \ud835\udc41 times to generate \ud835\udc362 plans in generation 2 3 .\nRCE as exact cardinality matching. One interpretation of RCE is that it efficiently builds a covering set of exact-cardinality plans. The RCE-generated candidate set contains plans generated from a diverse range of perturbed sub-plan cardinalities. If there are sufficiently many perturbations, likely at least one will be reasonably close to the exact cardinalities for any particular query instance and their respective induced plans will also likely be similar.\nMultiplicative perturbations. Applying multiplicative perturbations is well-motivated by the standard metric of Q-error in cardinality estimation. RCE further uses an exponentially-spaced perturbation set in order to have a similar support as the optimizer\u2019s Q-error distribution.\nPerturbing only relevant sub-plans. Instead of perturbing all 2\ud835\udc5b \u2212 1 sub-plans (for a query joining \ud835\udc5b tables), RCE only perturbs the cardinalities of the \ud835\udc5b \u2212 1 sub-plans that actually appear in the sampled query plans. This significantly increases the efficiency of RCE with only a small loss of generality: since the set of perturbations is inherited between generations, a misestimated sub-plan cardinality will only never be perturbed if its cardinality is significantly overestimated by the query optimizer. However, this is an unlikely scenario since query optimizers tend to underestimate sub-plan cardinalities due to the independence assumption.\nThis re-optimization of only the sub-plan cardinalities that appear in the optimizer plan bears a strong resemblance to the re-optimization procedure in [36], which iteratively re-optimizes using sampling-based cardinality estimates. The key differences in our setting are (1) we do not have to return a single plan, and (2) we require a fast procedure since we repeat it for each query instance, motivating the use of perturbations over sampling.\nRCE as local search. RCE effectively explores the plan space via a random walk in the lowdimensional subspace of sub-plan cardinalities, initialized at the optimizer\u2019s cardinality estimates. This formulation implicitly leverages the fact that while these initial estimates are typically incorrect, they are still more informative than random estimates.\nRCE hyperparameters. Our implementation of RCE includes a variety of hyperparameters that allow one to flexibly trade off the number of generated plans against the potential total speedup (Table 1). \u2022 Width and depth of the perturbation tree. Increasing the number of generations \ud835\udc3a increases the number of plans, making it more likely a good plan is found. However, plans in later generations are perturbed further from the original plan, and may have a lower likelihood of being relevant. To ensure constant-time processing for each generation, we sample (up to) a fixed number \ud835\udc46 of base plans in each generation, and perturb each one \ud835\udc41 times. \u2022 Perturbation values. The exponent base \ud835\udc4f and range\ud835\udc5a limits the magnitude of a single perturbation. We also introduce a sub-plan perturbation limit that controls the number of\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\ntimes a specific sub-plan can be perturbed, effectively controlling the total perturbation range of any given sub-plan. \u2022 Direct limits on number of plans. We implement limits on the number of plans that can be generated from a single parameter and in total. Once the limit is reached, the evolutionary candidate generation process is terminated and only default plans are kept for remaining parameters. The total plans limit is a soft limit since the final evolutionary iteration may produce up to the single-parameter limit and the remaining parameters may contribute new default plans."
        },
        {
            "heading": "5 TRAINING DATA COLLECTION",
            "text": "After generating candidate plan set \ud835\udc43 , we execute each plan over a training workload to generate a dataset of execution latencies for supervised best-plan prediction. The training workload may be provided by the user or captured in a DBMS query log [34]. Rather than executing all candidate plans for each query instance, we use adaptive timeouts and construct near-optimal plan covers to prune suboptimal plans.\nExecutionmechanics. We force the optimizer to produce a candidate plan by providing all join/scan methods and the join order via hints. We parallelize the execution of query instances and their candidate plans in multiple databases. We simulate a warm buffer cache scenario by executing each plan multiple times and taking the minimum as the estimated latency [22]. This repeated execution strategy also reduces the potential noise in our execution time measurements; though we observed the amount of noise to be inconsequential in our experimental setup. We leave a full analysis of query execution time under different caching, concurrency, and resource availability settings to future work.\nAdaptive timeouts and plan execution reordering. We use a timeout policy to minimize wasted resources on executing sub-optimal candidate plans. The timeout policy adapts from [37] with two main modifications. First, we always execute the default plan first and adaptively reorder the remaining plans to maximize the impact of the timeout\u2019s progressive tightening on a per queryinstance basis. We execute plans in ascending order of their historical execution latencies across query instances as a simple heuristic for tightening the timeout as quickly as possible. Second, we do not apply the tightened timeout for the first iteration of each plan in order to ensure that each plan simulates a warm-cache scenario.\nOnline plan cover pruning. We also use an online plan pruning technique to eliminate plans based on actual execution time. Specifically, we initially execute all plans for the first \ud835\udc41 query instances of a query template, then use a Set Cover formulation to prune down to a minimal plan cover set for the remaining query instances. The pruned set becomes our \ud835\udc58 candidate plans for the query template, i.e. our models only attempt to predict from those plans. We consider a plan to be near-optimal for a query instance \ud835\udc5e if its execution time is within a 1 + \ud835\udf16 factor of the fastest time for \ud835\udc5e we have seen so far. Each plan has an associated set of query instances for which it is near-optimal. The plan cover is the smallest set of plans such that each query instance has a near-optimal plan in the set. We construct the plan cover using the standard greedy approximation for Set Cover, which iteratively picks the plan that is near-optimal for the most remaining query instances. We additionally relax the problem to require that only 1 \u2212 \ud835\udeff of all query instances be covered, allowing us to trade off the plan cover size and the achievable speedup.\nTail latency reordering. For many query templates, the distribution of default execution latencies is heavy-tailed. Parameters in the tail tend to be more sub-optimal, and therefore have an outsized impact on the total speedup. To ensure the plan cover computation includes these parameters, we\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nevaluate these query instances first. This reordering produces a 7-8x reduction in total execution time and number of plans over the entire Stack dataset."
        },
        {
            "heading": "6 ROBUST BEST-PLAN PREDICTION",
            "text": "After collecting a full training dataset of actual execution latencies over our candidate plan set, we use supervised ML to predict the best plan for any query instance. Kepler trains one model for each query template with the objective to maximize workload speedup while minimizing regressions. Kepler also falls back to the optimizer\u2019s plan when its predicted confidence is low. Section 7 shows that the inference time of our model is negligible compared to the typical query planning time of a classical optimizer."
        },
        {
            "heading": "6.1 Features",
            "text": "Given a template\ud835\udc44 with\ud835\udc5a parameters, Kepler uses solely the\ud835\udc5a parameter values as input features. The supported types include numerics (float/int), strings, and dates/timestamps. We apply standard preprocessing techniques to each type: embeddings for strings/low-dimensional integer features, normalization to \ud835\udc41 (0, 1) for numerics, and numeric conversion for date/time features. We do not convert the parameter values to their respective base table selectivities as in [34] for the following reasons. First, selectivity is inherently a lossy representation and may obscure information when two distinct values have the same selectivity. Second, selectivity is inferior when the optimizer\u2019s cardinality estimation is sub-optimal, see Figure 3.\nString columns and vocabulary selection. For each string-valued column, we construct a fixed-size vocabulary in order to limit model size. String features are one-hot encoded via a lookup table, with buckets for out-of-vocabulary values. An embedding layer is then applied on this one-hot encoding, creating a learnable embedding for each value in the vocabulary. We choose the vocabulary as the top-\ud835\udc58 values ordered by the total possible improvement of all query instances with that value. We define the max marginal improvement strategy as selecting the top-\ud835\udc58 column values \ud835\udc63 in column \ud835\udc56 under the following objective:\n\ud835\udc5a(\ud835\udc63, \ud835\udc56) = \u2211\ufe01\n\ud835\udc5e\u2208\ud835\udc4a,\ud835\udc65\ud835\udc56=\ud835\udc63 \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5ddefault, \ud835\udc5e) \u2212 \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d\ud835\udc43opt, \ud835\udc5e) (3)\nOur evaluation shows that this strategy is effective. For columns with significantly more distinct values, one may factorize embeddings over subcolumns [38]."
        },
        {
            "heading": "6.2 Training Objectives",
            "text": "Kepler models maximize the model speedup defined in Equation 2 while minimizing the number of regressions. This objective is not directly differentiable, so we discuss various surrogate learning objectives.\nMulti-label classification. We model best-plan prediction as a multi-label classification problem in which each near-optimal plan has a positive label (as opposed to just the optimal plan) [33]. The multi-label objective also provides a richer supervised signal, improving the quality of the learned intermediate representations. We use the single-label transformation of multi-label classification loss by training the near-optimal probability of each candidate plan with binary cross-entropy loss. Although our models only predict plans in the plan cover, which may not necessarily contain every query instances\u2019 default plan, our definition of near-optimality does exploit the availability of default plan execution data during training. We define a plan to be near-optimal if its estimated latency improvement is within a 1 + \ud835\udf0f factor of the optimal improvement latency. Namely, we say a plan \ud835\udc5d is near-optimal if (\u2113\ud835\udc51 \u2212 \u2113\ud835\udc5d ) (1 + \ud835\udf0f) \u2265 (\u2113\ud835\udc51 \u2212 \u2113\ud835\udc5c ), where \ud835\udf0f > 0, \u2113\ud835\udc51 = \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5ddefault, \ud835\udc5e), \u2113\ud835\udc5d =\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\n\ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d, \ud835\udc5e), and \u2113\ud835\udc5c = min\ud835\udc5d\u2208\ud835\udc43 \ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5d, \ud835\udc5e). Computing near-optimality requires execution times for all query instances for all plans.\nPrior work formulate best-plan prediction as a regression [6, 27, 34] and multi-class classification problem [34]. Both formulations are unsatisfactory for a variety of reasons. Regression across a significant range can be unstable, a problem that is exacerbated by our timeout procedure, which obscures the true latency of suboptimal plans. Regression attempts a more challenging problem with finer granularity than required, imposing unnecessary constraints and objectives on the training. We only need to predict the identity of the optimal plan rather than its execution time. Inversions and gross over-estimates of non-optimal plans are acceptable to us but will weigh heavily in regression loss. Meanwhile, classification objectives that predict a single optimal plan perform poorly in scenarios when multiple plans can be near-optimal and empirical execution latencies can be subject to noise. For example, consider a problem where plans \ud835\udc5d1, \ud835\udc5d2 execution latencies\u2019 are both drawn from \ud835\udc36 + \ud835\udc41 (0, 1) for some large \ud835\udc36 . Then a multi-class classifier will have equal predicted likelihood for \ud835\udc5d1 and \ud835\udc5d2 and thus have low confidence, when in actuality being confident in \ud835\udc5d1 and/or \ud835\udc5d2 is desirable.\nExample-dependent loss. Different query instances may have disproportionate impact on the overall objective Equation 2. We leverage the standard sample-weighting approach example-dependent cross entropy [8, 16] to prioritize those with the largest improvement delta. For plans worse than the default plan, we upweight them by a factor \ud835\udc36 . For all near-optimal plans, we apply a soft weighting based on their empirical execution improvement, i.e. 1 + \ud835\udc37 log(\u2113\ud835\udc51 \u2212 \u2113\ud835\udc5d ), where \ud835\udc36 and \ud835\udc37 are both tunable hyperparameters."
        },
        {
            "heading": "6.3 Models",
            "text": "We use simple feedforward neural networks as our base models. For inference efficiency, we consider a neural network with one output head per plan on top of a shared representation, which improves inference speed and model size over approaches that have separate models for each plan [34]. We train our neural network models with standard minibatch SGD. In a real-world setting, the model\u2019s hyperparameters can be tuned via simple search techniques or more sophisticated algorithms by partitioning the training data into a train and validation set.\nUncertainty. Kepler models incorporate calibrated predictions and uncertainty estimates to avoid predicting significantly suboptimal plans. Two state-of-the-art approaches for incorporating uncertainty into neural networks are ensembling and Spectral-normalized Neural Gaussian Processes (SNGPs) [23]. The former trains\ud835\udc40 distinct models simultaneously and estimates the uncertainty from their joint outputs. The latter applies spectral normalization to all layers, providing a biLipschitz guarantee on all intermediate representations, and uses a Gaussian process output layer to efficiently estimate the uncertainty. Since ensembling increases the training and inference cost by a linear factor\ud835\udc40 , Kepler uses the SNGP approach due to its lower overhead."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "Our evaluation of Kepler seeks to demonstrate that it robustly achieves state-of-the-art execution latency speedups on parameterized query workloads. We summarize our main results as follows: \u2022 An end-to-end implementation of Kepler on PostgreSQL substantially outperforms the builtin optimizer and Bao. Both RCE and ML models play large roles in achieving this speedup. (Section 7.2) \u2022 RCE discovers significantly better plans than existing candidate generation baselines. We also observe that RCE plans are frequently superior to exact-cardinality plans. (Section 7.3)\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\n\u2022 Using SNGP models is crucial to capturing speedups generated by RCE while minimizing query regressions. (Section 7.4). \u2022 We release a dataset consisting of \u223c14.2 years of query executions as a benchmark for future research in modeling approaches (Section 7.5).\nObjectives. To evaluate our methods, we use both RCE speedup \ud835\udc46opt (\ud835\udc45\ud835\udc36\ud835\udc38) (shortened as \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 ) and model speedup \ud835\udc46model, defined in Equations 1 and 2 respectively. We note that \ud835\udc46model = \ud835\udc5d \u00b7 \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 , where 0 \u2264 \ud835\udc5d \u2264 1 corresponds to the proportion of the speedup the model captures. Since the model may predict worse plans than the built-in optimizer, we also measure the query regression frequency \ud835\udc43reg, defined as the proportion of test query instances the model does at least 10% worse than the default optimizer on. The primary metrics for each of our components are: (1) End-to-end performance: \ud835\udc46model (2) Candidate generation performance: \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 (3) Model performance: \ud835\udc5d , \ud835\udc43reg\nKepler aims to maximize \ud835\udc46model by maximizing \ud835\udc5d and \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 , while minimizng \ud835\udc43reg. We also report the 99th-percentile tail latency speedup, which may be relevant in applied scenarios. We define this as \ud835\udc43method99 (\ud835\udc4a ) = \ud835\udc5d99 ({\ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5ddefault,\ud835\udc5e)\u2200\ud835\udc5e\u2208\ud835\udc4a }) \ud835\udc5d99 ({\ud835\udc38\ud835\udc65\ud835\udc52\ud835\udc50\ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52 (\ud835\udc5dmethod,\ud835\udc5e)\u2200\ud835\udc5e\u2208\ud835\udc4a }) , where \ud835\udc5d99 (\ud835\udc36) denotes the 99th percentile highest value in a collection \ud835\udc36 ."
        },
        {
            "heading": "7.1 Setup",
            "text": "Datasets and query extraction. We use two synthetic benchmarks: TPC-H (uniform and skewed with Zipf factor = 1, 10 GB [4]), and Stack, a database consisting of real-world StackExchange data [25]. TPC-H consists of 22 parameterized queries. We use an augmented version of Stack with 87 parameterized queries: 42 from the original benchmark and 45 additional manually-written query templates.\nAll experiments were run using PostgreSQL 13.5 on Google Cloud Platform (GCP) n1-highmem-16 instances with 16 CPU cores, 108 GB of RAM, and 2 TB of SSD. Following [22], we set shared_buffers to 75 GB, effective_cache_size to 80 GB, and work_mem to 4 GB to ensure that the entire dataset fits in memory. For TPC-H, we use the indexes defined in BenchBase [11]. For Stack, we add indexes on all primary keys, foreign keys, and columns that appear in a predicate of any query.\nQuery instance generation. We follow the official TPC-H specification [5] to generate parameter values of each query template. For Stack, we synthetically generate parameter values so that every query instance returns nonempty results. This is accomplished by uniformly sampling rows from the result set of a derived query that selects column values for which parameterized predicates would produce at least one value. Range predicates are constructed by first sampling a single value in the manner, then sampling lower/upper bounds around this value.\nTraining query execution. For each query instance and plan hints, we execute the resulting plan three times to simulate a warm-cache scenario, and take the minimum latency as the ground truth. For slow queries, we executed each up to 8 times in parallel on the same machine, and observed negligible differences with the serial execution setting. We leave a full analysis of different execution scenarios to future work.\nRCE hyperparameters. Unless stated otherwise, we use the same values for all RCE hyperparameters in all of our experiments, demonstrating its efficacy even when untuned for specific benchmarks. We set the number of generations \ud835\udc3a to 3, the exponent base \ud835\udc4f to 10, the exponent range\ud835\udc5a to 2, the number of perturbations per plan \ud835\udc41 to 20, and the number of samples extracted\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\n\ud835\udc46model % of queries >1.2 64.4% >2x 32.2% >10x 14.9% >20x 4.6%\n(a) Summary of PostgreSQL Kepler speedups on Stack.\nfrom each generation \ud835\udc46 to 20. For each query template, we run RCE on the first 50000 query instances for Stack, and all query instances for TPC-H.\nModel details. All of our experiments use a fixed base neural network with three layers of 64 hidden units each. We use Adam with learning rate 3e-4, ReLU activation functions, and 10- dimensional string embeddings. For SNGP models, we additionally apply spectral normalization to all dense layers, and replace the output dense layer with a random Fourier feature Gaussian Process with 128 random features. For all models, we fall back to the default plan if the predicted confidence is less than 0.9. For all queries, we use a 80/20 train/test split, and report results (speedups, regressions) on the test workload.We did not attempt to tune our models or performmodel selection, although it is straightforward to do so by reserving a validation set from the training dataset.\n7.2 Kepler ImprovesQuery Execution Latency\nEnd-to-end performance. We integrate Kepler into the PostgreSQL query optimizer to demonstrate that it delivers large speedups in a real deployment. Our implementation loads Tensorflow Lite models on the database server for fast CPU model inference and uses the pg_hint_plan extension to force specific plans via hints. Providing the query id as a comment with the SQL query text from any PostgresSQL client connection triggers Kepler query plan prediction.\nWe executed a sample of 1000 evaluation set query instances per query on the integrated Kepler PostgresSQL system. Table 5a summarizes the speedups of Kepler over Stack, demonstrating that our PostgreSQL implementation achieves nontrivial speedups on the majority of queries, with over 2x speedup over the entire workload for 32.2% of queries. These speedups indicate that Kepler is able to bypass inaccuracies PostgreSQL\u2019s cardinality estimation and cost model via RCE.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nFigure 5b confirms that the Kepler deployment achieves near-identical speedups to those expected based on the pre-collected execution dataset. This is because the use of lightweight ML models and planning hints incur low planning-time overhead. Figure 5c shows the distribution of the ratio of model inference times to PostgreSQL planning time for all queries in Stack. The model inference time is mostly under 5% of PostgreSQL planning time and at most 30%.\nOur total speedup results over entire workloads are quite significant since our workloads \u2013 query instances sampled uniformly from the space of non-empty query instances \u2013 are not designed to adversarially challenge the optimizer. Next, we summarize the contributions from the two key components: (1) RCE to uncover the potential speedups and (2) the ML models to capture speedups. Finally, we compare the results to Bao as a baseline.\nRCE speedups. We illustrate the efficacy of RCE by showing that it achieves large speedups on both Stack and TPC-H. Figure 6 shows the per-template \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 and \ud835\udc43\ud835\udc45\ud835\udc36\ud835\udc3899 , with RCE achieving over 2x speedup on 32/87 queries and over 1.2x speedup on 78/87 queries. Similarly, RCE improves 6/22 queries on TPC-H uniform (Figure 7a) and 9/22 queries on TPCH skewed (Figure 7b). In particular, RCE finds larger speedups on TPC-H skewed due to the non-uniformity in its data distribution.\nML models predict fastest plans and avoid regressions. Next, we show that our ML models are able to robustly capture the speedup produced by RCE, i.e. they maximize \ud835\udc5d while minimizing \ud835\udc43reg. In Figures 8a and 8b, we plot what proportion of \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 and \ud835\udc43\ud835\udc45\ud835\udc36\ud835\udc3899 on Stack we respectively capture per query. These distributions show that our models can reliably predict near-optimal plans: our models capture over 80% of \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 on over 80% of Stack queries. In Figure 9, we plot the distribution\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nof the absolute magnitude of model improvements and regressions compared to the default plan, illustrating that the frequency and magnitude of the regressions are minimal compared to those of the improvements.\nBao on parameterized queries. We evaluate Bao, one of the few prior approaches that demonstrates actual improvement in execution latency, on our parameterized version of Stack [25]. For illustrative purposes, we run Bao for 2000 query instances on six representative templates from the original Stack dataset [25]. As shown in Table 2, we observed that Kepler outperforms Bao on 5 out of 6 templates, and in particular is able to find far greater speedups on q15_0 and q16_0. As we later show in Figure 10a, this is because the candidate generation algorithm in Bao is severely suboptimal.\nTraining data collection cost. The speedups achieved by Kepler come at a nontrivial training query execution cost - on average, we used 39 CPU days worth of query execution time per query template. Hence, Kepler is most applicable to workloads where query templates are executed at high frequency. We discuss directions to significantly reduce this training data collection cost at the end of Section 7.4 as well as Sections 7.5 and 8."
        },
        {
            "heading": "7.3 Analyzing RCE",
            "text": "Having observed the behavior of the overall system, we now discuss characteristics of the first major component of Kepler: RCE. We evaluate RCE\u2019s performance against candidate generation and plan pruning baselines before exploring the effects of index configuration and key hyperparameter choices. After discussing RCE\u2019s empirical performance against those of exact cardinality (EC) plans, we close the section by offering perspectives and empirical justifications for why RCE works well.\nComparison against baselines. We compare against two main candidate generation baselines: \u2022 PG: the default candidate generation method (Section 2) using the PostgreSQL optimizer.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023."
        },
        {
            "heading": "1 9 6 1.369",
            "text": ""
        },
        {
            "heading": "2 29 7 2.185",
            "text": ""
        },
        {
            "heading": "3 62 9 2.235",
            "text": ""
        },
        {
            "heading": "4 99 8 2.166",
            "text": "Since [34] consider PG + cost-based pruning (CBP) as their candidate generation method, we simultaneously compare RCE against PG and our plan-cover pruning (PCP) algorithm against CBP in Table 10b. RCE finds significantly more speedups than PG: \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 is over 1.2 on 89% of Stack queries, as opposed to 49.4% for PG. Although CBP achieves little speedup loss when applied to PG, it incurs far greater loss when applied to RCE, indicating that optimizer cost estimates cannot reliably predict the quality of RCE plans. By contrast, PCP achieves almost no degradation in speedup since it uses actual execution data to evaluate plans.\nThe Bao candidate generation method produces far more plans than RCE, so for computational reasons we evaluated it on a diverse subset of queries designed to be representative of the entire Stack dataset. Figure 10a shows that RCE achieves similar or better speedup on all queries, and is notably able to find non-trivial speedup on each query.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nNumber of generations \ud835\udc3a . The hyperparameter \ud835\udc3a has a significant impact on the number and quality of RCE-generated plans, which we illustrate by varying \ud835\udc3a for q11_0 in Stack, with results shown in Table 3. The number of plans and plan cover size steadily grows up to three generations, although most of the speedup is captured in plans found using two generations. Increasing \ud835\udc3a to four generations does not produce any marginal benefit.\nExponential row count perturbation range: \ud835\udc4f and\ud835\udc5a. To justify our choice of perturbation range hyperparameters \ud835\udc4f = 10 and\ud835\udc5a = 2, we analyzed the perturbation factor necessary to induce a change at a single level in the plan. For each of \ud835\udc5b \u2212 1 sub-plans in a query tree (for a query \ud835\udc44 with \ud835\udc5b tables), we use binary search to identify the factor by which the cardinality estimate for that sub-plan must be perturbed in order to induce a change in the optimizer\u2019s plan. On Stack, we observed that although some plans needed a 106 perturbation factor to induce a plan change, the vast majority of factors were less than 102.\nRobustness to index configuration. RCE finds better plans on databases with different index configurations. We ran RCE over the following index configurations for Stack: primary keys only (PK), foreign keys (FK), predicate columns, and database administrator defined additional indexes [25]. Table 4 shows RCE finds faster plans in all configurations. Similar to [22], we find that more indexes leads to a larger speedup.\nCan RCE discover optimal plans? Although RCE demonstrably generates faster plans than a variety of baselines, we would also like to know how close are RCE-generated plans to the true optimal plans \ud835\udc5d\u2217opt (\ud835\udc5e). Since it is infeasible to determine \ud835\udc5d\u2217opt (\ud835\udc5e) in practice, we instead compare against the standard benchmark of exact cardinality plans [22, 29]. We executed exact cardinality plans for a subset of the training workload and compared them against their respective best RCE plans in Table 5. We again used a subset of queries from the Stack dataset for computational reasons. Due to the large number of joins in some query templates, we additionally set the exact cardinality for a subset of tables to be a high constant if its corresponding query did not finish within 15 minutes. Notably, RCE plans are substantially faster than exact cardinality plans on 5 out of 6 queries. This illustrates that even accurate cardinality estimation methods can lead to suboptimal plans due to incorrect assumptions and other deficiencies in the cost model.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nQuery clusters facilitate RCE.. Query instances with similar parameter binding values often have similar query plan behavior, e.g. as visualized by plan diagrams [15]. We hypothesize that these groups of similar instances, or clusters, can dramatically increase the efficacy of RCE via plan sharing. Recall that in our candidate generation procedure, we execute RCE for each query instance and take the union over all resulting plan sets. Hence, each cluster only requires a single query instance\u2019s RCE process to reach the cluster-wide optimal plan. For a cluster of size \ud835\udc41 and probability P(\ud835\udc5e\ud835\udc56 ) of query instance \ud835\udc5e\ud835\udc56 discovering the optimal plan, the overall probability of discovering the plan over the cluster is 1 \u2212\u220f\ud835\udc41\ud835\udc56=1 (1 \u2212 P(\ud835\udc5e\ud835\udc56 )), which rapidly approaches 1 for sufficiently large \ud835\udc41 and a reasonable distribution of P(\ud835\udc5e\ud835\udc56 ). To demonstrate the existence and impact of clusters, we define a cluster for each plan \ud835\udc5d as all query instances for which \ud835\udc5d is the fastest plan. Then, for each query instance \ud835\udc5e, we compare the execution latencies of the fastest plan from three plan sets: (1) RCE-all, containing all plans from all query instances, (2) RCE-cluster, containing all plans from query instances in the same cluster as \ud835\udc5e, and (3) RCE-instance, containing only the plans generated from the RCE process for \ud835\udc5e. As shown in Table 6, RCE-all and RCE-cluster have very similar execution times, while RCE-instance is often slower, indicating that intra-cluster plan sharing plays a large role in the efficacy of RCE."
        },
        {
            "heading": "7.4 ML Models",
            "text": "We first motivate the use of ML models by demonstrating that Stack is highly parameter sensitive \u2013 i.e. different query instances have different optimal plans. We then justify modeling design choices with an ablation of using SNGP in our models and evaluation of varying confidence thresholds highlight the importance of incorporating robustness as a primary design component in Kepler. We conclude with extensive analyses around feature space selection, embedding vocabularies, and training data size.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nParameter sensitivity. We investigate the parameter sensitivity of Stack and TPC-H queries based on our execution data. On Stack, all but four query templates had plan cover size greater than one (Figure 11a). In Figure 11b, we show the distribution of single-best plan suboptimality ratios, defined as the ratio of total latency of the oracle best plan against the total latency of the single best plan (i.e., the fixed plan with minimum total execution time). Ratios less than 1 indicate that using only a single plan incurs a loss in speedup, with lower values being more severely suboptimal. Thus, Figure 11b implies that multiple plans are necessary to capture the full speedup. On the other hand, TPC-H is designed to not be parameter sensitive, which we confirmed by observing a plan cover of size 1 for all queries. Hence, our modelling results focus solely on Stack.\nLoss functions/training objectives. Figure 12 compares various loss functions and models: SNGP with multilabel loss, SNGP with multiclass loss, and a vanilla NN with multilabel loss. Multilabel loss + SNGP achieves similar or better speedup to other methods, while having a lower regression frequency for all queries.\nModel calibration and uncertainty. We further investigate the ability of our SNGP models in producing calibrated output probabilities. In Figure 13, we plot the test workload model speedup and regression frequency as a function of confidence threshold varying from 0 (no falling back) to 1 (always falling back). The captured speedup and regression frequency both decay smoothly as a function of confidence, which allows the user to specify their tolerance for regressions. This\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nfigure also demonstrates the importance of the fallback mechanism: one can dramatically reduce the regression frequency while only sacrificing a small portion of the speedup.\nSNGP out-of-distribution detection. Although Kepler is designed for relatively static workloads, it is robust to dynamic workloads by falling back to the default plan for out-of-distribution (OOD) inputs. We evaluate SNGP\u2019s OOD detection ability by holding out specific slices of the training distribution for an example query, q21_2 on Stack. We consider two variants: (1) holding out sites totaling up to 20% of the workload, and (2) holding out the last 20% of last_activity_date values on the question table. Figure 14 shows that in both scenarios, Kepler\u2019s fallback mechanism allows it accurately detect OOD inputs and drastically reduce \ud835\udc43reg while still preserving some speedup.\nRaw parameter values vs selectivity features. In Figure 15a, we ablate our feature choice using raw features and selectivity features. We compare their model speedups to \ud835\udc46\ud835\udc45\ud835\udc36\ud835\udc38 on a subset of Stack queries. Selectivity features perform far worse due to the poor cardinality estimates in PostgreSQL.\nVocabulary. In Figure 15b, we ablate how we select the embedding vocabulary for string features. In particular, we evaluate our max marginal improvement method (described in Section 6) against choosing the most frequent values based on the PostgreSQL histogram. Our results confirm that the best strategy is choosing the vocabulary to the be the values with the most potential impact on the speedup.\nHow much training data is required? We evaluate the performance of our models when using less data by subsampling the training data size, as shown in Figure 16. As expected, model speedup improves with more training data while maintaining a regression frequency below 0.2%. The leftmost point uses only 5% of the data, or 200 training query instances, demonstrating that a large amount of speedup can be robustly captured with small amounts of training data.\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023."
        },
        {
            "heading": "7.5 Dataset Contribution",
            "text": "To train models and evaluate Kepler, we generated a query execution dataset that comprises \u223c14.2 years of CPU time across 200 million query executions of 131 query templates. By releasing this dataset online4, we envision that it can be used to facilitate future research in modeling and efficiency without having to execute any queries. For example, possible use cases include simulating active learning approaches that only selectively execute a subset of queries, or developing better models or loss functions. We believe that the techniques developed using this dataset \u2013 and not the specifically trained models \u2013 will be directly transferable to other parameterized query settings."
        },
        {
            "heading": "8 CONCLUSION AND FUTUREWORK",
            "text": "We introduced Kepler, a system that can robustly speed up parameterized queries using a learningbased approach.We extensively evaluated Kepler on PostgreSQL and demonstrated that (1) our novel candidate generation algorithm RCE can provide significant speedups in query execution latency, and (2) robust ML models can reliably predict faster plans while avoiding regressions. Interestingly, we observed that RCE-generated plans were often far better than exact cardinality plans, indicating that even a widely-used system as PostgreSQL has significant room for improvement. Evaluating Kepler on database platforms other than PostgreSQL is a natural next step; we believe that the empirical nature of Kepler allows it to discover performance gains regardless of the DBMS.\nThere are a myriad of future directions for improving the efficiency and performance of Kepler. For example, prior work in cardinality estimation can benefit Kepler in several ways. Instead of perturbing uniformly, RCE can leverage generative cardinality distributions to sample higher likelihood perturbations, e.g. from NeuroCard [38]. Another possibility is to augment the model features with the query plan tree and selectivity estimates, allowing the model to determine when cardinality estimates are accurate, as well as leveraging shared structure between similar query templates. Our models are the first demonstration that speedups can be robustly captured; they can likely be substantially improved via additional modeling techniques and tuning. Similarly, while our end-to-end PostgreSQL integration is sufficient to demonstrate Kepler\u2019s performance gains on a real system, every aspect of this implementation can be further tuned.\nWe utilized an expensive training data collection procedure in order to make more robust claims about our results and produce a complete dataset for further modeling and efficiency research. For practical purposes, our training procedure can likely be made much more efficient, e.g. via active\n4https://github.com/google/kepler\nProc. ACM Manag. Data, Vol. 1, No. 1, Article 109. Publication date: May 2023.\nlearning. In conjunction with Figure 16, this implies that similar performance can be achieved with significantly less training cost."
        }
    ],
    "title": "Kepler: Robust Learning for Faster ParametricQuery Optimization",
    "year": 2023
}