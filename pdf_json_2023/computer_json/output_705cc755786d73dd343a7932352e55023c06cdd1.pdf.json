{
    "abstractText": "Recent studies have shown that higher accuracy on ImageNet usually leads to better robustness against different corruptions. Therefore, in this paper, instead of following the traditional research paradigm that investigates new out-of-distribution corruptions or perturbations deep models may encounter, we conduct model debugging in indistribution data to explore which object attributes a model may be sensitive to. To achieve this goal, we create a toolkit for object editing with controls of backgrounds, sizes, positions, and directions, and create a rigorous benchmark named ImageNet-E(diting) for evaluating the image classifier robustness in terms of object attributes. With our ImageNet-E, we evaluate the performance of current deep learning models, including both convolutional neural networks and vision transformers. We find that most models are quite sensitive to attribute changes. A small change in the background can lead to an average of 9.23% drop \u2217 Corresponding author. This research is supported in part by the National Key Research and Development Progrem of China under Grant No.2020AAA0140000. on top-1 accuracy. We also evaluate some robust models including both adversarially trained models and other robust trained models and find that some models show worse robustness against attribute changes than vanilla models. Based on these findings, we discover ways to enhance attribute robustness with preprocessing, architecture designs, and training strategies. We hope this work can provide some insights to the community and open up a new avenue for research in robust computer vision. The code and dataset are available at https://github.com/",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiaodan Li"
        },
        {
            "affiliations": [],
            "name": "Yuefeng Chen"
        },
        {
            "affiliations": [],
            "name": "Yao Zhu"
        },
        {
            "affiliations": [],
            "name": "Shuhui Wang"
        },
        {
            "affiliations": [],
            "name": "Rong Zhang"
        },
        {
            "affiliations": [],
            "name": "Hui Xue"
        }
    ],
    "id": "SP:17b341864d6aa910ea0ed16237faa67c26221bce",
    "references": [
        {
            "authors": [
                "Omri Avrahami",
                "Dani Lischinski",
                "Ohad Fried"
            ],
            "title": "Blended diffusion for text-driven editing of natural images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tao Bai",
                "Jinqi Luo",
                "Jun Zhao",
                "Bihan Wen",
                "Qian Wang"
            ],
            "title": "Recent advances in adversarial training for adversarial robustness",
            "venue": "arXiv preprint arXiv:2102.01356,",
            "year": 2021
        },
        {
            "authors": [
                "Yutong Bai",
                "Jieru Mei",
                "Alan L Yuille",
                "Cihang Xie"
            ],
            "title": "Are transformers more robust than cnns",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Beit: Bert pre-training of image transformers",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Andrei Barbu",
                "David Mayo",
                "Julian Alverio",
                "William Luo",
                "Christopher Wang",
                "Dan Gutfreund",
                "Josh Tenenbaum",
                "Boris Katz"
            ],
            "title": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Adversarial examples are not easily detected: Bypassing ten detection methods",
            "venue": "In Proceedings of the 10th ACM workshop on artificial intelligence and security,",
            "year": 2017
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Peng Cui",
                "Susan Athey"
            ],
            "title": "Stable learning establishes some common ground between causal inference and machine learning",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Noam Eshed"
            ],
            "title": "Novelty detection and analysis in convolutional neural networks",
            "venue": "Master\u2019s thesis, Cornell University,",
            "year": 2020
        },
        {
            "authors": [
                "Shanghua Gao",
                "Zhong-Yu Li",
                "Ming-Hsuan Yang",
                "Ming- Ming Cheng",
                "Junwei Han",
                "Philip Torr"
            ],
            "title": "Large-scale unsupervised semantic segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Robert Geirhos",
                "J\u00f6rn-Henrik Jacobsen",
                "Claudio Michaelis",
                "Richard Zemel",
                "Wieland Brendel",
                "Matthias Bethge",
                "Felix A Wichmann"
            ],
            "title": "Shortcut learning in deep neural networks",
            "venue": "Nature Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Robert Geirhos",
                "Patricia Rubisch",
                "Claudio Michaelis",
                "Matthias Bethge",
                "Felix A Wichmann",
                "Wieland Brendel"
            ],
            "title": "Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness",
            "venue": "arXiv preprint arXiv:1811.12231,",
            "year": 2018
        },
        {
            "authors": [
                "Charles J Geyer"
            ],
            "title": "Practical markov chain monte carlo",
            "venue": "Statistical science,",
            "year": 1992
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Robert M Haralick",
                "Karthikeyan Shanmugam",
                "Its\u2019 Hak Dinstein"
            ],
            "title": "Textural features for image classification",
            "venue": "IEEE Transactions on systems, man, and cybernetics,",
            "year": 1973
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "venue": "Proceedings of the International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Norman Mu",
                "Ekin D. Cubuk",
                "Barret Zoph",
                "Justin Gilmer",
                "Balaji Lakshminarayanan"
            ],
            "title": "AugMix: A simple data processing method to improve robustness and uncertainty",
            "venue": "Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Rui Huang",
                "Andrew Geng",
                "Yixuan Li"
            ],
            "title": "On the importance of gradients for detecting distributional shifts in the wild",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaowei Huang",
                "Daniel Kroening",
                "Wenjie Ruan",
                "James Sharp",
                "Youcheng Sun",
                "Emese Thamo",
                "Min Wu",
                "Xinping Yi"
            ],
            "title": "A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability",
            "venue": "Computer Science Review,",
            "year": 2020
        },
        {
            "authors": [
                "Peng-Tao Jiang",
                "Chang-Bin Zhang",
                "Qibin Hou",
                "Ming-Ming Cheng",
                "Yunchao Wei"
            ],
            "title": "Layercam: Exploring hierarchical class activation maps for localization",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Oguzhan Fatih Kar",
                "Teresa Yeo",
                "Amir Zamir"
            ],
            "title": "3d common corruptions for object recognition",
            "venue": "In ICML 2022 Shift Happens Workshop,",
            "year": 2022
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Jones",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Fine-tuning can distort pretrained features and underperform out-of-distribution",
            "venue": "arXiv preprint arXiv:2202.10054,",
            "year": 2022
        },
        {
            "authors": [
                "Yingwei Li",
                "Qihang Yu",
                "Mingxing Tan",
                "Jieru Mei",
                "Peng Tang",
                "Wei Shen",
                "Alan Yuille",
                "Cihang Xie"
            ],
            "title": "Shapetexture debiased neural network training",
            "venue": "arXiv preprint arXiv:2010.05981,",
            "year": 2010
        },
        {
            "authors": [
                "Geert Litjens",
                "Thijs Kooi",
                "Babak Ehteshami Bejnordi",
                "Arnaud Arindra Adiyoso Setio",
                "Francesco Ciompi",
                "Mohsen Ghafoorian",
                "Jeroen Awm Van Der Laak",
                "Bram Van Ginneken",
                "Clara I S\u00e1nchez"
            ],
            "title": "A survey on deep learning in medical image analysis",
            "venue": "Medical image analysis,",
            "year": 2017
        },
        {
            "authors": [
                "Weitang Liu",
                "Xiaoyun Wang",
                "John Owens",
                "Yixuan Li"
            ],
            "title": "Energy-based out-of-distribution detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ahmet Murat Ozbayoglu",
                "Mehmet Ugur Gudelek",
                "Omer Berat Sezer"
            ],
            "title": "Deep learning for financial applications: A survey",
            "venue": "Applied Soft Computing,",
            "year": 2020
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Evgenia Rusak",
                "Lukas Schott",
                "Roland S Zimmermann",
                "Julian Bitterwolf",
                "Oliver Bringmann",
                "Matthias Bethge",
                "Wieland Brendel"
            ],
            "title": "A simple way to make neural networks robust against diverse image corruptions",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Hadi Salman",
                "Andrew Ilyas",
                "Logan Engstrom",
                "Ashish Kapoor",
                "Aleksander Madry"
            ],
            "title": "Do adversarially robust imagenet models transfer better",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Steffen Schneider",
                "Evgenia Rusak",
                "Luisa Eck",
                "Oliver Bringmann",
                "Wieland Brendel",
                "Matthias Bethge"
            ],
            "title": "Improving robustness against common corruptions by covariate shift adaptation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Vikash Sehwag",
                "Caner Hazirbas",
                "Albert Gordo",
                "Firat Ozgenel",
                "Cristian Canton"
            ],
            "title": "Generating high fidelity data from low-density regions using diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Mingxing Tan",
                "Quoc Le"
            ],
            "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Ross Wightman",
                "Hugo Touvron",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Resnet strikes back: An improved training procedure in timm",
            "venue": "arXiv preprint arXiv:2110.00476,",
            "year": 2021
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Xiao",
                "Logan Engstrom",
                "Andrew Ilyas",
                "Aleksander Madry"
            ],
            "title": "Noise or signal: The role of image backgrounds in object recognition",
            "venue": "Proceedings of the International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zheng Zhang",
                "Yue Cao",
                "Yutong Lin",
                "Jianmin Bao",
                "Zhuliang Yao",
                "Qi Dai",
                "Han Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Hang Zhang",
                "Chongruo Wu",
                "Zhongyue Zhang",
                "Yi Zhu",
                "Haibin Lin",
                "Zhi Zhang",
                "Yue Sun",
                "Tong He",
                "Jonas Mueller",
                "R Manmatha"
            ],
            "title": "Resnest: Split-attention networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chuanxia Zheng",
                "Tat-Jen Cham",
                "Jianfei Cai",
                "Dinh Phung"
            ],
            "title": "Bridging global context interactions for high-fidelity image completion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "\u2217 Corresponding author. This research is supported in part by the National Key Research and\nDevelopment Progrem of China under Grant No.2020AAA0140000.\non top-1 accuracy. We also evaluate some robust models including both adversarially trained models and other robust trained models and find that some models show worse robustness against attribute changes than vanilla models. Based on these findings, we discover ways to enhance attribute robustness with preprocessing, architecture designs, and training strategies. We hope this work can provide some insights to the community and open up a new avenue for research in robust computer vision. The code and dataset are available at https://github.com/ alibaba/easyrobust."
        },
        {
            "heading": "1. Introduction",
            "text": "Deep learning has triggered the rise of artificial intelligence and has become the workhorse of machine intelligence. Deep models have been widely applied in various fields such as autonomous driving [27], medical science [32], and finance [37]. With the spread of these techniques, the robustness and safety issues begin to be essential, especially after the finding that deep models can be easily fooled by negligible noises [15]. As a result, more researchers contribute to building datasets for benchmark-\nar X\niv :2\n30 3.\n17 09\ning model robustness to spot vulnerabilities in advance. Most of the existing work builds datasets for evaluating the model robustness and generalization ability on outof-distribution data [6, 21, 29] using adversarial examples and common corruptions. For example, the ImageNetC(orruption) dataset conducts visual corruptions such as Gaussian noise to input images to simulate the possible processors in real scenarios [21]. ImageNet-R(enditions) contains various renditions (e.g., paintings, embroidery) of ImageNet object classes [20]. As both studies have found that higher accuracy on ImageNet usually leads to better robustness against different domains [21,50]. However, most previous studies try to achieve this in a top-down way, such as architecture design, exploring a better training strategy, etc. We advocate that it is also essential to manage it in a bottom-up way, that is, conducting model debugging with the in-distribution dataset to provide clues for model repairing and accuracy improvement. For example, it is interesting to explore whether a bird with a water background can be recognized correctly even if most birds appear with trees or grasses in the training data. Though this topic has been investigated in studies such as causal and effect analysis [8], the experiments and analysis are undertaken on domain generalization datasets. How a deep model generalizes to different backgrounds is still unknown due to the vacancy of a qualified benchmark. Therefore, in this paper, we provide a detached object editing tool to conduct the model debugging from the perspective of object attribute and construct a dataset named ImageNet-E(diting).\nThe ImageNet-E dataset is a compact but challenging test set for object recognition that contains controllable object attributes including backgrounds, sizes, positions and directions, as shown in Fig. 1. In contrast to ObjectNet [5] whose images are collected by their workers via posing objects according to specific instructions and differ from the target data distribution. This makes it hard to tell whether the degradation comes from the changes of attribute or distribution. Our ImageNet-E is automatically generated with our object attribute editing tool based on the original ImageNet. Specifically, to change the object background, we provide an object background editing method that can make the background simpler or more complex based on diffusion models [24, 46]. In this way, one can easily evaluate how much the background complexity can influence the model performance. To control the object size, position, and direction to simulate pictures taken from different distances and angles, an object editing method is also provided. With the editing toolkit, we apply it to the large-scale ImageNet dataset [41] to construct our ImageNet-E(diting) dataset. It can serve as a general dataset for benchmarking robustness evaluation on different object attributes.\nWith the ImageNet-E dataset, we evaluate the performance of current deep learning models, including both con-\nvolutional neural networks (CNNs), vision transformers as well as the large-scale pretrained CLIP [39]. We find that deep models are quite sensitive to object attributes. For example, when editing the background towards high complexity (see Fig. 1, the 3rd row in the background part), the drop in top-1 accuracy reaches 9.23% on average. We also find that though some robust models share similar top-1 accuracy on ImageNet, the robustness against different attributes may differ a lot. Meanwhile, some models, being robust under certain settings, even show worse results than the vanilla ones on our dataset. This suggests that improving robustness is still a challenging problem and the object attributes should be taken into account. Afterward, we discover ways to enhance robustness against object attribute changes. The main contributions are summarized as follows:\n\u2022 We provide an object editing toolkit that can change the object attributes for manipulated image generation.\n\u2022 We provide a new dataset called ImageNet-E that can be used for benchmarking robustness to different object attributes. It opens up new avenues for research in robust computer vision against object attributes.\n\u2022 We conduct extensive experiments on ImageNet-E and find that models that have good robustness on adversarial examples and common corruptions may show poor performance on our dataset."
        },
        {
            "heading": "2. Related Work",
            "text": "The literature related to attribute robustness benchmarks can be broadly grouped into the following themes: robustness benchmarks and attribute editing datasets. Existing robustness benchmarks such as ImageNet-C(orruption) [21], ImageNet-R(endition) [20], ImageNet-Stylized [13] and ImageNet-3DCC [29] mainly focus on the exploration of the corrupted or out-of-distribution data that models may encounter in reality. For instance, the ImageNet-R dataset contains various renditions (e.g., paintings, embroidery) of ImageNet object classes. ImageNet-C analyzes image models in terms of various simulated image corruptions (e.g., noise, blur, weather, JPEG compression, etc.). Attribute editing dataset creation is a new topic and few studies have explored it before. Among them, ObjectNet [5] and ImageNet-9 (a.k.a. background challenge) [50] can be the representative. Specifically, ObjectNet collects a large realworld test set for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. The images in ObjectNet are collected by their workers who image objects in their homes. It consists of 313 classes which are mainly household objects. ImageNet9 mainly creates a suit of datasets that help disentangle the impact of foreground and background signals on classification. To achieve this goal, it uses coarse-grained classes\nwith corresponding rectangular bounding boxes to remove the foreground and then paste the cut area with other backgrounds. It can be observed that there lacks a dataset that can smoothly edit the object attribute."
        },
        {
            "heading": "3. Preliminaries",
            "text": "Since the editing tool is developed based on diffusion models, let us first briefly review the theory of denoising diffusion probabilistic models (DDPM) [24,46] and analyze how it can be used to generate images.\nAccording to the definition of the Markov Chain, one can always reach a desired stationary distribution from a given distribution along with the Markov Chain [14]. To get a generative model that can generate images from random Gaussian noises, one only needs to construct a Markov Chain whose stationary distribution is Gaussian distribution. This is the core idea of DDPM. In DDPM, given a data distribution x0 \u223c q(x0), a forward noising process produces a series of latents x1, ...,xT of the same dimensionality as the data x0 by adding Gaussian noise with variance \u03b2t \u2208 (0, 1) at time t:\nq(xt|xt\u22121) = N ( \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI), s.t. 0 < \u03b2t < 1,\n(1) where \u03b2t is the diffusion rate. Then the distribution q(xt|x0) at any time t is: q(xt|x0) = N ( \u221a \u03b1\u0304t, (1\u2212 \u03b1\u0304t)I), xt = \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5 (2) where \u03b1\u0304t = \u220ft s=1(1 \u2212 \u03b2t), \u03f5 \u223c N (0, I). It can be proved that limt\u2192\u221e q(xt) = N (0, I). In other words, we can map the original data distribution into a Gaussian distribution with enough iterations. Such a stochastic forward process is named as diffusion process since what the process q(xt|xt\u22121) does is adding noise to xt\u22121.\nTo draw a fresh sample from the distribution q(x0), the Markov process is reversed. That is, beginning from a\nGaussian noise sample xT \u223c N (0, I), a reverse sequence is constructed by sampling the posteriors q(xt\u22121|xt). To approximate the unknown function q(xt\u22121|xt), in DDPMs, a deep model p\u03b8 is trained to predict the mean and the covariance of xt\u22121 given xt instead. Then the xt\u22121 can be sampled from the normal distribution defined as:\np\u03b8(xt\u22121|xt) = N (\u00b5\u03b8(xt, t),\u03a3\u03b8(xt, t)). (3)\nIn stead of inferring \u00b5\u03b8(xt, t) directly, [24] propose to predict the noise \u03f5\u03b8(xt, t) which was added to x0 to get xt with Eq. (2). Then \u00b5\u03b8(xt, t) is:\n\u00b5\u03b8(xt, t) = 1\u221a \u03b1\u0304t\n( xt \u2212\n\u03b2t\u221a 1\u2212 \u03b1\u0304t \u03f5\u03b8(xt, t)\n) . (4)\n[24] keep the value of \u03a3\u03b8(xt, t) to be constant. As a result, given a sample xt at time t, with a trained model that can predict the noise \u03f5\u03b8(xt, t), we can get \u00b5\u03b8(xt, t) according to Eq. (4) to reach the xt\u22121 with Equation (3) and eventually we can get to x0.\nPrevious studies have shown that diffusion models can achieve superior image generation quality compared to the current state-of-the-art generative models [1]. Besides, there have been plenty of works on utilizing the DDPMs to generate samples with desired properties, such as semantic image translation [36], high fidelity data generation from low-density regions [44], etc. In this paper, we also choose the DDPM adopted in [1] as our generator."
        },
        {
            "heading": "4. Attribute Editing with Diffusion Models and",
            "text": "ImageNet-E\nMost previous robustness-related work has focused on the important challenges of robustness on adversarial examples [6], common corruptions [21]. They have found that higher clean accuracy usually leads to better robustness.\nTherefore, instead of exploring a new corruption that models may encounter in reality, we pay attention to the model debugging in terms of object attributes, hoping to provide new insights to clean accuracy improvement. In the following, we describe our object attribute editing tool and the generated ImageNet-E dataset in detail. The whole pipeline can be found in Fig. 2."
        },
        {
            "heading": "4.1. Object Attribute Editing with Diffusion Models",
            "text": "Background editing. Most existing corruptions conduct manipulations on the whole image, as shown in Fig. 1. Compared to adding global corruptions that may hinder the visual quality, a more likely-to-happen way in reality is to manipulate the backgrounds to fool the model. Besides, it is shown that there exists a spurious correlation between labels and image backgrounds [12]. From this point, a background corruption benchmark is needed to evaluate the model\u2019s robustness. However, the existing background challenge dataset achieves background editing with copy-paste operation, resulting an obvious artifacts in generated images [50]. This may leave some doubts about whether the evaluation is precise since the dataset\u2019s distribution may have changed. To alleviate this concern, we adopt DDPM approach to incorporate background editing by adding a guiding loss that can lead to backgrounds with desired properties to make the generated images stay in/close to the original distribution. Specifically, we choose to manipulate the background in terms of texture complexity due to the hypothesis that an object should be observed more easily from simple backgrounds than from complicated ones. In general, the texture complexity can be evaluated with the gray-level cooccurrence matrix (GLCM) [16], which calculates the graylevel histogram to show the texture characteristic. However, the calculation of GLCM is non-differentiable, thus it cannot serve as the conditional guidance of image generation. We hypothesize that a complex image should contain more frequency components in its spectrum and higher amplitude indicates greater complexity. Thus, we define the objective of complexity as:\nLc = \u2211 |A(F(x))| , (5)\nwhere F is the Fourier transform [45], A extracts the amplitude of the input spectrum. x is the evaluated image. Since minimizing this loss helps us generate an image with desired properties and should be conducted on the x0, we need a way of estimating a clean image x0 from each noisy latent representation xt during the denoising diffusion process. Recall that the process estimates at each step the noise \u03f5\u03b8(xt, t) added to x0 to obtain xt. Thus, x\u03020 can be estimated via Equation (6) [1]. The whole optimization procedure is shown in Algorithm 1.\nx\u03020 = xt\u221a \u03b1\u0304t\n\u2212 \u221a 1\u2212 \u03b1\u0304t\u03f5\u03b8(xt, t)\u221a\n\u03b1\u0304t . (6)\nAs shown in Fig. 3(a), with the proposed method, when we guide the generation procedure with the proposed objective towards the complex direction, it will return images with visually complex backgrounds. We also provide the GLCM dissimilarity and contrast of each image to make a quantitative analysis of the generated images. A higher dissimilarity/contrast score indicates a more complex image background [16]. It can be observed that the complexity is consistent with that calculated with GLCM, indicating the effectiveness of the proposed method. Controlling object size, position and direction. In general, the human vision system is robust to position, direction and small size changes. Whether the deep models are also robust to these object attribute changes is still unknown to researchers. Therefore, we conduct the image editing with controls of object sizes, positions and directions to find the answer. For a valid evaluation on different attributes, all other variables should remain unchanged, especially the background. Therefore, we first disentangle the object and background with the in-painting strategy provided by [54]. Specifically, we mask the object area in input image x. Then we conduct in-painting to remove the object and get the pure background image xb, as shown in Fig. 3(b) column 3. To realize the aforementioned object attribute controlling, we adopt the orthogonal transformation. Denote P as the pixel locations of object in image x where P \u2208 R3\u00d7No . No is the number of pixels belong to object and pi = [xi, yi, 1]T is the position of object\u2019s i-th pixel. h\u2032 \u2208 [0, H \u2212 h], w\u2032 \u2208 [0,W \u2212 w] where [x, y, w, h] stand for the enclosing rectangle of the object with mask M . Then the newly edited x[Tattribute \u00b7 P ] = x[P ] and M [Tattribute \u00b7 P ] = M [P ], where\nTsize = s 0 \u2206x0 s \u2206y 0 0 1  , Tposition = 1 0 w\u20320 1 h\u2032 0 0 1  , Tdirection =  cos \u03b8 sin \u03b8 0\u2212 sin \u03b8 cos \u03b8 0 0 0 1  . (7)\nwhere s is the resize scale. \u03b8 is the rotation angle. \u2206x = (1\u2212 s) \u00b7 (x+ w/2),\u2206y = (1\u2212 s) \u00b7 (y + h/2).\nWith the background image xb and edited object xo, a naive way is to place the object in the original image to the corresponding area of background image xb as M \u2299 xo + (1 \u2212 M) \u2299 xb. However, the result generated in this manner may look disharmonic, lacking a delicate adjustment to blending them together. Besides, as shown in Fig. 3(b) column 3, the object-removing operation may leave some artifacts behind, failing to produce a coherent and seamless result. To deal with this problem, we leverage DDPM models to blend them at different noise levels along the diffusion process. Denote the image with desired object attribute as xo. Starting from the pure background image xb at time t0, at each stage, we perform a guided diffusion step with a latent xt to obtain the xt\u22121 and at the same time, obtain a noised version of object image\nAlgorithm 1: Background editing input : source image x, mask M , diffusion model\n(\u00b5\u03b8(xt),\u03a3\u03b8(xt)), \u03b1\u0304t, \u03bb, iteration steps t0 output: edited image x0\n1 xt0 \u223c N ( \u221a \u03b1\u0304t0x, (1\u2212 \u03b1\u0304t0)I); 2 for t\u2190 t0 to 1 do 3 x\u03020 \u2190 xt\u221a\u03b1\u0304t \u2212 \u221a 1\u2212\u03b1\u0304t\u03f5\u03b8(xt,t)\u221a \u03b1\u0304t ; 4 \u2207bg \u2190 \u2207x\u03020Lc(x\u03020); 5 xbt\u22121 \u223c N (\u00b5\u03b8(xt) + \u03bb\u03a3\u03b8(xt)\u2207bg,\u03a3\u03b8(xt)); 6 xo \u223c N ( \u221a \u03b1\u0304tx, (1\u2212 \u03b1\u0304t)I); 7 xt\u22121 \u2190M \u2299 xo + (1\u2212M)\u2299 xbt\u22121; 8 end\nxot\u22121. Then the two latents are blended with the mask M as xt\u22121 = M \u2299 xot\u22121 + (1\u2212M)\u2299 xt\u22121. The DDPM denoising procedure may change the background. Thus a proper initial timing is required to maintain a high resemblance to the original background. We set the iteration steps t0 as 50 and 25 in Algorithm 1 and 2 respectively."
        },
        {
            "heading": "4.2. ImageNet-E dataset",
            "text": "With the tool above, we conduct object attribute editing including background, size, direction and position changes based on the large-scale ImageNet dataset [41] and ImageNet-S [11], which provides the mask annotation. To guarantee the dataset quality, we choose the animal classes from ImageNet classes such as dogs, fishes and birds, since they appear more in nature without messy backgrounds. Classes such as stove and mortarboard are removed. Finally, our dataset consists of 47872 images with 373 classes based on the initial 4352 images, each of which is applied 11 transforms. Detailed information can be found in Appendix A. For background editing, we choose three levels of the complexity, including \u03bb = \u221220, \u03bb = 20 and \u03bb = 20-adv with adversarial guidance (see Sec.B for details) instead of complexity. Larger \u03bb indicates stronger guidance towards high complexity. For the object size, we design\nAlgorithm 2: Object size controlling input : source image x, mask M , diffusion model\n(\u00b5\u03b8(xt),\u03a3\u03b8(xt)), \u03b1\u0304t, iteration steps t0, ratio s output: edited image x0\n1 xb \u2190 ObjectRemoving(x,M ); 2 x,M \u2190 Rescale (x,M, s); 3 xt0 \u223c N ( \u221a \u03b1\u0304t0x\nb, (1\u2212 \u03b1\u0304t0)I); 4 for t\u2190 t0 to 1 do 5 xbt\u22121 \u223c N (\u00b5\u03b8(xt),\u03a3\u03b8(xt)); 6 xo \u223c N ( \u221a \u03b1\u0304tx, (1\u2212 \u03b1\u0304t)I); 7 xt\u22121 \u2190M \u2299 xo + (1\u2212M)\u2299 xbt\u22121;\n8 end\nfour levels of sizes in terms of the object pixel rates (= sum(M > 0.5)/sum(M \u2265 0)): [Full, 0.1, 0.08, 0.05] where \u2018Full\u2019 indicates making the object as large as possible while maintaining its whole body inside the image. Smaller rates indicate smaller objects. For object position, we find that some objects hold a high object pixel rate in the whole image, resulting in a small H \u2212 h. Take the first picture in Fig. 3 for example, the dog is big and it will make little visual differences after position changing. Thus, we adopt the data whose pixel rate is 0.05 as the initial images for the position-changing operation.\nIn contrast to benchmarks like ImageNet-C [21] giving images from different domains so that the model robustness in these situations may be assessed, our effort aims to give an editable image tool that can conduct model debugging with in-distribution (ID) data, in order to identify specific shortcomings of different models and provide some insights for clean accuracy improving. Thus, the data distribution should not differ much from the original ImageNet. We choose the out-of-distribution (OOD) detection methods Energy [33] and GradNorm [26] to evaluate whether our editing tool will move the edited image out of its original distribution. These OOD detection methods aim to distinguish the OOD examples from the ID exam-\nples. The results are shown in Fig. 4. x-axis is the ID score in terms of the quantities in Energy and GradNorm and y-axis is the frequency of each ID score. A high ID score indicates the detection method takes the input sample as the ID data. Compared to other datasets, our method barely changes the data distribution under both Energy (the 1st row) and GradNorm (the 2nd row) evaluation methods. Besides, the Fre\u0301chet inception distance (FID) [23] for our ImageNet-E is 15.57 under the random background setting, while it is 34.99 for ImageNet-9 (background challenge). These all imply that our editing tool can ensure the proximity to the original ImageNet, thus can give a controlled evaluation on object attribute changes. To find out whether the DDPM will induce some degradation to our evaluation, we have conducted experiment in Tab. 1 with the setting \u03bb = 0 during background editing. This operation will first add noises to the original and then denoise them. It can be found in \u201cInver\u201d column that the degradation is negligible compared to degradation induced by attribute changes."
        },
        {
            "heading": "5. Experiments",
            "text": "We conduct evaluation experiments on various architectures including both CNNs (ResNet (RN) [19], DenseNet [25], EfficientNet (EF) [47], ResNest [53], ConvNeXt [35]) and transformer-based models (VisionTransformer (ViT) [9], Swin-Transformer (Swin) [34]). Other state-of-the-art models that trained with extra data such as CLIP [39], EfficientNet-L2-Noisy-Student [51] are also evaluated in the Appendix. Apart from different sizes of these models, we have also evaluated their adversarially trained versions for comprehensive studies. We report the drop of top-1 accuracy as metric based on the idea that the attribute changes should induce little influence to a robust trained model. More experimental details and results of top1 accuracy can be found in the Appendix."
        },
        {
            "heading": "5.1. Robustness evaluation",
            "text": "Normally trained models. To find out whether the widely used models in computer vision have gained robustness against changes on different object attributes, we conduct extensive experiments on different models. As shown in Tab. 1, when only the background is edited towards high complexity, the average drop in top-1 accuracy is 9.23% (\u03bb = 20). This indicates that most models are sensitive to object background changes. Other attribute changes such as size and position can also lead to model performance degradation. For example, when changing the object pixel rate to 0.05, as shown in Fig. 1 row 4 in the \u2018size\u2019 column, while we can still recognize the image correctly, the performance drop is 18.34% on average. We also find that the robustness under different object attributes is improved along with improvements in terms of clean accuracy (Original) on different models. Accordingly, a switch from an RN50 (92.69% top-1 accuracy) to a Swin-S (96.21%) leads to the drop in accuracy decrease from 15.72% to 10.20% on average. By this measure, models have become more and more capable of generalizing to different backgrounds, which implies that they indeed learn some robust features. This shows that object attribute robustness can be a good way to measure future progress in representation learning. We also observe that larger networks possess better robustness on the attribute editing. For example, swapping a Swin-S (96.21% top-1 accuracy) with the larger SwinB (95.96% top-1 accuracy) leads to the decrease of the dropped accuracy from 10.20% to 8.99% when \u03bb = 20. In a similar fashion, a ConvNeXt-T (9.32% drop) is less robust than the giant ConvNeXt-B (7.26%). Consequently, models with even more depth, width, and feature aggregation may attain further attribute robustness. Previous studies [30] have shown that zero-shot CLIP exhibits better outof-distribution robustness than the finetuned CLIP, which is opposite to our ImageNet-E as shown in Tab. 1. This may serve as the evidence that our ImageNet-E has a good proximity to ImageNet. We also find that compared with fully-\nsupervised trained model under the same backbone (ViTB), the CLIP fails to show a better attribute robustness. We think this may be caused by that the CLIP has spared some capacity for OOD robustness.\nHF\nAll\nOriginal\nHF\nAll\nOriginal\nFigure 5. Comparisons between vanilla models and adversarially trained models across different architectures in terms of size changes (left). Evaluation of adversarial models (RN50) trained with different perturbation budgets is provided in the right figure.\nAdversarially trained models. Adversarial training [42] is one of the state-of-the-art methods for improving the adversarial robustness of deep models and has been widely studied [2]. To find out whether they can boost the attribute robustness, we conduct extensive experiments in terms of different architectures and perturbation budgets (constraints of l2 norm bound). As shown in Fig. 5, the adversarially trained ones are not robust against attribute changes including both backgrounds and size-changing situations. The dropped accuracies are much greater compared to normally trained models. As the perturbation budget grows, the situation gets worse. This indicates that adversarial training can do harm to robustness against attributes."
        },
        {
            "heading": "5.2. Robustness enhancements",
            "text": "Based on the above evaluations, we step further to discover ways to enhance the attribute robustness in terms of preprocessing, network design and training strategies. More details including training setting and numerical experimental results can be found in Appendix C.5. Preprocessing. Given that an object can be inconspicuous due to its small size or subtle position, viewing an object at several different locations may lead to a more stable prediction. Having this intuition in mind, we perform the classical Ten-Crop strategy to find out if this operation can help to get a robustness boost. The Ten-Crop operation is executed by cropping all four corners and the center of the input image. We average the predictions of these crops together with their horizontal mirrors as the final result. We find this operation can contribute a 0.69% and 1.24% performance boost on top-1 accuracy in both background and size changes scenarios on average respectively. Network designs. Intuitively, a robust model should tend to focus more on the object of interest instead of the background. Therefore, recent models begin to enhance the model by employing attention modules. Of these, the\nResNest [53] can be a representative. The ResNest is a modularized architecture, which applies channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. As it has achieved a great boost in the ImageNet dataset, it also shows superiority on ImageNet-E compared to ResNet. For example, a switch from RN50 decreases the average dropped accuracy from 15.72% to 12.57%. This indicates that the channel-wise attention module can be a good choice to improve the attribute robustness. Another representative model can be the vision transformer, which consists of multiple self-attention modules. To study whether incorporating transformer\u2019s self-attention-like architecture into the model design can help attribute robustness generalization, we establish a hybrid architecture by directly feeding the output of res 3 block in RN50 into ViTS as the input feature like [3]. The dropped accuracy decreases by 1.04% compared to the original RN50, indicating the effectiveness of the self-attention-like architectures.\nTraining strategy. a) Robust trained. There have been plenty of studies focusing on the robust training strategy to improve model robustness. To find out whether these works can boost the robustness on our dataset, we further evaluate these state-of-the-art models including SIN [13], DebiasedCNN [31], Augmix [22], ANT [40], DeepAugment [20] and model trained with lots of standard augmentations (RN50T) [48]. As shown in Tab. 2, apart from the RN50-T, while the Augmix model shows the best performance against the background change scenario, the Debiased model holds the best in the object size change scenario. What we find unexpectedly is the SIN performance. The SIN method features the novel data augmentation scheme where ImageNet images are stylized with style transfer as the training data to force the model to rely less on textural cues for classification. Though the robustness boost is achieved on ImageNetC (mCE 69.32%) compared to its vanilla model (mCE 76.7%), it fails to improve the robustness in both object background and size-changing scenarios. The drops of top1 accuracy for vanilla RN50 and RN50-SIN are 21.26% and 24.23% respectively, when the object size rate is 0.05, though they share similar accuracy on original ImageNet. This indicates that existing benchmarks cannot reflect the real robustness in object attribute changing. Therefore, a dataset like ImageNet-E is necessary for comprehensive evaluations on deep models. b) Masked image modeling. Considering that masked image modeling has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches [4], it may be robust to the attribute changes. Therefore, we choose the Masked AutoEncoder (MAE) [17] as the training strategy since its objective is recovering images with only 25% patches. Specifically, we adopt the MAE training strategy with ViT-B backbone and then finetune it with ImageNet\ntraining data. We find that the robustness is improved. For example, the dropped accuracy decreases from 10.62% to 9.05% on average compared to vanilla ViT-B."
        },
        {
            "heading": "5.3. Failure case analysis",
            "text": "To explore the reason why some robust trained models may fail, we leverage the LayerCAM [28] to generate the heat map for different models including vanilla RN50, RN50+SIN and RN50+Debiased for comprehensive studies. As shown in Fig. 6, the heat map of the Debiased model aligns better with the objects in the image than that of the original model. It is interesting to find that the SIN model sometimes makes wrong predictions even with its attention on the main object. We suspect that the SIN model relies too much on the shape. for example, the \u2018sea urchin\u2019 looks like the \u2018acron\u2019 with the shadow. However, its texture clearly indicates that it is the \u2018sea urchin\u2019. In contrast, the Debiased model which is trained to focus on both the shape and texture can recognize it correctly. More studies can be found in Appendix C.4."
        },
        {
            "heading": "5.4. Model repairing",
            "text": "To validate that the evaluation on ImageNet (IN)-E can help to provide some insights for model\u2019s applicability and enhancement, we conduct a toy example for model repair-\nSINEdited DebiasedVanillaOriginal\ning. Previous evaluation shows that the ResNet50 is vulnerable to background changes. Based on this observation, we randomly replace the backgrounds of objects with others during training and get a validation accuracy boost from 77.48% to 79.00%. Note that the promotion is not small as only 8781 training images with mask annotations are available in ImageNet. We also step further to find out if the improved model can get a boost the OOD robustness, as shown in the Tab. 3. It can be observed that with the insights provided by the evaluation on ImageNet-E, one can explore the model\u2019s attribute vulnerabilities and manage to\nrepair the model and get a performance boost accordingly."
        },
        {
            "heading": "6. Conclusion and Future work",
            "text": "In this paper, we put forward an image editing toolkit that can take control of object attributes smoothly. With this tool, we create a new dataset called ImageNet-E that can serve as a general dataset for benchmarking robustness against different object attributes. Extensive evaluations conducted on different state-of-the-art models show that most models are vulnerable to attribute changes, especially the adversarially trained ones. Meanwhile, other robust trained models can show worse results than vanilla models even when they have achieved a great robustness boost on other robustness benchmarks. We further discover ways for robustness enhancement from both preprocessing, network designing and training strategies.\nLimitations and future work. This paper proposes to edit the object attributes in terms of backgrounds, sizes, positions and directions. Therefore, the annotated mask of the interest object is required, resulting in a limitation of our method. Besides, since our editing toolkit is developed based on diffusion models, the generalization ability is determined by DDPMs. For example, we find synthesizing high-quality person images is difficult for DDPMs. Under the consideration of both the annotated mask and data quality, our ImageNet-E is a compact test set. In our future work, we would like to explore how to leverage the edited data to enhance the model\u2019s performance, including both the validation accuracy and robustness."
        },
        {
            "heading": "A. Details for ImageNet-E",
            "text": "To guarantee the visual quality of the generated examples, we choose the animal classes from ImageNet since they appear more in nature without messy backgrounds. Specifically, images whose coarse labels in [fish, shark, bird, salamander, frog, turtle, lizard, crocodile, dinosaur, snake, trilobite, arachnid, ungulate, monotreme, marsupial, coral, mollusk, crustacean, marine mammals, dog, wild dog, cat, wild cat, bear, mongoose, butterfly, echinoderms, rabbit, rodent, hog, ferret, armadillo,primate] are picked. The corresponding coarse labels of each class we refer to can be found in [10]1. Finally, our ImageNet-E consists of 373 classes. Since the number of masks provided in ImageNet-S [11] in these classes is 4352, thus the number of images in each edited kind is 4352. The ImageNet-E contains 11 kinds of attributes editing, including 5 kinds of background editing and 4 kinds of size editing, as well as one kind of position editing and one kind of direction editing. Finally, our ImageNet-E contains 47872 images. Experiments on more images can be found in section C.3. The comprehensive comparisons with the stateof-the-art robustness benchmarks are shown in Figure 7. In contrast to other benchmarks that investigate new out-ofdistribution corruptions or perturbations deep models may encounter, w conduct model debugging with in-distribution data to explore which object attributes a model may be sensitive to. The examples in ImageNet-E are shown in Figure 9. A demo video for our editing toolkit can be found at this url:https://drive.google.com/file/d/ 1h5EV3MHPGgkBww9grhlvrl--kSIrD5Lp/view? usp=sharing. Our code can be found at an anonymous url: https://huggingface.co/spaces/ Anonymous-123/ImageNet-Editing.\n1https://github.com/noameshed/noveltydetection/blob/master/imagenet categories synset.csv"
        },
        {
            "heading": "B. Background editing",
            "text": "Intuitively, an image with complicated background tends to contain more high-frequency components, such as edges. Therefore, a straight-forward way is to define the background complexity as the amplitude of high-frequency components. However, this operation can result in noisy backgrounds, instead of the ones with complicated textures. Therefore, we directly define complexity as the amplitude of all frequency components. The compared results are shown in Figure 8. It can be observed that the amplitude supervision on high-frequency components tends to make the model generate images with more noise. In contrast, amplitude supervision on all frequency components can help to generate images with texture-complex backgrounds. To edit the background adversarially, we set Lc = CE(f(x), y) where \u2018CE\u2019 is the cross entropy loss. f and y are the classifier and label of x respectively. We adopt the classifier f from guided-diffusion2."
        },
        {
            "heading": "C. Experimental details",
            "text": "C.1. Details for metrics\nIn this paper, we care more about how different attributes impact different models. Therefore, we choose the drop of top-1 accuracy as our evaluation metric. A lower dropped accuracy indicates higher robustness against our attribute\n2https://github.com/openai/guided-diffusion\nchanges. The dropped accuracy is defined as:\nDA = accoriginal \u2212 acc. (8)\nThe detailed top-1 accuracy (Top-1) and dropped accuracy (DA)on our ImageNet-E are listed in Table 4, Table 5 and Table 6, Table 7. All the experiments are conducted for 5 runs and we report the mean value in the tables.\nC.2. Classes whose accuracy drops the greatest\nTo find out which class gets the worst robustness against attribute changes, we plot the dropped accuracy in Figure 11. The evaluated models are vanilla RN50 and its Debiased model. It can be observed that objects that have tentacles with simple backgrounds are more easily to be attacked. For example, the dropped accuracy of the \u2018black widow\u2019 class reaches 47% for both vanilla and Debiased models. In contrast, the impact is smaller for images with complicated backgrounds such as pictures from \u2018squirrel monkey\u2019.\nC.3. Experiments on more data\nTo explore the model robustness against object attributes on large-scale datasets, we step further to conduct the image editing on all the images in the ImageNet-S validation set. Finally, the edited dataset ImageNet-E-L shares the same size as ImageNet-S, which consists of 919 classes and 10919 images. We conduct both background editing and size editing to them. The evaluation results are shown in Table 8. The same conclusion can also be observed. For instance, most models show vulnerability against attribute changing since the average dropped accuracies reach 12.22% and 22.21% in background and size changes respectively. When the model gets larger, the robustness is improved. The consistency implies that using our ImageNet-E can already reflect the model robustness against object attribute changes.\nC.4. Bad case analysis\nTo make a comprehensive study of how the model behaves, we step further to make a comparison of the heat maps of the originals and edited ones. We choose the images that are recognized correctly at first but misclassified after editing. All the attributes editing including background, size, directions are explored. The heat maps are visualized in Figure 12. It can be observed that compared to the SIN and Debiased models, the vanilla RN50 is more likely to lose its focus on the interest area, especially in the size change scenario. For example, in the second row, as it puts his focus on the background, it returns a result with the \u2018nail\u2019 label. The same fashion is also observed in the background change scenario. The predicted label of \u2018night snake\u2019 turns into \u2018spider web\u2019 as the complex background has attracted its attention. In contrast, the SIN and Debiased models have robust attention mechanisms. The quantitative results in Table 5 also validate this. The dropped accuracy of RN50 (13.35%) is higher than SIN (12.19%) and Debiased (11.45%) even though the original accuracy of SIN (0.9157) is lower than vanilla RN50 (0.9269). However, the SIN also has its weakness. We find that though the SIN pays attention to the desired region, it can also make wrong predictions. As shown in the second row of Figure 12, when the object size gets smaller, the shape-based SIN model tends to make wrong predictions, e.g., mistaking the \u2018sea urchin\u2019 as \u2019acorn\u2019 due to the lack of texture analysis. As a result, the dropped accuracy in the size change scenario is 24.23% for SIN, even lower than vanilla RN50, whose dropped accuracy is 21.26%. On the contrary, the Debiased model can recognize it correctly, profiting from its shape and texture-biased module. From the above observation, we can conclude that the texture matters in the small object scenario.\nC.5. Details for robustness enhancements\nNetwork design\u2014-self-attention-like architecture. The results in Table 1 show that most vision transformers show better robustness than CNNs in our scenario. Previous study has shown that the self-attention-like architecture may be the key to robustness boost [3]. Therefore, to ablate whether incorporating this module can help attribute robustness generalization, we create a hybrid architecture (RN50d-hybrid) by directly feeding the output of res 3 block in RN50d into ViT-S as the input feature. The results are shown in Table 9. As we can find that while the added module maintains the robustness on background changes, it can help to boost the robustness against size changes. Moreover, the RN50-hybrid can also boost the overall performance compared to ViT-S.\nTraining strategy\u2014-Masked image modeling. Considering that masked image modeling has demonstrated impressive results in self-supervised representation learning by recovering corrupted image patches [4], it may be robust to the attribute changes. Thus, we test the Masked AutoEncoder (MAE) [18] and SimMIM [52] training strategy based on ViT-B backbone. As shown in Table 10, the dropped ac-\ncuracies decrease a lot compared to vanilla ViT-B, validating the effectiveness of the masked image modeling strategy. Motivated by this success, we also test another kind of self-supervised-learning strategy. To be specific, we choose the representative method MoCo-V3 [7] in the contrastive learning family. After the end-to-end finetuning, it achieves top-1 83.0% accuracy on ImageNet. It can also improve the attribute robustness when compared to the vanilla ViT-B, showing the effectiveness of contrastive learning.\nC.6. Hardware\nOur experiments are implemented by PyTorch [38] and runs on RTX-3090TI."
        },
        {
            "heading": "D. Further exploration on backgrounds",
            "text": "Motivated by the models\u2019 vulnerability against background changes, especially for those complicated backgrounds. Apart from randomly picking the backgrounds from the ImageNet dataset as final backgrounds (random bg), we also collect background templates with abundant textures, including leopard, eight diagrams, checker and stripe to explore the performance on out-of-distribution\nbackgrounds. The evaluation results are shown in Table 12. It can be observed that the background changes can lead to a 13.34% accuracy drop. When the background is set to be a leopard or other images, the dropped accuracy can even reach 35.52%. Sometimes the robust models even show worse robustness. For example, when the background is eight diagrams, all the robust models show worse results than the vanilla RN50, which is quite unexpected. To comprehend the behaviour behind it, we visualize the heat maps of the different models in Figure 7. An interesting finding is that deep models tend to make decisions with dependency on the backgrounds, especially when the background is complicated and can attract some attention. For example, when the background is the eight diagrams, the SIN takes the goldfish as a dishwasher. We suspect it has mistaken the background as dishes. In the same fashion, the Debiased model and ANT take the \u2018sea slug\u2019 with eight diagrams as a \u2018shopping basket\u2019, which seems to make sense since the \u2018sea slug\u2019 looks like a vegetable."
        },
        {
            "heading": "E. Further discussion on the distribution",
            "text": "In this paper, our effort aims to give an editable image tool that can edit the object\u2019s attribute in the given image while maintaining it in the original distribution for model debugging. Thus, we choose the out-\nof-distribution (OOD) detection methods including Energy [33] and GradNorm [26] following DRA [55] as the evaluation methods to find out whether our editing tool will move the edited image out of its original distribution. In contrast to FID which indicates the divergence of two datasets, the OOD detection is used to indicate the extent of the deviance of a single input image from the in-distribution dataset.\nCovariate shift adaptation(a.k.a batch-norm adaptation, BNA) is a way for improving robustness against common corruptions [43]. Thus, it can help to get a top-1 accuracy performance boost in OOD data. One can easily find out if the provided dataset is OOD by checking whether the BNA can get a performance boost on its data. We have tested the full adaptation results using BNA on ResNet50. In contrast to the promotion on other out-of-distribution dataset, we find that this operation induces little changes to top-1 accuracy on both ImageNet validation set (0.7615 \u2192 0.7613) and our ImageNet-E (0.7934 \u2192 0.7933 under \u03bb = 20, 0.6521 \u2192 0.6514 under random position scenario, mean accuracy of 5 runs). This similar tendency implies that our ImageNet-E shares a similar property with ImageNet."
        },
        {
            "heading": "F. Further evaluation on more state-of-the-art",
            "text": "models\nTo provide evaluations on more state-of-the-art models, we step further to evaluate the CLIP [39] and EfficientNetL2-Noisy-Student [51]. The average dropped accuracy in terms of different models can be found in Figure 13. CLIP shows a good robustness to out-of-distribution data [30]. Therefore, to find out whether the CLIP can also show a good robustness against attribute editing, we evaluate the CLIP model (Backbone ViT-B) with both the zero-shot and end-to-end finetuned version. To achieve this, we finetune the pretrained CLIP on the ImageNet training dataset based on prompt-initialized model following [49]. It acquires a 81.2% top-1 accuracy on ImageNet validation set while it is 68.3% for zero-shot version. The evaluation on ImageNet-E is shown in Table 11 and Table 13. Though previous studies have shown that the zero-shot CLIP model exhibits better out-of-distribution robustness than the finetuned ones, the finetuned CLIP shows better attribute robustness on ImageNet-E, as shown in Table 11 and Table 13. The tendency on ImageNet-E is the same with Im-\nageNet (IN) validation set and ImageNet-V2 (IN-V2). This implies that the ImageNet-E shows a better proximity to ImageNet than other out-of-distribution benchmarks such as ImageNet-C (IN-C), ImageNet-A (IN-A). Another finding is that the CLIP model fails to show better robustness than ViT-B while they share the same architectures. We suspect that this is caused by that CLIP may have spared some capacity for out-of-distribution robustness. As the network gets larger, its attribute robustness gets better.\nWhile EfficientNet-L2-Noisy-Student is one of the top models on ImageNet-A benchmark [51], it also shows superiority on ImageNet-E. To delve into the reason behind this, we test EfficientNet-L2-Noisy-Student-475 (EF-L2NT-475) and EfficientNet-B0-Noisy-Student (EF-B0-NT). The EF-L2-NT-475 differs from EF-L2-NT in terms of input size, which former is 475 while it is 800 for the latter. It can be found that the input size can induce little improvement to the attribute robustness. In contrast, larger networks can benefit a lot to attribute robustness, which is consistent with the finding in Section 5.1.\nEvaluations on 91 state-of-the-art models can be found in Figure 14. All the evaluated models in this figure are all\nprovided by the timm library, except for the MoCo-V3-FT and CLIP-FT, which are finetuned by us."
        },
        {
            "heading": "G. Failure cases of generated images",
            "text": "The failure cases of generated images are shown in Figure 16. The diffusion model fails to generate high-quality\nperson images. Though the object is reserved, the whole image looks quite wired. Therefore, we only keep the animal classes, resulting a compact set of ImageNet-E. However, extensive evaluations to 919 in Section C.3 have witnessed a same conclusion with evaluations on 373 classes. This implies that using our ImageNet-E can already reflect the model robustness against object attribute changes."
        },
        {
            "heading": "H. Related literature to robustness enhancements",
            "text": "Adversarial training. [42] focus on adversarially robust ImageNet classifiers and show that they yield improved accuracy on a standard suite of downstream classification tasks. It provides a strong baseline for adversarial training. Therefore, we choose their officially released adversarially trained models3 as the evaluation model. Models with different architectures are adopted here4.\nSIN [13] provides evidence that machine recognition today overly relies on object textures rather than global object shapes, as commonly assumed. It demonstrates the advantages of a shape-based representation for robust inference\n3https://github.com/microsoft/robust-models-transfer 4https://github.com/alibaba/easyrobust\n(using their Stylized-ImageNet dataset to induce such a representation in neural networks)\nDebiased [31] shows that convolutional neural networks are often biased towards either texture or shape, depending on the training dataset, and such bias degenerates model performance. Motivated by this observation, it develops a simple algorithm for shape-texture Debiased learning. To prevent models from exclusively attending to a single cue in representation learning, it augments training data with images with conflicting shape and texture information (e.g., an image of chimpanzee shape but with lemon texture) and provides the corresponding supervision from shape and texture simultaneously. It empirically demonstrates the advantages of the shape-texture Debiased neural network training on boosting both accuracy and robustness.\nAugmix [22] focuses on the robustness improvement to\nunforeseen data shifts encountered during deployment. It proposes a data processing technique named Augmix that helps to improve robustness and uncertainty measures on challenging image classification benchmarks.\nANT [40] demonstrates that a simple but properly tuned training with additive Gaussian and Speckle noise generalizes surprisingly well to unseen corruptions, easily reaching the previous state of the art on the corruption benchmark ImageNet-C and on MNIST-C.\nDeepAugment [20]. Motivated by the observation that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. It introduces a new data augmentation method named DeepAugment, which uses image-to-image neural networks for data augmentation. It improves robustness on their newly introduced ImageNet-R benchmark and can also be combined with other augmentation methods to outperform a model pretrained on 1000\u00d7 more labeled data.\nThere are some more tables and figures in the next pages."
        }
    ],
    "title": "ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing",
    "year": 2023
}