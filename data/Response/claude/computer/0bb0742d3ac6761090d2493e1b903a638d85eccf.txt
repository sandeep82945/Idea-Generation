 Here are 5 possible future research ideas after reading the paper:

1. Expand the dataset to include more Arabic dialects beyond the four focused on in this paper. There are many dialects spoken across the Arab world that could benefit from translation systems optimized for informal social media language.

2. Develop methods to automatically detect the dialect of a given piece of Arabic text. This could allow translation systems to handle code-switching between dialects more seamlessly.

3. Explore semi-supervised or unsupervised methods for dialect translation, to reduce reliance on large parallel corpora. Techniques like backtranslation or pretraining on monolingual data could be promising.

4. Evaluate the translation models on downstream NLP tasks beyond machine translation, such as sentiment analysis, hate speech detection, etc. This could demonstrate their broader utility.

5. Build a multidialectal Arabic language model pretrained on social media text. This model could then be fine-tuned for various dialect-related tasks, similar to what has been done for English with models like BERT.