{
    "abstractText": "Diabetic retinopathy (DR) is a leading cause of blindness worldwide. Early diagnosis is essential in the treatment of diabetes and can assist in preventing vision impairment. Since manual annotation of medical images is time-consuming, costly, and prone to subjectivity that leads to inconsistent diagnoses, several deep learning segmentation approaches have been proposed to address these challenges. However, these networks often rely on simple loss functions, such as binary cross entropy (BCE), which may not be sophisticated enough to effectively segment lesions such as those present in DR. In this paper, we propose a loss function that incorporates a global segmentation loss, a patch-wise density loss, and a patch-wise edge-aware loss to improve the performance of these networks on the detection and segmentation of hard exudates. Comparing our proposed loss function against the BCE loss on several state-of-the-art networks, our experimental results reveal substantial improvement in network performance achieved by incorporating the patch-wise contrastive loss.",
    "authors": [
        {
            "affiliations": [],
            "name": "Wei Tang"
        },
        {
            "affiliations": [],
            "name": "Yinxiao Wang"
        },
        {
            "affiliations": [],
            "name": "Kangning Cui"
        },
        {
            "affiliations": [],
            "name": "Raymond H. Chan"
        }
    ],
    "id": "SP:9239fe45f33943015f6a5eec1ad091d729766045",
    "references": [
        {
            "authors": [
                "M.D. Abr\u00e0moff",
                "M.K. Garvin",
                "M. Sonka"
            ],
            "title": "Retinal imaging and image analysis",
            "venue": "IEEE Rev. Biomed. Eng., vol. 3, pp. 169\u2013208, 2010.",
            "year": 2010
        },
        {
            "authors": [
                "W.L. Alyoubi",
                "W.M. Shalash",
                "M.F. Abulkhair"
            ],
            "title": "Diabetic retinopathy detection through deep learning techniques: A review",
            "venue": "Inform. Med. Unlocked, vol. 20, p. 100 377, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "R. Kawasaki"
            ],
            "title": "Is diabetic retinopathy related to subclinical cardiovascular disease?",
            "venue": "Ophthalmology, vol. 118,",
            "year": 2011
        },
        {
            "authors": [
                "N. Cheung",
                "T.Y. Wong"
            ],
            "title": "Diabetic retinopathy and systemic vascular complications",
            "venue": "Prog. Retin. Eye Res., vol. 27, no. 2, pp. 161\u2013176, 2008.",
            "year": 2008
        },
        {
            "authors": [
                "M.R.K. Mookiah"
            ],
            "title": "Computer-aided diagnosis of diabetic retinopathy: A review",
            "venue": "Comput. Biol. Med, vol. 43, no. 12, pp. 2136\u20132155, 2013.",
            "year": 2013
        },
        {
            "authors": [
                "N. Asiri",
                "M. Hussain",
                "F. Al Adel",
                "N. Alzaidi"
            ],
            "title": "Deep learning based computer-aided diagnosis systems for diabetic retinopathy: A survey",
            "venue": "Artif. Intell. Med., vol. 99, p. 101 701, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "Z. Si",
                "D. Fu",
                "Y. Liu",
                "Z. Huang"
            ],
            "title": "Hard exudate segmentation in retinal image with attention mechanism",
            "venue": "IET Image Process., vol. 15, no. 3, pp. 587\u2013597, 2021.",
            "year": 2021
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "Proc MICCAI, Springer, 2015, pp. 234\u2013241.",
            "year": 2015
        },
        {
            "authors": [
                "Z. Zhou",
                "M.M. Rahman Siddiquee",
                "N. Tajbakhsh",
                "J. Liang"
            ],
            "title": "Unet++: A nested u-net architecture for medical image segmentation",
            "venue": "Proc. Deep Learn. Med. Image Anal. Multimodal Learn. Clin. Decis. Support., Springer, 2018, pp. 3\u201311.",
            "year": 2018
        },
        {
            "authors": [
                "C.I. S\u00e1nchez",
                "M. Gar\u0107\u0131a",
                "A. Mayo",
                "M.I. L\u00f3pez",
                "R. Hornero"
            ],
            "title": "Retinal image analysis based on mixture models to detect hard exudates",
            "venue": "Med. Image Anal., vol. 13, no. 4, pp. 650\u2013658, 2009.",
            "year": 2009
        },
        {
            "authors": [
                "Z. Gu"
            ],
            "title": "Ce-net: Context encoder network for 2d medical image segmentation",
            "venue": "IEEE Trans. Med. Imaging, vol. 38, no. 10, pp. 2281\u20132292, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "F. Pan"
            ],
            "title": "Dual-view selective instance segmentation network for unstained live adherent cells in differential interference contrast images",
            "venue": "arXiv preprint arXiv:2301.11499, 2023.",
            "year": 2023
        },
        {
            "authors": [
                "Z. Zhang",
                "Q. Liu",
                "Y. Wang"
            ],
            "title": "Road extraction by deep residual u-net",
            "venue": "IEEE Geosci. Remote. Sens. Lett., vol. 15, no. 5, pp. 749\u2013753, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "Proc. ICML, PMLR, 2020, pp. 1597\u20131607.",
            "year": 2020
        },
        {
            "authors": [
                "P. Khosla"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Adv. Neural Inf. Process. Syst., vol. 33, pp. 18 661\u2013 18 673, 2020.",
            "year": 2020
        },
        {
            "authors": [
                "S. Camalan"
            ],
            "title": "Detecting change due to alluvial gold mining in peruvian rainforest using recursive convolutional neural networks and contrastive learning",
            "venue": "Fall Meeting, AGU, 2022. 7",
            "year": 2022
        },
        {
            "authors": [
                "H. Wu",
                "Z. Wang",
                "Y. Song",
                "L. Yang",
                "J. Qin"
            ],
            "title": "Cross-patch dense contrastive learning for semisupervised segmentation of cellular nuclei in histopathologic images",
            "venue": "Proc. CVPR, 2022, pp. 11 666\u2013 11 675.",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhao"
            ],
            "title": "Cross-level contrastive learning and consistency constraint for semi-supervised medical image segmentation",
            "venue": "Proc. IEEE Int. Symp. Biomed. Imaging, IEEE, 2022, pp. 1\u20135.",
            "year": 2022
        },
        {
            "authors": [
                "Q. Liu",
                "C. Chen",
                "J. Qin",
                "Q. Dou",
                "P. Heng"
            ],
            "title": "Feddg: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space",
            "venue": "Proc. CVPR, 2021, pp. 1013\u20131023.",
            "year": 2021
        },
        {
            "authors": [
                "P. Porwal"
            ],
            "title": "Indian diabetic retinopathy image dataset (idrid): A database for diabetic retinopathy screening research",
            "venue": "Data, vol. 3, no. 3, p. 25, 2018.",
            "year": 2018
        },
        {
            "authors": [
                "A. Paszke"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Adv. Neural Inf. Process. Syst., vol. 32, 2019.",
            "year": 2019
        },
        {
            "authors": [
                "C. Chen"
            ],
            "title": "Deep learning for cardiac image segmentation: A review",
            "venue": "Front. Cardiovasc. Med., vol. 7, p. 25, 2020. 8",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Index Terms: Patch-wise Contrastive Loss, Hard Exudates, Diabetic Retinopathy, Medical Image, Deep Learning."
        },
        {
            "heading": "1 Introduction",
            "text": "Fundus images are medical images of the interior surface of the eye, including the retina, optic disc, blood vessels, and other textures, which are typically captured using a specialized camera called a fundus camera [1]. One of the most significant tasks using fundus images is the detection of diabetic retinopathy (DR), which is a complication of diabetes that affects the eyes. The failure to detect DR can result in loss of vision or even blindness. Early detection of DR is therefore essential in the treatment of diabetes and can assist in preventing vision impairment [2]. Besides, DR can increase the risk of cardiovascular diseases (CVDs) as well [3], [4].\nComputer-assisted diagnosis (CAD), which uses algorithms implemented in computers to analyze medical images and aid in the diagnosis, is increasingly being used in the segmentation of DR lesions [5]. The manual labeling of DR lesions can be both time-consuming and costly, as well as prone to subjectivity, resulting in inconsistent diagnoses. Hence, deep learning-based CAD methods are preferred as they have the capability to precisely identify and segment DR lesions, making them a useful tool for assisting in the diagnostic process with consistency [6], [7].\nHowever, many of the aforementioned networks use relatively simple loss functions, such as the dice loss or the binary cross entropy (BCE) loss, which may not be sufficiently advanced to accurately segment the lesions commonly found in fundus images [8], [9]. This work introduces a loss function that incorporates a global segmentation loss (the BCE loss), a patch-wise density loss that contrasts lesion-dense and lesionsparse patches, and a patch-wise edge-aware loss that contrasts lesion boundaries and backgrounds. Since\n\u2217Corresponding Author.\nar X\niv :2\n30 2.\n11 51\n7v 1\n[ ee\nss .I\nV ]\nthe density and spatial distribution of lesions in medical images is non-uniform and the lesion boundaries are often blurred, the incorporation of patch-wise contrastive losses enhances the network performances on automated exudate segmentation substantially.\nThis article is organized as follows. Section 2 gives the related works on automated detection of exudates, deep learning in medical image segmentation, and contrastive learning. The proposed loss function is introduced in Section 3. Experimental results on a benchmark DR lesion dataset are presented in Section 4. The conclusion and future works are discussed in Section 5."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Segmentation of Diabetic Retinopathy",
            "text": "Image segmentation refers to the process of dividing an image into distinct segments, each of which corresponds to a different object or portion of the image. One of the primary tasks for DR is the segmentation of hard exudates (see Figure 1), which are irregular white or yellowish-white accumulations caused by the leakage of plasma that manifest in the retina as dots, patches, or circles [2]. Automated exudate detection is a challenging task due to the uneven illumination, poor contrast, and color variation present in retinal images, but it is of significant practical importance as early detection can prevent potential blindness [7], [10]."
        },
        {
            "heading": "2.2 Deep Learning in Medical Image Segmentation",
            "text": "Numerous deep neural networks have been developed for the purpose of segmenting medical images [8], [9], [11], [12]. In [8], a segmentation network called U-Net is proposed to segment medical images by integrating a contracting path that captures image context and a symmetric expansive path that ensures the end-to-end structure.\nSeveral variants of the U-Net architecture have been proposed in the literature owing to its high efficacy and robustness in medical image segmentation. UNet++ introduces nested U-Net blocks and deep supervision to capture features at multiple scales and thus enable more accurate segmentation [9]. Deep Residual U-Net (ResUnet) improves upon the U-Net architecture by incorporating residual connections between the encoding and decoding pathways that facilitate information flow and save computations [13].\nWhile U-Net can achieve satisfying segmentation results for medical images, it heavily relies on skip connections, which can dilute spatial information as the network deepens and it may struggle to capture fine-grained details due to its small receptive field [8], [11]. Context encoder network (CE-Net) uses dense\natrous convolution block and residual multi-kernel pooling, enabling it to preserve spatial information and capture fine-grained details [11]."
        },
        {
            "heading": "2.3 Contrastive Learning",
            "text": "Contrastive learning is designed as a self-supervised technique that learns a representation of data by contrasting similar (positive) and dissimilar (negative) pairs of samples. The contrastive learning models learn feature spaces in which similar samples are mapped closely together while dissimilar samples are mapped far apart [14]. Unsupervised contrastive learning has demonstrated considerable efficacy in various applications, however, the learned feature representation may not be optimized for the specific task due to the lack of labels [15], [16].\nSupervised contrastive learning addresses the limitations of unsupervised contrastive learning by incorporating labels into the learning process. Unlike unsupervised contrastive learning, which generates positive and negative pairs using data augmentation without labels, supervised contrastive learning defines positive and negative pairs as samples with the same and different labels, respectively [15]. The supervised contrastive learning loss function is then optimized to maximize the similarity between positive pairs while minimizing the similarity between negative pairs. By doing so, supervised contrastive learning can learn a more discriminative feature representation that is well-suited for downstream tasks such as exudate detection.\nThere has emerged a group of approaches that utilize multi-level features to conduct contrastive learning for semi-supervised semantic segmentation [17], [18]. It has been demonstrated that applying contrastive learning in both local and global feature spaces is promising in refining the model performance. However, there is still a lack of exploration of supervised cross-level contrastive learning in medical image analysis. In the next section, we introduce the proposed loss function that takes both global and local features into account."
        },
        {
            "heading": "3 Methodology",
            "text": "Given a medical image dataset containing N samples D = {(xi,yi), i = 1, ..., N}, where each pair of the data record is composed of an input RGB image xi and the corresponding binary ground truth (GT) mask yi. The objective is to learn a model that produces a pixel-wise segmentation map y\u0302i for each xi while minimizing the difference between y\u0302i and yi. In this work, we propose a supervised loss function that incorporates a global segmentation loss that is the BCE loss, and the patch-wise density and edge-aware loss motivated by supervised contrastive learning:\nLtotal = Lsup + \u03b1Lpd + \u03b2Lpe. (1)\nThe \u03b1 and \u03b2 are hyper-parameters adopted to balance the three terms in the loss function."
        },
        {
            "heading": "3.1 Patch-wise Density Loss",
            "text": "One of the principal challenges in medical image segmentation is the treatment of highly heterogeneous and sophisticated anatomical structures. Namely, the non-uniform density and spatial distribution of foreground pixels within the source image is one of the typical difficulties of enhancing deep learning models\u2019 reliability. To tackle this issue, we design a patch-wise density loss based on lesion pixel proportions of patches to sufficiently contrast the localized representations of similar patches against dissimilar patches in a supervised way.\nConcretely, for each input pair (xi,yi), we divide the raw image and the label into n\u00d7n patches denoted by xpi and y p i , where p = 1, ..., n\u00d7n. For example, retinal images commonly exhibit dispersed patterns of DR lesions, such as hard exudates, resulting in a diverse portion of pixels belonging to the target segmentation class in each patch [2], [7]. Hence, we divide all the xpi \u2019s into two sets depending on the proportion of lesion pixels: lesion-sparse patch (proportion \u2264 0.5) and lesion-dense patch (proportion > 0.5). Patches that differ in label distributions, i.e., in lesion-sparse and lesion-dense patches, should have distinct representations in the desired feature space, thus automatically forming a negative pair. Inspired by [19], we regularize the\nfeature map for each patch into a one-dimensional vector by:\nfpi =\n\u2211 h,w F p i \u25e6 y\np i\u2211\nh,w y p i\n, (2)\nwhere Fpi represents the activation map of patch y p i in the penultimate layer, and \u25e6 refers to the element-wise multiplication operation. After obtaining the feature vectors, the contrastive loss for a particular fpi and the patch-wise density loss Lpd for the entire mini-batch is formulated as:\nLpd = 1 |M| \u2211\nfpi\u2208M\nL(fpi ,P(p),N (p)), (3)\nwith\nL(fpi ,P(p),N (p)) = \u22121 |P(p)| \u2211 fqi\u2208P(p) log exp(sim(fqi ,f p i )/\u03c4)\u2211\nfki \u2208P(p)\u222aN (p) exp(sim(fki ,f p i )/\u03c4)\n, (4)\nwhere | \u00b7 | counts the cardinality of a set, P(p) denotes the set of positives that excludes fpi , N (p) is the corresponding negative set, andM represents the set of stored features in every mini-batch. The cosine similarity function sim(\u00b7, \u00b7) is applied here to measure the similarity between two feature vectors, and \u03c4 stands for the temperature. Considering the constrained computing resources, we perform a random sampling of all positive and negative patches and maintain a memory bankM to store the features in every mini-batch."
        },
        {
            "heading": "3.2 Patch-wise Edge-aware Loss",
            "text": "To enable efficient detection of lesion locations, it is essential to accurately identify the boundaries of the region of interest and distinguish it from the surrounding structures. In order to improve the model\u2019s sensitivity in recognizing boundaries between the foreground and background, we implement a patch-wise edge-aware loss to dynamically analyze the pixels around the lesion boundaries.\nGiven the patches (xpi ,y p i ), morphological operations are exploited to extract the inner and outer contour masks of each patch\u2019s binary GT mask ypi . To derive the inner contour which only contains lesion pixels that lie inside the edge, the eroded label map is subtracted from the original binary mask ypi , whereas the outer contour is acquired by subtracting ypi from the dilated one. Specifically, for lesion-dense patches, the iterations for dilation and erosion are set as two. In contrast, since lesion-sparse patches often involve scattered and tiny pathological spots together with a wide area of background, the iterations for dilation and erosion are set as five and one, respectively, to gain extra information about the background. We compose the patches together to form the final inner and outer contour masks after the morphological operations, demonstrated in Figure 2.\nSimilar to (2), the normalized edge-related features fei and background-related features f b i are obtained from the activation map with masked average pooling over inner and outer contour masks. Finally, the patch-wise edge-aware loss Lpe is defined as:\nLpe = 1 2|B| \u2211\nft\u2208{fei ,fbi}\nL(f t,P(t),N (t)). (5)\nHere, B denotes the images in each batch. If f t is an edge-related feature, then P(t) is the set of other edgerelated features and N (t) designates all the background-related features in a mini-batch, and vice versa. The model is expected to develop the necessary ability to distinguish pixels around the border regions of pathological areas, especially for those fine-grained lesions that are strenuous to detect by adopting the patch-wise edge-aware loss Lpe."
        },
        {
            "heading": "4 Numerical Results",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "To validate the efficacy of our proposed method, we leverage the Indian diabetic retinopathy image dataset (IDRiD) [20] dataset for experiments. For our specific segmentation task, we utilize the subset which contains annotations for hard exudates. This subset consists of 81 color fundus JPEG images with a resolution of 4288\u00d72848, of which 54 are for training. Due to the limited size of the dataset, we take advantage of several data augmentation techniques, including random horizontal flipping, rotation, and brightness and contrast adjustment to prevent overfitting. In practice, the rotation degree is randomly selected from -180\u25e6 to +180\u25e6, and the scales of brightness and contrast variation are both from 50% to 150%. All images are resized and cropped into 256\u00d7256 pixels in size as network inputs."
        },
        {
            "heading": "4.2 Experimental Setup",
            "text": "The following state-of-the-art image segmentation models are employed as backbones: U-Net [8], UNet++ [9], ResUnet [13], and CE-Net [11]. Each model is optimized using Adam optimizer with a batch size of 8. The initial learning rate is set as 10\u22123 and decays by 0.1 after every 80 epochs. For the hyper-parameters, every image is partitioned into 16 \u00d7 16 patches, and the temperature parameter \u03c4 is set as 0.05. The \u03b1 and \u03b2 in the total loss function Ltotal are set to be 0.02 and 0.1, respectively. In the training process, all models will be trained for 240 epochs to ensure convergence. The experiments are conducted based on the PyTorch library using an NVIDIA GeForce RTX 3090 GPU [21]."
        },
        {
            "heading": "4.3 Results and Discussion",
            "text": "The quantitative results of the proposed loss function utilizing different backbones are shown in Table 1, measured in four metrics: precision, recall, F1 score, and IoU. Experiments show that the proposed loss significantly outperforms the BCE loss of the compared networks in most cases. Although the BCE loss achieves better precision in U-Net and CE-Net, the superior performance of the proposed loss yields in terms of F1 score and IOU indicates a better alignment with the ground truth.\nThe qualitative results of two instances from the testing set are visualized in Figure 3. Notably, our proposed loss demonstrates the ability to effectively recognize small lesion spots and unclear boundaries while avoiding misclassification of the optic disc."
        },
        {
            "heading": "5 Conclusions",
            "text": "The study demonstrates that integrating patch-wise lesion density and edge information through the supervised contrastive loss significantly enhances the performances of deep neural networks on the automated detection of hard exudates. In the future, we plan to extend the application of the proposed loss to accurately detect and segment other DR lesions. Additionally, applying deep learning approaches with the proposed loss to cardiac segmentation is promising and warrants further investigation as well [22]."
        },
        {
            "heading": "6 Acknowledgement",
            "text": "This work was partially supported by HKRGC Grants No. CUHK14301718, CityU11301120, CityU Grant 9380101, CityU Grant 11309922, N CityU214/19, C1013-21GF. We thank COCHE (https://www.hkcoche.org/) for the support of this study."
        }
    ],
    "title": "A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection",
    "year": 2023
}