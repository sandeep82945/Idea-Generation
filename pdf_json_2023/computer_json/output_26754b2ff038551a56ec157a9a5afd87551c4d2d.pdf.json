{
    "abstractText": "To address the issues of insufficient accuracy and low training efficiency in general musical emotion prediction models, we propose the muSi-ABC architecture for predicting music emotions. Specifically, in the feature extraction stage of music emotions, we use a benchmark feature set to ensure that the extracted music emotion features adhere to standardization. In the prediction stage, we introduce the muSi-ABC architecture which first utilizes a 2D-ConvNet (two dimensional-Convolutional Neural network) to extract partial critical features in music emotions. Then, the BiLSTM (Bi-directional Long Short Term Memory) neural network is employed to learn contextual sequential information of past and future music emotions from the obtained partial critical features. Furthermore, the SA (Self-Attention) module is applied to obtain the complete critical features highly relevant to music emotions, thereby improving prediction accuracy and training efficiency. Through ablation experiments conducted at different time term lengths, the roles of ConvNet model and SA module, as well as the advantages of the proposed muSi-ABC architecture over other ablated models in terms of training efficiency and prediction accuracy, are verified. Additionally, it is observed that representing music emotions using long term feature information for the same song can enhance prediction accuracy. Finally, contrast experimental results demonstrate that the proposed architecture outperforms other benchmark methods in terms of prediction accuracy. Moreover, it is validated that the outlier points contained in the music emotions features extracted based on the benchmark feature set help discover the variations trends of music emotions. INDEX TERMS Predicting musical emotions; Long term dependency; Partial critical features; Complete critical points",
    "authors": [
        {
            "affiliations": [],
            "name": "Jing Yang"
        }
    ],
    "id": "SP:568f4d59f25ae423c83cb029da3adf563783f735",
    "references": [
        {
            "authors": [
                "R. Chiragkumar S"
            ],
            "title": "Chord Recognition-Music and Audio Information Retrieval[J",
            "venue": "arXiv preprint arXiv:2105.07019,",
            "year": 2021
        },
        {
            "authors": [
                "Z Hu",
                "L Chen",
                "Y Luo"
            ],
            "title": "Eeg-based emotion recognition using convolutional recurrent neural network with multi-head self-attention[J",
            "venue": "Applied Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "L Jiang",
                "H Liu",
                "H Zhu"
            ],
            "title": "Improved YOLO v5 with balanced feature pyramid and attention module for traffic sign detection[J",
            "venue": "MATEC Web of Conferences,",
            "year": 2022
        },
        {
            "authors": [
                "L Cai",
                "S Ferguson",
                "H Lu"
            ],
            "title": "Feature Selection Approaches for Optimising Music Emotion Recognition Methods[J",
            "venue": "arXiv preprint arXiv:2212.13369,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Xiang"
            ],
            "title": "Computer analysis and automatic recognition technology of music emotion[J",
            "venue": "Mathematical Problems in Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "X. Yu"
            ],
            "title": "Adaptability of Simple Classifier and Active Learning in Music Emotion Recognition[C]//The",
            "venue": "4th International Conference on Electronics, Communications and Control Engineering",
            "year": 2021
        },
        {
            "authors": [
                "C Huang",
                "Q. Zhang"
            ],
            "title": "Research on Music Emotion Recognition Model of Deep Learning Based on Musical Stage Effect[J",
            "venue": "Scientific Programming,",
            "year": 2021
        },
        {
            "authors": [
                "J Dutta",
                "D. Chanda"
            ],
            "title": "Music emotion recognition in assamese songs using mfcc features and mlp classifier[C]//2021 International Conference on Intelligent Technologies (CONIT)",
            "year": 2021
        },
        {
            "authors": [
                "D Dang W",
                "M Lv D",
                "M Li R"
            ],
            "title": "Multilayer network-based CNN model for emotion recognition[J",
            "venue": "International Journal of Bifurcation and Chaos,",
            "year": 2022
        },
        {
            "authors": [
                "N Satayarak",
                "C. Benjangkaprasert"
            ],
            "title": "On the Study of Thai Music Emotion Recognition",
            "venue": "Based on Western Music Model[C]//Journal of Physics: Conference Series. IOP Publishing,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Liu"
            ],
            "title": "Neural Network Technology in Music Emotion Recognition[J",
            "venue": "International Journal of Frontiers in Sociology,",
            "year": 2021
        },
        {
            "authors": [
                "F Hasanzadeh",
                "M Annabestani",
                "S. Moghimi"
            ],
            "title": "Continuous emotion recognition during music listening using EEG signals: A fuzzy parallel cascades model[J",
            "venue": "Applied Soft Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Z Huang",
                "S Ji",
                "Z Hu"
            ],
            "title": "ADFF: Attention Based Deep Feature Fusion Approach for Music Emotion Recognition[J",
            "venue": "arXiv preprint arXiv:2204.05649,",
            "year": 2022
        },
        {
            "authors": [
                "J Qiu",
                "L Chen C",
                "T. Zhang"
            ],
            "title": "A novel multi-task learning method for symbolic music emotion recognition[J",
            "venue": "arXiv preprint arXiv:2201.05782,",
            "year": 2022
        },
        {
            "authors": [
                "K Treerattanapitak",
                "C. Jaruskulchai"
            ],
            "title": "Outlier detection with possibilistic exponential fuzzy clustering[C]//2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)",
            "year": 2011
        },
        {
            "authors": [
                "M Soleymani",
                "N Caro M",
                "M Schmidt E"
            ],
            "title": "1000 songs for emotional analysis of music[C]//Proceedings of the 2nd ACM international workshop on Crowdsourcing for multimedia",
            "year": 2013
        },
        {
            "authors": [
                "P Kantan",
                "G Spaich E",
                "S. Dahl"
            ],
            "title": "A technical framework for musical biofeedback in stroke rehabilitation[J",
            "venue": "IEEE Transactions on Human- Machine Systems,",
            "year": 2022
        },
        {
            "authors": [
                "B Bahmei",
                "E Birmingham",
                "S. Arzanpour"
            ],
            "title": "CNN-RNN and data augmentation using deep convolutional generative adversarial network for environmental sound classification[J",
            "venue": "IEEE Signal Processing Letters,",
            "year": 2022
        },
        {
            "authors": [
                "K Zhang",
                "Z Cao",
                "J. Wu"
            ],
            "title": "Circular shift: An effective data augmentation method for convolutional neural network on image classification[C]//2020",
            "venue": "IEEE International conference on image processing (ICIP)",
            "year": 2020
        },
        {
            "authors": [
                "W Xue",
                "C Cucchiarini",
                "R van Hout"
            ],
            "title": "Acoustic correlates of speech intelligibility[J]. The usability of the eGeMAPS feature set for atypical speech, 2019",
            "year": 2019
        },
        {
            "authors": [
                "D Deutsch",
                "L Ray",
                "M Dolson"
            ],
            "title": "Computer evaluation of musical performance from the acoustic signal: An exploratory study on performance anxiety[J",
            "venue": "The Journal of the Acoustical Society of America,",
            "year": 1990
        },
        {
            "authors": [
                "A Wadhawan",
                "A. Aggarwal"
            ],
            "title": "Towards emotion recognition in Hindi- English code-mixed data: A transformer based approach[J",
            "venue": "arXiv preprint arXiv:2102.09943,",
            "year": 2021
        },
        {
            "authors": [
                "M Sajid",
                "M Afzal",
                "M. Shoaib"
            ],
            "title": "Multimodal Emotion Recognition using Deep Convolution and Recurrent Network[C]//2021 International Conference on Artificial Intelligence (ICAI)",
            "year": 2021
        },
        {
            "authors": [
                "S Chowdhury",
                "V Praher",
                "G. Widmer"
            ],
            "title": "Tracing back music emotion predictions to sound sources and intuitive perceptual qualities[J",
            "venue": "arXiv preprint arXiv:2106.07787,",
            "year": 2021
        },
        {
            "authors": [
                "A Aljanaki",
                "H Yang Y",
                "M. Soleymani"
            ],
            "title": "Developing a benchmark for emotional analysis of music[J",
            "venue": "PloS one, 2017,",
            "year": 2017
        },
        {
            "authors": [
                "F Korzeniowski",
                "O Nieto",
                "M Mccallum"
            ],
            "title": "et al.Mood Classification Using Listening Data[J",
            "year": 2020
        },
        {
            "authors": [
                "H Yang Y",
                "H. Chen H"
            ],
            "title": "Music emotion recognition[M",
            "venue": "CRC Press,",
            "year": 2011
        },
        {
            "authors": [
                "M Schedl",
                "E G\u00f3mez",
                "J. Urbano"
            ],
            "title": "Music information retrieval: Recent developments and applications[J",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2014
        },
        {
            "authors": [
                "L. Turchet",
                "J. Pauwels"
            ],
            "title": "Music emotion recognition: intention of composers-performers versus perception of musicians, nonmusicians, and listening machines",
            "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "S Gomez-Canon J",
                "E Cano",
                "T Eerola",
                "et"
            ],
            "title": "al.Music Emotion Recognition: Toward new, robust standards in personalized and contextsensitive applications[J].IEEE Signal Processing Magazine, 2021,38(6):106-144. Yang Jing was born in Zaozhuang, Shandong Province in 1986. She graduated from the Conservatory of Music, Shandong Normal University and is a member of Shandong Musicians Association",
            "year": 2011
        }
    ],
    "sections": [
        {
            "text": "VOLUME XX, 2017 1\nINDEX TERMS Predicting musical emotions; Long term dependency; Partial critical features; Complete critical points\nI. INTRODUCTION Music is a language that can express emotions, specifically, the composers and performers express their inner emotions through music, and the listeners resonate with the emotions expressed in the music, leading to an understanding of the emotional essence of the music. Musical emotions are the subjective description of one's inner psychological state while listening to music, which is influenced by a combination of internal subjective factors and external objective factors [1][31]. Musical emotions evolve over time as the melody, harmony, and rhythm of the music change, and they encompass subjectivity and complexity, as well as the temporal and continuous nature of music. The emotional features of music are complex and diverse, providing listeners with rich emotions. While humans have the ability to perceive the rich emotions in music, computers are still unable to do so. Therefore, predicting the emotions expressed in music poses a great challenge for computers. Computers attempt to develop the ability to predict musical emotions like humans by intelligent computation [32]. Specifically, by using neural networks, the computer can analyze the features in music emotions that are input into the model, thus predicting the music emotions. At\npresent, the prediction network mainly analyzes the input musical emotion features through the recurrent neural network (RNN) and identifies the music emotions. In addition, different time slices of a song represent different emotional forms, and in order to find the critical information representing musical emotions in a slice, the convolutional neural network (ConvNet) is introduced into the RNN to effectively capture critical musical emotion information within partial time slices [2]. Furthermore, there is a different correlation between the musical emotion feature information contained in different time slices of the song and the musical emotion. In order to capture the most relevant feature information related to musical emotions, the attention module [3] is introduced into the neural network, which can effectively capture the feature information most relevant to the musical emotion in the complete data and thereby improve the accuracy of emotions predicting. Recent related studies focus on the design of network models based on RNN, with emphasis on the impact of partial and complete critical information on musical emotions. Additionally, most studies verify the performance of the model by predicting the musical emotions of labeled songs [33][34].\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nIn practical applications, people predict the main melody of a song through the auditory system. By considering the relevance of the context and combining the emotional information obtained with the stored musical emotion memory in the brain, they analyze the complete critical musical emotion information. This process allows humans to predict the emotions expressed in music. Taking inspiration from this, we propose the muSi-ABC architecture, which combines ConvNet, BiLSTM (Bidirectional Long Short-Term Memory), and SA (Self-Attention) models, to simulate the process of predicting musical emotions similar to humans. Specifically, based on ConvNet, the proposed method extracts partial critical features of musical emotion, uses BiLSTM neural network to learn the context sequences of musical emotions past and future from extracted partial critical features, and introduces SA mechanism to obtain complete critical features information highly relevant to musical emotions. Finally, The contrast and ablation experimental results validate the effectiveness of the proposed method. II. RELATED WORK In studying tasks about musical emotions prediction , the existent models can be divided into two categories: traditional machine learning methods and deep learning methods [35]. Most traditional machine learning methods for predicting musical emotions are statistical probability models. The selection and combination of handcrafted features have a significant impact on the learning effectiveness of the model, making them suitable for handling the limited-sample problems. Initially, researchers often used Support Vector Machines (SVM) or combined that with other statistical probability models to classify musical emotions. Although they achieved good prediction results, there is uncertainty in the emotional classification criteria. To address this issue, Cai et al. [4] first introduced using regression training to solve the music emotions prediction problem. They concatenated the features extracted from different feature tools into 114-dimensional musical features and used Support Vector Regression (SVR) models to identify the Valence and the Arousal of each music sample. Xiang et al.[5] used seven different music features to identify continuous dimensional emotional values based on the SVR model and compared it with the SVM model. The experimental results showed that SVR performed better than SVM in predicting dimensional emotions. In recent years, with the development of deep learning, the accuracy of using deep learning methods to predicting musical emotions has greatly improved [6]. Most deep learning music emotion prediction methods are based on neural network models. The design of the network model affects the prediction accuracy, making it suitable for handling large-sample data problems. The most commonly used neural network models can be divided into three\ncategories: A. RNNs, B. a combination of ConvNets and RNNs, and C. Neural networks with fused attention models.\nA. RNNS Huang et al. [7] incorporated psychoacoustic features into the ComPareE feature set and used LSTM-RNN to model longer term contextual information, capture the temporal emotion features, and predicting musical emotions. Dutta et al. [8] proposed a Deep Bidirectional Long Short-Term Memory Extreme Learning Machine (DBLSTM-ELM) model that combines extreme learning machine to fuse the prediction results of DBLSTM of music emotions with different time intervals, and obtain the final decision. RNNs have performed well in solving temporal problems, but they do not consider the influence of partial critical information on musical emotions. Meanwhile, LSTM is at risk of overfitting during the training phase, and there are issues with low training efficiency and long term dependence.\nB. A COMBINATION OF CONVNETS AND RNNS Naser et al. [9] used two ConvNet-based L3-Net and\nVGGish models with the deep audio embedding method to aggregate high-dimensional spectrogram features for predicting musical emotions, considering the influence of partial critical information. However, ConvNets did not consider the temporality of musical emotions, so the use of the single ConvNet or RNN cannot solve the musical prediction problem well. Dang et al. [10] introduced a deep learning model that combines 2D-ConvNet and RNN to analyze spectrogram features for predicting musical emotions. Satayarak et al. [11] proposed a method that combines transfer learning and CRNN (Convolutional Recurrent Neural Network) to extract emotional features in both the time-frequency domains of spectrograms for speech emotion prediction. Liu et al. [12] introduced a Convolutional Long Short-Term Memory Deep Neural Network (CLDNN) that combines Mel-Frequency Cepstral Coefficients (MFCC) spectrograms and Mel filterbank energy spectrogram features on base of standard acoustic statistics for predicting musical emotions. To address the low training efficiency problem of LSTM, Hasanzadeh et al. [13] found that ConvNet can learn directly from input data in image recognition tasks, thereby reducing the parameter size of spatial structure information and improving training efficiency.\nC. NEURAL NETWORKS WITH FUSED ATTENTION MODELS To address the problem of long term dependence in LSTM,\nHuang et al. [14] proposed a hybrid LSTM model with attention mechanism to alleviate the reduction of learning contextual information with increasing input term over time in music. Traditional attention-based models rely heavily on external information, but the complex and diverse nature of musical emotions means that the overall emotional expression is not simply a simple summation of time and\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nemotional features, but largely depends on the correlation with musical emotional features. To tackle this problem, Jiang et al. [3] introduced a Bidirectional Gate Recurrent Unit (BiGRU) network model with self-attention mechanism, for predicting musical emotions and themes. Compared with\nthe hybrid LSTM model that integrates the traditional attention mechanism, the experimental results showed that the self-attention module exhibits stronger fitting ability and higher training efficiency than the traditional attention model. In summary, considering the temporal and continuous nature of music emotions, BiLSTM is chosen as the basic model (referred to as musi-B) in this paper. To address the problem that LSTM do not consider the influence of partial critical information on musical emotions and have low training efficiency, a ConvNet-BiLSTM (in other words, musi-BC) model is constructed by integrating 2D-ConvNet. For the long term dependency problem of LSTM, a selfattention module is further integrated into the musi-BC model, forming the overall muSi-ABC architecture. By capturing partial critical information, sequential information, and complete critical information of musical emotions, the proposed considerate architecture addresses the limitations of LSTM in predicting long term musical emotions and improves training efficiency. Thus, it provides an effective method for enhancing the accuracy and efficiency of long term musical emotion prediction"
        },
        {
            "heading": "III. MATERIALS AND METHODS",
            "text": ""
        },
        {
            "heading": "A. OVERALL ARCHITECTURE AND FORMAL DEFINITION",
            "text": "The proposed muSi-ABC architecture comprises the twodimensional convolutional layer, the bidirectional long shortterm memory layer, and the self-attention layer (see Figure 1 for the overall model structure). Firstly, each input song is represented as an IM\u00d7N= {i1, i2, ..., iM} of music emotional features, whereM represents the time dimension and N represents the dimension of music emotional features. Furthermore, the output of musi-C (i.e., ConvNet) is denoted as NA\u00d7B . Subsequently, the output of musi-BC is represented as LD\u00d7H . Lastly, the holistic output of muSi-ABC is denoted by AV\u00d7H."
        },
        {
            "heading": "B. BACKBONE MODEL",
            "text": "The proposed muSi-ABC architecture simulates the process of human music prediction and emotional expression. It utilizes the two-dimensional ConvNet model to extract melody slices, the BiLSTM network to obtain emotional context information, and the SA module to combine obtained emotional information with stored emotional memory, resulting in complete critical music emotion information.\na. 2D-ConvNet To obtain partial critical features of musical emotions from the two dimensions of time and music emotion features in the feature matrix, a two-dimensional CNN is used for processing, as shown in Figure 2. Taking the prediction of the continuous emotional values of a song as an example, the musical emotion feature matrix, i.e., IM\u00d7N, is first input into the two-dimensional convolutional layer, which extracts\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nmusic emotion features with a K(3\u00d73) filter while preserving edge information. Then, BatchNorm2d is used for data normalization processing to ensure consistent distribution of the output data after convolution. Next, the ReLU activation function is used to add non-linear factors and enhance the ability of the two-dimensional convolutional layer to express music emotions. Finally, the maximum pooling (MaxPooling) method is selected to reduce the matrix dimension and preserve some critical information in the music emotion features, thus obtaining the feature matrix NA\u00d7B about partial critical music emotions.\nb. BiLSTM LSTM has a unidirectional transmission direction, from the previous time step to the next time step. However, music emotions have strong internal correlations, and the current state is not only related to the previous state but also to the next state. Therefore, the bidirectional LSTM network is constructed using two LSTM layers [15] to predict past and future emotional information in music and model the contextual information of music emotions. The recurrent unit structure of LSTM includes three gates and two states, i.e., the input gate it, the forget gate ft, the output gate ot, the internal state ct, and the candidate state tc , as shown in Figure 3. Assuming that the external state at time t is ht and the external state at the previous time step is ht-1. LSTM combines the previous external state ht-1 with the current input music emotion feature vector nt. The three gate values and the candidate state value of the LSTM recurrent unit are calculated using (1)-(4). The memory unit ct is updated using the forget gate ft and the input gate it through (5), and the output gate ot transfers the emotional information of the internal state to the external state ht through (6).\n1( )t i t i t ii W n U h b    (1)\n1( )t f t f t ff W n U h b    (2)\n1( )t o t o t oo W n U h b    (3) 1( )t c t c t cc tanh W n U h b    (4)\n1t t t t tc f c i c    (5) ( )t t th o tanh c  (6)\nHere, x \u2208 {i, f, o, c} represents the components of Wx, Ux, and bx, Wx is the weight matrix at the current time step, Ux is the weight matrix at the previous time step, and bx is the bias vector, \u03c3 represents the sigmoid function, while tanh represents the hyperbolic tangent function.\nThe BiLSTM model consists of a forward layer and a backward layer of LSTM. (7) and (8) are utilized to extract and retain emotional information from both past and future music. Figure 4 illustrates the structure of the single-layer BiLSTM network. Assuming that the forward layer follows the time order while the backward layer follows the reverse time order, the hidden layer states at time t are defined as h1t and h2t. The output vector lt of the bidirectional Long ShortTerm Memory layer at time t is computed based on the hidden layer states in both directions, as depicted in (9).\n1 1 1 1 1 1( )t t th f U h W n b   (7)\n2 2 2 2 2 1( )t t th f U h W n b   (8)\n1 1 2 2 0)t tt t tl W h W h b   (9) In which,Wx (x\u2208 {1, 2}) represents the weight matrix at the current time step, U1 and U2 represent the weight matrices at the previous and next time steps, f represents the activation function of the hidden layer, Wtx (x \u2208 {1, 2}) represents the weight matrix of the hidden layer state at the current time step, and bx (x\u2208 {0, 1, 2}) represents the bias vector. After two layers of BiLSTM, a serialized music emotion feature matrix LD\u00d7H is obtained.\nc. Self-Attention The musical emotion feature matrix LD\u00d7H, which represents the output of the bidirectional LSTM layer, is\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\ninputted to the self-attention layer. Each music emotion feature vector in matrix L at every time step is treated as a query vector and compared with the music emotion feature vectors at different time steps in the song to calculate similarity scores. After performing weighted averaging, the complete critical feature information about music emotions is obtained. The self-attention module structure, with the number of rows and columns labeled outside each box, is illustrated in Figure 5. The computation process is as follows (1) For the input matrix L, the linear mapping is performed to obtain the Q, K, and V matrices, as shown in (10)-(12). K H K D\nqQ W L   (10) K H K D kK W L   (11) V H V D\nvV W L   (12)\nIn which, Wq, Wk, and Wv are parameter matrices for linear mapping, and Q, K, and V are matrices composed of query vectors, key vectors, and value vectors, respectively. (2) The dot product of the transpose matrices of Q and K produces the musical emotion feature similarity score matrix ScoreH\u00d7H. To address the issue of imbalanced softmax distributions resulting in small gradients when the dot product result is large, the dot product result is smoothed by scaling it with the square root K of the row-wise scaling of matrix Q, as shown in (13).\nTQKScore K  (13)\n(3) Softmax is applied to normalize the musical emotion similarity score matrix Score into the probability distribution matrix. The probability distribution matrix is then multiplied element-wise with matrix V to obtain the complete critical feature matrix AV\u00d7Haboutmusic emotions, as shown in (14).\n( )A VSoftMax Score (14)"
        },
        {
            "heading": "C. LOSS FUNCTION",
            "text": "As an essential part of deep learning-based model training, loss functions such as Mean Squared Error (MSE) and Mean Absolute Error (MAE) are commonly used in regression problems. MAE is insensitive to outliers, and the gradient does not decrease with the decrease of the loss value during the gradient update process, which is not\nconducive to model convergence. On the contrary, MSE is more sensitive to outliers, and the gradient decreases as the loss value decreases during the gradient update process, which is beneficial to model convergence. Outliers refer to a very small portion of data with distribution patterns significantly different from the main data, often containing the trends of things. Therefore, outliers cannot be simply equated with noise [16]. Considering the complex and diverse features of music emotions, outliers in music emotion information may represent sudden changes in music emotions, but they may also be noise data. Considering the sensitivity to outliers and convergence, MSE is chosen as the loss function for model training in this paper. Its calculation is shown in (15):\n2\n1\n1 \u02c6( ) ( ) N\ni i i MSE i y y N    (15) Where N is the total number of musical emotion data points, yi is the ground truth of the i-th musical emotion data point, and \u02c6iy is the regression value of the i-th music emotion data point. IV. EXPERIMENTS"
        },
        {
            "heading": "A. EXPERIMENT SETTINGS",
            "text": "The experiment is conducted based on the audio data of the EmoMusic dataset [17], the DEAM dataset [18] and PMEmo dataset [30]. To ensure that the musical emotion feature information analyzed by the proposed muSi-ABC model adheres to standardization, the eGeMAPS feature set, which has been validated and achieved significant results by researchers, was selected as the standard. The music emotional features were extracted from the audio data based on this feature set.\na. dataset The EmoMusic dataset, DEAM dataset, and PMEmo dataset were used in the experiment to train and evaluate the effectiveness of the proposed muSi-ABC architecture in predicting musical emotions. The EmoMusic dataset consists of 744 songs, and 45-second music slices were extracted starting from 15 seconds of each song. The slices were annotated by Amazon Mechanical Turk workers, with at least 10 annotations per slice. Each slice was labeled with a static Valence-Arousal (VA) value and dynamic VA values at intervals of 0.5 seconds. The DEAM dataset expanded the EmoMusic dataset to 1744 songs. Besides the increased number of songs, the annotation mode and the length of music slices remained the same. To validate the generalizability of the proposed method, in addition to the two Perceived-style collected datasets mentioned above, the Induced-style collected PMEmo dataset was also used. The PMEmo dataset contains 794 full songs, and similar to the above two datasets, the annotation in the PMEmo dataset was done with the slider to collect dynamic annotations at a sampling rate of 2 Hz. Additionally, annotators should make a static annotation for the whole music excerpt on a\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nnine-point scale after finishing dynamic labeling. To obtain long term musical emotion information, static musical emotions were predicted based on continuous time, and the ground truth labels were normalized to the [0, 1] range. In addition, the whole dataset was randomly divided into two parts, i.e., training set and testing set, in an 8:2 ratio.\nb. feature extraction The eGeMAPS feature set was used as the standard for extracting music emotion features. This feature set is an audio emotion feature set that consists of 88 statistical acoustic features derived from 7 spectral features, 11 frequency-related features, and 7 energy/amplitude-related features through statistical calculations [22]. The features in the set and their correlations have been theoretically and practically validated, making it a standardized audio emotional feature set [23]. It is widely used in research related to audio emotion prediction [24] and music emotion prediction [25]. Based on the eGeMAPS feature set, the OpenSmile tool was used to extract continuous-time music emotion features from the audio dataset. The time term length, which is defined as the total length of different time sequences based on different frame intervals of the same song. Larger frame intervals generate long-term sample data, while smaller frame intervals generate long-term sample data. In this paper, a simplified variation approach was taken, considering only sample features of different term lengths for music emotion prediction, without considering the rationality of frame intervals, and ignoring the last frame information. Each song was represented in the form of time \u00d7 features and saved in the .csv file. In addition, the advantages of feeding feature sets into the two-dimensional ConvNets are as follows: 1) Efficient feature extraction: Feature sets (such as eGeMAPS in this paper) provide pre-computed audio features that have been carefully selected and processed to capture key information from the audio. Using feature sets instead of raw audio signals reduces the computational requirements and number of parameters in the network, thus improving efficiency and training speed. 2) Enhanced representational power: Feature sets contain rich audio features that capture information related to different frequencies, temporal features, and semantics. By employing the two-dimensional ConvNets, we can leverage convolutional layers to model the spatial relationships of these features locally and globally, extracting more discriminative representations. This helps capture the structure, patterns, and crucial emotion-related information in the audio. 3) Network interpretability: Using feature sets as input makes it easier to understand and interpret the network's predictions. Since the features in the set have semantic interpretations, we can infer the network's attention to different audio attributes and emotion dimensions based on these features, thereby increasing the model's interpretability.\nc. model parameters and evaluation metrics The experimental setup included using the Adam optimization algorithm, a weight decay coefficient of 0.0001, a learning rate of 0.0001, and a batch size of 4 samples. ReLU was used as the activation function in the model, and the number of training epochs was set to 80. Based on the eGeMAPS feature set, 88-dimensional features were extracted from the source music. Taking a time term of 99 as an example, the specific parameters of the model are shown in Table I, where both the ConvNet and BiLSTM parts consist of two neural network layers. Connection was used to avoid repeated representation of input and output layers since the output of the previous layer serves as the input for the next layer. The SA module used Q=K=V, and the output layer's temporal dimension information was aggregated using the summation method. The batch size, which is the first dimension of each tensor and has the same value, is not presented in the model training parameters. Root Mean Square Error (RMSE) was used as the accuracy metric, and R2 (R-Squared) was used as the fitting metric. Additionally, we also used the concordance correlation coefficient (CCC), which focuses more on the dynamic trends of prediction results, as the metric."
        },
        {
            "heading": "B. ABLATION EXPERIMENT",
            "text": "Due to the uncertainty of whether the proposed muSiABC architecture can improve the training efficiency and accuracy of music emotion prediction, experiments were conducted to verify the effectiveness of the muSi-ABC architecture and its components by extracting music emotion features with different temporal distance lengths from the EmoMusic dataset. The muSi-ABC architecture and its ablation models were tested. Firstly, the BiLSTM was used as the baseline model. Then, the two-dimensional ConvNet model and SA module were added to the BiLSTM separately. Finally, the ablation models obtained were BiLSTM (i.e., musi-B), ConvNet-BiLSTM (i.e., musi-BC), and BiLSTM-SA (i.e., muSi-AB). Ablation experiments were conducted to evaluate the prediction accuracy (i.e., RMSE), goodness of fit (i.e., R2), and training efficiency on data with time term lengths of 99, 199, and 299. The RMSE and R2 of each model with the minimum loss during training were compared, and the training efficiency (TE)\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nwas calculated as the ratio of the total training time to the total number of training epochs, representing the time required for one training epoch in seconds. Based on the different term lengths, the regression evaluation results of the ablation models in the valence dimension and arousal dimension are shown in Table II and Table III. The proposed muSi-ABC architecture outperformed the other three ablation models in terms of prediction performance on datasets with three different term lengths. Moreover, the prediction accuracy improved as the term length increased.\nTABLE II THE REGRESSION EVALUATION RESULTS OF EACH ABLATION MODEL IN THE VALENCE DIMENSION\nModelValance /Term length musi-B musi-BC muSi-AB muSi-ABC\n99 RMSE 0.0893 0.0894 0.0850 0.0850 R2 0.497 0.466 0.516 0.515 TE/s 5.5 3.9 5.8 4.3 199 RMSE 0.0921 0.0880 0.0839 0.0832 R2 0.499 0.503 0.531 0.555 TE/s 9.0 4.7 9.2 5.2 299 RMSE 0.0968 0.0862 0.0827 0.0825 R2 0.351 0.456 0.584 0.567 TE/s 12.3 5.8 12.4 6.1\nTABLE III THE REGRESSION EVALUATION RESULTS OF EACH ABLATION MODEL IN THE AROUSAL DIMENSION\nModelArousal /Term length musi-B musi-BC muSi-AB muSi-ABC\n99 RMSE 0.0853 0.0811 0.0755 0.0755 R2 0.616 0.632 0.695 0.646 TE/s 5.5 3.8 5.9 4.1 199 RMSE 0.0837 0.0781 0.0744 0.0744 R2 0.622 0.675 0.716 0.698 TE/s 8.8 4.8 9.1 5.1 299 RMSE 0.0852 0.0763 0.0729 0.0725 R2 0.594 0.679 0.733 0.712 TE/s 12.2 5.8 12.8 6.4\na. analysis of the effectiveness of the two-dimensional ConvNet and Self Attention Taking Arousal at the term length of 99 as an example, compared to using musi-B, musi-BC and muSi-AB reduced the RMSE by 0.0042 and 0.0098, respectively. The result from Table III indicates that the inclusion of the twodimensional ConvNet and Self Attention has a positive effect on improving the prediction accuracy. In terms of the model performance of fusing SA across the three term lengths, BiLSTM-SA reduced the RMSE by 0.0098, 0.0093, and 0.0123, respectively, compared to BiLSTM. In terms of the model performance of fusing ConvNet across the three term lengths, ConvNet-BiLSTM reduced the RMSE by 0.0042, 0.0056, and 0.0089, respectively, compared to BiLSTM. Additionally, the training efficiency decreased by 1.7, 4, and 6.4 for ConvNet-BiLSTM across the three term lengths. These results further demonstrate the beneficial impact of combining Self Attention and the two-\ndimensional ConvNet in enhancing the overall performance of the final muSi-ABC architecture.\nb. analysis of RMSE and R2 curves Combining Table III and taking Arousal at a term length of 99 as an example, the RMSE of the overall muSi-ABC architecture is reduced by 0.0098, 0.0056, and 0 compared to musi-B, musi-BC, and muSi-AB, respectively, as shown in Figure 6.\nIn Figure 6, although the final muSi-ABC architecture and muSi-AB architecture have the same RMSE under the minimum loss, the overall trend of the muSi-ABC architecture's RMSE is lower than that of muSi-AB. This result demonstrates that the overall prediction accuracy of the muSi-ABC architecture is higher than that of muSi-AB. The R2 of the muSi-ABC architecture relative to musi-B, musi-BC, and muSi-AB is increased by 0.03, 0.014, and - 0.049, respectively, as shown in Figure 7.\nAlthough the final muSi-ABC architecture is 0.049 lower than muSi-AB, the R2 is influenced by multiple factors. This result only indicates that the fitting performance of the proposed muSi-ABC architecture is slightly lower than that\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nof muSi-AB and does not affect the comparison of their prediction accuracy.\nc. analysis of RMSE and training efficiency for different term lengths Combining Table III and taking Arousal as an example, the variation of RMSE under the minimum loss for each model with increasing distance length is shown in Figure 8.\nAs shown in Figure 8, for musi-B, the RMSE at a term\nlength of 299 is 0.0015 higher than that at a term length of 199. This result indicates a decrease in learning ability for LSTM beyond a certain term length. Using muSi-AB, musi-BC, and the final muSi-ABC architecture, relative to a term length of 99, the RMSE values at term lengths of 199 and 299 are reduced by 0.0011, 0.0026, 0.003, 0.0048, 0.001, and 0.003, respectively. This result demonstrates that using long term data compared to short term data can improve prediction accuracy. As the term length increases, the prediction accuracy of the proposed muSi-ABC architecture gradually surpasses other ablated models, further confirming the muSi-ABC architecture's ability to improve the accuracy of long term musical emotion prediction. In terms of training efficiency, Figure 9 clearly shows that the training efficiency of each model gradually increases with the term length.\nCompared to muSi-AB, the muSi-ABC architecture's training efficiency is reduced by 1.8, 4, and 6.4 for different term lengths. This result demonstrates that integrating ConvNet can reduce model complexity and improve training efficiency. In conclusion, although musi-BC has lower training efficiency than the muSi-ABC architecture, its RMSE is higher. muSi-AB has a similar RMSE to the muSi-ABC architecture, but lower training efficiency. Therefore, the construction of the muSi-ABC model, which simulates the perception process of music for people to express emotions, has certain advantages in predicting continuous-time, long term static music emotions, and can improve the accuracy and training efficiency of long term music emotion prediction.\nd. comparison of prediction accuracy between BiLSTM and ConvNet with different numbers of layers Adjusting the number of layers in BiLSTM and ConvNet networks to achieve higher music emotion prediction accuracy in the final muSi-ABC architecture. Firstly, experiments were conducted to determine the optimal number of layers in the BiLSTM network. Based on the determined number of BiLSTM layers, the number of ConvNet layers in the muSi-ABC model was determined. To ensure a suitable time term length, the RMSE at a term length of 199 was chosen as the evaluation metric for the model's layer configuration. The RMSE of the BiLSTM network represents the prediction accuracy when using that network alone, while the RMSE of the 2D-ConvNet network represents the prediction accuracy when adjusting the number of ConvNet layers in the muSi-ABC architecture based on the determined number of BiLSTM layers. The experiments compared the prediction accuracy of BiLSTM networks with 1 to 3 layers and the prediction accuracy of the muSi-ABC model using 1 to 3 layers of ConvNet. The goal was to identify the impact of the number of layers in BiLSTM and ConvNet on prediction\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\naccuracy. The experimental results are summarized in Table IV.\nTABLE IV COMPARISON OF PREDICTION ACCURACY FOR DIFFERENT BILSTM AND CONVNET LAYER NUMBERS\nModel Layer numbers Valence RMSE Arousal RMSE\nBiLSTM 1 0.1013 0.0994 2 0.0921 0.0837 3 0.0961 0.0856 ConvNet 1 0.0866 0.0762 2 0.0832 0.0744 3 0.0895 0.0754\nTaking Arousal as an example, the RMSE of the BiLSTM network with two layers was reduced by 0.0157 and 0.0019 compared to the network with one layer and three layers, respectively. Similarly, the RMSE of the ConvNet network with two layers was reduced by 0.0018 and 0.001 compared to the network with one layer and three layers, respectively. These results indicate that the prediction accuracy of the BiLSTM network with two layers and the ConvNet network with two layers is higher than the other configurations, and increasing the number of layers does not necessarily improve the prediction results. Therefore, based on the BiLSTM network with two layers and the ConvNet network with two layers, the ConvNetBiLSTM model was constructed and combined with the self-attention model to form the muSi-ABC architecture for music emotion regression training.\ne. the impact of loss functions on predicting musical emotions To verify whether the outliers in the music emotion features obtained from the eGeMAPS feature set are the turning points that affect the trend of music emotion changes, considering the complex and diverse nature of music emotion features and the sensitivity to outliers, MAE and MSE were used as the model training loss functions in the muSi-ABC architecture. RMSE was used as the evaluation metric for prediction accuracy. The experimental results are shown in Figure 10.\nFIGURE 10.The impact of loss functions on predicting musical emotions.\nIt is clear that using MSELoss for Valence and Arousal achieves good prediction accuracy compared to using MAELoss. Therefore, the music emotion features extracted based on the eGeMAPS feature set have standardization, and the outliers in the information contain the trend of music emotion changes, which can improve the model's prediction accuracy."
        },
        {
            "heading": "C. CONTRAST EXPERIMENT",
            "text": "To further validate the performance effectiveness, the proposed method is compared with benchmark methods and state-of-the-art music emotion prediction methods using the EmoMusic dataset and the DEAM dataset, based on the same evaluation metrics. The following provides an overview of each comparative method: MLR, BLSMT-RNN, SVR, and GPR [26]: These four models represent the benchmark prediction methods used for training and evaluating the EmoMusic dataset by the Technical University of Munich, Aizu University, and Utrecht University, respectively. ConvNet_D-SVM [27]: This method explores the contextual information of emotion computation by increasing the receptive field of the network layers using dilated convolution (ConvNet_D) and feeding it into an SVM regression model. AC2DConv [28]: This method analyzes audio features represented by a combination of raw audio, audio signals, and spectrograms using an audio and computed 2D convolution (AC2DConv) network model. ResNets-audioLIME [29]: This method combines the source separation explainer audioLIME with residual networks (ResNets) to analyze intermediate perceptual features and spectrogram features. The evaluation metric results of these methods are shown in Table V.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJing Yang: musi-ABC for Predicting Musical Emotions\n8 VOLUME XX, 2017\nCompared to other methods, the proposed method achieves the lowest RMSE and highest R2 in music emotion prediction tasks. It improves the accuracy of music emotion prediction and exhibits the best fitting ability. Furthermore, to validate the generalization ability of the proposed method, comparative experiments were conducted on the PMEmo dataset, and the evaluation metric CCC was used. The results are also shown in Table V. It can be seen that the fitting ability of the proposed method is still the best among the comparison methods, and this result once again proves the effectiveness of the muSi-ABC model."
        },
        {
            "heading": "V. CONCLUSION",
            "text": "With the continuous advancement of music technology research, music emotions prediction has been widely applied in all kinds of fields. Inspired by this, we aim to simulate the perception process of music as people to express emotions. In response to the problems of long term dependencies and low training efficiency in music emotions prediction with LSTM neural networks, a novel and comprehensive network model called muSi-ABC is proposed for regression training of long-term music emotions prediction. Specifically, the proposed model uses the 2D-ConvNet to extract partial critical features of music emotions, employs the BiLSTM neural network to extract sequential information of music emotions from the obtained partial critical features, and utilizes the SA model to dynamically adjust the weights of the obtained sequential information, highlighting the complete critical points of music emotions. The ablation and contrast experimental results demonstrate that the proposed muSi-ABC model can reduce the training time for analyzing the regularities in music emotions information and effectively improve the accuracy of predicting music emotions. In conclusion, the proposed model for predicting musical emotions can capture the regularities of music emotions information from longer continuous durations, thereby improving prediction accuracy and training efficiency, and effectively achieving\nemotions music regression. It provides a new feasible idea for the direction of predicting music emotions. The limitations of this study include the lack of consideration for additional modal information, and this may limit the generalization of the proposed method in capturing music emotions. In future research, we will explore the integration of audio data with listening data, lyrics text, and even video frames for a more comprehensive multimodal music emotion prediction."
        }
    ],
    "title": "musi-ABC for Predicting Musical Emotions",
    "year": 2023
}