{
    "abstractText": "Creating the photo-realistic version of people\u2019s sketched portraits is useful to various entertainment purposes. Existing studies only generate portraits in the 2D plane with fixed views, making the results less vivid. In this paper, we present Stereoscopic Simplified Sketch-to-Portrait (SSSP), which explores the possibility of creating Stereoscopic 3D-aware portraits from simple contour sketches by involving 3D generative models. Our key insight is to design sketchaware constraints that can fully exploit the prior knowledge of a triplane-based 3D-aware generative model. Specifically, our designed region-aware volume rendering strategy and global consistency constraint further enhance detail correspondences during sketch \u2217Equal contribution. encoding. Moreover, in order to facilitate the usage of layman users, we propose a Contour-to-Sketch module with vector quantized representations, so that easily drawn contours can directly guide the generation of 3D portraits. Extensive comparisons show that our method generates high-quality results that match the sketch. Our usability study verifies that our system is preferred by users.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yasheng Sun"
        },
        {
            "affiliations": [],
            "name": "Qianyi Wu"
        },
        {
            "affiliations": [],
            "name": "Hang Zhou"
        },
        {
            "affiliations": [],
            "name": "Tianshu Hu"
        },
        {
            "affiliations": [],
            "name": "Chen-Chieh Liao"
        },
        {
            "affiliations": [],
            "name": "Shio Miyafuji"
        },
        {
            "affiliations": [],
            "name": "Ziwei Liu"
        },
        {
            "affiliations": [],
            "name": "Hideki Koike"
        }
    ],
    "id": "SP:b33e7a4d2a90216f3a8ecee94e110400eb1b0bc1",
    "references": [
        {
            "authors": [
                "Shengqu Cai",
                "Anton Obukhov",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Pix2NeRF: Unsupervised Conditional p-GAN for Single Image to Neural Radiance Fields Translation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3D generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Eric R Chan",
                "Marco Monteiro",
                "Petr Kellnhofer",
                "Jiajun Wu",
                "Gordon Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2021
        },
        {
            "authors": [
                "Shu-Yu Chen",
                "Feng-Lin Liu",
                "Yu-Kun Lai",
                "Paul L Rosin",
                "Chunpeng Li",
                "Hongbo Fu",
                "Lin Gao"
            ],
            "title": "DeepFaceEditing: deep face generation and editing with disentangled geometry and appearance control",
            "venue": "ACM Transactions on Graphics (TOG)",
            "year": 2021
        },
        {
            "authors": [
                "Shu-Yu Chen",
                "Wanchao Su",
                "Lin Gao",
                "Shihong Xia",
                "Hongbo Fu"
            ],
            "title": "Deep- FaceDrawing: Deep generation of face images from sketches",
            "venue": "ACM Transactions on Graphics (TOG) 39,",
            "year": 2020
        },
        {
            "authors": [
                "Tao Chen",
                "Ming-Ming Cheng",
                "Ping Tan",
                "Ariel Shamir",
                "Shi-Min Hu"
            ],
            "title": "Sketch2photo: Internet image montage",
            "venue": "ACM transactions on graphics (TOG) 28,",
            "year": 2009
        },
        {
            "authors": [
                "Wengling Chen",
                "James Hays"
            ],
            "title": "Sketchygan: Towards diverse and realistic sketch to image synthesis",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Xuejin Chen",
                "Sing Bing Kang",
                "Ying-Qing Xu",
                "Julie Dorsey",
                "Heung-Yeung Shum"
            ],
            "title": "Sketching reality: Realistic interpretation of architectural designs",
            "venue": "ACM Transactions on Graphics (TOG) 27,",
            "year": 2008
        },
        {
            "authors": [
                "Yuedong Chen",
                "Qianyi Wu",
                "Chuanxia Zheng",
                "Tat-Jen Cham",
                "Jianfei Cai"
            ],
            "title": "Sem2NeRF: Converting Single-View Semantic Masks to Neural Radiance Fields",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Tali Dekel",
                "Chuang Gan",
                "Dilip Krishnan",
                "Ce Liu",
                "William T Freeman"
            ],
            "title": "Sparse, smart contours to represent and edit images",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2018
        },
        {
            "authors": [
                "Yu Deng",
                "Jiaolong Yang",
                "Jianfeng Xiang",
                "Xin Tong"
            ],
            "title": "Gram: Generative radiance manifolds for 3d-aware image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Mathias Eitz",
                "Ronald Richter",
                "Kristian Hildebrand",
                "Tamy Boubekeur",
                "Marc Alexa"
            ],
            "title": "Photosketcher: interactive sketch-based image synthesis",
            "venue": "IEEE Computer Graphics and Applications 31,",
            "year": 2011
        },
        {
            "authors": [
                "Patrick Esser",
                "Robin Rombach",
                "Bjorn Ommer"
            ],
            "title": "Taming transformers for high-resolution image synthesis",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2021
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde- Farley",
                "Sherjil Ozair",
                "Aaron C. Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative Adversarial Nets",
            "venue": "NeurIPS",
            "year": 2014
        },
        {
            "authors": [
                "Jiatao Gu",
                "Lingjie Liu",
                "Peng Wang",
                "Christian Theobalt"
            ],
            "title": "Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "arXiv preprint",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to- Image Translation with Conditional Adversarial Networks",
            "year": 2017
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2020
        },
        {
            "authors": [
                "Cheng-Han Lee",
                "Ziwei Liu",
                "LingyunWu",
                "Ping Luo"
            ],
            "title": "MaskGAN: towards diverse and interactive facial image manipulation",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Yuhang Li",
                "Xuejin Chen",
                "Feng Wu",
                "Zheng-Jun Zha"
            ],
            "title": "Linestofacephoto: Face photo generation from lines with conditional self-attention generative adversarial networks",
            "venue": "In Proceedings of the 27th ACM International Conference on Multimedia",
            "year": 2019
        },
        {
            "authors": [
                "Yuhang Li",
                "Xuejin Chen",
                "Binxin Yang",
                "Zihan Chen",
                "Zhihua Cheng",
                "Zheng- Jun Zha"
            ],
            "title": "Deepfacepencil: Creating face images from freehand sketches",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia",
            "year": 2020
        },
        {
            "authors": [
                "Nelson Max"
            ],
            "title": "Optical models for direct volume rendering",
            "venue": "IEEE Transactions on Visualization and Computer Graphics 1,",
            "year": 1995
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis",
            "year": 2020
        },
        {
            "authors": [
                "Niranjan D Narvekar",
                "Lina J Karam"
            ],
            "title": "A no-reference perceptual image sharpness metric based on a cumulative probability of blur detection",
            "venue": "In 2009 International Workshop on Quality of Multimedia Experience",
            "year": 2009
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "Giraffe: Representing scenes as compositional generative neural feature fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2021
        },
        {
            "authors": [
                "Roy Or-El",
                "Xuan Luo",
                "Mengyi Shan",
                "Eli Shechtman",
                "Jeong Joon Park",
                "Ira Kemelmacher-Shlizerman"
            ],
            "title": "Stylesdf: High-resolution 3d-consistent image and geometry generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Taesung Park",
                "Ming-Yu Liu",
                "Ting-Chun Wang",
                "Jun-Yan Zhu"
            ],
            "title": "Semantic image synthesis with spatially-adaptive normalization",
            "year": 2019
        },
        {
            "authors": [
                "Elad Richardson",
                "Yuval Alaluf",
                "Or Patashnik",
                "Yotam Nitzan",
                "Yaniv Azar",
                "Stav Shapiro",
                "Daniel Cohen-Or"
            ],
            "title": "Encoding in style: a stylegan encoder for image-to-image translation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
            "year": 2021
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training gans",
            "venue": "Advances in neural information processing systems",
            "year": 2016
        },
        {
            "authors": [
                "Katja Schwarz",
                "Yiyi Liao",
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "Graf: Generative radiance fields for 3d-aware image synthesis",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Yujun Shen",
                "Ceyuan Yang",
                "Xiaoou Tang",
                "Bolei Zhou"
            ],
            "title": "InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs",
            "venue": "TPAMI",
            "year": 2020
        },
        {
            "authors": [
                "Jingxiang Sun",
                "Xuan Wang",
                "Yichun Shi",
                "Lizhen Wang",
                "Jue Wang",
                "Yebin Liu"
            ],
            "title": "IDE-3D: Interactive Disentangled Editing for High-Resolution 3D-aware Portrait Synthesis",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Jingxiang Sun",
                "Xuan Wang",
                "Yong Zhang",
                "Xiaoyu Li",
                "Qi Zhang",
                "Yebin Liu",
                "Jue Wang"
            ],
            "title": "Fenerf: Face editing in neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning. Advances in neural information processing systems",
            "year": 2017
        },
        {
            "authors": [
                "Ting-Chun Wang",
                "Ming-Yu Liu",
                "Jun-Yan Zhu",
                "Andrew Tao",
                "Jan Kautz",
                "Bryan Catanzaro"
            ],
            "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
            "year": 2018
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE transactions on image processing 13,",
            "year": 2004
        },
        {
            "authors": [
                "Shangzhe Wu",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2020
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss",
            "venue": "In Computer Vision (ICCV),",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "\u2217Equal contribution.\nencoding. Moreover, in order to facilitate the usage of layman users, we propose a Contour-to-Sketch module with vector quantized representations, so that easily drawn contours can directly guide the generation of 3D portraits. Extensive comparisons show that our method generates high-quality results that match the sketch. Our usability study verifies that our system is preferred by users.\nKEYWORDS Virtual Character Creation, Cross-Modal Generation"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Sketching is a wonderful activity that can be enjoyed by people of all ages, which can be a valuable tool for both educational and\ncreative purposes. Sketching faces, in particular, can be a means of expressing desired characters or identifying individuals. However, there is often a significant gap between the casual sketches that many people create and realistic portraits. Bridging this gap is a valuable skill that can have many practical applications, such as entertainment through the use of augmented reality (AR) and virtual character design and creation for Metaverse.\nWith the development of generative models [14, 19, 21] and their applications in image translation [18, 38, 41], semantic maps can already be faithfully transformed to real faces [22, 30]. However, facial details such as wrinkles and hairstyles are lost in this kind of representation. As a result, researchers study specifically the problem of sketch-to-portrait generation [4\u20136, 8, 10, 23]. They leverage real faces and their corresponding edge maps to build paired data and formulate the problem in a similar way as image translation tasks. Recent studies can generate realistic results aligned with various kinds of sketches [5, 24]. Nevertheless, previous methods are basically 2D-based, which means that their generated faces are constrained to the exact views as the sketches are drawn. For scenarios such as character appearance design, a fixed-view image cannot be served as a sufficient reference. Even though face rotation techniques can be applied as post-processing, the results cannot guarantee 3D consistency. The stereoscopic nature of human faces makes it essential for designers to watch multi-view avatars. Moreover, the next step towards creating vivid agents in virtual realities is to build a geometry-aware portrait in the 3D space.\nIn this paper, we study the problem of creating stereoscopic portraits with high-quality 3D geometry from simple sketches. With the recent development of 3D-aware generative models [2, 3, 15, 29], researchers have succeeded in generating realistic portraits by involving 3D volume rendering techniques [26]. However, it is still challenging to build the connection between sketches and these models: 1) most 3D-aware GANs are generative models which do not support conditional inputs. Themodel building and training protocol designs are not trivial. 2) Unlike face parsing maps, sketches appear randomly without fixed semantics. It is difficult to constrain the generation process. 3) This task has rarely been explored before, leaving learning protocols uncertain.\nTo tackle this problem, we propose a system called Stereoscopic Simplified Sketch-to-Portrait (SSSP), which renders 3Dconsistent realistic portraits that arewell alignedwith simple sketches. The key is to delicately design sketch-aware constraints that can fully take the advantage of a tri-plane-based 3D-aware generative model. In detail, we propose to encode sketches into the prior latent space of tri-plane-based 3D-aware GAN [2]. A region-aware volume rendering strategy is proposed so that crucial regions can be directly rendered at higher resolutions for sketch matching. At the same time, we enforce symmetric sketches to produce symmetric 3D spaces, which greatly enhances global consistency.\nMoreover, we ensure that our system is interactive-friendly. While previous steps build the mapping between a detailed sketch and real faces, it is non-plausible for amateur users to draw complicated sketches that match the sketch dataset. On the other hand, we assume that users should be able to sketch their desired portrait at both the coarse and fine levels. Thus a Contour-to-Sketch module is proposed to reduce the difficulty for the amateur user to use our system. This module is novelly designed based on vector quantized\nrepresentations [13] so that it can handle robust types of contour inputs.\nOur contributions can be summarized as follows:\n\u2022 We propose to generate 3D-aware portraits from sketches by latent space encoding in tri-plane-based generative models with sketch-aware rendering constraints. \u2022 We design a novel Contour-to-Sketch module which can robustly convert simple contours to delicate sketches with vector quantized representations. \u2022 Extensive quantitative and qualitative experiments illustrate the effectiveness of our Stereoscopic Simplified Sketch-toPortrait (SSSP) system. Studies on our developed interface proved that stereoscopic portraits are crucial to the satisfaction of users."
        },
        {
            "heading": "2 RELATEDWORK",
            "text": ""
        },
        {
            "heading": "2.1 Sketch-based Portrait Synthesis",
            "text": "Synthesizing realistic portraits from given hand-drawn sketches has been a longstanding topic in both computer-human interaction and computer graphics communities. It is valuable for various applications in virtual reality, augmented reality and digital human creation. Early methods [6, 12] retrieve local image patches from a large-scale human face collection and then compose these local patches back to an entire image according to the input sketches. However, these methods were designed without considering hallucination consistency, which inevitably leads to unrealistic results. Taking advantage of deep neural networks, plenty of works [5, 7, 23, 24] are proposed recently for high-fidelity portrait synthesis by carefully devising the network architecture. To improve user experience, DeepFaceDrawing [5] introduces a real-time interactive system that enables users to input hand-drawn sketches with merely facial structures (i.e., five fixed components). However, almost all previous methods are conducted in the 2D space. Their results are static thus lack realism. Moreover, some of the methods are trained on a specific sketch domain. It will be difficult for them to handle rough or incomplete sketches.\nDifferent from the previous approaches, we propose to generate 3D-aware realistic portraits with substantial 3D representations. Moreover, our proposed system employs a two-stage synthesis strategy for robust and high-quality portrait synthesis."
        },
        {
            "heading": "2.2 Generative 3D-aware Image Synthesis",
            "text": "Recent generative models [1\u20133, 9, 11, 28, 33, 35] involving neural implicit representation (INR) techniques [26] have demonstrated great potential on high-quality image synthesis. Specifically, EG3d [2] proposed an efficient tri-plane hybrid explicit-implicit 3D representation to synthesize high-resolution images in real-time with view consistency together with high-quality 3D geometry. Despite promising image quality, these approaches cannot provide interactive local editing on the synthesized portraits.\nSome concurrent works [9, 35, 36] have made their attempts on conditional generation setting. FENeRF [36] proposed to use the semantic mask for editing the 3D facial volume of the target portrait via GAN inversion [21]. However, it fails to support high-resolution image synthesis and real-time user-interactive applications due to\nits optimization-based inversion. In order to achieve real-time interactive editing, IDE-3D [35] introduced a high-resolution semanticaware 3D generative model, which enables disentangled control over local shape and texture by leveraging a hybrid GAN inversion strategy. Although IDE-3D is exploring similar settings (i.e., semantic-aware 3D portrait synthesis and editing), our approach mainly focuses on building a user-friendly real-time interactive system even for non-artists by taking only hand-drawn sketches as input, which are much rougher and more abstract signals than the semantic mask used in IDE-3D."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we dive into the details of our proposed system, Stereoscopic Simplified Sketch-to-Portrait (SSSP). Our target is to create 3D-aware realistic human portraits from 2D sketches with simple user interaction."
        },
        {
            "heading": "3.1 3D-Aware Portrait Generation from Sketch",
            "text": "Problem Formulation. Given an arbitrary sketch \ud835\udc46 , our training goal is to recover the implicit or explicit 3D representation of it, and render it back to the real domain following the guidance of its paired image \ud835\udc3c\ud835\udc3a\ud835\udc47 . To fulfill high-fidelity 3D-awareness, we take inspiration from the recent Nerf based 3D generative models [2, 15, 29] that are able to produce high quality portraits via volume rendering in the 3D space. Our solution is to pose this challenging task in the form of \u201c3D GAN inversion\u201d, where an encoder is trained to learn the prior latent space of a pre-trained generator. Thus the task is formulated into the problem of render-guided encoder learning. Preliminaries on Tri-Plane-Based Generator. The tri-planebased generator [2] that enjoys high 3D consistency and rendering quality, is one of the best choices. As shown in the blue box of Fig. 2, it leverages an explicit tri-plane 3D representation which stores 3D scene information into three orthogonal feature images of size 2\u210e \u00d7 2\u210e\u00d7\ud835\udc36 . For each point \ud835\udc5d \u2208 R3 in the 3D space, its projections on the three planes (\ud835\udc65\ud835\udc66/\ud835\udc65\ud835\udc67/\ud835\udc66\ud835\udc67-plane) query corresponding interpolated features (\ud835\udc39\ud835\udc65\ud835\udc66, \ud835\udc39\ud835\udc65\ud835\udc67 , \ud835\udc39\ud835\udc66\ud835\udc67) and aggregate them to \ud835\udc39 for representing the information of this spatial location. Afterwards, a tiny decoder processes \ud835\udc39 to predict the color feature and density of this position \ud835\udc5d . These tri-plane feature images can be effectively sampled from a latent vector \ud835\udc67 with a StyleGAN2 generator [21].\nVolume rendering [25] is then performed by casting rays from the camera as NeRF [26]. Differently, here the color features are accumulated to a 2D feature image \ud835\udc3c\ud835\udc39 also with \ud835\udc36 channels. In the original design, a moderate resolution of \u210e \u00d7 \u210e is selected. \ud835\udc3c\ud835\udc39 is then sent into two different paths. One is to directly produce an RGB image \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 , and the other is to upsample its spatial resolution through more layers of modulated convolutions [21]. Thus a highresolution RGB image \ud835\udc3c+\n\ud835\udc45\ud835\udc3a\ud835\udc35 can be rendered.\nBasic Learning Objective. Our next goal is to accurately restore the radiance fields (tri-planes) that render images that strictly match the input sketch. As the tri-planes are created by the StyleGAN2 generator, the sketches could be intuitively encoded to the\ud835\udc4a or \ud835\udc4a + space. The encoder \ud835\udc38\ud835\udc46 encodes the sketch into a feature\ud835\udc64\ud835\udc46 in the\ud835\udc4a space and sent into the generator. While two images \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 and \ud835\udc3c+\n\ud835\udc45\ud835\udc3a\ud835\udc35 are produced within the generator, we identify that the directly rendered low-resolution image \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 is more suitable for\napplying constraints, as the backward path is shorter. The first training constraint is to recover the downsampled ground truth image \ud835\udc3c\u2212\n\ud835\udc3a\ud835\udc47 . The basic training objectives are the L1 reconstruction\nloss and the VGG perceptual loss [30, 38] between the rendered results and ground truth,\nL\ud835\udc5f\ud835\udc52 (\ud835\udc3c\u2212\ud835\udc3a\ud835\udc47 , \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35) =\u2225\ud835\udc3c \u2212 \ud835\udc3a\ud835\udc47 \u2212 \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 \u22251+\n\ud835\udc41\ud835\udc63\ud835\udc54\ud835\udc54\u2211\ufe01 \ud835\udc5a=1 \u2225VGG\ud835\udc5a (\ud835\udc3c\u2212\ud835\udc3a\ud835\udc47 ) \u2212 VGG\ud835\udc5a (\ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35)\u22251, (1)\nwhere VGG\ud835\udc5a denotes the\ud835\udc5a-th layer\u2019s results of a VGG19 network. We have also tried computing losses on the final outputs and found similar results."
        },
        {
            "heading": "3.2 Sketch-Aware Rendering Constraints",
            "text": "However, using the universal pixel-level constraint cannot guarantee the detailed consistency between drawn lines and textures. While the original tri-plane generator is trained without local perception, the entire tri-plane formulation is vulnerable to the tiny changes in the\ud835\udc4a space of the generator. To tackle this problem, we propose two novel sketch-aware rendering constraints, so that the model can grasp the local information from input sketch and achieve global consistency."
        },
        {
            "heading": "3.2.1 Region-Aware Volume Rendering Strategy for Encoding Sketches.",
            "text": "While depicting a human portrait, some local areas such as the eyes, nose and mouse play important roles. In practice, users usually spend more strokes on these regional parts. Thus we argue that the network should be sensitive to local sketch differences.\nTo this end, a region-aware volume rendering strategy is proposed to encourage the mapped latent code to focus on important local sketch areas. Specifically, 4 particular regions {\ud835\udc45\ud835\udc56 |\ud835\udc56 \u2208 [1, 4]} representing the left and right eyes, the nose and the mouth are selected on our rendered low-resolution image \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 of size \u210e \u00d7 \u210e. The corresponding size of the \ud835\udc56-th region is {\ud835\udeff\ud835\udc56\u210e\u00d7\ud835\udeff\ud835\udc56\u210e}. We perform volume rendering on each of these regions by densely sampling more rays, and then synthesize RGB images \ud835\udc3c\ud835\udc45\ud835\udc56\n\ud835\udc45\ud835\udc3a\ud835\udc35 which has the\nsame resolution as \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 . The reconstruction constraints are applied to all the rendered local regions. The loss can be written as:\nL\ud835\udc45\ud835\udc5f\ud835\udc52 = 4\u2211\ufe01\n\ud835\udc56=1 L\ud835\udc5f\ud835\udc52 (\ud835\udc3c\ud835\udc45\ud835\udc56\ud835\udc3a\ud835\udc47 , \ud835\udc3c \ud835\udc45\ud835\udc56 \ud835\udc45\ud835\udc3a\ud835\udc35 ), (2)\nwhere \ud835\udc3c\ud835\udc45\ud835\udc56 \ud835\udc3a\ud835\udc47 is the cropped and resized region of \ud835\udc45\ud835\udc56 . Compared with \ud835\udc45\ud835\udc56 on \ud835\udc3c\ud835\udc45\ud835\udc3a\ud835\udc35 , \ud835\udc3c \ud835\udc45\ud835\udc56 \ud835\udc45\ud835\udc3a\ud835\udc35\npreserves much more details. With the additional refinement of these important patches, the encoder can learn the fine-grained correlations between sketches and portraits, leading to more convincing results.\n3.2.2 Symmetric Constraint. Involving the region-aware volume rendering strategy could significantly boost the model\u2019s sketch consistency on local details. However, it is also challenging to capture the global information such as the facial outline. As demonstrated in [2], certain global attributes like general facial expressions are biased towards the input camera pose. This phenomenon still exists when the model is trained with camera pose serving as conditions. An underlining cause is that the constraints are too limited to force the encoder network to capture the global pattern.\nWith explicit 3D information leveraged in our setting, global constraints can be more easily added to both the 3D representations and the rendered image. Particularly, the simple flipping data augmentation is used here. We would expect the flipped sketch representing a person with symmetric geometry in the 3D space.\nIn detail, we take inspiration from unsupervised symmetric learning of 3D faces [40], but explore a new solution on tri-plane.Without loss of generality, we locate the portrait in the origin of tri-plane space and look towards the\ud835\udc66-axis without any rotation. Our insight is that the flipping of an image in the 2D space leads to a flip along the \ud835\udc66\ud835\udc67-plane of its tri-plane space. More specifically, if one camera is located at position \ud835\udc5d0 in the tri-plane space and takes a picture of the portrait, we can move the camera to the symmetric position\n\ud835\udc5d0 about \ud835\udc66\ud835\udc67-plane and flip the tri-plane features of the \ud835\udc65\ud835\udc66 and \ud835\udc65\ud835\udc67- plane to get a horizontally flipped image. We denote \ud835\udc46 as an input sketch and \ud835\udc46 as its horizontal flipped sketch. By encoding them to the generator, two tri-planes, \ud835\udc39\ud835\udc46 (\ud835\udc5d, \ud835\udc5d0) = (\ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc66, \ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc67 , \ud835\udc39\ud835\udc46\ud835\udc66\ud835\udc67) and \ud835\udc39\ud835\udc46 (\ud835\udc5d\u2032, \ud835\udc5d0) = (\ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc66, \ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc67 , \ud835\udc39\ud835\udc46\ud835\udc66\ud835\udc67), can be generated, respectively. Here \ud835\udc5d and \ud835\udc5d\u2032 are symmetric sample points about the \ud835\udc66\ud835\udc67-plane, \ud835\udc5d0 and \ud835\udc5d0 are symmetric camera pose about \ud835\udc66\ud835\udc67-plane as shown in Fig. 3. As the mirror constraint of 3D face naturally existed, we could further encourage the tri-plane features to satisfy the following equations: \ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc66 = flip(\ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc66, \ud835\udc65), \ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc67 = flip(\ud835\udc39\ud835\udc46\ud835\udc65\ud835\udc67 , \ud835\udc65), \ud835\udc39\ud835\udc46\ud835\udc66\ud835\udc67 = \ud835\udc39 \ud835\udc46 \ud835\udc66\ud835\udc67 , (3)\nwhere filp(\ud835\udc39, \ud835\udc65) means flipping the coordinates of tri-plane F along the \ud835\udc65-axis. Therefore, we introduce a novel constraint, which is termed as Symmetric Constraint on the tri-plane feature together with the image reconstruction loss in Eq. 1 and Eq. 2."
        },
        {
            "heading": "3.3 Coutour-to-Sketch Generation via Vector-Quantized Representation",
            "text": "It is very challenging for amateur users to draw a detailed sketch as the input data we use in the portrait generation module (3.1). Also, painters usually produce contours first before adding details. Thus it seems more reasonable and convenient that users could choose to produce and edit both the coarse and fine levels of sketches. This leads to a new challenge: how to bridge the domain gap between a coarse contour and the detailed sketch input for our model?\nTo mitigate this issue, we propose the Contour-to-Sketch module to transfer simplified contour to a detailed sketch under an image translation protocol [18]. However, we find that directly applying existing models [30, 38] on this type of data will lead to failure\ncases. For example, some important parts of the face such as the eyes will be missing.\nThus it is essential to seek a more robust design. We observe that the stroke types of facial sketches seem limited from a local perspective. Thus we propose to build a discrete codebook for the detailed sketches via vector-quantized (VQ) representations [37]. This guarantees that all generated results lie in the distribution of the realistic sketches. The learning procedures are introduced in the following. Quantized Sketch Representation. Firstly, we derive a learnable codebook with all data of detailed sketches. A sketch tokenizer \ud835\udc47\ud835\udc46 and a sketch decoder \ud835\udc37\ud835\udc46 are designed following VQGAN [13]. Given an arbitrary detailed sketch from the dataset, \ud835\udc46 , it is first encoded to a feature map z\u0302\ud835\udc58 through the tokenizer. Then each value on its spatial vector is later transformed into discrete values according to the closest codebook entry. During decoding, these tokens can be de-quantized to feature vectors through querying the features stored in the codebook. They can be recovered back to a sketch with the decoder \ud835\udc37\ud835\udc46 . The encoding-decoding scheme is simplified as:\n\ud835\udc46 \u2032 = \ud835\udc37\ud835\udc46 (q\u22121 (q(\ud835\udc47\ud835\udc46 (\ud835\udc46)))), (4)\nwhere q and q\u22121 denote the vector quantization and de-quantization operations. QuantizedRepresentationPrediction.Once themodel is trained, our next step is to map the coarse contour to the learned entities in the codebook. We design the contour-to-token mapping in a teacher-student manner. Specifically, we aim to train a new encoder \ud835\udc38\ud835\udc36 for the coarse contour \ud835\udc36 and map it into the latent space of learnable codebook V . The supervisions are two-folded. Firstly on each local patch position, the cross-entropy softmax classification loss is performed for predicting the correct token encoded by \ud835\udc47\ud835\udc46 . The next supervision is to directly constrain the distances between the encoded feature \ud835\udc38\ud835\udc36 (\ud835\udc36) from the contour and the de-quantized feature value q\u22121 (q(\ud835\udc47\ud835\udc46 (\ud835\udc46))). The final mapping procedure would be:\n\ud835\udc46 \u2032\u2032 = \ud835\udc37\ud835\udc46 (q\u22121 (q(\ud835\udc38\ud835\udc36 (\ud835\udc36)))) . (5)\nWith the help of this module, rich details can be added to basic contour sketches robustly. This module is essential to the usability of our model."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets. For training the Sketch-Guided Stereoscopic Portrait Generation module, we leverage the dataset proposed in DeepFaceDrawing [5]. It is created using the faces and parsing maps provided in CelebAMask-HQ [22] and then processed with Photocopy and sketch simplification procedures. It contains 17K pairs of sketchimage pairs. As for training the Contour-to-Sketch module, we process the above-mentioned dataset and extract contours using the tools provided in [9]. Implementation Details. The model of our generator is the same as EG3D [2]. The resolution of the final output image \ud835\udc3c+\n\ud835\udc45\ud835\udc3a\ud835\udc35 is 512 \u00d7\n512. The size of the low-solution image is 128 \u00d7 128 (\u210e = 128). The StyleGAN2 generator backbone produces a feature map of size\n256 \u00d7 256 \u00d7 96, which is later reshaped to the tri-planes, each of size 256 \u00d7 256 \u00d7 32. The sketch encoder architecture is designed as ResNet34 [16]. Our Sketch Guided Portrait generation model is trained on 2 Tesla A100 GPUs for 3 days. Quantitative Evaluation Metrics. We conduct quantitative evaluations on metrics that have previously been used in the field of image generation and image quality assessment. We use PSNR, SSIM[39], CPBD [27] to measure the generated images over image fidelity, structure similarity, and sharpness perspective. We also report two metrics, FID [17] and IS [32], commonly used in the image generation task to evaluate the image quality in latent distribution. Comparison Methods. To the best of our knowledge, there does not exist a method that shares our setting. Thus we leverage several related state-of-the-art methods including pSp [31], pix2pixHD [38], and DeepFaceDrawing [5].\nPix2pixHD is still one of the best general image-to-image translation frameworks, which supports various types of guidance. We directly train a sketch-to-portrait model using their released codes. pSp is proposed for StyleGAN2 inversion, it works in a similar way to ours and can be used for performing image translation while fixing the generator. DeepFaceDrawing opens an online interactive system, allowing users to input hand-drawn sketches."
        },
        {
            "heading": "4.2 Evaluation on Fix-View Synthesis",
            "text": "We first evaluate the generation quality of our method under a fix-view setting quantitatively and qualitatively. For fair comparisons, we use the sketches extracted from FFHQ [20] dataset as input without using the Contour-to-Sketch module. The qualitative results are shown in Fig. 4. We can observe that DeepFaceDrawing can only synthesize results roughly consistent with the input\nICMI \u201923, October 9\u201313, 2023, Paris, France Yasheng Sun, Hang Zhou, et al.Quali_Cmp_Image, Free-View-Point PhotoRealistic Image\nsketches, which even lack necessary identity information and reasonable texture on some local facial components. pSp produces high-quality portraits for each input sketch, while its results suffer from facial structure or hairstyle mismatch issues and occasionally incorrect generated genders. Pix2pixHD generates the most satisfactory resultant portraits among all the comparison methods with consistent facial boundaries, semantics, and detailed textures. Our SSSP system achieves comparable performance with pix2pixHD, where the generated high-quality results match the input sketches well but with the identity slightly changed."
        },
        {
            "heading": "4.3 Evaluation on Free-View Synthesis",
            "text": "Since our goal is to render 3D-aware portraits from given sketches, we also design additional experiments under a free-view-point setting to evaluate the novel-view synthesis ability of our system. The novel view synthesis results of 2D StyleGAN2-based models [34] do not enjoy 3D consistency. Thus as common practice, the results generated by previous methods are converted also to the 3D space for novel view rendering. For fair comparisons, we directly leverage the model of our generator EG3D [2] for performing 3D GAN inversion.\nSpecifically, the results of Pix2pixHD and pSp are firstly generated on the 2D plane. Then we map the image to a corresponding latent code in EG3D via optimization-based GAN-inversion. Based on the recovered tri-plane representation, free-view images can be directly obtained by changing the camera pose information. Note that DeepFaceDrawing [5] focuses on frontal face synthesis and\nre-training it on our training data with various poses will inevitably result in degradation in the generation quality, thus, we do not involve it in this comparison. Quantitative Comparisons.We first perform quantitative comparison between our SSSP and its counterparts. The results are reported in Table 1 in which the two comparison methods are denoted as \u201cpix2pixHD (w inv)\u201d and \u201cpSp (w inv)\u201d, respectively. Our model outperforms its counterparts on most of the evaluation metrics, which demonstrates the high-quality image generation ability of our approach. Qualitative Comparisons. Qualitative comparisons are depicted in Fig. 5. It can be seen that both pSp and pix2pixHD suffer from blurry textures under novel views. We analyze that most problems are caused by the imperfect 3D inversion of their results. Though we have set a long optimization time of about 20 minutes and tried altering the inversion weights, the inversion results of EG3D on these methods are still unsatisfactory. On the other hand, with our encoder, our method produces faces in less than a second. Moreover, the generation quality of our method is clearly better. This proves the effectiveness of our method."
        },
        {
            "heading": "4.4 Ablation Study",
            "text": "To further evaluate the contributions of our proposed techniques, we conduct an ablation study and the results are reported in Table 2. Concretely, we construct three variants. They are the model without region-aware rendering strategy (w/o RAS), the model without symmetric constraint (w/o SC), and encoding the\ud835\udc4a + space. Among all the variants,\u201cw/o RAS\u201d achieves the worst score on CPBD, which demonstrates that our proposed RAS can effectively alleviate blurry textures in the results. \u201cw/o SC\u201d achieves the worst SSIM and PSNR performance when omitting this proposed constraint. This indicates that the symmetric constraint can effectively enhance the learning of the encoder. Furthermore, we also attempt to explore the generation ability of our model when the sketches are encoded\nTable 2: Quantitative Results of Ablation Study.\nMethod SSIM \u2191 PSNR \u2191 FID \u2193 IS \u2191 CPBD \u2191 w/o RAS 0.420 14.128 101.24 2.43 0.471 w/o SC 0.412 13.662 102.53 2.69 0.564 Full Model (W space) 0.448 15.237 98.50 2.96 0.625\nw/o RAS FullW+ Space w/o SCGTSketch\n55_01457\nFigure 6: Qualitative Comparison of Ablation Study. The visualization of different varieties of our method.\nto\ud835\udc4a + space instead of\ud835\udc4a space in our full pipeline \u201cFull model (W space)\u201d. Compared with our full model, \u201c\ud835\udc4a + space\u201d obtains comparable performance on the metrics related to image quality but achieves a much lower score on CPBD. This indicates that encoding the sketches to W space contributes to better 3D portrait synthesis quality on novel views.\nDespite the quantitative comparisons, we also provide visualisation results of these variants. Compared with our full model, \u201cw/o RAS\u201d tends to render over-smooth facial textures on local parts (e.g., eyes, mouth and teeth) which degrade the quality of generated images. While for \u201cw/o SC\u201d, we provide an additional comparison in the last two rows at the bottom with a pair of symmetric input sketches. \u201cw/o SC\u201d fails to reconstruct symmetric images for these two sketches, which intuitively demonstrates the effectiveness of our proposed SC. By using all the components in our full model, we are able to synthesize high-quality and multi-view consistent portraits."
        },
        {
            "heading": "5 USABILITY STUDY",
            "text": "To evaluate the usability of our SSSP system, we invite 15 participants (denoted as U1-U15) to conduct a usability study, composed of a fixed-task study and an open-ended study with a graphical interface we developed.\nAmong the users, 6 are females and 9 are males. Their ages are uniformly distributed from 20 to 40. Two of the users are product\ndesigners, three are computer science researchers, and two are artists. The others do not work on related topics. We first ask all the participants to rate their drawing skills from 1 to 5 (rate 5 is the highest) and 80% of them are amateur or middle users (scores 1 to 3). Then each participant requires to perform a fixed-task drawing session for learning how to use our graphical interface. After they warm up and are familiar with the system, an openended drawing session is conducted to create expected portraits without any limitations."
        },
        {
            "heading": "5.1 Graphical Interface Design",
            "text": "The visualization of the interface is shown in Fig. 7. It provides tools for fast editing and creating users\u2019 desired portraits from both coarse and fine sketches. Contour Selection. Though the users can directly create their sketches from scratch, drawing a reasonable contour has already been proven difficult for most amateur users. We provide a collection of portrait contours with different genders, hairstyles, facial shapes, and poses. The users can select one of the given contours as an easy start of this system, which saves quite a lot of effort. Contour Editing.We provide contour drawing, erasing and undo functions in the contour editing stage like other common image editing applications. The users could create or edit the coarse contour at this step. After they finish the contour creation, they can click on the \u201cSketch-Gen\u201d button. The drawn contour will be sent into our Contour-to-Sketch module to synthesize a detailed sketch. The whole generation process takes less than 1 second and thus does not affect the users\u2019 experiences. Sketch Editing. The initial sketches here are created by the contours from the previous step. Users can still interact with the sketches in the same way as the contour editing. They can erase the unsatisfactory lines created by the networks and add their expected details. After clicking the \u201cPortrait-Gen\u2019, the sketch is fixed and sent into our Sketch-Guided Stereoscopic Portrait Generation module to guide the generation of a high-quality 3D-aware portrait. This process takes less than 2 seconds."
        },
        {
            "heading": "5.2 Fixed-task Study",
            "text": "In this session, we provide several real portraits as target images for helping the users learn our system and practice their drawing skills. Each participant is requested to fill in a questionnaire and rate from 1 to 5 (1 for strongly disagree, 5 for strongly agree) on the following six aspects:\n1) if our system is easy to use; 2) if our system is compactly designed; 3) if our system generates results consistent with your inten-\ntions; 4) if our system offers sufficient guidance; 5) if the coarse-to-fine process eases your creation; 6) if the 3D demonstration increases your motivation.\nThe rating results are shown in the upper part of Fig. 9, and we can observe that over 60% of the participants rate 4 (agree) or 5 (strongly agree) in all the six aspects, especially all the participants reach an agreement that the 3D demonstration truly increases their motivation for creating. U1 who is an amateur comments that \u201cThe generated 3D portraits are really impressive, and I can somehow quickly find out where and how to edit after a few times trying. The 3D demonstration can easily show me which part I should pay attention to when drawing contours. This improves not only my drawing skills, but also my imagination for the 3D space\u201d.\nMost participants are satisfied with the design of our system and give high scores on \u201cEasy Use\u201d and \u201cCompactly Designed\u201d. U5 who is a product designer comments that \u201cI feel the system is userfriendly and well-designed. The 3D effect of the results is really impressive. It will indeed attract users after perfection.\u201d. Only very few (i.e., drawing skill 4 or 5 scores) rate 1 or 2 on \u201cCoarse-to-Fine\u201d and \u201cSufficient Guidance\u201d, as their explanation is they have already raised their own drawing habits and styles, they prefer creating works freely all by themselves without any guidance. According to the results, our system is able to provide sufficient guidance and assistance which can effectively help amateur or middle users practice their drawing skills and create their expected portraits."
        },
        {
            "heading": "5.3 Open-ended Study",
            "text": "In the open-ended study session, the participants are requested to freely create their expected portraits with their hand-drawn sketches. Similarly, they are asked to fill in a questionnaire and rate from 1 to 5 (1 for strongly disagree, 5 for strongly agree) on five aspects:\n1) if our system generates diverse results; 2) if our system generates high-quality results; 3) if our system fits your expectation; 4) if our system supplies sufficient guidance; 5) if the 3D demonstration increases your motivation.\nThe rating results are shown in the lower part of Fig. 9. More than 73% of the participants rate 4 (agree) or 5 (strongly agree) in all five\naspects, among which all the participants agree or strongly agree that our 3D-aware portrait synthesis is impressive and effective when creating portraits. U7 comments that \u201cThe 3D generation effect is reasonable, just exactly as people imagined\u201d. U10 who is a content creation researcher comments that \u201cThe quality of the generated images under multiple views are quite amazing. It would be very convenient for the artists to create 3D virtual human or other kind of 3D assets if this technique could be generalized to other categories.\u201d\n12 out of 15 participants agree or strongly agree with \u201cHigh Quality\u201d and \u201cDiverse Results\u201d, which indicates that our proposed SSSP achieves robust performance even when the input free-hand sketches come from amateur users thanks to the delicately designed network architecture and training strategy.\nThough we have shown the advantages of our SSSP system above, some participants also kindly leave constructive advice to us for improving our interactive system. There are barely participants rate 1 or 2 on \u201cSufficient Guidance\u201d, \u201cHigh Quality\u201d and \u201cDiverse Results\u201d Few participants with better drawing skills do not agree with the \u201cExpectation Fitness\u201d aspect. U9 points that \u201cMore editing functions should be developed, such as skin color, hair color and curly/straight hair\u201d. U12 comments that \u201cCommon functions in the drawing software, such as zoom in/out and various line widths should be integrated into the system\u201d. We will make our efforts to incorporate those functions commonly used in the commercial software into our interactive system to make it more user-friendly."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we propose the Stereoscopic Simplified Sketch-toPortrait (SSSP) system,which generates high-quality stroke-consistent portraits with 3D awareness. Compared with previous methods, our system enjoys several intriguing properties. 1) Our system directly converts sketches to portraits with 3D representations, thus we can freely render its views from the 3D space. This makes our system inherently different from previous studies. 2)We propose the Contour-to-Sketch module which robustly converts sparse contours to detailed sketches. Such a topic has rarely been discussed. 3) We build a user-friendly graphical interface where the users can freely choose to edit simple contours or sketches. Extensive user studies show that our system is greatly preferred by users."
        }
    ],
    "title": "Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation",
    "year": 2023
}