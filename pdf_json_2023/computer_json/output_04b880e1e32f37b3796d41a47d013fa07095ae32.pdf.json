{
    "abstractText": "Large Language Models (LLMs) have achieved remarkable success, demonstrating powerful instruction-following capabilities across diverse tasks. Instruction fine-tuning is critical in enabling LLMs to align with user intentions and effectively follow instructions. In this work, we investigate how instruction fine-tuning modifies pre-trained models, focusing on two perspectives: instruction recognition and knowledge evolution. To study the behavior shift of LLMs, we employ a suite of local and global explanation methods, including a gradient-based approach for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. Our findings reveal three significant impacts of instruction fine-tuning: 1) It empowers LLMs to better recognize the instruction parts from user prompts, thereby facilitating high-quality response generation and addressing the \u201clost-in-the-middle\u201d issue observed in pre-trained models; 2) It aligns the knowledge stored in feed-forward layers with user-oriented tasks, exhibiting minimal shifts across linguistic levels. 3) It facilitates the learning of word-word relations with instruction verbs through the self-attention mechanism, particularly in the lower and middle layers, indicating enhanced recognition of instruction words. These insights contribute to a deeper understanding of the behavior shifts in LLMs after instruction fine-tuning and lay the groundwork for future research aimed at interpreting and optimizing LLMs for various applications1.",
    "authors": [
        {
            "affiliations": [],
            "name": "INSTRUCTION TUNING"
        },
        {
            "affiliations": [],
            "name": "Xuansheng Wu"
        },
        {
            "affiliations": [],
            "name": "Wenlin Yao"
        },
        {
            "affiliations": [],
            "name": "Jianshu Chen"
        },
        {
            "affiliations": [],
            "name": "Xiaoman Pan"
        },
        {
            "affiliations": [],
            "name": "Xiaoyang Wang"
        },
        {
            "affiliations": [],
            "name": "Ninghao Liu"
        },
        {
            "affiliations": [],
            "name": "Dong Yu"
        }
    ],
    "id": "SP:4b4dc2017fa43414a94b4a9706d8a47143ef321a",
    "references": [
        {
            "authors": [
                "Oren Barkan",
                "Edan Hauon",
                "Avi Caciularu",
                "Ori Katz",
                "Itzik Malkiel",
                "Omri Armstrong",
                "Noam Koenigstein"
            ],
            "title": "Grad-sam: Explaining transformers via gradient self-attention maps",
            "venue": "In Proceedings of the 30th ACM International Conference on Information & Knowledge Management,",
            "year": 2021
        },
        {
            "authors": [
                "Steven Bills",
                "Nick Cammarata",
                "Dan Mossing",
                "Henk Tillman",
                "Leo Gao",
                "Gabriel Goh",
                "Ilya Sutskever",
                "Jan Leike",
                "Jeff Wu",
                "William Saunders"
            ],
            "title": "Language models can explain neurons in language models",
            "venue": "URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date",
            "year": 2023
        },
        {
            "authors": [
                "Yihan Cao",
                "Yanbin Kang",
                "Lichao Sun"
            ],
            "title": "Instruction mining: High-quality instruction data selection for large language models",
            "venue": "arXiv preprint arXiv:2307.06290,",
            "year": 2023
        },
        {
            "authors": [
                "Zeyu Cui",
                "Jianxin Ma",
                "Chang Zhou",
                "Jingren Zhou",
                "Hongxia Yang"
            ],
            "title": "M6-rec: Generative pretrained language models are open-ended recommender systems",
            "venue": "arXiv preprint arXiv:2205.08084,",
            "year": 2022
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Yaru Hao",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "Knowledge neurons in pretrained transformers",
            "venue": "arXiv preprint arXiv:2104.08696,",
            "year": 2021
        },
        {
            "authors": [
                "Guy Dar",
                "Mor Geva",
                "Ankit Gupta",
                "Jonathan Berant"
            ],
            "title": "Analyzing transformers in embedding space",
            "venue": "arXiv preprint arXiv:2209.02535,",
            "year": 2022
        },
        {
            "authors": [
                "Jinhao Duan",
                "Hao Cheng",
                "Shiqi Wang",
                "Chenan Wang",
                "Alex Zavalny",
                "Renjing Xu",
                "Bhavya Kailkhura",
                "Kaidi Xu"
            ],
            "title": "Shifting attention to relevance: Towards the uncertainty estimation of large language models",
            "venue": "arXiv preprint arXiv:2307.01379,",
            "year": 2023
        },
        {
            "authors": [
                "Javid Ebrahimi",
                "Anyi Rao",
                "Daniel Lowd",
                "Dejing Dou"
            ],
            "title": "Hotflip: White-box adversarial examples for text classification",
            "venue": "arXiv preprint arXiv:1712.06751,",
            "year": 2017
        },
        {
            "authors": [
                "Nelson Elhage",
                "Neel Nanda",
                "Catherine Olsson",
                "Tom Henighan",
                "Nicholas Joseph",
                "Ben Mann",
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Tom Conerly"
            ],
            "title": "A mathematical framework for transformer circuits",
            "venue": "Transformer Circuits Thread,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph Enguehard"
            ],
            "title": "Sequential integrated gradients: a simple but effective method for explaining language models",
            "venue": "arXiv preprint arXiv:2305.15853,",
            "year": 2023
        },
        {
            "authors": [
                "Shi Feng",
                "Eric Wallace",
                "Alvin Grissom II",
                "Mohit Iyyer",
                "Pedro Rodriguez",
                "Jordan Boyd-Graber"
            ],
            "title": "Pathologies of neural models make interpretations difficult",
            "venue": "arXiv preprint arXiv:1804.07781,",
            "year": 2018
        },
        {
            "authors": [
                "Mor Geva",
                "Roei Schuster",
                "Jonathan Berant",
                "Omer Levy"
            ],
            "title": "Transformer feed-forward layers are key-value memories",
            "venue": "arXiv preprint arXiv:2012.14913,",
            "year": 2020
        },
        {
            "authors": [
                "Raffaele Giancarlo"
            ],
            "title": "A generalization of the suffix tree to square matrices, with applications",
            "venue": "SIAM Journal on Computing,",
            "year": 1995
        },
        {
            "authors": [
                "Jing Huang",
                "Atticus Geiger",
                "Karel D\u2019Oosterlinck",
                "Zhengxuan Wu",
                "Christopher Potts"
            ],
            "title": "Rigorously assessing natural language explanations of neurons",
            "venue": "arXiv preprint arXiv:2309.10312,",
            "year": 2023
        },
        {
            "authors": [
                "Niall Hurley",
                "Scott Rickard"
            ],
            "title": "Comparing measures of sparsity",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2009
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas"
            ],
            "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Enja Kokalj",
                "Bla\u017e \u0160krlj",
                "Nada Lavra\u010d",
                "Senja Pollak",
                "Marko"
            ],
            "title": "Robnik-\u0160ikonja. Bert meets shapley: Extending shap explanations to transformer-based classifiers",
            "venue": "In Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation,",
            "year": 2021
        },
        {
            "authors": [
                "Jiwei Li",
                "Xinlei Chen",
                "Eduard Hovy",
                "Dan Jurafsky"
            ],
            "title": "Visualizing and understanding neural models in nlp",
            "venue": "arXiv preprint arXiv:1506.01066,",
            "year": 2015
        },
        {
            "authors": [
                "Zekun Li",
                "Baolin Peng",
                "Pengcheng He",
                "Xifeng Yan"
            ],
            "title": "Do you really follow me? adversarial instructions for evaluating the robustness of large language models",
            "venue": "arXiv preprint arXiv:2308.10819,",
            "year": 2023
        },
        {
            "authors": [
                "Zongxia Li",
                "Paiheng Xu",
                "Fuxiao Liu",
                "Hyemi Song"
            ],
            "title": "Towards understanding in-context learning with contrastive demonstrations and saliency maps",
            "venue": "arXiv preprint arXiv:2307.05052,",
            "year": 2023
        },
        {
            "authors": [
                "Hanmeng Liu",
                "Ruoxi Ning",
                "Zhiyang Teng",
                "Jian Liu",
                "Qiji Zhou",
                "Yue Zhang"
            ],
            "title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "venue": "arXiv preprint arXiv:2304.03439,",
            "year": 2023
        },
        {
            "authors": [
                "Nelson F Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang"
            ],
            "title": "Lost in the middle: How language models use long contexts",
            "venue": "arXiv preprint arXiv:2307.03172,",
            "year": 2023
        },
        {
            "authors": [
                "Sheng Lu",
                "Irina Bigoulaeva",
                "Rachneet Sachdeva",
                "Harish Tayyar Madabushi",
                "Iryna Gurevych"
            ],
            "title": "Are emergent abilities in large language models just in-context learning",
            "venue": "arXiv preprint arXiv:2309.01809,",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Meng",
                "David Bau",
                "Alex Andonian",
                "Yonatan Belinkov"
            ],
            "title": "Locating and editing factual associations in gpt",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Beren Millidge",
                "Sid Black"
            ],
            "title": "The singular value decompositions of transformer weight matrices are highly interpretable",
            "year": 2022
        },
        {
            "authors": [
                "Jesse Mu",
                "Jacob Andreas"
            ],
            "title": "Compositional explanations of neurons",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Catherine Olsson",
                "Nelson Elhage",
                "Neel Nanda",
                "Nicholas Joseph",
                "Nova DasSarma",
                "Tom Henighan",
                "Ben Mann",
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen"
            ],
            "title": "In-context learning and induction heads",
            "venue": "arXiv preprint arXiv:2209.11895,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sibabrata Paladhi",
                "Sivaji Bandyopadhyay"
            ],
            "title": "Generation of referring expression using prefix tree structure",
            "venue": "In Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-II,",
            "year": 2008
        },
        {
            "authors": [
                "Baolin Peng",
                "Chunyuan Li",
                "Pengcheng He",
                "Michel Galley",
                "Jianfeng Gao"
            ],
            "title": "Instruction tuning with gpt-4",
            "venue": "arXiv preprint arXiv:2304.03277,",
            "year": 2023
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Richard Socher",
                "Christopher D Manning"
            ],
            "title": "Glove: Global vectors for word representation",
            "venue": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),",
            "year": 2014
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander H Miller",
                "Sebastian Riedel"
            ],
            "title": "Language models as knowledge bases",
            "year": 1909
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A Smith",
                "Omer Levy"
            ],
            "title": "Improving transformer models by reordering their sublayers",
            "venue": "arXiv preprint arXiv:1911.03864,",
            "year": 2019
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": " why should i trust you?\u201d explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Michael Cogswell",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Why did you say that",
            "venue": "arXiv preprint arXiv:1611.07450,",
            "year": 2016
        },
        {
            "authors": [
                "Y Shan",
                "X Chen",
                "Y Shi",
                "J Liu"
            ],
            "title": "Fast language model look-ahead algorithm using extended n-gram model",
            "venue": "Acta Automatica Sinica,",
            "year": 2012
        },
        {
            "authors": [
                "Chandan Singh",
                "Aliyah R Hsu",
                "Richard Antonello",
                "Shailee Jain",
                "Alexander G Huth",
                "Bin Yu",
                "Jianfeng Gao"
            ],
            "title": "Explaining black box text modules in natural language with language models",
            "venue": "arXiv preprint arXiv:2305.09863,",
            "year": 2023
        },
        {
            "authors": [
                "Bills Steven",
                "Cammarata Nick",
                "Mossing Dan",
                "Tillman Henk",
                "Gao Leo",
                "Goh Gabriel",
                "Sutskever Ilya",
                "Leike Jan",
                "Wu Jeff",
                "Saunders William"
            ],
            "title": "Language models can explain neurons in language models",
            "year": 2022
        },
        {
            "authors": [
                "Yusheng Su",
                "Xiaozhi Wang",
                "Yujia Qin",
                "Chi-Min Chan",
                "Yankai Lin",
                "Huadong Wang",
                "Kaiyue Wen",
                "Zhiyuan Liu",
                "Peng Li",
                "Juanzi Li"
            ],
            "title": "On transferability of prompt tuning for natural language processing",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Edouard Grave",
                "Guillaume Lample",
                "Herve Jegou",
                "Armand Joulin"
            ],
            "title": "Augmenting self-attention with persistent memory",
            "venue": "arXiv preprint arXiv:1907.01470,",
            "year": 2019
        },
        {
            "authors": [
                "Xianghui Sun",
                "Yunjie Ji",
                "Baochang Ma",
                "Xiangang Li"
            ],
            "title": "A comparative study between fullparameter and lora-based fine-tuning on chinese instruction data for instruction following large language model",
            "venue": "arXiv preprint arXiv:2304.08109,",
            "year": 2023
        },
        {
            "authors": [
                "Mukund Sundararajan",
                "Ankur Taly",
                "Qiqi Yan"
            ],
            "title": "Axiomatic attribution for deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Alpaca: A strong, replicable instruction-following model",
            "venue": "Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html,",
            "year": 2023
        },
        {
            "authors": [
                "MosaicML NLP Team"
            ],
            "title": "Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023",
            "venue": "URL www.mosaicml.com/blog/mpt-7b",
            "year": 2023
        },
        {
            "authors": [
                "James J Thomas"
            ],
            "title": "Illuminating the path:[the research and development agenda for visual analytics",
            "venue": "IEEE Computer Society,",
            "year": 2005
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Neeraj Varshney",
                "Wenlin Yao",
                "Hongming Zhang",
                "Jianshu Chen",
                "Dong Yu"
            ],
            "title": "A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation",
            "venue": "arXiv preprint arXiv:2307.03987,",
            "year": 2023
        },
        {
            "authors": [
                "Jesse Vig"
            ],
            "title": "Bertviz: A tool for visualizing multihead self-attention in the bert model",
            "venue": "In ICLR workshop: Debugging machine learning models,",
            "year": 2019
        },
        {
            "authors": [
                "Elena Voita",
                "Javier Ferrando",
                "Christoforos Nalmpantis"
            ],
            "title": "Neurons in large language models: Dead, n-gram, positional",
            "venue": "arXiv preprint arXiv:2309.04827,",
            "year": 2023
        },
        {
            "authors": [
                "Tu Vu",
                "Brian Lester",
                "Noah Constant",
                "Rami Al-Rfou",
                "Daniel Cer"
            ],
            "title": "Spot: Better frozen model adaptation through soft prompt transfer",
            "venue": "arXiv preprint arXiv:2110.07904,",
            "year": 2021
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Jerry Wei",
                "Jason Wei",
                "Yi Tay",
                "Dustin Tran",
                "Albert Webson",
                "Yifeng Lu",
                "Xinyun Chen",
                "Hanxiao Liu",
                "Da Huang",
                "Denny Zhou"
            ],
            "title": "Larger language models do in-context learning differently",
            "venue": "arXiv preprint arXiv:2303.03846,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 1910
        },
        {
            "authors": [
                "Tongshuang Wu",
                "Marco Tulio Ribeiro",
                "Jeffrey Heer",
                "Daniel S Weld"
            ],
            "title": "Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models",
            "venue": "arXiv preprint arXiv:2101.00288,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengxuan Wu",
                "Desmond C Ong"
            ],
            "title": "On explaining your explanations of bert: An empirical study with sequence classification",
            "venue": "arXiv preprint arXiv:2101.00196,",
            "year": 2021
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma"
            ],
            "title": "An explanation of in-context learning as implicit bayesian inference",
            "venue": "arXiv preprint arXiv:2111.02080,",
            "year": 2021
        },
        {
            "authors": [
                "Miao Xiong",
                "Zhiyuan Hu",
                "Xinyang Lu",
                "Yifei Li",
                "Jie Fu",
                "Junxian He",
                "Bryan Hooi"
            ],
            "title": "Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms",
            "venue": "arXiv preprint arXiv:2306.13063,",
            "year": 2023
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Renrui Zhang",
                "Jiaming Han",
                "Aojun Zhou",
                "Xiangfei Hu",
                "Shilin Yan",
                "Pan Lu",
                "Hongsheng Li",
                "Peng Gao",
                "Yu Qiao"
            ],
            "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
            "venue": "arXiv preprint arXiv:2303.16199,",
            "year": 2023
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Shuohang Wang",
                "Yichong Xu",
                "Yuwei Fang",
                "Wenhao Yu",
                "Yang Liu",
                "Hai Zhao",
                "Chenguang Zhu",
                "Michael Zeng"
            ],
            "title": "Task compass: Scaling multi-task pre-training with task prefix",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Chunting Zhou",
                "Pengfei Liu",
                "Puxin Xu",
                "Srini Iyer",
                "Jiao Sun",
                "Yuning Mao",
                "Xuezhe Ma",
                "Avia Efrat",
                "Ping Yu",
                "Lili Yu"
            ],
            "title": "Lima: Less is more for alignment",
            "venue": "arXiv preprint arXiv:2305.11206,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The remarkable capability of Large Language Models (LLMs) to align with user intentions is wellrecognized across various real-world applications, where they are expected to be helpful, honest, and harmless AI assistants (Ouyang et al., 2022; OpenAI, 2023). Central to these roles, being \u201chelpful\u201d is the most fundamental requisite, emphasizing that LLMs should help users to complete various tasks, known as the \u201cinstruction following\u201d capability. Many studies (Raffel et al., 2020; Wang et al., 2022; Zhou et al., 2023) show that instruction fine-tuning, also called supervised fine-tuning (Ouyang et al., 2022), is critical to acquire such capability, by fine-tuning pre-trained models on high-quality prompt-response pairs. However, the impact of instruction tuning on the helpfulness of language models remains inadequately understood, limiting the improvements toward better fine-tuned models.\nWhile many studies on interpreting LLMs have delved into pre-training (Dai et al., 2021; Elhage et al., 2021; Olsson et al., 2022; Meng et al., 2022), in-context learning (Xie et al., 2021; Olsson et al., 2022; Liu et al., 2023b), and single-task fine-tuning (Kokalj et al., 2021; Wu & Ong, 2021; Enguehard, 2023), their findings cannot be extrapolated to instruction fine-tuning. This is because instruction tuning updates model weights (differing from pre-training and in-context learning) to achieve robust generalization across a range of downstream tasks (unlike single-task fine-tuning). Therefore, we seek to explore how large language models undergo a shift in behavior\u2014from primarily modeling language to effectively following instructions\u2014after being subjected to the instruction tuning process.\nIn this work, we focus on answering the research question of how instruction fine-tuning changes the pre-trained model\u2019s behavior from two perspectives, namely instruction recognition and knowledge evolution. In particular, what role do user instructions play when an instruction fine-tuned model generates responses? Also, what the encoded knowledge evolves after instruction fine-tuning? We consider these two perspectives under a well-accepted belief that an LLM becomes a helpful assistant by first recognizing human instructions and then retrieving corresponding knowledge to respond.\n\u2217The work done during the internship at Tencent AI Lab. 1We will release our code and data soon.\nar X\niv :2\n31 0.\n00 49\n2v 1\n[ cs\n.C L\n] 3\n0 Se\np 20\n23\nThe primary way we study these questions is to compare the outcomes of pre-trained and fine-tuned models. However, it is still non-trivial to technically interpret recent large language models. For one, LLMs demonstrate their instruction following ability by generating textual responses, yet many explanation methods for language models are designed for text classification (Sundararajan et al., 2017; Mu & Andreas, 2020; Wu & Ong, 2021; Enguehard, 2023). Moreover, instruction fine-tuned models retain massive knowledge from various topics and domains, but existing knowledge probing methods are limited to a specific task or domain (Press et al., 2019; Dai et al., 2021; Meng et al., 2022). To fill these gaps, we develop a series of local and global explanation methods as a toolbox to study large language models. Specifically, we first introduce a gradient-based local explanation method to determine the attribution of input tokens in the prompt to output tokens in the response. Next, we propose two global explanation methods to interpret the textual patterns and concepts encoded in self-attention as well as feed-forward layers with natural language. We further adopt a scalable automated interpretation method to interpret entire LLMs that billions of parameters. Through our interpretation tools, we elucidate the crucial behavior shift in LLMs as their transition from language modeling to instruction following. While our emphasis is on behavior shifts after instruction tuning, future research might also apply our toolbox to understand LLMs for various other purposes. To this end, we obtain three main findings related to instruction fine-tuning as follows:\n\u2022 Finding-1: Instruction fine-tuning enables models to recognize instruction words from user input prompts, guiding high-quality response generation. We study the influence of input prompts by detailing their instruction (e.g., \u201cCorrect grammar:\u201d) and context (e.g., \u201cMe and him goes to a stores for buying a apple.\u201d) words. We introduce a gradient-based method to evaluate how much each input word contributes to output words, where we observe that key instruction words constantly guide the generation process, while the models perform a \u201cmatch-and-reference\u201d behavior on context words (Sec. 3.1). Based on this, we propose a density score to inherently assess if the model aligns with user intentions by computing the density of importance on instruction words (Sec. 3.2). Moreover, we explore the effects of prompt position by extending the density score to all input words, and show that instruction fine-tuning alleviates the \u201clost-in-the-middle\u201d issue seen in pre-trained models (Sec. 3.3).\n\u2022 Finding-2: Instruction fine-tuning aligns the knowledge in feed-forward layers with useroriented tasks, with minimal shifts across linguistic levels. We study the knowledge in feed-forward layers at the \u201cconcept\u201d level by interpreting the principal components of their stored patterns with human language (Sec. 4.1). Our analysis to these concepts spans two dimensions: user-oriented tasks 2 and linguistic levels 3 (Sec. 4.3). A notable shift is observed concerning user-oriented tasks, where the fine-tuned model more explicitly adjusts the concepts toward specific downstream tasks such as writing, coding, and solving math problems. In terms of linguistic levels, while there is no shift after instruction tuning, we observe an interesting phenomenon: as model depth increases, the proportion of semantic knowledge initially grows, but in the last few layers, it drops significantly while that of morphology knowledge (e.g., prefix \u201cthe-\u201d and suffix \u201c-ed\u201d) increases. This observation differs from the conventional belief that deeper layers mainly capture high-level concepts (Zeiler & Fergus, 2014; Selvaraju et al., 2016). We provide discussions under the assumption that the last few feed-forward networks simulate a prefix tree for efficient text generation.\n\u2022 Finding-3: Instruction fine-tuning encourages self-attention heads from lower layers to learn word-word relations with instruction verbs. We study the knowledge in self-attentions by identifying word pairs that strongly activate corresponding column vectors in their query and key matrices, which pairs are also likely to co-occur within a context. (Sec. 5.1). We observe that (Sec. 5.3) only approximately 65% of the word pairs from the same selfattention head remained consistent after instruction tuning. Furthermore, when analyzing the shifts in word pairs of instruction verbs (e.g., classify, summarize, translate) versus universal frequent verbs, we notice a higher percentage of self-attention heads from lower and middle layers stored instruction verb relations after fine-tuning, indicating that pre-trained models become adept at recognizing instruction words during instruction fine-tuning.\nIn the following, we present notations and general experiment settings, followed by methods and results for each main finding, concluding with instruction fine-tuning tips and related work.\n2User-oriented tasks include \u201cwriting\u201d, \u201ccoding\u201d, \u201ctranslation\u201d, and \u201csolving math problem\u201d. 3Linguistic levels include \u201cphonology\u201d, \u201cmorphology\u201d, \u201csyntax\u201d, and \u201csemantic\u201d (Thomas, 2005)."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": ""
        },
        {
            "heading": "2.1 BACKGROUND",
            "text": "Notations. In this work, we use boldface lowercase letters (e.g., x) to denote vectors, boldface uppercase letters (e.g., W ) to denote matrices, and calligraphic capital letters (e.g., V) to denote sets. Considering V as a pre-defined vocabulary set, then X = [x1, ..., xN ] denotes a N -length prompting text from a user and Y = [y1, ..., yM ] is a M -length generated response from a language model f . Here, individual tokens xn \u2208 X and ym \u2208 Y come from the vocabulary set V . Transformer Architecture. The transformer-based language model f is designed to model natural language in a D-dimensional space, starting with an input word embedding Ei \u2208 R|V|\u00d7D that maps tokens X to a matrix X \u2208 RN\u00d7D. X will be passed into L transformer blocks, where each block consists of a self-attention module and a feed-forward network. Each self-attention module has H self-attention heads lying on a D\u2032-dimensional space and one output weight Wo \u2208 RD\u00d7D aggregating the outputs from different heads. Each self-attention head captures relations among the words by Ah = softmax(XWhq (XW h k) \u22a4/\u03f5), where Whq ,W h k ,W h v \u2208 RD\u00d7D \u2032 , and \u03f5 is a constant. The aggregation of heads\u2019 outputs is [A1XW1v; ...;A HXWHv ]Wo. Each feed-forward network 4 remains two weight matrices, denoted as Wu,Wp \u2208 RD \u2032\u2032\u00d7D. Then the output of which is defined as \u03c3(XW\u22a4u )Wp, where \u03c3 refers to a non-linear function. At the end, the processed word embeddings dot product with the transpose of output word embedding matrix Eo \u2208 R|V|\u00d7D for the next word prediction. In practice, normalization is applied at the beginning of each module to standardize the variance. We treat this normalization as an integrated parameter within the weight matrices."
        },
        {
            "heading": "2.2 GENERAL EXPERIMENTAL SETTINGS",
            "text": "Language Models. We choose LLaMa family (Touvron et al., 2023) as the focus for two reasons. Firstly, LLaMa stands out as one of the most advanced publicly accessible pre-trained language model families. Secondly, LLaMa serves as the foundation for many instruction fine-tuned models, providing a vast array for further research. In this research, we mainly use the fully fine-tuned versions of Alpaca (Taori et al., 2023) and Vicuna (Zheng et al., 2023) as the instruction fine-tuned models, using LLama (Touvron et al., 2023) as the corresponding pre-trained model5. During generation, we employ a greedy search (for reproduce) to generate up to 300 tokens for each input prompt.\nInstruction Datasets. We sourced user-oriented prompting texts from three publicly available datasets: Self-Instruct (Wang et al., 2022), LIMA (Zhou et al., 2023), and MT-Bench (Zheng et al., 2023). To assess the generalization capabilities of the instruction fine-tuned models, we exclusively focused on the test set from each dataset. In total, we amassed 632 prompting texts covering a wide range of tasks (e.g., writing, coding, and math) as well as topics (e.g., politics, history, and chemistry)."
        },
        {
            "heading": "3 IMPACT OF USER PROMPTS TO INSTRUCTION TUNED MODELS",
            "text": ""
        },
        {
            "heading": "3.1 VISUALIZING PROMPT INFLUENCE ON GENERATION PROCESS",
            "text": ""
        },
        {
            "heading": "3.1.1 TOOL: QUANTIFY IMPORTANCE BETWEEN INPUT AND OUTPUT TOKENS",
            "text": "We aim to measure the importance of each input token to each response token. In classification, input feature importance is typically measured by monitoring confidence changes upon its removal (Ribeiro et al., 2016; Ebrahimi et al., 2017; Feng et al., 2018). Treating text generation as word classification tasks, the importance of an input to an output token is gauged by examining confidence changes in output generation without the input token. We define importance In,m of input xn to output ym as:\nIn,m = p(ym|Zm)\u2212 p(ym|Zm,/n), (1) where Zm is the context to generate ym by concatenating the inquire X and the first m\u2212 1 tokens of response Y , Zm,/n omits token xn from Zm, and p(\u00b7|\u00b7) is the conditional probability computed\n4Without losing generalization, we introduce the feed-forward network as a two layers multi-layers perceptron, while it may be various across different model families.\n5We implement these models with the code and checkpoints available from Hugginface library (Wolf et al., 2019). We use lmsys/vicuna-7b-delta-v1.1 for Vicuna and tatsu-lab/alpaca-7b-wdiff for Alpaca.\nby language model f . We accelerate Equation 1 with the first-order approximation (Ebrahimi et al., 2017): In,m \u2248 \u2202f(ym|Zm)\u2202 Ei[xn] \u00b7 Ei[xn]\n\u22a4, where Ei[xn] is the input word embedding of token xn. The importance of input tokens cannot be compared across different output tokens due to its dependency on the confidence f(ym|Zm). It\u2019s crucial to recognize that a word with a lower confidence doesn\u2019t necessarily imply it is a trivial word. We propose independent normalization and introduce sparsity for robustness. The normalized pairwise score Sn,m between xn and ym is\nSn,m = ReLU (\u2308 L\u00d7 In,m maxN\nn\u2032=1 In\u2032,m\n\u2309 \u2212 b ) , where \u2308\u00b7\u2309 is the ceiling function, and b \u2208 [1, L] is a\nhyper-parameter determining the minimal interested importance level."
        },
        {
            "heading": "3.1.2 EXPERIMENT DESIGNS",
            "text": "We describe the generation process via visualizing salient maps based on the normalized pairwise importance sn,m, where L = 10 and b = 0. Figure 1 provides two salient maps to the same prompt and golden response for LLaMA-13B and Vicuna-13B respectively (see Appendix A for more).\n3.1.3 EXPERIMENT RESULTS\nIn stru ctio n\nC o n te xt\nGold Response (LLaMA) Gold Response (Vicuna)"
        },
        {
            "heading": "3.2 ASSESSING INSTRUCTION FOLLOWING WITH IMPORTANCE DENSITY",
            "text": ""
        },
        {
            "heading": "3.2.1 TOOL: ATTRIBUTE INPUT TOKENS FOR ENTIRE GENERATION PROCESS",
            "text": "We aim to measure the overall attribution of each input token to the entire response generation process. Based on our observations in Sec. 3.1.3, an input token should acquire more attribution score if it is important to more output tokens. Following this intuition, the input token xn\u2019s attribution an is measured by leveraging \u21131/\u2113p density function over the normalized importance to all output tokens: an = ||Sn||1/||Sn||p, where Sn = [Sn,1, ..., Sn,M ], and p \u2208 R+ serves as a hyper parameter. One of the nice property by using \u21131/\u2113p density function is if two input tokens have the same total sparse importance, then the one having a greater maximum importance would receives more attribution score (check (Hurley & Rickard, 2009) for proof)."
        },
        {
            "heading": "3.2.2 EXPERIMENT DESIGNS",
            "text": "To verify our assumption that a model is following instruction if it identify and use instruction words to guide the generation, we collect an annotated dataset, where each prompt was manually annotated the instruction part, and each response was labeled as either \u201cfollowed\u201d or \u201cunfollowed\u201d. Here, the\n6The prompt highlighted with the same colors of their boxes and underlined with instruction words: The tone of the email should be changed and rewritten in a more professional manner. Subject: Asking for Vacation Time, \\nHi, [Employer], \\nI hope all is well. I am writing ... concerns you may have.\\nThank you for your time,\\n[Your name]\ninstruction part includes the sentences that describe background information and required actions to a task. On the other hand, \u201cfollowed\u201d indicates that the model provides information pertinent to the user intention, regardless of the response\u2019s factual correctness. For each prompt-response pair sourced from our datasets, we compute the importance density score with L = 10, b = 7, and p = 4. We further normalize the scores to ensure comparability across different instances and remove the instances with a short response (less than 5 tokens) as their estimations of density are not stable. Table 1 presents the average importance density scores for both followed and unfollowed instances. Please check Appendix B for detailed settings and studies to outlier cases."
        },
        {
            "heading": "3.2.3 EXPERIMENT RESULTS",
            "text": "Obs-2: The importance density on instruction words reflects the models\u2019 behaviors in following user intentions. From Table 1, it becomes evident that attribution scores for \u201cfollowed\u201d instances consistently outperform those of \u201cunfollowed\u201d across all datasets. This distinction is sta-\ntistically validated by notably low p-values, ranging from 9.2e\u22127 to 7.2e\u22124. In addition, the LIMA dataset, with its typical instruction-only prompts, shows a higher importance density on instructions than that on the Self-Instruct and MT-Bench datasets. This is expected as models can get distracted when they operate the prompts containing both instructions and contexts. Further exploration for case studies in Appendix B.2 suggests that instruction fine-tuned models may pretend to follow instructions without realizing user instructions. In essence, Table 1 underscores that instruction fine-tuned models excel in identifying and harnessing instruction words from user prompts for response generation."
        },
        {
            "heading": "3.3 EXPLORING PROMPT POSITION WITH IMPORTANCE DENSITY",
            "text": ""
        },
        {
            "heading": "3.3.1 EXPERIMENT DESIGNS",
            "text": "Each input prompting text from our datasets is divided into individual sentences, with each sentence further split into five same-length segments. We normalize the density scores for a sentence by dividing by their sum and then accumulate them for each segment. The averaged attribution proportions for each segment within the input sentences are depicted in Figure 2."
        },
        {
            "heading": "3.3.2 EXPERIMENT RESULTS",
            "text": "Obs-3: Instruction fine-tuned models still overlook the middle and tail of input prompts, but less so than pre-trained models. Figure 2 shows the importance density score distributed on different segments of input sentences. Both pre-trained and fine-tuned models reveal a notable \u201cU\u201d-shape across all datasets. This is also known as \u201clost in the middle\u201d (Liu et al., 2023b), where they show that SOTA models (e.g. ChatGPT (Ouyang et al., 2022)) can overlook central inputs. Unlike their focus on machine reading comprehension, our analysis is grounded on our gradient-based importance density score on diverse prompting texts, suggesting that this issue is commonly and intrinsically existing. When comparing pre-trained to fine-tuned models, we spot a sharper \u201cU\u201d in the former, which becomes less obviously after instruction fine-tuning, pointing to a behavior shift. Contrarily, Liu et al. (2023b) found no such change after fine-tuning, possibly due to their reliance on MPT (Team, 2023), which is optimized for long text understanding, differed to typical language models."
        },
        {
            "heading": "4 EVOLUTION OF KNOWLEDGE IN FEED-FORWARD NETWORKS",
            "text": ""
        },
        {
            "heading": "4.1 TOOL: INTERPRET FEED-FORWARD NETWORKS WITH \u201cCONCEPT\u201d DESCRIPTION",
            "text": "We aim to interpret the knowledge of feed-forward networks in the concept level. We treat each feed-forward network \u03c3(XW\u22a4u )Wp as key-value memories (Geva et al., 2020), where each row vector of Wu and Wp stores a textual pattern. However, if we simply map each pattern vector to word embeddings for interpretation as Dar et al. (2022) did, it might not give us the full picture of all these patterns. Our main interest is to understand the broader concepts behind them. Thus, we propose to seek a set of orthogonal vectors that captures the major directions of these patterns spread. Formally, given the patterns stored in Wp, we first construct the covariance matrix of dimensions across these pattern as C = W\u0303\u22a4p W\u0303p, where W\u0303p is the centralized matrix of Wp \u2208 RD\n\u2032\u2032\u00d7D with zero mean of each column. Then the orthogonal basis vectors V of these patterns should satisfy:\nCV = \u039bV, (2)\nwhere each column vector of V \u2208 RD\u00d7D is unit length, \u039b = diag([\u03bb1, ..., \u03bbD]), and \u03bb1 \u2265 ... \u2265 \u03bbD \u2265 0. In this context, our primary focus lies on the top-R values of \u039b along with their corresponding column vectors in V. This is due to the fact that they show the principal directions of the encoded patterns from Wp. We then project each principal vector to the word embedding space Eo and find the top-K relevant words for interpretation: Er = argmaxV\u2032\u2208V,|V\u2032|=K \u2211 w\u2208V\u2032 V\n\u22a4[r]Eo[w], where V\u22a4[r] is the r-th column vector of V, Eo[w] is the output word embedding of w. Since vr is an unit vector, V\u22a4[r]Eo[w] measures the projection length of the word vector on this direction. Thus, it is naturally to represent this vector with the words having largest projection length, and the word list could be further summarized as a textual description by a human/machine annotator."
        },
        {
            "heading": "4.2 EXPERIMENT DESIGNS",
            "text": "Before applying our method, we crease a new vocabulary derived from ShareGPT (RyokoAI, 2023) to make the candidate words V more understandable compared to a large number of sub-tokens from the build-in LLaMA vocabulary. We then analyze the first 300 basis vectors of each feed-forward network from LLaMA and Vicuna with their top-15 relevant words. Table 2 provides sample word lists and their descriptions, and more cases are available in Appendix D. The detailed settings and statistics of concept descriptions are shown in Appendix C.2.\nTo study the knowledge evolution, we condense tasks from (Zheng et al., 2023; Ouyang et al., 2022) to scenarios including writing, math, coding, and translation. We then identify which scenarios a concept could be used for (see Appendix C). Note that, some concepts may fit multiple scenarios. Also, we sort concepts into phonology 7, morphology 8, syntax, or semantics linguistic levels based on (Thomas, 2005). Table 3 displays the percentage of scenario applicability and linguistic levels."
        },
        {
            "heading": "4.3 EXPERIMENT RESULTS",
            "text": "Obs-4: The principal vectors of feed-forward network neurons provide concept-level understanding to the encoded knowledge. In Table 2, the word lists for five principal vectors span a diverse array of topics, ranging from medical (\u201cmedical\nabbreviation\u201d) to linguistic (\u201cstarting with the\u201d). Notably, the concept of medical abbreviations stands out, as it\u2019s often difficult for human annotators to discern its medical relevance. This indicates the advantage of utilizing machine annotators for their vast knowledge. The well interpretability of principal vector demonstrated by Table 2 suggests that there is no necessity to interpret individual neurons in the model, which would be impractical to study near 0.35M neurons from Vicuna-7B.\n7Phonology studies sound systems, e.g. words with \u201cle\u201d sound: brittle, tackle, chuckle, pickle. 8Morphology studies word structure, e.g. words with \u201csub-\u201d prefix: subarray, subculture, subway.\nObs-5: Instruction fine-tuning shifts the principal vectors of feed-forward network neurons toward user-oriented tasks without moving them across linguistic levels. We observe from Table 3 that Vicuna encodes more concepts than LLaMA for writing, coding, and math tasks, with the difference in writing and coding being statistic significant (p < 0.1). However,\nthat of concepts for translation is significantly reduced after fine-tuning, indicating multi-linguistic knowledge is forgetting. Although we could observe the changes over the user view, that from the linguistic view remains the same. In particular, Vicuna and LLaMA show nearly identical distributions across the four linguistic levels. None of them are statistically significant (p > 0.1). This observation suggests instruction fine-tuning does not alter the distribution of concepts across linguistic levels.\n1-4 5-8 9-1 2 13 -16 17 -20 21 -24 25 -28 29 -32\nLayers\n0.0\n0.2\n0.4\n0.6\n0.8\n% o\nf C on\nce pt\ns\nLlama-Semantic Vicuna-Semantic Llama-Syntax Vicuna-Syntax Llama-Morphology Vicuna-Morphology Llama-Phonology Vicuna-Phonology\npositional knowledge are learned in the top layers (Zeiler & Fergus, 2014; Selvaraju et al., 2016). However, keeping in mind that LLaMA is a generative model designed for predicting next words, this unusual trend makes some sense. Specifically, we conjecture that LLaMA learns more morphology knowledge (e.g., prefix and suffix patterns) in the last few layers for simulating a prefix-tree structure. Prefix-tree is a traditional data structure to achieve efficient text decoding (Fredkin, 1960; Giancarlo, 1995; Paladhi & Bandyopadhyay, 2008; Shan et al., 2012). By doing so, LLaMA could use fewer parameters to memorize more word collocations to complete the next-word prediction task. We leave explorations to this assumption as future work."
        },
        {
            "heading": "5 EVOLUTION OF KNOWLEDGE IN SELF-ATTENTION HEADS",
            "text": ""
        },
        {
            "heading": "5.1 TOOL: INTERPRET SELF-ATTENTION HEADS WITH WORD-WORD PATTERNS",
            "text": "We aim to interpret the behaviors of self-attention heads with word pairs. Given a self-attention head, relation between a pair of words (wa, wb) for wa, wb \u2208 V could be approximated by Aa,b \u221d \u2211D d Ei[wa]W h\u22a4 q [d] \u00d7 Ei[wb]Wh\u22a4k [d], indicating that the relation Aa,b linearly relates to the activations of column vectors (neurons) of weights Whq and W h k . Therefore, we interpret the behavior of a self-attention head by aggregating the word-pairs activated by its neuronpairs. Specifically, we first interpret neurons Wh\u22a4q [d] and W h\u22a4 k [d] by collecting the top-K words\nthat could most activate them, i.e. Edq = argmaxV\u2032\u2286V,|V\u2032|=K \u2211\nw\u2208V\u2032 Ei[w] \u00b7 (Wh\u22a4q [d])\u22a4 and Edk = argmaxV\u2032\u2286V,|V\u2032|=K \u2211 w\u2208V\u2032 Ei[w] \u00b7 (Wh\u22a4k [d])\u22a4. We then form word pairs cross Eq and Ek by connecting the words having a high probability to appear within the same context, since self-attention mechanism is designed to capture word relations within input texts. Practically, we approximate this probability by computing the cosine similarity of their GloVe (Pennington et al., 2014) word embeddings, which has been proved as a decomposition of word-word co-occurrence matrix Edqk = {(wq, wk) : cos(eq, ek) > \u03b8}, where wq \u2208 Edq , wk \u2208 Edk , eq, ek are their GloVe embeddings, and \u03b8 is a threshold. Finally, the behavior of the a self-attention head is described with frequent word-pairs activated by its neurons."
        },
        {
            "heading": "5.2 EXPERIMENT DESIGNS",
            "text": "We applied the method to both LLaMA and Vicuna, isolating the top-100 most activated words for neurons Whq [d] and W h k [d]). From these, we form word pairs with a dynamically determined threshold \u03b8. This threshold is established by calculating the cosine similarity between a word and the top 1000 words from our ShareGPT vocabulary with GloVe word embeddings. We then derived the mean and standard deviation of these cosine similarities for each word. The threshold for a given word was set at its mean plus 1.96 times its standard deviation. For any pair of neurons, the threshold is determined by the greater of the two individual word thresholds. Once the word pairs of neurons are obtained, the self-attention head\u2019s behavior is described with word pairs recognized by at least two of its neuron pairs. We show some examples in Table 11 and Table 12.\nFor a deeper understanding of self-attention evolution, we executed two analyses. First, we tract alterations in word pairs after instruction fine-tuning, represented by the intersection rate M = Ept\u2229Eft Ept\u222aEft , Ept and Eft denote the top-100 word pairs of pre-trained and fine-tuned models, respectively. Notably, the \u201ctop\u201d word pairs for a neuron pair exhibit the highest cosine similarity, while those for a self-attention head are the pairs most frequently mentioned by its neuron pairs. Figure 4 visualizes the change rate 1\u2212M across neuron pairs and heads over various layer groups. Furthermore, we investigated shifts in instruction verbs (e.g., \u201cwrite\u201d, \u201ccreate\u201d, and \u201cclassify\u201d) within the self-attention heads. Initially, we identify the 16 most recurrent verbs from the self-instruct dataset (Wang et al., 2022) as foundational instruction verbs. This list is manually expanded to 34 verbs to encompass common GPT3 use cases (Ouyang et al., 2022). For contrast, we assemble a control set of 1000 frequent verbs from general English corpus (Speer, 2022). For each verb, we count the proportion of self-attention heads transitioning from non-inclusion to inclusion of the certain verb after fine-tuning, and report this proportion for each group of verbs in Table 4.\n5.3 EXPERIMENT RESULTS\nObs-7: Instruction fine-tuning marginally changes the behaviors of self-attention neurons, but significantly modifies self-attention heads. Figure 4 reveals a noticeable climbing change rates of word pairs to both neurons and heads as layer depth increases, which is not such surprising. Specifically, the change rate of neuron word pairs smoothing ascend from 20.52% to 22.99%, while that of head levels raise volatility from 30.95% to 35.81% and reaches the maximum change at layers 21-28. Moreover, while the change rate of word pairs for neurons is notably low at around 20%, that for self-attention\nheads varies between 30% and 37%. This suggests that individual self-attention neurons encode diverse knowledge, leading the self-attention heads to extract varied word relationships accordingly.\nObs-8: Instruction fine-tuning encourages lower self-attention heads to encode more word-word patterns related to instruction verbs. Table 4 shows that more self-attention heads from lower (1-8) and middle (8-24) layers encode word-word relations with instruction verbs after instruction fine-tuning, where the layer groups 1-8 and 9-16 reach statistic signifi-\ncance (p < 0.1). In particular, about 28.78% of the heads from 1-8 layers transits from not encoding instruction verbs to encoding instruction verbs, nearly double the 14.53% of that encodes new general verbs after instruction tuning. Overall, averaging over 20% self-attention heads that encode more instruction relations, only about 14% encode more general verbs. This difference indicates that instruction fine-tuning teaches self-attentions to identify various detailed instructions."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "Our findings are consistent with observations from recent studies. 1) The importance of prompt diversity is highlighted by both us and Zhou et al. (2023); Wang et al. (2022). Since our three findings suggest that instruction fine-tuning links the pre-trained model to user tasks, we could expect a better alignment with human intentions if the model is exposed to broader tasks. 2) The efficacy of training self-attention with first priority (LoRA fine-tuning) (Taori et al., 2023; Zhang et al., 2023; Juletx, 2023) is corroborated by Finding-1 and Finding-3. Specifically, Finding-1 illustrates the capability to distinguish instruction words is essential to the instruction following, while Finding-3 highlights that self-attention heads learn more instruction knowledge. 3) The advantages of training feed-forward networks (fully fine-tuning) (Sun et al., 2023) are evident from Finding-2 and Finding-3, demonstrated that feed-forward networks update their knowledge toward user tasks, as a supplement to instruction recognition by self-attention. (iv) We also validate the \u201cLost-in-the-Middle\u201d observed by Liu et al. (2023b) and note that instruction fine-tuning alleviates this inherent behavior in pre-trained models.\nOur findings also pose three open questions: 1) Can we use the importance density score as a training object in instruction fine-tuning to reflect the instruction following capability inherently? 2) Do LLMs simulate a prefix tree in their upper layer for efficient decoding? 3) How do self-attention modules and feed-forward networks collaborate to generate a helpful response for the user?"
        },
        {
            "heading": "7 RELATED WORK",
            "text": "Interpreting LLMs. The interpretability of LLMs has received significant attention in recent years. Majority investigations aimed to understand the decision-making processes of LLMs for a specific task or dataset, which involves feature attribution methods (Li et al., 2015; Vig, 2019; Kokalj et al., 2021), attention-based methods (Vig, 2019; Barkan et al., 2021),and example-based methods (Kim et al., 2018; Wu et al., 2021). With the emergent capabilities of LLMs, a wave of research turned to understanding why LLMs could perform in-context learning (Xie et al., 2021; Olsson et al., 2022; Li et al., 2023b; Wei et al., 2023; Xiong et al., 2023; Duan et al., 2023; Varshney et al., 2023), where they typically conduct empirical control experiments to prove their assumption. In parallel, some delved deeper into the core components of LLMs, including the self-attention mechanism (Elhage et al., 2021; Sukhbaatar et al., 2019) and feed-forward networks (Press et al., 2019; Geva et al., 2020; Voita et al., 2023; Petroni et al., 2019; Meng et al., 2022; Huang et al., 2023). Recent work also use LLMs as tools to enhance the interpretability of themselves (Bills et al., 2023; Singh et al., 2023). Compared with these studies, we propose a series of advanced interpretation methods that could produce explanations for massive size large language models for open-domain text generation.\nInstruction Fine-tuning. Instructing LLMs to conduct multiple tasks in an open-domain has intrigued numerous researchers. Initial efforts focused on utilizing various downstream datasets, where researchers may pre-train prompting templates (Vu et al., 2021; Su et al., 2022; Zhang et al., 2022) or even the entire models Raffel et al. (2020); Cui et al. (2022). Recently, Ouyang et al. (2022) suggested that pre-trained models can not directly serve their users because the pretraining objectives do not align with the user\u2019s intentions. To address this misalignment, they proposed fine-tuning on user prompt-response paired datasets. Following this path, scholars (Wang et al., 2022; Zhou et al., 2023; Peng et al., 2023; Cao et al., 2023) train models on diverse paired datasets. Additionally, some researchers Liu et al. (2023b); Li et al. (2023a); Varshney et al. (2023); Liu et al. (2023a); Lu et al. (2023) build well-designed datasets to highlight the limitations of fine-tuned models, showcasing the inadequacies of the alignment approach. Unlike previous research, we study the evolution of LLMs driven by instruction fine-tuning, providing an inherent view to understand this process."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "This paper presents an inherently comprehensive analysis of instruction fine-tuning for user intention alignment by quantitatively and qualitatively comparing the interpretations between pre-trained and fine-tuned models. Our findings indicate that instruction fine-tuning links the pre-trained model to user intentions, including encoding more instruction words\u2019 knowledge within self-attention, and rotating general knowledge from feed-forward networks towards user usage. It is worth mentioning that the interpretability toolbox used in this study can also support future general research on LLMs."
        },
        {
            "heading": "A.1 EXPERIMENT SETTINGS",
            "text": "Contrary to the examples shown in the primary content, which utilize golden responses, our focus here is on the connections between user inputs and model outputs. To achieve this, we generate responses from LLaMA and Vicuna, following the protocol laid out in Sec.2.2. Subsequently, we derive the salient maps as per the technique introduced in Sec.3.1.1.\nTo ensure the maps provide an accurate depiction of the generation process, we set L = 10 and b = 0. Each map\u2019s vertical axis denotes the prompting texts, whereas the horizontal axis symbolizes the generated responses. The intensity of each data point corresponds to the association strength between the respective input and output tokens, with brighter points indicating stronger relationships (visualizing with the best colors)."
        },
        {
            "heading": "A.2 EXPERIMENT RESULTS",
            "text": "Figure 8-13 validate our qualitative assessment that instruction words in user inputs are critical in guiding the generation process. It\u2019s evident that each context word typically has limited influence on the response. Collectively, these salient maps underscore the validity of input attribution, achieved by gauging the density of the sparse and normalized importance scores."
        },
        {
            "heading": "B ANALYZING IMPORTANCE DENSITY",
            "text": ""
        },
        {
            "heading": "B.1 EXPERIMENT SETTINGS",
            "text": "For each collected prompting text from the three public datasets , we let Vicuna and LLaMA generate its corresponding response (Sec. 2.2); we then manually identify the instruction sentences from each input prompt and annotate whether the response provides helpful information (\u201cfollowed\u201d) or not (\u201cunfollowed\u201d).\nAnnotate instruction and context. Specifically, the instruction usually describes the user intention with some background (optional), which could be both very long 9 or very concise 10. Note that we annotate the instruction words on the sentence level, and the template words as \u201cInput:\u201d and \u201cOutput:\u201d are not considered. For some prompts, the instruction words may be distributed in both the head and tail of the input text, and we will consider them together. Among these instruction sentences, we define the rest of the input prompt as context words, which is unnecessary to the input prompting text.\nAnnotate Followed or Unfollowed Response We consider the helpfulness of the response as the ability of instruction following described by Ouyang et al. (2022). Therefore, if a response is helpful to the user, then we label it with \u201cfollowed\u201d. Specifically, we consider four levels of helpfulness: L1 - the model is randomly saying something or just repeating itself; L2 - the model provides some information that could be used to answer the question, but the model fails to organize it well; L3 - the model generates a response that generally follows the prompts, but missing some detailed instructions; L4 - the response is perfect as a human response. In our study, we consider the responses from L2 to L4 as \u201cfollowed\u201d. Note that we are not concerned about hallucination issues in our study.\n9A long instruction: \u201cHow do social media platforms influence the way people consume and share news, and what are the potential implications for the spread of misinformation?\u201d\n10A short instruction: \u201cto English:\u201d\nautomation is becoming increasingly prevalent, it is more important to prioritize job creation.\\n\\nIn a world where automation is becoming increasingly prevalent, it is more important to prioritize technological\nprogress.\\n\\nIn a world where automation is becoming increasingly prevalent, it is important to prioritize both job creation and technological progress.\\n\\nIn a world where automation is becoming increasingly prevalent, it is not necessary to prioritize either job creation or technological progress.\nB.2 EXPERIMENT RESULTS\nRewrite the sentence and make your writing clearer by\nkeeping it concise. Whenever possible, break complex\nsentences into multiple sentences and eliminate unnecessary\nwords.\\n\\nInput: If you have any questions about my rate or if you find it necessary to increase or decrease the scope\nfor this project, please let me know.\\n\\nOutput: Vicuna: Do you have any questions about my rate or do you need to\nadjust the project scope? Please let me know.\nInstruction fine-tuned models may pretend to follow the instructions. Figure 5 visualizes a salient map of an instance related to writing enhancement (please see the caption for details). Vicuna\u2019s response addresses grammatical errors and modifies sentence structures for improved clarity. A key observation from the figure is that only the first three instruction tokens guide the response generation (Red Box). Specifically, the first three words are \u201cThe sentence you\u201d, which seems to be not the key instruction verbs like \u201cRewrite\u201d from the second sentence. Also, some words from the context part are acted as instruction words (Blue Box), which are \u201c\\\\nInput:\u201d and \u201c\\\\nOutput:\u201d from the prompt template. These are the words that should be considered as the instruction words since they do not provide the user\u2019s intentions. Additionally, a distinctive diagonal line spans the context section, hinting at the model\u2019s predisposition to echo context from preceding content. This figure suggests that the model is leveraging inherent language modeling ability rather than identifying and acting upon the given instructions. Thus, we point out that assessing instruction-following abilities based solely on the correlation between input prompts and output responses might not provide an accurate reflection of the model\u2019s internal behaviors, while it is still a common strategy to develop the Reward model for RLHF process (Ouyang et al., 2022).\nHowever, we have identified certain instances where our importance density fails. This is predominantly due to our density function\u2019s lack of positional awareness. For instance, in Figure 6, the entire user input comprises instruction words. The map suggests that these words play a crucial role in guiding the generation, even towards the latter part of the responses. Under our hypothesis, it would appear the model is following user instructions. Yet, Vicuna seems to merely reiterate the input prompt repetitively, resulting in recurring diagonal patterns. We recommend future research to address this shortcoming, either by adopting a density function that\u2019s positionally aware or by integrating a step to identify and handle repetitive responses early on."
        },
        {
            "heading": "C SCALING UP WITH AUTOMATED TOOLS",
            "text": "We build upon recent advancements in automated interpretation, using cutting-edge large language models (Taori et al., 2023; Peng et al., 2023; Steven et al., 2022) to emulate human annotators in generating high-level interpretations. By leveraging machine annotators, we could easily scale up our methods to analysis the entire model, providing a more solid results to our findings."
        },
        {
            "heading": "C.1 EXPERIMENT SETTINGS",
            "text": "Generating Configuration. We employ ChatGPT 11 as our machine annotator. Our experiments utilize the gpt-3.5-turbo-0613 model with a hyper-parameter top-p=0.9 for nuclear sampling. To mitigate the variability in language model outputs, we repeat the experiment five times. In each iteration, we first condense the top-K words of a specific basis vector into a distinct concept, then pinpoint the user-oriented tasks and linguistic levels associated with these concepts. For our initial interaction with ChatGPT, the temperature is set to 0\u2014signifying a greedy search strategy. In subsequent interactions, we set the temperature to 1. Nevertheless, when identifying tasks and levels, we consistently maintain the temperature at 0.0.\nPrompt Design. Effective automated interpretation hinges on well-crafted prompts. We meticulously design these prompts using three strategies: role-play, in-context conversational examples, and exclusively high-quality examples.\nTemplate-1: Describing words with concise concepts. The top-15 most activated words coming from the method presented in Sec. 4.1 will be directly appended to this template. System: You are a neuron interpreter for neural networks. Each neuron looks for one particular concept/topic/theme/behavior/pattern. Look at some words the neuron activates for and summarize in a single concept/topic/theme/behavior/pattern what the neuron is looking for. Don\u2019t list examples of words and keep your summary as concise as possible."
        },
        {
            "heading": "If you cannot summarize more than half of the given words within one clear concept/topic/theme/behavior/pattern, you should say \u2019Cannot Tell\u2019.",
            "text": "User: Words: January, terday, cember, April, July, September, December, Thursday, quished, November, Tuesday. Agent: dates.\nUser: Words: B., M., e., R., C., OK., A., H., D., S., J., al., p., T., N., W., G., a.C., or, St., K., a.m., L.."
        },
        {
            "heading": "Agent: abbrevations and acronyms.",
            "text": "User: Words: actual, literal, real, Real, optical, Physical, REAL, virtual, visual. Agent: perception of reality.\nUser: Words: Go, Python, C++, Java, c#, python3, cuda, java, javascript, basic. Agent: programing languages.\nUser: Words: 1950, 1980, 1985, 1958, 1850, 1980, 1960, 1940, 1984, 1948."
        },
        {
            "heading": "Agent: years.",
            "text": ""
        },
        {
            "heading": "User: Words:",
            "text": "11https://platform.openai.com/docs/guides/gpt\nTemplate-2: Identifying applicable user-oriented tasks. Any automated summarized concept will be directly concatenated to this template. We check the writing task into three detailed scenarios, as our preliminary experiments indicated that ChatGPT often deemed nearly every concept suitable for writing. During post-processing, we regard any of these detailed tasks as the primary writing purpose. System: Which of the following assistant tasks can the given concept is used for?\\n\\nTasks: daily writing, literary writing, professional writing, solving math problems, coding, translation. Return \u2019None\u2019 if it cannot be used for any of the above tasks. If it could be used for multiple tasks, list all of them and seperate with \u2019;\u2019.\nUser: Concept: Words are social media post tags. Agent: daily writing\nUser: Concept: Words are Latex code for drawing a grouped barchart. Agent: professional writing\nUser: Concept: Words are foreign words or names. Agent: translation"
        },
        {
            "heading": "User: Concept: Words are URLs. Agent: None",
            "text": "User: Concept: Words are Words related to configuration files and web addresses. Agent: coding\nUser: Concept: Words are rhyming words. Agent: literary writing\nUser: Concept: Words are programming commands and terms. Agent: coding"
        },
        {
            "heading": "User: Concept: Words are",
            "text": "Template-3: Identifying linguistic level. Any automated summarized concept will be directly concatenated to this template. System: You are a linguist. Classify the provided concept into one of the following categories: Phonology, Morphology, Syntax, and Semantic.\nUser: Concept: Words are dates. Agent: semantic\nUser: Concept: Words are perception of reality. Agent: Semantic\nUser: Concept: Words are abbrevations and acronyms. Agent: Morphology\nUser: Concept: Words are related to actions or activities. Agent: Syntax\nUser: Concept: Words are medical abbrivations. Agent: Semantic"
        },
        {
            "heading": "User: Concept: Words are URLs. Agent: Morphology",
            "text": ""
        },
        {
            "heading": "User: Concept: Words are verbs.",
            "text": "Agent: Syntax"
        },
        {
            "heading": "User: Concept: Words are adjective. Agent: Syntax",
            "text": "User: Concept: Words are rhyming words. Agent: Phonology\nUser: Concept: Words are programming languages. Agent: Semantic"
        },
        {
            "heading": "User: Concept: Words are",
            "text": "C.2 EXPERIMENT RESULTS\nFigure 7 illustrates the proportion of word lists that can be induced to a concise concept by our machine annotator. According to our template, if \u201cCannot Tell\u201d exists in the word list descriptions, we consider that this concept has failed to be interpreted. We have observed that the Vicuna and LLaMA models display comparable levels of interpretability, with no significant distinctions between them. A noticeable trend emerges as the number of layers increases: the ability to explain their encoded concepts improves. Specifically, within layers 24-28, the average interpretability rate for the first\n30 concepts peaks at 91.67%. This high interpretability rate underscores the effectiveness of our proposed method. It can aptly convey in clear, concise text the knowledge encoded by these models. However, there\u2019s a caveat: knowledge encoded closer to the output layer, specifically between layers 28-32, becomes more challenging to elucidate. Interestingly, this particular challenge wasn\u2019t present when applying automated interpretation tools to GPT-2 (Millidge & Black, 2022), indicating the behaviors between small and large models are different. Additionally, our findings indicate a decreasing trend in interpretability for concepts that are ranked further back. Overall, these results validate the efficacy of our proposed method in analyzing the knowledge encoded within models.\nTable 5-8 enumerates the words that experienced the most significant changes in frequency after instruction fine-tuning, we also show the change of rank following. These words are meaningful words (at least four characters and not a stopword) extracted from the concept descriptions generated by our machine annotator. From the tables, certain words, notably \u201dlanguage\u201d, \u201dprogramming\u201d, and \u201dprocess\u201d, displayed significant shifts in frequency after instruction fine-tuning. Linguistic terms (\u201cSpanish\u201d, \u201ctranslation\u201d) and technical terms (\u201cmethod\u201d, \u201cprogramming\u201d and \u201csoftware\u201d) exhibited noticeable changes in various layers. Interestingly, \u201clanguage\u201d consistently surfaced in almost every layer group, with its frequency both rising and dropping. This observation indicates that different layers are responsible for encoding different categories of knowledge. Specifically, the bottom layers are responsible for storing more general basic knowledge (\u201cbehavior\u201d, \u201coperation\u201d, \u201cadjective\u201d), the middle layers are responsible for learning more abstract knowledge for serving users (\u201cfunctions/methods\u201d, \u201cprogramming\u201d, \u201csoftware development\u201d), and the higher layers are responsible for learning more knowledge for efficient text generation (\u201cstart with\u201d, \u201crhyming\u201d, \u201csound\u201d, \u201cletter\u201d,). Broadly, the increased mention of words pertinent to user scenarios after finetuning underscores the model\u2019s refined focus on user-centric tasks and applications.\nD INTERPRETING FEED-FORWARD NETWORKS\nTable 9 and Table 10 list more cases that are well interpreted by ChatGPT-turbo-3.5-0613. From these cases, we found that the concept descriptions are generally well to reflect the behind of the word lists.\nE INTERPRETING SELF-ATTENTION HEADS\nTable 11 and Table 12 list more word pairs for the self-attention heads from the first and the last layers. Typically, these cases are evidence that the extracted word pairs show some insightful relations when we read each one individually. However, when we read them together, it cannot reflect such a concise concept as the feed-forward networks.\nTa bl\ne 9:\nR ep\nre se\nnt in\ng w\nor ds\n,a pp\nlic at\nio n\nsc en\nar io\ns, an\nd lin\ngu is\ntic le\nve lo\nft he\nco nc\nep ts\nen co\nde d\nby th\ne 32\ned (l\nas t)\nfe ed\n-f or\nw ar\nd ne\ntw or\nk in\nV ic\nun a.\nR an\nk Sc\nen ar\nio L\nin gu\nis tic\nC on\nce pt\nTo p-\n15 W\nor ds\n1 w\nri tin\ng; tr\nan sl\nat io\nn m\nor ph\nol og\ny ph\nra se\ns an\nd ab\nbr ev\nia tio\nns ev\ner yb\nod y,\nre ga\nrd le\nss ,i\nn, w\nhi ls\nt, a.\nC .,\nvs .,\nam id\n,I .,\nU .S\n., Ph\n.D .,\nan yw\nay ,a\n.m .,\n6. ,9\n., .. . 6 w ri tin g; tr an sl at io n m or ph ol og y m ed ic al ab br ev ia tio ns C B T, R\nT K\n,R T,\nR H\n,H RV\n,M T,\nH B\n,P H\n,P T,\nG nR\nH ,H\nR M\n,P W\nV, R\nS, T\nB ,R L 7 w ri tin g; co di ng se m an tic U R L s an d w eb -r el at ed te rm s //e xa m pl e. co m /d at a, //e xa m pl e. co m /d at a. js on , //w w w .y ou tu be .c om /w at\nch ,\n//i m\nag e.\npo lli\nna tio\nns .a\ni/p ro\nm pt\n/A ,\n//i m\nag e.\npo lli\nna tio\nns .a\ni/p ro\nm pt /, th e, ev en t.t ar ge t.v al ue , //a pi .e xa m pl e. co m /d at a, //w w w .n pm js .c om /p ac ka ge /m at ri xm at h, //e xa m pl e. co m /a pi /d at a, co m m un ity - ba se d, se cu ri ty -r el at ed ,i nd us tr ysp ec ifi c, //l ee tc od e. co m /d is cu ss ,/ /e ng ag el ea ds - ga lle ry .s 3. us -w es t2. am az on aw s. co m /P ro fit Q uo te sV 2\n8 w\nri tin\ng se\nm an\ntic st\nar tin\ng w\nith \u201dt\nhe \u201d\nth e,\nth eo\nlo gi\nca l,\nth eo\nlo gy\n,t he\nad ,p\nrim ar\nily ,i\nnv ol\nvi ng\n,m ai\nnl y,\nth ea\nte r,\nal on\ngs id e, th ro ug ho ut ,t he at ri ca l, sp ec ifi ca lly ,t he ta ,t he or is t, re ga rd le ss\n9 co\ndi ng\nse m\nan tic\nso ft\nw ar\ne de\nve lo\npm en\ntt oo\nls an d co nc ep ts re\nC A\nPT C\nH A\n, R\nE ST\nFR A\nM E\nW O\nR K\n, C\nA PT\nC H\nA ,\nA R\nIS T\nO C\nR A T, so ph is tic at io n, R E G E X P, so ph is tic at ed , PE T SC C O M M W O R L D , JE D I M IN D T R IC K S 01 , IN ST A L L E D A PP S, A R G FR A ,c re de nt ia ls .y am l, O W A SP ,G A R E T H ,s H SA D L\nH 11 co di ng se m an tic pr og ra m m in g ta sk s or ac tio ns pr ov id e, nl tk .c or pu s. st op w\nor ds\n.w or\nds ,\nsk le\nar n.\nm et\nri cs\n, in\nst al\nl.s h,\nfil e.\nw ri\nte ,\nse rv\ne, gi\nve ,\nre s.\nse nd\n, cl\nf.fi t,\npi ck\nle ba\nll, pr\nom ot\ne, ui\nnt 25\n6, gi\nve aw\nay ,\ncr ea\nte ,\nSt .\n13 co\ndi ng\nse m\nan tic\npr og\nra m\nm in\ng fu\nnc tio\nns /m\net h-\nod s\nsk le\nar n.\nfe at\nur e\nex tr\nac tio\nn. te\nxt ,\nre .s\nub ,\nsu bp\nro ce\nss .ru\nn, a.\nC .,\nz. st\nri ng\n, a.\nm .,\ne. ta\nrg et\n.v al\nue ,\nre qu\nes t.d\nat a.\nge t,\np. m\n., da\nta .le\nng th\n, re\n.s ea\nrc h,\nf.w ri\nte ,\n//e xa\nm -\npl e.\nco m\n/d at\na. js\non ,n\nltk .c\nor pu\ns. st\nop w\nor ds\n.w or\nds ,e\nve nt\n.ta rg\net .v\nal ue\n14 w\nri tin\ng; co\ndi ng\nm or\nph ol\nog y\nac ro\nny m\ns an\nd sp\nec ifi\nc te\nrm s\nsH SA\nD L\nH ,r\neC A\nPT C\nH A\n,C A\nR R\nE R\nA S,\ncv 2.\nC O\nL O\nR B\nG R\n2G R\nA Y,\nV X\nL A\nN ,\nA R\nIS TO\nC R\nA T,\nO W\nA SP\n,C A\nPT C\nH A\n,L G\nB T\nQ ,S\nO U\nT H\nG A\nT E\n,S A\nR A\nN T\nID IS , R T K ,R E SO ,S IL V IA ,O M E N S\n16 m\nat h;\nco di\nng se\nm an\ntic nu\nm be\nrr an\nge s\nG -4\n-8 ,5\n-, 4-\n,1 -,\na,a\n-z A\n-Z 0-\n9, 3-\n,2 -,\n1- 5,\n5- 6,\n5- 7,\n2- 5,\n4- 5,\n4- 6,\n3- 5\n21 m\nat h\nse m\nan tic\nnu m\nbe rs\n3, 4,\n5, 2,\n50 0,\n8, 10\n,3 ,5\n00 ,7\n5, 00\n0, 1,\n50 0,\n25 ,0\n00 ,6\n,0 00\n,4 ,0\n00 ,5\n,0 00\n,7 ,0\n00 ,\n0, 0,\n0, 8,\n00 0,\n0, 1,\n40 1k\n24 no\ntl is\nta bo\nve se\nm an\ntic ch\nar ac\nte ri\nst ic\ns/ at\ntr ib\nut es\nhe al\nth -c\non sc\nio us\n,l ea\nrn in\ngba\nse d,\nC on\nte nt\n-S ec\nur ity\n-P ol\nic y,\nW in\ndo w\nsSu\npp or tC or e, he al th -r el at ed ,a -, pr of es si on al -l oo ki ng ,p ay -p er -c lic k, W ri te -H os t, us er -, cr ue lty -f re e, X -R ea lIP ,e ne rg yef fic ie nt ,Q -l ea rn in g, ea sy -t ous e\n32 tr\nan sl\nat io\nn se\nm an\ntic fo\nre ig\nn la\nng ua\nge s\n(p os\nsi bl y Ja pa ne se an d Fr en ch\n) itu\n, de\nsu ,\nD el\neu ze\n, Sh\nu, ba\nru ,\nm eu\n, -r\n, at\nra er\n, Pu\ntu ,\n-u ,\npu dd\nle ,\nsu\u0302 r,\nke lu\nar ,\nV eu\nill ez\n,M er u 35 w ri tin g ph on ol og y W or ds w ith th e \u201dl e\u201d so un d oo oO O 0O O\noo o,\nile ,\nbr itt\nle ,\ntl, ta\nck le\n, itl\ne, Is\nle ,\nJi ng\nle ,\npo st\n-a po\nca ly\npt ic\n, hl , M ic he le ,t ol ,p re ci so ,M ar le ne ,n ee dl\ne 42 tr an sl at io n se m an tic In do ne si an w or ds itu ,d es u, m eu ,T hu ,b ar u, va cu um\n,- u,\nSh u,\nsa tu\n,P ut\nu, flu\nct ua\ntio n,\nin di\nvi du , ch ih ua hu a, pe rp et ua tin g, A bu\n44 no\ntl is\nta bo\nve se\nm an\ntic de\nca de\ns 19\n60 s,\n19 50\ns, 19\n40 s,\n19 70\ns, 19\n80 s,\n19 30\ns, 19\n20 s,\n15 -m\nin ut\ne, 20\n16 /6\n79 ,6 0- m in ut e, 14 40 p, 75 5- 10 14 59 57 ,1 96 5, 19 63 ,1 94 6\n71 tr\nan sl\nat io\nn se\nm an\ntic ve\nrb s\nin di\nff er\nen tl\nan gu\nag es\nin cl\nuy en\n,w er\nen ,s\nof te\nn, br\nig ht\nen ,s\nho rt\nen ,p\ner m\nite n,\nbe ho\nef te\nn, ko\nnt en\n,c iti\nze n,\nte en\n,w ill\nen ,K\nar en\n,d ig\nita le\nn, st\nar te\nn, cr\nim en\n73 w\nri tin\ng se\nm an\ntic W\nor ds\nre la\nte d\nto \u201dw\nor ds\nen di ng w ith \u2019b \u2019\u201d 4b\n,b ub\nbl y,\n1b ,r\nb, -b\n,M ba\npp e,\nch ild\nbi rth\n,c ar\nb, bu\nlb ,h\ner b,\nhe b,\nC ol\nab ,l\nim b,\nb0 ,\n/b 74 co di ng se m an tic da ta ty pe s an d nu m be rs\n24 h,\nui nt\n25 6,\nin t2\n56 ,u\n32 ,i\nnt 32\n,K ag\ngl e,\nby te\ns3 2,\n23 2,\n22 5,\n27 28\n22 ,w\nag ,u h, 32 ,2 3, 32 5\n75 w\nri tin\ng m\nor ph\nol og\ny w\nor ds\nco nt\nai ni\nng th\ne sy\nlla bl e \u201dj ab \u201d\nsh ad\n,h ija\nb, m\nar ia\ndb ,C\nha d,\nsl ab\n,p ed\nic ab\n,t bs\np, ja\nb, sc\nar ab\n,r eb\nou nd\n,T as\nkR ab\nbi t,\nbe ad\n,C ol\nab ,s\ncr ee\nch ,A\nbd ul\n-J ab\nba r\n10 2\nw ri\ntin g;\ntr an\nsl at\nio n\nse m\nan tic\noc cu\npa tio\nns /jo\nbs co\nm pr\nes so\nr, vr\naa g,\nde st\nru ct\nor ,j\nur or\n,S ur\nve yo\nr, ka\nnt or\n,t re\nm or\n,e ff\nec to\nr, fla\nvo r,\nin ve\nst or\n,s ci\nss or\n,e xp\nlo ra\nr, pr\noj ec\nto r,\nes cr\nito r,\nla nj\nut\nTa bl\ne 10\n:R ep\nre se\nnt in\ng w\nor ds\n,a pp\nlic at\nio n\nsc en\nar io\ns, an\nd lin\ngu is\ntic le\nve lo\nft he\nco nc\nep ts\nen co\nde d\nby th\ne 1s\nt( fir\nst )f\nee d-\nfo rw\nar d\nne tw\nor k\nin V\nic un a. R an k Sc en ar io L in gu is tic C on ce pt To p15 W or ds 1 w ri tin g; tr an sl at io n m or ph ol og y A bb re vi at io ns an d ac ro ny m s in ,I ., an d, a. C ., O K ., a. m ., U .S ., Ph .D ., M ., B ., fo r, D .C ., vs ., Fe b. ,t o 7 no tl is ta bo ve se m an tic w or ds re la te d to \u201dv ar ia bi lit y\u201d ar ch ite ct ur al ,c om pr is e, re ce iv ab le ,u sa bl e, D ra w ab le ,v ar ia bi lit y, us ab ili ty ,v\nan -\nity ,c\nir cu\nla ri\nty ,s\nal iv\nar y,\nen d=\n,e ge\nt, vi\nse ,e\nnd ,P\nar ad\nis e\n25 w\nri tin\ng se\nm an\ntic W\nor ds\nre la\nte d\nto rh\nym in\ng im\nm er\nsi ng\n,l ee\nr, sa\nbe r,\nye r,\ndr ea\nm er\n,p ok\ner ,d\nee r,\nro lle\nr, va\nlu in\ng, re\nst er\n,t ra\nci ng , Tu ne r, sh ow er ,l os er ,b lo ck er\n27 co\ndi ng\nse m\nan tic\npr og\nra m\nm in\ng co\nnc ep\nts al\nta ,r\nel at\niv ed\nel ta\n,c on\nsu m\ned ,S\nys te\nm .T\nex t,\nac tio\nna bl\ne, \u2019p\nri ce\n,p ay\nab le\n,i sl\nan d,\n\u2019h re\nf, be\nla ka\nng ,r\nen ew\nab le\n,S ys\nte m\n.o ut\n.p ri\nnt ln\n,\u2019 ne\nw ,\u2019\nG en\ner al\n,a ct\nio n-\nor ie\nnt ed\n36 w\nri tin\ng se\nm an\ntic w\nor ds\nre la\nte d\nto ac\ntio ns\nor ac\ntiv -\niti es\ntin gl\nin g,\nB oo\nki ng\n, Ji\nng le\n, ci\ntin g,\nbi dd\nin g,\nad vi\nsi ng\n, T\nhi ng\n, am\naz in\ng, C\nM S,\nst riv\nin g,\nin fr\nin ge\nm en\nt, oc\ncu rr\nin g,\nJi ng\ndi ao\n,g ra\nbb in\ng, fa\nst -g\nro w\nin g\n37 w\nri tin\ng m\nor ph\nol og\ny W\nor ds\nre la\nte d\nto su\nffi x\n\u201d- ic\n\u201d cy\nst ic\n,M yt\nhi c,\npo lit\nic ,p\nan or\nam ic\n,fl av\nor fu\nl, op\nic ,a\nnt ic\n,p hy\nsi ci\nst ,i\non ic\n,c hr\non ic , em pl oy ab ili ty ,e ff ec to r, sp ic ,s ili co ne ,o bs tr uc tiv e\n38 w\nri tin\ng se\nm an\ntic ve\nrb s\nan d\nad je\nct iv\nes re\nla te\nd to\nac tio\nns an\nd be\nha vi\nor s\nva lu\nin g,\nbe ha\nvi ng\n,s ac\nrifi ci\nng ,a\ndv is\nin g,\nen vi\nro nn\nem en\nta l,\nco m\npo si\nng ,o\ncc ur\nrin g,\nen co\nur ag\nin g,\nup br\nin gi\nng ,o\npp os\nin g,\nB ri\nng ,p\net al\n,c ha\nrg in\ng, ar\nriv in\ng, re\nga l\n44 no\ntl is\nta bo\nve se\nm an\ntic qu\nal iti\nes or\nch ar\nac te\nri st\nic s\nbr id\ngi ng\n,y ou\nng es\nt, \u00b1,\nsm ar\nte st\n,b rig\nht es\nt, da\nrk es\nt, fu\nlle st\n,p es\nt, co\nm m\nase\npa ra\nte d,\nce le\nst ia\nl, vo\nw ,r\nic he\nst ,c\nhe st\n,i lm\nai se\ne, en\ndo w\n58 w\nri tin\ng se\nm an\ntic ve\nrb sr\nel at\ned to\nac tio\nns or\nbe ha vio rs Po\ne, w\nee ,a\ndv oc\nat e,\nre la\ntin g,\nm od\ner at\ne, ad\nvo ca\ntin g,\nad vo\nca te\nd, to\nm at\ne, pa\nrt ic ipa te ,fl ee ,m od er at in g, co m pl ex e, an tic ip at e, pa rt ic ip at in g, re ite ra te\n64 w\nri tin\ng m\nor ph\nol og\ny ad\nje ct\niv es\nw ith\n\u201d- iv\ne\u201d su\nffi x\nin si\ngn ifi\nca nt\n,r es\ntr ic\ntiv e,\npe rs\nua si\nve ,s\npo rt\niv e,\ndi st\nin ct\niv e,\nde ce\niv e,\nex pr\nes si\nve ,\nde ci\nsi ve\n,c ap\ntiv e,\nse cr\net iv\ne, ad\ndi ct\niv e,\nde fe\nct iv\ne, di\nge st\niv e,\nin tr\nus iv\ne, ab\nus iv e 70 w ri tin g m or ph ol og y W or ds en di ng in \u201dy \u201d or co nt ai nin g \u201dy \u201d th is .y ,fl as hy ,- y, so y, sh y, 3y ,2 y, to y, M s. ,p re y, nn .C on v2 d, un is td .h ,\u2019\ns3 ,\u2019 s, pr esa\nle 72 w ri tin g se m an tic ad je ct iv es re la te d to ch ar ac te ri stic s or pr op er tie s co ns\ntit ut\nio na\nl, in\nst itu\ntio na\nl, w\nith dr\naw al\n,c on\nvo lu\ntio na\nl, ca\nus al\n,b ea\nch te\nn, ge\nra l,\net he\nre al\n,u nc\non st\nitu tio\nna l,\nin st\nru m\nen ta\nl, po\nsi tio\nna l,\nkw al\nite it,\nen vi\nro nn\nem en\nta l,\nin cr\nem en\nta l,\ntid al\n.a dj\nec tiv\nes re\nla te\nd to\nch ar\nac te\nri st\nic s\nor pr\nop er\ntie s\n13 3\nw ri\ntin g\nsy nt\nax ve\nrb s\nle ss\ner -k\nno w\nn, le\nss er\n,r e\u0301a\nlis er\n,d ry\ner ,b\noo st\ner ,r\nes ea\nrc he\nr, pr\ne\u0301p ar\ner ,u\npl oa\nde r,\nfo st\ner ,\nph ot\nog ra\nph er\n,c re\u0301\ner ,c\non oc\ner ,f\net ch\ner ,s\ntr ea\nm er\n,m in\nde r\n13 5\nw ri\ntin g\nsy nt\nax ad\nve rb\ns su\nbs ta\nnt ia\nlly ,\nen vi\nro nm\nen ta\nlly ,\nla tte\n, su\nrp ri\nsi ng\nly ,\nw ee\nkl y,\ncu rl\ny, re\ncu rs\niv el y, be au tif ul ly ,c on cu rr en tly ,t ex te ,c on fid en tly ,a fo re m en tio ne d, Sa lly ,s ad ly ,h on - es tly\n13 7\nw ri\ntin g\nsy nt\nax ad\nve rb\ns lig\nht ly\n,r ep\nur po\nsi ng\n,e ag\ner ly\n,f ra\nnk ly\n,c al\nm ly\n,P ol\nly ,p\nre ci\nso ,q\nua rte\nrly ,a\nna ly\nzi ng , op en ly ,T hi rd ly ,e le ct ro ly te ,i m po rt an tly ,s ha m po o, Se co nd\nly 14 5 w ri tin g sy nt ax ad ve rb s an d ad je ct iv es en vi ro nm en ta lly , sc ie nt ifi ca lly , ve rb al ly , pr od uc t na m e,\nco nd\niti on\nal ly\n, la\nte st , //w w w .e xa m pl e. co m , C at hy , m in im al ly , so ci al ly , G ak ka i, /i, m od i, an nu al ly , ac ci de nt al ly\n15 8\nw ri\ntin g\nsy nt\nax ad\nve rb\ns di\nsp en\nsa ry\n, se\nro nt\n, E\ndm on\nto n,\nho ne\nst ly\n, ca\nlm ly\n, un\nin te\nnt io\nna lly\n, su\npp os\ned ly , op en ly ,g ra ce fu lly ,p ro fe ss io na lly ,c on di tio na lly ,e ld er ly ,y ou ng es t, in fe st at io n, th in ly\n16 1\nw ri\ntin g\nsy nt\nax ad\nve rb\ns an\nd ad\nje ct\niv es\nO pt\nio na\nlly ,c\non di\ntio na\nlly ,e\nnv ir\non m\nen ta\nlly -f\nri en\ndl y,\nm or\nal ly\n,w av\nin g,\nen vi\nro n-\nm en\nta lly\n,t ra\ndi tio\nna lly\n,B al\nly ,i\nnc re\nm en\nta lly\n,e m\not io\nna lly\n,i nt\nen tio\nna lly\n,c om\npu ta - tio na lly ,w ak in g, Id ea lly ,s la sh\n16 3\nw ri\ntin g\nsy nt\nax ad\nve rb\ns an\nd ad\nje ct\niv es\nhe av\nen ly\n,P lo\ntly ,p\nro m\npt ly\n,c on\nve ni\nen tly\n,l ei\nsu re\nly ,v\nas tly\n,s ur\npr is\nin gl\ny, re\npo rte\ndl y,\nbr ig\nht ly\n,s ub\nst an\ntia lly\n,w ar\nm ly\n,e\u0301 qu\nip em\nen ts\n,i nd\nir ec\ntly ,f\nal te\nr, el\nde rl y 16 6 no tl is ta bo ve ph on ol og y la ng ua ge pa tte rn s or ph on et ic si m ila ri tie s th in ne r, Sk in ne r, ch ili es ,p ro ba bl y, hy po th et ic al ,s pi nn er ,S M T P, bi\net et\n,s a\u030a,\nJa y,\nw itt\ny, se\nri ou\nsn es\ns, af\nor em\nen tio\nne d,\nra pi\ndl y,\nak tif\nPreprint Ta bl e 11 :M os tf re qu en tw or dpa ir s ac tiv at ed by se lf -a tte nt io n he ad \u2019s ne ur on s fr om th e 32 nd (l as t) la ye rs in V ic un a an d L L aM A .\nH ea\nd K\nee p\n(% )\nV ic\nun a\nL L\naM A\n1 66\n.6 7\nM an\nn= au\nf, cl\not h=\ntr im\n,p ro\nm in\nen t=\nsc ho\nla r,\nM or\nri s=\nV ic\n,l ite\nr= se\nl, L\nak e=\nM on\ntr ea l, D uk e= U ni ve rs ity ,p re se rv ed =r et ri ev ed ,b ea ch =s itu at ed ,a ng ry =u gl y, C hi le =C ru z, m ag ne tic =m ec ha ni ca l, as si gn =r es tr ic t, A us tr al ia n= R us se ll, ad m in is tr ativ e= pe rs on ne l\ncl ot\nh= tr\nim ,p\nro m\nin en\nt= sc\nho la\nr, M\nor ri\ns= V\nic ,L\nak e=\nM on\ntr ea\nl, D\nuk e=\nU ni\nve rs\nity ,\npr es\ner ve\nd= re\ntr ie\nve d,\nbe ac\nh= si\ntu at\ned ,\nan gr\ny= ug\nly ,\nC hi\nle =C\nru z,\nC ub\n=R C\n, m\nag -\nne tic\n=m ec\nha ni\nca l,\nas si\ngn =r\nes tr\nic t,\nad m\nin is\ntr at\niv e=\npe rs\non ne\nl, a\u030ar\n=\u0153 uv\nre ,e\nle m\n=s au\n2 76\n.4 7\nco rn\n=d ry\n,b ul\nle t=\ntri pl\ne, de\nlle =s\neg ,m\nit= ni\ne, El\nle n=\nw ife\n,C ol\nlin s=\nLu cy\n,J oh\nn= Ju\nst in , in te nt =j us tif y, A m st er da m =N ap ol i, E nd e= be i, br ok e= br ok en ,J o= W an g, A m st er - da m =M ic he l, R i= so u, ch r= in st an ce of\nbu lle\nt= tr\nip le\n, de\nlle =s\neg ,\nm it=\nni e,\nE lle\nn= w\nif e,\nE lle\nn= M\ney er\n, C\nol lin\ns= L\nuc y,\nJo hn\n=J us\ntin ,\nin te\nnt =j\nus tif\ny, A\nm st\ner da\nm =N\nap ol\ni, E\nnd e=\nbe i,\nso br\ne= vo\ntr e,\nbr ok\ne= br\nok en\n,J o=\nW an\ng, R\ni= so\nu, ch\nr= in\nst an\nce of\n3 57\n.8 9\nab or\nt= ar\ngs ,\ndw el\nl= se\nrv an\nt, ex\nc= nu\nr, bl\nue =p\nal e,\nJa ck\nso n=\nJu lia\n, H\nen ry\n=J ul\nia ,\nbl ue\n=p en\nda nt\n, sc\nho n=\ntr a,\nro of\n=s m\nok e,\np= sl\n, ah\nea d=\nw ai\nt, m\net a=\nto do\n, ga\n=o nt , B ru ce =G ra ha m ,M ar cu s= M ar tin\nab or\nt= ar\ngs ,\ncu rv\ne= gr\nad ie\nnt ,\npr ed\nic t=\npr ed\nic tio\nn, bl\nue =p\nal e,\nJa ck\nso n=\nJu lia , H en ry =J ul ia , bl ue =p en da nt , sc ho n= tr a, ro of =s m ok e, p= sl , co us in =w en t, ah ea d= w ai t, fo rm al ly =p re vi ou sl y, m et a= to do ,g a= on t\n4 57\n.8 9\nar gs\n=i ni\ntia liz\nat io\nn, E\nlis ab\net h=\nFi nl\nan d,\nIE =p\nop up\n, Fe\nrd in\nan d=\nSr ,\nIB =S r, qu =v oi r, A nd er so n= C hr is to ph er , Pi er re =W ri gh t, C hr is tia n= W ri gh t, A nd er - so n= C hr is tia n, ba ld =t on gu e, al ge br ai c= di m en si on al ,a pp ro xi m at io n= di m en si on al , Fl or en ce =S pr in g, de cl ar ed =r ul ed\nar gs\n=i ni\ntia liz\nat io\nn, co\nnc er\nn= si\ngn ifi\nca nt\n,c on\nfu si\nng =d\nan ge\nro us\n,E lis\nab et\nh= Fi\nnl an d, Fe rd in an d= Sr ,I B =S r, A nd er so n= C hr is to ph er ,P ie rr e= W ri gh t, C hr is tia n= W ri gh t, A nd er so n= C hr is tia n, ba ld =t on gu e, be id e= \u03bb , al ge br ai c= di m en si on al , Fl ore nc e= M ila n, ap pr ox im at io n= di m en si on\nal 5 42 .8 6 cl e\u0301s =t er m e, da t= si ch ,J oh ns on =W ay ne ,d el le =l ac ,M ar x= W ag ne r, le n= un si gn ed , de i= la c, A nt ho ny =L au ra , co m pl et ed =r ec en tly , A C =r ep la ce m en t, pe u= ro , sp he re =u ni fo rm ly ,e xa m in e= ex pl or e, ba ll= th ro w ,r ef er en ce d= re fe rr ed da t= si ch , Jo hn so n= W ay ne , fi= pu , de lle\n=l ac\n, M\nar x=\nW ag\nne r,\nle n=\nun si\ngn ed , de i= la c, co m pl et ed =r ec en tly ,l e= pe u, pe u= ro ,t om b= tu nn el ,r ef er en ce d= re fe rr ed , A rn ol d= C ar ol ,C hr is =D av e, cl ue =m is ta\nke 6 57 .8 9 ce =v a, ha l= or d, or d= po rt a, Jo na th an =W al ke r, di vi ne =m ig ht y, Fr an kl in =T ow ns hi p, Fr an kl in =N an cy , ar r= au x, no m =t ot , bl =f e, R ei ch =n ur , in eq ua lit y= la bo ur , en - tir el y= pe rf ec tly ,a z= vo lt, by te =l en tr an sf er =t ra ns m is si on , ce =v a, ha l= or\nd, or\nd= po\nrt a,\nhu n=\nja ,\nJo na\nth an\n=W al\nke r,\nFr an\nkl in\n=T ow\nns hi\np, Fr\nan kl\nin =N\nan cy\n,o u=\nvo or\n,n om\n=t ot\n,R ei\nch =n\nur ,a\nle =n\nic ht , en tir el y= pe rf ec tly ,a z= vo lt, by te =l\nen 7 36 .3 6 sc re en =t ou ch , di vi ne =v ir tu e, va n= ze , cr ea tio n= di vi ne , Fe rd in an d= V II , H am ilto n= N or th er n, de pt h= w id e, M ar ie =V in ce nt ,l ay er =w ir e, ta k= tu t, M ar y= V ir gi ni a, hi tti ng =s co re d, la ug h= m ou th ,C N N =M ia m i, di sp ut e= ow ne rs hi p di vi ne =v ir tu e, A li= C am pb el l,\nFe rd\nin an\nd= V\nII ,\nde pt\nh= w\nid e,\nM ar\nie =V\nin ce\nnt ,\nla ye\nr= w\nir e,\nM ar\ny= V\nir gi\nni a,\nla ug\nh= m\nou th\n, C\nN N\n=M ia\nm i,\nag re\ne= do\nub t,\nG re\nec e=\nIn di\na, go\nve rn\nor =p\nro vi\nnc ia\nl, es\npe ci\nal =l\nug ar\n,B ro\nok =G\nab ri\nel ,c\num =f at 8 57 .8 9 A lic e= R ob er t, O liv er =p oe t, Sy dn ey =e xh ib iti on ,q ua nt o= w ir d, fr ie nd ly =h el pf ul , ob =t yp eo f, V at er =v id a, ac id =i ro n, Je rs ey =W as hi ng to n, A nt ho ny =P hi lip , K en - ne th =R ob er t, D ic k= M or ga n, ha st a= tr ab aj o, gr ad ua lly =o cc as io na lly , co m - pr es s= m er ge O liv er =p oe t, m an us cr ip t= ph ot og ra ph , H on g= Sy dn ey , Sy dn ey =e xh\nib iti\non ,\nqu an\nto =w\nir d,\nfr ie\nnd ly\n=h el\npf ul\n, ob\n=t yp\neo f,\nV at\ner =v\nid a,\nA nt\nho ny\n=P hi\nlip ,\nPh ili\np= V\nI, K\nen ne\nth =R\nob er\nt, D\nic k=\nM or\nga n,\nha st\na= tra\nba jo\n,g ra\ndu al\nly =o\ncc as\nio na\nlly ,\nA nd\ny= E\nri c\n9 66\n.6 7\nbi n=\nfi, sc\nho n=\ntr e,\nri b=\nru b,\nac c=\nnu r,\nch =fi\n, ja\nm =j\nar ,\nob =t\nre ,\nor ig\nin al\n=v er\nsi on , br ill ia nt =p le as an t, al to =l is ta ,C am pb el l= Fr an kl in ,g eg en =h ur ,L eo na rd =O xf or d, A m y= G le n, A tla nt a= In st itu te\nbi n=\nfi, ri\nb= ru\nb, ac\nc= nu\nr, ch\n=fi ,\nga r=\nre m\n, ja\nm =j\nar ,\nSt =W\nal ke\nr, ob\n=t re\n, or\nig -\nin al\n=v er\nsi on\n, br\nill ia\nnt =p\nle as\nan t,\nal to\n=l is\nta ,\nC am\npb el\nl= Fr\nan kl\nin ,\nA pr\nil= L\nis a,\nL eo\nna rd\n=O xf\nor d,\nA m\ny= G\nle n\n10 50\n.0 0\nbe ac\nh= ya\nrd ,\nde riv\ne= nu\nm er\nic al\n, K\nei th\n=P at\nri ck\n, di\nce =e\u0300\n, B\nill y=\nC la\nrk ,\nlin =t\nym ,\nvo lta\nge =w\nir e,\nL aw\nre nc\ne= Ti\nm ,\nJo se\nph =P\nau l,\ncr ea\ntu re\n=d ar\nkn es\ns, Pa\nla ce\n=X V I, K el ly =M or ri s, di am et er =r ec ta ng ul ar ,A us tin =J oh ns on ,M on um en t= ei ge n\nbe ac\nh= ya\nrd ,d\ner iv\ne= nu\nm er\nic al\n,B ro\nok =K\nei th\n,K ei\nth =P\nat ri\nck ,d\nic e=\ne\u0300, B\nill y=\nC la\nrk ,\nov er\nla y=\nw id\nth ,\nlin =t\nym ,\nvo lta\nge =w\nir e,\nau to\nr= ba\njo ,\nJo se\nph =P\nau l,\nqu ite\n=w ro\nng ,\ncr ea\ntu re\n=d ar\nkn es\ns, ha\nte =m\nis s,\nPa la\nce =X\nV I\n11 66\n.6 7\nen jo\ny= re\nla x,\nm ut\n=s ob\nre ,m\nou th\n=t ra\nct ,d\nan =m\no, pu\nb= si\ntu at\ned ,e\nxc ite\nm en\nt= pr\nid e,\ndr in\nk= ta\nst e,\nba s=\nty m\n, zi\njn =\u00a1\n\u00a1, pr\nev =\u00b7\n, Ir\nis h=\nT ho\nm as\n, B\nru ce\n=R ic\nha rd\n, H ele n= R ic ha rd ,b le ss =s w ee t, Iv an =J oa n\nen jo\ny= re\nla x,\nm ou\nth =t\nra ct\n, A\nnn e=\nB ar\nba ra\n, da\nn= m\no, pu\nb= si\ntu at\ned ,\nex ci\nte -\nm en\nt= pr\nid e,\ndr in\nk= ta\nst e,\nba s=\nty m\n,z ijn\n=\u00a1 \u00a1,\npr ev\n=\u00b7 ,I\nri sh\n=T ho\nm as\n,H el\nen =R\nic ha\nrd ,\nbl es\ns= sw\nee t,\npr im\ner a=\nw or\nde n,\nes ta\n=p ri\nm er a 12 50 .0 0 ei ne =t ym ,f ol k= pu nk ,L uc as =P hi lip ,a cc es si bl e= se cu re ,a z= m n, cl as se =s ig ui en te , gr up o= si gu ie nt e, bo w =w oo de n, ke pt =s ta y, M an ue l= R ic o, C am br id ge =C am pb el l, D uk e= Ja so n, C am pb el l= D al la s, Ja so n= Te rr y, L ad y= Pa la ce ei ne =t ym ,f ol k= pu nk ,c y= re m ,u nd er st an\nd= un\nde rs\nto od\n,m ol\n=r em\n,L uc\nas =P\nhi lip , ac ce ss ib le =s ec ur e, az =m n, cl as se =s ig ui en te ,e r= w or de n, gr up o= si gu ie nt e, bo n= ra p, bo w =w oo de n, ke pt =s ta y, M an ue l= R ic\no 13 50 .0 0 hi =u nc le , im pa ct =i nc re as in g, R ic ha rd =R ut h, Ja m es =R ic ha rd , br an d= lo go , lo go =p os te r, In tr od uc tio n= Pa sc al ,J oh n= Y or k, ca sa =c om m e, cu r= fil ,m ed io =p ar ti, pe rs on a= to da ,b en ea th =g ra ve ,c ur =d iff ,a ur =r es id hi =u nc le ,i m pa ct =i nc re as in g, R ic ha rd\n=R ut\nh, Ja\nm es\n=R ic\nha rd\n,I nt\nro du\nct io\nn= Pa\nsc al , Jo hn =Y or k, cu r= fil ,m ed io =p ar ti, gr an =p ar ti, E li= H ar ri s, ob se rv ed =s ub se qu en tly , cu r= di ff ,a ur =r es id ,i m ag in at io n= in st in ct ,C ap e= co as\nt 14 76 .4 7 hu r= \u00b5 , al or s= w ei te r, L iv er po ol =V ill a, B ak er =G or do n, A nd re w =R ya n, G or - do n= Pe rr y, Il lin oi s= L ou is ,G or do n= St ua rt ,v ill e= \u03b4, L lo yd =M ar tin ,f as te r= sl ow er , L lo yd =R ol an d, K ay =M ar ia ,D av e= D ou gl as ,a nc ie nt =t em pl e hu r= \u00b5 , al or s= w ei te r, L iv er po ol =V ill a, B ak er =G\nor do\nn, A\nnd re\nw =R\nya n,\nG or - do n= Pe rr y, Il lin oi s= L ou is , G or do n= St ua rt , L lo yd =M ar tin , fa st er =s lo w er , K ay =M ar ia ,D av e= D ou gl as ,a nc ie nt =t em pl e, Je n= R oy ,R oy =W ar re n 15 66 .6 7 G eo rg ia =W at so n, L ou is e= V ill a, H ow ar d= St ua rt , A nt on io =C en tr o, H ar - va rd =M ar yl an d, m ix =t ex tu re ,L ut he r= V in ce nt ,C ol on el =E ar l, E dw ar d= In st itu te , L iv er po ol =S w ed en ,A lb er t= E dw ar d, C ol on el =D uk e, ge ne =r eg re ss io n, IE =S Q L , A m st er da m =m eh r L ou is e= V ill a, H ow ar d= St ua rt , ex po rt =t ra de , flo or =t en t, A nt on io =C en tr o, H ar - va rd =M ar yl an d, m ix =t ex tu re , L ut he r= V in ce nt , C ru z= Sa nt ia go , C ol on el =E ar l, E dw ar d= In st itu te , L iv er po ol =S w ed en , A lb er t= E dw ar d, C ol on el =D uk e, ge ne =r eg re ss io n 16 50 .0 0 A le x= M an n, G L =R C , ei ne =m eh r, D V D =Y ou Tu be , B er lin =I ns tit ut e, at - ta ch m en t= se pa ra to r, Ja y= M ar ie , co nc lu de d= st at in g, M ill er =h is to ri an , W ik ip ed ia =Y ou Tu be , H itl er =a rm y, bi rd =s he ep , do lo r= sc al ar , do lo r= zi jn , cm =p p A le x= M an n, G L =R C , ei ne =m eh r, D V D =Y ou Tu be , at ta ch m en t= se pa ra to r, la\u0300 =q ua nd o, Ja y= M ar ie , co nc lu de d= st at in g, bi rd =s he ep , do lo r= sc al ar , cm =p p, ca m p= re fu ge ,d t= xm ln s, fa ct or =r at io ,g ro te =n el le\nPreprint Ta bl e 12 :M os tf re qu en tw or dpa ir s ac tiv at ed by se lf -a tte nt io n he ad \u2019s ne ur on s fr om th e 32 nd (fi na l) la ye rs in V ic un a an d L L aM A .( co nt in ue d) H ea d K ee p (% ) V ic un a L L aM A 17 66 .6 7 M au ri ce =S te ve ,A nd er so n= C ou nt y, M ar tin =S te ve ,J oh nn y= M ar tin ,D uk e= M ar tin , M ey er =d ir ec to r, ge ne ra to r= vo lta ge , de le te d= re se t, D J= SD , pi e= te nd er , m an us cr ip t= po et ry , m an us cr ip t= su bm is si on , m an us cr ip t= po em , bo s= tip o, M ey er =V ic to r M au ri ce =S te ve ,A nd er so n= C ou nt y, D uk e= M ey er ,M ar tin =S te ve ,D uk e= M ar tin , M ey er =d ir ec to r, ge ne ra to r= vo lta ge , de le te d= re se t, la rg o= st a, D J= SD , m an us cr ip t= po et ry , m an us cr ip t= su bm is si on , m an us cr ip t= po em , na ch =u no , M ey er =V ic to r 18 42 .8 6 C le ve la nd =J en ki ns , ea s= so w oh l, ob je ct iv e= th eo re tic al , co ns eq ue nc e= ju st if y, Si e= de sd e, cr ue l= tr iu m ph , E lle n= si ng er , se g= ve rt , A br ah am =L ew is , G ot t= m it, bi o= do t, m an ip ul at e= re pr od uc e, ga r= pr in ,i m m er =v el ,m eg =p ri n C le ve la nd =J en ki ns , ea s= so w oh l, ob je ct iv e= th eo re tic al , he at =r oo f, Si e= de sd e, cr ue l= tr iu m ph , se g= ve rt , A br ah am =V ic to r, ei ge ne n= kt o\u0301r y, bi o= do t, m an ip ula te =r ep ro du ce ,v el =w el ,f am =m o, en g= fa m ,g ar =p ri n 19 10 0. 0 pr ed ic t= pr ob ab le ,d ry =s lig ht ly ,c om =s ob re ,l ar ge r= w id er ,F re nc h= Ita lia n, co al =s oi l, flo w er =g ar de n, gu ita r= po p, A ca de m y= Sh ak es pe ar e, C la rk =C ro w ,C la rk =D av is , C ar te r= Sc ot t, pr ob ab le =s ev er e, co m bi ne d= co m pl em en t, E ri c= W ag ne r pr ed ic t= pr ob ab le ,d ry =s lig ht ly ,c om =s ob re ,l ar ge r= w id er ,F re nc h= Ita lia n, co al =s oi l, flo w er =g ar de n, gu ita r= po p, A ca de m y= Sh ak es pe ar e, C la rk =C ro w ,C la rk =D av is , C ar te r= Sc ot t, pr ob ab le =s ev er e, co m bi ne d= co m pl em en t, E ri c= W ag ne r 20 50 .0 0 A ri zo na =G eo rg ia ,b ou t= ra p, G ar y= Ti m ,i nt er ru pt =p au se ,V en ez ue la =c ua l, co m - fo rt ab le =p le as an t, sw or d= ta il, A lle n= R ic k, D ic k= N ei l, an yb od y= fig ur ed ,v oc e= \u03bb , an gl ai s= zo na ,r iv er =t er ra in ,d an =u nd ,K u= ga r np =p ou vo ir, bo ut =r ap , G ar y= Ti m , V en ez ue la =c ua l, co m pl et ed =s ub se qu en tly , A lle n= R ic k, an yb od y= fig ur ed , vo ce =\u03bb , an gl ai s= zo na , riv er =t er ra in , da n= un d, K u= ga r, as su m ed =d et er m in ed ,L eo n= W ill ia m ,m et ad at a= qu er y 21 57 .8 9 co ns id er ab le =e xt en si ve , so on er =w on t, D av e= Je ss , K en =P ar ke r, da rk - ne ss =h ap pi ne ss ,L ou is =M ar sh al l, B al tim or e= St ad iu m ,L ui s= St ad iu m ,M P= N S, M ig ue l= V ic to r, O pe ra =S ta di um ,N ap ol i= St ad iu m ,f og =r ai n, in de pe nd en ce =n at io n, co m pa ct =s en so r co ns id er ab le =e xt en si ve , so on er =w on t, D av e= Je ss , K en =P ar ke r, st r= va l, da rk ne ss =h ap pi ne ss , L ui s= St ad iu m , M P= N S, M ig ue l= V ic to r, T ho r= de m on , O pe ra =S ta di um ,f og =r ai n, in de pe nd en ce =n at io n, cy =s l, flu sh =m ud 22 50 .0 0 ne ar =o pp os ite , M on tr ea l= Pa ri s, Jo an =L es , B ra zi l= Ta iw an , M as - sa ch us et ts =T ho m ps on , et w as =\u03c0 , m ag ni tu de =u nc er ta in ty , de st =i ns t, lo =t al , cu rs or =s lid er ,e lif =\u03b8 ,c on tr e= gl i, tr op =u\u0308 be r, A fr ic a= C ap e, bi sn is =i m pl ne ar =o pp os ite ,C T =I nn ,n a\u0308r =\u0131\u0302 n, M on tr ea l= Pa ri s, II I= M an ue l, II I= Po pe ,J oa n= L es , ni gh t= te nt ,e tw as =\u03c0 ,A na =B ro ok ,m ag ni tu de =u nc er ta in ty ,h ov er =m is t, de st =i ns t, lo =t al ,c ur so r= sl id er 23 57 .8 9 az =m ir, m aa r= w ie ,m er el y= ob vi ou s, ev il= m or al ,d iv in e= ev il, fe r= pr of ,P ak =R aj , cl oc k= pi n, co nt ra di ct io n= ev il, K ei th =M ic ha el , lit er at ur e= w id el y, br ot he r= ki ll, br ot he r= ev il, bl =m il, m od =s ub az =m ir, m aa r= w ie , m er el y= ob vi ou s, ev il= m or al , cl oc k= pi n, Pa k= R aj , lit er atu re =w id el y, m od =s ub , br ot he r= ki ll, br ot he r= ev il, bl =m il, C am pb el l= H ou st on , ca st le =g ra nd ,G ib =j ak o, af =d is t 24 50 .0 0 IS B N =f ra\u030a n, el ec tr on =v ol ta ge ,fi l= pe ,e en =u na ,C ru z= W al ke r, In st itu te =W ill ia m , bi na ry =q ua nt um ,v id e= \u00a1\u00a1 ,b ou nd ar y= po rt io n, C ol lin s= sc or ed ,l o= m ak ,s al t= w et , as id e= pa rt ly ,fi l= lu x, m ul tip lic at io n= re cu rs io n IS B N =f ra\u030a n, el ec tr on =v ol ta ge , fil =t ou s, fil =p e, ee n= un a, In st itu te =W ill ia m , bi na ry =q ua nt um , vi de =\u00a1 \u00a1, de m on st ra te d= ob se rv at io n, an ci en t= sa cr ed , bo un dar y= po rt io n, C ol lin s= sc or ed ,a si de =p ar tly ,g ua rd =s ho t, G or do n= Se na to r 25 57 .8 9 B er lin =F ra nc e, A rn ol d= G ab ri el , m at he m at ic s= pr of es so r, je st =m ir, C hi =C hi na , va r= ve rt ex , sp he re =s ym m et ry , la m p= tu b, E di nb ur gh =S in ga po re , bi na ry =fi ni te , Pa ul =S eb as tia n, H ow ar d= W u, da ug ht er =s w ee t, L ou is =O pe ra ,a ltr i= co m m e B er lin =F ra nc e, m at he m at ic s= pr of es so r, C hi =C hi na ,v ar =v er te x, sp he re =s ym m et ry , la m p= tu b, E di nb ur gh =S in ga po re ,b in ar y= fin ite ,P au l= Se ba st ia n, da ug ht er =s w ee t, be au tif ul =l an ds ca pe ,L ou is =O pe ra ,I nt ro du ct io n= O xf or d, Sa nt o= di ,f oo l= pl ai n 26 50 .0 0 B er na rd =G ra ha m , ba nd =h op , j= r, du r= tu n, Pa s= du r, B ri ta in =V ic to ri a, L ew is =M itc he ll, C ar lo =v on , cl ea re r= fa rt he r, di ch =i nt e, ni =s eu , ha n= se u, R ob er t= Te nn es se e, pr im a= zi jn ,S in g= W an g B er na rd =G ra ha m ,d ip =t os s, ba nd =h op ,j =r ,d ur =t un ,P as =d ur ,B ri ta in =V ic to ri a, L ew is =M itc he ll, A la n= W ri gh t, C ur t= W ri gh t, C ar lo =v on , C re ek =W ri gh t, jo ue ur =l ue go ,n i= se u, ha n= se u 27 57 .8 9 E liz ab et h= II I, ex te nt =n ec es sa ri ly , E ar l= M r, ei ne =p ro pr io , aa n= du r, E liz - ab et h= H ar ri s, N or m an =V ic , K ev in =V ic , af =a v, A nd re w =C am pb el l, Fr an kl in =L aw re nc e, co nc re te =g ra ss , Fr an ci a= Pe ru , C la ud e= vo us , A nt oni o= Ju an E liz ab et h= II I, ex te nt =n ec es sa ri ly , E ar l= M r, ei ne =p ro pr io , co nt ra ry =o bs er ve , aa n= du r, E liz ab et h= H ar ri s, E dw ar d= II I, N or m an =V ic ,s uo =t ut ti, K ev in =V ic ,E dw ar d= V ic ,a f= av ,A nd re w =C am pb el l, Fr an kl in =L aw re nc e 28 57 .8 9 H el en =N ic k, Fr an ci sc o= V in ce nt , ap pl ic at io n= im pl em en ta tio n, he at =s m ok e, Po pe =S m ith ,p ai nt =s ha do w ,c ir cl e= cu rv e, de lla =v om ,k am =k o, go ld en =m in t, pr eci se ly =p ur el y, m an ie\u0300 re =t ro uv e, C ar ol =H ar ry ,C ar ol =C ro w ,c ac he d= in te ge r bo =fi ,A ca de m y= di re ct or ,H el en =N ic k, ap pl ic at io n= im pl em en ta tio n, he at =s m ok e, Po pe =S m ith ,d el la =v om ,k am =k o, nu m =e\u0302 tr e, pr ec is el y= pu re ly ,m an ie\u0300 re =t ro uv e, C ar ol =H ar ry ,C ar ol =C ro w ,c ac he d= in te ge r, bi g= ni ce 29 42 .8 6 av an t= vi de , \u00a1\u00a1 =\u00b6 , od er =\u00a1 \u00a1, m oi =s ur , M as sa ch us et ts =O hi o, da m ag e= di se as e, co nc ur re nt =d is cr et e, co nt ra di ct io n= in te rp re ta tio n, af =s ig , A le xa nd er =O liv er , Ph ili p= Q ue en ,g el =i ns ec t, le ar =t ro is ,G or do n= R on ,J oh ns on =K at e av an t= vi de ,m oi =s ur ,M as sa ch us et ts =O hi o, da m ag e= di se as e, co nc ur re nt =d is cr et e, co nt ra di ct io n= in te rp re ta tio n, af =s ig , le ar =t ro is , G or do n= R on , A da m =J o, L aw re nc e= Te nn es se e, co m pi le r= he lp er , di am et er =g au ge , E gy pt =s ei ze d, co ur ag e= si m pl ic ity 30 76 .4 7 ca lc ul =l eu r, al ph a= p, do ck =i Ph on e, m ot =p ri n, C ol or ad o= Ju an , co lla bo - ra tio n= pa rt ic ip at io n, an no =l ui , gu es s= pr et ty , fe st =r ap , C ha rl ot te =M on tr ea l, L in ux =i Ph on e, C ha rl ot te =C la ud e, le ur =w ir, C ar l= N ew to n, es te =w ir ca lc ul =l eu r, al ph a= p, do ck =i Ph on e, m ot =p ri n, br ot he r= ol de r, C ol or ad o= Ju an , co lla bo ra tio n= pa rt ic ip at io n, ho ok =r ib , an no =l ui , gu es s= pr et ty , fe st =r ap , C ha rlo tte =M on tr ea l, L in ux =i Ph on e, C ha rl ot te =C la ud e, le ur =w ir 31 66 .6 7 hi d= pr ie st , ce as ed =m id st , go ld en =h ai r, go ld en =m ou nt ai n, V ie tn am =s ol di er , bl ac k= m al e, A m er ic an =C at ho lic , R ep ub lic an =o pp os iti on , Ph ili p= W ill ia m , C at ho lic =L at in , B ro ad w ay =P al ac e, av ec =c ad a, H ar ri s= K ni gh t, du pl ica te =u nn ec es sa ry ,d ir ty =k itc he n hi d= pr ie st , ce as ed =m id st , go ld en =h ai r, V ie tn am =s ol di er , go ld en =m ou nt ai n, Ph ili p= W ill ia m ,C at ho lic =L at in ,F ra nz =p ia no ,B ro ad w ay =P al ac e, av ec =c ad a, H ar - ri s= K ni gh t, du pl ic at e= un ne ce ss ar y, di rt y= ki tc he n, fa lle n= gr av e, ite r= po ly no m ia l 32 66 .6 7 lik ed =l ov ed , la d= yo un ge r, M an ch es te r= Ph ila de lp hi a, St oc kh ol m =e tt, R ot h= St an le y, K ha n= K in g, Je nk in s= M ill er ,m is ta ke n= su dd en ,m is ta ke n= su dd en ly , ja ko =s ig ui en te , In n= O xf or d, ta nt o= zo na , lu eg o= pa rt i, ev en tu al ly =l ite ra lly , B ut le r= Sh aw lik ed =l ov ed , la d= yo un ge r, M an ch es te r= Ph ila de lp hi a, St oc kh ol m =e tt, R ot h= St an le y, K ha n= K in g, na da =v ita , Je nk in s= M ill er , cu ltu re =s pi ri t, m is - ta ke n= su dd en , m is ta ke n= su dd en ly , ja ko =s ig ui en te , In n= O xf or d, ta nt o= zo na , A rk =B ib le\nsound to the reader? These tones include Disheartening, Accusatory,"
        },
        {
            "heading": "Worried, Curious, Surprised, Disapproving, Unassuming, Formal, Assertive, Confident, Appreciative, Concerned, Sad, Informal, Regretful, Encouraging,",
            "text": "Egocentric, Joyful, Optimistic, and Excited.\\n\\nInput: Hi Jen, \\nI hope you\u2019re well. Can we catch up today? I\u2019d appreciate your input on my presentation for tomorrow\u2019s meeting. I\u2019d especially love it if you could double-check the sales numbers with me. There\u2019s a coffee in it for you!\\n\\nOutput: Vicuna: Hi Jen,\\nI hope you\u2019re doing well. Can we catch up today? I would appreciate your input on my presentation for tomorrow\u2019s meeting. I would especially love it if you could double-check the sales numbers with me. There\u2019s a coffee in it for you!\\n\\nAnalysis: The word choice in this email is informal and conversational, which may convey a friendly and approachable tone to the reader. The phrasing is straightforward and direct, which may convey a confident and assertive tone. The punctuation is appropriate for an email, with a mix of sentence-level punctuation and no capitalization for the salutation or closing. The capitalization is consistent with standard email conventions. Overall, the tone of this email is likely to be friendly and approachable, with a hint of assertiveness.\nquestions about my rate or do you need to adjust the project scope?"
        },
        {
            "heading": "Please let me know.",
            "text": "1 . D ea r ly \\n 2 . Mo st sin cer ely \\n 3 . Tr uly \\n 4 . Ve ry sin cer ely \\n 5 . Mo st tru ly \\n 6 . Ve ry de ar ly \\n 7 . Ve ry tru ly \\n 8 . Mo st de ar ly \\n 9 . Ve ry tru ly \\n 1 0 . Mo st sin cer ely\nDes\nk\njobs\nrequire\nwriting\na\nlot\nof\nemails\n,\nso\nit\nisn\n'\nt\nsurprising\nwe\nget\ntired\nof\nrepeating\nourselves\n.\nCome\nup\nwith\nseveral\nsyn\nonym\ns\nfor\nthe\ngiven\nword\n.\n\\n\n\\n\nInput\n:\nSin\ncer\nely\n\\n\n\\n\nOutput\n:\nFigure 10: Prompt: Desk jobs require writing a lot of emails, so it isn\u2019t surprising we get tired of repeating ourselves. Come up with several synonyms for the given word.\\n\\nInput: Sincerely\\n\\nOutput: Vicuna: 1. Dearly\\n2. Most sincerely\\n3. Truly\\n4. Very sincerely\\n5. Most truly\\n6. Very dearly\\n7. Very truly\\n8. Most dearly\\n9. Very truly\\n10. Most sincerely.\n* \" Th ere are gre at\nop tio\nns\nav aila\nble for yo u .\" ( For be s ) \\n * \" Th ere are ma ny gre at\nop tio ns to cho ose fro m .\" ( Th e\nGu ard\nian )\nFind\nsentences\nfrom\nreliable\nsources\nsuch\nas\nthe\nGuardian\nor\nFor\nbes\nthat\ncontain\nthe\nexact\nmatch\nfor\nthe\ngiven\nsentence\nor\nphrase\n.\nAlso\n,\nmention\nthe\nsource\nof\neach\nsentence\n.\n\\n\n\\n\nInput\n:\nThere\nare\ngreat\noptions\n\\n\n\\n\nOutput\n:\nFigure 11: Prompt: Find sentences from reliable sources such as the Guardian or Forbes that contain the exact match for the given sentence or phrase. Also, mention the source of each sentence.\\n\\nInput: There are great options\\n\\nOutput: Vicuna: * \"There are great options available for you.\" (Forbes)/n* \"There are many great options to choose from.\" (The Guardian).\nwe \\ * to kn ow \\n \\n Inp ut : I \\ * to go to the sto re \\n \\n Ou tpu t : I \\ * to go to the sto re \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\ * \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\ * \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\ * \\ * \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\ * \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\ * \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\ * \\ * \\n \\n Inp ut : I \\ * to go to the sto re \\ * \\ * \\ * \\ * \\ * \\ * \\ * \\n \\n Ou tpu t : I \\ * to go to the sto re \\ * \\ * \\\nto demonstrate all the words that can be used in the sentence instead of the asterisk.\\n\\nInput: we * to know\\n\\nOutput: Vicuna: we * to know\\n\\nInput: I * to go to the store\\n\\nOutput: I * to go to the store\\n\\nInput: I * to go to the store *\\n\\nOutput: I * to go to the store *\\n\\nInput: I * to go to the store * *\\n\\nOutput: I * to go to the store * *\\n\\nInput: I * to go to the store * * *\\n\\nOutput: I * to go to the store * * *\\n\\nInput: I * to go to the store * * * *\\n\\nOutput: I * to go to the store * * * *\\n\\nInput: I * to go to the store * * * * *\\n\\nOutput: I * to go to the store"
        },
        {
            "heading": "Efficiency (HE) Compatible, Laundry Soap, 62 Count/n- Sentiment:",
            "text": "Positive\\n\\nOutput: Vicuna: \u2018\u2018Persil Discs Laundry Detergent Pacs are a great option for those looking for a high-quality, HE compatible laundry detergent. The Original Scent is fresh and invigorating, and the 62-count pack provides plenty of detergent for multiple loads. The pacs are easy to use and can be added directly to the washing machine, making them a convenient choice for busy families. Overall, we highly recommend Persil Discs Laundry Detergent Pacs for their effectiveness, affordability, and positive sentiment.\u2019\u2019"
        }
    ],
    "year": 2023
}