{
    "abstractText": "In recent years there has been an increased interest in neural networks, particularly with regard to their ability to approximate partial differential equations. In this regard, research has begun on so-called physics-informed neural networks (PINNs) which incorporate into their loss function the boundary conditions of the functions they are attempting to approximate. In this paper, we investigate the viability of obtaining the quasi-normal modes (QNMs) of non-rotating black holes in 4-dimensional space-time using PINNs, and we find that it is achievable using a standard approach that is capable of solving eigenvalue problems (dubbed the eigenvalue solver here). In comparison to the QNMs obtained via more established methods (namely, the continued fraction method and the 6th-order Wentzel, Kramer, Brillouin method) the PINN computations share the same degree of accuracy as these counterparts. In other words, our PINN approximations had percentage deviations as low as (\u03b4\u03c9Re , \u03b4\u03c9Im) = (< 0.01%, < 0.01%). In terms of the time taken to compute QNMs to this accuracy, however, the PINN approach falls short, leading to our conclusion that the method is currently not to be recommended when considering overall performance. PACS numbers: 04.20.-q, 07.05.Mh, 02.60.Cb \u2217 acornell@uj.ac.za \u2020 ncubeanele4@gmail.com \u2021 gerhard.harmsen5@gmail.com 1 ar X iv :2 20 5. 08 28 4v 2 [ ph ys ic s. co m pph ] 2 4 O ct 2 02 2",
    "authors": [
        {
            "affiliations": [],
            "name": "Alan S Cornell"
        },
        {
            "affiliations": [],
            "name": "Anele Ncube"
        },
        {
            "affiliations": [],
            "name": "Gerhard Harmsen"
        }
    ],
    "id": "SP:02bac1b0d1ac9bc30a12c877bc8da8adf52f7255",
    "references": [
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe",
                "H. White"
            ],
            "title": "Multilayer feedforward networks are universal approximators",
            "venue": "Neural Networks 2 no. 5, (1989) 359.",
            "year": 1989
        },
        {
            "authors": [
                "S. Wu",
                "M.J. Er"
            ],
            "title": "Dynamic fuzzy neural networks - a novel approach to function approximation",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 30 no. 2, (2000) 358.",
            "year": 2000
        },
        {
            "authors": [
                "M. Raissi",
                "P. Perdikaris",
                "G. Karniadakis"
            ],
            "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
            "venue": "J. Computat. Phys. 378 (2019) 686.",
            "year": 2019
        },
        {
            "authors": [
                "Y. Lu",
                "J. Lu"
            ],
            "title": "A universal approximation theorem of deep neural networks for expressing distributions",
            "venue": "Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020). 2020. arXiv:2004.08867 [cs.LG].",
            "year": 2020
        },
        {
            "authors": [
                "J. Nordstr\u00f6m",
                "O. Alund"
            ],
            "title": "Neural network enhanced computations on coarse grids",
            "venue": "J. Comput. Phys. 425 (2021) 109821. 39",
            "year": 2021
        },
        {
            "authors": [
                "H. Jin",
                "M. Mattheakis",
                "P. Protopapas"
            ],
            "title": "Unsupervised neural networks for quantum eigenvalue problems",
            "venue": "2020 NeurIPS Workshop on Machine Learning and the Physical Sciences. 2020. arXiv:2010.05075 [physics.comp-ph].",
            "year": 2020
        },
        {
            "authors": [
                "K.D. Kokkotas",
                "B.G. Schmidt"
            ],
            "title": "Quasinormal modes of stars and black holes",
            "venue": "Living Rev. Relativ. 2 (1999) 2.",
            "year": 1999
        },
        {
            "authors": [
                "R.A. Konoplya",
                "A. Zhidenko"
            ],
            "title": "Quasinormal modes of black holes: From astrophysics to string theory",
            "venue": "Rev. Mod. Phys. 83 (2011) 793.",
            "year": 2011
        },
        {
            "authors": [
                "E. Berti",
                "V. Cardoso",
                "A.O. Starinets"
            ],
            "title": "Quasinormal modes of black holes and black branes",
            "venue": "Class. Quant. Grav. 26 (2009) 163001.",
            "year": 2009
        },
        {
            "authors": [
                "S. Iyer",
                "C.M. Will"
            ],
            "title": "Black-hole normal modes: A WKB approach. I. Foundations and application of a higher-order WKB analysis of potential-barrier scattering",
            "venue": "Phys. Rev. D 35 (Jun, 1987) 3621.",
            "year": 1987
        },
        {
            "authors": [
                "E.W. Leaver"
            ],
            "title": "An analytic representation for the quasi-normal modes of Kerr black holes",
            "venue": "Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences 402 no. 1823, (1985) 285.",
            "year": 1823
        },
        {
            "authors": [
                "H.T. Cho",
                "A.S. Cornell",
                "J. Doukas",
                "T.R. Huang",
                "W. Naylor"
            ],
            "title": "A new approach to black hole quasinormal modes: A review of the asymptotic iteration method",
            "venue": "Adv. Math. Phys. 2012 no. 281705, (2012) 42.",
            "year": 2012
        },
        {
            "authors": [
                "M. Isi",
                "M. Giesler",
                "W.M. Farr",
                "M.A. Scheel",
                "S.A. Teukolsky"
            ],
            "title": "Testing the no-hair theorem with GW150914",
            "venue": "Phys. Rev. Lett. 123 no. 11, (2019) 111102.",
            "year": 2019
        },
        {
            "authors": [
                "A. Ghosh",
                "R. Brito",
                "A. Buonanno"
            ],
            "title": "Constraints on quasinormal mode frequencies with LIGO-Virgo binary black-hole observations",
            "venue": "Phys. Rev. D 103 no. 12, (2021) 124041.",
            "year": 2021
        },
        {
            "authors": [
                "S.M. Carroll",
                "J. Traschen"
            ],
            "title": "Spacetime and geometry: An introduction to general relativity",
            "venue": "Phys. Today 58 no. 1, (Jan., 2005) 52.",
            "year": 2005
        },
        {
            "authors": [
                "D. McMahon"
            ],
            "title": "Quantum field theory demystified",
            "venue": "McGraw-Hill Education, New York City,",
            "year": 2008
        },
        {
            "authors": [
                "S. Iyer"
            ],
            "title": "Black hole normal modes: A WKB approach. II. Schwarzschild black holes",
            "venue": "Phys. Rev. D 35 (1987) 3632.",
            "year": 1987
        },
        {
            "authors": [
                "T. Regge",
                "J.A. Wheeler"
            ],
            "title": "Stability of a Schwarzschild singularity",
            "venue": "Phys. Rev. 108 (Nov, 1957) 1063.",
            "year": 1957
        },
        {
            "authors": [
                "S. Dey",
                "S. Chakrabarti"
            ],
            "title": "A note on electromagnetic and gravitational perturbations of 40 the Bardeen de Sitter black hole: Quasinormal modes and greybody factors",
            "venue": "Eur. Phys. J. C 79 no. 6, (2019) 504.",
            "year": 2019
        },
        {
            "authors": [
                "V. Cardoso",
                "J.P.S. Lemos"
            ],
            "title": "Quasinormal modes of Schwarzschild anti-de Sitter black holes: Electromagnetic and gravitational perturbations",
            "venue": "Phys. Rev. D 64 (2001) 084017.",
            "year": 2001
        },
        {
            "authors": [
                "C. Molina"
            ],
            "title": "Quasinormal modes of D-dimensional spherical black holes with near extreme cosmological constant",
            "venue": "Phys. Rev. D 68 (2003) 064007.",
            "year": 2003
        },
        {
            "authors": [
                "V. Cardoso",
                "J.P.S. Lemos"
            ],
            "title": "Quasinormal modes of the near extremal Schwarzschild-de Sitter black hole",
            "venue": "Phys. Rev. D 67 (2003) 084020.",
            "year": 2003
        },
        {
            "authors": [
                "V. Ferrari",
                "B. Mashhoon"
            ],
            "title": "New approach to the quasinormal modes of a black hole",
            "venue": "Phys. Rev. D 30 (1984) 295.",
            "year": 1984
        },
        {
            "authors": [
                "B.F. Schutz",
                "C.M. Will"
            ],
            "title": "Black hole normal modes: A semianalytic approach",
            "venue": "Astrophys. J. 291 (1985) L33.",
            "year": 1985
        },
        {
            "authors": [
                "H.-P. Nollert"
            ],
            "title": "Quasinormal modes of Schwarzschild black holes: The determination of quasinormal frequencies with very large imaginary parts",
            "venue": "Phys. Rev. D 47 (Jun, 1993) 5253.",
            "year": 1993
        },
        {
            "authors": [
                "E.W. Leaver"
            ],
            "title": "Quasinormal modes of Reissner-Nordstr\u00f6m black holes",
            "venue": "Phys. Rev. D 41 (May, 1990) 2986.",
            "year": 1990
        },
        {
            "authors": [
                "R. Konoplya",
                "A. Zhidenko",
                "A. Zinhailo"
            ],
            "title": "Higher order WKB formula for quasinormal modes and grey-body factors: Recipes for quick and accurate calculations",
            "venue": "Class. and Quantum Gravity 36 no. 15, (2019) 155002.",
            "year": 2019
        },
        {
            "authors": [
                "R.G. Daghigh",
                "M.D. Green",
                "J.C. Morey",
                "G. Kunstatter"
            ],
            "title": "Scalar perturbations of a single-horizon regular black hole",
            "venue": "Phys. Rev. D 102 no. 10, (2020) 104040.",
            "year": 2020
        },
        {
            "authors": [
                "L. Lu",
                "X. Meng",
                "Z. Mao",
                "G.E. Karniadakis"
            ],
            "title": "DeepXDE: A deep learning library for solving differential equations",
            "venue": "SIAM Rev. 63 no. 1, (2021) 208.",
            "year": 2021
        },
        {
            "authors": [
                "G.F. Montufar",
                "R. Pascanu",
                "K. Cho",
                "Y. Bengio"
            ],
            "title": "On the number of linear regions of deep neural networks",
            "venue": "Advances in neural information processing systems, Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, eds., vol. 27. Curran Associates, Inc., 2014. https://proceedings.neurips.cc/paper/2014/file/ 109d2dd3608f669ca17920c511c2a41e-Paper.pdf.",
            "year": 2014
        },
        {
            "authors": [
                "O. \u00c5lund",
                "G. Iaccarino",
                "J. Nordstr\u00f6m"
            ],
            "title": "Learning to differentiate",
            "venue": "J. Comput. Phys. 424 (Jan., 2021) 109873.",
            "year": 2021
        },
        {
            "authors": [
                "K. Hornik",
                "M. Stinchcombe",
                "H. White"
            ],
            "title": "Universal approximation of an unknown 41 mapping and its derivatives using multilayer feedforward networks",
            "venue": "Neural Networks 3 no. 5, (1990) 551.",
            "year": 1990
        },
        {
            "authors": [
                "K. Hornik"
            ],
            "title": "Approximation capabilities of multilayer feedforward networks",
            "venue": "Neural Networks 4 no. 2, (1991) 251.",
            "year": 1991
        },
        {
            "authors": [
                "A. Pinkus"
            ],
            "title": "Approximation theory of the MLP model in neural networks",
            "venue": "Acta Numerica 8 (1999) 143.",
            "year": 1999
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "3rd International Conference for Learning Representations, San Diego, 2015. 2017. arXiv:1412.6980 [cs.LG].",
            "year": 2015
        },
        {
            "authors": [
                "X. Chen",
                "R. Chen",
                "Q. Wan",
                "R. Xu",
                "J. Liu"
            ],
            "title": "An improved data-free surrogate model for solving partial differential equations using deep neural networks",
            "venue": "Scientific Reports 11 (09, 2021) 2045.",
            "year": 2021
        },
        {
            "authors": [
                "G. Karniadakis",
                "Y. Kevrekidis",
                "L. Lu",
                "P. Perdikaris",
                "S. Wang",
                "L. Yang"
            ],
            "title": "Physics-informed machine learning",
            "venue": "Nature Reviews Physics 3 no. 6, (05, 2021) 1.",
            "year": 2021
        },
        {
            "authors": [
                "H.-T. Cho",
                "C.-L. Ho"
            ],
            "title": "Quasi)-exactly solvable quasinormal modes",
            "venue": "J. Phys. A 40 (2007) 1325.",
            "year": 2007
        },
        {
            "authors": [
                "S. Cuomo",
                "V.S. Di Cola",
                "G. Rozza",
                "M. Raissi",
                "F. Piccialli"
            ],
            "title": "Scientific machine learning through physics\u2013informed neural networks: Where we are and what\u2019s next",
            "venue": "J Sci Comp 92 (2022) 88.",
            "year": 2022
        },
        {
            "authors": [
                "R. Gnanasambandam",
                "B. Shen",
                "J. Chung",
                "X. Yue",
                "Z. Kong"
            ],
            "title": "Self-scalable tanh (stan): Faster convergence and better generalization in physics-informed neural networks",
            "venue": "2022.",
            "year": 2022
        },
        {
            "authors": [
                "C. Cheng",
                "G.-T. Zhang"
            ],
            "title": "Deep learning method based on physics informed neural network with resnet block for solving fluid flow problems",
            "venue": "Water 13 (2021) 4.",
            "year": 2021
        },
        {
            "authors": [
                "G. P\u00f6schl",
                "E. Teller"
            ],
            "title": "Bemerkungen zur quantenmechanik des anharmonischen oszillators",
            "venue": "Zeitschrift fur Physik 83 no. 3-4, (Mar., 1933) 143.",
            "year": 1933
        },
        {
            "authors": [
                "M. Mattheakis",
                "D. Sondak",
                "A.S. Dogra",
                "P. Protopapas"
            ],
            "title": "Hamiltonian neural networks for solving equations of motion",
            "venue": "Phys. Rev. E 105 (Jun, 2022) 065305. 42",
            "year": 2022
        },
        {
            "authors": [
                "R.A. Konoplya"
            ],
            "title": "Quasinormal behavior of the D-dimensional Schwarzschild black hole and the higher order WKB approach",
            "venue": "Phys. Rev. D 68 (Jul, 2003) 024018.",
            "year": 2003
        },
        {
            "authors": [
                "P. Grohs",
                "F. Hornung",
                "A. Jentzen",
                "P. von Wurstemberger"
            ],
            "title": "A proof that artificial neural networks overcome the curse of dimensionality in the numerical approximation of black-scholes partial differential equations",
            "venue": "2018.",
            "year": 2018
        },
        {
            "authors": [
                "Y. Shin"
            ],
            "title": "On the convergence of physics informed neural networks for linear second-order elliptic and parabolic type PDEs",
            "venue": "Commun. Comput. Phys. 28 no. 5, (Jun, 2020) 2042\u20132074.",
            "year": 2020
        },
        {
            "authors": [
                "Y. Shin",
                "Z. Zhang",
                "G.E. Karniadakis"
            ],
            "title": "Error estimates of residual minimization using neural networks for linear PDEs",
            "venue": "2020. 43",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "In recent years there has been an increased interest in neural networks, particularly with regard\nto their ability to approximate partial differential equations. In this regard, research has begun\non so-called physics-informed neural networks (PINNs) which incorporate into their loss function\nthe boundary conditions of the functions they are attempting to approximate. In this paper,\nwe investigate the viability of obtaining the quasi-normal modes (QNMs) of non-rotating black\nholes in 4-dimensional space-time using PINNs, and we find that it is achievable using a standard\napproach that is capable of solving eigenvalue problems (dubbed the eigenvalue solver here). In\ncomparison to the QNMs obtained via more established methods (namely, the continued fraction\nmethod and the 6th-order Wentzel, Kramer, Brillouin method) the PINN computations share the\nsame degree of accuracy as these counterparts. In other words, our PINN approximations had\npercentage deviations as low as (\u03b4\u03c9Re , \u03b4\u03c9Im) = (< 0.01%, < 0.01%). In terms of the time taken to compute QNMs to this accuracy, however, the PINN approach falls short, leading to our conclusion\nthat the method is currently not to be recommended when considering overall performance.\nPACS numbers: 04.20.-q, 07.05.Mh, 02.60.Cb\n\u2217 acornell@uj.ac.za \u2020 ncubeanele4@gmail.com \u2021 gerhard.harmsen5@gmail.com\nar X\niv :2\n20 5.\n08 28\n4v 2\n[ ph\nys ic\ns. co\nm p-\nph ]\n2 4\nO ct\nI. INTRODUCTION\nIn recent years there has been an increased interest in the use of neural networks (NNs) as functional approximators [1\u20133]. The interest lies in the fact that NNs are versatile as demonstrated in their success in various applications such as natural language processing, image recognition and, more recently, scientific computing [4\u20136]. In scientific computing, they have been shown to be robust and data-efficient solvers of partial differential equations that govern diverse systems studied in mathematics, science and engineering [4]. In general, NNs can be trained once and used in a variety of situations that are within the scope of the problem it was trained on. The advantage of applying a trained NN is that it expedites the computation of later solutions whereas, by contrast, more traditional numerical approximating methods would require an inefficient process beginning from scratch each time a solution is derived. Furthermore, NNs are also natively parallelisable, which adds to their higher computational efficiency compared to other numerical approximations.\nA new technique has recently been developed to assist in creating NNs that can act as functional approximators, which takes inspiration from boundary type problems where the boundary conditions of the function are used to solve for the underlying function as-is ; namely, physics-informed neural networks (PINNs) [4, 7]. In this regard, we are interested in determining if these types of NNs could be used to compute the quasi-normal modes (QNMs) of black holes. The QNMs of black holes have been studied for many years and it is well-known that they are correlated to the parameters of the black holes that generate them, and as such, they act as a telltale sign to probe the properties of black holes [8\u201310].\nOver the years numerous techniques have been used to determine the numerical values of black holes using the radial equations that govern the perturbations of black holes [9]. Some examples are the Wentzel, Kramer, Brillouin (WKB) method, asymptotic iteration method (AIM), and the continued fraction method (CFM) [11\u201313]. Although all of these approaches have been successful in solving the radial equations of black holes to determine the numerical values of the QNMs; however, they do have computational limitations [8]. The WKB, in particular, becomes progressively difficult to apply when more accurate results are needed since achieving this requires painstaking derivations of higher-order approximations. In this work, we intend to show that PINNs can potentially supplement the extant techniques as a new alternative method for obtaining the black hole QNMs, with its unique advantages and\nlimitations. Furthermore, we will compare the accuracy of PINNs to the already established methods and test their generalisability when applied to black hole perturbations equations.\nOur motivation for using the equations of QNMs to test the usefulness of PINNs is that the equations that govern the QNMs are based on only a few parameters, namely a black hole\u2019s physical properties, and their boundary conditions are well-defined for the system. As such, the boundary conditions act as a regularisation mechanism that sufficiently limits the space of admissible solutions and contributes to the NN\u2019s stability [4]. Furthermore, in astrophysical circles, there has been an increased interest in black hole QMNs given the recent landmark detections of gravitational waves at the VIRGO and LIGO detectors [14, 15].\nThe paper is set out in the following manner. In the next section, we describe the equations that govern the QNMs for various space-times. In section III we present the currently accepted methods for determining the QNMs and proceed to touch on the new PINN approach in section IV. Finally, in sections V and VI, we discuss the results obtained from applying PINNs and compare them to the QNMs obtained from the canonical methods."
        },
        {
            "heading": "II. THE RADIAL PERTURBATION EQUATIONS OF BLACK HOLES",
            "text": "In this section, we will derive the equations required to determine the numerical value of QNMs beginning with the simplest space-times and then building up to more complex ones, which will eventually be encoded into the numerical methods. We begin with the Schwarzschild metric:\nA. The asymptotically flat Schwarzschild solution\nWe consider scalar-type perturbations (and later, electromagnetic, Dirac and gravitational perturbations) then in order to derive the radial equations required to determine the QNMs we begin by considering the equation of motion, which is given by the Klein-Gordon equation [16, 17]:\n\u2202\u00b5\u2202 \u00b5\u03a6 +m2\u03a6 = 1\u221a |g| \u2202\u00b5( \u221a |g|g\u00b5\u03bd\u2202\u03bd\u03a6) +m2\u03a6 = 0, (2.1)\nwhere \u03a6 is a scalar field with mass m perturbing the black hole\u2019s space-time as given by the\nmetric g. In the case of the Schwarzschild black hole, the metric is given as:\nds2 = \u2212fdt2 + 1 f dr2 + r2(d\u03b82 + sin2\u03b8d\u03c6), (2.2)\nwhere f = 1 \u2212 2M/r is the metric function, with M and r representing the mass of the black hole and the radial distance from the centre of the black hole, respectively. The last two terms on the right-hand side of this equation represent the metric of a 2-sphere [16]. As r \u2192 \u221e, we expect to recover a weak-field approximation of the metric wherein the components of the metric tensor can be decomposed into the flat Minkowski metric tensor \u03b7\u00b5\u03bd plus a small perturbation |h\u00b5\u03bd | 1; that is: g\u00b5\u03bd \u2248 \u03b7\u00b5\u03bd + h\u00b5\u03bd [16]. Considering the massless form of the Klein-Gordon equation, where m = 0 in equation (2.1), and plugging in it the metric given in equation (2.2) we obtain:\n1\u221a |g| \u2202\u00b5( \u221a |g|g\u00b5\u03bd\u2202\u03bd\u03a6) = \u2212\n( 1\u2212 2M\nr\n)\u22121 \u22022\u03a6\n\u2202t2 +\n1 r2 \u2202 \u2202r\n[ r2 (\n1\u2212 2M r\n) \u2202\u03a6\n\u2202r\n]\n+ 1\nr2 sin \u03b8\n\u2202\n\u2202\u03b8\n[ sin \u03b8 \u2202\u03a6\n\u2202\u03b8\n] +\n1\nr2 sin2 \u03b8\n\u22022\u03a6 \u2202\u03c62 = 0. (2.3)\nIn this explicit form, we can derive the equation of massless scalar fields in the Schwarzschild background in terms of the radial coordinate r via a separation of variables [18]. By mapping the resulting one-dimensional differential co-ordinate into an infinite domain given by a tortoise co-ordinate, x, we find [11, 19]:\nd2\u03c8 dx2 + { \u03c92 \u2212 V (r) } \u03c8 = 0, (2.4)\nwhere:\nV (r) = ( 1\u2212 2M\nr\n)[ `(`+ 1)\nr2 +\n2M\nr3\n] , (2.5)\nf(r) = dr\ndx =\n( 1\u2212 2M\nr\n)\u22121 . (2.6)\nHere n, ` and m are the principal, multipole, azimuthal and numbers, respectively [9]. The tortoise coordinate maps the location of the event horizon of the Schwarzschild black hole from r = 2M (in geometric units) to x = \u2212\u221e. As such, it maps the space from a semi-infinite domain to an infinite one. Note that equation (2.4) is quite similar in form to the one-dimensional time-independent Schro\u0308dinger equation, but in this case, V (r) is the effective potential for a which scalar field perturbs an asymptotically flat Schwarzschild metric [13, 18]. The QNM frequencies, \u03c9, are complex-valued solutions to equation (2.4),\nwhich is a non-Hermitian problem, unlike the Schro\u0308dinger equation [10]. For asymptotically flat astrophysical black holes, the eigenfunctions, \u03c8, that solve this equation have asymptotic behaviour governed by [13]:\n\u03c8(x) =    e\u2212i\u03c9x, x\u2192 \u2212\u221e e+i\u03c9x, x\u2192 +\u221e . (2.7)\nTransforming from the infinite domain of the tortoise co-ordinate to the finite domain of\na new variable, \u03be = 1\u2212 2M/r, it can be shown that equation (2.4) takes the form [13]:\n\u03c7\u2032\u2032 = \u03bb0(\u03be)\u03c7 \u2032 + s0(\u03be)\u03c7, (2.8)\nwhere\n\u03bb0(\u03be) = 4Mi\u03c9(2\u03be2 \u2212 4\u03be + 1)\u2212 (1\u2212 3\u03be)(1\u2212 \u03be)\n\u03be(1\u2212 \u03be)2 , (2.9)\ns0(\u03be) = 16M2\u03c92(\u03be \u2212 2)\u2212 8Mi\u03c9(1\u2212 \u03be)\u2212 `(`+ 1) + (1\u2212 s2)(1\u2212 \u03be)\n\u03be(1\u2212 \u03be)2 , (2.10)\nand \u03c7(\u03be) is a complex-valued scale factor. The boundary conditions have been incorporated into equation (2.8). The importance of this transformation is that it maps the domain from one that is infinite, i.e. \u2212\u221e < x < +\u221e to one that is finite, i.e. 0 \u2264 \u03be < 1. As such, it is now possible to numerical solve the perturbation equation since the domain is now finite and the QNM boundary conditions are implicitly accounted for.\nFor electromagnetic field perturbations of Schwarzschild black holes, the same Schro\u0308dinger-\nlike radial equations are obtained by following the same procedure for deriving the massless scalar fields. However, in this case, the equation of motion considered is the source-free Gauss-Ampe\u0300re law of Maxwell\u2019s equations [16, 20]:\nF \u00b5\u03bd;\u03bd = 1\u221a |g| \u2202\u03bd\n(\u221a |g|F \u00b5\u03bd ) = 0, (2.11)\nwhere F \u00b5\u03bd is the electromagnetic field tensor. Applying the components of the electromagnetic field tensor F \u00b5\u03bd , we can determine the radial perturbation equation from Maxwell\u2019s equations: \u2202\n\u2202t F \u00b5t +\n1 r2 \u2202 \u2202r (r2F \u00b5r) + 1 sin\u03b8 \u2202 \u2202\u03b8 (sin\u03b8F \u00b5\u03b8) + \u2202 \u2202\u03c6 F \u00b5\u03b8 = 0. (2.12)\nWe can simplify the equations with indices \u00b5 = \u03b8 and \u00b5 = \u03c6 to obtain the Schro\u0308dinger-like perturbation equations. In short, we arrive at:\n\u2212 \u2202 2a0(t, r)\n\u2202t2 + f 2\n\u22022a0(t, r)\n\u2202r2 \u2212 f`(`+ 1) r2 a0(t, r) = 0, (2.13)\nwhere a0(t, r) represents the electromagnetic field perturbations. Thus, if we have a0(t, r) = a0(r)e i\u03c9t, converting to tortoise co-ordinates we retrieve equation (2.4), where \u03c8(r) = a0(r) and V (r) = `(` + 1)f(r)/r 2 is the effective potential of an asymptotically Schwarzschild black hole perturbed by an electromagnetic field. For gravitational perturbations, the equations have the same form except for the effective potential V (r). Refs. [20, 21] outline concisely the steps for arriving at the wave equations for these direct metric perturbations on a Schwarzschild black hole.\nB. The Schwarzschild (anti)-de Sitter solution\nWe shall also consider asymptotically curved space-times that are solutions to Einstein\u2019s equations with a non-zero cosmological constant. The cosmological constant, denoted by \u039b, encodes the curvature of space-time via the relation \u039b = \u00b13/a2, where a is the cosmological radius [22, 23]. The metric in this case is:\nds2 = \u2212 (\n1\u2212 rs r \u2212 \u039br\n2\n3\n) dt2 + ( 1\u2212 rs\nr \u2212 \u039br\n2\n3\n)\u22121 dr2 + r2d\u21262. (2.14)\nWith this metric as a starting point, the radial perturbation equation derived for a 4- dimensional (anti)-de Sitter Schwarzschild black hole is the same form as equation (2.4) but, with a more general effective potential given as [13]:\nV (r) = f(r)\n[ `(`+ 1)\nr2 + (1\u2212 s2)\n( 2M\nr3 \u2212 (4\u2212 s 2)\u039b 6\n)] , (2.15)\nwhere f(r) = 1 \u2212 2M/r \u2212 (\u039br2)/3 is the metric function for (anti)-de Sitter Schwarzschild space-times and s = 0, 1/2, 1 and 2 denote the spins of scalar, Dirac, electromagnetic and gravitational fields, respectively.\nC. Near extremal Schwarzschild and Reissner-Nordstro\u0308m-de Sitter solutions\nA final case we shall consider are Reissner-Nordstro\u0308m-de Sitter black holes, albeit in the\nnear extremal case. The metric of a Reissner-Nordstro\u0308m-de Sitter black hole is [22]:\nds2 = \u2212 (\n1\u2212 rs r + r2Q r2 \u2212 \u039br 2 3\n) dt2 + ( 1\u2212 rs\nr + r2Q r2 \u2212 \u039br 2 3\n)\u22121 dr2 + r2d\u21262, (2.16)\nwhere rs = 2M (the Schwarzschild radius) and r 2 Q = Q 2/4\u03c0 0. Generally, when solving radial perturbation equations, the nature of the effective potentials preclude applying a direct, analytical approach to deriving exact QNMs [11]. However, in special cases, such as this one involving near extremal Schwarzschild and Reissner-Nordstro\u0308m-de Sitter black holes, the effective potentials can be transformed to yield differential equations with known analytic solutions [9].\nTo obtain the effective potentials of non-rotating black holes in the near extremal (anti)de Sitter case, we consider the relevant metric function, f(r) = 1 \u2212 2M/r \u2212 \u039br2/3. The solutions to f(r) = 0 are rb and rc, which are the black hole\u2019s event horizon and the spacetime\u2019s cosmological radius, respectively (where rc > rb). For r0 = \u2212(rb + rc), the metric function can be given as [23]:\nf(r) = 1\na2r (r \u2212 rb)(rc \u2212 r)(r \u2212 r0), (2.17)\nwhere a2 = r2b + rbrc + r 2 c and 2Ma 2 = rbrc(rb + rc). The surface gravity, \u03ba, associated with the black hole event horizon r = rb is defined as [23]:\n\u03ba = 1\n2\ndf dr \u2223\u2223\u2223\u2223 r=rb = (rc \u2212 rb)(rb \u2212 r0) 2a2rb . (2.18)\nIn the near extremal de Sitter case, the cosmological horizon rc of the space-time is very close (in the co-ordinate r) to the black hole horizon rb so that (rc \u2212 rb)/rb 1, and the following approximations apply [23]:\nr0 \u223c \u22122r2b ; a2 \u223c 3r2b ; M \u223c rb 3 ; \u03ba \u223c rc \u2212 rb 2r2b . (2.19)\nAlso since the domain of r is within (rb, rc) and rb \u223c rc, we find that r\u2212 r0 \u223c rb\u2212 r0 \u223c 3r0. In turn the metric function equation (2.17) becomes:\nf \u223c (r \u2212 rb)(rc \u2212 r) r2b . (2.20)\nWith this new form of the metric, the relation between the tortoise co-ordinate and the radial co-ordinate (2.6) reduces to:\nr = rce 2\u03bax + rb 1 + e2\u03bax . (2.21)\nSubstituting this expression for r into the f(r) equation (2.17), we find the expression for f(x) as [23]:\nf(x) = (rc \u2212 rb)2\n4r2bcosh 2(\u03bax)\n. (2.22)\nWith this metric function, the effective potential of a near extremal Schwarzschild-de Sitter black hole is an inverted Po\u0308schl-Teller potential [23]:\nV (x) = V0\ncosh2(\u03bax) , (2.23)\nwhere V0 = \u03ba 2`(`+ 1) for massless scalar and electromagnetic perturbations and V0 = \u03ba 2(` + 2)(` \u2212 1) for gravitational perturbations. With the effective potential in this form, the perturbation equation (2.4) can now be solved analytically to derive the QNMs of near-extremal Schwarzschild-de Sitter black holes.\nFor astrophysical near-extremal de Sitter black holes, the asymptotic behaviour of the solution is similar to that of an asymptotically flat Schwarzschild black hole equation (2.7); considering that they force the solution near the event horizon (cosmological horizon) not to generate outgoing (incoming) waves.\nConsidering the boundary conditions for astrophysical black holes, equation (2.7), Ref. [24] determined the analytic expressions of the QNM eigenfunctions and eigenfrequencies [22\u201324] as:\n\u03c8(x) = [\u03be(\u03be \u2212 1)]i\u03c9/2\u03ba \u00b7 2F1 ( 1 + \u03b2 + i \u03c9\n\u03ba ,\u2212\u03b2 + i\u03c9 \u03ba ; 1 + i \u03c9 \u03ba ; \u03be ) , (2.24)\n\u03c9 \u03ba =\n\u221a( `(`+ 1)\u2212 1\n4\n) \u2212 i ( n+ 1\n2\n) , n = Z+0 , (2.25)\nwhere \u03be\u22121 = 1 + exp (\u22122\u03bax) and \u03b2 = \u22121/2 + (1/4\u2212 V0/\u03ba2)1/2. Extending from near extremal Schwarzschild-de Sitter black holes, Ref. [22] showed that an inverted Po\u0308schl-Teller potential can also be used to represent the effective potential of Reissner-Nordstro\u0308m black holes perturbed by scalar fields. This is due to the fact that for any de Sitter black hole in the near extremal limit, the metric function f(r) is given as [22]:\nf(r(x)) = (r2 \u2212 r1)\u03ba1 2cosh2\u03ba1x +O(\u03b43), (2.26)\nwhere \u03b4 = (r2\u2212r1)/r1, \u03ba1 is the surface gravity at the horizon, r1 and r2 are two consecutive positive roots of f(r), and x is the tortoise coordinate whose domain lies within (r1, r2).\nFor both Schwarzschild and Reissner-Nordstro\u0308m-de Sitter cases, the terms r1 and r2 are the event and cosmological horizons, respectively, with r2 > r1. In the near extremal limit where r2 \u223c r1, the metric function for a near-extremal Reissner-Nordstro\u0308m-de Sitter black hole would take the same form as equation (2.22). Therefore, when considering the near extremal limit, non-rotating black holes share the same mathematical expression for the metric function, which in turn results in the same expression for the effective potential, equation (2.23). From that, we can infer that the corresponding analytic expressions for QNMs of a near extremal Reissner-Nordstro\u0308m-de Sitter black holes are the same as for Schwarzschild-de Sitter black holes as given by equations (2.24 - 2.25)."
        },
        {
            "heading": "III. ESTABLISHED NUMERICAL METHODS FOR DETERMINING QNMS",
            "text": "The perturbation equations of near extremal non-rotating black holes are among a few known cases with exact QNMs as solutions. As we shall use these equations to measure the accuracy of the PINN approach applied in the context of QNMs. More generally, though, the radial perturbations equations of Schwarzschild and Reissner-Nordstro\u0308m black hole perturbations are difficult to solve analytically, though not as challenging as cases involving rotating black holes, which require a more arduous investigation. Therefore, approximation techniques have been employed in the past to determine QNMs. We outline here a few prominent techniques used in the literature on black hole QNMs.\nA. Ferrari and Mashhoon approach\nRef. [24] showed the connection between the QNMs of black holes and the bound states of inverted black hole effective potentials. The effective potential, denoted by U in Ref. [24] is parametrised by some constant p and is invariant under the transformations p\u2192 p\u2032 = \u03a0(p) and x\u2192 \u2212ix, as in:\nU(\u2212ix; p\u2032) = U(x; p). (3.1)\nBy considering x\u2192 \u2212ix, the Schro\u0308dinger-like perturbation equation (2.4) transforms to: d2\u03c6\ndx2 + (\u2212\u21262 + U)\u03c6 = 0, (3.2)\nwhere \u03c6(x; p) = \u03c8(\u2212ix; p\u2032) and \u2126(p) = \u03c9(p\u2032). The QNM boundary conditions then become:\n\u03c6\u2192 exp(\u2213\u2126x), as x\u2192 \u00b1\u221e. (3.3)\nIn this new form, the problem has become a bound state problem with the original black hole effective potential inverted to \u2212U . The transformed boundary conditions, equation (3.3), now correspond to vanishing states at both infinities as expected for bound state problems. After solving this problem to find \u2126 and \u03c6, the corresponding QNMs can then be found using inverse transformations:\n\u03c9(p) = \u2126(\u03a0\u22121(p)), \u03c8(x; p) = \u03c6(ix; \u03a0\u22121(p)). (3.4)\nThe values of \u03c9, that are determined from the bound states \u2126, are known as proper QNMs. Ref. [24] demonstrated this approach using an inverted Po\u0308schl-Teller potential to approximate the effective potential of a Schwarzschild black hole. The former was used because the bound states of a Po\u0308schl-Teller potential are well-known and could then provide approximate analytic formulas for the QNMs of the Schwarzschild black hole [24].\nB. WKB Method\nThe WKB method is a semi-analytic technique that has been used to approximately solve the radial equation of black hole perturbations since 1985, as first proposed by Schutz and Will [25], where they computed the QNMs of an asymptotically flat Schwarzschild black hole. It had already been established as an approximating technique for solving the time-independent Schro\u0308dinger equation.\nC. Continued Fraction Method\nIn a 1985 paper [12], Leaver put forward the method of continued fractions (previously used to compute the electronic spectra of the hydrogen molecule ion) to compute the QNM spectra of both stationary and rotating black holes. Overall, this approach was found to be very accurate for higher-order n modes, especially after the improvement made by Nollert [26]. It has been used in the context of Schwarzschild, Kerr and Reissner-Nordstro\u0308m black holes [8, 12, 27].\nD. Asymptotic Iteration Method\nThe AIM is another semi-analytic technique for solving black hole perturbations. In the context of black hole QNMs, this approach was developed by Ref. [13] who made improvements to a more traditional algorithm to make it markedly more efficient. In Ref. [13] the improved AIM was used to compute of QNMs for cases involving (A)dS, Reissner-Nordstro\u0308m and Kerr black holes. In later research, it was used to calculate QNMs of general dimensional and non-singular Schwarzschild black holes [28, 29]. Compared to other extant approximation techniques, the improved AIM was shown to be as accurate as Leaver\u2019s CFM [13]."
        },
        {
            "heading": "IV. PHYSICS-INFORMED NEURAL NETWORKS",
            "text": "As briefly recapped above, there are several techniques that already exist for solving radial equations in order to obtain the QNMs of black holes. To supplement them, we now introduce PINNs as an alternative to these methods. Firstly, we introduce the idea of deep neural networks and how they can act as universal function approximators. We then introduce PINNs and how they can be used to solve ordinary differential equations (ODEs) and partial differential equations (PDEs).\nA. Deep Neural Networks\nDeep neural networks are a system of interconnected computational nodes loosely based on biological neural networks and, mathematically, can be formulated as compositional functions [6, 30]. In contrast to shallow neural networks, which are networks with just a single hidden layer, these NNs are composed of two or more hidden layers [3]. In many applications, the latter are favoured because they are capable of replicating the complexity of functions and, at the same time, generalise well to unseen data better than shallow models [31].\nOf several available types of structures (or architectures) of deep neural networks, the\nsimplest and most common one is the feed-forward neural network (FNN).\nDefinition 4.1. The FNN is comprised of neurons that hold single numerical values (called activations) combine to form a NN N L(x) that is a series of L layers with N` neu-\nrons in the `-th layer. There are L \u2212 1 hidden layers, N0 number of neurons in the input layer (` = 0) and NL number of neurons in the output layer (` = L). The transformations combining the neurons in the (`\u2212 1)-th layer to those in the `-th layer are weight matrices and bias vectors W` \u2208 RN`\u00d7N`\u22121 and b` \u2208 RN` , respectively. With these transformations, a FNN is generally structured as follows [30]:\ninput layer: N 0(x) = x \u2208 RN0 ,\nhidden layers: N `(x) = \u03c3(W`N `\u22121(x) + b`) \u2208 RN` , for 1 \u2264 ` \u2264 L\u2212 1, output layers: N L(x) = \u03c3(W`N L\u22121(x) + bL) \u2208 RNL ,\nwhere \u03c3 denotes nonlinear activation functions that operate on W`N `\u22121(x) + b` elementwise. Examples of frequently used activation functions are the hyperbolic tangent (tanh) and the logistic sigmoid 1/(1 + e\u2212x). Given that these are nonlinear functions, this makes values at each of the output nodes nonlinear combinations of the values at the nodes in the hidden and input layers [32].\nKey seminal research on NNs, such as Refs. [33\u201335], has shown that deep neural networks are universal function approximators. That is to say, when NNs have a sufficient number of neurons they can approximate any function and its partial derivatives [30], though in practice this is constrained by the limit in the size of NNs that can be set up before they lead to overfitting. In such cases, the NN model gives the illusion of a good model that captures the underlying pattern in data, while a true test of its accuracy by means of exposing it to an unseen test dataset reveals a fallible model that gives poor predictions and a high generalisation error [3, 30]. In general, training deep NNs entails minimising a loss function that measures the deviation of its approximations from the expected solutions. Analogous to linear least squares regression, the loss function is minimised via tuning of the many parameters in the deep neural network (which are the elements of its weight matrices and bias vectors) with the effect of steering their approximations closer to the target functions.\nMathematically, the weights and biases are tuned according to the equations:\nw`jk \u2192 w`jk \u2212 \u03b7\nm\n\u2211\nx\n\u2202Cx \u2202w`jk , (4.1)\nb`j \u2192 b`j \u2212 \u03b7\nm\n\u2211\nx\n\u2202Cx \u2202b`j , (4.2)\nwhere Cx is the loss function of the FNN computed for a single training example x that is taken from a minibatch of m training examples, which in turn are taken from a training\ndataset with n samples. These equations govern stochastic gradient descent optimisation, an algorithm that entails randomly selecting different minibatches from the training dataset of n examples until all of them are exhausted (this constitutes one epoch of training). In equations (4.1) and (4.2), \u03b7 is a small, positive parameter known as the learning rate. Ultimately, the Adam optimiser is employed in our investigation of PINNs. It is a standard optimisation algorithm that extends from classical methods of stochastic gradient descent [3, 36].\nB. Physics-informed neural networks\nInspired by deep neural networks, PINNs follow the same modus operandi as traditional NNs. Similar to traditional NNs, PINNs are trained through gradient-descent optimisation, whereby the partial derivatives of the loss function (with respect to the network\u2019s weight and biases) are minimised by tuning the weights and biases of the FNN. However, the difference is in the constraints that are embedded within the loss function of the PINNs which enable them to solve PDEs. These constraints are the PDEs themselves (or the governing equations) and their associated initial/boundary conditions [37].\nAutodiff is a technique that is used in PINNs to compute the partial derivatives of the NN approximations and thus embed the governing PDEs and associated boundary conditions in the loss function. Given that it facilitates \u201cmesh-less\u201d numerical computations of derivatives, it endows PINNs with several advantages over traditional numerical discretisation approaches for solving PDEs (such as the finite difference and finite element methods) that can be computationally expensive due to complex mesh-generation [6, 30, 38].\nFor example, Refs. [6, 30] demonstrated the advantage of applying NN-aided techniques over using traditional mesh-based techniques to approximate solutions with steep gradients. The latter give rise to unphysical oscillations when the meshes have low resolution, hence higher resolutions are required to remove these undesirable oscillations, which can be prohibitively expensive and lead to excessive execution times [6]. Remarkably, the same level of accuracy that is achieved by higher resolution meshes (in mesh-based schemes) can be achieved more efficiently in PINNs. In such cases, PINNs could be a viable alternative for solving PDEs.\nIt is worth noting that derivatives of Pade\u0301 approximations can be utilised as an alternative\nto PINNs and autodiff, which is a part of the PINN algorithm. They have indeed been applied in extensions of the WKB method in computing black hole QNMs [28]; therefore, the focus has been to compare their performance (and that of other established approaches in black hole QNMs) with the novel PINNs in this physical context.\nThe basic structure of PINNs can be divided into two components [30, 38]:\n1. A deep neural network with a particular architecture, such as a FNN. It represents\nthe NN approximation of the PDE\u2019s solution (figure 1 (left)).\n2. A loss-function that measures the deviation of the FNN solution from the physical\nconstraints of the problem (figure 1 (right)). The NN learns the solution of the PDE through gradient-based optimisation, an algorithm that minimises the loss function through an iterative tuning of the weights and biases in the deep neural network.\nFeed-forward Neural Network\nIn general, to expand on the methodology, PINNs solve PDEs that are parameterised by\n\u03bb, satisfied by a dependent variable[39] u(x), and are expressed generally as [30]:\nf(x; \u2202u\n\u2202x1 , . . . ,\n\u2202u\n\u2202xd ;\n\u22022u\n\u2202x1\u2202x1 , . . . ,\n\u22022u\n\u2202xd\u2202xd ;\u03bb) = 0 on \u2126, (4.3)\nwhere x = (x1, . . . , xd) defined on a domain \u2126 \u2282 Rd. Along with a given PDE are its boundary conditions:\nB(u,x) = 0 on \u2202\u2126, (4.4)\nwhere B(u,x) stands for Dirichlet, Neumann Robin or periodic boundary conditions. Note that both steady-state and dynamic systems can be solved using PINNs; where, for the latter, time t is considered to be special component of x and \u2126 contains the time domain. As such, initial conditions are treated as a type of Dirichlet boundary condition on the spatio-temporal domain [30].\nRemark 4.1. It is worth noting the special nature of PINNs compared to traditional NNs, particularly in the case of classification and regression problems. While many FNNs are typically data-driven and highly dependent on labelled datasets, PINNs, however, are suitable within the scant data regime provided the physical laws governing a system are known [38]. In fact, PINNs are unsupervised and learn from based purely on the PDEs and boundary conditions in the case of forward problems and eigenvalue problems. The goal of forward problems is to find the dependent variable u(x) for every x provided \u03bb are known parameters. Eigenvalue problems are more challenging because both u(x) and \u03bb are unknown. In the case when PINNs utilise a labelled dataset these are inverse problems, where the goal is to determine \u03bb given a dataset (which can be small) of u(x) at given points x \u2282 \u2126.\n1. The PINN algorithm for solving PDEs\nPINNs follow these steps when solving forward, inverse and eigenvalue problems [30]:\n1. Build a neural network u\u0302(x; \u03b8) with parameters \u03b8: The neural network u\u0302(x; \u03b8)\ntakes in x as input and is a surrogate of the function u(x) that satisfies the governing PDE and boundary/initial conditions. Whereas, \u03b8 = {W`,b`}1\u2264`\u2264L is a set of all weight matrices and vectors in the neural network [30].\n2. Specify the training dataset T : In the case of forward and eigenvalue problems, we specify a dataset of \u201cunlabelled\u201d randomly distributed points in the domain (also\nknown as residual points). The points within the Tf \u2282 \u2126 are used to restrict the NN approximation u\u0302(x; \u03b8) to satisfy the physics imposed by the PDE. Similarly, the boundary points of the spatio-temporal domain Tb \u2282 \u2202\u2126 are used to restrict the NN to satisfy the physics represented by the initial/boundary conditions. For inverse\nproblems, since \u03bb is missing from the PDE, a \u201clabelled\u201d dataset of u(x), denoted by To, is required in addition.\n3. Specify a loss function by adding the weighted Euclidean norm of the PDE,\nboundary conditions, and other regularisation functions: In general, the loss function of PINNs will be given as [30]:\nL(\u03b8; T ) = wfLf (\u03b8; Tf ) + wbLb(\u03b8; Tb) + wrLr(\u03b8; Tr), (4.5)\nwhere wf , wb, wr are weights and still \u03b8 = {W`,b`}1\u2264`\u2264L, with ` specifying a hidden layer as defined in section IV A [5]. Additionally Lf and Lb are loss terms due to the PDE and initial/boundary conditions, respectively:\nLf (\u03b8; Tf ) = 1 |Tf | \u2211\nx\u2208Tf\n\u2225\u2225\u2225\u2225f(x; \u2202u\u0302\n\u2202x1 , . . . ,\n\u2202u\u0302\n\u2202xd ;\n\u22022u\u0302\n\u2202x1\u2202x1 , . . . ,\n\u22022u\u0302\n\u2202xd\u2202xd ; \u03bb\u0302)\n\u2225\u2225\u2225\u2225 2\n2\n, (4.6)\nLb(\u03b8; Tb) = 1 |Tb| \u2211\nx\u2208Tb \u2016B(u\u0302,x)\u201622 , (4.7)\nwhere the circumflex in u\u0302 and \u03bb\u0302 denotes that these are the NN\u2019s approximations of the dependent variable and any unknown PDE paramters of inverse problems. The loss term Lr represents regularisation functions in general. For example, for forward problems this term is left out while for inverse problems it is the error between the NN approximations and a \u201clabelled dataset\u201d of u(x):\nLf (\u03b8; Tf ) = 1 |Tr| \u2211\nx\u2208Tr \u2016u(x)\u2212 u\u0302(x)\u201622 . (4.8)\n4. Train the FNN towards the optimal weights and biases \u03b8\u2217 by minimising\nthe loss function L(\u03b8; T ): The goal of training is to optimise \u03b8, u\u0302 and \u03bb\u0302 such that we have:\n\u03b8\u2217, u\u0302\u2217, \u03bb\u0302\u2217 = argmin\u03b8,u,\u03bbL(\u03b8, u\u0302, \u03bb\u0302; T ) (4.9)\nNote that the loss function is highly nonlinear and nonconvex with respect to \u03b8, thus gradient-descent optimisers such as Adam are often used during training. The disadvantage of a nonconvex optimisation problems is the difficulty to find unique solutions compared to traditional numerical methods of solving PDEs [30].\nRemark 4.2. Two important differences between PINNs and typical NNs are worth noting. Firstly, the former has the approximate function u\u0302(x) bound by the domain \u2126 where the governing PDE is defined. Secondly, PINNs learn from their own predictions (which is to say the governing PDEs and initial/boundary conditions are sufficient to optimise u\u0302(x, ) with respect to \u03b8. The \u201cunlabelled\u201d dataset of points randomly selected from \u2126 are split into training and validation/test sets, which is not done in some variations of PINNs such as the eigenvalue solvers (see section IV B 3) since the FNN hyperparamters are fixed. In the case of inverse problems, when a dataset of the true values of u(x) is available, part of that dataset can be used for validating the approximate u\u0302(x) by computing the L2 relative error, one example of a test metric.\nIn the following, we discuss two examples of Python libraries which have been employed\nto construct PINNs; namely, DeepXDE [30] and Pytorch [7].\n2. The DeepXDE package\nThe DeepXDE package is customised primarily for constructing PINN models. To help elaborate on the DeepXDE package, we consider here a toy problem that was discussed in Ref. [13], which involves the same Schro\u0308dinger-like differential equation in equation (2.4) but with an inverted symmetric Po\u0308schl-Teller potential VPT (x) [13]:\nVPT (x) = 1\n2cosh2(x) . (4.10)\nIn the tortoise co-ordinate x, the domain of our problem is infinite, i.e. x \u2208 (\u2212\u221e,+\u221e), where the QNM boundary conditions are given by equation (2.7). Via quasi-exactly solvable theory, Ref. [40] found the exact solutions of equation (2.4) with V = VPT to be given as [13]:\n\u03c8n(x) = (cosh(x)) (i+1)/2\u03c7n(sinh(x)), (4.11)\n\u03c9n = \u00b1 1 2 \u2212 i(n+ 1 2 ), (4.12)\nwhere \u03c7n is a polynomial of degree n in sinh(x) and n = Z+0 .\nAs a first step to finding the approximate solutions using PINNs, we need to change to a new coordinate y = tanh(x), which maps the infinite domain \u2212\u221e < x < +\u221e to a finite domain of \u22121 < y < +1, so that equation (2.4) becomes [13]:\n(1\u2212 y2)2d 2\u03c8(y) dy2 \u2212 2y(1\u2212 y2)d\u03c8(y) dy +\n[ \u03c92 \u2212 1\n2 (1\u2212 y2)\n] \u03c8(y) = 0. (4.13)\nIn this form, numerical implementation of this problem in PINNs becomes possible. We test the feasibility of solving equation (4.13) given as an inverse problem using DeepXDE. We specify \u03c9 as an unknown to be tuned while the PINN undergoes training. The total loss function L(\u03b8; T ) of the PINN, in this case, is a weighted sum of the squared Euclidean (L2-) norm of the physical constraints, similar to equation (4.5) [30]:\nL(\u03b8; T ) = wfLf (\u03b8; Tf ) + wbLb(\u03b8; Tb) + woLo(\u03b8; To), (4.14)\nwhere:\nLf (\u03b8; Tf ) = 1 |Tf | \u2211\ny\u2208Tf\n\u2225\u2225\u2225\u2225(1\u2212 y2)2\u03c8\u0302\u2032\u2032 \u2212 2y(1\u2212 y2)\u03c8\u0302\u2032 + [ \u03c9\u03022 \u2212 (1\u2212 y 2)\n2\n] \u03c8\u0302 \u2225\u2225\u2225\u2225 2\n2\n, (4.15)\nLb(\u03b8; Tb) = 1\n2\n\u2211\ny\u2208Tb\n\u2225\u2225\u2225\u03c8\u0302(y)\u2212 \u03c8b(y) \u2225\u2225\u2225 2\n2 , Lo(\u03b8; To) =\n1 |To| \u2211\ny\u2208To\n\u2225\u2225\u2225\u03c8\u0302(y)\u2212 \u03c8(y) \u2225\u2225\u2225 2\n2 . (4.16)\nNote that wf , wb, wo are weights that are typically set to one and \u03b8 is as defined as in section IV B 1. T = {y1, y2, ..., y|T |} is a set which consists of all training points randomly selected from our 1D spatial domain (\u22121 < y < 1). The subset Tf are points chosen from the domain to train the FNN based on the governing equation (4.13). The subsets Tb(= {\u22121, 1}), To are the boundary points for training on the boundary conditions, and the dataset of the true values of the dependent variable (4.16), respectively. Training of this PINN proceeds as outlined in section IV B 1.\nFigure 2 illustrates the PINN for solving this problem. The input layer of the FNN consists of one input for co-ordinate y, while the output layer has two output nodes for real and imaginary parts of the approximate solution \u03c8\u0302.\nIn building PINNs, the code we used mirrors the two-component structure of PINNs discussed in section IV B. The code is fairly intuitive as it is a high-level representation that closely resembles the mathematical formulation [30]. Beginning with the physics constraints, our ODE is defined using the DeepXDE functions for executing the first and second-order derivatives via auto diff; that is, dde.grad.jacobian and dde.grad.hessian, respectively. We define \u03c9 with the function tf.Variable and have represented it with \u03c9\u0302Re and \u03c9\u0302Im in figure 2.\nTo provide Dirichlet boundary conditions and a labelled dataset, as needed to solve our inverse problem, we define both the real and imaginary parts of the known eigenfunction \u03c8(y) that satisfies equation (4.13). Numerically, at the true boundary points, y = \u22121 and\nPhysics constraints\ny = 1, the solution \u03c8(y) yields a complex-infinity. As such, a narrower domain \u22120.9 < y < 0.9 is specified in the definition of the domain of our problem using the function dde.geometry.Interval(-0.9, 0.9). The exact values of \u03c8 at these artificial boundary points are considered to be the Dirichlet boundary conditions. The DeepXDE function for defining these boundary conditions is dde.DirichletBC. To create a labelled dataset to train our PINN, we generate 50 equidistant points in the domain (\u22120.9, 0.9) and their associated exact eigenfunctions using equation (4.11). This dataset is the set To in equation (4.16).\nAt this stage, we have defined the physics constraints of the PINN, but for completeness, we set up the deep neural network (our surrogate model). In the code we also define a FNN with one input node, two output nodes and three hidden layers with 20 nodes per layer. In each of the hidden layer nodes, we use the nonlinear activation function \u201ctanh\u201d considering that it is a smooth, infinitely differentiable function [41]. Generally for PINNs, \u201csmooth\u201d activation functions are preferred over the ReLU-like non-smooth activation functions since the former have demonstrated significant empirical success [42]. For this reason, the tanh function is chosen here by default, however it is worth noting that (of late) adjustable, smooth function such as Swish have proven to outperform fixed functions such as tanh in\nterms of convergence rate and accuracy [41, 43]. Swish is defined by x\u00b7 Sigmoid(\u03b2x), where \u03b2 is a trainable parameter.\nThe loss function dde.Model combines the FNN with the physical constraints to form a complete PINN. We also add the \u201ccallback\u201d function dde.callback in the algorithm so as to keep track of the FNN approximations of \u03c9 during training. Finally, our PINN model is compiled and trained. Compilation defines the learning rate and algorithm for optimising our model. For training the model, we choose 20 000 training epochs wherein the model will be iteratively tuned based on the physics constraints. Figure 3 displays the evolution of the loss function and model accuracy over 20 000 epochs and compares the NN approximation of \u03c8(y) with the exact function. The PINN algorithm in DeepXDE illustrated here works well for inverse problems where \u03c8 is known at some points in the domain. However, for more general scenarios of black hole QNMs, where both \u03c9 and \u03c8 are unknown, we require an algorithm capable of solving eigenvalue problems.\n3. The eigenvalue solver\nOne such algorithm that we have investigated was initiated in Ref. [7] to solve quantum eigenvalue problems using unsupervised NNs (also called, data-free surrogate models). The authors experimented with their \u201ceigenvalue solvers\u201d on well-known equations in quantum mechanics; namely, the time-independent Schro\u0308dinger equation with an infinite square well potential and, in another case, a quadratic potential function of a quantum harmonic oscilla-\ntor. Although their approach is similar to the PINNs, in terms of embedding learning biases in the loss function, there is an additional feature which allows the eigenvalue solver to scan the eigenvalue space in a scheduled manner and progressively find several eigenvalues in a single training.\nTo help visualise this approach, we consider one well-known bound-state eigenvalue prob-\nlem [44]:\n\u2212 1 2 \u03c8\u2032\u2032(x) + V (x)\u03c8(x) = E\u03c8(x), (4.17)\nwhere:\nV (x) = \u2212\u03bb(\u03bb+ 1) 2 sech2(x), (4.18)\nwhich is a Po\u0308schl-Teller potential and \u03bb = 1, 2... . We can now change to a new co-ordinate u = tanh(x). As such, equation (4.17) can be written in the form of a Legendre differential equation:\n[(1\u2212 u2)\u03c8\u2032(u)]\u2032 + \u03bb(\u03bb+ 1)\u03c8(u) + 2E 1\u2212 u2\u03c8(u) = 0, (4.19)\nwhich is solved exactly by associated Legendre functions, i.e. \u03c8(x) = P \u00b5\u03bb (tanh(x)) with E = \u2212\u00b52/2 and \u00b5 = 1, 2, 3..., \u03bb. These are bound states that vanish at the boundaries of the eigenvalue problem, i.e. \u03c8(x = \u00b1\u221e) = 0 or \u03c8(u = \u00b11) = 0.\nThe eigenvalue solvers in Ref. [7] are built using the PyTorch library. To solve equation (4.19) using the eigenvalue solvers, we embed them in the loss function of the NN along with some regularisation terms, similar to equation (4.5):\nL(\u03b8; T ) = LODE(\u03b8; T ) + Lreg(\u03b8; T ), (4.20)\nwhere:\nLODE(\u03b8; T ) = 1 |T | \u2211\nu\u2208T\n[ ((1\u2212 u2)\u03c8\u0302\u2032(u))\u2032 + \u03bb(\u03bb+ 1)\u03c8\u0302(u) + 2E\u0302 1\u2212 u2 \u03c8\u0302(u) ]2 , (4.21)\nLreg(\u03b8; T ) = wfLf (\u03b8; T ) + wELE(\u03b8; T ) + wdriveLdrive(\u03b8; T ). (4.22)\nAs defined in section IV B 1, T is a set of training points randomly selected from the domain u \u2208 (\u22121, 1). Figure 4 illustrates how the boundary conditions (i.e. a vanishing solution at the boundary points) are enforced using a parametric function (1\u2212u)(1 +u). Note also the absence of the observational bias term (the reason our eigenvalue solver is called a data-free model).\nin DeepXDE, these FNNs are unsupervised. Instead, the unknown eigenpairs can be determined\nonly from the governing equations and boundary conditions (that are enforced using a parametric\nfunction (1\u2212 u)(1 + u), a \u201chard constraint\u201d, to ensure they are satisfied exactly [30]).\nIn equation (4.22) Lreg is a weighted sum of regularisation functions, where the weights wf , wE, wdrive are typically set to one [7]. Individually, the regularisation functions are:\nLf = 1 |T | \u2211\nu\u2208T\n1\n\u03c8\u03022 , LE =\n1 |T | \u2211\nu\u2208T\n1\nE\u03022 , Ldrive =\n1 |T | \u2211\nu\u2208T exp(\u2212E\u0302 + c), (4.23)\nwhere Lf and LE steer the learning algorithm away from zero as a possible value for the eigenfunction and eigenvalue, respectively. For this purpose, the mathematical form of these loss terms have the PINN approximations (\u03c8\u0302 and E\u0302) inversely proportional to the loss so that as they approach zero they are penalised by high loss values. The crucial term in these unsupervised NNs is Ldrive, which motivates the NN to scan through the space of eigenvalues. This is achieved by adding within the training algorithm a mechanism that increases the constant c in Ldrive at regular intervals, after an arbitrary number of training epochs. It is important to note that without the Ldrive loss component the PINNs lack the necessary constraint to learn other eigenvalues than the first energy level it initially gravitates towards during training, which is often but not always the ground energy level. In this case, the algorithm has more limitations, similar to the original, baseline PINN loss function (equation 4.5) where forward and inverse approaches can learn, respectively, only one of the\neigenfunctions and eigenvalues at a time, and only when at least one of the other eigenpairs is known. As a consequence, a classification approach (with, for example, output nodes of the PINN representing the dependent variable) may not be applicable because the loss function will only have the wherewithal to learn a single eigenstate in each training, regardless of the input data since it is unlabelled and is randomly selected from a domain where one eigenstate cannot be separated spatially from the other solutions.\nThe key Pytorch functions used in defining our physics constraints include\ntorch.autograd, which executes automatic differentiation to find the first and second derivatives in LODE given by equation (4.21). With the physics constraints defined, we set the structure of our FNN: 2 input nodes, 1 output node and 2 hidden layers with 50 nodes each (see figure 4), where our chosen activation function is the trigonometric function, sine. This activation function has been found to greatly accelerate the NN\u2019s convergence to eigenstates compared to more common functions, e.g. sigmoid and ReLU [7, 45].\nCompared to the code in DeepXDE, the eigenvalue solvers provided more flexibility when customising the training algorithm. The total loss function in our training algorithm was defined according to equations (4.20 - 4.23). To generate n train points from the domain of our example problem u \u2208 (\u22121, 1), we used the Pytorch function torch.linspace. In terms of optimisation, the standard Adam optimiser is applied [36].\nUltimately, the training phase follows after all parameters for training the model (such as the number of training epochs) have been defined. In our case, we chose the following\nparameters: 100 training points, 100 000 training epochs and a learning rate of 8 \u00d7 10\u22123. Figure 5 shows the resulting NN approximations of the eigenvalues and eigenfunctions. Note that the Ldrive is only included in the loss function of this example, for complete demonstration of the method. However, it is not applied in the QNM computations resulting in the PINNs converging on one eigenvalue (as we will see), rather than several (as in the many plateaus of figure 5). As seen in the example, the flips between eigenvalues occur arbitrarily, without any method of controlling when they occur. Therefore, this loss term requires further investigation, outside this present work, to make it less random."
        },
        {
            "heading": "V. RESULTS: QNM COMPUTATIONS WITH THE EIGENVALUE SOLVER",
            "text": "The results from our investigation of the performance of PINNs when applied to the computation of QNMs shall now be presented, where it is important to note that, generally for deep neural networks, there are no set rules for customising them since they are statistical tools with too many parameters to admit any meaningful physical interpretability. Taking this into account in this work, we have carried out grid-search-like experimentation of the NN hyperparameters to discover the most optimal choices with the best performance. Specifically, we considered a range of values for three hyperparameters, which are the number of training points, number of training epochs, and the number of nodes per layer, keeping the other hyperparameters (e.g. optimiser and activation function) fixed. In this work, we have focussed on computing the QNMs of a Schwarzschild black hole in the asymptotically flat and near extremal de Sitter cases. For the former, we considered massless scalar, Dirac, electromagnetic and gravitational field perturbations; while, for the latter, we only considered massless scalar fields where the equations look the same as near extremal ReissnerNordstro\u0308m-de Sitter black holes. Due to this, the QNMs of near extremal Schwarzschild-de Sitter black holes can be more generally treated as the QNMs of near extremal non-rotating de Sitter black holes.\nA. Scanning hyperparameters\nFigure 6 graphs the results we obtained from testing different hyperparameter configurations for computing the QNMs of an asymptotically flat Schwarzschild black hole\nTable 1: Training time efficiency: training epochs\n100 150 200 # of n eu ro n s p er la ye r 10 5.760 8.560 11.52 50 # of train in g p oin ts 20 6.420 8.710 11.94 50 6.950 9.910 13.31 10 6.110 8.750 11.89 100 20 6.060 9.100 12.53\n50 7.460 11.30 15.59\n10 6.130 8.910 12.25 150 20 6.580 9.940 13.53\n50 8.200 12.33 16.69\n1\nTable 1: Absolute Percentage deviation Real[\u03c9] Im[\u03c9]\ntraining epochs\n100 150 200 100 150 200\n# of\nn eu ro n s p er\nla ye r 10 0.011 0.009 0.009 0.043 0.042 0.043 50 # of\ntrain in g p oin ts\n20 0.009 0.009 0.009 0.039 0.043 0.042 50 9.006 0.008 0.008 188.3 0.053 0.055 10 0.018 3.980 0.009 0.069 206.1 0.042 100 20 0.008 0.011 0.009 0.046 0.045 0.045 50 0.009 0.010 0.009 0.065 0.046 0.042 10 0.009 0.010 0.008 0.042 0.041 0.042 150 20 0.009 0.009 0.009 0.041 0.046 0.042 50 0.009 0.010 0.009 0.065 0.046 0.042\n1\nFIG. 6. The training times, in minutes, (right panel) and percentage deviations (left panel) obtained for different hyperparameter choices. To compute the QNMs of asymptotically flat Schwarzschild BHs (s = 0, ` = 2, n = 0), we tested different permutations of the number of training points, number of neurons per layer and number of training epochs (\u00d71000).\n(s = 0, ` = 2, n = 0). The accuracy of the NN approximations (measured in terms of percentage deviation) and the execution times for training our NNs have been measured as a function of the number of training points, number of training epochs, and number of nodes per layer. The fixed hyperparameters were: learning rate of 8\u00d7 10\u22123, 2 hidden layers, and sine as the activation function.\nNote that the accuracy values measure the deviation of the NN approximations from Leaver\u2019s QNMs, whose precision is up to 4 decimal places [12, 18]. As seen in figure 6, the percentage deviations of our computations remain the same across all hyperparameter configurations. But for a few cases, the percentage deviations for the real and imaginary parts of the QNMs hover around about 0.009% and \u22120.042%, respectively. Both these values correspond to a 4 decimal place precision, making the NN approximations as good as Leaver\u2019s CFM. Note that beyond 4 decimal places we cannot reliably determine the accuracy of our NN approximations based on the QNMs given in the literature [12, 18].\nThe red cells given in the right panel of figure 6 correspond to cases where the eigenvalue solvers veer from determining the QNMs with a minimum loss, which are the n = 0 modes. These are few in comparison to \u201cnormal\u201d cases where the eigenvalue solvers converge to\na loss minimising solution. The displayed training times and percentage deviations were obtained by iterating the eigenvalue solver algorithm automatically and scanning through the specified range of hyperparameter combinations. Note that the total loss was set as:\nL(\u03b8; T ) = LODE(\u03b8; T ) + Lf (\u03b8; T ), (5.1)\nwhere:\nLODE(\u03b8; T ) = 1 |T | \u2211\n\u03be\u2208T [\u03c7\u2032\u2032 \u2212 \u03bb0(\u03be)\u03c7\u2032 \u2212 s0(\u03be)\u03c7]2 , (5.2)\nLf (\u03b8; T ) = 1 |T | \u2211\n\u03be\u2208T\n1\n\u03c7\u03022 . (5.3)\nHere \u03b8, T ,\u03c7\u0302 and \u03c9\u0302 have their definitions from sections II A and IV B 1. We have considered the ODE given by Ref. [13], which is a transformation of the radial perturbation equation (2.4) to a finite domain of the coordinate \u03be \u2208 (\u22121, 1). In this form of the loss function given by equation (5.1), the NN is motivated to converge on the QNMs with the highest amplitude, |\u03c7|, because of the regularisation loss term Lf . Incidentally, it turns out that the QNM with the highest |\u03c7| (for any given multipole number) is the n = 0 mode. This is consistent with the fact that for black hole QNMs, the higher overtones are damped faster [14].\nSome observations from figure 6 are that varying the hyperparameters, as we did, has no significant effect on the accuracy. However, there is an increase in the training time with the number of epochs for a fixed number of training points and neurons per layer. Additionally, an increase in the number of neurons per layer also leads to a slight increase in the training time. Therefore, to obtain a favourable trade-off between accuracy and efficiency, one may train for 100 000 epochs instead of 200 000 to achieve the same level of accuracy in less time. This reduction in training time becomes significant when running a large batch of computations.\nB. QNMs of near extremal non-rotating black holes\nIn our discussion of black hole perturbation equations in section II C, we have seen a special case where the effective potential is given exactly by an inverted Po\u0308schl-Teller potential; namely, the near extremal Schwarzschild and Reissner-Nordstro\u0308m-de Sitter black holes.\nIn these cases, analytic expressions of the QNMs are known and we could reliably test the accuracy of our NN approximations compared to the exact QNMs given as:\n\u03c9 = \u221a( `(`+ 1)\u2212 1\n4\n) \u2212 i ( n+ 1\n2\n) , n = 0, 1, 2, ... (5.4)\nwhere ` and n are as defined in section II A.\nIn table I, the exact QNMs for n = 0 and ` = 1, ..., 3 are compared with the NN ap-\nproximations (\u03c9 eigeNN ). The latter were obtained by embedding the governing differential equation of near extremal non-rotating de Sitter black holes and extra regularisation terms in the loss function. In the last column of table I are values that were produced by adding to the loss function a seed value loss term given as:\nLseed(\u03b8, \u03c9\u0302; T ) = 1 |T | \u2211\n\u03be\u2208T [\u03c9\u0302 \u2212 \u03c9seed]2 . (5.5)\nThe seed value loss term measures the deviation of the NN approximations from specific n and ` dependent seed values close to the exact QNMs (i.e. accurate up to a certain number of decimal places, e.g. 2 decimal places, in this case). The goal of the seed loss term is to steer the NN towards specific QNMs of the several possible differential equation residual minimisers (or eigenstates) that exist for a chosen multipole number `.\nThe plots in figure 7 are the NN approximations of the eigenpairs (\u03c9, \u03c8) associated with table I, where the first three multipole numbers for the n = 0 mode are superimposed. These are the QNM eigenfunctions that obey the asymptotic behaviour expected for astrophysical asymptotically de Sitter black holes [9]. As was pointed out previously, this is:\n\u03c8 \u223c pure outgoing wave, x\u2192 +\u221e. (5.6)\nMore importantly, figure 8 shows the evolution of the real (\u03c9Re) and imaginary (\u03c9Im) parts of the NN\u2019s approximations of the QNMs as they train for 100 000 epochs. These plots were obtained from our computations without the seed loss term in the loss function.\nFigure 8 shows that the convergence of the NN approximation (\u03c9eigeNN) towards the expected QNMs (\u03c9exact) given by equation (5.4), occurs swiftly after training begins. Table I and figure 8 indicate that the NN learns QNMs with similar levels of accuracy for different multipole numbers, regardless of the presence of a seed loss term in the loss function.\nFor this scenario, we have considered perturbations of asymptotically flat Schwarzschild black holes by massless scalar, Dirac, electromagnetic and gravitational fields given by equations (2.4) and (2.5) in the tortoise co-ordinate. Tables II - V compare our NN approximations of the QNMs with those given in the literature for the CFM and WKB approaches for solving the perturbation equations.\nWith regards to the set-up of our eigenvalue solvers, the same FNN configuration was used for all our computations. That is, we set up 2 hidden layers, 50 nodes per layer, and sine as the nonlinear activation function. Moreover, we employed the Adam optimiser, set up 90 000 training epochs and used a learning rate of 8\u00d7 10\u22123. Our training data consisted of 100 points randomly selected from the domain \u03be \u2208 [0, 1]. Note also that the percentage deviation values given inside the parentheses in the tables are:\npercentage deviation = |Re/Im[\u03c9eigeNN/WKB]| \u2212 |Re/Im[\u03c9Leaver]|\n|Re/Im[\u03c9Leaver]| \u00d7 100. (5.7)\nIn the plots of figure 9 - 12, the green line represents the seed values of \u03c9 that were embedded in the loss function of our eigenvalue solvers. Note that the NN converges towards the expected QNMs, rather than the seed values, which are given up to 2 decimal places. The QNM values given in tables I - V are in geometrical units.\nIn figures 9 - 12, it is clear that the eigenvalue solvers are able to learn the expected values of \u03c9Re and \u03c9Im for various perturbation scenarios of an asymptotically flat Schwarzschild black hole. For the fundamental mode, n = 0 and various choices of s and `, the physical constraints provided in the loss function are sufficient to steer the NN toward the exact, non-trivial solutions of our perturbation equations. This is remarkable, considering the conceptual simplicity of the NN optimisation algorithm.\nAs is evidenced by the tables II - V, the QNMs computed by our eigenvalues solvers are as accurate as the values computed with the CFM and the 6th-order WKB method. Since we have used the QNMs from Leaver [12, 18], which are given up to 4 decimal places, as our closest approximation to an exact solution, we only confirm accuracy up to that level. Needless to say, an ideal measure of accuracy of our approximations would be exact QNMs obtained via analytical methods for solving our differential equations. Note that the QNMs obtained using the AIM (which are not listed in the tables but can be found in Ref. [13]) were shown to be as accurate as Leaver\u2019s method.\nRegarding the time taken by eigenvalue solvers to complete executing computations, there is a significant difference between the duration for training our NNs compared to that of running the other approximation techniques. While it takes around 10 minutes to run each of the 90 000 epoch-long training sessions to solve our perturbation equations, the computation takes much less time. For example, in the case of the WKB and AIM techniques, many QNM values can be computed in less than a minute. Note that this comparison is tentative and reflects just the outcome of the present work, which is a baseline for potential future improvements. For example, the computational speeds of PINNs could be enhanced by tapping into the parallelisable nature of NNs. So far we have focussed on fine-tuning the accuracy of NN approximations as that is particularly important for black hole QNMs.\nIn terms of other performance measures, the scalability of PINNs, with regard to the ability to handle a large number of input dimensions, is one major advantage of PINNs over other numerical methods. As pointed out in Refs. [30, 47], NNs overcome the curse of dimensionality and, therefore, have the capacity to approximate high-dimensional functions quite efficiently. This attribute justifies the potential future extension of PINNs to compute the QNMs associated with high-dimensional perturbation scenarios, where traditional meshbased approximation techniques could suffer.\nOur results show signs of the expected limitations, listed in Ref. [38], of solving PDEs with\nNNs that have been observed in various applications of physics-informed machine learning. One is the fact that complicated loss functions (with many terms in the governing equations) lead to highly non-convex optimisation problems. As a result, the training process may not be sufficiently stable and convergence to the global minimum may not be guaranteed [38].\nWe can see this by contrasting the results obtained from our computations involving the relatively simple differential equation for near extremal non-rotating black holes versus the relatively more complex perturbation equations of asymptotically flat Schwarzschild black holes. For the former, figure 8 shows that our NN quickly converges towards the expected solution regardless of the multipole number, even in the absence of a seed loss term to further constrain the eigenvalue solvers. However, for the latter, the NN has more difficulty converging in lower multipole number cases as seen in figure 9 where the NN converges faster for ` = 2 when compared to ` = 0. In fact, without the seed loss term to solve asymptotically flat Schwarzschild black holes, the NN fails to converge when we have ` = 0, 1, 2 but does for ` > 3.\nIn our attempts to solve even more challenging problems, such as the perturbation equations of asymptotically flat Reissner-Nordstro\u0308m and asymptotically (anti)-de Sitter Schwarzschild black holes, the instability appears to be more pronounced as the NNs fail to converge on the expected QNMs for these cases. To alleviate this constraint and broaden the scope of PDEs to be solved we will need to add to our eigenvalue solvers some stronger constraints or features that address instability."
        },
        {
            "heading": "VI. DISCUSSIONS AND CONCLUSION",
            "text": "In summary, we have explored the possibility of implementing PINNs as a new technique to solve black hole perturbation equations. We considered two variations of PINNs built with the DeepXDE and Pytorch packages in Python. To give some background on the underlying physics, we began by revisiting the perturbation equations for static, spherically symmetric black holes, particularly asymptotically flat and (anti)-de Sitter Schwarzschild black holes whose perturbations are described by one-dimensional Schro\u0308dinger-like eigenvalue problems. Our goal was to determine when and how PINNs can be best applied to solve these equations, which are generally difficult to solve analytically and compute the QNMs of black holes.\nSince PINNs are extensions of deep neural networks, we outlined NNs in section IV A,\nin terms of their structure and the mechanisms behind their function approximation abilities. Afterwards, PINNs were described with illustrative examples showing how physics constraints are embedded in the loss function of a NN. These constraints include the governing PDE, its associated boundary conditions and regularisation functions.\nOf the two variations of PINNs considered in this work, the eigenvalue solvers were implemented to compute the QNMs of asymptotically flat Schwarzschild and near extremal non-rotating de Sitter black holes. Given that the latter scenario has known exact formulae for the QNM frequencies (given by Ref. [22\u201324]), we were able to reliably validate the accuracy of our NN approximations. We obtained QNM values with up to 6 digit accuracy and plots showing the evolution of the NN\u2019s approximation of the QNMs over a 100 000 epoch training phase. The plots showed that the NN\u2019s approximation quickly converged towards the expected solutions, regardless of the multipole number ` or the existence of a seed loss term in the loss function.\nRegarding the more analytically intractable problems, we managed to solve the perturbation equations of asymptotically flat Schwarzschild black holes by embedding the equations themselves, the QNM boundary conditions and a seed loss term into the loss function of the eigenvalue solvers. The computed QNMs have the same level of accuracy as those obtained through Leaver\u2019s CFM [12] or Konoplya\u2019s 6th order WKB method [46] (at least up to 4 decimal places as given in the literature [18]). However, in terms of efficiency, our eigenvalue solvers take several minutes to train, compared to the few seconds to a minute it takes to generate accurate results using other techniques such as the WKB method. We also found that the efficiency of PINNs could be optimised by setting up the NN using lower values in the range of values of the hyperparameters that we tested; that is, the number of training epochs, number of training points and number of nodes per layer.\nTo date, we have been able to compute only the fundamental mode frequencies (i.e. n = 0, ` \u2265 0) that, as it turns out, are the least damped, longest-lived modes compared to higher overtones with n > 0. This is because we have added regularisation terms that simultaneously penalise the NN for learning trivial eigenfunctions and encourage it to learn the most energetic QNMs, which happen to occur when n = 0 for any given `. Potential future work would seek a modification of the eigenvalue scanning mechanism similar to that introduced by Ref. [7], which will allow for the computation of higher overtones for our complex-valued QNMs.\nConcerning the question of the stability of PINNs as they increase in depth, that is still very much an open problem, in general, within the literature and is in the early stages of investigation. When studying PINNs to mimic the analysis of numerical discretisation techniques, convergence and stability are related to how well the NN learns from the physical laws embodied by the governing PDEs and initial/boundary conditions [41]. It is well-known that there is a bias-variance trade-off that comes with choosing the right depth of a neural network, where too deep a neural network may lead to overfitting and inefficiency. The most recent literature in laying out a theoretical framework for PINNs includes Refs. [48, 49] that provide formal findings regarding PINNs and their convergence when dealing with linear problems including second order elliptic, hyperbolic and parabolic type PDEs. Utilising these recent developments is important to understand and improve on PINNs in continued use to compute QNMs.\nAs discussed in section II A, our NNs exhibit signs of instability which we suspect to be a result of the level of complexity in the loss function, which makes for a highly non-convex optimisation process [38]. This is counter-intuitive to our initial expectation that PINNs can accurately solve any PDE (regardless of complexity) if they are formulated in a finite domain and their associated boundary conditions are properly set up. This was not the case for our attempts when applying eigenvalue solvers to the Reissner-Nordstro\u0308m case. To overcome this instability in future work, one plausible approach would be to consider the recent work in Ref. [42] that shows that a \u201cself-scalable\u201d activation function leads to PINNs which are less susceptible to spurious stationary points, an obstacle in highly non-convex loss functions.\nA final point to note concerning the limitations of PINNs is their relative inefficiency compared to the extant methods for computing QNMs. Further investigation needs to be done to improve the performance of the eigenvalue solvers as they currently do not surpass the efficiency of established methods such as the WKB method and the AIM. Overall, PINNs have not developed far enough to be applied broadly in the study of black hole perturbations. In conclusion to their seminal work on PINNs, Ref. [4] pointed out that this method should not be viewed as a replacement for classical numerical methods for solving PDEs, but rather as methods that can bring added merits such as implementation simplicity to accelerate the rate of testing new ideas. In a similar vein, the application of PINNs to QNMs brings at least a new angle to study the perturbation equations, even though they are not as efficient\nas canonical methods.\nAs is, the PINN approach may only work in computing the fundamental QNMs of not only four-dimensional Schwarzschild black holes, but also general dimensional Schwarzschild black holes (described in Ref. [9]) given the similarity of the differential equations. Despite the present challenges (which are expected for a burgeoning method) this approach to computing QNMs is worth pursuing further as it demonstrates the same level of accuracy as the leading existing methods.\nACKNOWLEDGMENTS\nThe authors would like to thank Jan Nordstro\u0308m for the enlightening discussions, and Wesley Doorsamy for insightful suggestions provided to drafts of this paper. ASC is supported in part by the National Research Foundation of South Africa (NRF). GEH is supported by UJ. AMN is supported by the UJ GES 4IR.\n[1] K. Hornik, M. Stinchcombe, and H. White, \u201cMultilayer feedforward networks are universal\napproximators,\u201d Neural Networks 2 no. 5, (1989) 359.\n[2] S. Wu and M. J. Er, \u201cDynamic fuzzy neural networks - a novel approach to function\napproximation,\u201d IEEE Transactions on Systems, Man, and Cybernetics, Part B\n(Cybernetics) 30 no. 2, (2000) 358.\n[3] M. A. Nielsen, Neural networks and deep learning, vol. 25. Determination Press, San\nFrancisco, CA, USA, (2015).\n[4] M. Raissi, P. Perdikaris, and G. Karniadakis, \u201cPhysics-informed neural networks: A deep\nlearning framework for solving forward and inverse problems involving nonlinear partial\ndifferential equations,\u201d J. Computat. Phys. 378 (2019) 686.\n[5] Y. Lu and J. Lu, \u201cA universal approximation theorem of deep neural networks for expressing\ndistributions,\u201d in Thirty-fourth Conference on Neural Information Processing Systems\n(NeurIPS 2020). 2020. arXiv:2004.08867 [cs.LG].\n[6] J. Nordstro\u0308m and O. Alund, \u201cNeural network enhanced computations on coarse grids,\u201d J.\nComput. Phys. 425 (2021) 109821.\n[7] H. Jin, M. Mattheakis, and P. Protopapas, \u201cUnsupervised neural networks for quantum\neigenvalue problems,\u201d in 2020 NeurIPS Workshop on Machine Learning and the Physical\nSciences. 2020. arXiv:2010.05075 [physics.comp-ph].\n[8] K. D. Kokkotas and B. G. Schmidt, \u201cQuasinormal modes of stars and black holes,\u201d Living\nRev. Relativ. 2 (1999) 2.\n[9] R. A. Konoplya and A. Zhidenko, \u201cQuasinormal modes of black holes: From astrophysics to\nstring theory,\u201d Rev. Mod. Phys. 83 (2011) 793.\n[10] E. Berti, V. Cardoso, and A. O. Starinets, \u201cQuasinormal modes of black holes and black\nbranes,\u201d Class. Quant. Grav. 26 (2009) 163001.\n[11] S. Iyer and C. M. Will, \u201cBlack-hole normal modes: A WKB approach. I. Foundations and\napplication of a higher-order WKB analysis of potential-barrier scattering,\u201d Phys. Rev. D 35\n(Jun, 1987) 3621.\n[12] E. W. Leaver, \u201cAn analytic representation for the quasi-normal modes of Kerr black holes,\u201d\nProceedings of the Royal Society of London. A. Mathematical and Physical Sciences 402\nno. 1823, (1985) 285.\n[13] H. T. Cho, A. S. Cornell, J. Doukas, T. R. Huang, and W. Naylor, \u201cA new approach to\nblack hole quasinormal modes: A review of the asymptotic iteration method,\u201d Adv. Math.\nPhys. 2012 no. 281705, (2012) 42.\n[14] M. Isi, M. Giesler, W. M. Farr, M. A. Scheel, and S. A. Teukolsky, \u201cTesting the no-hair\ntheorem with GW150914,\u201d Phys. Rev. Lett. 123 no. 11, (2019) 111102.\n[15] A. Ghosh, R. Brito, and A. Buonanno, \u201cConstraints on quasinormal mode frequencies with\nLIGO-Virgo binary black-hole observations,\u201d Phys. Rev. D 103 no. 12, (2021) 124041.\n[16] S. M. Carroll and J. Traschen, \u201cSpacetime and geometry: An introduction to general\nrelativity,\u201d Phys. Today 58 no. 1, (Jan., 2005) 52.\n[17] D. McMahon, Quantum field theory demystified. McGraw-Hill Education, New York City,\n2008.\n[18] S. Iyer, \u201cBlack hole normal modes: A WKB approach. II. Schwarzschild black holes,\u201d Phys.\nRev. D 35 (1987) 3632.\n[19] T. Regge and J. A. Wheeler, \u201cStability of a Schwarzschild singularity,\u201d Phys. Rev. 108\n(Nov, 1957) 1063.\n[20] S. Dey and S. Chakrabarti, \u201cA note on electromagnetic and gravitational perturbations of\nthe Bardeen de Sitter black hole: Quasinormal modes and greybody factors,\u201d Eur. Phys. J.\nC 79 no. 6, (2019) 504.\n[21] V. Cardoso and J. P. S. Lemos, \u201cQuasinormal modes of Schwarzschild anti-de Sitter black\nholes: Electromagnetic and gravitational perturbations,\u201d Phys. Rev. D 64 (2001) 084017.\n[22] C. Molina, \u201cQuasinormal modes of D-dimensional spherical black holes with near extreme\ncosmological constant,\u201d Phys. Rev. D 68 (2003) 064007.\n[23] V. Cardoso and J. P. S. Lemos, \u201cQuasinormal modes of the near extremal Schwarzschild-de\nSitter black hole,\u201d Phys. Rev. D 67 (2003) 084020.\n[24] V. Ferrari and B. Mashhoon, \u201cNew approach to the quasinormal modes of a black hole,\u201d\nPhys. Rev. D 30 (1984) 295.\n[25] B. F. Schutz and C. M. Will, \u201cBlack hole normal modes: A semianalytic approach,\u201d\nAstrophys. J. 291 (1985) L33.\n[26] H.-P. Nollert, \u201cQuasinormal modes of Schwarzschild black holes: The determination of\nquasinormal frequencies with very large imaginary parts,\u201d Phys. Rev. D 47 (Jun, 1993) 5253.\n[27] E. W. Leaver, \u201cQuasinormal modes of Reissner-Nordstro\u0308m black holes,\u201d Phys. Rev. D 41\n(May, 1990) 2986.\n[28] R. Konoplya, A. Zhidenko, and A. Zinhailo, \u201cHigher order WKB formula for quasinormal\nmodes and grey-body factors: Recipes for quick and accurate calculations,\u201d Class. and\nQuantum Gravity 36 no. 15, (2019) 155002.\n[29] R. G. Daghigh, M. D. Green, J. C. Morey, and G. Kunstatter, \u201cScalar perturbations of a\nsingle-horizon regular black hole,\u201d Phys. Rev. D 102 no. 10, (2020) 104040.\n[30] L. Lu, X. Meng, Z. Mao, and G. E. Karniadakis, \u201cDeepXDE: A deep learning library for\nsolving differential equations,\u201d SIAM Rev. 63 no. 1, (2021) 208.\n[31] G. F. Montufar, R. Pascanu, K. Cho, and Y. Bengio, \u201cOn the number of linear regions of\ndeep neural networks,\u201d in Advances in neural information processing systems,\nZ. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, eds., vol. 27.\nCurran Associates, Inc., 2014. https://proceedings.neurips.cc/paper/2014/file/\n109d2dd3608f669ca17920c511c2a41e-Paper.pdf.\n[32] O. A\u030alund, G. Iaccarino, and J. Nordstro\u0308m, \u201cLearning to differentiate,\u201d J. Comput. Phys.\n424 (Jan., 2021) 109873.\n[33] K. Hornik, M. Stinchcombe, and H. White, \u201cUniversal approximation of an unknown\nmapping and its derivatives using multilayer feedforward networks,\u201d Neural Networks 3\nno. 5, (1990) 551.\n[34] K. Hornik, \u201cApproximation capabilities of multilayer feedforward networks,\u201d Neural\nNetworks 4 no. 2, (1991) 251.\n[35] A. Pinkus, \u201cApproximation theory of the MLP model in neural networks,\u201d Acta Numerica 8\n(1999) 143.\n[36] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in 3rd\nInternational Conference for Learning Representations, San Diego, 2015. 2017.\narXiv:1412.6980 [cs.LG].\n[37] X. Chen, R. Chen, Q. Wan, R. Xu, and J. Liu, \u201cAn improved data-free surrogate model for\nsolving partial differential equations using deep neural networks,\u201d Scientific Reports 11 (09,\n2021) 2045.\n[38] Karniadakis, G., Kevrekidis, Y., Lu, L., Perdikaris, P., Wang, S., and Yang, L.,\n\u201cPhysics-informed machine learning,\u201d Nature Reviews Physics 3 no. 6, (05, 2021) 1.\n[39] \u201cDependent variable\u201d or \u201ceigenfunction\u201d is used in this paper rather than \u201csolution\u201d, to\navoid misunderstading PINNs whose goal may not always be the find u(x) as a solution; but,\nin some cases, a labelled dataset of the predictors x and associated u(x) may be used to find\nunknown parameters in a PDE for inverse problems (for example).\n[40] H.-T. Cho and C.-L. Ho, \u201c(Quasi)-exactly solvable quasinormal modes,\u201d J. Phys. A 40\n(2007) 1325.\n[41] S. Cuomo, V. S. Di Cola, , G. Rozza, M. Raissi, and F. Piccialli, \u201cScientific machine learning\nthrough physics\u2013informed neural networks: Where we are and what\u2019s next,\u201d J Sci Comp 92\n(2022) 88.\n[42] R. Gnanasambandam, B. Shen, J. Chung, X. Yue, and Z. Kong, \u201cSelf-scalable tanh (stan):\nFaster convergence and better generalization in physics-informed neural networks,\u201d 2022.\n[43] C. Cheng and G.-T. Zhang, \u201cDeep learning method based on physics informed neural\nnetwork with resnet block for solving fluid flow problems,\u201d Water 13 (2021) 4.\n[44] G. Po\u0308schl and E. Teller, \u201cBemerkungen zur quantenmechanik des anharmonischen\noszillators,\u201d Zeitschrift fur Physik 83 no. 3-4, (Mar., 1933) 143.\n[45] M. Mattheakis, D. Sondak, A. S. Dogra, and P. Protopapas, \u201cHamiltonian neural networks\nfor solving equations of motion,\u201d Phys. Rev. E 105 (Jun, 2022) 065305.\n[46] R. A. Konoplya, \u201cQuasinormal behavior of the D-dimensional Schwarzschild black hole and\nthe higher order WKB approach,\u201d Phys. Rev. D 68 (Jul, 2003) 024018.\n[47] P. Grohs, F. Hornung, A. Jentzen, and P. von Wurstemberger, \u201cA proof that artificial neural\nnetworks overcome the curse of dimensionality in the numerical approximation of\nblack-scholes partial differential equations,\u201d 2018.\n[48] Y. Shin, \u201cOn the convergence of physics informed neural networks for linear second-order\nelliptic and parabolic type PDEs,\u201d Commun. Comput. Phys. 28 no. 5, (Jun, 2020)\n2042\u20132074.\n[49] Y. Shin, Z. Zhang, and G. E. Karniadakis, \u201cError estimates of residual minimization using\nneural networks for linear PDEs,\u201d 2020."
        }
    ],
    "title": "Using physics-informed neural networks to compute quasinormal modes",
    "year": 2022
}