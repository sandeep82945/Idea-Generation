{
    "abstractText": "As supercomputers become larger with powerful Graphics Processing Unit (GPU), traditional direct eigensolvers struggle to keep up with the hardware evolution and scale efficiently due to communication and synchronization demands. Conversely, subspace eigensolvers, like the Chebyshev Accelerated Subspace Eigensolver (ChASE), have a simpler structure and can overcome communication and synchronization bottlenecks. ChASE is a modern subspace eigensolver that uses Chebyshev polynomials to accelerate the computation of extremal eigenpairs of dense Hermitian eigenproblems. In this work we show how we have modified ChASE by rethinking its memory layout, introducing a novel parallelization scheme, switching to amore performing communication-avoiding algorithm for one of its inner modules, and substituting the MPI library by the vendor-optimized NCCL library. The resulting library can tackle dense problems with size up to N = O(106), and scales effortlessly up to the full 900 nodes\u2014each one powered by 4\u00d7A100 NVIDIA GPUs\u2014of the JUWELS Booster hosted at the J\u00fclich Supercomputing Centre.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinzhe Wu"
        },
        {
            "affiliations": [],
            "name": "Edoardo Di Napoli"
        }
    ],
    "id": "SP:ee13f03d14ecf5e87f177f02a6519b0b4b66dea4",
    "references": [
        {
            "authors": [
                "G. Ballard",
                "J. Demmel",
                "L. Grigori",
                "M. Jacquelin",
                "N. Knight",
                "H.D. Nguyen"
            ],
            "title": "Reconstructing Householder vectors from tall-skinny QR",
            "venue": "J. Parallel and Distrib. Comput",
            "year": 2015
        },
        {
            "authors": [
                "F.L. Bauer"
            ],
            "title": "Das verfahren der treppeniteration und verwandte verfahren zur l\u00f6sung algebraischer eigenwertprobleme",
            "venue": "Zeitschrift fu\u0308r angewandte Mathematik und Physik ZAMP",
            "year": 1957
        },
        {
            "authors": [
                "J.D. Collins",
                "W.T. Thomson"
            ],
            "title": "The Eigenvalue Problem for Structural Systems with Statistical Properties",
            "venue": "AIAA journal 7,",
            "year": 1969
        },
        {
            "authors": [
                "J. Demmel",
                "L. Grigori",
                "M. Hoemmen",
                "J. Langou"
            ],
            "title": "Communicationavoiding parallel and sequential QR factorizations",
            "venue": "CoRR abs/0806.2159",
            "year": 2008
        },
        {
            "authors": [
                "E. Di Napoli",
                "S. Bl\u00fcgel",
                "P. Bientinesi"
            ],
            "title": "Correlations in sequences of generalized eigenproblems arising in Density Functional Theory",
            "venue": "Computer physics communications 183,",
            "year": 2012
        },
        {
            "authors": [
                "T. Fukaya",
                "R. Kannan",
                "Y. Nakatsukasa",
                "Y. Yamamoto",
                "Y. Yanagisawa"
            ],
            "title": "Shifted Cholesky QR for computing the QR factorization of ill-conditioned matrices",
            "venue": "SIAM Journal on Scientific Computing",
            "year": 2020
        },
        {
            "authors": [
                "T. Fukaya",
                "Y. Nakatsukasa",
                "Y. Yanagisawa",
                "Y. Yamamoto"
            ],
            "title": "CholeskyQR2: a simple and communication-avoiding algorithm for computing a tall-skinny QR factorization on a large-scale parallel system",
            "year": 2014
        },
        {
            "authors": [
                "A. Marek",
                "V. Blum",
                "R. Johanni",
                "V. Havu",
                "B. Lang",
                "T. Auckenthaler",
                "A. Heinecke",
                "H-J Bungartz",
                "H. Lederer"
            ],
            "title": "The ELPA library: Scalable Parallel Eigenvalue Solutions for Electronic Structure Theory and Computational Science",
            "venue": "Journal of Physics: Condensed Matter 26,",
            "year": 2014
        },
        {
            "authors": [
                "O. A Marques",
                "C. V\u00f6mel",
                "J. Demmel",
                "B.N. Parlett"
            ],
            "title": "Algorithm 880: A testing infrastructure for symmetric tridiagonal eigensolvers",
            "venue": "ACM Transactions on Mathematical Software (TOMS)",
            "year": 2008
        },
        {
            "authors": [
                "H. Rutishauser"
            ],
            "title": "Simultaneous iteration method for symmetric matrices",
            "venue": "Numer. Math. 16,",
            "year": 1970
        },
        {
            "authors": [
                "F. Tisseur",
                "J. Dongarra"
            ],
            "title": "A Parallel Divide and Conquer Algorithm for the Symmetric Eigenvalue Problem on Distributed Memory Architectures",
            "venue": "SIAM Journal on Scientific Computing 20,",
            "year": 1999
        },
        {
            "authors": [
                "F. G Van Zee",
                "R. A Van De Geijn"
            ],
            "title": "BLIS: A Framework for Rapidly Instantiating BLAS Functionality",
            "venue": "ACM Transactions on Mathematical Software (TOMS) 41,",
            "year": 2015
        },
        {
            "authors": [
                "J. Winkelmann",
                "P. Springer",
                "E. Di Napoli"
            ],
            "title": "ChASE: Chebyshev Accelerated Subspace iteration Eigensolver for sequences of Hermitian eigenvalue problems",
            "venue": "ACM Transactions on Mathematical Software (TOMS) 45,",
            "year": 2019
        },
        {
            "authors": [
                "X. Wu",
                "D. Davidovi\u0107",
                "S Achilles",
                "E. Di Napoli"
            ],
            "title": "ChASE: a distributed hybrid CPU-GPU eigensolver for large-scale hermitian eigenvalue problems",
            "venue": "In Proceedings of the Platform for Advanced Scientific Computing Conference",
            "year": 2022
        },
        {
            "authors": [
                "V.W. Yu",
                "J. Moussa",
                "P. K\u016fs",
                "A. Marek",
                "P. Messmer",
                "M. Yoon",
                "H. Lederer",
                "V. Blum"
            ],
            "title": "GPU-acceleration of the ELPA2 Distributed Eigensolver for Dense Symmetric and Hermitian Eigenproblems",
            "venue": "Computer Physics Communications",
            "year": 2021
        },
        {
            "authors": [
                "X. Zhang",
                "S. Achilles",
                "J. Winkelmann",
                "R. Haas",
                "A. Schleife",
                "E. Di Napoli"
            ],
            "title": "Solving the Bethe-Salpeter equation on massively parallel architectures",
            "venue": "Computer Physics Communications",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhou",
                "J.R. Chelikowsky",
                "Y. Saad"
            ],
            "title": "Chebyshev-filtered subspace iteration method free of sparse diagonalization for solving the Kohn\u2013Sham equation",
            "venue": "J. Comput. Phys",
            "year": 2014
        },
        {
            "authors": [
                "Y. Zhou",
                "Y. Saad",
                "M.L. Tiago",
                "J.R. Chelikowsky"
            ],
            "title": "Parallel self-consistentfield calculations via Chebyshev-filtered subspace acceleration",
            "venue": "Physical Review E 74,",
            "year": 2006
        }
    ],
    "sections": [
        {
            "text": "cessing Unit (GPU), traditional direct eigensolvers struggle to keep up with the hardware evolution and scale efficiently due to communication and synchronization demands. Conversely, subspace eigensolvers, like the Chebyshev Accelerated Subspace Eigensolver (ChASE), have a simpler structure and can overcome communication and synchronization bottlenecks. ChASE is a modern subspace eigensolver that uses Chebyshev polynomials to accelerate the computation of extremal eigenpairs of dense Hermitian eigenproblems. In this work we show how we have modified ChASE by rethinking its memory layout, introducing a novel parallelization scheme, switching to amore performing communication-avoiding algorithm for one of its inner modules, and substituting the MPI library by the vendor-optimized NCCL library. The resulting library can tackle dense problems with size up to \ud835\udc41 = O(106), and scales effortlessly up to the full 900 nodes\u2014each one powered by 4\u00d7A100 NVIDIA GPUs\u2014of the JUWELS Booster hosted at the J\u00fclich Supercomputing Centre.\nCCS Concepts \u2022 Computing methodologies\u2192 Parallel algorithms; \u2022 Mathematics of computing\u2192Mathematical software performance.\nKeywords Subspace iteration eigensolver, Dense Hermitian matrix, Chebyshev polynomial, Communication-Avoiding, CholeskyQR, condition number estimation, multi-level parallelism."
        },
        {
            "heading": "1 Introduction",
            "text": "Eigenproblems are ubiquitous in many distinct application domains of scientific computing. Algebraic eigenproblems also come in many different flavors, from dense to sparse and from symmetric to complex valued. The sought after solution can also vary widely ranging from the full eigenspectrum to just few eigenvalues in a small interval. Because of this variety, no single algorithm can solve for all the possible eigenproblem flavors. This paper describes the advances made in the ChASE library, an iterative eigensolver targeting the extremal portion of the spectrum of dense Hermitian eigenproblems (\ud835\udf06 \u2208 [\ud835\udf061, \ud835\udf06nev] \u2282 R and \ud835\udf061 < \ud835\udf06nev < \ud835\udf06\ud835\udc41 )\n\ud835\udc34\ud835\udc65 = \ud835\udf06\ud835\udc65 with \ud835\udc34\ud835\udc3b = \ud835\udc34 \u2208 C\ud835\udc41\u00d7\ud835\udc41 .\nIn particular, we showcase how the recent changes in the algorithm and its parallelization scheme enables ChASE to take full advantage of some of the largest heterogeneous supercomputing platforms to solve for dense eigenproblems with size \ud835\udc41 up to O(106).\nSome of the biggest applications where these types of problems need to be solved for the extremal portion of the spectrum is Condensed Matter Physics and Quantum Chemistry. Indeed, ChASE was originally developed to tackle problems emerging from the standard model of Condensed Matter, namely Density Functional Theory (DFT). Contrary to the standard lore which dictates that dense problems should in general be solved by direct methods (e.g., ScaLAPACK [3], ELPA [11, 19]), ChASE takes the opposite path and uses an iterative method leveraging on the frequent use of the workhorse of numerical linear algebra: the GEMM subroutine. The rational for this choice was the ability of an iterative algorithm to be inputted approximate solutions which are available in DFT computations [5].\nIn recent years, ChASE evolved to go beyond its application to eigenproblems in DFT, whose size rarely go beyond few tens of thousands, and it has been adapted to distributed heterogeneous GPU platforms to tackle problems of the size of few hundreds thousands [18, 20]. In the course of this evolution, the library encountered a number of shortcomings which precluded its further use to more challenging cases. Due to its initial communication-avoiding design, some of the kernels (e.g., QR-factorization) which were executed redundantly on each MPI rank became the new bottleneck. In addition, the quadratic increase in the memory footprint of the internal buffers hampered the scaling beyond a couple of hundreds of nodes.\nIn this work we report on a number of improvements we made on ChASE. They include: i) a redesign of the buffer structure so as to move from computations executed redundantly on each process to an MPI parallelization over one of the dimensions of the 2-dimensional MPI grid; ii) a switching from a Householder QR factorization to a communication-avoiding (CA) CholeskyQR algorithm; iii) a mechanism to avoid the instabilities introduced by the CholeskyQR based on accurate estimates of the condition number of the filtered vectors; iv) a substitution of the MPI library by NCCL (NVIDIA Collective Communications Library) [10] for collective communications. In fact, the novel parallelization scheme and the employment of NCCL altogether avoid most data movement between host and device memory, which makes ChASE a library capable to easily execute at scale on the third largest accelerated cluster in Europe equipped with four NVIDIA A100 GPU cards per node.\nOrganization. In Section 2, we give a short overview of the ChASE algorithm and the parallel implementation of the version preceding this work, followed by a description of its limitations. In Section 3, we present a newly designed parallel implementation to overcome these limitations. The numerical and parallel performance and a comparison with the currently available eigensolvers on distributed\nar X\niv :2\n30 9.\n15 59\n5v 1\n[ cs\n.D C\n] 2\n7 Se\np 20\n23\nmulti-GPUs architectures are illustrated in Section 4. Section 5 summarizes the achievements and concludes the paper."
        },
        {
            "heading": "2 Background and Related Work",
            "text": "ChASE (https://github.com/ChASE-library/ChASE) is a numerical library written in C++, templated for complex/real type and double/single precision and based on the subspace iteration algorithm. The subspace iteration algorithm is one of the earliest iterative methods for solving Symmetric/Hermitian eigenproblems [2]. This type of algorithm with a Chebyshev polynomial filter has typically been used to solve electronic structure eigenproblems [13, 22, 23]. Recently, ChASE library has evolved from one of these efforts to become a full-fledged numerical eigensolver that can also be used outside the electronic structure domain."
        },
        {
            "heading": "2.1 ChASE algorithm",
            "text": "ChASE\u2019s algorithm is inspired by the work of Rutishauser [13] and Zhou et al. [23], and features several additional components: It has an internal loop iterating over the Chebyshev polynomial filter and the Rayleigh-Ritz projection to the subspace quotient; It implements a Density of States (DoS) method to determine spectral bounds of the search subspace; It contains a deflation and locking mechanism within the internal loop. One of the most important features of ChASE is the optimization of the degree of the polynomial filter so as to minimize the number of matrix-vector operations (MatVecs) required to achieve convergence of the desired eigenpairs.\nAlgorithm 1 ChASE algorithm Require: Hermitian matrix\ud835\udc3b , number of desired eigenpairs nev, threshold tolerance\nfor residuals tol, initial polynomial degree deg, search space increment nex, vector ?\u0302? \u2261 [\ud835\udc631, \u00b7 \u00b7 \u00b7 , \ud835\udc63nev+nex ].\nEnsure: nev extreme eigenpairs (\u039b, \ud835\udc4c ) 1: degrees[1 : nev + nex] \u2190 deg 2: (\ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d , \ud835\udf071, \ud835\udf07nev+nex ) \u2190 Lanczos(\ud835\udc3b ) 3: while size(\ud835\udc4c <nev) do 4: ?\u0302? \u2190 Filter(\ud835\udc3b,\ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d , \ud835\udf071, \ud835\udf07nev+nex, ?\u0302? , degrees) 5: ?\u0302? \u2190 QR([\ud835\udc4c ?\u0302? ]) 6: (?\u0302? , \u039b\u0303) \u2190 Rayleigh-Ritz(\ud835\udc3b, ?\u0302? ) 7: Compute the Residual \ud835\udc45\ud835\udc52\ud835\udc60 (?\u0302? , \u039b\u0303) 8: (?\u0302? ,\u039b, \ud835\udc4c ) \u2190 Deflation & Locking(?\u0302? ,\u039b\u0303, \ud835\udc45\ud835\udc52\ud835\udc60 (?\u0302? , \u039b\u0303),\ud835\udc4c ) 9: \ud835\udf071 \u2190 min( [\u039b \u039b\u0302] ) , \ud835\udf07nev+nex \u2190 max( [\u039b \u039b\u0302] ) 10: \ud835\udc50 \u2190 \ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d+\ud835\udf07nev+nex2 , \ud835\udc52 \u2190 \ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d \u2212\ud835\udf07nev+nex 2 11: degrees[:] \u2190 degreeOpt(tol, \ud835\udc45\ud835\udc52\ud835\udc60 [:],\u039b[:], \ud835\udc50, \ud835\udc52 ) 12: Sort \ud835\udc45\ud835\udc52\ud835\udc60 (?\u0302? , \u039b\u0303) , ?\u0302? , \u039b\u0303 according to degrees\nAlgorithm 1 gives a high level description of ChASE main parts. To gain a more comprehensive understanding, we refer the reader to [17]. The ChASE library first estimates the necessary spectral bounds by executing a small number of repeated Lanczos steps (Line 2). It then filters a number of (only initially random) vectors using an optimized degree for each vector (Line 4), and orthonormalizes the filtered vectors using QR factorization (Line 5). The Q factor is used to reduce the original large eigenproblem to the size of the subspace through a Rayleigh-Ritz projection (Line 6). The resulting \"small\" eigenproblem is solved using a standard dense solver such as Divide&Conquer [14]. Residuals are then computed, and eigenpairs below the tolerance threshold are deflated and locked (Line 8). Finally, a new set of filtering degrees is computed for the nonconverged vectors, and the procedure is repeated (Line 11)."
        },
        {
            "heading": "2.2 Original Distributed Implementation",
            "text": "The implementation of ChASE relies on a number of numerical kernels which can be separated in dense linear algebra operations to exploit optimized BLAS/LAPACK libraries (e.g., MKL [16], OpenBLAS [21], BLIS [15]), and cuBLAS/cuSOLVER for GPU builds. In ChASE, MPI processes are organized as a 2D grid whose shape is as square as possible. The Hermitian matrix \ud835\udc3b is distributed either following a block distribution or a block-cyclic distribution.\nChASE most significant kernel is the Hermitian Matrix-Matrix Multiplications (HEMMs) which, in previous versions of the library [17, 18], has been implemented for hybrid distributed-memory architectures. When NVIDIA GPUs are available, the local computation of HEMMs on each MPI process are offloaded to the corresponding GPU(s). The HEMM is implemented with a custom MPI scheme, and is used in the Filter, Rayleigh-Ritz, and Residual parts of the ChASE Algorithm. For instance, in the Chebyshev Filter, which is the most computationally intensive kernel of ChASE, the matrixmatrix multiplications appears as a three-terms recurrence relation\n\ud835\udc36\ud835\udc56+1 = \ud835\udefc\ud835\udc56 (\ud835\udc3b \u2212 \ud835\udefe\ud835\udc56 \ud835\udc3c\ud835\udc5b)\ud835\udc36\ud835\udc56 + \ud835\udefd\ud835\udc56\ud835\udc36\ud835\udc56\u22121, \ud835\udc56 \u2208 [1, deg], (1)\nwhere \ud835\udc36 is (a subset of) the rectangular matrix \ud835\udc49 , deg is the degree of the Chebyshev polynomial, and \ud835\udefc\ud835\udc56 , \ud835\udefd\ud835\udc56 , \ud835\udefe\ud835\udc56 are scalar parameters related to each iteration.\nA customized MPI scheme was proposed in [17], in which the rectangular matrix \ud835\udc36 is distributed over the rows of each column communicator, which also keeps a copy of\ud835\udc36 . Alternatively, when\ud835\udc3b is assigned to MPI processes in block-cyclic fashion,\ud835\udc36 is distributed on each column communicator using the same block size for the distribution of the rows of \ud835\udc3b . After a series of HEMMs are performed in the Chebyshev Filter, \ud835\udc36 should be re-distributed from the iteration \ud835\udc56 to \ud835\udc56 + 1. In ChASE, this re-distribution is avoided because ?\u0302? is symmetric/Hermitian and the \ud835\udc3b\ud835\udc36 HEMM can be replaced by an HEMM on \ud835\udc3b\ud835\udc3b\ud835\udc36 when the iteration number \ud835\udc56 in Equation (1) is even. This customized MPI scheme provides for an extreme good parallel performance for HEMMs in the Filter.\nThe other operations, such as QR factorization and the eigendecomposition within the Rayleigh-Ritz projection, were implemented with vendor-optimized BLAS/LAPACK by collecting redundantly a distributed matrix of vectors on each MPI process. The QR factorization is offloaded to GPU devices through calls to the corresponding cuSOLVER functions."
        },
        {
            "heading": "2.3 Limitations",
            "text": "A number of concerns were reported in [18], when ChASE v1.2 was ported to distributed multi-GPUs architectures. First, the code has large memory footprint limiting its use for very large problems. The large memory usage originates from the redundant execution on eachMPI process of some of the numerical kernels, includingQR, the Rayleigh-Ritz projection and the computation of Residuals. The redundant computation requires the storing of two buffers of size O(\ud835\udc41 (nev + nex)) on the physical memory assigned to each MPI process. Because the memory device is limited compared to the mainmemory, this limitation is particularly severe in the distributed multi-GPUs build of ChASE. Similarly, the redundant execution of a portion of the computation also results in a non-scalable part (mainly linked to the QR factorization) of the distributed ChASE.\nThis limitation has a direct influence on the maximum size of matrix \ud835\udc3b that could be tackled and the number of eigenpairs that could be computed.\nA second concern regards the communication overhead for the kernels executed redundantly on each MPI process. These kernels need the collection of a distributed matrix of vectors within the row (or column) communicator into a redundant buffer on each task. The collection is obtained by the individual broadcasting of a buffer for each task within the row (or column) communicator. When the count of MPI tasks quadruples, the number of messages doubles. As shown in Fig. 2, this limitation particularly harms the weak scaling performance of ChASE, when the word size per message is fixed while the the number of messages increases.\nWhen using accelerators, version v1.2 follows a host-devicemode which offloads the most computationally intensive operations to the GPU devices. Once the computation is completed, the related data are immediately copied back to CPUs introducing a substantial data movement overhead between host and device memory. The use of traditional MPI libraries for the collective communications requires further data movement from GPU to CPU once communication operations are invoked.\nIn summary, ChASE v1.2 suffers from the layout of its parallelization scheme, in which some numerical kernels are redundantly executed on each MPI process. These limitations were insignificant when ChASE was initially designed because it was mainly targeting problems from electronic structure calculations, whose size typically ranges from a few thousands to several tens of thousands. However, when the ambition of the ChASE library grew to encompass much larger problems, its parallel structure became the source of several bottlenecks. Porting ChASE to GPUs, which are extremely powerful for the computation of standard linear algebra operations, amplified further these shortcomings."
        },
        {
            "heading": "3 Optimization and Implementation",
            "text": "In this paper, we present a novel scheme for ChASE that parallelizes QR, Rayleigh-Ritz and Residual on a subset of the MPI grid\u2014either row or column communicators\u2014and in doing so removes the need of large and redundant buffers on each MPI process. Importantly, it enables a distributed multi-GPU implementation to avoid almost all host-device data movement by keeping the computations of all major numerical kernels on the GPUs and communicating data via NCCL. Moreover, we introduce new faster QR algorithms and a mechanism to select the most appropriate based on the condition number of the filtered vectors."
        },
        {
            "heading": "3.1 Novel Parallelization Scheme",
            "text": "In this section, we present the novel parallelization scheme in a general way which is applicable for both CPU and GPU builds. As shown in Algorithm 2, a Hermitian matrix \ud835\udc3b is distributed onto a 2D MPI grid following either a block distribution or a block-cyclic distribution, and the local block on each MPI process is of size \ud835\udc5b\ud835\udc5f \u00d7 \ud835\udc5b\ud835\udc50 . Arrays \ud835\udc36,\ud835\udc362 and \ud835\udc35, \ud835\udc352 of size, respectively \ud835\udc5b\ud835\udc5f \u00d7 \ud835\udc5b\ud835\udc52 and \ud835\udc5b\ud835\udc50 \u00d7 \ud835\udc5b\ud835\udc52 , are allocated on each MPI process, with \ud835\udc5b\ud835\udc52 = nev + nex. The set of all arrays\ud835\udc36 and\ud835\udc362 represent a rectangular matrix of size \ud835\udc41 \u00d7\ud835\udc5b\ud835\udc52 within each column communicator, while the set of all \ud835\udc35 and \ud835\udc352 represent a matrix of same size within each row communicator. The entries of the global matrices represented by \ud835\udc36 and \ud835\udc362 are\nidentical among different column communicators, and \ud835\udc35 and \ud835\udc352 are identical among row communicators. \ud835\udc36 is designed as the buffer which receives the initial input vectors and returns the computed eigenvectors after completing the execution.\nThe implementation of Filter remains unchanged. If the iteration index in Equation (1) is odd, the algorithm performs \ud835\udc3b\ud835\udc36 and stores the result in \ud835\udc35. If the index is even, the algorithm executes \ud835\udc3b\ud835\udc3b\ud835\udc35 and writes the result to \ud835\udc36 . As ChASE enforces even-degree Chebyshev polynomials, the filtered vectors are always stored in \ud835\udc36 . At the next step, a 1D parallelized QR factorization is performed on \ud835\udc36 within each column communicator. We introduce a mechanism that can switch between different communication-avoiding QR (CAQR) implementations to strike a balance between optimal performance and good numerical stability. Details of the CAQR method are provided in Section 3.2. After the QR factorization, the first locked (converged) columns in \ud835\udc36 are replaced with the corresponding columns stored in \ud835\udc362 at the previous iteration, while the remaining columns of \ud835\udc362 are replaced with the corresponding newly orthogonalized columns of \ud835\udc36 . Algorithm 2 ChASE with Distributed QR factorization Require: nev: number of eigenpairs to be computed; nex: search space increment; \ud835\udc5b\ud835\udc52\n= nev + nex; rcomm & ccomm: row and column communicator of 2D MPI grid; \ud835\udc3b : \ud835\udc5b\ud835\udc5f \u00d7\ud835\udc5b\ud835\udc50 dense matrix;\ud835\udc36 &\ud835\udc362 : \ud835\udc5b\ud835\udc5f \u00d7\ud835\udc5b\ud835\udc52 dense matrix; \ud835\udc35 & \ud835\udc352 : \ud835\udc5b\ud835\udc50\u00d7\ud835\udc5b\ud835\udc52 dense matrix; \ud835\udc34: \ud835\udc5b\ud835\udc52\u00d7\ud835\udc5b\ud835\udc52 dense matrix; MaxIter: maximal iteration number; deg: degree of filter; tol: threshold tolerance for residuals; opt: indicating if degree optimization enabled Ensure: first nev lowest eigenpairs of matrix represented by H within 2D MPI grid. 1: (\ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d , \ud835\udf071, \ud835\udf07\ud835\udc5b\ud835\udc52 ) \u2190 Lanczos(\ud835\udc3b ) 2: iter = 1, locked = 0, degs[:]=deg, \u039b[:] = 0 3: \ud835\udc50 \u2190 \ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d+\ud835\udf07\ud835\udc5b\ud835\udc522 , \ud835\udc52 \u2190 \ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d \u2212\ud835\udf07\ud835\udc5b\ud835\udc52 2\n4: while locked < nev && iter \u2264 MaxIter do 5: if iter \u2260 1 then 6: \ud835\udf071, \ud835\udf07\ud835\udc5b\ud835\udc52 \u2190updateBounds(\u039b) 7: \ud835\udc50 \u2190 \ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d+\ud835\udf07\ud835\udc5b\ud835\udc522 , \ud835\udc52 \u2190 \ud835\udc4f\ud835\udc60\ud835\udc62\ud835\udc5d \u2212\ud835\udf07\ud835\udc5b\ud835\udc52 2\n8: if opt then 9: degs[:] \u2190 degreeOpt(tol,Res[:],\u039b[:], \ud835\udc50, \ud835\udc52 ) 10: \ud835\udc36 [:, locked + 1 :] \u2190Filter(\ud835\udc3b ,\ud835\udc36 [:, locked + 1 :], degs, \ud835\udc50) 11: cond\u2190CondEst(\u039b, \ud835\udc50 , \ud835\udc52 , degs, locked) 12: \ud835\udc36 \u21901D-CAQR(\ud835\udc36 , cond, ccomm) 13: \ud835\udc36 [:, 1 : locked] \u2190 \ud835\udc362 [:, 1 : locked],\ud835\udc362 [:, locked+1 :] \u2190 \ud835\udc36 [:, locked+1 :] 14: \ud835\udc352\u2190 Bcast(\ud835\udc362, ccomm) 15: \ud835\udc35 [:, locked + 1 :] \u2190 \ud835\udc3b\ud835\udc36 [:, locked + 1 :] 16: \ud835\udc34\u2190 (\ud835\udc352 [:, locked + 1 :] ) \u2032\ud835\udc35 [:, locked + 1 :] 17: \ud835\udc34\u2190AllReduce(\ud835\udc34, SUM, rcomm) 18: \u039b, \ud835\udc34\u2190 HE(SY)EVD(\ud835\udc34) 19: \ud835\udc36 [:, locked + 1 :] \u2190 \ud835\udc362 [:, locked + 1 :]\ud835\udc34 20: \ud835\udc362 [:, locked + 1 :] \u2190 \ud835\udc36 [:, locked + 1 :] 21: \ud835\udc352\u2190 Bcast(\ud835\udc362, ccomm) 22: \ud835\udc35 [:, locked + 1 :] \u2190 \ud835\udc3b\ud835\udc36 [:, locked + 1 :] 23: \ud835\udc35 [:, locked+1 :] \u2190 \ud835\udc35 [:, locked+1 :] \u2212 ritzv[locked+1 :]\ud835\udc352 [:, locked+1 :] 24: nrm[locked+1:]\u2190SqaredNorm(\ud835\udc35 [:, locked + 1 :]) 25: nrm\u2190AllReduce(nrm, SUM, rcomm) 26: resd[locked + 1 :] \u2190 \u221a\ufe01 nrm[locked + 1 :] 27: \ud835\udc36 ,\ud835\udc362 , new_converged\u2190Locking(\ud835\udc36 ,\ud835\udc362 , resd, tol) 28: locked=locked+new_converged, iter=iter + 1 29: return : \u039b[1 : nev],\ud835\udc36 [:, 1 : nev]\nThe Rayleigh-Ritz step projects the original problem onto a search subspace, from which approximate solutions are computed. The active subspace is obtained by forming a \ud835\udc5b\ud835\udc52 \u00d7 \ud835\udc5b\ud835\udc52 RayleighRitz quotient \ud835\udc34 = \ud835\udc36\ud835\udc3b\ud835\udc3b\ud835\udc36 , with \ud835\udc36 the \ud835\udc41 \u00d7 \ud835\udc5b\ud835\udc52 orthonormal matrix outputted by the QR factorization, which is distributed within each column communicator. The right-multiplication of \ud835\udc36 with \ud835\udc3b is implemented by employing the same distributed HEMM used in the Filter. The result of \ud835\udc3b\ud835\udc36 is stored in \ud835\udc35, which is distributed over\nthe row communicator. The left-multiplication of \ud835\udc36\ud835\udc3b with \ud835\udc3b\ud835\udc36 , realized as \ud835\udc36\ud835\udc3b\ud835\udc35, requires first to copy \ud835\udc36 buffers from the column communicator into the \ud835\udc352 buffers distributed within the row communicator. If the 2D MPI grid is square, then a single broadcasting operation is sufficient. However, if the MPI grid is non-square, multiple broadcasting operations may be required depending on the shape of the MPI grid and the way in which \ud835\udc3b is distributed, especially for block-cyclic distributions. Squared MPI grids are the optimal configuration for ChASE, as with other state-of-the-art eigensolvers such as ELPA.\n\ud835\udc36\ud835\udc3b\ud835\udc35 is deployed as \ud835\udc35\ud835\udc3b2 \ud835\udc35 in Algorithm 2 Line 16, which is naturally parallel within the row communicator. Each \ud835\udc35\ud835\udc3b2 \ud835\udc35 is a just a call to GEMM on the local blocks \ud835\udc35 and \ud835\udc352 stored on each MPI process, and the result is written in the redundant buffer\ud835\udc34, through an AllReduce operation with addition operation along the row communicator. \ud835\udc34 is then diagonalized redundantly on each MPI process as (\ud835\udc4c,\u039b) \u2190 \ud835\udc34 using a LAPACK eigensolver, where \u039b are the approximate eigenvalues of \ud835\udc34, and \ud835\udc34 is overwritten by the related eigenvectors \ud835\udc4c . The eigenvectors of the original problem, which are designed to be stored in \ud835\udc36 , can be obtained through the back-transform\ud835\udc362\ud835\udc34 in Algorithm 2 Line 19, which is naturally parallel within the column communicator, since \ud835\udc362, which is identical to \ud835\udc36 in this step, is distributed, and \ud835\udc34 is redundant on all processes.\nThe Residual step computes the Euclidean norm of each column of \ud835\udc36\u039b \u2212\ud835\udc3b\ud835\udc36 , which is equivalent to computing the Euclidean norm of each column of \ud835\udc352\u039b \u2212 \ud835\udc35, as \ud835\udc352 is re-distributed from \ud835\udc362. The complexity of the required memory on each MPI process is\n\ud835\udc40\ud835\udc50\ud835\udc5d\ud835\udc62 = \ud835\udc41 2\n\ud835\udc5d\ud835\udc5e + 2\ud835\udc41\ud835\udc5b\ud835\udc52 \ud835\udc5d + 2\ud835\udc41\ud835\udc5b\ud835\udc52 \ud835\udc5e + \ud835\udc5b2\ud835\udc52 , (2)\nwhere \ud835\udc5d \u00d7 \ud835\udc5e is the dimension of the 2D MPI grid. The first term corresponds to the local block of \ud835\udc3b held by each MPI process, the second term corresponds to the\ud835\udc36 and\ud835\udc362 buffers, and the third term corresponds to the \ud835\udc35 and \ud835\udc352 buffers. The fourth and smallest term comes from the \ud835\udc34 blocks."
        },
        {
            "heading": "3.2 Communication-avoiding QR factorization",
            "text": "In order to gain better performance, we replace the Householder QR (HHQR) by variants of communication-avoiding (CA) CholeskyQR [8, 9]. For each iteration in ChASE, a QR factorization is performed on a rectangular matrix, here referred to as \ud835\udc4b , whose size is\ud835\udc5a \u00d7 \ud835\udc5b with\ud835\udc5a > \ud835\udc5b. We preferred CholeskyQR rather than tall-skinny QR (TSQR) [4], a CAQR with equivalent communication costs as CholeskyQR, because the reduction operator of CholeskyQR is addition, while that of TSQR is the QR factorization of a small matrix [9]. To our knowledge, TSQR is advantageous over ScaLAPACK-HHQR in 1D MPI grid only if\ud835\udc5a \u226b \ud835\udc5b [1]. This is not the case for ChASE, which, in typical Condensed Matter problems, is expected to solve for around 10% of the extremal eigenpairs. Algorithm 3 1D-CholeskyQR for \ud835\udc4b = \ud835\udc44\ud835\udc45 1: function CholeskyQR(\ud835\udc4b , comm, cholDegree) 2: for \ud835\udc56 = 1, \u00b7 \u00b7 \u00b7 ,cholDegree do 3: \ud835\udc45 \u2190 SYRK(\ud835\udc4b ) 4: \ud835\udc45 \u2190AllReduce(\ud835\udc45, SUM, comm) 5: [\ud835\udc45, info]\u2190 POTRF(\ud835\udc45) 6: \ud835\udc4b \u2190 TRSM(\ud835\udc4b , \ud835\udc45) 7: return \ud835\udc4b\nAlgorithm 4 Distributed 1D-CAQR for ChASE 1: function 1D-CAQR(\ud835\udc4b , estCond, comm) 2: if estCond> 108 then 3: \ud835\udc45 \u2190 SYRK(\ud835\udc4b ) 4: \ud835\udc45 \u2190AllReduce(\ud835\udc45, SUM, comm) 5: norm\u2190AllReduce( | |\ud835\udc4b | |2\n\ud835\udc39 , SUM, comm)\n6: \ud835\udc60 = 11(\ud835\udc5a\ud835\udc5b + \ud835\udc5b (\ud835\udc5b + 1) )unorm 7: [\ud835\udc45, info]\u2190 POTRF(\ud835\udc45 + \ud835\udc60\ud835\udc3c ) 8: if info \u2260 0 then 9: \ud835\udc4b \u2190ScaLAPACK-HHQR(\ud835\udc4b ,comm) 10: else 11: \ud835\udc4b \u2190 TRSM(\ud835\udc4b ,\ud835\udc45) 12: \ud835\udc4b \u2190 CholeskyQR(\ud835\udc4b , comm,2) 13: else if estCond< 20 then 14: \ud835\udc4b \u2190 CholeskyQR(\ud835\udc4b , comm, 1) 15: else 16: \ud835\udc4b \u2190 CholeskyQR(\ud835\udc4b , comm, 2) 17: return \ud835\udc4b\nDespite the better performance, CholeskyQR is afflicted by a rapid decline in orthogonality across the columns of the \ud835\udc44 factor for increasing condition number of \ud835\udc4b . This instability can be significantly mitigated by performing the algorithm twice, which is known as CholeskyQR2 [9]. The applicability of CholeskyQR2 is still limited by the requirement that the Cholesky factorization of the Gram matrix \ud835\udc45 = \ud835\udc4b\ud835\udc3b\ud835\udc4b runs to completion, which requires the matrix \ud835\udc4b to have a condition number \ud835\udf052 (\ud835\udc4b ) no larger than O(u\u22121/2), where u is the unit round-off and \ud835\udf052 (\ud835\udc4b ) = \ud835\udf0e\ud835\udc5a\ud835\udc4e\ud835\udc65 (\ud835\udc4b )\ud835\udf0e\ud835\udc5a\ud835\udc56\ud835\udc5b (\ud835\udc4b ) , with \ud835\udf0e\ud835\udc5a\ud835\udc4e\ud835\udc65 (\ud835\udc4b ) and \ud835\udf0e\ud835\udc5a\ud835\udc56\ud835\udc5b (\ud835\udc4b ) respectively its maximal and minimal singular values. This limitation has been addressed in [8], where a preconditioning step is added to CholeskyQR2 to reduce the condition number of \ud835\udc4b to a point where CholeskyQR2 is applicable. This improved version is called shifted CholeskyQR2 or \ud835\udc60-CholeskyQR2, and it can handle matrices with condition numbers up to O(u\u22121).\nSince computing the exact condition number of a rectangular matrix is computationally expensive, we introduce an accurate and cost-free mechanism to estimate \ud835\udf052 (\ud835\udc4b ) which ultimately guides us in choosing the best QR-factorization variant for the array of filtered vectors. This estimation is the result of a numerical analysis of the spectral properties of \ud835\udc4b and it will appear in an upcoming manuscript. The estimate is implemented in Algorithm 5, using input arguments that are already available in ChASE. In Section 4.2, we illustrate the effectiveness of our estimation methodology, while in Section 4.3, we compare CholeskyQR, with the proposed heuristic, against HHQR and show that both achieve a similar convergence behavior while CholeskyQR returns a much better performance.\nA distributed-memory implementation of CholeskyQR is given in Algorithm 3, where \ud835\udc4b represents the matrix to be factorized, and comm is the MPI communicator. The variable cholDegree specifies the number of repetitions of CholeskyQR, so, if it equals to 2, it is indeed CholeskyQR2. For each repetition, it starts by partially calculating the Gram matrix \ud835\udc4b\ud835\udc3b\ud835\udc4b on each MPI process, through LAPACK ZHERK (DSYRK) routines. The final Gram matrix is obtained via an AllReduce operation with addition. Then a Cholesky factorization is performed on \ud835\udc45 with LAPACK POTRF, and \ud835\udc44 is computed via back substitution with the LAPACK TRSM.\nBased on the estimated condition number of \ud835\udc4b , a heuristic for selecting the appropriate CholeskyQR variant is proposed as Algorithm 4. If the estimated condition number is larger than O(u\u22121/2), which is approximately 108 for double precision, we select the \ud835\udc60- CholeskyQR2 variant. If the estimated condition number is smaller\nAlgorithm 5 Condition number estimation of filtered vectors 1: function CondEst(\u039b, \ud835\udc50 , \ud835\udc52 , degs, locked) 2: \ud835\udc61 \u2032 = \u039b[1]\u2212\ud835\udc50\n\ud835\udc52 , \ud835\udc61 = \u039b[locked+1]\u2212\ud835\udc50 \ud835\udc52\n3: |\ud835\udf0c | = max{ |\ud835\udc61 \u2212 \u221a \ud835\udc612 \u2212 1 | , |\ud835\udc61 + \u221a \ud835\udc612 \u2212 1 |} 4: |\ud835\udf0c \u2032 | = max{ |\ud835\udc61 \u2032 \u2212 \u221a \ud835\udc61 \u20322 \u2212 1 | , |\ud835\udc61 \u2032 + \u221a \ud835\udc61 \u20322 \u2212 1 |} 5: d = degs[locked + 1], d\ud835\udc40 = max(degs[locked + 1 :] ) 6: cond = |\ud835\udf0c |d |\ud835\udf0c \u2032 | (d\ud835\udc40 \u2212d) 7: return cond\nthan a fixed threshold (in practice set to 20), CholeskyQR is sufficient. Otherwise, CholeskyQR2 should be used. To ensure robustness, we revert to HHQR in Algorithm 4 (Line 9) to prevent failures of \ud835\udc60-CholeskyQR2 for any corner case."
        },
        {
            "heading": "3.3 Porting to GPUs",
            "text": "A straightforwardway to port ChASE to distributed GPU clusters is to offload its most computation-intensive operations onto GPUs. Specifically, the local computation of matrix-matrix multiplication in Line 10, 15, 22, 19, 16, the SYRK, POTRF and TRSM of Line 12, the HE(SY)EVD in Line 18 of Algorithm 2 have been ported to GPUs by using the corresponding routines provided by cuBLAS and cuSOLVER. The BLAS-1 operations of Line 23, 24 and 26 for Residuals stay on CPUs. The computed results of the operations on GPUs are copied back to CPUs once the computations are done.\nIn fact, the memory copying operations for the collective operations can be bypassed by exploring the GPUDirect technology. Specifically, we can use the optimized GPU-driven NCCL library to replace the MPI library for all the collective communications (allreduce and broadcast) in ChASE. NCCL provides communication primitives for the collective communications. Starting from NCCL2, it has support for InfiniBand based communication and can span multiple nodes. Since the NCCL APIs are not MPI-compliant, a 2D NCCL communicator has been built on top of the 2D MPI grid in ChASE so that each MPI process is mapped to a single GPU device within this 2D NCCL communicator. If the buffers \ud835\udc36 , \ud835\udc362, \ud835\udc35, \ud835\udc352 and \ud835\udc34 reside on the device, ccomm and rcomm are the corresponding column/row communicator within the 2D NCCL communicator, and all the operations of AllReduce and Bcast are substituted by their equivalents in NCCL. This means that all the computations are executed on the GPUs by using the corresponding routines provided by cuBLAS and cuSOLVER, and all the host-device data movement for all major kernels have been eliminated. Furthermore, the Line 24 in Algorithm 2 for Residuals has been also offloaded to GPUs as a single batched kernel for a series of BLAS-1 operations.\nIn this paper, we refer respectively to the implementation of ChASE with and without NCCL support as ChASE(NCCL) and ChASE(STD), where STD, referring to standard, implies the standard way of distributed GPU communication with explicit hostdevice data movement. We prefer NCCL over other CUDA-Aware MPI libraries, such as OpenMPI and MVAPICH2, as the latter are rather designed with many optimized GPU-based point-to-point communication schemes while the former is targeting collective communications."
        },
        {
            "heading": "4 Numerical Experiments",
            "text": "ChASE has been tested on the supercomputer JUWELS-Booster at J\u00fclich Supercomputing Centre in Germany, which consists of 936 NVIDIA GPU-accelerated compute nodes. The configuration of each node is two 24 cores AMD EPYC 7402 CPUs @ 2.25 GHz\n(16 \u00d7 32 GB DDR4 Memory), 4\u00d7NVIDIA A100 GPU with 40 GB memory. The interconnect are 4\u00d7 InfiniBand HDR (Connect-X6).\nThe C/C++ compiler used is GCC 11.3.0, MPI library is OpenMPI 4.1.4, and BLAS/LAPACK libraries are Intel MKL 2022.1.0, CUDA version is 11.7. For ChASE(STD) and ChASE(NCCL), the number of MPI ranks per node is 4, with 1 GPUs and 12 OpenMP threads per rank. ChASE v1.2 is marked as ChASE(LMS), in which LMS refers to Limited Memory and Scaling. It is configured with 1MPI rank per node, with 4 GPU and 12 OpenMP threads per rank. The threshold tolerance for residuals tol is fixed as 10\u221210, and the degree optimization of the ChASE filter is always enabled unless otherwise specified. All tests in this paper are performed in doubleprecision."
        },
        {
            "heading": "4.1 Test Matrix Suite",
            "text": "For the tests we use an heterogeneous collection of eigenproblems either coming from domain applications or artificially generated. 4.1.1 DFT and BSE matrices Eigenproblems from applications are extracted from DFT and Bether-Salpeter simulations. Details of these problems are listed in Table 1, including an acronym, the size, the number of eigenpairs sought after nev, the size of extra searching space nex, the application software used to extract them, and the type of each problem. The FLEUR problems are generated by FLEUR [7] code, The BSE UIUC problems are obtained through a fork of the Jena BSE code developed and maintained at the University of Illinois Urbana-Champaign [20].\n4.1.2 Artificial Matrices For benchmarking the parallel performance of ChASE, artificial matrices are generated with a given spectrum, which is inspired by the testing infrastructure in LAPACK [12]. To generate them, we construct a diagonal matrix \ud835\udc37 filled with the prescribed eigenvalues. Then a dense matrix \ud835\udc34 with the given spectra is generated as\ud835\udc34 = \ud835\udc44\ud835\udc47\ud835\udc37\ud835\udc44 , with\ud835\udc44 the first factor of the QR factorization of a random square matrix. In this paper, the eigenvalues of the artificial matrices are distributed uniformly within an interval and will be referred to as Uniform matrices."
        },
        {
            "heading": "4.2 Estimating the condition number",
            "text": "In this section we illustrate the effectiveness of the upper bounds introduced in Section 3.2 for the condition number of the rectangular matrix of vectors\ud835\udc36 outputted by the Chebyshev filter1. The test problems are the ones listed in Table 1. The results are shown in Fig. 1 which compares the estimated condition number \ud835\udf05\ud835\udc52\ud835\udc60\ud835\udc612 against an accurate computation of the condition number \ud835\udf05\ud835\udc50\ud835\udc5c\ud835\udc5a2 of the filtered matrices for each iterations of ChASE up to completion. We carried on the tests with the polynomial degree optimization turned\n1We indicate with\ud835\udc36 both the array stored in each MPI process and the union of such arrays representing the full matrix of vectors. The difference in usage can be easily evinced from the context.\neither on (opt) or off (no-opt) to show how the condition number estimation intrinsically depends on the optimization mechanism. For the case no-opt, the Chebyshev polynomial degree is fixed to 20 at every iteration. For the case opt, the initial degree for the first iteration is set as 20 and changes at every later iteration for each of the filtered vectors in \ud835\udc36 . A maximal allowed degree is fixed to 36 to avoid the matrix of vectors becoming too ill-conditioned. The \u21132-norm condition number \ud835\udf05\ud835\udc50\ud835\udc5c\ud835\udc5a2 (\ud835\udc36) are computed by LAPACK SVD solvers after collecting the distributed blocks along the column communicator into a redundant matrix.\nFig. 1 shows that \ud835\udf05\ud835\udc52\ud835\udc60\ud835\udc612 , for both no-opt and opt, always bounds from above\ud835\udf05\ud835\udc50\ud835\udc5c\ud835\udc5a2 , making it an effective and reliable estimate. Exceptions may happen for the first iteration index where the estimated condition number is slightly lower than the computed one differing only in the last computed digit. This mismatch has its origin from the implicit assumption, used to derive estimation formula in Algorithm 5, that the condition number of the input matrix of the filter is always 1, which is not necessarily true for arrays of randomly generated orthonormal vectors.\nOften, the ratio between\ud835\udf05\ud835\udc52\ud835\udc60\ud835\udc612 and\ud835\udf05 \ud835\udc50\ud835\udc5c\ud835\udc5a 2 is below 2. For some cases (i.e., opt case for AuAg 13k) this ratio can reach at most O(104) for a few initial iterations. This is likely caused by the inaccuracy of the estimates for the parameters \ud835\udc52 and \ud835\udc50 during the first 2-3 iterations, where no eigenpairs have been locked yet. Independently from its strictness, \ud835\udf05\ud835\udc52\ud835\udc60\ud835\udc612 always bound \ud835\udf05 \ud835\udc50\ud835\udc5c\ud835\udc5a 2 from above and so it constitutes a reliable parameter for ChASE to switch from a more stable QR factorization, such as \ud835\udc60-Cholesky QR factorization, to the less stable but more efficient CholeskyQR2, even CholeskyQR1 in the last one or two iterations where the estimated condition number is O(1).\nIn the no-opt case, the highest condition number comes at the first iteration. Therefore, if the condition number of \ud835\udc36 at the first iteration is below a certain threshold, the \ud835\udc60-CholeskyQR2 can be avoided in any of the following iterations. Conversely, in the opt case this condition number at the early stage can be much larger than the one at the first iteration. This effect is caused by the higher maximal degree allowed during the degree optimization procedure and it is controllable by the user. Conversely, ChASE with degree optimization can always converge much faster than ChASE without degree optimization. In conclusion, our upper bound estimation of the condition number for the filtered matrix ensure that the proposed heuristic to switch between QR variants is reliable throughout the entire execution cycle of ChASE."
        },
        {
            "heading": "4.3 ChASE with CholeskyQR vs with HHQR",
            "text": "We compare the numerical behaviour of ChASE (NCCL) equipped with HHQR (for all ChASE iterations) with the automatic selection mechanism introduced in Section 3.2. Here, HHQR specifically refers to the Householder QR implementation provided by ScaLAPACK, which uses a 1D MPI grid and is executed independently over each column communicator. The block size of ScaLAPACK block-cyclic distribution for the rows is the same as the number of rows of \ud835\udc36 , and the block size for the columns is fixed at 32. Data of each test are obtained as the averages of 5 repetitions.\nThe test problems are the same listed in Table 1. ChASE are tested using 4 compute nodes on JUWELS-Booster with results reported in Table 2. This table shows the total number of MatVec operations, the number of iterations to convergence, the total time-to-solution and the execution time of the QR factorization.\nFor all the tests, the usage of either HHQR or CholeskyQR results in the same convergence behaviour with the same number of MatVec operations and iterations. The speedup of ChASE with CholeskyQR over ChASE with HHQR is clearly noticeable. Moreover, the employment of CholeskyQR greatly enhances the performance for ChASE-GPU when more than 1, 000 eigenpairs are sought after."
        },
        {
            "heading": "4.4 Kernel Profiling",
            "text": "In this section, we compare the communication, computation and data movement for the main parts of different implementations of ChASE: the Filter, QR, Rayleigh-Ritz and Residuals. We designed a weak-scaling experiment, in which the count of compute nodes increases from 1 to 64, while the matrix size increases from 30k to 240k. For this experiment, we used artificial matrices of type\nUniform with nev and nex being fixed to 2, 250 and 750. Only the first iteration is reported, which ensures a fixed workload per task with the increase of compute nodes count.\nThe results are shown Fig. 2, where stacked bar plots are used to show the portion of computation (marked in green), communication (marked in red) and data movement (marked in blue). We use different color shades to distinguish versions of ChASE. ChASE(LMS) is marked with the brightest colors, ChASE(NCCL) is marked with the lightest color, and ChASE(STD) is in middle.\nFor all the kernels, ChASE(STD) has already obtained significant reduction of the communication overhead compared with ChASE(LMS) except for the Filter using only 1 node; in this case ChASE(LMS) takes advantage of a configuration with 4 GPUs per MPI rank. Furthermore, the data movement has been fully removed in ChASE(NCCL), and the overhead of collective communications provided by NCCL is negligible compared with the overhead of communication in ChASE(STD) based on MPI.\nThe runtime of computation and communication in ChASE(LMS) increases substantially for larger number of computing nodes and matrix sizes, while the new ChASE can maintain a good weak scaling performance, especially for ChASE(NCCL). In summary, ChASE(STD) attains respectively speedups of 1.6\u00d7, 22\u00d7, 10\u00d7, 8\u00d7 over ChASE(LMS) with 64 compute nodes on JUWELS-Booster for the Filter, QR, Rayleigh-Ritz and Residuals. Meanwhile, ChASE(NCCL) attains respectively speedups of 3.8\u00d7, 1149\u00d7, 23\u00d7, 33\u00d7 over ChASE(LMS), and speedups of 2.3\u00d7, 51\u00d7, 2.2\u00d7, 4\u00d7 over ChASE(STD). The speedup of QR in ChASE(NCCL) is extremely large as only CholeskyQR2 is employed which does not have any data movement at all. This is also reflected by the practically invisible bars in fig. 2b."
        },
        {
            "heading": "4.5 Scalability",
            "text": "In this last part, we describe the behavior of ChASE(NCCL) in the weak and strong scaling regimes and compared it with both ChASE(LMS) and ChASE(STD). 4.5.1 Weak scaling Weak scaling experiments are particularly important to domain scientists, because they represent the potential of a library to execute computations of systems of increasingly larger size. For the weak scaling tests, we employed up to 900 of the JUWELS-Booster, and a total of 3, 600 NVIDIA A100 GPUs. The compute nodes count is chosen to be a square integer 1, 4, 9, \u00b7 \u00b7 \u00b7 , 144, 256, 400, 625, 900 to ensure square 2D MPI grids.\nFor all weak scaling tests, we used artificial matrices of type Uniform, with a size increment of 30k (30k, 60k, 90k, \u00b7 \u00b7 \u00b7 ). The maximal matrix size tested is 900k. nev and nex were fixed to 2, 250 and 750. Only a single iteration of ChASE has been executed for all the experiments of weak scaling to ensure a fixed workload per task. This is because being an iterative method, ChASE might require different number of iterative steps for matrices with increasing size.\nFig. 3a shows that the weak-scaling behaviour of ChASE(NCCL) is close to optimal: when the matrix size increases from 30k to 900k, the time-to-solution for a single iterative step increases by a factor of 1.8 from 2.3s to 3.9\ud835\udc60 . With the novel designed parallelization scheme and algorithm, ChASE achieves much better weak-scaling performance even without NCCL and CUDA-awareness; the time-tosolution of ChASE(STD) increases only by a factor of 3.1 from 5.1s to 16s. There are some special points on the curve of ChASE(STD) (i.e., the cases with compute nodes count equal to 4, 16, 64 and 256) where the time-to-solution drops down. The improvement in performance on these points is caused by the MPI_Allreduce collective communication, within the row or column communication, which is implemented based on a binary tree scheme. When the numbers of MPI ranks in the row or column communicator is a power of 2, it offers an advantage to these configurations over the others.\nThe improvement over ChASE(LMS) is quite evident. The weakscaling experiments of ChASE(LMS) could only test up to 144 nodes, because of the large memory footprint [18]. ChASE(NCCL) and ChASE(STD) on 144 compute nodes reaches a 14.1\u00d7 and 4.6\u00d7 speedup over ChASE(LMS), respectively. 4.5.2 Strong scaling Fig. 3b illustrates the results of the strong scaling experiments using the In2O3 115k eigenproblem, listed in Table 1. The number of eigenpairs sought after is set at 1, 200,\nrepresenting \u223c 1% of the full spectrum. The size of the external searching space nex is fixed as 400. As a reference, the strong-scaling performance of ChASE is compared with ELPA [6], the state-ofthe-art eigensolver for solving dense Hermitian eigenproblems on distributed-memory heterogeneous systems. The version of ELPA used is 2022.11.001.rc1, compiled with the same software stack for ChASE. In ELPA, the block size of the block-cyclic distribution is fixed at 16. Data are obtained with 8 repetitions. The compute nodes count are selected to be square of integers 4, 9, \u00b7 \u00b7 \u00b7 , 121, 144.\nChASE(STD) featuring the novel parallel scheme attains already much better strong-scaling performance than ChASE(LMS). The time-to-solution of the former drops from \u223c 92s on 4 nodes to \u223c 14s on 144 nodes achieving 6.6\u00d7 speedup. On the contrary, the time-to-solution of ChASE(LMS) only decreases from \u223c 135s on 4 nodes to \u223c 55s on 144 nodes gaining only a speedup of 2.5\u00d7\nThe strong-scaling performance achieved by ChASE(NCCL) is close to ideal. Comparing the execution on 4 and 144 compute nodes, ChASE(NCCL) achieves 18.6\u00d7 speedup, with the time-tosolution dropping from \u223c 65s to \u223c 3.5s. When 4 nodes are utilized, ChASE(STD) and ChASE(NCCL) achieve 1.5\u00d7 and 2.09\u00d7 speedup over ChASE(LMS), respectively. These speedup are enlarged to 3.9\u00d7 and 15.7\u00d7 when 144 nodes are utilized.\nConversely, ELPA1-GPU and ELPA2-GPU display only 6.7\u00d7 and 5.9\u00d7 speedup. When comparing ChASE with ELPA, ChASE(NCCL) experiences an increase in the speedup over ELPA2-GPU to reach a 28\u00d7 speedup, as the node count increases. On 144 compute nodes\u2014 576 NVIDIA A100 GPUs on JUWELS-Booster\u2014ELPA2-GPU computes the 1, 200 exterior eigenpairs of the 115k Hermitian dense eigenproblems in \u223c 98s, while ChASE(NCCL) takes \u223c 3.5s. We want to emphasize that the performance gain of ChASE over ELPA is obtained when only a relatively small portion of exterior eigenpairs are desired, which is the target usage of the ChASE. Despite being justified, such choice may put ELPA at a disadvantage."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we present a number of major improvements carried out on the ChASE library targeting distributed GPU systems for solving large-scale symmetric and Hermitian eigenproblems. ChASE targets dense eigenproblemswhen a relatively small fraction (\u2264 10%) of extremal eigenpairs is sought after. The improvement includes i) a redesign of the buffer structure, ii) a novel parallelization scheme, iii) a mechanism to switch between different variants of communication avoiding QR based on accurate estimates of the condition number of the matrix of vectors outputted by the Chebyshev filter, and iv) a substitution of the MPI library by the NCCL library for collective communications. The accuracy of the estimation of the condition number and the replacement of Householder QR by CholeskyQR has been verified and validated by numerical tests with a series of eigenproblems extracted from Condensed Matter applications. The parallel performance of the new ChASE v1.4 version has been benchmarked on the supercomputer JUWELS-Booster and is able to attain excellent strong and weak scaling performance. The resulting library can tackle dense problems with size up to \ud835\udc5b = O(106), and scale up to the full 900 nodes of JUWELS-Booster comprising 3, 600 NVIDIA GPUs in total. In the future, we plan to port ChASE to AMD GPUs using the RCCL library."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors gratefully acknowledge the computing time granted by J\u00fclich Supercomputing Centre (JSC) on JUWELS-Booster. We thank the members of the department of High-Performance Computing Systems at JSC for support and coordination of Full-Node Scale benchmarks on JUWELS-Booster."
        }
    ],
    "title": "Advancing the distributed Multi-GPU ChASE library through algorithm optimization and NCCL library",
    "year": 2023
}