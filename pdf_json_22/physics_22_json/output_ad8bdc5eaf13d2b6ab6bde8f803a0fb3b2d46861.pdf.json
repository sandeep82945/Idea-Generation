{
    "abstractText": "We present the first application of a Nested Sampling algorithm to explore the high-dimensional phase space of particle collision events. We describe the adaptation of the algorithm, designed to perform Bayesian inference computations, to the integration of partonic scattering cross sections and the generation of individual events distributed according to the corresponding squared matrix element. As a first concrete example we consider gluon scattering processes into 3-, 4and 5-gluon final states and compare the performance with established sampling techniques. Starting from a flat prior distribution Nested Sampling outperforms the Vegas algorithm and achieves results comparable to a dedicated multichannel importance sampler. We outline possible approaches to combine Nested Sampling with non-flat prior distributions to further reduce the variance of integral estimates and to increase unweighting efficiencies.",
    "authors": [
        {
            "affiliations": [],
            "name": "David Yallup"
        },
        {
            "affiliations": [],
            "name": "Timo Jan\u00dfen"
        },
        {
            "affiliations": [],
            "name": "Steffen Schumann"
        },
        {
            "affiliations": [],
            "name": "Will Handley"
        }
    ],
    "id": "SP:673bb8cad43ce85c97bc724c224fba30af84e9e2",
    "references": [
        {
            "authors": [
                "A. Buckley"
            ],
            "title": "General-purpose event generators for LHC physics",
            "venue": "Phys. Rep. 504,",
            "year": 2011
        },
        {
            "authors": [
                "J.M. Campbell"
            ],
            "title": "Event Generators for High-Energy Physics Experiments",
            "venue": "Snowmass Summer Study,",
            "year": 2022
        },
        {
            "authors": [
                "R. Kleiss",
                "R. Pittau"
            ],
            "title": "Weight optimization in multichannel Monte Carlo",
            "venue": "Comput. Phys. Commun. 83,",
            "year": 1994
        },
        {
            "authors": [
                "C.G. Papadopoulos"
            ],
            "title": "PHEGAS: a phase space generator for auto- matic cross-section computation",
            "venue": "Comput. Phys. Commun. 137,",
            "year": 2001
        },
        {
            "authors": [
                "F. Krauss",
                "G.R. Kuhn"
            ],
            "title": "Soff, AMEGIC++ 1.0: a matrix element generator in C++",
            "venue": "JHEP 02,",
            "year": 2002
        },
        {
            "authors": [
                "F. Maltoni",
                "T. Stelzer"
            ],
            "title": "MadEvent: automatic event generation with MadGraph",
            "venue": "JHEP 02,",
            "year": 2003
        },
        {
            "authors": [
                "S.T. Gleisberg"
            ],
            "title": "Hoeche, Comix, a new matrix element generator",
            "venue": "JHEP 12,",
            "year": 2008
        },
        {
            "authors": [
                "G.P. Lepage"
            ],
            "title": "A new algorithm for adaptive multidimensional inte- gration",
            "venue": "J. Comput. Phys. 27,",
            "year": 1978
        },
        {
            "authors": [
                "T. Ohl",
                "Vegas"
            ],
            "title": "revisited: adaptive Monte Carlo integration beyond factorization",
            "venue": "Comput. Phys. Commun. 120,",
            "year": 1999
        },
        {
            "authors": [
                "S. Jadach"
            ],
            "title": "Foam: multidimensional general purpose Monte Carlo generator with selfadapting symplectic grid",
            "venue": "Com- put. Phys. Commun. 130,",
            "year": 2000
        },
        {
            "authors": [
                "T. Hahn"
            ],
            "title": "CUBA: a Library for multidimensional numerical inte- gration",
            "venue": "Comput. Phys. Commun. 168,",
            "year": 2005
        },
        {
            "authors": [
                "A. van"
            ],
            "title": "Hameren, PARNI for importance sampling and density esti- mation",
            "venue": "Acta Phys. Pol. B Ser 40,",
            "year": 2009
        },
        {
            "authors": [
                "H. Kharraziha",
                "S. Moretti"
            ],
            "title": "The Metropolis algorithm for on-shell four momentum phase space",
            "venue": "Comput. Phys. Commun. 127,",
            "year": 2000
        },
        {
            "authors": [
                "S. Weinzierl"
            ],
            "title": "A general algorithm to generate unweighted events for next-to-leading order calculations in electron positron annihi- lation",
            "venue": "JHEP 08,",
            "year": 2001
        },
        {
            "authors": [
                "K. Kr\u00f6ninger",
                "S. Schumann",
                "B. Willenberg"
            ],
            "title": "MC)**3: a multi- channel Markov chain Monte Carlo algorithm for phase-space sam- pling",
            "venue": "Comput. Phys. Commun. 186,",
            "year": 2015
        },
        {
            "authors": [
                "J. Bendavid"
            ],
            "title": "Efficient Monte Carlo Integration Using Boosted Decision Trees and Generative Deep Neural Networks",
            "venue": "Eur. Phys. J. C (2022)",
            "year": 2022
        },
        {
            "authors": [
                "M.M.D. Klimek"
            ],
            "title": "Perelstein, Neural network-based approach to phase space integration",
            "venue": "SciPost Phys. 9,",
            "year": 2020
        },
        {
            "authors": [
                "S. Otten",
                "S. Caron",
                "W. de Swart",
                "M. van Beekveld",
                "L. Hendriks",
                "C. van Leeuwen"
            ],
            "title": "Event generation and statistical sampling for physics with deep generative models and a density information",
            "venue": "buffer. Nat. Commun",
            "year": 2021
        },
        {
            "authors": [
                "R. Di Sipio",
                "M.F. Giannelli",
                "S.K. Haghighat",
                "S. Palazzo"
            ],
            "title": "DijetGAN: a generative-adversarial network approach for the simulation of QCD Dijet Events at the LHC",
            "venue": "JHEP 08,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Alanazi"
            ],
            "title": "Simulation of electron\u2013proton scattering events by a Feature-Augmented and Transformed Generative Adversarial Network (FAT-GAN)",
            "year": 2020
        },
        {
            "authors": [
                "Y. Alanazi"
            ],
            "title": "AI-based Monte Carlo event generator for electron-proton scattering",
            "year": 2020
        },
        {
            "authors": [
                "S. Diefenbacher",
                "E. Eren",
                "G. Kasieczka",
                "A. Korol",
                "B. Nachman",
                "D. Shih"
            ],
            "title": "DCTRGAN: Improving the Precision of Generative Mod- els with Reweighting",
            "venue": "JINST",
            "year": 2020
        },
        {
            "authors": [
                "A. Butter",
                "S. Diefenbacher",
                "G. Kasieczka",
                "B. Nachman",
                "T. Plehn"
            ],
            "title": "GANplifying event samples",
            "venue": "SciPost Phys. 10,",
            "year": 2021
        },
        {
            "authors": [
                "K.T. Matchev",
                "A. Roman",
                "P. Shyamsundar"
            ],
            "title": "Uncertainties associ- ated with GAN-generated datasets in high energy physics",
            "venue": "SciPost Phys. 12,",
            "year": 2022
        },
        {
            "authors": [
                "E. Bothmann",
                "T. Jan\u00dfen",
                "M. Knobbe",
                "T. Schmale",
                "S. Schumann"
            ],
            "title": "Exploring phase space with neural importance sampling",
            "venue": "Sci- Post Phys. 8,",
            "year": 2020
        },
        {
            "authors": [
                "C. Gao",
                "J. Isaacson",
                "C. Krause"
            ],
            "title": "i-flow: high-dimensional inte- gration and sampling with normalizing flows",
            "venue": "Mach. Learn. Sci. Tech",
            "year": 2001
        },
        {
            "authors": [
                "C. Gao",
                "S. H\u00f6che",
                "J. Isaacson",
                "C. Krause",
                "H. Schulz"
            ],
            "title": "Event generation with normalizing flows",
            "venue": "Phys. Rev. D",
            "year": 2001
        },
        {
            "authors": [
                "B. Stienen",
                "R. Verheyen"
            ],
            "title": "Phase space sampling and inference from weighted events with autoregressive flows",
            "venue": "SciPost Phys. 10,",
            "year": 2021
        },
        {
            "authors": [
                "K. Danziger",
                "T. Jan\u00dfen",
                "S. Schumann",
                "F. Siegert"
            ],
            "title": "Accelerating Monte Carlo event generation\u2014rejection sampling using neural network",
            "venue": "event-weight estimates,",
            "year": 2021
        },
        {
            "authors": [
                "M. Bellagente",
                "M. Hau\u00dfmann",
                "M. Luchmann",
                "T. Plehn"
            ],
            "title": "Under- standing event-generation networks via uncertainties",
            "year": 2021
        },
        {
            "authors": [
                "A. Butter",
                "T. Heimel",
                "S. Hummerich",
                "T. Krebs",
                "T. Plehn",
                "A. Rous- selot"
            ],
            "title": "Generative networks for precision enthusiasts",
            "year": 2021
        },
        {
            "authors": [
                "J. Skilling"
            ],
            "title": "Nested sampling for general Bayesian computation",
            "venue": "Bayesian Anal. 1,",
            "year": 2006
        },
        {
            "authors": [
                "W.J. Handley",
                "A.N.M.P. Hobson"
            ],
            "title": "Lasenby, polychord: next- generation nested sampling",
            "venue": "Mon. Not. R. Astron. Soc",
            "year": 2015
        },
        {
            "authors": [
                "D.J.C. MacKay"
            ],
            "title": "Information Theory. Inference & Learning Algo- rithms (Cambridge",
            "year": 2002
        },
        {
            "authors": [
                "G. Ashton"
            ],
            "title": "Nested sampling for physical scientists. Nature 2 (2022)",
            "year": 2022
        },
        {
            "authors": [
                "P. Mukherjee",
                "D. Parkinson",
                "A.R. Liddle"
            ],
            "title": "A nested sam- pling algorithm for cosmological model selection",
            "venue": "Astro- phys. J. Lett. 638,",
            "year": 2006
        },
        {
            "authors": [
                "R. Shaw",
                "M. Bridges",
                "M.P. Hobson"
            ],
            "title": "Clustered nested sampling: efficient Bayesian inference for cosmology",
            "venue": "Mon. Not. R. Astron. Soc. 378,",
            "year": 2007
        },
        {
            "authors": [
                "F. Feroz",
                "M.P. Hobson"
            ],
            "title": "Multimodal nested sampling: an efficient and robust alternative to MCMC methods for astronomical data analysis",
            "venue": "Mon. Not. R. Astron. Soc. 384,",
            "year": 2008
        },
        {
            "authors": [
                "F. Feroz",
                "M.P. Hobson",
                "M. Bridges"
            ],
            "title": "MultiNest: an efficient and robust Bayesian inference tool for cosmology and particle physics",
            "venue": "Mon. Not. R. Astron. Soc. 398,",
            "year": 2009
        },
        {
            "authors": [
                "F. Feroz",
                "M.P. Hobson",
                "E. Cameron",
                "A.N. Pettitt"
            ],
            "title": "Importance nested sampling and the MultiNest algorithm",
            "venue": "Open J. Astro- phys. 2,",
            "year": 2019
        },
        {
            "authors": [
                "W.J. Handley",
                "A.N.M.P. Hobson"
            ],
            "title": "Lasenby, PolyChord: nested sam- pling for cosmology",
            "venue": "Mon. Not. R. Astron. Soc. 450,",
            "year": 2015
        },
        {
            "authors": [
                "W. Handley",
                "P. Lemos"
            ],
            "title": "Quantifying dimensionality: Bayesian cos- mological model complexities",
            "venue": "Phys. Rev. D 100,",
            "year": 2019
        },
        {
            "authors": [
                "S.S. AbdusSalam"
            ],
            "title": "Simple and statistically sound recommen- dations for analysing physical theories",
            "year": 2020
        },
        {
            "authors": [
                "A. Fowlie",
                "W. Handley",
                "L. Su"
            ],
            "title": "Nested sampling with plateaus",
            "venue": "Mon. Not. R. Astron. Soc. 503,",
            "year": 2021
        },
        {
            "authors": [
                "E. Higson",
                "W. Handley",
                "M. Hobson",
                "A. Lasenby"
            ],
            "title": "Sampling errors in nested sampling parameter estimation",
            "venue": "Bayesian Analysis series",
            "year": 2018
        },
        {
            "authors": [
                "A. van Hameren",
                "C.G. Papadopoulos"
            ],
            "title": "A hierarchical phase space generator for QCD antenna structures",
            "venue": "Eur. Phys. J. C 25,",
            "year": 2002
        },
        {
            "authors": [
                "R. Kleiss",
                "W.J. Stirling",
                "S.D. Ellis"
            ],
            "title": "A new Monte Carlo treatment of multiparticle phase space at high-energies",
            "venue": "Com- put. Phys. Commun. 40,",
            "year": 1986
        },
        {
            "authors": [
                "A. Fowlie",
                "S. Hoof",
                "W. Handley"
            ],
            "title": "Nested sampling for frequentist computation: fast estimation of small p values",
            "venue": "Phys. Rev. Lett. 128,",
            "year": 2022
        },
        {
            "authors": [
                "E. Carragher",
                "W. Handley",
                "D. Murnane",
                "P. Stangl",
                "W. Su",
                "M. White"
            ],
            "title": "Convergent Bayesian global fits of 4D compos- 123 Eur",
            "venue": "Phys. J. C (2022)",
            "year": 2021
        },
        {
            "authors": [
                "A. Fowlie",
                "W. Handley",
                "L. Su"
            ],
            "title": "Nested sampling cross-checks using order statistics",
            "venue": "Mon. Not. R. Astron. Soc.497,",
            "year": 2020
        },
        {
            "authors": [
                "C. Bierlich"
            ],
            "title": "Robust independent validation of experiment and theory: rivet version 3",
            "venue": "SciPost Phys. 8,",
            "year": 2020
        },
        {
            "authors": [
                "M. Cacciari",
                "G.G.P. Salam"
            ],
            "title": "Soyez, The anti-kt jet clustering algo- rithm",
            "venue": "JHEP 04,",
            "year": 2008
        },
        {
            "authors": [
                "W. Handley"
            ],
            "title": "anesthetic: nested sampling visualisation",
            "venue": "J. Open Sour. Softw",
            "year": 1905
        },
        {
            "authors": [
                "S. H\u00f6che",
                "S. Prestel",
                "H. Schulz"
            ],
            "title": "Simulation of vector boson plus many jet final states at the high luminosity LHC",
            "venue": "Phys. Rev. D 100,",
            "year": 2019
        },
        {
            "authors": [
                "T. Gleisberg",
                "F. Krauss"
            ],
            "title": "Automating dipole subtraction for QCD NLO calculations",
            "venue": "Eur. Phys. J. C 53,",
            "year": 2008
        },
        {
            "authors": [
                "A. Petrosyan",
                "W. Handley"
            ],
            "title": "SuperNest: accelerated nested sam- pling applied to astrophysics and cosmology, Maximum Entropy (accepted for Oral presentation",
            "year": 2022
        },
        {
            "authors": [
                "E. Higson",
                "W. Handley",
                "M. Hobson",
                "A. Lasenby"
            ],
            "title": "Dynamic nested sampling: an improved algorithm for parameter estimation and evi- dence calculation",
            "venue": "Stat. Comput",
            "year": 2018
        },
        {
            "authors": [
                "S. Badger"
            ],
            "title": "Machine Learning and LHC Event Generation",
            "venue": "mann, eds.,",
            "year": 2022
        },
        {
            "authors": [
                "J. Alsing",
                "W. Handley"
            ],
            "title": "Nested sampling with any prior you like",
            "venue": "Mon. Not. R. Astron. Soc. 505,",
            "year": 2021
        },
        {
            "authors": [
                "L. Del Debbio",
                "J.M. Rossney",
                "M. Wilson"
            ],
            "title": "Efficient modeling of trivializing maps for lattice \u03c64 theory using normalizing flows: a first look at scalability",
            "venue": "Phys. Rev. D 104,",
            "year": 2021
        },
        {
            "authors": [
                "E. Bothmann",
                "W. Giele",
                "S. H\u00f6che",
                "J. Isaacson",
                "M. Knobbe"
            ],
            "title": "Many- gluon tree amplitudes on modern GPUs: a case study for novel event generators",
            "year": 2021
        },
        {
            "authors": [
                "E. Higson",
                "W. Handley",
                "M. Hobson",
                "A. Lasenby"
            ],
            "title": "Nestcheck: diag- nostic tests for nested sampling calculations",
            "venue": "Mon. Not. R. Astron. Soc. 483,",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Realistic simulations of scattering events at particle collider experiments play an indispensable role in the analysis and interpretation of actual measurement data for example at the Large Hadron Collider (LHC) [1,2]. A central component of such event simulations is the generation of hard scattering configurations according to a density given by the squared transition matrix element of the concrete process under consideration. This is needed both for the evaluation of corresponding cross sections, as well as the explicit generation of individual events that potentially get further processed, e.g. by attaching parton showers, invoking phenomenological models to account for the parton-to-hadron transition, and eventually, a detector simulation. To adequately address the physics needs of the LHC experiments requires the evaluation of a wide range of high-multiplicity hard processes that feature a highly non-trivial multimodal target density\na e-mail: dy297@cam.ac.uk (corresponding author)\nthat is rather costly to evaluate. The structure of the target is thereby affected by the appearance of intermediate resonances, quantum interferences, the emission of soft and/or collinear massless gauge bosons, or non-trivial phase space constraints, due to kinematic cuts on the final state particles. Dimensionality and complexity of the phase space sampling problem make the usage of numerical methods, and in particular Monte Carlo techniques, for its solution indispensable.\nThe most widely used approach relies on adaptive multichannel importance sampling, see for example [3\u20137]. However, to achieve good performance detailed knowledge of the target distribution, i.e. the squared matrix element, is needed. To this end information about the topology of scattering amplitudes contributing to the considered process is employed in the construction of individual channels. Alternatively, and also used in combination with importance sampling phase space maps, variants of the self-adaptive VEGAS algorithm [8] are routinely applied [9\u201312].\nAn alternative approach for sampling according to a desired probability density is offered by Markov Chain Monte Carlo (MCMC) algorithms. However, in the context of phase space sampling in high energy physics these techniques attracted rather limited attention, see in particular [13,14]. More recently a mixed kernel method combining multi-channel sampling and MCMC, dubbed (MC)3, has been presented [15]. A typical feature of such MCMC based algorithms is the potential autocorrelation of events that can affect their direct applicability in typical use case scenarios of event generators.\nTo meet the computing challenges posed by the upcoming and future LHC collider runs and the corresponding event simulation campaigns, improvements of the existing phase space sampling and event unweighting techniques will be crucial [16,17]. This has sparked renewed interest in the subject, largely driven by applications of machine learning techniques, see for instance [18\u201336].\nIn this article we explore an alternative direction. We here study the application of Nested Sampling [37] as imple-\nmented in PolyChord [38] to phase space integration and event generation for high energy particle collisions. We here assume no prior knowledge about the target and investigate the ability of the algorithm to adapt to the problem. Nested Sampling has originally been proposed to perform Bayesian inference computations for high dimensional parameter spaces, providing also the evidence integral, i.e. the integral of the likelihood over the prior density. This makes it ideally suited for our purpose. In Sect. 2 we will introduce Nested Sampling as a method to perform cross section integrals and event generation, including a reliable uncertainty estimation. In Sect. 3 we will apply the method to gluon scattering to 3-, 4-, and 5-gluon final states as a benchmark for jet production at hadron colliders, thereby comparing results for total cross sections and differential distributions with established standard techniques. Evaluation of the important features of the algorithm when applied in the particle physics context is also discussed in this section. In Sect. 4 we illustrate several avenues for future research, extending the work presented here. Finally, we present our conclusions in Sect. 5."
        },
        {
            "heading": "2 Nested sampling for event generation",
            "text": "The central task when exploring the phase space of scattering processes in particle physics is to compute the cross section integral, \u03c3 . This requires the evaluation of the transition squared matrix element, |M|2, integrated over the phase space volume, \u03a9 , where \u03a9 is composed of all possible kinematic configurations, , of the external particles. Up to some constant phase space factors this amounts to performing the integral, \u03c3 = \u222b\n\u03a9\nd |M|2( ) . (1)\nIn practice rather than sampling the physical phase space variables, i.e. the particles\u2019 four-momenta, it is typical to integrate over configurations, \u03b8 \u2208 [0, 1]D , from the D-dimensional unit hypercube. Some mapping, : [0, 1]D \u2192 \u03a9 , is then employed to translate the sampled variables to the physical momenta. The mapping is defined as, = (\u03b8), and the integral in Eq. (1) is written, \u03c3 = \u222b\n[0,1]D d\u03b8 |M|2( (\u03b8))J (\u03b8) =\n\u222b\n[0,1]D d\u03b8L(\u03b8) . (2)\nA Jacobian associated with the change of coordinates between \u03b8 and has been introduced, J , and then absorbed into the definition of L(\u03b8) = |M|2( (\u03b8))J (\u03b8). With no general analytic solution to the sorts of scatterings considered\nat the high energy frontier, this integral must be estimated with numerical techniques. Numerical integration involves sampling from the |M|2 distribution in a manner that gives a convergent estimate of the true integral when the samples are summed. As a byproduct this set of samples can be used to estimate integrals of arbitrary sub-selections of the integrated phase space volume, decomposing the total cross section into differential cross section elements, d\u03c3 . Additionally these samples can be unweighted and used as pseudo-data to emulate the experimental observations of the collisions. The current state of the art techniques for performing these tasks were briefly reviewed in Sect. 1.\nImportance Sampling (IS) is a Monte Carlo technique used extensively in particle physics when one needs to draw samples from a distribution with an unknown target probability density function, P( ). Importance Sampling approaches this problem by instead drawing from a known sampling distribution, Q( ) (A number of standard texts for inference give more thorough exposition of the general sampling theory used in this paper, see e.g. [39]). Samples drawn from Q are assigned a weight, w = P( )/Q( ), adjusting the importance of each sampled point. The performance of IS rests heavily on how well the sampling distribution can be chosen to match the target, and adaptive schemes like VEGAS are employed to refine initial proposals. It is well established that as the dimensionality and complexity of the target increase, the task of constructing a viable sampling distribution becomes increasingly challenging.\nMarkov Chain based approaches fundamentally differ in that they employ a local sampling distribution and define an acceptance probability with which to accept new samples. Markov Chain Monte Carlo (MCMC) algorithms are widely used in Bayesian inference. Numerical Bayesian methods have to be able to iteratively refine the prior distribution to the posterior, even in cases where the two distributions are largely disparate, making stochastic MCMC refinement an indispensable tool in many cases. This is an important conceptual point; in the particle physics problems presented in this work we are sampling from exact theoretically derived distributions. The lack of noise and a priori well known structure make methods with deterministic proposal distributions such as IS more initially appealing, however at some point increasing the complexity and dimensionality of the problem forces one to use stochastic methods. Lattice QCD calculations are a prominent example set of adjacent problems sampling from theoretical distributions that make extensive use of MCMC approaches [40]. MCMC algorithms introduce an orthogonal set of challenges to IS; a local proposal is inherently simpler to construct, however issues with exploration of multimodal target distributions and autocorrelation of samples become new challenges to address.\nNested Sampling (NS) is a well established algorithm for numerical evaluation of high dimensional integrals [37]. NS\ndiffers from typical MCMC samplers as it is primarily an integration algorithm, hence by definition has to overcome a lot of the difficulties MCMC samplers face in multimodal problems. A recent community review of its various applications in the physical sciences, and various implementations of the algorithm has been presented in [41].\nAt its core NS operates by maintaining a number, nlive, of live point samples. This ensemble of live points is initially uniformly sampled from \u03b8 \u2208 [0, 1]D \u2013 distributed in the physical volume \u03a9 according to the shape of the mapping . These live points are sorted in order of L(\u03b8) evaluated at the phase space point, and the point with the lowest L, Lmin, in the population is identified. A replacement for this point is found by sampling uniformly under a hard constraint requiring, L > Lmin. The volume enclosed by this next iteration of live points has contracted and the procedure of identifying the lowest L point and replacing it is repeated. An illustration of three different stages of this iterative compression on an example two-dimensional function are shown in Fig. 1. The example function used in this case has four identical local maxima to find, practical exploration and discovery of the modes is achieved by having a sufficient (O (10)) initial samples in the basis of attraction of each mode. This can either be achieved by brute force sampling a large number of initial samples, or by picking an initial mapping distribution that better reflects the multi-modal structure. By continually uniformly sampling from a steadily compressing volume, NS can estimate the density of points which is necessary for computing an integral as given by Eq. (1). Once the iterative procedure reaches a point where the live point ensemble occupies a predefined small fraction of the initial volume, TC , the algorithm terminates. The fraction TC can be characterised as the termination criterion. The discarded points throughout the evolution are termed dead points which can be joined with the remaining live points to form a representative sample of the function, that can be used to estimate the integral or to provide a random sample of events.\nTo estimate the integral and generate (weighted) random samples, Nested Sampling achieves this by probabilistically estimating the volume of the shell between the two outermost points as approximately 1nlive of the current live volume. The volume X j within the contour L j \u2013 defined by the point with Lmin \u2013 at iteration j may therefore be estimated as,\nX j = \u222b L(\u03b8)>L j d\u03b8\n\u21d2 X0 = 1,\nP(X j |X j\u22121) = Xnlive\u22121j nliveX nlive j\u22121\n\u21d2 log X j \u2248 \u2212 j \u00b1 \u221a j\nnlive .\nThe cross section and probability weights can therefore be estimated as, \u03c3 = \u222b d\u03b8L(\u03b8) = \u222b dXL(X)\n\u2248 \u2211 j L j X j , w j \u2248 X jL j \u03c3 . (3)\nImportantly, for all of the above the approximation signs indicate errors in the procedure of probabilistic volume estimation, which are fully quantifiable.\nThe method to sample new live points under a hard constraint can be realised in multiple ways, and this is one of the key differences in the various implementations of NS. In this work we employ the PolyChord implementation of Nested Sampling [38], which uses slice sampling [42] MCMC steps to evolve the live points. NS can be viewed as being an ensemble of many short Markov Chains.\nMuch of the development and usage of NS has focused on the problem of calculation of marginal likelihoods (or evidences) in Bayesian inference, particularly within the field of Cosmology [43\u201348]. We can define the Bayesian evidence, Z , analogously to the particle physics cross section, \u03c3 . NS in this context evaluates the integral, Z = \u222b d\u03b8L(\u03b8)\u03c0(\u03b8) , (4)\nwhere the likelihood function,L, plays a similar role to |M|2. In the Bayesian inference context, the phase space over which we are integrating, \u03b8 , has a measure defined by the prior distribution, \u03c0(\u03b8), which without loss of generality under a suitable coordinate transformation can be taken to be uniform over the unit hypercube. Making the analogy between the evidence and the cross section explicit will allow us to apply some of the information theoretic metrics commonly used in Bayesian inference to the particle physics context [49], and provide terminology used throughout this work. Among a wide array of sampling methods for Bayesian inference, NS possesses some unique properties that enable it to successfully compute the high dimensional integral associated with Eq. (4). These properties also bear a striking similarity to the requirements one would like to have to explore particle physics phase spaces. These are briefly qualitatively described as follows:\n\u2013 NS is primarily a numerical integration method that produces posterior samples as a by product. In this respect it is comfortably similar to Importance Sampling as the established tool in particle physics event generation. It might initially be tempting to approach the particle physics event generation task purely as a posterior sampling problem. Standard Markov Chain based sampling\ntools cannot generically give good estimates of the integral, so are not suited to compute the cross section. Additionally issues with coverage of the full phase space from the resulting event samples are accounted for by default by obtaining a convergent estimate of the integral over all of the phase space. \u2013 NS naturally handles multimodal problems [45,46]. The iterative compression can be augmented by inserting steps that cluster the live points periodically throughout the run. Defining subsets of live points and evolving them separately allows NS to naturally tune itself to the modality of unseen problems. \u2013 NS requires a construction that can handle sampling under a hard likelihood constraint in order to perform the compression of the volume throughout the run. Hard boundaries in the physics problem, such as un-physical or deliberately cut phase space regions, manifest themselves in the sampling space as a natural extension of these constraints. \u2013 NS is largely self tuning. Usage in Bayesian inference has found that NS can be applied to a broad range of problems with little optimisation of hyper-parameters necessary [50\u201352]. NS can adapt to different processes in particle physicswithout anyprior knowledgeof the underlying process needed.\nThe challenge to present NS in this new context is to find an even comparison of sampling performance between NS and IS. It is typical in phase space sampling to compare the difference between the target and the sampling distribution as reducing the variation between these two distributions gives a clear metric of performance for IS. For NS there is no such global sampling distribution; the closest analogue being the prior which is then iteratively refined with local proposals to an estimate of the target. In Sect. 2.1 we attempt to compare the sampling distribution between NS and IS using a toy problem, however in the full physical gluon scattering example presented in Sect. 3 we instead focus directly on the properties of the estimated target distribution as this is the most direct equitable point of comparison.\n2.1 Illustrative example\nTo demonstrate the capabilities of NS we apply the algorithm to an illustrative sampling problem in two dimensions. Further examples validating PolyChord on a number of challenging sampling toy problems are included in the original paper [38], here we present a modified version of the Gaussian Shells scenario. An important distinction of the phase space use case not present in typical examples is the emphasis on calculating finely binned differential histograms of the total integral. As a comparison to NS, we sample the same problem with a method that is well-known in high energy\nphysics \u2013 adaptive Importance Sampling (IS), realised using the VEGAS algorithm.\nFor our toy example we introduce a \u201cstop sign\u201d target density, whose unnormalised distribution is defined by\nf (x, y) = 1 2\u03c02 r(\u221a (x \u2212 x0)2 + (y \u2212 y0)2 \u2212 r0 )2 + ( r)2 \u00b7 1\u221a\n(x \u2212 x0)2 + (y \u2212 y0)2 + 1\n2\u03c0r0\nr\n((y \u2212 y0) \u2212 (x \u2212 x0))2 + ( r)2\n\u00b7 ( r0 \u2212 \u221a (x \u2212 x0)2 + (y \u2212 y0)2 ) , (5)\nwhere (x) is the Heaviside function. It is the sum of a ring and a line segment, both with a (truncated) Cauchy profile. The ring is centred at (x0, y0) = (0.5, 0.5) and has a radius of r0 = 0.4. The line segment is located in the inner part of the ring and runs through the entire diameter. We set the width of the Cauchy profile to r = 0.002. This distribution can be seen as an example of a target where it makes sense to tackle the sampling problem with a multi-channel distribution. One channel could be chosen to sample the ring in polar coordinates and one to sample the line segment in Cartesian coordinates. However, here we deliberately use VEGAS as a single channel in order to highlight the limitations of the algorithm. From the perspective of a single channel, there is no coordinate system to factorise the target distribution. That poses a serious problem for VEGAS, as it uses a factorised sampling distribution where the variables are sampled individually. Both algorithms are given zero prior knowledge of the target, thus starting with a uniform prior distribution.\nOur VEGAS grid has 200 bins per dimension. We train it over 10 iterations where we draw 30k points from the current VEGAS mapping and adapt the grid to the data. The distribution defined by the resulting grid is then used for IS without further adaptation. This corresponds to the typical use in an event generator, where there is first an integration phase in which, among other things, VEGAS is adapted, followed by a non-adaptive event generation phase. We note that VEGAS gets an advantage in this example comparison as we do not include the target evaluations from the training into the counting. However, it should be borne in mind that in a realistic application with a large number of events to be generated, the costs for training are comparatively low. For NS we use PolyChord with a number of live points nlive = 1000 and a chain length nrepeats = 4, more complete detail of PolyChord settings and their implication are given in Sect. 3.1. Figure 2a shows the bivariate target distribution along with the marginal x and y distributions of the target, VEGAS and PolyChord. For this plot (as well as for Fig. 2c) we merged 70 independent runs of PolyChord to get a better visual\nrepresentation due to the larger sample size. It can be seen that both algorithms reproduce the marginal distributions reasonably well. There is some mismatch at the boundaries for VEGAS. This can be explained by the fact that VEGAS, as a variance-reduction method, focuses on the high-probability regions, where it puts many bins, and uses only few bins for the comparably flat low-probability regions. As a result, the bins next to the boundaries are very wide and overestimate the tails. PolyChord also oversamples the tails, reflecting the fact that in this example the prior is drastically different from the posterior, meaning the initial phase of prior sampling in PolyChord is very inefficient. In addition it puts too many points where the ring and the line segment join, which is where we find the highest values of the target function. This is not a generic feature of NS at the termination of the algorithm, rather it reflects the nature of having two intersecting sweeping degenerate modes in the problem, a rather unlikely scenario in any physical integral.\nFigure 2b shows the ratio between the target distribution and the sampling distribution of VEGAS, representing the IS weights. It can be seen that the marginals of the ratio are relatively flat, with values between 0.1 and 5.7. However, in two dimensions the ratio reaches values up to 1 \u00d7 10\u22122. By comparing Fig. 2a and b, paying particular attention to the very similar ranges of function values, it can be deduced that VEGAS almost completely misses to learn the structure of the target. It tries to represent the peak structure from the ring and the line segment by an enclosing square with nearly uniform probability distribution.\nThe same kind of plot is shown in Fig. 2c for the PolyChord data. NS does not strictly define a sampling distribution, however a proxy for this can be visualised by plotting the density of posterior samples. Here the values of the ratio are much smaller, between 1 \u00d7 10\u22122 and 7. PolyChord produces a flatter ratio function than VEGAS while not introducing additional artifacts that are not present in the original function. The smallest/largest values of the ratio are found in the same regions as the smallest/largest values of the target function, implying that PolyChord tends to overestimate the tails and to underestimate the peaks. This can be most clearly explained by examining the profile of where posterior mass is distributed throughout a run, an important diagnostic tool for NS runs [53]. It is shown in Fig. 3, where the algorithm runs from left to right; starting with the entire prior volume remaining enclosed by the live points, log X = 0, and running to termination, when the live points contain a vanishingly small remaining prior volume. The posterior mass profile, shown in blue, is the analogue to the sampling density in VEGAS. To contextualise this against the target function, a profile of the log-likelihood of the lowest live point in the live point ensemble is similarly shown as a function of the\nremaining prior volume, X . Nested Sampling can be motivated as a likelihood scanner, sampling from monotonically increasing likelihood shells. These two profiles indicate some features of this problem, firstly a phase transition is visible in the posterior mass profile. This occurs when the degenerate peak of the ring structure is reached, the likelihood profile reaches a plateau where the iterations kill off the degenerate points at the peak of the ring, before proceeding to scan up the remaining line segment feature. An effective second plateau is found when the peak of the line segment is reached, with a final small detail being the superposition of the ring likelihood on the line segment. Once the live points are all occupying the extrema of the line segment, there is a sufficiently small prior volume remaining that the algorithm terminates. The majority of the posterior mass, and hence sampling density is distributed around the points where the two peaks are ascended. This reflects the stark contrast between the prior initial sampling density and the target, the samples are naturally distributed where the most information is needed to effectively compress the prior to the posterior.\nWe compare the efficiencies of the two algorithms for the generation of equal-weight events in Table 1. It shows that PolyChord achieves an overall efficiency of = 0.0113(90) which is almost three times as high as the efficiency of VEGAS. While for VEGAS the overall efficiency is identical to the unweighting efficiency uw, determined by the ratio of the average event weight over the maximal weight in the sample, for PolyChord we also have to take the slice sampling efficiency ss into account, which results from the thinning of the Markov Chain in the slice sampling step. Here, the total efficiency = ss uw is dominated by the slice sampling efficiency. We point out that it is in the nature of the NS algorithm that the sample size is not deterministic. However, the variance is not very large and it is easily possible to merge several NS runs to obtain a larger sample.\nTable 2 shows the integral estimates along with the corresponding uncertainty measures. While the pure Monte Carlo errors are of the same size for both algorithms, there is an additional uncertainty for NS. It carries an uncertainty on the weights of the sampled points, listed as w. This arises due to the nature of NS using the volume enclosed by the live points at each iteration to estimate the volume of the likelihood shell. The variance in this volume estimate can be sampled, which is reflected as a sample of alternative weights for each dead point in the sample. Summing up these alternative weight samples gives a spread of predictions for the total integral estimate, and the standard deviation of these is quoted as w. This additional uncertainty compounds the familiar statistical uncertainty, listed as MC for all calculations. In Appendix A, we present the procedure needed to combine the two NS uncertainties to quote a total uncertainty, \u03c3tot, as naively adding in quadrature will overestimate the true error."
        },
        {
            "heading": "3 Application to gluon scattering",
            "text": "As a first application and benchmark for the Nested Sampling algorithm, we consider partonic gluon scattering processes into three-, four- and five-gluon final states at fixed centre-of-mass energies of \u221a s = 1 TeV. These channels have a complicated phase space structure that is similar to processes with quarks or jets, while the corresponding amplitude expressions are rather straightforward to generate. The fixed initial and final states allow us to focus on the underlying sampling problem. For regularisation we apply cuts to the invariant masses of all pairs of final state gluons such thatmi j > 30 GeV and on the transverse momenta of all final state gluons such that pT,i > 30 GeV. The renormalisation scale is fixed to\u03bcR = \u221as. The matrix elements are calculated\nusing a custom interface betweenPolyChord and the matrix element generator AMEGIC [5] within the SHERPA event generator framework [54]. Three established methods are used to provide benchmarks to compare NS to. Principle comparison is drawn to the HAAG sampler, optimised for QCD antenna structures [55], illustrating the exploration of phase space with the best a priori knowledge of the underlying physics included. It uses a cut-off parameter of s0 = 900 GeV2. Alongside this, two algorithms that will input no prior knowledge of the phase space, i.e. the integrand, are used; adaptive importance sampling as realised in the VEGAS algorithm [8] and a flat uniform sampler realised using the RAMBO algorithm [56,57]. VEGAS remaps the variables of the RAMBO parametrisation using 50, 70, 200 bins per dimension for the three-, four-, and five-gluon case, respectively. The grid is trained in 10 iterations using 100k training points each. Note, the dimensionality of the phase space for n-gluon production is D = 3n\u22124, where total four-momentum conservation and on-shell conditions for the external particles are implicit.\nAs a first attempt to establish NS in this context, we treat the task of estimating the total and differential cross sections of the three processes starting with no prior knowledge of the underlying phase space distribution. For the purposes of running PolyChord we provide the flat RAMBO sampler as the prior, and the likelihood function provided is the squared matrix element. In contrast to HAAG, PolyChord performs the integration without any decomposition into channels, removing the need for any multichannel mapping. NS is a flexible procedure, and the objective of the algorithm can be modified to perform a variety of tasks, a recent example has presented NS for computation of small p-values in the particle physics context [58]. To establish NS for the task of phase space integration in this study, a standard usage of PolyChord is employed, mostly following default values used commonly in Bayesian inference problems.\nThe discussion of the application of NS to gluon-scattering processes is split into four parts. Firstly, the hyperparameters and general setup of PolyChord are explained in Sect. 3.1. In Sect. 3.2 a first validation of NS performing the core tasks of (differential) cross-section estimation from weighted events \u2013 against the HAAG algorithm \u2013 is presented. In Sect. 3.3 further information is given to contextualise the computational efficiency of NS against the alter-\nnative established tools for these tasks. Finally a consideration of unweighted event generation with NS is presented in Sect. 3.4.\n3.1 PolyChord hyperparameters\nThe hyperparameters chosen to steer PolyChord are listed in Table 3. These represent a typical set of choices for a high resolution run with the objective of producing a large number of posterior samples. The number of live points is one of the parameters that is most free to tune, being effectively the resolution of the algorithm. Typically nlive larger than O (1000) gives diminishing returns on accuracy, Bayesian inference usage in particle physics has previously employed nlive = 4000 [59] to provide some context for the choice made in this work. The particular event generation use case, partitioning the integral into arbitrarily small divisions (differential cross sections), logically favours a large nlive (resolution). The number of repeats is a parameter that controls the length of the slice sampling chains, the value chosen is the recommended default for reliable posterior sampling, whereas nrep = ndim \u00d75 is recommended for evidence (total integral) estimation. As this study aims to cover both differential and total cross sections, the smaller value is favoured as there is a strong limit on the overall efficiency imposed by how many samples are needed to decorrelate the Markov Chains.\nAn important point to note is in how PolyChord treats unphysical values of the phase space variables, e.g. if they fall outside the fiducial phase space defined by cuts on the particle momenta. This is not an explicit hyperparameter of PolyChord, rather how the algorithm treats points with zero likelihood. In both the established approaches and in PolyChord the sampling is performed in the unit hypercube, which is then translated to the physical variables which can be evaluated for consistency and rejected if they are not phys-\nically valid. One of the strengths of NS is that the default behavior is to consider points which return zero likelihood1 as being excluded at the prior level. During the initial prior sampling phase, unphysical points are set to log-zero and the sampling proceeds until nprior initial physical samples have been obtained. Provided each connected physical region contains some live points after this initial phase, the iterative phase of MCMC sampling will explore up to the unphysical boundary. This effect necessitates a correction factor to be applied to the integral, derived as the ratio of total initial prior samples to the physically valid prior samples. In practice the correction factor is found in the prior_info file written out by PolyChord. An uncertainty on this correction can be derived from order statistics [60], however it was found to be negligibly small for the purposes of this study so is not included.\nAnother standout choice of hyperparameter is the chosen value of nprior. The number of prior samples is an important hyperparameter that would typically be set to some larger multiple of nlive in a Bayesian inference context, nprior = 10 \u00d7 nlive would be considered sensible for a broad range of tasks. For the purpose of generating weighted events, using a larger value would generally be advantageous, however increasing nprior will strongly decrease the efficiency in generating unweighted events. As the goal is to construct a generator taking an uninformed prior all the way through to unweighted events, the default value listed is used. However it is notable that this is a particular feature of starting from an uninformed prior, if more knowledge were to be included in the prior then a longer phase of prior sampling becomes advantageous. The final parameter noted, the factor by which to boost posterior samples, has no effect on PolyChord at runtime. Setting this to be equal to the number of repeats simply writes out the maximum number of dead points, hence is needed in this scenario. All plots and tables in the remainder of this section are composed of one single run of PolyChord with these settings, with the additional entries in Table 4 demonstrating a join of ten such runs.\n3.2 Exploration and integrals\nBefore examining the performance of NS in detail, it is first important to validate that the technique is capable of fully exploring particle physics phase spaces in these chosen examples. The key test to validate this is to compare if various differential cross sections calculated with NS are statistically consistent with the established techniques. To do this, a single NS and HAAG sample of weighted events is produced, using approximately similar levels of computational\n1 Since PolyChord operates in log space, to avoid the infinity associated with log(0), log-zero is defined as a settable parameter. By default this is chosen to \u22121 \u00d7 10\u221225.\noverhead (more detail on this is given in Sect. 3.3). Both sets of weighted events are analysed using the default MC_JETS Rivet routine [61]. Rivet produces binned differential cross sections as functions of various physical observables of the outgoing gluons. For each process, the total cross section for the NS sample is normalised to the HAAG sample, and a range of fine grained differential cross sections is calculated using both algorithms covering the following observables; \u03b7i , yi , pT,i , \u03c6i j , mi j , Ri j , \u03b7i j , where i = j label the final state jets, reconstructed using the anti-kT algorithm [62] with a radius parameter of R = 0.4 and pT > 30 GeV. The normalised difference between the NS and HAAG differential cross section in each bin can be computed as,\n\u03c7 = d\u03c3HAAG \u2212 d\u03c3NS\u221a 2HAAG + 2NS , (6)\nin effect this is the differences between the two algorithms normalised by the combined standard deviation. By summing up this \u03c7 deviation across all the available bins in each process, a test to see if the two algorithms are convergent within their quoted uncertainties can be performed. Since over 500 bins are populated and considered in each process, it is expected that the rate of these \u03c7 deviations should be approximately normally distributed. This indeed appears to hold, and these summed density estimates across all observables are shown in Fig. 4, alongside an overlaid normal distribution with mean zero and variance one,N (0, 1), to illustrate the expected outcome. Two example variables that were used to build this global deviation are also shown; the leading jet\npT in Fig. 5a and R12, the distance of the two leading jets in the (\u03b7, \u03c6) plane, in Fig. 5b.\nThe composition of the quoted uncertainty for the two algorithms differs, demonstrating an important feature of an NS calculation. For HAAG, and IS in general, it is conventional to quote the uncertainty as the standard error from the effective number of fills in a bin. Nested Sampling on the other hand introduces an uncertainty on the weights used to fill the histograms themselves, effectively giving rise to multiple weight histories that must be sampled to derive the correct uncertainty on the NS calculation. Details on this calculation are supplied in Appendix A. In summary the alternative weight histories give an overlapping measure of the statistical uncertainty, so this effect must be accounted for in situ alongside taking the standard deviation of the weight histories. To contextualise this, the middle panels in Fig. 5 show the correct combined uncertainty (using the recipe from Appendix A) as a grey band, against the bands derived from the standard error of each individual algorithm (henceforth MC) as dashed lines, and the complete NS error treatment as a dotted line. The standard error (dashed) NS band in these panels is a naive estimation of the full NS uncertainty (dotted), however this illustrates an important point; at the level of fine grained differential observables the NS uncertainty is dominated by statistics and is hence reducible as one would expect by repeated runs. Based on the example observables we can initially conclude that whilst both algorithms appear compatible, when using weighted events NS generally has a larger uncertainty than HAAG across most of the range (given a roughly equivalent computational overhead). However, further inspection of the resulting unweighted event samples\nderived from these weighted samples in the remaining sections reveals a more competitive picture between the two algorithms.\nThe estimates of the total cross sections, derived from the sum of weighted samples, provided in Table 4, give an alternative validation that NS is sufficiently exploring the phase space by ensuring that compatible estimates of the cross sections are produced between all the methods reviewed in this study. The central estimates of the total cross sections are generally consistent within the established error sources for all calculations considered. In this table the components of the error calculation for NS are listed separately; w being the standard deviation resulting from the alternative weight histories and MC being the standard error naively taken from the mean of the alternative NS weights. In contrast to the differential observables, the naive counting uncertainty is small so has negligible effect at the level of total cross sections. In summary, for a total cross section the spread of alternative weight histories gives a rough estimate of the total error, whereas for a fine grained differential cross section the standard error dominates. The way to correctly account for the effect of counting statistics within the weight histories is given in Appendix A.\nRepeated runs of NS will reduce these uncertainties. The anesthetic package [63] is used to analyse the NS runs throughout this paper, and contains a utility to join samples. Once samples are joined consistently into a larger sample, the uncertainties can be derived as already detailed. The result of joining 10 equivalent NS runs with the previously motivated hyperparameters is also listed in Table 4. Joining 10 runs affects the \u03c3tot for NS in two ways; reducing the spread of weighted sums composing w (i.e. reducing MC), and reducing the variance of distribution for each weight itself\n(i.e. the part of w that does not overlap with MC). The former is reduced by simply having an increased size of samples produced, increasing the number of effective fills by a factor of \u223c10 in this case, with the latter reduced due to the increased effective number of live points used for the volume estimation.\n3.3 Efficiency of event generation\nAn example particle physics workflow on this gluon scattering problem would be to take HAAG as an initial mapping of the phase space (effectively representing the best prior knowledge of the problem), and using VEGAS to refine the proposal distribution to optimally efficiently generate weighted events. Of the three existing tools presented in this study for comparison (HAAG, RAMBO, and VEGAS), NS bears most similarity to VEGAS, in that both algorithms learn the structure of the target integrand. To this end an atypical usage of VEGAS is employed, testing how well VEGAS could learn a proposal distribution from an uninformed starting point (RAMBO). This is equivalent to how NS was employed, starting from an uninformed prior (RAMBO) and generating posterior samples via Nested Sampling. It was motivated so far that roughly similar computational cost was used for the previous convergence checks, and that the hyperparameters of PolyChord were chosen to emphasise efficient generation of unweighted events. In what follows, we analyse more precisely this key issue of computational efficiency.\nThe statistics from a single run of the four algorithms for the three selected processes is listed in Table 5. NS is non deterministic in terms of number of matrix element evaluations (NL), instead terminating from a pre determined convergence criterion of the integral. HAAG, VEGAS, RAMBO\nare all used to generate exactly 10M weighted events. The chosen PolyChord hyperparameters roughly align the NS method with the other three in terms of computational cost. One striking difference comes from the Markov Chain nature of NS. Default usage only retains a fraction of the total L evaluations, inversely proportional to nrep. This results in a smaller number of retained weighted events, NW , than the number of L evaluations, NL, for NS. However the retained weighted events by construction match the underlying distribution much closer than the other methods, resulting in a higher unweighting efficiency, uw, for the NS sample. Exact equal-weight unweighting can be achieved by accepting events with a probability proportional to the share of the sample weight they carry, this operation is performed for all samples of weighted events and the number of retained events is quoted as Nequal. NS as an unweighted event generator has some additional complexity due to the uncertainty in the weights themselves, this is given more attention in Sect. 3.4.\nDue to differences in NL between NS and the other methods, it is most effective to compare the total efficiency in producing unweighted events, = Nequal/NL. RAMBO as the baseline illustrates the performance one would expect, inputting no prior knowledge and not adapting to any acquired knowledge. As such RAMBO yields a tiny . HAAG represents the performance using the best state of prior knowledge but without any adaptation, in these tests this represents the best attainable . VEGAS and NS start from a similar point, both using RAMBO as an uninformed state of prior knowledge, but adapting to better approximate the phase space distribution as information is acquired. VEGAS starts with a higher efficiency than NS for the 3-gluon process, but the VEGAS efficiency drops by approximately an\norder of magnitude as the dimensionality of phase space is increased to the 5-gluon process. NS maintains a consistent efficiency of approximately a percent, competitive with the consistent approximately three percent efficiency obtained by HAAG.\nAs the key point of comparison for this issue is the efficiency, , this is highlighted with an additional visualisation in Fig. 6. The scaling behavior of the efficiency of each algorithm as a function of the number of outgoing gluons (corresponding to an increase in phase space dimensionality) is plotted for NS, HAAG and VEGAS. From the same starting point, NS and VEGAS can both learn a representation of the phase space, and do so in a way that yields a comparable efficiency to the static best available prior knowledge in HAAG. As the dimensionality of the space increases it appears that VEGAS starts to suffer in how accurately it can learn the mapping, however NS is still able to learn the mapping in a consistently efficient manner.\n3.4 Unweighted event generation\nThe fact that NS leads to a set of alternative weight histories poses a technical challenge in operating as a generator of unweighted events in the expected manner. Exact unweighting, compressing the weighted sample to strictly equally weighted events leads to a different set of events being accepted for each weight history. Representative yields of unweighted events can be calculated as shown in Table 5 using the mean weight for each event, but the resulting differential distributions will underestimate the uncertainty if this is quoted simply as the standard error in the bin, as described in Appendix A. The correct uncertainty recipe can be propagated through naively, by separately unweight-\ning each weight history, however this requires saving as many event samples as required weight variations. Partial unweighting is commonly used in HEP event generation to allow a slight deviation from strict unit weights, to increase efficiency in practical settings. A modification to the partial unweighting procedure could be used to propagate the spread of weights to variations around accepted, approximate unit weight, events.\nTo conclude the exploration of the properties of NS as a generator for particle physics, a representative physical distribution calculated from a sample of exact unit-weight events is shown in Fig. 7. This sample is derived from the same weighted sample described in Table 5 and previously presented as a weighted event sample in Fig. 5a. The full set of NS variation weights is used to calculate the mean weight for each event, which is used to unweight the sample, for the chosen observable this is a very reasonable approximation as the fine binning means the standard error is the dominant uncertainty. The range of the leading jet transverse momenta has been extended into the tail of this distribution by modifying the default Rivet routine. This distribution largely reflects the information about the total efficiency previously illustrated in Fig. 6, projected onto a familiar differential observable. The total efficiency, , was noted as being approximately one percent from NS, compared to approximately three percent from HAAG across all processes. If the total number of matrix element evaluations, NL, were to be made equal across all algorithms and processes, the performance would be further consistent."
        },
        {
            "heading": "4 Future research directions",
            "text": "Throughout Sect. 3, the performance of Nested Sampling in the context of particle physics phase space sampling and event generation was presented. A single choice of hyperpa-\nrameters was made, effectively performing a single NS run as an entire end-to-end event generator; starting from zero knowledge of the phase space all the way through to generating unweighted events. Simplifying the potential options of NS to a single version of the algorithm was a deliberate choice to more clearly illustrate the performance of NS in this new context, using the same settings for multiple tasks gives multiple orthogonal views on how the algorithm performs. However this was a limiting choice, NS has a number of variants and applications that could more effectively be tuned to a subset of the tasks presented. Some of the possible simple alterations \u2013 such as increasing nprior to improve weighted event generation at the expense of unweighting efficiency \u2013 were motivated already in this paper. In this section we outline four broad topics that extend the workflow presented here, bringing together further ideas from the worlds of Nested Sampling and phase space exploration.\n4.1 Physics challenges in event generation\nThe physical processes studied in this work, up to 5-gluon scattering problems, are representative of the complexity of phase space calculation needed for the current precision demands of the LHC experiment collaborations [64]. However part of the motivation for this work, and indeed the broader increased interest in phase space integration methods, is due to the impending breaking point current pipelines face under the increased precision challenges of the HL-LHC programme. Firstly we observe that the phase space dimensionality of the highest multiplicity process studied here is 11. In broader Bayesian inference terms this is rather small, with NS being typically used for problemsO (10) toO (100) dimensions, where it is uniquely able to perform numerical integration without approximation or strictly matching prior knowledge. The PolyChord implementation is styled as next-generation Nested Sampling, designed to have polynomial scaling with dimensionality aiming for robust performance as inference is extended to O (100) dimensions. Earlier implementations of NS, such as MultiNest [46], whilst having worse dimensional scaling properties, may be a useful avenue of investigation for the lower dimensional problems considered in this paper.\nThis work validated NS in a context where current tools still can perform the required tasks, albeit at times at immense computational costs. Requirements from the HL-LHC strain the existing LHC event generation pipeline in many ways and pushing the sampling problem to higher dimensions is no exception [2]. Importance Sampling becomes exponentially more sensitive to how close the proposal distribution matches the target in higher dimensions, a clear challenge for particle physics in two directions; multileg processes rapidly increasing the sampling dimension [65] and corresponding radiative corrections (real and virtual) make it increasingly\nhard to provide an accurate proposal, e.g. through the sheer number of phase space channels needed and by having to probe deep into singular phase space regions [66]. We propose that NS is an excellent complement to further investigation on both these fronts. The robust dimensional scaling of NS illustrated against VEGAS in Fig. 6 encapsulates both solid performance with increasing dimension, and the adherence to an uninformed prior whilst still attaining this scaling is promising for scenarios where accurate proposals are harder to construct.\n4.2 Using prior knowledge\nPerhaps the most obvious choice that makes the application here stylised is in always starting from an uninformed prior state of knowledge. Using Equations (2) and (4), the cross section integral with a phase space mapping was motivated as being exactly the Bayesian evidence integral with a choice of prior. To that end there is no real distinction between taking the non-uniform HAAG distribution as the prior instead of the flat RAMBO density that was used in this study. In this respect NS could be styled as learning an additional compression to the posterior distribution, refining the static proposal distributions typically employed to initiate the generation of a phase space mapping (noting that this is precisely what VEGAS aims to do in this context).\nNaively applying a non-flat mapping exposes the conflicting aims at play in this set of problems however; efficiently generating events from a strongly peaked distribution, and generating high statistics estimates of the tails of the same distribution. Taking a flat RAMBO prior is well suited to the latter problem, whereas taking a HAAG prior is better suited to the former. One particular hyperparameter of PolyChord that was introduced can be tuned to this purpose; the number of prior samples, nprior. If future work is to use a non flat, partially informed starting point, increasing nprior well above the minimum (equal to the number of live points required) used in this study would be needed. A more complete direction for further work would be to investigate the possibility of mixing multiple proposal distributions [67,68].\nAs a demonstration, we again apply NS to the toy example of Sect. 2.1 but this time using a non-uniform prior distribution. While a good prior would be an approximation of the target distribution, we choose to purposely miss an important feature of the target, the straight line segment, that the sampler still has to explore. Considering that in HEP applications the prior knowledge may be encoded in the mixture distributions of a multi-channel importance sampler, this is an extreme version of a realistic situation. As typically the number of channels grows dramatically with increasing finalstate particle multiplicity, e.g. factorially when channels correspond to the topologies of contributing Feynman diagrams, one might choose to disable some sub-dominant channels in\norder to avoid a prohibitively large set of channels. However, this would lead to a mis-modelling of the target in certain phase-space regions.\nHere we use only the ring part of the target, truncated on a circle that covers the unit hypercube, as our prior. Without an additional coordinate transformation this prior would not be of much use for VEGAS as the line part remains on the diagonal. To sample from the prior, we first transform to polar coordinates. Then we sample the angle uniformly and the radial coordinate using a Cauchy distribution truncated to the interval (0, 1/ \u221a 2]. In order to have good coverage of the tails, despite the strongly peaked prior, we increase nprior to 50 \u00d7 nlive. This results in a total efficiency of = 0.037(4), more than three times the value obtained with a uniform prior, cf. Table 1. While the unweighting efficiency reduces to uw = 0.17(2), the slice sampling efficiency increases to ss = 0.216(7). In Fig. 8 we show the ratio between the target function and the PolyChord sampling distribution. Compared to Fig. 2c, the ratio has a smaller range of values. Along the peak of the ring part of the target function, the ratio is approximately one. The largest values can be found around the line segment with PolyChord generating up to ten times less samples than required by the target distribution. It can be concluded that even with an intentionally poor prior distribution, PolyChord benefits from the prior knowledge in terms of efficiency and still correctly samples the target distribution including the features absent from the prior.\n4.3 Dynamic nested sampling\nIn addition to using a more informed prior to initiate the Nested Sampling process, a previous NS run can be used to further tune the algorithm itself to a particular problem. This is an existing idea in the literature known as dynamic Nested Sampling [69]. Dynamic NS uses information acquired about the likelihood shells in a previous NS run to varying the number of live points dynamically throughout the run. This results in a more efficient allocation of the computation towards the core aim of compressing the prior to the posterior. We expect that this would only increase the efficiency of the unweighting process, as the density of weighted events would be trimmed to even more closely match the underlying phase space density. Dynamic Nested Sampling naturally combines with the proposal of using prior knowledge to make a more familiar generator chain, however one that is driven primarily by NS. This mirrors the current established usage of VEGAS in this context; using VEGAS to refine the initial mapping by a redistribution of the input variables, to more efficiently generate from the acquired mapping.\n4.4 Connection to modern machine learning techniques\nThere has been a great deal of recent activity coincident to this work, approaching similar sets of problems in particle physics event generation using modern Machine Learning (ML) techniques [70]. Much of this work is still exploratory in nature, and covers such a broad range of activity that comprehensively reviewing the potential for combining ML and NS is beyond the scope of this work. It is however clear that there is strong potential to include NS into a pipeline that modern ML is already aiming to optimise. To that aim, we identify a particular technique that has been studied previously in the particle physics context; using Normalising Flows to train phase space mappings [29\u201331]. In spirit a flow based approach, training an invertible probabilistic mapping between prior and posterior, bears a great deal of similarity to the core compression idea behind Nested Sampling. The potential in dovetailing Nested Sampling with a flow based approach has been noted in the NS literature [71], further motivating the potential for synergy here.\nThe ability of NS to construct mappings of high dimensional phase spaces without needing any strong prior knowledge, can be motivated as being an ideal forward model with\nwhich to train a Normalising Flow. In effect this replaces the generator part of the process with an importance sampler, whilst still using NS to generate the mappings. This is particularly ideal in this context, as the computational overhead required to decorrelate the Markov Chains imposes a harsh limit on the efficiency of a pure NS based approach. Combining these techniques in this way could retain the desirable features of both and serve to mitigate the ever increasing computational demands of energy frontier particle physics.\nWe close by noting that also in the area of lattice field theory Normalising Flows have recently attracted attention, see e.g. [72,73], to address the sampling of multimodal target function. We envisage that also in these applications Nested Sampling could be applied."
        },
        {
            "heading": "5 Conclusions",
            "text": "The establishing study presented here had two main aims. Firstly to introduce the technique of Nested Sampling, applied to a realistic problem, to researchers in the particle physics community. Secondly to provide a translation back to researchers working on Bayesian inference techniques, presenting an important and active set of problems in particle physics that Nested Sampling could provide a valuable contribution to. The physical example presented used PolyChord to perform an end-to-end generation of events without any input prior knowledge. This is a stylised version of the event generator problem, intended to validate Nested Sampling in this new context and demonstrate some key features. For the considered multi-gluon production processes Nested Sampling was able to learn a mapping in an efficient manner that exhibits promising scaling properties with phase space dimension. We have outlined some potential future research directions; highlighting where the strengths of this approach could be most effective, and how to embed Nested Sampling in a more complete event generator workflow. Along these lines, we envisage an implementation of the Nested Sampling technique for the SHERPA event generator framework [54], possibly also supporting operation on GPUs [74]. This will provide additional means to address the computing challenges for event generation posed by the upcoming LHC runs.\nAcknowledgements This work has received funding from the European Union\u2019s Horizon 2020 research and innovation programme as part of the Marie Sk\u0142odowska-Curie Innovative Training Network MCnetITN3 (Grant agreement no. 722104). SS and TJ acknowledge support from BMBF (contract 05H21MGCAB). SS acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - project number 456104544. WH and DY are funded by the Royal Society. This work was performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (www.csd3.cam.ac.uk), provided by Dell EMC and Intel using Tier-2\nfunding from the Engineering and Physical Sciences Research Council (capital Grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk).\nDataAvailability Statement This manuscript has no associated data or the data will not be deposited. [Authors\u2019 comment: The results involve purely theoretical comparisons, publicly available software packages to recreate the calculations are listed and referenced where appropriate.]\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/. Funded by SCOAP3. SCOAP3 supports the goals of the International Year of Basic Sciences for Sustainable Development."
        },
        {
            "heading": "Appendix: Uncertainties in nested sampling",
            "text": "Typical Nested Sampling literature focuses on two main sources of uncertainty; an uncertainty on the weights of the dead points due to the uncertainty in the volume contraction at each iteration, and an uncertainty on the overall volume arising from the path the Markov Chain takes through the space to perform each iteration. The former source is what we consider in this work, and can be calculated as a sample of weights for each dead point using anesthetic. The latter source can be estimated using the nestcheck package [75], the method presented here uses combinations of multiple runs to form integral estimates meaning the best strategy to minimise this effect is already baked in. Further use cases would benefit from more thorough cross checks using nestcheck.\nThe usual source of uncertainty in a binned histogram in particle physics comes from the standard error. Importance Sampling draws sample events with associated weights wi , with the sum of these sample weights giving the estimated cross section in a bin. The effective number of fills in a bin using weighted samples is,\nN = ( \u2211 i wi )2\n\u2211 i w 2 i . (7)\nThe inverse square root of N then constitutes the standard error on the cross section in the bin. In practice this means that the standard deviation of an integral estimated with Impor-\ntance Sampling can be quoted as MC = \u221a\u2211 i (w 2 i ). In typical NS applications this is significantly smaller than the pre-\nviously mentioned sources, and thus often not considered. However, when using NS as a phase space event generator for finely binned differential observables, the statistical uncertainty can become a significant effect so must be taken into account. Adding the standard error to the weight uncertainty in quadrature is a suitable upper bound for the NS uncertainty but is found to overestimate the uncertainty in some bins. While the standard error gives a measure of the spread of weights around the mean weight in a bin, alternative weights from the sampling history in NS also give an overlapping measure of this.\nTo correctly account for the statistical error in this context a revised recipe is needed. The following proposed procedure reweights the alternative weight samples to account for the spread of the resulting effective fills in each bin. The effective number of entries in a bin arising from a NS run can be written as,\nN j = ( \u2211 i w j,i )2\n\u2211 i w 2 j,i , (8)\nwhere i indexes the number of weighted samples in each bin, and j indexes the alternative weights. The result of the j sampled weight variations is a set of j different effective counts in each bin. These counts can be modelled as j trials of a multinomial distribution with j categories, written as,\nP(N | \u03b1) = j !\u220f j N j ! \u220f j \u03b1 N j j , (9)\nwhere a probability of sampling each category, \u03b1 j , has been introduced. The desired unknown distribution of \u03b1 j can be found using Bayes theorem to invert the arguments. If an uninformative conjugate prior to the multinomial distribution is used, the Dirichlet distribution, the desired inverted probability can also be written in the form of a Dirichlet distribution, P(\u03b1 | N ) = \u0393 \u239b \u239d\u2211\nj\nN j\n\u239e \u23a0 \u220f\nj\n\u03b1 N j\u22121 j \u0393 (N j ) . (10)\nA sample vector of \u03b1 j from this Dirichlet distribution, will give a probability of observing each category N j . This probability can be used to weight the categories giving a weighted set of effective number of fills, {\u03b1 j N j }. This considers each alternative weight sample as a discrete sample from an underlying continuous distribution N j is sampled from. The set of weighted effective fills can be used to quote a weighted set of samples of the bin cross section by multiplying by the square of the sum of the weights, {\u03c3 j } = {\u03b1 j N j \u2211i (w2j,i )}. The estimated cross section in the bin is then the expected value of this\nset, \u03c3 = E[\u03c3 j ], and the total standard deviation on this cross section is derived from the variance, \u03c3tot = (Var[\u03c3 j ])2."
        }
    ],
    "title": "Exploring phase space with nested sampling",
    "year": 2022
}