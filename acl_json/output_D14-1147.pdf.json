{
    "abstractText": "We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. Different to previous character tagging methods, we introduce the minimum semantic unit, which is more fine-grained than character but more coarse-grained than word, to capture word level information in the sequence labeling framework. To solve the \u201ccharacter duplication\u201d problem in Chinese abbreviation prediction, we also use a substring tagging strategy to generate local substring tagging candidates. We use an integer linear programming (ILP) formulation with various constraints to globally decode the final abbreviation from the generated candidates. Experiments show that our method outperforms the state-of-the-art systems, without using any extra resource.",
    "authors": [
        {
            "affiliations": [],
            "name": "Longkai Zhang"
        },
        {
            "affiliations": [],
            "name": "Li Li"
        },
        {
            "affiliations": [],
            "name": "Houfeng Wang"
        },
        {
            "affiliations": [],
            "name": "Xu Sun"
        }
    ],
    "id": "SP:0072d01a7cc49113e348db4539f36d35ff8d6c26",
    "references": [
        {
            "authors": [
                "E. Adar"
            ],
            "title": "Sarad: A simple and robust abbreviation dictionary",
            "venue": "Bioinformatics, 20(4):527\u2013533.",
            "year": 2004
        },
        {
            "authors": [
                "H. Ao",
                "T. Takagi"
            ],
            "title": "Alice: an algorithm to extract abbreviations from medline",
            "venue": "Journal of the American Medical Informatics Association, 12(5):576\u2013586.",
            "year": 2005
        },
        {
            "authors": [
                "J. Barrett",
                "M. Grems"
            ],
            "title": "Abbreviating words systematically",
            "venue": "Communications of the ACM, 3(5):323\u2013324.",
            "year": 1960
        },
        {
            "authors": [
                "C. Bourne",
                "D. Ford"
            ],
            "title": "A study of methods for systematically abbreviating english words and names",
            "venue": "Journal of the ACM (JACM), 8(4):538\u2013552.",
            "year": 1961
        },
        {
            "authors": [
                "Y. HaCohen-Kerner",
                "A. Kass",
                "A. Peretz"
            ],
            "title": "Combined one sense disambiguation of abbreviations",
            "venue": "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Tech-",
            "year": 2008
        },
        {
            "authors": [
                "M.S. Hearst"
            ],
            "title": "A simple algorithm for identifying abbreviation definitions in biomedical text",
            "year": 2003
        },
        {
            "authors": [
                "A. Jain",
                "S. Cucerzan",
                "S. Azzam"
            ],
            "title": "Acronym-expansion recognition and ranking on the web",
            "venue": "Information Reuse and Integration, 2007. IRI 2007. IEEE International Conference on, pages 209\u2013214. IEEE.",
            "year": 2007
        },
        {
            "authors": [
                "A.F. Martins",
                "N.A. Smith",
                "E.P. Xing"
            ],
            "title": "Concise integer linear programming formulations for dependency parsing",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th Interna-",
            "year": 2009
        },
        {
            "authors": [
                "Y. Park",
                "R. Byrd"
            ],
            "title": "Hybrid text mining for finding abbreviations and their definitions",
            "venue": "Proceedings of the 2001 conference on empirical methods in natural language processing, pages 126\u2013133.",
            "year": 2001
        },
        {
            "authors": [
                "V. Punyakanok",
                "D. Roth",
                "Yih",
                "W.-t.",
                "D. Zimak"
            ],
            "title": "Semantic role labeling via integer linear programming inference",
            "venue": "Proceedings of the 20th international conference on Compu-",
            "year": 2004
        },
        {
            "authors": [
                "S. Riedel",
                "J. Clarke"
            ],
            "title": "Incremental integer linear programming for non-projective dependency parsing",
            "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 129\u2013137. Associ-",
            "year": 2006
        },
        {
            "authors": [
                "D. Roth",
                "Yih",
                "W.-t."
            ],
            "title": "Integer linear programming inference for conditional random fields",
            "venue": "Proceedings of the 22nd international conference on Machine learning, pages 736\u2013 743. ACM.",
            "year": 2005
        },
        {
            "authors": [
                "X. Sun",
                "W. Li",
                "F. Meng",
                "H. Wang"
            ],
            "title": "Generalized abbreviation prediction with negative full forms and its application on improving chinese web search",
            "venue": "Proceedings of the Sixth International Joint Conference on Natural",
            "year": 2013
        },
        {
            "authors": [
                "X. Sun",
                "N. Okazaki",
                "J. Tsujii"
            ],
            "title": "Robust approach to abbreviating terms: A discriminative latent variable model with global information",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and",
            "year": 2009
        },
        {
            "authors": [
                "X. Sun",
                "H. Wang",
                "B. Wang"
            ],
            "title": "Predicting chinese abbreviations from definitions: An empirical learning approach using support vector regression",
            "venue": "Journal of Computer Science and Technology, 23(4):602\u2013611.",
            "year": 2008
        },
        {
            "authors": [
                "K. Taghva",
                "J. Gilbreth"
            ],
            "title": "Recognizing acronyms and their definitions",
            "venue": "International Journal on Document Analysis and Recognition, 1(4):191\u2013198.",
            "year": 1999
        },
        {
            "authors": [
                "Y. Tsuruoka",
                "S. Ananiadou",
                "J. Tsujii"
            ],
            "title": "A machine learning approach to acronym generation",
            "venue": "Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining Biological Seman-",
            "year": 2005
        },
        {
            "authors": [
                "J. Wren",
                "H Garner"
            ],
            "title": "Heuristics for identification of acronym-definition patterns within text: towards an automated construc",
            "year": 2002
        },
        {
            "authors": [
                "H. Yu",
                "W. Kim",
                "V. Hatzivassiloglou",
                "J. Wilbur"
            ],
            "title": "A large scale, corpus-based approach for automatically disambiguating biomedical abbreviations",
            "venue": "ACM Transactions on Information Systems (TOIS), 24(3):380\u2013404.",
            "year": 2006
        },
        {
            "authors": [
                "L. Zhang",
                "S. Li",
                "H. Wang",
                "N. Sun",
                "X. Meng"
            ],
            "title": "Constructing Chinese abbreviation dictionary: A stacked approach",
            "venue": "Proceedings of COLING 2012, pages 3055\u20133070, Mumbai, India. The COLING 2012 Organizing Commit-",
            "year": 2012
        },
        {
            "authors": [
                "Q. Zhao",
                "M. Marcus"
            ],
            "title": "Exploring deterministic constraints: from a constrained english pos tagger to an efficient ilp solution to chinese word segmentation",
            "venue": "Proceedings of the 50th Annual Meeting of the Association for Compu-",
            "year": 2012
        }
    ],
    "sections": [
        {
            "text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1405\u20131414, October 25-29, 2014, Doha, Qatar. c\u00a92014 Association for Computational Linguistics"
        },
        {
            "heading": "1 Introduction",
            "text": "Abbreviation is defined as a shortened description of the original fully expanded form. For example, \u201cNLP\u201d is the abbreviation for the corresponding full form \u201cNatural Language Processing\u201d. The existence of abbreviations makes it difficult to identify the terms conveying the same concept in the information retrieval (IR) systems and machine translation (MT) systems. Therefore, it is important to maintain a dictionary of the prevalent original full forms and the corresponding abbreviations.\nPrevious works on Chinese abbreviation generation focus on the sequence labeling method, which give each character in the full form an extra label to indicate whether it is kept in the abbreviation. One drawback of the character tagging strategy is that Chinese characters only contain\nlimited amount of information. Using characterbased method alone is not enough for Chinese abbreviation generation. Intuitively we can think of a word as the basic tagging unit to incorporate more information. However, if the basic tagging unit is word, we need to design lots of tags to represent which characters are kept for each unit. For a word with n characters, we should design at least 2n labels to cover all possible situations. This reduces the generalization ability of the proposed model. Besides, the Chinese word segmentation errors may also hurt the performance. Therefore we propose the idea of \u201cMinimum Semantic Unit\u201d (MSU) which is the minimum semantic unit in Chinese language. Some of the MSUs are words, while others are more fine-grained than words. The task of selecting representative characters in the full form can be further broken down into selecting representative characters in the MSUs. We model this using the MSU-based tagging method, which can both utilize semantic information while keeping the tag set small.\nMeanwhile, the sequence labeling method performs badly when the \u201ccharacter duplication\u201d phenomenon exists. Many Chinese long phrases contain duplicated characters, which we refer to as the \u201ccharacter duplication\u201d phenomenon. There is no sound criterion for the character tagging models to decide which of the duplicated character should be kept in the abbreviation and which one to be skipped. An example is \u201c\u5317\u4eac\u822a\u7a7a\u822a\u5929\u5927 \u5b66\u201d(Beijing University of Aeronautics and Astronautics) whose abbreviation is \u201c\u5317\u822a\u201d. The character \u201c\u822a\u201d appears twice in the full form and only one is kept in the abbreviation. In these cases, we can break the long phase into local substrings. We can find the representative characters in the substrings instead of the long full form and let the decoding phase to integrate useful information globally. We utilize this sub-string based approach and obtain this local tagging information by labeling\n1405\non the sub-string of the full character sequence. Given the MSU-based and substring-based methods mentioned above, we can get a list of potential abbreviation candidates. Some of these candidates may not agree on keeping or skipping of some specific characters. To integrate their advantages while considering the consistency, we further propose a global decoding strategy using Integer Linear Programming(ILP). The constraints in ILP can naturally incorporate \u2018non-local\u2019 information in contrast to probabilistic constraints that are estimated from training examples. We can also use linguistic constraints like \u201cadjacent identical characters is not allowed\u201d to decode the correct abbreviation in examples like the previous \u201c\u5317\u822a\u201d example.\nExperiments show that our Chinese abbreviation prediction system outperforms the state-ofthe-art systems. In order to reduce the size of the search space, we further propose pruning constraints that are learnt from the training corpus. Experiment shows that the average number of constraints is reduced by about 30%, while the top-1 accuracy is not affected.\nThe paper is structured as follows. Section 1 gives the introduction. In section 2 we describe our method, including the MSUs, the substringbased tagging strategy and the ILP decoding process. Experiments are described in section 3. We also give a detailed analysis of the results in section 3. In section 4 related works are introduced, and the paper is concluded in the last section."
        },
        {
            "heading": "2 System Architecture",
            "text": ""
        },
        {
            "heading": "2.1 Chinese Abbreviation Prediction",
            "text": "Chinese abbreviations are generated by selecting representative characters from the full forms. For example, the abbreviation of \u201c\u5317\u4eac\u5927\u5b66\u201d (Peking University) is \u201c\u5317\u5927\u201d which is generated by selecting the first and third characters, see TABLE 1. This can be tackled from the sequence labeling point of view.\nFrom TABLE 1 we can see that Chinese abbreviation prediction is a problem of selecting repre-\nsentative characters from the original full form1. Based on this assumption, previous works mainly focus on this character tagging schema. In these methods, the basic tagging unit is the Chinese character. Each character in the full form is labeled as \u2018K\u2019 or \u2018S\u2019, where \u2018K\u2019 means the current character should be kept in abbreviation and \u2018S\u2019 means the current character should be skipped.\nHowever, a Chinese character can only contain limited amount of information. Using characterbased method alone is not enough for Chinese abbreviation generation. We introduce an MSUbased method, which models the process of selecting representative characters given local MSU information."
        },
        {
            "heading": "2.2 MSU Based Tagging",
            "text": ""
        },
        {
            "heading": "2.2.1 Minimum Semantic Unit",
            "text": "Because using the character-based method is not enough for Chinese abbreviation generation, we may think of word as the basic tagging unit to incorporate more information intuitively. In English, the abbreviations (similar to acronyms) are usually formed by concatenating initial letters or parts of a series of words. In other words, English abbreviation generation is based on words in the full form. However, in Chinese, word is not the most suitable abbreviating unit. Firstly, there is no natural boundary between Chinese words. Errors from the Chinese word segmentation tools will accumulate to harm the performance of abbreviation prediction. Second, it is hard to design a reasonable tag set when the length of a possible Chinese word is very long. The second column of TABLE 2 shows different ways of selecting representative characters of Chinese words with length 3. For a Chinese compound word with 3 characters, there are 6 possible ways to select characters. In this case we should have at least 6 kinds of tags to cover all possible situations. The case is even worse for words with more complicated structures. A suitable abbreviating unit should be smaller than word.\nWe propose the \u201cMinimum Semantic Unit (MSU)\u201d as the basic tagging unit. We define MSU as follows:\n1. A word whose length is less or equal to 2 is an MSU.\n1A small portion of Chinese abbreviations are not generated from the full form. For example, the abbreviation of \u201c\u5c71 \u4e1c\u201d(Shan Dong Province) is \u201c\u9c81\u201d. However, we can use a look-up table to get this kind of abbreviations.\n2. A word whose length is larger than 2, but does not contain any MSUs with length equal to 2. For example, \u201c\u706b\u8f66\u7ad9\u201d(Railway Station) is not an MSU because the first two characters \u201c\u706b\u8f66\u201d(Train) can form an MSU.\nBy this definition, all 6 strings in TABLE 2 are often thought as a word, but they are not MSUs in our view. Their corresponding MSU forms are shown in TABLE 2.\nWe collect all the MSUs from the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff2. We choose the Peking University (PKU) data because it is more fine-grained than all other corpora. Suppose we represent the segmented data as L (In our case L is the PKU word segmentation data), the MSU selecting algorithm is shown in TABLE 3.\nFor a given full form, we first segment it using a standard word segmenter to get a coarsegrained segmentation result. Here we use the Stanford Chinese Word Segmenter 3. Then we use the MSU set to segment each word using the strategy of \u201cMaximum Forward Matching\u201d4 to get the finegrained MSU segmentation result."
        },
        {
            "heading": "2.2.2 Labeling strategy",
            "text": "For MSU-based tagging, we use a labeling method which uses four tags, \u201cKSFL\u201d. \u201cK\u201d stands for \u201cKeep the whole unit\u201d, \u201cS\u201d stands for \u201cSkip the whole unit\u201d, \u201cF\u201d stands for \u201ckeep the First character of the unit\u201d, and Label \u201cL\u201d stands for \u201ckeep the Last character of the unit\u201d. An example is shown in TABLE 4.\nThe \u201cKSFL\u201d tag set is also applicable for MSUs whose length is greater than 2 (an example is \u201c\u5de7 \u514b\u529b/chocolate\u201d). By examining the corpus we find that such MSUs are either kept of skipped in\n2http://www.sighan.org/bakeoff2005/ 3http://nlp.stanford.edu/software/\nsegmenter.shtml 4In Chinese, \u201cForward\u201d means from left to right.\nthe final abbreviations. Therefore, the labels of these long MSUs are either \u2018K\u2019 or \u2018S\u2019. Empirically, this assumption holds for MSUs, but does not hold for words5."
        },
        {
            "heading": "2.2.3 Feature templates",
            "text": "The feature templates we use are as follows. See TABLE 5.\nTemplates 1, 2 and 3 express word uni-grams and bi-grams. In MSU-based tagging, we can utilize the POS information, which we get from the Stanford Chinese POS Tagger6. In template 4, the type of word refers to whether it is a number, an English word or a Chinese word. Because the basic tagging unit is MSU, which carries word information, we can use many features that are infeasible in character-based tagging.\n5In table 2, all examples are partly kept. 6http://nlp.stanford.edu/software/\ntagger.shtml"
        },
        {
            "heading": "2.2.4 Sequence Labeling Model",
            "text": "The MSU-based method gives each MSU an extra indicative label. Therefore any sequence labeling model is appropriate for the method. Previous works showed that Conditional Random Fields (CRFs) can outperform other sequence labeling models like MEMMs in abbreviation generation tasks (Sun et al., 2009; Tsuruoka et al., 2005). For this reason we choose CRFs model in our system.\nFor a given full form\u2019s MSU list, many candidate abbreviations are generated by choosing the k-best results of the CRFs. We can use the forward-backward algorithm to calculate the probability of a specified tagging result. To reduce the searching complexity in the ILP decoding process, we delete those candidate tagged sequences with low probability."
        },
        {
            "heading": "2.3 Substring Based Tagging",
            "text": "As mentioned in the introduction, the sequence labeling method, no matter character-based or MSU-based, perform badly when the \u201ccharacter duplication\u201d phenomenon exists. When the full form contains duplicated characters, there is no sound criterion for the sequence tagging strategy to decide which of the duplicated character should be kept in the abbreviation and which one to be skipped. On the other hand, we can tag the substrings of the full form to find the local representative characters in the substrings of the long full form. Therefore, we propose the sub-string based approach to given labeling results on sub-strings. These results can be integrated into a more accurate result using ILP constraints, which we will describe in the next section.\nAnother reason for using the sub-string based methods is that long full forms contain more characters and are much easier to make mistakes during the sequence labeling phase. Zhang et al. (2012) shows that if the full form contains less than 5 characters, a simple tagger can reach an accuracy of 70%. Zhang et al. (2012) also shows that if the full form is longer than 10 characters, the average accuracy is less than 30%. The numerous potential candidates make it hard for the tagger to choose the correct one. For the long full forms, although the whole sequence is not correctly labeled, we find that if we only consider its short substrings, we may find the correct representative characters. This information can be integrated into the decoding model to adjust the final result.\nWe use the MSU-based tagging method in the sub-string tagging. The labeling strategy and feature templates are the same to the MSU-based tagging method. In practice, enumerating all subsequences of a given full form is infeasible if the full form is very long. For a given full form, we use the boundary MSUs to reduce the possible sub-sequence set. For example, \u201c\u4e2d\u56fd\u79d1 \u5b66\u9662\u201d(Chinese Academy of Science) has 5 subsequences: \u201c\u4e2d\u56fd\u201d, \u201c\u4e2d\u56fd\u79d1\u5b66\u201d, \u201c\u79d1\u5b66\u201d, \u201c\u79d1\u5b66 \u9662\u201d and \u201c\u9662\u201d."
        },
        {
            "heading": "2.4 ILP Formulation of Decoding",
            "text": "Given the MSU-based and sub-sequence-based methods mentioned above as well as the prevalent character-based methods, we can get a list of potential abbreviation candidates and abbreviated substrings. We should integrate their advantages while keeping the consistency between each\ncandidate. Therefore we further propose a global decoding strategy using Integer Linear Programming(ILP). The constraints in ILP can naturally incorporate \u2019non-local\u2019 information in contrast to probabilistic constraints that are estimated from training examples. We can also use linguistic constraints like \u201cadjacent identical characters is not allowed\u201d to decode the correct abbreviation in examples like the \u201c\u5317\u822a\u201d example in section 1.\nFormally, given the character sequence of the full form c = c1...cl, we keep Q top-ranked MSU-based tagging results T=(T1, ..., TQ) and M tagged substrings S=(S1, ..., SM ) using the methods described in previous sections. We also use N top-ranked character-based tagging results R=(R1, ..., RN ) based on the previous characterbased works. We also define the setU = S\u222aR\u222aT as the union of all candidate sequences. Our goal is to find an optimal binary variable vector solution ~v = ~x~y~z = (x1, ..., xM , y1, ..., yN , z1, ..., zQ) that maximizes the object function:\n\u03bb1 M\u2211 i=1 score(Si) \u00b7 xi + \u03bb2 N\u2211 i=1 score(Ri) \u00b7 yi\n+\u03bb3 Q\u2211\ni=1\nscore(Ti) \u00b7 zi\nsubject to constrains in TABLE 6. The parameters \u03bb1, \u03bb2, \u03bb3 controls the preference of the three parts, and can be decided using cross-validation.\nConstraint 1 indicates that xi, yi, zi are all boolean variables. They are used as indicator variables to show whether the corresponding tagged sequence is in accordance with the final result.\nConstraint 2 is used to guarantee that at most one candidate from the character-based tagging is preserved. We relax the constraint to allow the sum to be zero in case that none of the top-ranked candidate is suitable to be the final result. If the sum equals zero, then the sub-sequence based tagging method will generate a more suitable result. Constrain 3 has the same utility for the MSUbased tagging.\nConstraint 4, 5, 6 are inter-method constraints. We use them to guarantee that the labels of the preserved sequences of different tagging methods do not conflict with each other. Constraint 7 is used to guarantee that the labels of the preserved sub-strings do not conflict with each other.\nConstraint 8 is used to solve the \u201ccharacter duplicate\u201d problem. When two identical characters\nare kept adjacently, only one of them will be kept. Which one will be kept depends on the global decoding score. This is the advantage of ILP against traditional sequence labeling methods."
        },
        {
            "heading": "2.5 Pruning Constraints",
            "text": "The efficiency of solving the ILP decoding problem depends on the number of candidate tagging sequences N and Q, as well as the number of subsequences M. Usually, N and Q is less than 10 in our experiment. Therefore, M influences the time complexity the most. Because we use the boundary of MSUs instead of enumerating all possible subsequences, the value of M can be largely reduced.\nSome characters are always labeled as \u201cS\u201d or \u201cK\u201d once the context is given. We can use this phenomenon to reduce the search space of decoding. Let ci denote the ith character relative to the current character c0 and ti denote the tag of ci. The context templates we use are listed in TABLE 7.\nWith respect to a training corpus, if a context C relative to c0 always assigns a certain tag t to c0, then we can use this constraint in pruning. We judge the degree of \u201calways\u201d by checking whether count(C\u2227t0=t)\ncount(C) > threshold. The threshold is a non-negative real number under 1.0."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Data and Evaluation Metric",
            "text": "We use the abbreviation corpus provided by Institute of Computational Linguistics (ICL) of Peking University in our experiments. The corpus is similar to the corpus used in Sun et al. (2008, 2009); Zhang et al. (2012). It contains 8, 015 Chinese abbreviations, including noun phrases, organization names and some other types. Some examples are presented in TABLE 8. We use 80% abbreviations as training data and the rest as testing data. In some cases, a long phrase may contain more than one abbreviation. For these cases, the corpus just keeps their most commonly used abbreviation for each full form.\nThe evaluation metric used in our experiment is the top-K accuracy, which is also used by Tsuruoka et al. (2005), Sun et al. (2009) and\nZhang et al. (2012). The top-K accuracy measures what percentage of the reference abbreviations are found if we take the top N candidate abbreviations from all the results. In our experiment, top-10 candidates are considered in re-ranking phrase and the measurement used is top-1 accuracy (which is the accuracy we usually refer to) because the final aim of the algorithm is to detect the exact abbreviation.\nCRF++7 , an open source linear chain CRF tool, is used in the sequence labeling part. For ILP part, we use lpsolve8, which is also an open source tool. The parameters of these tools are tuned through cross-validation on the training data."
        },
        {
            "heading": "3.2 Results",
            "text": "TABLE 9 shows the top-K accuracy of the character-based and MSU-based method. We can see that the MSU-based tagging method can utilize word information, which can get better performance than the character-based method. We can also figure out that the top-5 candidates include the reference abbreviation for most full forms. Therefore reasonable decoding by considering all possible labeling of sequences may improve the performance. Although the MSU-based methods only outperforms character-based methods by 0.75%\n7http://crfpp.sourceforge.net/ 8http://lpsolve.sourceforge.net/5.5/\nfor top-1 accuracy, it is much better when considering top-2 to top-5 accuracy (+2.5%). We further select the top-ranked candidates for ILP decoding. Therefore the MSU-based method can further improve the performance in the global decoding phase.\nWe then use the top-5 candidates of characterbased method and MSU-based method, as well as the top-2 results of sub-sequence labeling in the ILP decoding phase. Then we select the topranked candidate as the final abbreviation of each instance. TABLE 10 shows the results. We can see that the accuracy of our method is 61.0%, which improved by +3.89% compared to the characterbased method, and +3.14% compared to the MSUbased method.\nWe find that the ILP decoding phase do play an important role in generating the right an-\nswer. Some reference abbreviations which are not picked out by either tagging method can be found out after decoding. TABLE 11 shows the example of the organization name \u201c\u9ad8\u7b49\u5b66\u6821\u7edf\u4e00\u62db\u751f \u529e\u516c\u5ba4\u201d (Higher Education Admissions Office). Neither the character-based method nor the MSUbased method finds the correct answer \u201c\u9ad8\u62db\u529e\u201d, while after ILP decoding, \u201c\u9ad8\u62db\u529e\u201d becomes the final result. TABLE 12 and TABLE 13 give two more examples."
        },
        {
            "heading": "3.3 Improvements Considering Length",
            "text": "Full forms that are longer than five characters are long terms. Long terms contain more characters, which is much easier to make mistakes. Figure 1 shows the top-1 accuracy respect to the term length using different tagging methods and using ILP decoding. The x-axis represents the length of the full form. The y-axis represents top-1 accuracy. We find that our method works especially\nbetter than pure character-based or MSU-based approach when the full form is long. By decoding using ILP, both local and global information are incorporated. Therefore many of these errors can be eliminated."
        },
        {
            "heading": "3.4 Effect of pruning",
            "text": "As discussed in previous sections, if we are able to pre-determine that some characters in a certain context should be kept or skipped, then the number of possible boolean variable x can be reduced. TABLE 14 shows the differences. To guarantee a high accuracy, we set the threshold to be 0.99. When the original full form is partially tagged by the pruning constraints, the number of boolean variables per full form is reduced from 34.4 to 25.5. By doing this, we can improve the prediction speed over taking the raw input.\nFrom TABLE 14 we can also see that the top1 accuracy is not affected by these pruning constraints. This is obvious, because CRF itself has a strong modeling ability. The pruning constraints cannot improve the model accuracy. But they can help eliminate those false candidates to make the ILP decoding faster."
        },
        {
            "heading": "3.5 Compare with the State-of-the-art Systems",
            "text": "We also compare our method with previous methods, including Sun et al. (2009) and Zhang et al. (2012). Because we use a different corpus, we re-implement the system Sun et al. (2009), Zhang\net al. (2012) and Sun et al. (2013), and experiment on our corpus. The first two are CRF+GI and DPLVM+GI in Sun et al. (2009), which are reported to outperform the methods in Tsuruoka et al. (2005) and Sun et al. (2008). For DPLVM we use the same model in Sun et al. (2009) and experiment on our own data. We also compare our approach with the method in Zhang et al. (2012). However, Zhang et al. (2012) uses different sources of search engine result information to re-rank the original candidates. We do not use any extra web resources. Because Zhang et al. (2012) uses web information only in its second stage, we use \u201cBIEP\u201d(the tag set used by Zhang et al. (2012)) to denote the first stage of Zhang et al. (2012), which also uses no web information. TABLE 15 shows the results of the comparisons. We can see that our method outperforms all other methods which use no extra resource. Because Zhang et al. (2012) uses extra web resource, the top-1 accuracy of Zhang et al. (2012) is slightly better than ours."
        },
        {
            "heading": "4 Related Work",
            "text": "Previous research mainly focuses on \u201cabbreviation disambiguation\u201d, and machine learning approaches are commonly used (Park and Byrd, 2001; HaCohen-Kerner et al., 2008; Yu et al., 2006; Ao and Takagi, 2005). These ways of linking abbreviation pairs are effective, however, they cannot solve our problem directly. In many cases the full form is definite while we don\u2019t know the corresponding abbreviation.\nTo solve this problem, some approaches maintain a database of abbreviations and their corresponding \u201cfull form\u201d pairs. The major problem of pure database-building approach is obvious. It is impossible to cover all abbreviations, and the building process is quit laborious. To find these pairs automatically, a powerful approach is to find the reference for a full form given the context,\nwhich is referred to as \u201cabbreviation generation\u201d.\nThere is research on heuristic rules for generating abbreviations Barrett and Grems (1960); Bourne and Ford (1961); Taghva and Gilbreth (1999); Park and Byrd (2001); Wren et al. (2002); Hearst (2003). Most of them achieved high performance. However, hand-crafted rules are time consuming to create, and it is not easy to transfer the knowledge of rules from one language to another.\nRecent studies of abbreviation generation have focused on the use of machine learning techniques. Sun et al. (2008) proposed a supervised learning approach by using SVM model. Tsuruoka et al. (2005); Sun et al. (2009) formalized the process of abbreviation generation as a sequence labeling problem. In Tsuruoka et al. (2005) each character in the full form is associated with a binary value label y, which takes the value S (Skip) if the character is not in the abbreviation, and value P (Preserve) if the character is in the abbreviation. Then a MEMM model is used to model the generating process. Sun et al. (2009) followed this schema but used DPLVM model to incorporate both local and global information, which yields better results. Sun et al. (2013) also uses machine learning based methods, but focuses on the negative full form problem, which is a little different from our work.\nBesides these pure statistical approaches, there are also many approaches using Web as a corpus in machine learning approaches for generating abbreviations.Adar (2004) proposed methods to detect such pairs from biomedical documents. Jain et al. (2007) used web search results as well as search logs to find and rank abbreviates full pairs, which show good result. The disadvantage is that search log data is only available in a search engine backend. The ordinary approaches do not have access to search engine internals. Zhang et al. (2012) used web search engine information to rerank the candidate abbreviations generated by statistical approaches. Compared to their approaches, our method uses no extra resource, but reaches comparable results.\nILP shows good results in many NLP tasks. Punyakanok et al. (2004); Roth and Yih (2005) used it in semantic role labeling (SRL). Martins et al. (2009) used it in dependency parsing. (Zhao and Marcus, 2012) used it in Chinese word segmentation. (Riedel and Clarke, 2006) used ILP\nin dependency parsing. However, previous works mainly focus on the constraints of avoiding boundary confliction. For example, in SRL, two argument of cannot overlap. In CWS, two Chinese words cannot share a same character. Different to their methods, we investigate on the conflict of labels of character sub-sequences."
        },
        {
            "heading": "5 Conclusion and Future work",
            "text": "We propose a new Chinese abbreviation prediction method which can incorporate rich local information while generating the abbreviation globally. We propose the MSU, which is more coarsegrained than character but more fine-grained than word, to capture word information in the sequence labeling framework. Besides the MSUbased method, we use a substring tagging strategy to generate local substring tagging candidates. We use an ILP formulation with various constraints to globally decode the final abbreviation from the generated candidates. Experiments show that our method outperforms the state-of-the-art systems, without using any extra resource. This method is not limited to Chinese abbreviation generation, it can also be applied to similar languages like Japanese.\nThe results are promising and outperform the baseline methods. The accuracy can still be improved. Potential future works may include using semi-supervised methods to incorporate unlabeled data and design reasonable features from large corpora. We are going to study on these issues in the future."
        },
        {
            "heading": "Acknowledgments",
            "text": "This research was partly supported by National Natural Science Foundation of China (No.61370117,61333018,61300063),Major National Social Science Fund of China(No.12&ZD227), National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101), and Doctoral Fund of Ministry of Education of China (No. 20130001120004). The contact author of this paper, according to the meaning given to this role by Key Laboratory of Computational Linguistics, Ministry of Education, School of Electronics Engineering and Computer Science, Peking University, is Houfeng Wang. We thank Ke Wu for part of our work is inspired by his previous work at KLCL."
        }
    ],
    "title": "Predicting Chinese Abbreviations with Minimum Semantic Unit and Global Constraints",
    "year": 2014
}