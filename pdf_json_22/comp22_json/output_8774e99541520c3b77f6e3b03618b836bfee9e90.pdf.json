{
    "abstractText": "Tensor decomposition is a powerful tool for extracting physically meaningful latent factors from multi-dimensional nonnegative data, and has been an increasing interest in a variety of fields such as image processing, machine learning, and computer vision. In this paper, we propose a sparse nonnegative Tucker decomposition and completion method for the recovery of underlying nonnegative data under noisy observations. Here the underlying nonnegative data tensor is decomposed into a core tensor and several factor matrices with all entries being nonnegative and the factor matrices being sparse. The loss function is derived by the maximum likelihood estimation of the noisy observations, and the l0 norm is employed to enhance the sparsity of the factor matrices. We establish the error bound of the estimator of the proposed model under generic noise scenarios, which is then specified to the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations, respectively. Our theoretical results are better than those by existing tensor-based or matrix-based methods. Moreover, the minimax lower bounds are shown to be matched with the derived upper bounds up to logarithmic factors. Numerical examples on both synthetic and real-world data sets demonstrate the superiority of the proposed method for nonnegative tensor data completion.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xiongjun Zhang"
        },
        {
            "affiliations": [],
            "name": "Michael K. Ng"
        }
    ],
    "id": "SP:7fc290b2404ad16473f8c5d2a1d3370884778cda",
    "references": [
        {
            "authors": [
                "E. Acar",
                "D.M. Dunlavy",
                "T.G. Kolda",
                "M. M\u00f8rup"
            ],
            "title": "Scalable tensor factorizations for incomplete data",
            "venue": "Chemometrics Intell. Lab. Syst.,",
            "year": 2011
        },
        {
            "authors": [
                "K. Allab",
                "L. Labiod",
                "M. Nadif"
            ],
            "title": "A semi-NMF-PCA unified framework for data clustering",
            "venue": "IEEE Trans. Knowl. Data Eng.,",
            "year": 2017
        },
        {
            "authors": [
                "X. Bai",
                "F. Xu",
                "L. Zhou",
                "Y. Xing",
                "L. Bai",
                "J. Zhou"
            ],
            "title": "Nonlocal similarity based nonnegative Tucker decomposition for hyperspectral image denoising",
            "venue": "IEEE J. Select. Topics in Appl. Earth Obs. Remote Sens.,",
            "year": 2018
        },
        {
            "authors": [
                "A. Beck"
            ],
            "title": "First-Order Methods in Optimization",
            "venue": "SIAM: Philadelphia,",
            "year": 2017
        },
        {
            "authors": [
                "R.I. Bo\u0163",
                "D.-K. Nguyen"
            ],
            "title": "The proximal alternating direction method of multipliers in the nonconvex setting: convergence analysis and rates",
            "venue": "Math. Oper. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "S. Boyd",
                "N. Parikh",
                "E. Chu",
                "B. Peleato",
                "J. Eckstein"
            ],
            "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
            "venue": "Found. Trends. Mach. Learn.,",
            "year": 2011
        },
        {
            "authors": [
                "Y. Cao",
                "Y. Xie"
            ],
            "title": "Poisson matrix recovery and completion",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Chen",
                "T.-Z. Huang",
                "X.-L. Zhao"
            ],
            "title": "Destriping of multispectral remote sensing image using low-rank tensor decomposition",
            "venue": "IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,",
            "year": 2018
        },
        {
            "authors": [
                "A. Cichocki",
                "D. Mandic",
                "L. De Lathauwer",
                "G. Zhou",
                "Q. Zhao",
                "C. Caiafa",
                "H.A. Phan"
            ],
            "title": "Tensor decompositions for signal processing applications: From two-way to multiway component analysis",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2015
        },
        {
            "authors": [
                "A. Cichocki",
                "R. Zdunek",
                "A.H. Phan",
                "S.-i. Amari"
            ],
            "title": "Nonnegative matrix and tensor factorizations: applications to exploratory multi-way data analysis and blind source separation. U.K..",
            "year": 2009
        },
        {
            "authors": [
                "T.M. Cover",
                "J.A. Thomas"
            ],
            "title": "Elements of Information Theory. 2nd Edition",
            "year": 2006
        },
        {
            "authors": [
                "L. De Lathauwer",
                "B. De Moor",
                "J. Vandewalle"
            ],
            "title": "A multilinear singular value decomposition",
            "venue": "SIAM. J. Matrix Anal. Appl.,",
            "year": 2000
        },
        {
            "authors": [
                "L. De Lathauwer",
                "B. De Moor",
                "J. Vandewalle"
            ],
            "title": "On the best rank-1 and rank-(r1",
            "venue": "SIAM J. Matrix Anal. Appl.,",
            "year": 2000
        },
        {
            "authors": [
                "D. Donoho",
                "V. Stodden"
            ],
            "title": "When does non-negative matrix factorization give a correct decomposition into parts",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2003
        },
        {
            "authors": [
                "N. Gillis"
            ],
            "title": "Nonnegative Matrix Factorization",
            "venue": "Philadelphia, SIAM,",
            "year": 2020
        },
        {
            "authors": [
                "N. Gillis",
                "F. Glineur"
            ],
            "title": "Using underapproximations for sparse nonnegative matrix factorization",
            "venue": "Pattern Recognit.,",
            "year": 2010
        },
        {
            "authors": [
                "N. Gillis",
                "R. Luce"
            ],
            "title": "Robust near-separable nonnegative matrix factorization using linear optimization",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2014
        },
        {
            "authors": [
                "C.J. Hillar",
                "L.-H. Lim"
            ],
            "title": "Most tensor problems are NP-hard",
            "venue": "J. ACM,",
            "year": 2013
        },
        {
            "authors": [
                "F.L. Hitchcock"
            ],
            "title": "The expression of a tensor or a polyadic as a sum of products",
            "venue": "J. Math. Physics,",
            "year": 1927
        },
        {
            "authors": [
                "D. Hong",
                "T.G. Kolda",
                "J.A. Duersch"
            ],
            "title": "Generalized canonical polyadic tensor decomposition",
            "venue": "SIAM Rev.,",
            "year": 2020
        },
        {
            "authors": [
                "M. Hong",
                "Z.-Q. Luo",
                "M. Razaviyayn"
            ],
            "title": "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems",
            "venue": "SIAM J. Optim.,",
            "year": 2016
        },
        {
            "authors": [
                "R.A. Horn",
                "C.R. Johnson"
            ],
            "title": "Topics in Matrix analysis",
            "year": 1991
        },
        {
            "authors": [
                "P.O. Hoyer"
            ],
            "title": "Nonnegative matrix factorization with sparseness constraints",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2004
        },
        {
            "authors": [
                "S. Jain",
                "A. Gutierrez",
                "J. Haupt"
            ],
            "title": "Noisy tensor completion for tensors with a sparse canonical polyadic factor",
            "venue": "IEEE International Symposium on Inf. Theory,",
            "year": 2017
        },
        {
            "authors": [
                "M.E. Kilmer",
                "C.D. Martin"
            ],
            "title": "Factorization strategies for third-order tensors",
            "venue": "Linear Algebra Appl.,",
            "year": 2011
        },
        {
            "authors": [
                "O. Klopp",
                "K. Lounici",
                "A.B. Tsybakov"
            ],
            "title": "Robust matrix completion",
            "venue": "Probab. Theory Relat. Fields,",
            "year": 2017
        },
        {
            "authors": [
                "T.G. Kolda",
                "B.W. Bader"
            ],
            "title": "Tensor decompositions and applications",
            "venue": "SIAM Rev.,",
            "year": 2009
        },
        {
            "authors": [
                "L.G. Kraft"
            ],
            "title": "A device for quantizing, grouping, and coding amplitude-modulated pulses",
            "venue": "Master\u2019s thesis, Massachusetts Institute of Technology,",
            "year": 1949
        },
        {
            "authors": [
                "D.D. Lee",
                "H.S. Seung"
            ],
            "title": "Learning the parts of objects by non-negative matrix factorization",
            "year": 1999
        },
        {
            "authors": [
                "N. Lee",
                "A.-H. Phan",
                "F. Cong",
                "A. Cichocki"
            ],
            "title": "Nonnegative tensor train decompositions for multi-domain feature extraction and clustering",
            "venue": "In Int. Conf. Neural Inform. Process.,",
            "year": 2016
        },
        {
            "authors": [
                "Q.J. Li"
            ],
            "title": "Estimation of Mixture Models",
            "venue": "PhD thesis, Yale University,",
            "year": 1999
        },
        {
            "authors": [
                "X. Li",
                "M.K. Ng",
                "G. Cong",
                "Y. Ye",
                "Q. Wu"
            ],
            "title": "MR-NTD: Manifold regularization nonnegative Tucker decomposition for tensor data dimension reduction and representation",
            "venue": "IEEE Trans. Neural Netw. Learn. Syst.,",
            "year": 2017
        },
        {
            "authors": [
                "J. Liu",
                "P. Wonka",
                "J. Ye"
            ],
            "title": "Sparse non-negative tensor factorization using columnwise",
            "venue": "Pattern Recognit.,",
            "year": 2012
        },
        {
            "authors": [
                "X. Lu",
                "H. Wu",
                "Y. Yuan",
                "P. Yan",
                "X. Li"
            ],
            "title": "Manifold regularized sparse NMF for hyperspectral unmixing",
            "venue": "IEEE Trans. Geosci. Remote Sens.,",
            "year": 2013
        },
        {
            "authors": [
                "B. McMillan"
            ],
            "title": "Two inequalities implied by unique decipherability",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 1956
        },
        {
            "authors": [
                "M. M\u00f8rup",
                "L.K. Hansen",
                "S.M. Arnfred"
            ],
            "title": "Algorithms for sparse nonnegative Tucker decompositions",
            "venue": "Neural Comput.,",
            "year": 2008
        },
        {
            "authors": [
                "E. Newman",
                "M.E. Kilmer"
            ],
            "title": "Non-negative tensor patch dictionary approaches for image compression and deblurring applications",
            "venue": "SIAM J. Imaging Sci.,",
            "year": 2020
        },
        {
            "authors": [
                "I.V. Oseledets"
            ],
            "title": "Tensor-train decomposition",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2011
        },
        {
            "authors": [
                "J. Pan",
                "N. Gillis"
            ],
            "title": "Generalized separable nonnegative matrix factorization",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2021
        },
        {
            "authors": [
                "J. Pan",
                "M.K. Ng",
                "Y. Liu",
                "X. Zhang",
                "H. Yan"
            ],
            "title": "Orthogonal nonnegative Tucker decomposition",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2021
        },
        {
            "authors": [
                "M. Raginsky",
                "R.M. Willett",
                "Z.T. Harmany",
                "R.F. Marcia"
            ],
            "title": "Compressed sensing performance bounds under Poisson noise",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2010
        },
        {
            "authors": [
                "M. Rajapakse",
                "J. Tan",
                "J. Rajapakse"
            ],
            "title": "Color channel encoding with NMF for face recognition",
            "venue": "In 2004 International Conference on Image Processing,",
            "year": 2004
        },
        {
            "authors": [
                "A.V. Sambasivan",
                "J.D. Haupt"
            ],
            "title": "Minimax lower bounds for noisy matrix completion under sparse factor models",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 2018
        },
        {
            "authors": [
                "N.D. Sidiropoulos",
                "L. De Lathauwer",
                "X. Fu",
                "K. Huang",
                "E.E. Papalexakis",
                "C. Faloutsos"
            ],
            "title": "Tensor decomposition for signal processing and machine learning",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2017
        },
        {
            "authors": [
                "S. Soltani",
                "M.E. Kilmer",
                "P.C. Hansen"
            ],
            "title": "A tensor-based dictionary learning approach to tomographic image reconstruction",
            "venue": "BIT Numer. Math.,",
            "year": 2016
        },
        {
            "authors": [
                "A. Soni",
                "S. Jain",
                "J. Haupt",
                "S. Gonella"
            ],
            "title": "Noisy matrix completion under sparse factor models",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 2016
        },
        {
            "authors": [
                "A.B. Tsybakov"
            ],
            "title": "Introduction to Nonparametric Estimation",
            "year": 2009
        },
        {
            "authors": [
                "L.R. Tucker"
            ],
            "title": "Some mathematical notes on three-mode factor analysis",
            "year": 1966
        },
        {
            "authors": [
                "N. Vannieuwenhoven",
                "R. Vandebril",
                "K. Meerbergen"
            ],
            "title": "A new truncation strategy for the higherorder singular value decomposition",
            "venue": "SIAM J. Sci. Comput.,",
            "year": 2012
        },
        {
            "authors": [
                "Y. Wang",
                "W. Yin",
                "J. Zeng"
            ],
            "title": "Global convergence of ADMM in nonconvex nonsmooth optimization",
            "venue": "J. Sci. Comput.,",
            "year": 2019
        },
        {
            "authors": [
                "Y. Xu"
            ],
            "title": "Alternating proximal gradient method for sparse nonnegative Tucker decomposition",
            "venue": "Math. Program.Comput.,",
            "year": 2015
        },
        {
            "authors": [
                "Q. Zhang",
                "H. Wang",
                "R.J. Plemmons",
                "V.P. Pauca"
            ],
            "title": "Tensor methods for hyperspectral data analysis: a space object material identification study",
            "venue": "J. Opt. Soc. Amer. A,",
            "year": 2008
        },
        {
            "authors": [
                "X. Zhang",
                "M.K. Ng"
            ],
            "title": "Low rank tensor completion with Poisson observations",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 2022
        },
        {
            "authors": [
                "X. Zhang",
                "M.K. Ng"
            ],
            "title": "Sparse nonnegative tensor factorization and completion with noisy observations",
            "venue": "IEEE Trans. Inf. Theory,",
            "year": 2022
        },
        {
            "authors": [
                "G. Zhou",
                "A. Cichocki",
                "S. Xie"
            ],
            "title": "Fast nonnegative matrix/tensor factorization based on low-rank approximation",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2012
        },
        {
            "authors": [
                "G. Zhou",
                "A. Cichocki",
                "Q. Zhao",
                "S. Xie"
            ],
            "title": "Efficient nonnegative Tucker decompositions: Algo- 35 rithms and uniqueness",
            "venue": "IEEE Trans. Image Process.,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "ar X\niv :2\n20 8.\n08 28\n7v 1\n[ cs\n.L G\n] 1\n7 A\nfrom multi-dimensional nonnegative data, and has been an increasing interest in a variety of fields such as image processing, machine learning, and computer vision. In this paper, we propose a sparse nonnegative Tucker decomposition and completion method for the recovery of underlying nonnegative data under noisy observations. Here the underlying nonnegative data tensor is decomposed into a core tensor and several factor matrices with all entries being nonnegative and the factor matrices being sparse. The loss function is derived by the maximum likelihood estimation of the noisy observations, and the \u21130 norm is employed to enhance the sparsity of the factor matrices. We establish the error bound of the estimator of the proposed model under generic noise scenarios, which is then specified to the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations, respectively. Our theoretical results are better than those by existing tensor-based or matrix-based methods. Moreover, the minimax lower bounds are shown to be matched with the derived upper bounds up to logarithmic factors. Numerical examples on both synthetic and real-world data sets demonstrate the superiority of the proposed method for nonnegative tensor data completion.\nKey Words: Sparse nonnegative Tucker decomposition, maximum likelihood estimation, noisy observations, error bound\nMathematics Subject Classification 2020: 15A69, 90C26"
        },
        {
            "heading": "1 Introduction",
            "text": "Tensors, also called multi-dimensional data, are high-order generalizations of vectors and matrices, and have a variety of applications including signal processing, machine learning, computer vision, and so on [9, 27]. The data required from real-world applications are usually represented as high-order tensors, for example, color images are third-order tensors, color videos are fourth-order tensors. In\n*School of Mathematics and Statistics and Hubei Key Laboratory of Mathematical Sciences, Central China Normal University, Wuhan 430079, China (e-mail: xjzhang@mail.ccnu.edu.cn). The research of this author was supported in part by the National Natural Science Foundation of China under Grant No. 12171189 and the Knowledge Innovation Project of Wuhan under Grant No. 2022010801020279. \u2020Department of Mathematics, The University of Hong Kong, Pokfulam, Hong Kong (e-mail: mng@maths.hku.hk). The research of this author was supported in part by the Hong Kong Research Grant Council GRF 12300218, 12300519, 17201020, 17300021, C1013-21GF, C7004-21GF and Joint NSFC-RGC N-HKU76921.\norder to explore the intrinsic structure of tensor data, tensor decomposition is a powerful and effective approach to represent the data, and has been widely used in several domains [8, 44]. Besides, tensor decomposition can identify the inherent structure of tensor data and extract meaningful and explanatory information by a lower-dimensional set of latent factors, which is due to the fact that many highdimensional data reside in a low-dimensional subspace [10].\nThe observation tensor data and latent factors are naturally nonnegative in several real-world applications such as images, video volumes, and text data. Nonnegative tensor decomposition can be employed to extract the intrinsic structure of a tensor by the decomposition formulation [32]. For example, many high-dimensional data, such as nonnegative hyperspectral images or video images, are factorized to find meaningful latent nonnegative components [40]. Besides dimensionality reduction, nonnegative tensor decomposition can be modeled and interpreted better by means of nonnegative and sparse components, which can achieve a unique additive parts-based representation.\nIn particular, when the order of a tensor is second, nonnegative tensor decomposition reduces to nonnegative matrix factorization (NMF), which has been attracted much attention in the past decades, see [15, 17, 29, 39] and references therein. The NMF, which requires the factors of the low-rank decomposition to be componentwise nonnegative, is capable of giving physically meaningful and more interpretable results, and has the ability of leaning the local parts of objects [29]. There are plenty of real-world applications for NMF [15]. For example, the NMF method can extract parts of faces, such as eyes, noses, and lips, in a series of facial images [42], identify topics in a set of documents [2], and extract materials and their proportions in hyperspectral images unmixing [34]. Moreover, sparse NMF was introduced further because it enhances the ability of NMF to learn a parts-based representation and produces more easily interpretable factors. For example, in facial feature extraction, sparsity leads to more localized features and identifiable solutions, while fewer features are used to reconstruct each input image [16]. Besides, Hoyer [23] proposed a sparse NMF method with sparse constraints to improve the decomposition for the observations with additive Gaussian noise, and designed a projected gradient descent algorithm to solve the resulting model. In theory, Soni et al. [46] proposed a noisy matrix completion method under a class of general noise models, and established the error bound of the estimator of their proposed model, which can reduce to sparse NMF and completion under nonnegative constraints. Moreover, Sambasivan et al. [43] showed the error bound in [46] achieves minimax error rates up to multiplicative constants and logarithmic factors. However, for multi-dimensional data, the matrix based method may destroy the structure of a tensor via unfolding a tensor into a matrix.\nFor nonnegative tensor decomposition, the key issue is the decomposition formulation of a tensor. There are some popular decompositions for tensors such as CANDECOMP/PARAFAC (CP) decomposition [19], Tucker decomposition [48], tensor train decomposition [38], tensor decomposition via tensor-tensor product [25]. Due to the nonnegativity of a tensor, the nonnegative CP decomposition provides an interpretable, low tensor rank representation of the data and has been used in a variety of applications related to sparse image representation and image processing. For example, Zhang et al. [52] employed a nonnegative CP decomposition for hyperspectral unmixing with the goal to identify materials and material abundance in the data and to compress the data. Moreover, Hong et al. [20] proposed a generalized CP low-rank tensor decomposition that allows other loss functions besides squared error, and presented a gradient descent type algorithm to solve the resulting model, which can handle missing data by a similar approach in [1]. Besides, Jain et al. [24] proposed a CP decomposition and completion method with one sparse factor under a general class of noise models for third-order tensors, and established the error bound of the estimator of their proposed model, which was further derived to the error bound for the observations with additive Gaussian noise. The method in [24] can reduce to nonnegative CP decomposition with one sparse factor under nonnegative constraint of factor matrices. However, the previous methods need to give the CP rank of a tensor in advance, which is NP-hard in general [18].\nBased on the algebra framework of tensor-tensor product in [25], Soltani et al. [45] proposed a nonnegative tensor decomposition method with one sparse factor for third-order tensors under additive Gaussian noise, which was constructed by a tensor dictionary prior from training data for tomographic image reconstruction. Furthermore, Newman et al. [37] proposed a sparse nonnegative tensor patchbased dictionary approach based on tensor-tensor product for image compression and deblurring, where the observations were corrupted by additive Gaussian noise. Recently, Zhang [54] proposed a sparse nonnegative tensor factorization and completion based on the algebra framework of tensor-tensor product for third-order tensors, where one factor tensor was sparse and the observations were corrupted by a general class of noise models. Then the error bound of the estimator in [54] was established, and the minimax lower bound was derived, which matched with the established upper bounds up to a logarithmic factor of the sizes of the underlying tensor. However, for n-dimensional m-th order tensors, there are nm\u22122 n-by-n matrix singular value decompositions to be computed [25] and its computational cost may be quite high in the tubular singular value decomposition approach. For nonnegative TT decomposition, Lee et al. [30] proposed a hierarchical alternating least squares algorithm for feature extraction and clustering. Although the performance of nonnegative TT decomposition was better than that of standard nonnegative Tucker decomposition, the TT decomposition typically lacks interpretability and the efficient implementation depends on the tensor already being in TT format [37].\nTucker decomposition is to decompose a given tensor into the product of a core tensor with smaller dimensions and a series of factor matrices. And the best low-rank approximation of Tucker decomposition of a tensor was discussed and studied in [12, 13]. Nonnegative Tucker decomposition (NTD) provides an additional multiway structured representation of tensor data with all entries of the core tensor and factor matrices being nonnegative in Tucker decomposition [40], which has a wide range of applications including image denoising and hyperspectral image restoration [3, 32, 56]. Moreover, M\u00f8rup et al. [36] proposed two multiplicative update algorithms for sparse NTD with the observations including additive Gaussian noise and Poisson observations, respectively, which yields a parts-based representation and is a more interpretable decomposition. Liu et al. [33] presented a fast and flexible algorithm for sparse NTD with a special core tensor based on columnwise coordinate descent, where the observations were corrupted by additive Gaussian noise and the factor matrices were nonnegative and sparse. Besides, Xu [51] designed an alternating proximal gradient algorithm for sparse NTD and completion with global convergence guarantee, where the observations were corrupted by additive Gaussian noise. However, there is no theoretical result about the error bounds of the models for sparse NTD with missing values in the previous work. Also a general class of noise models is not discussed and studied for sparse NTD and completion in the existing literature.\nIn this paper, we propose a sparse NTD and completion approach for the observations with a general class of noise models. The loss function is derived by the maximum likelihood estimation of observations, and the nonnegativity for the entries of the core tensor and the factor matrices in Tucker decomposition are imposed. The \u21130 norm is used to characterise the sparsity of the factor matrices. Moreover, the error bound of the estimator of the proposed model is established, and then error bounds of the estimators for the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations are derived, respectively. Besides, the minimax lower bound of general class of noisy observations is established, which matches to the upper bound up to a logarithmic factor. Then an alternating direction method of multipliers (ADMM) is designed to solve the resulting model. Numerical experiments on synthetic data and image data demonstrate the effectiveness of the proposed model compared with the matrix based method [46] and the sparse nonnegative tensor factorization and completion via tensor-tensor product [54]. We summarize the existing literature and our contribution for sparse nonnegative tensor decomposition/factorization in Table 1.\nThe remaining parts of this paper are organized as follows. In Section 2, some preliminaries about tensor Tucker decomposition and related information theoretical quality are provided. In Section 3, a\nsparse NTD and completion model is proposed under a general class of noisy observations. An upper error bound of the estimator of the proposed model is established in Section 4. Moreover, the error bounds of the estimators are further derived for the observations with special noise models. Section 5 is devoted to the minimax lower bound of the estimator. In Section 6, an ADMM based algorithm is developed to solve the resulting model. In Section 7, numerical experiments are presented to demonstrate the effectiveness of the proposed model. Discussions and list of future work are given in Section 8. All proofs of theorems are provided in the appendices."
        },
        {
            "heading": "2 Preliminaries",
            "text": "Let Rn and R n1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd + denote the n-dimensional Euclidean space with real numbers and the set of n1 \u00d7 n2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nd tensors with nonnegative real entries, respectively. Scalars, vectors, matrices, and tensors are represented by lowercase letters, lowercase boldface letters, uppercase letters, and capital Euler script letters, respectively, e.g., x,x,X, X . log refers to natural logarithm throughout this paper. \u230ax\u230b and \u2308x\u2309 are the integer part of x and smallest integer that is larger or equal to x, respectively. We denote nm = max{n1, n2, . . . , nd}, a \u2228 b = max{a, b}, and a \u2227 b = min{a, b}, respectively. For any positive integer n, we let [n] = {1, 2, . . . , n}. For an arbitrary set C, |C| denotes the number of entries in C. \u03b4C(\u00b7) denotes the indicator function over C, i.e., \u03b4C(x) = 0 if x \u2208 C, and \u03b4C(x) = \u221e if x 6= C.\nLet Xi1i2\u00b7\u00b7\u00b7id denote the (i1, i2, . . . , id)th entry of a tensor X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd . The number of ways or dimensions of a tensor is called the order [27]. The mode-i unfolding of a tensor X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd , denoted by X(i), arranges the mode-n fibers to be the columns of the resulting matrix, where the (i1, i2, . . . , id)th entry of X maps to matrix entry (in, j) with\nj = 1 +\nd\u2211\nk=1 k 6=n\n(ik \u2212 1)Jk and Jk = k\u22121\u220f\nl=1 l 6=n\nnl.\nHere a fiber is defined by fixing every index but one. The vectorization of a tensor X is a vector, which is obtained by stacking all mode-1 fibers of X and denoted by vec(X ).\nThe Tucker decomposition of a dth-order tensor X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd is given by [48]\nX = C \u00d71 A1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7d Ad,\nwhere C \u2208 Rr1\u00d7r2\u00d7\u00b7\u00b7\u00b7\u00d7rd is called the core tensor, Ai \u2208 Rni\u00d7ri are the factor matrices, i \u2208 [d], and (r1, r2, . . . , rd) is called the Tucker rank of X . Here the n-mode product of a tensor C by a matrix A \u2208 Rn\u00d7ri , denoted by C \u00d7i A, is a r1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 ri\u22121 \u00d7 n \u00d7 ri+1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 rd-tensor whose entries are\ngiven by [12, Definition 3]\n(C \u00d7i A)j1j2\u00b7\u00b7\u00b7ji\u22121kiji+1\u00b7\u00b7\u00b7jd = ri\u2211\nji=1\nCj1j2\u00b7\u00b7\u00b7ji\u22121jiji+1\u00b7\u00b7\u00b7jdAkiji .\nIt is easy to verify that if X = C \u00d71 A1 \u00d72 \u00b7 \u00b7 \u00b7 \u00d7d Ad, then\nvec(X ) = (\u22971i=dAi)vec(C), (1)\nwhere \u22971i=dAi := Ad \u2297 Ad\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297 A1 and A \u2297 B denotes the Kronecker product of A and B. Moreover, the mode-n unfolding of X is given by [27]\nX(n) = AnC(n)(\u22971i=d,i 6=nAi)T . (2)\nFor an arbitrary vector x \u2208 Rn, \u2016x\u2016 and \u2016x\u20161 denote the Euclidean norm and the \u21131 norm of x, respectively, where \u2016x\u20161 = \u2211n i=1 |xi| and xi is the ith component of x, i \u2208 [n]. \u2016X\u20160 denotes the number of nonzero entries of a matrix X. The tensor Frobenius norm of X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd is defined as \u2016X\u2016F = \u221a \u3008X ,X\u3009, where the inner product of two same-sized tensors is defined as\n\u3008X ,Y\u3009 =\u2211i1,i2,...,id Xi1i2\u00b7\u00b7\u00b7idYi1i2\u00b7\u00b7\u00b7id . The tensor infinity norm of X , denoted by \u2016X\u2016\u221e, is defined as \u2016X\u2016\u221e = maxi1,i2,...,id |Xi1i2\u00b7\u00b7\u00b7id |.\nLet px1(y) and px2(y) denote the probability density functions or probability mass functions of a real scalar random variable y with corresponding parameters x1 and x2, respectively. The KullbackLeibler (KL) divergence of px1(y) from px2(y) is denoted and defined as follows:\nK(px1(y)||px2(y)) = Epx1(y) [ log px1(y)\npx2(y)\n] .\nThe Hellinger affinity between px1(y) and px2(y) is denoted and defined as\nH(px1(y)||px2(y)) = Epx1\n[\u221a px2(y)\npx1(y)\n] = Epx2 [\u221a px1(y)\npx2(y)\n] .\nThe joint probability distributions of random tensors, denoted by pX1(Y), pX2(Y), are the joint probability distributions of the vectorization of the tensors. Then the KL divergence of pX1(Y) from pX2(Y) is defined as\nK(pX1(Y)||pX2(Y)) := \u2211\ni1,i2,...,id\nK(p(X1)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)||p(X2)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)),\nand its Hellinger affinity is defined as\nH(pX1(Y)||pX2(Y)) := \u220f\ni1,i2,...,id\nH(p(X1)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id), p(X2)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)).\nLet R be a Euclidean space endowed with the Euclidean norm \u2016 \u00b7 \u2016. For an arbitrary closed proper function g : R \u2192 (\u2212\u221e,+\u221e], the proximal mapping of g at y is the operator given by\nProxg(y) = argmin x\u2208R\n{ g(x) + 1\n2 \u2016x\u2212 y\u20162\n} ."
        },
        {
            "heading": "3 Sparse NTD and Completion",
            "text": "Let X \u2217 \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd+ be an unknown nonnegative tensor with Tucker rank (r1, r2, . . . , rd). Assume that X \u2217 admits a following sparse nonnegative Tucker decomposition:\nX \u2217 = C\u2217 \u00d71 A\u22171 \u00d72 A\u22172 \u00b7 \u00b7 \u00b7 \u00d7d A\u2217d, (3)\nwhere C\u2217 \u2208 Rr1\u00d7r2\u00d7\u00b7\u00b7\u00b7\u00d7rd+ is the core tensor and A\u2217i \u2208 Rni\u00d7ri+ are sparse factor matrices, i \u2208 [d]. Suppose that ri \u2264 ni, i \u2208 [d]. We also assume that each entries of X \u2217, C\u2217, A\u2217i are bounded, i.e.,\n0 \u2264 X \u2217i1i2\u00b7\u00b7\u00b7id \u2264 c\n2 , (i1, i2, . . . , id) \u2208 [n1]\u00d7 [n2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd],\n0 \u2264 C\u2217i1i2\u00b7\u00b7\u00b7id \u2264 1, (i1, i2, . . . , id) \u2208 [r1]\u00d7 [r2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [rd], 0 \u2264 (A\u2217i )lm \u2264 ai, (l,m) \u2208 [ni]\u00d7 [ri], i \u2208 [d],\n(4)\nwhere c, ai > 0 are given constants. Here we use c 2 for brevity in the subsequence analysis.\nHowever, only a noisy and incompleted version of the underlying tensor X \u2217 is available in practice. Let \u2126 \u2286 [n1] \u00d7 [n2] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd] be a subset at which the entries of the observations Y are collected. Denote Y\u2126 \u2208 Rm to be a vector such that the entries of Y in the index \u2126 are vectorized by lexicographic order, where m is the number of the observed entries, i.e., m = |\u2126|. Assume that n1, n2, . . . , nd \u2265 2 and d \u2265 3 throughout this paper. Let the location \u2126 set be generated according to an independent Bernoulli model with probability p = mn1n2\u00b7\u00b7\u00b7nd (denoted by Bern(p)), i.e., each index (i1, i2, . . . , id) belongs to \u2126 with probability p, which is denoted as \u2126 \u223c Bern(p). Suppose that the entries of observations are conditionally independent, which implies that the overall likelihood is just the product of the likelihoods. Therefore, the joint probability density function or probability mass function of observations Y\u2126 is given by\npX \u2217\u2126(Y\u2126) := \u220f\n(i1,i2,...,id)\u2208\u2126 pX \u2217i1i2\u00b7\u00b7\u00b7id\n(Yi1i2\u00b7\u00b7\u00b7id). (5)\nBy taking the negative logarithm of the probability distribution, we propose the following sparse\nNTD and completion model with nonnegative and bounded constraints:\nmin X\u2208\u03a5\n{ \u2212 log pX\u2126(Y\u2126) + d\u2211\ni=1\n\u03bbi\u2016Ai\u20160 } , (6)\nwhere \u03bbi > 0 are the regularization parameters, i \u2208 [d], \u2016Ai\u20160 is employed to characterize the sparsity of the factor matrix Ai in Tucker decomposition, and \u03a5 is defined as\n\u03a5 := { X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad : C \u2208 C, Ai \u2208 Bi, i \u2208 [d],\n0 \u2264 Xi1i2\u00b7\u00b7\u00b7id \u2264 c, (i1, i2, . . . , id) \u2208 [n1]\u00d7 [n2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd] } .\n(7)\nHere \u03a0 is a countable set of candidate estimates, and C and Bi are the nonnegative and bounded sets constructed as follows: First, let\n\u03c4 := 2\u2308log2(nm) \u03b2\u2309 (8)\nfor a specified \u03b2 \u2265 1, we construct C to be the set of all nonnegative tensors C \u2208 Rr1\u00d7r2\u00d7\u00b7\u00b7\u00b7\u00d7rd+ whose entries are discretized to one of \u03c4 uniformly sized bins in the range [0, 1], and Bi to be the set of all nonnegative matrices Ai \u2208 Rni\u00d7ri+ whose entries either take the value 0, or are discretized to one of \u03c4 uniformly sized bins in the range [0, ai], i \u2208 [d].\nRemark 3.1 The model in (6) can address the observations with general noise distributions, where one just needs to know the probability mass function or probability density function of the observations Y\u2126. In particular, the noise models including additive Gaussian noise, additive Laplace noise, and Poisson observations will be discussed in detail in the next section.\nRemark 3.2 For the observations with additive Gaussian noise, Liu et al. [33] proposed a sparse NTD model with special core tensor C, which is effective for image compression. Moreover, Xu [51] proposed a gradient descent type algorithm for sparse NTD and completion with additive Gaussian noise, which used the relaxation of \u21130 norm for each factor matrix. Besides, M\u00f8rup et al. [36] proposed two multiplicative update algorithms for sparse NTD, where additive Gaussian noise and Poisson observations were considered with full observations. However, there is no theoretical analysis of error bounds about these models in [33, 36, 51]. Only efficient algorithms were proposed and studied to solve their resulting sparse NTD models. Moreover, a general loss function framework has been proposed in (6) for sparse NTD and completion, while the special loss function with commonly used noise types is considered in the existing literature, such as additive Gaussian noise [33, 51], Poisson observations [36].\nRemark 3.3 Recently, Zhang et al. [54] proposed a sparse nonnegative tensor factorization and completion model with general noise distributions based on the algebra framework of tensor-tensor product for third-order tensor, where the underlying tensor was decomposed into the tensor-tensor product of one sparse nonnegative tensor and one nonnegative tensor. The difference between model (6) and [54] is the factorization of the underlying tensor, where the tensor-tensor product with block circulant structure [25] is used in [54] and the Tucker decomposition of the underlying tensor is utilized in model (6). And the nonnegative Tucker decomposition has more widely applications and explanatory information than the nonnegative tensor factorization based on tensor-tensor product, where the last one is mainly applied in X-ray CT imaging, image compression and deblurring based on tensor patch dictionary learning [37]. Besides, the model in [54] is only effective for third-order tensors, while the model in (6) can be utilized to any order tensors and can extract more physically meaningful latent components by the sparse and nonnegative factors [56].\nRemark 3.4 Jain et al. [24] proposed a noisy tensor completion method based on CP decomposition, where one factor is sparse. The model (6) can reduce to sparse nonnegative CP decomposition and completion when the core tensor is diagonal. However, the core tensor of Tucker decomposition is not diagonal in general in many real-world applications [56]. Moreover, Hong et al. [20] proposed a general loss function framework for CP decomposition under various scenarios and designed a gradient descent algorithm to solve the resulting model, while there are no nonnegative and sparse constraints for the factor matrices, and there is no theoretical analysis of error bounds about the model in [20]."
        },
        {
            "heading": "4 Error Bounds",
            "text": "In this section, an upper error bound of the estimator of the sparse NTD and completion model in (6) is established under a general class of noise distributions, and then the upper error bounds of the estimators for the observations with special noise models are derived, including additive Gaussian noise, additive Laplace noise, and Poisson observations.\nLet X \u03bb be a minimizer of (6). Next we state the main theorem about the error bound of the estimator in (6).\nTheorem 4.1 Let the sampling set \u2126 be drawn from the independent Bernoulli model with probability p = mn1n2\u00b7\u00b7\u00b7nd , i.e., \u2126 \u223c Bern(p), and the joint probability density/mass function of Y\u2126 be defined as (5). For any\n\u03bbi \u2265 4(\u03b2 + 2) ( 1 + 2\u03b3\n3\n) log(nm), i \u2208 [d],\nwhere \u03b3 is a constant satisfying\n\u03b3 \u2265 max X\u2208\u03a5 max i1,i2,...,id K ( pX \u2217i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)||pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) ) , (9)\nthen\nE\u2126,Y\u2126 [\u22122 logH(pX\u03bb(Y), pX \u2217(Y))] n1n2 \u00b7 \u00b7 \u00b7nd \u2264 8\u03b3 log(m) m + 3 \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7nd\n+ ( max\ni {\u03bbi}+\n8\u03b3(\u03b2 + 2) log(nm)\n3\n) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016Ai\u20160\nm\n} ,\nwhere the expectation is taken with respect to the joint distribution of \u2126 and Y\u2126.\nTheorem 4.1 is the upper error bound of the estimator in (6) under a general class of noise distributions, which can be specified to some special noise distributions, such as additive Gaussian noise, additive Laplace noise, and Poisson observations. The key point for showing the upper error bound with special noise distributions is to specify the logarithmic Hellinger affinity logH(pX\u03bb(Y), pX \u2217(Y)) and the minimum KL divergence minX\u2208\u03a5K(pX \u2217(Y)||pX (Y)).\nFrom Theorem 4.1, we can see that the negative logarithmic Hellinger affinity with respect to the\nunderlying tensor and the recovered tensor is on the order of O( r1r2\u00b7\u00b7\u00b7rd+\n\u2211d i=1 \u2016Ai\u20160\nm log(nm)) if the KL divergence minX\u2208\u03a5 K(pX \u2217(Y)||pX (Y)) is not too large and \u03bbi are fixed, where Ai are the factor matrices of X \u2208 \u03a5 in the Tucker decomposition. Therefore, the Tucker rank is lower, the error bound of the estimator in (6) is smaller.\nRemark 4.1 For the problem of sparse NTD with partial observations, the upper error bound is related to the degree of freedoms of the tensor, i.e., r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d\ni=1 \u2016Ai\u20160. Moreover, it follows from Theorem 4.1 that the upper error bound decreases as the number of observed samples increases.\nIn the following subsections, we specify the noise models and establish the detailed upper error\nbounds based on Theorem 4.1. In the discretization levels (8), we choose\n\u03b2 = 1 + log ( (2d+1\u22121) \u221a dr1r2\u00b7\u00b7\u00b7rda1a2\u00b7\u00b7\u00b7ad c \u221a nm + 1 )\nlog(nm) , (10)\nwhich implies that \u03b2 \u2265 1. For the regularization parameters \u03bbi, we consider the specific choice\n\u03bbi = 4(\u03b2 + 2)\n( 1 + 2\u03b3\n3\n) log(nm), i \u2208 [d], (11)\nwhere \u03b3 satisfies (9) and is related to the maximum KL divergence between pX \u2217i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) and pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) for any X \u2208 \u03a5."
        },
        {
            "heading": "4.1 Additive Gaussian Noise",
            "text": "In this subsection, we establish the upper error bound of the estimator in (6) for the observations corrupted by additive Gaussian noise, which has been widely used in a variety of applications. In this case, the observation Y\u2126 is generated by\nYi1i2\u00b7\u00b7\u00b7id = X \u2217i1i2\u00b7\u00b7\u00b7id +Ni1i2\u00b7\u00b7\u00b7id , (i1, i2, . . . , id) \u2208 \u2126,\nwhere Ni1i2\u00b7\u00b7\u00b7id are independent zero-mean Gaussian noise with standard deviation \u03c3 > 0. The distribution of Y\u2126 obeys a multivariate Gaussian density with dimension |\u2126| whose mean and covariance matrix are X \u2217\u2126 and \u03c32I\u2126, respectively, where I\u2126 denotes the identity matrix with dimension |\u2126| \u00d7 |\u2126|. Then the joint probability density function of Y\u2126 is given by\npX \u2217\u2126(Y\u2126) = 1\n(2\u03c0\u03c32)|\u2126|/2 exp\n( \u2212\u2016Y\u2126 \u2212 X \u2217 \u2126\u20162\n2\u03c32\n) . (12)\nNow we specify the upper error bound in Theorem 4.1 for the observations with additive Gaussian noise in the following theorem, where the joint probability density function of the observations is given by (12).\nTheorem 4.2 Let \u03b2 and \u03bbi be defined as (10) and (11), i \u2208 [d], where \u03b3 = c 2\n2\u03c32 in (11). Assume that\nthe sampling set \u2126 \u223c Bern(p) with p = mn1n2\u00b7\u00b7\u00b7nd and the joint probability density function of Y\u2126 is given by (12). Then the estimator in (6) satisfies\nE\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 22c\n2 log(m)\nm\n+ 16(\u03b2 + 2)(2c2 + 3\u03c32)\n( r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016A\u2217i \u20160\nm\n) log(nm).\nBy Theorem 4.2, we know that the upper error bound of the estimator of (6) for the observations\nwith additive Gaussian noise is on the order of\nO\n( r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016A\u2217i \u20160\nm log(nm)\n) .\nConsequently, if the factor matrices are sparser, the error upper bound of the estimator is lower. Moreover, when m = n1 \u00b7 \u00b7 \u00b7nd and \u2016Ai\u20160 = rini, i \u2208 [d], which implies that Ai are dense, the error bound in Theorem 4.2 is just the error bound of the estimator of NTD with additive Gaussian noise, which has been widely used and studied, see [10, 36, 55] and references therein.\nRemark 4.2 For sparse NTD with additive Gaussian noise, Liu et al. [33] proposed a novel model with a special core tensor in the Tucker decomposition, where the \u21130 norm is replaced by the \u21131 norm for the factor matrices. In this case, Theorem 4.2 establishes the error bound of the model in [33] by employing the \u21130 norm to characterize the sparsity of the factor matrices. Moreover, Xu et al. [51] proposed a sparse NTD and completion approach for the observations with additive Gaussian noise, which replaced the \u21130 norm by the \u21131 norm in (6). This implies that Theorem 4.2 established the error bound of the estimator of the model in [51] when the \u21130 norm is utilized to measure the sparsity of the factor matrices.\nRemark 4.3 If we ignore the internal structure of a tensor and unfold the tensor into a matrix, we can compare the error bound in Theorem 4.2 with that in [46, Corollary 3], where the matrix based method in [46] can reduce to sparse nonnegative matrix factorization and completion with nonnegative and bounded constraints. For a given tensor X \u2217 in the form of (3), without loss of generality, we unfold it along the first mode, i.e., X \u2217(1) = A\u22171(C\u2217(1)(\u22972i=dA\u2217i )T ). In this case, one is capable of applying the matrix based method in [46] to (X \u2217(1))T \u2208 R (n2\u00b7\u00b7\u00b7nd)\u00d7n1 + , where the sparse factor is A \u2217 1. Then the resulting error bound of the estimator of the matrix based method [46, Corollary 3] is on the order of\nO\n( r1n2 \u00b7 \u00b7 \u00b7nd + \u2016A\u22171\u20160\nm log (n1 \u2228 (n2 \u00b7 \u00b7 \u00b7nd))\n) .\nNow we compare it with the case that only nonnegative tensor decomposition is considered. In this case, the error bound of the matrix based method in [46] for the tensor with mode-1 unfolding is the order of O(n1r1+r1n2n3\u00b7\u00b7\u00b7ndm log(n1 \u2228 (n2 \u00b7 \u00b7 \u00b7nd)). And the error bound in Theorem 4.2 is the order of O( r1r2\u00b7\u00b7\u00b7rd+ \u2211d\ni=1 rini m log(nm)). Therefore, the error bound in Theorem 4.2 is smaller than that in\n[46] if ri is much smaller than ni, i \u2208 [d] and d is not too large. In particular, if r1 = \u00b7 \u00b7 \u00b7 = rd = r and n1 = \u00b7 \u00b7 \u00b7 = nd = n, the error bounds in Theorem 4.2 and in [46] are O( r d+drn m log(n)) and O( rn+rn d\u22121\nm log(n(d \u2212 1)), respectively, where the error bound of the matrix based method is larger than that in Theorem 4.2 if r, d are much smaller than n.\nRemark 4.4 We compare the error bound in Theorem 4.2 with that in [54, Proposition 4.1], which is based on the algebra framework of tensor-tensor product and is only effective for third-order tensors. In general, the two error bound results are not comparable directly since the sparse factors are different for the two factorizations. However, when the factors matrices in Tucker decomposition and the factor tensor in the algebra framework of tensor-tensor product are dense, the two error bounds can be comparable. For a third-order tensor with size n1 \u00d7 n2 \u00d7 n3, the error bound of the estimator in Theorem 4.2 is O( r1r2r3+ \u22113\ni=1 rini m log(nm)), while it is O( rn1n3+rn2n3 m log(n1 \u2228n2)) in [54], where r\nis the tubal rank of the underlying tensor. In this case, the error bound in Theorem 4.2 is smaller than that in [54] if ri is much smaller than ni and r is close to ri. In particular, if n1 = n2 = n3 = n and r1 = r2 = r3 = r, the error bound in Theorem 4.2 is O( r3+3rn m log(n)), which is smaller than that (i.e., O(2rn 2\nm log(n))) in [54] if r \u2264 2n \u2212 3 and n \u2265 3. These conditions can be satisfied easily in real-world applications since n is generally large.\nRemark 4.5 Jain et al. [24] proposed a noisy tensor completion model based on CP decomposition with a special sparse factor for an n1 \u00d7 n2 \u00d7 n3 tensor, where the third factor matrix is sparse in CP decomposition. In particular, when the constraints are nonnegative, the model in [24] reduces to sparse nonnegative CP decomposition and completion. In this case, the error bound of the estimator of their model for the observations with additive Gaussian noise was\nE\u2126,Y\u2126 [\u2016Xcp \u2212 X \u2217\u20162F ] n1n2n3 = O\n( (n1 + n2)r + \u2016C\u2217\u20160\nm log(max{n1, n2, n3})\n) , (13)\nwhere Xcp is the estimator by [24], r is the CP rank of X \u2217, and C\u2217 is the third factor matrix in the CP decomposition of X \u2217. However, (13) is hard to compare with the error bound in Theorem 4.2 directly since the CP rank of a tensor is not comparable to the Tucker rank of a tensor in general. Also the factor matrices are typically different between the CP and Tucker decompositions of a tensor. In particular, if the core tensor in Tucker decomposition is diagonal and r1 = r2 = r3 = r, the error bound in Theorem 4.2 is comparable with (13). In fact, the error bound in Theorem 4.2 is smaller than (13) if r is much smaller than ni."
        },
        {
            "heading": "4.2 Additive Laplace Noise",
            "text": "In this subsection, we establish the error bound of the estimator of model (6) for the observations with additive Laplace noise. In this case, the observation Y\u2126 is modeled by\nYi1\u00b7\u00b7\u00b7id = X \u2217i1\u00b7\u00b7\u00b7id +Ni1\u00b7\u00b7\u00b7id , (i1, . . . , id) \u2208 \u2126, (14)\nwhere Ni1\u00b7\u00b7\u00b7id are independent Laplace distribution with parameters (0, \u03c4), \u03c4 > 0, which is denoted by Laplace(0, \u03c4). The joint probability density function of Y\u2126 is given by\npX \u2217\u2126(Y\u2126) = (\u03c4 2 )|\u2126| exp ( \u2212\u2016Y\u2126 \u2212 X \u2217 \u2126\u20161 \u03c4 ) . (15)\nNext we establish an upper error bound of the estimator of model (6) for the observations satisfying\nthe noisy model (14).\nTheorem 4.3 Let \u03b2 and \u03bbi be defined as (10) and (11), i \u2208 [d], where \u03b3 = c 2\n2\u03c42 . Assume that the\nsampling set \u2126 \u223c Bern(p) with p = mn1n2\u00b7\u00b7\u00b7nd and the joint probability density function of Y\u2126 is given by (15). Then the estimator of (6) satisfies\nE\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7 nd \u2264 11c\n2(2\u03c4 + c)2 log(m)\n2\u03c42m\n+ 12 ( 1 + 2c2\n3\u03c42\n) (2\u03c4 + c)2(\u03b2 + 2) log(nm) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d\ni=1 \u2016A\u2217i \u20160 m .\nFrom Theorem 4.3, we know that the upper error bound of the estimator of model (6) is on the order\nof O( r1r2\u00b7\u00b7\u00b7rd+\n\u2211d i=1 \u2016A\u2217i \u20160\nm log(nm)). Similar to the case of additive Gaussian noise, the error bound of the estimator of model (6) decreases as the number of observed samples increases. Moreover, if the factor matrices of Tucker decomposition are sparser, the error bound obtained by (6) is lower.\nRemark 4.6 For the observations with additive Laplace noise, we compared the error bound in Theorem 4.3 with that of the matrix based method in [46, Corollary 5]. For a dth-order tensor X \u2217 \u2208 R n1\u00d7\u00b7\u00b7\u00b7\u00d7nd + with Tucker decomposition in (3), we unfold it into a matrix X \u2217(1) = A\u22171(C\u2217(1)(\u22972i=dA\u2217i )T ) \u2208 R n1\u00d7(n2\u00b7\u00b7\u00b7nd) + along the first mode, where A \u2217 1 is sparse. The matrix based method in [46] is then applied to (X \u2217(1))T \u2208 R (n2\u00b7\u00b7\u00b7nd)\u00d7n1 + . And the error bound of the estimator in [46, Corollary 5] is on the order of O( r1n2n3\u00b7\u00b7\u00b7nd+\u2016A\u22171\u20160\nm log(n1 \u2228 (n2 \u00b7 \u00b7 \u00b7nd))). Therefore, if ri is much smaller than ni, i \u2208 [d], and d is not too large, the error bound of the estimator by model (6) is smaller than that of the matrix based method in [46]. In particular, if r1 = \u00b7 \u00b7 \u00b7 = rd = r and n1 = \u00b7 \u00b7 \u00b7 = nd = n, the error bound of the estimator in Theorem 4.3 is the order of O( rd+ \u2211d\ni=1 \u2016A\u2217i \u20160 m log(n)), while the error bound of the\nestimator of the matrix based method in [46] is O( rnd\u22121+\u2016A\u22171\u20160\nm (d \u2212 1) log(n)). This implies that the error bound of the estimator in Theorem 4.3 is smaller than that of the matrix based method in [46] as long as r is much smaller than n and \u2016A\u2217i \u20160 is close to \u2016A\u22171\u20160, i = 2, . . . , d.\nRemark 4.7 Similar to the case for the observations with additive Gaussian noise, we compare the error bound in Theorem 4.3 with that of the sparse nonnegative tensor factorization method under the tensor-tensor product framework in [54], which is only effective for third-order tensors. Since the spare factors are different between the sparse NTD and sparse nonnegative tensor factorization with tensor-tensor product in [54], it is difficult to compare the error bounds of the two methods directly.\nHowever, if the factor tensor is not sparse, the error bound of the estimator in [54, Proposition 4.2] is on the order of O( rn1n3+rn2n3m log(n1 \u2228 n2)), where r is the tubal rank of the underlying tensor. The error upper bound of the estimator by model (6) is on the order of O( r1r2r3+ \u22113\ni=1 rini m log(nm)), which\nis smaller than that of [54] if r is close to ri and ri is much smaller than ni, i = 1, 2, 3."
        },
        {
            "heading": "4.3 Poisson Observations",
            "text": "In this subsection, we establish an upper error bound of the estimator for Poisson observations, where the observations are modeled as\nYi1\u00b7\u00b7\u00b7id = Poisson(X \u2217i1\u00b7\u00b7\u00b7id), (i1, . . . , id) \u2208 \u2126.\nHere y = Poisson(x) denotes that y obeys a Poisson distribution with parameter x > 0. The joint probability mass function of Y\u2126 is given by\npX \u2217\u2126(Y\u2126) = \u220f\n(i1,i2,...,id)\u2208\u2126\n(X \u2217i1i2\u00b7\u00b7\u00b7id) Yi1i2\u00b7\u00b7\u00b7id exp(\u2212X \u2217i1i2\u00b7\u00b7\u00b7id) (Yi1i2\u00b7\u00b7\u00b7id)! . (16)\nNow we establish an upper error bound of the estimator of model (6), where the joint probability\nmass function of Y\u2126 satisfies (16).\nTheorem 4.4 Suppose that mini1,i2,...,id X \u2217i1i2\u00b7\u00b7\u00b7id \u2265 \u033a and each entry of the tensor in the candidate set \u03a5 is also not smaller than \u033a, where \u033a > 0 is a given constant. Let \u03b2 and \u03bbi be defined as (10) and (11), i \u2208 [d], where \u03b3 = c2\u033a in (11). Assume that the sampling set \u2126 \u223c Bern(p) with p = mn1n2\u00b7\u00b7\u00b7nd and the joint probability mass function of Y\u2126 is given by (16). Then the estimator in (6) satisfies E\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 44c\n3 log(m)\n\u033am + 48c\n( 1 + 4c2\n3\u033a\n) (\u03b2 + 2) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d\ni=1 \u2016A\u2217i \u20160 m log(nm).\nBy Theorem 4.4, one can obtain that the upper error bound of the estimator of model (6) with\nPoisson observations is on the order of O( r1r2\u00b7\u00b7\u00b7rd+\n\u2211d i=1 \u2016A\u2217i \u20160\nm log(nm)). Moreover, the error bound will decrease as the number of observations increases. The lower bound of X \u2217 will also influence the error bound of the estimator, and it will be difficult to recover X \u2217 as \u033a is close to zero.\nRemark 4.8 In order to compare the error bound in Theorem 4.4 with that of the matrix based method in [46, Corollary 6], we need to unfold the underlying nonnegative tensor X \u2217 (n1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nd) into a matrix, where the multi-linear structure of a tensor is ignored. Without loss of generality, assume that X \u2217 is unfolded along the first mode, in which the resulting matrix is X \u2217(1) = A\u22171(C\u2217(1)(\u22972i=dA\u2217i )T ) \u2208 R n1\u00d7(n2\u00b7\u00b7\u00b7nd) + . Then, by applying the matrix based method in [46] to (X \u2217(1))T , the error bound of the resulting estimator is on the order of O( r1n2n3\u00b7\u00b7\u00b7nd+\u2016A\u22171\u20160\nm log(n1 \u2228 (n2 \u00b7 \u00b7 \u00b7nd))), which is larger than that of Theorem 4.4 if ri is much smaller than ni, i \u2208 [d], and d is not too large. In particular, when r1 = \u00b7 \u00b7 \u00b7 = rd = r, n1 = \u00b7 \u00b7 \u00b7 = nd = n, and \u2016A\u22171\u20160 = \u00b7 \u00b7 \u00b7 = \u2016A\u2217d\u20160, the error bound of the estimator in [46, Corollary 6] is on the order of O(\nrnd\u22121+\u2016A\u22171\u20160 m (d\u2212 1) log(n)), and the error bound in Theorem\n4.4 is O( rd+d\u2016A\u22171\u20160\nm log(n)), which is smaller than that of [46] as long as d \u2265 3 and r is much smaller than n.\nRemark 4.9 Cao et al. [7] proposed a matrix completion method with the nuclear norm constraint for Poisson observations and established the error bound of the estimator of their proposed model.\nFor higher-order tensor completion, without loss of generality, we unfolding the underlying tensor X \u2217 \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd+ into a matrix along the first mode, i.e., X \u2217(1). In this case, when m \u2265 (n1 + n2 \u00b7 \u00b7 \u00b7 nd) log(n1 \u00b7 \u00b7 \u00b7nd), the error bound of the estimator obtained by [7, Theorem 2] is\n\u2016Xmt \u2212 X \u2217\u20162F n1n2 \u00b7 \u00b7 \u00b7nd\n\u2264 C \u221a\nr1(n1 + n2 \u00b7 \u00b7 \u00b7 nd) m log(n1 \u00b7 \u00b7 \u00b7nd) (17)\nwith high probability, where Xmt is the estimator obtained by [7] and C > 0 is a constant. In particular, when n1 = \u00b7 \u00b7 \u00b7 = nd = n and r1 = \u00b7 \u00b7 \u00b7 = rd, the upper bound in (17) reduces to O(d \u221a r1(n+nd\u22121)\nm log(n)), which is larger than O( rd1+ \u2211d i=1 \u2016A\u2217i \u20160 m log(n)) if r1 is much smaller than n\n(e.g., r1 < ( nd\u22121 4 ) 1 2d\u22121 ). Moreover, the factor matrix Ai is sparser, the error bound in Theorem 4.4 is lower, while the factor matrix Ai has not influence on the error bound in [7, Theorem 2].\nRemark 4.10 Similar to the observations with additive Laplace noise, we can only compare the error bound in Theorem 4.4 with that in [54, Proposition 4.3] for third-order tensors, which has smaller error bound than that of [53, Theorem 3.1] for low-rank tensor completion with Poisson observations. More detailed comparisons about the error bounds of the two methods can be referred to [53, Section IV.C]. We do not consider the sparse case since the factor tensor based on the algebra framework of tensor-tensor product is not comparable with the factor matrices in Tucker decomposition. In this case, the error bound of the estimator in [54, Proposition 4.3] is O( rn1n3+rn2n3m log(n1\u2228n2)), where r is the tubal rank of the underlying tensor. By Theorem 4.4, we obtain that the error bound of the estimator of model (6) is on the order of O( r1r2r3+ \u22113 i=1 rini\nm log(n1 \u2228 n2 \u2228 n3)), which is smaller than that of [54] if r is close to ri and ri is much smaller than ni, i = 1, 2, 3."
        },
        {
            "heading": "5 Minimax Lower Bounds",
            "text": "In this section, we establish the minimax lower bound of an estimator for the observations satisfying (5). The accuracy of an estimator X\u0303 for estimating the true tensor X \u2217 can be measured in terms of its risk [47], which is defined as\nE\u2126,Y\u2126 [\u2016X\u0303 \u2212 X \u2217\u20162F ] n1n2 \u00b7 \u00b7 \u00b7nd .\nNow we consider a class of tensors with Tucker rank r = (r1, . . . , rd) satisfying (3), where each factor matrix in Tucker decomposition obeys \u2016Ai\u20160 \u2264 si, i \u2208 [d], and the amplitudes of each entry of the core tensor and factor matrices are not larger than 1 and ai, respectively. In this case, we define the following set\nL(s, r,a) := { X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad : C \u2208 Rr1\u00d7\u00b7\u00b7\u00b7\u00d7rd+ , Ai \u2208 Rni\u00d7ri+ , 0 \u2264 Ci1\u00b7\u00b7\u00b7id \u2264 1, \u2016Ai\u20160 \u2264 si,\n0 \u2264 (Ai)lm \u2264 ai, (i1, . . . , id) \u2208 [r1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [rd], (l,m) \u2208 [ni]\u00d7 [ri], i \u2208 [d] } .\n(18)\nThe worst-case performance of an estimator X\u0303 of X \u2217 is measured by the maximum risk on the set L(s, r,a), which is defined as\nsup X \u2217\u2208L(s,r,a) E\u2126,Y\u2126 [\u2016X\u0303 \u2212 X \u2217\u20162F ] n1n2 \u00b7 \u00b7 \u00b7nd .\nThe estimator, which has the smallest maximum risk among all possible estimators, is said to achieve the minimax risk [47]. In this case, for the sparse NTD and completion problem, the minimax risk is defined as\ninf X\u0303 sup X \u2217\u2208L(s,r,a) E\u2126,Y\u2126[\u2016X\u0303 \u2212 X \u2217\u20162F ] n1n2 \u00b7 \u00b7 \u00b7 nd . (19)\nNext we will estimate the lower bound of (19), which is stated in the following theorem.\nTheorem 5.1 Suppose that the probability density function or probability mass function of any two entries of the noisy observations satisfies\nK(Px,Py) \u2264 (x\u2212 y)2\n2\u00b5 , (20)\nwhere \u00b5 > 0 depends on the noise distribution. Assume that ri \u2264 si in the set L(s, r,a) in (18), i \u2208 [d]. Then, for the joint probability density function or probability mass function of the observations Y\u2126 obeying (5), there exist two constants \u03b1\u0303m, \u03b3m > 0 such that\ninf X\u0303 sup X \u2217\u2208L(s,r,a) E\u2126,Y\u2126[\u2016X\u0303 \u2212 X \u2217\u20162F ] n1n2 \u00b7 \u00b7 \u00b7 nd\n\u2265 \u03b1\u0303m 2d+5(d+ 1) min\n{ d\u220f\ni=1\n\u2206i(si, ni)a 2 i , \u03b3 2 m\u00b5 2\n( r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 si\nm\n)} ,\nwhere\n\u2206i(si, ni) = min\n{ 1,\nsi ni\n} , i \u2208 [d]. (21)\nFor a fixed d, Theorem 5.1 shows that the lower bound of the estimator satisfying (18) is on the\norder of O( r1r2\u00b7\u00b7\u00b7rd+\n\u2211d i=1 si\nm ), which matches to the upper bound in Theorem 4.1 up to a logarithmic factor. This demonstrates the upper bound in Theorem 4.1 is nearly optimal.\nRemark 5.1 The proof of the minimax risk bound in Theorem 5.1 utilized the technique of the tools for matrix completion in [26, 43], see also [54]. The key issue of the proof is to construct the packing sets for the core tensors and sparse factor matrices, respectively. Then the Varshamov-Gilbert bound [47, Lemma 2.9] is used to determine the Frobenius norm of any two tensors in the set. Finally, one can get the minimax lower bound by the minimax analysis techniques in [47, Theorem 2.5].\nRemark 5.2 Based on Theorem 5.1, for the explicit lower bounds of the minimax risk with special noise models in Section 4, we just need to specify \u00b5 in (20) except for Poisson observations. In particular, for the observations of sparse NTD and completion with additive Gaussian noise and additive Laplace noise, we can set \u00b5 = \u03c3 and \u00b5 = \u03c4 , respetively. For Poisson observations, we need each entry of the underlying tensor to be strictly positive in the construction of the packing set. Similar results can be found in [43, 54]. For brevity, we omit the detailed proof here for sparse NTD and completion with Poisson observations."
        },
        {
            "heading": "6 Optimization Algorithm",
            "text": "In this section, we design an ADMM based algorithm [6] to solve problem (6). Notice that the feasible set \u03a0 in (7) is discrete, which leads to the difficulty for the algorithm design. Similar to [24, 54],\nwe drop the discrete assumption of the core tensor and factor matrices in order to use continuous optimization techniques, which may be justified by choosing a very large value of \u03c4 and by noting that continuous optimization algorithms use finite precision arithmetic when executed on a computer. Hence, we consider to solve the following problem:\nmin X ,C,Ai\n\u2212 log pX\u2126(Y\u2126) + d\u2211\ni=1\n\u03bbi\u2016Ai\u20160\ns.t. X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad, 0 \u2264 Xi1i2\u00b7\u00b7\u00b7id \u2264 c, (i1, i2, . . . , id) \u2208 [n1]\u00d7 [n2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd], 0 \u2264 Ci1i2\u00b7\u00b7\u00b7id \u2264 1, (i1, i2, . . . , id) \u2208 [r1]\u00d7 [r2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [rd], 0 \u2264 (Ai)ml \u2264 ai, (m, l) \u2208 [ni]\u00d7 [ri], i \u2208 [d]. (22)\nLet\n\u039e1 := { X \u2208 Rn1\u00d7n2\u00d7\u00b7\u00b7\u00b7\u00d7nd+ : 0 \u2264 Xi1i2\u00b7\u00b7\u00b7id \u2264 c, (i1, i2, . . . , id) \u2208 [n1]\u00d7 [n2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd] } , \u039e2 := { C \u2208 Rr1\u00d7r2\u00d7\u00b7\u00b7\u00b7\u00d7rd+ : 0 \u2264 Ci1i2\u00b7\u00b7\u00b7id \u2264 1, (i1, i2, . . . , id) \u2208 [r1]\u00d7 [r2]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [rd] } , \u03a8i := { Ai \u2208 Rni\u00d7ri+ : 0 \u2264 (Ai)ml \u2264 ai, (m, l) \u2208 [ni]\u00d7 [ri] } , i \u2208 [d].\nThen problem (22) can be rewritten as\nmin X ,C,Ai\n\u2212 log pX\u2126(Y\u2126) + d\u2211\ni=1\n\u03bbi\u2016Ai\u20160 + \u03b4\u039e1(X ) + \u03b4\u039e1(C) + d\u2211\ni=1\n\u03b4\u03a8i(Ai)\ns.t. X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad, (23)\nwhere \u03b4(\u00b7) denotes the indicator function over a set. Let X = Z, C = B, Ai = Hi, Ai = Si, i \u2208 [d]. Then problem (23) is equivalent to\nmin \u2212 log pX\u2126(Y\u2126) + d\u2211\ni=1\n\u03bbi\u2016Hi\u20160 + \u03b4\u039e1(Z) + \u03b4\u039e2(B) + d\u2211\ni=1\n\u03b4\u03a8i(Si)\ns.t. X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad,X = Z, C = B, Ai = Hi, Ai = Si, i \u2208 [d]. (24)\nThe augmented Lagrangian function associated with problem (24) is given by\nL(X , C, Ai,Z,B,Hi, Si,Ti,Mi, Ni)\n=\u2212 log pX\u2126(Y\u2126) + d\u2211\ni=1\n\u03bbi\u2016Hi\u20160 + \u03b4\u039e1(Z) + \u03b4\u039e2(B) + d\u2211\ni=1\n\u03b4\u03a8i(Si)\n+ \u3008T1,X \u2212 C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad\u3009+ \u3008T2,X \u2212 Z\u3009+ \u3008T3, C \u2212 B\u3009 + d\u2211\ni=1\n\u3008Mi, Ai \u2212Hi\u3009+ d\u2211\ni=1\n\u3008Ni, Ai \u2212 Si\u3009\n+ \u03b21 2 \u2016X \u2212 C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad\u20162F + \u03b22 2 \u2016X \u2212 Z\u20162F + \u03b23 2 \u2016C \u2212 B\u20162F\n+\nd\u2211\ni=1\n(\u03c1i 2 \u2016Ai \u2212Hi\u20162F + \u03b1i 2 \u2016Ai \u2212 Si\u20162F ) ,\nwhere T1,T2,T3,Mi, Ni are the Lagrangian multipliers and \u03b21, \u03b22, \u03b23, \u03c1i, \u03b1i > 0 are penalty parameters, i \u2208 [d]. The iteration template of ADMM is given as follows:\nX k+1 = argmin X L(X , Ck, Aki ,Zk,Bk,Hki , Ski ,T ki ,Mki , Nki ), (25) Ck+1 = argmin C L(X k+1, C, Aki ,Zk,Bk,Hki , Ski ,T ki ,Mki , Nki ), (26) Ak+1i = argmin Ai L(X k+1, Ck+1, Ai,Zk,Bk,Hki , Ski ,T ki ,Mki , Nki ), (27) Zk+1 = argmin Z L(X k+1, Ck+1, Ak+1i ,Z,Bk,Hki , Ski ,T ki ,Mki , Nki ), (28) Bk+1 = argmin B L(X k+1, Ck+1, Ak+1i ,Zk+1,B,Hki , Ski ,T ki ,Mki , Nki ), (29) Hk+1i = argmin Hi L(X k+1, Ck+1, Ak+1i ,Zk+1,Bk+1,Hi, Ski ,T ki ,Mki , Nki ), (30) Sk+1i = argmin Si L(X k+1, Ck+1, Ak+1i ,Zk+1,Bk+1,Hk+1i , Si,T ki ,Mki , Nki ), (31) T k+11 = T k1 + \u03b21(X k+1 \u2212 Ck+1 \u00d71 Ak+11 \u00b7 \u00b7 \u00b7 \u00d7d Ak+1d ), (32) T k+12 = T k2 + \u03b22(X k+1 \u2212Zk+1), T k+13 = T k3 + \u03b23(Ck+1 \u2212 Bk+1), (33) Mk+1i = M k i + \u03c1i(A k+1 i \u2212Hk+1i ), Nk+1i = Nki + \u03b1i(Ak+1i \u2212 Sk+1i ), i \u2208 [d]. (34)\nNext we give the explicit solutions of each subproblem in the ADMM. Let\nf(X ) := \u2212 log pX\u2126(Y\u2126).\nThe optimal solution of (25) with respect to X is given by\nX k+1 = Prox 1 \u03b21+\u03b22 f\n( 1\n\u03b21 + \u03b22\n( \u03b21Ck \u00d71 Ak1 \u00b7 \u00b7 \u00b7 \u00d7d Akd \u2212 T k1 + \u03b22Zk \u2212 T k2 )) . (35)\nAccording to (1), problem (26) is equivalent to\nmin C \u03b21 2 \u2225\u2225\u2225\u2225vec ( X k+1 + 1 \u03b21 T k1 ) \u2212 (\u22971i=dAki )vec(C) \u2225\u2225\u2225\u2225 2\nF\n+ \u03b23 2 \u2225\u2225\u2225\u2225vec(C)\u2212 vec ( Bk \u2212 1 \u03b23 T k3 )\u2225\u2225\u2225\u2225 2\nF\n.\n(36)\nThe optimal solution of (36) is given by\nvec(Ck+1) = ( \u03b21(\u22971i=dAki )T (\u22971i=dAki ) + \u03b22I )\u22121 qk\n= ( \u03b21(\u22971i=d(Aki )TAki ) + \u03b22I )\u22121 qk,\n(37)\nwhere qk := (\u22971i=dAki )T vec ( \u03b21X k+1 + T k1 ) + vec ( \u03b23Bk \u2212 T k3 ) and the second equality holds by [22, Lemma 4.2.10].\nThe problem (27) can be reformulated as\nmin Ai \u03b21 2\n\u2225\u2225\u2225\u2225X k+1(i) \u2212AiRki + 1\n\u03b21 (T k1 )(i)\n\u2225\u2225\u2225\u2225 2\nF\n+ \u03c1i 2 \u2225\u2225\u2225\u2225Ai \u2212Hki + 1 \u03c1i Mki \u2225\u2225\u2225\u2225 2\nF\n(38)\nwhere\nRki := Ck+1(i) (Akd \u2297 \u00b7 \u00b7 \u00b7 \u2297Aki+1 \u2297A k+1 i\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297Ak+11 )T . (39)\nThe optimal solution of (38) is represented as\nAk+1i = (( \u03b21X k+1(i) + (T k 1 )(i) ) (Rki ) T + \u03c1iH k i \u2212Mki )( \u03b21R k i (R k i ) T + \u03c1iI )\u22121 . (40)\nThe optimal solution of (28) with respect to Z is given by\nZk+1 = argmin Z \u03b4\u039e1(Z) + \u03b22 2 \u2225\u2225\u2225\u2225Z \u2212 ( X k+1 + 1 \u03b22 T k2 )\u2225\u2225\u2225\u2225 2\nF\n= P\u039e1\n( X k+1 + 1 \u03b22 T k2 ) , (41)\nwhere P\u039e1(\u00b7) denotes the projection onto a given set \u039e1. Similarly, the optimal solution of (29) with respect to B is given by\nBk+1 = argmin B \u03b4\u039e2(B) + \u03b23 2 \u2225\u2225\u2225\u2225B \u2212 ( Ck+1 + 1 \u03b23 T k3 )\u2225\u2225\u2225\u2225 2\nF\n= P\u039e2\n( Ck+1 + 1 \u03b23 T k3 ) . (42)\nThe optimal solutions of (30) and (31) are given by\nHk+1i = argmin Hi \u03bbi\u2016Hi\u20160 + \u03c1i 2\n\u2225\u2225\u2225\u2225Hi \u2212 ( Ak+1i + 1\n\u03c1i Mki\n)\u2225\u2225\u2225\u2225 2\nF\n= Prox\u03bbi \u03c1i \u2016\u00b7\u20160\n( Ak+1i + 1\n\u03c1i Mki\n) (43)\nand\nSk+1i = argmin Si \u03b4\u03a5i(Si) + \u03b1i 2\n\u2225\u2225\u2225\u2225Si \u2212 ( Ak+1i + 1\n\u03b1i Nki\n)\u2225\u2225\u2225\u2225 2\nF\n= P\u03a8i ( Ak+1i + 1\n\u03b1i Nki\n) , (44)\nwhere the proximal mapping of the \u21130 norm can be implemented in a point-wise manner, and for any \u03bb > 0, its scalar form is given by (see [4, Example 6.10], for example)\nProx\u03bb\u2016\u00b7\u20160(y) =    0, if |y| < \u221a 2\u03bb, {0, y}, if |y| = \u221a 2\u03bb,\ny, if |y| > \u221a 2\u03bb.\nThe template of the ADMM for solving (24) is presented in Algorithm 1.\nAlgorithm 1 Alternating Direction Method of Multipliers for Solving Problem (24).\n1: Initialization: C0, A0i ,Z0,B0,T 01 ,T 02 ,T 03 ,M0i , i \u2208 [d]. 2: repeat 3: Step 1. Compute X k+1 by (35). 4: Step 2. Update Ck+1 by (37). 5: Step 3. Compute Ak+1i via (40), i \u2208 [d]. 6: Step 4. Compute Zk+1 by (41). 7: Step 5. Update Bk+1 according to (42). 8: Step 6. Compute Hk+1i and S k+1 i by (43) and (44), respectively, i \u2208 [d].\n9: Step 7. Update T k+11 , T k+12 , T k+13 , Mk+1i , Nk+1i by (32), (33) and (34), respectively. 10: until A stopping condition is satisfied.\nIn the implementable of Algorithm 1, one needs to compute the proximal mapping of 1\u03b21+\u03b22 f at H, where\nH := 1 \u03b21 + \u03b22\n( \u03b21Ck \u00d71 Ak1 \u00b7 \u00b7 \u00b7 \u00d7d Akd \u2212 T k1 + \u03b22Zk \u2212 T k2 ) . (45)\nIn particular, for the special noise observation models including additive Gaussian noise, additive Laplace noise, and Poisson observations, the proximal mapping in (35) are given in detail in [54].\n\u2022 Additive Gaussian noise: The joint probability density function satisfies (12) and then\nX k+1 = 1 1 + \u03c32(\u03b21 + \u03b22)\nP\u2126 ( Y + \u03c32(\u03b21 + \u03b22)H ) + P\u2126(H),\nwhere \u2126 is the complementary set of \u2126 and P\u2126 is the projection operator onto the index \u2126 such that\n(P\u2126 (A))i1i2\u00b7\u00b7\u00b7id = { Ai1i2\u00b7\u00b7\u00b7id , if (i1, i2, \u00b7 \u00b7 \u00b7 , id) \u2208 \u2126, 0, otherwise.\n\u2022 Additive Laplace noise: The joint probability density function satisfies (15) and then\nX k+1 = P\u2126 ( Y + sign(H\u2212 Y) \u25e6max { |H \u2212 Y| \u2212 1\n\u03c4(\u03b21 + \u03b22) , 0\n}) + P\u2126(H),\nwhere sign(\u00b7) and \u25e6 represents the signum function and the Hadamard point-wise product, respectively.\n\u2022 Poisson observations: The joint probability density function satisfies (16) and then\nX k+1 = 1 2(\u03b21 + \u03b22)\nP\u2126 ( (\u03b21 + \u03b22)H\u2212 1+ \u221a ((\u03b21 + \u03b22)H\u2212 1)2 + 4(\u03b21 + \u03b22)Y ) +P\u2126(H),\nwhere 1 represents an n1 \u00d7 n2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 nd tensor with all entries being 1, the square root and square are performed in the point-wise manner.\nNow the computational complexity of ADMM for solving (24) is given as follows. If the observations are chosen as additive Gaussian noise, additive Laplace noise, or Poisson observations, the computation cost of X k+1 is on the order of O(\u2211dj=1( \u220fj i=1 ni)( \u220fd i=j ri)), which mainly follows from the computational cost of (45). The computational cost of Ck+1 is O(\u2211di=1 r2i ni + \u220fd\ni=1 r 3 i +\u2211d\nj=1( \u220fj i=1 ri)( \u220fd i=j ni)). In the implementation of R k i in (39), we do not compute the Kronecker product directly. Let Ki := Ck+1 \u00d71 Ak+11 \u00b7 \u00b7 \u00b7 \u00d7i\u22121 Ak+1i\u22121 \u00d7i+1 Aki+1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7d Akd. Then (Ki)(i) = Rki , which leads to the computation of Rki via (Ki)(i). The computational cost of Ki is [51, Appendix B]\nO\n  i\u22121\u2211\nj=1\n( j\u220f\nt=1\nnt\n)  d\u220f\nt=j\nrt  + ri ( i\u22121\u220f\nt=1\nnt\n) d\u2211\nj=i+1\n( j\u220f\nt=i+1\nnt\n)  d\u220f\nt=j\nrt\n    . (46)\nFor any i \u2208 [d], the computational cost of each factor matrix Ai is the sum of (46) and O(rin1 \u00b7 \u00b7 \u00b7 nd + r3i +nir 2 i ). The computational costs of Zk+1 and Bk+1 are O( \u220fd i=1 ni) and O( \u220fd i=1 ri), respectively. The computational cost of Hk+1i and S k+1 i are both O(niri). The computational costs of the multipliers\nT1,T2,T3,Mi, Ni are O( \u2211d j=1( \u220fj i=1 ni)( \u220fd\ni=j ri)). Note that ri \u2264 ni, i \u2208 [d], and the quantities of (46) and O( \u2211d j=1( \u220fj i=1 ri)( \u220fd i=j ni)) are similar, where the detailed discussions can be found in [51, Appendix B]. Therefore, the computational cost of ADMM in Algorithm 1 is\nO  d d\u2211\nj=1\n( j\u220f\ni=1\nni\n)  d\u220f\ni=j\nri\n + d\u2211\nj=1\n( j\u220f\ni=1\nri\n)  d\u220f\ni=j\nni\n + d\u2211\ni=1\nr2i ni + d\u220f\ni=1\nr3i +\n( d\u220f\ni=1\nni\n) d\u2211\ni=1\nri\n  .\nRemark 6.1 Since the constraint and the \u21130 norm in the objective of (24) are both nonconvex, the convergence of ADMM cannot be guaranteed in general. Although great efforts have been made about the convergence of ADMM for solving nonconvex problems, the problem with both constraints and objective are nonconvex, where the objective is not Lipschitz continuous for general loss function mentioned in Section 4. Therefore, the existing literature about the convergence of ADMM for nonconvex problems (e.g., see [5, 21, 50]) cannot be applied to our model directly."
        },
        {
            "heading": "7 Numerical Experiments",
            "text": "In this section, we evaluate the effectiveness the proposed sparse NTD and completion model (SNTDC) using both synthetic data and real image data. We compare SNTDC with a matrix based method in [46] (denoted by Matrix) and sparse nonnegative tensor factorization and completion with tensor-tensor product (SNTFTP) [54]. All experiments are performed in MATLAB 2020a on a computer with Intel Xeon W-2133 and 32 GB of RAM.\nAlgorithm 1 will be stopped if the maximum number of iterations 300 is reached or the following condition is satisfied\n\u2016X k+1 \u2212 X k\u2016F \u2016X k\u2016F \u2264 10\u22124,\nwhere X k is the kth iteration of Algorithm 1. We use a sequentially truncated high order singular value decomposition [49] on P\u2126(Y) to get the initial values C0, A0i , i \u2208 [d]. Other initial values in Algorithm 1 are the same as P\u2126(Y), or C0, or A0i , i \u2208 [d] as long as their sizes are the same. For the regularization parameters \u03bbi, we set \u03bbi to the same for any i \u2208 [d] and choose them from the set {5, 10, 50} to get the best recovery performance. Moreover, \u03b2i are set the same for different i and chosen from the set {50, 70, 100, 250, 350, 450, 550}. For \u03b1i and \u03c1i, we set them the same in each case for any i \u2208 [d], and choose them from the set {0.1, 0.01, 0.001} to get the best recovery performance."
        },
        {
            "heading": "7.1 Synthetic Data",
            "text": "In this subsection, we test the synthetic data to demonstrate the effectiveness of the proposed method. The synthetic tensor is generated as follows: The underlying tensor has the Tucker decomposition X = C\u00d71A1\u00d72A2 \u00b7 \u00b7 \u00b7\u00d7dAd,where the core tensor C is generated by MATLAB command rand([n1, . . . , nd]), and the factor matrices Ai are generated by ai \u00b7 sptenrand([ni, ri], \u03c5i), i \u2208 [d]. Here \u03c5i is the sparse ratio of Ai, i.e., \u03c5i =\n\u2016Ai\u20160 niri , i \u2208 [d]. We set c = 2\u2016X\u2016\u221e. For these synthetic data, we test 10 trials in each case and the average result of these trials is the last result.\nWe first test the third-order tensors with n1 = n2 = n3 = 100, whose Tucker rank is (5, 5, 5). For the observations with additive Gaussian noise and additive Laplace noise, we set \u03c32 = 0.01 and \u03c4 = 0.01, respectively. In Figure 1, we show the relative error versus sampling ratios of different methods for the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations, respectively, where \u03c5i = 0.3, i = 1, 2, 3. It can be observed that the relative errors of the matrix based method, SNTFTP, and SNTDC decrease as the sampling ratio increases. Moreover, the relative errors obtained by SNTDC are smaller than those obtained by the matrix based method and SNTFTP for the three kinds of observations.\nNow we test a fourth-order tensor 50\u00d750\u00d750\u00d750 with Tucker rank (5, 5, 5, 5), where \u03c32 = 0.01 for additive Gaussian noise, \u03c4 = 0.01 for additive Laplace noise, and \u03c5i = 0.3, i = 1, 2, 3. Since the SNTFTP is only effective for third-order tensors, we do not compare with it for fourth-order tensors in this experiment. In Figure 2, we show the relative error versus sampling ratio for the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations, respectively. We can see that the relative errors of the SNTDC are much smaller than those of the matrix based method for the observations with different noise distributions. Similarly, the relative errors decrease as the number of samples increase for both the matrix based method and SNTDC."
        },
        {
            "heading": "7.2 Image Data",
            "text": "In this subsection, we test the Swimmer dataset1 (32\u00d7 32\u00d7 256) [14] and the Columbia Object Image Library (COIL-100)2 for sparse nonnegative Tucker decomposition and completion. For the Swimmer dataset, the size of each image is 32 \u00d7 32 and there are 256 swimmer images. Similar to [51], the Tucker rank of the Swimmer dataset is set to (24, 20, 20) in Algorithm 1. For the COIL-100, which contains 100 objects, we resize each image to 64\u00d7 64\u00d7 3 and choose first 50 images for the fifty-sixth object, In this case, the size of the resulting tensor is 64 \u00d7 64 \u00d7 3 \u00d7 50, whose Tucker rank is set to (15, 15, 3, 10) in Algorithm 1.\nIn Figure 3, we show the relative error versus sampling ratio of different methods for the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations, respectively, where \u03c32 = 0.01 for additive Gaussian noise and \u03c4 = 0.01 for additive Laplace noise. It can be seen from this figure that the relative errors of the SNTDC are smaller than those of the matrix based method and SNTFC for different sampling ratios. Moreover, the performance of SNTF is better than that of the matrix based method in terms of relative errors for the three kinds of observations. And the relative errors of the matrix based method, SNTFC, and SNTDC decrease as the number of samples increases for the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations.\nSince the SNTFTP is only effective for third-order tensors, we stack all images of the COIL-100\n1https://stodden.net/Papers.html 2 https://www.cs.columbia.edu/CAVE/software/softlib/coil-100.php\nbased on the color channels into a third-order tensors along the frontal slices for the SNTFTP, where the size of the resulting tensor is 64 \u00d7 64 \u00d7 150. In Figure 4, we display the relative error versus sampling ratio of different methods for the COIL-100 dataset, where \u03c32 = 0.05 for additive Gaussian noise and \u03c4 = 0.05 for additive Laplace noise. We can see that the relative errors obtained by the matrix based method, SNTFTP, and SNTDC decrease as the sampling ratio increases for different noise observations. And the relative errors obtained by SNTDC are smaller than those obtained by the matrix based method and SNTFTP. Moreover, the SNTFTP performs better than the matrix based method in terms of relative error for the observations with additive Gaussian noise, additive Laplace noise, and Poisson observations."
        },
        {
            "heading": "8 Conclusions and Future Work",
            "text": "In this paper, we have proposed and studied the problem of sparse NTD and completion with a general class of noise observations, where the underlying tensor is decomposed into a core tensor and several factor matrices with nonnegativity and sparsity constraints. Moreover, the loss function is derived by the maximum likelihood estimation of the noisy observations. The error bound of the estimator of the SNTDC model is established under a class of noise distributions. Then the error bounds are specified to some widely used noise models including additive Gaussian noise, additive Laplace noise, and Poisson observations. Besides, the minimax lower bound of the observed model is derived, which matches the upper bound up to a logarithmic factor. Numerical experiments demonstrate the effectiveness of the proposed sparse NTD and completion model compared with other methods.\nIn the experiments, we need to set the Tucker rank of the underlying tensor in advance, which is\nunknown in general for real-world datasets. An interesting direction for future work on this problem is to update the Tucker rank adaptively. Besides, in the sparse NTD and completion model, the \u21130 norm is employed to characterise the sparsity of the factor matrices. However, the \u21130 norm is nonconvex and it is challenge to get the unique and global optimal solution. Therefore, it is also of great interest to replace the \u21130 norm by the \u21131 norm for the factor matrices and then analyze the error bound of the corresponding model.\nThe convergence of Algorithm 1 cannot be guaranteed in general since our model in (24) is nonconvex and multi-block. Future work is devoted to establishing the convergence of ADMM for our nonconvex multi-block model. Moreover, due to effectiveness and explanatory information of orthogonal constraints on the factor matrices for nonnegative Tucker decomposition (c.f. [40]), it will also be of great interest to extend the error bound of our model to that of orthogonal nonnegative Tucker decomposition under general loss function."
        },
        {
            "heading": "Appendix A. Proof of Theorem 4.1",
            "text": "At the beginning, the following lemma is first recorded, which plays a vital role for the proof of Theorem 4.1.\nLemma 8.1 Let \u03a5 be a countable collection of candidate reconstructions X of X \u2217 and its penalty pen(X ) \u2265 1 satisfying \u2211X\u2208\u03a5 2\u2212pen(X ) \u2264 1. For any integer 2d \u2264 m \u2264 n1n2 \u00b7 \u00b7 \u00b7nd, let \u2126 \u223c Bern(p). Moreover, the joint probability density/mass function of the corresponding observations follows pX \u2217\u2126(Y\u2126) = \u220f (i,j,k)\u2208\u2126 pX \u2217ijk(Yijk), which are assumed to be conditionally independent on the given \u2126. Let \u03b3 be a constant satisfying\n\u03b3 \u2265 max X\u2208\u03a5 max i1,i2,...,id K ( pX \u2217i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)||pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) ) . (47)\nConsider the following complexity penalized maximum likelihood estimator\nX \u00b5 \u2208 arg min X\u2208\u03a5 {\u2212 log pX\u2126(Y\u2126) + \u00b5 \u00b7 pen(X )} . (48)\nThen for any \u00b5 \u2265 2 ( 1 + 2\u03b33 ) log(2), one has\nE\u2126,Y\u2126 [\u22122 logH(pX\u00b5(Y), pX \u2217(Y))] n1n2 \u00b7 \u00b7 \u00b7nd\n\u2264 3 \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y)\nn1n2 \u00b7 \u00b7 \u00b7nd +\n( \u00b5+ 4\u03b3 log(2)\n3\n) pen(X )\nm\n} + 8\u03b3 log(m)\nm ,\nwhere the expectation is taken with respect to the joint distribution of \u2126 and Y\u2126.\nThe proof of Lemma 8.1 can be obtained easily based on the matrix case [46, Lemma 8] and [31], see also for the cases of CP decomposition in [24, Lemma 1] and tensor factorization via tensor-tensor product in [54, Lemma 1.1]. The three steps of proof in [46, Lemma 8] are giving the \u201cgood\u201d sample set characteristics, a conditional error guarantee and some simple conditioning arguments, which are in point-wise manners in fact for the KL divergence, negative logarithmic Hellinger affinity, and maximum likelihood estimation. As a consequence, we can extend them to the tensor case with Tucker decomposition easily. Here we omit the details.\nThe proof of Theorem 4.1 follows the line of the proof of [46, Theorem 1], see also the proofs of [41, Theorem 3] and [54, Theorem 4.1]. The main technique of this proof is the well-known KraftMcMillan inequality [28, 35]. The penalty of the underlying tensor X is constructed by the codes of\nthe core tensors and sparse factor matrices, where the underlying tensor has the Tucker decomposition form with the core tensor being nonnegative and the factor matrices being nonnegative and sparse. Next we return to the proof of Theorem 4.1.\nBased on the result in Lemma 8.1, one should define the penalties pen(X ) \u2265 1 in the candidate reconstructions X of X \u2217 such that the penalties X in \u03a5 satisfy\n\u2211 X\u2208\u03a5 2\u2212pen(X ) \u2264 1. (49)\nThe condition (49) is the well-known Kraft-McMillan inequality for coding elements of \u03a5 with an alphabet of size 2, which is satisfied automatically if we choose the penalties to be code lengths for the unique decodable binary code for the elements X \u2208 \u03a5 [28, 35], see also [11, Section 5]. This also gives the constructions of the penalties.\nLet \u03a51 := {X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad : C \u2208 C, Ai \u2208 Bi}, where C and Bi, i \u2208 [d] are the same as those constructed in Section 3. Note that \u03a51 \u2286 \u03a5 by the construction of \u03a51. Now we consider the discretized core tensor C \u2208 C and sparse factor matrices Ai \u2208 Bi, i \u2208 [d].\n(1) We encode the amplitude of each element of C using log2(\u03c4) bits, where \u03c4 is defined as (8). Then a total of r1r2 \u00b7 \u00b7 \u00b7 rd log2(\u03c4) bits are used to encode the core tensor C.\n(2) Let \u03b6i := 2 \u2308log2(rini)\u2309, i \u2208 [d]. We encode each nonzero element of Ai using log2(\u03b6i) to denote\nthe location and log2(\u03c4) bits for its amplitude. In this case, a total of \u2016Ai\u20160(log2(\u03c4) + log2(\u03b6i)) bits is used to encode the sparse factor matrix Ai.\n(3) Finally, we can assign each X \u2208 \u03a51 whose code length satisfies\npen(X ) = r1r2 \u00b7 \u00b7 \u00b7 rd log2(\u03c4) + d\u2211\ni=1\n\u2016Ai\u20160(log2(\u03c4) + log2(\u03b6i)).\nBy the constructions, we know that such codes are uniquely decodable, which implies that\u2211 X\u2208\u03a51 2\n\u2212pen(X ) \u2264 1 [28, 35]. Since \u03a5 \u2286 \u03a51, one has \u2211\nX\u2208\u03a5 2\u2212pen(X ) \u2264\n\u2211\nX\u2208\u03a51 2\u2212pen(X ) \u2264 1.\nConsequently, by Lemma 8.1, we get that the estimator X \u00b5 in the following\nX \u00b5 \u2208 arg min X\u2208\u03a5 {\u2212 log pX\u2126(Y\u2126) + \u00b5 \u00b7 pen(X )}\n= arg min X\u2208\u03a5\n{ \u2212 log pX\u2126(Y\u2126) + \u00b5 d\u2211\ni=1\n\u2016Ai\u20160(log2(\u03c4) + log2(\u03b6i)) } ,\n(50)\nsatisfies\nE\u2126,Y\u2126 [\u22122 logH(pX\u00b5 , pX \u2217)] n1n2 \u00b7 \u00b7 \u00b7nd \u2264 8\u03b3 log(m) m + 3 \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y)\nn1n2 \u00b7 \u00b7 \u00b7nd +\n( \u00b5+ 4\u03b3 log(2)\n3\n) r1r2 \u00b7 \u00b7 \u00b7 rd log2(\u03c4) + \u2211d i=1 \u2016Ai\u20160(log2(\u03c4) + log2(\u03b6i)) m } ,\nwhere \u00b5 \u2265 2 ( 1 + 2\u03b33 ) log(2) and \u03b3 satisfies (47).\nLet \u03bbi = \u00b5(log2(\u03c4) + log2(\u03b6i)), i \u2208 [d]. Note that ri \u2264 ni and\nlog2(\u03c4) + log2(\u03b6i) \u2264 2(\u03b2 + 2) log(nm)\nlog(2) . (51)\nTherefore, for any \u03bbi \u2265 4(\u03b2 + 2)(1 + 2\u03b33 ) log(nm), i \u2208 [d], the estimator of\nX \u03bb \u2208 arg min X\u2208\u03a5\n{ \u2212 log pX\u2126(Y\u2126) + d\u2211\ni=1\n\u03bbi\u2016Ai\u20160 }\n(52)\nsatisfies\nE\u2126,Y\u2126 [\u22122 logH(pX\u03bb(Y), pX \u2217(Y))] n1n2 \u00b7 \u00b7 \u00b7 nd \u2264 8\u03b3 log(m) m + 3 \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7nd\n+ ( max\ni {\u03bbi}+\n8\u03b3(\u03b2 + 2) log(nm)\n3\n) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016Ai\u20160\nm\n} ,\nwhere the inequality holds by (51). This completes the proof. \u2737"
        },
        {
            "heading": "Appendix B. Proof of Theorem 4.2",
            "text": "First, we establish an upper bound of the tensor infinity norm between the underlying tensor X \u2217 and its closest surrogate in \u03a5, where X \u2217 = C\u2217 \u00d71 A\u22171 \u00d72 A\u22172 \u00b7 \u00b7 \u00b7 \u00d7d A\u2217d, and C\u2217, A\u2217i \u2208 [d] are defined as (4). The following estimate will be useful in the sequel.\nLemma 8.2 Let X \u2217s = Cs\u00d71As1 \u00b7 \u00b7 \u00b7\u00d7dAsd, where the entries of Cs are the closest discretized surrogates of the entries of C\u2217 in \u03a5, and the entries of Asi are the closest discretized surrogates of the entries of A\u2217i in \u03a5, i \u2208 [d]. Then it holds that\n\u2016X \u2217s \u2212 X \u2217\u2016\u221e \u2264 2d+1 \u2212 1 \u03c4 \u2212 1\nd\u220f\ni=1\n(airi).\nwhere \u03c4 is defined as (8).\nProof. Let Cs = C\u2217 +\u25b3C and Asi = A\u2217i +\u25b3Ai, i \u2208 [d]. By the definition of C\u2217 and A\u2217i , i \u2208 [d] in (4), we get that \u2016\u25b3C\u2016\u221e \u2264 1\u03c4\u22121 and \u2016\u25b3Ai\u2016\u221e \u2264 ai\u03c4\u22121 . It follows from (1) that\nX \u2217s \u2212 X \u2217 = (\u22971i=d(A\u2217i +\u25b3Ai))vec(C\u2217 +\u25b3C)\u2212 (\u22971i=dA\u2217i )vec(C\u2217) = (\u22971i=d(A\u2217i +\u25b3Ai))vec(C\u2217) + (\u22971i=d(A\u2217i +\u25b3Ai))(\u25b3C)\u2212 (\u22971i=dA\u2217i )vec(C\u2217).\n(53)\nFurthermore, we have\n\u22971i=d (A\u2217i +\u25b3Ai) = A\u2217d \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22171 +\u25b3Ad \u2297A\u2217d\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22171 + \u00b7 \u00b7 \u00b7 +A\u2217d \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22172 \u2297\u25b3A1\n+\u25b3Ad \u2297\u25b3Ad\u22121 \u2297A\u2217d\u22123 \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22171 + \u00b7 \u00b7 \u00b7+A\u2217d \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22173 \u2297\u25b3A2 \u2297\u25b3A1 + \u00b7 \u00b7 \u00b7 +\u25b3Ad \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u25b3A2 \u2297A\u22171 + \u00b7 \u00b7 \u00b7+A\u2217d \u2297\u25b3Ad\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u25b3A1 +\u25b3Ad \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u25b3A1.\nSince 0 \u2264 (A\u2217i )lm \u2264 ai and 0 \u2264 C\u2217i1i2\u00b7\u00b7\u00b7id \u2264 1, \u2200(l,m) \u2208 [ni] \u00d7 [ri], (i1, i2, . . . , id) \u2208 [r1] \u00d7 [r2] \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [rd], i \u2208 [d], we have that\n\u2016(A\u2217d \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22171)vec(C\u2217)\u2016\u221e \u2264 d\u220f\ni=1\n(airi),\n\u2016(\u25b3Ad \u2297A\u2217d\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22171 + \u00b7 \u00b7 \u00b7 +A\u2217d \u2297 \u00b7 \u00b7 \u00b7 \u2297A\u22172 \u2297\u25b3A1)vec(C\u2217)\u2016\u221e \u2264 C1d \u03c4 \u2212 1 d\u220f\ni=1\n(airi),\n...\n\u2016(A\u2217d \u2297\u25b3Ad\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u25b3A1)vec(C\u2217)\u2016\u221e \u2264 Cd\u22121d (\u03c4 \u2212 1)d\u22121 d\u220f\ni=1\n(airi),\n\u2016(\u25b3Ad \u2297 \u00b7 \u00b7 \u00b7 \u2297 \u25b3A1)vec(C\u2217)\u2016\u221e \u2264 1 (\u03c4 \u2212 1)d d\u220f\ni=1\n(airi),\n(54)\nwhich yields that\n\u2016(\u22971i=d(A\u2217i +\u25b3Ai))vec(C\u2217)\u2212 (\u22971i=dA\u2217i )vec(C\u2217)\u2016\u221e \u2264 ( 1 + 1\n\u03c4 \u2212 1\n)d d\u220f\ni=1\n(airi)\u2212 d\u220f\ni=1\n(airi). (55)\nSimilarly, by \u2016\u25b3C\u2016\u221e \u2264 1\u03c4\u22121 , we can easily get that\n\u2016(\u22971i=d(A\u2217i +\u25b3Ai))(\u25b3C)\u2016\u221e \u2264 1\n\u03c4 \u2212 1\n( 1 + 1\n\u03c4 \u2212 1\n)d d\u220f\ni=1\n(airi). (56)\nNote that\n( 1 + 1\n\u03c4 \u2212 1\n)d+1 \u2212 1 = C1d+1 1\n\u03c4 \u2212 1 + C 2 d+1\n1\n(\u03c4 \u2212 1)2 + \u00b7 \u00b7 \u00b7+ C d+1 d+1\n1\n(\u03c4 \u2212 1)d+1\n= 1\n\u03c4 \u2212 1\n( C1d+1 + C 2 d+1 1\n\u03c4 \u2212 1 + \u00b7 \u00b7 \u00b7+ C d+1 d+1\n1 (\u03c4 \u2212 1)d )\n\u2264 1 \u03c4 \u2212 1\n( C1d+1 + C 2 d+1 + \u00b7 \u00b7 \u00b7 + Cd+1d+1 ) = 2d+1 \u2212 1 \u03c4 \u2212 1 ,\n(57)\nwhere the inequality holds by \u03c4 \u2265 2. Therefore, combining (53), (54), (55) and (56), we obtain that\n\u2016X \u2217s \u2212 X \u2217\u2016\u221e \u2264 ( 1 + 1\n\u03c4 \u2212 1\n)d d\u220f\ni=1\n(airi)\u2212 d\u220f\ni=1\n(airi) + 1\n\u03c4 \u2212 1\n( 1 + 1\n\u03c4 \u2212 1\n)d d\u220f\ni=1\n(airi)\n= (( 1 + 1\n\u03c4 \u2212 1\n)d+1 \u2212 1 ) d\u220f\ni=1\n(airi)\n\u2264 2 d+1 \u2212 1 \u03c4 \u2212 1\nd\u220f\ni=1\n(airi),\n(58)\nwhere the last inequality follows form (57). This furnishes the desired statement. \u2737\nNow we return to the proof of Theorem 4.2. Recall that the negative logarithmic Hellinger affinity\nand KL divergence for additive Gaussian noise are (see, e.g., [46, 54])\n\u2212 log(H(pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id), p(X \u2217)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id))) = (Xi1i2\u00b7\u00b7\u00b7id \u2212 (X \u2217)i1i2\u00b7\u00b7\u00b7id)2\n8\u03c32\nand\nK ( pX \u2217i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)||pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) ) = (Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id)2 2\u03c32 .\nTherefore, we obtain that\n\u2212 2 logH(pX\u03bb(Y), pX \u2217(Y)) = \u2016X \u03bb \u2212 X \u2217\u20162F\n4\u03c32 . (59)\nSince X \u2208 \u03a5 and 0 \u2264 X \u2217i1i2\u00b7\u00b7\u00b7id \u2264 c 2 , we can take \u03b3 = c2 2\u03c32 in (47). Therefore, it follows from Theorem 4.1 that\nE\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 16c\n2 log(m)\nm + 12\u03c32 \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7 nd\n+ ( max\ni {\u03bbi}+\n4c2(\u03b2 + 2) log(nm)\n3\u03c32\n) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016Ai\u20160\nm\n} . (60)\nBy the definition of \u03b2 in (10), we know that\n\u03c4 = 2\u2308log2(nm) \u03b2\u2309 \u2265 2\u03b2 log2(nm) \u2265 2\nlog2(nm)+log2\n( (2d+1\u22121)\n\u221a dr1r2\u00b7\u00b7\u00b7rda1a2\u00b7\u00b7\u00b7ad c \u221a nm +1\n)\n\u2265 (2 d+1 \u2212 1)\u221adnmr1r2 \u00b7 \u00b7 \u00b7 rda1a2 \u00b7 \u00b7 \u00b7 ad\nc + 1.\n(61)\nFor any X s constructed in Lemma 8.2, we have\n\u2016X s \u2212 X \u2217\u2016\u221e \u2264 c\u221a dnm \u2264 c 2 ,\nwhere the last inequality follows from d, nm \u2265 2. Therefore, \u2016X s\u2016\u221e \u2264 \u2016X \u2217\u2016\u221e + c2 \u2264 c, which implies X s \u2208 \u03a5. As a consequence, we get that\nmin X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7nd\n} = min\nX\u2208\u03a5\n{ \u2016X \u2212 X \u2217\u20162F\n2\u03c32n1n2 \u00b7 \u00b7 \u00b7nd\n}\n\u2264 \u2016X s \u2212 X \u2217\u20162F 2\u03c32n1n2 \u00b7 \u00b7 \u00b7nd \u2264 \u2016X s \u2212X \u2217\u20162\u221e 2\u03c32 \u2264 c 2\n2\u03c32dnm \u2264 c\n2\n2\u03c32m ,\n(62)\nwhere the third inequality holds by (61) and Lemma 8.2, and the last inequality holds by the fact that m \u2264 dnm. Moreover, by the construction of Asi in Lemma 8.2, we know that \u2016Asi\u20160 = \u2016A\u2217i \u20160,\u2200i \u2208 [d]. Therefore, plugging (62) and (11) into (60), we have\nE\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 22c\n2 log(m)\nm\n+ 16(\u03b2 + 2)(2c2 + 3\u03c32)\n( r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016A\u2217i \u20160\nm\n) log(nm),\nwhich leads to the desired conclusion. \u2737"
        },
        {
            "heading": "Appendix C. Proof of Theorem 4.3",
            "text": "Proof. For the observations with additive Laplace noise, by [54, Lemma 3.1], we know that\nK ( pX \u2217i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)||pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) )\n= |Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id |\n\u03c4 \u2212 1 + exp\n( \u2212 |Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id |\n\u03c4\n)\n\u2264 (Xi1i2\u00b7\u00b7\u00b7id \u2212X \u2217i1i2\u00b7\u00b7\u00b7id)2\n2\u03c42 ,\nwhere the inequality follows from the fact that e\u2212x \u2264 1 \u2212 x + x22 for any x > 0. Therefore, we can choose \u03b3 = c 2\n2\u03c42 in (47).\nLet f(t) := log(1 + t2\u03c4 ), where t \u2208 [0, c]. Then by Taylor\u2019s expansion, we get that\nf(t) = f(0) + f \u2032(0)t+ f \u2032\u2032(\u03be) 2 t2, (63)\nwhere \u03be \u2208 [0, c]. By [54, Lemma 3.1], we obtain that\n\u2212 2 log(H(pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id), p(X \u2217)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)))\n= |Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id |\n\u03c4 \u2212 2 log\n( 1 + |Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id | 2\u03c4 )\n\u2265 (Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id)2\n(2\u03c4 + c)2\nwhere the inequality holds by letting t = |Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id | and the fact that f \u2032\u2032(\u03be) \u2264 \u2212 1 (2\u03c4+c)2 in (63). Therefore, by Theorem 4.1, we can deduce\nE\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 8c\n2(2\u03c4 + c)2 log(m)\n2\u03c42m + 3(2\u03c4 + c)2 \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7 nd\n+ ( max\ni {\u03bbi}+\n4c2(\u03b2 + 2) log(nm)\n3\u03c42\n) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016Ai\u20160\nm\n} .\n(64)\nThen by a similar argument of (62) in the proof of Theorem 4.2, we know that\nmin X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7nd\n} \u2264 min\nX\u2208\u03a5\n{ \u2016X \u2212 X \u2217\u20162F\n2\u03c42n1n2 \u00b7 \u00b7 \u00b7nd\n}\n\u2264 \u2016X s \u2212 X \u2217\u20162F 2\u03c42n1n2 \u00b7 \u00b7 \u00b7 nd \u2264 \u2016X s \u2212 X \u2217\u20162\u221e 2\u03c42 \u2264 c 2 2\u03c42m ,\n(65)\nwhere X s is defined in Lemma 8.2. By the construction of Asi in Lemma 8.2, we know that \u2016Asi\u20160 = \u2016A\u2217i \u20160,\u2200i \u2208 [d]. As a consequence, plugging (65) and (11) into (64), the estimator of (6) satisfies\nE\u2126,Y\u2126 [ |X \u03bb \u2212X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7 nd \u2264 11c\n2(2\u03c4 + c)2 log(m)\n2\u03c42m\n+ 12 ( 1 + 2c2\n3\u03c42\n) (2\u03c4 + c)2(\u03b2 + 2) log(nm) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d\ni=1 \u2016A\u2217i \u20160 m ,\nwhich is the desired statement. \u2737"
        },
        {
            "heading": "Appendix D. Proof of Theorem 4.4",
            "text": "Proof. By [7, Lemma 8], we obtain that the KL divergence of Poisson observations is\nK ( pX \u2217i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id)||pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id) ) \u2264 1Xi1i2\u00b7\u00b7\u00b7id (Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id) 2\n\u2264 1 \u033a (Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id) 2.\nConsequently, we can choose \u03b3 = c 2\n\u033a in (11). Moreover, according to the proof of [41, Appendix IV],\nwe have\n\u22122 log(H(pXi1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id), p(X \u2217)i1i2\u00b7\u00b7\u00b7id (Yi1i2\u00b7\u00b7\u00b7id))) = (\u221a X \u2217i1i2\u00b7\u00b7\u00b7id \u2212 \u221a Xi1i2\u00b7\u00b7\u00b7id )2\n\u2265 1 4c ( Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id )2 ,\nwhere the inequality holds by\n(Xi1i2\u00b7\u00b7\u00b7id \u2212 X \u2217i1i2\u00b7\u00b7\u00b7id) 2 = ((\u221a X \u2217i1i2\u00b7\u00b7\u00b7id \u2212 \u221a Xi1i2\u00b7\u00b7\u00b7id )(\u221a X \u2217i1i2\u00b7\u00b7\u00b7id + \u221a Xi1i2\u00b7\u00b7\u00b7id ))2\n\u2264 4c (\u221a X \u2217i1i2\u00b7\u00b7\u00b7id \u2212 \u221a Xi1i2\u00b7\u00b7\u00b7id )2 .\nHence, by Theorem 4.1, we get that\nE\u2126,Y\u2126 [ \u2016X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 32c\n3 log(m)\nm\u033a + 12c \u00b7 min X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7nd\n+ ( max\ni {\u03bbi}+\n8c2(\u03b2 + 2) log(nm)\n3\u033a\n) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 \u2016Ai\u20160\nm\n} . (66)\nLet X s be defined as in Lemma 8.2. Then by a similar argument of (62), we get that\nmin X\u2208\u03a5\n{ K(pX \u2217(Y)||pX (Y))\nn1n2 \u00b7 \u00b7 \u00b7nd\n} \u2264 min\nX\u2208\u03a5 { \u2016X \u2212 X \u2217\u20162F \u033an1n2 \u00b7 \u00b7 \u00b7nd }\n\u2264 \u2016X s \u2212 X \u2217\u20162F \u033an1n2 \u00b7 \u00b7 \u00b7 nd \u2264 \u2016X s \u2212 X \u2217\u20162\u221e \u033a \u2264 c 2 \u033am .\n(67)\nNotice that \u2016Asi\u20160 = \u2016A\u2217i \u20160,\u2200i \u2208 [d]. By combining (11), (66), and (67), after some rearrangements, we obtain that the estimator of (6) satisfies E\u2126,Y\u2126 [ |X \u03bb \u2212 X \u2217\u20162F ]\nn1n2 \u00b7 \u00b7 \u00b7nd \u2264 44c\n3 log(m)\n\u033am + 48c\n( 1 + 4c2\n3\u033a\n) (\u03b2 + 2) log(nm) r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d\ni=1 \u2016A\u2217i \u20160 m .\nThis concludes the proof. \u2737"
        },
        {
            "heading": "Appendix E. Proof of Theorem 5.1",
            "text": "Let\nL = {X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad : C \u2208 D, Ai \u2208 Xi, i \u2208 [d]} ,\nwhere\nD = { C \u2208 Rr1\u00d7r2\u00d7\u00b7\u00b7\u00b7\u00d7rd+ : Ci1\u00b7\u00b7\u00b7id \u2208 {0, 1, c0}, (i1, . . . , id) \u2208 [n1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd] }\nwith\nc0 = min\n{ 1,\n\u03b3c\u00b5\u220fd i=1(ai \u221a \u2206i(si, ni))\n\u221a r1r2 \u00b7 \u00b7 \u00b7 rd\nm\n} , (68)\nand Xi = {Ai \u2208 Rni\u00d7ri+ : (Ai)jk \u2208 {0, ai, bi}, \u2016Ai\u20160 \u2264 si, (j, k) \u2208 [ni]\u00d7 [ri]} with\nbi = min    ai, \u03b3i\u00b5(\u220f j 6=i aj )\u221a\u220fd j=1\u2206(sj, nj) \u221a si m    , i \u2208 [d].\nWe will specify \u03b3c, \u03b3i, i \u2208 [d] in detail later. Therefore, by the construction of L, we know that L \u2286 L(s, r,a). Next we consider to construct the packing sets of the core tensor and factor matrices, respectively.\nCase I. We construct the following set\nLC = {X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7d Ad : C \u2208 D0},\nwhere D0 is defined as\nD0 = { C \u2208 Rr1\u00d7r2\u00d7\u00b7\u00b7\u00b7\u00d7rd+ : Ci1i2\u00b7\u00b7\u00b7id \u2208 {0, c0}, (i1, . . . , id) \u2208 [n1]\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 [nd] } (69)\nand\nAi = ai   Iri ...\nIri 0ri\n  \u2208 R ni\u00d7ri + with Iri \u2208 Rri\u00d7ri , i \u2208 [d].\nThere are \u230asi\u2227niri \u230b blocks identity matrix Iri in Ai and 0ri \u2208 R (ni\u2212ri\u230a si\u2227niri \u230b)\u00d7ri is a zero matrix. Consequently, LC \u2286 L. For any X \u2208 LC , by (2), we have\nX(1) = A1C(1)(Ad \u2297Ad\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297A2)T\n= a1   C(1) ...\nC(1) 0r12\n  (Ad \u2297Ad\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297A2) T\nwhere 0r12 \u2208 R (n1\u2212r1\u230a s1\u2227n1r1 \u230b)\u00d7(r2\u00b7\u00b7\u00b7rd) is a zero matrix. By some simple calculations, we get\n\u2016X\u20162F = \u2016X(1)\u20162F = (a1 \u00b7 \u00b7 \u00b7 ad)2\u2016C(1)\u20162F d\u220f\ni=1\n\u230a si \u2227 ni\nri\n\u230b . (70)\nNote that the entries of C only takes 0 or c0 by the construction of D0 in (69). By the Varshamov-Gilbert bound [47, Lemma 2.9], we know that there exists a subset L0C \u2286 LC such that\n|L0C | \u2265 2r1\u00b7\u00b7\u00b7rd/8 + 1, (71)\nand for any X1,X2 \u2208 L0C,\n\u2016X1 \u2212 X2\u20162F = \u2016(X1)(1) \u2212 (X2)(1)\u20162F\n\u2265 r1 \u00b7 \u00b7 \u00b7 rd 8\n( d\u220f\ni=1\n\u230a si \u2227 ni\nri\n\u230b) (c0a1 \u00b7 \u00b7 \u00b7 ad)2\n\u2265 n1n2 \u00b7 \u00b7 \u00b7 nd 2d+3\n( d\u220f\ni=1\n\u2206i(si, ni) ) (c0a1 \u00b7 \u00b7 \u00b7 ad)2\n= n1n2 \u00b7 \u00b7 \u00b7 nd\n2d+3 min\n{ d\u220f\ni=1\na2i\u2206i(si, ni), \u03b3 2 c\u00b5 2 r1r2 \u00b7 \u00b7 \u00b7 rd m\n} ,\nwhere the second inequality follows from the fact that \u230ax\u230b \u2265 x2 for any x \u2265 1, \u2206i(si, ni) is defined as (21), and the last equality follows from the definition of c0 in (68).\nFor an arbitrary tensor X \u2208 L0C , we have that\nK(PX ,P0) \u2264 m\nn1n2 \u00b7 \u00b7 \u00b7 nd\n( 1\n2\u00b52\n) \u2211\ni1,i2,...,id\n|Xi1i2\u00b7\u00b7\u00b7id |2\n\u2264 m n1n2 \u00b7 \u00b7 \u00b7 nd\n( 1\n2\u00b52\n) c20\n( d\u220f\ni=1\nria 2 i\n\u230a si \u2227 ni\nri\n\u230b)\n\u2264 m 2\u00b52 min\n{ d\u220f\ni=1\na2i\u2206i(si, ni), \u03b3 2 c\u00b5 2 r1r2 \u00b7 \u00b7 \u00b7 rd m\n}\n\u2264 \u03b3 2 c r1 \u00b7 \u00b7 \u00b7 rd\n2 \u2264 4\u03b32c log2(|L0C | \u2212 1),\n(72)\nwhere the first inequality holds by (20) and \u2126 \u223c Bern(p) with p = mn1n2\u00b7\u00b7\u00b7nd , the second inequality holds by (70) and L0C \u2286 LC , the third inequality holds by \u230ax\u230b \u2264 x for any x > 0 and the definition of c0 in (68), and the last inequality holds by (71). Consequently, summing up the inequality in (72) for all X \u2208 L0C except for X = 0, we deduce\n1 |L0C | \u2212 1 \u2211\nX\u2208L0C\nK(PX ,P0) \u2264 \u03b1 log(|L0C | \u2212 1)\nwhere \u03b3c :=\n\u221a \u03b1 log(2)\n2 with \u03b1 \u2208 (0, 1/8). Hence, by [47, Theorem 2.5], we have\ninf X\u0303 sup X \u2217\u2208L(s,r,a) P ( \u2016X\u0303 \u2212 X \u2217\u20162F n1 \u00b7 \u00b7 \u00b7nd \u2265 1 2d+4 min { d\u220f\ni=1\na2i\u2206i(si, ni), \u03b3 2 c\u00b5 2 r1r2 \u00b7 \u00b7 \u00b7 rd m\n})\n\u2265 inf X\u0303 sup X \u2217\u2208L0C P ( \u2016X\u0303 \u2212 X \u2217\u20162F n1 \u00b7 \u00b7 \u00b7 nd \u2265 1 2d+4 min { d\u220f i=1 a2i\u2206i(si, ni), \u03b3 2 c\u00b5 2 r1r2 \u00b7 \u00b7 \u00b7 rd m }) \u2265 \u03b1\u0303,\n(73)\nwhere \u03b1\u0303 is defined as\n\u03b1\u0303 :=\n\u221a |L0C | \u2212 1\n1 + \u221a\n|L0C | \u2212 1\n( 1\u2212 2\u03b1\u2212 \u221a 2\u03b1\nlog(|L0C | \u2212 1)\n) .\nCase II. Now we consider the packing set about the factor matrices. For a fixed i, then for any j 6= i, we let\nAj = aj   Irj ...\nIrj 0j\n  \u2208 R nj\u00d7rj + ,\nwhere there are \u230asj\u2227njrj \u230b blocks identity matrix Irj in Aj and 0j is an ( nj\u2212rj\u230asj\u2227njrj \u230b ) \u00d7rj zero matrix. Let\nA\u0303i := { Ai = (Ar\u2032i 0r\u2032i) \u2208 R ni\u00d7ri + : Ar\u2032i \u2208 R ni\u00d7r\u2032i + with (Ar\u2032i)kj \u2208 {0, bi},0r\u2032i \u2208 R ni\u00d7(ri\u2212r\u2032i) } ,\nwhere r\u2032i := \u2308 sini \u2309, there are at most si nonzero entries in Ar\u2032i , and 0r\u2032i is an ni \u00d7 (ri \u2212 r \u2032 i) zero matrix. Now we construct the following set\nLAi = { X = C \u00d71 A1 \u00b7 \u00b7 \u00b7 \u00d7i\u22121 Ai\u22121 \u00d7i Ai \u00d7i+1 Ai+1 \u00b7 \u00b7 \u00b7 \u00d7d Ad : Ai \u2208 A\u0303i } ,\nwhere the mode-i unfolding of C is defined as\nC(i) = (\nIr\u2032i Ir\u2032i \u00b7 \u00b7 \u00b7 Ir\u2032i 01r\u2032i 02r\u2032i 02r\u2032i \u00b7 \u00b7 \u00b7 02r\u2032i 03r\u2032i\n) .\nHere there are \u230a \u220f\nj 6=i rj r\u2032i \u230b blocks identity matrices Ir\u2032i \u2208 R r\u2032i\u00d7r\u2032i in C(i), and\n01r\u2032i \u2208 Rr\n\u2032 i\u00d7 (\u220f j 6=i rj\u2212r\u2032i\u230a \u220f\nj 6=i rj r\u2032 i \u230b ) , 02r\u2032i \u2208 R (ri\u2212r\u2032i)\u00d7r\u2032i , 03r\u2032i \u2208 R (ri\u2212r\u2032i)\u00d7 (\u220f j 6=i rj\u2212r\u2032i\u230a \u220f j 6=i rj r\u2032 i \u230b )\nare zero matrices. From this construction, we know that LAi \u2286 L. Therefore, for any X \u2208 LAi , we have\nX(i) = AiC(i)(Ad \u2297 \u00b7 \u00b7 \u00b7 \u2297Ai+1 \u2297Ai\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297A1)T\n= (Ar\u2032i \u00b7 \u00b7 \u00b7 Ar\u2032i 0ni)(Ad \u2297 \u00b7 \u00b7 \u00b7 \u2297Ai+1 \u2297Ai\u22121 \u2297 \u00b7 \u00b7 \u00b7 \u2297A1) T ,\nwhere the first equality follows from (2) and 0ni \u2208 R ni\u00d7 (\u220f j 6=i rj\u2212r\u2032i\u230a \u220f j 6=i rj r\u2032 i \u230b ) is a zero matrix. Hence, we can obtain through some simple calculations that\n\u2016X\u20162F = \u2016X(i)\u20162F = \u230a\u220f\nj 6=i rj r\u2032i\n\u230b  \u220f\nj 6=i\n(\u230a sj \u2227 nj\nrj\n\u230b a2j )  \u2016Ar\u2032i\u2016 2 F . (74)\nNotice that the entries of Ar\u2032i only takes 0 or bi. By the Varshamov-Gilbert bound [47, Lemma 2.9], we know that there exists L0Ai \u2286 LAi such that\n|L0Ai | \u2265 2si/8 + 1, (75)\nand for any X1,X2 \u2208 L0Ai ,\n\u2016X1 \u2212 X2\u20162F \u2265 si 8 \u230a\u220f j 6=i rj r\u2032i \u230b\u220f\nj 6=i\n(\u230a sj \u2227 nj\nrj\n\u230b a2j ) b2i\n\u2265 sib 2 i\n2d+3r\u2032i\n\u220f j 6=i (sj \u2227 nj) a2j\n\u2265 n1 \u00b7 \u00b7 \u00b7nd 2d+4\n  d\u220f\nj=1\n\u2206j(sj, nj)\n    \u220f\nj 6=i a2j\n min { a2i ,\n\u03b32i \u00b5 2\n( \u220f j 6=i a 2 j ) \u220fd j=1\u2206j(sj , nj)\n( si m\n)}\n= n1 \u00b7 \u00b7 \u00b7nd 2d+4 min    d\u220f\nj=1\n\u2206j(sj, nj)a 2 j , \u03b3 2 i \u00b5 2 ( si m )    ,\n(76)\nwhere the second inequality holds by \u230ax\u230b \u2265 x2 for any x \u2265 1 and the third inequality holds by x/\u2308x\u2309 \u2265 1 2 min{x, 1} for any x > 0.\nIn addition, for any X \u2208 L0Ai , we have that\nK(PX ,P0) \u2264 m\nn1n2 \u00b7 \u00b7 \u00b7nd\n( 1\n2\u00b52\n) \u2211\ni1,i2,...,id\n|Xi1i2\u00b7\u00b7\u00b7id |2\n\u2264 m n1n2 \u00b7 \u00b7 \u00b7nd\n( 1\n2\u00b52 )\u230a\u220f j 6=i rj r\u2032i \u230b\u220f\nj 6=i\n(\u230a sj \u2227 nj\nrj\n\u230b a2j ) (nir \u2032 i \u2227 si)b2i\n\u2264 m 2\u00b52\n  d\u220f\nj=1\n\u2206j(sj, nj)\n    \u220f\nj 6=i aj\n  b2i\n= m\n2\u00b52 min    d\u220f\nj=1\na2j\u2206j(sj , nj), \u03b3 2 i \u00b5 2 si m   \n\u2264 \u03b3 2 i si 2 \u2264 4\u03b32i log2(|L0Ai | \u2212 1),\n(77)\nwhere the first inequality holds by (20) and \u2126 \u223c Bern(p) with p = mn1n2\u00b7\u00b7\u00b7nd , the second inequality holds by (74) and \u2016Ar\u2032i\u2016 2 F \u2264 (nir\u2032i \u2227 si)b2i , the third inequality holds by the fact that \u230ax\u230b \u2264 x for any\nx > 0, and the last inequality holds by (75). Let \u03b3i = \u221a\n\u03b1i log(2)/2 with \u03b1i \u2208 (0, 18). Summing up the inequality in (77) for all 0 6= X \u2208 LAi yields\n1 |L0Ai | \u2212 1 \u2211\nX\u2208L0Ai\nK(PX ,P0) \u2264 \u03b1i log(|L0Ai | \u2212 1).\nIt follows from [47, Theorem 2.5] that\ninf X\u0303 sup X \u2217\u2208L(s,r,a) P\n \u2016X\u0303 \u2212 X\n\u2217\u20162F n1 \u00b7 \u00b7 \u00b7nd \u2265 1 2d+5 min    d\u220f\nj=1\n\u2206j(sj , nj)a 2 j , \u03b3 2 i \u00b5 2 ( si m )     \ninf X\u0303 sup X \u2217\u2208L0\nAi\nP\n \u2016X\u0303 \u2212 X\n\u2217\u20162F n1 \u00b7 \u00b7 \u00b7nd \u2265 1 2d+5 min    d\u220f\nj=1\n\u2206j(sj , nj)a 2 j , \u03b3 2 i \u00b5 2 ( si m )     \n\u2265 \u03b1\u0303i,\n(78)\nwhere\n\u03b1\u0303i =\n\u221a |L0Ai | \u2212 1\n1 + \u221a\n|L0Ai | \u2212 1\n( 1\u2212 2\u03b1i \u2212 \u221a 2\u03b1i\nlog(|L0Ai | \u2212 1)\n) .\nLet\n\u03b7 := min    d\u220f\nj=1\n\u2206j(sj, nj)a 2 j , \u03b3 2 m\u00b5 2\n( r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 si\nm\n)  ,\nwhere \u03b3m = min{\u03b3c, \u03b31, . . . , \u03b3d}. Combining (73) with (78), we deduce that\ninf X\u0303 sup X \u2217\u2208L(s,r,a) P ( \u2016X\u0303 \u2212 X \u2217\u20162F n1 \u00b7 \u00b7 \u00b7nd \u2265 1 2d+5(d+ 1) \u03b7 ) \u2265 \u03b1\u0303m,\nwhere \u03b1\u0303m = min{\u03b1\u0303, \u03b1\u03031, . . . , \u03b1\u0303d} \u2208 (0, 1). Therefore, by Markov\u2019s inequality, we get that\ninf X\u0303 sup X \u2217\u2208L(s,r,a) E\u2126,Y\u2126\u2016X\u0303 \u2212 X \u2217\u20162F n1 \u00b7 \u00b7 \u00b7nd\n\u2265 \u03b1\u0303m 2d+5(d+ 1) min\n{ d\u220f\ni=1\n\u2206i(si, ni)a 2 i , \u03b3 2 m\u00b5 2\n( r1r2 \u00b7 \u00b7 \u00b7 rd + \u2211d i=1 si\nm\n)} ,\nwhich concludes the proof. \u2737"
        }
    ],
    "title": "Sparse Nonnegative Tucker Decomposition and Completion under Noisy Observations",
    "year": 2022
}