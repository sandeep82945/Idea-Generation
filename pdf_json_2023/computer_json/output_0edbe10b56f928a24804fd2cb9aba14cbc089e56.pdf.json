{
    "abstractText": "State-of-the-art object detection and segmentation methods for microscopy images rely on supervised machine learning, which requires laborious manual annotation of training data. Here we present a self-supervised method based on time arrow prediction pre-training that learns dense image representations from raw, unlabeled live-cell microscopy videos. Our method builds upon the task of predicting the correct order of time-flipped image regions via a single-image feature extractor followed by a time arrow prediction head that operates on the fused features. We show that the resulting dense representations capture inherently time-asymmetric biological processes such as cell divisions on a pixel-level. We furthermore demonstrate the utility of these representations on several live-cell microscopy datasets for detection and segmentation of dividing cells, as well as for cell state classification. Our method outperforms supervised methods, particularly when only limited ground truth annotations are available as is commonly the case in practice. We provide code at https://github.com/weigertlab/tarrow.",
    "authors": [
        {
            "affiliations": [],
            "name": "Benjamin Gallusser"
        },
        {
            "affiliations": [],
            "name": "Max Stieber"
        },
        {
            "affiliations": [],
            "name": "Martin Weigert"
        }
    ],
    "id": "SP:52813e254ce4d6878ee9bcf87f88f20233769f89",
    "references": [
        {
            "authors": [
                "T. Chen",
                "S. Kornblith",
                "M. Norouzi",
                "G. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "ICML. pp. 1597\u20131607",
            "year": 2020
        },
        {
            "authors": [
                "M. Dorkenwald",
                "F. Xiao",
                "B. Brattoli",
                "J. Tighe",
                "D. Modolo"
            ],
            "title": "SCVRL: Shuffled Contrastive Video Representation Learning",
            "venue": "CVPR. pp. 4132\u20134141",
            "year": 2022
        },
        {
            "authors": [
                "L. Ericsson",
                "H. Gouk",
                "C.C. Loy",
                "T.M. Hospedales"
            ],
            "title": "Self-Supervised Representation Learning: Introduction, advances, and challenges",
            "venue": "IEEE Signal Processing Magazine 39(3), 42\u201362",
            "year": 2022
        },
        {
            "authors": [
                "R. Etournay",
                "M. Popovi\u0107",
                "M. Merkel",
                "A. Nandi",
                "C. Blasse",
                "B Aigouy"
            ],
            "title": "Interplay of cell dynamics and epithelial tension during morphogenesis of the Drosophila pupal wing",
            "venue": "eLife 4, e07090",
            "year": 2015
        },
        {
            "authors": [
                "S. Gidaris",
                "P. Singh",
                "N. Komodakis"
            ],
            "title": "Unsupervised representation learning by predicting image rotations",
            "venue": "ICLR. OpenReview.net",
            "year": 2018
        },
        {
            "authors": [
                "N.F. Greenwald",
                "G. Miller",
                "E. Moen",
                "A. Kong",
                "A Kagel"
            ],
            "title": "Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning",
            "venue": "Nature Biotechnology pp. 1\u201311",
            "year": 2021
        },
        {
            "authors": [
                "H. Han",
                "M. Dmitrieva",
                "A. Sauer",
                "K.H. Tam",
                "J. Rittscher"
            ],
            "title": "Self-supervised voxel-level representation rediscovers subcellular structures in volume electron microscopy",
            "venue": "CVPRW. pp. 1874\u20131883",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Chen",
                "S. Xie",
                "Y. Li",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "CVPR. pp. 16000\u201316009",
            "year": 2022
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR. pp. 770\u2013778",
            "year": 2016
        },
        {
            "authors": [
                "J. Hsu",
                "J. Gu",
                "G. Wu",
                "W. Chiu",
                "S. Yeung"
            ],
            "title": "Capturing implicit hierarchical structure in 3d biomedical images with self-supervised hyperbolic representations",
            "venue": "NeurIPS. vol. 34, pp. 5112\u20135123",
            "year": 2021
        },
        {
            "authors": [
                "K. Hu",
                "J. Shao",
                "Y. Liu",
                "B. Raj",
                "M. Savvides",
                "Z. Shen"
            ],
            "title": "Contrast and Order Representations for Video Self-Supervised Learning",
            "venue": "ICCV. pp. 7939\u20137949",
            "year": 2021
        },
        {
            "authors": [
                "T. Hua",
                "W. Wang",
                "Z. Xue",
                "S. Ren",
                "Y. Wang",
                "H. Zhao"
            ],
            "title": "On Feature Decorrelation in SelfSupervised Learning",
            "venue": "CVPR. pp. 9598\u20139608",
            "year": 2021
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "ICLR",
            "year": 2015
        },
        {
            "authors": [
                "H.Y. Lee",
                "J.B. Huang",
                "M. Singh",
                "M.H. Yang"
            ],
            "title": "Unsupervised Representation Learning by Sorting Sequences",
            "venue": "ICCV. pp. 667\u2013676",
            "year": 2017
        },
        {
            "authors": [
                "I. Misra",
                "C.L. Zitnick",
                "M. Hebert"
            ],
            "title": "Shuffle and learn: Unsupervised learning using temporal order verification",
            "venue": "ECCV. pp. 527\u2013544",
            "year": 2016
        },
        {
            "authors": [
                "F. Padovani",
                "B. Mairh\u00f6rmann",
                "P. Falter-Braun",
                "J. Lengefeld",
                "K.M. Schmoller"
            ],
            "title": "Segmentation, tracking and cell cycle analysis of live-cell imaging data with Cell-ACDC",
            "venue": "BMC Biology 20, 174",
            "year": 2022
        },
        {
            "authors": [
                "F. Padovani",
                "B. Mairh\u00f6rmann",
                "J. Lengefeld",
                "P. Falter-Braun",
                "K. Schmoller"
            ],
            "title": "Cell-ACDC: segmentation, tracking, annotation and quantification of microscopy imaging data (dataset)",
            "venue": "https://zenodo.org/record/6795124",
            "year": 2022
        },
        {
            "authors": [
                "D. Pathak",
                "P. Krahenbuhl",
                "J. Donahue",
                "T. Darrell",
                "A.A. Efros"
            ],
            "title": "Context Encoders: Feature Learning by Inpainting",
            "venue": "CVPR. pp. 2536\u20132544",
            "year": 2016
        },
        {
            "authors": [
                "L.C. Pickup",
                "Z. Pan",
                "D. Wei",
                "Y. Shih",
                "C. Zhang",
                "A. Zisserman",
                "B. Scholkopf",
                "W.T. Freeman"
            ],
            "title": "Seeing the Arrow of Time",
            "venue": "CVPR. pp. 2043\u20132050",
            "year": 2014
        },
        {
            "authors": [
                "R. Piscitello-G\u00f3mez",
                "F.S. Gruber",
                "A. Krishna",
                "C. Duclut",
                "Modes",
                "C.D"
            ],
            "title": "Core PCP mutations affect short time mechanical properties but not tissue morphogenesis in the Drosophila pupal wing",
            "venue": "bioRxiv",
            "year": 2022
        },
        {
            "authors": [
                "O. Ronneberger",
                "P. Fischer",
                "T. Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI. pp. 234\u2013241. Springer",
            "year": 2015
        },
        {
            "authors": [
                "M.C. Schiappa",
                "Y.S. Rawat",
                "M. Shah"
            ],
            "title": "Self-Supervised Learning for Videos: A Survey",
            "venue": "ACM Computing Surveys",
            "year": 2022
        },
        {
            "authors": [
                "R.R. Selvaraju",
                "M. Cogswell",
                "A. Das",
                "R. Vedantam",
                "D. Parikh",
                "D. Batra"
            ],
            "title": "Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization",
            "venue": "ICCV. pp. 618\u2013626",
            "year": 2017
        },
        {
            "authors": [
                "E.H.K. Stelzer",
                "F. Strobl",
                "B.J. Chang",
                "F. Preusser",
                "S. Preibisch",
                "K. McDole",
                "R. Fiolka"
            ],
            "title": "Light sheet fluorescence microscopy",
            "venue": "Nature Reviews Methods Primers 1(1), 1\u201325",
            "year": 2021
        },
        {
            "authors": [
                "C. Stringer",
                "T. Wang",
                "M. Michaelos",
                "M. Pachitariu"
            ],
            "title": "Cellpose: a generalist algorithm for cellular segmentation",
            "venue": "Nature methods 18(1), 100\u2013106",
            "year": 2021
        },
        {
            "authors": [
                "R. Tomer",
                "K. Khairy",
                "P.J. Keller"
            ],
            "title": "Shedding light on the system: studying embryonic development with light sheet microscopy",
            "venue": "Current Opinion in Genetics & Development 21(5), 558\u2013565",
            "year": 2011
        },
        {
            "authors": [
                "K. Ulicna",
                "G. Vallardi",
                "G. Charras",
                "A.R. Lowe"
            ],
            "title": "Automated Deep Lineage Tree Analysis Using a Bayesian Single Cell Tracking Approach",
            "venue": "Frontiers in Computer Science 3",
            "year": 2021
        },
        {
            "authors": [
                "V. Ulman",
                "M. Ma\u0161ka",
                "K.E.G. Magnusson",
                "O. Ronneberger",
                "C Haubold"
            ],
            "title": "An objective comparison of cell-tracking algorithms",
            "venue": "Nature Methods 14(12), 1141\u20131152",
            "year": 2017
        },
        {
            "authors": [
                "D. Wei",
                "J. Lim",
                "A. Zisserman",
                "W.T. Freeman"
            ],
            "title": "Learning and Using the Arrow of Time",
            "venue": "CVPR. pp. 8052\u20138060",
            "year": 2018
        },
        {
            "authors": [
                "M. Weigert",
                "U. Schmidt",
                "R. Haase",
                "K. Sugawara",
                "G. Myers"
            ],
            "title": "Star-convex polyhedra for 3d object detection and segmentation in microscopy",
            "venue": "WACV. pp. 3666\u20133673",
            "year": 2020
        },
        {
            "authors": [
                "M. Zaheer",
                "S. Kottur",
                "S. Ravanbakhsh",
                "B. Poczos",
                "R.R. Salakhutdinov",
                "A.J. Smola"
            ],
            "title": "Deep Sets",
            "venue": "NeurIPS",
            "year": 2017
        },
        {
            "authors": [
                "J. Zbontar",
                "L. Jing",
                "I. Misra",
                "Y. LeCun",
                "S. Deny"
            ],
            "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
            "venue": "ICML. pp. 12310\u201312320",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Keywords: Self-supervised learning \u00b7 Live-cell microscopy"
        },
        {
            "heading": "1 Introduction",
            "text": "Live-cell microscopy is a fundamental tool to study the spatio-temporal dynamics of biological systems [26,4,24]. The resulting datasets can consist of terabytes of raw videos that require automatic methods for downstream tasks such as classification, segmentation, and tracking of objects (e.g. cells or nuclei). Current state-of-the-art methods rely on supervised learning using deep neural networks that are trained on large amounts of ground truth annotations [31,25,6]. The manual creation of these annotations, however, is laborious and often constitutes a practical bottleneck in the analysis of microscopy experiments [6]. Recently, self-supervised representation learning (SSL) has emerged as a promising approach to alleviate this problem [3,1]. In SSL one first defines a pretext task which can be formulated solely based on unlabeled images (e.g. inpainting [8], or rotation prediction [5]) and tasks a neural network to solve it, with the aim of generating latent representations that capture high-level image semantics. In a second step, these representations can then be either finetuned or used directly (e.g. via linear probing) for a downstream task (e.g. image classification) with available ground truth [18,10,7]. Importantly, a proper choice of the pretext task is crucial for the resulting representations to be beneficial for a specific downstream task.\nIn this paper we investigate whether time arrow prediction, i.e. the prediction of the correct order of temporally shuffled image frames extracted from live-cell microscopy\nar X\niv :2\n30 5.\n05 51\n1v 2\n[ cs\n.C V\n] 2\n6 Ju\nl 2 02\nb\nctime Permutation-equivariant time arrow prediction head\nInput crops Dense TAP representations Time Arrow Prediction\nforward backward\nForward/backward with p=0.5\nchannels\nImage sequence\nSingle-image feature extractor\nd\nfixed/finetune\nClassification Detection\nSegmentation ...Downstream\nmodel\nchannels\nDense TAP representationsInput image Downstream task\naa\nTime\nFigure 1: a) Example frames from two live-cell microscopy videos. Top: MDCK cells with labeled nuclei [28], Bottom: Drosophila wing with labeled membrane [4]. Insets show three consecutive time points containing cell divisions. b) Overview of TAP: We create crops (x1, x2) from consecutive time points of a given video. After randomly flipping the input order (forward/backward), each crop is passed through a dense feature extractor f creating pixel-wise TAP representations (z1, z2). These are stacked and fed to the time arrow prediction head h. c) We design h to be permutation-equivariant ensuring consistent classification of temporally flipped inputs. d) The learned TAP representations z are used as input to a downstream model d.\nvideos, can serve as a suitable pretext task to generate meaningful representations of microscopy images. We are motivated by the observation that for most biological systems the temporal dynamics of local image features are closely related to their semantic content: whereas static background regions are time-symmetric, processes such as cell divisions or cell death are inherently time-asymmetric (cf. Fig. 1a). Importantly, we are interested in dense representations of individual images as they are useful for both imagelevel (e.g. classification) or pixel-level (e.g. segmentation) downstream tasks. To that end, we propose a time arrow prediction pre-training scheme, which we call TAP, that uses a feature extractor operating on single images followed by a time arrow prediction head operating on the fused representations of consecutive time points. The use of time arrow prediction as a pretext task for natural (e.g. youtube) videos was introduced by Pickup et al. [19] and has since then seen numerous applications for image-level tasks, such as action recognition, video retrieval, and motion classification [15,14,30,22,2,11]. However, to the best of our knowledge, SSL via time arrow prediction has not yet been studied in the context of live-cell microscopy. Concretely our contributions are: i) We introduce the time arrow prediction pretext task to the domain of live-cell microscopy and propose the TAP pre-training scheme, which learns dense representations (in contrast to only image-level representations) from raw, unlabeled live-cell microscopy videos, ii) we propose a custom (permutation-equivariant) time arrow prediction head that enables robust training, iii) we show via attribution maps that the representations learned by TAP capture biologically relevant processes such as cell divisions, and finally iv) we demonstrate that TAP representations are beneficial for common image-level and pixel-level downstream tasks in live-cell microscopy, especially in the low training data regime."
        },
        {
            "heading": "2 Method",
            "text": "Our proposed TAP pre-training takes as input a set {I} of live-cell microscopy image sequences I \u2208 RT\u00d7H\u00d7W with the goal to produce a feature extractor f that generates cdimensional dense representations z = f(x) \u2208 Rc\u00d7H\u00d7W from single images x \u2208 RH\u00d7W (cf. Fig. 1b for an overview of TAP). To that end, we randomly sample from each sequence I pairs of smaller patches x1, x2 \u2208 Rh\u00d7w from the same spatial location but consecutive time points x1 \u2282 It, x2 \u2282 It+1. We next flip the order of each pair with equal probability p = 0.5, assign it the corresponding label y (forward or backward) and compute dense representations z1 = f(x1) and z2 = f(x2) with z1, z2 \u2208 Rc\u00d7h\u00d7w via a fully convolutional feature extractor f . The stacked representations z = [z1, z2] \u2208 R2\u00d7c\u00d7h\u00d7w are fed to a time arrow prediction head h, which produces the classification logits y\u0302 = [y\u03021, y\u03022] = h([z1, z2]) = h([f(x1), f(x2)]) \u2208 R2. Both f and h are trained jointly to minimize the loss\nL = LBCE(y, y\u0302) + \u03bbLDecorr(z) , (1)\nwhere LBCE denotes the standard softmax + binary cross-entropy loss between the ground truth label y and the logits y\u0302 = h(z), and LDecorr is a loss term that promotes z to be decorrelated across feature channels [33,12] via maximizing the diagonal of the softmax-normalized correlation matrix Aij :\nLDecorr(z\u0303) = \u2212 1\nc log c\u2211 i=1 Aii , Aij = softmax(z\u0303 T i \u00b7 z\u0303j/\u03c4) = ez\u0303 T i \u00b7z\u0303j/\u03c4\u2211c j=1 e z\u0303Ti \u00b7z\u0303j/\u03c4\n(2)\nHere z\u0303 \u2208 Rc\u00d72hw denotes the stacked features z flattened across the non-channel dimensions, and \u03c4 is a temperature parameter. Throughout the experiments we use \u03bb = 0.01 and \u03c4 = 0.2. Note that instead of creating image pairs from consecutive video frames we can as well choose a custom time step \u2206t \u2208 N and sample x1 \u2282 It and x2 \u2282 It+\u2206t, which we empirically found to work better for datasets with high frame rate.\nPermutation-equivariant time arrow prediction head: The time arrow prediction task has an inherent symmetry: flipping the input [z1, z2] \u2192 [z2, z1] should flip the logits [y\u03021, y\u03022] \u2192 [y\u03022, y\u03021]. In other words, h should be equivariant wrt. to permutations of the input. In contrast to common models (e.g. ResNet [9]) that lack this symmetry, we here directly incorporate this inductive bias via a permutation-equivariant head h that is a generalization of the set permutation-equivariant layer proposed in [32] to dense inputs. Specifically, we choose h = h1 \u25e6 . . . \u25e6 hL as a chain of permutation-equivariant layers hl:\nhl : R2\u00d7c\u00d7h\u00d7w \u2192 R2\u00d7c\u0303\u00d7h\u00d7w hl(z)tmij = \u03c3 (\u2211\nn Lmnzt,n,i,j + \u2211 s,n Gmnzs,n,i,j ) , (3)\nwith weight matrices L,G \u2208 Rc\u0303\u00d7c and a non-linear activation function \u03c3. Note that L operates independently on each temporal axis and thus is trivially permutation equivariant, while G operates on the temporal sum and thus is permutation invariant. The last layer hL includes an additional global average pooling along the spatial dimensions to yield the final logits y\u0302 \u2208 R2.\nba\nAugmentations: To avoid overfitting on artificial image cues that could be discriminative of the temporal order (such as a globally consistent cell drift, or decay of image intensity due to photo-bleaching) we apply the following augmentations (with probability 0.5) to each image patch pair x1, x2: flips, arbitrary rotations and elastic transformations (jointly for x1 and x2), translations for x1 and x2 (independently), spatial scaling, additive Gaussian noise, and intensity shifting and scaling (jointly+independently)."
        },
        {
            "heading": "3 Experiments",
            "text": ""
        },
        {
            "heading": "3.1 Datasets",
            "text": "To demonstrate the utility of TAP for a diverse set of specimen and microscopy modalities we use the following four different datasets:\nHELA Human cervical cancer cells expressing histone 2B\u2013GFP imaged by fluorescence microscopy every 30 minutes [29] . The dataset consists of four videos with overall 368 frames of size 1100\u00d7 700. We use \u2206t = 1 for TAP training. MDCK Madin-Darby canine kidney epithelial cells expressing histone 2B\u2013GFP (cf. Fig. 3b), imaged by fluorescence microscopy every 4 minutes [28,27]. The dataset consists of a single video with 1200 frames of size 1600\u00d7 1200. We use \u2206t \u2208 {4, 8}. FLYWING Drosphila melanogaster pupal wing expressing Ecad::GFP (cf. Fig. 3a), imaged by spinning disk confocal microscopy every 5 minutes [20,4]. The dataset consists of three videos with overall 410 frames of size 3900\u00d7 1900. We use \u2206t = 1. YEAST S. cerevisiae cells (cf. Fig. 3c) imaged by phase-contrast microscopy every 3 minutes [16,17]. The dataset consists of five videos with overall 600 frames of size 1024\u00d71024. We use \u2206t \u2208 {1, 2, 3}. For each dataset we heuristically choose \u2206t to roughly correspond to the time scale of observable biological processes (i.e. larger \u2206t for higher frame rates)."
        },
        {
            "heading": "3.2 Implementation details:",
            "text": "For the feature extractor f we use a 2D U-NET [21] with depth 3 and c = 32 output features, batch normalization and leaky ReLU activation (approx. 2M params). The time\narrow prediction head h consists of two permutation-equivariant layers with batch normalization and leaky ReLU activation, followed by global average pooling and a final permutation-equivariant layer (approx. 5k params). We train all TAP models for 200 epochs and 105 samples per epoch, using the Adam optimizer [13] with a learning rate of 4\u00d7 10\u22124 with cyclic schedule, and batch size 256. Total training time for a single TAP model is roughly 8h on a single GPU. TAP is implemented in PyTorch."
        },
        {
            "heading": "3.3 Time arrow prediction pretraining",
            "text": "We first study how well the time arrow prediction pretext task can be solved depending on different image structures and used data augmentations. To that end, we train TAP networks with an increasing number of augmentations on HELA and compute the TAP classification accuracy for consecutive image patches x1, x2 that contain either background, interphase (non-dividing) cells, or mitotic (dividing) cells. As shown in Fig. 2a, the accuracy on background regions is approx. 50% irrespective of the used augmentations, suggesting the absence of predictive cues in the background for this dataset. In contrast, on regions with cell divisions the accuracy reaches almost 100%, confirming that TAP is able to pick up on strong time-asymmetric image features. Interestingly, the accuracy for regions with non-dividing cells ranges from 68% to 80%, indicating the presence of weak visual cues such as global drift or cell growth. When using more data augmentations the accuracy decreases by roughly 12 percentage points, suggesting that data augmentation is key to avoid overfitting on confounding cues.\nNext we investigate which regions in full-sized videos are most discriminative for TAP. To that end, we apply a trained TAP network on consecutive full-sized frames x1, x2 and compute the dense attribution map of the classification logits y wrt. to the TAP representations z via Grad-CAM [23]. In Fig. 3 we show example attribution maps on top of single raw frames for three different datasets. Strikingly, the attribution maps highlight only a few distributed, yet highly localized image regions. When inspecting the top six most discriminative regions and their temporal context for a single image frame, we find that virtually all of them contain cell divisions (cf. Fig. 3). Moreover, when examining the attribution maps for full videos, we find that indeed most highlighted regions correspond to mitotic cells, underlining the strong potential of TAP to reveal time-asymmetric biological phenomena from raw microscopy videos alone (cf. Supplementary Video 1).\nFinally, we emphasize the positive effect of the permutation-equivariant time arrow prediction head on the training process. When we originally used a regular CNNbased head, we consistently observed that the TAP loss stagnated during the initial training epochs and decreased only slowly thereafter (cf. Fig. 2b). Using the permutationequivariant head alleviated this problem and enabled a consistent loss decrease already from the beginning of training."
        },
        {
            "heading": "3.4 Downstream tasks",
            "text": "We next investigate whether the learned TAP representations are useful for common supervised downstream tasks, where we especially focus on their utility in the low training data regime. First we test the learned representations on two image-level classification tasks, and later on two dense segmentation tasks.\nMitosis classification on FLYWING: Since TAP attribution maps strongly highlight cell divisions, we consider predicting mitotic events an appropriate first downstream task to evaluate TAP. To that end, we generate a dataset of 97k crops of size 2 \u00d7 96 \u00d7 96 from FLYWING and label them as mitotic/non-mitotic (16k/81k) based on available tracking data [20]. We train TAP networks on FLYWING and use a small ResNet architecture (\u2248 5M params) that is trained from scratch as a supervised baseline. In Fig. 4a we show average precision (AP) on a held-out test set while varying the amount of available training\ndata. As expected, the performance of the supervised baseline drops substantially for low amounts of training data and surprisingly is already outperformed by a linear classifier (100 params) on top of TAP representations (e.g. 0.90 vs. 0.77 for 76 labeled crops). Training a small ResNet on fixed TAP representations consistently outperforms the supervised baseline even if hundreds of annotated cell divisions are available for training (e.g. 0.96 vs. 0.95 for 2328 labeled crops with \u223c 400 cell divisions), confirming the value of TAP representations to detect mitotic events.\nCell state classification on MDCK: Next we turn to the more challenging task of distinguishing between cells in interphase, prometaphase and anaphase from MDCK. This dataset consists of 4800 crops of size 80\u00d7 80 that are labeled with one of the three classes (1600 crops/class). Again we use a ResNet as supervised baseline and report in Fig. 4b test classification accuracy for varying amount of training data. As before, both a linear classifier as well as a ResNet trained on fixed TAP representations outperform the baseline especially in the low data regime, with the latter showing better or comparable results across the whole data regime (e.g. 0.90 vs. 0.83 for 117 annotated cells). Additionally, we finetune the pretrained TAP feature extractor for this downstream task, which slightly improves the results given enough training data. Notably, already at 30% training data it reaches the same performance (0.97) as the baseline model trained on the full training set.\nMitosis segmentation on FLYWING: We now apply TAP on a pixel-level downstream task to fully exploit that the learned TAP representations are dense. We use the same dataset as for FLYWING mitosis classification, but now densely label post-mitotic cells. We predict a pixel-wise probability map, threshold it at 0.5 and extract connected components as objects. To evaluate performance, we match a predicted/ground truth object if their intersection over union (IoU) is greater than 0.5, and report the F1 score after matching. The baseline model is a U-NET trained from scratch. Training a U-NET on fixed TAP representations always outperforms the baseline, and when only using 3% of\nthe training data it reaches similar performance as the baseline trained on all available labels (0.67 vs. 0.68, Fig. 5a). Interestingly, fine-tuning TAP only slightly outperforms the supervised baseline for this task even for moderate amounts of training data, suggesting that fixed TAP representations generalize better for limited-size datasets.\nEmerging bud detection on YEAST: Finally, we test TAP on the challenging task of segmenting emerging buds in phase contrast images of yeast colonies. We train TAP networks on YEAST and generate a dataset of 1205 crops of size 5 \u00d7 192 \u00d7 192 where we densely label yeast buds in the central frame (defined as buds that appeared less than 13 frames ago) based on available segmentation data [17]. We evaluate all methods on held out test videos by interpreting the resulting 2D+time segmentations as 3D objects and computing the F1 score using an IoU threshold of 0.25. The baseline model is again a U-NET trained from scratch. Surprisingly, training with fixed TAP representations performs slightly worse than the baseline for this dataset (Fig. 5b), possibly due to cell density differencess between TAP training and test videos. However, fine-tuning TAP features outperforms the baseline by a large margin (e.g. 0.64 vs. 0.39 for 120 frames) across the full training data regime, yielding already with 15% labels the same F1 score as the baseline using all labels."
        },
        {
            "heading": "4 Discussion",
            "text": "We have presented TAP, a self-supervised pretraining scheme that learns biologically meaningful representations from live-cell microscopy videos. We show that TAP uncovers sparse time-asymmetric biological processes and events in raw unlabeled recordings without any human supervision. Furthermore, we demonstrate on a variety of datasets that the learned features can substantially reduce the required amount of annotations for downstream tasks. Although in this work we focus on 2D+t image sequences, the principle of TAP should generalize to 3D+t datasets, for which dense ground truth creation is often prohibitively expensive and therefore the benefits of modern deep learning are not fully tapped into. We leave this to future work, together with the application of TAP to cell tracking algorithms, in which accurate mitosis detection is a crucial component.\nAcknowledgements We thank Albert Dominguez (EPFL) and Uwe Schmidt for helpful comments, Natalie Dye (PoL Dresden) and Franz Gruber for providing the FLYWING dataset, Benedikt Mairh\u00f6rmann and Kurt Schmoller (Helmholtz Munich) for providing additional YEAST training data, and Alan Lowe (UCL) for providing the MDCK dataset. M.W. and B.G. are supported by the EPFL School of Life Sciences ELISIR program and CARIGEST SA."
        }
    ],
    "title": "Self-supervised dense representation learning for live-cell microscopy with time arrow prediction",
    "year": 2023
}