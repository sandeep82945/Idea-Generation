{
    "abstractText": "Training computer vision models usually requires collecting and labeling vast amounts of imagery under a diverse set of scene configurations and properties. This process is incredibly time-consuming, and it is challenging to ensure that the captured data distribution maps well to the target domain of an application scenario. Recently, synthetic data has emerged as a way to address both of these issues. However, existing approaches either require human experts to manually tune each scene property or use automatic methods that provide little to no control; this requires rendering large amounts of random data variations, which is slow and is often suboptimal for the target domain. We present the first fully differentiable synthetic data pipeline that uses Neural Radiance Fields (NeRFs) in a closed-loop with a target application\u2019s loss function. Our approach generates data on-demand, with no human labor, to maximize accuracy for a target task. We illustrate the effectiveness of our method on synthetic and real-world object detection tasks. We also introduce a new \u201cYCB-in-the-Wild\u201d dataset and benchmark that provides a test scenario for object detection with varied poses in real-world environments. Code and data could be found at https://github.com/gyhandy/Neural-Sim-NeRF.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yunhao Ge"
        },
        {
            "affiliations": [],
            "name": "Harkirat Behl"
        },
        {
            "affiliations": [],
            "name": "Jiashu Xu"
        },
        {
            "affiliations": [],
            "name": "Suriya Gunasekar"
        },
        {
            "affiliations": [],
            "name": "Neel Joshi"
        },
        {
            "affiliations": [],
            "name": "Yale Song"
        },
        {
            "affiliations": [],
            "name": "Xin Wang"
        },
        {
            "affiliations": [],
            "name": "Laurent Itti"
        },
        {
            "affiliations": [],
            "name": "Vibhav Vineet"
        }
    ],
    "id": "SP:060621cd1694c00cab987b3d600b5f5a44a41997",
    "references": [
        {
            "authors": [
                "Ali Jahanian",
                "P.I. Lucy Chai"
            ],
            "title": "On the \u201dsteerability\u201d of generative adversarial networks",
            "venue": "CoRR",
            "year": 2019
        },
        {
            "authors": [
                "A. Barbu",
                "D. Mayo",
                "J. Alverio",
                "W. Luo",
                "C. Wang",
                "D. Gutfreund",
                "J. Tenenbaum",
                "B. Katz"
            ],
            "title": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "A. Barbu",
                "D. Mayo",
                "J. Alverio",
                "W. Luo",
                "C. Wang",
                "D. Gutfreund",
                "J. Tenenbaum",
                "B. Katz"
            ],
            "title": "Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",
            "venue": "Advances in neural information processing systems 32",
            "year": 2019
        },
        {
            "authors": [
                "H.S. Behl",
                "A.G. Baydin",
                "R. Gal",
                "P.H. Torr",
                "V. Vineet"
            ],
            "title": "Autosimulate:(quickly) learning synthetic data generation",
            "venue": "European Conference on Computer Vision. pp. 255\u2013271. Springer",
            "year": 2020
        },
        {
            "authors": [
                "S. Bi",
                "Z. Xu",
                "P. Srinivasan",
                "B. Mildenhall",
                "K. Sunkavalli",
                "M. Ha\u0161an",
                "Y. HoldGeoffroy",
                "D. Kriegman",
                "R. Ramamoorthi"
            ],
            "title": "Neural reflectance fields for appearance acquisition",
            "venue": "arXiv preprint arXiv:2008.03824",
            "year": 2020
        },
        {
            "authors": [
                "A. Brock",
                "J. Donahue",
                "K. Simonyan"
            ],
            "title": "Large scale GAN training for high fidelity natural image synthesis",
            "venue": "In: International Conference on Learning Representations (2019),",
            "year": 2019
        },
        {
            "authors": [
                "B. Calli",
                "A. Walsman",
                "A. Singh",
                "S. Srinivasa",
                "P. Abbeel",
                "A.M. Dollar"
            ],
            "title": "Benchmarking in manipulation research: The ycb object and model set and benchmarking protocols",
            "venue": "arXiv preprint arXiv:1502.03143",
            "year": 2015
        },
        {
            "authors": [
                "B. Colson",
                "P. Marcotte",
                "G. Savard"
            ],
            "title": "An overview of bilevel optimization",
            "venue": "Annals of operations research 153(1), 235\u2013256",
            "year": 2007
        },
        {
            "authors": [
                "S.M. Danilo Jimenez Rezende"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "ICML",
            "year": 2015
        },
        {
            "authors": [
                "M. Denninger",
                "M. Sundermeyer",
                "D. Winkelbauer",
                "Y. Zidan",
                "D. Olefir",
                "M. Elbadrawy",
                "A. Lodhi",
                "H. Katam"
            ],
            "title": "Blenderproc",
            "venue": "arXiv preprint arXiv:1911.01911",
            "year": 2019
        },
        {
            "authors": [
                "J. Devaranjan",
                "A. Kar",
                "S. Fidler"
            ],
            "title": "Meta-sim2: Unsupervised learning of scene structure for synthetic data generation",
            "venue": "European Conference on Computer Vision. pp. 715\u2013733. Springer",
            "year": 2020
        },
        {
            "authors": [
                "M.W. Diederik Kingma"
            ],
            "title": "Autoencoding variational bayes",
            "venue": "ICLR",
            "year": 2014
        },
        {
            "authors": [
                "C. Doersch",
                "A. Zisserman"
            ],
            "title": "Sim2real transfer learning for 3d human pose estimation: motion to the rescue",
            "venue": "NeurIPS",
            "year": 2019
        },
        {
            "authors": [
                "D. Dwibedi",
                "I. Misra",
                "M. Hebert"
            ],
            "title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection",
            "venue": "ICCV",
            "year": 2017
        },
        {
            "authors": [
                "L. Franceschi",
                "P. Frasconi",
                "S. Salzo",
                "R. Grazzi",
                "M. Pontil"
            ],
            "title": "Bilevel programming for hyperparameter optimization and meta-learning",
            "venue": "International Conference on Machine Learning. pp. 1568\u20131577. PMLR",
            "year": 2018
        },
        {
            "authors": [
                "G. Gafni",
                "J. Thies",
                "M. Zollhofer",
                "M. Nie\u00dfner"
            ],
            "title": "Dynamic neural radiance fields for monocular 4d facial avatar reconstruction",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8649\u20138658",
            "year": 2021
        },
        {
            "authors": [
                "Y. Ganin",
                "T. Kulkarni",
                "I. Babuschkin",
                "S.M.A. Eslami",
                "O. Vinyals"
            ],
            "title": "Synthesizing programs for images using reinforced adversarial learning",
            "venue": "ICML",
            "year": 2018
        },
        {
            "authors": [
                "Y. Ge",
                "S. Abu-El-Haija",
                "G. Xin",
                "L. Itti"
            ],
            "title": "Zero-shot synthesis with groupsupervised learning",
            "venue": "arXiv preprint arXiv:2009.06586",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ge",
                "J. Xu",
                "B.N. Zhao",
                "L. Itti",
                "V. Vineet"
            ],
            "title": "Dall-e for detection: Language-driven context image synthesis for object detection",
            "venue": "arXiv preprint arXiv:2206.09592",
            "year": 2022
        },
        {
            "authors": [
                "Y. Ge",
                "J. Zhao",
                "L. Itti"
            ],
            "title": "Pose augmentation: Class-agnostic object pose transformation for object recognition",
            "venue": "European Conference on Computer Vision. pp. 138\u2013155. Springer",
            "year": 2020
        },
        {
            "authors": [
                "I. Georgiev",
                "T. Ize",
                "M. Farnsworth",
                "R. Montoya-Vozmediano",
                "A. King",
                "B.V. Lommel",
                "A. Jimenez",
                "O. Anson",
                "S. Ogaki",
                "E Johnston"
            ],
            "title": "Arnold: A brute-force production path tracer",
            "venue": "TOG",
            "year": 2018
        },
        {
            "authors": [
                "I. Goodfellow",
                "J. Pouget-Abadie",
                "M. Mirza",
                "B. Xu",
                "D. Warde-Farley",
                "S. Ozair",
                "A. Courville",
                "Y. Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in neural information processing systems. pp. 2672\u20132680",
            "year": 2014
        },
        {
            "authors": [
                "A. Handa",
                "V. Patraucean",
                "V. Badrinarayanan",
                "S. Stent",
                "R. Cipolla"
            ],
            "title": "Understanding real world indoor scenes with synthetic data",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "G. Gkioxari",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "ICCV",
            "year": 2017
        },
        {
            "authors": [
                "I. Higgins",
                "L. Matthey",
                "A. Pal",
                "C. Burgess",
                "X. Glorot",
                "M. Botvinick",
                "S. Mohamed",
                "A. Lerchner"
            ],
            "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "venue": "5th International Conference on Learning Representations, ICLR 2017, Toulon, France",
            "year": 2017
        },
        {
            "authors": [
                "T. Hodan",
                "F. Michel",
                "E. Brachmann",
                "W. Kehl",
                "A. GlentBuch",
                "D. Kraft",
                "B. Drost",
                "J. Vidal",
                "S. Ihrke",
                "X Zabulis"
            ],
            "title": "Bop: Benchmark for 6d object pose estimation",
            "venue": "Proceedings of the European Conference on Computer Vision (ECCV). pp. 19\u201334",
            "year": 2018
        },
        {
            "authors": [
                "T. Hoda\u0148",
                "V. Vineet",
                "R. Gal",
                "E. Shalev",
                "J. Hanzelka",
                "T. Connell",
                "P. Urbina",
                "S. Sinha",
                "B. Guenter"
            ],
            "title": "Photorealistic image synthesis for object instance detection",
            "venue": "ICIP",
            "year": 2019
        },
        {
            "authors": [
                "E. Ilg",
                "N. Mayer",
                "T. Saikia",
                "M. Keuper",
                "A. Dosovitskiy",
                "T. Brox"
            ],
            "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "W. Jang",
                "L. Agapito"
            ],
            "title": "Codenerf: Disentangled neural radiance fields for object categories",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 12949\u201312958",
            "year": 2021
        },
        {
            "authors": [
                "A. Kar",
                "A. Prakash",
                "M.Y. Liu",
                "E. Cameracci",
                "J. Yuan",
                "M. Rusiniak",
                "D. Acuna",
                "A. Torralba",
                "S. Fidler"
            ],
            "title": "Meta-sim: Learning to generate synthetic datasets",
            "venue": "ICCV",
            "year": 2019
        },
        {
            "authors": [
                "T.Y. Lin",
                "P. Goyal",
                "R. Girshick",
                "K. He",
                "P. Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "Proceedings of the IEEE international conference on computer vision. pp. 2980\u20132988",
            "year": 2017
        },
        {
            "authors": [
                "G. Louppe",
                "K. Cranmer"
            ],
            "title": "Adversarial variational optimization of non-differentiable simulators",
            "venue": "AISTATS",
            "year": 2019
        },
        {
            "authors": [
                "R. Martin-Brualla",
                "N. Radwan",
                "M.S. Sajjadi",
                "J.T. Barron",
                "A. Dosovitskiy",
                "D. Duckworth"
            ],
            "title": "Nerf in the wild: Neural radiance fields for unconstrained photo collections",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 7210\u20137219",
            "year": 2021
        },
        {
            "authors": [
                "B. Mildenhall",
                "P.P. Srinivasan",
                "M. Tancik",
                "J.T. Barron",
                "R. Ramamoorthi",
                "R. Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "European conference on computer vision. pp. 405\u2013421. Springer",
            "year": 2020
        },
        {
            "authors": [
                "A. Ng"
            ],
            "title": "Mlops: From model-centric to data-centric ai. https://www.deeplearning.ai/wp-content/uploads/2021/06/ MLOps-From-Model-centric-to-Data-centric-AI.pdf",
            "year": 2021
        },
        {
            "authors": [
                "K. Park",
                "U. Sinha",
                "J.T. Barron",
                "S. Bouaziz",
                "D.B. Goldman",
                "S.M. Seitz",
                "R. MartinBrualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "ICCV",
            "year": 2021
        },
        {
            "authors": [
                "J. Reizenstein",
                "R. Shapovalov",
                "P. Henzler",
                "L. Sbordone",
                "P. Labatut",
                "D. Novotny"
            ],
            "title": "Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 10901\u201310911",
            "year": 2021
        },
        {
            "authors": [
                "S. Ren",
                "K. He",
                "R. Girshick",
                "J. Sun"
            ],
            "title": "Faster r-cnn: towards real-time object detection with region proposal networks",
            "venue": "PAMI",
            "year": 2017
        },
        {
            "authors": [
                "S.R. Richter",
                "Z. Hayder",
                "V. Koltun"
            ],
            "title": "Playing for benchmarks",
            "venue": "ICCV",
            "year": 2017
        },
        {
            "authors": [
                "S.R. Richter",
                "V. Vineet",
                "S. Roth",
                "V. Koltun"
            ],
            "title": "Playing for data: Ground truth from computer games",
            "venue": "European conference on computer vision. pp. 102\u2013118. Springer",
            "year": 2016
        },
        {
            "authors": [
                "G. Ros",
                "L. Sellart",
                "J. Materzynska",
                "D. V\u00e1zquez",
                "A.M. L\u00f3pez"
            ],
            "title": "The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes",
            "venue": "CVPR",
            "year": 2016
        },
        {
            "authors": [
                "N. Ruiz",
                "S. Schulter",
                "M. Chandraker"
            ],
            "title": "Learning to simulate",
            "venue": "ICLR",
            "year": 2019
        },
        {
            "authors": [
                "J.L. Schonberger",
                "J.M. Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
            "year": 2016
        },
        {
            "authors": [
                "E. Shelhamer",
                "J. Long",
                "T. Darrell"
            ],
            "title": "Fully convolutional networks for semantic segmentation",
            "venue": "PAMI",
            "year": 2017
        },
        {
            "authors": [
                "P.P. Srinivasan",
                "B. Deng",
                "X. Zhang",
                "M. Tancik",
                "B. Mildenhall",
                "J.T. Barron"
            ],
            "title": "Nerv: Neural reflectance and visibility fields for relighting and view synthesis",
            "venue": "CVPR",
            "year": 2021
        },
        {
            "authors": [
                "J. Tremblay",
                "T. To",
                "S. Birchfield"
            ],
            "title": "Falling things: A synthetic dataset for 3d object detection and pose estimation",
            "venue": "CVPR",
            "year": 2018
        },
        {
            "authors": [
                "R.J. Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine Learning",
            "year": 1992
        },
        {
            "authors": [
                "R.J. Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning 8(3), 229\u2013256",
            "year": 1992
        },
        {
            "authors": [
                "Y. Xiang",
                "T. Schmidt",
                "V. Narayanan",
                "D. Fox"
            ],
            "title": "Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes",
            "venue": "arXiv preprint arXiv:1711.00199",
            "year": 2017
        },
        {
            "authors": [
                "Xiaogang Xu",
                "J.J. Ying-Cong Chen"
            ],
            "title": "View independent generative adversarial network for novel view synthesis",
            "venue": "ICCV",
            "year": 2019
        },
        {
            "authors": [
                "D. Yang",
                "J. Deng"
            ],
            "title": "Learning to generate synthetic 3d training data through hybrid gradient",
            "venue": "CVPR",
            "year": 2020
        },
        {
            "authors": [
                "L. Yen-Chen"
            ],
            "title": "Nerf-pytorch",
            "venue": "https://github.com/yenchenlin/nerf-pytorch/",
            "year": 2020
        },
        {
            "authors": [
                "X. Zhang",
                "P.P. Srinivasan",
                "B. Deng",
                "P. Debevec",
                "W.T. Freeman",
                "J.T. Barron"
            ],
            "title": "Nerfactor: Neural factorization of shape and reflectance under an unknown illumination",
            "venue": "ACM Transactions on Graphics (TOG) 40(6), 1\u201318",
            "year": 2021
        },
        {
            "authors": [
                "Y. Zhang",
                "S. Song",
                "E. Yumer",
                "M. Savva",
                "J. Lee",
                "H. Jin",
                "T.A. Funkhouser"
            ],
            "title": "Physically-based rendering for indoor scene understanding using convolutional neural networks",
            "venue": "CVPR",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": "Keywords: Synthetic data, NeRF, Bilevel optimization, Detection"
        },
        {
            "heading": "1 Introduction",
            "text": "The traditional pipeline for building computer vision models involves collecting and labelling vast amounts of data, training models with different configurations, and deploying it to test environments [24,38,44]. Key to achieving good performance is collecting training data that mimics the test environment with similar properties relating to the object (pose, geometry, appearance), camera (pose and angle), and scene (illumination, semantic structures)[2].\nHowever, the traditional pipeline does not work very well in many real-world applications as collecting large amounts of training data which captures all variations of objects and environments is quite challenging. Furthermore, in many\n* Equal contribution as second author\nar X\niv :2\n20 7.\n11 36\n8v 1\n[ cs\n.C V\n] 2\n2 Ju\nl 2 02\n2 Y. Ge et al.\napplications, users may want to learn models for unique objects with novel structures, textures, or other such properties. Such scenarios are very common particularly in business scenarios where there is desire to create object detectors for new products introduced in the market.\nRecent advances in rendering, such as photo-realistic renderers [10,21] and generative models (GANs [6], VAEs [12,25]), have brought the promise of generating high-quality images of complex scenes. This has motivated the field to explore synthetic data as source of training data [13,28,14,23,27,39,41,46,18,54,19]. However, doing so in an offline fashion has similar issues as the traditional pipeline. While it alleviates certain difficulties, e.g., capturing camera/lighting variations, it create dependency on 3D asset creation, which is time-consuming.\nRecently, a new image generation technique called the Neural Radiance Field (NeRF) [34] was introduced as a way to replace the traditional rasterization and ray-tracing graphics pipelines with a neural-network based renderer. This approach can generate high-quality novel views of scenes without requiring explicit 3D understanding. More recent advancements in NeRFs allow to control other rendering parameters, like illumination, material, albedo, appearance, etc. [45,33,53,5,29]. As a result, they have attracted significant attention and have been widely adopted in various graphics and vision tasks [16,5,45,36]. NeRF and their variants possess some alluring properties: (i) differentiable rendering, (ii) control over scene properties unlike GANs and VAEs, and (iii) they are data-driven in contrast to traditional renderers which require carefully crafting 3D models and scenes. These properties make them suitable for generating the optimal data on-demand for a given target task.\nTo this end, we propose a bilevel optimization process to jointly optimize neural rendering parameters for data generation and model training. Further, we\nNeural-Sim 3\nalso propose a reparameterization trick, sample approximation, and patch-wise optimization methods for developing a memory efficient optimization algorithm.\nTo demonstrate the efficacy of the proposed algorithm, we evaluate the algorithm on three settings: controlled settings in simulation, on the YCB-video dataset [49], and in controlled settings on YCB objects captured in the wild. This third setting is with our newly created \u201cYCB-in-the-wild\u201d dataset, which involves capturing YCB objects in real environments with control over object pose and scale. Finally, we also provide results showing the interpretability of the method in achieving high performance on downstream tasks. Our key contributions are as follows:\n(1) To the best of our knowledge, for the first time, we show that NeRF can substitute the traditional graphics pipeline and synthesize useful images to train downstream tasks (object detection).\n(2) We propose a novel bilevel optimization algorithm to automatically optimize rendering parameters (pose, zoom, illumination) to generate optimal data for downstream tasks using NeRF and its variants.\n(3) We demonstrate the performance of our approach on controlled settings in simulation, controlled settings in YCB-in-wild and YCB-video datasets. We release YCB-in-wild dataset for future research."
        },
        {
            "heading": "2 Related work",
            "text": "Traditional Graphics rendering methods can synthesize high-quality images with controllable image properties, such as object pose, geometry, texture, camera parameters, and illumination [39,10,21,27,40]. Interestingly, NeRF has some important benefits over the traditional graphics pipelines, which make it more suitable for learning to generate synthetic datasets. First, NeRF learns to generate data from new views based only on image data and camera pose information. In contrast, the traditional graphics pipeline requires 3D models of objects as input. Getting accurate 3D models with correct geometry, material, and texture properties generally requires human experts (i.e. an artist or modeler). This, in turn, limits the scalability of the traditional graphics pipeline in large-scale rendering for many new objects or scenes. Second, NeRF is a differentiable renderer, thus allowing backpropagation through the rendering pipeline for learning how to control data generation in a model and scene-centric way.\nDeep generative models, such as GANs [22,6], VAEs [12,25] and normalizing flows [9] are differentiable and require less human involvement. However, most of them do not provide direct control of rendering parameters. While some recent GAN approaches allow some control [50,1,20] over parameters, it is not as explicit and can mostly only change the 2D properties of images. Further, most generative models need a relatively large dataset to train. In comparison, NeRF can generate parameter-controllable high-quality images and requires a lesser number of images to train. Moreover, advancements in NeRF now allow the control of illumination, materials, and object shape alongside camera pose and\n4 Y. Ge et al.\nscale [45,33,53,5,29]. We use NeRF and their variants (NeRF-in-the-wild [33]) to optimize pose, zoom and illumination as representative rendering parameters. Learning simulator parameters. Related works in this space focus on learning non-differentiable simulator parameters for e.g., learning-to-simulate (LTS) [42], Meta-Sim [30], Meta-Sim2 [11], Auto-Sim [4], and others [51,17,32]. Our work in contrast has two differences: (i) a difference in the renderer used (NeRF vs traditional rendering engines), and (ii) a difference in the optimization approach. We discuss the different renderers and their suitability for this task in the previous subsection.\nLTS [42] proposed a bilevel optimization algorithm to learn simulator parameters that maximized accuracy on downstream tasks. It assumed both datageneration and model-training as a black-box optimization process and used REINFORCE-based [47] gradient estimation to optimize parameters. This requires many intermediate data generation steps. Meta-sim [30] is also a REINFORCE based approach, which requires a grammar of scene graphs. Our approach does not use scene grammar. Most similar to our work is the work of Auto-Simulate [4] that proposed a local approximation of the bilevel optimization to efficiently solve the problem. However, since they optimized non-differentiable simulators like Blender [10] and Arnold [21], they used REINFORCE-based [47] gradient update. Further, they have not shown optimization of pose parameter whose search space is very large. In comparison, our proposed Neural-Sim approach can learn to optimize over pose parameters as well."
        },
        {
            "heading": "3 Neural-Sim",
            "text": "The goal of our method is to automatically synthesize optimal training data to maximize accuracy for a target task. In this work, we consider object detection as our target task. Furthermore, in recent times, NeRF and its variants (NeRFs) have been used to synthesize high-resolution photorealistic images for complex scenes [45,33,53,5,29]. This motivates us to explore NeRFs as potential sources of generating training data for computer vision models. We propose a technique to optimize rendering parameters of NeRFs to generate the optimal set of images for training object detection models.\nNeRF model: NeRF [34,52] takes as input the viewing direction (or camera pose) denoted as V = (\u03d5, \u03c1), and renders an image x = NeRF(V ) of a scene as viewed along V . Note that our proposed technique is broadly applicable to differentiable renderers in general. In this work, we also optimize NeRF-in-the-wild (NeRF-w) [33] as it allows for appearance and illumination variations alongside pose variation. We first discuss our framework for optimizing the original NeRF model and later we discuss optimization of NeRF-w in Section 3.2.\nSynthetic training data generation: Consider a parametric probability distribution p\u03c8 over rendering parameters V , where \u03c8 denotes the parameters of the distribution. It should be noted that \u03c8 corresponds to all rendering parameters including pose/zoom/illumination, here, for simplicity, we consider \u03c8 to denote pose variable. To generate the synthetic training data, we first sample ren-\nNeural-Sim 5\ndering parameters V1, V2, ..., VN \u223c p\u03c8. We then use NeRF to generate synthetic training images xi = NeRF(Vi) with respective rendering parameters Vi. We use an off-the-shelf foreground extractor to obtain labels y1, y2, . . . , yN . the training dataset thus generated is denoted as Dtrain = {(x1, y1), (x2, y2), . . . , (xN , yN )}.\nOptimizing synthetic data generation Our goal is to optimize over the rendering distribution p\u03c8 such that training an object detection model on Dtrain leads to good performance on Dval. We formulate this problem as a bi-level optimization [8,15,4] as below:\nmin \u03c8 Lval(\u03b8\u0302(\u03c8)); s.t. \u03b8\u0302(\u03c8) \u2208 argmin \u03b8 Ltrain(\u03b8, \u03c8), (1a)\nwhere \u03b8 denotes the parameters of the object detection model, Ltrain(\u03b8, \u03c8) = EV\u223cp\u03c8 l(x, \u03b8) \u2248 1N \u2211N i=1 l(xi, \u03b8) is the training loss over the synthetic dataset from NeRF,3 and Lval is the loss on the task-specific validation set Dval. The bi-level optimization problem in (1) is challenging to solve; for example, any gradient based algorithm would need access to an efficient approximation of \u2207\u03c8 \u03b8\u0302(\u03c8), which in turn requires propagating gradients through the entire training trajectory of a neural network. Thus, we look to numerical approximations to solve this problem. Recently, Behl et. al. [4] developed a technique for numerical gradient computation based on local approximation of the bi-level optimization. Without going into their derivation, we borrow the gradient term for the outer update, which at time step t takes the form:\n\u2202Lval(\u03b8\u0302(\u03c8)) \u2202\u03c8 \u2223\u2223\u2223\u2223\u2223 \u03c8=\u03c8t \u2248\u2212 \u2207NeRF\ufe37 \ufe38\ufe38 \ufe37 \u2202 \u2202\u03c8 [\u2202Ltrain(\u03b8\u0302(\u03c8t), \u03c8) \u2202\u03b8 ]T \u2223\u2223\u2223\u2223\u2223 \u03c8=\u03c8t H(\u03b8\u0302(\u03c8t), \u03c8)\u22121 dLval(\u03b8\u0302(\u03c8t))\nd\u03b8\ufe38 \ufe37\ufe37 \ufe38 \u2207TV .\n(2)\n3 For simplicity, we have dropped the dependence of loss \u2113 on labels y\n6 Y. Ge et al.\nWe have divided the gradient term into two parts: \u2207NeRF corresponds to backpropagation through the dataset generation from NeRF, and \u2207TV corresponds to approximate backpropagation through training and validation (Fig. 2). \u2207TV is computed using the conjugate gradient method [4]. However, [4] treated the data generation as a black box and used REINFORCE [48] to compute the approximate gradient because they used non-differentiable renderers for data generation. However, REINFORCE is considered noisy process and is known to lead to high-variance estimates of gradients. In contrast, NeRF is differentiable, which gives us tools to obtain more accurate gradients. We propose an efficient technique for computing \u2207NeRF , which we discuss in the next section."
        },
        {
            "heading": "3.1 Backprop through data generation from NeRF",
            "text": "A good gradient estimation should possess the following properties: (i) high accuracy and low noise, (ii) computational efficiency, (iii) low memory footprint. We leverage different properties of NeRF, i.e., its differentiability and pixel-wise rendering, to design a customized technique which satisfies the above properties.\nIn computation of \u2207NeRF in (2), we approximate Ltrain(\u03b8, \u03c8) using samples in Dtrain as Ltrain(\u03b8, \u03c8) \u2248 1N \u2211N i=1 l(xi, \u03b8). Using chain rule we then have partial derivative computation over l(x, \u03b8) as follows:\n\u2202\n\u2202\u03c8\n[ \u2202l(xi, \u03b8\u0302(\u03c8t))\n\u2202\u03b8\n] = [ \u2202(\u2202l(xi,\u03b8\u0302(\u03c8t))\u2202\u03b8 )\n\u2202xi\n][ \u2202xi \u2202Vi ][ dVi d\u03c8 ] (3)\nThe first term is the second order derivative through an object detection network and can be computed analytically for each image xi. The second term is the gradient of the rendered image w.r.t NeRF inputs, which again is well defined and can be obtained by backpropagating through the differentiable NeRF rendering xi = NeRF(Vi). While both these terms have exact analytical expressions, naively computing and using them in (2) becomes impractical even for small problems (see below in Tool2 and Tool3 for details and proposed solutions). Finally the third term dVid\u03c8 requires gradient computation over probabilistic sampling Vi \u223c p\u03c8. We consider p\u03c8 over discretized bins of pose parameters. For such discrete distributions dVid\u03c8 is not well defined. Instead, we approximate this term using a reparameterization technique described below in Tool1. We summarize our technical tools below: \u2013 For distributions p\u03c8 over a discrete bins of pose parameters, we propose a\nreparametrization of \u03c8 that provides efficient approximation of dVid\u03c8 (Tool1). \u2013 We dramatically reduce memory and computation overhead of implementing\nthe gradient approximation in (2) using a new twice-forward-once-backward approach (Tool2). Without this new technique the implementation would require high computation involving large matrices and computational graphs. \u2013 Even with the above technique, the computation of first and second terms in (3) has a large overhead in terms of GPU memory that depends on image size. We overcome this using a patch-wise gradient computation approach described in Tool 3.\nNeural-Sim 7\nTool 1: Reparametrization of pose sampling NeRF renders images xj using camera pose Vj=(\u03d5i, \u03c1j), where \u03d5j \u2208 [0, 360], \u03c1j \u2208 [0, 360]. For simplicity we describe our method for optimizing over just \u03d5, while keeping \u03c1 fixed to be uniform.\nWe discretize the pose space into k equal sized bins over the range of \u03d5 as B1 = [ 0, 360k ) , B2 = [ 360 k , 360\u00d72 k ) , . . .. and define the distribution over \u03d5 as the categorical distribution with pi as the probability of \u03d5 belonging to Bi. This distribution is thus parametrized by \u03c8 \u2261 p = [p1, ..., pk].\nTo back propagate through the sampling process, we approximate the sample from the categorical distribution by using Gumble-softmax \u201creparameterization trick\u201d with parameters y \u2208 Rk, where yi are given as follows:\nyi = GSi(p) = exp[(Gi + log(pi))/\u03c4 ]/ \u2211 j exp[(Gi + log(pj))/\u03c4 ], (4)\nwhere Gi \u223c Gumbel(0, 1) are i.i.d. samples from the standard Gumbel distribution and \u03c4 is temperature parameter. The random vector y defined as above satisfies the property that the coordinate (index) of the largest element in y \u2208 Rk follows the categorical distribution with parameter p.\nWe now approximate sampling from the categorical distribution (see Figures 3 and 4 for depiction). Denote the bin center of Bi as B\u0304 ce i = 360(i\u2212 0.5)/k; and the bin range as b\u0304ra = 360/k. We generate Vj = (\u03d5j , \u03c1j) \u223c p\u03c8 as below: \u2013 Generate yi\u2019s for i = 1, 2, . . . k from (4) \u2013 Define bcej = \u2211 i yiB\u0304 ce i as the approximate bin center.\n\u2013 Define the bin for the jth sample centered around bcej as [b st j , b en j ] = [b ce j \u2212\nb\u0304ra/2, bcej + b\u0304 ra/2]\n\u2013 We sample \u03d5j from uniform distribution over [b st j , b en j ] which has a standard\nreparametrization for diffentiability: U(bstj , benj ) \u2261 (1 \u2212 \u03f5)bstj + \u03f5benj s.t. \u03f5 \u223c U(0, 1). \u2013 \u03c1j \u223c U [0, 360], or can follow same process as \u03d5j . Note that in general the approximate bin centers bcej need not be aligned with original categorical distribution, however we can control the approximation using the temperature parameter \u03c4 . As \u03c4 \u2192 0, y will be a one-hot vector and exactly emulate sampling from categorical distribution.\n8 Y. Ge et al.\nPhi: optimize (-180~180) Theta: fix (85~95) 10\u00b0 range\n\ud835\udf13\ud835\udc57\nWe now have the full expression for approximate gradient of \u2207NeRF using (3) and reparametrization as follows:\n\u2207NeRF \u2248 1\nN N\u2211 j=1 \u2202( \u2202l(xj ,\u03b8\u0302(\u03c8t)) \u2202\u03b8 ) \u2202xj \u2202xj \u2202Vj \u2202Vj \u2202(bstj , b en j ) \u2202(bsti , b en i ) \u2202y \u2202y \u2202p . (5)\nBelow we present tools that drastically improve the compute and memory efficiency and are crucial for our pipeline.\nTool 2: Twice-forward-once-backward The full gradient update of our bilevel optimization problem involves using the approximation of\u2207NeRF in (5) and back in (2). This computation has three terms with the following dimensions: (1) \u2202( \u2202l(xj,\u03b8\u0302(\u03c8t)) \u2202\u03b8 )\n\u2202xj \u2208 Rm\u00d7d, (2) \u2202xj\u2202\u03c8 \u2208 R d\u00d7k, (3)\u2207TV = H(\u03b8\u0302(\u03c8t), \u03c8)\u22121 dLval(\u03b8\u0302(\u03c8t))d\u03b8 \u2208 Rm\u00d71, where m = |\u03b8| is the # of parameters in object detection model, d is the # of pixels in x, and k is # of pose bins.\nImplementing eq. (2) with the naive sequence of (1)-(2)-(3) involves computing and multiplying large matrices of sizes m \u00d7 d and d \u00d7 k. Further, this sequence also generates a huge computation graph. These would lead to prohibitive memory and compute requirements as m is often in many millions. On the other hand, if we could follow the sequence of (3)-(1)-(2), then we can use the produce of 1\u00d7m output of (3) to do a weighted autograd which leads computing and storing only vectors rather than matrices. However, the computation of (3) needs the rendered image involving forward pass of (2) (more details in supp.).\nTo take advantage of the efficient sequence, we propose a twice-forward-once backward method where we do two forward passes over NeRF rendering. In the first forward path, we do not compute the gradients, we only render images to form Dtrain and save random samples of y, \u03d5j used for rendering. We then compute (3) by turning on gradients. In the second pass through NeRF, we keep the same samples and this time compute the gradient (1) and (2).\nTool 3: Patch-wise gradient computation Even though we have optimized the computation dependence on m = |\u03b8| with the tool described above, computing (1)-(2) sequence in the above description still scales with the size of images\nNeural-Sim 9\nd. This too can lead to large memory footprint for even moderate size images (e.g., even with the twice-forward-once-backward approach, the pipeline over a 32\u00d732 image already does not fit into a 2080Ti GPU). To optimize the memory further, we propose patch-wise computation, where we divide the image into S patches x = (x1, x2, . . . , xS)) and compute (3) as follows:\n\u2202\n\u2202\u03c8\n\u2202l(x, \u03b8\u0302(\u03c8t))\n\u2202\u03b8 = S\u2211 c=1 \u2202(\u2202l(x c,\u03b8\u0302(\u03c8t)) \u2202\u03b8 ) \u2202xc \u2202xc \u2202\u03c8 . (6)\nSince NeRF renders an image pixel by pixel, it is easy to compute the gradient of patch with respect to \u03c8 in the memory efficient patch-wise optimization."
        },
        {
            "heading": "3.2 Nerf-in-the-wild",
            "text": "NeRF-in-the-wild (NeRF-w) extends the vanilla NeRF model to allow image dependent appearance and illumination variations such that photometric discrepancies between images can be modeled explicitly. NeRF-w takes as input an appearance embedding denoted as \u2113 alongside the viewing direction V to render an image as x = NeRF(V, \u2113). For NERF-w, the optimization of pose (V) remains the same as discussed above. For efficient optimization of lighting we exploit a noteworthy property of NeRF-w: it allows smooth interpolations between color and lighting. This enables us to optimize lighting as a continuous variable, where the lighting (\u2113) can be written as an affine function of the available lighting embeddings (\u2113i) as \u2113 = \u2211 i \u03c8i \u2217 \u2113i where \u2211 i \u03c8i = 1. To calculate the gradient from Eq. 3, \u2202xi\u2202\u2113 is computed in the same way as described above utilizing our tools 2 and 3, and the term d\u2113d\u03c8 is straightforward and is optimized with projected gradient descent."
        },
        {
            "heading": "4 Experiments",
            "text": "We now evaluate the effectiveness of our proposed Neural-Sim approach in generating optimal training data on object detection task. We provide results under two variations of our Neural-Sim method. In the first case, we use Neural-Sim without using bi-level optimization steps. In this case, data from NeRF are always generated from the same initial distribution. The second case involves our complete Neural-Sim pipeline with bi-level optimization updates (Eq. 2). In the following sections, we use terms NS and NSO for Neural-Sim without and Neural-Sim with bi-level optimization respectively.\nWe first demonstrate that NeRF can successfully generate data for downstream tasks as a substitute for a traditional graphic pipeline (e.g., BlenderProc) (Sec. 4.1) with similar performance. Then we conduct experiments to demonstrate the efficacy of Neural-Sim in three different scenarios: controllable synthetic tasks on YCB-synthetic dataset (Sec. 4.2); controllable real-world tasks on YCB-in-the-wild dataset (Sec. 4.3); general real-world tasks on YCB-Video dataset (Sec. 4.4). We also show the interpretable properties of the Neural-Sim\n10 Y. Ge et al.\napproach (NSO) during training data synthesis (Sec. 4.5). All three datasets are based on the objects from the YCB-video dataset [49,26,7]. It contains 21 objects from daily life and provides high-resolution RGBD images with ground truth annotation for object bounding boxes. The dataset consists of both digital and physical objects, which we use to create both real and synthetic datasets. Implementation details: We train one NeRF-w model for each YCB object using 100 images with different camera pose and zoom factors using BlenderProc. We use RetinaNet [31] as our downstream object detector. To accelerate the optimization, we fix the backbone during training. During bi-level optimization steps, we use Gumble-softmax temperature \u03c4 = 0.1. In each optimization iteration, we render 50 images for each object class and train RetinaNet for two epochs. More details are in the supplementary material. Baselines: We compare our proposed approach against two popular state-ofthe-art approaches that learn simulator parameters. The first baseline is Learning to simulate [42] which proposed a REINFORCE-based approach to optimize simulator parameters. Also note that the meta-sim [30] is a REINFORCE-based approach. Next, we consider Auto-Sim [4] which proposed an efficient optimization method to learn simulator parameters. We implemented our own version of Learning to simulate work and we received code from the authors of Auto-Sim."
        },
        {
            "heading": "4.1 NeRF to generate data for downstream tasks",
            "text": "First, it is important to show that NeRF is a suitable replacement for a traditional renderer like BlenderProc [10] when generating data for object detection. To test this, we use YCB-video dataset objects and we render images from NeRF and BlenderProc [10] using the same camera pose and zoom parameters. We use these images to conduct object detection tasks under same training and test setting. Both object detectors trained on NeRF synthesized images and BlenderProc images have nearly same accuracy. (More details in Supplementary)."
        },
        {
            "heading": "4.2 YCB-synthetic dataset",
            "text": "Next, we conduct a series of experiments on a YCB-synthetic dataset to show how NSO helps to solve a drop in performance due to distribution shifts between the training and test data. Dataset setting We select six objects that are easily confused with each other: masterchef and pitcher are both blue cylinders and cheezit, gelatin, mug and driller are all red colored objects. To conduct controlled experiments, we generate data with a gap in the distribution of poses between the training and test sets. For this, we divide the object pose space into k= 8 bins. For each objects oj and pose bin i combination, we use BlenderProc 4 to synthesize 100 images. These images of the six selected objects with pose bin-labels form YCB-synthetic data.\n4 BlenderProc is a popular code-base to generate photo realistic synthetic data using traditional graphics pipeline."
        },
        {
            "heading": "Train/test",
            "text": "010 2030 4050 6070 8090\nfull overlap no overlap\nTrain/test biasness We create controlled experiments by varying the degree of pose distribution overlap between the training and test sets. For each object (e.g. pitcher) we fix its pose distribution in the test set (e.g. images are generated with pose from bin 1) and change its pose distribution in training set in three ways. First, images are generated with pose with same distribution as test set (bin1 is dominant), uniform distribution (pose values uniformly selected from bin1 to bin 8) and totally different distribution from the test set (other bins are dominant except bin 1). We introduce such pose biasness in two of the six objects, pitcher and driller. For other four objects, test images are generated from an uniform distribution. The test set has 600 images (100 images per object).\nResults Quantitative results are shown in Fig. 5. First, we show the performance of our NS based training images rendered using three initial distributions described earlier. We observe that the object detection performance drops by almost 30% and 10% for pitcher and driller objects respectively when there is object pose gap between training and test distributions.\nNext we show that our Neural-Sim with bi-level optimization (NSO) is able to automatically find the optimal pose distribution of the test set. NeRF then uses the optimal distribution to synthesize training data. The object detection model trained on the optimal data helps improve performance significantly; average precision accuracy for the pticher and driller objects have been improved by almost 30% and 10%, respectively. The blue lines in Fig. 5 represent the performance of NSO which fill the gap caused by distribution mismatch. Note there is similar significant improvement in experiments where there is gap in camera zoom when using the proposed NSO approach.\nWe compare our NSO with the recent work Learning-to-simulate (LTS) [42] and Auto-Sim [4] that use REINFORCE for non-differentiable simulator optimization (Fig. 5(a)(b)). We observe that on pose optimization, the proposed NSO achieves almost 34% improvement over LTS and 11% improvement over Auto-Sim on on the pitcher object. On zoom optimization, NSO achieves almost 27% improvement over LTS and 26% improvement over Auto-Sim on Masterchef\n12 Y. Ge et al.\nobject. This highlights the gradients from differentiable NSO are more effective and can generate better data than REINFORCE based LTS and Auto-Sim. Experiments on illumination optimization. To verify the effectiveness of Neural-Sim on illumination, we substitute vanilla NeRF model with NeRF-w. We conduct similar experiments as the pose and zoom experiments in Sec. 4.2 on illumination with YCB-synthetic dataset. The results show in Fig. 5(c). NSO has great performance on illumination optimization with 16% and 15% improvements on driller and banana objects respectively. Large scale YCB-Synthetic dataset experiments Here we highlight the results of our large-scale experiments on the YCB-synthetic dataset. Experiments demonstrate that our proposed NSO approach helps to solve a drop in performance due to distribution shifts between the train and test sets. We use the same setting as previous experiment except we conduct object detection on all 21 objects on the YCB-Synthetic dataset. We create controlled experiments by varying the degree of pose distribution overlap between the training and test sets. For each object, we fix its pose distribution in the test set and change its pose distribution in the training set: training images are generated from totally different distributions from the test set. The test set has 2100 images (100 images per object). The experiment results are shown in Table. 1. We compare the proposed NS and NSO approaches with the baseline Auto-Sim [4] method. Note that our proposed NSO achieves improvements of almost 14 % and 13 % points over NS and Auto-Sim baselines respectively."
        },
        {
            "heading": "4.3 YCB-in-the-wild dataset",
            "text": "To evaluate the performance of the proposed NS and NSO approaches on a real world dataset, we have created a real world YCB-in-the-wild dataset. The dataset has 6 YCB objects in it, which are same as in the YCB-synthetic dataset: masterchef, cheezit, gelatin, pitcher, mug and driller. All images are captured using a smartphone camera in a common indoor environments: living room, kitchen, bedroom and bathroom, under natural pose and illumination. We manually labelled each image with object bounding boxes. Further, to explore the effect of distribution shifts on the object detection task, we manually labelled the object pose in each image using the the same eight bins discussed earlier. The dataset\nNeural-Sim 13\nBin-1 Bin-2 Bin-3 Bin-4 Bin-1 Bin-2 Bin-3 Bin-1,2 Bin-1,3 Bin-1,4 Bin-1,2,3 Bin-1,3,4\nAP AP AP\nNS [Ours w/o Opt] Fix as Uniform bins NSO [Ours with Opt] Initialize as Uniform bins\nNS [Ours w/o Opt ] Fix as Random bin NSO[Ours with Opt] Initialize as Random bin\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPitcher\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\nPitcher-multi model\n0\n10\n20\n30\n40\n50\n60\n70\n80\nCheezit box\n[Auto-Sim] Initialize as Uniform bins [Auto-Sim] Initialize as Random bin\nconsists of total around 1300 test images with each object having over 200 images. Some of the images from the dataset are shown in the Fig 1. We will release the original images, ground truth object detection labels and pose bin-labels.\nTo explore the performance of the NS and NSO under the training and test distribution gap on the YCB-in-the-wild, we use the same experiment setup as in Sec. 4.2. The test images are selected from YCB-in-the-wild and training images are synthesized by NeRF. The training data is generated under two categorical distributions: uniform distribution and a random bin as dominant bin.\nQuantitative results are provided in the Fig. 6. First we highlight the performance achieved by our NS approach to generate data according two different initial pose distributions. We observe that NS generated data helps achieve up to 30% in object detection accuracy on different objects starting from two different initial distributions. Moreover, our NSO approach achieves remarkable improvement in every experimental setup. For example, on pitcher, starting from uniform and random distributions, our optimization improve performance by almost 60%. Compared with other optimization methods LTS and Auto-Sim, we observe large improvement upto 58% improvement over LTS and 60% improvement over Auto-Sim on the pitcher object. We observe a similar behavior on the cheeze box and also on multi-modal experiment setting. This highlights three points. First, NeRF can be used to generate good data to solve object detection task in the wild; far more importantly, our Neural-Sim with bi-level optimization (NSO) approach can automatically find the optimal data that can help achieve remarkable improvements in accuracy on images captured in the wild. Third, the gradients from NSO are more effective and can generate better data than REINFORCE based LTS and Auto-Sim.\n14 Y. Ge et al."
        },
        {
            "heading": "4.4 YCB Video dataset",
            "text": "To show the performance of the proposed NS and NSO approaches on a general real world dataset, we also conduct experiments on the YCB-Video dataset [49,26]. Each image in this dataset consists of multiple YCB objects (usually 3 to 6 different objects) in a real world scene. The YCB-Video training dataset consists of 80 videos from different setups in the real world. Since there are many duplicate frames in each video, we select every 50th frame to form the training set, which results in just over 2200 training images (YCBVtrain). YCBVideo testset contains 900 images. YCB-Video train and test sets have all 21 YCB objects. In order to show the benefit of synthetic data, we create two different training scenarios (1) Few-shot setting, where we randomly select 10 and 25 images from (YCBVtrain) to form different few shot training sets. (2) Limited dataset setting, where we randomly select 1%, 5%, 10% images from (YCBVtrain) to form limited training sets.\nUsing a similar setting as in Sec. 4.3, we demonstrate performance of the proposed NS and NSO approaches starting from uniform distributions and compare with four baselines. First baseline-1 involves training RetinaNet using few-shot or limited training images from YCBVtrain data, and baseline-2 involves training RetinaNet using the images that were used to train NeRF. Baseline-3 is Learning-to-sim and baseline-4 is Auto-Sim. Further, we also combine the realworld few-shot or limited training images from YCBVtrain along with NeRF synthesized images during our Neural-Sim optimization steps for training object detection model. This Combined setting reduces the domain gap between synthetic and real data. All the models have been evaluated on YCB-Video testset.\nFor the normal Few-shot setting (rows 2, 3, 4 in Tab. 2(a)), NS starting from the uniform distribution achieves almost 3.45 and 4.11% improvement over the baseline in 10 and 25 shots settings, respectively. Further, when we optimize the parameters using NSO, we observe improvements of 4.45, 4.41% over the baseline and 1.0, 0.3% improvements over the NS case in 10, 25 shot settings respectively. We also observe almost 1.8% improvement in the zero-shot case.\nNeural-Sim 15\nIn addition, for the Combined Few-shot setting (rows 5,6,7,8 in Table. 2(a)), we observe similar large improvements in accuracy. For example, an improvement of 22.51% over the baseline and 2% improvements over the without optimization cases respectively have been observed in the 25 shot settings. Compared with Learning-to-sim and Auto-Sim, NSO shows consistent improvement on both 10 shot and 25 shot.\nWe observe similar large performance improvements in the limited data settings (Table. 2(b)). For example, in the Combined limited data settings (rows 6, 7, 8, 9 in Table. 2(b)), we observe that the the proposed NS achieves an improvement of almost 30.93, 34.72, 35.4% over the baseline in the 1, 5, 10% data regime, respectively. Further, after using NSO we observe an improvement of almost 31.63, 36.02, 36.4% over the baseline. Finally, we also find 0.7, 1.3, 1.0% improvements over NS approach, 0.5, 0.8, 0.7% improvements over Learning-to-sim and 0.3, 1.2, 0.6% improvements over Auto-Sim in 1, 5, 10% settings respectively. Please refer to the appendix for more results and discussion including the results on ObjectNet[3] dataset."
        },
        {
            "heading": "4.5 Interpretability of Neural-Sim",
            "text": "We have observed significant improvement in accuracy even when there exists large distribution gap between training and test sets using the proposed NeuralSim approach. This raises a question: does the Neural-Sim optimization provide interpretable results?\nIn order to demonstrate this behavior, we conduct experiment on YCB-inthe-wild dataset illustrated in Fig 7. As shown, the test set images are sampled from the categorical distribution where bin one is dominant. As described in\n16 Y. Ge et al.\nSec. 4.3, we consider two starting pose bin distributions for our Neural-Sim approach: a uniform distribution and a randomly selected bin as a dominant bin (e.g., most images come from bin four). After optimization, we visualize the learned object pose distribution (Fig 7 (b)). We find that no matter what the starting distributions the Neural-Sim approach used, the learned optimal \u03c8\u2217 is always aligned with the test distribution. This explains the reason why Neural-Sim can improve the downstream object detection performance: it is because Neural-Sim can automatically generate data that will closely matching distribution as the test set. We can find similar interpretable results in camera zoom experiments. More such visualization highlighting interpretable outputs are provided in the supplementary material."
        },
        {
            "heading": "5 Discussion and Future Work",
            "text": "It has been said that \u201cData is food for AI\u201d[35]. While computer vision has made wondrous progress in neural network models in the last decade, the data side has seen much less advancement. There has been an explosion in the number and scale of datasets, but the process has evolved little, still requiring a painstaking amount of labor.\nSynthetic data is one of the most promising directions for transforming the data component of AI. While it has been used to show some impressive results, its wide-spread use has been limited, as creating good synthetic data still requires a large investment and specialized expertise.\nWe believe we have taken a big step towards making synthetic data easier to use for a broader population. By optimizing for how to synthesize data for training a neural network, we have shown big benefits over current synthetic data approaches. We have shown through extensive experiment that the data found by our system is better for training models. We have removed the need for any 3D modeling and for an expert to hand-tune the rendering parameters. This brings the promise of synthetic data closer for those that don\u2019t have the resources to use the current approaches.\nWe have handled camera pose, zoom and illumination; and our approach can be extended to other parameters (such as materials, etc.), by incorporating new advances in neural rendering. For future work, we hope to improve the ease of use of our approach, such as performing our optimization using lower quality, faster rendering using a smaller network for the neural rendering component, and then using the learned parameters to generate high quality data to train the final model. We hope that our work in this space will inspire future research.\nAcknowledgments We want to thank Yen-Chen Lin for his help on using the nerf-pytorch code. This work was supported in part by C-BRIC (one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA), DARPA (HR00112190134) and the Army Research Office (W911NF2020053). The authors affirm that the views expressed herein are solely their own, and do not represent the views of the United States government or any agency thereof.\nNeural-Sim 17"
        },
        {
            "heading": "Appendix",
            "text": "We provide additional information about the implementation details (Sec. A), different datasets used (Sec. B), and experimental results (Sec. C).\nA Implementation Details"
        },
        {
            "heading": "A.1 Memory Efficiency",
            "text": "Tool 2: Twice-forward-once-backward As discussed in the main paper Sec. 3.1 (Tool2), the full gradient update of our bi-level optimization problem involves using the approximation of \u2207NeRF in Eq. 5 and back in Eq. 2. There are three terms in this computation with the following dimensions: (1) \u2202( \u2202l(xj,\u03b8\u0302(\u03c8t)) \u2202\u03b8 )\n\u2202xj \u2208 Rm\u00d7d, (2) \u2202xj\u2202\u03c8 \u2208 R d\u00d7k, and (3)\u2207TV = H(\u03b8\u0302(\u03c8t), \u03c8)\u22121 dLval(\u03b8\u0302(\u03c8t))d\u03b8 \u2208 Rm\u00d71, where m = |\u03b8| is the # of parameters in object detection model, d is the # of pixels in x, and k is # of pose bins.\nSpecifically, if we follow the sequence of (3)-(1)-(2), first, the output of (3) is a 1 \u00d7m vector and can be used as the weight to compute (1) with Pytorch weighted autograd. In this case, we do not need to explicitly store the huge matrix (Rm\u00d7d) of (1) and the corresponding large computation graphs of each element in it. Similarly, the result of previous step ((3)-(1)) is a 1\u00d7 d dimension vector and we can use it as the weight to compute (2) with Pytorch weighted autograd. Finally, we obtain the gradient as a 1\u00d7 k dimension vector.\nTool 3: Patch-wise gradient computation As discussed in the main paper Sec. 3.1 (Tool3), patch-wise gradient computation helps to save the memory cost of computing (1)-(2) sequence of the gradient. Table. 3 shows the details about the memory cost when we use different patch sizes. Specifically, if we keep NeRF chunk as 512 fix, when we increase the patch size by multiplying 2 each time, the memory cost of gradient computation will also approximately doubled. With Patch-wise gradient computation, image size would not be the bottleneck of gradient computation.\nNeural-Sim 21\nA.2 Comparison with the graphics pipeline\nWe provide additional quantitative results to demonstrate that NeRF can replace traditional graphics pipelines like BlenderProc [10] when generating data for downstream computer vision tasks such as object detection. Results are provided in the Fig. 8.\nTo test this, we consider objects from YCB-video datasets. We render images from NeRF and BlenderProc [10] using the same camera pose and zoom parameters. We use these images to conduct multi-class object detection tasks under the same training and test setting. As shown NeRF generated data can achieve the same accuracy as that of BlenderProc on downstream object detection tasks.\nA.3 Influence of optimization parameters\nWe briefly describe the effects of different parameters used in our bilevel optimization updates. In particular, we show the effect of Gubmel softmax temperature parameter used in the main paper. Fig. 9 shows the Gaumble softmax performance under different temperatures. If the temperature parameter is very large, initial categorical distribution takes form of uniform distribution after Gumbel updates and at a lower temperature, the distribution becomes peaky. In our experiments, we have used a parameter value of 0.1.\nWe used stochastic gradient descent with momentum for \u03c8 parameter updates with learning rates of 1e-5 and momentum value of 0.9 value. Further, on Y CB \u2212 video dataset, we use warm start to conduct experiment.\n22 Y. Ge et al."
        },
        {
            "heading": "A.4 Optimization Runtime",
            "text": "We now provide running time details of our end-to-end pipeline. Each iteration involves data generation through NeRF, detection model training, backpropagation through detection model including hessian-vector product evaluation, and backpropagation through data generation process. For Y CB \u2212 synthetics experiments described in Sec.4.2 in the main paper, it takes roughly ten minutes to complete one end-to-end computation. Further, time depends on the image resolution generated by NeRF, and detection model training.\nFinally, it should be noted that this pipeline does not involve any human effort. In comparison, the traditional graphics pipeline will involve human expert involvement for creating good 3D models of objects."
        },
        {
            "heading": "A.5 Rendering from SFM",
            "text": "In order to generate images from a traditional graphics pipeline, one needs to have accurate 3D object models including accurate geometry, texture, materials of objects. Capturing these accurate properties of the objects requires human expert involvement. However, if we use a standard computer vision pipeline like structure-from-motion [43] pipeline to generate 3D models, the quality of images generated by these models are not as high as that of NeRF. Please refer to the Fig. 10. Thus involving human experts to improve 3D model quality for traditional graphics limits their scalability, and is also expensive. In contrast, NeRF only requires images along with camera pose information, providing benefits over traditional graphics pipelines."
        },
        {
            "heading": "A.6 NeRF-in-the-wild",
            "text": "Fig. 11 shows the results of NeRF-in-the-wild (nerf-w) on controllable illumination change which allows smooth interpolations between color and lighting.\nNeural-Sim 23 Real image , obtain pose by (msr-reconstruction.azurewebsites.net)\n24 Y. Ge et al.\nThe experiments have been conducted on the YCB-objects. For each object, we conduct interpolations between the appearance embedding of two training images (left, right), which results in rendering (middle) where illumination are interpolated but geometry is fixed."
        },
        {
            "heading": "B Dataset Information",
            "text": ""
        },
        {
            "heading": "B.1 YCB object visualization",
            "text": "Experiments have been conducted on 21 objects from YCB-video datasets. These objects are: master chef can, cracker box, sugar box, tomato soup can, mustard bottle, tuna fish can, pudding box, gelatin box, potted meat can, banana, pitcher base, bleach cleanser, bowl, mug, power drill, wood block, scissor, large marker, large clamp, extra large clamp, foam brick. These objects are visualized in Fig. 12.\nNeural-Sim 25"
        },
        {
            "heading": "B.2 YCB synthetic dataset details",
            "text": "In order to train NeRF, we first use 3D YCB object models from the BOPbenchmark page [26]. We use BlenderProc [10] to generate 100 images per object. These images are captured from poses that are sampled from a uniform distribution. These images along with their corresponding pose values are used to train NeRF. NeRF training takes almost 20 hours for each object."
        },
        {
            "heading": "B.3 YCB-in-the-wild dataset",
            "text": "As described in the main paper, in order to evaluate the performance of the proposed NS and NSO approaches on a real-world dataset, we have created a real-world YCB-in-the-wild dataset. The dataset has 6 YCB objects in it, which are the same as in the YCB-synthetic dataset: masterchef, cheezit, gelatin, pitcher, mug, driller. All images are captured using a smartphone camera in common indoor environments: living room, kitchen, bedroom and bathroom, under natural pose and illumination. Images from the dataset capturing different environment properties are shown in Fig. 13."
        },
        {
            "heading": "B.4 YCB-video dataset",
            "text": "Images from the YCB-video dataset captured in different scenes are shown in Fig. 14.\n26 Y. Ge et al."
        },
        {
            "heading": "C Additional Experiments",
            "text": "We provide additional experiments below.\nC.1 Interpretable experiments visualization\nIn order to support the claim that the proposed Neural-Sim Optimization (NSO) approach can learn interpretable results, we provide additional visualization on the YCB-in-the-wild dataset illustrated in Fig. 15, Fig. 16 and Fig. 17. In particular, we demonstrate interpretability of our method on two scenarios. First, we conduct experiments where test images are generated from multi-modal distributions - two modal and three modal distributions. Second, we also show results on zoom experiments on cheeze box and driller objects. In both these experiment setup, we consider two starting bin distributions for training: a uniform distribution and a randomly selected bin as a dominant bin. As shown in the figure, we observe that no matter what the starting training distributions our NSO approach use, the final learnt distributions match with the test distributions.\nFinally, we also provide qualitative comparison between images generated from the learned distributions using the proposed NSO approach and the baseline Auto-Sim approach. Fig. 18 provides visualization for Cheeze box and Fig. 19 for pitcher object. We have visualized eighteen images sampled from the learned distributions from the NSO and Auto-Sim approaches. We observe that the proposed NSO approach can generate images that resemble the test images in both these objects. However, Auto-Sim generates images where objects are not always aligned with test images. These visualizations provide ample evidence to support the significant improvement in performance achieved by our NSO approach over the baseline."
        },
        {
            "heading": "C.2 Detection visualization",
            "text": "Object detection results from our pipeline on YCB-in-the-wild and YCB-video datasets are shown in Fig. 20.\nC.3 Full YCB-Video dataset results\nIf we use 100% full YCB-Video training images to train RetinaNet, the mean Average Precision (mAP) reaches 58.5% on all 21 classes. After we use NSO with\nNeural-Sim 27\nOne variation for single object Pitcher multi-model: two models Pitcher multi-model: three models Pitcher zoom\n2 5\nOne variation for single object Pitcher multi-model: two models Pitcher multi-model: three models Pitcher zoom 2 7 8\nOne variation for single object Pitcher multi-model: two models Pitcher multi-model: three models Pitcher zoom 2 7 8\n28 Y. Ge et al.\nNeural-Sim 29\n30 Y. Ge et al.\ncombine optimization which combines both real-world training images and NeRF synthesized images into training and optimization, the accuracy can improve from 58.3% to 58.8%."
        },
        {
            "heading": "C.4 Extension to ObjectNet dataset",
            "text": "We conduct experiments on ObjectNet [3] dataset, which is a large real-world dataset for object recognition. The dataset consists of 313 object classes with 113 overlapping ImageNet classes. In order to synthesize training images, we use CO3D [37] dataset that provides data to train NeRF. There are 17 classes that overlap with ImageNet and ObjectNet classes. After we trained NeRFs for these classes, we find by using NeRF synthesized data to finetune an ImageNet pretrained model provides a 4% improvement on the 17 ObjectNet classes."
        },
        {
            "heading": "D Limitations and Dataset copyright",
            "text": "In this work, we have focused on optimizing camera pose, zoom factor, and illumination parameters. In the real world, there are other scene parameters that affect accuracy, like materials, etc. However, our approach can be extended to include other parameters by incorporating new advances in neural rendering.\nNeural-Sim 31\nDataset copyright. We used publically available data. The YCB-Video dataset is released under the MIT License. Further, we will release our YCB-in-the-wild dataset under the creative commons license.\nSocietal impact Our work focuses on using neural rendering for generating images for solving downstream computer vision tasks. This provides an opportunity to reduce reliance on human or web-captured training data that has potential privacy issues."
        }
    ],
    "title": "Neural-Sim: Learning to Generate Training Data with NeRF",
    "year": 2022
}