{
    "abstractText": "We study the connection between multicalibration and boosting for squared error regression. First we prove a useful characterization of multicalibration in terms of a \u201cswap regret\u201d like condition on squared error. Using this characterization, we give an exceedingly simple algorithm that can be analyzed both as a boosting algorithm for regression and as a multicalibration algorithm for a class H that makes use only of a standard squared error regression oracle for H. We give a weak learning assumption on H that ensures convergence to Bayes optimality without the need to make any realizability assumptions \u2014 giving us an agnostic boosting algorithm for regression. We then show that our weak learning assumption on H is both necessary and sufficient for multicalibration with respect to H to imply Bayes optimality. We also show that if H satisfies our weak learning condition relative to another class C then multicalibration with respect to H implies multicalibration with respect to C. Finally we investigate the empirical performance of our algorithm experimentally using an open source implementation that we make available on GitHub.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ira Globus-Harris"
        },
        {
            "affiliations": [],
            "name": "Declan Harrison"
        },
        {
            "affiliations": [],
            "name": "Michael Kearns"
        },
        {
            "affiliations": [],
            "name": "Aaron Roth"
        },
        {
            "affiliations": [],
            "name": "Jessica Sorrell"
        }
    ],
    "id": "SP:c9f199f7d1c4b26c89a7d6ec0d7a24b5da39a596",
    "references": [
        {
            "authors": [
                "Avrim Blum",
                "Yishay Mansour"
            ],
            "title": "From external to internal regret",
            "venue": "In International Conference on Computational Learning Theory,",
            "year": 2005
        },
        {
            "authors": [
                "A Philip Dawid"
            ],
            "title": "The well-calibrated bayesian",
            "venue": "Journal of the American Statistical Association,",
            "year": 1982
        },
        {
            "authors": [
                "Frances Ding",
                "Moritz Hardt",
                "John Miller",
                "Ludwig Schmidt"
            ],
            "title": "Retiring adult: New datasets for fair machine learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nigel Duffy",
                "David Helmbold"
            ],
            "title": "Boosting methods for regression",
            "venue": "Machine Learning,",
            "year": 2002
        },
        {
            "authors": [
                "Dean P Foster",
                "Rakesh Vohra"
            ],
            "title": "Regret in the on-line decision problem",
            "venue": "Games and Economic Behavior,",
            "year": 1999
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences,",
            "year": 1997
        },
        {
            "authors": [
                "Jerome H Friedman"
            ],
            "title": "Greedy function approximation: a gradient boosting machine",
            "venue": "Annals of statistics,",
            "year": 2001
        },
        {
            "authors": [
                "Ursula H\u00e9bert-Johnson",
                "Michael Kim",
                "Omer Reingold",
                "Guy Rothblum"
            ],
            "title": "Multicalibration: Calibration for the (computationally-identifiable) masses",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Christopher Jung",
                "Changhwa Lee",
                "Mallesh Pai",
                "Aaron Roth",
                "Rakesh Vohra"
            ],
            "title": "Moment multicalibration for uncertainty estimation",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Jung",
                "Georgy Noarov",
                "Ramya Ramalingam",
                "Aaron Roth"
            ],
            "title": "Batch multivalid conformal prediction",
            "venue": "arXiv preprint arXiv:2209.15145,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Kalai"
            ],
            "title": "Learning monotonic linear functions",
            "venue": "In International Conference on Computational Learning Theory,",
            "year": 2004
        },
        {
            "authors": [
                "Adam Tauman Kalai",
                "Adam R Klivans",
                "Yishay Mansour",
                "Rocco A Servedio"
            ],
            "title": "Agnostically learning halfspaces",
            "venue": "SIAM Journal on Computing,",
            "year": 2008
        },
        {
            "authors": [
                "Varun Kanade",
                "Adam Kalai"
            ],
            "title": "Potential-based agnostic boosting",
            "venue": "Advances in neural information processing systems,",
            "year": 2009
        },
        {
            "authors": [
                "Michael P Kim",
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "Multiaccuracy: Black-box post-processing for fairness in classification",
            "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2019
        },
        {
            "authors": [
                "Michael P Kim",
                "Christoph Kern",
                "Shafi Goldwasser",
                "Frauke Kreuter",
                "Omer Reingold"
            ],
            "title": "Universal adaptability: Target-independent inference that competes with propensity scoring",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Balas K Natarajan"
            ],
            "title": "On learning sets and functions",
            "venue": "Machine Learning,",
            "year": 1989
        },
        {
            "authors": [
                "David Pollard"
            ],
            "title": "Convergence of stochastic processes",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Robert E Schapire"
            ],
            "title": "The strength of weak learnability",
            "venue": "Machine learning,",
            "year": 1990
        },
        {
            "authors": [
                "Eliran Shabat",
                "Lee Cohen",
                "Yishay Mansour"
            ],
            "title": "Sample complexity of uniform convergence for multicalibration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shai Shalev-Shwartz",
                "Shai Ben-David"
            ],
            "title": "Understanding machine learning: From theory to algorithms",
            "venue": "Cambridge university press,",
            "year": 2014
        },
        {
            "authors": [
                "V.N. Vapnik",
                "A. YA. Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events",
            "year": 1971
        },
        {
            "authors": [
                "Shabat"
            ],
            "title": "2020]) are either stated and proven for finite classes H, or are proven for algorithms that do not operate as empirical risk minimization algorithms, but instead gain access to a fresh sample of data from the distribution at each iteration, or are proven for hypotheses classes that are fixed independently of the algorithm. We have a different challenge: Like H\u00e9bert-Johnson et al",
            "year": 2021
        },
        {
            "authors": [
                "H. of"
            ],
            "title": "But we wish to study the algorithms as they are used\u2014as empirical risk minimization algorithms\u2014so we do not want our analysis to depend on using a fresh sample of data at each iteration. And unlike the analysis in Jung et al. [2022], for us H is continuously large (since it is closed under affine transformations), so we cannot rely on bounds that depend on log |H|. Instead we give a uniform convergence analysis that depends on the pseudo-dimension of our class of weak learners H",
            "year": 2022
        },
        {
            "authors": [
                "Shalev-Shwartz",
                "Ben-David"
            ],
            "title": "binary classifiers from binary class Hbin and a rule r : t0, 1u \u00d1 rks that determines a multiclass label according to the predictions of the ` binary classifiers. Define the hypothesis class corresponding to this rule as H \u201c trph1p \u0308q",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "We revisit the problem of boosting for regression, and develop a new agnostic regression boosting algorithm via a connection to multicalibration. In doing so, we shed additional light on multicalibration, a recent learning objective that has emerged from the algorithmic fairness literature [H\u00e9bert-Johnson et al., 2018]. In particular, we characterize multicalibration in terms of a \u201cswap-regret\u201d like condition, and use it to answer the question \u201cwhat property must a collection of functions H have so that multicalibration with respect to H implies Bayes optimality?\u201d, giving a complete answer to problem asked by Burhanpurkar et al. [2021]. Using our swap-regret characterization, we derive an especially simple algorithm for learning a multicalibrated predictor for a class of functions H by reduction to a standard squared-error regression algorithm for H. The same algorithm can also be analyzed as a boosting algorithm for squared error regression that makes calls to a weak learner for squared error regression on subsets of the original data distribution without the need to relabel examples (in contrast to Gradient Boosting as well as existing multicalibration algorithms). This lets us specify a weak learning condition that is sufficient for convergence to the Bayes optimal predictor (even if the Bayes optimal predictor does not have zero error), avoiding the kinds of realizability assumptions that are implicit in analyses of boosting algorithms that converge to zero error. We conclude that ensuring multicalibration with respect to H corresponds to boosting for squared error regression in which H forms the set of weak learners. Finally we define a weak learning condition for H relative to a constrained class of functions C (rather than with respect to the Bayes optimal predictor). We show that multicalibration with respect to H implies multicalibration with respect to C if H satisfies the weak learning condition with respect to C, which in turn implies accuracy at least that of the best function in C.\nMulticalibration Consider a distribution D P \u2206Z defined over a domain Z \u201c X \u02c6 R of feature vectors x P X paired with real valued labels y. Informally, a regression function f : X \u00d1 R is calibrated if for every v in the range of f , Epx,yq\u201eDry|fpxq \u201c vs \u201c v. In other words, fpxq must be an unbiased estimator of y,\n1Our code repository can be found at https://github.com/Declancharrison/Level-Set-Boosting\nar X\niv :2\n30 1.\n13 76\n7v 1\n[ cs\n.L G\n] 3\n1 Ja\nn 20\n23\neven conditional on the value of its own prediction. Calibration on its own is a weak condition, because it only asks for f to be unbiased on average over all points x such that fpxq \u201c v. For example, the constant predictor that predicts fpxq \u201c Epx,yq\u201eDrys is calibrated. Thus calibration does not imply accuracy\u2014a calibrated predictor need not make predictions with lower squared error than the best constant predictor. Calibration also does not imply that f is equally representative of the label distribution on different subsets of the feature space X . For example, given a subset of the feature space G \u010e X , even if f is calibrated, it may be that f is not calibrated on the conditional distribution conditional on x P G\u2014it might be e.g. that Ery|fpxq \u201c v, x P Gs \" v, and Ery|fpxq \u201c v, x R Gs ! v. To correct this last deficiency, H\u00e9bert-Johnson et al. [2018] defined multi-calibration, which is a condition parameterized by a subset of groups G \u010e X each defined by an indicator function h : X \u00d1 t0, 1u in some class H. It asks (informally) that for each such h P H, and for each v in the range of f , that Erhpxqpy\u00b4vq|fpxq \u201c vs \u201c 0. Since h is a binary indicator function for some set G, this is equivalent to asking for calibration not just marginally over D, but simultaneously for calibration over D conditional on x P G. Kim et al. [2019] and Gopalan et al. [2022] generalize multicalibration beyond group indicator functions to arbitrary real valued functions h : X \u00d1 R. Intuitively, as H becomes a richer and richer set of functions, multicalibration becomes an increasingly stringent condition. But if H consists of the indicator functions for e.g. even a very large number of randomly selected subsets G \u010e X , then the constant predictor fpxq \u201c Epx,yq\u201eDrys will still be approximately multicalibrated with respect to H. What property of H ensures that multicalibration with respect to H implies that f is a Bayes optimal regression function? This question was recently asked by Burhanpurkar et al. [2021] \u2014 and we provide a necessary and sufficient condition.\nBoosting for Regression Boosting refers broadly to a collection of learning techniques that reduce the problem of \u201cstrong learning\u201d (informally, finding an error optimal model) to a series of \u201cweak learning\u201d tasks (informally, finding a model that has only a small improvement over a trivial model)\u2014See Schapire and Freund [2013] for a textbook treatment. The vast majority of theoretical work on boosting studies the problem of binary classification, in which a weak learner is a learner that obtains classification error bounded below 1{2. Several recent papers Kim et al. [2019], Gopalan et al. [2022] have made connections between algorithms for guaranteeing multicalibration and boosting algorithms for binary classification.\nIn this paper, we show a direct connection between multicalibration and the much less well-studied problem of boosting for squared error regression [Friedman, 2001, Duffy and Helmbold, 2002]. There is not a single established notion for what constitutes a weak learner in the regression setting (Duffy and Helmbold [2002] introduce several different notions), and unlike boosting algorithms for classification problems which often work by calling a weak learner on a reweighting of the data distribution, existing algorithms for boosting for regression typically resort to calling a learning algorithm on relabelled examples. We give a boosting algorithm for regression that only requires calling a squared error regression learning algorithm on subsets of examples from the original distribution (without relabelling), which lets us formulate a weak learning condition that is sufficient to converge to the Bayes optimal predictor, without making the kinds of realizability assumptions implicit in the analysis of boosting algorithms that assume one can drive error to zero."
        },
        {
            "heading": "1.1 Our Results",
            "text": "We focus on classes of real valued functions H that are closed under affine transformations \u2014 i.e. classes such that if fpxq P H, then for any pair of constants a, b P R, pafpxq ` bq P H as well. Many natural classes of models satisfy this condition already (e.g. linear and polynomial functions and regression trees), and any neural network architecture that does not already satisfy this condition can be made to satisfy it by adding two additional parameters (a and b) while maintaining differentiability. Thus we view closure under affine transformations to be a weak assumption that is enforceable if necessary.\nFirst in Section 3 we prove the following characterization for multicalibration over H, for any class H that is closed under affine transformations. Informally, we show that a model f is multicalibrated with respect\nto H if and only if, for every v in the range of f :\nE px,yq\u201eD rpfpxq \u00b4 yq2|fpxq \u201c vs \u010f min hPH E px,yq\u201eD rphpxq \u00b4 yq2|fpxq \u201c vs\n(See Theorem 3.2 for the formal statement). This is a \u201cswap regret\u201d-like condition (as in Foster and Vohra [1999] and Blum and Mansour [2005]), that states that f must have lower squared error than any model h P H, even conditional on its own prediction. Using this characterization, in Section 4 we give an exceedingly simple algorithm for learning a multicalibrated predictor over H given a squared error regression oracle for H. The algorithm simply repeats the following over t rounds until convergence, maintaining a model f : X \u00d1 t0, 1{m, 2{m, . . . , 1u with a discrete range with support over multiples of 1{m for some discretization factor m:\n1. For each level set v P t0, 1{m, 2{m, . . . , 1u, run a regression algorithm to find the htv P H that minimizes squared error on the distribution D|pft\u00b41pxq \u201c vq, the distribution conditional on ft\u00b41pxq \u201c v.\n2. Replace each level set v of ft\u00b41pxq with htvpxq to produce a new model ft, and round its output to the discrete range t0, 1{m, 2{m, . . . , 1u\nEach iteration decreases the squared error of ft, ensuring convergence, and our characterization of multicalibration ensures that we are multicalibrated with respect to H at convergence. Compared to existing multicalibration algorithms (e.g. the split and merge algorithm of Gopalan et al. [2022]), our algorithm is exceptionally simple and makes use of a standard squared-error regression oracle on subsets of the original distribution, rather than using a classification oracle or requiring example relabelling.\nWe can also view the same algorithm as a boosting algorithm for squared error regression. Suppose H (or equivalently our weak learning algorithm) satisfies the following weak learning assumption: informally, that on any restriction of D on which the Bayes optimal predictor is non-constant, there should be some h P H that obtains squared error better than that of the best constant predictor. Then our algorithm converges to the Bayes optimal predictor. In Section A we give uniform convergence bounds which guarantee that the algorithm\u2019s accuracy and multicalibration guarantees generalize out of sample, with sample sizes that are linear in the pseudodimension of H.\nWe then show in Section 5 that in a strong sense this is the \u201cright\u201d weak learning assumption: Multicalibration with respect to H implies Bayes optimality if and only if H satisfies this weak learning condition. This gives a complete answer to the question of when multicalibration implies Bayes optimality.\nIn Section 6, we generalize our weak learning condition to a weak learning condition relative to a constrained class of functions C (rather than relative to the Bayes optimal predictor), and show that if H satisfies the weak learning condition relative to C, then multicalibration with respect to H implies multicalibration with respect to C, and hence error that is competitive with the best model in C.\nWe give a fast, parallelizable implementation of our algorithm and in Section 7 demonstrate its convergence to Bayes optimality on two-dimensional datasets useful for visualization, as well as evaluate the accuracy and calibration guarantees of our algorithm on real Census derived data using the Folktables package Ding et al. [2021]."
        },
        {
            "heading": "1.2 Additional Related Work",
            "text": "Calibration as a statistical objective dates back at least to Dawid [1982]. Foster and Vohra [1999] showed a tight connection between marginal calibration and internal (equivalently swap) regret. We extend this characterization to multicalibration. Multicalibration was introduced by H\u00e9bert-Johnson et al. [2018], and variants of the original definition have been studied by a number of works [Kim et al., 2019, Jung et al., 2021, Gopalan et al., 2022, Kim et al., 2022, Roth, 2022]. We use the `2 variant of multicalibration studied in Roth [2022]\u2014but this definition implies all of the other variants of multicalibration up to a change in parameters. Burhanpurkar et al. [2021] first asked the question \u201cwhen does multicalibration with respect to H imply accuracy\u201d, and gave a sufficient condition: when H contains (refinements of) the levelsets of the\nBayes optimal regression function, together with techniques for attempting to find these. This can be viewed as a \u201cstrong learning\u201d assumption, in contrast to our weak learning assumption on H.\nBoosting for binary classification was introduced by Schapire [1990] and has since become a major topic of both theoretical and empirical study \u2014 see Schapire and Freund [2013] for a textbook overview. Both Kim et al. [2019] and Gopalan et al. [2022] have drawn connections between algorithms for multicalibration and boosting for binary classification. In particular, Gopalan et al. [2022] draw direct connections between their split-and-merge multicalibration algorithm and agnostic boosting algorithms of Kalai [2004], Kanade and Kalai [2009], Kalai et al. [2008]. Boosting for squared error regression is much less well studied. Freund and Schapire [1997] give a variant of Adaboost (Adaboost.R) that reduces regression examples to infinite sets of classification examples, and requires a base regressor that optimizes a non-standard loss function. Friedman [2001] introduced the popular gradient boosting method, which for squared error regression corresponds to iteratively fitting the residuals of the current model and then applying an additive update, but did not give a theoretical analysis. Duffy and Helmbold [2002] give a theoretical analysis of several different boosting algorithms for squared error regression under several different weak learning assumptions. Their algorithms require base regression algorithms that can be called (and guaranteed to succeed) on arbitrarily relabelled examples from the training distribution, and given their weak learning assumption, their analysis shows how to drive the error of the final model arbitrarily close to 0. Weak learning assumptions in this style implicitly make very strong realizabilty assumptions (that the Bayes error is close to 0), but because the weak learner is called on relabelled samples, it is difficult to enunciate a weak learning condition that is consistent with obtaining Bayes optimal error, but not better. The boosting algorithm we introduce only requires calling a standard regression algorithm on subsets of the examples from the training distribution, which makes it easy for us to define a weak learning condition that lets us drive error to the Bayes optimal rate without realizability assumptions \u2014 thus our results can be viewed as giving an agnostic boosting algorithm for regression."
        },
        {
            "heading": "2 Preliminaries",
            "text": "We study prediction tasks over a domain Z \u201c X \u02c6Y. Here X represents the feature domain and Y represents the label domain. We focus on the bounded regression setting where Y \u201c r0, 1s (the scaling to r0, 1s is arbitrary). We write D P \u2206Z to denote a distribution over labelled examples, DX to denote the induced marginal distribution over features, and write D \u201e Dn to denote a dataset consisting of n labelled examples sampled i.i.d. from D. We will be interested in the squared error of a model f with respect to distribution D, Epx,yq\u201eDrpy \u00b4 fpxqq2s. We abuse notation and identify datasets D \u201c tpx1, y1q, . . . , pxn, ynqu with the empirical distribution over the examples they contain, and so we can write the empirical squared error over D: as Epx,yq\u201eDrpy \u00b4 fpxqq2s \u201c 1n \u0159n i\u201c1pyi \u00b4 fpxiqq2. When taking expectations over a distribution that is clear from context, we will frequently suppress notation indicating the relevant distribution for readability. We write Rpfq to denote the range of a function f , and when Rpfq is finite, usem to denote the cardinality of its range: m \u201c |Rpfq|. We are interested in finding models that are multicalibrated with respect to a class of real valued functions H. We use an `2 notion of multicalibration as used in Roth [2022]:\nDefinition 2.1 (Multicalibration). Fix a distribution D P \u2206Z and a model f : X \u00d1 r0, 1s that maps onto a countable subset of its range. Let H be an arbitrary collection of real valued functions h : X \u00d1 R. We say that f is \u03b1-approximately multicalibrated with respect to D and H if for every h P H:\nK2pf, h,Dq \u201c \u00ff\nvPRpfq Pr px,yq\u201eD rfpxq \u201c vs\n\u02c6\nE px,yq\u201eD\nrhpxqpy \u00b4 vq|fpxq \u201c vs \u02d92 \u010f \u03b1.\nWe say that f is \u03b1-approximately calibrated if:\nK2pf,Dq \u201c \u00ff\nvPRpfq Pr px,yq\u201eD rfpxq \u201c vs\n\u02c6\nE px,yq\u201eD\nrpy \u00b4 vq|fpxq \u201c vs \u02d92 \u010f \u03b1.\nIf \u03b1 \u201c 0, then we simply say that a model is multicalibrated or calibrated. We will sometimes refer to K2pf,Dq as the mean squared calibration error of a model f . Remark 2.2. When the functions hpxq have binary range, we can view them as indicator functions for some subset of the data domain S \u010e X , in which case multicalibration corresponds to asking for calibration conditional on membership in these subsets S. Allowing the functions h to have real valued range is only a more general condition. Our notion of approximate multicalibration takes a weighted average over the level sets v of the predictor f , weighted by the probability that fpxq \u201c v. This is necessary for any kind of out of sample generalization statement \u2014 otherwise we could not even necessarily measure calibration error from a finite sample. Other work on multicalibration use related measures of multicalibration that we think of as `1 or `8 variants, that we can write as K1pf, h,Dq \u201c \u0159 vPRpfq Prpx,yq\u201eDrfpxq \u201c vs \u02c7 \u02c7Epx,yq\u201eDrhpxqpy \u00b4 vq|fpxq \u201c vs \u02c7\n\u02c7 and K8pf, h,Dq \u201c maxvPRpfq Prpx,yq\u201eDrfpxq \u201c vs ` Epx,yq\u201eDrhpxqpy \u00b4 vq|fpxq \u201c vs \u02d8\n. These notions are related to each other: K2pf, h,Dq \u010f K1pf, h,Dq \u010f a\nK2pf, h,Dq and K8pf, h,Dq \u010f K1pf, h,Dq \u010f mK8pf, h,Dq [Roth, 2022].\nWe will characterize the relationship between multicalibration and Bayes optimality.\nDefinition 2.3 (Bayes Optimal Predictor). Let f\u02da : X \u00d1 r0, 1s. We say that f\u02da is the Bayes optimal predictor for D if:\nE px,yq\u201eD rpy \u00b4 f\u02dapxqq2s \u010f min f :X\u00d1r0,1s rpy \u00b4 fpxqq2s\nThe Bayes Optimal predictor satisfies: f\u02dapxq \u201c Epx1,yq\u201eD ry|x1 \u201c xs . We say that a function f : X \u00d1 r0, 1s is \u03b3-approximately Bayes optimal if\nE px,yq\u201eD rpy \u00b4 fpxqq2s \u010f E px,yq\u201eD rpy \u00b4 f\u02dapxqq2s ` \u03b3.\nThroughout this paper, we will denote the Bayes optimal predictor as f\u02da."
        },
        {
            "heading": "3 A Characterization of Multicalibration",
            "text": "In this section we give a simple \u201cswap-regret\u201d like characterization of multicalibration for any class of functions H that is closed under affine transformations:\nDefinition 3.1. A class of functions H is closed under affine transformations if for every a, b P R, if hpxq P H then h1pxq :\u201c ahpxq ` b P H.\nAs already discussed, closure under affine transformation is a mild assumption: it is already satisfied by many classes of functions H like linear and polynomial functions and decision trees, and can be enforced for neural network architectures when it is not already satisfied by adding two additional parameters a and b without affecting our ability to optimize over the class.\nThe first direction of our characterization states that if f fails the multicalibration condition for some h P H, then there is some other h1 P H that improves over f in terms of squared error, when restricted to a level set of f . The second direction states the opposite: if f is calibrated (but not necessarily multicalibrated), and if there is some level set of f on which h improves over f in terms of squared error, then in fact f must fail the multicalibration condition for h.\nTheorem 3.2. Suppose H is closed under affine transformation. Fix a model f : X \u00d1 R and a levelset v P Rpfq of f . Then:\n1. If there exists an h P H such that:\nErhpxqpy \u00b4 vq|fpxq \u201c vs \u011b \u03b1,\nfor \u03b1 \u0105 0, then there exists an h1 P H such that:\nErpfpxq \u00b4 yq2 \u00b4 ph1pxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b12\nErhpxq2|fpxq \u201c vs ,\n2. If f is calibrated and there exists an h P H such that\nErpfpxq \u00b4 yq2 \u00b4 phpxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b1,\nthen: Erhpxqpy \u00b4 vq|fpxq \u201c vs \u011b \u03b1\n2 .\nProof. We prove each direction in turn.\nLemma 3.3. Fix a model f : X \u00d1 R. Suppose for some v P Rpfq there is an h P H such that:\nErhpxqpy \u00b4 vq|fpxq \u201c vs \u011b \u03b1\nLet h1 \u201c v ` \u03b7hpxq for \u03b7 \u201c \u03b1Erhpxq2|fpxq\u201cvs . Then:\nErpfpxq \u00b4 yq2 \u00b4 ph1pxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b12\nErhpxq2|fpxq \u201c vs\nProof. We calculate:\nErpfpxq \u00b4 yq2 \u00b4 ph1pxq \u00b4 yq2|fpxq \u201c vs \u201c Erpv \u00b4 yq2 \u00b4 pv ` \u03b7hpxq \u00b4 yq2|fpxq \u201c vs \u201c Erv2 \u00b4 2vy ` y2 \u00b4 pv ` \u03b7hpxqq2 ` 2ypv ` \u03b7hpxqq \u00b4 y2|fpxq \u201c vs \u201c Er2y\u03b7hpxq \u00b4 2v\u03b7hpxq \u00b4 \u03b72hpxq2|fpxq \u201c vs \u201c Er2\u03b7hpxqpy \u00b4 vq \u00b4 \u03b72hpxq2|fpxq \u201c vs \u011b 2\u03b7\u03b1\u00b4 \u03b72 Erhpxq2|fpxq \u201c vs\n\u201c \u03b1 2\nErhpxq2|fpxq \u201c vs\nWhere the last line follows from the definition of \u03b7.\nThe first direction of Theorem 3.2 follows from Lemma 3.3, and the observation that since H is closed under affine transformations, the function h1 defined in the statement of Lemma 3.3 is in H. Now for the second direction.\nLemma 3.4. Fix a model f : X \u00d1 R. Suppose for some v P Rpfq there is an h P H such that:\nErpy\u0304v \u00b4 yq2 \u00b4 phpxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b1,\nwhere y\u0304v \u201c Ery | fpxq \u201c vs. Then it must be that:\nErhpxqpy \u00b4 y\u0304vq|fpxq \u201c vs \u011b \u03b1\n2\nProof. We calculate:\nE px,yq\u201eD rhpxqpy \u00b4 y\u0304vq|fpxq \u201c vs\n\u201c E px,yq\u201eD rhpxqy|fpxq \u201c vs \u00b4 y\u0304v E px,yq\u201eD rhpxq|fpxq \u201c vs\n\u201c 1 2\n\u02c6\n2 E px,yq\u201eD rhpxqy|fpxq \u201c vs \u00b4 2y\u0304v E px,yq\u201eD\nrhpxq|fpxq \u201c vs \u02d9\n\u011b 1 2\n\u02c6\n2 E px,yq\u201eD rhpxqy|fpxq \u201c vs \u00b4 2y\u0304v E px,yq\u201eD rhpxq|fpxq \u201c vs \u00b4 E px,yq\u201eD\nrphpxq \u00b4 y\u0304vq2|fpxq \u201c vs \u02d9\n\u201c 1 2\n\u02c6\nE px,yq\u201eD\nr2hpxqy \u00b4 hpxq2 \u00b4 y\u03042v |fpxq \u201c vs \u02d9\n\u201c 1 2\n\u02c6\nE px,yq\u201eD\nr2hpxqy \u00b4 hpxq2 \u00b4 2y\u0304vy ` y\u03042v |fpxq \u201c vs \u02d9\n\u201c 1 2\n\u02c6\nE px,yq\u201eD\nrpy\u0304v \u00b4 yq2 \u00b4 phpxq \u00b4 yq2|fpxq \u201c vs \u02d9\n\u011b \u03b1 2\nwhere the 3rd to last line follows from adding and subtracting y\u03042v .\nFor any calibrated f it follows that v \u201c Ery | fpxq \u201c vs \u201c y\u0304v, and so for calibrated f we have that if\nErpv \u00b4 yq2 \u00b4 phpxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b1,\nthen: Erhpxqpy \u00b4 vq|fpxq \u201c vs \u011b \u03b1\n2 ."
        },
        {
            "heading": "4 An Algorithm (For Multicalibration And Regression Boosting)",
            "text": "We now give a single algorithm, and then show how to analyze it both as an algorithm for obtaining a multicalibrated predictor f , and as a boosting algorithm for squared error regression.\nLet m P N` be a discretization term, and let r1{ms :\u201c t0, 1m , . . . , m\u00b41 m , 1u denote the set of points in r0, 1s that are multiples of 1{m. We will learn a model f whose range is r1{ms, which we will enforce by rounding its outputs to this range as necessary using the following operation:\nDefinition 4.1 (Roundpf ;mq). Let F be the family of all functions f : X \u00d1 R. Let Round : F \u02c6 N` \u00d1 F be a function such that Roundpf ;mq outputs h\u0303pxq \u201c minvPr1{ms |hpxq \u00b4 v|.\nUnlike other algorithms for multicalibration which make use of agnostic learning oracles for binary classification, our algorithm makes use of an algorithm for solving squared-error regression problems over H:\nDefinition 4.2. AH is a squared error regression oracle for a class of real valued functions H if for every D P \u2206Z, AHpDq outputs a function h P H such that\nh P arg min h1PH E px,yq\u201eD\nrph1pxq \u00b4 yq2s.\nFor example, if H is the set of all linear functions, then AH simply solves a linear regression problem (which has a closed form solution). Algorithm 1 (LSBoost2)repeats the following operation until it no longer decreases overall squared error: it runs squared error regression on each of the level-sets of ft, and then replaces those levelsets with the solutions to the regression problems, and rounds the output to r1{ms.\nWe will now analyze the algorithm first as a multicalibration algorithm, and then as a boosting algorithm. For simplicity, in this section we will analyze the algorithm as if it is given direct access to the distribution D. In practice, the algorithm will be run on the empirical distribution over a dataset D \u201e Dn, and the multicalibration guarantees proven in this section will hold for this empirical distribution. In Section A we prove generalization theorems, which allow us to translate our in-sample error and multicalibration guarantees over D to out-of-sample guarantees over D. Algorithm 1: LSBoost(f, \u03b1,AH,D, B)\nLet m \u201c 2B\u03b1 . Let f0 \u201c Roundpf ;mq, err0 \u201c Epx,yq\u201eDrpf0pxq \u00b4 yq2s, err\u00b41 \u201c 8 and t \u201c 0. while perrt\u00b41 \u00b4 errtq \u011b \u03b12B do for each v P r1{ms do Let Dt`1v \u201c D|pftpxq \u201c vq. Let ht`1v \u201c AHpDt`1v q.\nLet: f\u0303t`1pxq \u201c \u00ff\nvPr1{ms 1rftpxq \u201c vs \u00a8 ht`1v pxq ft`1 \u201c Roundpf\u0303t`1,mq\nLet errt`1 \u201c Epx,yq\u201eDrpft`1pxq \u00b4 yq2s and t \u201c t` 1. Output ft\u00b41."
        },
        {
            "heading": "4.1 Analysis as a Multicalibration Algorithm",
            "text": "Theorem 4.3. Fix any distribution D P \u2206Z, any model f : X \u00d1 r0, 1s, any \u03b1 \u0103 1, any class of real valued functions H that is closed under affine transformations, and a squared error regression oracle AH for H. For any bound B \u0105 0 let:\nHB \u201c th P H : max xPX hpxq2 \u010f Bu\nbe the set of functions in h with squared magnitude bounded by B. Then LSBoostpf, \u03b1,AH,D, Bq (Algorithm 1) halts after at most T \u010f 2B\u03b1 many iterations and outputs a model fT\u00b41 such that fT\u00b41 is \u03b1-approximately multicalibrated with respect to D and HB.\nRemark 4.4. Note the form of this theorem \u2014 we do not promise multicalibration at approximation parameter \u03b1 for all of H, but only for HB \u2014 i.e. those functions in H satisfying a bound on their squared value. This is necessary, since H is closed under affine transformations. To see this, note that if Erhpxqpy\u00b4vqs \u011b \u03b1, then it must be that Erc \u00a8 hpxqpy \u00b4 vqs \u011b c \u00a8 \u03b1. Since h1pxq \u201c chpxq is also in H by assumption, approximate multicalibration bounds must always also be paired with a bound on the norm of the functions for which we promise those bounds.\nProof. Since f0 takes values in r0, 1s and y P r0, 1s, we have err0 \u010f 1, and by definition errT \u011b 0 for all T . By construction, if the algorithm has not halted at round t it must be that errt \u010f errt\u00b41 \u00b4 \u03b12B , and so the algorithm must halt after at most T \u010f 2B\u03b1 many iterations to avoid a contradiction.\nIt remains to show that when the algorithm halts at round T , the model fT\u00b41 that it outputs is \u03b1approximately multi-calibrated with respect to D and HB . We will show that if this is not the case, then errT\u00b41 \u00b4 errT \u0105 \u03b12B , which will be a contradiction to the halting criterion of the algorithm.\n2LSBoost can be taken to stand for either \u201cLevel Set Boost\" or \u201cLeast Squares Boost\u201d, at the reader\u2019s discretion.\nSuppose that fT\u00b41 is not \u03b1-approximately multicalibrated with respect to D and HB . This means there must be some h P HB such that:\n\u00ff\nvPr1{ms Pr px,yq\u201eD rfT\u00b41pxq \u201c vs\n\u02c6\nE px,yq\u201eD\nrhpxqpy \u00b4 vq|fT\u00b41pxq \u201c vs \u02d92 \u0105 \u03b1\nFor each v P r1{ms define\n\u03b1v \u201c Pr px,yq\u201eD\nrfT\u00b41pxq \u201c vs \u02c6\nE px,yq\u201eD\nrhpxqpy \u00b4 vq|fT\u00b41pxq \u201c vs \u02d92\nSo we have \u0159 vPr1{ms \u03b1v \u0105 \u03b1. Applying the 1st part of Theorem 3.2 we learn that for each v, there must be some hv P H such that:\nErpfT\u00b41pxq \u00b4 yq2 \u00b4 phvpxq \u00b4 yq2|fT\u00b41pxq \u201c vs \u0105 1 Erhpxq2|fT\u00b41pxq \u201c vs \u00a8 \u03b1v Prpx,yq\u201eDrfT\u00b41pxq \u201c vs\n\u011b 1 B \u03b1v Prpx,yq\u201eDrfT\u00b41pxq \u201c vs\nwhere the last inequality follows from the fact that h P HB Now we can compute:\nE px,yq\u201eD\nrpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u0303T pxq \u00b4 yq2s\n\u201c \u00ff\nvPr1{ms Pr px,yq\u201eD rfT\u00b41pxq \u201c vs E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u0303T pxq \u00b4 yq2|fT\u00b41pxq \u201c vs\n\u201c \u00ff\nvPr1{ms Pr px,yq\u201eD rfT\u00b41pxq \u201c vs E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 phTv pxq \u00b4 yq2|fT\u00b41pxq \u201c vs\n\u011b \u00ff\nvPr1{ms Pr px,yq\u201eD rfT\u00b41pxq \u201c vs E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 phvpxq \u00b4 yq2|fT\u00b41pxq \u201c vs\n\u011b \u00ff\nvPr1{ms\n\u03b1v B\n\u0105 \u03b1 B\nHere the third line follows from the definition of f\u0303T and the fourth line follows from the fact hv P H and that hTv minimizes squared error on DTv amongst all h P H.\nFinally we calculate:\nerrT\u00b41 \u00b4 errT \u201c E\npx,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2s\n\u201c E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u0303T pxq \u00b4 yq2s ` E px,yq\u201eD rpf\u0303T pxq \u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2s \u0105 \u03b1 B ` E px,yq\u201eD rpf\u0303T pxq \u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2s\n\u0105 \u03b1 B \u00b4 1 m \u011b \u03b1 2B\nwhere the last equality follows from the fact that m \u011b 2B\u03b1 .\nThe 2nd inequality follows from the fact that for every pair px, yq:\npf\u0303T pxq \u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2 \u011b \u00b4 1\nm\nTo see this we consider two cases. Since y P r0, 1s, if f\u0303T pxq \u0105 1 or f\u0303T pxq \u0103 0 then the Round operation decreases squared error and we have pf\u0303T pxq \u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2 \u011b 0. In the remaining case we have fT pxq P r0, 1s and \u2206 \u201c f\u0303T pxq \u00b4 fT pxq is such that |\u2206| \u010f 12m . In this case we can compute:\npf\u0303T pxq \u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2 \u201c pfT pxq `\u2206\u00b4 yq2 \u00b4 pfT pxq \u00b4 yq2\n\u201c 2\u2206pfpxq \u00b4 yq `\u22062\n\u011b \u00b42|\u2206| `\u22062\n\u011b \u00b4 1 m"
        },
        {
            "heading": "4.2 Analysis as a Boosting Algorithm",
            "text": "We now analyze the same algorithm (Algorithm 1) as a boosting algorithm designed to boost a \u201cweak learning\u201d algorithm AH to a strong learning algorithm. Often in the boosting literature, a \u201cstrong learning\u201d algorithm is one that can obtain accuracy arbitrarily close to perfect, which is only possible under strong realizability assumptions. In this paper, by \u201cstrong learning\u201d, we mean that Algorithm 1 should output a model that is close to Bayes optimal, which is a goal we can enunciate for any distribution D without needing to make realizability assumptions. (Observe that if the Bayes optimal predictor has zero error, then our meaning of strong learning corresponds to the standard meaning, so our analysis is only more general).\nWe now turn to our definition of weak learning. Intuitively, a weak learning algorithm should return a hypothesis that makes predictions that are slightly better than trivial whenever doing so is possible. We take \u201ctrivial\u201d predictions to be those of the best constant predictor as measured by squared error \u2014 i.e. the squared error obtained by simply returning the label mean. A \u201cweak learning\u201d algorithm for us can be run on any restriction of the data distribution D to a subset S \u010e X , and must return a hypothesis with squared error slightly better than the squared error of the best constant prediction, whenever the Bayes optimal predictor f\u02da has squared error slightly better than a constant predictor; on restrictions for which the Bayes optimal predictor also does not improve over constant prediction, our weak learning algorithm is not required to do better either.\nTraditionally, \u201cweak learning\u201d assumptions do not distinguish between the optimization ability of the algorithm and the representation ability of the hypothesis class it optimizes over. Since we have defined a squared error regression oracle AH as exactly optimizing the squared error over some class H, we will state our weak learning assumption as an assumption on the representation ability of H\u2014but this is not important for our analysis here. To prove Theorem 4.6 we could equally well assume that AH returns a hypothesis h that improves over a constant predictor whenever one exists, without assuming that h optimizes squared error over all of H.\nDefinition 4.5 (Weak Learning Assumption). Fix a distribution D P \u2206Z and a class of functions H. Let f\u02dapxq \u201c Ey\u201eDpxqrys denote the true conditional label expectation conditional on x. We say that H satisfies the \u03b3-weak learning condition relative to D if for every S \u010e X with Prx\u201eDX rx P Ss \u0105 0, if:\nErpf\u02dapxq \u00b4 yq2|x P Ss \u0103 min cPR Erpc\u00b4 yq2|x P Ss \u00b4 \u03b3\nthen there exists an h P H such that:\nErphpxq \u00b4 yq2|x P Ss \u0103 min cPR Erpc\u00b4 yq2|x P Ss \u00b4 \u03b3\nWhen \u03b3 \u201c 0 we simply say that H satisfies the weak learning condition relative to D.\nObserve why our weak learning assumption is \u201cweak\u201d: the Bayes optimal predictor f\u02da may improve arbitrarily over the best constant predictor on some set S in terms of squared error, but in this case we only require of H that it include a hypothesis that improves by some \u03b3 which might be very small.\nSince the \u03b3-weak learning condition does not make any requirements on H on sets for which f\u02dapxq improves over a constant predictor by less than \u03b3, the best we can hope to prove under this assumption is \u03b3-approximate Bayes optimality, which is what we do next.\nTheorem 4.6. Fix any distribution D P \u2206Z, any model f : X \u00d1 r0, 1s, any \u03b3 \u0105 0, any class of real valued functions H that satisfies the \u03b3-weak learning condition relative to D, and a squared error regression oracle AH for H. Let \u03b1 \u201c \u03b3 and B \u201c 1{\u03b3 (or any pair such that \u03b1{B \u201c \u03b32). Then LSBoostpf, \u03b1,AH,D, Bq halts after at most T \u010f 2\u03b32 many iterations and outputs a model fT\u00b41 such that fT\u00b41 is 2\u03b3-approximately Bayes optimal over D:\nE px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2s \u010f E px,yq\u201eD rpf\u02dapxq \u00b4 yq2s ` 2\u03b3\nwhere f\u02dapxq \u201c Epx,yq\u201eDrys is the function that minimizes squared error over D. Proof. At each round t before the algorithm halts, we have by construction that errt \u010f errt\u00b41 \u00b4 \u03b12B , and since the squared error of f0 is at most 1, and squared error is non-negative, we must have T \u010f 2B\u03b1 \u201c 2 \u03b32 .\nNow suppose the algorithm halts at round T and outputs fT\u00b41. It must be that errT \u0105 errT\u00b41 \u00b4 \u03b3 2\n2 . Suppose also that fT\u00b41 is not 2\u03b3-approximately Bayes optimal:\nE px,yq\u201eD\nrpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u02dapxq \u00b4 yq2s \u0105 2\u03b3\nWe can write this condition as: \u00ff\nvPr1{ms PrrfT\u00b41pxq \u201c vs \u00a8 E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u02dapxq \u00b4 yq2|fT\u00b41pxq \u201c vs \u0105 2\u03b3\nDefine the set:\nS \u201c tv P r1{ms : E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u02dapxq \u00b4 yq2|fT\u00b41pxq \u201c vs \u011b \u03b3u\nto denote the set of values v in the range of fT\u00b41 such that conditional on fT\u00b41pxq \u201c v, fT\u00b41 is at least \u03b3-sub-optimal. Since we have both y P r0, 1s and fT\u00b41pxq P r0, 1s, for every v we must have that ErpfT\u00b41pxq\u00b4 yq2 \u00b4 pf\u02dapxq \u00b4 yq2|fT\u00b41pxq \u201c vs \u010f 1. Therefore we can bound:\n2\u03b3 \u0103 \u00ff\nvPr1{ms PrrfT\u00b41pxq \u201c vs \u00a8 E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 pf\u02dapxq \u00b4 yq2|fT\u00b41pxq \u201c vs\n\u010f Pr px,yq\u201eD rx P Ss ` p1\u00b4 Pr px,yq\u201eD rx P Ssq\u03b3\nSolving we learn that:\nPr px,yq\u201eD rx P Ss \u011b 2\u03b3 \u00b4 \u03b3p1\u00b4 \u03b3q \u011b 2\u03b3 \u00b4 \u03b3 \u201c \u03b3\nNow observe that by the fact that H is assumed to satisfy the \u03b3-weak learning assumption with respect to D, at the final round T of the algorithm, for every v P S we have that hTv satisfies:\nE px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 phTv pxq \u00b4 yq2|fT\u00b41pxq \u201c vs \u011b \u03b3\nLet e\u0303rrT \u201c Epx,yq\u201eDrpf\u0303T pxq \u00b4 yq2s Therefore we have:\nerrT\u00b41 \u00b4 e\u0303rrT \u201c \u00ff\nvPr1{ms Pr px,yq\u201eD rfT\u00b41pxq \u201c vs E px,yq\u201eD rpfT\u00b41pxq \u00b4 yq2 \u00b4 phTv pxq \u00b4 yq2|fT\u00b41pxq \u201c vs\n\u011b Pr px,yq\u201eD rfT\u00b41pxq P Ss\u03b3\n\u011b \u03b32\nWe recall that |e\u0303rrT \u00b4 errT | \u010f 1{m \u201c \u03b3 2\n2 and so we can conclude that\nerrT\u00b41 \u00b4 errT \u011b \u03b32\n2\nwhich contradicts the fact that the algorithm halted at round T , completing the proof."
        },
        {
            "heading": "5 When Multicalibration Implies Accuracy",
            "text": "We analyzed the same algorithm (Algorithm 1) as both an algorithm for obtaining multicalibration with respect to H, and, when H satisfied the weak learning condition given in Definition 4.5, as a boosting algorithm that converges to the Bayes optimal model. In this section we show that this is no coincidence: multicalibration with respect to H implies Bayes optimality if and only if H satisfies the weak learning condition from Definition 4.5,\nFirst we define what we mean when we say that multicalibration with respect to H implies Bayes optimality. Note that the Bayes optimal model f\u02dapxq is multicalibrated with respect to any set of functions, so it is not enough to require that there exist Bayes optimal functions f that are multicalibrated with respect to H. Instead, we have to require that every function that is multicalibrated with respect to H is Bayes optimal:\nDefinition 5.1. Fix a distribution D P \u2206Z. We say that multicalibration with respect to H implies Bayes optimality over D if for every f : X \u00d1 R that is multicalibrated with respect to D and H, we have:\nE px,yq\u201eD rpfpxq \u00b4 yq2s \u201c E px,yq\u201eD rpf\u02dapxq \u00b4 yq2s\nWhere f\u02dapxq \u201c Ey\u201eDpxqrys is the function that has minimum squared error over the set of all functions.\nRecall that when the weak learning parameter \u03b3 in Definition 4.5 is set to 0, we simply call it the \u201cweak learning condition\u201d relative to D. We first state and prove our characterization for the exact case when \u03b3 \u201c 0, because it leads to an exceptionally simple statement. We subsequently extend this characterization to relate approximate Bayes optimality and approximate multicalibration under quantitative weakenings of the weak learning condition.\nTheorem 5.2. Fix a distribution D P \u2206Z. Let H be a class of functions that is closed under affine transformation. Multicalibration with respect to H implies Bayes optimality over D if and only if H satisfies the weak learning condition relative to D.\nProof. To avoid measurability issues we assume that models f have a countable range (which is true in particular whenever X is countable).\nFirst we show that if H satisfies the weak learning condition relative to D, then multicalibration with respect to H implies Bayes optimality over D. Suppose not. Then there exists a function f that is multicalibrated with respect to D and H, but is such that:\nE px,yq\u201eD rpfpxq \u00b4 yq2s \u0105 E px,yq\u201eD rpf\u02dapxq \u00b4 yq2s\nBy linearity of expectation we have: \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8 E px,yq\u201eD rpfpxq \u00b4 yq2 \u00b4 pf\u02dapxq \u00b4 yq2|fpxq \u201c vs \u0105 0\nIn particular there must be some v P Rpfq with Prx\u201eDX rfpxq \u201c vs \u0105 0 such that:\nE px,yq\u201eD rpfpxq \u00b4 yq2|fpxq \u201c vs \u0105 E px,yq\u201eD rpf\u02dapxq \u00b4 yq2|fpxq \u201c vs\nLet S \u201c tx : fpxq \u201c vu. Observe that if H is closed under affine transformation, the constant function hpxq \u201c 1 is in H, and hence multicalibration with respect to H implies calibration. Since f is calibrated, we know that:\nE px,yq\u201eD rpv \u00b4 yq2|x P Ss \u201c min cPR E px,yq\u201eD rpc\u00b4 yq2|x P Ss\nThus by the weak learning assumption there must exist some h P H such that:\nErpv \u00b4 yq2 \u00b4 phpxq \u00b4 yq2|x P Ss \u201c Erpfpxq \u00b4 yq2 \u00b4 phpxq \u00b4 yq2|fpxq \u201c vs \u0105 0\nBy Theorem 3.2, there must therefore exist some h1 P H such that:\nE px,yq\u201eD\nrh1pxqpy \u00b4 vq|fpxq \u201c vs \u0105 0\nimplying that f is not multicalibrated with respect to D and H, a contradiction. In the reverse direction, we show that for any H that does not satisfy the weak learning condition with respect to D, then multicalibration with respect to H and D does not imply Bayes optimality over D. In particular, we exhibit a function f such that f is multicalibrated with respect to H and D, but such that:\nE px,yq\u201eD rpfpxq \u00b4 yq2s \u0105 E px,yq\u201eD rpf\u02dapxq \u00b4 yq2s\nSince H does not satisfy the weak learning assumption over D, there must exist some set S \u010e X with Prrx P Ss \u0105 0 such that\nE px,yq\u201eD rpf\u02dapxq \u00b4 yq2|x P Ss \u0103 min cPR E px,yq\u201eD rpc\u00b4 yq2|x P Ss\nbut for every h P H: E\npx,yq\u201eD rphpxq \u00b4 yq2|x P Ss \u011b min cPR E px,yq\u201eD rpc\u00b4 yq2|x P Ss\n. Let cpSq \u201c Epx,yq\u201eDry|x P Ss. We define fpxq as follows:\nfpxq \u201c # f\u02dapxq x R S cpSq x P S\nWe can calculate that:\nE px,yq\u201eD\nrpfpxq \u00b4 yq2s\n\u201c Pr px,yq\u201eD rx P Ss E px,yq\u201eD rpcpSq \u00b4 yq2|x P Ss ` Pr px,yq\u201eD rx R Ss E px,yq\u201eD rpf\u02dapxq \u00b4 yq2|x R Ss\n\u0105 Pr px,yq\u201eD rx P Ss E px,yq\u201eD rpf\u02dapxq \u00b4 yq2|x P Ss ` Pr px,yq\u201eD rx R Ss E px,yq\u201eD rpf\u02dapxq \u00b4 yq2|x R Ss\n\u201c E px,yq\u201eD\nrpf\u02dapxq \u00b4 yq2s\nIn other words, f is not Bayes optimal. So if we can demonstrate that f is multicalibrated with respect to H and D we are done. Suppose otherwise. Then there exists some h P H and some v P Rpfq such that\nE px,yq\u201eD rhpxqpy \u00b4 vq|fpxq \u201c vs \u0105 0\nBy Theorem 3.2, there exists some h1 P H such that:\nE px,yq\u201eD rph1pxq \u00b4 yq2|fpxq \u201c vs \u0103 E px,yq\u201eD rpfpxq \u00b4 yq2|fpxq \u201c vs\nWe first observe that it must be that v \u201c cpSq. If this were not the case, by definition of f we would have that:\nE px,yq\u201eD rph1pxq \u00b4 yq2|fpxq \u201c vs \u0103 E px,yq\u201eD rpf\u02dapxq \u00b4 yq2|fpxq \u201c vs\nwhich would contradict the Bayes optimality of f\u02da. Having established that v \u201c cpSq we can calculate:\nE px,yq\u201eD\nrph1pxq \u00b4 yq2|fpxq \u201c cpSqs\n\u201c Pr px,yq\u201eD rx P Ss E px,yq\u201eD\nrph1pxq \u00b4 yq2|x P Ss `\nPr px,yq\u201eD rx R S, fpxq \u201c cpSqs E px,yq\u201eD\nrph1pxq \u00b4 yq2|x R S, fpxq \u201c cpSqs\n\u011b Pr px,yq\u201eD rx P Ss E px,yq\u201eD\nrph1pxq \u00b4 yq2|x P Ss `\nPr px,yq\u201eD rx R S, fpxq \u201c cpSqs E px,yq\u201eD rpfpxq \u00b4 yq2|x R S, fpxq \u201c cpSqs\nwhere in the last inequality we have used the fact that by definition, fpxq \u201c f\u02dapxq for all x R S, and so is pointwise Bayes optimal for all x R S.\nHence the only way we can have Epx,yq\u201eDrph1pxq \u00b4 yq2|fpxq \u201c cpSqs \u0103 Epx,yq\u201eDrpfpxq \u00b4 yq2|fpxq \u201c cpSqs is if:\nE px,yq\u201eD rph1pxq \u00b4 yq2|x P Ss \u0103 E px,yq\u201eD rpcpSq \u00b4 yq2|x P Ss\nBut this contradicts our assumption that H violates the weak learning condition on S, which completes the proof.\nWe now turn our attention to deriving a relationship between approximate multicalibration and approximate Bayes optimality. To do so, we\u2019ll introduce an even weaker weak learning condition that has one additional parameter \u03c1, lower bounding the mass of sets S that we can condition on while still requiring the weak learning condition to hold. We remark that Algorithm 1 can be analyzed as a boosting algorithm under this weaker weak learning assumption as well, with only minor modifications in the analysis.\nDefinition 5.3 ( p\u03b3, \u03c1q-weak learning condition). Fix a distribution D P \u2206Z and let H be a class of arbitrary real-valued functions. We say that H satisfies the p\u03b3, \u03c1q-weak learning condition for D if the following holds. For every set S \u010e X such that Prx\u201eDX rx P Ss \u0105 \u03c1, if\nE px,yq\u201eD rpf\u02da \u00b4 yq2 | x P Ss \u0103 E px,yq\u201eD rpy\u0304S \u00b4 yq2 | x P Ss \u00b4 \u03b3,\nwhere y\u0304S \u201c Epx,yq\u201eDry | x P Ss, then there exists h P H such that\nE px,yq\u201eD rphpxq \u00b4 yq2 | x P Ss \u0103 E px,yq\u201eD rpy\u0304S \u00b4 yq2 | x P Ss \u00b4 \u03b3.\nWe may now prove our theorem showing that approximate multicalibration with respect to a class H implies approximate Bayes optimality if and only if H satisfies the p\u03b3, \u03c1q-weak learning condition. We recall Remark 4.4, which notes that we must restrict approximate multicalibration to a bounded subset of H, as we will assume that H is closed under affine transformation.\nTheorem 5.4. Fix any distribution D P \u2206Z, any model f : X \u00d1 r0, 1s, and any class of real valued functions H that is closed under affine transformation. Let:\nH1 \u201c th P H : max xPX hpxq2 \u010f 1u\nbe the set of functions in H upper-bounded by 1 on X . Let m \u201c |Rpfq|, \u03b3 \u0105 0, and \u03b1 \u010f \u03b3 3\n16m . Then if H satisfies the p\u03b3, \u03b3{mq-weak learning condition and f is \u03b1-approximately multicalibrated with respect to H1 on D, then f has squared error\nE px,yq\u201eD rpfpxq \u00b4 yq2s \u010f E px,yq\u201eD rpf\u02da \u00b4 yq2s ` 3\u03b3.\nConversely, if H does not satisfy the p\u03b3, \u03b3{mq-weak learning condition, there exists a model f : X \u00d1 r0, 1s that is \u03b1-approximately multicalibrated with respect to H1 on D, for \u03b1 \u201c \u03b3, and is perfectly calibrated on D, but f has squared error\nE px,yq\u201eD rpfpxq \u00b4 yq2s \u011b E px,yq\u201eD rpf\u02da \u00b4 yq2s ` \u03b32{m.\nProof. We begin by arguing that \u03b1-approximate multicalibration with respect to H1 on D implies approximate Bayes optimality when H satisfies the p\u03b3, \u03b3{mq-weak learning condition. Suppose not, and there exists a function f that is \u03b1-multicalibrated with respect to H1, but\nE px,yq\u201eD rpf\u02da \u00b4 yq2s \u0103 E px,yq\u201eD rpfpxq \u00b4 yq2s \u00b4 3\u03b3.\nThen there must exist some v P Rpfq such that Prpx,yq\u201eDrfpxq \u201c vs \u0105 \u03b3{m and\nE px,yq\u201eD rpf\u02da \u00b4 yq2 | fpxq \u201c vs \u0103 E px,yq\u201eD rpfpxq \u00b4 yq2 | fpxq \u201c vs \u00b4 2\u03b3.\nWe observe that since H is closed under affine transformation, the constant function hpxq \u201c 1 is in H, and so \u03b1-approximate multicalibration with respect to H1 implies \u03b1-approximate calibration as well. Thus by definition,\nPrrfpxq \u201c vs \u00a8 \u02c6\nE px,yq\u201eD\nrv \u00b4 y | fpxq \u201c vs \u02d92 \u010f \u03b1.\nLetting y\u0304v \u201c Ery | fpxq \u201c vs, our lower-bound that Prrfpxq \u201c vs \u0105 \u03b3{m gives us that pv \u00b4 y\u0304vq2 \u0103 \u03b1m{\u03b3 \u010f `\n\u03b3 4 \u02d82. We now use this upper-bound on calibration error in conjuction with our lower-bound on distance from Bayes optimality to show that the squared error of the constant predictor y\u0304v must also be far from Bayes optimal.\nE px,yq\u201eD rpf\u02dapxq \u00b4 yq2 | fpxq \u201c vs \u0103 E px,yq\u201eD rpfpxq \u00b4 yq2 | fpxq \u201c vs \u00b4 2\u03b3\n\u201c E px,yq\u201eD rpv \u00b4 y\u0304v ` y\u0304v \u00b4 yq2 | fpxq \u201c vs \u00b4 2\u03b3\n\u201c E px,yq\u201eD rpy\u0304v \u00b4 yq2 | fpxq \u201c vs ` pv \u00b4 y\u0304vq2 \u00b4 2\u03b3\n\u0103 E px,yq\u201eD rpy\u0304v \u00b4 yq2 | fpxq \u201c vs \u00b4 \u03b3.\nThe p\u03b3, \u03b3{mq-weak learning condition then guarantees that there exists some h P H such that\nE px,yq\u201eD rph\u00b4 yq2 | fpxq \u201c vs \u0103 E px,yq\u201eD rpy\u0304v \u00b4 yq2 | fpxq \u201c vs \u00b4 \u03b3.\nBy Lemma 3.4, the fact that h improves on the squared loss of y\u0304v by an additive factor \u03b3, on the set of x such that fpxq \u201c v, implies that Erhpxqpy \u00b4 y\u0304vq | fpxq \u201c vs \u0105 \u03b3{2. Because f is \u03b1-approximately calibrated on D, we can use the existence of such an h to witness a failure of multicalibration:\nErhpy \u00b4 vq | fpxq \u201c vs \u201c Erhpxqpy \u00b4 y\u0304v ` y\u0304v \u00b4 vq | fpxq \u201c vs \u201c Erhpxqpy \u00b4 y\u0304vq | fpxq \u201c vs ` Erhpxqpy\u0304v \u00b4 vq | fpxq \u201c vs \u0105 \u03b3{2\u00b4 |y\u0304v \u00b4 v| \u0105 \u03b3{4.\nThen\nPrrfpxq \u201c vs \u00a8 \u02c6\nE px,yq\u201eD\nrhpxqpy \u00b4 vq | fpxq \u201c vs \u02d92 \u0105 \u03b3 3\n16m ,\ncontradicting our assumption that f is \u03b1-approximately multicalibrated with respect to H1 for \u03b1 \u0103 \u03b3 3\n16m . Therefore approximate multicalibration with respect to H1 must imply that f is approximately Bayes optimal.\nIt remains to show the other direction, that \u03b1-approximate multicalibration with respect to a class H1 implies approximate Bayes optimality only if H satisfies the p\u03b3, \u03b3{mq-weak learning condition. If this claim were not true for the stated parameters, then there must exist a class H such that every predictor f that:\n\u2022 is \u03b1-approximately multicalibrated with respect to H1 \u2022 is perfectly calibrated on D\n\u2022 has range with cardinality |Rpfq| \u201c m also has squared error within \u03b32{m of Bayes optimal, but H does not satisfy the weak learning condition. We will show that no such class exists by defining, for any class H not satisfying the weak learning condition, a predictor f that is \u03b1-approximately multicalibrated with respect to that class, but has squared error that is not within \u03b32{m of Bayes optimal.\nRecall that if a class H does not satisfy the p\u03b3, \u03b3{mq-weak learning condition, then there must be some set SH such that Prrx P SHs \u0105 \u03b3{m, there does not exist an h P H such that\nE px,yq\u201eD rph\u00b4 yq2 | x P SHs \u0103 E px,yq\u201eD rpy\u0304SH \u00b4 yq2 | x P SHs \u00b4 \u03b3,\nbut for the Bayes optimal predictor, it holds that its squared loss satisfies\nE px,yq\u201eD rpf\u02da \u00b4 yq2 | x P SHs \u0103 E px,yq\u201eD rpy\u0304SH \u00b4 yq2 | x P SHs \u00b4 \u03b3,\nwhere y\u0304SH \u201c Ery | x P SHs. For some hypothesis class H not satisfying the weak learning condition, and associated set SH, let fH be defined as follows:\nfHpxq \u201c # f\u02dapxq, x R SH y\u0304SH , x P SH. .\nNote that, because fH is constant on SH, there must be some v P Rpfq such that the level set Sv \u201c tx P X : fpxq \u201c vu contains SH. To see that fH is \u03b1-approximately multicalibrated with respect to H1, we first consider the contribution to multicalibration error from the level sets not containing SH. For all h P H and v P Rpfq such that v \u2030 y\u0304SH ,\nE px,yq\u201eD rhpxqpy \u00b4 fHpxqq | fHpxq \u201c vs \u201c E px,yq\u201eD rhpxqpy \u00b4 f\u02dapxqq | fHpxq \u201c vs\n\u201c E x\u201eDx E y\u201eDypxq rhpxqy | fHpxq \u201c vs \u00b4 E x\u201eDx rhpxqf\u02dapxq | fHpxq \u201c vs\n\u201c E x\u201eDx E y\u201eDypxq rhpxqy | fHpxq \u201c vs \u00b4 E x\u201eDx E y\u201eDypxq rhpxqy | fHpxq \u201c vs\n\u201c 0.\nFor the level set Sv for which SH \u010e Sv, we know from the argument above that the elements x P SvzSH contribute nothing to the multicalibration error, as fpxq \u201c f\u02dapxq on these elements. So,\nE px,yq\u201eD rhpxqpy \u00b4 fHpxqq | fpxq \u201c vs \u201c Pr x\u201eDX rx P SHs \u00a8 E px,yq\u201eD rhpxqpy \u00b4 y\u0304SHq | x P SHs\n` Pr x\u201eDX rx R SHs \u00a8 E px,yq\u201eD rhpxqpy \u00b4 f\u02dapxqq | x P SvzSHs\n\u201c Pr x\u201eDX rx P SHs \u00a8 E px,yq\u201eD rhpxqpy \u00b4 y\u0304SHq | x P SHs\nTherefore if fH is not \u03b1-approximately multicalibrated with respect to H1 on D, it must be the case that there exists some h P H1 such that Erhpxqpy\u00b4 y\u0304SHq | x P SHs \u0105 ? \u03b1. Then by Theorem 3.2, there must exist a h1 P H such that E\npx,yq\u201eD rpy\u0304SH \u00b4 yq2 \u00b4 ph1pxq \u00b4 yq2 | x P SHs \u0105 \u03b1 \u201c \u03b3.\nBut SH was defined to be a subset of X for which no such h1 exists and for which Prrx P SHs \u0105 \u03b3{m. This would contradict our assumption that H does not satisfy the p\u03b3, \u03b3{mq-weak learning condition on D, and therefore fH is \u03b1-approximately multicalibrated with respect to H1 on D.\nIt remains to prove that fH is far from Bayes optimal.\nE px,yq\u201eD rpfHpxq \u00b4 yq2s \u201c Pr x\u201eDX rx P SHs E px,yq\u201eD rpy\u0304SH \u00b4 yq2 | x P SHs ` Prrx R SHs E px,yq\u201eD rpf\u02dapxq \u00b4 yq2 | x R SHs\n\u011b Pr x\u201eDX\nrx P SHs \u02c6\nE px,yqD\nrpf\u02da \u00b4 yq2 | x P SHs ` \u03b3 \u02d9\n` Prrx R SHs E px,yq\u201eD rpf\u02dapxq \u00b4 yq2 | x R SHs\n\u201c E px,yq\u201eD rpf\u02da \u00b4 yq2s ` \u03b3 Pr x\u201eDX rx P SHs\n\u011b E px,yq\u201eD\nrpf\u02da \u00b4 yq2s ` \u03b32{m."
        },
        {
            "heading": "6 Weak Learners With Respect to Constrained Classes",
            "text": "Thus far we have studied function classes H that satisfy a weak learning condition with respect to the Bayes optimal predictor f\u02da. But we can also study function classes H that satisfy a weak learning condition defined with respect to another constrained class of real valued functions.\nDefinition 6.1 (Weak Learning Assumption Relative to C). Fix a distribution D P \u2206Z and two classes of functions H and C. We say that H satisfies the \u03b3-weak learner condition relative to C and D if for every S \u010e X with Prx\u201eDX rx P Ss \u0105 0, if:\nmin cPC E px,yq\u201eD rpcpxq \u00b4 yq2 | x P Ss \u0103 E px,yq\u201eD rpy\u0304S \u00b4 yq2 | x P Ss \u00b4 \u03b3,\nwhere y\u0304S \u201c Epx,yq\u201eDry | x P Ss, then there exists h P H such that\nE px,yq\u201eD rphpxq \u00b4 yq2 | x P Ss \u0103 E px,yq\u201eD rpy\u0304S \u00b4 yq2 | x P Ss \u00b4 \u03b3.\nWhen \u03b3 \u201c 0 we simply say that H satisfies the weak learning condition relative to C and D.\nWe will show that if a predictor f is multicalibrated with respect to H, and H satisfies the weak learning assumption with respect to C, then in fact:\n1. f is multicalibrated with respect to C, and\n2. f has squared error at most that of the minimum error predictor in C.\nIn fact, Gopalan et al. [2022] show that if f is multicalibrated with respect to C, then it is an omnipredictor for C, which implies that f has loss no more than the best function cpxq P C, where loss can be measured with respect to any Lipschitz convex loss function (not just squared error). Thus our results imply that to obtain an omnipredictor for C, it is sufficient to be multicalibrated with respect to a class H that satisfies our weak learning assumption with respect to C.\nTheorem 6.2. Fix a distribution D P \u2206Z and two classes of functions H and C that are closed under affine transformations. Then if f : X \u00d1 r0, 1s is multicalibrated with respect to D and H, and if H satisfies the weak learning condition relative to C and D, then in fact f is multicalibrated with respect to D and C as well.\nProof. We assume for simplicity that f has a countable range (which is without loss of generality e.g. whenever X is countable). Suppose for contradiction that f is not multicalibrated with respect to C and D. In this case there must be some c P C such that:\n\u00ff\nvPRpfq Prrfpxq \u201c vs\n\u02c6\nE px,yq\u201eD\nrcpxqpy \u00b4 vq|fpxq \u201c vs \u02d92 \u0105 0\nSince C is closed under affine transformations (and so both c and \u00b4c are in C), there must be some c1 P C and some v P Rpfq with Prrfpxq \u201c vs \u0105 0 such that:\nE px,yq\u201eD\nrc1pxqpy \u00b4 vq|fpxq \u201c vs \u0105 0\nTherefore, by the first part of Theorem 3.2, there must be some c2 P C such that:\nE px,yq\u201eD rpc2pxq \u00b4 yq2|fpxq \u201c vs \u0103 E px,yq\u201eD rpv \u00b4 yq2|fpxq \u201c vs\nSince H is closed under affine transformations, the function hpxq \u201c 1 is in H and so multicalibration with respect to H implies calibration. Thus v \u201c y\u0304Sv for Sv \u201c tx : fpxq \u201c vu. Therefore, the fact that H satisfies the weak learning condition relative to C and D implies that there must be some h P H such that:\nE px,yq\u201eD rphpxq \u00b4 yq2|fpxq \u201c vs \u0103 E px,yq\u201eD rpv \u00b4 yq2|fpxq \u201c vs\nFinally, the second part of Theorem 3.2 implies that:\nE px,yq\u201eD rhpxqpy \u00b4 vq|fpxq \u201c vs \u0105 0\nwhich is a violation of our assumption that f is multicalibrated with respect to H and D, a contradiction.\nTheorem 6.3. Fix a distribution D P \u2206Z and two classes of functions H and C. Then if f : X \u00d1 r0, 1s is calibrated and multicalibrated with respect to D and H, and if H satisfies the weak learning condition relative to C and D, then:\nE px,yq\u201eD rpfpxq \u00b4 yq2s \u010f min cPC E px,yq\u201eD rpcpxq \u00b4 yq2s\nProof. We assume for simplicity that f has a countable range (which is without loss of generality e.g. whenever X is countable). Suppose for contradiction that there is some c P C such that:\nE px,yq\u201eD rpcpxq \u00b4 yq2s \u0103 E px,yq\u201eD rpfpxq \u00b4 yq2s\nThen there must be some v P Rpfq with Prrfpxq \u201c vs \u0105 0 and:\nE px,yq\u201eD rpcpxq \u00b4 yq2|fpxq \u201c vs \u0103 E px,yq\u201eD rpv \u00b4 yq2|fpxq \u201c vs\nSince f is calibrated, v \u201c y\u0304Sv for Sv \u201c tx : fpxq \u201c vu. Therefore, the fact that H satisfies the weak learning condition relative to C and D implies that there must be some h P H such that:\nE px,yq\u201eD rphpxq \u00b4 yq2|fpxq \u201c vs \u0103 E px,yq\u201eD rpv \u00b4 yq2|fpxq \u201c vs\nFinally, the second part of Theorem 3.2 implies that:\nE px,yq\u201eD rhpxqpy \u00b4 vq|fpxq \u201c vs \u0105 0\nwhich is a violation of our assumption that f is multicalibrated with respect to H and D, a contradiction.\nWe now turn to approximate versions of these statements. To do so, we need a refined version of one direction of Theorem 3.2 that shows us that if f witnesses a failure of multicalibration with respect to some h P H, then there is another function h1 P H that can be used to improve on f \u2019s squared error, while controlling the norm of h1.\nLemma 6.4. Suppose H is closed under affine transformation. Fix a model f : X \u00d1 r0, 1s, a levelset v P Rpfq, and a bound B \u0105 0. Then if there exists an h P H such that maxxPX hpxq2 \u010f B and\nErhpxqpy \u00b4 vq|fpxq \u201c vs \u011b \u03b1,\nfor \u03b1 \u011b 0, then there exists an h1 P H such that maxxPX h1pxq2 \u010f p1` ? B \u03b1 q 2 and:\nErpfpxq \u00b4 yq2 \u00b4 ph1pxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b12\nB .\nProof. Let h1pxq \u201c v ` \u03b7hpxq where \u03b7 \u201c \u03b1Erhpxq2|fpxq\u201cvs , as in Theorem 3.2. Because hpxq 2 is uniformly bounded by B on X , it follows that Erhpxq2s \u010f B, and we have already shown in the proof of Theorem 3.2 that this implies\nErpfpxq \u00b4 yq2 \u00b4 ph1pxq \u00b4 yq2|fpxq \u201c vs \u011b \u03b12\nB .\nIt only remains to bound maxxPX h1pxq2. We begin by lower-bounding Erhpxq2 | fpxq \u201c vs in terms of \u03b1.\nErhpxq2 | fpxq \u201c vs \u011b Erhpxq | fpxq \u201c vs2\n\u011b Erhpxqpy \u00b4 vq | fpxq \u201c vs2\n\u011b \u03b12.\nIt follows that \u03b7 \u010f 1{\u03b1, and so\nmax xPX h1pxq2 \u201c max xPX pv ` \u03b7hpxqq2\n\u010f p1` \u03b7 ? Bq2\n\u010f \u02dc 1` ? B\n\u03b1\n\u00b82\n.\nWe will also need a parameterized version of our weak learning condition. Recalling Remark 4.4, for approximate multicalibration to be meaningful with respect to a class that is closed under affine transformation, we must specify a bounded subset of that class with respect to which a predictor is approximately multicalibrated. Then to show that approximate multicalibration with respect to one potentially unbounded class implies approximate multicalibration with respect to another, we will need to specify the subsets of each class with respect to which a predictor is claimed to be approximately multicalibrated. This motivates a parameterization of our previous weak learning condition relative to a class C. We will need to assume that whenever there is a B-bounded function in C that improves over the best constant predictor on a restriction of D, there also exists a B-bounded function in H that improves on the restriction as well.\nDefinition 6.5 (B-Bounded Weak Learning Assumption Relative to C). Fix a distribution D P \u2206Z and two classes of functions H and C. Fix a bound B \u0105 0 and let HB and CB denote the sets\nHB \u201c th P H : max xPX hpxq2 \u010f Bu\nand CB \u201c tc P C : max\nxPX cpxq2 \u010f Bu\nrespectively. We say that H satisfies the B-bounded \u03b3-bounded weak learning condition relative to C and D if for every S \u010e X with Prx\u201eDX rx P Ss \u0105 0, if:\nmin cPCB E px,yq\u201eD rpcpxq \u00b4 yq2 | x P Ss \u0103 E px,yq\u201eD rpy\u0304S \u00b4 yq2 | x P Ss \u00b4 \u03b3,\nwhere y\u0304S \u201c Ery | x P Ss, then there exists h P HB such that\nE px,yq\u201eD rphpxq \u00b4 yq2 | x P Ss \u0103 E px,yq\u201eD rpy\u0304S \u00b4 yq2 | x P Ss \u00b4 \u03b3.\nTheorem 6.6. Fix a distribution D P \u2206Z and two classes of functions H and C that are closed under affine transformations. Fix \u03b1C , B \u0105 0. Let B1 \u201c p1` b\n2B \u03b1C q2 and \u03b3 \u201c \u03b1C4B . Fix a function f : X \u00d1 r0, 1s that maps\ninto a countable subset of its range, and let m \u201c |Rpfq|, \u03b1H \u0103 \u03b1 3 C 29mB12 , and \u03b1 \u0103 \u03b1C\u03b3 2 32mB12 . Then if\n\u2022 H satisfies the B1-bounded \u03b3-weak learning condition relative to C and D\n\u2022 f is \u03b1H-approximately multicalibrated with respect to D and HB1\n\u2022 f is \u03b1-approximately calibrated on D,\nthen f is \u03b1C-approximately multicalibrated with respect to D and CB.\nProof. Suppose not and there exists some c P CB such that \u00ff\nvPRpfq Pr x\u201eDx rfpxq \u201c vs \u00a8\n\u02c6\nE px,yq\u201eD\nrcpxqpy \u00b4 vq | fpxq \u201c vs \u02d92 \u0105 \u03b1C .\nThen there must exist some v P Rpfq such that Prrfpxq \u201c vs \u0105 \u03b1C2m and\nE px,yq\u201eD\nrcpxqpy \u00b4 vq | fpxq \u201c vs2 \u0105 \u03b1C{2.\nBecause C is closed under affine transformations, CB is closed under negation, so there must also exist some c1 P CB such that\nE px,yq\u201eD\nrc1pxqpy \u00b4 vq | fpxq \u201c vs \u0105 a \u03b1C{2.\nThen Lemma 3.3 shows that there is a c2 P Cp1` b 2B \u03b1C q2 \u201c CB1 such that\nE px,yq\u201eD rpy \u00b4 fpxqq2 \u00b4 py \u00b4 c2pxqq2 | fpxq \u201c vs \u011b \u03b1C 2B \u201c 2\u03b3.\nBecause f is \u03b1-calibrated on D, by definition we have\nPr x\u201eDx\nrfpxq \u201c vs \u00a8 \u02c6\nE px,yq\u201eD\nrv \u00b4 y | fpxq \u201c vs \u02d92 \u0103 \u03b1.\nLetting y\u0304v \u201c Ery | fpxq \u201c vs, our lower-bound that Prrfpxq \u201c vs \u0105 \u03b1C2m gives us that pv \u00b4 y\u0304vq 2 \u0103 2\u03b1m\u03b1C \u010f\n\u03b32\n16B12 \u0103 \u03b3. So, because v is close to y\u0304v, we can show the squared error of f must be close to the squared error of y\u0304v on this level set.\nE px,yq\u201eD rpy \u00b4 fpxqq2 | fpxq \u201c vs \u201c E px,yq\u201eD rpy \u00b4 y\u0304v ` y\u0304v \u00b4 fpxqq2 | fpxq \u201c vs\n\u201c E px,yq\u201eD rpy \u00b4 y\u0304vq2 ` 2py \u00b4 y\u0304vqpy\u0304v \u00b4 vq | fpxq \u201c vs ` py\u0304v \u00b4 vq2\n\u201c E px,yq\u201eD rpy \u00b4 y\u0304vq2 | fpxq \u201c vs ` py\u0304v \u00b4 vq2\n\u0103 E px,yq\u201eD rpy \u00b4 y\u0304vq2 | fpxq \u201c vs ` \u03b3.\nThen, because the squared error of c2 on this level set is much less than the squared error of f , we find that c2 must also have squared error less than that of y\u0304v:\nE px,yq\u201eD rpy \u00b4 y\u0304vq2 \u00b4 py \u00b4 c2pxqq2 | fpxq \u201c vs \u0105 E px,yq\u201eD rpy \u00b4 fpxqq2 \u00b4 \u03b3 \u00b4 py \u00b4 c2pxqq2 | fpxq \u201c vs\n\u011b 2\u03b3 \u00b4 \u03b3 \u201c \u03b3\nWe assumed H satisfies the B1-bounded \u03b3-weak learning condition relative to C, so this gives us a function h P HB1 such that\nE px,yq\u201eD\nrpy \u00b4 y\u0304vq2 \u00b4 py \u00b4 hpxqq2 | fpxq \u201c vs \u0105 \u03b3.\nThen Lemma 3.3 shows that Erhpxqpy \u00b4 y\u0304vq | fpxq \u201c vs \u0105 \u03b3{2.\nSo h witnesses a failure of multicalibration of f , since it follows that\nErhpxqpy \u00b4 vq | fpxq \u201c vs \u201c Erhpxqpy \u00b4 y\u0304vq | fpxq \u201c vs ` Erhpxqpy\u0304v \u00b4 vq | fpxq \u201c vs \u0105 \u03b3{2\u00b4B1 |y\u0304v \u00b4 v|\n\u011b \u03b3{2\u00b4 B 1\u03b3\n4B1\n\u201c \u03b3{4\nand so\nPr x\u201eDx\nrfpxq \u201c vs \u02c6\nE px,yq\u201eD\nrhpxqpy \u00b4 vq | fpxq \u201c vs \u02d92 \u0105 \u03b1C\u03b3 2\n32m \u0105 \u03b1H,\ncontradicting \u03b1H-approximate multicalibration of f on HB1 and D.\nIn Gopalan et al. [2022], Gopalan, Kalai, Reingold, Sharan, and Wieder show that any predictor that is approximately multicalibrated for a class H and distribution D can be efficiently post-processed to approximately minimize any convex, Lipschitz loss function relative to the class H. The theorem we have just proved can now be used to extend their result to approximate loss minimization over any other class C, so long as H satisfies the B-bounded \u03b3-weak learning assumption relative to C. Intuitively, this follows from the fact that if f is approximately multicalibrated with respect to H on D, it is also approximately multicalibrated with respect to C. However, the notion of approximate multicalibration adopted in Gopalan et al. [2022] differs from the one in this work. So, to formalize our intuition above, we will first state the covariance-based definition of approximate multicalibration appearing in Gopalan et al. [2022] and prove a lemma relating it to our own. We note that, going forward, we will restrict ourselves to distributions D over X \u02c6 t0, 1u, as in this case the two definitions of approximate multicalibration are straightforwardly connected.\nDefinition 6.7 (Approximate Covariance Multicalibration Gopalan et al. [2022]). Fix a distribution D over X \u02c6 t0, 1u and a function f : X \u00d1 r0, 1s that maps onto a countable subset of its range, denoted Rpfq. Let H be an arbitrary collection of real valued functions h : X \u00d1 R. Then f is \u03b1-approximately covariance multicalibrated with respect to H on D if\n\u00ff\nvPRpfq Pr x\u201eDX rfpxq \u201c vs \u00a8\n\u02c7 \u02c7Erphpxq \u00b4 h\u0304vqpy \u00b4 y\u0304vq | fpxq \u201c vs \u02c7 \u02c7 \u010f \u03b1,\nwhere h\u0304v \u201c Erhpxq | fpxq \u201c vs and y\u0304v \u201c Ery | fpxq \u201c vs.\nLemma 6.8. Fix a distribution D over X \u02c6 t0, 1u and a class of functions on X , H. Let HB denote the subset\nHB \u201c th P H : max xPX hpxq2 \u010f Bu.\nFix a function f : X \u00d1 r0, 1s that maps onto a countable subset of its range, denoted Rpfq. Then if f is \u03b1-approximately multicalibrated with respect to HB on D, then f is p ? \u03b1p1` ? Bqq-approximately covariance multicalibrated. That is, for all h P HB, f satisfies \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8\n\u02c7 \u02c7Erphpxq \u00b4 h\u0304vqpy \u00b4 y\u0304vq | fpxq \u201c vs \u02c7 \u02c7 \u010f ? \u03b1p1` ? Bq.\nProof. \u00ff\nvPRpfq Prrfpxq \u201c vs\u00a8\n\u02c7 \u02c7Erphpxq \u00b4 h\u0304vqpy \u00b4 y\u0304vq | fpxq \u201c vs \u02c7 \u02c7\n\u201c \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8\n\u02c7 \u02c7Erhpxqy | fpxq \u201c vs \u00b4 y\u0304vh\u0304v \u02c7 \u02c7\n\u201c \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8\n\u02c7 \u02c7Erhpxqy | fpxq \u201c vs \u00b4 vh\u0304v ` vh\u0304v \u00b4 y\u0304vh\u0304v \u02c7 \u02c7\n\u201c \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8\n\u02c7 \u02c7Erhpxqpy \u00b4 vq | fpxq \u201c vs ` h\u0304vpv \u00b4 y\u0304vq \u02c7 \u02c7\n\u010f \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8\n` |Erhpxqpy \u00b4 vq | fpxq \u201c vs| ` \u02c7 \u02c7h\u0304vpv \u00b4 y\u0304vq \u02c7 \u02c7 \u02d8\n\u010f ? \u03b1` ? B \u00ff\nvPRpfq Prrfpxq \u201c vs \u00a8 |v \u00b4 y\u0304v|\n\u010f ? \u03b1p1` ? Bq.\nwhere the second inequality follows from the fact that Erxs \u010f a Erx2s and the bound maxxPX hpxq2 \u010f B.\nWe now recall a theorem of Gopalan et al. [2022], showing that approximate covariance multicalibration with respect to a class H implies approximate loss minimization relative to H, for convex, Lipschitz losses.\nTheorem 6.9. Fix a distribution D over X \u02c6 t0, 1u and a class of real-valued functions on X , H. Fix a function f : X \u00d1 r0, 1s that maps onto a countable subset of its range, denoted Rpfq. Let L be a class of functions on t0, 1u \u02c6 R that are convex and L-Lipschitz in their second argument. If f is \u03b1-approximately covariance multicalibrated with respect to HB on D, then for every ` P L there exists an efficient postprocessing function k` such that\nE px,yq\u201eD r`py, k`pfpxqqqs \u010f min hPHB E px,yq\u201eD r`py, hpxqqs ` 2\u03b1L.\nCorollary 6.10. Fix a distribution D over X \u02c6 t0, 1u and two classes of real-valued functions on X that are closed under affine transformation, H and C. Fix a function f : X \u00d1 r0, 1s that maps onto a countable subset of its range, denoted Rpfq. Let L be a class of functions on t0, 1u\u02c6R that are convex and L-Lipschitz in their second argument. Fix \u03b1C , B \u0105 0. Let B1 \u201c p1 ` b\n2B \u03b1C q2 and \u03b3 \u201c \u03b1C4B . Let \u03b1H \u0103 \u03b13C 29mB12 , and\n\u03b1 \u0103 \u03b1C\u03b3 2\n32mB12 . Then if\n\u2022 H satisfies the B1-bounded \u03b3-weak learning condition relative to C and D\n\u2022 f is \u03b1H-approximately multicalibrated with respect to D and HB1\n\u2022 f is \u03b1-approximately calibrated on D,\nthen for every ` P L there exists an efficient post-processing function k` such that\nE px,yq\u201eD r`py, k`pfpxqqqs \u010f min cPCB E px,yq\u201eD\nr`py, cpxqqs ` 2L ? \u03b1Cp1` ? Bq.\nProof. We have from Theorem 6.6 that given the assumed conditions, f will be \u03b1C-approximately multicalibrated with respect to CB on D. It follows from Lemma 6.8 that f is ? \u03b1Cp1` ? Bq-approximately covariance multicalibrated with respect to CB on D. The result of Gopalan et al. [2022] then gives us that for all ` P L, there exists an efficient post-processing function k` such that\nE px,yq\u201eD r`py, k`pfpxqqqs \u010f min cPCB E px,yq\u201eD\nr`py, cpxqqs ` 2L ? \u03b1Cp1` ? Bq."
        },
        {
            "heading": "7 Empirical Evaluation",
            "text": "In this section, we study Algorithm 1 empirically via an efficient, open-source Python implementation of our algorithm on both synthetic and real regression problems. Our code is available here: https://github.com/ Declancharrison/Level-Set-Boosting. An important feature of Algorithm 1 which distinguishes it from traditional boosting algorithms is the ability to parallelize not only during inference, but also during training. Let ft be the model maintained by Algorithm 1 at round t with m level sets. Given a data set X, ft creates a partition of X defined by Xt`1i \u201c tx|ftpxq \u201c viu. Since the Xi are disjoint, each call h t`1 i \u201c AHpX t`1 i q can be made on a separate worker followed by a combine and round operation to obtain f\u0303t`1 and ft`1 respectively, as shown in Figure 1. A parallel inference pass at round t works nearly identically, but uses the historical weak learners ht`1i obtained from training and applies them to each set X t`1 i ."
        },
        {
            "heading": "7.1 Prediction on Synthetic Data",
            "text": "From Theorem 5.2, we know that multicalibration with respect to a hypothesis class H satisfying our weak learning condition implies Bayes optimality. To visualize the fast convergence of our algorithm to Bayes optimality, we create two synthetic datasets; each dataset contains one million samples with two features. We label these points using two functions, C0 and C1, defined below and pictured in Figure 2). We attempt to learn the underlying function with Algorithm 1.\nC0pxq \u201c\n$\n\u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 % px` 1q2 ` py \u00b4 1q2, if x \u010f 0, y \u011b 0 px\u00b4 1q2 ` py \u00b4 1q2, if x \u0105 0, y \u011b 0 px` 1q2 ` py ` 1q2, if x \u010f 0, y \u0103 0 px\u00b4 1q2 ` py ` 1q2, if x \u0105 0, y \u0103 0\n(C0)\nC1pxq \u201c\n$\n\u2019 \u2019 \u2019 \u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 \u2019 \u2019 \u2019 %\nx` 20xy2 cosp\u00b48xq sinp8yq \u00b4 p1.5x`4qpx`1q2 y`3 ` py \u00b4 1q 2 \u00af , if x \u010f 0, y \u011b 0 x` 20xy2 cosp8xq sinp8yq \u00b4\np1.5x`4qpx\u00b41q2 y`3 ` py \u00b4 1q 2 \u00af , if x \u0105 0, y \u011b 0\nx` 20xy2 cosp\u00b48xq sinp8yq \u00b4 p1.5x`4qpx`1q2 y`3 ` py ` 1q 2 \u00af , if x \u010f 0, y \u0103 0 x` 20xy2 cosp8xq sinp8yq \u00b4\np1.5x`4qpx\u00b41q2 y`3 ` py ` 1q 2 \u00af , if x \u0105 0, y \u0103 0\n(C1)\nIn Figure 3, we show an example of Algorithm 1 learning C0 using a discretization of five-hundred level sets and a weak learner hypothesis class of depth one decision trees. Each image in figure 3 corresponds to the map produced by Algorithm 1 at the round listed in the top of the image. As the round count increases, the number of non-empty level sets increases until each level set is filled, at which point the updates become more granular. The termination round titled \u2018final round\u2019 occurs at T \u201c 199 and paints an approximate map of C0. The image titled \u2018out of sample\u2019 is the map produced on a set of one million points randomly drawn outside of the training sample, and shows that Algorithm 1 is in fact an approximation of the Bayes Optimal C0.\nFigure 4 plots the same kind of progression as Figure 3, but with a more complicated underlying function C1 using a variety of weak learner classes. We are able to learn this more complex surface out of sample with all base classes except for linear regression, which results in a noisy out-of-sample plot."
        },
        {
            "heading": "7.2 Prediction on Census Data",
            "text": "We evaluate the empirical performance of Algorithm 1 on US Census data compiled using the Python folktables package Ding et al. [2021]. In this dataset, the feature space consists of demographic information about individuals (see Table 1), and the labels correspond to the individual\u2019s annual income.\nWe cap income at $100,000 and then rescale all labels into r0, 1s. On an 80/20% train-test split with 500,000 total samples, we compare the performance of Algorithm 1 with Gradient Boosting with two performance metrics: mean squared error (MSE), and mean squared calibration error (MSCE). For less expressive weak learner classes (such as DT(1), see Figure 5), Algorithm 1 has superior MSE out of sample compared to Gradient Boosting through one hundred rounds while maintaining significantly lower MSCE, and converges quicker. However, as the weak learning class becomes more expressive (e.g. increasing decision tree depths), Algorithm 1 is more prone to overfitting than gradient boosting (see Figure 6).\nIn Table 2, we compare the time taken to train n weak learners with Algorithm 1 and with scikit-learn\u2019s version of Gradient Boosting on our census data. Recall that our algorithm trains multiple weak learners per round of boosting, and so comparing the two algorithms for a fixed number of calls to the weak learner is distinct from comparing them for a fixed number of rounds. Because models output by Algorithm 1 may be more complex than those produced by Gradient Boosting run for the same number of rounds, we use number of weak learners trained as a proxy for model complexity, and compare the two algorithms holding this measure fixed. We see the trend for Gradient Boosting is linear with respect to number of weak learners, whereas Algorithm 1 does not follow the same linear pattern upfront. This is due to not being able to fully\nleverage parallelization of training weak learners in early stages of boosting. At each round, Algorithm 1 calls the weak learner on every large enough level set of the current model, and it is these independent calls that can be easily parallelized. However, in the early rounds of boosting the model may be relatively simple, and so many level sets may be sparsely populated. As the model becomes more expressive over subsequent rounds, the weak learner will be invoked on more sets per round, allowing us to fully utilize parallelizability.\nIn Figure 6, we measure MSE and MSCE for Algorithm 1 and Gradient Boosting over rounds of training on our census data. Again, we note that one round of Algorithm 1 is not equivalent to one round of Gradient Boosting, but intend to demonstrate error comparisons and rates of convergence. For the linear regression plots, Gradient Boosting does not reduce either error since combinations of linear models are also linear. As the complexity of the underlying model class increases, Gradient Boosting surpasses Algorithm 1 in terms of MSE, though it does not minimize calibration error.\nWe notice that Algorithm 1, like most machine learning algorithms, is prone to overfitting when allowed. Future performance hueristics we intend to investigate include validating updates, complexity penalties, and weighted mixtures of updates."
        },
        {
            "heading": "A Generalization Bounds",
            "text": "Our analysis of Algorithm 1 assumed direct access to the data distribution D. In practice, we will run the algorithm on the empirical distribution over a sample of n points D \u201e Dn. In this section, we show that when we do this, so long as n is sufficiently large, both our squared error and our multicalibration guarantees carry over from the empirical distribution over D to the distribution D from which D was sampled. Most generalization bounds for multicalibration algorithms (e.g. H\u00e9bert-Johnson et al. [2018], Jung et al. [2021, 2022], Shabat et al. [2020]) are either stated and proven for finite classes H, or are proven for algorithms that do not operate as empirical risk minimization algorithms, but instead gain access to a fresh sample of data from the distribution at each iteration, or are proven for hypotheses classes that are fixed independently of the algorithm. We have a different challenge: Like H\u00e9bert-Johnson et al. [2018], Jung et al. [2021] we study an iterative algorithm whose final hypothesis class is not fixed up front, but implicitly defined as a function of H. But we wish to study the algorithms as they are used\u2014as empirical risk minimization algorithms\u2014so we do not want our analysis to depend on using a fresh sample of data at each iteration. And unlike the analysis in Jung et al. [2022], for us H is continuously large (since it is closed under affine transformations), so we cannot rely on bounds that depend on log |H|. Instead we give a uniform convergence analysis that depends on the pseudo-dimension of our class of weak learners H:\nDefinition A.1. Pseudodimension[Pollard [2012]] Let H be a class of functions from X to R. We say that a set S \u201c px1, . . . , xm, y1, . . . , ymq P Xm\u02c6Rm is pseudo-shattered by H if for any pb1, . . . , bmq P t0, 1um there exists h P H such that @i, hpxiq \u0105 y \u00f0\u00f1 bi \u201c 1 The pseudodimension of H, denoted PdimpHq is the largest integer m for which H pseudo-shatters some set S of cardinality m.\nAlthough hypotheses in H are continuously valued, Algorithm 1 outputs functions that have finite range r1{ms, and so we can view them as multi-class classification functions. Our analysis will proceed by studying the generalization properties of these multiclass functions, which we will characterize using Natarajan dimension:\nDefinition A.2 (Shattering for multiclass functions). Natarajan [1989], Shalev-Shwartz and Ben-David [2014] A set C \u010e X is shattered by H if there exists two functions f0, f1 : C \u00d1 rks such that\n1. For every x P C, f0pxq \u2030 f1pxq.\n2. For every B \u010e C there exists a function h P H such that\n@x P B, hpxq \u201c f0pxq and @x P C B, hpxq \u201c f1pxq.\nDefinition A.3 (Natarajan dimension). Natarajan [1989], Shalev-Shwartz and Ben-David [2014] The Natarajan dimension of H, denoted NdimpHq, is the maximal size of a shattered set C \u010e X .\nWe can then rely the following standard uniform convergence bound for multiclass classification. This statement is slightly modified from the result in Shalev-Schwartz and Ben-David to account for our use of squared error. The result still holds on account of the fact that the Cherhoff bound only relies on the loss function being bounded, and ours is indeed bounded between 0 and 1.\nTheorem A.4 (Multiclass uniform convergence). Shalev-Shwartz and Ben-David [2014] Let , \u03b4 \u0105 0 and let H be a class of functions h : X \u00d1 r1{ks such that the Natarajan dimension of H is d. Let D P \u2206pX \u02c6 r0, 1sq be an arbitrary distribution and let D \u201c tpx1, y1q, . . . , pxn, ynqupxi,yiq\u201eD be a sample of n points from D. Then for\nn \u201c O \u02c6 d logpkq ` logp1{\u03b4q \u03b52 \u02d9 ,\nPr\n\u201e\nmax hPH\n\u02c7 \u02c7 \u02c7 \u02c7\nE px,yq\u201eD rpy \u00b4 hpxqq2s \u00b4 E px,yq\u201eD\nrpy \u00b4 hpxqq2s \u02c7 \u02c7 \u02c7\n\u02c7\n\u011b s  \u010f \u03b4.\nOur strategy will be to bound the Natarajan dimension of the class of models that can be output by Algorithm 1 in terms of the pseudodimension of the underlying weak learner, then apply the above uniform convergence result. To do so, we will first use the following lemma, which bounds the Natarajan dimension of functions that can be described as post-processings of binary valued-functions from a class of bounded VC-dimension.\nLemma A.5. Shalev-Shwartz and Ben-David [2014] Suppose we have ` binary classifiers from binary class Hbin and a rule r : t0, 1u` \u00d1 rks that determines a multiclass label according to the predictions of the ` binary classifiers. Define the hypothesis class corresponding to this rule as\nH \u201c trph1p\u00a8q, . . . , h`p\u00a8qq : ph1, . . . , h`q P pHbinq`u.\nThen, if d \u201c VCdimpHbinq, NdimpHq \u010f 3`d logp`dq.\nRecall that the VC-dimension of a binary classifier is defined as follows:\nDefinition A.6 (VC-dimension). Vapnik and Chervonenkis [1971] Let H be a class of binary classifiers h : X \u00d1 t0, 1u. Let S \u201c tx1, . . . , xmu and let \u03a0HpSq \u201c tphpx1q, . . . , hpxmqq : h P Hu \u010e t0, 1um. We say that S is shattered by H if \u03a0HpSq \u201c t0, 1um. The Vapnik-Chervonenkis (VC) dimension of H, denoted VCdimpHq, is the cardinality of the largest set S shattered by H.\nLemma A.7. Let Hboost be the class of models output by RegressionMulticalibratepf, \u03b1,AH, \u00a8, Bq (Algorithm 1) for any input distribution D and let d be the pseudodimension of its input weak learner class H.\nNdim pHboostq \u010f 24pB{\u03b1q3d log ` p2B{\u03b1q3d \u02d8 .\nProof. Let m be defined (as in RegressionMulticalibratepf, \u03b1,AH,D, Bq) to be 2B{\u03b1. Because our models are always rounded to the nearest value in r1{ms, we can think of the model ft generated in every round of the algorithm multiclass classification problems over m classes. We will show that our final model can be written as a decision rule that maps the outputs of some ` Boolean classifiers to r1{ms, and that these Boolean classifiers have VC dimension that is bounded by the pseudodimension of the weak learner class. Then, we will apply Lemma A.5 to get an upper bound on the Natarajan dimension of the class of models in terms of \u03b1,B, and the pseudodimension of the input weak learner class H.\nConsider the initial round of the algorithm. We can convert our (rounded) initial regressor f0 to a series of m Boolean thresholdings gv which return 1 when f0pxq \u011b v:\ng0v \u201c # 1 if f0pxq \u011b v, 0 otherwise. .\nThese m Boolean thresholdings can then be mapped back to the original prediction over r1{ms using a decision rule r : t0, 1um \u00d1 r1{ms which picks the largest of the thresholds that evaluates to 1, and assigns that index to the prediction:\nr0ptg0vuvPr1{msqpxq \u201c arg max iPr1{ms i1rgvpxq \u201c 1s.\nNote that since our initial predictor f0 was already rounded to take values in r1{ms, the largest v such that f0pxq \u011b v will be exactly f0pxq, so r0 is exactly equivalent to f0. Similarly, at round t ` 1 of RegressionMulticalibratepf, \u03b1,AH,D, Bq, we will show that the model ft`1 can be written as a decision rule rt`1 over m` pt` 1qm2 binary classifiers g, where\ngtv,i \u201c # 1 if htvpxq \u011b i\u00b4 1{p2mq, 0 otherwise. ,\nHere, the thresholds measure halfway between each level set, as htvpxq has yet to be rounded to the nearest level set. We can write a decision rule that maps these thresholds to classifications over r1{ms:\nrt`1 ` rt, tgt`1v,i ui,vPr1{ms \u02d8 pxq \u201c \u00ff\nvPr1{ms 1rrtpxq \u201c vs arg max iPr1{ms\n` i \u00a8 1rgt`1v,i pxq \u201c 1s \u02d8 ,\nNow, we need to show that this decision rule evaluated at round t is equivalent to ft. We proceed inductively. For our base case, we have already argued that our initial decision rule r0 is equivalent to the classifier f0. Now, say that we have decision rule rt over binary classifiers g that is equivalent to model ft. Then, we can write\nrt`1 ` rt, tgt`1v,i ui,vPr1{ms \u02d8 pxq \u201c \u00ff\nvPr1{ms 1rrtpxq \u201c vs arg max iPr1{ms\n` i \u00a8 1rgt`1v,i pxq \u201c 1s \u02d8 ,\n\u201c \u00ff\nvPr1{ms 1rftpxq \u201c vs arg max iPr1{ms\n` i \u00a8 1rgt`1v,i pxq \u201c 1s \u02d8\n\u201c \u00ff\nvPr1{ms 1rftpxq \u201c vs arg max iPr1{ms\n` i \u00a8 1rht`1v pxq \u011b i\u00b4 1{p2mqs \u02d8\n\u201c \u00ff vPr1{ms 1rftpxq \u201c vsRoundpht`1v pxqq\n\u201c ft`1pxq,\nwhere the second line comes from the inductive hypothesis and the second to last line\u2019s equality comes from the fact that the largest i such that ht`1v pxq \u00b4 1{p2mq \u011b i will be the exact rounded prediction of ht`1v pxq.\nNow, we need to show that at round t` 1, the decision rule is a decision rule over m` pt` 1qm2 binary classifiers. Note that our initial decision rule r0 has m \u201c m`0 \u00a8m2 binary classifiers. Say that at round t we have a decision rule rt over m` tm2 classifiers. In the following round, we build m2 new Boolean classifiers gv, i for v, i P r1{ms. So, at round t` 1 we have m` tm2 `m2 \u201c m` pt` 1qm2 classifiers total.\nFrom Theorem 4.3, we know that Algorithm 1 halts after at most T \u010f 2B{\u03b1 rounds, at which point it outputs model fT\u00b41. So, we can rewrite fT\u00b41 as a decision rule rT\u00b41 composed of at most m`pT \u00b4 1qm2 \u0103 Tm2 Boolean models. Plugging in our bound for T and definition of m, this gives us a decision rule rT\u00b41 composed of at most `\n2B \u03b1 \u02d83 Boolean classifiers. Let G be the class of Boolean threshold functions over H, i.e. functions g : X \u00d1 t0, 1u such that\ngpxq \u201c #\n1 hpxq \u011b i 0 hpxq \u0103 i,\nfor some h P H and i P R. Say that the VC-dimension of G is d1. Then, applying lemma A.5, it follows that\nNdimpHboostq \u010f 3 \u02c6 2B\n\u03b1\n\u02d93\nd1 log\n\u02dc\n\u02c6\n2B\n\u03b1\n\u02d93\nd1\n\u00b8\n,\n\u201c 24 \u02c6 B\n\u03b1\n\u02d9\nd1 log\n\u02dc\n\u02c6\n2B\n\u03b1\n\u02d93\nd1\n\u00b8\n.\nNow, it remains to show that we can bound the VC-dimension of these thresholding functions by the pseudodimension of the weak learner class H. Note that G as we have defined it above is a richer hypothesis class than the actual class of thresholding functions used in the above analysis, because it can threshold at any value in R rather than being restricted to r1{ms. Thus, its VC dimension can only be greater than the VC dimension of the class of threshold functions over H restricted to r1{ms, and hence an upper bound on the VC dimension of G in terms of the pseudodimension of H will also be an upper bound on the VC dimension of the restricted class of threshold functions.\nLet d be the pseudodimension of H, and say that d \u0103 d1. By the definition of VC-dimension, t0, 1ud`1 must be shattered by G. I.e., for any set of d` 1 points x1, . . . , xd`1 P X with arbitrary labels b1, . . . , bd`1, there is some hypothesis g P G that realizes those labels on px1, . . . , xd`1q. Consider the function g that, given the d` 1 points in X , realizes the labels b1, . . . , bd`1. By the construction of G, g is a thresholding of some function h P H at some point i. So, there is be some i P R such that hpxiq \u0105 i\u00f1 bi \u201c 1 and such that bi \u201c 1 \u00f1 hpxiq \u0105 i. But this means that t0, 1ud`1 is pseudo-shattered by H, and thus the pseudodimension of H is not d. Thus, it cannot be the case that d \u0103 d1, and hence d1 \u010f d, i.e. the VC dimension of G is bounded above by the pseudodimension of H. Plugging this bound into the above bound on the Natarajan dimension gives us that\nNdimpHboostq \u010f 24 \u02c6 B\n\u03b1\n\u02d9\nd1 log\n\u02dc\n\u02c6\n2B\n\u03b1\n\u02d93\nd1\n\u00b8\n,\n\u010f 24 \u02c6 B\n\u03b1\n\u02d9\nd log\n\u02dc\n\u02c6\n2B\n\u03b1\n\u02d93\nd\n\u00b8\n.\nNow, we can state the following uniform convergence theorem for our final model.\nTheorem A.8 (Squared Error Generalization for Algorithm 1.). Let , \u03b4, \u03b1,B \u0105 0. Let Hboost be the class of models that can be output by RegressionMulticalibratepf, \u03b1,AH, \u00a8, Bq (Algorithm 1) for any input distribution D and let d be the pseudodimension of its input weak learner class H. Let D \u201c tpx1, y1q, . . . , pxn, ynqupxi,yiq\u201eD be a sample of n points drawn i.i.d. from D. Then if\nn \u201c O \u02c6 dB3 log2pdB{\u03b1q \u03b13 2 ` logp1{\u03b4q 2 \u02d9\nPr\n\u201e\nmax hPHboost\n\u02c7 \u02c7 \u02c7 \u02c7\nE px,yq\u201eD rpy \u00b4 hpxqq2s \u00b4 E px,yq\u201eD\nrpy \u00b4 hpxqq2s \u02c7 \u02c7 \u02c7\n\u02c7\n\u011b s  \u010f \u03b4.\nProof. This follows directly from Theorem A.4 and the bound on the Natarajan dimension in Lemma A.7.\nWe also would like to know that our multicalibration guarantees are generalizable. Rather than doing a bespoke analysis here, we can rely on the connection that we have established between failure of multicalibration and ability to improve squared error and argue that if the final hypothesis output by the algorithm was not multicalibrated with high probability then it would be possible to improve its squared error out-ofsample. Thus, by our previous generalization result for squared error, it would be possible to improve the squared error in-sample as well, giving us a contradiction.\nTheorem A.9 (Multicalibration generalization guarantee). Let , \u03b4, \u03b1,B \u0105 0 and consider the model fT\u00b41 output by RegressionMulticalibratepf, \u03b1,AH, D,Bq for some sample D of n points drawn i.i.d. from distribution D such that\nn \u201c O \u02c6 dB3 log2pdB{\u03b1q \u03b13 2 ` logp1{\u03b4q 2 \u02d9\nThen if \u010f \u03b14B , with probability greater than or equal to 1 \u00b4 2\u03b4 it follows that fT\u00b41 is 2\u03b1-approximately multicalibrated with respect to the distribution D.\nProof. Let D \u201c tpx1, y1q, . . . , pxn, ynqupxi,yiq\u201eD. Consider the model fT\u00b41 output by RegressionMulticalibratepf, \u03b1,AH, D,Bq, and recall that within the run of the algorithm there was also a model fT defined in the final round. Say that model fT\u00b41 is not 2\u03b1-approximately multicalibrated with respect to HB and the true distribution D.\nSince the algorithm running on the sample halted, it must have been that the model in the final round improved in squared error by less than \u03b1{p2Bq when measured with respect to the sample D:\nE px,yq\u201eD rpfT\u00b41 \u00b4 yq2s \u00b4 E px,yq\u201eD rpfT \u00b4 yq2s \u010f p\u03b1{2Bq.\nConsider what happens if we run the algorithm again, but with fT\u00b41 as its initial model and now with the underlying distribution as input rather than the sample of n points. Let f 1T be the model found in the first round of running this process RegressionMulticalibratepfT\u00b41, \u03b1,AH,D, Bq. Since fT\u00b41 is not 2\u03b1\u00b4approximately multicalibrated with respect to D and HB , then by an identical argument as in the proof of Theorem 4.3, it it must be that a single round of the algorithm improves the squared error on D by at least \u03b1{B. Thus, Epx,yq\u201eDrpfT\u00b41 \u00b4 yq2s \u00b4 Epx,yq\u201eDrpf 1T \u00b4 yq2s \u0105 \u03b1{B.\nWe know from our previous convergence bound, Theorem A.8, that with probability 1\u00b4\u03b4, |Epx,yq\u201eDrpf 1T\u00b4 yq2s \u00b4 Epx,yq\u201eDrpfT \u00b4 yq2s| \u0103 . So, f 1T must with high probability also improve on the sample D:\n\u03b1 B \u0103 E px,yq\u201eD rpfT\u00b41 \u00b4 yq2s \u00b4 E px,yq\u201eD rpf 1T \u00b4 yq2s\n\u0103 E px,yq\u201eD rpfT\u00b41 \u00b4 yq2s \u00b4 E px,yq\u201eD rpf 1T \u00b4 yq2s ` (with probability \u011b 1\u00b4 \u03b4)\n\u0103 E px,yq\u201eD rpfT\u00b41 \u00b4 yq2s \u00b4 E px,yq\u201eD rpf 1T \u00b4 yq2s ` 2 (with probability \u011b 1\u00b4 2\u03b4) \u0103 \u03b1 2B ` 2 ,\nwhere the last line comes from the fact that the error of f 1T on D cannot be less than the error of fT on D, or else the regression oracle would have found it. Now we have a contradiction: since we have set \u010f \u03b14B ,\n\u03b1 B \u0103 \u03b1 2B ` 2 \u03b1 4B\n\u201c \u03b1 B .\nSo, it must follow that fT\u00b41 is, with probability 1\u00b4 2\u03b4, 2\u03b1\u00b4approximately multicalibrated."
        }
    ],
    "title": "Multicalibration as Boosting for Regression",
    "year": 2023
}