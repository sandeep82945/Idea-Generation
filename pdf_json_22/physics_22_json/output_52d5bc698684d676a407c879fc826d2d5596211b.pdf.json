{
    "abstractText": "Neural Ordinary Differential Equations (NeuralODEs) present an attractive way to extract dynamical laws from time series data, as they bridge neural networks with the differential equation-based modeling paradigm of the physical sciences. However, these models often display long training times and suboptimal results, especially for longer duration data. While a common strategy in the literature imposes strong constraints to the NeuralODE architecture to inherently promote stable model dynamics, such methods are ill-suited for dynamics discovery as the unknown governing equation is not guaranteed to satisfy the assumed constraints. In this paper, we develop a new training method for NeuralODEs, based on synchronization and homotopy optimization, that does not require changes to the model architecture. We show that synchronizing the model dynamics and the training data tames the originally irregular loss landscape, which homotopy optimization can then leverage to enhance training. Through benchmark experiments, we demonstrate our method achieves competitive or better training loss while often requiring less than half the number of training epochs compared to other model-agnostic techniques. Furthermore, models trained with our method display better extrapolation capabilities, highlighting the effectiveness of our method.",
    "authors": [
        {
            "affiliations": [],
            "name": "Joon-Hyuk Ko"
        }
    ],
    "id": "SP:5bc30cfdf83dcc1aaf215d139ed9c42418b45abb",
    "references": [
        {
            "authors": [
                "H.D.I. Abarbanel",
                "D.R. Creveling",
                "R. Farsian",
                "M. Kostuk"
            ],
            "title": "Dynamical State and Parameter Estimation",
            "venue": "SIAM Journal on Applied Dynamical Systems, 8(4):1341\u20131381,",
            "year": 2009
        },
        {
            "authors": [
                "H.D.I. Abarbanel",
                "D.R. Creveling",
                "J.M. Jeanne"
            ],
            "title": "Estimation of parameters in nonlinear systems using balanced synchronization",
            "venue": "Physical Review E, 77(1):016208,",
            "year": 2008
        },
        {
            "authors": [
                "M. Arjovsky",
                "A. Shah",
                "Y. Bengio"
            ],
            "title": "Unitary Evolution Recurrent Neural Networks",
            "venue": "Proceedings of The 33rd International Conference on Machine Learning, pages 1120\u20131128. PMLR,",
            "year": 2016
        },
        {
            "authors": [
                "E. Baake",
                "M. Baake",
                "H.G. Bock",
                "K.M. Briggs"
            ],
            "title": "Fitting ordinary differential equations to chaotic data",
            "venue": "Physical Review A, 45(8):5524\u20135529,",
            "year": 1992
        },
        {
            "authors": [
                "H.G. Bock"
            ],
            "title": "Recent Advances in Parameter Identification Techniques for O.D.E",
            "venue": "Proceedings of an International Workshop, Heidelberg, Fed. Rep. of Germany,",
            "year": 1982
        },
        {
            "authors": [
                "B. Chang",
                "M. Chen",
                "E. Haber",
                "E.H. Chi"
            ],
            "title": "AntisymmetricRNN: A Dynamical System View on Recurrent Neural Networks",
            "venue": "International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "R.T.Q. Chen",
                "Y. Rubanova",
                "J. Bettencourt",
                "D.K. Duvenaud"
            ],
            "title": "Neural Ordinary Differential Equations",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.,",
            "year": 2018
        },
        {
            "authors": [
                "K.M. Choromanski",
                "J.Q. Davis",
                "V. Likhosherstov",
                "X. Song",
                "J.-J. Slotine",
                "J. Varley",
                "H. Lee",
                "A. Weller",
                "V. Sindhwani"
            ],
            "title": "Ode to an ODE",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 3338\u20133350. Curran Associates, Inc.,",
            "year": 2020
        },
        {
            "authors": [
                "K. Doya"
            ],
            "title": "Bifurcations of recurrent neural networks in gradient descent learning",
            "venue": "IEEE Transactions on neural networks, 1(75):218\u2013229,",
            "year": 1993
        },
        {
            "authors": [
                "D.M. Dunlavy",
                "D.P. O\u2019Leary"
            ],
            "title": "Homotopy optimization methods for global optimization",
            "venue": "Technical Report SAND2005-7495, Sandia National Laboratories (SNL),",
            "year": 2005
        },
        {
            "authors": [
                "C. Finlay",
                "J.-H. Jacobsen",
                "L. Nurbekyan",
                "A. Oberman"
            ],
            "title": "How to Train Your Neural ODE: The World of Jacobian and Kinetic Regularization",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, volume 37, pages 3154\u20133164. PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "M. Finzi",
                "K.A. Wang",
                "A.G. Wilson"
            ],
            "title": "Simplifying Hamiltonian and Lagrangian Neural Networks via Explicit Constraints",
            "venue": "H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 13880\u201313889. Curran Associates, Inc.,",
            "year": 2020
        },
        {
            "authors": [
                "B. Ghorbani",
                "S. Krishnan",
                "Y. Xiao"
            ],
            "title": "An Investigation into Neural Net Optimization via Hessian Eigenvalue Density",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, pages 2232\u20132241. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "A. Ghosh",
                "H. Behl",
                "E. Dupont",
                "P. Torr",
                "V. Namboodiri"
            ],
            "title": "STEER : Simple Temporal Regularization For Neural ODE",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 14831\u201314843. Curran Associates, Inc.,",
            "year": 2020
        },
        {
            "authors": [
                "S. Greydanus",
                "M. Dzamba",
                "J. Yosinski"
            ],
            "title": "Hamiltonian Neural Networks",
            "venue": "H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,",
            "year": 2019
        },
        {
            "authors": [
                "N. Gruver",
                "M.A. Finzi",
                "S.D. Stanton",
                "A.G. Wilson"
            ],
            "title": "Deconstructing the Inductive Biases of Hamiltonian Neural Networks",
            "venue": "International Conference on Learning Representations, volume 9,",
            "year": 2021
        },
        {
            "authors": [
                "R. Hasani",
                "M. Lechner",
                "A. Amini",
                "D. Rus",
                "R. Grosu"
            ],
            "title": "Liquid Time-constant Networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 7657\u20137666,",
            "year": 2021
        },
        {
            "authors": [
                "G.A. Johnson",
                "D.J. Mar",
                "T.L. Carroll",
                "L.M. Pecora"
            ],
            "title": "Synchronization and imposed bifurcations in the presence of large parameter mismatch",
            "venue": "Physical Review Letters, 80(18):3956\u2013 3959,",
            "year": 1998
        },
        {
            "authors": [
                "S. Kanai",
                "Y. Fujiwara",
                "S. Iwamura"
            ],
            "title": "Preventing Gradient Explosions in Gated Recurrent Units",
            "venue": "Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,",
            "year": 2017
        },
        {
            "authors": [
                "P. Kidger",
                "R.T.Q. Chen",
                "T.J. Lyons"
            ],
            "title": "Hey, that\u2019s not an ODE\": Faster ODE Adjoints via Seminorms",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, pages 5443\u20135452. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "S. Kim",
                "W. Ji",
                "S. Deng",
                "Y. Ma",
                "C. Rackauckas"
            ],
            "title": "Stiff neural ordinary differential equations",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 31(9):093122,",
            "year": 2021
        },
        {
            "authors": [
                "M. Lezcano-Casado",
                "D. Mart\u00ednez-Rubio"
            ],
            "title": "Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group",
            "venue": "Proceedings of the 36th International Conference on Machine Learning, pages 3794\u20133803. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "I. Loshchilov",
                "F. Hutter"
            ],
            "title": "Decoupled Weight Decay Regularization",
            "venue": "International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "S. Massaroli",
                "M. Poli",
                "S. Sonoda",
                "T. Suzuki",
                "J. Park",
                "A. Yamashita",
                "H. Asama"
            ],
            "title": "Differentiable Multiple Shooting Layers",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 16532\u201316544. Curran Associates, Inc.,",
            "year": 2021
        },
        {
            "authors": [
                "T. Matsubara",
                "Y. Miyatake",
                "T. Yaguchi"
            ],
            "title": "Symplectic Adjoint Method for Exact Gradient of Neural ODE with Minimal Memory",
            "venue": "Advances in Neural Information Processing Systems, volume 34, pages 20772\u201320784. Curran Associates, Inc.,",
            "year": 2021
        },
        {
            "authors": [
                "A. Maybhate",
                "R.E. Amritkar"
            ],
            "title": "Use of synchronization and adaptive control in parameter estimation from a time series",
            "venue": "Physical Review E, 59(1):284\u2013293,",
            "year": 1999
        },
        {
            "authors": [
                "J. Mikhaeil",
                "Z. Monfared",
                "D. Durstewitz"
            ],
            "title": "On the difficulty of learning chaotic dynamics with RNNs",
            "venue": "Advances in Neural Information Processing Systems, 35:11297\u201311312,",
            "year": 2022
        },
        {
            "authors": [
                "J. Miller",
                "M. Hardt"
            ],
            "title": "Stable Recurrent Models",
            "venue": "International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "J.D. Murray"
            ],
            "title": "Mathematical Biology",
            "venue": "1: An Introduction. Number 17 in Interdisciplinary Applied Mathematics. Springer-Verlag GmbH, Berlin Heidelberg, softcover reprint of the hardcover 3rd edition 2002, corrected second printing edition,",
            "year": 2004
        },
        {
            "authors": [
                "U. Parlitz"
            ],
            "title": "Estimating Model Parameters from Time Series by Autosynchronization",
            "venue": "Physical Review Letters, 76(8):1232\u20131235,",
            "year": 1996
        },
        {
            "authors": [
                "R. Pascanu",
                "T. Mikolov",
                "Y. Bengio"
            ],
            "title": "On the difficulty of training recurrent neural networks",
            "venue": "Proceedings of the 30th International Conference on Machine Learning, pages 1310\u20131318. PMLR,",
            "year": 2013
        },
        {
            "authors": [
                "L.M. Pecora",
                "T.L. Carroll"
            ],
            "title": "Synchronization in chaotic systems",
            "venue": "Physical Review Letters, 64(8):821\u2013824,",
            "year": 1990
        },
        {
            "authors": [
                "L.M. Pecora",
                "T.L. Carroll"
            ],
            "title": "Driving systems with chaotic signals",
            "venue": "Physical Review A, 44(4):2374\u20132383,",
            "year": 1991
        },
        {
            "authors": [
                "L.M. Pecora",
                "T.L. Carroll"
            ],
            "title": "Synchronization of chaotic systems",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 25(9):097611,",
            "year": 2015
        },
        {
            "authors": [
                "M. Peifer",
                "J. Timmer"
            ],
            "title": "Parameter estimation in ordinary differential equations for biochemical processes using the method of multiple shooting",
            "venue": "IET Systems Biology, 1(2):78\u201388,",
            "year": 2007
        },
        {
            "authors": [
                "J.H. Peng",
                "E.J. Ding",
                "M. Ding",
                "W. Yang"
            ],
            "title": "Synchronizing hyperchaos with a scalar transmitted signal",
            "venue": "Physical Review Letters, 76(6):904\u2013907,",
            "year": 1996
        },
        {
            "authors": [
                "V.G. Polisetty",
                "S.K. Varanasi",
                "P. Jampana"
            ],
            "title": "Stochastic state-feedback control using homotopy optimization and particle filtering",
            "venue": "International Journal of Dynamics and Control, 10(3):942\u2013955,",
            "year": 2022
        },
        {
            "authors": [
                "J.C. Quinn",
                "P.H. Bryant",
                "D.R. Creveling",
                "S.R. Klein",
                "H.D.I. Abarbanel"
            ],
            "title": "Parameter and state estimation of experimental chaotic systems using synchronization",
            "venue": "Physical Review E, 80(1):016201,",
            "year": 2009
        },
        {
            "authors": [
                "C. Rackauckas",
                "M. Innes",
                "Y. Ma",
                "J. Bettencourt",
                "L. White",
                "V. Dixit"
            ],
            "title": "DiffEqFlux.jl - A Julia Library for Neural Differential Equations, 2019",
            "year": 2019
        },
        {
            "authors": [
                "C. Rackauckas",
                "Y. Ma",
                "J. Martensen",
                "C. Warner",
                "K. Zubov",
                "R. Supekar",
                "D. Skinner",
                "A. Ramadhan",
                "A. Edelman"
            ],
            "title": "Universal Differential Equations for Scientific Machine Learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "J.O. Ramsay",
                "G. Hooker",
                "D. Campbell",
                "J. Cao"
            ],
            "title": "Parameter estimation for differential equations: A generalized smoothing approach",
            "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 69(5):741\u2013796,",
            "year": 2007
        },
        {
            "authors": [
                "A.H. Ribeiro",
                "K. Tiels",
                "L.A. Aguirre",
                "T. Sch\u00f6n"
            ],
            "title": "Beyond exploding and vanishing gradients: Analysing RNN training using attractors and smoothness",
            "venue": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, pages 2370\u20132380. PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "K. Sch\u00e4fer",
                "K. Fla\u00dfkamp",
                "J. Fliege",
                "C. B\u00fcskens"
            ],
            "title": "A Combined Homotopy-Optimization Approach to Parameter Identification for Dynamical Systems",
            "venue": "PAMM, 19(1),",
            "year": 2019
        },
        {
            "authors": [
                "M. Schmidt",
                "H. Lipson"
            ],
            "title": "Distilling Free-Form Natural Laws from Experimental Data",
            "venue": "Science, 324(5923):81\u201385,",
            "year": 2009
        },
        {
            "authors": [
                "N.B. Toomarian",
                "J. Barhen"
            ],
            "title": "Learning a trajectory using adjoint functions and teacher forcing",
            "venue": "Neural Networks, 5(3):473\u2013484,",
            "year": 1992
        },
        {
            "authors": [
                "E.M. Turan",
                "J. J\u00e4schke"
            ],
            "title": "Multiple shooting for training neural differential equations on time series",
            "venue": "IEEE Control Systems Letters, 6:1897\u20131902,",
            "year": 2022
        },
        {
            "authors": [
                "B. van Domselaar",
                "P. Hemker"
            ],
            "title": "Nonlinear parameter estimation in initial value problems",
            "venue": "Technical Report MC-NW-75-18, Amsterdam Univ. Math. Cent. Num. Sci.,",
            "year": 1975
        },
        {
            "authors": [
                "S.K. Varanasi",
                "P. Jampana",
                "C.P. Vyasarayani"
            ],
            "title": "Minimum attention stochastic control with homotopy optimization",
            "venue": "International Journal of Dynamics and Control, 9(1):266\u2013274,",
            "year": 2021
        },
        {
            "authors": [
                "H.U. Voss",
                "J. Timmer",
                "J. Kurths"
            ],
            "title": "Nonlinear Dynamical System Identification from Uncertain and Indirect Measurements",
            "venue": "International Journal of Bifurcation and Chaos, 14(06):1905\u20131933,",
            "year": 2004
        },
        {
            "authors": [
                "C.P. Vyasarayani",
                "T. Uchida",
                "A. Carvalho",
                "J. McPhee"
            ],
            "title": "Parameter identification in dynamic systems using the homotopy optimization approach",
            "venue": "Multibody System Dynamics, 26(4):411\u2013 424,",
            "year": 2011
        },
        {
            "authors": [
                "C.P. Vyasarayani",
                "T. Uchida",
                "J. McPhee"
            ],
            "title": "Single-shooting homotopy method for parameter identification in dynamical systems",
            "venue": "Physical Review E, 85(3):036201,",
            "year": 2012
        },
        {
            "authors": [
                "L.T. Watson",
                "R.T. Haftka"
            ],
            "title": "Modern homotopy methods in optimization",
            "venue": "Computer Methods in Applied Mechanics and Engineering, 74(3):289\u2013305,",
            "year": 1989
        },
        {
            "authors": [
                "Z. Yao",
                "A. Gholami",
                "K. Keutzer",
                "M.W. Mahoney"
            ],
            "title": "PyHessian: Neural Networks Through the Lens of the Hessian",
            "venue": "2020 IEEE International Conference on Big Data (Big Data), pages 581\u2013590. IEEE,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Yin",
                "V. Le Guen",
                "J. Dona",
                "E. de B\u00e9zenac",
                "I. Ayed",
                "N. Thome",
                "P. Gallinari"
            ],
            "title": "Augmenting physical models with deep networks for complex dynamics forecasting",
            "venue": "Journal of Statistical Mechanics: Theory and Experiment,",
            "year": 2021
        },
        {
            "authors": [
                "W. Zhi",
                "T. Lai",
                "L. Ott",
                "E.V. Bonilla",
                "F. Ramos"
            ],
            "title": "Learning Efficient and Robust Ordinary Differential Equations via Invertible Neural Networks",
            "venue": "Proceedings of the 39th International Conference on Machine Learning, pages 27060\u201327074. PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "J. Zhuang",
                "N. Dvornek",
                "X. Li",
                "S. Tatikonda",
                "X. Papademetris",
                "J. Duncan"
            ],
            "title": "Adaptive Checkpoint Adjoint Method for Gradient Estimation in Neural ODE",
            "venue": "Proceedings of the 37th International Conference on Machine Learning, pages 11639\u201311649. PMLR,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "1 Introduction\nPredicting the evolution of a time varying system and discovering mathematical models that govern it is paramount to both deeper scientific understanding and potential engineering applications. The centuriesold paradigm to tackle this problem was to either ingeniously deduce empirical rules from experimental data, or mathematically derive physical laws from first principles. However, the complexities of the systems of interest have grown so much that these traditional approaches are now often insufficient. This\nPreprint. Under review.\nar X\niv :2\n21 0.\n01 40\n7v 5\n[ cs\n.L G\nhas led to a growing interest in using machine learning methods to infer dynamical laws from data.\nOne line of research stems from the dynamical systems literature, where the problem of interest was to determine the unknown coefficients of a given differential equation from to data. While the task seems rather simple, it was found that naive optimization cannot properly recover the desired coefficients due to irregularities in the optimization landscape, which worsens with increasing nonlinearity and data length[49, 35, 41, 1].\nA parallel development occurred in the study of recurrent neural networks (RNNs), which are neural network analogues of discrete maps. It was quickly found that training these models can be quite unstable and results in exploding or vanishing gradients. While earlier works in the field resulted in techniques such as teacher forcing or gradient clipping that are agnostic to the RNN architecture[9, 31], more recent approaches have been directed towards circumventing the problem entirely by incorporating specific mathematical forms into the model architecture to constrain the eigenvalues of the model Jacobian and stabilize RNN dynamics[3, 19, 6, 22, 28]. While these latter methods can be effective, for the purpose of discovering dynamical equations underlying the data, these approaches are inadequate as one cannot ascertain the hidden equations conform to the rigid mathematical structure of these approaches - especially so if the dynamics to be predicted is incompatible with the constrained model structure [42, 27].\nIn this work, we focus on Neural Ordinary Differential Equations (NeuralODEs) [7]. These models are powerful tools in modeling natural phenomena, as they bridge the expressibility and flexibility of neural networks with the de facto mathematical language of the physical sciences. This has led to researchers amalgamating NeuralODEs with partial information about the governing equation to produce \u201cgrey-box\" dynamics models [40, 54], and endowing NeuralODEs with mathematical structures that the system must satisfy [15, 12]. Despite the conceptual elegance of NeuralODEs, training these models tend to result in long training times and sub-optimal results, a problem that is further exacerbated as the length of the training data grows [14, 11]. Similar to the case of RNNs, methods have been proposed to tackle the problem involve placing either strong [8, 17], or semistrong constraints[11, 20] to the functional form the NeuralODE can take - something the underlying governing equation does not guarantee satisfying.\nContributions. We introduce a method to accurately train NeuralODEs on long time series data by leveraging tools from the chaos and mathematical optimization literature: synchronization and homotopy optimization [1, 10]. From loss landscape analyses, we show that the longer training data leads to more irregular loss landscape, which in turn lead to deteriorating performance of the trained models. We show that synchronzing NeuralODE dynamics with the data can smooth the loss landscape, on which homotopy optimization can be applied to enhance training. Through performance benchmarks, we show that not only does our model far outperform conventional gradient descent training, it also surpasses multiple shooting in terms of convergence speed and extrapolation accuracy."
        },
        {
            "heading": "2 Preliminaries",
            "text": ""
        },
        {
            "heading": "2.1 Neural ordinary differential equations",
            "text": "A NeuralODE [7] is a model of the form,\ndu dt = U(t,u;\u03b8), u(t0) = u0. (1)\nwhere u0 \u2208 Rn is the initial condition or input given to the model, and U(...;\u03b8) : R\u00d7 Rn \u2192 Rn is a neural network with parameters \u03b8 \u2208 Rm that governs the dynamics of the model state u(t) \u2208 Rn. While this model can be used as a input-output mapping1, we are interested in the problem of training NeuralODEs on time series data to learn the underlying governing equation and forecast future.\nGiven an monotonically increasing sequence of time points and the corresponding measurements {t(i), u\u0302(i)}Ni=0, NeuralODE training starts with using an ordinary differential equation (ODE) solver\n1In this case, NeuralODEs become the continuous analog of residual networks [7].\nto numerically integrate Equation (1) to obtain the model state u at given time points :\nu(i)(\u03b8) = u(0)(\u03b8) + \u222b t(i) t(0) U(t,u;\u03b8)dt = ODESolve ( U(t,u;\u03b8), t(i),u0 ) . (2)\nAfterwards, gradient descent is performed on the loss function L(\u03b8) = 1N+1 \u2211 i \u2223\u2223u(i)(\u03b8)\u2212 u\u0302(i)\u2223\u222322 to arrive at the optimal parameters \u03b8\u2217."
        },
        {
            "heading": "2.2 Homotopy optimization method",
            "text": "Consider the problem of finding the parameter vector \u03b8\u2217 \u2208 Rm that minimizes a function L(\u03b8). While minimization algorithms can directly be applied to the target function, these tend to struggle if L is non-convex and has a complicated landscape riddled with local minima. To address such difficulties, the homotopy optimization method [52, 10] introduces an alternative objective function\nH(\u03b8, \u03bb) = { F(\u03b8), if \u03bb = 1 L(\u03b8), if \u03bb = 0 (3)\nwhere F(\u03b8) is an auxillary function whose minimum is easily found, andH(\u03b8, \u03bb) : Rm \u00d7 R\u2192 R is a continuous function that smoothly interpolates between F and L as the homotopy parameter \u03bb varies from 1 to 0.\nThe motivation behind this scheme is similar to simulated annealing: one starts out with a relaxed version of the more complicated problem of interest, and finds a series of approximate solutions while slowly morphing the relaxed problem back into its original non-trivial form. This allows the optimization process to not get stuck in spurious sharp minima and accurately converge to the minimum of interest."
        },
        {
            "heading": "2.3 Synchronization of dynamical systems",
            "text": "Here, we briefly summarize the mathematical phenomenon of synchronization, as described in the literature [36, 18, 1, 34]. In later sections, we will show that synchronization can be used mitigate the difficulties arising in NeuralODE training. Consider two states u\u0302,u \u2208 Rn are n-dimensional state vectors, where the first state evolves via du\u0302dt = U\u0302(t, u\u0302; \u03b8\u0302) and the second state by,\ndu dt = U(t,u;\u03b8)\u2212K(u\u2212 u\u0302). (4)\nwhere U\u0302,U : R\u00d7 Rn \u2192 Rn are the dynamics of the two systems parameterized \u03b8\u0302,\u03b8, respectively, and K = diag(k1, ..., kn), is the diagonal coupling matrix between the two systems. The two systems are said to synchronize if the error between u\u0302(t) and u(t) vanishes with increasing time.\nTheorem 1 (Synchronization). Assuming U = U\u0302 and u(t) in the vicinity of u\u0302(t), the elements of K can be chosen so that the two states synchronize: that is, for error dynamics \u03be(t) = u(t)\u2212 u\u0302(t), ||\u03be(t)||2 = ||u(t)\u2212 u\u0302(t)||2 \u2192 0 as t\u2192\u221e.\nProof. By subtracting the two state equations, the error, up to first order terms, can be shown to follow:\nd\u03be(t)\ndt =\ndu(t) dt \u2212 du\u0302(t) dt =\n[ \u2202U(t,u;\u03b8)\n\u2202u\n\u2223\u2223\u2223\u2223\u2223 u=u\u0302 \u2212K ] \u03be(t) = A\u03be(t). (5)\nThe Equation (5) is actually the matrix representation of the linear homogeneous system of differential equations, whose general solution is \u03be(t) = eAt\u03be(0). Increasing the elements of K causes the matrix A to become negative definite \u2014which in turn, causes the largest Lyapunov exponent of the system,\n\u039b := lim t\u2192\u221e\n1 t log ||\u03be(t)|| ||\u03be(0)|| . (6)\nto also become negative as well. Then ||\u03be(t)||2 = ||u(t)\u2212 u\u0302(t)||2 \u2192 0 as t\u2192\u221e, which means the dynamics of u and u\u0302 synchronize with increasing time regardless of the parameter mismatch.\n2It is also possible to use other types of losses, such as the L1 loss used in [12, 21].\nRemark. Our assumption of U = U\u0302, that the functional form of the data dynamics is the same as the equation to be fitted can also be thought to hold for NeuralODE training. This is because if the model is sufficiently expressive, universal approximation theorem will allow the model to match the data equation for some choice of model parameter."
        },
        {
            "heading": "2.3.1 Connection to homotopy optimization",
            "text": "To combine homotopy optimization with synchronization, we slightly modify the coupling term of Equation (4) by multiplying it with the homotopy parameter \u03bb:\n\u2212K(u\u2212 u\u0302) \u2192 \u2212\u03bbK(u\u2212 u\u0302). (7) With this modification, applying homotopy optimation to the problem is straightforward. When \u03bb = 1 and the coupling matrix K has sufficiently large elements, synchronization occurs and the resulting loss function L(\u03b8, 1 \u00b7K) is well-behaved, serving the role of the auxillary function F in Equation (3). When \u03bb = 0, the coupled Equation (4) reduces to the original one, and the corresponding loss function L(\u03b8) = L(\u03b8, 0 \u00b7 K) is the complicated loss function we need to ultimately minimize. Therefore, starting with \u03bb = 1 and successively decreasing its value to 0 in discrete steps, all the while optimizing for the coupled loss function L(\u03b8, \u03bbK) allows one to leverage the well-behaved loss function landscape from synchronization while being able to properly uncover the system parameters [51, 43]."
        },
        {
            "heading": "3 Related works",
            "text": "Synchronization in ODE parameter estimation. After the discovery of synchronization in chaotic systems by Pecora & Carroll [32, 33], methods to leverage this phenomenon to identify unknown ODE parameters were first proposed by Parlitz [30], and later refined by Abarbanel et al [2, 1]. To obtain the parameter from the synchronized systems, these earlier works would either design an auxillary dynamics for the ODE parameters [30, 26], or include a regularization term on the coupling strength in the loss function [38, 1]. The idea of applying homotopy optimization to this problem was pioneered by Vyasarayani et al. [50, 51] and further expanded upon by later works [43, 48, 37].\nImproving the training of RNNs and NeuralODEs. As we previously discussed, many of the recent efforts to improve RNN and NeuralODE tend to accomplish this by constraining the expressivity of the neural network models. The limitations of this strategy is demonstrated in [42], where it is demonstrated that such \"stabilized\" models cannot learn oscillatory or highly complex data that does not match the model restrictions. Another recent development with a similar spirit as ours is Mikhaeil [27], where the classical RNN training strategy of teacher forcing [45] was revisited and combined it to the chaos theory concept of the Lyapunov exponent, to develop a architecture agnostic training method for RNNs. In recent NeuralODE literature, Zhi et al. [55] introduces a powerful method that drastically increases the training speed but uses a specialized invertible neural network-based architecture. Other aspects of NeuralODE training, such as better gradient calculation, have been studied to improve NeuralODE performance[20, 56, 25, 21]. Such methods are fully compatible with our proposed work, and we showcase an example in Appendix G, by changing the gradient calculation scheme to the \u201csymplectic-adjoint-method\" [25]."
        },
        {
            "heading": "4 Loss landscapes in learning ODEs",
            "text": ""
        },
        {
            "heading": "4.1 Complicated loss landscapes in ODE parameter estimation",
            "text": "As we previously mentioned, NeuralODEs suffer from the same training difficulties of ODE fitting and RNNs. Here we illustrate that the underlying cause for this difficulty is also identical by analyzing the loss landscape of NeuralODEs for increasing lengths of training data. The data were generated from the periodic Lotka-Volterra system (we provide more details in Section 6) and the landscapes were visualized by calculating the two dominant eigenvectors of the loss Hessian, then perturbing the model parameter along those directions [53]. We also provide analogous analyses for ODE parameter estimation on the same system in Appendix B to further emphasize the similarities between the problem domains. From Figure 2 (upper left), we find that the loss landscape is well-behaved for short data, but becomes pathological as the data lengthens. In fact, the landscape for t \u2208 [0, 9.1] features a stiff cliff, which\nis known to cause exploding gradients in RNN training[9, 31]. We also observe that the magnitude of the loss increases with data length. This is a direct consequence of model dynamics u(t) being independent of the data dynamics u\u0302(t) and therefore tending to diverge away with time.\nTo further confirm our findings and compensate for the fact that the landscapes represent only a subset of the original high dimensional loss function, we computed the eigenvalue spectrum of the loss function hessian for increasing lengths of training data (Figure 2, lower left). From the results, we find that the eigenvalues spread away from zero as the training data increases, indicating that the loss morphs from a relatively flat to a more corrugated landscape. We also observe that there are positive outlier peaks, whose values also rapidly increase with the length of training data. As such large outliers have been report to slow down training [13], this also provides additional insights as to why NeuralODE training stagnates on long time series data."
        },
        {
            "heading": "4.2 Loss landscape of synchronized dynamics.",
            "text": "With coupling, the loss function L between the model predictions and the data now becomes a function of both the model parameters \u03b8 and the coupling strength k. Increasing the coupling strength k effectively acts as shortening the data because increased coupling causes the two trajectories to start synchronizing earlier in time, after which the error between the two will diminish and contribute negligibly to the total loss. Therefore, synchronizing the model and the data generating equations leads to a more favorable loss landscape, which gradient descent based optimizers will not struggle on.\nOnce again, we confirm this point by visualizing the loss landscape and the Hessian eigenvalue spectrum for a long time series dataset while increasing the strength of the coupling. From Figure 2 (upper right) we find that with increasing coupling strength, the steep cliff and the flat basin of the original loss surface are tamed into a well-behaved slope. This is reflected in the eigenvalue spectrum as well (Figure 2, lower right), with the all of the spectrum gathering near zero as the coupling is turned up. In fact, when the coupling strength is increased too much, we find that all the eigenvalues bunch together at zero, meaning that the loss function becomes completely flat. Qualitatively, this corresponds to the case where the coupling completely dominates the synchronized dynamics and forces the model predictions onto the data regardless of the model parameters."
        },
        {
            "heading": "5 Homotopy optimization for NeuralODE training",
            "text": "Having established that applying synchronization between the model and data can tame the irregular loss landscape during training, we now turn towards adapting the synchronization-homotopy approach of Section 2.3.1 to a NeuralODE setting. In doing so, there are couple details that needs addressing to arrive at a specific implementations. We discuss such points below."
        },
        {
            "heading": "5.1 Construction of the coupling term",
            "text": "While the coupling term for synchronization\u2212K(u\u2212u\u0302) (Equation (4)) depends on the data dynamics u\u0302, in practice this information is only known for measurement times {t(i)}Ni=0. This is at a disaccord with the ODE solvers used during training, which require evaluating the model at intermediate time points as well. Therefore, to construct a control term that is defined for any time point, we used a cubic smoothing splines of the data points to supply the measurement values at unseen, but required time points. Smoothing was chosen over interpolation to make our approach robust to measurement noise in the data.\nNote that cubic spline is not the only solution - even defining the coupling term as a sequence of impulses is possible, only applying it where it is defined, and keeping it zero at other times. This is an interesting avenue of future research, as this scheme can be shown[38] to be mathematically equivalent to teacher forcing[45]."
        },
        {
            "heading": "5.2 Scheduling the homotopy parameter",
            "text": "While the homotopy parameter \u03bb can be decremented in various different ways, the most common approach is to use constant decrements. In our study, we instead used power law decrements of the form {\u2206\u03bb(j)}s\u22121j=0 = \u03baj/ \u2211s\u22121 j=0 \u03ba\nj where \u2206\u03bb(j) denotes the j-th decrement and s is number of homotopy steps. This was inspired from our observations of the synchronized loss landscape (Figure 2, right), where the overall shape change as well as the width of the eigenvalue spectrum seemed to be a power law function of the coupling strength.\nAlgorithm 1 Homotopy-based NeuralODE training Input: Data {t(i), u\u0302(i)}Ni=0, cubic smoothing spline u\u0302(t; t(i), u\u0302(i)), model U(...;\u03b8)\nHyperparameters: Coupling strength(k), homotopy parameter decrement ratio(\u03ba), number of homotopy steps(s), epochs per homotopy step(nepoch), learning rate(\u03b7) \u03bb\u2190 1 while \u03bb\u2190 0 do\nFor each homotopy step {u(i)}Ni=0 = ODESolve(du/dt = U(t,u;\u03b8)) if MSE \u2264MSEbest then \u03b8\u2217 \u2190 \u03b8ep end if {u(i)}Ni=0 = ODESolve(du/dt = U(t,u;\u03b8)\u2212 \u03bbK(u\u2212 u\u0302)) L(\u03b8) = 1N+1 \u2211 i\n\u2223\u2223u(i)(\u03b8)\u2212 u\u0302(i)\u2223\u22232 \u25b7 \u03b8ep+1 \u2190 OptimizerStep(\u03b8ep,\u2207\u03b8L) end while \u03bb\u2190 \u03bb\u2212\u2206\u03bb\nOutput: trained model parameters \u03b8\u2217, \u03bb = 0"
        },
        {
            "heading": "5.3 Hyperparameter overview",
            "text": "Our final implementation of the homotopy training algorithm has five hyperparameters describing as follows.\nCoupling strength (k) This determines how smoothed the synchronized loss will be. The value much be large enough so that the irregularities disappear, but not too large to make the landscape completely flat.\nHomotopy parameter decrement ratio (\u03ba) This determines how rapidly the homotopy parameter \u03bb is decremented at each step. Too large a value causes the loss to suddenly become complicated, leading to poor optimization. We empirically find that around 0.6 works well.\nNumber of homotopy steps (s) This determines how many relaxed problem the optimization process will pass through to get to the final solution. Systems with complicated time behaviors will require more steps.\nEpochs per homotopy steps (nepoch) This determines how long the model will train on a given homotopy parameter value. When it is too small, the model lacks the time to properly converge on the loss function, whereas too large, the model overfits on the simpler loss landscape.\nLearning rate (\u03b7) This is as same as in conventional NeuralODE training. We found values in the range of 0.002-0.1 to be adequate for our experiments."
        },
        {
            "heading": "6 Experiments",
            "text": "To benchmark our homotopy training algorithm against existing methods, we trained multiple NeuralODE models across time-series datasets of differing complexities. The performance metrics we used in our experiments were mean squared error on the training dataset and on model rollouts of 100% the training data length, which from now on we refer to as interpolation and extrapolation MSE, respectively. For each training method, the hyperparameters were determined through a grid search ahead of time. All experiments were repeated three times with different random seeds, and we report the corresponding means and standard errors whenever applicable. A brief overview of the datasets, models and the baseline methods used is provided below, and we refer readers to Appendix E and Appendix F for extensive details on data generation, model configuration and hyperparameter selection."
        },
        {
            "heading": "6.1 Datasets and model architectures",
            "text": "Lotka-Volterra system The Lotka-Volterra system is a simplified model of prey and predator populations[29]. The system dynamics are periodic, meaning even models trained on short trajectories should have strong extrapolation properties, provided the training data contains a full period of oscillation. We considered two types of models for this system: a \"black-box\" model of Equation (1), and a \"gray-box\" model used in [39] given by dxdt = \u03b1x+ U1(x, y;\u03b81), dy dt = \u2212\u03b3y + U2(x, y;\u03b82).\nDouble pendulum For this system, we used the real-world measurement data from Schmidt & Lipson[44]. While the double pendulum system can exhibit chaotic behavior, we used the portion of the data corresponding to the aperiodic regime. We trained two types of models on this data: a black-box model and a model with second-order structure. Our motivation for including the latter model stems from the ubiquity of second-order systems in physics and from [16], which reports these models can match the performance of Hamiltonian neural networks.\nLorenz system The Lorenz system displays highly chaotic behavior, and serves as a stress test for time series prediction tasks. For this system, we only employed a black-box model."
        },
        {
            "heading": "6.2 Baselines",
            "text": "As the focus of this paper is to effectively train NeuralODEs without employing any specialized model design or constraining model expressivity, this was criteria by which we chose the baseline training method for our experiments. Two training methods were chosen as our baseline: vanilla gradient descent, which refers to the standard method of training NeuralODEs, and multiple-shooting method. As the latter also stems from the ODE parameter estimation literature, and its inner working relevant to the discussion of our results, we provide a brief description below and additional details in Appendix D.\nMultiple shooting The muliple shooting algorithm[47, 5, 4, 49] circumvents the difficulty of fitting models on long sequences by dividing the training data into shorter segments and solve initial value problems for each segments, and optimizing a loss function that has an additional continuity enforcing term to keep the different predicted segments in sync. Unlike synchronization and homotopy, which we employ in NeuralODE training for the first time, works applying multiple shooting to NeuralODEs are scant but do exist[39, 24, 46]3. As the three previous references differ in their implementations, we chose the scheme of [39] for its simplicity and implemented a PyTorch adaptation to use for our experiments.\n3While not explicitly mentioned in the paper, DiffEqFlux.jl library has an example tutorial using this technique to train NeuralODEs."
        },
        {
            "heading": "7 Results",
            "text": ""
        },
        {
            "heading": "7.1 Training NeuralODEs on time series data of increasing lengths",
            "text": "From the previous sections, we found that longer training data leads to a more irregular loss landscape. To confirm whether this translates into deteriorated optimization, we trained NeuralODEs on LotkaVolterra system trajectories of different lengths. Inspecting the results (Figure 3, first panel) we find while all training methods show low interpolation MSE for shortest data, this is due to overfitting, as evidenced by the diverging extrapolation MSE4. As the length of the data grows and encompasses a full oscillation period, we find that the model performances from homotopy and multiple-shooting stabilize for both interpolation and extrapolation. This is clearly not the case for the vanilla gradient descent, whose model performances degrade exponentially with as data length increases.\nModel capacity is not the problem The reason NeuralODEs struggle on long sequences can easily be attributed to insufficient model capacity. We demonstrate this is not the case by repeating the previous experiment for the mid-length data while decreasing the model size used (Figure 3, second panel). In the case of vanilla gradient descent, the model size is directly correlated to its performance, lending the claim some support. However, we find that both homotopy and multiple shooting methods can effectively train much smaller models to match the accuracy of a larger model trained with vanilla gradient descent. This signifies that it is the inability of training algorithms to fully capitalize on the model capacity, and not the capacity itself, that is the cause of the problem.\nRobustness against data degradation We performed an additional set of experiments to evaluate the performances of different training methods against increasing data sparsity and noise. This is particularly of interest for our homotopy training as we employ smoothing cubic splines (Section 5.1) whose reliability can be adversely affected by the data quality. From the results (Figure 3, third, fourth panels) we find this is not the case and that that our algorithm performs well even for degraded data. One unexpected result is that the multiple shooting fails to as the sampling period increases, which is a side effect from the fact we held the time interval fixed for this experiment. Due to the space constraint, we continue this discussion in Appendix G.3."
        },
        {
            "heading": "7.2 Performance benchmarks of the training algorithms",
            "text": "Having confirmed that both the homotopy and multiple-shooting methods successfully mitigate the NeuralODE training problem, we now move to a more comprehensive benchmark, presented in Figure 4.\nFrom the results, it is clear both our homotopy method and multiple shooting can both boost the interpolation performance of NeuralODEs, with our method being slightly more performant. Inspecting the extrapolation performance, however, we do find a clear performance gap between our\n4Indeed, plots of the model prediction trajectory in Appendix G.3 also confirm this fact.\nhomotopy method and multiple shooting, with the latter even performing worse than vanilla gradient descent for the double pendulum dataset (Figure 4, center, Figure 5, left). This underperformance of multiple shooting for extrapolation tasks can be attributed to the nature of the algorithm: during training time, multiple shooting integrates NeuralODEs on shorter, subdivided time intervals, whereas during inference time the NeuralODE needs to be integrated for the entire data time points and then some more for extrapolation. This disparity between training and inference render models trained by multiple shooting to be less likely to compensate for error accumulation in long term predictions, resulting in poor extrapolation performance.\nAnother stark difference between our homotopy training and the other methods is in the number of epochs required to arrive at minimum interpolation MSE (Figure 4, right panel). While the exact amount of speedup is variable and seems to depend on the dataset difficulty, we find a 90%-40% reduction compared to vanilla training, highlighting the effectiveness of our approach.\nWhat makes homotopy training effective: inspecting training dynamics To obtain a complementary insight to the NeuralODE training dynamics, and to relate the effectiveness of our method with the properties of the loss landscape, we calculated the trace of the loss hessian during training (Figure 5, right panel). A small, but positive hessian values indicates more favorable training dynamics [53], and we find that our method results in the smallest trace, followed by multiple shooting then vanilla gradient descent. The fact that the hessian trace maintains a low value throughout the entire homotopy training, even as the loss surface is morphed back to its unsynchronized, irregular state, also confirms that our homotopy schedule works well and managed to gradually guide the optimizer into a well performing loss minimum."
        },
        {
            "heading": "8 Limitations and discussion",
            "text": "In this paper, we adapted the concepts of synchronization and homotopy optimization for the first time in the NeuralODE literature, and demonstrated that our homotopy method can successfully train NeuralODE models with strong interpolation and extrapolation accuracies. Note that our work serves as an initial confirmation that the methods developed in the ODE parameter estimation literature can be adapted to be competitive in the NeuralODE setting. As such, there are multiple avenues of future research, including theoretical investigations on the optimal coupling strength and different ways to schedule the homotopy parameter, such as incorporating it into the loss function as a regularization term [1, 43]. Extending our algorithm to high-dimensional systems or systems with partially observed\nstates is another important direction as well. Nevertheless, our work hold merit in that it lays the groundwork upon which ideas from ODE parameter estimation, control theory, and RNN/NeuralODE training can be assimilated, which we believe will facilliate further research connecting these domains.\nAuthor Contributions\nJ. Ko and H. Koh contributed equally to the paper."
        },
        {
            "heading": "Acknowledgments",
            "text": "This work was supported by grants from the National Research Foundation of Korea (No. 2016R1A3B1908660) to W. Jhe."
        },
        {
            "heading": "A Additional figures",
            "text": "In this section, we present some of the figures that were excluded from the main text for brevity.\nFigure 6 below highlights the efficiency of our training approach with respect to the number of training epochs by plotting the trajectory predictions of the second-order models during training.\nSt at\ne 1, 2 St at e 1, 2\nHomotopy Epoch = 59 Epoch = 99 Epoch = 199 Epoch = 900(best)\nBaseline Epoch = 59 Epoch = 99 Epoch = 199 Epoch = 3833(best)\nSt at\ne 1, 2 St at e 1, 2\nMulti.shoot. Epoch = 59 Epoch = 99 Epoch = 199 Epoch = 3286(best)\nFigures 7 to 9 show model prediction trajectories corresponding to our benchmark results of Figure 4. One intriguing point is that even though both the gray-box model (Lotka-Volterra system, Figure 7) and the second-order model (double pendulum, Figure 8) contain more prior information about their respective systems, this does not necessarily translate to better predictive capabilities if an effective training method is not used. This suggests that introducing physics into the learning problem can obfuscate optimization, something that has been reported for physics-informed neural networks. It also highlights the effectiveness of our homotopy training algorithm, as our method can properly train such difficult-to-optimize models."
        },
        {
            "heading": "B Loss landscape and synchronization in ODE parameter estimation",
            "text": "Here, we illustrate the similarity between the ODE parameter estimation problem and the NeuralODE training problem by repeating analyses analogous to those of Section 4 in the setting of estimating the unknown coefficients of a given ordinary differential equation.\nIn the left and middle panels of Figure 10, we show the trajectories of the Lotka-Volterra and the Lorenz systems were a single parameter was perturbed. We can easily observe that the original trajectory and the perturbed trajectories evolve independently in time. Furthermore, while trajectories from the simpler Lotka-Volterra system retain the same periodic behavior and differ only in their amplitudes and phases, trajectories from the Lorenz system display much different characteristics with increasing time. This translates over to the shape of the loss landscape (Figure 10, right) with the loss for the Lotka-Volterra system only becoming steeper with increasing data length, whereas the loss for the Lorenz system displays more and more local minima.\nOnce synchronization is introduced to the perturbed trajectory, its dynamics tends to converge with the reference trajectory as time increases, with the rate of convergence increasing for larger values of the coupling strength k (Figure 11, left and middle panels). In terms of the loss landscape (right panel), larger coupling does result in a smoother landscape, with excessive amount of it resulting in a flat landscape that is also detrimental to effective learning. These results are a direct parallel to our observations in Figure 2 and provide additional justifications as to why techniques developed in the field of ODE parameter estimation also work so well on NeuralODE training."
        },
        {
            "heading": "C Further description of the homotopy optimization procedure",
            "text": "To implement homotopy optimization, one usually selects the number of discrete steps for optimization, s, as well as a series of positive decrement values for the homotopy parameter {\u2206\u03bb(k)}s\u221210 that sum to 1. Afterwards, optimization starts with an initial \u03bb value of \u03bb(0) = 1, which gives H(0)(\u03b8) = G(\u03b8). At each step, the objective function at the current iteration is minimized with respect to \u03b8, using the output from the previous step \u03b8\u2217(k\u22121) is as the initial guess:\nH(k)(\u03b8) = H(\u03b8, \u03bb = \u03bb(k)) \u2192 \u03b8\u2217(k) = argmin \u03b8 H(k)(\u03b8) (8)\nAfterwards, \u03bb decremented to its next value, \u03bb(k+1) = \u03bb(k) \u2212\u2206\u03bb(k), and this iteration continues until the final step s where \u03bb(s) = 0, H(s)(\u03b8) = F(\u03b8), and the final minimizer \u03b8\u2217(s) = \u03b8\u2217 is the sought-after solution to the original problem F(\u03b8)."
        },
        {
            "heading": "D Multiple shooting algorithm",
            "text": "Our implementation of the multiple shooting algorithm for training NeuralODEs closely mirrors the example code provided in the DiffEqFlux.jl package [39] of the Julia programming language ecosystem.\nGiven time series data {ti, u\u0302i}Ni=0, multiple shooting method partitions the data time points into m overlapping segments:\n[t0 = t (0) 0 , . . . , t (0) n ], ..., [t (m\u22121) 0 , . . . , t (m\u22121) n = tN ]; t (i\u22121) n = t (i) 0 , i \u2208 1 . . .m\u2212 1.\nDuring training, the NeuralODE is solved for each time segment, resulting in m segmented trajectories:\n[u (0) 0 , . . . ,u (0) n ], ..., [u (m\u22121) 0 , . . . ,u (m\u22121) n ].\nTaking into account the overlapping time point between segments, these trajectories can then all be concatenated to produce the trajectory prediction:\n[u (0) 0 , . . . ,u (0) n\u22121,u (1) 0 , . . . ,u (m\u22121) n ] = [u0, . . .un].\nFrom this, the data loss is defined identically to conventional NeuralODE training: Ldata(\u03b8) = 1 N+1 \u2211 i |ui(\u03b8)\u2212 u\u0302i| 2.\nHowever, due to the segmented manner in which the above trajectory is generated, optimizing only over this quantity will result in a model that could generate good piecewise predictions, but be unable to generate a proper, continuous global trajectory. Therefore, to ensure the model can generate smooth trajectories, continuity constraints u(i\u22121)n = u (i) 0 , (i \u2208 1 . . .m\u2212 1) must be enforced. How this is achieved is the point of difference in the literature, and our implementation - following that of [39] - introduces a regularization term in the loss:\nLcontinuity(\u03b8) = 1\nm \u2211 i \u2223\u2223\u2223u(i\u22121)n \u2212 u(i)0 \u2223\u2223\u22232 . Finally, the train loss for the multiple shooting method is then defined as\nL(\u03b8, \u03b2) = Ldata(\u03b8) + \u03b2 \u00b7 Lcontinuity(\u03b8)\nand is minimized using gradient descent, where \u03b2 is a hyperparameter that tunes the relative importance of the two terms during training."
        },
        {
            "heading": "E Experiment details",
            "text": "In all experiments, the AdamW optimizer [23] was used to minimize the respective loss functions for the vanilla and homotopy training. Each experiment was repeated using random seed values of 10, 20, and 30 to compute the prediction means and standard errors.\nE.1 Lotka-Volterra system\nThe Lotka-Volterra system is a simplified model of predator-prey dynamics given by,\ndx dt = \u03b1x\u2212 \u03b2xy, dy dt = \u2212\u03b3y + \u03b4xy (9)\nwhere the parameters \u03b1, \u03b2, \u03b3, \u03b4 characterize interactions between the populations.\nData preparation. Following the experimental design of Rackauckas [40], we numerically integrate Equation (9) in the time interval t \u2208 [0, 6.1], \u2206t = 0.1 with the parameter values \u03b1, \u03b2, \u03b3, \u03b4 = 1.3, 0.9, 0.8, 1.8 and initial conditions x(0), y(0) = 0.44249296, 4.6280594. Continuing with the recipe, Gaussian random noise with zero mean and standard deviations with magnitude of 5% of the mean of each trajectory was added to both states. For both data generation and NeuralODE prediction, integration was performed using the adaptive step size dopri5 solver from the torchdiffeq package [7] with an absolute tolerance of 1e-9 and a relative tolerance of 1e-7.\nFor the experiments of Figure 3, training data were generated in a similar manner, but with specific parameters varied to match the corresponding experiments. Data varying train data length (Figure 3, left panel) were generated using time spans of t \u2208 [0, 3.1], [0, 6.1], [0.9.1] respectively with shared parameter values of \u2206t = 0.1 and noise amplitude of 5% of the mean. Data with differing sampling periods (Figure 3, third panel) were generated with a fixed time span of t \u2208 [0, 6.1], noise amplitude of 5% of the mean and sampling periods of \u2206t = 0.1, 0.3, 0.5. Data with different noise amplitudes (Figure 3, fourth panel) were generated using a time span of t \u2208 [0, 6.1], sampling period of \u2206t = 0.1 and noise amplitudes of 5%, 10%, 20%, 50% of the mean.\nModel architecture. We used two types of models for this dataset, a black-box NeuralODE of Equation (1), and a gray-box NeuralODE used in Rackauckas et al. [39] that incorporates partial information about the underlying equation, given by:\ndx dt = \u03b1x+ U1(x, y;\u03b81), dy dt = \u2212\u03b3y + U2(x, y;\u03b82). (10)\nIn our default setting, the black-box NeuralODE had 3 layers with [2, 32, 2] nodes respectively. The gray-box NeuralODE had 4 layers with [2, 20, 20, 2] nodes, with each node in the output layer corresponding to the output of U1, U2 of Equation (10). Following the results from [21], a nonsaturating gelu activation was used for all layers except for the final layer, where identity activation was used. For the model capacity experiment (Figure 3, second panel), the number of nodes in the hidden layer was changed accordingly.\nE.2 Double pendulum\nThe double pendulum system is a canonical example in classical mechanics, and has four degrees of freedom \u03b81, \u03b82, \u03c91, \u03c92 corresponding to the angles and angular velocities of the two pendulums with respect to the vertical. The governing equation for the system can be derived using Lagrangian formulation of classical mechanics and is given by: d\u03b81 dt = \u03c91, d\u03b82 dt = \u03c92 (11)\nd\u03c91 dt = m2l1\u03c9\n2 1 sin\u2206\u03b8 cos\u2206\u03b8 +m2l2\u03c9 2 2 sin\u2206\u03b8 +m2g sin \u03b82 cos\u2206\u03b8 \u2212 (m1 +m2)g sin \u03b81\n(m1 +m2)l1 \u2212m2l1 cos2 \u2206\u03b8 ,\n(12)\nd\u03c92 dt = \u2212m2l2\u03c9 2 2 sin\u2206\u03b8 cos\u2206\u03b8 + (m1 +m2)(\u2212l1\u03c921 sin\u2206\u03b8 + g sin \u03b81 cos\u2206\u03b8 \u2212 g sin \u03b82) (m1 +m2)l2 \u2212m2l2 cos2 \u2206\u03b8 (13) where \u2206\u03b8 = \u03b82 \u2212 \u03b81, m1,m2 are the masses of each rod, and g is the gravitational acceleration.\nData preparation. While simulated trajectories for the double pendulum can be generated using the equations above, we instead used the experimental data from Schmidt & Lipson [44]. This consists of two trajectories from the double pendulum, captured using multiple cameras. The noise in the data is subdued due to the LOESS smoothing performed by the original authors. For our experiments, we used the first 100 points of the first trajectory for training and the next 100 to evaluate the extrapolation capabilities of the trained model.\nModel architecture. Two types of models were used for this dataset: a black-box NeuralODE (Equation (1)) and a NeuralODE with secondorder structure[16], which for this system, takes the form:\nd\u03b8i dt = \u03c9i, d\u03c9i dt = Ui(\u03b81, \u03b82, \u03c91, \u03c92;\u03b8i); i = 1, 2.\nThe black-box NeuralODE had 4 layers with [4, 50, 50, 4] nodes, with the input and output node numbers corresponding to the degrees of freedom of the system. The second-order model also had 4 layers with [4, 50, 50, 2] nodes. Note that there are now 2 output nodes instead of 4 since incorporating second-order structure requires the neural network to only model the derivatives of \u03c91 and \u03c92. For both models, gelu activations were used for all layers except the last, which used identity activation. Identical to the previous dataset, the NeuralODEs were integrated using the dopri5 solver from the torchdiffeq package [7] with an absolute tolerance of 1e-9 and a relative tolerance of 1e-7.\nE.3 Lorenz system\nThe Lorenz system is given by the equations dx\ndt = \u03c3(y \u2212 x), dy dt = x(\u03c1\u2212 z)\u2212 y, dz dt = xy \u2212 \u03b2z. (14)\nData preparation. To generate the data, we followed experimental settings of Vyasarayani et al. [51] and used parameter values \u03c3, \u03c1, \u03b2 = 10, 28, 8/3 and the initial condition x0, y0, z0 = 1.2, 2.1, 1.7. These conditions, upon integration, gives rise to the well-known \u201cbutterfly attractor\". The training data was generated in the interval t \u2208 [0, 3.1], and still adhering to the paper, a Gaussian noise of mean 0 and standard deviation 0.25 was added to the simulated trajectories to emulate experimental noise. For data generation and NeuralODE prediction, the adaptive step size dopri5 solver from the torchdiffeq package [7] was used with an absolute tolerance of 1e-9 and a relative tolerance of 1e-7.ODE solver and the tolerance values were kept identical to the previous Lotka-Volterra experiment.\nModel architecture. We used a single NeuralODE for this dataset - a black-box model having 4 layers with [3, 50, 50, 3] nodes. The number of nodes for the input and output layers correspond to the three degrees of freedom of the state vector u = [x, y, z]T of the system. The activations for the model was kept identical to the previous experiments - gelu activation for all layers, except for the last layer which used identity activation."
        },
        {
            "heading": "F Hyperparameter selection",
            "text": "F.1 Overview of the hyperparameters\nOur final implementation of the homotopy training algorithm has five hyperparameters. Here, we briefly describe the effects and the tips for tuning each.\n\u2022 Number of homotopy steps (s) : This determines how many relaxed problem the optimization process will pass through to get to the final solution. Similar to scheduling the temperature in simulated annealing, fewer steps results in the model becoming stuck in a sharp local minima, and too many steps makes the optimization process unnecessarily long. We find using values in the range of 6-8 or slightly larger values for more complex systems yields satisfactory results.\n\u2022 Epochs per homotopy steps (nepoch) : This determines how long the model will train on a given homotopy parameter value \u03bb. Too small, and the model lacks the time to properly converge on the loss function; too large, and the model overfits on the simpler loss landscape of \u03bb \u0338= 0, resulting in a reduced final performance when \u03bb = 0. We find for simpler monotonic or periodic systems, values of 100-150 work well; for more irregular systems, 200-300 are suitable.\n\u2022 Coupling strength (k) : This determines how trivial the auxillary function for the homotopy optimization will be. Too small, and even the auxillary function will have a jagged landscape; too large, and the initial auxillary function will become flat (Figures 2 and 11, left panel, k = 1.0, 1.5) resulting in very slow parameter updates. We find good choices of k tend to be comparable to the scale of the measurement values.\n\u2022 Homotopy parameter decrement ratio (\u03ba) : This determines how the homotopy parameter \u03bb is decremented for each step. Values close to 1 cause \u03bb to decrease in nearly equal decrements, whereas smaller values cause a large decrease of \u03bb in the earlier parts of the training, followed by subtler decrements later on. We empirically find that \u03ba values near 0.6 tends to work well.\n\u2022 Learning rate (\u03b7) : This is as same as in conventional NeuralODE training. We found values in the range of 0.002-0.1 to be adequate for our experiments.\nF.2 Details on hyperparameter sweep & selected values\nFor all combinations of models and datasets used, hyperparameters for the optimization were chosen by running sweeps prior to the experiment with a fixed random seed of 10, then selecting hyperparameter values that resulted in the lowest mean squared error value during training.\nNote that for the experiment of Figure 3, the same hyperparameters used in Figure 4 was used for all experiments, due to the sheer cost of sweeping for the hyperparameters for every change of independent variables.\nVanilla gradient descent. Vanilla gradient descent has learning rate as its only hyperparameter. Due to finite computation budget, we set the maximum training epochs to 4000 for all sweeps and experiments. We list the values used in the hyperparameter sweep as well as the final selected values for each dataset in Table 1. We found that larger learning rates lead to numerical underflow in the adaptive ODE solver while training on the more difficult Lorenz and double datasets; hence, the sweep values for the learning rate parameters were taken to be lower than those for the Lotka-Volterra dataset.\nMultiple shooting. The multiple shooting method has three hyperparameters: learning rate, number of segments, and continuity penalty. For the majority of experiments, we set number of segments to 5, and performed hyperparameter sweep on the remaining two parameters.\nHomotopy optimization. Our homotopy-based NeuralODE training has five hyperparameters: learning rate (\u03b7), coupling strength (k), number of homotopy steps (s), epochs per homotopy steps (nepoch), and homotopy parameter decrement ratio (\u03ba).\nWhile sweeping over all five hyperparameters would yield the best possible results, searching such a high dimensional space can pose a computation burden. In practice, we found that sweeping over only the first two hyperparameters and fixing the rest at predecided values still returned models that functioned much better than their vanilla counterparts. We list the predecided hyperparameter values for each dataset in Table 3. Note that we increased the epochs per homotopy step from 100 the Lotka-Volterra model to 300 for the Lorenz system and double pendulum datasets to account for the increased difficulty of the problem.\nTable 4 shows the sweeped hyperparameters as well as the final chosen values for each dataset. The total number of training epochs is given by multiplying the number of homotopy steps with epochs per homotopy steps. This resulted in 600 epochs for the Lotka-Volterra dataset, and 1800 epochs for both the Lorenz system and the double pendulum datasets."
        },
        {
            "heading": "G Further results",
            "text": "G.1 Table of the benchmark results\nHere, we present the benchmark results of Figure 4 in a table format as an alternative data representation.\nG.2 Example of the training curves\nHere, we include some of the training curves for our experiments. Figure 13 displays the averaged training curves for the black-box model trained on the Lotka-Volterra dataset. From the right panel, it can clearly be seen that our homotopy method optimizes the MSE much rapidly than the other methods, arriving at the noise floor in less than 500 epochs. The abrupt jumps in the MSE curve for homotopy is due to the discontinous change in the train loss that occurs every the the homotopy parameter is adjusted. To make this connection clearer, we show the train loss as well as the homotopy parameter for our method in the left panel of the same figure.\nWe present analogous results for the second-order model trained on the double pendulum dataset in Figure 14. Once again, we find that our homotopy method converges much quicker than its competitors.\nG.3 Further discussion on the Lotka-Volterra system results\nIn this section, we present additional results regarding our experiment of Figure 3 and continue our discussion.\nIncreasing data length. Figure 15 depicts the predicted trajectories for differently trained models. For the shortest data, it can be seen that all models did overfit on the training data and failed to capture the periodic nature of the system. This justifies our attributing the large extrapolation error of the models in Figure 3 to overfitting. However, we emphasize that this overfitting is not due to failings in the training algorithms, but rather due to the insufficient information in the training data, as the models cannot be expected to learn periodicity without being given at least a single period worth of data. As the data is increased, we find that both homotopy and multiple shooting properly capture the dynamics of the system, whereas the vanilla method was unable to properly learn the system for the longest data.\nReducing model capacity. In Figure 16, we show the prediction trajectories corresponding to the model capacity experiment. It is clear from the results that both the homotopy and multiple shooting methods can produce models that accurately portray the dynamics, regardless of the reduction in the model size. In contrast, predictions from vanilla training deteriorate as the model size decreases, with the smallest model inaccurately estimating both the amplitude and the phase of the oscillations.\nIncreasing sampling period. The model trajectories for decreasing sampling period in the training data is shown below in Figure 17. Intriguingly, we find that the multiple shooting method results suffer greatly with the increased data sparsity. This is a side effect of our experiment setting, where we set the time interval constant while increasing the sampling period. To elaborate, our choice of fixed time interval causes the number of training data points decrease as the period increases. However, as\nmultiple shooting trains by subdividing the training data, it struggles on small datasets because this leads to each segment containing even less data points that do not convey much information about the data dynamics.\nAnother interesting observation that can be made is that vanilla training gives better results when sparser data is used, which is also confirmed by the error values in the third panel of Figure 3. This suggests an alternate training improvement strategy for training NeuralODEs on long time series data - by training on subsampled data, then if necessary, gradually anneal the data intervals back to its original form as training proceeds. Of course, the effectiveness of such a scheme will need to be tested for more complex systems, which is outside the scope of this paper.\nAs our homotopy training method utilized a cubic smoothing spline to supply the coupling term, we also inspected the quality of this interpolant as the data period was increased. From Figure 18, we find that the cubic spline relatively resistant to the increased data sparsity, which may be one of the reasons why our homotopy method is very robust against this mode of data degradation.\nIncreasing data noise. Finally, Figure 19 shows the model trajectories as the noise in the training data is increased. Here, we do find that while all methods return deteriorating predictions as the noise is increased, our homotopy method is found to be most robust to noise, followed by multiple shooting, and finally vanilla gradient descent. A part of our algorithm\u2019s robustness to noise can be explained by the use of the cubic smoothing splines. As our use of synchronization couples the NeuralODE dynamics to the cubic spline trajectory during training, one could argue that the denoising effect of cubic splines is thus directly transferred to the model.\nHowever, this does not seem to be the full picture. Figure 20 shows the cubic smoothing splines used during modeling training for each of the noise levels. As we held the amount of smoothing constant throughout our experiments, we see that for larger noise amplitudes, the spline fails to reject all noise, and displays irregular high frequency behaviors. On the other hand, the corresponding trained trajectories for the homotopy method (Figure 19, third and fourth panels) do not mirror such irregular oscillations, indicating that the homotopy optimization procedure itself also has an intrinsic robustness to noise.\nG.4 Comparison to a non-deep learning algorithm: SINDy\nIn this section, we briefly compare our algorithm to a more traditional symbolic regression-based method. We choose the well-known SINDy algorithm, which is readily available in the pysindy package. This algorithm takes as input, time series measurements of the states of the dynamical system in question, as well as a dictionary of candidate terms that could constitute the governing equation. Afterwards, the time derivative of the states is estimated numerically, and a sparse regression is performed to determine which terms exist in the governing equation, as well as what their coefficient values are.\nCompared to neural network-based approaches, the unique feature of SINDy is that its results are given in the form of analytical expressions. Therefore if the governing equation for the data is simple (e.g. consists of low-order polynomial terms), or if the user has sufficient prior information about the system and can construct a very small dictionary that contains all the terms that appear in the true equation, SINDy can accurately recover the ground truth equation.\nOn the other hand, if the system is high dimensional so that the space of possible candidate terms start to grow drastically, or if the governing equation has a arbitrary, complex form that cannot be easily guessed, SINDy either does not fit the data properly, or even if it does, recovers inaccurate equations that are difficult to assign meaning to. These correspond to situations where NeuralODEs, and hence our method for effectively training them, shines.\nTo illustrate this point, we choose the double pendulum dataset and compare the predicted trajectories from SINDy and our homotopy algorithm. As one can see from Equation (13), the governing equation of this system has an extremely complicated form that cannot be guessed easily. To reflect this difficulty, we chose the basis set of the dictionary of the candidate terms to be the state variable themselves, their sines and cosines, and the higher harmonics of the sines and cosines.\nFrom Figure 21, we find that SINDy is able to fit the data to a modest accuracy. While this may seem weak compared to the result from our homotopy method, the accuracy of the SInDy prediction is likely to improve if the candidate dictionary is updated to better reflect the actual form of Equation (13). However, this amounts to supplying the algorithm with excessive amounts of prior information about the system, which is rarely available in practice. This is also constrasted with NeuralODE training with our homotopy method - which does not require any additional information about the system to produce the results above.\nG.5 Changing the gradient calculation scheme\nAs we commented in Section 3, methods that focus on other aspects of NeuralODE training, such as alternate gradient calculation schemes, are fully compatible with our homotopy training method. To demonstrate this point, we used the symplectic-adjoint method [25] provided in the torch-symplectic-adjoint library as a substitute for the direct backpropagation used for gradient calculation in our experiments.\nComparing the above results to our benchmark results in Figure 4 and Table 5, we see that the overall results remain the same - our homotopy method is able to train the models effectively, in much small number of epochs."
        }
    ],
    "title": "Homotopy-based training of NeuralODEs for accurate dynamics discovery",
    "year": 2023
}