{
    "abstractText": "Humans and animals have a rich and flexible understanding of the physical world, which enables them to infer the underlying dynamical trajectories of objects and events, plausible future states, and use that to plan and anticipate the consequences of actions. However, the neural mechanisms underlying these computations are unclear. We combine a goal-driven modeling approach with dense neurophysiological data and high-throughput human behavioral readouts that contain thousands of comparisons to directly impinge on this question. Specifically, we construct and evaluate several classes of sensory-cognitive networks to predict the future state of rich, ethologically-relevant environments, ranging from self-supervised end-to-end models with pixel-wise or object-slot objectives, to models that future predict in the latent space of purely static image-pretrained or dynamic videopretrained foundation models. We find that \u201cscale is not all you need\u201d, and that many state-of-the-art machine learning models fail to perform well on our neural and behavioral benchmarks for future prediction. In fact, only one class of models matches these data well overall. We find that neural responses are currently best predicted by models trained to predict the future state of their environment in the latent space of pretrained foundation models optimized for dynamic scenes in a self-supervised manner. These models also approach the neurons\u2019 ability to predict the environmental state variables that are visually hidden from view, despite not being explicitly trained to do so. Finally, we find that not all foundation model latents are equal. Notably, models that future predict in the latent space of video foundation models that are optimized to support a diverse range of egocentric sensorimotor tasks, reasonably match both human behavioral error patterns and neural dynamics across all environmental scenarios that we were able to test. Overall, these findings suggest that the neural mechanisms and behaviors of primate mental simulation have strong inductive biases associated with them, and are thus far most consistent with being optimized to future predict on reusable visual representations that are useful for Embodied AI more generally.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aran Nayebi"
        },
        {
            "affiliations": [],
            "name": "Rishi Rajalingham"
        },
        {
            "affiliations": [],
            "name": "Mehrdad Jazayeri"
        },
        {
            "affiliations": [],
            "name": "Guangyu Robert Yang"
        }
    ],
    "id": "SP:90d95e3a39487fa8bf6754dcea03a4d52ca8e54f",
    "references": [
        {
            "authors": [
                "M. Babaeizadeh",
                "M.T. Saffar",
                "S. Nair",
                "S. Levine",
                "C. Finn",
                "D. Erhan"
            ],
            "title": "Fitvid: Overfitting in pixel-level video prediction",
            "venue": "arXiv preprint arXiv:2106.13195,",
            "year": 2021
        },
        {
            "authors": [
                "R. Baillargeon",
                "E.S. Spelke",
                "S. Wasserman"
            ],
            "title": "Object permanence in five-month-old infants",
            "year": 1985
        },
        {
            "authors": [
                "A. Bakhtin",
                "L. van der Maaten",
                "J. Johnson",
                "L. Gustafson",
                "R. Girshick"
            ],
            "title": "Phyre: A new benchmark for physical reasoning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "R. Balestriero",
                "M. Ibrahim",
                "V. Sobal",
                "A. Morcos",
                "S. Shekhar",
                "T. Goldstein",
                "F. Bordes",
                "A. Bardes",
                "G. Mialon",
                "Y. Tian"
            ],
            "title": "A cookbook of self-supervised learning",
            "venue": "arXiv preprint arXiv:2304.12210,",
            "year": 2023
        },
        {
            "authors": [
                "P. Battaglia",
                "R. Pascanu",
                "M. Lai",
                "D. Jimenez Rezende"
            ],
            "title": "Interaction networks for learning about objects, relations and physics",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "P.W. Battaglia",
                "J.B. Hamrick",
                "J.B. Tenenbaum"
            ],
            "title": "Simulation as an engine of physical scene understanding",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "P.W. Battaglia",
                "J.B. Hamrick",
                "V. Bapst",
                "A. Sanchez-Gonzalez",
                "V. Zambaldi",
                "M. Malinowski",
                "A. Tacchetti",
                "D. Raposo",
                "A. Santoro",
                "R. Faulkner"
            ],
            "title": "Relational inductive biases, deep learning, and graph networks",
            "venue": "arXiv preprint arXiv:1806.01261,",
            "year": 2018
        },
        {
            "authors": [
                "D. Bear",
                "E. Wang",
                "D. Mrowca",
                "F.J. Binder",
                "H.-Y. Tung",
                "R. Pramod",
                "C. Holdaway",
                "S. Tao",
                "K.A. Smith",
                "F.-Y. Sun"
            ],
            "title": "Physion: Evaluating physical prediction from vision in humans and machines",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "D.M. Bear",
                "K. Feigelis",
                "H. Chen",
                "W. Lee",
                "R. Venkatesh",
                "K. Kotar",
                "A. Durango",
                "D.L. Yamins"
            ],
            "title": "Unifying (machine) vision via counterfactual world modeling",
            "venue": "arXiv preprint arXiv:2306.01828,",
            "year": 2023
        },
        {
            "authors": [
                "R. Cao",
                "D. Yamins"
            ],
            "title": "Explanatory models in neuroscience: Part 1\u2013taking mechanistic abstraction seriously",
            "venue": "arXiv preprint arXiv:2104.01490,",
            "year": 2021
        },
        {
            "authors": [
                "M. Caron",
                "H. Touvron",
                "I. Misra",
                "H. J\u00e9gou",
                "J. Mairal",
                "P. Bojanowski",
                "A. Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "J. Carreira",
                "E. Noland",
                "C. Hillier",
                "A. Zisserman"
            ],
            "title": "A short note on the kinetics-700 human action dataset",
            "year": 1907
        },
        {
            "authors": [
                "L.A. Cooperau",
                "R.N. Shepard"
            ],
            "title": "The time required to prepare for a rotated stimulus",
            "venue": "Memory & Cognition,",
            "year": 1973
        },
        {
            "authors": [
                "K.J.W. Craik"
            ],
            "title": "The nature of explanation, volume 445",
            "venue": "CUP Archive,",
            "year": 1943
        },
        {
            "authors": [
                "E.D. Cubuk",
                "B. Zoph",
                "J. Shlens",
                "Q.V. Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2020
        },
        {
            "authors": [
                "S. Dasari",
                "F. Ebert",
                "S. Tian",
                "S. Nair",
                "B. Bucher",
                "K. Schmeckpeper",
                "S. Singh",
                "S. Levine",
                "C. Finn"
            ],
            "title": "Robonet: Large-scale multi-robot learning",
            "venue": "Conference on Robot Learning (CoRL), arXiv preprint arXiv:1910.11215,",
            "year": 2019
        },
        {
            "authors": [
                "E. Denton",
                "R. Fergus"
            ],
            "title": "Stochastic video generation with a learned prior",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "A. Didolkar",
                "A. Goyal",
                "Y. Bengio"
            ],
            "title": "Cycle consistency driven object discovery",
            "venue": "arXiv preprint arXiv:2306.02204,",
            "year": 2023
        },
        {
            "authors": [
                "A. Dosovitskiy",
                "L. Beyer",
                "A. Kolesnikov",
                "D. Weissenborn",
                "X. Zhai",
                "T. Unterthiner",
                "M. Dehghani",
                "M. Minderer",
                "G. Heigold",
                "S. Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "International Conference on Learning Representations (ICLR), arXiv preprint arXiv:2010.11929,",
            "year": 2021
        },
        {
            "authors": [
                "J. Fischer",
                "J.G. Mikhael",
                "J.B. Tenenbaum",
                "N. Kanwisher"
            ],
            "title": "Functional neuroanatomy of intuitive physical inference",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2016
        },
        {
            "authors": [
                "C. Gan",
                "J. Schwartz",
                "S. Alter",
                "D. Mrowca",
                "M. Schrimpf",
                "J. Traer",
                "J. De Freitas",
                "J. Kubilius",
                "A. Bhandwaldar",
                "N. Haber"
            ],
            "title": "Threedworld: A platform for interactive multi-modal physical simulation",
            "venue": "Advanced in Neural Information Processing Systems (NeurIPS)",
            "year": 2021
        },
        {
            "authors": [
                "P. Goyal",
                "P. Doll\u00e1r",
                "R. Girshick",
                "P. Noordhuis",
                "L. Wesolowski",
                "A. Kyrola",
                "A. Tulloch",
                "Y. Jia",
                "K. He"
            ],
            "title": "Accurate, large minibatch sgd: Training imagenet in 1 hour",
            "venue": "arXiv preprint arXiv:1706.02677,",
            "year": 2017
        },
        {
            "authors": [
                "K. Grauman",
                "A. Westbury",
                "E. Byrne",
                "Z. Chavis",
                "A. Furnari",
                "R. Girdhar",
                "J. Hamburger",
                "H. Jiang",
                "M. Liu",
                "X. Liu"
            ],
            "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "O. Groth",
                "F.B. Fuchs",
                "I. Posner",
                "A. Vedaldi"
            ],
            "title": "Shapestacks: Learning vision-based physical intuition for generalised object stacking",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "J.B. Hamrick",
                "P.W. Battaglia",
                "T.L. Griffiths",
                "J.B. Tenenbaum"
            ],
            "title": "Inferring mass in complex scenes by mental",
            "venue": "simulation. Cognition,",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Zhang",
                "S. Ren",
                "J. Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "K. He",
                "X. Chen",
                "S. Xie",
                "Y. Li",
                "P. Doll\u00e1r",
                "R. Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "S. Hochreiter",
                "J. Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "H. Hong",
                "D.L. Yamins",
                "N.J. Majaj",
                "J.J. DiCarlo"
            ],
            "title": "Explicit information for category-orthogonal object properties increases along the ventral stream",
            "venue": "Nature neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "D.P. Kingma",
                "J. Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations (ICLR), arXiv preprint arXiv:1412.6980,",
            "year": 2015
        },
        {
            "authors": [
                "T. Kipf",
                "E. Van der Pol",
                "M. Welling"
            ],
            "title": "Contrastive learning of structured world models",
            "venue": "International Conference on Learning Representations (ICLR), arXiv preprint arXiv:1911.12247,",
            "year": 2020
        },
        {
            "authors": [
                "W. Li",
                "S. Azimi",
                "A. Leonardis",
                "M. Fritz"
            ],
            "title": "To fall or not to fall: A visual approach to physical stability prediction",
            "venue": "arXiv preprint arXiv:1604.00066,",
            "year": 2016
        },
        {
            "authors": [
                "Y. Li",
                "J. Wu",
                "R. Tedrake",
                "J.B. Tenenbaum",
                "A. Torralba"
            ],
            "title": "Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids",
            "venue": "International Conference on Learning Representations (ICLR), arXiv preprint arXiv:1810.01566,",
            "year": 2019
        },
        {
            "authors": [
                "Y.J. Ma",
                "S. Sodhani",
                "D. Jayaraman",
                "O. Bastani",
                "V. Kumar",
                "A. Zhang"
            ],
            "title": "Vip: Towards universal visual reward and representation via value-implicit pre-training",
            "venue": "International Conference on Learning Representations (ICLR), arXiv preprint arXiv:2210.00030,",
            "year": 2023
        },
        {
            "authors": [
                "A. Majumdar",
                "K. Yadav",
                "S. Arnaud",
                "Y.J. Ma",
                "C. Chen",
                "S. Silwal",
                "A. Jain",
                "V.-P. Berges",
                "P. Abbeel",
                "J. Malik"
            ],
            "title": "Where are we in the search for an artificial visual cortex for embodied intelligence",
            "venue": "arXiv preprint arXiv:2303.18240,",
            "year": 2023
        },
        {
            "authors": [
                "K.D. Miller",
                "F. Fumarola"
            ],
            "title": "Mathematical equivalence of two common forms of firing rate models of neural networks",
            "venue": "Neural computation,",
            "year": 2012
        },
        {
            "authors": [
                "S. Nair",
                "A. Rajeswaran",
                "V. Kumar",
                "C. Finn",
                "A. Gupta"
            ],
            "title": "R3m: A universal visual representation for robot manipulation",
            "venue": "6th Annual Conference on Robot Learning (CoRL),",
            "year": 2022
        },
        {
            "authors": [
                "C. Nash",
                "J. Carreira",
                "J. Walker",
                "I. Barr",
                "A. Jaegle",
                "M. Malinowski",
                "P. Battaglia"
            ],
            "title": "Transframer: Arbitrary frame prediction with generative models",
            "venue": "arXiv preprint arXiv:2203.09494,",
            "year": 2022
        },
        {
            "authors": [
                "A. Nayebi",
                "A. Attinger",
                "M. Campbell",
                "K. Hardcastle",
                "I. Low",
                "C.S. Mallory",
                "G. Mel",
                "B. Sorscher",
                "A.H. Williams",
                "S. Ganguli"
            ],
            "title": "Explaining heterogeneity in medial entorhinal cortex with task-driven neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "M. Oquab",
                "T. Darcet",
                "T. Moutakanni",
                "H. Vo",
                "M. Szafraniec",
                "V. Khalidov",
                "P. Fernandez",
                "D. Haziza",
                "F. Massa",
                "A. ElNouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "A. Paszke",
                "S. Gross",
                "F. Massa",
                "A. Lerer",
                "J. Bradbury",
                "G. Chanan",
                "T. Killeen",
                "Z. Lin",
                "N. Gimelshein",
                "L. Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "R. Pramod",
                "M.A. Cohen",
                "J.B. Tenenbaum",
                "N. Kanwisher"
            ],
            "title": "Invariant representation of physical stability in the human",
            "venue": "brain. Elife,",
            "year": 2022
        },
        {
            "authors": [
                "A. Radford",
                "J.W. Kim",
                "C. Hallacy",
                "A. Ramesh",
                "G. Goh",
                "S. Agarwal",
                "G. Sastry",
                "A. Askell",
                "P. Mishkin",
                "J. Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "R. Rajalingham",
                "A. Piccato",
                "M. Jazayeri"
            ],
            "title": "Recurrent neural networks with explicit representation of dynamic latent variables can mimic behavioral patterns in a physical inference task",
            "venue": "Nature Communications,",
            "year": 2022
        },
        {
            "authors": [
                "R. Rajalingham",
                "H. Sohn",
                "M. Jazayeri"
            ],
            "title": "Dynamic tracking of objects in the macaque dorsomedial frontal cortex",
            "venue": "bioRxiv, pages 2022\u201306,",
            "year": 2022
        },
        {
            "authors": [
                "A. Sanchez-Gonzalez",
                "J. Godwin",
                "T. Pfaff",
                "R. Ying",
                "J. Leskovec",
                "P. Battaglia"
            ],
            "title": "Learning to simulate complex physics with graph networks",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "M. Sarafyazd",
                "M. Jazayeri"
            ],
            "title": "Hierarchical reasoning by neural circuits in the frontal cortex",
            "venue": "Science, 364",
            "year": 2019
        },
        {
            "authors": [
                "S. Schwettmann",
                "J.B. Tenenbaum",
                "N. Kanwisher"
            ],
            "title": "Invariant representations of mass in the human brain",
            "venue": "Elife, 8:e46619,",
            "year": 2019
        },
        {
            "authors": [
                "R.N. Shepard",
                "J. Metzler"
            ],
            "title": "Mental rotation of three-dimensional objects",
            "venue": "Science, 171(3972):701\u2013703,",
            "year": 1971
        },
        {
            "authors": [
                "K. Simonyan",
                "A. Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "K.A. Smith",
                "E. Vul"
            ],
            "title": "Sources of uncertainty in intuitive physics",
            "venue": "Topics in cognitive science,",
            "year": 2013
        },
        {
            "authors": [
                "E.S. Spelke"
            ],
            "title": "Principles of object perception",
            "venue": "Cognitive science,",
            "year": 1990
        },
        {
            "authors": [
                "H. Touvron",
                "M. Cord",
                "M. Douze",
                "F. Massa",
                "A. Sablayrolles",
                "H. Jegou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "M. Traub",
                "S. Otte",
                "T. Menge",
                "M. Karlbauer",
                "J. Thuemmel",
                "M.V. Butz"
            ],
            "title": "Learning what and where: Disentangling location and identity tracking without supervision",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "T.D. Ullman",
                "E. Spelke",
                "P. Battaglia",
                "J.B. Tenenbaum"
            ],
            "title": "Mind games: Game engines as an architecture for intuitive physics",
            "venue": "Trends in cognitive sciences,",
            "year": 2017
        },
        {
            "authors": [
                "R. Villegas",
                "A. Pathak",
                "H. Kannan",
                "D. Erhan",
                "Q.V. Le",
                "H. Lee"
            ],
            "title": "High fidelity video prediction with large stochastic recurrent neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "B. Wu",
                "S. Nair",
                "R. Martin-Martin",
                "L. Fei-Fei",
                "C. Finn"
            ],
            "title": "Greedy hierarchical variational autoencoders for large-scale video prediction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "D.L. Yamins",
                "H. Hong",
                "C.F. Cadieu",
                "E.A. Solomon",
                "D. Seibert",
                "J.J. DiCarlo"
            ],
            "title": "Performance-optimized hierarchical models predict neural responses in higher visual cortex",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2014
        },
        {
            "authors": [
                "J. Yuan",
                "T. Chen",
                "B. Li",
                "X. Xue"
            ],
            "title": "Compositional scene representation learning via reconstruction: A survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "J.M. Zacks"
            ],
            "title": "Neuroimaging studies of mental rotation: a meta-analysis and review",
            "venue": "Journal of cognitive neuroscience,",
            "year": 2008
        },
        {
            "authors": [
                "A. Zador",
                "S. Escola",
                "B. Richards",
                "B. \u00d6lveczky",
                "Y. Bengio",
                "K. Boahen",
                "M. Botvinick",
                "D. Chklovskii",
                "A. Churchland",
                "C. Clopath"
            ],
            "title": "Catalyzing next-generation artificial intelligence through neuroai",
            "venue": "Nature Communications,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Within the span of a couple seconds, we are able to draw rich inferences and make predictions about novel scenes [Smith and Vul, 2013, Battaglia et al., 2013]. A dominant cognitive theory has been that the brain builds mental models of the physical world, using those models to make inferences about the\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\nar X\niv :2\n30 5.\n11 77\n2v 2\n[ cs\n.A I]\n2 5\nfuture state of its environment [Craik, 1943]. In the past decade, this hypothesis has been supported by comparisons of human behavior to computational models which predict what will happen next in physical scenarios via forward simulations resembling those of game engines in modern video games [Battaglia et al., 2013, Hamrick et al., 2016, Ullman et al., 2017]. Both neuroimaging work in humans [Zacks, 2008, Fischer et al., 2016, Schwettmann et al., 2019, Pramod et al., 2022] and recent electrophysiological work in monkeys [Rajalingham et al., 2022a,b] has further provided evidence for the neurobiological basis of mental simulations in the frontoparietal network (FPN) of primates, a large-scale network consisting of several interacting brain regions. In this work, we make progress towards understanding the neural and behavioral mechanisms of mental simulation by constructing models that perform this behavior in rich, naturalistic environments. Specifically, we aim to determine what inductive biases (in the form of a loss function, architecture class, and pretraining environment) enable the brain to generally perform mental simulation across a range of environments and scenarios, from unstructured, continuous sensory inputs. In particular, we assess not only model generalization to novel, high-variation examples within the same environment, but also structural generalization to new environments and scenarios altogether.\nPredicting the physical dynamics of environments is also critical to progress in Embodied AI. One common paradigm for learning these dynamics has been as a next frame prediction problem via a pixel-wise loss [Villegas et al., 2019, Wu et al., 2021, Babaeizadeh et al., 2021, Nash et al., 2022]. These losses emphasize prioritizing accurate prediction of every detail of a given scene\u2019s dynamics. However, fine-grained prediction of upcoming video frames would require near-perfect knowledge of the world\u2019s physical state (akin to Laplace\u2019s Demon), which may explain the observation why many of these models tend to underfit in high variation, naturalistic visual environments, with recent efforts aimed at scaling these methods up primarily by increasing their parameter count [Dasari et al., 2019, Babaeizadeh et al., 2021]. It is therefore unclear how much these resultant learned representations are able to successfully capture general physical understanding.\nAnother recent class of approaches involves the design of visual \u201cfoundation models\u201d [Bommasani et al., 2021], trained on large amounts of webscale images and egocentric videos to develop an implicit representation of the world, that can then be deployed to downstream robotic manipulation tasks [Nair et al., 2022, Ma et al., 2023, Majumdar et al., 2023]. Of course, these models are not directly designed to do explicit physical simulation, but we equip them with a forward dynamics model that can be rolled out for an arbitrary number of timesteps. We ask whether such dynamicallyequipped foundation models have learned physical knowledge by evaluating their generalization both to new scenarios and environments, and whether their representations bear any similarity to humans and non-human primates performing the same tasks?\nIn particular, we find strong constraints on primate mental simulation, especially when examining generalization within and across diverse environments. Our core result is that a small class of models best match primate frontal cortex neural dynamics while the animal plays a ball interception task in which the ball trajectory is partially occluded prior to hitting the paddle (\u201cMental-Pong\u201d), developed previously by Rajalingham et al. [2022a]. Overall, one model class can match both neural response dynamics and human behavioral patterns reasonably well \u2013 namely, dynamics that are optimized to future predict in the latent space of VC-1, a video foundation model pretrained on the largest variety of egocentric sensorimotor settings overall. We therefore currently observe a tight correspondence between the ability to predict fine-grained neural and behavioral responses for the mental simulation phenomenon, and developing useful representations for Embodied AI more generally."
        },
        {
            "heading": "2 Related Work",
            "text": "Mental simulations have been studied at the level of neural activity only very recently. Prior human neuroimaging studies [Zacks, 2008, Fischer et al., 2016, Pramod et al., 2022] in the past decades showed elevated levels of blood-oxygen-level-dependent (BOLD) signal to mental simulation, although they do not have the required resolution to verify that these dynamics are actually represented in underlying neural activity. Rajalingham et al. [2022b] was the first study to show that neural dynamics recorded from macaque dorsomedial frontal cortex (DMFC), track the occluded ball by comparing these dynamics to Recurrent Neural Networks (RNNs) that simulate the occluded ball\u2019s position in Mental-Pong, and finding that they better match these dynamics than RNNs that only perform ball endpoint prediction. However, monkeys can perform these tasks without substantial training, suggesting that they are already equipped with the necessary neural foundations for mental\nsimulation in this environment. Therefore, we aim to also build networks that are not explicitly trained on Mental-Pong itself, but are tasked to generalize to this novel setting as a test of their general understanding of physical scene dynamics \u2013 chiefly developed through three factors: their architecture, optimization objective, and pretraining on a naturalistic environment.\nAdditionally, we constrain our models by evaluating them against high-throughput human behavioral data (from Bear et al. [2021]) in more naturalistic, 3D environments than Mental-Pong alone, which goes beyond prior behavioral studies that either rely on a narrow range of physical scenarios [Shepard and Metzler, 1971, Cooperau and Shepard, 1973], such as block towers with several cubes of different colors [Groth et al., 2018, Li et al., 2016], or 2D environments that may not generalize to the real world [Bakhtin et al., 2019]. A key challenge to addressing these questions is a common standard to evaluating the everyday physical scene understanding and neural predictivity of these models, especially since they are usually trained on vastly different scenarios and input types. Towards this end, we require models to operate under similar constraints as the brain, namely (i) to take in unstructured visual inputs across a range of physical phenomena, (ii) to generate physical predictions for each scene (i.e. producing \u201cbehavioral outputs\u201d), and (iii) to consist of internal units that can be compared to biological units (i.e. containing \u201cartificial neurons\u201d).\nTaken together, these three requirements encompass a large class of functionally reasonable hypotheses that we call \u201csensory-cognitive networks\u201d, and includes the two broad approaches mentioned in \u00a71. However, they do exclude some approaches \u2013 for example, particle-based graph neural network dynamics predictors [Battaglia et al., 2016, 2018, Li et al., 2019, Sanchez-Gonzalez et al., 2020] that take the ground truth simulator state as input (which may not be readily available in real-world situations, failing to satisfy requirement (i)) or probabilistic programs [Battaglia et al., 2013, Hamrick et al., 2016, Ullman et al., 2017] (which fail to satisfy requirements (i) and (iii)).\nNonetheless, we believe these latter approaches from cognitive science are a useful guide for building improved models from pixels that satisfy the three requirements \u2013 especially in terms of assessing whether the prediction problem lies at the level of vision, or the dynamics that interfaces with it. For example, [Bear et al., 2021, Figure 5] demonstrated that particle-based graph neural networks (e.g. DPI-Net [Li et al., 2019]) with access to the ground truth simulator state approach human-level physical predictions on the OCP task that we consider in Section 4 across a diverse range of scenarios. This observation indicates that the onus mainly rests on learning a good visual representation, as these models assume perfect observability of physical dynamics. This further motivates our enforcement of requirement (i) as being a worthwhile challenge, as well as our latent future prediction approach of \u201cfactorizing\u201d the mental simulation problem into separately pretrained vision and dynamics modules."
        },
        {
            "heading": "3 Methods",
            "text": "To tackle this question, we took a hypothesis-driven approach and built sensory-cognitive networks that performed mental simulations of their environment in a variety of different ways (schematized in Figure 1). Specifically, we tasked models to operate on the Physion dataset [Bear et al., 2021], a large-scale video dataset that focuses on everyday physical understanding, consisting of eight different scenarios in a simulated Unity3D-based environment (the ThreeDWorld simulator [Gan et al., 2021]) with roughly 2,000 scenes each. Models are pretrained on all eight scenarios of Dominoes, Support, Collide, Contain, Drop, Link, Roll, and Drape; which together cover many scenes involving rigid and soft bodies.\nWe additionally pretrain a subset of models on Kinetics-700 [Carreira et al., 2019], which consists of over 500,000 training videos from real-world (rather than simulated) scenes from 700 different action categories, in order to assess whether a different dataset of increased scale is beneficial or not, relative to Physion.\nWe consider models from several sensory-cognitive hypothesis classes:\n1. End-to-end self-supervised future prediction: (a) Pixel-wise: This class of models consists of three parts: encoder, dynamics, and\ndecoder; which are altogether pretrained end-to-end with a pixel-wise loss to predict the next frame. We consider a current state-of-the-art model in this model family, FitVid [Babaeizadeh et al., 2021], and we pretrain it on Physion (with and without temporal augmentations such as RandAugment [Cubuk et al., 2020]); along with an\nearlier variant, SVG [Denton and Fergus, 2018], that can be additionally trained at larger image scales (128\u00d7 128 rather than 64\u00d7 64 pixels).\n(b) Object-slot: This class of models is based on the Contrastive Learning of Structured World Models framework (\u201cC-SWM\u201d [Kipf et al., 2020]) which contrastively learns more structured object-slot latent states. It consists of an encoder (whose size we vary: \u201csmall\u201d, \u201cmedium\u201d, and \u201clarge\u201d) that outputs a fixed number of object slots N = 10, and a graph neural network forward dynamics module that operates on this representation. Since the dynamics module itself does not have temporal dependencies (only spatial), we enable temporal dependencies by passing frames in the input channel dimension of the encoder, as a sliding window of temporal dimension of T context frames to predict the object-slot latent at timestep T + 1. This model family is one instantiation of the cognitive theory that humans tend to reason about scenes with regards to objects and their relations [Baillargeon et al., 1985, Spelke, 1990, Spelke and Kinzler, 2007]. These Gestalt principles therefore may provide physical knowledge that are better-positioned to generalize than more fine-grained (pixel-wise) prediction \u2013 a hypothesis that we explicitly evaluate on high-throughput neural and behavioral data, in what follows.\n2. Latent self-supervised future prediction: These models consist of two parts: an encoder E that produces a latent state space h and a dynamics module D that predicts the future state\npurely in the latent space h of E . E is fixed and pretrained to perform a challenging vision task (\u201cfoundation model\u201d), through which it learns a partial, implicit representation of the physical world. However, since E is not pretrained to do explicit physical simulation, we train additional dynamics D on Physion, that can later be \u201crolled out\u201d an arbitrary number of steps. More concretely, given T context frames, E produces the latent sequence h1:T , from which D is trained to predict hT+1. D is relatively simple, being either an LSTM [Hochreiter and Schmidhuber, 1997] or a continuous-time RNN (CTRNN) [Miller and Fumarola, 2012]; or \u201cNo Dynamics\u201d as a control, which always outputs the last context frame latent hT of E . We consider a variety of foundation encoders, divided into two primary classes based on their type of large-scale pretraining dataset:\n(a) Image Foundation Models (Static scenes): \u2022 Standard Convolutional Neural Networks (CNNs) VGG16 [Simonyan and Zis-\nserman, 2014] and ResNet-50 [He et al., 2016] pretrained on ImageNet with a supervised categorization objective.\n\u2022 Vision Transformer (ViT) [Dosovitskiy et al., 2021] based architectures such as DeiT [Touvron et al., 2021] and DINO [Caron et al., 2021] pretrained on ImageNet with a self-supervised objective.\n\u2022 DINOv2 [Oquab et al., 2023], which is a very recent ViT pretrained on a larger curated dataset called LVD-142M (142 million images).\n\u2022 CLIP [Radford et al., 2021], which is a ViT pretrained on 400 million image-text pairs curated from the Internet (250 times larger than ImageNet).\n(b) Video Foundation Models (Dynamic scenes): \u2022 R3M [Nair et al., 2022], which is a ResNet-50 architecture pretrained with a\ntemporally-contrastive video-language alignment objective on 5 million frames from a subset of the recent large-scale Ego4D human video dataset [Grauman et al., 2022].\n\u2022 VIP [Ma et al., 2023], which is a ResNet-50 architecture pretrained with a goalconditioned value function objective on 5 million frames from a subset of Ego4D.\n\u2022 VC-1 [Majumdar et al., 2023], which is a very recent ViT pretrained on 7 different egocentric video sources (over 5.6 million frames, including Ego4D) relevant to sensorimotor skills, using a self-supervised masked autoencoding (MAE) [He et al., 2022] objective. While MAE is pretrained on individual frames, the statistics of the frames are informed by egocentric motion, unlike webscale images.\nIn total, these networks encompass both qualitatively distinct hypotheses (pixel-wise vs. objectslot vs. latent future prediction), alongside several variations within each hypothesis class. This combination of diverse networks allows us to potentially differentiate hypothesis classes across a range of functionally capable instantiations of each hypothesis."
        },
        {
            "heading": "4 Comparison to Human Physical Judgements",
            "text": ""
        },
        {
            "heading": "4.1 OCP task evaluation",
            "text": "In the object contact prediction (OCP) task [Bear et al., 2021], each evaluation scenario involves a red \u201cagent\u201d object and a yellow \u201cpatient\u201d object (which did not appear during model pretraining). Both humans and models are tasked to predict the probability that they will come into contact. This prediction requires understanding of the relevant physical phenomenon in the given scenario, corresponding to a higher-order readout of the underlying scene dynamics.\n4.2 End-to-end pixel-wise future predictors best predict human behavior in the same environment\nThe model comparison results are summarized in Figure 2. In Figure 2A, we examine held-out scene accuracy of models across all eight scenarios in the OCP task, where chance performance is 50%. We can see that humans are quite reliable on this task, attaining 74.04% average held-out accuracy across scenarios (grey horizontal line). Furthermore, the best models that approach human accuracy are models that are pretrained end-to-end with pixel-wise losses (FitVid and SVG). Despite\ntheir differences in architecture, both FitVid and SVG are comparable to one another in their OCP test set accuracy (61.12%-63.31%), attaining non-significant differences in the distribution of their accuracies across scenarios (Paired t-test, minimum Bonferroni corrected p-value of 0.444 across pairwise comparisons). For a fixed architecture, pretraining with increased image size helps improve accuracy somewhat on the OCP prediction task (SVG 128 \u00d7 128, rightmost periwinkle bar, vs. SVG 64\u00d7 64, leftmost one), along with temporal augmentations for a high capacity model (FitVid, rightmost dark green bar vs. leftmost one), but these differences are overall non-significant (Paired t-test p = 0.461 and p = 0.196 within the SVG and FitVid architectures, respectively). However, not all end-to-end models match human accuracy well. In particular, we see that the object-slotted C-SWM class of models matches human accuracies least well compared to other model classes, despite varying the encoder size. The class of latent future prediction models overall better matches these behavioral patterns than the more explicitly structured C-SWM models, but there does not appear to be strong differentiation across foundation models, and also not much differentiation between having a dynamics module vs. using the encoder latents directly (rightmost two bars vs. leftmost bar in each group). This suggests that at least for the OCP task, either Physion is not sufficiently high-variation enough to pretrain the dynamics module or most of the predictivity comes from the physical scene understanding encapsulated in the foundation model encoder latents. The latter appears to be more likely, since pretraining the dynamics module on a larger-scale dataset such as Kinetics-700 for a subset of the models (VIP+LSTM/CTRNN, VC-1+LSTM/CTRNN, and R3M+LSTM/CTRNN; hatched bars) does not improve OCP accuracy over the base encoder latents either, relative to pretraining them on Physion (Paired t-test, minimum Bonferroni corrected p-value of 0.066 across architectures).\nAdditionally, we can look at finer-grained error patterns of the probabilities reported by humans compared to those of models (rather than accuracies alone), summarized in Figure 2B. Here we see that despite the metric being more detailed, humans are quite consistent with each other, suggesting that this type of behavioral metric is quite reliable (grey horizontal line). In fact, all models appear to be further from the human-to-human consistency than the OCP accuracy. However, overall, similar trends appear to hold across models as with the OCP accuracy measure \u2013 where the end-to-end pixel-wise models (FitVid and SVG) match these consistency scores the best across models, and the object-slotted C-SWM models match them the least well. From an AI perspective, it is actually quite relevant to work towards matching human error patterns, as it is highly correlated with the primarily performance based measure of OCP accuracy, as seen in Figure 2C (R \u2248 0.865, p \u226a 0.001). And as shown in Figure 2D, all models have the most room to improve to match human error patterns and OCP accuracy (Figure S4) for soft-body interactions (the \u201cDrape\u201d scenario). On the other hand, models seem to do reasonably well on certain rigid-body scenarios, especially \u201cSupport\u201d relations where stacks of objects fall over depending on their shapes and arrangement; \u201cCollide\u201d, where pairs of objects collide depending on their trajectories and placement; and \u201cRoll\u201d, where objects move across a surface either by rolling or sliding. In these scenarios, the best Physion-pretrained models, namely the pixel-wise future predictors, can attain human consistency scores much higher than what we report in Figures 2B and D of around 0.6 (cf. Figure S5B)."
        },
        {
            "heading": "5 Comparison to Dense Neurophysiological Data",
            "text": "To gain more insight into model generalization, we compared the above models to neural dynamics recorded from macaque dorsomedial frontal cortex (DMFC), which was shown by Rajalingham et al. [2022b] to simulate the ball\u2019s position in Mental-Pong while behind an occluder, until it was intercepted by the paddle. This environment is completely out-of-distribution for the models, unlike Physion, and therefore tests structural generalization."
        },
        {
            "heading": "5.1 Inter-animal consistency and neural behavioral decoders",
            "text": "For each of the 79 conditions (different randomized start position of the ball), we present the frames in the visible epoch (the time up until the ball reaches the occluder) as context frames to the models, and unroll the model dynamics during the occluded epoch of the condition. We then build detailed, physical mappings (schematized in Figure 3A) from the committed model layer latent dynamics to match every single neuron in the population, and the ground truth ball state while occluded, when mental simulation takes place. We enforce the requirement, established in prior work [Nayebi et al., 2021, Cao and Yamins, 2021], that models are at least as similar to a given animal\u2019s neural responses as two conspecifics\u2019 neural responses are to each other. When we perform these mappings, we see that both the inter-animal neural predictivity consistency and ground truth ball state decoding from DMFC is quite high (grey horizontal lines in Figures 3B and C, respectively), indicating that these are very reliable neural and behavioral measures of the mental simulation phenomenon."
        },
        {
            "heading": "5.2 Neural response predictivity strongly separates models",
            "text": "When we map model units to DMFC units across 79 conditions (different randomized ball start position), we see in Figure 3B that across all architectures, only dynamically-equipped models from the video foundation model class predict DMFC dynamics best (46.34-48.83% neural predictivity), specifically in the latent space of the VC-1 and R3M encoders. However, the VC-1 and R3M encoder\u2019s latents alone are insufficient to predict the data (19.03% and 24.79% neural predictivity, respectively), although their latents best predict neural responses compared to other foundation models (cf. Figure S3C). Pretraining end-to-end on Physion with either a pixel-wise loss (SVG and FitVid; 14.44%-25.35% neural predictivity) or an object-slot loss (C-SWM with varying encoder sizes; 13.69%-23.05% neural predictivity) is not sufficient either, indicating a strong constraint on the neural mechanisms of mental simulation being performed on a suitable latent space. However, it is not the case that any latent space works, since all dynamics trained on encoder latents pretrained on static images, regardless of their imageset scale, attained at most 29.67% neural predictivity. This is suggestive that for a relatively small but time-varying stimulus such as the Mental-Pong ball, models that are pretrained on static images may not be equipped to handle the temporal coherence of a single object moving through time, as prior studies have mainly examined generalization of\nImageNet-optimized CNNs to novel yet static, single-object scenes [Hong et al., 2016]. Moreover, even for video optimized foundation models, we see differentiation at the level of loss function. In particular, goal-conditioned self-supervised learning on videos, as instantiated by VIP (which differs from R3M primarily in the choice of objective function, as it shares the same ResNet-50 architecture and is also pretrained on Ego4D), fails to match DMFC neural dynamics well (7.37%-18.12% neural predictivity). We also see parsimony at the level of the architecture and pretraining dataset of the dynamics module, as the VC-1/R3M+CTRNN either attains comparable or improved neural predictivity over the more sophisticated VC-1/R3M+LSTM (middle vs. rightmost bars for each encoder type), even when pretraining on a larger dataset like Kinetics-700 (Figure S1). This suggests that simple dynamics may be sufficient at predicting neural response dynamics given the appropriate visual encoder and highlights the limits of dataset scale alone without changing inductive biases. Both the VC-1/R3M+LSTM and VC-1/R3M+CTRNN models approach the neural predictivity of the ground truth ball position oracle model (50.74%, leftmost golden bar in Figure 3B), with the joint position + velocity oracle performing the best (60.65% neural predictivity, rightmost golden\nbar, which also corresponds to the \u201cPerfect Simulation Oracle\u201d dotted line). This suggests that a substantial amount of the neural response variability is devoted to simulating the ball\u2019s state while it is occluded, as opposed to other static features of the environment. The remaining gap between the \u201cPerfect Simulation Oracle\u201d and the inter-animal consistency may be due to other factors such as time since stimulus onset, attentional demands/eye movements, and perhaps uncertainty when the ball bounces, which can be explored in future work.\nIn Figure 3C, we see that the most neurally predictive dynamically-equipped VC-1/R3M-based models generalize to the Mental-Pong task, approaching DMFC\u2019s ability to track the ground truth position and velocity of the ball while it is occluded, despite not being explicitly pretrained in this environment. In particular, there is a linear relationship (R \u2248 0.701, p \u226a 0.001) between the model\u2019s ability to generalize to the Mental-Pong environment and its ability to predict DMFC neural dynamics (Figure 3D). This relationship indicates that predicting the underlying neural dynamics is in fact behaviorally relevant to effectively simulating the ball\u2019s dynamics, which Rajalingham et al. [2022a,b] judged in humans based on eye tracking data. Furthermore, the dynamically-equipped VC1/R3M-based models can best represent the ball position and velocity separately as well (Figure S6), demonstrating that they can independently track these variables."
        },
        {
            "heading": "6 Dynamically-Equipped Video Foundation Models Can Match Both Human",
            "text": "Behavioral and Neural Response Patterns Across Environments\nBoth the OCP human behavioral error pattern metric and the DMFC Mental-Pong neural predictivity metric are linearly correlated to their corresponding behavior of OCP accuracy and ball position prediction (as seen in Figures 2C and 3D), which suggests that these more fine-grained metrics are relevant to the underlying behavioral goal that they represent. But how do they relate to each other, and is there a model class that can match both reasonably well?\nAs we can see in Figure 4A, the two behavioral goals of Mental-Pong ball state predictivity and Physion OCP accuracy do not appear to be very related across models (R \u2248 \u22120.241, p \u2248 0.134). This suggests that held-out accuracy to novel scenes within environment as in OCP, does not imply that the same model will generalize to completely new environmental dynamics as in Mental-Pong. In particular, it is important to consider generalization to new environments, since the end-to-end pixel-wise models such as SVG and FitVid subtly overfit to this environment (attaining the highest OCP held-out scene accuracy), but they fail to generalize as well to the completely out-of-distribution Mental-Pong setting (rightmost periwinkle and dark green points in Figure 4A). For example, for FitVid, this failure is visualizable, as it predicts the Mental-Pong ball to be static, regardless of enlarging the ball during evaluation or pretraining with temporal augmentations (Figure S2).\nDelving into the more challenging, fine-grained measures of DMFC Mental-Pong neural predictivity and correlation to human error patterns, we observe in particular that the dynamically-equipped\nVC-1-based models (VC-1+CTRNN and VC-1+LSTM in cyan) reasonably match both human error patterns in Physion and Mental-Pong DMFC neural dynamics (Figure 4B).\nJust as VC-1 on its own does not universally dominate on every individual sensorimotor task, but instead outperforms the best prior existing visual foundation models on average across the 17 CortexBench tasks [Majumdar et al., 2023], our finding that future prediction in the latent space of this model class reasonably matches both human behavioral patterns and neural responses is therefore consistent with this observation. Taken together, our results suggest that future prediction in the latent space of video foundation models for Embodied AI is a promising paradigm for developing models of physical dynamics that are both a better match to neural and behavioral recordings, and can structurally generalize across diverse environments and scenarios within."
        },
        {
            "heading": "7 Discussion",
            "text": "Overall, we find that structural generalization to completely new environments and matching dense neurophysiological data to be a strong constraint on sensory-cognitive hypotheses of mental simulation. In a manner evocative of Yamins et al. [2014]\u2019s discovery of then-novel image categorizationoptimized CNNs outperforming all prior models in matching primate inferotemporal cortex, our dynamically-equipped video foundation models notably outperform all other models in DMFC neural response matching, illustrating the evolving nature of goal-driven models, especially when applied to higher-cognitive brain areas while the animal is performing a task. Going forward, we believe that we are at a crucial turning point whereby foundation models that engage with the visually rich, dynamic scenes that humans and animals naturally interface with, will be jointly critical for progress in Embodied AI and neuroscience, addressing the recent call to action to build AI that can be as grounded in the physical world as animals are [Zador et al., 2023]. We observe that both the popular machine learning paradigm of pixel-wise future prediction and visual foundation models pretrained on webscale imagesets, typically favored for classic computer vision tasks like image segmentation and classification, underperform here. Instead, future prediction within the latent space of a video foundation model, pretrained on diverse egocentric sources, aligns best with high-throughput neural and behavioral patterns in scenes that it was not originally trained on. Our findings indicate that primate mental simulation harbors robust inductive biases, and is so far most consistent with predicting the future state of its environment in a latent space that is reusable across dynamic environments.\nOn our existing benchmarks, there are a few ways that we envision our current models can be improved. First, we believe that major strides can still be made in the encoder module of the models, namely by better leveraging temporal relationships to build a more \u201cfactorized\u201d and reusable, objectcentric representation. The need for a more factorized, object-centric representation is suggested by the high neural predictivity of the joint, ground truth position + velocity oracle in Figure 3B, compared to either the position or velocity oracles alone. At the same time, it is crucial to also maintain reusability in this representation since the fixed object-slot C-SWM models do not match these data well, and can be considered an example of an object-centric, but not reusable, representation. Therefore, a couple ideas in this direction would be to employ dynamic object-slots [Traub et al., 2022, Didolkar et al., 2023] or structured masking [Bear et al., 2023, Yuan et al., 2023] pretrained on Ego4D or even larger egocentric datasets such as CortexBench [Majumdar et al., 2023], to yield object-centric, video foundation models. Additionally, simpler modifications such as differentiating between agent-based egomotion vs. external object motion may likely improve video foundation models by better closing the loop between taking actions in the world vs. simulating it.\nThe encoder modifications mentioned above may enable the models to learn more explicit representations of objects and their material properties, compared to current self-supervised methods that are more statistical in nature [Balestriero et al., 2023]. This is especially relevant given our finding that all current models struggle most with the soft-body interactions of the \u201cDrape\u201d scenario in Physion, both in terms of accuracy (Figure S4D) and matching human error patterns (Figure 2), yet Ego4D has many examples of deformable objects in it (such as baking, clay, etc). Thus, it is not the pretraining dataset that is impoverished, but rather the current model architectures and loss functions are failing to pick up on these in existing datasets. The dynamics architecture could also benefit from including multiple timescales of hierarchy, with some recent neurobiological evidence of this type of temporal hierarchy already existing in frontal cortex [Sarafyazd and Jazayeri, 2019]. Such dynamics could sync well with dynamic object-slot encoders, capturing more rapid object state changes at one timescale and slower material property shifts across scenes at a higher-level, more abstracted timescale."
        },
        {
            "heading": "8 Acknowledgments",
            "text": "We thank the anonymous reviewers for their helpful feedback on the initial manuscript. A.N. thanks Christopher J. Cueva, Leslie P. Kaelbling, Thomas Serre, Kevin A. Smith, Joshua B. Tenenbaum, Rahul Venkatesh, Nicholas Watters, and Chengxu Zhuang for helpful discussions. A.N., M.J., and G.R.Y. acknowledge the generous support of the K. Lisa Yang Integrative Computational Neuroscience (ICoN) Center at MIT. M.J. is also supported by NIH (NIMH-MH122025), the Simons Foundation, the McKnight Foundation, and the McGovern Institute. R.R. is supported by the Helen Hay Whitney Foundation.\nBroader Impact\nAlmost everything humans and animals do revolves around an intuitive understanding of the physical dynamics of the world we are embedded in, enabling us to make long-range plans and perform actions that ensure our survival. Furthermore, engineering at least this same level of intuitive physical understanding in silico will be critical for progress in robotics, autonomous vehicles, and any other embodied applications that involve safely taking actions in the real world. Our work not only provides a strong measurement of the degree of alignment between our current best engineered systems to those of humans and animals, but through the differentiation across choices of the architecture, objective function, and pretraining dataset, also provides scientific insight into the evolutionary constraints underlying the neural mechanisms of mental simulation. In particular, we quantitatively observe that future prediction in a latent space optimized for diverse, dynamic scenes is our current best theory that predicts neural dynamics during mental simulation behavior, compared to popular pixel-wise and object-slot alternatives. This set of observations suggests that making progress in Embodied AI will also correspondingly yield an improved understanding of mental simulation in human and animal brains."
        },
        {
            "heading": "A Model Pretraining",
            "text": "Model code and weights are available here: https://github.com/anayebi/mental-sim.\nAll models are supplied with T = 7 initial context frames, and are trained with the Adam optimizer [Kingma and Ba, 2015] using PyTorch 2.0 [Paszke et al., 2019]. For all models except FitVid and SVG 128\u00d7 128 (SVG trained on 128\u00d7 128 pixel images), a single NVIDIA A100 GPU was sufficient to train it. FitVid required 8 A100 GPUs, and SVG 128\u00d7 128 required 2 A100 GPUs.\nA.1 Physion Dataset\nAll models were simultaneously trained across all eight scenarios of the Physion Dynamics Training Set, constituting around 16,000 total training scenarios (2,000 scenes per scenario) [Bear et al., 2021], with a temporal subsample factor of 6 frames for a total sequence length of 25 frames that is randomly sampled.\nPre-existing end-to-end models were trained with their previously prescribed hyperparameters, and we always ensured that their training loss converged on the Physion dataset. FitVid Babaeizadeh et al. [2021] was trained on its standard 64 \u00d7 64 pixel input (both with and without RandAugment [Cubuk et al., 2020]), with its recommended batch size of 128 with a learning rate of 1e-3 and gradient clipping of magnitude 100, for 3125 epochs. When applying RandAugment across frames, it was crucial to fix the random seed of the cropping operations, to ensure that the same random transformation was applied across all frames in a single sequence. SVG [Denton and Fergus, 2018] was trained on either its usual 64 \u00d7 64 or 128 \u00d7 128 pixel inputs, with a batch size of 100 with a learning rate of 2e-3 for 300 epochs, as recommended in the original paper. Each C-SWM [Kipf et al., 2020] model was trained on 224 \u00d7 224 pixel inputs, with N = 10 object slots and its recommended batch size of 32 with a learning rate of 5e-4 for 100 epochs.\nGiven T context frames, all of the two-stage dynamics models (LSTM and CTRNN) are trained on top of a frozen pretrained foundation model, to predict the encoder latent at the next timestep T +1 with a mean squared error loss function, using a batch size of 32 with a learning rate of 1e-4 (except for VGG16 and DeiT, which used a learning rate of 1e-2) for 100 epochs. All models were trained with 224\u00d7 224 pixel inputs, using the same image augmentations used in the original evaluation of the corresponding foundation model. \u201cNo Dynamics\u201d corresponds to fixing and propagating the last context latent of the foundation model through time, and therefore does not require any additional training.\nA.2 Kinetics-700 Dataset\nWe additionally trained the dynamics of the two-stage VIP+LSTM, VIP+CTRNN, VC-1+LSTM, VC-1+CTRNN, R3M+LSTM, and R3M+CTRNN models on Kinetics-700 [Carreira et al., 2019]. We first wrote out each frame of the mp4 videos as a jpeg with the smallest side set to 480 pixels via inter-area interpolation. We then trained the models on videos of length at least 8 or more frames (since we need at least T = 7 context frames and one additional frame to predict), that is randomly sampled during training. This constraint resulted in 536,640 training videos, and 33,966 validation videos. We trained models for 100 epochs each on a single A100 GPU with a batch size of 256 with a linearly rescaled learning rate of 8e-4 [Goyal et al., 2017]."
        },
        {
            "heading": "B Comparison to Human Physical Judgements",
            "text": "Given T = 7 initial context frames, each model that we consider above is stimulus-computable and can be decomposed into an encoder E that yields a state representation per frame and a dynamics module D predicts unseen states given the prior state observations from the encoder. We first freeze model parameters and present all models with the eight Physion Readout Training Set scenarios (\u223c 1, 000 stimuli per scenario) for 25 timesteps with a subsample factor of 6 frames (same as during Dynamics Training in Section A.1). We combine the observed context and simulated model dynamics, which corresponds to the recommended all observed+simulated protocol in Physion, as it is agnostic to any particular scenario and best tests general physical understanding insofar as it can be assessed by Physion [Bear et al., 2021]. Specifically, we flatten these outputs and train a logistic regression classifier (cross-validated via a stratified 5-fold procedure) for 20,000 total training iterations to ensure convergence. To test the models after training the readout, we then fix the readout and present the same stimuli that the 100 human participants saw. For each stimulus, we compute the proportion of \u201chit\u201d responses by humans, and correspondingly we will extract the hit probability generated by the logistic regression readout from the models. The Correlation to Average Human Response is the Pearson\u2019s correlation between the model probability-hit vector and the human proportion-hit vector, across stimuli per scenario. The Heldout OCP Accuracy of humans and models is the average accuracy, across stimuli per scenario.\nTo give the final values of the two quantities, we then compute the weighted mean and s.e.m. of the above per scenario quantities xi across scenarios, weighted by the number of stimuli wi per scenario1, computed as such:\nweighted_mean = \u2211s\ni=1 wixi\u2211s i=1 wi .\nvariance = \u2211s i=1 wi(xi \u2212 weighted_mean) 2\u2211s\ni=1 wi ,\neffective_sample_size =\n(\u2211s i=1 wi )2\u2211s i=1 w 2 i ,\nweighted_sem =\n\u221a variance\neffective_sample_size ,\nwhere s = 8 is the number of scenarios."
        },
        {
            "heading": "C Comparison to Neurophysiological Recordings from Macaque Dorsomedial Frontal Cortex (DMFC)",
            "text": "To perform the model to brain (and brain to brain) comparisons, we first produce an (NcondNframes) \u00d7 Nunits matrix representation for both models and primate DMFC, following a similar procedure used by Rajalingham et al. [2022b]. For each of the Ncond = 79 conditions, we present the models with T = 7 uniformly sampled context frames from the Mental-Pong stimulus up until the last frame that the ball is visible. We then use the above determined temporal spacing from the context frames to determine the number of roll-out steps for the model up until the total number of frames for the current video (89 \u2264 Nframes \u2264 217 frames across conditions). Note that these values are therefore different for each condition, but always the same across all models. Neural responses from dorsomedial frontal cortex (DMFC) are originally in 50ms bins, which we also interpolate to match the number of frames for the current video. Monkey P had 1, 552 units recorded with 64-channel linear probes (Plexon V-probes) and monkey M had 337 units recorded with high-density 384-channel silicon probes (Neuropixels), resulting in 1,889 recorded units in total.\nOnce model and neural responses were temporally upsampled to match the number of frames of each video, we then compared their dynamics in the timepoints when the Mental-Pong ball was occluded. This was done by training a single cross-validated Ridge regressor LN shared across timepoints and conditions that maximized the neural predictivity of responses between monkey P and monkey M . The Ridge regressor was always refit to each source but using the same cross-validated hyperparameters found between animals, and was trained on 50% of the conditions (and their corresponding occluded epoch timepoints), and tested on the remaining 50% of conditions, across five train-test splits (cross-validation was done separately per train-test split via a grouped five-fold iterator). All neural predictivities are reported on heldout conditions and their timepoints. For each animal A \u2208 {P,M} and a given train-test split, heldout neural predictivity per unit in A is measured on the test\n1Specifically, there were 150 stimuli in Dominoes, 149 in Support, 150 in Collide, 150 in Contain, 150 in Drop, 150 in Link, 94 in Roll, and 149 in Drape.\nset as:\nNP(S,A) := Corr(LNS 7\u2212\u2192A(S), A)\u221a C\u0303orr ( LN\nS1 1/2 7\u2212\u2192A1 1/2\n( S11/2 ) , LN\nS2 1/2 7\u2212\u2192A2 1/2\n( S21/2 )) \u00b7 C\u0303orr ( A11/2, A 2 1/2 ) , (1) where the source S \u2208 {Model, P,M}; A is the trial-averaged neural response of animal A; A11/2 and A21/2 are the trial-averaged responses to the first and second random split-half of responses in A, respectively; S11/2 and S21/2 are the trial-averaged responses to the first and second random split-half of responses in S, respectively (when S is a model, then S = S11/2 = S 2 1/2); Corr is Pearson\u2019s correlation; and C\u0303orr is Spearman-Brown correction applied to Pearson\u2019s correlation since the numerator uses the full set of trials and the denominator uses half the available trials. The rationale of using a reliability-adjusted correlation is to account for variance that arises from noise in neural responses that no model can be expected to predict, as it is not replicable by experiment condition (for a detailed derivation, refer to [Nayebi et al., 2021, Appendix \u00a7B.1]). This quantity is then averaged across the five train-test splits, and aggregated across units in both monkeys P and M , and we report the median and s.e.m. across all 1,889 units in both monkeys. Note that if S is a perfect replica of A, then this quantity will be 1.0, regardless of the finite amount of data collected.\nWhen building neural behavioral decoders to the ball\u2019s (x, y) position and velocity, we concatenated monkey P and M \u2019s neural responses during the occluded epoch, and regressed it against the interpolated ground truth ball position and velocity (measured in degrees of visual angle). This five-fold cross-validated Ridge regressor LB was optimized to maximize the median of these four values using the same measure in equation (1) with A replaced by the ball state instead. The ball state (position + velocity) predictivity of each model is therefore the median and s.e.m. of these four values as measured by equation (1), with the regressor LN replaced by LB .\nD Supplementary Figures\nVC-1+CTRNN\nVC-1+LSTM\nVIP+CTRNN VIP\n+LS TM\nR3M+LSTM R3M+CTRNN\nPhysion\nKi ne\nti cs\n7 00\nR\u22480.979, p < 0.001\nFigure S1: Pretraining on Kinetics-700 Does Not Improve Neural Predictivity Over Pretraining on Physion: Neural predictivity of VIP, VC-1, and R3M equipped with CTRNN or LSTM dynamics, which are pretrained on Physion (x-axis) vs. Kinetics-700 (y-axis). Dotted black line represents unity line.\nPredicted Frame 95 Predicted Frame 120\nFi tV\nid +\nR an\ndA ug\nm en t Fi tV id + R an dA ug m en t Fi tV id (n o au gm en ta tio ns ) (A) (B) (C)\nFigure S2: FitVid Fails to Track Mental-Pong Ball: Example predictions of FitVid across frames, when evaluated on a given Mental-Pong video, after being (A) pretrained on Physion with RandAugment, (B) pretrained without augmentations, and (C) pretrained with RandAugment but with the Mental-Pong ball enlarged to be a square during evaluation.\n(A) (B)\nImage Foundation Models Video Foundation Models\nVGG16 ResNet-50 DeiT DINO DINOv2 CLIP VIP VC-1 R3M VGG16 ResNet-50 DeiT DINO DINOv2 CLIP VIP VC-1 R3M\nImage Foundation Models Video Foundation Models\nH el\ndo ut\nO CP\nA cc\nur ac\ny\nCo rr\nel at\nio n\nto A\nve ra\nge H\num an\nR es\npo ns e (P ea rs on \u2019s R)\n(C) (D)\nVGG16 ResNet-50 DeiT DINO DINOv2 CLIP VIP VC-1 R3M\nImage Foundation Models\nVideo Foundation Models\nVGG16 ResNet-50 DeiT DINO DINOv2 CLIP VIP VC-1 R3M\nImage Foundation Models\nVideo Foundation Models\nN eu\nra l P\nre di\nct iv\nit y\n(P ea\nrs on\n\u2019s R)\nBa ll\nPo si\nti on\n+ V\nel oc\nit y\nPr ed\nic ti\nvi ty\n(P ea\nrs on\n\u2019s R)\nFigure S3: Predictivity of Foundation Model Latents: Evaluations of the fixed latent representations (no dynamics) of the nine classes of foundation models on (A) accuracy on the OCP task (cf. Figure 2A), (B) human judgement probabilities on the OCP task (cf. Figure 3B), (C) DMFC neural predictivity on Mental-Pong (cf. Figure 3B), and (D) Mental-Pong ball position and velocity predictivity (cf. Figure 3C). These were originally included in Figures 2-4.\nH el\ndo ut\nO CP\nA cc\nur ac\ny\nPhysion Scenario Figure S4: Per Scenario OCP Accuracy: Per scenario model comparisons to binary (\u201cyes/no\u201d) accuracy on the OCP task (chance is 50%). Mean and s.e.m. across all 41 models.\n(A)\nH el\ndo ut\nO CP\nA cc\nur ac\ny\nPhysion Scenario Physion Scenario\nCo rr\nel at\nio n\nto A\nve ra\nge H\num an\nR es\npo ns e (P ea rs on \u2019s R)\n(B)\nFigure S5: Per Scenario OCP Evaluation of Best Pixel-Wise Future Predictors: (A) Per scenario model comparisons to binary (\u201cyes/no\u201d) accuracy on the OCP task (chance is 50%). (B) Per scenario model comparisons to human subject judgement probabilities for the OCP task. Mean and s.e.m. across the two best pixel-wise future predictors: FitVid pretrained on Physion with RandAugment and SVG pretrained on Physion with 128\u00d7 128 images. Demonstrates that there are some models which match accuracy and human judgement probabilities on the OCP task quite well for certain scenarios (e.g. \u201cSupport\u201d, \u201cCollide\u201d, and \u201cRoll\u201d), compared to the aggregate in Figure 2.\n(A) (B)\nBa ll\nPo si\nti on\nP re\ndi ct\niv it y (P ea rs on \u2019s R)\nBa ll\nVe lo\nci ty\nP re\ndi ct\niv it y (P ea rs on \u2019s R)\nDMFC Predictivity DMFC Predictivity\nEnd-to-End Latent Future Prediction End-to-End Latent Future Prediction C-SWMSVG FitVid VGG16 ResNet-50 DeiT DINO DINOv2 CLIP VIP VC-1 R3M C-SWMSVG FitVid VGG16 ResNet-50 DeiT DINO DINOv2 CLIP VIP VC-1 R3M\nPixel-wise\nImage Foundation Models\nVideo Foundation Models\nObject -slot\nPixel-wise\nImage Foundation Models Video Foundation Models\nObject -slot\nSm al l L ar ge M ed iu m\nN o\nD yn\nam ic s LS TM D yn am\nic s\nCT RN\nN D\nyn am\nic s\nN o\nD yn\nam ic s LS TM D yn am\nic s\nCT RN\nN D\nyn am\nic s\nCT RN\nN D\nyn am\nic s\nLS TM\nD yn\nam ic\ns\n64 x6 4 12\n8x 12\n8\n64 x6 4 64 x6 4+\nR an\ndA ug\nm en\nt\nSm al l L ar ge M ed iu m\nN o\nD yn\nam ic s LS TM D yn am\nic s\nCT RN\nN D\nyn am\nic s\nN o\nD yn\nam ic s LS TM D yn am\nic s\nCT RN\nN D\nyn am\nic s\nCT RN\nN D\nyn am\nic s\nLS TM\nD yn\nam ic\ns\n64 x6 4 12\n8x 12\n8\n64 x6 4 64 x6 4+\nR an\ndA ug\nm en\nt\nFigure S6: Models Can Separately Represent Mental-Pong Ball Position and Velocity: Same as Figure 3C, but separate predictivity of the (A) ball position and (B) velocity predictivity of each model, averaged across five train-test splits, from the best DMFC fitting model layer used in Figure 3B via the linear transform LB schematized in Figure 3A. The grey horizontal bar represents the ball position in (A) and velocity predictivity in (B) from DMFC units. Median and s.e.m. across two quantities: the ground truth (x, y) position in (A) and velocity in (B) of the ball while it was occluded (in degrees of visual angle)."
        }
    ],
    "title": "Neural Foundations of Mental Simulation: Future Prediction of Latent Representations on Dynamic Scenes",
    "year": 2023
}