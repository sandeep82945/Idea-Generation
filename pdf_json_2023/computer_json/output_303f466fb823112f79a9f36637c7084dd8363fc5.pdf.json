{
    "abstractText": "We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to the misalignment between the geometry and the texture, particularly at face region. To address these limitations, TADA leverages the synergy of a 2D diffusion model and a parametric body model. Specifically, we derive a high-resolution upsampled SMPL-X with displacement layer and a texture map, and use hierarchical *denotes equal contribution. rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings during SDS optimization process. We further drive the face of character with multiple expressions during optimization, ensuring that its semantics remain consistent with the original SMPL-X model, for realistic animation with semantic alignment. Both qualitative and quantitative evaluations show that TADA significantly surpasses existing approaches. TADA enables large-scale creation of digital characters ready for animation and rendering, while also allows for text-guided editing. The code will be public for research purposes at tada.is.tue.mpg.de. 1 ar X iv :2 30 8. 10 89 9v 1 [ cs .A I] 2 1 A ug 2 02 3",
    "authors": [
        {
            "affiliations": [],
            "name": "Tingting Liao"
        },
        {
            "affiliations": [],
            "name": "Hongwei Yi"
        },
        {
            "affiliations": [],
            "name": "Yuliang Xiu"
        },
        {
            "affiliations": [],
            "name": "Jiaxiang Tang"
        },
        {
            "affiliations": [],
            "name": "Yangyi Huang"
        },
        {
            "affiliations": [],
            "name": "Justus Thies"
        },
        {
            "affiliations": [],
            "name": "Michael J. Black"
        }
    ],
    "id": "SP:41d93f5d9e4b1358d40ffea6c6f8c0b2d14bf57a",
    "references": [
        {
            "authors": [
                "Thiemo Alldieck",
                "Hongyi Xu",
                "Cristian Sminchisescu"
            ],
            "title": "imghum: Implicit generative models of 3d human shape and articulated pose",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Shivangi Aneja",
                "Justus Thies",
                "Angela Dai",
                "Matthias Nie\u00dfner"
            ],
            "title": "Clipface: Text-guided editing of textured 3d morphable models",
            "venue": "In SIGGRAPH \u201923 Conference Proceedings,",
            "year": 2023
        },
        {
            "authors": [
                "Bernd Bickel",
                "Mario Botsch",
                "Roland Angst",
                "Wojciech Matusik",
                "Miguel Otaduy",
                "Hanspeter Pfister",
                "Markus Gross"
            ],
            "title": "Multi-scale capture of facial geometry and motion",
            "venue": "Transactions on Graphics (TOG),",
            "year": 2007
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Yukang Cao",
                "Yan-Pei Cao",
                "Kai Han",
                "Ying Shan",
                "Kwan-Yee K Wong"
            ],
            "title": "DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models",
            "venue": "arXiv preprint:2304.00916,",
            "year": 2023
        },
        {
            "authors": [
                "Rui Chen",
                "Yongwei Chen",
                "Ningxin Jiao",
                "Kui Jia"
            ],
            "title": "Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Yongwei Chen",
                "Rui Chen",
                "Jiabao Lei",
                "Yabin Zhang",
                "Kui Jia"
            ],
            "title": "TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yao Feng",
                "Jinlong Yang",
                "Marc Pollefeys",
                "Michael J. Black",
                "Timo Bolkart"
            ],
            "title": "Capturing and Animation of Body and Clothing from Monocular Video",
            "venue": "In SIGGRAPH Asia 2022 Conference Papers,",
            "year": 2022
        },
        {
            "authors": [
                "Jianglin Fu",
                "Shikai Li",
                "Yuming Jiang",
                "Kwan-Yee Lin",
                "Chen Qian",
                "Chen-Change Loy",
                "Wayne Wu",
                "Ziwei Liu"
            ],
            "title": "Stylegan-human: A data-centric odyssey of human generation",
            "venue": "European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Daiheng Gao",
                "Yuliang Xiu",
                "Kailin Li",
                "Lixin Yang",
                "Feng Wang",
                "Peng Zhang",
                "Bang Zhang",
                "Cewu Lu",
                "Ping Tan"
            ],
            "title": "DART: Articulated Hand Model with Diverse Accessories and Rich Textures",
            "venue": "In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2022
        },
        {
            "authors": [
                "William Gao",
                "Noam Aigerman",
                "Groueix Thibault",
                "Vladimir Kim",
                "Rana Hanocka"
            ],
            "title": "Textdeformer: Geometry manipulation using text guidance",
            "venue": "In ACM Transactions on Graphics (SIGGRAPH),",
            "year": 2023
        },
        {
            "authors": [
                "William Gao",
                "Noam Aigerman",
                "Groueix Thibault",
                "Vladimir Kim",
                "Rana Hanocka"
            ],
            "title": "Textdeformer: Geometry manipulation using text guidance",
            "venue": "In ACM Transactions on Graphics (SIGGRAPH),",
            "year": 2023
        },
        {
            "authors": [
                "Marc Habermann",
                "Weipeng Xu",
                "Michael Zollhoefer",
                "Gerard Pons-Moll",
                "Christian Theobalt"
            ],
            "title": "Deepcap: Monocular human performance capture using weak supervision",
            "venue": "In Computer Vision and Pattern Recognition (CVPR). IEEE,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Han",
                "Yukang Cao",
                "Kai Han",
                "Xiatian Zhu",
                "Jiankang Deng",
                "Yi-Zhe Song",
                "Tao Xiang",
                "Kwan-Yee K. Wong"
            ],
            "title": "Headsculpt: Crafting 3d head avatars with text",
            "venue": "arXiv preprint arXiv:2306.03038,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars",
            "venue": "Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Yukun Huang",
                "Jianan Wang",
                "Ailing Zeng",
                "He Cao",
                "Xianbiao Qi",
                "Yukai Shi",
                "Zheng-Jun Zha",
                "Lei Zhang"
            ],
            "title": "Dreamwaltz: Make a scene with complex 3d animatable avatars",
            "venue": "arXiv preprint arXiv:2305.12529,",
            "year": 2023
        },
        {
            "authors": [
                "Yangyi Huang",
                "Hongwei Yi",
                "Weiyang Liu",
                "Haofan Wang",
                "Boxi Wu",
                "Wenxiao Wang",
                "Binbin Lin",
                "Debing Zhang",
                "Deng Cai"
            ],
            "title": "One-shot implicit animatable avatars with model-based priors",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Ajay Jain",
                "Ben Mildenhall",
                "Jonathan T. Barron",
                "Pieter Abbeel",
                "Ben Poole"
            ],
            "title": "Zero-shot text-guided object generation with dream fields",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Hanbyul Joo",
                "Hao Liu",
                "Lei Tan",
                "Lin Gui",
                "Bart Nabbe",
                "Iain Matthews",
                "Takeo Kanade",
                "Shohei Nobuhara",
                "Yaser Sheikh"
            ],
            "title": "Panoptic studio: A massively multiview system for social motion capture",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of StyleGAN",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Byungjun Kim",
                "Patrick Kwon",
                "Kwangho Lee",
                "Myunggi Lee",
                "Sookwan Han",
                "Daesik Kim",
                "Hanbyul Joo"
            ],
            "title": "Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Nikos Kolotouros",
                "Thiemo Alldieck",
                "Andrei Zanfir",
                "Eduard Gabriel Bazavan",
                "Mihai Fieraru",
                "Cristian Sminchisescu"
            ],
            "title": "Dreamhuman: Animatable 3d avatars from text",
            "venue": "arXiv preprint:2306.09329,",
            "year": 2023
        },
        {
            "authors": [
                "Samuli Laine",
                "Janne Hellsten",
                "Tero Karras",
                "Yeongho Seol",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Modular primitives for high-performance differentiable rendering",
            "venue": "Transactions on Graphics (TOG),",
            "year": 2020
        },
        {
            "authors": [
                "Ruilong Li",
                "Kyle Olszewski",
                "Yuliang Xiu",
                "Shunsuke Saito",
                "Zeng Huang",
                "Hao Li"
            ],
            "title": "Volumetric human teleportation",
            "venue": "In ACM SIGGRAPH 2020 Real-Time Live,",
            "year": 2020
        },
        {
            "authors": [
                "Ruilong Li",
                "Yuliang Xiu",
                "Shunsuke Saito",
                "Zeng Huang",
                "Kyle Olszewski",
                "Hao Li"
            ],
            "title": "Monocular real-time volumetric performance capture",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Tingting Liao",
                "Xiaomei Zhang",
                "Yuliang Xiu",
                "Hongwei Yi",
                "Xudong Liu",
                "Guo-Jun Qi",
                "Yong Zhang",
                "Xuan Wang",
                "Xiangyu Zhu",
                "Zhen Lei"
            ],
            "title": "High-Fidelity Clothed Avatar Reconstruction from a Single Image",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3D: High-Resolution Text-to-3D Content Creation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Loper",
                "Naureen Mahmood",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J. Black"
            ],
            "title": "SMPL: A skinned multi-person linear model",
            "venue": "ACM Transactions on Graphics, (Proc. SIGGRAPH Asia),",
            "year": 2015
        },
        {
            "authors": [
                "Wan-Chun Ma",
                "Andrew Jones",
                "Jen-Yuan Chiang",
                "Tim Hawkins",
                "Sune Frederiksen",
                "Pieter Peers",
                "Marko Vukovic",
                "Ming Ouhyoung",
                "Paul Debevec"
            ],
            "title": "Facial performance synthesis using deformation-driven polynomial displacement maps",
            "venue": "Transactions on Graphics (TOG),",
            "year": 2008
        },
        {
            "authors": [
                "Gal Metzer",
                "Elad Richardson",
                "Or Patashnik",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Latent-nerf for shape-guided generation of 3d shapes and textures",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Oscar Michel",
                "Roi Bar-On",
                "Richard Liu",
                "Sagie Benaim",
                "Rana Hanocka"
            ],
            "title": "Text2Mesh: Text-Driven Neural Stylization for Meshes",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Nasir Mohammad Khalid",
                "Tianhao Xie",
                "Eugene Belilovsky",
                "Tiberiu Popa"
            ],
            "title": "Clip-mesh: Generating textured meshes from text using pretrained image-text models",
            "venue": "In SIGGRAPH Asia 2022 Conference Papers,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Mou",
                "Xintao Wang",
                "Liangbin Xie",
                "Jian Zhang",
                "Zhongang Qi",
                "Ying Shan",
                "Xiaohu Qie"
            ],
            "title": "T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models",
            "venue": "arXiv preprint:2302.08453,",
            "year": 2023
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Georgios Pavlakos",
                "Vasileios Choutas",
                "Nima Ghorbani",
                "Timo Bolkart",
                "Ahmed A.A. Osman",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "Expressive Body Capture: 3D Hands, Face, and Body from a Single Image",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "DreamFusion: Text-to-3d using 2d diffusion",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Zeju Qiu",
                "Weiyang Liu",
                "Haiwen Feng",
                "Yuxuan Xue",
                "Yao Feng",
                "Zhen Liu",
                "Dan Zhang",
                "Adrian Weller",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Controlling Text-to-Image Diffusion by Orthogonal Finetuning",
            "venue": "arXiv preprint:2306.07280,",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Elad Richardson",
                "Gal Metzer",
                "Yuval Alaluf",
                "Raja Giryes",
                "Daniel Cohen-Or"
            ],
            "title": "Texture: Text-guided texturing of 3d shapes",
            "venue": "arXiv preprint:2302.01721,",
            "year": 2023
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "High-resolution image synthesis with latent diffusion models",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Yael Pritch",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Shunsuke Saito",
                "Zeng Huang",
                "Ryota Natsume",
                "Shigeo Morishima",
                "Angjoo Kanazawa",
                "Hao Li"
            ],
            "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Shunsuke Saito",
                "Tomas Simon",
                "Jason Saragih",
                "Hanbyul Joo"
            ],
            "title": "Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3d human digitization",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Sanghi",
                "Hang Chu",
                "Joseph G Lambourne",
                "Ye Wang",
                "Chin-Yi Cheng",
                "Marco Fumero",
                "Kamal Rahimi"
            ],
            "title": "Malekshan. Clip-forge: Towards zero-shot text-to-shape generation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Igor Santesteban",
                "Nils Thuerey",
                "Miguel A Otaduy",
                "Dan Casas"
            ],
            "title": "Self-Supervised Collision Handling via Generative 3D Garment Models for Virtual Try-On",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Igor Santesteban",
                "Miguel A. Otaduy",
                "Nils Thuerey",
                "Dan Casas"
            ],
            "title": "ULNeF: Untangled layered neural fields for mix-and-match virtual try-on",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yonatan Shafir",
                "Guy Tevet",
                "Roy Kapon",
                "Amit H Bermano"
            ],
            "title": "Human motion diffusion as a generative prior",
            "venue": "arXiv preprint arXiv:2303.01418,",
            "year": 2023
        },
        {
            "authors": [
                "Ruizhi Shao",
                "Zerong Zheng",
                "Hongwen Zhang",
                "Jingxiang Sun",
                "Yebin Liu"
            ],
            "title": "Diffustereo: High quality human reconstruction via diffusion-based stereo using sparse cameras",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Tianchang Shen",
                "Jun Gao",
                "Kangxue Yin",
                "Ming-Yu Liu",
                "Sanja Fidler"
            ],
            "title": "Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Vanessa Sklyarova",
                "Jenya Chelishev",
                "Andreea Dogaru",
                "Igor Medvedev",
                "Victor Lempitsky",
                "Egor Zakharov"
            ],
            "title": "Neural Haircut: Prior-Guided Strand-Based Hair Reconstruction",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Guy Tevet",
                "Sigal Raab",
                "Brian Gordon",
                "Yoni Shafir",
                "Daniel Cohen-or",
                "Amit Haim Bermano"
            ],
            "title": "Human motion diffusion model",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Can Wang",
                "Menglei Chai",
                "Mingming He",
                "Dongdong Chen",
                "Jing Liao"
            ],
            "title": "Clip-nerf: Text-and-image driven manipulation of neural radiance fields",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "Lingjie Liu",
                "Yuan Liu",
                "Christian Theobalt",
                "Taku Komura",
                "Wenping Wang"
            ],
            "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Shuyang Gu",
                "Jianmin Bao",
                "Tadas Baltrusaitis",
                "Jingjing Shen",
                "Dong Chen",
                "Fang Wen",
                "Qifeng Chen"
            ],
            "title": "Rodin: A generative model for sculpting 3d digital avatars using diffusion",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyi Wang",
                "Cheng Lu",
                "Yikai Wang",
                "Fan Bao",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation",
            "venue": "arXiv preprint arXiv:2305.16213,",
            "year": 2023
        },
        {
            "authors": [
                "Max Welling",
                "Yee W Teh"
            ],
            "title": "Bayesian learning via stochastic gradient langevin dynamics",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2011
        },
        {
            "authors": [
                "Yuliang Xiu",
                "Jinlong Yang",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "ICON: Implicit Clothed humans Obtained from Normals",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Yuliang Xiu",
                "Jinlong Yang",
                "Xu Cao",
                "Dimitrios Tzionas",
                "Michael J. Black"
            ],
            "title": "ECON: Explicit Clothed humans Optimized via Normal integration",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Jiale Xu",
                "Xintao Wang",
                "Weihao Cheng",
                "Yan-Pei Cao",
                "Ying Shan",
                "Xiaohu Qie",
                "Shenghua Gao"
            ],
            "title": "Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Xueting Yang",
                "Yihao Luo",
                "Yuliang Xiu",
                "Wei Wang",
                "Hao Xu",
                "Zhaoxin Fan"
            ],
            "title": "D-IF: Uncertainty-aware Human Digitization via Implicit Distribution Field",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Hongwei Yi",
                "Chun-Hao P. Huang",
                "Shashank Tripathi",
                "Lea Hering",
                "Justus Thies",
                "Michael J. Black"
            ],
            "title": "MIME: Human-aware 3D scene generation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Hongwei Yi",
                "Hualin Liang",
                "Yifei Liu",
                "Qiong Cao",
                "Yandong Wen",
                "Timo Bolkart",
                "Dacheng Tao",
                "Michael J Black"
            ],
            "title": "Generating holistic 3d human motion from speech",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Huichao Zhang",
                "Bowen Chen",
                "Hao Yang",
                "Liao Qu",
                "Xu Wang",
                "Li Chen",
                "Chao Long",
                "Feida Zhu",
                "Kang Du",
                "Min Zheng"
            ],
            "title": "Avatarverse: High-quality & stable 3d avatar creation from text and pose",
            "venue": "arXiv preprint:2308.03610,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Longwen Zhang",
                "Qiwei Qiu",
                "Hongyang Lin",
                "Qixuan Zhang",
                "Cheng Shi",
                "Wei Yang",
                "Ye Shi",
                "Sibei Yang",
                "Lan Xu",
                "Jingyi Yu"
            ],
            "title": "Dreamface: Progressive generation of animatable 3d faces under text guidance",
            "venue": "arXiv preprint arXiv:2304.03117,",
            "year": 2023
        },
        {
            "authors": [
                "Rui Zhao",
                "Wei Li",
                "Zhipeng Hu",
                "Lincheng Li",
                "Zhengxia Zou",
                "Zhenwei Shi",
                "Changjie Fan"
            ],
            "title": "Zero-shot text-to-parameter translation for game character auto-creation",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Yang Zheng",
                "Ruizhi Shao",
                "Yuxiang Zhang",
                "Tao Yu",
                "Zerong Zheng",
                "Qionghai Dai",
                "Yebin Liu"
            ],
            "title": "Deepmulticap: Performance capture of multiple characters using sparse multiview cameras",
            "venue": "In International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Zerong Zheng",
                "Xiaochen Zhao",
                "Hongwen Zhang",
                "Boning Liu",
                "Yebin Liu"
            ],
            "title": "Avatarrex: Real-time expressive full-body avatars",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2023
        },
        {
            "authors": [
                "Heming Zhu",
                "Lingteng Qiu",
                "Yuda Qiu",
                "Xiaoguang Han"
            ],
            "title": "Registering explicit to implicit: Towards high-fidelity garment mesh reconstruction from single images",
            "venue": "In Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "*denotes equal contribution.\nrendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings during SDS optimization process. We further drive the face of character with multiple expressions during optimization, ensuring that its semantics remain consistent with the original SMPL-X model, for realistic animation with semantic alignment. Both qualitative and quantitative evaluations show that TADA significantly surpasses existing approaches. TADA enables large-scale creation of digital characters ready for animation and rendering, while also allows for text-guided editing. The code will be public for research purposes at tada.is.tue.mpg.de.\nar X\niv :2"
        },
        {
            "heading": "1. Introduction",
            "text": "Digital avatars are a foundation for applications in augmented and virtual reality, immersive telepresence [27, 28, 49, 68, 78], virtual try-on [51, 52, 79], and video games [15, 77, 80]. Creating high-quality and expressive 3D avatars is challenging since the geometry and appearance of the character has to be modelled under a variety of different poses. Traditional pipelines used in the entertainment industry often use sophisticated multi-view capture studios [3, 22, 32] to create complex 3D models. Manual processes like cleaning and rigging the scans make creating an animatable character time-consuming and expensive. While there is recent progress on automatic learning-based body reconstruction from single image [20, 28, 29, 48, 49, 67, 68, 70], or sparse images [54], such methods are limited to real humans, fail on fictional characters, and are hard to edit and control. Thanks to the rapid progress on Large Language Models [4, 42] and Diffusion Models [17, 57\u201359, 66], recent work has shown that text-to-image models [43, 45] can be combined with differentiable neural 3D scene representations such as DeepSDF [38], NeRF [35] and DMTET [55] to generate realistic 3D models solely from textual descriptions. However, these methods have many limitations. The generated objects or characters are often rigid and lack of animation [7, 14, 30, 34, 44], they have difficulty in producing high-quality realistic 3D avatars in terms of geometry and texture [18], or the characters are incompatible with traditional CG workflows (NeRF based [5, 25, 40]).\nHere we address these limitations with TADA, illustrated in Fig. 13. Since our goal is animatable avatars that are compatible with existing rendering engines, we build upon the SMPL-X body model [39]. SMPL-X, however, only represents a realistic, minimally clothed human body shape. Our goal is to create diverse avatars with a wider variety of body shapes and textures. Like recent work that generates avatars from text, we use Score Distillation Sampling (SDS) [40] but do so in several novel ways. Specifically, we make three key contributions. (i) First, we devise a hierarchical optimization of a hybrid mesh representation which is based on a subdivided version of SMPL-X [39] with additional learnable displacements and a texture map. To produce high-quality details, especially on the face region, we perform hierarchical optimization over hierarchically rendered images with different focal lengths, where the entire body, or only specific parts, are visible. (ii) Second, existing text-to-3D object methods [7, 40] suffer from inconsistent alignment between the reconstructed geometry and texture (see Fig. 2), as they evaluate the texture and geometry individually. This makes animation of the resulting avatars infeasible. (iii) Third, we want the generated character\nPrompt:\u201cninja\u201d\nto be semantically consistent with SMPL-X so that it can be easily animated; that is, body parts and vertices on output avatars correspond to the same body parts and vertices on SMPL-X. To that end, we introduce animations throughout the optimization process. Specifically, we deform the generated character in each optimization step by sampling predefined SMPL-X body poses and facial expressions. This ensures that our generated characters can be animated accurately and coherently, as depicted in Fig. 2 (c). Notably, once optimization is finished, our generated characters can be animated with any novel set of SMPL-X parameters. Especially, combined with existing text to motion generation [53, 60] or text-to-audio-to-motion methods [61, 72], we can animate the generated characters to interact with the scenes or communicate with others. This paves the way towards creating virtual 3D worlds with animatable digital avatars fully from text.\nIn summary, with TADA we propose a user-friendly tool for avatar creation and editing, that can be solely controlled by the textual input and is fully compatible with traditional graphics pipelines. The output model is graphics-ready because the underlying model is SMPL-X with displacements and a texture map. Our method can generate realistic iconic celebrities, customized humans, and cartoon characters. We validate our contributions with ablation studies, show qualitative comparisons to the state of the art, and conduct a user study that quantifies the performance of our method on the task of high-quality 3D avatar generation."
        },
        {
            "heading": "2. Related Work",
            "text": "Recently, there has been rapid progress on extending text-to-2D-image generation methods [11, 23, 46] to text-to-3D-content generation [33, 40, 65]. Here, we\ndiscuss the most relevant text-to-3D-content generation methods, while focusing primarily on text-to-3D-avatar generation (both human and anime characters). Text-to-3D-Content Generation. The successes of Text-to-Image (T2I) generative modeling [9, 45, 47] have sparked a surge of interest in the field of text-to-3D generation [8, 50, 62, 69]. Despite progress, effectively describing and controlling 3D properties of an object using language, while ensuring coherence in the three-dimensional space, remains a challenge. One line of work [21, 34, 36] utilizes CLIP-space similarities to guide shape and texture optimization. However, these methods often fail to generate convincing and realistic 2D renderings. CLIP-based optimization can be combined with a generative appearance model to improve the quality, as shown in CLIPFace [2]. However, this requires learning a GAN-model for the 3D appearance, which is challenging for full-body avatars that can vary from real humans to cartoon characters.\nTo circumvent the training of a 3D generative model and the problem of missing datasets, recent publications [7, 30, 40, 44] make significant strides by leveraging the power of score distillation sampling (SDS) [45] derived from 2D text-to-image diffusion models to create content from textual descriptions by optimizing a 3D representation. TEXTure [44] takes a mesh as input and only optimizes the texture map based on a given text prompt. In contrast, DreamFusion [40] optimizes a Neural Radiance Field (NeRF) [35] to represent the 3D content in terms of a density and radiance field. It can generate 3D models of arbitrary (fictional) objects. However, it faces challenges due to slow optimization of NeRF and low-resolution image space supervision, resulting in long processing times and low-quality 3D models. To overcome these limitations, Magic3D [30] introduces a two-stage optimization framework, using NeRF in the first stage and a\ntextured mesh in the second stage. Fantasia3D [7] extends this to generate 3D meshes by disentangling geometry and texture, and optimizes them separately. All these methods focus on general, static, object/scene generation; they output is not animation-ready, which is necessary for 3D character creation. Text-to-3D-Avatar Generation. Several methods generate 3D head avatars from text [13, 16, 64, 75, 76]. In contrast, we focus on generating full-body characters including the detailed face. AvatarCLIP [18] leverages NeuS [63] and the SMPL-X model with a CLIP-guide loss to facilitate the generation of avatars. Similarly, DreamAvatar [5] utilizes the shape parameters from SMPL as a prior to learn a NeRF-based color field. DreamHuman [25] leverages imGHUM [1] as a prior, which represents a signed distance field conditioned on pose and shape parameters, to learn a NeRF of the human. However, the NeRF representation remains problematic due to its relatively low geometry and appearance quality, and it is not compatible with traditional graphics workflows, especially for animation. In the domain of explicit representations, Text2Mesh [34] and Chupa [24] employ vertex displacement on a predefined mesh template. Nonetheless, the inherent limitation of fixed topology poses challenges in accurately generating diverse character shapes. In contrast, our approach jointly optimizes the shape, expression, and displacement. Thus, the generated characters exhibit superior quality, can be easily animated with SMPL-X motions, and seamlessly integrate into existing CG rendering and animation workflows."
        },
        {
            "heading": "3. Preliminaries",
            "text": "SMPL-X [39] is an animatable parametric 3D body model that consists of the human body, hands and face. It hasN = 10, 475 vertices andK = 54 joints. Given the shape \u03b2, pose\n\u03b8 (including body joints pose \u03b8b, jaw pose \u03b8f and finger pose \u03b8h) and expression \u03c8 parameters, SMPL-X models the human body as M(\u03b2, \u03b8, \u03c8):\nM(\u03b2, \u03b8, \u03c8) = W(T(\u03b2, \u03b8, \u03c8), J(\u03b2), \u03b8,W ) T(\u03b2, \u03b8, \u03c8) = T +Bs(\u03b2) +Be(\u03c8) +Bp(\u03b8),\n(1)\nwhere T is a mean shape template, Bs, Be and Bp are shape, expression and pose blend shapes, respectively. W is the linear blend-skinning function transforming T(\u03b2, \u03b8, \u03c8) to the target pose \u03b8, with the skeleton joints J(\u03b2) and skinning weights W \u2208 RN\u00d7K . Score Distillation Sampling [40] has been proposed in DreamFusion to utilize a pre-trained 2D diffusion model to optimize the parameters \u03b7 of a 3D model, given a text y as input. Given the diffusion model \u03d5 with the noise prediction network \u03f5\u0302\u03d5(xt; y, t), SDS optimizes parameters \u03b7 by directly minimizing the injected noise \u03f5 added to the rendered images x = g(\u03b7) and the predicted noise:\n\u2207\u03b7LSDS(\u03d5, x) = Et,\u03f5 [ w(t)(\u03f5\u0302\u03d5(xt; y, t)\u2212 \u03f5) \u2202x\n\u2202\u03b8\n] , (2)\nwhere g(\u03b7) denotes the differentiable rendering of the 3D model parameterized by \u03b7, xt is the noised image, and w(t) is a weighting function that depends on the noise level t."
        },
        {
            "heading": "4. Method",
            "text": "Given an input text prompt, TADA aims to generate a high-fidelity animatable full-body avatar. As illustrated in Fig. 3, our method initializes the 3D avatar with upsampled SMPL-X, which is parameterized with shape, pose, and expression parameters. Based on it, learnable displacements are incorporated, resulting in a \u201cclothed\u201d avatar with increased density (Sec. 4.1). Then, we optimize the 3D character with consistent geometry and texture using SDS losses that considers both the rendered normal and RGB images in the latent space (Sec. 4.2). To encourage semantic consistency with the SMPL-X, we sample different gestures and expressions during training (Sec. 4.3). This enables the future animation using the SMPL-X pose and expression space."
        },
        {
            "heading": "4.1. SMPL-X+D Representation",
            "text": "TADA adopts an SMPL-X+D to model animatable clothed avatars. The learnable displacement (D) accounts for personalized details that are independent of pose, shape, and expression. To generate a high-quality character with a detailed face, we apply a partial mesh subdivision on the original SMPL-X model, which is adapted as (Eq. (1)):\nM\u0302(\u03b2, \u03b8, \u03c8,D) = W(T\u0302(\u03b2, \u03b8, \u03c8,D), J(\u03b2), \u03b8, W\u0302 )\nT\u0302(\u03b2, \u03b8, \u03c8,D) = S(T(\u03b2, \u03b8, \u03c8)) +D, (3)\nwhere S : RN\u00d73 \u2192 RNs\u00d73 is the mesh subdivision operation, D \u2208 RNs\u00d73, W\u0302 \u2208 RNs\u00d7J and Ns are the vertex displacement, skinning weights and vertices number of the subdivided body, respectively. Note that besides the displacement D, the parameters \u03b2, \u03b8, \u03c8 are also learnable. This helps to generate various kinds of characters with various shapes, such as human-like characters and anime characters with large deformed body shapes, like exaggerated proportions, elongated limbs, large eyes, and etc.\nPartial Mesh Subdivision. The vertices on the surface of the SMPL-X body are irregularly distributed, i.e., around 4,000 vertices are located on the head with the remaining 6,000 on the body. The sparsity of vertices on the body surface results in less detailed deformations there. Simply increasing the mesh density by subdividing the whole body mesh leads to noisy results, especially, in the face area during geometry optimization. To address this issue, we employ an adaptive upsampling technique on the triangles and interpolate their skinning weights within areas of low mesh density, such as the body region and the back of the head. This process yields a more refined mesh with uniformly distributed vertices and smoother skinning weights."
        },
        {
            "heading": "4.2. Consistent Geometry and Texture Learning",
            "text": "To generate animatable characters, we need to ensure the consistency between geometry and the texture. Therefore, we propose to blend the SDS loss of the rendered normal and RGB images to achieve a well-aligned geometry and texture. Given a mesh M\u0302 parameterized by D, \u03b2 and \u03c8 and albedo \u03a8, we render its normal image N and colored image I using a differentiable render [26], denoted as R:\nN = R(M\u0302, \u03c0), I = R(\u03a8, M\u0302, \u03c0) (4)\nwhere \u03c0 are the camera parameters. In each iteration, the camera is randomly positioned in one of two perspectives: a full-body view or a zoom-in head view. The head zoom-in allows us to reconstruct a detailed face region. Texture SDS Objective. Given a text prompt, the texture generation is guided by a pretrained Stable Diffusion (SD) model [45], denoted as \u03d5, which measures the similarity between the rendered image and the provided text prompt within the added and predicted noise space:\n\u2207\u03a8Ltex(\u03d5, I) = Et,\u03f5 [ w(t)(\u03f5\u0302\u03d5(z I t ; y, t)\u2212 \u03f5)\n\u2202I \u2202\u03a8 \u2202zI \u2202I\n] ,\n(5) where zI is the latent feature of I, encoded by image encoder (SD), \u03f5\u0302\u03d5(zIt ; y, t) is the predicted noise given text embedding y and noise level t, \u03f5 is the pre-computed noise.\nGeometry Consistency SDS Objective. Similarly, rendered normal images can be used for the diffusion model as shape encoding to facilitate the geometry synthesis. However, this approach may encounter challenges in ensuring perfect consistency between geometry and texture. To address this issue, we compute the SDS loss on the interpolation between normal and color image latents.\n\u2207\u03b3Lc(\u03d5, x) = Et,\u03f5 [ w(t)(\u03f5\u0302\u03d5(z\u0303t; y, t)\u2212 \u03f5)\n\u2202N \u2202\u03b3 \u2202z \u2202N\n] ,\n(6) where \u03b3 = {\u03b2, \u03c8,D} are the geometry related parameters, z\u0303 = \u03b1zI + (1 \u2212 \u03b1)zN denotes the resulting interpolated latent code, while zI and zN represent the latent codes corresponding to the RGB and normal image, respectively. Overall Optimization Objective. The learning objectives can be formulated as a combination of the texture SDS objective Ltex and the geometry consistency loss Lc, where \u03bbtex and \u03bbc are the corresponding loss weights:\nL = \u03bbtexLtex + \u03bbcLc, (7)\nBased on Eq. (7), the geometry and texture are optimized jointly. We employ a progressive optimization strategy for the rendered color image I in the Eq. (5). Initially, this image is generated at a low resolution (32 \u00d7 32), which gradually increases during the optimization process, ultimately reaching 512 \u00d7 512 resolution. In contrast, both the rendered normal image N and color image I in the Eq. (6) remain 512 \u00d7 512 resolution throughout the entire procedure. Additionally, we detach the gradients of zI in Eq. (6), allowing only geometric updates, while optimizing textures using the texture SDS loss. This approach ensures both texture-text consistency and geometry-texture alignment, preventing misalignment that could result in unrealistic animation."
        },
        {
            "heading": "4.3. Training with Animations",
            "text": "To ensure plausible animations, particularly for the face region, it is essential to maintain semantic correspondence with the SMPL-X model. However, during optimization, certain parts may undergo changes and not align perfectly\nwith the original ones (e.g. the mouth may be mapped to the chin area or become distorted). If not addressed, animated results will have severe artifacts as the wrong parts will be deformed with the SMPL-X model. To tackle this problem, we optimize the avatar using various animations (see Fig. 3). In particular, we find that using different jaw poses during training helps produce well aligned faces. We found that animating the SMPL-X expression parameters, made little visible difference. We suspect that these would become relevant with an even higher-resolution face mesh. Specifically, during optimization, we randomly sample one jaw pose in each iteration from an expression gallery \u0398, i.e., a motion sequence from TalkSHOW [72]. The final optimization process minimizes the following objective:\nmin \u03b2,\u03c8,D,\u03a8\nE\u03b8\u2208\u0398 [L(\u03d5, x(\u03b2, \u03b8, \u03c8,D,\u03a8))] . (8)"
        },
        {
            "heading": "5. Experiments",
            "text": "We first demonstrate our expressive, holistic, animation of the avatars, then evaluate their quality, and the consistency between texture and geometry. Finally, ablation studies are conducted to analyze the effectiveness of each component."
        },
        {
            "heading": "5.1. Expressive Holistic Body Animation",
            "text": "One crucial feature that distinguishes our method from others is that TADA enables natural full-body animations over the face, body and hands. Figure 4 illustrates the animation of characters generated by TADA using only with text as input. In the first case, we convert text to audio [61] and then use TalkSHOW [72] create expressive SMPL-X animations of the upper body, face and hands. In the second case we use priorMDM [53] to convert text into SMPL [31] animations, which we convert to SMPL-X [39]. Thanks semantic correspondence with SMPL-X, the characters are easily animated with natural movements of the fully body and face. This consistency with SMPL-X means that avatars generated by TADA can be animated using any of the recent text to animation methods that output SMPL-X."
        },
        {
            "heading": "5.2. Diverse Range of Avatars",
            "text": "As shown in Fig. 5, TADA produces a wide variety of 3D avatars characterized by their high-quality geometry and realistic textures. These avatars contain fictional\ncharacters from animated films, real-life celebrities, and custom-made characters based on prompts generated by ChatGPT. This capability opens up numerous real-world applications, enabling users to effortlessly generate avatars\nwith a wide range of shapes, appearances, and clothing styles."
        },
        {
            "heading": "5.3. Qualitative Comparison",
            "text": "We compare our method with existing approaches on the task of text-to-3D human avatar generation. We consider four state-of-the-art methods for full body avatar generation, where the original implementation is available: TEXTure [44], AvatarCLIP [18], DreamAvatar [5] and Fantasia3D [7]. For head-only avatar generation, we compare our approach with DreamFace [75] and HeadSculpt [16].\nFull Body Avatar. Figure 6 provides a qualitative comparison of avatars generated by [5, 18, 44] and by TADA. In comparison to the baselines, TADA generates avatars with considerably more realistic textures. Also TADA produces a wide range of 3D body shapes (cf. TEXTure), without geometric artifacts (cf. AvatarCLIP, DreamAvatar and Fantasia3D), and with a semantically correct texture that is consistent with the geometry.\nHead Avatar. Furthermore, TADA generates high-quality head avatars as shown in Fig. 7. We compare with DreamFace [75] and HeadSculpt [16], a shape sculpting method specifically designed for head avatar generation. Note that TADA creates visually appealing head avatars with consistent and well-aligned geometry as well as high-fidelity textures. However, others have different limitations. DreamFace [75] avatars can look realistic but are strongly biased towards natural head shapes and cannot capture more varied facial details like mustaches or cartoon shapes. Like our results, the head avatars can be animated.\nMeanwhile, HeadScuplt [16] generates noisy geometry and texture, making the output less useful for downstream tasks like animation."
        },
        {
            "heading": "5.4. Quantitative Evaluation.",
            "text": "To quantitatively evaluate TADA, we conducted a A/B user study with 17 CV students.We used a Google Survey Form to assess the (1) geometry quality, (2) texture quality, and (3) consistency with input prompts. We used ChatGPT to automatically generate a large set of character descriptions, including celebrities, characters in movies and anime, and general occupation character descriptions, select 27 of these at random, and generate the corresponding avatars; see Sup. Mat. for details. In A/B tests, the participants were asked to select the preferred reconstruction from randomly selected videos from the baselines (AvatarCLIP and DreamAvatar) and our method (see Tab. 1). The results show that our proposed method achieves considerable higher preference over the baseline methods over all three metrics."
        },
        {
            "heading": "5.5. Ablation Study",
            "text": "We conducted ablation studies to evaluate the effects of the geometry consistency loss and the optimization with animations in our method. The results shown in Fig. 8 demonstrate the effectiveness of these components. The consistency loss improves the alignment between the geometry and texture on the backside of the \u201cSuperman\u201d, while training with animations improves the face geometry by enforcing the semantic correspondence with SMPL-X, particularly at mouth region. These advancements enable us to effortlessly animate our high-resolution avatars, leveraging the pose and expression space of the SMPL-X model."
        },
        {
            "heading": "6. Editing Applications",
            "text": "TADA facilitates several applications, such as virtual try-on, text-guided texture editing, and local geometry transferring.\nVirtual Try-on. TADA can be used for virtual try-on, i.e., we can ask ChatGPT [6] to design fashion outfits for a specific person as depicted in Fig. 9. The visual results indicate that our method can generate avatars with text-guided personalized textures while preserving the identity.\nTexture Editing. Figure 10 shows examples of modifying outfit textures by changing the input text. This is particularly valuable for film or game character design, allowing easy alterations such as changing the color palette of a character. Designers can quickly visualize their desired aesthetic appeal and bring their creative vision to life.\nLocal Shape Editing. Thanks to the body-part segments of SMPL-X, our method supports direct local body and face swapping between two avatars without any additional effort. Fig. 11 gives an example of face editing on four individuals. This is also applicable to body or clothing transferring. In addition to geometry or texture transferring, TADA can also be utilized for local shape sculpting through user-friendly prompts as input. This feature is particularly helpful for artists in designing customized avatars."
        },
        {
            "heading": "7. Discussion",
            "text": "While TADA shows promising results, it still has several limitations. Additionally, further investigation is needed to assess any potential negative social impact.\nLimitations & Future works. One aspect that requires improvement is the relighting capabilities in different environments, e.g. generated indoor rooms from MIME [71], thus enabling photo-realistic rendering with human-scene interactions. This can benefit from using BRDF, by separating the texture into separate components (i.e., material, albedo, and lighting) like Fantasia3D [7]. Furthermore, TADA can generate avatars with diverse body shapes, some of which may deviate largely from the base SMPL-X model. In such cases, using the original skinning weights may lead to unrealistic animations. Therefore, exploring the joint learning of adaptable skinning weights specifically tailored to text input could be a promising direction. Textual descriptions alone may not fully capture the nuanced and intricate aspects of a character\u2019s appearance. Combining existing controllable text-to-image models [37, 41, 74] can be beneficial to provide more detailed control over a character\u2019s face or clothing. And the compositional generation of separate haircut [56],\naccessories [12], and decoupled outfits [10] could also be a valuable exploration direction.\nSocial Impact. As the technique progresses, it raises concerns about deep-fake and intellectual property (IP) when we generate iconic characters. Regulations should be established to address these issues alongside the benefits in the entertainment industry. Additionally, it is crucial to prioritize gender and cultural diversity. For instance, if the term \u201cpolice officer\u201d consistently generates a male instead of considering both genders, it implies potential gender bias. Ensuring inclusivity and avoiding stereotypes are essential in mitigating any adverse social impact."
        },
        {
            "heading": "8. Conclusion",
            "text": "We introduce TADA, a simple yet effective method for generating high-quality and animatable 3D textured avatars solely from text input. These avatars cover a wide range of individuals, including celebrities and customized characters. They seamlessly integrate into existing CG pipelines, catering to various industries like fashion and entertainment. The key contributions include: 1) utilizing a subdivided version of SMPL-X with learned displacement layer and UV texture, 2) employing hierarchical optimization with adaptive focal lengths, 3) enforcing geometry-texture alignment through geometric consistency loss, and 4) training with animation to keep semantic correspondence with SMPL-X. We validate these components through ablation studies and demonstrate the superiority of TADA over other SOTAs with both qualitative and quantitative results.\nAcknowledgments. Thanks Zhen Liu and Weiyang Liu for their fruitful discussion, Haofan Wang and Xu Tang for their technical support, and Benjamin Pelkofer for IT support. Hongwei Yi is supported in part by the German Federal Ministry of Education and Research (BMBF): Tu\u0308bingen AI Center, FKZ: 01IS18039B. Yuliang Xiu is funded by the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No.860768 (CLIPE). Jiaxiang Tang is supported by National Natural Science Foundation of China (Grant Nos: 61632003, 61375022, 61403005). Yangyi Huang is supported by the National Nature Science Foundation of China (Grant Nos: 62273302, 62036009, 61936006).\nDisclosure. MJB has received research gift funds from Adobe, Intel, Nvidia, Meta/Facebook, and Amazon. MJB has financial interests in Amazon, Datagen Technologies, and Meshcapade GmbH. While MJB is a consultant for Meshcapade, his research in this project was performed solely at, and funded solely by, the Max Planck Society."
        },
        {
            "heading": "A. Additional Qualitative Comparisons",
            "text": "We provide additional qualitative comparisons with existing baselines in three categories: head avatar generation, full-body iconic and customized avatar generation. Head Avatar Generation. In Fig. 12, we provide examples generated by HeadSculpt [16] and our method TADA. Unlike HeadSculpt often generates noisy geometry and suffers from inconsistency between texture and geometry, TADA could produce high-quality character geometries with well-aligned textures. Full-body Iconic Avatar Generation. We conduct a comprehensive comparison of the full-body iconic avatar generation with existing methods [5, 18, 19, 40, 73] in Figs. 13 to 15. NeRF-based methods [5, 19, 40, 73] tends to generate low-quality geometric fields, which are not compatible with existing traditional CG workflows, such as rasterization and animation. The mesh-based method AvatarClip [18] tends to generate minimal clothed geometry, and low-quality texture with severe artifacts (see Fig. 15). In contrast, TADA has the capability to produce a wide range of characters with superior geometries and well-aligned textures. These outputs seamlessly integrate with conventional graphics workflows, making them readily suitable for animation and rendering. Full-body Customized Avatars Generation. To explore the generalization of our method, we compare TADA with DreamHuman [25] on the task of full-body customized avatars generation. In Fig. 16, we can see that TADA generates high-quality characters with more consistent geometry and texture than DreamHuman.\nB. Implementation details We select camera positions (r, \u03b8, \u03d5) in a spherical coordinate system, where r, \u03b8, and \u03d5 denote the radius, elevation, and azimuth angle, respectively. During the optimization, the virtual cameras are positioned as: 1) full-body mode: camera with full-body field of view (FOV), and 2) head-mode: zoom-in camera focusing the head. The head-mode camera is sampled with a probability of 30%, and full-body model with 70%. We sample \u03b8b values within the range of [60\u25e6, 90\u25e6] under full-body mode. Conversely, for the head view \u03b8h, we opt for values from the range of [75\u25e6, 85\u25e6]. Additionally, we sample \u03d5 within the intervals of [\u2212180\u25e6, 180\u25e6] for the full body, and [\u221230\u25e6, 30\u25e6] for the head. In each iteration, the camera radius is adjusted according to the body height and the head size."
        },
        {
            "heading": "C. Prompts used by ChatGPT",
            "text": "We generate characters of three groups: celebrity, characters in movies & anime (fictional characters), and general job descriptions. We also provide the prompts used to imagine outfits for Virtual Try-on application. In the following, we list the prompts used for generation.\nCelebrities. We ask ChatGPT to output the names of superstars, scientists, businessmen and presidents. Here are the corresponding generated prompts: Donald Trump Abraham Lincoln Barack Obama Hilary Clinton Yao Ming Kobe Bryant Messi Bruce Lee Steven Paul Jobs Mark Elliot Zuckerberg Joe Biden Bill Gates Warren Buffett Elon Musk Jeff Bezos Jack Ma Albert Einstein Marie Curie Stephen Hawking Vincent van Gogh Michelangelo Wolfgang Amadeus Mozart Ludwig van Beethoven Michael Jackson Kim Kardashian\nFictional Characters. We collect superheroes, Disney princesses, and characters in films such as Forzen, Aladdin, South Park, Simpson, Rick and Morty, etc. superman Deadpool Batman Ant-man spiderman Iron Man Captain America Woody in Toy Story Buzz Lightyear in Toy Story Elsa in Frozen Anna in Frozen Kristoff in Frozen Aladdin in Aladdin Jasmine in Aladdin Mulan in Mulan Olaf in Frozen Jessie in Toy Story Sun Wukong groot Moana in Moana Judy Hopps in Zootopia Goku in Dragon Ball series Naruto Uzumaki in Naruto series Luffy in One Piece Kiki in Kiki\u2019s Delivery Service San in Princess Mononoke Eric Cartman in South Park: Bigger, Longer & Uncut Dipper Pines in Gravity Falls Mabel Pines in Gravity Falls Rick Sanchez in Rick and Morty Morty Smith in Rick and Morty Stan Marsh in South Park: Bigger, Longer & Uncut Grunkle Stan in Gravity Falls Soos Ramirez in Gravity Falls Kyle Broflovski in South Park: Bigger, Longer & Uncut Kenny McCormick in South Park: Bigger, Longer & Uncut Bojack Horseman in Bojack Horseman Diane Nguyen in Bojack Horseman Ariel in The Little Mermaid Belle in Beauty and the Beast Homer Simpson in The Simpsons Marge Simpson in The Simpsons Bart Simpson in The Simpsons Lisa Simpson in The Simpsons astronaut Michelangelo style statue of an astronaut A highly detailed stone bust of Theodoros Kolokotronis Captain Jack Sparrow from Pirates of the Caribbean Mobile suit Gundam Harry Potter, adult Optimus Prime, transformer Ronald Weasley, adult stormtrooper Casino Dealer\nAncient Prince of India Nick Fury Captain Marvel robot Thor Doctor Strange Snow White Cinderella Crayon Shin-chan Misae Nohara\nGeneral Job Descriptions. In addition, we ask GPT to generate general job descriptions such as policeman, doctor, chef and etc. gardener warrior knight witch wizard Joker Clown Chef Nurse Dentist Doctor Artist Carpenter Farmer Athlete Pilot Firefighter Police officer astronaut\nImagined Outfits for Virtual Try-on. Here is an example, how we use ChatGPT to generate different outfit descriptions for Barack Obama. We: please generate 50 prompts describing Obama wearing different clothes\n----------------------------------------------------------------\nChatGPT: Sure, here are 50 writing prompts that describe Barack Obama wearing different clothes:\nBarack Obama wearing a sharp navy blue suit and a red tie. Barack Obama wearing a casual grey t-shirt and jeans. Barack Obama wearing a black leather jacket and aviator sunglasses. Barack Obama wearing a traditional African dashiki. Barack Obama wearing a classic American baseball cap and polo shirt. Barack Obama wearing a white dress shirt and black dress pants. Barack Obama wearing a bright yellow raincoat and rain boots. Barack Obama wearing a navy blue hoodie and sneakers. Barack Obama wearing a tan safari jacket and khaki pants. Barack Obama wearing a red and black plaid flannel shirt and jeans. Barack Obama wearing a crisp white dress shirt and a black suit. Barack Obama wearing a light blue chambray shirt and khaki shorts. Barack Obama wearing a navy blue blazer and chinos. Barack Obama wearing a black and white houndstooth suit and tie. Barack Obama wearing a green military-style jacket and combat boots. Barack Obama wearing a purple dress shirt and grey suit. Barack Obama wearing a denim shirt and khaki pants. Barack Obama wearing a pink dress shirt and navy blue suit. Barack Obama wearing a brown leather jacket and brown boots. Barack Obama wearing a yellow polo shirt and white shorts. Barack Obama wearing a grey sweatshirt and athletic pants. Barack Obama wearing a black leather bomber jacket and jeans. Barack Obama wearing a white t-shirt and denim jacket. Barack Obama wearing a burgundy sweater and dress pants. Barack Obama wearing a black and white checkered suit and tie. Barack Obama wearing a green army jacket and cargo pants. Barack Obama wearing a navy blue blazer and grey dress pants. Barack Obama wearing a grey hoodie and black sweatpants. Barack Obama wearing a yellow rain jacket and rain boots. Barack Obama wearing a light blue dress shirt and navy blue suit. Barack Obama wearing a black turtleneck and khaki pants. Barack Obama wearing a red plaid shirt and jeans. Barack Obama wearing a grey suit and a red tie. Barack Obama wearing a white polo shirt and black shorts. Barack Obama wearing a blue suit and a white dress shirt. Barack Obama wearing a black t-shirt and black jeans. Barack Obama wearing a purple dress shirt and black suit. Barack Obama wearing a brown leather jacket and jeans. Barack Obama wearing a white t-shirt and cargo pants. Barack Obama wearing a grey suit and a blue tie. Barack Obama wearing a black hoodie and black sweatpants. Barack Obama wearing a green sweater and khaki pants. Barack Obama wearing a navy blue suit and a white dress shirt. Barack Obama wearing a red polo shirt and khaki shorts. Barack Obama wearing a brown leather bomber jacket and jeans. Barack Obama wearing a white dress shirt and black suit pants. Barack Obama wearing a green camouflage jacket and cargo pants. Barack Obama wearing a grey sweater and black dress pants. Barack Obama wearing a blue plaid shirt and jeans. Barack Obama wearing a black suit and a light blue dress shirt."
        }
    ],
    "title": "TADA! Text to Animatable Digital Avatars",
    "year": 2023
}