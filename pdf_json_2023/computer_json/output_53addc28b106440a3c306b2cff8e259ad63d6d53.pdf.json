{
    "abstractText": "Large Language models (LLMs) possess the capability to engage In-context Learning (ICL) by leveraging a few demonstrations pertaining to a new downstream task as conditions. However, this particular learning paradigm suffers from high instability stemming from substantial variances induced by factors such as the input distribution of selected examples, their ordering, and prompt formats. In this work, we demonstrate that even when all these factors are held constant, the random selection of examples still results in high variance. Consequently, we aim to explore the informative ability of data examples by quantifying the Information Gain (IG) obtained in prediction after observing a given example candidate. Then we propose to sample those with maximum IG. Additionally, we identify the presence of template bias, which can lead to unfair evaluations of IG during the sampling process. To mitigate this bias, we introduce Calibration Before Sampling strategy. The experimental results illustrate that our proposed method can yield an average relative improvement of 14.3% across six classification tasks using three LLMs.",
    "authors": [
        {
            "affiliations": [],
            "name": "Hongfu Liu"
        }
    ],
    "id": "SP:ab98ab46f9bae7596669e4415179e4619df40539",
    "references": [
        {
            "authors": [
                "Robert B Ash."
            ],
            "title": "Information theory",
            "venue": "Courier Corporation.",
            "year": 2012
        },
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Ting-Yun Chang",
                "Robin Jia."
            ],
            "title": "Careful data curation stabilizes in-context learning",
            "venue": "arXiv preprint arXiv:2212.10378.",
            "year": 2022
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova."
            ],
            "title": "BoolQ: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "Proceedings of the 2019 Conference of the North American Chap-",
            "year": 2019
        },
        {
            "authors": [
                "Ido Dagan",
                "Sean P Engelson."
            ],
            "title": "Committeebased sampling for training probabilistic classifiers",
            "venue": "Machine Learning Proceedings 1995, pages 150\u2013 157. Elsevier.",
            "year": 1995
        },
        {
            "authors": [
                "Ido Dagan",
                "Oren Glickman",
                "Bernardo Magnini."
            ],
            "title": "The pascal recognising textual entailment challenge",
            "venue": "Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First",
            "year": 2006
        },
        {
            "authors": [
                "Marie-Catherine De Marneffe",
                "Mandy Simons",
                "Judith Tonhauser."
            ],
            "title": "The commitmentbank: Investigating projection in naturally occurring discourse",
            "venue": "proceedings of Sinn und Bedeutung, volume 23, pages 107\u2013124.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Ranit Aharonov",
                "Yoav Katz",
                "Noam Slonim."
            ],
            "title": "Active Learning for BERT: An Empirical Study",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7949\u20137962, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen."
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natu-",
            "year": 2021
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International conference on machine learning, pages 1321\u20131330. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "SU Hongjin",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A Smith"
            ],
            "title": "Selective annotation makes language models better fewshot learners",
            "year": 2023
        },
        {
            "authors": [
                "Xiaonan Li",
                "Xipeng Qiu."
            ],
            "title": "Finding supporting examples for in-context learning",
            "venue": "arXiv preprint arXiv:2302.13539.",
            "year": 2023
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Katerina Margatina",
                "Giorgos Vernikos",
                "Lo\u00efc Barrault",
                "Nikolaos Aletras."
            ],
            "title": "Active learning by acquiring contrastive examples",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650\u2013663, Online and",
            "year": 2021
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho."
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in neural information processing systems, 34:11054\u201311070.",
            "year": 2021
        },
        {
            "authors": [
                "John Platt"
            ],
            "title": "Probabilistic outputs for support",
            "year": 1999
        },
        {
            "authors": [
                "Christopher Potts"
            ],
            "title": "Recursive deep models for",
            "year": 2013
        },
        {
            "authors": [
                "Tomas Pfister"
            ],
            "title": "2023a. Better zero-shot reasoning",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Bowman"
            ],
            "title": "Superglue: A stick",
            "year": 2019
        },
        {
            "authors": [
                "peng Kong"
            ],
            "title": "Self-adaptive in-context",
            "year": 2023
        },
        {
            "authors": [
                "Yue Yu",
                "Rongzhi Zhang",
                "Ran Xu",
                "Jieyu Zhang",
                "Jiaming Shen",
                "Chao Zhang."
            ],
            "title": "Cold-start data selection for few-shot language model fine-tuning: A prompt-based uncertainty propagation approach",
            "venue": "arXiv preprint arXiv:2209.06995.",
            "year": 2022
        },
        {
            "authors": [
                "Michelle Yuan",
                "Hsuan-Tien Lin",
                "Jordan BoydGraber."
            ],
            "title": "Cold-start active learning through selfsupervised language modeling",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7935\u20137948,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Yiming Zhang",
                "Shi Feng",
                "Chenhao Tan."
            ],
            "title": "Active example selection for in-context learning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134\u2013 9148, Abu Dhabi, United Arab Emirates. Association",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "International Conference on Machine Learning, pages 12697\u201312706. PMLR.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "These days the In-context Learning (ICL) ability of pre-trained Large Language Models (LLMs) has garnered significant attention in the community. ICL represents a new paradigm for few-shot learning, which entails performing new downstream tasks based on prompts. These prompts consist of a few input-output pairs, commonly referred to as demonstrations. Such prompts serve as explicit task descriptions for the LLM. LLMs have showcased the formidable capacity of ICL and achieved remarkable performances across various downstream tasks (Brown et al., 2020). In comparison to approaches that involve fine-tuning LLMs on downstream tasks (Devlin et al., 2019; Gao et al., 2021), ICL obviates the need for parameter updates,\nthereby allowing higher efficiency in adapting to new tasks and easier deployment.\nHowever, ICL tends to suffer from substantial variance in performance. Existing studies attribute it to factors including the input distribution of demonstrations, their ordering, and the prompt formats employed during prompt construction (Zhao et al., 2021; Lu et al., 2022; Zhang et al., 2022; Min et al., 2022). Our investigation reveals that even when the input distribution, the ordering of demonstrations, and prompt formats remain fixed, the random selection of different demonstrations still leads to significant variance, as shown in Figure 1. This observation indicates that data samples within the same category can offer distinct information and contribute differently to the ICL performance. We refer to the ability of a data sample to provide valuable information as its informative ability. To the best of our knowledge, there has been no prior study exploring this aspect in the existing literature.\nIn this paper, we examine the informative ability\nar X\niv :2\n31 0.\n08 92\n3v 1\n[ cs\n.C L\n] 1\n3 O\nct 2\n02 3\nof data examples from the perspective of information theory and investigate its correlation with the ICL performance. Specifically, we assess the contribution of individual data samples to the specific downstream tasks by quantitatively measuring their informative ability. To accomplish this, we propose to evaluate the Information Gain (IG) of prediction, which quantifies the amount of information gained after observing one example candidate in the context. We construct a prompt for each example candidate and utilize the LLM to obtain the corresponding output distribution, thus enabling the evaluation of IG. Furthermore, we uncover the presence of Template Bias, which can lead to biased evaluations of IG. To address this issue, we introduce a Calibration Before Sampling strategy to ensure a fair assessment of IG. Subsequently, we select the example candidates with maximum IG and annotate them as demonstrations for enhancing ICL.\nTo validate the effectiveness of our method, we conduct empirical evaluations across six classification datasets across three LLMs of varying model sizes. The experimental results demonstrate an average relative improvement of 14.3% on oneshot learning. It is important to emphasize that our proposed method is orthogonal to existing methods such as calibration (Zhao et al., 2021) and reordering methods (Lu et al., 2022). Moreover, we demonstrate that our method can be combined with these approaches to achieve further improvements. Additionally, we analyze the relationship between data informative ability with the correctness of target labels and find that data examples with high IG tend to rely more on the accuracy of target labels.\nIn summary, our contributions can be summarized as follows:\n\u2022 We investigate the relationship between data informative ability and ICL performance.\n\u2022 We propose the use of Information Gain (IG) to measure the data informative ability and select demonstrations with maximum IG to enhance ICL performance.\n\u2022 We identify Template Bias and introduce the Calibration Before Sampling strategy to address it.\n\u2022 Our proposed method yields significant improvements, achieving an average relative improvement of 14.3% across six classification tasks using three LLMs."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Active Data Sampling for ICL",
            "text": "Active data sampling has been employed in natural language processing tasks since their early stage (Settles, 2009). The primary objective is to achieve comparable or superior performance while reducing the annotation cost. With the advent of pre-trained LLMs, recent studies (Ein-Dor et al., 2020; Yuan et al., 2020; Margatina et al., 2021; Yu et al., 2022) have successfully introduced active learning to minimize the amount of data required for fine-tuning. In the context of the ICL paradigm, the standard ICL involves the random selection of training examples as prompts. However, it has been observed that the performance and stability of ICL can be enhanced by selecting high-quality examples, which aligns with the concept of active data sampling. One common method is to retrieve semantically similar samples for each test query (Rubin et al., 2021; Liu et al., 2021; Hongjin et al., 2023), thereby enabling the utilization of instancelevel prompts in downstream tasks. Another approach involves retrieving task-level examples as prompts for all test samples, eliminating the need for instance-level retrieval. (Zhang et al., 2022) introduces reinforcement learning to learn a generalized policy for example selection and (Chang and Jia, 2022) focuses on carefully choosing training subsets to improve stability. However, previous studies either rely on the performances of validation sets as reward signals or require the training of additional models to score example candidates. In contrast, our work centers on retrieving task-level examples from unlabeled datasets for all test samples, without the necessity of extra validation sets or the training of additional models."
        },
        {
            "heading": "2.2 Confidence-based ICL Selection",
            "text": "Confidence-based evaluation is widely used for in-context examples. Prior works take the confident model outputs as in-context examples (Wan et al., 2023a) and search confident in-context example organizations in a self-adaptive manner (Wu et al., 2023). In concurrent works, USP (Wan et al., 2023b) utilizes confidence-based prediction for pseudo-demos generation and also handles generation tasks. LENS (Li and Qiu, 2023) proposed informative examples filtering and diversity-guided search method. Our work focuses on addressing the template bias of ICL selection by calibration before sampling."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Problem Statement",
            "text": "In this study, we focus on a problem setting that closely resembles true few-shot learning (Perez et al., 2021), which is to retrieve prompts from an unlabeled text dataset, denoted as Dunlab = {xi}Ni=1, for a specific task. To accomplish it, we utilize a pre-trained LLM to make predictions on all candidate examples in Dunlab, resulting in the corresponding prediction set Y = {yi}Ni=1, where yi represents the normalized predicted label distribution given input xi. The goal is to select a subset {xj}Kj=1 from Dunlab, where K \u226a N , in order to facilitate few-shot learning, specifically, K-shot learning. We annotate the chosen K examples with their respective target labels yt and construct task-level prompts using the input-label pairs and task-specific formats (See Appendix A.3 for details). The task-specific prompts are then incorporated as the prefix sequences for test samples."
        },
        {
            "heading": "3.2 Information Gain",
            "text": "Information Gain (IG) serves as a metric to quantify the amount of information obtained about a random variable through the observation of another random variable (Ash, 2012). In our context, to measure the informative ability of data examples, we define\nthe IG as the information obtained in predicted label distribution Y when observing one example candidate X = xob in Dunlab. Specifically,\nIG(Y, xob) = H(Y )\u2212H(Y |xob) (1)\nwhere H(Y ) represents the information entropy of Y and H(Y |xob) denotes the conditional entropy of Y given the observation xob. However, computing the exact value of IG(Y, xob) is intractable due to the unknown H(Y ). Fortunately, H(Y ) remains constant for a given task, allowing us to reframe the problem of sampling examples with maximum IG as selecting those with minimum conditional entropy H(Y |xob). Specifically, considering the LLM parameterized by \u03b8,\nH(Y |xob) = \u2212 \u2211 y\u2208Y p\u03b8(y|xob) log p\u03b8(y|xob) (2)\nHowever, it is challenging to compute p\u03b8(y|xob) directly by inputting xob into the LLM. Instead, we adopt the approach of constructing the zeroshot prompt. One example is shown in the prompt construction of Figure 2. We fill in the task template with text input only and utilize the LLM to make predictions. In other words, each example candidate is taken as a test sample in zero-shot ICL. As such, p\u03b8(y|xob) is actually approximated by p\u03b8(y|xob, T ), where T denotes the task template."
        },
        {
            "heading": "3.3 Template Bias",
            "text": "Taking inspiration from (Zhao et al., 2021), we have observed that the predictive bias also persists even when solely providing the template as input to the LLM. In Figure 3, the red line illustrates the bias associated with the template we employed in SST-2. Notably, when presented with context-free input, the LLM exhibits a tendency for a positive prediction with the possibility over 90%. We refer to this bias as Template Bias 1 (See Appendix A.4 for more details). Essentially, it characterizes the inherent bias present in the zero-shot prompt, whereby the mere utilization of a template prompts the LLM to generate predictions that favor specific answers, in the absence of any demonstration."
        },
        {
            "heading": "3.4 Calibration Before Sampling",
            "text": "We have observed that the presence of template bias may result in an unfair evaluation of the IG when sampling examples. For instance, the data example located near the red line in Figure 3, exhibits similar information to content-free input but demonstrates high IG (low conditional entropy).\nTo mitigate this template bias, we propose Calibration Before Sampling strategy2. This involves applying the linear transformation3 (Platt et al., 1999; Guo et al., 2017) to the output probabilities p = p\u03b8(y|xob, T ) to obtain calibrated probabilities q = q\u03b8(y|xob, T ) . That is,\nq = \u03c3(Wp+ b) (3)\nwhere \u03c3 denotes the normalization function, and the weight matrix W is constrained to be a diagonal matrix, known as vector scaling. To estimate W,\n1We use the term \"Template Bias\" to distinguish it from the Bias discussed in (Zhao et al., 2021).\n2We refer to the contextual calibration method in (Zhao et al., 2021) as post-calibration. Calibration alone in our paper refers to the one before sampling proposed in our work.\n3We apply the transformation to the output probabilities, although it is typically used for logits. The reason is that we only have access to the output probabilities of GPT-3 via OpenAI API. To maintain consistency across different LLMs, we apply the same transformation for GPT-2 XL and GPT-J.\nwe leverage the content-free strategy (Zhao et al., 2021). We construct the zero-shot prompt using the task-specific template T and a set of contentfree strings Dcf , which includes the empty string, \"N/A\" and \"[MASK]\". By averaging the output probabilities obtained from each content-free string as input, followed by normalization \u03c3, we obtain,\npcf = \u03c3( 1 |Dcf | \u2211\nxcf\u2208Dcf\np\u03b8(y|xcf , T )) (4)\nConsequently, to correct the bias stemming from the template, we set W = diag(pcf )\u22121 and b to be the zero vector. The calibrated conditional entropy can be then computed as,\nH(Y |xob) = \u2212 \u2211 y\u2208Y q\u03b8(y|xob, T ) log q\u03b8(y|xob, T )\n(5) The example depicted in Figure 3 demonstrates that after calibration, the data sample located around the red line shifts to a position near the balanced line with low IG (high conditional entropy).\nThe algorithm of our proposed method can be summarized as follows.\nAlgorithm 1 Maximum Information Gain Sampling with Calibration Before Sampling\nInput: unlabeled dataset Dunlab, number of examples to be sampled K, task-specific template T , LLM \u03b8\n1: for x in Dunlab do 2: Construct prompt for x using T 3: Calculate p\u03b8(y|x, T ) via LLM 4: Calculate pcf using Eq.4 5: Calculate q via calibrating p using Eq.3 6: Evaluate IG via calculating H(Y|x) using Eq.5 7: end for 8: Rank all examples in Dunlab based on IG\nOutput: Examples {xj}Kj=1 with top K highest IG;"
        },
        {
            "heading": "4 Experimental Setup",
            "text": "Evaluation Datasets. We experiment on six text classification datasets including binary sentiment analysis SST-2 (Socher et al., 2013), 6-way question classification TREC (Voorhees and Tice, 2000), 3-way textual entailment CB (De Marneffe\n.\net al., 2019), RTE (Dagan et al., 2006) from SuperGLUE (Wang et al., 2019), 4-way topic classification AGNews (Zhang et al., 2015), and 14-way DBPedia (Zhang et al., 2015). We use a fixed template (prompt format) for each dataset as per (Zhao et al., 2021). Detailed information regarding each dataset can be found in Appendix A.2.\nModels. For our experiments, we employ three distinct LLMs with different sizes: GPT-2 XL (1.5B parameters), GPT-J (Wang and Komatsuzaki, 2021) (6B parameters), and GPT-3 davinci (Brown et al., 2020) (175B parameters). We get access to GPT-3 by using OpenAI API.\nBaselines. In addition to the Random baseline, which randomly selects demonstration examples, we also incorporate the widely utilized uncertainty-based baseline MaxEntropy in active learning (AL) (Dagan and Engelson, 1995; Settles, 2009). The MaxEntropy baseline greedily selects the demonstration example with the highest conditional entropy. The sampling objective of MaxEntropy is opposite to that of our proposed method. We refer to our initial IG-based method as MaxIG, while the variant that incorporates Calibration Before Sampling (CBS) is denoted as CBS MaxIG.\nOther Details. We use the original ICL, namely the direct method, for all experiments in our work. In order to manage the inference cost during sampling, we do not evaluate the entire original train-\ning set. Instead, we first randomly sub-sample N = 100 examples from the original training set to form the Dunlab in our experiments. Subsequently, we evaluate all the examples within Dunlab and perform sampling from this subset. For each experiment involving GPT-2 XL and GPT-J, we report the results based on five different random seeds. For experiments involving GPT-3 davinci, we report results using two different random seeds. For evaluation, we sub-sample 300 samples of the test sets for all datasets as per (Zhao et al., 2021; Lu et al., 2022) due to limited resources."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Main Results",
            "text": "One-shot Learning. Our main experiments are conducted in the case of one-shot learning to mitigate potential confounding factors such as the ordering of demonstrations that could influence performance outcomes. By focusing on one-shot learning, we aim to isolate the impact of data informative ability on the performance of ICL. The main results are presented in Table 1. In terms of the average accuracies, our proposed method, CBS MaxIG, exhibits superior performances, achieving relative improvements of 12.7%, 19.4%, 10.9% over the random baseline for GPT-2 XL, GPT-J, and GPT-3 davinci, respectively. These results underscore the effectiveness of our proposed CBS MaxIG. Furthermore, when compared to the MaxIG approach, the Calibration Before Sampling strategy yields improvements in performances for GPT-J and GPT-3\ndavinci, suggesting that this strategy is particularly beneficial for larger models. Notably, our MaxIGbased methods consistently outperform the Random and MaxEntropy baselines, demonstrating the effectiveness of sampling examples with maximum IG and substantiating the validity of employing IG as a metric for evaluating data informative ability.\nFour-shot Learning. To further evaluate our method for few-shot learning, we extend our experiments to the four-shot learning scenario on SST-2 using GPT-2 XL. We consider all possible combinations of class balances and permutations for the four-shot case4, which encompass varying label distributions and orders. For each type, data examples are selected using Random, MaxEntropy, MaxIG, and CBS MaxIG methods respectively. This experimental design allows us to examine the impact of data informative ability, given that the class balance and order are fixed for one specific type. The results are presented in Figure 4. We observe that our proposed CBS MaxIG consistently outperforms all baselines in 13 out of 16 cases. For the remaining cases (PPPP, PPPN, and NNNN), we conjecture that factors other than data informative ability, such as the label distribution, may exert a\n4To sample data from different classes, we assume we have access to the target labels of the training set in this experiment. In our initial problem setting, we do not need the target labels during the sampling process.\nstronger influence on the performance. We leave comparing the impact of these different factors as future work. Overall, our experimental findings underscore the significance of the data informative ability and demonstrate the efficacy of our method in selecting the most informative data to enhance performance in scenarios involving more than one demonstration."
        },
        {
            "heading": "5.2 Integration with Existing Methods",
            "text": "In order to demonstrate that our method is orthogonal to prevailing techniques such as the postcalibration method and the order probing method, and illustrate their collaborative potential with our approach, we conduct experiments on two datasets, namely SST-2 (binary classification) and DBPedia (14-classification), for a comparative analysis.\nIntegration with Post-Calibration. To assess the performance of our method in conjunction with post-calibration, we compare the outcomes of Random and CBS MaxIG approaches on oneshot learning across three LLMs. The results, presented in Table 2, reveal that by employing postcalibration on the selected examples using CBS MaxIG, superior performance is achieved compared to random selection, across different model sizes. Furthermore, it is observed that our method without post-calibration achieves comparable or even superior results to the post-calibration coun-\nterpart specifically for GPT-3 davinci, thereby affirming the effectiveness of our proposed approach.\nIntegration with Order Probing. To assess the performance of Random and CBS MaxIG methods in conjunction with order probing and postcalibration for four-shot learning using GPT-J, we first sample four examples using Random and CBS MaxIG methods5, respectively. Subsequently, we perform ordering probing and sample the permutation with maximum global entropy on the probing set6. The results, depicted in Figure 5, reveal that ordering probing improves the performance for both sampling methods. Furthermore, it is discovered that order probing and post-calibration con-\n5Note that we naively sample the examples with the topfour highest IG without considering label distribution\n6We utilize the global entropy metric due to its superior performance in the original paper (Lu et al., 2022).\ntribute more significantly to enhancing the performance of the Random baseline compared to our CBS MaxIG approach, thereby suggesting that our proposed method is more robust to order and bias factors in comparison to the Random baseline."
        },
        {
            "heading": "5.3 Ablation on Calibration Before Sampling",
            "text": "Although the results in Table 1 demonstrate the effectiveness of our Calibration Before Sampling strategy, it should be noted that the evaluation of the MaxEntropy method may also be subject to bias introduced by the template utilized, as the calculation of conditional entropy relies on the output distribution of the LLM. To ensure a fair assessment, we apply the same Calibration Before Sampling strategy to the MaxEntropy method, named CBS MaxEntropy, and report the corresponding one-shot outcomes of SST-2 and DBPedia in Table 3. Notably, a significant decline in performance is observed across all three LLMs, with the exception of DBPedia when employing GPT-2 XL. The performance degradation can be attributed to the fact that examples selected by MaxEntropy may not have the highest entropy. Instead, these examples could correspond to the ones with high IG after calibration. Conversely, examples located near the Template Bias line in Figure 3 are the ones with high entropy after calibration, and CBS MaxEntropy selects those examples. We emphasize this observation as it further reinforces the superiority of our proposed MaxIG-based sampling methods over the MaxEntropy-based approaches. This insight highlights that the MaxEntropy method from conventional active learning, which relies on parameter updates, is not perfectly suitable for ICL where parameters remain static. In such cases, certain examples with high IG contribute more sig-\nnificantly to ICL compared to uncertain examples with high entropy. The distinction between ICL and traditional active learning settings, and how this distinction influences the choice of sampling strategy, warrants further investigation in future work."
        },
        {
            "heading": "6 Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Gold Labels vs. Random Labels",
            "text": "Since our sampling process from the unlabeled training set involves no utilization of target labels, it is pertinent to examine whether the highly informative data derived from it can benefit from the gold labels, as inspired by (Min et al., 2022). In this experiment, we compare the performance of using demonstrations selected by the Random baseline and our CBS MaxIG approach in one-shot learning using GPT-J. For each demonstration, we replace the gold label with a random label from a small discrete set of possible labels, and evaluate the performance accordingly. The results are presented in Table 4.\nWe observe a substantial decrease in performance when the gold labels are substituted with random labels for demonstrations selected by CBS MaxIG, whereas the drop is comparatively smaller for those randomly selected. This observation suggests that highly informative data heavily rely on the presence of accurate labels. We posit that providing incorrect labels for highly informative data may lead to confusion within the LLM and subsequently result in diminished information gain."
        },
        {
            "heading": "6.2 Consistency of Examples with High IG",
            "text": "In order to assess the consistency of performance across other examples with high IG, we individually select the examples with the top-K highest IG values and utilize them as demonstrations in oneshot learning. The results for each selected example\nare presented in Figure 6, together with Random baseline depicted with dashed lines. The experimental findings demonstrate that demonstrations with high IG consistently outperform the Random baseline, thereby reinforcing the significance of employing MaxIG-based sampling. Notably, it is observed that the demonstration with the third-highest IG outperforms the one with the second-highest IG. We attribute this discrepancy to potential errors in estimating the IG, which may arise from the utilization of the content-free strategy."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this study, we have highlighted the significance of the data informative ability in ICL. We have demonstrated that data samples with varying informative abilities, even when subjected to the same input distribution, order and prompt formats, make distinct contributions to the overall performance of ICL. To address this, we draw inspiration from information theory and proposed to quantify the informative ability by evaluating the information gain of data samples. Moreover, we identify the presence of template bias, which could introduce unfairness in the evaluation of IG. To mitigate this bias, we introduce the Calibration Before Sampling strategy. Through extensive experiments, we have validated the effectiveness of the proposed maximum information gain sampling strategy and calibration before sampling strategy. This validation underscores the reliability of measuring informative ability based on information gain. Furthermore, our experimental findings illustrate that our method is orthogonal to existing approaches and can synergistically collaborate with them to achieve further performance improvements. We hope that our work can provide valuable guidance for the development of data-efficient methods and facilitate the exploration of enhanced data-centric approaches for ICL in the future.\nLimitations\nThere are several limitations to consider in our work. Firstly, we focus solely on text classification tasks where the information gain can be welldefined based on the prediction distribution and tractable conditional entropy. Future research could extend our experiments to generation tasks. However, this extension poses challenges as it requires a tractable definition of information gain for output distributions that contain open words and have variable lengths.\nAdditionally, our sampling process does not explicitly consider the diversity of examples. Instead, we prioritize the data informative ability and conduct experiments in one-shot and four-shot scenarios where diversity is not as significant as in other cases with the goal of sampling many samples. Exploring methods to incorporate diversity during the sampling process is of importance for future work.\nAnother limitation lies in the model-aware evaluation of information gain, which relies on the specific LLM used. This implies that the evaluation results cannot be directly applied to different models. When using a new model, the information gain for each example needs to be recomputed, which incurs additional computational cost. Moreover, the computational cost depends on the size of the training data pool, as each candidate example in the pool needs to be evaluated. Although the parameters of LLMs do not need to be updated, the repeated inferences still consume substantial computational resources, particularly when dealing with extremely large LMs."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers for their valuable feedback on the paper. We also thank Hengguan Huang and Yisong Miao for their helpful discussions."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Implementation Details We use Pytorch and Huggingface Transformers in our implementation. We run all our evaluations on a single NVIDIA A40 GPU (48G). Our experiments should be also run on one single GPU of 24G. We access GPT-3 via the OpenAI API7.\nFor experiments of GPT-2 XL in Table 1, we rerun the Random baseline due to differences in the training set mentioned in the repository8. Nevertheless, our reimplemented results are similar to those reported in (Zhao et al., 2021). Therefore, we report the reimplemented ones for a fair comparison with our proposed method.\nA.2 Dataset Details We show the statistics of datasets in Table 5. For SST-2 (Socher et al., 2013), AGNews (Zhang et al., 2015), TREC (Voorhees and Tice, 2000), and DBPedia (Zhang et al., 2015), we use their official test sets. For CB (De Marneffe et al., 2019), RTE (Dagan et al., 2006), MNLI (Williams et al., 2018), SNLI (Bowman et al., 2015), and BoolQ (Clark et al., 2019), we use their original validation sets as test sets.\n7https://openai.com/ 8https://github.com/tonyzhaozh/few-shot-learning\nA.3 Template details for different tasks We show the templates used and corresponding label mappings for different tasks in Table 7.\nA.4 Template Bias for different tasks across three LLMs\nWe plot the Template Bias of templates used for all tasks across three LLMs in Figure 7 and Figure 8. It is observed that the template bias persists across different tasks and across LLMs with varying model sizes.\nA.5 More experiments We consider broader NLI and commonsense reasoning tasks. Specifically, we conducted additional one-shot experiments on 3-way classification MNLI, 3-way classification SNLI, and 2-way classification BoolQ using GPT-J 6B. We evaluate them with the same setting in the main experiments. Table 6 shows the performance comparison between our method and baselines. It is observed that ICL without additional reasoning techniques performs poorly on MNLI, SNLI, and BoolQ, which aligns with prior work (Chang and Jia, 2022; Hongjin et al., 2023). Nevertheless, our proposed method still outperforms other baselines on all three tasks.\n1 2 3 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-2 XL 1.5B\n1 2 3 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-J 6B\n1 2 3 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-3 davinci 175B\n(a) Template Bias of CB. Label Dictionary 1: True, 2: False, 3: Neither\n1 2 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-2 XL 1.5B\n1 2 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-J 6B\n1 2 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8 1.0 P ro ba bi lit y\nGPT-3 davinci 175B\n(b) Template Bias of RTE. Label Dictionary 1: True, 2: False\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-2 XL 1.5B\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-J 6B\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Label ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP ro\nba bi\nlit y\nGPT-3 davinci 175B\n(c) Template Bias of DBPedia. Label Dictionary 1: Company, 2: School, 3: Artist, 4: Athlete, 5: Politician, 6: Transportation, 7: Building, 8: Nature, 9: Village, 10: Animal, 11: Plant, 12: Album, 13: Film, 14: Book\nFigure 8"
        }
    ],
    "title": "Towards Informative Few-Shot Prompt with Maximum Information Gain for In-Context Learning",
    "year": 2023
}