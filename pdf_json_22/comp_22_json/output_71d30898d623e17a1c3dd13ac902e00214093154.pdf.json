{
    "abstractText": "A time series anomaly is a form of anomalous subsequence that indicates future faults will occur. The development of novel techniques for detecting this type of anomaly is significant for real-time system monitoring. Several algorithms have been used to classify anomalies successfully. However, the time series anomaly detection algorithm was not studied well. We use a new bidirectional LSTM and GRU neural networks-based hybrid autoencoder to detect if a machine is operating normally in this research. An autoencoder is trained on a set of 12 features taken from healthy operating data gathered promptly after a planned maintenance period using vibration sensors. The features taken from new data are then reconstructed using the trained model. If the model accurately reconstructs the features, the machine is in good working order. If the reconstruction exceeds a certain error threshold, the machine is functioning strangely and needs to be serviced.",
    "authors": [
        {
            "affiliations": [],
            "name": "KRISHNA PATRA"
        },
        {
            "affiliations": [],
            "name": "RABI NARAYAN SETHI"
        },
        {
            "affiliations": [],
            "name": "DHIREN KKUMAR BEHERA"
        },
        {
            "affiliations": [],
            "name": "Krishna Chandra Patra"
        },
        {
            "affiliations": [],
            "name": "Rabinarayan Sethi"
        },
        {
            "affiliations": [],
            "name": "Dhiren Kumar Behera"
        }
    ],
    "id": "SP:7cf7b4090ce70a635fecf237f66ae34848ee1995",
    "references": [
        {
            "authors": [
                "RA Ariyaluran Habeeb",
                "F Nasaruddin",
                "A Gani",
                "IA Targio Hashem",
                "E Ahmed"
            ],
            "title": "Real-time big data processing for anomaly detection: A Survey",
            "venue": "International Journal of Information Management 2019;",
            "year": 2019
        },
        {
            "authors": [
                "R Chalapathy",
                "S. Chawla"
            ],
            "title": "Deep Learning for Anomaly Detection: A Survey",
            "year": 1901
        },
        {
            "authors": [
                "T. Fu"
            ],
            "title": "A review on time series data mining",
            "venue": "Engineering Applications of Artificial Intelligence",
            "year": 2011
        },
        {
            "authors": [
                "EA Maharaj",
                "P. D\u2019Urso"
            ],
            "title": "A coherence-based approach for the pattern recognition of time series",
            "venue": "Physica A: Statistical Mechanics and its Applications",
            "year": 2010
        },
        {
            "authors": [
                "D Doma\u0144ska",
                "M. Wojtylak"
            ],
            "title": "Application of Fuzzy Time Series Models for Forecasting Pollution Concentrations",
            "venue": "Expert Syst. Appl",
            "year": 2012
        },
        {
            "authors": [
                "G Yang",
                "H Yang",
                "L. Dai"
            ],
            "title": "Time-series prediction modelling based on an efficient self-organization learning neural network. Application of fuzzy time series models for forecasting pollution concentrations",
            "venue": "doi: 10.1016/J.IFACOL.2015.08.189 1650 PATRA/Turk J Elec Eng & Comp Sci",
            "year": 2015
        },
        {
            "authors": [
                "M Canizo",
                "I Triguero",
                "A Conde",
                "E. Onieva"
            ],
            "title": "Multi-head CNN\u2013RNN for multi-time series anomaly detection: An industrial case study",
            "venue": "Neurocomputing",
            "year": 2019
        },
        {
            "authors": [
                "Hashemian HM",
                "Bean WC"
            ],
            "title": "State-of-the-Art Predictive Maintenance Techniques",
            "venue": "IEEE Transactions on Instrumentation and Measurement",
            "year": 2011
        },
        {
            "authors": [
                "A. Theissler"
            ],
            "title": "Detecting known and unknown faults in automotive systems using ensemble-based anomaly detection",
            "venue": "Knowledge-Based Systems",
            "year": 2017
        },
        {
            "authors": [
                "L Scime",
                "J. Beuth"
            ],
            "title": "Anomaly detection and classification in a laser powder bed additive manufacturing process using a trained computer vision algorithm",
            "venue": "Additive Manufacturing 2018;",
            "year": 2018
        },
        {
            "authors": [
                "C Liu",
                "S Ghosal",
                "Z Jiang",
                "S. Sarkar"
            ],
            "title": "An Unsupervised Spatiotemporal Graphical Modeling Approach to Anomaly Detection in Distributed CPS",
            "venue": "ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS)",
            "year": 2016
        },
        {
            "authors": [
                "C Angelo",
                "M Luvisotto",
                "G. Michieletto"
            ],
            "title": "Distributed Clustering Strategies in Industrial Wireless Sensor Networks",
            "venue": "IEEE Transactions on Industrial Informatics 2017;",
            "year": 2017
        },
        {
            "authors": [
                "E. Goceri"
            ],
            "title": "Skin Disease Diagnosis from Photographs Using Deep Learning",
            "venue": "Notes in Computational Vision and Biomechanics 2019;",
            "year": 2019
        },
        {
            "authors": [
                "I Yaqoob",
                "E Ahmed",
                "IAT Hashem",
                "AI Abdalla ahmed",
                "AB Gani"
            ],
            "title": "Internet of Things Architecture: Recent Advances, Taxonomy, Requirements, and Open Challenges",
            "venue": "IEEE Wireless Communications 2017;",
            "year": 2017
        },
        {
            "authors": [
                "S Du",
                "T Li",
                "Y Yang",
                "S. Horng"
            ],
            "title": "Multivariate time series forecasting via attention-based encoder\u2013decoder framework",
            "venue": "Neurocomputing",
            "year": 2020
        },
        {
            "authors": [
                "I Triguero",
                "D Garc\u00eda-Gil",
                "J Maillo",
                "J Luengo",
                "S Garc\u00eda"
            ],
            "title": "Transforming big data into smart data: An insight on the use of the k-nearest neighbors algorithm to obtain quality data. In: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery",
            "year": 2019
        },
        {
            "authors": [
                "A Fern\u00e1ndez",
                "S Garc\u00eda",
                "M Galar",
                "R Prati",
                "B Krawczyk"
            ],
            "title": "Learning from imbalanced data",
            "venue": "Cham 2020,",
            "year": 2020
        },
        {
            "authors": [
                "A Bayati",
                "KK Nguyen",
                "M. Cheriet"
            ],
            "title": "Multiple-Step-Ahead Traffic Prediction in High-Speed Networks",
            "venue": "IEEE Communications Letters 2018;",
            "year": 2018
        },
        {
            "authors": [
                "Z Li",
                "H Fang",
                "M Huang",
                "Y Wei",
                "L. Zhang"
            ],
            "title": "Data-driven bearing fault identification using improved hidden Markov model and self-organizing map",
            "venue": "Computers & Industrial Engineering",
            "year": 2018
        },
        {
            "authors": [
                "E Bigdeli",
                "B Raahemi",
                "M Mohammadi",
                "S. Matwin"
            ],
            "title": "A fast noise resilient anomaly detection using GMM-based collective labelling",
            "venue": "Science and Information Conference (SAI);",
            "year": 2015
        },
        {
            "authors": [
                "P Xiang",
                "H Zhou",
                "H Li",
                "S Song",
                "W Tan"
            ],
            "title": "Hyperspectral anomaly detection by local joint subspace process and support vector machine",
            "venue": "International Journal of Remote Sensing 2020;",
            "year": 2020
        },
        {
            "authors": [
                "P Zhou",
                "Z Li",
                "S Snowling",
                "BW Baetz",
                "D Na"
            ],
            "title": "A random forest model for inflow prediction at wastewater treatment plants",
            "venue": "Stochastic Environmental Research and Risk Assessment 2019;",
            "year": 2019
        },
        {
            "authors": [
                "C Ieracitano",
                "A Adeel",
                "FC Morabito",
                "A. Hussain"
            ],
            "title": "A novel statistical analysis and autoencoder driven intelligent intrusion detection approach",
            "venue": "Neurocomputing",
            "year": 2020
        },
        {
            "authors": [
                "X Hu",
                "J Dai",
                "Y Huang",
                "H Yang",
                "L Zhang"
            ],
            "title": "A weakly supervised framework for abnormal behavior detection and localization in crowded scenes",
            "venue": "Neurocomputing",
            "year": 2020
        },
        {
            "authors": [
                "K Singh",
                "S Rajora",
                "DK Vishwakarma",
                "G Tripathi",
                "S Kumar"
            ],
            "title": "Crowd anomaly detection using Aggregation of Ensembles of fine-tuned ConvNets",
            "venue": "Neurocomputing",
            "year": 2020
        },
        {
            "authors": [
                "J Fan",
                "Q Zhang",
                "J Zhu",
                "M Zhang",
                "Z Yang"
            ],
            "title": "Robust deep auto-encoding Gaussian process regression for unsupervised anomaly detection",
            "venue": "Neurocomputing",
            "year": 2020
        },
        {
            "authors": [
                "Z Che",
                "S Purushotham",
                "K Cho",
                "D Sontag",
                "Y. Liu"
            ],
            "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values",
            "venue": "Scientific Reports 2018;",
            "year": 2018
        },
        {
            "authors": [
                "P Malhotra",
                "L Vig",
                "G Shroff",
                "P. Agarwal"
            ],
            "title": "Long short term memory networks for anomaly detection in time series",
            "venue": "European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "HD Nguyen",
                "KP Tran",
                "S Thomassey",
                "M. Hamad"
            ],
            "title": "Forecasting and Anomaly Detection approaches using LSTM and LSTM Autoencoder techniques with the applications in supply chain management",
            "venue": "International Journal of Information Management",
            "year": 2021
        },
        {
            "authors": [
                "M Wang",
                "Z Wang",
                "J Lu",
                "J Lin",
                "Z. Wang"
            ],
            "title": "E-LSTM: An Efficient Hardware Architecture for Long Short-Term Memory",
            "venue": "IEEE Journal on Emerging and Selected Topics in Circuits and Systems 2019;",
            "year": 2019
        },
        {
            "authors": [
                "K Jiang",
                "W Wang",
                "A Wang",
                "H. Wu"
            ],
            "title": "Network Intrusion Detection Combined Hybrid Sampling with Deep Hierarchical Network",
            "venue": "IEEE Access 2020;",
            "year": 2020
        },
        {
            "authors": [
                "J Yuan",
                "Y. Tian"
            ],
            "title": "An Intelligent Fault Diagnosis Method Using GRU Neural Network towards Sequential Data in Dynamic Processes",
            "venue": "Processes 2019;",
            "year": 2019
        },
        {
            "authors": [
                "S Afrasiabi",
                "M Afrasiabi",
                "B Parang",
                "M. Mohammadi"
            ],
            "title": "Designing a composite deep learning based differential protection scheme of power transformers",
            "venue": "Applied Soft Computing Journal 2020;",
            "year": 2020
        },
        {
            "authors": [
                "JK Kimotho",
                "W. Sextro"
            ],
            "title": "An approach for feature extraction and selection from non-trending data for machinery prognosis",
            "venue": "PHM Society European Conference 2014;",
            "year": 2014
        },
        {
            "authors": [
                "DP Kingma",
                "J. Ba"
            ],
            "title": "Adam: A Method for Stochastic Optimization",
            "venue": "In: 3rd International Conference on Learning Representations",
            "year": 2015
        },
        {
            "authors": [
                "C Lee",
                "E. Choi"
            ],
            "title": "Bayes error evaluation of the Gaussian ML classifier",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing",
            "year": 2000
        },
        {
            "authors": [
                "E Choi",
                "C. Lee"
            ],
            "title": "Feature extraction based on the Bhattacharyya distance",
            "venue": "Pattern Recognition",
            "year": 2003
        },
        {
            "authors": [
                "Schuster M",
                "Paliwal KK"
            ],
            "title": "Bidirectional recurrent neural networks",
            "venue": "IEEE Transactions on Signal Processing",
            "year": 1997
        },
        {
            "authors": [
                "S Salehinejad",
                "S Sankar",
                "J Barfett",
                "E Colak",
                "S. Valaee"
            ],
            "title": "Recent Advances in Recurrent Neural Networks",
            "venue": "Neural and Evolutionary Computing",
            "year": 2021
        },
        {
            "authors": [
                "A Graves",
                "J. Schmidhuber"
            ],
            "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
            "venue": "Neural Networks",
            "year": 2005
        },
        {
            "authors": [
                "Jain LC",
                "Medsker LR"
            ],
            "title": "Recurrent Neural Networks: Design and Applications",
            "year": 1999
        },
        {
            "authors": [
                "N Elsayed",
                "A Maida",
                "M. Bayoumi"
            ],
            "title": "Effects of different activation functions for unsupervised convolutional LSTM spatiotemporal learning. Advances in Science, Technology and Engineering Systems",
            "year": 2019
        },
        {
            "authors": [
                "A Graves",
                "N Jaitly",
                "AR. Mohamed"
            ],
            "title": "Hybrid speech recognition with Deep Bidirectional LSTM",
            "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding",
            "year": 2013
        },
        {
            "authors": [
                "K Cho",
                "BV Merrienboer",
                "C Gulcehre",
                "D Bahdanau",
                "F Bougares"
            ],
            "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
            "venue": "Computation and Language",
            "year": 2014
        },
        {
            "authors": [
                "J Chung",
                "C Gulcehre",
                "K Cho",
                "Y. Bengio"
            ],
            "title": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling",
            "venue": "Neural and Evolutionary Computing",
            "year": 2014
        },
        {
            "authors": [
                "M Pahl",
                "F. Aubet"
            ],
            "title": "All Eyes on You: Distributed Multi-Dimensional IoT Microservice Anomaly Detection",
            "venue": "14th International Conference on Network and Service Management (CNSM),",
            "year": 2018
        },
        {
            "authors": [
                "AA Diro",
                "N. Chilamkurti"
            ],
            "title": "Distributed attack detection scheme using deep learning approach for Internet of Things",
            "venue": "Future Generation Computer Systems",
            "year": 2018
        },
        {
            "authors": [
                "SW Azumah",
                "N Elsayed",
                "V Adewopo",
                "S Zaghloul",
                "C. Li"
            ],
            "title": "A deep lstm based approach for intrusion detection iot devices network in smart home",
            "venue": "IEEE 7th World Forum on Internet of Things (WF-IoT)",
            "year": 2021
        },
        {
            "authors": [
                "I Alrashdi",
                "A Alqazzaz",
                "E Aloufi",
                "R Alharthi",
                "M Zohdy"
            ],
            "title": "AD-IoT: Anomaly detection of IoT cyberattacks in smart city using machine learning",
            "venue": "IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC)",
            "year": 2019
        },
        {
            "authors": [
                "CD McDermott",
                "F Majdani",
                "AV. Petrovski"
            ],
            "title": "Botnet Detection in the Internet of Things using Deep Learning Approaches",
            "venue": "In: International Joint Conference on Neural Networks (IJCNN) 2018;pp",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Key words: Autoencoders, anomaly detection, deep learning, predictive maintenance"
        },
        {
            "heading": "1. Introduction",
            "text": "Anomaly detection in time series is a critical subject in the fields of computer science and data mining [1\u20134]. It was widely used in a variety of practical uses, including signal processing, pattern identification [5], mathematical finance, forecasting the weather [6], and control engineering [7]. It has become a requirement in the industrial sector, as faults that go undetected can result in a catastrophic tragedy [8]. Industrial systems are subjected to a great deal of stress daily and are failure-prone, thus detection of anomalies can help to improve system availability and performance. This has a direct impact on productivity and lowers operating and maintenance expenses [9]. As a result, several studies in this field may be found in a variety of industries, including automotive [10], manufacturing [11], energy [12], Sensing networks in the industry [13], or even medicine, which uses a variety of pictures [14].\nA time-series anomaly is described as an out of the ordinary pattern that deviates from predicted behaviour. There have been three types of categories of time series anomalies: a single point, a context, and a group [3]. Individual samples that exceed the usual range are referred to as point anomalies and can be discovered using an ultralimit alert. When the sequences\u2019 context shifts, contextual anomalies develop, which is always tightly tied to the temporal element. Multiple occurrences of data may constitute an anomaly as a sequential sequence, however, the individuals in this series may be irrelevant. As a result, it is difficult for models in detecting anomalies to efficiently capture distinct properties of several abnormalities in a time series. \u2217Correspondence: kcpmechcvrce@gmail.com\nThis work is licensed under a Creative Commons Attribution 4.0 International License.\nAs a result of the growth of industry and the Internet of Things (IoT), in recent years, there have been breakthroughs in spotting anomalies in time series data [15]. Multiple sensors are equipped to capture a large number of time series, and technology has provided more efficient services to companies and dependable monitoring systems. Multiple events govern industrial systems, making it difficult to identify characteristics and particular locations in anomalous time series with several variables. As a result, intricate industrial systems include diverse sensors, which can have a variety of properties, sizes, and characteristics, as well as high dimensionality and dependence on space and time [16]. This fact frequently means that data has been cleaned, valuable features extracted, or the dimensionality of the data has been reduced [17]. This is usually a time-consuming procedure that necessitates domain knowledge. Aside from these difficulties, there are other intrinsic problems, like the difficulty to design boundaries between normal and abnormal data, and there is a lot of noise caused by a faulty sensor or incorrect measurements, which can cause false alarms to occur. Due to the scarcity of anomalous observations, data imbalance [18] is a prevalent problem in anomaly detection settings, affecting the models\u2019 robustness in detecting anomalies.\nMany researchers have looked into time series modelling, particularly with traditional statistical approaches like ARIMA [19], HMM [20], and GMM [21]. SVM [22] and Random Forest [23] for example, is a type of machine learning method. Statistical methods, on the other hand, have a hard time dealing with unknown statistical traits and data with a lot of dimensions, whereas methods for machine learning necessitate the generation and pre-processing of time-consuming features. Fortunately, for multitime series modelling, deep learning has shown promise, and numerous studies have used deep learning methods to solve these difficulties in the identification of anomalies in multiple time series. [8] provides a CNN and RNN-based supervised anomaly detection approach, which shows promise on industrial multitime series. To protect information technology infrastructures from harmful malware attacks, unique statistical analysis and autoencoder are used to create an intelligent method that has been devised [24]. Hu [25] uses Faster R-CNN to distinguish items in the scene and trains an enhanced detection using an SVM-based classifiers and locate anomalous behaviours. To anticipate anomalies in crowd frame sequences, a suggested method [26] leverages a collection of fine-tuned CNN architectures for training variations of SVM classifiers. To answer the industrial anomaly detection problem, Fan [27] introduces a unique anomaly detection using a hybrid unsupervised approach that combines convolutional neural network autoencoder with Gaussian process regression.\nRecurrent neural networks, or LSTM networks, have demonstrated anomaly detection and performance in sequence learning problems that is at the cutting edge [28, 29]. The input pattern is converted into a fixedlength latent vector representation by a long short-term memory encoder. The decoder, which is another LSTM network, then reconstructs the input sequence using the latent representation. When compared to autoencoders, autoencoders based on LSTM achieve even greater results in anomaly detection [30]. However, only complicated LSTM networks with significant computational complexity and enormous memory requirements may produce considerable gains in detection accuracy [31]. Intrusion detection using a deep neural network (CNN + BiLSTM) has been suggested by Jiang et al. [32]. Neural networks with gated recurrent units (GRUs) have been discovered to have simpler architectures and training times faster than LSTM neural networks. Yuan and Tian [33] developed a GRU neural network-based dynamic process fault detection technique, to extract dynamic properties and categorise industrial processes, GRU neural networks were used. Afrasiabi et al. [34] used a differential protection strategy that integrated light-gated recurrent neural networks units and CNNs to identify inrush current in power transformers caused by an internal fault. However, despite their many advantages, these data-driven approaches necessitate a significant number of abnormal samples, which can be difficult to get in\nreality. In addition, the detection of aberrant operating conditions in limited samples still has a lot of room for improvement. Unsupervised machine intrusion detection system in industry time-series data employing a novel framework, BiLSTM-GRU, is proposed in this research, and several enhancements are given to address those critical issues. The following are the primary contributions of this work:\n\u2022 While a model can be trained on raw data, it is often preferable to extract key features for the best accuracy. After extracting key features from industrial time-series data, we use the t-test, Wilcoxon, and Bhattacharyya tests to rank them.\n\u2022 Second, BiLSTM-GRU is offered as a unique framework for anomaly detection in industrial time series. The use of a sliding window to iteratively update the algorithm\u2019s parameters, which allows for quick identification of abnormalities, necessitates a constant amount of data, ensuring linear complexity in space and time.\n\u2022 In machine anomaly detection, this model can be used to perform regression and classification tasks. This model will assist to increase model resilience in time series anomaly detection when normal and abnormal data are imbalanced.\n\u2022 Lastly, tests on data sets are carried out to verify the efficacy of the proposed frameworks and effectiveness of a unique threshold-setting technique, with the outcomes demonstrating that our strategy outperforms comparable models in terms of robustness and performance in detecting anomalies under various imbalanced dataset ratios.\n\u2022 The model combines the memory strength of a linear regression model with the generalisation strength of a deep GRU neural network model for increased accuracy."
        },
        {
            "heading": "2. Dataset preprocessing",
            "text": "To create the dataset, the test rig Figure 1a consists of a 0.5 hp electric motor attached to the left, driving a shaft on which two bearings are mounted at both ends. The vibration data was obtained using a 3-axis accelerometer attached to the housing magnetically through Ch1, Ch2, and Ch3. The data was collected with a sampling rate of 70,000 samples per second from our test rig before and after scheduled maintenance periods and was processed using MATLAB. We can presume that all of the \u201dafter\u201d data is healthy because maintenance was completed appropriately. The \u201dbefore\u201d data, on the other hand, is less clear: the machine may have been brought down for maintenance owing to a fault, but machines are frequently taken offline for scheduled maintenance even when they are working normally. Let\u2019s view the data before and after maintenance to better comprehend it. We can observe that the data before and after maintenance Figure 1b looks notably different if we go through each of the members."
        },
        {
            "heading": "3. Featurization",
            "text": "Feature engineering is critical in every process of machine learning and can have a substantial impact on an algorithm\u2019s performance. The machine learning algorithm can benefit from feature engineering in identifying the underlying patterns and so increase the model\u2019s accuracy. Featurization of raw signal data requires extracting features from raw signals in a variety of domains, including time, time-frequency, and so on. Signals of vibration from machinery components are regarded as nonstationary in general. The term \u201dnonstationary signals\u201d refers\nto transmissions whose frequency change over time [35]. To convey the changing nature of the character over time present in a signal, it is necessary to extract time-domain and time-frequency-domain features. Multiple temporal and time-frequency domain features were recovered from raw signal data in this paper. Mean, variance, standard deviation, and root mean square (RMS) are some of the statistical temporal domain traits we glean. Since these signals are not stationary, features like kurtosis and skewness are retrieved as well. For anomaly, the kurtosis increases its value, and skewness shifts to the negative or positive side. Dimensionless characteristics such as the crest factor, shape factor, and impulse factor are also derived in addition to previous statistical features. The shape factor is influenced by its shape, although it is unaffected by its dimension. Signal to noise ratio (SNR) is the ratio of fundamental signal amplitude to noise signal amplitude. Total harmonic distortion (THD) is the ratio of the sum of the fundamental signal component\u2019s harmonics. For characterisation, the first 5 to 6 harmonic components are usually used. The signal to noise and distortion ratio (SINAD) is the\nproportion of the signal amplitude (in rms) to the sum of the other spectral components (in rms). Its value is approximately equal to THD + noise. Table 1 lists all 10 features retrieved from the raw signal data, as well as the mathematical techniques utilised to extract them.\nWhile a model can be trained on raw data, it is often preferable to extract key features for the best accuracy. Review and preprocess our data interactively using MATLAB software, establish data label as a condition variable, then extract time and frequency-domain features and rank them to see which are the most successful. This is because the data label indicates the machine\u2019s condition: before and after maintenance, and we can study these datasets individually. The goal is to teach a model to distinguish between these two scenarios. The distributions divided by labels for various features retrieved from Ch1, Ch2, and Ch3 are visualized in Figure 2.\nWhen a 3-axis accelerometer is employed, one true radial measurement, one tangential measurement, and one axial measurement are obtained, depending on the position of the accelerometer. As a result, each channel is equally important. We need to figure out which features are optimal for each channel. The histogram plot and feature importance ranking approach are used to find the best features. The figure 2 shows the parameters that determine the histogram\u2019s content and resolution. The resulting feature differs in Ch1, Ch2, and Ch3 because the vibration amplitude varies in the X, Y, and Z axes. By evaluating which features clearly separate blue data from orange data, we may get a general notion of which ones are effective. THD, SINAD, and SNR appear to be effective for channel-3, with relatively minor overlap. SINAD and mean, on the other hand, have a lot of overlap for channel-1. As a result, these characteristics appear to be ineffective. In addition, as described in subsection 3.1, we apply feature importance ranking to improve feature selection."
        },
        {
            "heading": "3.1. Feature importance ranking for deep learning",
            "text": "The task of measuring the contributions of individual input features (variables) to the performance of an unsupervised learning model is known as feature importance ranking (FIR) in deep learning. FIR has become one of the most powerful instruments in explainable/interpretable AI [36] for facilitating the understanding of a\nlearning system\u2019s decision-making and the discovery of essential features in a certain domain. To rank all of the features extracted from the raw signal data, we employ the t-test, Wilcoxon rank-sum test, and Bhattacharyya distance. A t-test is any statistical hypothesis test in which the test statistic follows a Student\u2019s t-distribution when the null hypothesis is true. The Wilcoxon rank-sum test is a nonparametric test of the null hypothesis that the likelihood of X being larger than Y given randomly picked values X and Y from two populations is equal to the probability of Y being greater than X . For two classes \u03c71, \u03c72 the Bayes classifier\u2019s minimal attainable classification error is expressed as:\nPe = \u222b \u221e \u2212\u221e min [P (\u03c7i) p (x | \u03c7i) , P (\u03c7j) p (x | \u03c7j)] dx. (1)\nIn the general case, analytic computation of Equation (1) is not possible. An upper bound, on the other hand, can be calculated. The inequality serves as the foundation for the derivation.\nmin [a, b] \u2264 asb1\u2212s for a, b \u2265 0, and 0 \u2264 s \u2264 1 (2)\nCombining Equation (1) and Equation (2), we get\nPe \u2264 P (\u03c7i)s P (\u03c7j)1\u2212s \u222b \u221e \u2212\u221e p (x | \u03c7i)s p (x | \u03c7j)1\u2212s dx \u2261 CB (3)\nCB is recognized as the Chernoff bound. By decreasing CB concerning s , the minimum bound can be found. The bound results for s =1 /2 in a specific form:\nPe \u2264 CB = \u221a P (\u03c7i)P (\u03c7j) \u222b \u221e \u2212\u221e \u221a p (x | \u03c7i) p (x | \u03c7j)dx. (4)\nAfter a little algebra, the following for Gaussian distributions N (\u00b5i,\u03a3i) ,N (\u00b5j ,\u03a3j) .\nCB = \u221a P (\u03c7i)P (\u03c7j)e (\u2212B), (5)\nwhere\n\u03f5CB = \u221a P (\u03c9i)P (\u03c9j) exp(\u2212B)\nB = 1\n8\n( \u00b5i \u2212 \u00b5j )T (\u03a3i +\u03a3j 2 )\u22121 ( \u00b5i \u2212 \u00b5j ) + 1 2 ln \u2223\u2223\u2223\u03a3i+\u03a3j2 \u2223\u2223\u2223\u221a |\u03a3i\u2225\u03a3j |\n(6)\nand the determinant of the related matrix is denoted by | | . The Bhattacharyya distance is a class separability measure that is defined by the letter B. Based on empirical research involving normal distributions, [37] proposes an equation that links the optimal Bayesian error with the Bhattacharyya distance. This was then used in [38] for feature selection. We may iterate on the features and rank them using the aforesaid feature ranking test. For each channel, we will use the top four ranking features.\n1. Ch1: RMS, crest factor, Std, kurtosis\n2. Ch2: Mean, skewness, Std, RMS\n3. Ch3: THD, crest factor, SNR, SINAD\nThe distributions for various features extracted from Ch1, Ch2, and Ch3 are visualised in Figure 3 and are divided by labels. The X-axis displays the value of the features for the Bhattacharya, Wilcoxon, and t-tests."
        },
        {
            "heading": "4. Background",
            "text": ""
        },
        {
            "heading": "4.1. Bidirectional LSTM",
            "text": "In 1997, Schuster and Paliwal [39] created the bidirectional recurrent neural network (BRNN), which connects the recurrent architecture, which has two hidden layers in the opposite direction to generate a result. This bidirectional tendency boosts the recurrent architecture\u2019s input data versatility. Furthermore, the recurrent bidirectional network improves inputs for the future state to the existing state\u2019s reachability and does not necessitate the fixation of input data before the training phase [40].\nIn this work, the recurrent unit of the bidirectional recurrent architecture was the long short-term memory (LSTM) because it avoids the problem of vanishing/exploding gradients that occurs in recurrent neural\nnetworks (RNN). Graves et al. [41] also found that utilising the LSTM in a bidirectional architecture improved classification accuracy significantly. Figure 4a depicts the LSTM architecture, with ht , Ct , and xt representing the output of the LSTM at time t, the memory state cell, and input at the moment t. The symbol \u2299 symbolises the multiplication of elements (Hadamard). The hyperbolic tangent function is tanh , and the logistic sigmoid function is \u03c3 [42]. The values of the LSTM components are determined as follows:\nit = \u03c3 (Wxixt + Uhiht\u22121 + bi) (7)\ngt = tanh (Wxgxt + Uhght\u22121 + bg) (8)\nft = \u03c3 (Wxfxt + Uhfht\u22121 + bf ) (9)\nOt = \u03c3 (Wxoxt + Uhoht\u22121 + bo) (10)\nCt = ft \u2299 Ct\u22121 + it \u2299 gt (11)\nht = tanh (Ct)\u2299Ot (12)\nThe input, forget, and output gates are denoted by it , ft , and ot respectively. The input-update value is gt . The biases of each gate are bi , ba , bf , and bo . The feedforward weights are W, and the recurrent weights are U. Update of the input and activation of the output are the two activation units in the model. It is recommended that you employ the tanh activation function [43]. The activation function tanh is a saturating activation function. In RNNs, it is commonly employed as the recurrent activation function.\nFigure 4b depicts the layout training process with BiLSTM, in which the Bi-LSTM algorithm calculates\ntwo hidden sequences: the \u2212\u2192h hidden layer (ahead) and \u2190\u2212h the hidden layer (backwards). Iterate the first layer increasing from time t = 1to t = T and the backward sequence layer decreasing from t = T to t = 1 time to create the output sequence y [44]. The following formulas are used to calculate the output and forward/backward sequences:\nyt = Wh\u20d7y \u2212\u2192 ht +W\u2190\u2212hy \u2190\u2212 ht + by (13)\n\u2212\u2192 ht = H ( Wxh\u20d7xt +Wh\u20d7h\u20d7 \u2212\u2212\u2192 ht\u22121 + bh\u20d7 ) (14)\n\u2190\u2212 ht = H ( W\nx \u2190\u2212 h xt +W\u2190\u2212h\u2190\u2212h \u2190\u2212\u2212 ht+1 + b\u2190\u2212h\n) (15)\nThe bidirectional structure takes into account the recurrent system\u2019s temporal dynamics as the model is fed both forward and backward [39]."
        },
        {
            "heading": "4.2. Gated recurrent unit",
            "text": "In 2014, Cho et al. [45] introduced gated recurrent units (GRUs) as a method for gating recurrent neural networks. The GRU is akin to a forget gated long short-term memory (LSTM) as seen in Figure 4c, but it does not have a gate that controls output, so it has limited options. Modelling polyphonic music, modelling of speech signals, and activities using natural language processing, GRU\u2019s performance was shown to be on par with that of the LSTM in some circumstances. On datasets that are smaller and less common, GRUs have been proven to be more effective [46]. The values of the GRU neural network components are determined as follows:\nInitially, the output vector for t=0 is h0=0.\nzt = \u03c3 (Wxixt + Uhiht\u22121 + bi) (16)\nrt = \u03c3 (Wxfxt + Uhfht\u22121 + bf ) (17)\nh\u0302t = tanh (Wxgxt + Uhg (rt \u2299 ht\u22121) + bg) (18)\nht = (1\u2212 zt)\u2299 ht\u22121 + zt \u2299 h\u0302t (19)\nFigure 5 depicts the suggested BiLSTM-GRU based anomaly detection model. There are nine layers in the suggested paradigm. The BiLSTM layer, which consists of 16 hidden units, minimizes the training parameters, total computational cost, and prevents the training model from overfitting. The GRU components have 32 hidden units come next, followed by BiLSTM layers. These layers are in charge of figuring out the network flow\u2019s temporal relationship and changing the temporal dynamics, owing to the GRU recurrent neural networks bidirectional design.\nThe loss function for this model autoencoder neural networks is MSE.\nLossMSE = 1\nn n\u2211 i=1 (xi \u2212 x\u0302i)2 (20)\nThe loss function error is reduced by adjusting network structure parameters. The benefit of this strategy over typical autoencoder neural models is that when the current hidden layer\nreceives hidden info on the state from the prior time ht\u22121 , it has been added to the present time. The hidden states are influenced not just by the most recent input xt , but also by the info stored in a hidden state during the update. The network aids in the re-construction of present input data by retrieving their temporal features, and the use of timing dependence improves anomalous sample reconstruction accuracy, allowing for the generation of lengthy sequence abnormal samples."
        },
        {
            "heading": "5. Autoencoder model training",
            "text": "To identify if a machine is operating normally, we train a BiLSTM-GRU autoencoder. An autoencoder is trained on a set of 12 features taken from healthy operating data gathered promptly after a planned maintenance period using vibration sensors. MATLABr was used to compute features from raw data. There are two labelled states in the data: before and after. Data acquired before and after maintenance is referred to as this. We will presume that the data gathered after maintenance reflects a typical (healthy) working condition. Because we were performing regular maintenance, we may not be able to claim the same for the previous data; this\ndata could be normal or abnormal. For training, we choose 90% of healthy data out of 17,642 raw data. The proposed model was implemented on a Ryzen 5-3600 CPU and NVIDIA GTX 1650 super graphic card with Windows 10 OS. The full Training progress is depicted in Figure 6."
        },
        {
            "heading": "6. Experimental results and analysis",
            "text": "When the autoencoder receives data that does not appear to be healthy, it will have a tougher job reconstructing the signal. This will suggest an anomaly. A sample from the before and after maintenance should be extracted and visualized as in Figure 7a. Note that the charts below compare the error in each of the 12 characteristics\u2019 values (indicated on the x-axis). We can see that features 10\u201312 in this sample do not reconstruct adequately for the anomalous input, resulting in a significant error value. This could be a sign that something isn\u2019t quite right.\nFigure 7b. shows the extract data of before and after maintenance using our suggested model. The reconstruction error for the data before maintenance is much higher than the data after maintenance, as seen in the graph. Because the autoencoder was trained on the data before maintenance, it will be able to reconstruct similar signals more accurately. Four standard metrics are used to calculate the recall, precision, accuracy, and F1-score of the proposed BiLSTM-GRU model for anomaly detection of rotating equipment. Based on the operator\u2019s experience, we will set a 0.5 threshold limit for our machine. As a result, an anomaly is defined as a point with a reconstruction error that is 0.5 times larger than the mean across all observations.\nRecall = TP\nTP + FN (21)\nPrecision = TP\nTP + FP (22)\nAccuracy = TP + TN\nTP + TN + FP + FN (23)\nF1 = 2\u00d7 Recall \u00d7 Precision Recall + Precision\n(24)\nTrue positives, true negatives, false negatives, and false positives are represented by TP, TN, FN, and FP, respectively."
        },
        {
            "heading": "6.1. Discussion",
            "text": "We compared the performance of our proposed BiLSTM-GRU model to state-of-the-art machine learning models in detecting rotating machinery abnormalities. The ROC curve of the model is shown in Figure 8a. The receiver operator characteristic (ROC) curve is a tool for evaluating binary classification issues. It is a probability curve that compares the TPR (true positive rate) to the FPR (false positive rate) at various threshold values, allowing the \u2018signal\u2019 to be distinguished from the \u2019noise\u2019. The area under the curve (AUC) is a summary of the ROC curve that measures a classifier\u2019s ability to distinguish between classes.\nThe following is a list of additional hyperparameters to consider. The number of epochs is 500, with a minibatch size of 500. The learning rate starts at 0.001, decays to 0.01, and the models are trained with a momentum of 0.9 using the adaptive moment estimation (Adam) optimizer. Figure 8b depicts the suggested model\u2019s confusion matrix and accuracy at the 0.5 threshold limit. The test is performed ten times to demonstrate that the proposed model is stable. Table 2a shows the average diagnosis performance of all studies. Many studies on anomaly diagnostic algorithms based on shallow and deep learning have been published in recent years. The suggested method is compared to recent approaches such as K-means [47], ANN [48], LSTM [49], RF-ET [50], and BiLSTM-RNN [51] to demonstrate its originality in the field of anomaly diagnosis. The average diagnostic accuracy of each technique is shown in Table 2b. When the results of the proposed strategy are compared to the results of other methods, it is obvious that the proposed strategy enhances diagnosis accuracy, proving its efficacy. Our suggested BiLSTM-GRU based model outperforms state-of-the-art anomaly detection methods, as shown in Table 2b."
        },
        {
            "heading": "7. Conclusion",
            "text": "For industrial rotating machinery, the suggested BiLSTM-GRU anomaly detection model outperforms state-ofthe-art algorithms. It can also be implemented and used on any industrial machine. It can also be linked to an alert system that either controls machine anomalies or identifies any irregularities and sends a notification to\npermitted members and takes necessary steps to mitigate the existing threat to maintain control of the situation and to protect their industrial machinery.\nA method for ranking all the features extracted from the time-series data from a 3-axis accelerometer was also proposed. When considering variations in machine wear circumstances, BiLSTM-GRU anomaly detection model techniques could be used reliably in industrial use. In practice, the machine maker could calibrate our automatic technique once by upgrading their threshold limits, removing the requirement for continuous recalibration and hand-tuning by the machine operator. The methods given here were easy to apply to a real-time monitoring procedure, and the anomalies response was instantaneous.\nDuring training, this model obtains optimal performance in a very short time. The broad component\u2019s memorising and regularisation capabilities can be increased in the future, allowing it to provide significantly better accuracy when paired with deep components. New reduction and feature crossing strategies can be developed to further improve the situation. Future research could include combining other regression models with additional deep neural network techniques."
        }
    ],
    "title": "Anomaly detection in rotating machinery using autoencoders based on bidirectional LSTM and GRU neural networks",
    "year": 2023
}