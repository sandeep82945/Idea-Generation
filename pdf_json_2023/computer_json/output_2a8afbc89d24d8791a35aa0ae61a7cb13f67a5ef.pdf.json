{
    "abstractText": "Current Spoken Dialogue Systems (SDSs) often serve as passive listeners that respond only after receiving user speech. To achieve human-like dialogue, we propose a novel future prediction architecture that allows an SDS to anticipate future affective reactions based on its current behaviors before the user speaks. In this work, we investigate two scenarios: speech and laughter. In speech, we propose to predict the user\u2019s future emotion based on its temporal relationship with the system\u2019s current emotion and its causal relationship with the system\u2019s current Dialogue Act (DA). In laughter, we propose to predict the occurrence and type of the user\u2019s laughter using the system\u2019s laughter behaviors in the current turn. Preliminary analysis of human-robot dialogue demonstrated synchronicity in the emotions and laughter displayed by the human and robot, as well as DA-emotion causality in their dialogue. This verifies that our architecture can contribute to the development of an anticipatory SDS.",
    "authors": [
        {
            "affiliations": [],
            "name": "You Do"
        },
        {
            "affiliations": [],
            "name": "Yuanchao Li"
        },
        {
            "affiliations": [],
            "name": "Koji Inoue"
        },
        {
            "affiliations": [],
            "name": "Leimin Tian"
        },
        {
            "affiliations": [],
            "name": "Changzeng Fu"
        },
        {
            "affiliations": [],
            "name": "Carlos Ishi"
        },
        {
            "affiliations": [],
            "name": "Hiroshi Ishiguro"
        },
        {
            "affiliations": [],
            "name": "Tatsuya Kawahara"
        },
        {
            "affiliations": [],
            "name": "Catherine Lai"
        }
    ],
    "id": "SP:cd98ba8668938b50e6e5089a61acb83e54a239ec",
    "references": [
        {
            "authors": [
                "Jaime C Acosta",
                "Nigel G Ward"
            ],
            "title": "Achieving rapport with turn-by-turn, user-responsive emotional coloring",
            "venue": "Speech Communication",
            "year": 2011
        },
        {
            "authors": [
                "Luc H Arnal",
                "Anne-Lise Giraud"
            ],
            "title": "Cortical oscillations and sensory predictions",
            "venue": "Trends in cognitive sciences 16,",
            "year": 2012
        },
        {
            "authors": [
                "Carlos Busso",
                "Murtaza Bulut",
                "Chi-Chun Lee",
                "Abe Kazemzadeh",
                "Emily Mower",
                "Samuel Kim",
                "Jeannette N Chang",
                "Sungbok Lee",
                "Shrikanth S Narayanan"
            ],
            "title": "IEMOCAP: Interactive emotional dyadic motion capture database. Language resources and evaluation",
            "year": 2008
        },
        {
            "authors": [
                "Shuyi Cao",
                "Lizhen Qu",
                "Leimin Tian"
            ],
            "title": "Causal Relationships Between Emotions and Dialog Acts",
            "venue": "9th International Conference on Affective Computing and Intelligent Interaction (ACII)",
            "year": 2021
        },
        {
            "authors": [
                "Cristiano Castelfranchi",
                "Maria Miceli"
            ],
            "title": "Anticipation and emotion",
            "venue": "In Emotion-oriented systems",
            "year": 2011
        },
        {
            "authors": [
                "Richard Craggs",
                "Mary McGee Wood"
            ],
            "title": "Annotating emotion in dialogue",
            "venue": "In Proceedings of the Fourth SIGdial Workshop of Discourse and Dialogue",
            "year": 2003
        },
        {
            "authors": [
                "Richard J Davidson"
            ],
            "title": "Toward a biology of personality and emotion",
            "venue": "Annals of the New York academy of sciences 935,",
            "year": 2001
        },
        {
            "authors": [
                "Daiga Deksne",
                "Raivis Skadin"
            ],
            "title": "Predicting Next Dialogue Action in Emotionally Loaded Conversation",
            "venue": "In Proceedings of the Future Technologies",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Estow",
                "Jeremy P Jamieson",
                "Jennifer R Yates"
            ],
            "title": "Self-monitoring and mimicry of positive and negative social behaviors",
            "venue": "Journal of Research in Personality 41,",
            "year": 2007
        },
        {
            "authors": [
                "Ceenu George",
                "Philipp Janssen",
                "David Heuss",
                "Florian Alt"
            ],
            "title": "Should I interrupt or not? Understanding interruptions in head-mounted display settings",
            "venue": "In Proceedings of the 2019 on Designing Interactive Systems",
            "year": 2019
        },
        {
            "authors": [
                "Dylan F Glas",
                "Takashi Minato",
                "Carlos T Ishi",
                "Tatsuya Kawahara",
                "Hiroshi Ishiguro"
            ],
            "title": "Erica: The erato intelligent conversational android",
            "venue": "In 2016 25th IEEE International symposium on robot and human interactive communication (RO-MAN)",
            "year": 2016
        },
        {
            "authors": [
                "Koji Inoue",
                "Divesh Lala",
                "Tatsuya Kawahara"
            ],
            "title": "Can a robot laugh with you?: Shared laughter generation for empathetic spoken dialogue",
            "venue": "Frontiers in Robotics and AI",
            "year": 2022
        },
        {
            "authors": [
                "Koji Inoue",
                "Pierrick Milhorat",
                "Divesh Lala",
                "Tianyu Zhao",
                "Tatsuya Kawahara"
            ],
            "title": "Talking with ERICA, an autonomous android",
            "venue": "In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
            "year": 2016
        },
        {
            "authors": [
                "Lakshmish Kaushik",
                "Abhijeet Sangwan",
                "John HL Hansen"
            ],
            "title": "Laughter and filler detection in naturalistic audio",
            "year": 2015
        },
        {
            "authors": [
                "Tatsuya Kawahara",
                "Takashi Yamaguchi",
                "Koji Inoue",
                "Katsuya Takanashi",
                "Nigel GWard"
            ],
            "title": "Prediction and Generation of Backchannel Form for Attentive Listening Systems",
            "venue": "In Interspeech",
            "year": 2016
        },
        {
            "authors": [
                "Matthias Kraus",
                "Nicolas Wagner",
                "Wolfgang Minker"
            ],
            "title": "Effects of proactive dialogue strategies on human-computer trust",
            "venue": "In Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization",
            "year": 2020
        },
        {
            "authors": [
                "Divesh Lala",
                "Pierrick Milhorat",
                "Koji Inoue",
                "Masanari Ishida",
                "Katsuya Takanashi",
                "Tatsuya Kawahara"
            ],
            "title": "Attentive listening system with backchanneling, response generation and flexible turn-taking",
            "venue": "In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue",
            "year": 2017
        },
        {
            "authors": [
                "Divesh Lala",
                "Shizuka Nakamura",
                "Tatsuya Kawahara"
            ],
            "title": "Analysis of Effect and Timing of Fillers in Natural Turn-Taking",
            "venue": "In Interspeech",
            "year": 2019
        },
        {
            "authors": [
                "Yuanchao Li"
            ],
            "title": "Towards Improving Speech Emotion Recognition for In- Vehicle Agents: Preliminary Results of Incorporating Sentiment Analysis by Using Early and Late Fusion Methods",
            "venue": "In Proceedings of the 6th International Conference on Human-Agent Interaction",
            "year": 2018
        },
        {
            "authors": [
                "Yuanchao Li",
                "Peter Bell",
                "Catherine Lai"
            ],
            "title": "Fusing ASR outputs in joint training for speech emotion recognition",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
            "year": 2022
        },
        {
            "authors": [
                "Yuanchao Li",
                "Carlos Toshinori Ishi",
                "Koji Inoue",
                "Shizuka Nakamura",
                "Tatsuya Kawahara"
            ],
            "title": "Expressing reactive emotion based on multimodal emotion recognition for natural conversation in human\u2013robot interaction",
            "venue": "Advanced Robotics 33,",
            "year": 2019
        },
        {
            "authors": [
                "Yuanchao Li",
                "Catherine Lai"
            ],
            "title": "Robotic Speech Synthesis: Perspectives on Interactions, Scenarios, and Ethics",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Yuanchao Li",
                "Catherine Lai",
                "Divesh Lala",
                "Koji Inoue",
                "Tatsuya Kawahara"
            ],
            "title": "Alzheimer\u2019s Dementia Detection through Spontaneous Dialogue with Proactive Robotic Listeners",
            "venue": "In HRI",
            "year": 2022
        },
        {
            "authors": [
                "Yuanchao Li",
                "Tianyu Zhao",
                "Xun Shen"
            ],
            "title": "Attention-based multimodal fusion for estimating human emotion in real-world HRI",
            "venue": "In Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction",
            "year": 2020
        },
        {
            "authors": [
                "Nurul Lubis",
                "Sakriani Sakti",
                "Koichiro Yoshino",
                "Satoshi Nakamura"
            ],
            "title": "Eliciting positive emotion through affect-sensitive dialogue response generation: A neural network approach",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Navonil Majumder",
                "Soujanya Poria",
                "Devamanyu Hazarika",
                "Rada Mihalcea",
                "Alexander Gelbukh",
                "Erik Cambria"
            ],
            "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Chiara Mazzocconi",
                "Ye Tian",
                "Jonathan Ginzburg"
            ],
            "title": "What\u2019s your laughter doing there? A taxonomy of the pragmatic functions of laughter",
            "venue": "IEEE Transactions on Affective Computing",
            "year": 2020
        },
        {
            "authors": [
                "Gary McKeown",
                "Michel F Valstar",
                "Roderick Cowie",
                "Maja Pantic"
            ],
            "title": "The SEMAINE corpus of emotionally coloured character interactions",
            "venue": "In 2010 IEEE International Conference on Multimedia and Expo",
            "year": 2010
        },
        {
            "authors": [
                "WillemAMelder",
                "Khiet P Truong",
                "MartenDenUyl",
                "David AVan Leeuwen",
                "MarkA Neerincx",
                "Lodewijk R Loos",
                "B Stock Plum"
            ],
            "title": "Affective multimodal mirror: sensing and eliciting laughter",
            "venue": "In Proceedings of the international workshop on Human-centered multimedia",
            "year": 2007
        },
        {
            "authors": [
                "Maria Miceli",
                "Cristiano Castelfranchi"
            ],
            "title": "Expectancy and emotion",
            "year": 2014
        },
        {
            "authors": [
                "Yukie Nagai",
                "Minoru Asada"
            ],
            "title": "Predictive learning of sensorimotor information as a key for cognitive development",
            "venue": "In Proc. of the IROS 2015 Workshop on Sensorimotor Contingencies for Robotics,",
            "year": 2015
        },
        {
            "authors": [
                "Ryosuke Nakanishi",
                "Koji Inoue",
                "Shizuka Nakamura",
                "Katsuya Takanashi",
                "Tatsuya Kawahara"
            ],
            "title": "Generating fillers based on dialog act pairs for smooth turn-taking by humanoid robot",
            "venue": "In 9th International Workshop on Spoken Dialogue System",
            "year": 2019
        },
        {
            "authors": [
                "Costanza Navarretta"
            ],
            "title": "Mirroring facial expressions and emotions in dyadic conversations",
            "venue": "In Proceedings of the Tenth International Conference on Language Resources and Evaluation",
            "year": 2016
        },
        {
            "authors": [
                "Hiroaki Noguchi",
                "Yasuharu Den"
            ],
            "title": "Prosody-based detection of the context of backchannel responses",
            "venue": "In Fifth International Conference on Spoken Language Processing",
            "year": 1998
        },
        {
            "authors": [
                "Anja Philippsen",
                "Yukie Nagai"
            ],
            "title": "Understanding the cognitive mechanisms underlying autistic behavior: a recurrent neural network study",
            "venue": "Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)",
            "year": 2018
        },
        {
            "authors": [
                "Soujanya Poria",
                "Navonil Majumder",
                "Rada Mihalcea",
                "Eduard Hovy"
            ],
            "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
            "venue": "IEEE Access",
            "year": 2019
        },
        {
            "authors": [
                "Robert R Provine"
            ],
            "title": "Contagious laughter: Laughter is a sufficient stimulus for laughs and smiles",
            "venue": "Bulletin of the Psychonomic Society",
            "year": 1992
        },
        {
            "authors": [
                "Norbert Reithinger",
                "Ralf Engel",
                "Michael Kipp",
                "Martin Klesen"
            ],
            "title": "Predicting dialogue acts for a speech-to-speech translation system",
            "venue": "In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996,",
            "year": 1996
        },
        {
            "authors": [
                "Gabriel Skantze"
            ],
            "title": "Turn-taking in conversational systems and human-robot interaction: a review",
            "venue": "Computer Speech & Language",
            "year": 2021
        },
        {
            "authors": [
                "Andreas Stolcke",
                "Klaus Ries",
                "Noah Coccaro",
                "Elizabeth Shriberg",
                "Rebecca Bates",
                "Daniel Jurafsky",
                "Paul Taylor",
                "Rachel Martin",
                "Carol Van Ess-Dykema",
                "Marie Meteer"
            ],
            "title": "Dialogue act modeling for automatic tagging and recognition of conversational speech",
            "venue": "Computational linguistics 26,",
            "year": 2000
        },
        {
            "authors": [
                "Koji Tanaka",
                "Junya Takayama",
                "Yuki Arase"
            ],
            "title": "Dialogue-act prediction of future responses based on conversation history",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
            "year": 2019
        },
        {
            "authors": [
                "Khiet P Truong",
                "David A Van Leeuwen"
            ],
            "title": "Automatic discrimination between laughter and speech",
            "venue": "Speech Communication 49,",
            "year": 2007
        },
        {
            "authors": [
                "Nigel Ward"
            ],
            "title": "Using prosodic clues to decide when to produce back-channel utterances",
            "venue": "In Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996,",
            "year": 1996
        },
        {
            "authors": [
                "Nigel G Ward"
            ],
            "title": "Prosodic patterns in English conversation",
            "year": 2019
        },
        {
            "authors": [
                "Wenquan Wu",
                "Zhen Guo",
                "Xiangyang Zhou",
                "Hua Wu",
                "Xiyuan Zhang",
                "Rongzhong Lian",
                "Haifeng Wang"
            ],
            "title": "Proactive human-machine conversation with explicit conversation",
            "year": 2019
        },
        {
            "authors": [
                "Shujie Zhou",
                "Leimin Tian"
            ],
            "title": "Would you help a sad robot? Influence of robots\u2019 emotional expressions on human-multi-robot collaboration",
            "venue": "In 2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "CCS CONCEPTS \u2022 Human-centered computing \u2192 Human computer interaction (HCI).\nKEYWORDS emotion, dialogue act, interaction, laughter, spoken dialogue system\n\u2217Part of this work was done while working on the JST ERATO ISHIGURO Symbiotic Human-Robot Interaction Project at Kawahara Lab at Kyoto University. \u2020Authors contributed equally and are listed in alphabetical order.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CHI EA \u201923, April 23\u201328, 2023, Hamburg, Germany \u00a9 2023 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9422-2/23/04. https://doi.org/10.1145/3544549.3585869\nACM Reference Format: Yuanchao Li, Koji Inoue, Leimin Tian, Changzeng Fu, Carlos Ishi, Hiroshi Ishiguro, Tatsuya Kawahara, and Catherine Lai. 2023. I Know Your Feelings Before You Do: Predicting Future Affective Reactions in Human-Computer Dialogue. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (CHI EA \u201923), April 23\u201328, 2023, Hamburg, Germany. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3544549.3585869"
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Spoken dialogue is a common and natural form of communication in human social interaction. Thus, we are witnessing a growing interest in advancing Spoken Dialogue Systems (SDSs) capable of delivering task-specific services, both in research and in commercial applications. For example, voice assistants, such as Amazon Echo or Google Home, are widely used for information queries in people\u2019s daily lives. Meanwhile, embodied SDSs, for instance the humanoid robot Pepper, have been deployed to enhance human workforce in application scenarios such as hospitality and elderly care. The majority of these SDSs, however, converse passively and utter words more as a matter of response than asking questions or leading the conversation on their own initiative. Furthermore, many existing SDSs are built on top of natural language processing and generation models developed with written text data, overlooking the rich conversational and affective phenomena unique to spoken dialogue, such as non-verbal vocalizations and affective bursts. As a consequence, current SDSs are often perceived as stagnant and mechanical. To mitigate this issue, researchers have been investigating various dialogue-specific behaviors, such as turn-taking, backchanneling, and laughter, as they have been found to serve important functions in conversation, including the marking of prominence, syntactic disambiguation, attitudinal reactions, uncertainty, and topic shifts [18, 27, 44, 45].\nStudies on these dialogue behaviors usually include both detection and synthesis tasks. The detection task aims at predicting user behaviors from the received signals, while the synthesis task generates system behaviors. For the detection task, acoustic features such\nar X\niv :2\n30 3.\n00 14\n6v 3\n[c s.H\nC ]\nas Mel Frequency Cepstral Coefficients (MFCCs) and prosodic features such as pitch, energy, and duration of pause-bounded phrases are typically used as cues for the detection of these human-like behaviors [14, 34, 40]. With the recent advance of deep learning techniques, such detection has started to become more robust and is being applied in real-world applications [15]. Compared to the detection task, the synthesis task is more challenging for two major reasons: 1) It depends on the accuracy of the detection task. If the user behaviors are classified incorrectly at the very beginning, the process for system behavior synthesis could become totally meaningless or even harmful to user engagement. For example, if the system detects turn-yielding cues but the user is actually holding the turn, then the user\u2019s speech will be interrupted by the system. 2) Unlike the detection task, where audio alone can achieve reasonable performance (although lexical cues often help), the synthesis task requires suitable generation of both acoustic and lexical behaviors. When synthesizing fillers and backchannels, the meanings largely depend on their morphological forms [15, 21, 22, 32]. To address this challenge, the majority of the synthesis task is still performed following a rule-based method. Take backchanneling as an example: First, the user\u2019s speech is converted into a sequence of words by Automatic Speech Recognition (ASR). Next, the focus word of the sequence is extracted. If the focus word matches any entries in the pre-built query-response database, the system generates a backchannel based on the query-response pair. Otherwise, the system generates a short backchannel, such as \u201cYeah\u201d to indicate it is listening [21].\nSuch a detection-rule-synthesis process is a widely adopted operation in SDSs, yet it has several limitations. First, this can lead to delayed responses due to the time taken (often correlates to the duration of the input speech) to process the user\u2019s speech and synthesize suitable responses. Such delay can accumulate when there are several detection components (e.g., dialogue act recognition, emotion recognition, and turn-taking detection). Second, previous research in linguistics and communication theories suggests that human listeners have the ability to anticipate the interlocutor\u2019s behavior in real time based on the dialogue context and history [8, 37, 42], and such predictive power is core to human brains [2, 31, 35]. Furthermore, human listeners can start planning their responses or even talking before the interlocutor finishes, resulting in cooperative overlaps or appropriate interruptions that are key to establishing rapport and sympathy [10, 43]. Current SDSs, however, are incapable of exhibiting such anticipatory and collaborative dialogue behaviors.\nTo alleviate this problem, recent SDS research has started to investigate the feasibility of enabling the system to actively lead the conversation instead of behaving as a passive follower. Wu et al. [46] proposed a knowledge graph that sequentially changed the discussion topics following a given conversation goal to keep the dialogue as natural and engaging as possible. Besides, Lala et al. [17] and Li et al. [23] proposed attentive and proactive listening systems. These systems have a proactive initiator that can make the dialogue systems behave somewhat actively to ask a follow-up question related to the most recent topic or start a new topic. Moreover, proactive behaviors have proven helpful in rendering a more competent and reliable system that could ultimately lead to a more trustworthy interaction partner [16]. Nevertheless, these functions\ndealt with only the linguistic aspect (e.g., spoken content) without considering the paralinguistic aspect (e.g., affective expressions).\nTherefore, we are motivated to build an anticipatory SDS by endowing it with the ability to predict the future affective reactions of the user. Inspired by findings in cognitive science that humans can anticipate certain future events, including affective ones [5, 7, 30], we propose an architecture that allows the SDS to mimic this human ability to predict affective reactions in the user\u2019s next turn based on its current turn. We consider two scenarios: speech and laughter, which are distinguished as two acoustic events (though co-occurrence also exists) [39, 43]. In the speech scenario, we look at the prediction of future emotions (valence and arousal). In the laughter scenario, we investigate the prediction of future laughter (occurrence and type). Moreover, we propose a self-correction and adaptation function that updates the future prediction model using the outputs of a recognition model on the user\u2019s speech. When the future prediction model has low confidence in its outputs, the recognition model generates outputs using the user\u2019s speech collected so far as ground truths to correct the prediction model. We conducted a preliminary analysis on human-robot dialogue, which confirms the feasibility of implementing the proposed anticipatory and adaptive architecture."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "A dialogue is made up of a series of utterances, with the previous response determined by the history information [39]. Previous studies have found that the emotions of the interlocutors have a mapping relationship in human-human conversation. In persuasion dialogue, Acosta and Ward [1] discovered that the listener\u2019s dimensional emotion (valence, arousal, dominance) can be predicted from the emotion expressed in the immediately preceding speaker\u2019s utterance. Majumder et al. [26] demonstrated that in dyadic conversation, the listener\u2019s discrete emotion can be predicted by the context given by the preceding utterances and the emotion expressed. Such relationships have been considered when designing SDSs: by having a virtual or embodied agent mirror a user\u2019s emotions, i.e., the \u201caffective mirror\u201d [36].\nFurthermore, emotion is affected by other aspects of dialogue, such as the Dialogue Act (DA), which represents the communicative function of an utterance. Despite the fact that there is a mutual influence between emotion and DA in conversations [6], understanding of such relationships is limited. However, a recent study has found that there is a clear temporal causal relationship between DAs and emotions, providing specific emotion-DA and DA-emotion pairs. For instance,Happiness of the speaker\u2019s utterance has a great chance of causing the DA of appreciation in the following response. The DA of signal-non-understanding and backchannel-question-form usually raise surprise [4]. Apart from emotions expressed through speech, affective bursts, especially laughter, are paralinguistic events that occur frequently in spontaneous dialogues [29, 43]. Laughter usually shows a contagious phenomenon that hearing laughter from others is known to trigger laughter in ourselves [38]. In dialogues, such a contagion is called \u201cshared laughter\u201d [9, 33]. A recent work has developed a shared laughter system that can decide whether to generate social laughter, mirthful laughter, or not laugh, depending on the detection of user laughter [12].\nBased on these novel findings and developments, we can expect to advance SDSs by applying the causal relationships between emotion and DA, as well as the laughter mapping relationship, i.e., predicting the affective reactions of the upcoming user utterance to prepare its next response towards realizing proactive and affective behaviors. To the best of our knowledge, this is the first work to propose such an anticipatory SDS framework, which has the potential to lead to human-like affective dialogue capabilities in SDSs."
        },
        {
            "heading": "3 PROPOSED ARCHITECTURE FOR AN ANTICIPATORY SDS",
            "text": ""
        },
        {
            "heading": "3.1 The Speech Scenario",
            "text": "In the speech scenario, as shown in Fig. 1a and Algorithm 1, there are three major components in the proposed architecture: emotion prediction, emotion recognition, and self-correction. The emotion prediction model works as a future prediction function by taking the emotion and DA of the system\u2019s current turn as input and outputting an emotion as the prediction of the user\u2019s emotion in the next turn. When the prediction probability is high (i.e., the system is confident about its prediction), the system will take it as the user\u2019s future emotion and use it to help plan its next turn before the user speaks. Otherwise, the emotion recognition model starts to work by detecting the user\u2019s emotion once they finish speaking. The recognition result will be taken as ground truth to fine-tune the emotion prediction model if it has low confidence in its outputs and update the parameters of the prediction model via the self-correction function.\nThe emotion prediction model can be built by drawing on prior knowledge of emotion-emotion mapping [21] and the DA-emotion causal relationship [4].We conducted a preliminary analysis demonstrating that in human-robot dialogue, the users indeed mimic the robot\u2019s emotion, but the correlations show different patterns in the arousal and valence dimensions (described in Sec. 4.2). Besides, there is limited work on the causal relationship between DAs and emotions. Thus, we aim to expand this research to formulate the emotion prediction problem as a logistic regression model:\n\ud835\udc38\ud835\udc40\ud835\udc5d\ud835\udc5f\ud835\udc51 = \ud835\udc3f\ud835\udc45(\ud835\udc38\ud835\udc40\ud835\udc50\ud835\udc62\ud835\udc5f , \ud835\udc37\ud835\udc34\ud835\udc50\ud835\udc62\ud835\udc5f ) (1)\nwhere the \ud835\udc38\ud835\udc40\ud835\udc5d\ud835\udc5f\ud835\udc51 is the predicted user emotion in the next turn, and the \ud835\udc38\ud835\udc40\ud835\udc50\ud835\udc62\ud835\udc5f and \ud835\udc37\ud835\udc34\ud835\udc50\ud835\udc62\ud835\udc5f are the system\u2019s emotion and DA in the current turn, respectively. The regression model can be pre-trained on suitable corpora before being incorporated in the proposed architecture.\nCompared to emotion prediction, emotion recognition has been well studied, but the majority of these works did not take into consideration the real-life environment (e.g., noise), which is a long-standing problem for SDSs [19]. Hence, we propose to incorporate text as additional information to tackle this problem. The text features are extracted from transcripts generated by the ASR component in SDSs, so the emotion recognition model needs to be robust to ASR errors. In a recent study, we developed a hierarchical cross-attention fusion model using both audio and text features for ASR error-robust emotion recognition [20], which can be adopted in the proposed architecture. When using ASR transcripts, this model performed similarly to when using ground-truth transcripts.\nAlgorithm 1: The speech scenario Input :System\u2019s current emotion \ud835\udc38\ud835\udc40\ud835\udc50\ud835\udc62\ud835\udc5f and dialogue act \ud835\udc37\ud835\udc34\ud835\udc50\ud835\udc62\ud835\udc5f ;\nPredicted user emotion \ud835\udc38\ud835\udc40\ud835\udc5d\ud835\udc5f\ud835\udc51 ; Recognized user emotion \ud835\udc38\ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc50 ; Pre-defined probability threshold \ud835\udc43\ud835\udc61\u210e\ud835\udc5f1\n1 repeat 2 for system utterance do 3 Generate \ud835\udc38\ud835\udc40\ud835\udc5d\ud835\udc5f\ud835\udc51 from \ud835\udc38\ud835\udc40\ud835\udc50\ud835\udc62\ud835\udc5f and \ud835\udc37\ud835\udc34\ud835\udc50\ud835\udc62\ud835\udc5f 4 if Probability of \ud835\udc38\ud835\udc40\ud835\udc5d\ud835\udc5f\ud835\udc51 \u2265 \ud835\udc43\ud835\udc61\u210e\ud835\udc5f1 then 5 Action // E.g., planning the system\u2019s next turn. 6 else 7 Generate \ud835\udc38\ud835\udc40\ud835\udc5f\ud835\udc52\ud835\udc50 8 Update the emotion prediction model 9 end if"
        },
        {
            "heading": "10 end for",
            "text": "11 until no system utterance\nThe self-correction component follows the rule that when the confidence (i.e., probability) of the emotion prediction result is low, it starts to work by updating (i.e., fine-tuning) the prediction model using the outputs from the emotion recognition model as ground truths. Such a setting allows the emotion prediction component to dynamically adapt to the emotional expression habits of the human participants as the dialogue progresses. Like our human ability to make predictions, the longer the dialogue goes on, the more accurate the prediction becomes.\nAlgorithm 2: The laughter scenario Input :System\u2019s current laughter \ud835\udc3f\ud835\udc34\ud835\udc50\ud835\udc62\ud835\udc5f ; Predicted user laughter\n\ud835\udc3f\ud835\udc34\ud835\udc5d\ud835\udc5f\ud835\udc51 ; Recognized user laughter \ud835\udc3f\ud835\udc34\ud835\udc5f\ud835\udc52\ud835\udc50 ; Pre-defined probability threshold \ud835\udc43\ud835\udc61\u210e\ud835\udc5f2\n1 repeat 2 for system laughter do 3 Generate \ud835\udc3f\ud835\udc34\ud835\udc5d\ud835\udc5f\ud835\udc51 from \ud835\udc3f\ud835\udc34\ud835\udc50\ud835\udc62\ud835\udc5f 4 if Probability of \ud835\udc3f\ud835\udc34\ud835\udc5d\ud835\udc5f\ud835\udc51 \u2265 \ud835\udc43\ud835\udc61\u210e\ud835\udc5f2 then 5 Action // E.g., planning the system\u2019s next turn. 6 else 7 Generate \ud835\udc3f\ud835\udc34\ud835\udc5f\ud835\udc52\ud835\udc50 8 Update the laughter prediction model 9 end if"
        },
        {
            "heading": "10 end for",
            "text": "11 until no system laughter"
        },
        {
            "heading": "3.2 The Laughter Scenario",
            "text": "Similar to the speech scenario, there are three major components in the laughter scenario, as shown in Fig. 1b and Algorithm 2: laughter prediction, laughter detection, and self-correction. Based on the system\u2019s laughter behavior in the current turn and the shared laughter relationship [12], the laughter prediction model predicts the occurrence and type of the user\u2019s laughter in the next turn. If the prediction probability is low, the laughter detection will work by detecting the type of laughter from the user\u2019s response and updating the laughter prediction model via the self-correction function.\nThe laughter prediction model can be built by drawing on recent work that detects the occurrence and type of the user\u2019s laughter to generate the system\u2019s laughter [12]. We can use this finding reversely by predicting the occurrence and type of the user\u2019s laughter based on the system\u2019s laughter, which is easy to manipulate in SDSs. The laughter detection model takes acoustic features (e.g., MFCCs) and prosodic features (e.g., pitch and power) as input and feeds them to a stacked recurrent neural network. The recurrent neural network will be implemented using the bi-directional gated recurrent unit, whose feed-forward processing can work in real time, which is essential for SDSs. The self-correction follows the same idea as the speech scenario by updating (fine-tuning) the prediction model when its prediction probability is low."
        },
        {
            "heading": "4 PRELIMINARY ANALYSIS",
            "text": ""
        },
        {
            "heading": "4.1 Corpora Description",
            "text": "Although there are existing emotional dialogue corpora, most of them are not suitable for the purpose of this work. For example, IEMOCAP [3] contains spontaneous dialogue sessions, yet the improvisation is limited to a set of hypothetical scenarios, which is different from open domain natural dialogue. SEMAINE [28] collected human-agent emotional dialogues, but the human users were not permitted to ask questions. The persuasive dialogue corpus used in [1] is not publicly available. Besides, none of the existing corpora contain sufficient occurrences and variations of laughter.\nTherefore, we used two corpora from the JST ERATO ISHIGURO Symbiotic Human-Robot Interaction Project1 that contain spontaneous dialogue and rich laughter. The corpus for the speech 1https://www.jst.go.jp/erato/ishiguro/en/index.html\nscenario consists of spontaneous dialogue between human participants and a teleoperated humanoid robot ERICA [11]. During data collection, ERICA was teleoperated by a human operator in a Wizard of Oz (WoZ) manner. The participants were students ranging from 18 to 22 years old. Each dialogue session contained two phases and lasted around 15 minutes, and six sessions were conducted. In the first phase, ERICA introduced herself and talked with the participants about their lives, hobbies, and future plans. In the second phase, they talked about robots, especially about ERICA herself. During the dialogue, the robot led the dialogue, and the participant acted as a \u201cfollower\u201d, which is the scenario we hope to apply our proposed architecture to. The emotions are annotated as Valence: -3 (extremely negative) to +3 (extremely positive), and Arousal: -3 (extremely passive) to +3 (extremely active).\nThe corpus for the laughter scenario was collected under almost identical conditions, except that the aim was to get the teleoperators and the participants to know each other quickly [13]. As a result, they behaved friendly by laughing frequently during such speed dating. Each dialogue lasts 10 to 15 minutes, and 82 dialogue sessions were conducted. The laughter was annotated as Social laughter, Mirthful laughter, and No laughter."
        },
        {
            "heading": "4.2 Exploring the Feasibility of Emotion Prediction",
            "text": "To explore the relationship between the human participant\u2019s and the robot\u2019s emotions in dialogue, we analyzed the human-robot dialogue sessions from the first corpus. Our preliminary analysis found similar patterns in all six sessions. Because different sessions have different durations and different numbers of utterances, we could not average the emotion labels over the six sessions. Thus, we report one session containing 123 utterance pairs as an illustrative example to discuss our findings.\nThe valence patterns of the robot and participant are shown in Fig. 2. The dialogue can be roughly divided into three phases.We can see that in the spontaneous dialogue phase, the human participant\u2019s valence does mimic the robot\u2019s to a large extent, especially when the robot changes its valence significantly (e.g., from -2 to +2 and from\n+1 to -3). Also, during the majority of the time in the spontaneous dialogue phase, the human valence is very close to its previous robot valence, showing a mimicry relationship. Note that, the human participant did not express extremely high valence (i.e., +3). This could be due to individual differences such as cultural background, personality, or expectation of the robot as a novel stimulus. The Pearson\u2019s Correlation Coefficient (PCC) of the valence pairs is 0.54 in the spontaneous dialogue phase, i.e., there is a moderate positive relationship between the human and robot valences. This suggests that it is possible to implement a mapping function between the human\u2019s valence and the valence expressed by the SDS. Note that, what we need to investigate is the correlation between emotions for a mapping pattern, not the exact values for classification, so we do not report accuracy or F1 scores. Even if the human participant\u2019s emotion values are completely different from the robot\u2019s, but highly correlated, e.g., [-3, -2, 1, 0, 1] and [-2, -1, 0, 1, 2], it still shows that the participant\u2019s emotion follows the robot, which could be used as a basis for implementing our proposed anticipatory SDS.\nInterestingly, during the ice-breaking and ending phases, the human\u2019s valence hardly resembles the robot\u2019s valence with a PCC of 0.09 in the ice-breaking phase and 0.07 in the ending phase. After examining the video recording, we found that both parties were performing the greeting and leave-taking dialogue acts that they consider to be socially appropriate, instead of mimicking their\ndialogue partner\u2019s expressions. That is, emotions in the dialogue were influenced by DAs. Thus, we annotated DAs following the categorization by Stolcke et al. [41] to understand how they impact emotion mimicry.\nAn annotation excerpt from the ice-breaking phase is shown in Table 1. When the robot asked a \u201cWh-question\u201d and the participant responded with a \u201cstatement\u201d, both valence and arousal display mimicry. However, when the robot expressed \u201csignal-nonunderstanding\u201d and the participant responded \u201creject\u201d, the valence has an obvious drop, but the arousal barely changes. This shows that unlike valence, arousal is less influenced by DAs.\nIn terms of arousal, we found significant mimicry in the participant\u2019s arousal toward the robot\u2019s (figure omitted for brevity). The PCC of the arousal pairs is 0.78 over the whole dialogue session, including the ice-breaking and ending phases. This demonstrates that the arousal behind the future response may be relatively easy to predict based on the current utterance.\nOur preliminary analysis indicates that it is feasible to predict a future emotion from the current emotion and DA for implementing the prediction component of our proposed architecture. We also found that valence is related to contextual information, such as DAs and personal factors of the interlocutor. We summarized a set of observations on the relationship between dialogue context and the valence of the robot and of the human participant in Table 2 and\nTable 3, respectively. We expect that these observations can contribute to the future implementation of the proposed anticipatory SDS and to the broader research community."
        },
        {
            "heading": "4.3 Exploring the Feasibility of Laughter Prediction",
            "text": "We analyzed a publicized dialogue demo that was built upon the second corpus with the robot\u2019s dialogue system replacing the teleoperator2. The results are presented in Table 4. As shown here, the robot generated suitable laughter behaviors based on the acoustic features of the previous user laughter. When the user\u2019s laughter had a flat pitch and moderate power, the robot responded with social laughter. When the user\u2019s laughter had a long duration and was jittery and shimmery, the robot responded with mirthful laughter. The robot also \u201cunderstood\u201d not to laugh when the user laughed only to relieve embarrassment. Based on previous research on the contagious phenomenon of laughter [38], we aim to expand on the shared laughter research by adjusting the acoustic features of the laughter generated by the SDS, allowing it to predict the future laughter behaviors of the user in response to the system\u2019s laughter."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "Our preliminary analysis of human-robot dialogue suggests that it is feasible to implement an anticipatory SDS that predicts the emotion and laughter of the user in the next turn using the current turn of the system. However, there remain open challenges in implementing the proposed architecture, which we will discuss in this section. Further, we will discuss potential application scenarios for the proposed architecture."
        },
        {
            "heading": "5.1 Implementation Challenges",
            "text": "The proposed architecture relies on accurate and robust recognition of emotions from speech, which remains an open challenge due to the variability in speech and emotion expression. The prediction component in the proposed architecture may not be applicable to all user turns, as humans can express emotions and laughter arbitrarily without considering the system\u2019s expression. In this case, the architecture can be \u201cdowngraded\u201d with only emotion recognition and laughter detection working and the prediction and self-correction components frozen. Further, noise in real-world applications of SDSs can reduce the reliability of both the emotion recognition and laughter detection models. Therefore, we plan to 2https://www.youtube.com/watch?v=6tMiWog4l00\nincorporate other communicative modalities (e.g., text and vision) to further improve the proposed architecture [24]."
        },
        {
            "heading": "5.2 Potential Application Scenarios",
            "text": "In social and open-domain dialogue, an SDS with our proposed architecture can generate appropriate conversational and affective behaviors, such as a backchannel \u201cYeah\u201d or laughter, in real time, instead of having delayed turn-taking that may interrupt the user\u2019s next turn. Further, it is especially useful in scenarios where the SDS is expected to take initiatives and lead the conversation, such as in healthcare and education. For example, the SDS can adjust its generated emotions and DAs to support a user\u2019s emotion regulation process by eliciting certain emotions in people with depression or autism [25]. In education, the SDS can express emotions during collaborative problem solving with children to increase their participation in the learning activities and resulting learning outcomes, as Zhou and Tian [47] found that when the robots exhibited emotional expressions, participants were more likely to collaborate with them and achieve task success faster."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this work, we propose an anticipatory SDS architecture that predicts the affective reactions of the user in a future turn using its behaviors in the current turn. We investigate its viability in both speech and laughter scenarios. Based on preliminary analysis of human-robot dialogue, we demonstrated that: 1) The emotion of a future turn can be predicted from the current turn. The arousal dimension has a significant mimicry relationship, in which the human user\u2019s arousal follows the robot\u2019s arousal during dialogue. The valence, however, is also related to the previous DA. 2) The laughter behavior of the human user in a future turn has a mapping pattern with the laughter behavior of the robot in the current turn. The preliminary analysis paves the way for our future research. In particular, we aim to identify the relationship between current DA and future emotion, as well as current laughter and future laughter to implement the emotion prediction model and the laughter prediction model of the proposed architecture. Moreover, we plan to include history information and dialogue context beyond the current turn to improve the prediction accuracy. Achieving anticipatory SDSs requires every individual component to be accurate and robust, as well as a seamless collaboration between the components. Thus, in the future, we plan to implement the complete architecture and evaluate its outcomes in user studies."
        }
    ],
    "title": "I Know Your Feelings Before You Do: Predicting Future Affective Reactions in Human-Computer Dialogue",
    "year": 2023
}